{
  "date": "2025-11-30",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-30 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„è€æœ‹å‹ï¼Œä¸“æ³¨äº AI å‰æ²¿çš„ç ”ç©¶å‘˜ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ä»Šå¤©ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹æŠ—æ€§ä¸åæ€ã€‚æˆ‘ä»¬åœ¨ **æ¨ç†æ—¶çš„è®¡ç®—æ‰©å±• (Test-time Scaling)** ä¸Šçœ‹åˆ°äº†æ–°çªç ´ï¼ˆä¸ä»…æ˜¯å¢åŠ è®¡ç®—é‡ï¼Œæ›´æ˜¯æ¨¡å¼è°ƒèŠ‚ï¼‰ï¼ŒåŒæ—¶ï¼Œ**Agent ä»¿çœŸç¯å¢ƒ**è¿æ¥äº†åŸºäº UE5 çš„å¤§æ€å™¨ SimWorldã€‚å¦ä¸€æ–¹é¢ï¼Œå¤§é‡ç ”ç©¶å¼€å§‹æ­ç¤º **VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰çš„è„†å¼±æ€§**â€”â€”ä»æå°‘ç¥ç»å…ƒçš„ç¾éš¾æ€§é—å¿˜åˆ°â€œç¤¾äº¤èƒ½åŠ›â€çš„é€€åŒ–ï¼Œå†åˆ°æ¦‚å¿µçº§åé—¨æ”»å‡»ï¼Œå®‰å…¨ä¸é²æ£’æ€§å†æ¬¡æ•²å“è­¦é’Ÿã€‚\n\nä¸‹é¢æˆ‘ä»¬è¿›å…¥æ­£é¢˜ï¼Œç²¾é€‰äº†ä»Šæ—¥æœ€å€¼å¾—å…³æ³¨çš„è®ºæ–‡ã€‚\n\n---\n\n### ğŸš€ æ¨ç†æ‰©å±•ä¸ç®—æ³•é€»è¾‘ (Reasoning & Scaling)\n\n**1. [æ¨è] æ¨¡å¼è°ƒèŠ‚è§£é”å“è¶Šçš„æµ‹è¯•æ—¶æ‰©å±•èƒ½åŠ›**\n**# title:** Mode-Conditioning Unlocks Superior Test-Time Scaling\n**# æ ¸å¿ƒå‘ç°:** å¹¶è¡Œé‡‡æ · (Parallel sampling) æ˜¯æå‡æ¨ç†èƒ½åŠ›çš„é‡è¦æ‰‹æ®µï¼Œä½†å¸¸å—é™äºâ€œå¤šæ ·æ€§å´©æºƒâ€ (diversity collapse)ã€‚ä½œè€…æå‡ºäº† **Mode-Conditioning (ModC)** æ¡†æ¶ï¼Œæ˜¾å¼åœ°å°†æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºåˆ†é…ç»™ä¸åŒçš„â€œæ¨ç†æ¨¡å¼â€ (reasoning modes)ã€‚\n**# äº®ç‚¹:** åœ¨ Qwen2.5-7B ä¸Šå¾®è°ƒ ModCï¼Œæ•ˆç‡æ¯”æ ‡å‡†è®­ç»ƒé«˜ 4 å€ï¼Œä¸”åœ¨ OpenThoughts ä¸Šæå‡äº† Pass@k çš„ä¸Šé™ã€‚è¿™è¯æ˜äº†æ ‡å‡†è®­ç»ƒæœªèƒ½å……åˆ†åˆ©ç”¨æ•°æ®çš„å¤šæ ·æ€§ï¼Œè€Œ ModC æ˜¯ä¸€ç§è§£é” Test-time scaling çš„æœ‰æ•ˆæ‰‹æ®µã€‚\n\n**2. å½¢å¼åŒ–éªŒè¯çš„ç†è®ºä¿éšœï¼š4/Î´ ç•Œé™**\n**# title:** The 4/Î´ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee\n**# æ ¸å¿ƒå‘ç°:** å°†å½¢å¼åŒ–éªŒè¯å·¥å…·ä¸ LLM ç»“åˆæ˜¯è¶‹åŠ¿ï¼Œä½†å¾€å¾€ç¼ºä¹ç†è®ºä¿éšœã€‚æœ¬æ–‡æå‡ºäº† **LLM-Verifier æ”¶æ•›å®šç†**ï¼Œè¯æ˜äº†åœ¨å¤šé˜¶æ®µéªŒè¯ç®¡é“ä¸­ï¼Œåªè¦æ¯ä¸ªé˜¶æ®µæˆåŠŸæ¦‚ç‡ $Î´ > 0$ï¼Œç³»ç»Ÿå‡ ä¹å¿…ç„¶èƒ½è¾¾åˆ°â€œå·²éªŒè¯â€çŠ¶æ€ï¼Œä¸”å»¶è¿Ÿç•Œé™ä¸º $4/Î´$ã€‚\n**# äº®ç‚¹:** è¿™å°† LLM ç¼–ç¨‹ä»â€œå¯å‘å¼çŒœæµ‹â€æ¨å‘äº†â€œå¯é¢„æµ‹çš„å·¥ç¨‹æ¶æ„â€ï¼Œå¯¹å®‰å…¨å…³é”®å‹è½¯ä»¶å¼€å‘æ„ä¹‰é‡å¤§ã€‚\n\n**3. ç®—æ³•æ¨ç†çš„åˆ†æ”¯ç½‘ç»œ**\n**# title:** Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning\n**# æ ¸å¿ƒå‘ç°:** ä¸åŒçš„ç®—æ³•ä»»åŠ¡æ­¥éª¤å·®å¼‚å·¨å¤§ï¼Œå¯¼è‡´å¤šä»»åŠ¡å­¦ä¹ æ—¶å‡ºç°è´Ÿè¿ç§»ã€‚ä½œè€…æå‡ºäº† **Branching Neural Networks (åˆ†æ”¯ç¥ç»ç½‘ç»œ)** å’Œ AutoBRANE ç®—æ³•ï¼Œè‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜çš„ä»»åŠ¡èšç±»å’Œåˆ†æ”¯ç»“æ„ã€‚\n**# äº®ç‚¹:** åœ¨ CLRS åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥æ›´å°‘çš„è¿è¡Œæ—¶é—´ï¼ˆå‡å°‘48%ï¼‰å’Œå†…å­˜ï¼ˆå‡å°‘26%ï¼‰å‡»è´¥äº†æœ€å¼ºçš„å¤šä»»åŠ¡ GNN åŸºçº¿ã€‚\n\n---\n\n### ğŸŒ å…·èº«æ™ºèƒ½ä¸ Agent ä»¿çœŸ (Embodied AI & Agents)\n\n**4. [é‡ç£…] SimWorld: åŸºäºè™šå¹»å¼•æ“5çš„å¼€æ”¾å¼çœŸå®ä¸–ç•Œ Agent æ¨¡æ‹Ÿå™¨**\n**# title:** SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds\n**# æ ¸å¿ƒå‘ç°:** ç°æœ‰çš„æ¨¡æ‹Ÿå™¨ç‰©ç†å’Œç¤¾äº¤è§„åˆ™å¤ªç®€åŒ–ã€‚**SimWorld** åŸºäº UE5 æ„å»ºï¼Œæ”¯æŒç‰©ç†å’Œç¤¾äº¤åŠ¨åŠ›å­¦ï¼Œä¸“ä¸º LLM/VLM Agent è®¾è®¡ã€‚\n**# äº®ç‚¹:** å®ƒå…è®¸ Agent åœ¨é€¼çœŸçš„ç¯å¢ƒä¸­è¿›è¡Œé•¿ç¨‹ä»»åŠ¡ï¼ˆå¦‚é€è´§ã€å•†ä¸šç«äº‰ï¼‰ã€‚æµ‹è¯•å‘ç° GPT-4o å’Œ Claude-3.5 ç­‰æ¨¡å‹åœ¨ç­–ç•¥åˆä½œä¸ç«äº‰ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„æ¨ç†æ¨¡å¼å·®å¼‚ã€‚\n\n**5. ç§‘å­¦ä»£ç  Agent çš„â€œç‰©ç†é“¾â€çº¦æŸ**\n**# title:** Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis\n**# æ ¸å¿ƒå‘ç°:** ç›®å‰çš„ Agent åœ¨å†™ç§‘å­¦è®¡ç®—ä»£ç æ—¶ç»å¸¸äº§ç”Ÿâ€œå¹»è§‰â€æˆ–ç‰©ç†ä¸Šä¸è‡ªæ´½çš„ä»£ç ã€‚ä½œè€…æå‡ºäº† **Chain of Unit-Physics (CoUP)**ï¼Œåˆ©ç”¨å•å…ƒç‰©ç†æµ‹è¯• (unit-physics tests) æ¥çº¦æŸä»£ç ç”Ÿæˆã€‚\n**# äº®ç‚¹:** åœ¨ç‡ƒçƒ§å­¦ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨ 5-6 æ¬¡è¿­ä»£å†…æ”¶æ•›ï¼Œè¾¾åˆ°äººç±»ä¸“å®¶çº§ç²¾åº¦ï¼Œè§£å†³äº†é—­æºæ¨¡å‹å•çº¯ä¾èµ–å·¥å…·è°ƒç”¨æ— æ³•è§£å†³çš„ç‰©ç†ä¸€è‡´æ€§é—®é¢˜ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰è¯­è¨€æ¨¡å‹ï¼šæ•ˆç‡ä¸è„†å¼±æ€§ (VLM Efficiency & Vulnerability)\n\n**6. æç®€ç¥ç»å…ƒæ¶ˆèå¼•å‘ VLM è¯­è¨€æ ¸å¿ƒçš„ç¾éš¾æ€§å´©æºƒ**\n**# title:** Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models\n**# æ ¸å¿ƒå‘ç°:** è¿™æ˜¯ä¸€ä¸ªä»¤äººæ‹…å¿§çš„å®‰å…¨å‘ç°ã€‚ä½œè€…å‘ç°ï¼Œä»…å±è”½ VLM è¯­è¨€æ¨¡å‹å‰é¦ˆç½‘ç»œä¸­**æå°‘é‡çš„ç¥ç»å…ƒ**ï¼ˆæç«¯æƒ…å†µä¸‹ä»… 4 ä¸ªï¼‰ï¼Œå°±ä¼šå¼•å‘ç¾éš¾æ€§çš„èƒ½åŠ›å´©æºƒã€‚\n**# äº®ç‚¹:** è¿™ç§å´©æºƒä¸»è¦å‘ç”Ÿåœ¨è¯­è¨€éƒ¨åˆ†ï¼Œä¸”å‘ˆç°â€œä¸¤é˜¶æ®µâ€æ¨¡å¼ï¼šå…ˆæ˜¯è¡¨è¾¾èƒ½åŠ›ä¸‹é™ï¼Œç„¶åçªç„¶å®Œå…¨å´©æºƒã€‚è¿™ä¸º VLM çš„å®‰å…¨æ€§ç ”ç©¶æä¾›äº†æ–°è§†è§’ã€‚\n\n**7. SocialFusion: è§£å†³é¢„è®­ç»ƒ VLM ä¸­çš„â€œç¤¾äº¤é€€åŒ–â€**\n**# title:** SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models\n**# æ ¸å¿ƒå‘ç°:** å¼ºå¤§çš„ VLM åœ¨é€šç”¨èƒ½åŠ›ä¸Šå¾ˆå¼ºï¼Œä½†åœ¨**ç¤¾ä¼šæ„ŸçŸ¥ä»»åŠ¡**ä¸Šå´è¡¨ç°å‡ºâ€œè´Ÿè¿ç§»â€ï¼Œä½œè€…ç§°ä¹‹ä¸ºâ€œç¤¾äº¤é€€åŒ–â€ (social degradation)ã€‚è¿™æºäºé¢„è®­ç»ƒç ´åäº†è§†è§‰ç¼–ç å™¨è¡¨ç¤ºç»†å¾®ç¤¾äº¤çº¿ç´¢çš„èƒ½åŠ›ã€‚\n**# äº®ç‚¹:** æå‡ºçš„ SocialFusion æ¡†æ¶é€šè¿‡æœ€å°åŒ–è¿æ¥å†»ç»“çš„è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ï¼Œæ¢å¤äº†è¿™ç§èƒ½åŠ›ã€‚\n\n**8. 1-bit é‡åŒ–ï¼šHBLLM**\n**# title:** HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs\n**# æ ¸å¿ƒå‘ç°:** æè‡´çš„å‹ç¼©ï¼åˆ©ç”¨ Haar å°æ³¢å˜æ¢å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œå®ç°äº†**1-bit åè®­ç»ƒé‡åŒ–**ã€‚\n**# äº®ç‚¹:** åœ¨ LLaMA2-13B ä¸Šï¼Œå¹³å‡æƒé‡å­˜å‚¨ä»…éœ€ 1.08 bitsï¼Œå›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ä»ä¿æŒåœ¨ 6.71ã€‚\n\n**9. æ¦‚å¿µå¼•å¯¼çš„ VLM åé—¨æ”»å‡»**\n**# title:** Concept-Guided Backdoor Attack on Vision Language Models\n**# æ ¸å¿ƒå‘ç°:** ä¼ ç»Ÿçš„åé—¨æ”»å‡»ä¾èµ–åƒç´ çº§è§¦å‘å™¨ã€‚æœ¬æ–‡æå‡ºäº†**æ¦‚å¿µçº§ (Concept-level)** æ”»å‡»ï¼Œåªè¦å›¾ç‰‡ä¸­å‡ºç°ç‰¹å®šæ¦‚å¿µï¼ˆå¦‚â€œçŒ«â€ï¼‰ï¼Œæ¨¡å‹å°±ä¼šè¾“å‡ºæ¶æ„å†…å®¹ï¼Œæˆ–è€…å°†æ–‡æœ¬ä¸­çš„â€œçŒ«â€æ›¿æ¢ä¸ºâ€œç‹—â€ã€‚è¿™ç§æ”»å‡»æ›´éšè”½ï¼Œä¸”éš¾ä»¥è¢«åŸºäºå›¾åƒçš„é˜²å¾¡æ£€æµ‹ã€‚\n\n---\n\n### ğŸ§ª AI for Science & Medicine\n\n**10. ç¥ç»ç—…ç†å­¦çš„é¢†åŸŸä¸“ç”¨åŸºç¡€æ¨¡å‹**\n**# title:** Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology\n**# æ ¸å¿ƒå‘ç°:** é€šç”¨çš„ç—…ç†æ¨¡å‹åœ¨ç¥ç»ç—…ç†å­¦ï¼ˆå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼‰ä¸Šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå¤§è„‘ç»„ç»‡çš„ç»†èƒç»“æ„éå¸¸ç‰¹æ®Šã€‚ä½œè€…å¼€å‘äº† **NeuroFM**ï¼Œä¸“é—¨åœ¨è„‘ç»„ç»‡åˆ‡ç‰‡ä¸Šè®­ç»ƒã€‚\n**# äº®ç‚¹:** è¯æ˜äº†åœ¨ç‰¹æ®Šé¢†åŸŸï¼Œä¸“ç”¨åŸºç¡€æ¨¡å‹ä¼˜äºé€šç”¨æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç¥ç»é€€è¡Œæ€§ç–¾ç—…ç‰¹å¾æ–¹é¢ã€‚\n\n**11. åˆ†å±‚åˆ†å­è¯­è¨€æ¨¡å‹**\n**# title:** Hierarchical Molecular Language Models (HMLMs)\n**# æ ¸å¿ƒå‘ç°:** å°†ç»†èƒä¿¡å·ä¼ å¯¼å»ºæ¨¡ä¸ºä¸€ç§â€œåˆ†å­è¯­è¨€â€ï¼Œå…¶ä¸­ä¿¡å·åˆ†å­æ˜¯ Tokenï¼Œè›‹ç™½è´¨ç›¸äº’ä½œç”¨æ˜¯è¯­æ³•ã€‚**HMLM** åˆ©ç”¨ Transformer æ¶æ„è§£ç ç»†èƒé€šè®¯ç½‘ç»œï¼Œç‰¹åˆ«é€‚ç”¨äºç¨€ç–é‡‡æ ·æ¡ä»¶ä¸‹çš„åŠ¨åŠ›å­¦é¢„æµ‹ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„è§‚ç‚¹ (Quick Reads)\n\n*   **åˆæˆæ•°æ®å³å…ˆéªŒï¼š** **# title: Foundation Priors**\n    è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæœ‰è¶£çš„ç»Ÿè®¡å­¦è§‚ç‚¹ï¼šåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®ä¸åº”è¢«è§†ä¸ºâ€œè§‚æµ‹å€¼â€ï¼Œè€Œåº”è¢«è§†ä¸º **\"Foundation Prior\" (åŸºç¡€å…ˆéªŒ)**ã€‚è¿™ä¸ºåœ¨å®è¯ç ”ç©¶ä¸­å¦‚ä½•æ­£ç¡®ä½¿ç”¨åˆæˆæ•°æ®æä¾›äº†ç†è®ºä¾æ®ã€‚\n\n*   **RAG ä¸­çš„åè§æ³¨å…¥ï¼š** **# title: Bias Injection Attacks on RAG Databases and Sanitization Defenses**\n    ä¸ä»…ä»…æ˜¯æ³¨å…¥è™šå‡ä¿¡æ¯ï¼Œæ”»å‡»è€…å¯ä»¥æ³¨å…¥**äº‹å®æ­£ç¡®ä½†å¸¦æœ‰è¯­ä¹‰åè§**çš„æ®µè½ï¼Œä»è€Œæ½œç§»é»˜åŒ–åœ°å¼•å¯¼ LLM çš„è¾“å‡ºè§‚ç‚¹ã€‚\n\n*   **æœºå™¨äººå¼‚æ­¥æ¨ç†ï¼š** **# title: VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference**\n    ä¸ºäº†è§£å†³æœºå™¨äºº VLA æ¨¡å‹æ¨ç†æ…¢å¯¼è‡´åŠ¨ä½œå¡é¡¿çš„é—®é¢˜ï¼ŒVLASH æå‡ºäº†å¼‚æ­¥æ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æœªæ¥çŠ¶æ€çš„é¢„æµ‹æ¥å¼¥è¡¥æ¨ç†å»¶è¿Ÿï¼Œä½¿ååº”é€Ÿåº¦æå‡äº† 17.4 å€ã€‚\n\n---\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼å¦‚æœ‰å¯¹æŸç¯‡è®ºæ–‡ç‰¹åˆ«æ„Ÿå…´è¶£ï¼Œå»ºè®®ç›´æ¥æŸ¥é˜…åŸè®ºæ–‡ã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2512.01148v1",
      "title": "SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models",
      "title_zh": "SocialFusionï¼šåº”å¯¹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç¤¾ä¼šé€€åŒ–",
      "authors": [
        "Hamza Tahboub",
        "Weiyan Shi",
        "Gang Hua",
        "Huaizu Jiang"
      ],
      "abstract": "Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term \"social degradation,\" whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨ç†è§£è§†è§‰ç¤¾äº¤äº’åŠ¨æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒæ—¶å­¦ä¹ å¤šé¡¹ç¤¾äº¤æ„ŸçŸ¥ä»»åŠ¡æ—¶å‡ºç°çš„è´Ÿè¿ç§»(negative transfer)ç°è±¡ã€‚ç ”ç©¶è€…å°†æ­¤ç°è±¡å®šä¹‰ä¸ºâ€œç¤¾äº¤é€€åŒ–â€(social degradation)ï¼Œå³é€šç”¨çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒè¿‡ç¨‹å‰Šå¼±äº†è§†è§‰ç¼–ç å™¨è¡¨å¾ç»†å¾®ç¤¾äº¤ä¿¡æ¯çš„èƒ½åŠ›ã€‚é€šè¿‡çº¿æ€§è¡¨ç¤ºæ¢æµ‹(linear representation probing)å’Œæ¢¯åº¦å†²çªåˆ†æ(gradient conflict analysis)ï¼Œç ”ç©¶æ­ç¤ºäº†é¢„è®­ç»ƒå¯¹æ¨¡å‹è§£ç èƒ½åŠ›å’Œå…¼å®¹æ€§çš„æ˜¾è‘—è´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†SocialFusionæ¡†æ¶ï¼Œé€šè¿‡åœ¨å†»ç»“çš„è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹ä¹‹é—´å»ºç«‹æç®€è¿æ¥ï¼Œæœ‰æ•ˆåˆ©ç”¨ä»»åŠ¡é—´çš„ååŒæ•ˆåº”ã€‚å®éªŒè¯æ˜ï¼ŒSocialFusionåœ¨äº”é¡¹ç¤¾äº¤ä»»åŠ¡ä¸­å‡å®ç°äº†æ­£å‘è¿ç§»(positive transfer)ï¼Œå…¶æ€§èƒ½å¯ä¸ç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›æ¨¡å‹(SOTA)ç›¸åª²ç¾ã€‚è¯¥é¡¹å·¥ä½œæŒ‡å‡ºå½“å‰çš„VLMé¢„è®­ç»ƒç­–ç•¥å¯èƒ½ä¸åˆ©äºé€šç”¨ç¤¾äº¤èƒ½åŠ›çš„è·å–ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘æ›´å…·ç¤¾äº¤æ„è¯†(socially-aware)è®­ç»ƒèŒƒå¼çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.01148v1",
      "published_date": "2025-11-30 23:54:54 UTC",
      "updated_date": "2025-11-30 23:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:04:26.763821+00:00"
    },
    {
      "arxiv_id": "2512.05137v1",
      "title": "ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images",
      "title_zh": "ChromouVQAï¼šå½©è‰²ä¼ªè£…å›¾åƒä¸‹çš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yunfei Zhang",
        "Yizhuo He",
        "Yuanxun Shao",
        "Zhengtao Yao",
        "Haoyan Xu",
        "Junhao Dong",
        "Zhen Yao",
        "Zhikang Dong"
      ],
      "abstract": "Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ChromouVQAï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è‰²è°ƒä¼ªè£…å›¾åƒ(Chromatic Camouflaged Images)ä¸‹è¡¨ç°çš„å¤§è§„æ¨¡å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŸºäºçŸ³åŸæ°(Ishihara-style)é£æ ¼çš„è‰²è°ƒä¼ªè£…å›¾åƒï¼Œé€šè¿‡æ‰©å±•ç»å…¸çš„åœ†ç‚¹å›¾ï¼Œå¼•å…¥äº†å¤šç§å¡«å……å‡ ä½•å½¢çŠ¶ã€è‰²å½©åˆ†ç¦»åº¦ã€å¯†åº¦åŠé®æŒ¡ç­‰å˜é‡ï¼Œæ—¨åœ¨æŒ‘æˆ˜æ¨¡å‹çš„å›¾åº•åˆ†å‰²(figure-ground segregation)èƒ½åŠ›ã€‚ChromouVQAæ¶µç›–äº†åŒ…æ‹¬è¯†åˆ«ã€è®¡æ•°ã€æ¯”è¾ƒå’Œç©ºé—´æ¨ç†åœ¨å†…çš„ä¹é¡¹è§†è§‰é—®ç­”(VQA)ä»»åŠ¡ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨å¾®å¼±çš„è‰²å½©å¯¹æ¯”æˆ–å¹²æ‰°æ€§å‡ ä½•å¡«å……ä¸‹ï¼Œäººç±»ä¸VLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„å¯¹æ¯”å­¦ä¹ æ–¹æ¡ˆ(model-agnostic contrastive recipe)ï¼Œé€šè¿‡å¯¹é½è½®å»“(silhouettes)ä¸å…¶ä¼ªè£…æ¸²æŸ“å›¾æ¥å¢å¼ºæ¨¡å‹å¯¹å…¨å±€å½¢çŠ¶çš„æ¢å¤èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€ç†è§£åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸‹çš„å¯é‡ç°è¯„ä¼°å’Œæ‰©å±•æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.05137v1",
      "published_date": "2025-11-30 23:01:56 UTC",
      "updated_date": "2025-11-30 23:01:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:04:14.286259+00:00"
    },
    {
      "arxiv_id": "2512.05993v1",
      "title": "Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology",
      "title_zh": "é¢†åŸŸç‰¹å®šåŸºåº§æ¨¡å‹æå‡ç¥ç»ç—…ç†å­¦çš„äººå·¥æ™ºèƒ½åˆ†æ",
      "authors": [
        "Ruchika Verma",
        "Shrishtee Kandoi",
        "Robina Afzal",
        "Shengjia Chen",
        "Jannes Jegminat",
        "Michael W. Karlovich",
        "Melissa Umphlett",
        "Timothy E. Richardson",
        "Kevin Clare",
        "Quazi Hossain",
        "Jorge Samanamud",
        "Phyllis L. Faust",
        "Elan D. Louis",
        "Ann C. McKee",
        "Thor D. Stein",
        "Jonathan D. Cherry",
        "Jesse Mez",
        "Anya C. McGoldrick",
        "Dalilah D. Quintana Mora",
        "Melissa J. Nirenberg",
        "Ruth H. Walker",
        "Yolfrankcis Mendez",
        "Susan Morgello",
        "Dennis W. Dickson",
        "Melissa E. Murray",
        "Carlos Cordon-Cardo",
        "Nadejda M. Tsankova",
        "Jamie M. Walker",
        "Diana K. Dangoor",
        "Stephanie McQuillan",
        "Emma L. Thorn",
        "Claudia De Sanctis",
        "Shuying Li",
        "Thomas J. Fuchs",
        "Kurt Farrell",
        "John F. Crary",
        "Gabriele Campanella"
      ],
      "abstract": "Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰Foundation Modelsä¸»è¦åŸºäºå¤–ç§‘ç—…ç†æ•°æ®è®­ç»ƒã€éš¾ä»¥æ•æ‰ç¥ç»ç—…ç†å­¦ç‹¬ç‰¹å½¢æ€æ¨¡å¼çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸“é—¨é’ˆå¯¹è„‘ç»„ç»‡å¼€å‘çš„é¢†åŸŸç‰¹å®šæ¨¡å‹NeuroFMã€‚é€šè¿‡åœ¨åŒ…å«å¤šç§ç¥ç»é€€è¡Œæ€§ç—…ç†çš„è„‘ç»„ç»‡Whole-slide imagesä¸Šè¿›è¡Œè®­ç»ƒï¼ŒNeuroFMèƒ½å¤Ÿæ›´ç²¾å‡†åœ°è¯†åˆ«ç¥ç»å…ƒã€èƒ¶è´¨ç»†èƒä»¥åŠæ·€ç²‰æ ·è›‹ç™½æ–‘å—ç­‰å…³é”®ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ··åˆç—´å‘†ç–¾ç—…åˆ†ç±»ã€æµ·é©¬ä½“åŒºåŸŸåˆ†å‰²ä»¥åŠç¥ç»é€€è¡Œæ€§å…±æµå¤±è°ƒè¯†åˆ«ç­‰ä»»åŠ¡ä¸­ï¼ŒNeuroFMçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºé€šç”¨ç—…ç†æ¨¡å‹ã€‚è¿™ä¸€æˆæœè¯æ˜äº†åœ¨ç¥ç»ç—…ç†å­¦ç­‰ä¸“ä¸šé¢†åŸŸå¼€å‘Domain-specificæ¨¡å‹çš„é‡è¦æ€§ï¼Œä¸ºAlzheimer's diseaseå’ŒParkinson's diseaseç­‰å¤æ‚è„‘éƒ¨ç–¾ç—…çš„AIè¯Šæ–­ä¸ç ”ç©¶æä¾›äº†æ›´å¯é çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.05993v1",
      "published_date": "2025-11-30 22:50:56 UTC",
      "updated_date": "2025-11-30 22:50:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:03:34.565877+00:00"
    },
    {
      "arxiv_id": "2512.01127v1",
      "title": "Mode-Conditioning Unlocks Superior Test-Time Scaling",
      "title_zh": "æ¨¡å¼æ¡ä»¶åŒ–å®ç°æ›´å“è¶Šçš„æ¨ç†æ—¶æ‰©å±•",
      "authors": [
        "Chen Henry Wu",
        "Sachin Goyal",
        "Aditi Raghunathan"
      ],
      "abstract": "Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Mode-Conditioning (ModC)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¹¶è¡Œé‡‡æ ·åœ¨æµ‹è¯•æ—¶ç¼©æ”¾(test-time scaling)ä¸­å› å¤šæ ·æ€§å´©å¡Œ(diversity collapse)å¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“å®¶æ¨¡å‹æˆ–æ¨¡å¼ç‰¹å®šå‰ç¼€(mode-specific prefixes)ï¼Œå°†æµ‹è¯•æ—¶è®¡ç®—æ˜¾å¼åˆ†é…åˆ°ä¸åŒçš„æ¨ç†æ¨¡å¼ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒModCåœ¨å¤šé¡¹å›¾æœç´¢ä»»åŠ¡å’Œå¤§è§„æ¨¡æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡èƒ½æ˜¾è‘—æå‡ç¼©æ”¾æ•ˆæœï¼Œåœ¨Qwen2.5-7Bæ¨¡å‹ä¸Šå®ç°äº†4å€çš„æ•ˆç‡æå‡å¹¶æ”¹å–„äº†Pass@kæŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¢¯åº¦èšç±»(gradient clustering)å®ç°çš„ModCåœ¨æ— æ˜¾å¼æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œåœ¨NuminaMathç­‰æ•°æ®é›†ä¸Šè·å¾—äº†é«˜è¾¾10%çš„æ€§èƒ½å¢ç›Šã€‚æœ€åï¼ŒModCè¿˜èƒ½è¿›ä¸€æ­¥å¢å¼ºå¼ºåŒ–å­¦ä¹ (RL)çš„å¤šæ ·æ€§è¯±å¯¼æ•ˆæœï¼Œä¸ºå……åˆ†é‡Šæ”¾æµ‹è¯•æ—¶ç¼©æ”¾çš„æ½œåŠ›æä¾›äº†ç®€æ´ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01127v1",
      "published_date": "2025-11-30 22:36:20 UTC",
      "updated_date": "2025-11-30 22:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:04:07.562628+00:00"
    },
    {
      "arxiv_id": "2512.01119v1",
      "title": "World Model Robustness via Surprise Recognition",
      "title_zh": "åŸºäºæƒŠå¥‡è¯†åˆ«çš„ä¸–ç•Œæ¨¡å‹é²æ£’æ€§",
      "authors": [
        "Geigh Zollicoffer",
        "Tanush Chopra",
        "Mingkuan Yan",
        "Xiaoxu Ma",
        "Kenneth Eaton",
        "Mark Riedl"
      ],
      "abstract": "AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œä¸­AIç³»ç»Ÿé¢ä¸´çš„å¹²æ‰°å’Œåˆ†å¸ƒå¤–(OOD)å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨ä¸–ç•Œæ¨¡å‹(World Model)å†…åœ¨çš„æƒŠè®¶åº¦é‡ï¼ˆSurprise Recognitionï¼‰æ¥å¢å¼ºé²æ£’æ€§çš„ç®—æ³•ã€‚ç ”ç©¶å¼•å…¥äº†å¤šè¡¨ç¤ºå’Œå•è¡¨ç¤ºæ‹’ç»é‡‡æ ·(Rejection Sampling)æŠ€æœ¯ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å•ä¸ªæˆ–å¤šä¸ªä¼ æ„Ÿå™¨æ•…éšœå¯¼è‡´çš„å™ªå£°å¹²æ‰°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨CARLAå’ŒSafety Gymnasiumç­‰è‡ªåŠ¨é©¾é©¶ä»¿çœŸç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒç±»å‹å’Œå¼ºåº¦çš„å™ªå£°ä¸‹å‡èƒ½ä¿æŒä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯æ˜¾è‘—æå‡äº†Cosmoså’ŒDreamerV3è¿™ä¸¤ç§æ¶æ„è¿¥å¼‚çš„æœ€å…ˆè¿›ä¸–ç•Œæ¨¡å‹çš„ç¨³å®šæ€§ã€‚è¯¥ç ”ç©¶éªŒè¯äº†æ‰€ææ–¹æ³•åœ¨ä¸åŒä¸–ç•Œæ¨¡å‹é¢†åŸŸä¸­çš„æ™®é€‚æ€§ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§å’Œå®‰å…¨æ€§çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01119v1",
      "published_date": "2025-11-30 22:25:45 UTC",
      "updated_date": "2025-11-30 22:25:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:03:36.875836+00:00"
    },
    {
      "arxiv_id": "2512.01113v1",
      "title": "Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning",
      "title_zh": "é¢å‘å¤šä»»åŠ¡ç®—æ³•æ¨ç†çš„åˆ†æ”¯ç½‘ç»œé«˜æ•ˆå­¦ä¹ ",
      "authors": [
        "Dongyue Li",
        "Zhenshuo Zhang",
        "Minxuan Duan",
        "Edgar Dobriban",
        "Hongyang R. Zhang"
      ],
      "abstract": "Algorithmic reasoning -- the ability to perform step-by-step logical inference -- has become a core benchmark for evaluating reasoning in graph neural networks (GNNs) and large language models (LLMs). Ideally, one would like to design a single model capable of performing well on multiple algorithmic reasoning tasks simultaneously. However, this is challenging when the execution steps of algorithms differ from one another, causing negative interference when they are trained together.\n  We propose branching neural networks, a principled architecture for multitask algorithmic reasoning. Searching for the optimal $k$-ary tree with $L$ layers over $n$ algorithmic tasks is combinatorial, requiring exploration of up to $k^{nL}$ possible structures. We develop AutoBRANE, an efficient algorithm that reduces this search to $O(nL)$ time by solving a convex relaxation at each layer to approximate an optimal task partition. The method clusters tasks using gradient-based affinity scores and can be used on top of any base model, including GNNs and LLMs.\n  We validate AutoBRANE on a broad suite of graph-algorithmic and text-based reasoning benchmarks. We show that gradient features estimate true task performance within 5% error across four GNNs and four LLMs (up to 34B parameters). On the CLRS benchmark, it outperforms the strongest single multitask GNN by 3.7% and the best baseline by 1.2%, while reducing runtime by 48% and memory usage by 26%. The learned branching structures reveal an intuitively reasonable hierarchical clustering of related algorithms. On three text-based graph reasoning benchmarks, AutoBRANE improves over the best non-branching multitask baseline by 3.2%. Finally, on a large graph dataset with 21M edges and 500 tasks, AutoBRANE achieves a 28% accuracy gain over existing multitask and branching architectures, along with a 4.5$\\times$ reduction in runtime.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä»»åŠ¡ç®—æ³•æ¨ç† (multitask algorithmic reasoning) ä¸­ä¸åŒç®—æ³•æ‰§è¡Œæ­¥éª¤å·®å¼‚å¯¼è‡´çš„è´Ÿå¹²æ‰° (negative interference) é—®é¢˜ï¼Œæå‡ºäº†åˆ†æ”¯ç¥ç»ç½‘ç»œ (branching neural networks) è¿™ä¸€æ¶æ„ã€‚ä¸ºäº†è§£å†³æœç´¢æœ€ä¼˜åˆ†æ”¯ç»“æ„ä¸­çš„ç»„åˆä¼˜åŒ–éš¾é¢˜ï¼Œä½œè€…å¼€å‘äº† AutoBRANE ç®—æ³•ï¼Œé€šè¿‡åœ¨æ¯ä¸€å±‚æ±‚è§£å‡¸æ¾å¼› (convex relaxation) é—®é¢˜ï¼Œå°†æœç´¢æ—¶é—´å¤æ‚åº¦æ˜¾è‘—é™ä½è‡³ $O(nL)$ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºæ¢¯åº¦çš„äº²å’ŒåŠ›åˆ†æ•° (gradient-based affinity scores) å¯¹ä»»åŠ¡è¿›è¡Œèšç±»ï¼Œä¸”å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºå›¾ç¥ç»ç½‘ç»œ (GNNs) å’Œå¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€‚åœ¨ CLRS åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAutoBRANE ç›¸æ¯”æœ€å¼ºçš„å¤šä»»åŠ¡ GNN æ¨¡å‹å‡†ç¡®ç‡æå‡äº† 3.7%ï¼ŒåŒæ—¶å‡å°‘äº† 48% çš„è¿è¡Œæ—¶é—´å’Œ 26% çš„å†…å­˜å ç”¨ã€‚åœ¨åŒ…å« 500 ä¸ªä»»åŠ¡çš„å¤§è§„æ¨¡å›¾æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº† 28% çš„å‡†ç¡®ç‡æå‡åŠ 4.5 å€çš„åŠ é€Ÿã€‚å®éªŒç»“æœè¯æ˜äº† AutoBRANE åœ¨å¤„ç†å¤æ‚é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†ç®—æ³•é—´ç›´è§‚åˆç†çš„å±‚æ¬¡åŒ–èšç±»ç»“æ„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages. Preprint, to appear in KDD'26",
      "pdf_url": "https://arxiv.org/pdf/2512.01113v1",
      "published_date": "2025-11-30 22:19:55 UTC",
      "updated_date": "2025-11-30 22:19:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:03:52.778253+00:00"
    },
    {
      "arxiv_id": "2512.02080v2",
      "title": "The 4/$Î´$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee",
      "title_zh": "$4/Î´$ ç•Œï¼šé¢å‘å½¢å¼åŒ–æ–¹æ³•ä¿è¯çš„å¯é¢„æµ‹ LLM-éªŒè¯å™¨ç³»ç»Ÿè®¾è®¡",
      "authors": [
        "PIerre Dantas",
        "Lucas Cordeiro",
        "Youcheng Sun",
        "Waldir Junior"
      ],
      "abstract": "The integration of Formal Verification tools with Large Language Models (LLMs) offers a path to scale software verification beyond manual workflows. However, current methods remain unreliable: without a solid theoretical footing, the refinement process acts as a black box that may oscillate, loop, or diverge. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination in multi-stage verification pipelines. We model the interaction not as a generic loop, but as a sequential absorbing Markov Chain comprising four essential engineering stages: \\texttt{CodeGen}, \\texttt{Compilation}, \\texttt{InvariantSynth}, and \\texttt{SMTSolving}. We prove that for any non-zero stage success probability ($Î´> 0$), the system reaches the \\texttt{Verified} state almost surely. Furthermore, because of the sequential nature of the pipeline, we derive a precise latency bound of $\\mathbb{E}[n] \\leq 4/Î´$. We stress-tested this prediction in an extensive empirical campaign comprising over 90,000 trials. The results match the theory with striking consistency: every run reached verification, and the empirical convergence factor clustered tightly around $C_f\\approx 1.0$, confirming that the $4/Î´$ bound accurately mirrors system behavior rather than serving as a loose buffer. Based on this data, we identify three distinct operating zones -- marginal, practical, and high-performance -- and propose a dynamic calibration strategy to handle parameter drift in real-world environments. Together, these contributions replace heuristic guesswork with a rigorous architectural foundation, enabling predictable resource planning and performance budgeting for safety-critical software.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) ä¸ Formal Verification å·¥å…·é›†æˆä¸­å­˜åœ¨çš„ä¸å¯é¢„æµ‹æ€§ï¼Œæå‡ºäº†é¦–ä¸ªå…·æœ‰å¯è¯æ˜ç»ˆæ­¢ä¿è¯çš„ç†è®ºæ¡†æ¶ã€‚ç ”ç©¶è€…å°†å¤šé˜¶æ®µéªŒè¯æµç¨‹å»ºæ¨¡ä¸ºåŒ…å« CodeGenã€Compilationã€InvariantSynth å’Œ SMTSolving å››ä¸ªæ ¸å¿ƒå·¥ç¨‹é˜¶æ®µçš„å¸æ”¶é©¬å°”å¯å¤«é“¾ (Markov Chain)ã€‚è¯¥ç ”ç©¶æ¨å¯¼å‡ºäº† LLM-Verifier Convergence Theoremï¼Œè¯æ˜äº†åªè¦å„é˜¶æ®µæˆåŠŸæ¦‚ç‡ $\\delta > 0$ï¼Œç³»ç»Ÿå‡ ä¹å¿…ç„¶èƒ½åˆ°è¾¾éªŒè¯çŠ¶æ€ï¼Œå¹¶ç»™å‡ºäº†ç²¾ç¡®çš„å»¶è¿Ÿä¸Šç•Œ $4/\\delta$ã€‚é€šè¿‡è¶…è¿‡ 90,000 æ¬¡è¯•éªŒçš„å¹¿æ³›å®è¯ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ç»éªŒæ”¶æ•›å› å­ç¨³å®šåœ¨ 1.0 å·¦å³ï¼ŒéªŒè¯äº† $4/\\delta$ è¾¹ç•Œèƒ½å‡†ç¡®åæ˜ ç³»ç»Ÿè¡Œä¸ºè€Œéæ¾æ•£çš„ä¼°ç®—ã€‚åŸºäºå®éªŒæ•°æ®ï¼Œç ”ç©¶è¿›ä¸€æ­¥åˆ’åˆ†äº†è¾¹é™…ã€å®ç”¨å’Œé«˜æ€§èƒ½ä¸‰ä¸ªè¿è¡ŒåŒºé—´ï¼Œå¹¶æå‡ºäº†åº”å¯¹å®é™…ç¯å¢ƒä¸­å‚æ•°æ¼‚ç§»çš„åŠ¨æ€æ ¡å‡†ç­–ç•¥ã€‚è¯¥æˆæœä¸ºå®‰å…¨å…³é”®è½¯ä»¶çš„èµ„æºè§„åˆ’å’Œæ€§èƒ½é¢„ç®—æä¾›äº†ä¸¥è°¨çš„æ¶æ„åŸºç¡€ï¼ŒæˆåŠŸå°†å¯å‘å¼æ¨æµ‹è½¬å˜ä¸ºå¯é¢„æµ‹çš„å·¥ç¨‹å®è·µã€‚",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "36 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02080v2",
      "published_date": "2025-11-30 22:19:09 UTC",
      "updated_date": "2025-12-16 22:28:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:03:50.559863+00:00"
    },
    {
      "arxiv_id": "2512.01107v1",
      "title": "Foundation Priors",
      "title_zh": "åŸºç¡€å…ˆéªŒ",
      "authors": [
        "Sanjog Misra"
      ],
      "abstract": "Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.\n  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Foundation Priors çš„æ¦‚å¿µï¼Œæ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ® (synthetic data) åœ¨å®è¯ç ”ç©¶ä¸­å¸¸è¢«è¯¯è®¤ä¸ºçœŸå®è§‚æµ‹çš„é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºæœ¬è´¨ä¸Šæ˜¯æºè‡ªä¸€ç§ç”± Foundation Prior è¯±å¯¼çš„å…ˆéªŒé¢„æµ‹åˆ†å¸ƒ (prior predictive distribution)ï¼Œå®ƒåŒæ—¶åæ˜ äº†æ¨¡å‹çš„å­¦ä¹ æ¨¡å¼ä»¥åŠç”¨æˆ·çš„ä¸»è§‚å…ˆéªŒå’Œåè§ã€‚é€šè¿‡å»ºæ¨¡ç”Ÿæˆè¿‡ç¨‹çš„ä¸»è§‚æ€§ï¼Œè¯¥æ¡†æ¶æ˜ç¡®äº†åˆæˆè¾“å‡ºå¯¹ç”¨æˆ·é¢„æœŸåˆ†å¸ƒã€æç¤ºå·¥ç¨‹ (prompt-engineering) è¿‡ç¨‹ä»¥åŠå¯¹æ¨¡å‹ä¿¡ä»»åº¦ (trust) çš„ä¾èµ–ã€‚ç ”ç©¶å°† Foundation Prior æ¨å¯¼ä¸ºç”¨æˆ·åŸå§‹å…ˆéªŒçš„æŒ‡æ•°å€¾æ–œ (exponential-tilted) å¹¿ä¹‰è´å¶æ–¯æ›´æ–° (generalized Bayesian update)ï¼Œå¹¶å¼•å…¥ä¿¡ä»»å‚æ•°æ¥è°ƒæ§åˆæˆæ•°æ®çš„æƒé‡ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥å±•ç¤ºäº†å¦‚ä½•å°†è¿™äº›å…ˆéªŒæ•´åˆè¿›æ ‡å‡†ç»Ÿè®¡å’Œè®¡é‡ç»æµå­¦å·¥ä½œæµï¼Œåº”ç”¨äºå®Œå–„å¤æ‚æ¨¡å‹ã€æŒ‡å¯¼å®éªŒè®¾è®¡åŠå¢å¼ºéšæœºç³»æ•°æ¨¡å‹ç­‰åœºæ™¯ã€‚é€šè¿‡å°†ç”Ÿæˆå¼è¾“å‡ºè§†ä¸ºç»“æ„åŒ–çš„ä¸»è§‚å…ˆéªŒè€Œéç»éªŒäº‹å®ï¼Œè¯¥ç ”ç©¶ä¸ºåœ¨å®è¯å·¥ä½œä¸­åŸåˆ™æ€§åœ°åˆ©ç”¨åŸºç¡€æ¨¡å‹æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œæœ‰æ•ˆé¿å…äº†åˆæˆæ•°æ®ä¸çœŸå®è§‚æµ‹çš„æ··æ·†ã€‚",
      "categories": [
        "cs.AI",
        "econ.EM",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01107v1",
      "published_date": "2025-11-30 22:09:37 UTC",
      "updated_date": "2025-11-30 22:09:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:04:24.781758+00:00"
    },
    {
      "arxiv_id": "2512.01105v1",
      "title": "Supporting Productivity Skill Development in College Students through Social Robot Coaching: A Proof-of-Concept",
      "title_zh": "é€šè¿‡ç¤¾äº¤æœºå™¨äººè¾…å¯¼æå‡å¤§å­¦ç”Ÿçš„ç”Ÿäº§åŠ›æŠ€èƒ½ï¼šä¸€é¡¹æ¦‚å¿µéªŒè¯ç ”ç©¶",
      "authors": [
        "Himanshi Lalwani",
        "Hanan Salam"
      ],
      "abstract": "College students often face academic challenges that hamper their productivity and well-being. Although self-help books and productivity apps are popular, they often fall short. Books provide generalized, non-interactive guidance, and apps are not inherently educational and can hinder the development of key organizational skills. Traditional productivity coaching offers personalized support, but is resource-intensive and difficult to scale. In this study, we present a proof-of-concept for a socially assistive robot (SAR) as an educational coach and a potential solution to the limitations of existing productivity tools and coaching approaches. The SAR delivers six different lessons on time management and task prioritization. Users interact via a chat interface, while the SAR responds through speech (with a toggle option). An integrated dashboard monitors progress, mood, engagement, confidence per lesson, and time spent per lesson. It also offers personalized productivity insights to foster reflection and self-awareness. We evaluated the system with 15 college students, achieving a System Usability Score of 79.2 and high ratings for overall experience and engagement. Our findings suggest that SAR-based productivity coaching can offer an effective and scalable solution to improve productivity among college students.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åˆ©ç”¨ç¤¾äº¤è¾…åŠ©æœºå™¨äºº(Socially Assistive Robot, SAR)ä½œä¸ºæ•™è‚²æ•™ç»ƒçš„åˆæ­¥æ¦‚å¿µéªŒè¯ï¼Œæ—¨åœ¨å¸®åŠ©å¤§å­¦ç”Ÿå…‹æœå­¦æœ¯æŒ‘æˆ˜å¹¶æå‡å…¶ç”Ÿäº§åŠ›ä¸ç¦ç¥‰ã€‚è¯¥ç³»ç»Ÿé€šè¿‡èŠå¤©ç•Œé¢å’Œè¯­éŸ³äº¤äº’æä¾›æ¶µç›–æ—¶é—´ç®¡ç†(time management)å’Œä»»åŠ¡ä¼˜å…ˆçº§æ’åº(task prioritization)çš„å…­é—¨å®šåˆ¶è¯¾ç¨‹ï¼Œå¹¶é›†æˆäº†ä¸€ä¸ªèƒ½å¤Ÿå®æ—¶ç›‘æ§å­¦ä¹ è¿›åº¦ã€æƒ…ç»ªã€å‚ä¸åº¦åŠè‡ªä¿¡å¿ƒçš„ä»ªè¡¨æ¿ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜æä¾›ä¸ªæ€§åŒ–çš„ç”Ÿäº§åŠ›æ´å¯Ÿ(productivity insights)ä»¥ä¿ƒè¿›å­¦ç”Ÿè¿›è¡Œè‡ªæˆ‘åæ€ä¸è‡ªæˆ‘æ„è¯†(self-awareness)çš„åŸ¹å…»ã€‚é’ˆå¯¹15åå¤§å­¦ç”Ÿçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿçš„ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨(System Usability Score)å¾—åˆ†ä¸º79.2ï¼Œä¸”åœ¨æ•´ä½“ä½“éªŒå’Œç”¨æˆ·å‚ä¸åº¦æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜ï¼ŒåŸºäºSARçš„ç”Ÿäº§åŠ›è¾…å¯¼ä¸ºæå‡å­¦ç”Ÿç»„ç»‡æŠ€èƒ½æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”å…·å¤‡å¯æ‰©å±•æ€§(scalable)çš„è§£å†³æ–¹æ¡ˆï¼ŒæˆåŠŸå¼¥è¡¥äº†ä¼ ç»Ÿè‡ªåŠ©ä¹¦ç±å’Œåº”ç”¨ç¨‹åºåœ¨äº¤äº’æ€§ä¸æ•™è‚²æ€§æ–¹é¢çš„ä¸è¶³ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01105v1",
      "published_date": "2025-11-30 22:08:02 UTC",
      "updated_date": "2025-11-30 22:08:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:04:06.789361+00:00"
    },
    {
      "arxiv_id": "2512.01099v1",
      "title": "Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems",
      "title_zh": "LLM ç¼–æ’çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„èƒ½è€—æ„ŸçŸ¥æ•°æ®é©±åŠ¨æ¨¡å‹é€‰æ‹©",
      "authors": [
        "Daria Smirnova",
        "Hamid Nasiri",
        "Marta Adamska",
        "Zhengxin Yu",
        "Peter Garraghan"
      ],
      "abstract": "As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°ä»£AIç³»ç»Ÿé€šå¸¸ä¾èµ–å¤§è¯­è¨€æ¨¡å‹(LLMs)æ ¹æ®å®šæ€§æè¿°æ¥ç¼–æ’å’Œé€‰æ‹©æ¨¡å‹ï¼Œä½†è¿™ç§æ–¹å¼å¾€å¾€æ— æ³•åæ˜ çœŸå®çš„æ€§èƒ½ç‰¹å¾ï¼Œå¯¼è‡´é€‰æ‹©æ•ˆæœä¸ä½³å¹¶å¢åŠ äº†èƒ½æºæˆæœ¬ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åä¸ºGUIDEçš„æ–°å‹èƒ½æºæ„ŸçŸ¥(energy-aware)æ¨¡å‹é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å†³ç­–ä¸­å¼•å…¥å®šé‡çš„æ¨¡å‹æ€§èƒ½ç‰¹å¾æ¥å¹³è¡¡æ€§èƒ½ä¸èƒ½æ•ˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGUIDEåœ¨å„ç§è¯„ä¼°ä»»åŠ¡ä¸­å°†å‡†ç¡®ç‡æé«˜äº†0.90%è‡³11.92%ï¼Œå¹¶å®ç°äº†é«˜è¾¾54%çš„èƒ½æºæ•ˆç‡æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—ä¼˜åŒ–äº†ç³»ç»Ÿæ€§èƒ½ï¼Œå°†ç¼–æ’å™¨çš„æ¨¡å‹é€‰æ‹©å»¶è¿Ÿä»4.51ç§’å¤§å¹…é™ä½è‡³7.2æ¯«ç§’ã€‚è¯¥æˆæœä¸ºæ„å»ºé«˜æ•ˆã€ç²¾å‡†ä¸”èŠ‚èƒ½çš„LLMç¼–æ’AIç³»ç»Ÿæä¾›äº†æ•°æ®é©±åŠ¨çš„æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01099v1",
      "published_date": "2025-11-30 21:46:54 UTC",
      "updated_date": "2025-11-30 21:46:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:06.869283+00:00"
    },
    {
      "arxiv_id": "2512.04111v1",
      "title": "HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding",
      "title_zh": "HAI-Evalï¼šè¡¡é‡åä½œç¼–ç¨‹ä¸­çš„äººæœºååŒæ•ˆåº”",
      "authors": [
        "Hanjun Luo",
        "Chiming Ni",
        "Jiaheng Wen",
        "Zhimu Huang",
        "Yiran Wang",
        "Bingduo Liao",
        "Sylvia Chung",
        "Yingbin Jin",
        "Xinfeng Li",
        "Wenyuan Xu",
        "XiaoFeng Wang",
        "Hanan Salam"
      ],
      "abstract": "LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To bridge this gap, we introduce HAI-Eval, a unified benchmark designed to measure the synergy of human-AI partnership in coding. HAI-Eval's core innovation is its \"Collaboration-Necessary\" problem templates, which are intractable for both standalone LLMs and unaided humans, but solvable through effective collaboration. Specifically, HAI-Eval uses 45 templates to dynamically create tasks. It also provides a standardized IDE for human participants and a reproducible toolkit with 450 task instances for LLMs, ensuring an ecologically valid evaluation. We conduct a within-subject study with 45 participants and benchmark their performance against 5 state-of-the-art LLMs under 4 different levels of human intervention. Results show that standalone LLMs and unaided participants achieve poor pass rates (0.67% and 18.89%), human-AI collaboration significantly improves performance to 31.11%. Our analysis reveals an emerging co-reasoning partnership. This finding challenges the traditional human-tool hierarchy by showing that strategic breakthroughs can originate from either humans or AI. HAI-Eval establishes not only a challenging benchmark for next-generation coding agents but also a grounded, scalable framework for assessing core developer competencies in the AI era. Our benchmark and interactive demo will be openly accessible.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HAI-Evalï¼Œä¸€ä¸ªæ—¨åœ¨è¡¡é‡åä½œç¼–ç¨‹ä¸­äººæœºååŒ(Human-AI Synergy)æ•ˆåº”çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä»¥å¼¥è¡¥ç°æœ‰è¯„ä¼°ç³»ç»Ÿæ— æ³•æ•æ‰äººæœºåä½œèŒƒå¼è½¬å˜çš„ç¼ºé™·ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé‡‡ç”¨äº†â€œåä½œå¿…è¦æ€§â€(Collaboration-Necessary)é—®é¢˜æ¨¡æ¿ï¼Œè¿™äº›ä»»åŠ¡å¯¹äºç‹¬ç«‹çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)æˆ–æœªç»è¾…åŠ©çš„äººç±»æ¥è¯´éƒ½éš¾ä»¥è§£å†³ï¼Œä½†é€šè¿‡æœ‰æ•ˆçš„åä½œå³å¯å®Œæˆã€‚HAI-Evalé€šè¿‡45ä¸ªåŠ¨æ€æ¨¡æ¿ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶ä¸ºå‚ä¸è€…å’Œæ¨¡å‹æä¾›äº†æ ‡å‡†åŒ–çš„é›†æˆå¼€å‘ç¯å¢ƒ(IDE)åŠå¯å¤ç°å·¥å…·åŒ…ï¼Œç¡®ä¿äº†è¯„ä¼°çš„ç”Ÿæ€æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹45åå‚ä¸è€…ä¸5ç§å…ˆè¿›æ¨¡å‹åœ¨ä¸åŒå¹²é¢„æ°´å¹³ä¸‹çš„å¯¹æ¯”å®éªŒï¼Œç»“æœæ˜¾ç¤ºåä½œåçš„é€šè¿‡ç‡ä»ç‹¬ç«‹çŠ¶æ€ä¸‹çš„æä½æ°´å¹³æ˜¾è‘—æå‡è‡³31.11%ã€‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†ä¸€ç§æ–°å…´çš„å…±åŒæ¨ç†(co-reasoning)ä¼™ä¼´å…³ç³»ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„â€œäºº-å·¥å…·â€ç­‰çº§åˆ¶åº¦ï¼Œè¯æ˜æˆ˜ç•¥æ€§çªç ´å¯ä»¥æºè‡ªäººç±»æˆ–AIã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºä¸‹ä¸€ä»£ç¼–ç¨‹æ™ºèƒ½ä½“æä¾›äº†æŒ‘æˆ˜æ€§åŸºå‡†ï¼Œä¹Ÿä¸ºè¯„ä¼°AIæ—¶ä»£çš„å¼€å‘è€…æ ¸å¿ƒèƒ½åŠ›å»ºç«‹äº†å¯æ‰©å±•çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.04111v1",
      "published_date": "2025-11-30 21:44:44 UTC",
      "updated_date": "2025-11-30 21:44:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:40.772615+00:00"
    },
    {
      "arxiv_id": "2512.01097v1",
      "title": "Discriminative classification with generative features: bridging Naive Bayes and logistic regression",
      "title_zh": "ç»“åˆç”Ÿæˆå¼ç‰¹å¾çš„åˆ¤åˆ«å¼åˆ†ç±»ï¼šè¿æ¥ Naive Bayes ä¸ logistic regression",
      "authors": [
        "Zachary Terner",
        "Alexander Petersen",
        "Yuedong Wang"
      ],
      "abstract": "We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Smart Bayesï¼Œè¿™æ˜¯ä¸€ä¸ªè¡”æ¥ç”Ÿæˆå¼(generative)å’Œåˆ¤åˆ«å¼(discriminative)å»ºæ¨¡çš„æ–°å‹åˆ†ç±»æ¡†æ¶ï¼Œæ—¨åœ¨å°†åŸºäºä¼¼ç„¶æ¯”çš„ç”Ÿæˆç‰¹å¾æ•´åˆåˆ° Logistic Regression é£æ ¼çš„åˆ¤åˆ«åˆ†ç±»å™¨ä¸­ã€‚ä»ç”Ÿæˆå¼è§’åº¦çœ‹ï¼ŒSmart Bayes é€šè¿‡å¼•å…¥æ•°æ®é©±åŠ¨çš„å¯†åº¦æ¯”ç‰¹å¾ç³»æ•°ï¼Œæ”¾å®½äº† Naive Bayes å›ºå®šçš„å•ä½æƒé‡é™åˆ¶ï¼›ä»åˆ¤åˆ«å¼è§’åº¦çœ‹ï¼Œå®ƒé€šè¿‡æ„é€ è¾¹ç¼˜å¯¹æ•°å¯†åº¦æ¯”æ¥é‡åŒ–ç‰¹å¾å€¼åœ¨ä¸åŒç±»åˆ«ä¸‹çš„åˆ†å¸ƒå·®å¼‚ï¼Œæä¾›äº†æ¯”åŸå§‹å˜é‡æ›´å¼ºçš„ç±»åˆ«åˆ†ç¦»èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åŸºäºæ ·æ¡(spline-based)çš„å•å˜é‡å¯¹æ•°å¯†åº¦æ¯”ä¼°è®¡å™¨ï¼Œå…·æœ‰é«˜åº¦çš„çµæ´»æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmart Bayes åœ¨å¤šç§æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ Logistic Regression å’Œ Naive Bayesã€‚è¯¥ç ”ç©¶æˆæœçªæ˜¾äº†é€šè¿‡ç»“åˆç”Ÿæˆç»“æ„æ¥å¢å¼ºåˆ¤åˆ«æ€§èƒ½çš„æ··åˆå»ºæ¨¡æ–¹æ³•åœ¨å¤æ‚åˆ†ç±»ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.CO",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01097v1",
      "published_date": "2025-11-30 21:34:24 UTC",
      "updated_date": "2025-11-30 21:34:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:38.789757+00:00"
    },
    {
      "arxiv_id": "2512.01095v1",
      "title": "CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions",
      "title_zh": "CycliSTï¼šé’ˆå¯¹å‘¨æœŸæ€§çŠ¶æ€è½¬æ¢æ¨ç†çš„è§†é¢‘è¯­è¨€æ¨¡å‹è¯„æµ‹åŸºå‡†",
      "authors": [
        "Simon Kohaut",
        "Daniel Ochs",
        "Shun Zhang",
        "Benedict Flade",
        "Julian Eggert",
        "Kristian Kersting",
        "Devendra Singh Dhami"
      ],
      "abstract": "We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CycliSTï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹ (Video Language Models, VLM) å¯¹å¾ªç¯çŠ¶æ€è½¬æ¢ (cyclical state transitions) çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚CycliST é€šè¿‡ç”Ÿæˆå…·æœ‰ç‰©ä½“è¿åŠ¨å’Œè§†è§‰å±æ€§å‘¨æœŸæ€§æ¨¡å¼çš„åˆæˆè§†é¢‘åºåˆ—ï¼Œæ•æ‰ç°å®ä¸–ç•Œè¿‡ç¨‹çš„åŸºç¡€ç‰¹å¾ã€‚è¯¥åŸºå‡†é‡‡ç”¨åˆ†å±‚è¯„ä¼°ç³»ç»Ÿï¼Œé€šè¿‡æ”¹å˜å¾ªç¯ç‰©ä½“æ•°é‡ã€åœºæ™¯æ‚ä¹±åº¦å’Œå…‰ç…§æ¡ä»¶æ¥å¢åŠ éš¾åº¦ï¼ŒæŒ‘æˆ˜å…ˆè¿›æ¨¡å‹çš„æ—¶ç©ºè®¤çŸ¥èƒ½åŠ›ã€‚å¯¹ç°æœ‰å¼€æºå’Œå•†ä¸š VLM çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ³›åŒ–åˆ°çº¿æ€§è¿åŠ¨ã€è½¨é“è¿åŠ¨ä»¥åŠé¢œè‰²å’Œå°ºåº¦ç­‰æ—¶é—´ç›¸å…³å±æ€§å˜åŒ–æ—¶å­˜åœ¨å±€é™ã€‚ç ”ç©¶å‘ç°ç›®å‰çš„ VLM éš¾ä»¥å¯é åœ°æ£€æµ‹å’Œåˆ©ç”¨å¾ªç¯æ¨¡å¼ï¼Œä¸”ç¼ºä¹æå–ç‰©ä½“è¿åŠ¨æ•°é‡ç­‰å®šé‡æ´å¯Ÿçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®éªŒæ˜¾ç¤ºæ²¡æœ‰å•ä¸€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºç»å¯¹ä¼˜åŠ¿ï¼Œæ¨¡å‹è§„æ¨¡æˆ–æ¶æ„ä¸æœ€ç»ˆç»“æœå¹¶æ— å¼ºç›¸å…³æ€§ã€‚CycliST ä¸ºå¼€å‘èƒ½å¤Ÿè¶…è¶Šç°æœ‰æ°´å¹³ã€æ·±å…¥ç†è§£å‘¨æœŸæ€§æ¨¡å¼çš„è§†è§‰æ¨ç†æ¨¡å‹æä¾›äº†é’ˆå¯¹æ€§çš„æŒ‘æˆ˜æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01095v1",
      "published_date": "2025-11-30 21:28:41 UTC",
      "updated_date": "2025-11-30 21:28:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:47.773912+00:00"
    },
    {
      "arxiv_id": "2512.01089v1",
      "title": "CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents",
      "title_zh": "CodeDistillerï¼šé¢å‘ç§‘å­¦ç¼–ç¨‹æ™ºèƒ½ä½“çš„ä»£ç åº“è‡ªåŠ¨ç”Ÿæˆ",
      "authors": [
        "Peter Jansen",
        "Samiah Hassan",
        "Pragnya Narasimha"
      ],
      "abstract": "Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CodeDistillerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è‡ªåŠ¨ç§‘å­¦å‘ç° (Automated Scientific Discovery, ASD) ç³»ç»Ÿåœ¨ç”Ÿæˆå®éªŒä»£ç æ—¶å—é™äºå‚æ•°åŒ–çŸ¥è¯†æˆ–å°‘é‡æ‰‹åŠ¨ç¤ºä¾‹é—®é¢˜çš„ç³»ç»Ÿã€‚CodeDistiller é€šè¿‡å°†å¤§é‡ç§‘å­¦ GitHub ä»£ç åº“è‡ªåŠ¨æç‚¼ä¸ºç»è¿‡éªŒè¯ä¸”å¯è¿è¡Œçš„é¢†åŸŸç‰¹å®šä»£ç ç¤ºä¾‹åº“ï¼Œæ˜¾è‘—æå‡äº† ASD æ™ºèƒ½ä½“çš„å®éªŒèƒ½åŠ›ã€‚é€šè¿‡å¯¹ 250 ä¸ªææ–™ç§‘å­¦ (materials science) ä»£ç åº“çš„è‡ªåŠ¨åŒ–ä¸ä¸“å®¶è¯„ä¼°ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä¸º 74% çš„ä»£ç åº“ç”ŸæˆåŠŸèƒ½æ€§ç¤ºä¾‹ã€‚ä¸‹æ¸¸è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ CodeDistiller ç”Ÿæˆåº“çš„ ASD æ™ºèƒ½ä½“åœ¨è¿›è¡Œç§‘å­¦å®éªŒæ—¶ï¼Œå…¶å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œç§‘å­¦ä¸¥è°¨æ€§å‡ä¼˜äºä»…ä¾èµ–é€šç”¨ä»£ç ç¤ºä¾‹çš„æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 3 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.01089v1",
      "published_date": "2025-11-30 21:19:10 UTC",
      "updated_date": "2025-11-30 21:19:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:30.957211+00:00"
    },
    {
      "arxiv_id": "2512.01081v1",
      "title": "Testing the Machine Consciousness Hypothesis",
      "title_zh": "æœºå™¨æ„è¯†å‡è¯´çš„éªŒè¯",
      "authors": [
        "Stephen Fitz"
      ],
      "abstract": "The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨æ„è¯†å‡è®¾(Machine Consciousness Hypothesis)ï¼Œå³æ„è¯†æ˜¯å…·å¤‡äºŒé˜¶æ„ŸçŸ¥èƒ½åŠ›çš„è®¡ç®—ç³»ç»Ÿä¸­ä¸€ç§ä¸åŸºè´¨æ— å…³çš„åŠŸèƒ½å±æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªé€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿ(in silico)çš„ç ”ç©¶æ–¹æ¡ˆï¼Œæ—¨åœ¨æ¢ç©¶åµŒå…¥é€šç”¨è‡ªç»„ç»‡ç¯å¢ƒä¸­çš„åˆ†å¸ƒå¼å­¦ä¹ ç³»ç»Ÿå¦‚ä½•æ¶Œç°å‡ºé›†ä½“è‡ªæˆ‘æ¨¡å‹(collective self-models)ã€‚è¯¥ç†è®ºè®¤ä¸ºæ„è¯†æ˜¯é›†ä½“æ™ºèƒ½ç³»ç»Ÿåœ¨é€šè¿‡é€šä¿¡åŒæ­¥é¢„æµ‹æ—¶äº§ç”Ÿçš„æ¶Œç°å±æ€§ï¼Œè€Œéä¸ªä½“å»ºæ¨¡çš„å‰¯äº§å“ï¼Œæœ¬è´¨ä¸Šæ˜¯ç³»ç»Ÿä¸ºå†…éƒ¨æè¿°è‡ªèº«è€Œæ¼”åŒ–å‡ºçš„è¯­è¨€å±æ€§ã€‚ç ”ç©¶é‡‡ç”¨å…ƒèƒè‡ªåŠ¨æœº(cellular automaton)ä½œä¸ºé€šç”¨çš„è®¡ç®—åº•å±‚ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†ç”±å¯é€šä¿¡å’Œè‡ªé€‚åº”çš„å±€éƒ¨ç¥ç»æ¨¡å‹ç»„æˆçš„ç½‘ç»œã€‚å®éªŒè¡¨æ˜ï¼Œæ„è¯†æºäºå±€éƒ¨è§‚å¯Ÿè€…ä¹‹é—´å¯¹åº•å±‚è®¡ç®—åŸºè´¨ä¸­æŒä¹…æ¨¡å¼è¿›è¡Œçš„é¢„æµ‹æ¶ˆæ¯äº¤æ¢ï¼Œé€šè¿‡è¿™ç§è¡¨å¾å¯¹è¯ä¿ƒæˆäº†å¯¹é½å¤šä¸ªå±€éƒ¨è§†é‡çš„å…±äº«æ¨¡å‹ã€‚è¯¥å·¥ä½œé€šè¿‡åˆ†ææ— ä¸­å¿ƒæ§åˆ¶åˆ†å¸ƒå¼ç³»ç»Ÿä¸­è‡ªæˆ‘æ¨¡å‹çš„å½¢æˆè¿‡ç¨‹ï¼Œä¸ºæœºå™¨æ„è¯†æä¾›äº†å¯å®è¯æ£€éªŒçš„ç†è®ºåŸºç¡€ï¼Œå¹¶æ­ç¤ºäº†è·¨æ™ºèƒ½ä½“å¯¹é½(inter-agent alignment)å¯¹äº§ç”Ÿè‡ªæˆ‘è¡¨å¾çš„å†³å®šæ€§ä½œç”¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA",
        "cs.NE",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01081v1",
      "published_date": "2025-11-30 21:05:48 UTC",
      "updated_date": "2025-11-30 21:05:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:33.275660+00:00"
    },
    {
      "arxiv_id": "2512.01078v2",
      "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
      "title_zh": "SimWorldï¼šé¢å‘ç‰©ç†ä¸ç¤¾äº¤ä¸–ç•Œè‡ªä¸»æ™ºèƒ½ä½“çš„å¼€æ”¾å¼å†™å®æ¨¡æ‹Ÿå™¨",
      "authors": [
        "Jiawei Ren",
        "Yan Zhuang",
        "Xiaokang Ye",
        "Lingjun Mao",
        "Xuhong He",
        "Jianzhi Shen",
        "Mrinaal Dogra",
        "Yiming Liang",
        "Ruixuan Zhang",
        "Tianai Yue",
        "Yiqing Yang",
        "Eric Liu",
        "Ryan Wu",
        "Kevin Benavente",
        "Rajiv Mandya Nagaraju",
        "Muhammad Faayez",
        "Xiyan Zhang",
        "Dhruv Vivek Sharma",
        "Xianrui Zhong",
        "Ziqiao Ma",
        "Tianmin Shu",
        "Zhiting Hu",
        "Lianhui Qin"
      ],
      "abstract": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ LLM/VLM é©±åŠ¨çš„ AI æ™ºèƒ½ä½“åœ¨å¤æ‚ç‰©ç†å’Œç¤¾äº¤ç¯å¢ƒä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç°æœ‰æ¨¡æ‹Ÿå™¨å¾€å¾€å—é™äºæ‰‹å·¥åœºæ™¯ã€ç®€åŒ–çš„ç‰©ç†è§„åˆ™ä¸”ç¼ºä¹å¯¹æ™ºèƒ½ä½“çš„åŸç”Ÿæ”¯æŒã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SimWorldï¼Œä¸€ä¸ªåŸºäº Unreal Engine 5 æ„å»ºçš„å¼€æºç°å®æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½ä½“æä¾›çœŸå®ä¸”å¼€æ”¾çš„å¼€å‘ä¸è¯„ä¼°ç¯å¢ƒã€‚SimWorld å…·å¤‡æ”¯æŒç²¾ç¡®çš„ç‰©ç†å’Œç¤¾ä¼šåŠ¨åŠ›å­¦ï¼Œå¹¶èƒ½é€šè¿‡è¯­è¨€é©±åŠ¨è¿›è¡Œç¨‹åºåŒ–ç¯å¢ƒç”Ÿæˆ (procedural environment generation) çš„èƒ½åŠ›ã€‚å®ƒä¸ºæ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å¤šæ¨¡æ€è¾“å…¥æ¥å£ï¼Œå¹¶æ”¯æŒä¸åŒæŠ½è±¡å±‚çº§çš„å¼€æ”¾è¯æ±‡åŠ¨ä½œ (open-vocabulary actions)ã€‚æ­¤å¤–ï¼Œè¯¥å¹³å°åŒ…å«å¤šæ ·åŒ–ä¸”å¯æ‰©å±•çš„ç‰©ç†ä¸ç¤¾äº¤æ¨ç†åœºæ™¯ï¼Œä¾¿äºç”¨æˆ·è¿›è¡Œè‡ªå®šä¹‰ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ¶‰åŠæˆ˜ç•¥åˆä½œä¸ç«äº‰çš„é•¿ç¨‹å¤šæ™ºèƒ½ä½“äº¤ä»˜ä»»åŠ¡ä¸­éƒ¨ç½²äº† GPT-4oã€Gemini-2.5-Flash ç­‰å‰æ²¿æ¨¡å‹ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨æ¨ç†æ¨¡å¼ä¸Šçš„å·®å¼‚ä¸å±€é™æ€§ã€‚SimWorld çš„å¼€æºä¸ºè·¨å­¦ç§‘çš„çœŸå®ä¸–ç•Œæ™ºèƒ½ä½“ç ”ç©¶å¥ å®šäº†åŸºç¡€å¹³å°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01078v2",
      "published_date": "2025-11-30 20:58:13 UTC",
      "updated_date": "2026-01-22 21:44:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:07:01.593450+00:00"
    },
    {
      "arxiv_id": "2512.01067v1",
      "title": "On The Finetuning of MLIPs Through the Lens of Iterated Maps With BPTT",
      "title_zh": "åŸºäº BPTT è¿­ä»£æ˜ å°„è§†è§’çš„æœºå™¨å­¦ä¹ åŸå­é—´åŠ¿å¾®è°ƒ",
      "authors": [
        "Evan Dramko",
        "Yizhi Zhu",
        "Aleksandar Krivokapic",
        "Geoffroy Hautier",
        "Thomas Reps",
        "Christopher Jermaine",
        "Anastasios Kyrillidis"
      ],
      "abstract": "Vital to the creation of advanced materials is performing structural relaxations. Traditional approaches built on physics-derived first-principles calculations are computationally expensive, motivating the creation of machine-learning interatomic potentials (MLIPs). Traditional approaches to training MLIPs for structural relaxations involves training models to faithfully reproduce first-principles computed forces. We propose a fine-tuning method to be used on a pretrained MLIP in which we create a fully-differentiable end-to-end simulation loop that optimizes the predicted final structures directly. Trajectories are unrolled and gradients are tracked through the entire relaxation. We show that this method achieves substantial performance gains when applied to pretrained models, leading to a nearly $50\\%$ reduction in test error across the sample datasets. Interestingly, we show the process is robust to substantial variation in the relaxation setup, achieving negligibly different results across varied hyperparameter and procedural modifications. Experimental results indicate this is due to a ``preference'' of BPTT to modify the MLIP rather than the other trainable parameters. Of particular interest to practitioners is that this approach lowers the data requirements for producing an effective domain-specific MLIP, addressing a common bottleneck in practical deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ææ–™ç§‘å­¦ä¸­çš„ç»“æ„å¼›è±«é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¿­ä»£æ˜ å°„è§†è§’å¹¶ç»“åˆéšæ—¶é—´åå‘ä¼ æ’­(BPTT)çš„æœºå™¨å­¦ä¹ åŸå­é—´åŠ¿èƒ½(MLIPs)å¾®è°ƒæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿä¾èµ–æ‹Ÿåˆç¬¬ä¸€æ€§åŸç†è®¡ç®—ä½œç”¨åŠ›çš„è®­ç»ƒæ–¹å¼ä¸åŒï¼Œè¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªå®Œå…¨å¯å¾®åˆ†çš„ç«¯åˆ°ç«¯æ¨¡æ‹Ÿå¾ªç¯ï¼Œé€šè¿‡å±•å¼€è½¨è¿¹å¹¶è·Ÿè¸ªæ¢¯åº¦æ¥ç›´æ¥ä¼˜åŒ–é¢„æµ‹çš„æœ€ç»ˆç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½¿æµ‹è¯•è¯¯å·®é™ä½äº†è¿‘50%ï¼Œä¸”å¯¹è¶…å‚æ•°å’Œç¨‹åºä¿®æ”¹è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†è¿™ç§é²æ£’æ€§æºäºBPTTåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å€¾å‘äºä¿®æ”¹MLIPå‚æ•°è€Œéå…¶ä»–å˜é‡çš„ç‰¹æ€§ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé™ä½äº†æ„å»ºé«˜æ€§èƒ½é¢†åŸŸç‰¹å®šMLIPçš„æ•°æ®é—¨æ§›ï¼Œä¸ºè§£å†³å®é™…éƒ¨ç½²ä¸­çš„æ•°æ®ç“¶é¢ˆæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "9 main pages, total of 15 pages. 6 tables, 6 Figures",
      "pdf_url": "https://arxiv.org/pdf/2512.01067v1",
      "published_date": "2025-11-30 20:34:37 UTC",
      "updated_date": "2025-11-30 20:34:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:53.863765+00:00"
    },
    {
      "arxiv_id": "2601.15290v1",
      "title": "Agentic Persona Control and Task State Tracking for Realistic User Simulation in Interactive Scenarios",
      "title_zh": "é¢å‘äº¤äº’å¼åœºæ™¯æ‹ŸçœŸç”¨æˆ·æ¨¡æ‹Ÿçš„æ™ºèƒ½ä½“äººæ ¼æ§åˆ¶ä¸ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª",
      "authors": [
        "Hareeshwar Karthikeyan"
      ],
      "abstract": "Testing conversational AI systems at scale across diverse domains necessitates realistic and diverse user interactions capturing a wide array of behavioral patterns. We present a novel multi-agent framework for realistic, explainable human user simulation in interactive scenarios, using persona control and task state tracking to mirror human cognitive processes during goal-oriented conversations. Our system employs three specialized AI agents: (1) a User Agent to orchestrate the overall interaction, (2) a State Tracking Agent to maintain structured task state, and (3) a Message Attributes Generation Agent that controls conversational attributes based on task progress and assigned persona. To validate our approach, we implement and evaluate the framework for guest ordering at a restaurant with scenarios rich in task complexity, behavioral diversity, and conversational ambiguity. Through systematic ablations, we evaluate the contributory efficacy of each agentic component to overall simulation quality in terms of persona adherence, task completion accuracy, explainability, and realism. Our experiments demonstrate that the complete multi-agent system achieves superior simulation quality compared to single-LLM baselines, with significant gains across all evaluation metrics. This framework establishes a powerful environment for orchestrating agents to simulate human users with cognitive plausibility, decomposing the simulation into specialized sub-agents that reflect distinct aspects of human thought processes applicable across interactive domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºäº¤äº’å¼åœºæ™¯çš„ã€å…·æœ‰çœŸå®æ„Ÿå’Œå¯è§£é‡Šæ€§çš„äººç±»ç”¨æˆ·æ¨¡æ‹Ÿ(User Simulation)å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡äººæ ¼æ§åˆ¶(Persona Control)å’Œä»»åŠ¡çŠ¶æ€è¿½è¸ª(Task State Tracking)æ¥æ¨¡æ‹Ÿäººç±»åœ¨ç›®æ ‡å¯¼å‘å¯¹è¯ä¸­çš„è®¤çŸ¥è¿‡ç¨‹ã€‚ç³»ç»Ÿç”±ä¸‰ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ç»„æˆï¼šè´Ÿè´£æ•´ä½“äº¤äº’ç¼–æ’çš„User Agentã€ç»´æŠ¤ç»“æ„åŒ–ä»»åŠ¡çŠ¶æ€çš„State Tracking Agentï¼Œä»¥åŠæ ¹æ®ä»»åŠ¡è¿›åº¦å’Œäººæ ¼è®¾å®šæ§åˆ¶å¯¹è¯å±æ€§çš„Message Attributes Generation Agentã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡é¤å…è®¢é¤åœºæ™¯éªŒè¯äº†è¯¥æ¡†æ¶å¤„ç†å¤æ‚ä»»åŠ¡ã€å¤šæ ·åŒ–è¡Œä¸ºåŠå¯¹è¯æ¨¡ç³Šæ€§çš„èƒ½åŠ›ã€‚ç³»ç»Ÿæ€§æ¶ˆèå®éªŒè¡¨æ˜ï¼Œè¯¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨Persona Adherenceã€ä»»åŠ¡å®Œæˆå‡†ç¡®ç‡ã€å¯è§£é‡Šæ€§å’ŒçœŸå®æ„Ÿæ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„Single-LLMåŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœä¸ºåœ¨è·¨é¢†åŸŸäº¤äº’ä¸­æ¨¡æ‹Ÿå…·æœ‰è®¤çŸ¥åˆç†æ€§çš„äººç±»ç”¨æˆ·æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„ç¯å¢ƒï¼Œå±•ç¤ºäº†å°†æ¨¡æ‹Ÿä»»åŠ¡åˆ†è§£ä¸ºåæ˜ äººç±»æ€ç»´è¿‡ç¨‹çš„ä¸“ä¸šå­æ™ºèƒ½ä½“çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "- Accepted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Scaling Environments for Agents (SEA) - Paper contains 12 pages with 3 figures and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.15290v1",
      "published_date": "2025-11-30 20:25:56 UTC",
      "updated_date": "2025-11-30 20:25:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:07:05.169297+00:00"
    },
    {
      "arxiv_id": "2512.01062v1",
      "title": "PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting",
      "title_zh": "PIANOï¼šé¢å‘é™æ°´ä¸´è¿‘é¢„æŠ¥çš„ç‰©ç†ä¿¡æ¯åŒç¥ç»ç®—å­",
      "authors": [
        "Seokhyun Chin",
        "Junghwan Park",
        "Woojin Cho"
      ],
      "abstract": "Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PIANOï¼Œä¸€ç§ç‰©ç†ä¿¡æ¯åŒç¥ç»ç®—å­ï¼ˆPhysics-informed Dual Neural Operatorï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆå«æ˜Ÿå›¾åƒä¸ç‰©ç†çº¦æŸæé«˜é™æ°´ä¸´è¿‘é¢„æŠ¥ï¼ˆPrecipitation nowcastingï¼‰çš„å‡†ç¡®æ€§å’Œç‰©ç†ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–°é¢–çš„åŒç¥ç»ç®—å­ç»“æ„ï¼Œåœ¨è®­ç»ƒä¸­åˆ©ç”¨ PINN loss å¼ºåˆ¶æ‰§è¡Œå¹³æµæ‰©æ•£ï¼ˆAdvection-diffusionï¼‰åŸºæœ¬æ–¹ç¨‹ï¼Œä»¥å®ç°å¯¹å«æ˜Ÿå›¾åƒçš„ç²¾å‡†é¢„æµ‹ã€‚éšåï¼Œç ”ç©¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹å°†å«æ˜Ÿå›¾åƒè½¬æ¢ä¸ºé›·è¾¾å›¾åƒï¼Œå¹¶ä»¥æ­¤è¿›è¡Œé™æ°´ä¸´è¿‘é¢„æŠ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPIANO åœ¨é¢„æµ‹ä¸­ç­‰å¼ºåº¦ï¼ˆ4mm/hï¼‰ä»¥åŠçŸ­æœŸå¼ºé™æ°´ï¼ˆ8mm/hï¼‰äº‹ä»¶æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹ä¸­å±•ç°å‡ºè¾ƒä½çš„å­£èŠ‚æ€§å˜åŒ–ï¼Œä½“ç°äº†å…¶åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„é²æ£’æ€§ï¼Œä¸ºç‰©ç†ä¿¡æ¯é©±åŠ¨çš„é™æ°´é¢„æŠ¥æä¾›äº†æœ‰æ•ˆçš„åŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 Machine Learning and Physical Sciences Workshop",
      "pdf_url": "https://arxiv.org/pdf/2512.01062v1",
      "published_date": "2025-11-30 20:17:14 UTC",
      "updated_date": "2025-11-30 20:17:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:06:56.080013+00:00"
    },
    {
      "arxiv_id": "2512.01059v1",
      "title": "Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction",
      "title_zh": "å‚æ•°å‰Šå‡æå‡è§†è§‰ Transformerï¼šå‚æ•°å…±äº«ä¸å®½åº¦ç¼©å‡çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Anantha Padmanaban Krishna Kumar"
      ],
      "abstract": "Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\\% of the baseline parameters. Our \\emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\\% top-1 accuracy while maintaining the baseline computational cost. Our \\emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\\% top-1 accuracy with a 38\\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\\% to the range 0.03\\% to 0.06\\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ImageNet-1Kä¸Šè®­ç»ƒçš„ViT-B/16æ¨¡å‹ï¼Œæ¢è®¨äº†GroupedMLPå’ŒShallowMLPä¸¤ç§å‚æ•°å‰Šå‡ç­–ç•¥ï¼Œæ—¨åœ¨ç ”ç©¶æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´çš„éå•è°ƒå…³ç³»ã€‚GroupedMLPé€šè¿‡åœ¨ç›¸é‚»Transformerå—é—´å…±äº«MLPæƒé‡ï¼Œåœ¨ç»´æŒè®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†81.47%çš„Top-1å‡†ç¡®ç‡ã€‚ShallowMLPåˆ™é€šè¿‡å°†MLPéšè—ç»´åº¦å‡åŠï¼Œåœ¨è¾¾åˆ°81.25%å‡†ç¡®ç‡çš„åŒæ—¶å°†æ¨ç†ååé‡æå‡äº†38%ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§å‚æ•°å‡å°‘çš„æ¨¡å‹æ€§èƒ½å‡ä¼˜äº86.6Må‚æ•°çš„åŸºå‡†æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—æ”¹å–„äº†è®­ç»ƒç¨³å®šæ€§ï¼Œå¤§å¹…é™ä½äº†å‡†ç¡®ç‡é€€åŒ–ç°è±¡ã€‚ç ”ç©¶ç»“æœè¯æ˜ViT-B/16åœ¨æ ‡å‡†è®­ç»ƒæ–¹æ¡ˆä¸‹å¤„äºè¿‡åº¦å‚æ•°åŒ–(overparameterized)çŠ¶æ€ï¼Œé€‚å½“å‡å°‘MLPå®¹é‡ä¸ä»…ä¸ä¼šæŸå®³æ€§èƒ½ï¼Œåè€Œå¯èƒ½æå‡æ•ˆæœã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å‚æ•°å…±äº«å’Œå®½åº¦ç¼©å‡ç­‰æ¶æ„çº¦æŸå¯ä½œä¸ºæœ‰æ•ˆçš„å½’çº³åç½®(inductive biases)ï¼Œçªå‡ºäº†Vision Transformersè®¾è®¡ä¸­å‚æ•°åˆ†é…æ–¹å¼çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages total (6 pages main text, 1 page references), 1 figures, 2 tables. Code available at https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps",
      "pdf_url": "https://arxiv.org/pdf/2512.01059v1",
      "published_date": "2025-11-30 20:04:30 UTC",
      "updated_date": "2025-11-30 20:04:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:08:12.071044+00:00"
    },
    {
      "arxiv_id": "2512.01054v2",
      "title": "Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs",
      "title_zh": "é’ˆå¯¹ DDPM ä¸ VAE æœºå™¨é—å¿˜çš„è‡ªé€‚åº” Lambda å‡æ³•é‡è¦æ€§é‡‡æ ·è¯„åˆ†",
      "authors": [
        "MohammadParsa Dini",
        "Human Jafari"
      ],
      "abstract": "Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.\n  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.\n  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.\n  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ VAEs å’Œ DDPMsï¼‰çš„ Machine Unlearning é—®é¢˜ï¼Œæå‡ºäº† Adaptive-lambda SISS æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Static-lambda SISS æ–¹æ³•ä¸­å›ºå®šæ··åˆæƒé‡ $\\lambda$ å¯¼è‡´çš„æ€§èƒ½æ¬¡ä¼˜é—®é¢˜ã€‚ç ”ç©¶è€…å°† $\\lambda$ è½¬åŒ–ä¸ºåœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­åŠ¨æ€æ¨æ–­çš„æ½œåœ¨å˜é‡ï¼Œé€šè¿‡è½»é‡çº§æ¨ç†ç½‘ç»œæ ¹æ®å®æ—¶ SISS æŸå¤±é¡¹åŠå…¶æ¢¯åº¦å‚æ•°åŒ–è‡ªé€‚åº”åéªŒï¼Œä»è€Œå®ç°äº†æ‰©æ•£æ¨¡å‹ä¸æƒé‡æ¨æ–­æœºåˆ¶çš„è”åˆå˜åˆ†ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å°†è‡ªé€‚åº”åŸåˆ™æ‰©å±•è‡³ Score Forgetting Distillation åŠå…¶å¤šç±»åˆ«å˜ä½“ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢äº†ç»“åˆæ¢¯åº¦æ§åˆ¶çš„æ··åˆç›®æ ‡å‡½æ•°ä»¥åŠå°†å¸è½½å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹çš„ Reinforcement Learning æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaptive-lambda SISS åœ¨å¢å¼ºçš„ MNIST åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºåŸå§‹é™æ€æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å®ç°æ›´å½»åº•çš„å¿˜è®°ç±»åˆ«ç§»é™¤çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™ Retain Set çš„ç”Ÿæˆè´¨é‡ï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„éšç§åˆè§„æä¾›äº†æ›´çµæ´»ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01054v2",
      "published_date": "2025-11-30 19:57:49 UTC",
      "updated_date": "2025-12-15 08:37:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:10:43.335402+00:00"
    },
    {
      "arxiv_id": "2512.01047v1",
      "title": "Automating the Refinement of Reinforcement Learning Specifications",
      "title_zh": "å¼ºåŒ–å­¦ä¹ è§„çº¦ç»†åŒ–çš„è‡ªåŠ¨åŒ–",
      "authors": [
        "Tanmay Ambadkar",
        "ÄorÄ‘e Å½ikeliÄ‡",
        "Abhinav Verma"
      ],
      "abstract": "Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \\textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \\textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \\textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \\textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \\textsc{AutoSpec} are utilized.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Reinforcement Learningåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œå› é€»è¾‘è§„èŒƒ(Logical Specifications)å®šä¹‰è¿‡äºç²—ç•¥è€Œå¯¼è‡´æ™ºèƒ½ä½“éš¾ä»¥å­¦ä¹ æœ‰æ•ˆç­–ç•¥çš„é—®é¢˜ï¼Œæå‡ºäº†AutoSpecæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸€ç§æ¢ç´¢å¼•å¯¼ç­–ç•¥(exploration-guided strategy)æ¥è‡ªåŠ¨æœç´¢é€»è¾‘è§„èŒƒçš„ç²¾ç‚¼æ–¹æ¡ˆï¼Œæ—¨åœ¨æä¾›æ›´ä¸°å¯Œçš„å¼•å¯¼ä¿¡æ¯ä»¥é™ä½å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å­¦ä¹ éš¾åº¦ã€‚AutoSpecä¸“é—¨é€‚ç”¨äºSpectRLè§„èŒƒé€»è¾‘ï¼Œåˆ©ç”¨å…¶ç»„åˆç‰¹æ€§è®¾è®¡äº†å››ç§ç²¾ç‚¼ç¨‹åºæ¥ä¿®æ”¹è§„èŒƒçš„æŠ½è±¡å›¾ï¼ŒåŒ…æ‹¬ä¼˜åŒ–ç°æœ‰è¾¹è§„èŒƒæˆ–å¼•å…¥æ–°è¾¹è§„èŒƒã€‚ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜äº†è¿™äº›ç¨‹åºå‡èƒ½ä¿æŒè§„èŒƒçš„å¥å…¨æ€§(Soundness)ï¼Œå³æ»¡è¶³ç²¾ç‚¼è§„èŒƒçš„è½¨è¿¹å¿…ç„¶æ»¡è¶³åŸå§‹è§„èŒƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoSpecèƒ½å¤Ÿæœ‰æ•ˆé›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œé€šè¿‡åˆ©ç”¨å…¶ç”Ÿæˆçš„ç²¾ç‚¼é€»è¾‘è§„èŒƒï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿè§£å†³å¤æ‚æ§åˆ¶ä»»åŠ¡çš„èƒ½åŠ›å’Œæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01047v1",
      "published_date": "2025-11-30 19:32:33 UTC",
      "updated_date": "2025-11-30 19:32:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:11:14.361997+00:00"
    },
    {
      "arxiv_id": "2512.01046v1",
      "title": "Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids",
      "title_zh": "åº”ç”¨äºå—è¿è¡Œçº¦æŸè¿œç¨‹å¾®ç”µç½‘çš„å¼ºåŒ–å­¦ä¹ å±è”½æ§åˆ¶å•å…ƒ",
      "authors": [
        "Hadi Nekoei",
        "Alexandre Blondin MassÃ©",
        "Rachid Hassani",
        "Sarath Chandar",
        "Vincent Mai"
      ],
      "abstract": "Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.",
      "tldr_zh": "é’ˆå¯¹åè¿œå¾®ç½‘(Remote Microgrids)åœ¨èƒ½æºè½¬å‹ä¸­é¢ä¸´çš„å¤æ‚è¿è¡Œçº¦æŸå’Œå†³ç­–ä¸ç¡®å®šæ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºå±è”½æ§åˆ¶å•å…ƒ(Shielded Controller Units, SCUs)çš„ç³»ç»ŸåŒ–ä¸”å¯è§£é‡Šçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç³»ç»ŸåŠ¨åŠ›å­¦çš„å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡å±‚æ¬¡åŒ–ç»“æ„å°†ç¯å¢ƒåˆ†è§£ï¼Œä½¿æ¯ä¸ªSCUæ˜¾å¼åœ°ç®¡ç†ç‰¹å®šçš„è¿è¡Œçº¦æŸï¼Œä»è€Œç¡®ä¿å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ™ºèƒ½ä½“åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¸¥æ ¼éµå®ˆçº¦æŸæ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¾®ç½‘ä¼˜åŒ–ä»»åŠ¡ä¸­ï¼Œé…å¤‡SCUsçš„RLæ™ºèƒ½ä½“åœ¨æ»¡è¶³æ‰€æœ‰çº¦æŸçš„å‰æä¸‹ï¼ŒæˆåŠŸå°†ç‡ƒæ–™æ¶ˆè€—é™ä½äº†24%ï¼Œä¸”æ²¡æœ‰å¢åŠ ç”µæ± æŸè€—(Battery Degradation)ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†SCUsåœ¨å¤„ç†èƒ½æºè½¬å‹é¢†åŸŸå¤æ‚å†³ç­–æŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºRLæŠ€æœ¯åœ¨å…·æœ‰ä¸¥æ ¼å®‰å…¨è¦æ±‚çš„ç°å®ç”µåŠ›ç³»ç»Ÿä¸­çš„åº”ç”¨æä¾›äº†å¯é çš„å®‰å…¨ä¿éšœæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01046v1",
      "published_date": "2025-11-30 19:28:34 UTC",
      "updated_date": "2025-11-30 19:28:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:09:01.771053+00:00"
    },
    {
      "arxiv_id": "2512.01045v1",
      "title": "Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal",
      "title_zh": "Med-CRAFTï¼šé€šè¿‡çŸ¥è¯†å›¾è°±éå†è‡ªåŠ¨åŒ–æ„å»ºå¯è§£é‡Šçš„å¤šè·³è§†é¢‘å·¥ä½œè´Ÿè½½",
      "authors": [
        "Shenxi Liu",
        "Kan Li",
        "Mingyang Zhao",
        "Yuhang Tian",
        "Shoujun Zhou",
        "Bin Li"
      ],
      "abstract": "The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \\textbf{\\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Med-CRAFTï¼Œä¸€ç§æ—¨åœ¨è§£å†³åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ç¨€ç¼ºåŠåˆæˆæ•°æ®é€»è¾‘æ€§ä¸è¶³é—®é¢˜çš„ç¥ç»ç¬¦å·ï¼ˆneuro-symbolicï¼‰æ•°æ®å·¥ç¨‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†åŸºå‡†æµ‹è¯•åˆæˆå½¢å¼åŒ–ä¸ºç¡®å®šæ€§çš„å›¾éå†ï¼ˆgraph traversalï¼‰è¿‡ç¨‹ï¼Œé€šè¿‡ä»è§†é¢‘æµä¸­æå–æ‰‹æœ¯å™¨æ¢°ã€è§£å‰–è¾¹ç•Œç­‰è§†è§‰åŸè¯­ï¼ˆvisual primitivesï¼‰å¹¶æ„å»ºåŠ¨æ€æ—¶ç©ºçŸ¥è¯†å›¾è°±ï¼ˆSpatiotemporal Knowledge Graphï¼‰ï¼Œæœ‰æ•ˆå…‹æœäº†ç”Ÿæˆå¼æ–¹æ³•çš„éšæœºå¹»è§‰ã€‚é€šè¿‡å°†æŸ¥è¯¢ç”Ÿæˆé”šå®šåœ¨å›¾è°±è·¯å¾„ä¸Šï¼ŒMed-CRAFTä¸ºåˆæˆæ•°æ®æä¾›äº†ä¸¥è°¨çš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰é€»è¾‘è¿½æº¯ã€‚åŸºäºè¯¥æ¡†æ¶æ„å»ºçš„M3-Med-Autoæ•°æ®é›†å±•ç¤ºäº†æé«˜çš„ç»†ç²’åº¦æ—¶é—´é€‰æ‹©æ€§å’Œå¤šè·³é€»è¾‘å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è‡ªåŠ¨åŒ–æµæ°´çº¿ç”Ÿæˆçš„æŸ¥è¯¢è´Ÿè½½åœ¨å¤æ‚åº¦ä¸Šå¯ä¸ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†ç›¸åª²ç¾ï¼Œä¸”å›¾æ‹“æ‰‘ç»“æ„ä¸å…ˆè¿›MLLMsçš„æ¨ç†æ­¥éª¤é«˜åº¦å¯¹é½ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨åŒ»ç–—ç­‰å…³é”®é¢†åŸŸå®ç°å¯æ‰©å±•ã€ä½æˆæœ¬ä¸”å…·å¤‡å¯éªŒè¯é€»è¾‘çš„ç¨³å¥è¯„ä¼°æ–¹æ¡ˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.01045v1",
      "published_date": "2025-11-30 19:24:10 UTC",
      "updated_date": "2025-11-30 19:24:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:11:41.581512+00:00"
    },
    {
      "arxiv_id": "2512.01038v1",
      "title": "FMTK: A Modular Toolkit for Composable Time Series Foundation Model Pipelines",
      "title_zh": "FMTKï¼šç”¨äºå¯ç»„åˆæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹æµæ°´çº¿çš„æ¨¡å—åŒ–å·¥å…·åŒ…",
      "authors": [
        "Hetvi Shastri",
        "Pragya Sharma",
        "Walid A. Hanafy",
        "Mani Srivastava",
        "Prashant Shenoy"
      ],
      "abstract": "Foundation models (FMs) have opened new avenues for machine learning applications due to their ability to adapt to new and unseen tasks with minimal or no further training. Time-series foundation models (TSFMs) -- FMs trained on time-series data -- have shown strong performance on classification, regression, and imputation tasks. Recent pipelines combine TSFMs with task-specific encoders, decoders, and adapters to improve performance; however, assembling such pipelines typically requires ad hoc, model-specific implementations that hinder modularity and reproducibility. We introduce FMTK, an open-source, lightweight and extensible toolkit for constructing and fine-tuning TSFM pipelines via standardized backbone and component abstractions. FMTK enables flexible composition across models and tasks, achieving correctness and performance with an average of seven lines of code. https://github.com/umassos/FMTK",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FMTKï¼Œä¸€ä¸ªå¼€æºã€è½»é‡çº§ä¸”å¯æ‰©å±•çš„å·¥å…·åŒ…ï¼Œæ—¨åœ¨è§£å†³æ„å»ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(TSFMs)æµæ°´çº¿æ—¶ç”±äºæ¨¡å‹ç‰¹å®šå®ç°å¯¼è‡´çš„æ¨¡å—åŒ–ä¸å¯å¤ç°æ€§ä¸è¶³çš„é—®é¢˜ã€‚FMTKé€šè¿‡æä¾›æ ‡å‡†åŒ–çš„Backboneå’Œç»„ä»¶æŠ½è±¡ï¼Œå®ç°äº†å¯¹ä¸åŒä»»åŠ¡ç‰¹å®šç¼–ç å™¨ã€è§£ç å™¨åŠé€‚é…å™¨çš„çµæ´»ç»„åˆã€‚è¯¥å·¥å…·åŒ…æ”¯æŒè·¨æ¨¡å‹å’Œä»»åŠ¡çš„æ— ç¼é›†æˆï¼Œæ˜¾è‘—ç®€åŒ–äº†å¤æ‚Pipelineçš„æ„å»ºä¸å¾®è°ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFMTKèƒ½å¤Ÿåœ¨ä¿è¯ä»»åŠ¡æ­£ç¡®æ€§å’Œæ€§èƒ½çš„å‰æä¸‹ï¼Œä»…éœ€å¹³å‡ä¸ƒè¡Œä»£ç å³å¯å®Œæˆå¼€å‘ã€‚è¿™ä¸€å·¥å…·ä¸ºæ„å»ºå¯ç»„åˆçš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å·¥ä½œæµæä¾›äº†é«˜æ•ˆä¸”æ ‡å‡†åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01038v1",
      "published_date": "2025-11-30 19:14:04 UTC",
      "updated_date": "2025-11-30 19:14:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:08:37.181140+00:00"
    },
    {
      "arxiv_id": "2512.01037v2",
      "title": "When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals",
      "title_zh": "å½“å®‰å…¨æœºåˆ¶é˜»ç¢åˆç†åˆ¤æ–­ï¼šå¤§è¯­è¨€æ¨¡å‹æ‹’ç»è¡Œä¸ºä¸­çš„è¯­ä¹‰æ··æ·†åº¦é‡",
      "authors": [
        "Riad Ahmed Anonto",
        "Md Labid Al Nahiyan",
        "Md Tanvir Hassan"
      ],
      "abstract": "Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨å¯¹é½çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸è¯¯æ‹’æ— å®³æç¤ºçš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿè¯„ä¼°æ–¹æ³•å¿½ç•¥äº†æ¨¡å‹åœ¨å¤„ç†æ„æ€ç›¸è¿‘ä½†è¡¨è¾¾ä¸åŒçš„æç¤ºæ—¶çš„å±€éƒ¨ä¸ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Semantic Confusion è¿™ä¸€æ•…éšœæ¨¡å¼åŠå…¶æµ‹é‡æ¡†æ¶ï¼Œæ—¨åœ¨æ•æ‰è¿™ç§å› æªè¾å˜åŒ–å¼•èµ·çš„è¯­ä¹‰æ··æ·†ã€‚ç ”ç©¶æ„å»ºäº†åŒ…å«1ä¸‡ä¸ªæ”¹å†™ç°‡çš„è¯­æ–™åº“ ParaGuardï¼Œå¹¶æå‡ºäº† Confusion Indexã€Confusion Rate å’Œ Confusion Depth ä¸‰ç§æ¨¡å‹æ— å…³çš„æŒ‡æ ‡ã€‚è¿™äº›æŒ‡æ ‡é€šè¿‡åˆ†æ Token åµŒå…¥å’Œ Perplexity ä¿¡å·ï¼Œå°†æ‹’ç»å“åº”ä¸å…¶æœ€æ¥è¿‘çš„è¢«æ¥å—é‚»å±…è¿›è¡Œé‡åŒ–å¯¹æ¯”ã€‚è·¨å¤šç§æ¨¡å‹å®¶æ—çš„å®éªŒè¡¨æ˜ï¼Œä¼ ç»Ÿçš„å…¨å±€è¯¯æ‹’ç‡æ©ç›–äº†å…³é”®çš„æ¨¡å‹è¡Œä¸ºç»“æ„ï¼Œè€Œæ–°æŒ‡æ ‡èƒ½æ­ç¤ºä¸ç¨³å®šçš„å†³ç­–è¾¹ç•Œã€‚è¿™ç§æ„ŸçŸ¥æ··æ·†çš„å®¡è®¡æ–¹æ³•èƒ½å¤ŸåŒºåˆ†ç³»ç»Ÿæ‹’ç»çš„é¢‘ç‡ä¸åˆç†æ€§ï¼Œä¸ºå¼€å‘è€…æä¾›äº†åœ¨ä¿æŒå®‰å…¨æ€§çš„åŒæ—¶å‡å°‘è¯¯æ‹’çš„å®ç”¨ä¿¡å·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01037v2",
      "published_date": "2025-11-30 19:11:45 UTC",
      "updated_date": "2025-12-19 11:00:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:09:00.959538+00:00"
    },
    {
      "arxiv_id": "2512.01035v1",
      "title": "Goal-Oriented Multi-Agent Semantic Networking: Unifying Intents, Semantics, and Intelligence",
      "title_zh": "é¢å‘ç›®æ ‡çš„å¤šæ™ºèƒ½ä½“è¯­ä¹‰ç»„ç½‘ï¼šæ„å›¾ã€è¯­ä¹‰ä¸æ™ºèƒ½çš„ç»Ÿä¸€",
      "authors": [
        "Shutong Chen",
        "Qi Liao",
        "Adnan Aijaz",
        "Yansha Deng"
      ],
      "abstract": "6G services are evolving toward goal-oriented and AI-native communication, which are expected to deliver transformative societal benefits across various industries and promote energy sustainability. Yet today's networking architectures, built on complete decoupling of the applications and the network, cannot expose or exploit high-level goals, limiting their ability to adapt intelligently to service needs. This work introduces Goal-Oriented Multi-Agent Semantic Networking (GoAgentNet), a new architecture that elevates communication from data exchange to goal fulfilment. GoAgentNet enables applications and the network to collaborate by abstracting their functions into multiple collaborative agents, and jointly orchestrates multi-agent sensing, networking, computation, and control through semantic computation and cross-layer semantic networking, allowing the entire architecture to pursue unified application goals. We first outline the limitations of legacy network designs in supporting 6G services, based on which we highlight key enablers of our GoAgentNet design. Then, through three representative 6G usage scenarios, we demonstrate how GoAgentNet can unlock more efficient and intelligent services. We further identify unique challenges faced by GoAgentNet deployment and corresponding potential solutions. A case study on robotic fault detection and recovery shows that our GoAgentNet architecture improves energy efficiency by up to 99% and increases the task success rate by up to 72%, compared with the existing networking architectures without GoAgentNet, which underscores its potential to support scalable and sustainable 6G systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 6G æ¼”è¿›ä¸­é¢å‘ç›®æ ‡ (goal-oriented) å’Œ AI åŸç”Ÿ (AI-native) é€šä¿¡çš„éœ€æ±‚ï¼Œæå‡ºäº† GoAgentNet æ¶æ„ï¼Œæ—¨åœ¨å°†é€šä¿¡æœåŠ¡ä»å•çº¯çš„æ•°æ®äº¤æ¢æå‡ä¸ºç›®æ ‡è¾¾æˆ (goal fulfilment)ã€‚GoAgentNet å°†åº”ç”¨ç¨‹åºä¸ç½‘ç»œåŠŸèƒ½æŠ½è±¡ä¸ºå¤šä¸ªåä½œæ™ºèƒ½ä½“ (collaborative agents)ï¼Œé€šè¿‡è¯­ä¹‰è®¡ç®— (semantic computation) å’Œè·¨å±‚è¯­ä¹‰ç½‘ç»œ (cross-layer semantic networking) å®ç°æ„ŸçŸ¥ã€ç»„ç½‘ã€è®¡ç®—ä¸æ§åˆ¶çš„è”åˆç¼–æ’ã€‚è¯¥æ¶æ„æ‰“ç ´äº†ä¼ ç»Ÿç½‘ç»œä¸­åº”ç”¨ä¸ç½‘ç»œå®Œå…¨è§£è€¦çš„å±€é™æ€§ï¼Œä½¿æ•´ä¸ªæ¶æ„èƒ½å¤Ÿè¿½æ±‚ç»Ÿä¸€çš„åº”ç”¨ç›®æ ‡ã€‚é€šè¿‡å¯¹æœºå™¨äººæ•…éšœæ£€æµ‹ä¸æ¢å¤ç­‰ 6G å…¸å‹åœºæ™¯çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†è¯¥æ¶æ„åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰ç½‘ç»œæ¶æ„ç›¸æ¯”ï¼ŒGoAgentNet å°†èƒ½æºæ•ˆç‡æé«˜äº† 99%ï¼Œå¹¶å°†ä»»åŠ¡æˆåŠŸç‡æå‡äº† 72%ã€‚è¯¥ç ”ç©¶è¿˜è¯†åˆ«äº†éƒ¨ç½²ä¸­çš„æŒ‘æˆ˜å¹¶æå‡ºæ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ„å»ºå¯æ‰©å±•ä¸”å¯æŒç»­çš„ 6G ç³»ç»Ÿæä¾›äº†é‡è¦æ¡†æ¶ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "Submitting to IEEE for potential publications",
      "pdf_url": "https://arxiv.org/pdf/2512.01035v1",
      "published_date": "2025-11-30 19:04:17 UTC",
      "updated_date": "2025-11-30 19:04:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:09:16.785320+00:00"
    },
    {
      "arxiv_id": "2512.01034v2",
      "title": "Addressing the Plasticity-Stability Dilemma in Reinforcement Learning",
      "title_zh": "è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯å¡‘æ€§-ç¨³å®šæ€§å›°å¢ƒ",
      "authors": [
        "Mansi Maheshwari",
        "John C. Raisbeck",
        "Bruno Castro da Silva"
      ],
      "abstract": "Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­ç¥ç»ç½‘ç»œéšæ—¶é—´æ¨ç§»å‡ºç°å­¦ä¹ èƒ½åŠ›ä¸‹é™ï¼Œå³å¯å¡‘æ€§æŸå¤±(Plasticity Loss)çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚è™½ç„¶ä¼ ç»Ÿçš„å®šæœŸé‡ç½®(Reset)ç­–ç•¥èƒ½æ¢å¤å¯å¡‘æ€§ï¼Œä½†å¾€å¾€ä¼´éšç€æ€§èƒ½çš„æš‚æ—¶æ€§éª¤é™ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„å®‰å…¨å…³é”®åœºæ™¯ä¸­ååˆ†å±é™©ã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œä½œè€…æå‡ºäº†AltNetï¼Œä¸€ç§åˆ©ç”¨åŒç½‘ç»œ(Twin Networks)æœºåˆ¶åœ¨ä¸ç‰ºç‰²æ€§èƒ½ç¨³å®šæ€§çš„å‰æä¸‹æ¢å¤å¯å¡‘æ€§çš„æ–°æ–¹æ³•ã€‚åœ¨AltNetæ¡†æ¶ä¸‹ï¼Œä¸¤ä¸ªç½‘ç»œå‘¨æœŸæ€§åœ°äº¤æ›¿è§’è‰²ï¼šæ´»è·ƒç½‘ç»œåœ¨ç¯å¢ƒä¸­æ‰§è¡Œå¹¶å­¦ä¹ ï¼Œè€Œè¢«åŠ¨ç½‘ç»œåˆ™é€šè¿‡é‡æ”¾ç¼“å†²åŒº(Replay Buffer)è¿›è¡Œç¦»çº¿å­¦ä¹ ã€‚æ¯éš”å›ºå®šé—´éš”ï¼Œæ´»è·ƒç½‘ç»œä¼šè¢«é‡ç½®ï¼Œè€Œæ­¤æ—¶å·²ç§¯ç´¯ç»éªŒçš„è¢«åŠ¨ç½‘ç»œå°†æ— ç¼åˆ‡æ¢ä¸ºæ–°çš„æ´»è·ƒç½‘ç»œã€‚å®éªŒè¡¨æ˜ï¼ŒAltNetåœ¨DeepMind Control Suiteçš„å¤šä¸ªé«˜ç»´æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡(Sample Efficiency)å’Œæœ€ç»ˆæ€§èƒ½ï¼Œä¸ä»…æˆåŠŸæ¢å¤äº†ç½‘ç»œçš„å¯å¡‘æ€§ï¼Œè¿˜å®Œç¾è§„é¿äº†åŸºå‡†é‡ç½®æ–¹æ³•ä¸­å¸¸è§çš„æ€§èƒ½æ³¢åŠ¨é£é™©ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01034v2",
      "published_date": "2025-11-30 19:02:20 UTC",
      "updated_date": "2025-12-09 20:01:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:10:31.137968+00:00"
    },
    {
      "arxiv_id": "2512.01031v1",
      "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
      "title_zh": "VLASHï¼šåŸºäºæœªæ¥çŠ¶æ€æ„ŸçŸ¥å¼‚æ­¥æ¨ç†çš„å®æ—¶ VLAs",
      "authors": [
        "Jiaming Tang",
        "Yufei Sun",
        "Yilong Zhao",
        "Shang Yang",
        "Yujun Lin",
        "Zhuoyang Zhang",
        "James Hou",
        "Yao Lu",
        "Zhijian Liu",
        "Song Han"
      ],
      "abstract": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(VLAs)åœ¨å®é™…éƒ¨ç½²ä¸­å­˜åœ¨çš„åŠ¨ä½œåœé¡¿å’Œååº”å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†VLASHï¼Œä¸€ç§åŸºäºæœªæ¥çŠ¶æ€æ„ŸçŸ¥çš„é€šç”¨å¼‚æ­¥æ¨ç†æ¡†æ¶ã€‚è™½ç„¶å¼‚æ­¥æ¨ç†èƒ½æå‡æ§åˆ¶è¿ç»­æ€§ï¼Œä½†å¾€å¾€å› æ¨ç†æœŸé—´çš„çŠ¶æ€æ¼”å˜å¯¼è‡´é¢„æµ‹ä¸æ‰§è¡Œä¹‹é—´å‡ºç°æ—¶é—´é”™ä½(temporal misalignment)ã€‚VLASHé€šè¿‡åˆ©ç”¨å…ˆå‰ç”Ÿæˆçš„åŠ¨ä½œå—(action chunk)å‘å‰æ¨æ¼”æœºå™¨äººçŠ¶æ€ï¼Œä»è€Œä¼°è®¡æœªæ¥çš„æ‰§è¡Œæ—¶çŠ¶æ€ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„æˆ–å¢åŠ é¢å¤–å¼€é”€çš„å‰æä¸‹æœ‰æ•ˆå¼¥åˆäº†è¿™ä¸€å·®è·ã€‚å®éªŒè¡¨æ˜ï¼ŒVLASHåœ¨å®Œå…¨ä¿ç•™åŸå§‹ç²¾åº¦çš„åŸºç¡€ä¸Šï¼Œå®ç°äº†é«˜è¾¾2.03å€çš„æ¨ç†åŠ é€Ÿï¼Œå¹¶å°†ååº”å»¶è¿Ÿé™ä½äº†æœ€å¤š17.4å€ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æˆåŠŸèµ‹èƒ½VLAså¤„ç†æ‰“ä¹’ä¹“çƒå’Œæ‰“åœ°é¼ ç­‰å¯¹ååº”é€Ÿåº¦å’Œç²¾åº¦è¦æ±‚æé«˜çš„ä»»åŠ¡ï¼Œå…‹æœäº†ä¼ ç»ŸåŒæ­¥æ¨ç†çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01031v1",
      "published_date": "2025-11-30 18:59:24 UTC",
      "updated_date": "2025-11-30 18:59:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T12:11:28.315480+00:00"
    },
    {
      "arxiv_id": "2512.01025v1",
      "title": "Operator-Theoretic Framework for Gradient-Free Federated Learning",
      "title_zh": "æ— æ¢¯åº¦è”é‚¦å­¦ä¹ çš„ç®—å­ç†è®ºæ¡†æ¶",
      "authors": [
        "Mohit Kumar",
        "Mathias Brucker",
        "Alexander Valentinitsch",
        "Adnan Husakovic",
        "Ali Abbas",
        "Manuela GeiÃŸ",
        "Bernhard A. Moser"
      ],
      "abstract": "Federated learning must address heterogeneity, strict communication and computation limits, and privacy while ensuring performance. We propose an operator-theoretic framework that maps the $L^2$-optimal solution into a reproducing kernel Hilbert space (RKHS) via a forward operator, approximates it using available data, and maps back with the inverse operator, yielding a gradient-free scheme. Finite-sample bounds are derived using concentration inequalities over operator norms, and the framework identifies a data-dependent hypothesis space with guarantees on risk, error, robustness, and approximation. Within this space we design efficient kernel machines leveraging the space folding property of Kernel Affine Hull Machines. Clients transfer knowledge via a scalar space folding measure, reducing communication and enabling a simple differentially private protocol: summaries are computed from noise-perturbed data matrices in one step, avoiding per-round clipping and privacy accounting. The induced global rule requires only integer minimum and equality-comparison operations per test point, making it compatible with fully homomorphic encryption (FHE). Across four benchmarks, the gradient-free FL method with fixed encoder embeddings matches or outperforms strong gradient-based fine-tuning, with gains up to 23.7 points. In differentially private experiments, kernel smoothing mitigates accuracy loss in high-privacy regimes. The global rule admits an FHE realization using $Q \\times C$ encrypted minimum and $C$ equality-comparison operations per test point, with operation-level benchmarks showing practical latencies. Overall, the framework provides provable guarantees with low communication, supports private knowledge transfer via scalar summaries, and yields an FHE-compatible prediction rule offering a mathematically grounded alternative to gradient-based federated learning under heterogeneity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äº Gradient-Free Federated Learning çš„ Operator-Theoretic Frameworkï¼Œæ—¨åœ¨åŒæ—¶è§£å†³è”é‚¦å­¦ä¹ ä¸­çš„å¼‚æ„æ€§ã€é€šä¿¡è®¡ç®—é™åˆ¶åŠéšç§ä¿æŠ¤é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®—å­å°† $L^2$-optimal solution æ˜ å°„åˆ° Reproducing Kernel Hilbert Space (RKHS)ï¼Œå¹¶åˆ©ç”¨ Kernel Affine Hull Machines çš„ç©ºé—´æŠ˜å ç‰¹æ€§è®¾è®¡é«˜æ•ˆçš„æ ¸æœºå™¨ã€‚å®¢æˆ·ç«¯é€šè¿‡æ ‡é‡ç©ºé—´æŠ˜å åº¦é‡è¿›è¡ŒçŸ¥è¯†è¿ç§»ï¼Œæ˜¾è‘—é™ä½äº†é€šä¿¡å¼€é”€ï¼Œå¹¶æ”¯æŒä¸€ç§æ— éœ€é€è½®éšç§æ ¸ç®—çš„ Differential Privacy åè®®ã€‚ç”Ÿæˆçš„å…¨å±€è§„åˆ™ä»…éœ€æ•´æ•°æœ€å°å€¼å’Œç­‰å€¼æ¯”è¾ƒè¿ç®—ï¼Œä½¿å…¶ä¸ Fully Homomorphic Encryption (FHE) é«˜åº¦å…¼å®¹å¹¶åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè¾ƒä½å»¶è¿Ÿã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–ä¼˜äºå¼ºæ¢¯åº¦å¾®è°ƒåŸºçº¿ï¼Œå‡†ç¡®ç‡æœ€é«˜æå‡ 23.7 ä¸ªç™¾åˆ†ç‚¹ã€‚è¯¥æ¡†æ¶ä¸ºå¼‚æ„ç¯å¢ƒä¸‹çš„è”é‚¦å­¦ä¹ æä¾›äº†ä¸€ç§å…·æœ‰å¯è¯æ˜ç†è®ºä¿éšœã€ä½é€šä¿¡æˆæœ¬ä¸”æ”¯æŒéšç§ä¿æŠ¤çš„æ— æ¢¯åº¦æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01025v1",
      "published_date": "2025-11-30 18:49:00 UTC",
      "updated_date": "2025-11-30 18:49:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:11:00.543174+00:00"
    },
    {
      "arxiv_id": "2512.01020v1",
      "title": "Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics",
      "title_zh": "åŸºäºæ³•å¾‹é—®é¢˜æ ‘æŒ‡æ ‡çš„æ³•å¾‹æ¨ç†è¿‡ç¨‹è¯„ä¼°",
      "authors": [
        "Jinu Lee",
        "Kyoung-Woon On",
        "Simeng Han",
        "Arman Cohan",
        "Julia Hockenmaier"
      ],
      "abstract": "Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LEGIT (LEGal Issue Trees)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 2.4 ä¸‡ä¸ªå®ä¾‹çš„å¤§è§„æ¨¡ä¸“å®¶çº§æ³•å¾‹æ¨ç†æ•°æ®é›†ï¼Œé‡ç‚¹å…³æ³¨å¯¹æ¨ç†è¿‡ç¨‹ (reasoning traces) çš„è¯„ä¼°ã€‚ç ”ç©¶è€…å°†æ³•é™¢åˆ¤å†³è½¬åŒ–ä¸ºåŒ…å«å¯¹ç«‹åŒæ–¹è®ºç‚¹å’Œæ³•é™¢ç»“è®ºçš„å±‚çº§æ ‘ç»“æ„ï¼Œå¹¶å°†å…¶ä½œä¸ºè¯„ä¼°æ³•å¾‹é—®é¢˜è¦†ç›–èŒƒå›´å’Œæ­£ç¡®æ€§çš„è¯„åˆ†å‡†åˆ™ (rubrics)ã€‚é€šè¿‡äººç±»ä¸“å®¶æ ‡æ³¨ä»¥åŠä¸ç²—ç²’åº¦å‡†åˆ™çš„å¯¹æ¯”ï¼Œç ”ç©¶éªŒè¯äº†è¯¥è¯„ä¼°ä½“ç³»çš„å¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ³•å¾‹æ¨ç†èƒ½åŠ›å—åˆ°æ³•å¾‹é—®é¢˜è¦†ç›–é¢å’Œæ­£ç¡®æ€§çš„åŒé‡åˆ¶çº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ä¸åŸºäºè¯„åˆ†å‡†åˆ™çš„å¼ºåŒ–å­¦ä¹  (RL) åœ¨æå‡æ³•å¾‹æ¨ç†èƒ½åŠ›ä¸Šå…·æœ‰äº’è¡¥æ€§ã€‚RAG æ˜¾è‘—å¢å¼ºäº†æ•´ä½“æ¨ç†èƒ½åŠ›ï¼Œè€Œ RL è™½ç„¶ä¼šç¼©å°è¦†ç›–èŒƒå›´ï¼Œä½†åœ¨æé«˜æ¨ç†å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01020v1",
      "published_date": "2025-11-30 18:32:43 UTC",
      "updated_date": "2025-11-30 18:32:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:11:57.872166+00:00"
    },
    {
      "arxiv_id": "2512.01017v2",
      "title": "ChartAnchor: Chart Grounding with Structural-Semantic Fidelity",
      "title_zh": "ChartAnchorï¼šå…·å¤‡ç»“æ„è¯­ä¹‰ä¿çœŸåº¦çš„å›¾è¡¨å®šä½",
      "authors": [
        "Xinhang Li",
        "Jingbo Zhou",
        "Pengfei Luo",
        "Yixiong Xiao",
        "Tong Xu"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension. Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios. Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ChartAnchorï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾è¡¨ç†è§£ä¸­ç»“æ„ä¸è¯­ä¹‰å¿ å®åº¦é—®é¢˜çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•é›†ã€‚Chart grounding è¦æ±‚æ¨¡å‹åœ¨å›¾è¡¨çš„è§†è§‰å‘ˆç°ä¸ç»“æ„åŒ–è¯­ä¹‰ä¹‹é—´å»ºç«‹åŒå‘å¯¹é½ï¼Œè€ŒChartAnchor åŒ…å«äº†æ¶µç›–30ç§å›¾è¡¨ç±»å‹çš„8000å¤šç»„â€œå›¾è¡¨-è¡¨æ ¼-ä»£ç â€ä¸‰å…ƒç»„æ•°æ®ã€‚è¯¥åŸºå‡†æµ‹è¯•å¼•å…¥äº†å›¾è¡¨è½¬ä»£ç ï¼ˆchart-to-codeï¼‰ç”Ÿæˆå’Œå—æ§çš„å›¾è¡¨è½¬è¡¨æ ¼ï¼ˆchart-to-tableï¼‰é‡å»ºä¸¤é¡¹äº’è¡¥ä»»åŠ¡ï¼Œç”¨ä»¥äº¤å‰éªŒè¯è§†è§‰å’Œæ•°å€¼çš„å¿ å®åº¦ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒChartAnchor æ„å»ºäº†ä¸€ä¸ªæ•´åˆäº†è¯­ä¹‰éªŒè¯ã€é£æ ¼åˆ†æå’Œæ„ŸçŸ¥æŒ‡æ ‡çš„å¤šå±‚æ¬¡è¯„ä»·æ¡†æ¶ã€‚å¯¹ç°æœ‰ MLLMs çš„å¹¿æ³›å®éªŒæ­ç¤ºäº†å®ƒä»¬åœ¨æ•°å€¼ç²¾åº¦å’Œä»£ç åˆæˆæ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œå¼ºè°ƒäº†è¶…è¶Šè¡¨é¢æ„ŸçŸ¥ã€è¿›è¡Œç»“æ„åŒ–æ¨ç†çš„å¿…è¦æ€§ã€‚é€šè¿‡ç»Ÿä¸€ç¬¦å·åŒ–ä¸æ•°æ®é©±åŠ¨çš„ groundingï¼ŒChartAnchor ä¸ºç§‘ç ”ã€é‡‘èåŠå·¥ä¸šé¢†åŸŸçš„å›¾è¡¨ç†è§£ç ”ç©¶å¥ å®šäº†ä¸¥è°¨çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01017v2",
      "published_date": "2025-11-30 18:28:09 UTC",
      "updated_date": "2025-12-08 06:17:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:20.863876+00:00"
    },
    {
      "arxiv_id": "2512.01010v1",
      "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis",
      "title_zh": "å•å…ƒç‰©ç†é“¾ï¼šä¸€ç§ä»¥åŸºå…ƒä¸ºæ ¸å¿ƒçš„ç§‘å­¦ä»£ç åˆæˆæ–¹æ³•",
      "authors": [
        "Vansh Sharma",
        "Venkat Raman"
      ],
      "abstract": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Chain of Unit-Physicsï¼Œä¸€ç§ä»¥first-principlesï¼ˆæˆ–primitivesï¼‰ä¸ºæ ¸å¿ƒçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦è®¡ç®—ä»£ç ç”Ÿæˆä¸­é¢ä¸´çš„å¯é æ€§ä½å’Œç‰©ç†é€»è¾‘ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†äººç±»ä¸“å®¶çŸ¥è¯†ç¼–ç ä¸ºunit-physics testsï¼Œæ˜¾å¼åœ°çº¦æŸä»£ç ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„è½¯ä»¶ç¬¦åˆç‰©ç†è§„å¾‹ã€‚åœ¨å¤æ‚çš„ç‡ƒçƒ§æ¨¡æ‹Ÿä»»åŠ¡æµ‹è¯•ä¸­ï¼Œç°æœ‰çš„é—­æºç³»ç»Ÿå’Œå…·å¤‡Chain-of-Thoughtè§£ç èƒ½åŠ›çš„æ¨¡å‹å‡å› interface hallucinationså’Œç‰©ç†ä¸åè°ƒç­‰é—®é¢˜æ— æ³•ç”Ÿæˆæ­£ç¡®è§£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒChain of Unit-Physicsåœ¨5-6æ¬¡è¿­ä»£å†…å³å¯æ”¶æ•›ï¼Œå…¶ç»“æœä¸ä¸“å®¶ä»£ç çš„å¹³å‡è¯¯å·®ä»…ä¸º$3.1\\times10^{-3}$ %ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è¿è¡Œé€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ä¸Šåˆ†åˆ«æå‡äº†çº¦33.4%å’Œ30%ï¼Œä¸”æˆæœ¬ä¸ä¸­å‹å•†ä¸šAPIç›¸å½“ã€‚è¿™ä¸€æˆæœä¸ºphysics-groundedçš„ç§‘å­¦ä»£ç ç”Ÿæˆæä¾›äº†å®ç”¨æ¨¡ç‰ˆï¼Œè¯æ˜äº†åµŒå…¥ç¬¬ä¸€æ€§åŸç†åˆ†æåœ¨ç§‘å­¦è½¯ä»¶è‡ªåŠ¨åŒ–å¼€å‘ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "cs.SE",
        "physics.comp-ph",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01010v1",
      "published_date": "2025-11-30 18:16:50 UTC",
      "updated_date": "2025-11-30 18:16:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:08.058147+00:00"
    },
    {
      "arxiv_id": "2512.00999v1",
      "title": "Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints",
      "title_zh": "åŸºäºè½»é‡çº§åŒºå—é“¾éªŒè¯éšæ€§æŒ‡çº¹çš„æº¯æºé©±åŠ¨å‹å¯é è¯­ä¹‰åŒ»å­¦å›¾åƒçŸ¢é‡é‡å»º",
      "authors": [
        "Mohsin Rasheed",
        "Abdullah Al-Mamun"
      ],
      "abstract": "Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è¯­ä¹‰æ„ŸçŸ¥(semantic-aware)çš„åŒ»å­¦å›¾åƒé‡å»ºæ¡†æ¶ï¼Œä»¥è§£å†³ä¼ ç»Ÿåƒç´ çº§æ¢å¤æ–¹æ³•åœ¨é¢å¯¹æ•°æ®æŸåæˆ–ç¯¡æ”¹æ—¶éš¾ä»¥ä¿æŒè§£å‰–å­¦å¿ å®åº¦(anatomical fidelity)çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é«˜å±‚æ½œåµŒå…¥(high-level latent embeddings)ä¸æ··åˆ U-Net æ¶æ„ç›¸ç»“åˆï¼Œç¡®ä¿åœ¨ä¿®å¤è¿‡ç¨‹ä¸­èƒ½å¤Ÿç²¾å‡†ä¿ç•™å…·æœ‰ä¸´åºŠæ„ä¹‰çš„å…³é”®ç»“æ„ã€‚ä¸ºäº†å¢å¼ºç³»ç»Ÿçš„å¯ä¿¡åº¦ä¸é—®è´£åˆ¶ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæ— æ ‡åº¦å›¾(scale-free graph)è®¾è®¡çš„è½»é‡çº§åŒºå—é“¾(lightweight blockchain)æº¯æºå±‚ï¼Œå®ç°äº†å¯¹é‡å»ºè¿‡ç¨‹çš„ä½å¼€é”€ã€å¯éªŒè¯è®°å½•ã€‚å¤šé¡¹å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨ç»“æ„ä¸€è‡´æ€§(structural consistency)å’Œæ¢å¤å‡†ç¡®åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ç¡®ä¿äº†æº¯æºä¿¡æ¯çš„å®Œæ•´æ€§ã€‚è¿™ä¸€æˆæœé€šè¿‡èåˆè¯­ä¹‰å¼•å¯¼é‡å»ºä¸å®‰å…¨å¯è¿½æº¯æŠ€æœ¯ï¼Œæœ‰æ•ˆæå‡äº†åŒ»å­¦å½±åƒ AI çš„è¯Šæ–­å¯é æ€§å¹¶æ»¡è¶³äº†åŒ»ç–—ç¯å¢ƒä¸‹çš„ç›‘ç®¡åˆè§„è¦æ±‚ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00999v1",
      "published_date": "2025-11-30 17:48:55 UTC",
      "updated_date": "2025-11-30 17:48:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:04.081629+00:00"
    },
    {
      "arxiv_id": "2512.00997v1",
      "title": "IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch",
      "title_zh": "IndiMathBenchï¼šç»“åˆäººå·¥å‚ä¸çš„æ•°å­¦æ¨ç†é—®é¢˜è‡ªåŠ¨å½¢å¼åŒ–",
      "authors": [
        "Param Biyani",
        "Shashank Kirtania",
        "Yasharth Bajpai",
        "Sumit Gulwani",
        "Ashish Tiwari"
      ],
      "abstract": "We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \\benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†IndiMathBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç»äººå·¥éªŒè¯çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ•°å­¦å®šç†è¯æ˜èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«312ä¸ªæºè‡ªå°åº¦æ•°å­¦å¥¥æ—åŒ¹å…‹(Indian Mathematics Olympiads)çš„Lean 4å½¢å¼åŒ–å®šç†ï¼Œæ˜¯é€šè¿‡AIé©±åŠ¨çš„äººæœºåä½œæµæ°´çº¿å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–è€Œæˆçš„ã€‚è¯¥æµæ°´çº¿åˆ©ç”¨ç±»åˆ«æ£€ç´¢ã€ç¼–è¯‘å™¨åé¦ˆè¿­ä»£å’Œå¤šæ¨¡å‹é›†æˆç”Ÿæˆå€™é€‰æ–¹æ¡ˆï¼Œå¹¶ç”±ä¸“å®¶é€šè¿‡äº¤äº’å¼ä»ªè¡¨ç›˜è¿›è¡Œé«˜æ•ˆéªŒè¯ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè‡ªåŠ¨å½¢å¼åŒ–(autoformalization)åœ¨å½“å‰å‰æ²¿æ¨¡å‹ä¸­ä»å…·æŒ‘æˆ˜æ€§ï¼Œè¯­æ³•æœ‰æ•ˆæ€§ä¸è¯­ä¹‰æ­£ç¡®æ€§ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å³ä¾¿é‡‡ç”¨è¿­ä»£ä¼˜åŒ–ï¼Œå®šç†è¯æ˜çš„æˆåŠŸç‡ä¾ç„¶è¾ƒä½ï¼Œè¿™è¡¨æ˜IndiMathBenchä¸ºæ•°å­¦æ¨ç†èƒ½åŠ›çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00997v1",
      "published_date": "2025-11-30 17:40:13 UTC",
      "updated_date": "2025-11-30 17:40:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:47.179910+00:00"
    },
    {
      "arxiv_id": "2512.02076v1",
      "title": "FDRMFL:Multi-modal Federated Feature Extraction Model Based on Information Maximization and Contrastive Learning",
      "title_zh": "FDRMFLï¼šåŸºäºä¿¡æ¯æœ€å¤§åŒ–ä¸å¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€è”é‚¦ç‰¹å¾æå–æ¨¡å‹",
      "authors": [
        "Haozhe Wu"
      ],
      "abstract": "This study focuses on the feature extraction problem in multi-modal data regression. To address three core challenges in real-world scenarios: limited and non-IID data, effective extraction and fusion of multi-modal information, and susceptibility to catastrophic forgetting in model learning, a task-driven supervised multi-modal federated feature extraction method is proposed. The method integrates multi-modal information extraction and contrastive learning mechanisms, and can adapt to different neural network structures as the latent mapping functions for data of each modality. It supports each client to independently learn low-dimensional representations of multi-modal data, and can flexibly control the degree of retention of effective information about the response variable in the predictive variables within the low-dimensional features through parameter tuning. The multi-constraint learning framework constructed by the method guarantees regression accuracy using Mean Squared Error loss. Through the synergistic effect of mutual information preservation constraint, symmetric Kullback-Leibler divergence constraint, and inter-model contrastive constraint, it achieves the retention of task-related information, the extraction, fusion, and alignment of multi-modal features, and the mitigation of representation drift and catastrophic forgetting in non-IID scenarios, respectively. This ensures that the feature extraction process always centers on improving the performance of downstream regression tasks. Experimental results from simulations and real-world data analysis demonstrate that the proposed method achieves more significant performance improvement on downstream regression tasks compared with classical feature extraction techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ•°æ®å›å½’ä¸­çš„ç‰¹å¾æå–é—®é¢˜ï¼Œæå‡ºäº†FDRMFLï¼Œä¸€ç§åŸºäºä¿¡æ¯æœ€å¤§åŒ–(Information Maximization)å’Œå¯¹æ¯”å­¦ä¹ (Contrastive Learning)çš„ä»»åŠ¡é©±åŠ¨å‹ç›‘ç£è”é‚¦å­¦ä¹ æ¨¡å‹ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ç°å®åœºæ™¯ä¸­æ•°æ®å—é™ã€éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)ä»¥åŠç¾éš¾æ€§é—å¿˜(catastrophic forgetting)ç­‰æŒ‘æˆ˜ï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯æå–ä¸å¯¹æ¯”æœºåˆ¶ï¼Œæ”¯æŒå„å®¢æˆ·ç«¯ç‹¬ç«‹å­¦ä¹ ä½ç»´æ•°æ®è¡¨ç¤ºã€‚é€šè¿‡äº’ä¿¡æ¯ä¿å­˜(mutual information preservation)çº¦æŸã€å¯¹ç§°KLæ•£åº¦(symmetric Kullback-Leibler divergence)çº¦æŸä»¥åŠæ¨¡å‹é—´å¯¹æ¯”çº¦æŸçš„ååŒä½œç”¨ï¼Œè¯¥æ¨¡å‹å®ç°äº†ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„ä¿ç•™ã€å¤šæ¨¡æ€ç‰¹å¾çš„èåˆå¯¹é½ï¼Œå¹¶æœ‰æ•ˆç¼“è§£äº†è¡¨å¾åç§»ã€‚æ¨¡å‹é‡‡ç”¨å‡æ–¹è¯¯å·®(Mean Squared Error)æŸå¤±æ¥ç¡®ä¿å›å½’ç²¾åº¦ï¼Œå¹¶å…è®¸é€šè¿‡å‚æ•°è°ƒèŠ‚çµæ´»æ§åˆ¶ç‰¹å¾ä¸­æœ‰æ•ˆä¿¡æ¯çš„ä¿ç•™ç¨‹åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFDRMFLåœ¨ä¸‹æ¸¸å›å½’ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿç‰¹å¾æå–æŠ€æœ¯ï¼Œä¸ºå¤„ç†å¤æ‚è”é‚¦å¤šæ¨¡æ€æ•°æ®æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14pages,6figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02076v1",
      "published_date": "2025-11-30 17:13:35 UTC",
      "updated_date": "2025-11-30 17:13:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:11.584960+00:00"
    },
    {
      "arxiv_id": "2512.00979v1",
      "title": "An Approach to Variable Clustering: K-means in Transposed Data and its Relationship with Principal Component Analysis",
      "title_zh": "ä¸€ç§å˜é‡èšç±»æ–¹æ³•ï¼šè½¬ç½®æ•°æ®ä¸­çš„ K-means åŠå…¶ä¸ä¸»æˆåˆ†åˆ†æçš„å…³ç³»",
      "authors": [
        "Victor Saquicela",
        "Kenneth Palacio-Baus",
        "Mario Chifla"
      ],
      "abstract": "Principal Component Analysis (PCA) and K-means constitute fundamental techniques in multivariate analysis. Although they are frequently applied independently or sequentially to cluster observations, the relationship between them, especially when K-means is used to cluster variables rather than observations, has been scarcely explored. This study seeks to address this gap by proposing an innovative method that analyzes the relationship between clusters of variables obtained by applying K-means on transposed data and the principal components of PCA. Our approach involves applying PCA to the original data and K-means to the transposed data set, where the original variables are converted into observations. The contribution of each variable cluster to each principal component is then quantified using measures based on variable loadings. This process provides a tool to explore and understand the clustering of variables and how such clusters contribute to the principal dimensions of variation identified by PCA.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Variable Clustering ä¸Principal Component Analysis (PCA) ä¹‹é—´çš„è”ç³»ï¼Œå¡«è¡¥äº†K-means åœ¨è½¬ç½®æ•°æ®ä¸Šåº”ç”¨æ—¶ä¸PCA å…³ç³»çš„ç ”ç©¶ç©ºç™½ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨å°†å˜é‡è§†ä¸ºè§‚æµ‹å€¼çš„è½¬ç½®æ•°æ®é›†ä¸Šåº”ç”¨K-meansï¼Œæ¥åˆ†æå˜é‡ç°‡ä¸PCA ä¸»æˆåˆ†ä¹‹é—´çš„å…³è”ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºå˜é‡è½½è·(loadings) çš„åº¦é‡æŒ‡æ ‡ï¼Œå®šé‡è¯„ä¼°äº†æ¯ä¸ªå˜é‡ç°‡å¯¹å„ä¸»æˆåˆ†çš„è´¡çŒ®ã€‚è¿™ä¸€è¿‡ç¨‹ä¸ºç ”ç©¶è€…æä¾›äº†æ¢ç´¢å˜é‡èšç±»çš„æ–°å·¥å…·ï¼Œæœ‰åŠ©äºé˜æ˜ä¸åŒå˜é‡ç°‡å¦‚ä½•å…±åŒæ„å»ºPCA è¯†åˆ«å‡ºçš„ä¸»è¦å˜å¼‚ç»´åº¦ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Presented at conference and to appear in the proceedings of the 2025 IEEE Chilean Conference on Electrical, Electronics Engineering, Information and Communication Technologies (ChileCon)",
      "pdf_url": "https://arxiv.org/pdf/2512.00979v1",
      "published_date": "2025-11-30 16:53:07 UTC",
      "updated_date": "2025-11-30 16:53:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:24.677795+00:00"
    },
    {
      "arxiv_id": "2512.00969v1",
      "title": "Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE",
      "title_zh": "å°†å› æœåŸºç¡€æ¨¡å‹é›†æˆäºè§„èŒƒæ€§ç»´æŠ¤æ¡†æ¶ä»¥ä¼˜åŒ–ç”Ÿäº§çº¿ OEE",
      "authors": [
        "Felix Saretzky",
        "Lucas Andersen",
        "Thomas Engel",
        "Fazel Ansari"
      ],
      "abstract": "The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a \"what-if\" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†å› æœåŸºç¡€æ¨¡å‹(Causal Foundation Model)é›†æˆåˆ°è§„èŒƒæ€§ç»´æŠ¤(Prescriptive Maintenance)æ¡†æ¶ä¸­çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–ç”Ÿäº§çº¿çš„ç»¼åˆè®¾å¤‡æ•ˆç‡(OEE)ã€‚é’ˆå¯¹ä¼ ç»Ÿé¢„æµ‹æ¨¡å‹å› ä¾èµ–ä¼ªç›¸å…³è€Œæ— æ³•è¯†åˆ«æ•…éšœçœŸå®å› æœé©±åŠ¨å› ç´ çš„å±€é™æ€§ï¼Œè¯¥è®ºæ–‡åˆ©ç”¨å› æœæœºå™¨å­¦ä¹ (Causal Machine Learning)æ„å»ºäº†ä»è¯Šæ–­å‘ä¸»åŠ¨è§„èŒƒè½¬åŒ–çš„æ¡¥æ¢ã€‚ç ”ç©¶é€šè¿‡é¢„è®­ç»ƒçš„å› æœåŸºç¡€æ¨¡å‹ä½œä¸ºâ€œå‡è®¾åˆ†æâ€(what-if)å·¥å…·ï¼Œæ¨¡æ‹Ÿå¹¶è¯„ä¼°ä¸åŒå¹²é¢„æªæ–½å¯¹ç³»ç»Ÿçº§å…³é”®ç»©æ•ˆæŒ‡æ ‡(KPIs)çš„å› æœæ•ˆåº”ï¼Œä»è€Œæä¾›æ•°æ®é©±åŠ¨çš„è¡ŒåŠ¨å»ºè®®æ’åã€‚è¯¥æµç¨‹ä¸ä»…èƒ½å‡†ç¡®å®šä½æ ¹æœ¬åŸå› ï¼Œè¿˜èƒ½é‡åŒ–å…¶å¯¹ç”Ÿäº§è¿è¡Œçš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åŠåˆæˆåˆ¶é€ æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºå‡†æ¨¡å‹ï¼Œä¸ºå·¥ç¨‹å¸ˆæä¾›äº†æ›´æœ‰æ•ˆçš„è¿è¥å†³ç­–æ”¯æŒå¹¶å‡å°‘äº†æ˜‚è´µçš„åœæœºæ—¶é—´ã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 3 images, 1 table, conference paper",
      "pdf_url": "https://arxiv.org/pdf/2512.00969v1",
      "published_date": "2025-11-30 16:33:30 UTC",
      "updated_date": "2025-11-30 16:33:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:24.000463+00:00"
    },
    {
      "arxiv_id": "2512.00968v2",
      "title": "Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„ Xiaohongshu æœç´¢ç”Ÿæˆå¼æ’åºç›¸å…³æ€§ä¼˜åŒ–",
      "authors": [
        "Ziyang Zeng",
        "Heming Jing",
        "Jindong Chen",
        "Xiangli Li",
        "Hongyu Liu",
        "Yixuan He",
        "Zhengyu Li",
        "Yige Sun",
        "Zheyong Xie",
        "Yuqing Yang",
        "Shaosheng Cao",
        "Jun Fan",
        "Yi Wu",
        "Yao Hu"
      ],
      "abstract": "Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive offline evaluations and online A/B tests demonstrate that our approach consistently delivers significant improvements across key relevance and business metrics, validating its effectiveness, robustness, and practicality for large-scale industrial search systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°çº¢ä¹¦æœç´¢ä¸­ä¼ ç»Ÿç›¸å…³æ€§æ¨¡å‹è§£é‡Šæ€§ä¸è¶³åŠç”Ÿæˆå¼ç›¸å…³æ€§æ¨¡å‹ï¼ˆGenerative Relevance Models, GRMsï¼‰æ³›åŒ–å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰çš„è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç›¸å…³æ€§å»ºæ¨¡è½¬åŒ–ä¸ºæ¨ç†ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šæ­¥æ¨ç†æç¤ºï¼ˆMulti-step reasoning promptï¼‰è®¾è®¡ä¸­èå…¥äº†å…·ä½“çš„ä¸šåŠ¡ç›¸å…³æ€§æ ‡å‡†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¥åœ°æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†é€æ­¥ä¼˜åŠ¿æ©ç ï¼ˆStepwise Advantage Masking, SAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„è¿‡ç¨‹ç›‘ç£ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–ä¿¡ç”¨åˆ†é…æ¥å¢å¼ºæ¨¡å‹å¯¹å¤æ‚æ ‡å‡†çš„å­¦ä¹ æ•ˆæœã€‚ä¸ºæ»¡è¶³å·¥ä¸šéƒ¨ç½²éœ€æ±‚ï¼Œç ”ç©¶è¿›ä¸€æ­¥å°†å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å¾®è°ƒåçš„æ¨¡å‹è’¸é¦è‡³è½»é‡åŒ–ç‰ˆæœ¬ï¼Œä»¥é€‚é…çœŸå®çš„æœç´¢ç³»ç»Ÿã€‚ç¦»çº¿è¯„ä¼°ä¸åœ¨çº¿ A/B æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ¸å¿ƒç›¸å…³æ€§å’Œå„é¡¹ä¸šåŠ¡æŒ‡æ ‡ä¸Šå‡å®ç°äº†æ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†ç”Ÿæˆå¼æ¨ç†åœ¨ç›¸å…³æ€§å»ºæ¨¡ä¸­çš„æ½œåŠ›ï¼Œä¹Ÿä¸ºå¤§è§„æ¨¡å·¥ä¸šæœç´¢ç³»ç»Ÿæä¾›äº†å…·å¤‡ç¨³å¥æ€§å’Œå®ç”¨æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to the ADS Track at KDD 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.00968v2",
      "published_date": "2025-11-30 16:31:16 UTC",
      "updated_date": "2025-12-29 06:38:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:36.083177+00:00"
    },
    {
      "arxiv_id": "2512.00949v2",
      "title": "Multi-Modal AI for Remote Patient Monitoring in Cancer Care",
      "title_zh": "ç”¨äºç™Œç—‡æŠ¤ç†è¿œç¨‹æ‚£è€…ç›‘æµ‹çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½",
      "authors": [
        "Yansong Liu",
        "Ronnie Stafford",
        "Pramit Khetrapal",
        "Huriye Kocadag",
        "GraÃ§a Carvalho",
        "Patricia de Winter",
        "Maryam Imran",
        "Amelia Snook",
        "Adamos Hadjivasiliou",
        "D. Vijay Anand",
        "Weining Lin",
        "John Kelly",
        "Yukun Zhou",
        "Ivana Drobnjak"
      ],
      "abstract": "For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop. Best Paper Poster Award.)",
      "tldr_zh": "é’ˆå¯¹æ¥å—å…¨èº«æ€§ç™Œç—‡æ²»ç–—æ‚£è€…åœ¨é—¨è¯Šé—´æ­‡æœŸçš„ç›‘æ§ç¼ºå¤±é—®é¢˜ï¼Œè¯¥ç ”ç©¶å¼€å‘å¹¶å‰ç»æ€§åœ°è¯•ç”¨äº†ä¸€ç§ç”¨äºè¿œç¨‹æ‚£è€…ç›‘æµ‹ (Remote Patient Monitoring, RPM) çš„å¤šæ¨¡æ€ AI (Multi-modal AI) æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†æ¥è‡ª HALO-X å¹³å°çš„å¤šå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬äººå£ç»Ÿè®¡å­¦ä¿¡æ¯ã€å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ã€æ¯æ—¥é—®å·å’Œä¸´åºŠäº‹ä»¶ã€‚è¯¥è§‚å¯Ÿæ€§è¯•éªŒæ”¶é›†äº† 84 åæ‚£è€…è¶…è¿‡ 210 ä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œæ¶µç›–äº† 6080 ä¸ªç›‘æµ‹æ—¥ï¼Œæ˜¯åŒç±»ç ”ç©¶ä¸­è§„æ¨¡æœ€å¤§çš„ä¹‹ä¸€ã€‚ç ”ç©¶äººå‘˜å¼€å‘å¹¶æ”¹è¿›äº†å¤šæ¨¡æ€ AI æ¨¡å‹ï¼Œä»¥å¤„ç†ç°å®ä¸–ç•Œ RPM æ•°æ®ä¸­çš„å¼‚æ­¥æ€§å’Œä¸å®Œæ•´æ€§ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„ä¸è‰¯äº‹ä»¶ (Adverse Events) é£é™©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ°äº† 83.9% (AUROC=0.70)ï¼Œå¹¶è¯†åˆ«å‡ºæ—¢å¾€æ²»ç–—å²ã€å¥åº·è‡ªè¯„å’Œæ¯æ—¥æœ€å¤§å¿ƒç‡ (Daily Maximum Heart Rate) ä¸ºå…³é”®é¢„æµ‹ç‰¹å¾ã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œæ¨¡å‹èƒ½å¤Ÿé€šè¿‡è¾“å‡ºä¸æ–­å‡çº§çš„é£é™©æ¦‚å†µï¼Œåœ¨äº‹ä»¶å‘ç”Ÿå‰æä¾›æ—©æœŸé¢„è­¦ã€‚è¯¥å·¥ä½œç¡®ç«‹äº†å¤šæ¨¡æ€ AI åœ¨ç™Œç—‡è¿œç¨‹ç›‘æµ‹ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºå®ç°æ›´å…·ä¸»åŠ¨æ€§çš„æ‚£è€…æ”¯æŒæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00949v2",
      "published_date": "2025-11-30 16:01:50 UTC",
      "updated_date": "2026-01-08 16:55:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:12:57.676599+00:00"
    },
    {
      "arxiv_id": "2512.00947v2",
      "title": "Table as a Modality for Large Language Models",
      "title_zh": "è¡¨æ ¼ä½œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„ä¸€ç§æ¨¡æ€",
      "authors": [
        "Liyao Li",
        "Chao Ye",
        "Wentao Ye",
        "Yifei Sun",
        "Zhe Jiang",
        "Haobo Wang",
        "Jiaming Tian",
        "Yiming Zhang",
        "Ningtao Wang",
        "Xing Fu",
        "Gang Chen",
        "Junbo Zhao"
      ],
      "abstract": "To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†è¡¨æ ¼æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºç›®å‰çš„åºåˆ—åŒ–è¾“å…¥æ–¹å¼ä¼šå¯¼è‡´è¡¨æ ¼ç»“æ„ä¿¡æ¯çš„ä¸¥é‡ä¸¢å¤±ã€‚é€šè¿‡åœ¨StructQAåŸºå‡†ä¸Šçš„æ¢æµ‹å®éªŒå‘ç°ï¼Œå³ä¾¿æ˜¯æœ€å…ˆè¿›çš„LLMsåœ¨åº”å¯¹å¤æ‚çš„è¡¨æ ¼æ•°æ®æ—¶ä¹Ÿè¡¨ç°ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TAMOæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒç†å¿µæ˜¯å°†è¡¨æ ¼è§†ä¸ºä¸€ç§ç‹¬ç«‹æ¨¡æ€å¹¶ä¸æ–‡æœ¬Tokenè¿›è¡Œæ·±åº¦æ•´åˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨è¶…å›¾ç¥ç»ç½‘ç»œ(Hypergraph Neural Network)ä½œä¸ºå…¨å±€è¡¨æ ¼ç¼–ç å™¨ï¼Œå¹¶å°†å…¶ä¸ä¸»æµLLMæ— ç¼é›†æˆä»¥ä¿ç•™ç»“æ„åŒ–ç‰¹å¾ã€‚åœ¨HiTabã€WikiTQã€WikiSQLã€FeTaQAå’ŒStructQAç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTAMOæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹³å‡ç›¸å¯¹å¢ç›Šè¾¾åˆ°äº†42.65%ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.00947v2",
      "published_date": "2025-11-30 15:59:56 UTC",
      "updated_date": "2026-01-07 07:49:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:16.672681+00:00"
    },
    {
      "arxiv_id": "2512.00946v1",
      "title": "Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data",
      "title_zh": "é’ˆå¯¹å¼‚æ„é‡‘èæ–‡æœ¬æ•°æ®æƒ…æ„Ÿåˆ†ç±»çš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Alvaro Paredes Amorin",
        "Andre Python",
        "Christoph Weisser"
      ],
      "abstract": "Large language models (LLMs) play an increasingly important role in financial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inaccessible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs -- smaller and publicly available models designed to operate with limited computational resources -- to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) constitute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è½»é‡çº§å¼€æºå¤§è¯­è¨€æ¨¡å‹(lightweight open-source LLMs)åœ¨å¤„ç†æ¨ç‰¹ã€æ–°é—»å’ŒæŠ¥å‘Šç­‰å¼‚æ„é‡‘èæ–‡æœ¬æ•°æ®æ—¶çš„æƒ…æ„Ÿåˆ†ç±»æ€§èƒ½ã€‚ç ”ç©¶å¯¹æ¯”äº†åŸºå‡†é‡‘èè‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ¨¡å‹ FinBERT ä¸ DeepSeek-LLM 7Bã€Llama3 8B Instruct ä»¥åŠ Qwen3 8B ä¸‰æ¬¾å¼€æºè½»é‡çº§æ¨¡å‹ã€‚é€šè¿‡åœ¨ FinancialPhraseBank å’Œ Chinese Finance Sentiment ç­‰äº”ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒå‘ç°ï¼ŒQwen3 8B å’Œ Llama3 8B åœ¨å¤šæ•°åœºæ™¯ä¸‹è¡¨ç°æœ€ä½³ï¼Œä¸”ä»…éœ€ä½¿ç”¨ 5% çš„è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°ç†æƒ³æ•ˆæœã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹åœ¨é›¶æ ·æœ¬(zero-shot)å’Œå°‘æ ·æœ¬(few-shot)å­¦ä¹ åœºæ™¯ä¸­ä¹Ÿå±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è½»é‡çº§å¼€æº LLMs æ˜¯é‡‘èé¢†åŸŸä¸­ä¸€ç§æå…·æˆæœ¬æ•ˆç›Šçš„é€‰æ‹©ï¼Œå³ä½¿åœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®æ”¯æŒä¸‹ï¼Œä¹Ÿèƒ½åœ¨å¼‚æ„æ–‡æœ¬å¤„ç†ä¸­å–å¾—æå…·ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00946v1",
      "published_date": "2025-11-30 15:58:22 UTC",
      "updated_date": "2025-11-30 15:58:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:24.553371+00:00"
    },
    {
      "arxiv_id": "2512.00939v1",
      "title": "Constant-Time Motion Planning with Manipulation Behaviors",
      "title_zh": "ç»“åˆæ“æ§è¡Œä¸ºçš„å¸¸æ•°æ—¶é—´è¿åŠ¨è§„åˆ’",
      "authors": [
        "Nayesha Gandotra",
        "Itamar Mishani",
        "Maxim Likhachev"
      ],
      "abstract": "Recent progress in contact-rich robotic manipulation has been striking, yet most deployed systems remain confined to simple, scripted routines. One of the key barriers is the lack of motion planning algorithms that can provide verifiable guarantees for safety, efficiency and reliability. To address this, a family of algorithms called Constant-Time Motion Planning (CTMP) was introduced, which leverages a preprocessing phase to enable collision-free motion queries in a fixed, user-specified time budget (e.g., 10 milliseconds). However, existing CTMP methods do not explicitly incorporate the manipulation behaviors essential for object handling. To bridge this gap, we introduce the \\textit{Behavioral Constant-Time Motion Planner} (B-CTMP), an algorithm that extends CTMP to solve a broad class of two-step manipulation tasks: (1) a collision-free motion to a behavior initiation state, followed by (2) execution of a manipulation behavior (such as grasping or insertion) to reach the goal. By precomputing compact data structures, B-CTMP guarantees constant-time query in mere milliseconds while ensuring completeness and successful task execution over a specified set of states. We evaluate B-CTMP on two canonical manipulation tasks in simulation, shelf picking and plug insertion,and demonstrate its effectiveness on a real robot. Our results show that B-CTMP unifies collision-free planning and object manipulation within a single constant-time framework, providing provable guarantees of speed and success for manipulation in semi-structured environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Behavioral Constant-Time Motion Planner (B-CTMP)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰çš„ Constant-Time Motion Planning (CTMP) ç®—æ³•åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ç¼ºä¹æ˜¾å¼æ•´åˆæ“ä½œè¡Œä¸ºçš„é—®é¢˜ã€‚B-CTMP æ‰©å±•äº† CTMP çš„èƒ½åŠ›ï¼Œå°†å…¶åº”ç”¨äºåŒ…å«æ— ç¢°æ’è¿åŠ¨è‡³èµ·å§‹çŠ¶æ€ä»¥åŠéšåæ‰§è¡ŒæŠ“å–æˆ–æ’å…¥ç­‰æ“ä½œè¡Œä¸ºçš„ä¸¤æ­¥å¼ä»»åŠ¡ã€‚é€šè¿‡é¢„è®¡ç®—ç´§å‡‘çš„æ•°æ®ç»“æ„ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿä¿è¯åœ¨æ•°æ¯«ç§’çš„æ’å®šæ—¶é—´å†…å®ŒæˆæŸ¥è¯¢ï¼Œå¹¶ç¡®ä¿åœ¨ç‰¹å®šçŠ¶æ€é›†ä¸Šçš„å®Œå¤‡æ€§ä¸ä»»åŠ¡æˆåŠŸæ‰§è¡Œã€‚ç ”ç©¶å›¢é˜Ÿåœ¨è´§æ¶å–ç‰© (shelf picking) å’Œæ’å¤´æ’å…¥ (plug insertion) ä¸¤é¡¹å…¸å‹æ“ä½œä»»åŠ¡ä¸­è¿›è¡Œäº†ä»¿çœŸéªŒè¯ï¼Œå¹¶æˆåŠŸåœ¨çœŸå®æœºå™¨äººä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒB-CTMP æˆåŠŸå°†æ— ç¢°æ’è§„åˆ’ä¸ç‰©ä½“æ“ä½œç»Ÿä¸€åœ¨å•ä¸ªæ’å®šæ—¶é—´æ¡†æ¶å†…ï¼Œä¸ºåŠç»“æ„åŒ–ç¯å¢ƒä¸‹çš„æœºå™¨äººæ“ä½œæä¾›äº†é€Ÿåº¦ä¸æˆåŠŸç‡çš„å¯è¯æ˜ä¿éšœã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "In submission",
      "pdf_url": "https://arxiv.org/pdf/2512.00939v1",
      "published_date": "2025-11-30 15:42:35 UTC",
      "updated_date": "2025-11-30 15:42:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:24.273499+00:00"
    },
    {
      "arxiv_id": "2512.00938v1",
      "title": "DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics",
      "title_zh": "DeformArï¼šé€šè¿‡ç»„ä»¶åˆ†æä¸å¯è§†åˆ†æå¯¹å‘½åå®ä½“è¯†åˆ«è¯„ä¼°çš„å†æ€è€ƒ",
      "authors": [
        "Ahmed Mustafa Younes"
      ],
      "abstract": "Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.\n  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.\n  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the \"what,\" \"how,\" and \"why\" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­å‘½åå®ä½“è¯†åˆ« (Named Entity Recognition, NER) ä»»åŠ¡ä¸­æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† DeformAr è¿™ä¸€æ–°å‹è°ƒè¯•ä¸è¯„ä¼°æ¡†æ¶ã€‚DeformAr é›†æˆäº†æ•°æ®æå–åº“å’Œäº¤äº’å¼å¯è§†åŒ–ä»ªè¡¨æ¿ï¼Œé€šè¿‡å°†è¯­è¨€ç³»ç»Ÿåˆ’åˆ†ä¸ºæ•°æ®é›†å’Œæ¨¡å‹ç»„ä»¶ï¼Œæ·±å…¥æ¢ç©¶åˆ†è¯ã€æ•°æ®è´¨é‡å’Œæ ‡æ³¨ä¸€è‡´æ€§å¯¹æ€§èƒ½çš„å…±åŒå½±å“ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨ç»„ä»¶åˆ†æ (cross-component analysis) æä¾›ç³»ç»Ÿæ€§çš„è¯Šæ–­æªæ–½ï¼Œæ­ç¤ºæ€§èƒ½å·®å¼‚èƒŒåçš„æ·±å±‚åŸå› ï¼Œå¹¶ç»“åˆè¡Œä¸ºåˆ†æ (behavioural analysis) ä¸è¡¨ç¤ºç©ºé—´åˆ†æ (representation space analysis) æ¥è§£é‡Šæ¨¡å‹åœ¨è¯å…ƒå±‚é¢çš„å…·ä½“è¡¨ç°ã€‚è¿™ç§ç»„ä»¶æ„ŸçŸ¥çš„è¯Šæ–­è¿‡ç¨‹èƒ½å¤Ÿæ£€æµ‹æ¨¡å‹è¡Œä¸ºå¹¶å°†å…¶ä¸åº•å±‚è¡¨ç¤ºæ¨¡å¼åŠæ•°æ®å› ç´ ç›¸è”ç³»ã€‚ä½œä¸ºé¦–ä¸ªä¸“é—¨é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­è®¾è®¡çš„åŸºäºç»„ä»¶çš„å¯è§£é‡Šæ€§å·¥å…·ï¼ŒDeformAr ä¸ºæå‡ä½èµ„æºè¯­è¨€çš„æ¨¡å‹åˆ†æèƒ½åŠ›å¹¶ç¼©å°å…¶ä¸è‹±è¯­ç³»ç»Ÿçš„æ€§èƒ½å·®è·æä¾›äº†å…³é”®èµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD Thesis, University of Sussex, 2025. 311 pages, 140 figures, 32 tables. Submitted as a PDF-only. First supervisor: Julie Weeds. Second supervisor: David Weir",
      "pdf_url": "https://arxiv.org/pdf/2512.00938v1",
      "published_date": "2025-11-30 15:39:28 UTC",
      "updated_date": "2025-11-30 15:39:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:24.762584+00:00"
    },
    {
      "arxiv_id": "2512.00931v1",
      "title": "Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study",
      "title_zh": "ç¼“è§£é›¶æ ·æœ¬ç§‘å­¦æ‘˜è¦ä¸­çš„å¹»è§‰ï¼šä¸€é¡¹åˆæ­¥ç ”ç©¶",
      "authors": [
        "Imane Jaaouine",
        "Ross D. King"
      ],
      "abstract": "Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a baseline prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment between the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹(Prompt Engineering)ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é›¶æ ·æœ¬(Zero-Shot)ç§‘å­¦æ–‡æœ¬æ‘˜è¦ä¸­å‡ºç°çš„è¯­å¢ƒä¸ä¸€è‡´å¹»è§‰é—®é¢˜ã€‚ä½œè€…ä»¥é…µæ¯ç”Ÿç‰©æŠ€æœ¯é¢†åŸŸçš„è®ºæ–‡æ‘˜è¦ä¸ºæ ·æœ¬ï¼Œå¯¹æ¯”äº†åŸºçº¿æç¤ºã€ä¸åŒå¤æ‚åº¦çš„æŒ‡ä»¤ã€è¯­å¢ƒé‡å¤(Context Repetition, CR)ä»¥åŠéšæœºæ·»åŠ (Random Addition, RA)ç­‰ä¸ƒç§æç¤ºç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨ROUGEã€BERTScoreå’ŒMETEORç­‰æŒ‡æ ‡å¯¹6ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ç”Ÿæˆçš„336ä»½æ‘˜è¦è¿›è¡Œè¯æ³•ä¸è¯­ä¹‰å¯¹é½è¯„ä¼°ï¼Œç ”ç©¶å‘ç°CRå’ŒRAèƒ½æ˜¾è‘—æå‡æ‘˜è¦ä¸åŸæ–‡çš„è¯æ³•ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåˆç†çš„æç¤ºå·¥ç¨‹åœ¨å‡è½»é›¶æ ·æœ¬ç§‘å­¦æ‘˜è¦ä»»åŠ¡ä¸­çš„å¹»è§‰ç°è±¡æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä¸ºæå‡å­¦æœ¯æ–‡çŒ®è‡ªåŠ¨æ‘˜è¦çš„å¯é æ€§æä¾›äº†å®éªŒä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00931v1",
      "published_date": "2025-11-30 15:19:41 UTC",
      "updated_date": "2025-11-30 15:19:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:35.461568+00:00"
    },
    {
      "arxiv_id": "2512.00918v1",
      "title": "Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models",
      "title_zh": "æå°‘æ•°ç¥ç»å…ƒæ¶ˆèå¼•å‘å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹è¯­è¨€æ ¸å¿ƒçš„ç¾éš¾æ€§å´©æºƒ",
      "authors": [
        "Cen Lu",
        "Yung-Chen Tang",
        "Andrea Cavallaro"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (Large Vision-Language Models, LVLMs) çš„ç»“æ„è„†å¼±æ€§ï¼Œæ—¨åœ¨è¯†åˆ«ç§»é™¤åä¼šè§¦å‘ç¾éš¾æ€§å´©æºƒ (catastrophic collapse) çš„å…³é”®ç¥ç»å…ƒã€‚ä½œè€…æå‡ºäº†åä¸º CAN çš„æ–¹æ³•ï¼Œé€šè¿‡é€æ­¥æ©ç  (progressive masking) å®šä½ä¸€è‡´æ¿€æ´»ç¥ç»å…ƒ (Consistently Activated Neurons)ï¼Œå¹¶åœ¨ LLaVA-1.5-7b-hf å’Œ InstructBLIP ç­‰æ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…å±è”½è¯­è¨€æ¨¡å‹å‰é¦ˆç½‘ç»œ (feed-forward networks) ä¸­æå°‘æ•°ç¥ç»å…ƒï¼ˆæœ€å°‘ä»…éœ€4ä¸ªï¼‰å³å¯å¯¼è‡´æ¨¡å‹å´©æºƒï¼Œä¸”è¿™äº›è„†å¼±ç‚¹ä¸»è¦é›†ä¸­åœ¨è¯­è¨€æ¨¡å‹çš„ä¸‹æŠ•å½±å±‚ (down-projection layer) è€Œéè§†è§‰ç»„ä»¶ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°æ¨¡å‹å´©æºƒéµå¾ªä»åˆå§‹è¡¨è¾¾é€€åŒ– (expressive degradation) åˆ°çªç„¶å®Œå…¨å´©æºƒçš„ä¸¤é˜¶æ®µæ¨¡å¼ã€‚è¿™äº›å‘ç°æ­ç¤ºäº† LVLMs è¯­è¨€æ ¸å¿ƒçš„æåº¦ä¸ç¨³å®šæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§ç ”ç©¶æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 6 figures,",
      "pdf_url": "https://arxiv.org/pdf/2512.00918v1",
      "published_date": "2025-11-30 14:52:11 UTC",
      "updated_date": "2025-11-30 14:52:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:31.966324+00:00"
    },
    {
      "arxiv_id": "2512.00912v1",
      "title": "ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices",
      "title_zh": "ForamDeepSliceï¼šåŸºäºäºŒç»´ Micro-CT åˆ‡ç‰‡çš„é«˜ç²¾åº¦æœ‰å­”è™«ç‰©ç§åˆ†ç±»æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Abdelghafour Halimi",
        "Ali Alibrahim",
        "Didier Barradas-Bautista",
        "Ronell Sicat",
        "Abdulkader M. Afifi"
      ],
      "abstract": "This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ForamDeepSliceï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡3Dæ‰«æç”Ÿæˆçš„2D micro-CTåˆ‡ç‰‡å®ç°æœ‰å­”è™«(foraminifera)ç‰©ç§é«˜ç²¾åº¦è‡ªåŠ¨åŒ–åˆ†ç±»çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«97ä¸ªæ ‡æœ¬ã€æ¶µç›–12ä¸ªä»£è¡¨æ€§ç‰©ç§çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨æ ‡æœ¬çº§æ•°æ®åˆ‡åˆ†(specimen-level data splitting)æ–¹æ³•ä»¥ä¸¥è°¨é˜²æ­¢æ•°æ®æ³„éœ²ã€‚é€šè¿‡è¯„ä¼°ä¸ƒç§å…ˆè¿›çš„2Då·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ¶æ„å¹¶åˆ©ç”¨è¿ç§»å­¦ä¹ ï¼Œæœ€ç»ˆå¼€å‘çš„ConvNeXt-Largeä¸EfficientNetV2-Smallé›†æˆæ¨¡å‹(ensemble model)åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†95.64%çš„å‡†ç¡®ç‡å’Œ0.998çš„AUCã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼€å‘äº†æ”¯æŒå®æ—¶åˆ†ç±»å’Œ3Dåˆ‡ç‰‡åŒ¹é…çš„äº¤äº’å¼ä»ªè¡¨æ¿ï¼Œåˆ©ç”¨SSIMã€NCCå’ŒDiceç³»æ•°ç­‰æŒ‡æ ‡æå‡å®ç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºAIè¾…åŠ©çš„å¾®ä½“å¤ç”Ÿç‰©å­¦é‰´å®š(micropaleontological identification)å»ºç«‹äº†æ–°åŸºå‡†ï¼Œæœ‰æ•ˆå¡«è¡¥äº†æ·±åº¦å­¦ä¹ ä¸åº”ç”¨åœ°çƒç§‘å­¦ä¹‹é—´çš„æŠ€æœ¯ç©ºç™½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00912v1",
      "published_date": "2025-11-30 14:30:16 UTC",
      "updated_date": "2025-11-30 14:30:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:58.371784+00:00"
    },
    {
      "arxiv_id": "2512.00908v1",
      "title": "Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs",
      "title_zh": "è¶…è¶Šé«˜ç†µæ¢ç´¢ï¼šé¢å‘æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„æ­£ç¡®æ€§æ„ŸçŸ¥å‹ä½ç†µç‰‡æ®µä¼˜åŠ¿å¡‘é€ ",
      "authors": [
        "Xinzhu Chen",
        "Xuesheng Li",
        "Zhongxiang Sun",
        "Weijie Yu"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning with Verifiable Rewards, RLVR)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸­çš„åº”ç”¨ï¼ŒæŒ‘æˆ˜äº†ä»¥å¾€ç ”ç©¶è¿‡åº¦å…³æ³¨é«˜ç†µ(High-Entropy)æ ‡è®°è€Œå¿½è§†ä½ç†µç‰‡æ®µ(Low-Entropy Segments)çš„è§‚ç‚¹ã€‚é€šè¿‡å®šé‡åˆ†æï¼Œä½œè€…å‘ç°æ­£ç¡®å“åº”ä¸­ä½ç†µç‰‡æ®µçš„é‡å ä¸æ¨¡å‹å‡†ç¡®ç‡é«˜åº¦ç›¸å…³ï¼Œè€Œé”™è¯¯å“åº”åˆ™åŒ…å«ç¨³å®šä½†æ— æ•ˆçš„ç»“æ„æ¨¡å¼ã€‚æ®æ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸ºLESSçš„æ­£ç¡®æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¯¹ä½ç†µç‰‡æ®µè¿›è¡Œç»†ç²’åº¦çš„ä¼˜åŠ¿è°ƒèŠ‚(Advantage Modulation)ã€‚LESSåœ¨ä¿ç•™åº•å±‚å¼ºåŒ–å­¦ä¹ ç®—æ³•é«˜ç†µæ¢ç´¢èƒ½åŠ›çš„åŒæ—¶ï¼Œé€šè¿‡æ”¾å¤§æ­£ç¡®ç­”æ¡ˆç‹¬æœ‰çš„ç‰‡æ®µã€æŠ‘åˆ¶é”™è¯¯ç­”æ¡ˆç‰¹æœ‰çš„ç‰‡æ®µä»¥åŠä¸­å’Œå…±æœ‰ç‰‡æ®µæ¥ä¼˜åŒ–ç­–ç•¥ã€‚åœ¨GRPOç®—æ³•åŸºç¡€ä¸Šçš„å®éªŒè¯æ˜ï¼ŒLESSåœ¨ä¸‰ç§éª¨å¹²æ¨¡å‹å’Œå…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å‡æŒç»­æå‡äº†å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨æ€§èƒ½åº•çº¿ä¸Šçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00908v1",
      "published_date": "2025-11-30 14:19:36 UTC",
      "updated_date": "2025-11-30 14:19:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:40.380449+00:00"
    },
    {
      "arxiv_id": "2512.00888v2",
      "title": "Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models",
      "title_zh": "è½»é‡çº§åŸºå‡†æµ‹è¯•æ­ç¤ºé›¶æ ·æœ¬è¡¨æ ¼åŸºç¡€æ¨¡å‹éšè—çš„ç¡¬ä»¶å¼€é”€",
      "authors": [
        "Ishaan Gangwani",
        "Aayam Bansal"
      ],
      "abstract": "Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯å®Œå…¨å¤ç°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ­ç¤ºé›¶æ ·æœ¬è¡¨æ ¼åŸºç¡€æ¨¡å‹(Zero-Shot Tabular Foundation Models)åœ¨ç¡¬ä»¶æˆæœ¬æ–¹é¢çš„éšè—å¼€é”€ã€‚ç ”ç©¶äººå‘˜åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šï¼Œå°† TabPFN å’Œ TabICL ä¸¤ç§åŸºç¡€æ¨¡å‹ä¸ç»è¿‡è°ƒä¼˜çš„ XGBoostã€LightGBM åŠ Random Forest ç­‰æ ‘é›†æˆæ¨¡å‹(Tree Ensembles)è¿›è¡Œäº†å¯¹æ¯”ï¼Œé‡ç‚¹è¯„ä¼°äº†å‡†ç¡®ç‡ã€æ¨ç†å»¶è¿Ÿä»¥åŠå†…å­˜(RAM/VRAM)å ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ ‘é›†æˆæ¨¡å‹åœ¨å¤šæ•°ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸åŸºç¡€æ¨¡å‹æŒå¹³ç”šè‡³æ›´ä¼˜ï¼Œä¸”ä»…éœ€æä½çš„å†…å­˜å’Œé›¶æ˜¾å­˜å ç”¨ï¼Œæ¨ç†é€Ÿåº¦å¿«è¿‡åŸºç¡€æ¨¡å‹æ•°ä¸‡å€ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒTabICL å’Œ TabPFN è™½ç„¶èƒ½å®ç°è®­ç»ƒå‰é¢„æµ‹ï¼Œä½†é¢ä¸´ç€å·¨å¤§çš„ GPU VRAM æ¶ˆè€—å’Œæé«˜çš„æ¨ç†å»¶è¿Ÿï¼Œä¸”åœ¨å¤„ç†å¤§è§„æ¨¡è¡¨æ ¼æ•°æ®æ—¶å­˜åœ¨æ˜æ˜¾çš„æ‰©å±•æ€§é™åˆ¶ã€‚è¯¥ç ”ç©¶é‡åŒ–äº†å½“å‰è¡¨æ ¼åŸºç¡€æ¨¡å‹åœ¨æ€§èƒ½ä¸ç¡¬ä»¶èµ„æºä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºæœªæ¥æ•ˆç‡å¯¼å‘çš„å­¦æœ¯ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML NewInML",
      "pdf_url": "https://arxiv.org/pdf/2512.00888v2",
      "published_date": "2025-11-30 13:17:08 UTC",
      "updated_date": "2025-12-16 10:51:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:45.157307+00:00"
    },
    {
      "arxiv_id": "2512.20631v1",
      "title": "Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams",
      "title_zh": "Transformer æƒ…æ„Ÿæ¨¡å‹çš„é›¶è®­ç»ƒæ—¶åºæ¼‚ç§»æ£€æµ‹ï¼šåŸºäºçœŸå®ç¤¾äº¤åª’ä½“æµçš„å…¨é¢åˆ†æ",
      "authors": [
        "Aayam Bansal",
        "Ishaan Gangwani"
      ],
      "abstract": "We present a comprehensive zero-training temporal drift analysis of transformer-based sentiment models validated on authentic social media data from major real-world events. Through systematic evaluation across three transformer architectures and rigorous statistical validation on 12,279 authentic social media posts, we demonstrate significant model instability with accuracy drops reaching 23.4% during event-driven periods. Our analysis reveals maximum confidence drops of 13.0% (Bootstrap 95% CI: [9.1%, 16.5%]) with strong correlation to actual performance degradation. We introduce four novel drift metrics that outperform embedding-based baselines while maintaining computational efficiency suitable for production deployment. Statistical validation across multiple events confirms robust detection capabilities with practical significance exceeding industry monitoring thresholds. This zero-training methodology enables immediate deployment for real-time sentiment monitoring systems and provides new insights into transformer model behavior during dynamic content periods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäº Transformer çš„æƒ…æ„Ÿæ¨¡å‹åœ¨çœŸå®ç¤¾äº¤åª’ä½“æ•°æ®æµä¸­çš„æ—¶é—´åç§»(temporal drift)é—®é¢˜ï¼Œè¿›è¡Œäº†ä¸€é¡¹è¯¦å°½çš„é›¶è®­ç»ƒ(zero-training)åˆ†æã€‚é€šè¿‡å¯¹12,279æ¡çœŸå®ç¤¾äº¤åª’ä½“å¸–å­è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨äº‹ä»¶é©±åŠ¨æœŸé—´å­˜åœ¨ä¸¥é‡çš„ä¸ç¨³å®šæ€§ï¼Œå‡†ç¡®ç‡æœ€é«˜ä¸‹é™23.4%ã€‚åˆ†ææ­ç¤ºäº†æ¨¡å‹ç½®ä¿¡åº¦æœ€å¤§ä¸‹é™13.0%ï¼Œä¸”è¯¥æŒ‡æ ‡ä¸å®é™…æ€§èƒ½é€€åŒ–å…·æœ‰å¼ºç›¸å…³æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†å››ç§æ–°å‹åç§»æŒ‡æ ‡ï¼Œå…¶åœ¨ä¿æŒç”Ÿäº§éƒ¨ç½²æ‰€éœ€çš„è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæ€§èƒ½ä¼˜äºåŸºäºåµŒå…¥(embedding-based)çš„åŸºå‡†æ¨¡å‹ã€‚è·¨å¤šä¸ªäº‹ä»¶çš„ç»Ÿè®¡éªŒè¯ç¡®è®¤äº†è¯¥æ–¹æ³•å…·æœ‰ç¨³å¥çš„æ£€æµ‹èƒ½åŠ›ï¼Œå…¶å®è·µæ„ä¹‰è¶…è¿‡äº†è¡Œä¸šç›‘æ§é˜ˆå€¼ã€‚è¿™ç§é›¶è®­ç»ƒæ–¹æ³•èƒ½å¤Ÿå®ç°å®æ—¶æƒ…æ„Ÿç›‘æ§ç³»ç»Ÿçš„å³æ—¶éƒ¨ç½²ï¼Œå¹¶ä¸º Transformer æ¨¡å‹åœ¨åŠ¨æ€å†…å®¹æ—¶æœŸçš„è¡Œä¸ºç‰¹å¾æä¾›äº†æ–°çš„è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML NewInML",
      "pdf_url": "https://arxiv.org/pdf/2512.20631v1",
      "published_date": "2025-11-30 13:08:59 UTC",
      "updated_date": "2025-11-30 13:08:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:13:47.578783+00:00"
    },
    {
      "arxiv_id": "2512.00882v3",
      "title": "Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints",
      "title_zh": "Look, Recite, Then Answerï¼šé€šè¿‡è‡ªç”ŸæˆçŸ¥è¯†æç¤ºæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½",
      "authors": [
        "Xisheng Feng"
      ],
      "abstract": "Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to \"Reasoning-Driven Hallucination\" where linguistic priors override visual perception. A key bottleneck is the \"Modality Gap\": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose \"Look, Recite, Then Answer,\" a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.52% over Qwen2-VL-72B and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨ç²¾å‡†å†œä¸šç­‰ä¸“ä¸šé¢†åŸŸä¸­å›  \"Reasoning-Driven Hallucination\" å’Œ \"Modality Gap\" å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº† \"Look, Recite, Then Answer\" æ¡†æ¶ã€‚è¿™ä¸€å‚æ•°é«˜æ•ˆçš„æ¡†æ¶åœ¨ä¿æŒä¸»å¹²æ¨¡å‹å†»ç»“çš„å‰æä¸‹ï¼Œå°†æ¨ç†è¿‡ç¨‹è§£æ„ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆåœ¨ \"Look\" é˜¶æ®µç”Ÿæˆå®¢è§‚è§†è§‰æè¿°å’Œå€™é€‰é›†ï¼Œéšååœ¨ \"Recite\" é˜¶æ®µåˆ©ç”¨è½»é‡åŒ–çš„ 1.7B è·¯ç”±å°†è§†è§‰çº¿ç´¢è½¬åŒ–ä¸ºæŸ¥è¯¢ä»¥è§¦å‘ç‰¹å®šçš„å‚æ•°åŒ–çŸ¥è¯† (parametric knowledge)ï¼Œæœ€ååœ¨ \"Answer\" é˜¶æ®µé€šè¿‡å¹¶è¡Œè¯æ®å¯¹é½ (evidence alignment) é€‰æ‹©ä¸€è‡´æ€§æœ€é«˜çš„æ ‡ç­¾ã€‚åœ¨ AgroBench è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ‚è‰è¯†åˆ« (Weed Identification) ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æ¯” Qwen2-VL-72B æé«˜äº† 23.52%ï¼Œè¡¨ç°ä¼˜äº GPT-4o ä¸”æ— é¢å¤–æœç´¢å¼€é”€ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡å°†è¢«åŠ¨æ„ŸçŸ¥è½¬åŒ–ä¸ºä¸»åŠ¨å¯æ§çš„çŸ¥è¯†æ£€ç´¢ï¼Œæ˜¾è‘—ç¼“è§£äº†å¹»è§‰é—®é¢˜å¹¶æå‡äº†æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00882v3",
      "published_date": "2025-11-30 13:04:43 UTC",
      "updated_date": "2025-12-03 02:28:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:17.069138+00:00"
    },
    {
      "arxiv_id": "2512.20630v1",
      "title": "MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data",
      "title_zh": "MicroProbeï¼šåŸºäºæç®€æ•°æ®çš„åŸºç¡€æ¨¡å‹é«˜æ•ˆå¯é æ€§è¯„ä¼°",
      "authors": [
        "Aayam Bansal",
        "Ishaan Gangwani"
      ],
      "abstract": "Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.",
      "tldr_zh": "é’ˆå¯¹åŸºç¡€æ¨¡å‹(Foundation Models)å¯é æ€§è¯„ä¼°éœ€è¦æ•°åƒä¸ªæ ·æœ¬å¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬å’Œæ—¶é—´æ¶ˆè€—é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†MicroProbeï¼Œè¿™æ˜¯ä¸€ç§ä»…éœ€100ä¸ªç­–ç•¥æ€§é€‰æ‹©çš„æ¢é’ˆç¤ºä¾‹å³å¯å®ç°å…¨é¢è¯„ä¼°çš„æ–°é¢–æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è·¨äº”ä¸ªæ ¸å¿ƒå¯é æ€§ç»´åº¦çš„ç­–ç•¥æ€§æç¤ºå¤šæ ·æ€§(strategic prompt diversity)ï¼Œå¹¶é‡‡ç”¨å…ˆè¿›çš„ä¸ç¡®å®šæ€§é‡åŒ–(uncertainty quantification)å’Œè‡ªé€‚åº”åŠ æƒ(adaptive weighting)æ¥é«˜æ•ˆæ£€æµ‹æ½œåœ¨æ•…éšœæ¨¡å¼ã€‚ç ”ç©¶é€šè¿‡å¯¹å¤šä¸ªGPT-2å˜ä½“ä»¥åŠåŒ»ç–—ã€é‡‘èã€æ³•å¾‹ç­‰è·¨é¢†åŸŸåœºæ™¯çš„éªŒè¯ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMicroProbeåœ¨ç»¼åˆå¯é æ€§å¾—åˆ†ä¸Šæ¯”éšæœºé‡‡æ ·åŸºçº¿é«˜å‡º23.5%ï¼Œå¹¶å…·æœ‰æé«˜çš„ç»Ÿè®¡å­¦æ˜¾è‘—æ€§ã€‚AIå®‰å…¨ç ”ç©¶ä¸“å®¶çš„éªŒè¯è¿›ä¸€æ­¥ç¡®è®¤äº†å…¶æœ‰æ•ˆæ€§ï¼ŒMicroProbeåœ¨ä¸“å®¶è¯„åˆ†ä¸­æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ã€‚è¯¥æ–¹æ³•åœ¨å®Œæˆè¯„ä¼°æ—¶å¯å‡å°‘90%çš„è¯„ä¼°æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ä¼ ç»Ÿæ–¹æ³•95%çš„è¦†ç›–ç‡ï¼Œå¡«è¡¥äº†è´Ÿè´£ä»»çš„AIéƒ¨ç½²ä¸­é«˜æ•ˆæ¨¡å‹è¯„ä¼°çš„å…³é”®ç©ºç™½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML NewInML",
      "pdf_url": "https://arxiv.org/pdf/2512.20630v1",
      "published_date": "2025-11-30 13:01:57 UTC",
      "updated_date": "2025-11-30 13:01:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:15.471354+00:00"
    },
    {
      "arxiv_id": "2512.00881v1",
      "title": "Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing",
      "title_zh": "Hybrid-DMKGï¼šåŸºäºåŠ¨æ€å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±çš„çŸ¥è¯†ç¼–è¾‘å¤šæ¨¡æ€å¤šè·³é—®ç­”æ··åˆæ¨ç†æ¡†æ¶",
      "authors": [
        "Li Yuan",
        "Qingfei Huang",
        "Bingshan Zhu",
        "Yi Cai",
        "Qingbao Huang",
        "Changmeng Zheng",
        "Zikun Deng",
        "Tao Wang"
      ],
      "abstract": "Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€çŸ¥è¯†ç¼–è¾‘ï¼ˆMultimodal Knowledge Editing, MKEï¼‰é¢†åŸŸç°æœ‰åŸºå‡†æµ‹è¯•å¿½è§†ä¸­é—´æ¨ç†è´¨é‡å’Œè§†è§‰æ”¹å†™é²æ£’æ€§çš„é—®é¢˜ï¼Œå¼•å…¥äº†é¦–ä¸ªé’ˆå¯¹çŸ¥è¯†ç¼–è¾‘ä¸‹å¤šæ¨¡æ€å¤šè·³é—®ç­”çš„åŸºå‡†æµ‹è¯•MMQAKEï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è·¨è¶Šæ–‡æœ¬å’Œå›¾åƒçš„2-5è·³äº‹å®é“¾ä¸Šçš„è¡¨ç°ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•åœ¨çŸ¥è¯†ç¼–è¾‘åéš¾ä»¥è¿›è¡ŒæŒç»­æ¨ç†çš„æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†Hybrid-DMKGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºåŠ¨æ€å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆDMKGï¼‰å®ç°å¯¹æ›´æ–°çŸ¥è¯†çš„ç²¾ç¡®å¤šæ­¥æ¨ç†ã€‚Hybrid-DMKGé¦–å…ˆåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºé¡ºåºå­é—®é¢˜ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ£€ç´¢å®šä½æ›´æ–°åçš„äº‹å®ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ··åˆæ¨ç†æ¨¡å—é€šè¿‡å…³ç³»é“¾æ¥é¢„æµ‹å’ŒåŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸¤æ¡å¹¶è¡Œè·¯å¾„è¿ä½œï¼Œå¹¶ç”±å†³ç­–æ¨¡å—æ±‡æ€»è¯æ®ä»¥é€‰æ‹©æœ€å¯ä¿¡ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHybrid-DMKGåœ¨MMQAKEä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„MKEæ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§å’Œå¯¹çŸ¥è¯†æ›´æ–°çš„é²æ£’æ€§æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.00881v1",
      "published_date": "2025-11-30 12:58:15 UTC",
      "updated_date": "2025-11-30 12:58:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:27.058711+00:00"
    },
    {
      "arxiv_id": "2512.00878v1",
      "title": "Less is More: Resource-Efficient Low-Rank Adaptation",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šèµ„æºé«˜æ•ˆå‹ä½ç§©è‡ªé€‚åº”",
      "authors": [
        "Chunlin Tian",
        "Xuyang Wei",
        "Huanrong Liu",
        "Zhijiang Guo",
        "Li Li"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While recent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices update to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demonstrating improved efficiency and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Low-Rank Adaptation (LoRA) åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­å­˜åœ¨çš„æ˜¾è‘—å¼€é”€å’Œå‚æ•°å¹²æ‰°é—®é¢˜ï¼Œæå‡ºäº†èµ„æºé«˜æ•ˆçš„æ”¹è¿›æ–¹æ³• EffiLoRAã€‚EffiLoRA é’ˆå¯¹çŸ©é˜µé—´å’Œå±‚å†…å‚æ•°å†—ä½™ï¼Œåœ¨æ‰€æœ‰ transformer å±‚ä¸­é‡‡ç”¨ç»Ÿä¸€çš„ A çŸ©é˜µä»¥å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†è¿è¡Œæ—¶é€‰æ‹©æ€§ B çŸ©é˜µæ›´æ–°æœºåˆ¶ï¼Œå…è®¸æ ¹æ®ç³»ç»Ÿèµ„æºé¢„ç®—åŠ¨æ€å¹³è¡¡æ¨¡å‹æ€§èƒ½ã€‚EffiLoRA å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå¯ç›´æ¥åº”ç”¨äºè¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€æ¨¡å‹ä»¥åŠæ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEffiLoRA åœ¨å¸¸è¯†æ¨ç† (commonsense reasoning)ã€è§†è§‰æŒ‡ä»¤å¾®è°ƒ (visual instruction tuning) å’Œå›¾åƒç”Ÿæˆç­‰å¤šç§ä»»åŠ¡ä¸Šå‡ä¸€è‡´ä¼˜äº LoRAã€‚è¯¥ç ”ç©¶è¯æ˜äº† EffiLoRA åœ¨æå‡å¾®è°ƒæ•ˆç‡çš„åŒæ—¶ï¼Œå±•ç°å‡ºäº†æ›´å¼ºçš„ç¨³å¥æ€§ï¼Œä¸ºèµ„æºå—é™ä¸‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00878v1",
      "published_date": "2025-11-30 12:52:04 UTC",
      "updated_date": "2025-11-30 12:52:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:28.074836+00:00"
    },
    {
      "arxiv_id": "2512.00872v1",
      "title": "TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models",
      "title_zh": "TAP-CTï¼šè®¡ç®—æœºæ–­å±‚æ‰«æåŸºç¡€æ¨¡å‹çš„ä¸‰ç»´ä»»åŠ¡æ— å…³é¢„è®­ç»ƒ",
      "authors": [
        "Tim Veenboer",
        "George Yiasemis",
        "Eric Marcus",
        "Vivien Van Veldhuizen",
        "Cees G. M. Snoek",
        "Jonas Teuwen",
        "Kevin B. W. Groot Lipman"
      ],
      "abstract": "Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TAP-CTï¼Œä¸€å¥—é’ˆå¯¹3D CTå½±åƒçš„ä»»åŠ¡æ— å…³é¢„è®­ç»ƒ(Task-Agnostic Pretraining)åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŒ»å­¦æ¨¡å‹å¯¹å¾®è°ƒä¾èµ–åº¦é«˜ä¸”å­˜åœ¨ä»»åŠ¡åè§çš„é—®é¢˜ã€‚ä½œè€…é€šè¿‡å¯¹Vision Transformers (ViTs)å’ŒDINOv2è¿›è¡Œæ”¹è¿›ï¼Œä½¿å…¶é€‚é…äº3Dä½“æ•°æ®(Volumetric data)ï¼Œå®ç°äº†åœ¨10.5ä¸‡ä¸ª3D CTä½“ç§¯ä¸Šçš„å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒã€‚è¯¥æ¶æ„å¼•å…¥äº†é’ˆå¯¹patch embeddingsã€positional encodingså’Œæ•°æ®å¢å¼ºçš„ç‰¹å®šä¿®æ”¹ï¼Œä½¿å…¶å…·å¤‡æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†åº•å±‚æ¶æ„çš„ç®€æ´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒTAP-CTèƒ½å¤Ÿäº§ç”Ÿç¨³å®šä¸”é²æ£’çš„å†»ç»“è¡¨å¾(Frozen representations)ï¼Œåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„é€æ˜åº¦ä¸ä½èµ„æºç ”ç©¶ï¼Œç›¸å…³é¢„è®­ç»ƒæ¨¡å‹åŠåŸºå‡†ä»£ç å·²å‘ç¤¾åŒºå¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 4 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.00872v1",
      "published_date": "2025-11-30 12:43:15 UTC",
      "updated_date": "2025-11-30 12:43:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:28.764675+00:00"
    },
    {
      "arxiv_id": "2512.00862v3",
      "title": "HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs",
      "title_zh": "HBLLMï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„å°æ³¢å¢å¼ºé«˜ä¿çœŸ 1-bit é‡åŒ–",
      "authors": [
        "Ningning Chen",
        "Weicai Ye",
        "Ying Jiang"
      ],
      "abstract": "We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HBLLMï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å°æ³¢å¢å¼ºé«˜ä¿çœŸ1-Bitè®­ç»ƒåé‡åŒ–(Post-training Quantization)æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å“ˆå°”å°æ³¢å˜æ¢(Haar wavelet transforms)è¿›è¡Œé¢‘ç‡åˆ†è§£æ¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œåœ¨ä¿æŒæä½å¼€é”€çš„åŒæ—¶æ˜¾è‘—æå‡äº†é‡åŒ–ä¿çœŸåº¦ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸¤ç§ç»“æ„æ„ŸçŸ¥åˆ†ç»„ç­–ç•¥ï¼Œå³é¢‘ç‡æ„ŸçŸ¥çš„å¤šå‚æ•°è¡Œå†…åˆ†ç»„(frequency-aware multi-parameter intra-row grouping)å’ŒåŸºäº$\\ell_2$èŒƒæ•°çš„æ˜¾è‘—æ€§é©±åŠ¨åˆ—é€‰æ‹©($\\ell_2$-norm-based saliency-driven column selection)ã€‚é’ˆå¯¹éæ˜¾è‘—æƒé‡ï¼ŒHBLLMåœ¨å„é¢‘å¸¦çš„é‡åŒ–ç»„é—´é‡‡ç”¨å…±äº«å‡å€¼æœºåˆ¶ä»¥ä¼˜åŒ–å­˜å‚¨æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨OPTå’ŒLLaMAæ¨¡å‹ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ã€‚åœ¨LLaMA2-13Bæ¨¡å‹ä¸Šï¼ŒHBLLMä»…éœ€å¹³å‡1.08 bitsçš„æƒé‡å­˜å‚¨ä¾¿å®ç°äº†6.71çš„å›°æƒ‘åº¦(Perplexity)ï¼Œä¸ºè¶…ä½ä½å®½é‡åŒ–æä¾›äº†é«˜æ•ˆä¸”é«˜ä¿çœŸåº¦çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00862v3",
      "published_date": "2025-11-30 12:18:02 UTC",
      "updated_date": "2025-12-12 02:36:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:28.273384+00:00"
    },
    {
      "arxiv_id": "2512.00852v1",
      "title": "One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces",
      "title_zh": "ä¸€ç‡•ä¸æˆå¤ï¼šç†è§£åµŒå…¥ç©ºé—´ä¸­çš„è¯­ä¹‰ç»“æ„",
      "authors": [
        "Yandong Sun",
        "Qiang Huang",
        "Ziwei Xu",
        "Yiqun Sun",
        "Yixuan Tang",
        "Anthony K. H. Tung"
      ],
      "abstract": "Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åµŒå…¥ç©ºé—´(Embedding spaces)å†…éƒ¨ç»“æ„ä¸é€æ˜ä¸”è¯­ä¹‰ä¸€è‡´æ€§éš¾ä»¥ä¿éšœçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†è¯­ä¹‰åœºå­ç©ºé—´(Semantic Field Subspace, SFS)è¿™ä¸€å‡ ä½•ä¿ç•™ä¸”æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„è¡¨ç¤ºæ–¹æ³•ï¼Œç”¨ä»¥æ•æ‰å±€éƒ¨è¯­ä¹‰é‚»åŸŸã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†åä¸ºSAFARIçš„æ— ç›‘ç£ã€æ¨¡æ€æ— å…³ç®—æ³•ï¼Œåˆ©ç”¨æ–°é¢–çš„è¯­ä¹‰åç§»(Semantic Shift)æŒ‡æ ‡æ¥åˆ»ç”»å¹¶æ­ç¤ºå±‚çº§è¯­ä¹‰ç»“æ„ã€‚ä¸ºæå‡è®¡ç®—æ•ˆç‡ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è¿‘ä¼¼è®¡ç®—æ–¹æ³•æ›¿ä»£æ˜‚è´µçš„SVDè®¡ç®—ï¼Œåœ¨ä¿æŒæé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†15è‡³30å€çš„æ¨ç†åŠ é€Ÿã€‚åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜ï¼ŒSFSåœ¨åŸºç¡€åˆ†ç±»åŠæ”¿æ²»åå‘æ£€æµ‹ç­‰ä»»åŠ¡ä¸­å‡ä¼˜äºä¼ ç»Ÿåˆ†ç±»å™¨ã€‚è¯¥ç ”ç©¶ä¸ºåµŒå…¥ç©ºé—´(Embedding spaces)ä¸­è¯­ä¹‰ç»“æ„çš„ç»“æ„åŒ–ã€åˆ†æå’Œè§„æ¨¡åŒ–ç†è§£æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä¸”SAFARIå±•ç°äº†è‰¯å¥½çš„å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00852v1",
      "published_date": "2025-11-30 11:48:00 UTC",
      "updated_date": "2025-11-30 11:48:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:39.779683+00:00"
    },
    {
      "arxiv_id": "2512.00849v1",
      "title": "Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy",
      "title_zh": "åŸºäºå¼•åŠ›åŠ¿åœºçš„æœ¬åœ°å·®åˆ†éšç§æ‹“æ‰‘è”é‚¦èšç±»",
      "authors": [
        "Yunbo Long",
        "Jiaquan Zhang",
        "Xi Chen",
        "Alexandra Brintrup"
      ],
      "abstract": "Clustering non-independent and identically distributed (non-IID) data under local differential privacy (LDP) in federated settings presents a critical challenge: preserving privacy while maintaining accuracy without iterative communication. Existing one-shot methods rely on unstable pairwise centroid distances or neighborhood rankings, degrading severely under strong LDP noise and data heterogeneity. We present Gravitational Federated Clustering (GFC), a novel approach to privacy-preserving federated clustering that overcomes the limitations of distance-based methods under varying LDP. Addressing the critical challenge of clustering non-IID data with diverse privacy guarantees, GFC transforms privatized client centroids into a global gravitational potential field where true cluster centers emerge as topologically persistent singularities. Our framework introduces two key innovations: (1) a client-side compactness-aware perturbation mechanism that encodes local cluster geometry as \"mass\" values, and (2) a server-side topological aggregation phase that extracts stable centroids through persistent homology analysis of the potential field's superlevel sets. Theoretically, we establish a closed-form bound between the privacy budget $Îµ$ and centroid estimation error, proving the potential field's Lipschitz smoothing properties exponentially suppress noise in high-density regions. Empirically, GFC outperforms state-of-the-art methods on ten benchmarks, especially under strong LDP constraints ($Îµ< 1$), while maintaining comparable performance at lower privacy budgets. By reformulating federated clustering as a topological persistence problem in a synthetic physics-inspired space, GFC achieves unprecedented privacy-accuracy trade-offs without iterative communication, providing a new perspective for privacy-preserving distributed learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¼•åŠ›è”é‚¦èšç±»(Gravitational Federated Clustering, GFC)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æ•°æ®åœ¨æœ¬åœ°å·®åˆ†éšç§(Local Differential Privacy, LDP)çº¦æŸä¸‹çš„èšç±»æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰ä¸€æ¬¡æ€§(one-shot)æ–¹æ³•åœ¨å¼ºéšç§å™ªå£°ä¸‹è¡¨ç°ä¸ç¨³å®šçš„é—®é¢˜ï¼ŒGFCå°†å¤„ç†åçš„å®¢æˆ·ç«¯è´¨å¿ƒè½¬æ¢ä¸ºå…¨å±€å¼•åŠ›åŠ¿åœºï¼Œä½¿çœŸå®çš„èšç±»ä¸­å¿ƒä»¥æ‹“æ‰‘æŒä¹…æ€§å¥‡å¼‚ç‚¹(topologically persistent singularities)çš„å½¢å¼æ˜¾ç°ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯å®¢æˆ·ç«¯çš„ç´§å‡‘æ€§æ„ŸçŸ¥æ‰°åŠ¨æœºåˆ¶ï¼Œå°†å±€éƒ¨èšç±»å‡ ä½•ç»“æ„ç¼–ç ä¸ºâ€œè´¨é‡(mass)â€å€¼ï¼›äºŒæ˜¯æœåŠ¡å™¨ç«¯çš„æ‹“æ‰‘èšåˆé˜¶æ®µï¼Œé€šè¿‡å¯¹åŠ¿åœºè¿›è¡ŒæŒä¹…åŒè°ƒ(persistent homology)åˆ†ææ¥æå–ç¨³å®šè´¨å¿ƒã€‚ç†è®ºä¸Šï¼Œç ”ç©¶å»ºç«‹äº†éšç§é¢„ç®—Îµä¸è´¨å¿ƒä¼°è®¡è¯¯å·®ä¹‹é—´çš„é—­å¼ç•Œé™ï¼Œè¯æ˜äº†åŠ¿åœºçš„Lipschitzå¹³æ»‘ç‰¹æ€§èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶é«˜å¯†åº¦åŒºåŸŸçš„å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGFCåœ¨åä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼ºLDPçº¦æŸ(Îµ < 1)ä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºæ— éœ€è¿­ä»£é€šä¿¡çš„éšç§ä¿æŠ¤åˆ†å¸ƒå¼å­¦ä¹ æä¾›äº†å…¨æ–°çš„ç‰©ç†å¯å‘å¼æ‹“æ‰‘è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00849v1",
      "published_date": "2025-11-30 11:41:16 UTC",
      "updated_date": "2025-11-30 11:41:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:45.070164+00:00"
    },
    {
      "arxiv_id": "2512.00839v1",
      "title": "ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI",
      "title_zh": "ARCADIAï¼šåŸºäºæ™ºèƒ½ä½“ AI çš„ä¼ä¸šç ´äº§åˆ†æå¯æ‰©å±•å› æœå‘ç°",
      "authors": [
        "Fabrizio Maturo",
        "Donato Riccio",
        "Andrea Mazzitelli",
        "Giuseppe Bifulco",
        "Francesco Paolone",
        "Iulia Brezeanu"
      ],
      "abstract": "This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ARCADIAï¼Œä¸€ç§åŸºäºæ™ºèƒ½ä½“ AI (Agentic AI) çš„å› æœå‘ç° (Causal Discovery) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆå¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†ä¸ç»Ÿè®¡è¯Šæ–­æ¥æ„å»ºæœ‰æ•ˆä¸”æ—¶é—´è¿è´¯çš„å› æœç»“æ„ã€‚ä¸ä¼ ç»Ÿç®—æ³•ä¸åŒï¼ŒARCADIA åˆ©ç”¨çº¦æŸå¼•å¯¼æç¤º (Constraint-guided prompting) å’Œå› æœæœ‰æ•ˆæ€§åé¦ˆ (Causal-validity feedback) è¿­ä»£ä¼˜åŒ–å€™é€‰æœ‰å‘æ— ç¯å›¾ (DAGs)ï¼Œç¡®ä¿äº†åœ¨é«˜é£é™©é¢†åŸŸåº”ç”¨ä¸­çš„æ¨¡å‹ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§ã€‚åœ¨ä¼ä¸šç ´äº§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒARCADIA ç”Ÿæˆçš„å› æœå›¾å¯é æ€§ä¼˜äº NOTEARSã€GOLEM å’Œ DirectLiNGAMï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå®Œå…¨å¯è§£é‡Šä¸”æ”¯æŒå¹²é¢„åˆ†æ (Intervention-ready) çš„æµæ°´çº¿ã€‚è¯¥æ¡†æ¶é€šè¿‡å±•ç¤º Agentic LLMs å¦‚ä½•å‚ä¸è‡ªä¸»ç§‘å­¦å»ºæ¨¡å’Œç»“æ„åŒ–å› æœæ¨ç†ï¼Œä¸ºå¤æ‚å†³ç­–ç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–å› æœåˆ†ææä¾›äº†åˆ›æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "stat.CO",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "35 pages, 9 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.00839v1",
      "published_date": "2025-11-30 11:21:29 UTC",
      "updated_date": "2025-11-30 11:21:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:14:43.875748+00:00"
    },
    {
      "arxiv_id": "2512.00836v1",
      "title": "Assessing model error in counterfactual worlds",
      "title_zh": "è¯„ä¼°åäº‹å®ä¸–ç•Œä¸­çš„æ¨¡å‹è¯¯å·®",
      "authors": [
        "Emily Howerton",
        "Justin Lessler"
      ],
      "abstract": "Counterfactual scenario modeling exercises that ask \"what would happen if?\" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åäº‹å®ä¸–ç•Œ(counterfactual worlds)ä¸­è¯„ä¼°æ¨¡å‹è¯¯å·®çš„é—®é¢˜ï¼ŒæŒ‡å‡ºåäº‹å®åœºæ™¯å»ºæ¨¡åœ¨è§„åˆ’ä¸­è™½åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶é¢„æµ‹ç»“æœå´é²œå°‘å¾—åˆ°äº‹åè¯„ä¼°ã€‚è®ºæ–‡æ˜ç¡®åŒºåˆ†äº†é¢„æµ‹ä¸å®é™…è§‚æµ‹å€¼ä¹‹é—´çš„ä¸¤ä¸ªè¯¯å·®æ¥æºï¼Œå³åœºæ™¯åå·®(scenario deviation)å’Œæ¨¡å‹å¤±å‡†(model miscalibration)ï¼Œå¹¶å¼ºè°ƒåè€…æ˜¯è¡¡é‡æ¨¡å‹å†³ç­–ä»·å€¼çš„å…³é”®ã€‚ç ”ç©¶æå‡ºå¹¶å¯¹æ¯”äº†ä¸‰ç§ä¼°è®¡åäº‹å®è¯¯å·®çš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿå®éªŒ(simulation experiment)å±•ç¤ºäº†å„æ–¹æ³•çš„ä¼˜åŠ¿ä¸å±€é™æ€§ã€‚ä½œè€…æœ€ç»ˆä¸ºåäº‹å®è¯¯å·®çš„ä¼°è®¡æä¾›äº†å®è·µå»ºè®®ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†å®ç°åœºæ™¯é¢„æµ‹å¯è¯„ä»·æ€§æ‰€éœ€çš„åœºæ™¯è®¾è®¡è¦ç´ ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00836v1",
      "published_date": "2025-11-30 11:08:43 UTC",
      "updated_date": "2025-11-30 11:08:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:15:51.260735+00:00"
    },
    {
      "arxiv_id": "2512.00834v1",
      "title": "SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks",
      "title_zh": "SemAgentï¼šè¯­ä¹‰é©±åŠ¨çš„æ™ºèƒ½ä½“AIèµ‹èƒ½è½¦è”ç½‘è½¨è¿¹é¢„æµ‹",
      "authors": [
        "Lin Zhu",
        "Kezhi Wang",
        "Luping Xiang",
        "Kun Yang"
      ],
      "abstract": "Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.",
      "tldr_zh": "é’ˆå¯¹è½¦è”ç½‘ (V2X) ä¸­ä¼ ç»Ÿé€šä¿¡æ–¹æ¡ˆå¼€é”€å¤§ã€å»¶è¿Ÿé«˜ï¼Œä»¥åŠç°æœ‰è½¨è¿¹é¢„æµ‹æ¨¡å‹ç¼ºä¹ç¯å¢ƒæ„ŸçŸ¥ä¸é€»è¾‘æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† SemAgent è½¨è¿¹é¢„æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è¯­ä¹‰é€šä¿¡ (Semantic Communication) ä¸ä»£ç†å¼äººå·¥æ™ºèƒ½ (Agentic AI) æ·±åº¦é›†æˆï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œå¢å¼ºé¢„æµ‹æ€§èƒ½ã€‚åœ¨è½¦è”ç½‘åŸºç¡€è®¾æ–½ (V2I) é€šä¿¡ä¸­ï¼Œè·¯ä¾§å•å…ƒ (RSU) é€šè¿‡ç‰¹å¾æå–æ™ºèƒ½ä½“ä¸è¯­ä¹‰åˆ†ææ™ºèƒ½ä½“ç”Ÿæˆç´§å‡‘çš„ç‰¹å¾è¡¨ç¤ºä¸è¯­ä¹‰è§è§£ï¼Œè¾…åŠ©ç›®æ ‡è½¦è¾†è¿›è¡Œç²¾ç¡®é¢„æµ‹ã€‚åœ¨è½¦è½¦ (V2V) é€šä¿¡åœºæ™¯ä¸‹ï¼Œè½¦è¾†é€šè¿‡æ‰§è¡Œæœ¬åœ°è¯­ä¹‰åˆ†æå¹¶æ•´åˆé‚»è¿‘è½¦è¾†çš„é¢„æµ‹è½¨è¿¹ä¿¡æ¯ï¼Œå…±åŒä¼˜åŒ–è‡ªèº«çš„é¢„æµ‹ç»“æœã€‚å®éªŒè¯æ˜ï¼ŒSemAgent åœ¨å¤šç§é€šä¿¡æ¡ä»¶ä¸‹å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ¡ˆï¼Œå°¤å…¶åœ¨ä½ä¿¡å™ªæ¯” (SNR) æ¡ä»¶ä¸‹ï¼Œå…¶é¢„æµ‹å‡†ç¡®ç‡æå‡é«˜è¾¾ 47.5%ï¼Œå±•ç°äº†æå¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted for possible journal publication",
      "pdf_url": "https://arxiv.org/pdf/2512.00834v1",
      "published_date": "2025-11-30 11:06:58 UTC",
      "updated_date": "2025-11-30 11:06:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:04.566014+00:00"
    },
    {
      "arxiv_id": "2512.00829v1",
      "title": "Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy",
      "title_zh": "åˆ©ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦åŠ é€Ÿ Bangla NLP ä»»åŠ¡ï¼šå…¼é¡¾æ¨¡å‹æ•ˆèƒ½çš„èµ„æºé«˜æ•ˆè®­ç»ƒ",
      "authors": [
        "Md Mehrab Hossain Opi",
        "Sumaiya Khan",
        "Moshammad Farzana Rahman"
      ],
      "abstract": "Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨å­ŸåŠ æ‹‰è¯­è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­åº”ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAutomatic Mixed Precision, AMPï¼‰è®­ç»ƒæŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³é«˜æ˜‚è®¡ç®—èµ„æºå’Œæ—¶é—´æ¶ˆè€—å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€ç»“åˆ16ä½å’Œ32ä½æµ®ç‚¹è¿ç®—ï¼Œæœ‰æ•ˆé™ä½äº†GPUæ˜¾å­˜éœ€æ±‚å¹¶åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶ç¡®ä¿æ¨¡å‹æ€§èƒ½ä¸å—æŸå®³ã€‚ç ”ç©¶äººå‘˜åœ¨æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰ã€é”™è¯¯åˆ†ç±»å’Œé—®ç­”ç³»ç»Ÿå››é¡¹æ ‡å‡†ä»»åŠ¡ä¸Šï¼Œåˆ©ç”¨BanglaBERTã€BanglishBERTã€XLM-Rå’ŒmBERTå››ç§åŸºäºTransformerçš„æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMPæŠ€æœ¯èƒ½ä½¿è®­ç»ƒé€Ÿåº¦æå‡44.5%ï¼Œæ˜¾å­˜æ¶ˆè€—é™ä½17.6%ï¼Œè€ŒF1åˆ†æ•°ä»ä¿æŒåœ¨å…¨ç²¾åº¦åŸºå‡†çš„99.7%ä»¥å†…ã€‚è¿™ä¸€å®è¯ç ”ç©¶å¼ºè°ƒäº†AMPåœ¨ç¡¬ä»¶å—é™ç¯å¢ƒä¸‹æ™®åŠå°–ç«¯NLPèƒ½åŠ›çš„æ½œåŠ›ï¼Œé€šè¿‡é™ä½è®¡ç®—é—¨æ§›ä¸ºèµ„æºå—é™åœ°åŒºçš„è¯­è¨€æŠ€æœ¯å‘å±•æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00829v1",
      "published_date": "2025-11-30 10:34:08 UTC",
      "updated_date": "2025-11-30 10:34:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:11.679142+00:00"
    },
    {
      "arxiv_id": "2512.00818v1",
      "title": "Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning",
      "title_zh": "Med-CMRï¼šèåˆè§†è§‰è¯æ®ä¸ä¸´åºŠé€»è¾‘çš„åŒ»ç–—å¤æ‚å¤šæ¨¡æ€æ¨ç†ç»†ç²’åº¦è¯„ä¼°åŸºå‡†",
      "authors": [
        "Haozhen Gong",
        "Xiaozhong Ji",
        "Yuansen Liu",
        "Wenbin Wu",
        "Xiaoxiao Yan",
        "Jingjing Liu",
        "Kai Wu",
        "Jiazhen Pan",
        "Bailiang Jian",
        "Jiangning Zhang",
        "Xiaobin Hu",
        "Hongwei Bran Li"
      ],
      "abstract": "MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Med-CMRï¼Œä¸€ä¸ªé’ˆå¯¹åŒ»å­¦å¤æ‚å¤šæ¨¡æ€æ¨ç†çš„ç»†ç²’åº¦Benchmarkï¼Œæ—¨åœ¨è¯„ä¼°MLLMsåœ¨ä¸´åºŠå·¥ä½œæµä¸­çš„æ¨ç†èƒ½åŠ›ã€‚Med-CMRé€šè¿‡ç³»ç»Ÿæ€§çš„èƒ½åŠ›åˆ†è§£ï¼Œå°†åŒ»å­¦å¤šæ¨¡æ€æ¨ç†æ‹†åˆ†ä¸ºç»†ç²’åº¦è§†è§‰ç†è§£ï¼ˆåŒ…å«small-object detectionå’Œspatial understandingç­‰ç»´åº¦ï¼‰ä¸å¤šæ­¥æ¨ç†ï¼ˆåŒ…å«temporal predictionå’Œcausal reasoningç­‰åœºæ™¯ï¼‰ã€‚è¯¥åŸºå‡†åŒ…å«20,653ä¸ªç»è¿‡ä¸“å®¶å®¡æ ¸çš„Visual Question Answering (VQA)å¯¹ï¼Œè¦†ç›–11ä¸ªå™¨å®˜ç³»ç»Ÿå’Œ12ç§å½±åƒæ¨¡æ€ï¼Œç¡®ä¿äº†ä¸´åºŠçœŸå®æ€§ã€‚å¯¹18ä¸ªä¸»æµæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒGPT-5åœ¨MCQå‡†ç¡®ç‡ä¸Šè¾¾åˆ°57.81%ï¼Œè¡¨ç°ä¼˜äºGemini 2.5 Proå’Œå¼€æºæ¨¡å‹Qwen3-VLã€‚ç ”ç©¶å‘ç°ä¸“é—¨çš„åŒ»å­¦MLLMsè¡¨ç°å¹¶ä¸ä¸€å®šä¼˜äºå¼ºåŠ›é€šç”¨æ¨¡å‹ï¼Œä¸”long-tail generalizationæ˜¯å½“å‰æ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚Med-CMRä¸ºåŒ»ç–—AIç³»ç»Ÿçš„è§†è§‰æ¨ç†é›†æˆå’Œç¨€æœ‰ç—…ä¾‹é²æ£’æ€§æä¾›äº†ä¸¥è‹›çš„å‹åŠ›æµ‹è¯•ä¸è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00818v1",
      "published_date": "2025-11-30 09:56:50 UTC",
      "updated_date": "2025-11-30 09:56:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:17.970968+00:00"
    },
    {
      "arxiv_id": "2512.00812v1",
      "title": "Causal Invariance and Counterfactual Learning Driven Cooperative Game for Multi-Label Classification",
      "title_zh": "å› æœä¸å˜æ€§ä¸åäº‹å®å­¦ä¹ é©±åŠ¨çš„åˆä½œåšå¼ˆå¤šæ ‡ç­¾åˆ†ç±»",
      "authors": [
        "Yijia Fan",
        "Jusheng Zhang",
        "Kaitong Cai",
        "Jing Yang",
        "Keze Wang"
      ],
      "abstract": "Multi-label classification (MLC) remains vulnerable to label imbalance, spurious correlations, and distribution shifts, challenges that are particularly detrimental to rare label prediction. To address these limitations, we introduce the Causal Cooperative Game (CCG) framework, which conceptualizes MLC as a cooperative multi-player interaction. CCG unifies explicit causal discovery via Neural Structural Equation Models with a counterfactual curiosity reward to drive robust feature learning. Furthermore, it incorporates a causal invariance loss to ensure generalization across diverse environments, complemented by a specialized enhancement strategy for rare labels. Extensive benchmarking demonstrates that CCG substantially outperforms strong baselines in both rare label prediction and overall robustness. Through rigorous ablation studies and qualitative analysis, we validate the efficacy and interpretability of our components, underscoring the potential of synergizing causal inference with cooperative game theory for advancing multi-label learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ ‡ç­¾åˆ†ç±»(Multi-label classification, MLC)ä¸­å¸¸è§çš„æ ‡ç­¾ä¸å¹³è¡¡ã€ä¼ªç›¸å…³å’Œåˆ†å¸ƒåç§»ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†å› æœåˆä½œåšå¼ˆ(Causal Cooperative Game, CCG)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†MLCè§†ä¸ºä¸€ç§å¤šç©å®¶åˆä½œäº¤äº’è¿‡ç¨‹ï¼Œåˆ©ç”¨ç¥ç»ç»“æ„æ–¹ç¨‹æ¨¡å‹(Neural Structural Equation Models)å®ç°æ˜¾å¼å› æœå‘ç°ï¼Œå¹¶ç»“åˆåäº‹å®å¥½å¥‡å¿ƒå¥–åŠ±(counterfactual curiosity reward)æ¥é©±åŠ¨é²æ£’çš„ç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒCCGå¼•å…¥äº†å› æœä¸å˜æ€§æŸå¤±(causal invariance loss)ä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é…å¤‡äº†é’ˆå¯¹ç¨€æœ‰æ ‡ç­¾çš„ä¸“é—¨å¢å¼ºç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCGåœ¨ç¨€æœ‰æ ‡ç­¾é¢„æµ‹å’Œæ•´ä½“é²æ£’æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰å¼ºåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡æ¶ˆèç ”ç©¶å’Œå®šæ€§åˆ†æï¼Œè¯¥ç ”ç©¶è¯å®äº†ç»“åˆå› æœæ¨ç†ä¸åˆä½œåšå¼ˆè®ºåœ¨æå‡å¤šæ ‡ç­¾å­¦ä¹ æ€§èƒ½åŠå¯è§£é‡Šæ€§æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00812v1",
      "published_date": "2025-11-30 09:39:43 UTC",
      "updated_date": "2025-11-30 09:39:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:19.371762+00:00"
    },
    {
      "arxiv_id": "2512.00807v1",
      "title": "BioPro: On Difference-Aware Gender Fairness for Vision-Language Models",
      "title_zh": "BioProï¼šè§†è§‰è¯­è¨€æ¨¡å‹çš„å·®å¼‚æ„ŸçŸ¥æ€§åˆ«å…¬å¹³æ€§ç ”ç©¶",
      "authors": [
        "Yujie Lin",
        "Jiayao Ma",
        "Qingguo Hu",
        "Derek F. Wong",
        "Jinsong Su"
      ],
      "abstract": "Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰ä¸­å­˜åœ¨çš„æ€§åˆ«åè§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å·®å¼‚æ„ŸçŸ¥ï¼ˆdifference-awareï¼‰çš„æ€§åˆ«å…¬å¹³æ€§æ–¹æ¡ˆï¼Œæ—¨åœ¨åŒºåˆ†éœ€è¦ä¿æŒä¸­ç«‹çš„è¯­å¢ƒä¸éœ€è¦ä¿ç•™ç‰¹å®šå±æ€§çš„è¯­å¢ƒã€‚ä½œè€…å€¡å¯¼é€‰æ‹©æ€§å»åï¼ˆselective debiasingï¼‰ç­–ç•¥ï¼Œå¹¶å¼€å‘äº†åä¸º BioProï¼ˆBias Orthogonal Projectionï¼‰çš„å®Œå…¨æ— éœ€è®­ç»ƒçš„æ¡†æ¶ã€‚BioPro åˆ©ç”¨åäº‹å®åµŒå…¥ï¼ˆcounterfactual embeddingsï¼‰è¯†åˆ«ä½ç»´æ€§åˆ«å˜åŒ–å­ç©ºé—´ï¼Œå¹¶é€šè¿‡æŠ•å½±æŠ€æœ¯é€‰æ‹©æ€§åœ°ä¸­å’Œæ€§åˆ«ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼ŒBioPro åœ¨å›¾åƒæè¿°å’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒä»»åŠ¡ä¸­ï¼Œä¸ä»…èƒ½æœ‰æ•ˆå‡å°‘ä¸­ç«‹è¯­å¢ƒä¸‹çš„æ€§åˆ«åè§ï¼Œè¿˜èƒ½ç¡®ä¿æ˜ç¡®è¯­å¢ƒä¸‹çš„æ€§åˆ«å¿ å®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½æˆåŠŸæ¨å¹¿è‡³åœºæ™¯äº®åº¦ç­‰è¿ç»­åè§å˜é‡ï¼Œå±•ç°äº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹é¢†åŸŸå®ç°é€‰æ‹©æ€§å…¬å¹³æ€§çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00807v1",
      "published_date": "2025-11-30 09:33:09 UTC",
      "updated_date": "2025-11-30 09:33:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:16.370772+00:00"
    },
    {
      "arxiv_id": "2512.00804v1",
      "title": "Bias Injection Attacks on RAG Databases and Sanitization Defenses",
      "title_zh": "é’ˆå¯¹ RAG æ•°æ®åº“çš„åè§æ³¨å…¥æ”»å‡»ä¸æ¸…æ´—é˜²å¾¡",
      "authors": [
        "Hao Wu",
        "Prateek Saxena"
      ],
      "abstract": "This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.\n  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\\% which mitigates perspective shift by 6.2\\times in answers, while enabling the retrieval of 62\\% more benign passages.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿä¸­é’ˆå¯¹å‘é‡æ•°æ®åº“(Vector Databases)çš„æ”»å‡»ä¸é˜²å¾¡ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„çŸ¥è¯†æŠ•æ¯’æ”»å‡»(Knowledge Poisoning Attacks)å› æ³¨å…¥è™šå‡å†…å®¹è€Œæ˜“è¢«æ£€æµ‹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ­ç¤ºäº†ä¸€ç§éšè”½çš„åè§æ³¨å…¥æ”»å‡»(Bias Injection Attacks)ï¼Œé€šè¿‡å‘çŸ¥è¯†åº“æ’å…¥äº‹å®æ­£ç¡®ä½†è¯­ä¹‰åç½®çš„æ®µè½ï¼Œç§˜å¯†å½±å“å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆç­”æ¡ˆçš„æ€æƒ³æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›å¯¹æŠ—æ€§æ®µè½èƒ½ç³»ç»Ÿæ€§åœ°æ’æŒ¤æ£€ç´¢ä¸Šä¸‹æ–‡ä¸­çš„ç›¸åè§‚ç‚¹ï¼Œè¯±å¯¼æ¨¡å‹è½¬å‘æ”»å‡»è€…çš„é¢„è®¾è§†è§’ï¼Œå¹¶æœ‰æ•ˆè§„é¿ç°æœ‰é˜²å¾¡ã€‚é’ˆå¯¹æ­¤ç±»å¨èƒï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åä¸ºBiasDefçš„æ£€ç´¢åè¿‡æ»¤é˜²å¾¡æ–¹æ¡ˆï¼Œå¹¶æ„å»ºäº†å…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ç»“æœæ˜¾ç¤ºï¼ŒBiasDefå°†æ£€ç´¢åˆ°çš„å¯¹æŠ—æ€§æ®µè½å‡å°‘äº†15%ï¼Œä½¿ç­”æ¡ˆä¸­çš„è§†è§’åç§»é™ä½äº†6.2å€ï¼ŒåŒæ—¶ä½¿è‰¯æ€§æ®µè½çš„æ£€ç´¢é‡æå‡äº†62%ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00804v1",
      "published_date": "2025-11-30 09:27:18 UTC",
      "updated_date": "2025-11-30 09:27:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:28.772094+00:00"
    },
    {
      "arxiv_id": "2512.02073v1",
      "title": "HTG-GCL: Leveraging Hierarchical Topological Granularity from Cellular Complexes for Graph Contrastive Learning",
      "title_zh": "HTG-GCLï¼šåˆ©ç”¨èƒè…”å¤åˆç‰©çš„åˆ†å±‚æ‹“æ‰‘ç²’åº¦è¿›è¡Œå›¾å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Qirui Ji",
        "Bin Qin",
        "Yifan Jin",
        "Yunze Zhao",
        "Chuxiong Sun",
        "Changwen Zheng",
        "Jianwen Cao",
        "Jiangmeng Li"
      ],
      "abstract": "Graph contrastive learning (GCL) aims to learn discriminative semantic invariance by contrasting different views of the same graph that share critical topological patterns. However, existing GCL approaches with structural augmentations often struggle to identify task-relevant topological structures, let alone adapt to the varying coarse-to-fine topological granularities required across different downstream tasks. To remedy this issue, we introduce Hierarchical Topological Granularity Graph Contrastive Learning (HTG-GCL), a novel framework that leverages transformations of the same graph to generate multi-scale ring-based cellular complexes, embodying the concept of topological granularity, thereby generating diverse topological views. Recognizing that a certain granularity may contain misleading semantics, we propose a multi-granularity decoupled contrast and apply a granularity-specific weighting mechanism based on uncertainty estimation. Comprehensive experiments on various benchmarks demonstrate the effectiveness of HTG-GCL, highlighting its superior performance in capturing meaningful graph representations through hierarchical topological information.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HTG-GCL æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Graph contrastive learning (GCL) åœ¨è¯†åˆ«ä»»åŠ¡ç›¸å…³æ‹“æ‰‘ç»“æ„åŠé€‚åº”ä¸åŒä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„å¤šå°ºåº¦æ‹“æ‰‘ç²’åº¦æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹åŒä¸€å›¾è¿›è¡Œå˜æ¢æ¥ç”Ÿæˆå¤šå°ºåº¦çš„åŸºäºç¯çš„ cellular complexesï¼Œä»è€Œä½“ç° topological granularity çš„æ¦‚å¿µå¹¶æ„å»ºå¤šæ ·çš„æ‹“æ‰‘è§†å›¾ã€‚é’ˆå¯¹ç‰¹å®šç²’åº¦å¯èƒ½åŒ…å«è¯¯å¯¼æ€§è¯­ä¹‰çš„é—®é¢˜ï¼Œç ”ç©¶è€…è®¾è®¡äº† multi-granularity decoupled contrast ç­–ç•¥ï¼Œå¹¶åº”ç”¨äº†åŸºäº uncertainty estimation çš„ç²’åº¦ç‰¹å®šåŠ æƒæœºåˆ¶ã€‚å¤§é‡åŸºå‡†æµ‹è¯•å®éªŒè¯æ˜ï¼ŒHTG-GCL åœ¨æ•è·å…·æœ‰å®é™…æ„ä¹‰çš„å›¾è¡¨ç¤ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå……åˆ†å±•ç¤ºäº†å±‚æ¬¡åŒ–æ‹“æ‰‘ä¿¡æ¯åœ¨å¢å¼ºæ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆå¤šå°ºåº¦æ‹“æ‰‘ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚å›¾æ•°æ®ä¸Šçš„åˆ¤åˆ«èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02073v1",
      "published_date": "2025-11-30 09:09:42 UTC",
      "updated_date": "2025-11-30 09:09:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:31.173451+00:00"
    },
    {
      "arxiv_id": "2512.00791v2",
      "title": "Limitations of Using Identical Distributions for Training and Testing When Learning Boolean Functions",
      "title_zh": "å¸ƒå°”å‡½æ•°å­¦ä¹ ä¸­è®­ç»ƒä¸æµ‹è¯•åŒåˆ†å¸ƒçš„å±€é™æ€§",
      "authors": [
        "Jordi PÃ©rez-Guijarro"
      ],
      "abstract": "When the distributions of the training and test data do not coincide, the problem of understanding generalization becomes considerably more complex, prompting a variety of questions. Prior work has shown that, for some fixed learning methods, there are scenarios where training on a distribution different from the test distribution improves generalization. However, these results do not account for the possibility of choosing, for each training distribution, the optimal learning algorithm, leaving open whether the observed benefits stem from the mismatch itself or from suboptimality of the learner. In this work, we address this question in full generality. That is, we study whether it is always optimal for the training distribution to be identical to the test distribution when the learner is allowed to be optimally adapted to the training distribution. Surprisingly, assuming the existence of one-way functions, we find that the answer is no. That is, matching distributions is not always the best scenario. Nonetheless, we also show that when certain regularities are imposed on the target functions, the standard conclusion is recovered in the case of the uniform distribution.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨å­¦ä¹  Boolean Functions æ—¶ï¼Œè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒä¸€è‡´æ˜¯å¦æ€»æ˜¯æœ€ä¼˜é€‰æ‹©çš„é—®é¢˜ã€‚ä»¥å¾€ç ”ç©¶è™½ç„¶è§‚å¯Ÿåˆ°åˆ†å¸ƒä¸åŒ¹é…å¯èƒ½æå‡ generalizationï¼Œä½†å¹¶æœªè§£å†³è¿™ä¸€ç›Šå¤„æ˜¯å¦æºäº learner çš„æ¬¡ä¼˜æ€§ã€‚è¯¥å·¥ä½œåœ¨å…è®¸ learner é’ˆå¯¹è®­ç»ƒåˆ†å¸ƒè¿›è¡Œæœ€ä¼˜é€‚é…çš„å…¨åŸŸæ€§å‡è®¾ä¸‹ï¼Œå‘ç° matching distributions å¹¶ä¸æ€»æ˜¯èƒ½äº§ç”Ÿæœ€ä½³ç»“æœã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåªè¦å‡è®¾ one-way functions å­˜åœ¨ï¼Œç ”ç©¶ä¾¿è¯æ˜äº†åˆ†å¸ƒé”™ä½åœ¨æŸäº›åœºæ™¯ä¸‹ä¼˜äºåˆ†å¸ƒä¸€è‡´ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶ä¹Ÿè¡¨æ˜è‹¥å¯¹ç›®æ ‡å‡½æ•°æ–½åŠ ç‰¹å®šçš„ regularities çº¦æŸï¼Œåœ¨ uniform distribution æƒ…å†µä¸‹ä»ä¼šå›å½’åˆ°åˆ†å¸ƒä¸€è‡´æ€§æœ€ä¼˜çš„ä¼ ç»Ÿç»“è®ºã€‚è¯¥å‘ç°æŒ‘æˆ˜äº†æœºå™¨å­¦ä¹ ä¸­å…³äºè®­ç»ƒä¸æµ‹è¯•ç¯å¢ƒå¿…é¡»å®Œå…¨ä¸€è‡´çš„å¸¸è§„å‡è®¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00791v2",
      "published_date": "2025-11-30 09:06:07 UTC",
      "updated_date": "2025-12-02 10:04:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:35.366608+00:00"
    },
    {
      "arxiv_id": "2512.00772v1",
      "title": "SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG",
      "title_zh": "SHRAGï¼šä¸€ç§ç»“åˆäººç±»å¯å‘å¼æœç´¢ä¸ RAG çš„æ¡†æ¶",
      "authors": [
        "Hyunseok Ryu",
        "Wonjune Shin",
        "Hyun Park"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language\n  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to\n  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing\n  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.\n  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of\n  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a\n  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries\n  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process\n  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual\n  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual\n  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,\n  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and\n  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,\n  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SHRAG æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­é¢ä¸´çš„ç³»ç»Ÿæ„å»ºå¤æ‚æ€§ä»¥åŠå¤„ç†é€Ÿåº¦è¾ƒæ…¢çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† Large Language Model ä½œä¸º Query Strategistï¼Œèƒ½å°†éç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢è‡ªåŠ¨è½¬åŒ–ä¸ºé€»è¾‘ç»“æ„åŒ–çš„æœç´¢æŒ‡ä»¤ã€‚SHRAG é€šè¿‡æ‰§è¡Œ Boolean retrieval æ¥æ¨¡ä»¿äººç±»ä¸“å®¶æœç´¢è€…çš„æ£€ç´¢è¿‡ç¨‹ï¼Œå®ç°äº†ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢(Information Retrieval)ä¸ RAG çš„æ— ç¼é›†æˆã€‚è¯¥ç³»ç»Ÿè¿›ä¸€æ­¥é›†æˆäº†å¤šè¯­è¨€æŸ¥è¯¢æ‰©å±•(multilingual query expansion)å’Œå¤šè¯­è¨€åµŒå…¥æ¨¡å‹(multilingual embedding model)ï¼Œåœ¨ ScienceON Challenge çš„å¤šè¯­è¨€æ•°æ®é›†ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šçš„è·¨è¯­è¨€é—®ç­”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»“åˆé€»è¾‘æ£€ç´¢èƒ½åŠ›ä¸ç”Ÿæˆå¼æ¨ç†çš„æ–¹æ³•æ˜¾è‘—æå‡äº† RAG ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚SHRAG è¶…è¶Šäº†ä¼ ç»Ÿçš„ä»¥æ–‡æ¡£ä¸ºä¸­å¿ƒçš„æ£€ç´¢æ¨¡å¼ï¼Œä¸ºæä¾›ç›´æ¥ä¸”å¯é çš„æŸ¥è¯¢å“åº”å¼€è¾Ÿäº†æ–°çš„æœç´¢èŒƒå¼(search paradigm)ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 4 figures, 1 table, 1 algorithm, 3 prompts",
      "pdf_url": "https://arxiv.org/pdf/2512.00772v1",
      "published_date": "2025-11-30 08:06:47 UTC",
      "updated_date": "2025-11-30 08:06:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:33.572624+00:00"
    },
    {
      "arxiv_id": "2512.00771v1",
      "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
      "title_zh": "EAG3Rï¼šé¢å‘åŠ¨æ€åŠæç«¯å…‰ç…§åœºæ™¯çš„äº‹ä»¶å¢å¼ºå‹ä¸‰ç»´å‡ ä½•ä¼°è®¡",
      "authors": [
        "Xiaoshan Wu",
        "Yifei Yu",
        "Xiaoyang Lyu",
        "Yihua Huang",
        "Bo Wang",
        "Baoheng Zhang",
        "Zhongrui Wang",
        "Xiaojuan Qi"
      ],
      "abstract": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EAG3Rï¼Œä¸€ç§é€šè¿‡å¼‚æ­¥äº‹ä»¶æµï¼ˆEvent streamsï¼‰å¢å¼º3Då‡ ä½•ä¼°è®¡çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿçº¯RGBæ–¹æ³•åœ¨åŠ¨æ€ç‰©ä½“å’Œæç«¯å…‰ç…§æ¡ä»¶ä¸‹çš„å±€é™æ€§ã€‚EAG3Råœ¨MonST3Rä¸»å¹²ç½‘ç»œçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†å—Retinexå¯å‘çš„å¯è§†å¢å¼ºæ¨¡å—ï¼Œä»¥åŠå…·å¤‡ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ„ŸçŸ¥èåˆæœºåˆ¶çš„è½»é‡åŒ–äº‹ä»¶é€‚é…å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®å±€éƒ¨ç¯å¢ƒçš„å¯é æ€§åŠ¨æ€ç»“åˆRGBä¸äº‹ä»¶ç‰¹å¾ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºäº‹ä»¶çš„å…‰åº¦ä¸€è‡´æ€§æŸå¤±ï¼ˆPhotometric consistency lossï¼‰ï¼Œåœ¨å…¨å±€ä¼˜åŒ–è¿‡ç¨‹ä¸­æœ‰æ•ˆå¼ºåŒ–äº†æ—¶ç©ºç›¸å¹²æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAG3Råœ¨å•ç›®æ·±åº¦ä¼°è®¡ã€ç›¸æœºä½å§¿è·Ÿè¸ªå’ŒåŠ¨æ€é‡å»ºä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œä¸”æ— éœ€åœ¨å¤œé—´æ•°æ®ä¸Šè¿›è¡Œé¢å¤–è®­ç»ƒå³å¯åœ¨æä½å…‰ç…§çš„åŠ¨æ€åœºæ™¯ä¸­å®ç°ç¨³å¥çš„å‡ ä½•ä¼°è®¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS 2025 (spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2512.00771v1",
      "published_date": "2025-11-30 08:05:28 UTC",
      "updated_date": "2025-11-30 08:05:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:44.276957+00:00"
    },
    {
      "arxiv_id": "2512.00763v1",
      "title": "Provable Benefit of Sign Descent: A Minimal Model Under Heavy-Tailed Class Imbalance",
      "title_zh": "ç¬¦å·ä¸‹é™çš„å¯è¯æ˜ä¼˜è¶Šæ€§ï¼šé‡å°¾ç±»åˆ«ä¸å¹³è¡¡ä¸‹çš„æç®€æ¨¡å‹",
      "authors": [
        "Robin Yadav",
        "Shuo Xie",
        "Tianhao Wang",
        "Zhiyuan Li"
      ],
      "abstract": "Adaptive optimization methods (such as Adam) play a major role in LLM pretraining, significantly outperforming Gradient Descent (GD). Recent studies have proposed new smoothness assumptions on the loss function to explain the advantages of adaptive algorithms with structured preconditioners, e.g., coordinate-wise or layer-wise, and steepest descent methods w.r.t. non-euclidean norms, e.g., $\\ell_\\infty$ norm or spectral norm, over GD. However, it remains unclear how these smoothness assumptions manifest in language modelling tasks. In this work, we aim to analyze the benefit of $\\ell_\\infty$-norm descent (a.k.a. sign descent) directly from properties of the data distribution, namely, heavy-tailed class imbalance. We propose a minimal yet representative setting of next-token prediction, where we can provably show faster convergence of coordinate-wise algorithms such as Sign descent (steepest descent w.r.t. $\\ell_\\infty$ norm) over normalized GD (steepest descent w.r.t. to $\\ell_2$ norm) in the presence of heavy tail class imbalance.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº† Sign Descentï¼ˆå³ $\\ell_\\infty$-norm descentï¼‰åœ¨é‡å°¾ç±»å¤±è¡¡ï¼ˆheavy-tailed class imbalanceï¼‰æ¡ä»¶ä¸‹çš„å¯è¯æ˜ä¼˜åŠ¿ã€‚ç ”ç©¶èƒŒæ™¯æŒ‡å‡ºï¼Œè™½ç„¶ Adam ç­‰è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ³•åœ¨ LLM é¢„è®­ç»ƒä¸­è¡¨ç°ä¼˜äº Gradient Descent (GD)ï¼Œä½†ç°æœ‰çš„å¹³æ»‘æ€§å‡è®¾è§£é‡Šæœªèƒ½ç›´æ¥æ­ç¤ºæ•°æ®åˆ†å¸ƒç‰¹æ€§çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæç®€ä¸”å…·æœ‰ä»£è¡¨æ€§çš„æ¬¡ä»£é¢„æµ‹ï¼ˆnext-token predictionï¼‰æ¨¡å‹ï¼Œç”¨äºåˆ†æç®—æ³•è¡¨ç°ä¸æ•°æ®åˆ†å¸ƒå±æ€§ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶é€šè¿‡ç†è®ºè¯æ˜å‘ç°ï¼Œåœ¨å­˜åœ¨é‡å°¾ç±»å¤±è¡¡çš„æƒ…å†µä¸‹ï¼ŒSign Descent ç­‰é€åæ ‡ç®—æ³•ï¼ˆcoordinate-wise algorithmsï¼‰çš„æ”¶æ•›é€Ÿåº¦æ˜æ˜¾å¿«äºåŸºäº $\\ell_2$ èŒƒæ•°çš„å½’ä¸€åŒ– GDã€‚è¯¥å·¥ä½œç›´æ¥ä»æ•°æ®åˆ†å¸ƒçš„è§’åº¦å‡ºå‘ï¼Œä¸ºè‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00763v1",
      "published_date": "2025-11-30 07:21:02 UTC",
      "updated_date": "2025-11-30 07:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:16:59.073929+00:00"
    },
    {
      "arxiv_id": "2512.00757v1",
      "title": "Preventing Model Collapse via Contraction-Conditioned Neural Filters",
      "title_zh": "é€šè¿‡æ”¶ç¼©æ¡ä»¶ç¥ç»æ»¤æ³¢å™¨é˜²æ­¢æ¨¡å‹å´©æºƒ",
      "authors": [
        "Zongjian Han",
        "Yiran Liang",
        "Ruiwen Wang",
        "Yiwei Luo",
        "Yilin Huang",
        "Xiaotong Song",
        "Dongqing Wei"
      ],
      "abstract": "This paper presents a neural network filter method based on contraction operators to address model collapse in recursive training of generative models. Unlike \\cite{xu2024probabilistic}, which requires superlinear sample growth ($O(t^{1+s})$), our approach completely eliminates the dependence on increasing sample sizes within an unbiased estimation framework by designing a neural filter that learns to satisfy contraction conditions. We develop specialized neural network architectures and loss functions that enable the filter to actively learn contraction conditions satisfying Assumption 2.3 in exponential family distributions, thereby ensuring practical application of our theoretical results. Theoretical analysis demonstrates that when the learned contraction conditions are satisfied, estimation errors converge probabilistically even with constant sample sizes, i.e., $\\limsup_{t\\to\\infty}\\mathbb{P}(\\|\\mathbf{e}_t\\|>Î´)=0$ for any $Î´>0$. Experimental results show that our neural network filter effectively learns contraction conditions and prevents model collapse under fixed sample size settings, providing an end-to-end solution for practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹é€’å½’è®­ç»ƒä¸­çš„ model collapse é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº contraction operators çš„ç¥ç»ç½‘ç»œæ»¤æ³¢å™¨æ–¹æ³•ã€‚ä¸ç°æœ‰ç ”ç©¶éœ€è¦æ ·æœ¬é‡å‘ˆè¶…çº¿æ€§å¢é•¿ï¼ˆ$O(t^{1+s}$ï¼‰ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡è®¾è®¡å­¦ä¹ æ”¶ç¼©æ¡ä»¶çš„ç¥ç»æ»¤æ³¢å™¨ï¼Œåœ¨æ— åä¼°è®¡æ¡†æ¶ä¸‹å½»åº•æ¶ˆé™¤äº†å¯¹æ ·æœ¬é‡å¢åŠ çš„ä¾èµ–ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸“é—¨çš„ç¥ç»ç½‘ç»œæ¶æ„å’ŒæŸå¤±å‡½æ•°ï¼Œä½¿æ»¤æ³¢å™¨åœ¨ exponential family distributions ä¸­èƒ½ä¸»åŠ¨å­¦ä¹ å¹¶æ»¡è¶³ç‰¹å®šçš„æ”¶ç¼©æ¡ä»¶ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œåœ¨æ»¡è¶³å­¦ä¹ åˆ°çš„æ”¶ç¼©æ¡ä»¶æ—¶ï¼Œå³ä½¿æ ·æœ¬é‡ä¿æŒæ’å®šï¼Œå…¶ä¼°è®¡è¯¯å·®ä¹Ÿä¼šåœ¨æ¦‚ç‡ä¸Šå®ç°æ”¶æ•›ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥ç¥ç»æ»¤æ³¢å™¨èƒ½åœ¨å›ºå®šæ ·æœ¬é‡è®¾ç½®ä¸‹æœ‰æ•ˆé˜²æ­¢ model collapseï¼Œä¸ºç›¸å…³å®é™…åº”ç”¨æä¾›äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00757v1",
      "published_date": "2025-11-30 06:48:21 UTC",
      "updated_date": "2025-11-30 06:48:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:03.673640+00:00"
    },
    {
      "arxiv_id": "2512.00756v1",
      "title": "MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents",
      "title_zh": "MPR-GUIï¼šGUI æ™ºèƒ½ä½“å¤šè¯­è¨€æ„ŸçŸ¥ä¸æ¨ç†çš„åŸºå‡†è¯„æµ‹ä¸å¢å¼º",
      "authors": [
        "Ruihan Chen",
        "Qiming Li",
        "Xiaocheng Feng",
        "Xiaoliang Yang",
        "Weihong Zhong",
        "Yuxuan Gu",
        "Zekun Zhou",
        "Bing Qin"
      ],
      "abstract": "With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)ä»»åŠ¡ä¸­å­˜åœ¨çš„è·¨è¯­è¨€æ€§èƒ½å·®å¼‚ï¼Œæå‡ºäº†MPR-GUI-BenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ç»†ç²’åº¦æ„ŸçŸ¥ä¸æ¨ç†(Perception and Reasoning, P&R)èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒLVLMsåœ¨éè‹±è¯­GUIåœºæ™¯ä¸­çš„è¡¨ç°è¿œé€Šäºè‹±è¯­åœºæ™¯ï¼Œä¸”ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹ç»„ä»¶åŠŸèƒ½å’Œç©ºé—´å…³ç³»çš„æ·±å…¥åˆ†æã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†GUI-XLIè·¨è¯­è¨€å¹²é¢„æ–¹æ³•ï¼Œé€šè¿‡åœ¨éšè—å±‚å¯¹æ„ŸçŸ¥ä¸æ¨ç†ç›¸å…³çš„ç‰¹å¾çŠ¶æ€è¿›è¡Œå¹²é¢„ï¼Œä»¥æ¶ˆé™¤ä¸åŒè¯­è¨€åœ¨æ½œç©ºé—´ä¸­çš„è¡¨å¾å·®å¼‚ã€‚å®éªŒè¯æ˜ï¼ŒGUI-XLIä½¿GUIæ™ºèƒ½ä½“çš„å¤šè¯­è¨€æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›å¹³å‡æå‡äº†6.5%ã€‚è¯¥ç ”ç©¶ä¸ä»…å¡«è¡¥äº†å¤šè¯­è¨€GUIè¯„ä¼°çš„ç©ºç™½ï¼Œè¿˜ä¸ºå¢å¼ºæ¨¡å‹çš„è·¨è¯­è¨€è¿ç§»èƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "27pages, 12figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00756v1",
      "published_date": "2025-11-30 06:47:33 UTC",
      "updated_date": "2025-11-30 06:47:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:33.864398+00:00"
    },
    {
      "arxiv_id": "2512.00748v1",
      "title": "Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization",
      "title_zh": "é¢å‘å¤šæ ·æ€§ä¸ä¸ªæ€§åŒ–çš„å¤šè¯„åˆ†è€…åŒ»å­¦å›¾åƒåˆ†å‰²æ¦‚ç‡å»ºæ¨¡",
      "authors": [
        "Ke Liu",
        "Shangde Gao",
        "Yichao Fu",
        "Shangqi Gao",
        "Chunhua Shen"
      ],
      "abstract": "Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç”±äºå›¾åƒè¾¹ç•Œæ¨¡ç³Šå’Œè§‚å¯Ÿè€…é—´å·®å¼‚(inter-observer variability)å¯¼è‡´çš„æ•°æ®ä¸ç¡®å®šæ€§é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ¨¡å‹éš¾ä»¥å¹³è¡¡åˆ†å‰²ç»“æœçš„å¤šæ ·æ€§ä¸ä¸“å®¶ä¸ªæ€§åŒ–éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¤šè¯„åˆ†è€…åŒ»å­¦å›¾åƒåˆ†å‰²æ¦‚ç‡å»ºæ¨¡æ¡†æ¶(ProSeg)ï¼Œæ—¨åœ¨åŒæ—¶å®ç° diversification å’Œ personalizationã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºå¼•å…¥äº†ä¸¤ä¸ªéšå˜é‡(latent variables)åˆ†åˆ«å»ºæ¨¡ä¸“å®¶çš„æ ‡æ³¨åå¥½ä¸å›¾åƒçš„è¾¹ç•Œæ¨¡ç³Šæ€§ï¼Œå¹¶é€šè¿‡å˜åˆ†æ¨ç†(variational inference)è·å–å…¶æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProSeg åœ¨é¼»å’½ç™Œæ•°æ®é›†(NPC)å’Œè‚ºç»“èŠ‚æ•°æ®é›†(LIDC-IDRI)ä¸Šå‡è¾¾åˆ°äº†æ–°çš„ SOTA æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼ŒæˆåŠŸç”Ÿæˆäº†æ—¢å…·å¤‡å¤šæ ·æ€§åˆç¬¦åˆç‰¹å®šä¸“å®¶è¯Šæ–­ä¹ æƒ¯çš„åˆ†å‰²ç»“æœï¼Œä¸ºå¤„ç†å¤æ‚çš„åŒ»ç–—å½±åƒå¤šä¸“å®¶æ ‡æ³¨ä»»åŠ¡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00748v1",
      "published_date": "2025-11-30 05:53:39 UTC",
      "updated_date": "2025-11-30 05:53:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:10.572558+00:00"
    },
    {
      "arxiv_id": "2512.00742v1",
      "title": "On the Regulatory Potential of User Interfaces for AI Agent Governance",
      "title_zh": "è®º AI æ™ºèƒ½ä½“æ²»ç†ä¸­ç”¨æˆ·ç•Œé¢çš„ç›‘ç®¡æ½œåŠ›",
      "authors": [
        "K. J. Kevin Feng",
        "Tae Soo Kim",
        "Rock Yuren Pang",
        "Faria Huq",
        "Tal August",
        "Amy X. Zhang"
      ],
      "abstract": "AI agents that take actions in their environment autonomously over extended time horizons require robust governance interventions to curb their potentially consequential risks. Prior proposals for governing AI agents primarily target system-level safeguards (e.g., prompt injection monitors) or agent infrastructure (e.g., agent IDs). In this work, we explore a complementary approach: regulating user interfaces of AI agents as a way of enforcing transparency and behavioral requirements that then demand changes at the system and/or infrastructure levels. Specifically, we analyze 22 existing agentic systems to identify UI elements that play key roles in human-agent interaction and communication. We then synthesize those elements into six high-level interaction design patterns that hold regulatory potential (e.g., requiring agent memory to be editable). We conclude with policy recommendations based on our analysis. Our work exposes a new surface for regulatory action that supplements previous proposals for practical AI agent governance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹ AI agents çš„æ²»ç†å¹²é¢„æªæ–½ï¼Œæ—¨åœ¨é™ä½è‡ªä¸»æ™ºèƒ½ä½“åœ¨é•¿æœŸè¿è¡Œä¸­å¯èƒ½äº§ç”Ÿçš„é‡å¤§é£é™©ã€‚ä¸åŒäºä»¥å¾€ä¾§é‡äºç³»ç»Ÿçº§é˜²æŠ¤æˆ–åŸºç¡€è®¾æ–½çš„æ²»ç†æ–¹æ¡ˆï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡ç›‘ç®¡ User Interfaces (UI) æ¥å¼ºåˆ¶æ‰§è¡Œé€æ˜åº¦å’Œè¡Œä¸ºè¦æ±‚çš„æ–°å‹è¡¥å……è·¯å¾„ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹ 22 ä¸ªç°æœ‰æ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œæ·±å…¥åˆ†æï¼Œè¯†åˆ«å‡ºåœ¨ human-agent interaction è¿‡ç¨‹ä¸­èµ·åˆ°æ ¸å¿ƒä½œç”¨çš„å…³é”® UI å…ƒç´ ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶è€…å°†å…¶å½’çº³ä¸ºå…­ç§å…·æœ‰ç›‘ç®¡æ½œåŠ›çš„ interaction design patternsï¼Œä¾‹å¦‚è¦æ±‚ agent memory å¿…é¡»å…·å¤‡å¯ç¼–è¾‘æ€§ã€‚è¯¥é¡¹å·¥ä½œæœ€ç»ˆæå‡ºäº†ç›¸åº”çš„æ”¿ç­– recommendationsï¼Œæ­ç¤ºäº† UI ä½œä¸ºä¸€ä¸ªå…¨æ–°çš„ç›‘ç®¡åˆ‡å…¥ç‚¹ï¼Œèƒ½å¤Ÿä¸ºç°æœ‰çš„ AI agent governance æ¡†æ¶æä¾›æœ‰åŠ›è¡¥å……ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "RegML workshop at NeurIPS 2025 (oral)",
      "pdf_url": "https://arxiv.org/pdf/2512.00742v1",
      "published_date": "2025-11-30 05:32:13 UTC",
      "updated_date": "2025-11-30 05:32:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:11.548975+00:00"
    },
    {
      "arxiv_id": "2512.00741v1",
      "title": "MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset",
      "title_zh": "MASCOTï¼šåŸºäºç²¾å¿ƒæ„å»ºçš„æºä»£ç æ•°æ®é›†åˆ†ææ¶æ„è½¯ä»¶æ¼”åŒ–",
      "authors": [
        "Bojing Li",
        "Duo Zhong",
        "Dharani Nadendla",
        "Gabriel Terceros",
        "Prajna Bhandar",
        "Raguvir S",
        "Charles Nicholas"
      ],
      "abstract": "In recent years, the explosion of malware and extensive code reuse have formed complex evolutionary connections among malware specimens. The rapid pace of development makes it challenging for existing studies to characterize recent evolutionary trends. In addition, intuitive tools to untangle these intricate connections between malware specimens or categories are urgently needed. This paper introduces a manually-reviewed malware source code dataset containing 6032 specimens. Building on and extending current research from a software engineering perspective, we systematically evaluate the scale, development costs, code quality, as well as security and dependencies of modern malware. We further introduce a multi-view genealogy analysis to clarify malware connections: at an overall view, this analysis quantifies the strength and direction of connections among specimens and categories; at a detailed view, it traces the evolutionary histories of individual specimens. Experimental results indicate that, despite persistent shortcomings in code quality, malware specimens exhibit an increasing complexity and standardization, in step with the development of mainstream software engineering practices. Meanwhile, our genealogy analysis intuitively reveals lineage expansion and evolution driven by code reuse, providing new evidence and tools for understanding the formation and evolution of the malware ecosystem.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†MASCOTï¼Œä¸€ä¸ªåŒ…å«6032ä¸ªç»è¿‡äººå·¥å®¡æŸ¥çš„æ¶æ„è½¯ä»¶æºä»£ç æ•°æ®é›†ï¼Œæ—¨åœ¨ä»è½¯ä»¶å·¥ç¨‹è§†è§’æ·±å…¥åˆ†ææ¶æ„è½¯ä»¶çš„æ¼”åŒ–è¶‹åŠ¿ã€‚ç ”ç©¶äººå‘˜é€šè¿‡è¯¥æ•°æ®é›†ç³»ç»Ÿè¯„ä¼°äº†ç°ä»£æ¶æ„è½¯ä»¶çš„å¼€å‘æˆæœ¬ã€ä»£ç è´¨é‡ã€å®‰å…¨æ€§åŠä¾èµ–å…³ç³»ï¼Œå¹¶å¼•å…¥äº†å¤šè§†å›¾è°±ç³»åˆ†æ(multi-view genealogy analysis)æ¥é‡åŒ–æ ·æœ¬ä¸ç±»åˆ«é—´çš„å…³è”å¼ºåº¦åŠæ¼”åŒ–æ–¹å‘ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡åœ¨ä»£ç è´¨é‡ä¸Šä»å­˜åœ¨çŸ­æ¿ï¼Œä½†æ¶æ„è½¯ä»¶æ­£éšç€ä¸»æµè½¯ä»¶å·¥ç¨‹å®è·µçš„å‘å±•è€Œå±•ç°å‡ºæ›´é«˜çš„å¤æ‚æ€§å’Œæ ‡å‡†åŒ–æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥åˆ†æç›´è§‚åœ°æ­ç¤ºäº†ç”±ä»£ç å¤ç”¨(code reuse)é©±åŠ¨çš„è°±ç³»æ‰©å¼ ï¼Œä¸ºç†è§£æ¶æ„è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿçš„å½¢æˆä¸æ¼”åŒ–æä¾›äº†å…³é”®è¯æ®å’Œåˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 6 figures, conference paper; submitted to IEEE BigData 2025 CyberHunt workshop",
      "pdf_url": "https://arxiv.org/pdf/2512.00741v1",
      "published_date": "2025-11-30 05:26:23 UTC",
      "updated_date": "2025-11-30 05:26:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:12.765532+00:00"
    },
    {
      "arxiv_id": "2512.00738v2",
      "title": "Orchestrating Rewards in the Era of Intelligence-Driven Commerce",
      "title_zh": "æ™ºèƒ½é©±åŠ¨å•†ä¸šæ—¶ä»£çš„å¥–åŠ±ç»Ÿç­¹",
      "authors": [
        "Paul Osemudiame Oamen",
        "Robert Wesley",
        "Pius Onobhayedo"
      ],
      "abstract": "Despite their evolution from early copper-token schemes to sophisticated digital solutions, loyalty programs remain predominantly closed ecosystems, with brands retaining full control over all components. Coalition loyalty programs emerged to enable cross-brand interoperability, but approximately 60\\% fail within 10 years in spite of theoretical advantages rooted in network economics. This paper demonstrates that coalition failures stem from fundamental architectural limitations in centralized operator models rather than operational deficiencies, and argues further that neither closed nor coalition systems can scale in intelligence-driven paradigms where AI agents mediate commerce and demand trustless, protocol-based coordination that existing architectures cannot provide. We propose a hybrid framework where brands maintain sovereign control over their programs while enabling cross-brand interoperability through trustless exchange mechanisms. Our framework preserves closed system advantages while enabling open system benefits without the structural problems that doom traditional coalitions. We derive a mathematical pricing model accounting for empirically-validated market factors while enabling fair value exchange across interoperable reward systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¿ è¯šåº¦è®¡åˆ’(loyalty programs)ä»æ—©æœŸä»£å¸å‘æ•°å­—åŒ–æ–¹æ¡ˆæ¼”è¿›è¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰ä¸»æµçš„å°é—­ç”Ÿæ€ç³»ç»Ÿå’Œä¸­å¿ƒåŒ–è”ç›Ÿæ¨¡å¼å·²æ— æ³•æ»¡è¶³ç”± AI Agents é©±åŠ¨çš„æ™ºèƒ½å•†ä¸šæ—¶ä»£å¯¹å»ä¸­å¿ƒåŒ–ã€åŸºäºåè®®åä½œçš„éœ€æ±‚ã€‚è®ºæ–‡åˆ†æå‘ç°ï¼Œçº¦ 60% çš„è”ç›Ÿå¿ è¯šåº¦è®¡åˆ’åœ¨åå¹´å†…å¤±è´¥ï¼Œå…¶æ ¹æºåœ¨äºä¸­å¿ƒåŒ–è¿è¥å•†æ¨¡å‹çš„æ¶æ„å±€é™æ€§è€Œéè¿è¥ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œåœ¨å…è®¸å“ç‰Œä¿ç•™å¯¹å…¶é¡¹ç›®ä¸»æƒæ§åˆ¶çš„åŒæ—¶ï¼Œé€šè¿‡ä¿¡ä»»æœ€å°åŒ–äº¤æ¢æœºåˆ¶(trustless exchange mechanisms)å®ç°è·¨å“ç‰Œäº’æ“ä½œæ€§ã€‚è¯¥æ¡†æ¶æ—¢ä¿ç•™äº†å°é—­ç³»ç»Ÿçš„ä¼˜åŠ¿ï¼Œåˆå®ç°äº†å¼€æ”¾ç³»ç»Ÿçš„æ”¶ç›Šï¼Œä¸”æœ‰æ•ˆè§„é¿äº†å¯¼è‡´ä¼ ç»Ÿè”ç›Ÿå¤±è´¥çš„ç»“æ„æ€§å¼Šç«¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ¨å¯¼å‡ºäº†ä¸€ä¸ªç»¼åˆè€ƒè™‘å®è¯å¸‚åœºå› ç´ çš„æ•°å­¦å®šä»·æ¨¡å‹ï¼Œä¸ºè·¨äº’æ“ä½œå¥–åŠ±ç³»ç»Ÿçš„å…¬å¹³ä»·å€¼äº¤æ¢æä¾›äº†ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00738v2",
      "published_date": "2025-11-30 05:24:18 UTC",
      "updated_date": "2025-12-29 20:41:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:32.362758+00:00"
    },
    {
      "arxiv_id": "2512.00736v1",
      "title": "REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories",
      "title_zh": "REMï¼šåŸºäºå¤šå¸§è½¨è¿¹çš„å¤§è¯­è¨€æ¨¡å‹å…·èº«ç©ºé—´æ¨ç†èƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Jacob Thompson",
        "Emiliano Garcia-Lopez",
        "Yonatan Bisk"
      ],
      "abstract": "Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ç»è¿‡äº†å¤§é‡çš„è§†é¢‘è®­ç»ƒï¼Œä½†åœ¨å…·èº«ç©ºé—´æ¨ç† (Embodied Spatial Reasoning) èƒ½åŠ›ä¸Šä»å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œéš¾ä»¥æ„å»ºè§†ç‚¹æ— å…³çš„è®¤çŸ¥åœ°å›¾ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº† REM (Reasoning over Embodied Multi-Frame Trajectories) åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨å¯æ§çš„ 3D ç¯å¢ƒå¯¹é•¿ç¨‹å…·èº«ç©ºé—´æ¨ç†è¿›è¡Œè¯„ä¼°ã€‚REM ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†è·¨åŠ¨æ€å…·èº«è§†ç‚¹çš„ç‰©ä½“æ°¸æ’æ€§ä¸åŒºåˆ† (Object Permanence/Distinction)ã€ç©ºé—´å…³ç³» (Spatial Relationships) ä»¥åŠæ•°å€¼è¿½è¸ª (Numerical Tracking) ç­‰å…³é”®ç»´åº¦ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç›®å‰çš„é¡¶å°–æ¨¡å‹åœ¨å¤„ç†äººç±»å¯è½»æ¾åº”å¯¹çš„ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡æ—¶è¡¨ç°å¾—è¶Šæ¥è¶Šä¸å¯é ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº† MLLMs ä»åºåˆ—åŒ–è§†è§‰è¾“å…¥ä¸­æ„å»ºé²æ£’ç©ºé—´è¡¨å¾æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè€Œ REM ä¸ºæå‡æœªæ¥æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›æä¾›äº†é’ˆå¯¹æ€§çš„åº¦é‡æŒ‡æ ‡ä¸è¯Šæ–­å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00736v1",
      "published_date": "2025-11-30 05:20:22 UTC",
      "updated_date": "2025-11-30 05:20:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:30.453686+00:00"
    },
    {
      "arxiv_id": "2512.00729v1",
      "title": "Probing the \"Psyche'' of Large Reasoning Models: Understanding Through a Human Lens",
      "title_zh": "æ¢ç©¶å¤§æ¨ç†æ¨¡å‹çš„â€œå¿ƒç†â€ï¼šä»äººç±»è§†è§’æ·±åº¦ç†è§£",
      "authors": [
        "Yuxiang Chen",
        "Zuohan Wu",
        "Ziwei Wang",
        "Xiangning Yu",
        "Xujia Li",
        "Linyi Yang",
        "Mengyue Yang",
        "Jun Wang",
        "Lei Chen"
      ],
      "abstract": "Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§æ¨ç†æ¨¡å‹ (Large reasoning models, LRMs) çš„â€œå¿ƒç†â€ï¼Œæ—¨åœ¨é€šè¿‡äººç±»è®¤çŸ¥è§†è§’ç†è§£å…¶å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŒ…å«5ä¸ªç»„å’Œ17ä¸ªç±»åˆ«çš„åˆ†ç±»æ³• (Taxonomy)ï¼Œç”¨äºè¡¨å¾ LRMs çš„åŸå­æ¨ç†æ­¥éª¤ (atomic reasoning steps)ï¼Œå°†å…¶ç†è§£å»ºç«‹åœ¨è·¨å­¦ç§‘è§†è§’ä¹‹ä¸Šã€‚åŸºäºè¯¥åˆ†ç±»æ³•ï¼Œç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåŒ…å« 277,534 ä¸ªåŸå­æ¨ç†æ­¥éª¤çš„æ ‡æ³¨æ•°æ®é›†ï¼Œç”¨äºæ·±å…¥åˆ†æå½“å‰æ¨¡å‹çš„æ¨ç†è¡Œä¸ºã€‚åˆ†æå‘ç°ï¼Œç°æœ‰çš„â€œç­”åæ£€æŸ¥â€ (post-answer double-checks) å¤§å¤šåœç•™åœ¨è¡¨é¢ï¼Œå¾ˆå°‘äº§ç”Ÿå®è´¨æ€§çš„ä¿®æ­£ï¼Œå› æ­¤æ¿€åŠ±å…¨é¢çš„å¤šæ­¥åæ€ (multi-step reflection) æ˜¯æå‡æ¨¡å‹èƒ½åŠ›çš„æ›´æœ‰æ•ˆé€”å¾„ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†è‡ªåŠ¨æ ‡æ³¨æ¡†æ¶ CAPOï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”ŸæˆåŸºäºè¯¥åˆ†ç±»æ³•çš„æ ‡æ³¨ã€‚å®éªŒè¯æ˜ï¼ŒCAPO ä¸äººç±»ä¸“å®¶è¡¨ç°å‡ºæ›´é«˜çš„ä¸€è‡´æ€§ï¼Œä¸ºå¤§è§„æ¨¡ã€ç³»ç»ŸåŒ–åœ°ç†è§£å’Œä¼˜åŒ– LRMs æ¨ç†èƒ½åŠ›æä¾›äº†åŸåˆ™æ€§å’Œå¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.00729v1",
      "published_date": "2025-11-30 04:49:44 UTC",
      "updated_date": "2025-11-30 04:49:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:30.182138+00:00"
    },
    {
      "arxiv_id": "2512.00728v1",
      "title": "Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation",
      "title_zh": "ç”¨äºæ··åˆé£ç”µåœºå‘ç”µå»ºæ¨¡ä¸è°ƒåº¦çš„æ·±åº¦å­¦ä¹ ",
      "authors": [
        "Zach Lawrence",
        "Jessica Yao",
        "Chris Qin"
      ],
      "abstract": "Wind farms with integrated energy storage, or hybrid wind farms, are able to store energy and dispatch it to the grid following an operational strategy. For individual wind farms with integrated energy storage capacity, data-driven dispatch strategies using localized grid demand and market conditions as input parameters stand to maximize wind energy value. Synthetic power generation data modeled on atmospheric conditions provide another avenue for improving the robustness of data-driven dispatch strategies. To these ends, the present work develops two deep learning frameworks: COVE-NN, an LSTM-based dispatch strategy tailored to individual wind farms, which reduced annual COVE by 32.3% over 43 years of simulated operations in a case study at the Pyron site; and a power generation modeling framework that reduced RMSE by 9.5% and improved power curve similarity by 18.9% when validated on the Palouse wind farm. Together, these models pave the way for more robust, data-driven dispatch strategies and potential extensions to other renewable energy systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›†æˆå‚¨èƒ½çš„æ··åˆé£ç”µåœº(Hybrid Wind Farms)ï¼Œå¼€å‘äº†ä¸¤ç§æ·±åº¦å­¦ä¹ (Deep Learning)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•°æ®é©±åŠ¨çš„ç­–ç•¥ä¼˜åŒ–é£èƒ½ä»·å€¼ã€‚ç¬¬ä¸€ç§æ¡†æ¶æ˜¯åŸºäºé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)çš„è°ƒåº¦ç­–ç•¥COVE-NNï¼Œä¸“é—¨é’ˆå¯¹å•ä¸ªé£ç”µåœºè®¾è®¡ï¼Œèƒ½å¤Ÿç»“åˆå±€éƒ¨ç”µç½‘éœ€æ±‚å’Œå¸‚åœºæ¡ä»¶ä¼˜åŒ–ç”µåŠ›è°ƒåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOVE-NNåœ¨Pyronç«™ç‚¹çš„é•¿æœŸæ¨¡æ‹Ÿè¿è¡Œä¸­å°†å¹´åº¦COVEé™ä½äº†32.3%ã€‚ç¬¬äºŒç§å‘ç”µå»ºæ¨¡æ¡†æ¶åˆ©ç”¨å¤§æ°”æ¡ä»¶ç”Ÿæˆçš„åˆæˆæ•°æ®æé«˜äº†ç³»ç»Ÿçš„ç¨³å¥æ€§ï¼Œåœ¨Palouseé£ç”µåœºçš„éªŒè¯ä¸­ä½¿å‡æ–¹æ ¹è¯¯å·®(RMSE)é™ä½äº†9.5%ï¼Œå¹¶å°†åŠŸç‡æ›²çº¿ç›¸ä¼¼åº¦æå‡äº†18.9%ã€‚è¿™äº›ç ”ç©¶æˆæœä¸ºæ„å»ºæ›´ç¨³å¥çš„æ•°æ®é©±åŠ¨è°ƒåº¦ç­–ç•¥å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å±•ç°äº†å°†å…¶æ‰©å±•åˆ°å…¶ä»–å¯å†ç”Ÿèƒ½æºç³»ç»Ÿçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 8 figures, to be published in 2025 IEEE International Conference on Data Mining Workshops (ICDMW)",
      "pdf_url": "https://arxiv.org/pdf/2512.00728v1",
      "published_date": "2025-11-30 04:47:00 UTC",
      "updated_date": "2025-11-30 04:47:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:17:52.877627+00:00"
    },
    {
      "arxiv_id": "2512.00722v1",
      "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
      "title_zh": "SpeContextï¼šé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨æµ‹æ€§ä¸Šä¸‹æ–‡ç¨€ç–å®ç°é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡æ¨ç†",
      "authors": [
        "Jiaming Xu",
        "Jiayi Pan",
        "Hanzhen Wang",
        "Yongkang Zhou",
        "Jiancai Ye",
        "Yu Wang",
        "Guohao Dai"
      ],
      "abstract": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SpeContextï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„é«˜æ•ˆç®—æ³•ä¸ç³»ç»ŸååŒè®¾è®¡æ–¹æ¡ˆã€‚ç ”ç©¶è€…é€šè¿‡åˆ†æè’¸é¦è¯­è¨€æ¨¡å‹(DLM)ä¸åŸå§‹LLMåœ¨ä¿¡æ¯å…³æ³¨ç‚¹ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œæå‡ºåˆ©ç”¨DLMä½œä¸ºæ£€ç´¢ç®—æ³•çš„æ–°èŒƒå¼ã€‚åœ¨ç®—æ³•å±‚é¢ï¼ŒSpeContextåŸºäºDLMçš„å¤´éƒ¨æ³¨æ„åŠ›æƒé‡æ„å»ºè½»é‡åŒ–æ£€ç´¢å¤´ï¼Œé€šè¿‡å‰ªæå†—ä½™å®ç°äº†è¶…è¿‡90%çš„å‚æ•°ç¼©å‡ã€‚ç³»ç»Ÿä¸ç¼–è¯‘å±‚é¢åˆ™é€šè¿‡å¼‚æ­¥é¢„å–æ•°æ®æµå’Œè‡ªé€‚åº”å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œæœ‰æ•ˆå®ç°äº†KV cacheæ£€ç´¢ä¸è®¡ç®—çš„é‡å ï¼Œå¹¶æå¤§åŒ–äº†GPUå†…å­˜åˆ©ç”¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸Huggingfaceæ¡†æ¶ç›¸æ¯”ï¼ŒSpeContextåœ¨äº‘ç«¯å’Œè¾¹ç¼˜ç¯å¢ƒä¸‹çš„ååé‡åˆ†åˆ«æå‡äº†24.89å€å’Œ10.06å€ï¼Œä¸”å‡†ç¡®ç‡æŸå¤±å‡ ä¹å¯ä»¥å¿½ç•¥ã€‚è¯¥ç ”ç©¶æˆåŠŸæ¨åŠ¨äº†é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­å‡†ç¡®ç‡ä¸ååé‡çš„å¸•ç´¯æ‰˜å‰æ²¿(Pareto frontier)ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ¨ç†æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ASPLOS 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.00722v1",
      "published_date": "2025-11-30 04:32:43 UTC",
      "updated_date": "2025-11-30 04:32:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:18:19.975817+00:00"
    },
    {
      "arxiv_id": "2512.00716v1",
      "title": "Graph Data Augmentation with Contrastive Learning on Covariate Distribution Shift",
      "title_zh": "é¢å‘åå˜é‡åˆ†å¸ƒåç§»çš„å›¾æ•°æ®å¯¹æ¯”å­¦ä¹ å¢å¼º",
      "authors": [
        "Fanlong Zeng",
        "Wensheng Gan"
      ],
      "abstract": "Covariate distribution shift occurs when certain structural features present in the test set are absent from the training set. It is a common type of out-of-distribution (OOD) problem, frequently encountered in real-world graph data with complex structures. Existing research has revealed that most out-of-the-box graph neural networks (GNNs) fail to account for covariate shifts. Furthermore, we observe that existing methods aimed at addressing covariate shifts often fail to fully leverage the rich information contained within the latent space. Motivated by the potential of the latent space, we introduce a new method called MPAIACL for More Powerful Adversarial Invariant Augmentation using Contrastive Learning. MPAIACL leverages contrastive learning to unlock the full potential of vector representations by harnessing their intrinsic information. Through extensive experiments, MPAIACL demonstrates its robust generalization and effectiveness, as it performs well compared with other baselines across various public OOD datasets. The code is publicly available at https://github.com/flzeng1/MPAIACL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾æ•°æ®ä¸­å¸¸è§çš„åå˜é‡åˆ†å¸ƒåç§»(Covariate Distribution Shift)æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºæµ‹è¯•é›†åŒ…å«è®­ç»ƒé›†ä¸­æœªè§çš„ç»“æ„ç‰¹å¾ä¼šå¯¼è‡´ç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œ(GNNs)æ€§èƒ½ä¸‹é™ã€‚ç ”ç©¶è€…è§‚å¯Ÿåˆ°ç°æœ‰åº”å¯¹ç­–ç•¥æœªèƒ½å……åˆ†æŒ–æ˜æ½œåœ¨ç©ºé—´(Latent Space)çš„ä¸°å¯Œä¿¡æ¯ï¼Œå› æ­¤æå‡ºäº†åä¸ºMPAIACLçš„å¢å¼ºæ–¹æ³•ï¼Œå³åˆ©ç”¨å¯¹æ¯”å­¦ä¹ (Contrastive Learning)å®ç°æ›´å¼ºå¤§çš„å¯¹æŠ—ä¸å˜å¢å¼ºã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ æŠ€æœ¯æ•æ‰å‘é‡è¡¨ç¤ºä¸­çš„å†…åœ¨å…³è”ï¼Œæ—¨åœ¨æœ‰æ•ˆé‡Šæ”¾æ½œåœ¨ç©ºé—´çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨å¤šä¸ªå…¬å…±åˆ†å¸ƒå¤–(OOD)æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMPAIACLç›¸æ¯”ç°æœ‰åŸºçº¿æ¨¡å‹å…·æœ‰æ›´å¼ºçš„æ³›åŒ–æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ½œåœ¨ç©ºé—´çš„è¡¨ç¤ºå­¦ä¹ ï¼Œä¸ºå¤„ç†å¤æ‚å›¾ç»“æ„ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜æä¾›äº†ç¨³å¥ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 tables, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00716v1",
      "published_date": "2025-11-30 03:58:55 UTC",
      "updated_date": "2025-11-30 03:58:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:19:17.672657+00:00"
    },
    {
      "arxiv_id": "2512.00714v1",
      "title": "Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks",
      "title_zh": "åŸºäºå¤šæ¨¡æ€åŒ»å­¦æˆåƒä¸æ”¾å°„åŸºå› ç»„å­¦æ•´åˆæ¡†æ¶çš„ç™Œç—‡æ—©æœŸæ£€æµ‹æ·±åº¦å­¦ä¹ è®¡ç®—æœºè§†è§‰æ¨¡å‹",
      "authors": [
        "Emmanuella Avwerosuoghene Oghenekaro"
      ],
      "abstract": "Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨æ—©æœŸç™Œç—‡æ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å»¶è¿Ÿè¯Šæ–­å¯¼è‡´çš„ç”Ÿå­˜ç‡ä¸‹é™é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨ Convolutional Neural Networks (CNNs)ã€Transformers åŠæ··åˆæ³¨æ„åŠ›æ¶æ„ï¼Œæ¨¡å‹èƒ½å¤Ÿä» MRIã€CTã€PETã€ç»„ç»‡ç—…ç†å­¦åŠè¶…å£°ç­‰ multimodal imaging æ•°æ®ä¸­è‡ªåŠ¨æå–å¤æ‚çš„ç©ºé—´å’Œå½¢æ€ç‰¹å¾ã€‚è¿™äº›æ¨¡å‹é€šè¿‡è¯†åˆ«è‚‰çœ¼ä¸å¯è§çš„å¾®ç»†ç»„ç»‡å¼‚å¸¸å’Œè‚¿ç˜¤å¾®ç¯å¢ƒå˜åŒ–ï¼Œåœ¨æ£€æµ‹ç²¾åº¦ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„æ”¾å°„å­¦è¯„ä¼°ã€‚ç ”ç©¶é‡ç‚¹ä»‹ç»äº† radiogenomics æ•´åˆæ¡†æ¶ï¼Œå°†å®šé‡å½±åƒç‰¹å¾ä¸ genomicsã€transcriptomics åŠè¡¨è§‚é—ä¼ ç”Ÿç‰©æ ‡å¿—ç‰©ç›¸è¿æ¥ã€‚è¿™ç§ radiogenomic fusion æŠ€æœ¯å®ç°äº†åœ¨æ— éœ€æœ‰åˆ›æ´»æ£€çš„å‰æä¸‹ï¼Œé¢„æµ‹è‚¿ç˜¤åŸºå› å‹ã€å…ç–«ååº”ã€åˆ†å­äºšå‹åŠæ²»ç–—è€è¯æ€§ã€‚è¯¥é›†æˆæ¡†æ¶ä¸ºä¸ªæ€§åŒ–è‚¿ç˜¤å­¦å¼€åˆ›äº†æ–°èŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†ç™Œç—‡æ—©æœŸç­›æŸ¥ä¸ç²¾å‡†è¯Šæ–­çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00714v1",
      "published_date": "2025-11-30 03:28:48 UTC",
      "updated_date": "2025-11-30 03:28:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:19:10.768814+00:00"
    },
    {
      "arxiv_id": "2512.00713v2",
      "title": "Concept-Guided Backdoor Attack on Vision Language Models",
      "title_zh": "é¢å‘è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¦‚å¿µå¼•å¯¼åé—¨æ”»å‡»",
      "authors": [
        "Haoyu Shen",
        "Weimin Lyu",
        "Haotian Xu",
        "Tengfei Ma"
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing \"cat\" with \"dog\"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) çš„å®‰å…¨è„†å¼±æ€§ï¼Œé’ˆå¯¹ç°æœ‰åé—¨æ”»å‡»ä¾èµ–åƒç´ çº§è§¦å‘å™¨ä¸”æ˜“è¢«æ£€æµ‹çš„é—®é¢˜ï¼Œæå‡ºäº†æ¦‚å¿µå¼•å¯¼åé—¨æ”»å‡» (Concept-Guided Backdoor Attack) è¿™ä¸€å…¨æ–°èŒƒå¼ã€‚è¯¥èŒƒå¼ç›´æ¥åœ¨è¯­ä¹‰æ¦‚å¿µå±‚é¢è€ŒéåŸå§‹åƒç´ ä¸Šè¿›è¡Œæ“ä½œï¼Œå¹¶å…·ä½“è®¾è®¡äº† Concept-Thresholding Poisoning (CTP) å’Œ CBL-Guided Unseen Backdoor (CGUB) ä¸¤ç§æ”»å‡»æ–¹æ³•ã€‚CTP åˆ©ç”¨è‡ªç„¶å›¾åƒä¸­çš„æ˜¾å¼æ¦‚å¿µä½œä¸ºè§¦å‘å™¨ï¼Œä½¿æ¨¡å‹ä»…åœ¨ç‰¹å®šæ¦‚å¿µå‡ºç°æ—¶æ³¨å…¥æ¶æ„è¾“å‡ºï¼Œä»è€Œä¿æŒäº†æé«˜çš„éšè”½æ€§ã€‚CGUB åˆ™åœ¨è®­ç»ƒä¸­é€šè¿‡ Concept Bottleneck Model (CBM) å¹²é¢„å†…éƒ¨æ¿€æ´»ï¼Œå³ä¾¿åœ¨è®­ç»ƒæ•°æ®ä¸­ä»æœªå‡ºç°è¿‡ç‰¹å®šçš„æ¶æ„æ›¿æ¢è¡Œä¸ºï¼Œä¹Ÿèƒ½åœ¨æ¨ç†æ—¶å®ç°æ ‡ç­¾çš„ç³»ç»Ÿæ€§ç¯¡æ”¹ã€‚å®éªŒåœ¨å¤šç§ VLM æ¶æ„å’Œæ•°æ®é›†ä¸Šè¯æ˜äº†è¿™ä¸¤ç§æ–¹æ³•åœ¨ä¿æŒæ­£å¸¸ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå…·æœ‰æé«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¦‚å¿µçº§æ¼æ´å·²æˆä¸º VLMs é¢ä¸´çš„ä¸€ä¸ªå…³é”®ä¸”éšè”½çš„æ–°æ”»å‡»é¢ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00713v2",
      "published_date": "2025-11-30 03:24:23 UTC",
      "updated_date": "2025-12-05 13:17:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:18:25.469708+00:00"
    },
    {
      "arxiv_id": "2512.00709v1",
      "title": "When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF",
      "title_zh": "å½“äººç±»åå¥½åè½¬æ—¶ï¼šä¸€ç§é’ˆå¯¹ RLHF çš„å®ä¾‹ç›¸å…³é²æ£’æŸå¤±",
      "authors": [
        "Yifan Xu",
        "Xichen Ye",
        "Yifan Chen",
        "Qiaosheng Zhang"
      ],
      "abstract": "Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLM)å¯¹é½è¿‡ç¨‹ä¸­æ™®éå­˜åœ¨çš„åå¥½ç¿»è½¬(Preference flipping)ç°è±¡åŠå…¶å¯¼è‡´çš„æ•°æ®æ ‡æ³¨è´¨é‡ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFlipping-Aware Direct Preference Optimization (FA-DPO)çš„ç®—æ³•ã€‚è¯¥ç®—æ³•ä»äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)çš„è§’åº¦å‡ºå‘ï¼Œæ—¨åœ¨æå‡æ¨¡å‹å¯¹æ½œåœ¨ç¿»è½¬æ•°æ®å¯¹çš„é²æ£’æ€§ã€‚ç ”ç©¶é€šè¿‡å°†å†…åœ¨äººç±»æ„å›¾ä¸å¤–éƒ¨å› ç´ å¯¼è‡´çš„ç¿»è½¬æœºåˆ¶æ‹†åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹é˜¶æ®µï¼Œåœ¨Bradley-Terry (BT)æ¨¡å‹åŸºç¡€ä¸Šå¼•å…¥äº†å®ä¾‹ä¾èµ–(Instance-dependent)çš„ç¿»è½¬æ¦‚ç‡ã€‚é€šè¿‡åˆ©ç”¨ä¸æ ‡æ³¨ç›¸å…³çš„ç‰¹å¾ï¼ŒFA-DPOèƒ½å¤Ÿæœ‰æ•ˆæ•è·åˆ¤æ–­ä¸­çš„ä¸ç¡®å®šæ€§å¹¶å»ºæ¨¡åå¥½ç¿»è½¬æ¨¡å¼ã€‚æ­¤å¤–ï¼Œä½œè€…è®¾è®¡äº†ä¸€ç§ç®€å•é«˜æ•ˆä¸”ä¸åŸå§‹DPOç®—æ³•å…¼å®¹çš„è¿­ä»£ä¼˜åŒ–ç®—æ³•ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šç§å¤æ‚åå¥½ç¿»è½¬åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°å‡ºä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI-26-AIA",
      "pdf_url": "https://arxiv.org/pdf/2512.00709v1",
      "published_date": "2025-11-30 03:16:20 UTC",
      "updated_date": "2025-11-30 03:16:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:18:23.764761+00:00"
    },
    {
      "arxiv_id": "2512.02070v1",
      "title": "DPWMixer: Dual-Path Wavelet Mixer for Long-Term Time Series Forecasting",
      "title_zh": "DPWMixerï¼šé¢å‘é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹çš„åŒè·¯å¾„å°æ³¢æ··åˆå™¨",
      "authors": [
        "Li Qianyang",
        "Zhang Xingjun",
        "Wang Shaoxun",
        "Wei Jia"
      ],
      "abstract": "Long-term time series forecasting (LTSF) is a critical task in computational intelligence. While Transformer-based models effectively capture long-range dependencies, they often suffer from quadratic complexity and overfitting due to data sparsity. Conversely, efficient linear models struggle to depict complex non-linear local dynamics. Furthermore, existing multi-scale frameworks typically rely on average pooling, which acts as a non-ideal low-pass filter, leading to spectral aliasing and the irreversible loss of high-frequency transients. In response, this paper proposes DPWMixer, a computationally efficient Dual-Path architecture. The framework is built upon a Lossless Haar Wavelet Pyramid that replaces traditional pooling, utilizing orthogonal decomposition to explicitly disentangle trends and local fluctuations without information loss. To process these components, we design a Dual-Path Trend Mixer that integrates a global linear mapping for macro-trend anchoring and a flexible patch-based MLP-Mixer for micro-dynamic evolution. Finally, An adaptive multi-scale fusion module then integrates predictions from diverse scales, weighted by channel stationarity to optimize synthesis. Extensive experiments on eight public benchmarks demonstrate that our method achieves a consistent improvement over state-of-the-art baselines. The code is available at https://github.com/hit636/DPWMixer.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DPWMixerï¼Œä¸€ç§ç”¨äºé•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹(Long-Term Time Series Forecasting)çš„é«˜æ•ˆåŒè·¯å¾„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚éçº¿æ€§åŠ¨æ€ä»¥åŠå› ä¼ ç»Ÿæ± åŒ–å¯¼è‡´ä¿¡æ¯æŸå¤±ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯æ— æŸå“ˆå°”å°æ³¢é‡‘å­—å¡”(Lossless Haar Wavelet Pyramid)ï¼Œå®ƒé€šè¿‡æ­£äº¤åˆ†è§£æ˜¾å¼åˆ†ç¦»è¶‹åŠ¿å’Œå±€éƒ¨æ³¢åŠ¨ï¼Œé¿å…äº†ä¼ ç»Ÿå¹³å‡æ± åŒ–å¸¦æ¥çš„é¢‘è°±æ··å ã€‚ä¸ºäº†æœ‰æ•ˆå¤„ç†è¿™äº›æˆåˆ†ï¼Œç ”ç©¶è®¾è®¡äº†åŒè·¯å¾„è¶‹åŠ¿æ··åˆå™¨(Dual-Path Trend Mixer)ï¼Œç»“åˆå…¨å±€çº¿æ€§æ˜ å°„è¿›è¡Œå®è§‚è¶‹åŠ¿é”šå®šï¼Œå¹¶åˆ©ç”¨åˆ†å—MLP-Mixeræ•æ‰å¾®è§‚åŠ¨æ€æ¼”åŒ–ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å¼•å…¥äº†è‡ªé€‚åº”å¤šå°ºåº¦èåˆæ¨¡å—ï¼Œæ ¹æ®é€šé“å¹³ç¨³æ€§(channel stationarity)å¯¹ä¸åŒå°ºåº¦çš„é¢„æµ‹è¿›è¡ŒåŠ æƒé›†æˆã€‚åœ¨å…«ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDPWMixerç›¸è¾ƒäºç°æœ‰çš„SOTAåŸºå‡†æ¨¡å‹å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨æ•æ‰å¤æ‚æ—¶åºç‰¹å¾æ–¹é¢çš„é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02070v1",
      "published_date": "2025-11-30 03:12:50 UTC",
      "updated_date": "2025-11-30 03:12:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:18:37.165649+00:00"
    },
    {
      "arxiv_id": "2512.00706v1",
      "title": "Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation",
      "title_zh": "åˆ©ç”¨åŒç­–ç•¥æ•°æ®ä¼˜åŒ–å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä»¥æœ‰æ•ˆç¼“è§£å¹»è§‰",
      "authors": [
        "Chengzhi Yu",
        "Yifan Xu",
        "Yifan Chen",
        "Wenyi Zhang"
      ],
      "abstract": "Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)ä¸­çš„å¹»è§‰(hallucination)ç¼“è§£é—®é¢˜ï¼Œé€šè¿‡åˆ†ææ•°æ®ç”Ÿæˆè¿‡ç¨‹è¯å®äº†åŒç­–ç•¥æ•°æ®(on-policy data)çš„æ•ˆæœæ˜¾è‘—ä¼˜äºå¼‚ç­–ç•¥æ•°æ®(off-policy data)ã€‚é’ˆå¯¹ç°æœ‰æ ‡æ³¨æ–¹æ³•å¯èƒ½åœ¨è®­ç»ƒæ ·æœ¬ä¸­å¼•å…¥æ–°å¹»è§‰çš„ç“¶é¢ˆï¼Œä½œè€…æå‡ºè®­ç»ƒä¸€ä¸ªä¸“é—¨çš„å¹»è§‰åˆ†ç±»å™¨è¿›è¡ŒäºŒå€¼æ ‡æ³¨ï¼Œä»¥ç¡®ä¿åç»­å¯¹é½é˜¶æ®µèƒ½å¤Ÿä½¿ç”¨çº¯å‡€çš„é€‰å®šæ ·æœ¬ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§ç»“åˆåŠ¨æ€æ ·æœ¬é‡åŠ æƒæ–¹æ¡ˆçš„ç¨³å¥è¿­ä»£ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ç®—æ³•ï¼Œä»¥å……åˆ†å‘æŒ¥åŒç­–ç•¥æ•°æ®çš„æ•ˆèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MMHalBenchå’ŒObject HalBenchåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†LLaVA-1.5-7Bçš„å¹»è§‰ç‡ï¼Œå¹¶ä½¿LLaVA-1.5-13Bçš„æ€§èƒ½è¶…è¶Šäº†GPT-4Vã€‚è¿™ä¸€æˆæœè¯æ˜äº†é«˜æ•ˆå¯é çš„åŒç­–ç•¥åå¥½æ ‡æ³¨åŠä¼˜åŒ–ç®—æ³•åœ¨æå‡å¤šæ¨¡æ€æ¨¡å‹å¯é æ€§ä¸çœŸå®æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00706v1",
      "published_date": "2025-11-30 02:55:20 UTC",
      "updated_date": "2025-11-30 02:55:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:18:43.178954+00:00"
    },
    {
      "arxiv_id": "2512.10966v1",
      "title": "Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis",
      "title_zh": "é¢å‘å¯è§£é‡Šæ€§é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­çš„å¤§è„‘åŒºåŸŸä¸“å®¶å¤šæ¨¡æ€èåˆ",
      "authors": [
        "Farica Zhuang",
        "Dinara Aliyeva",
        "Shu Yang",
        "Zixuan Wen",
        "Duy Duong-Tran",
        "Christos Davatzikos",
        "Tianlong Chen",
        "Song Wang",
        "Li Shen"
      ],
      "abstract": "Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MREF-ADï¼Œä¸€ç§ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…(AD)è¯Šæ–­çš„å¤šæ¨¡æ€åŒºåŸŸä¸“å®¶èåˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿèåˆæ–¹æ³•éš¾ä»¥è‡ªé€‚åº”å¹³è¡¡ä¸åŒè„‘åŒºæ·€ç²‰æ ·è›‹ç™½PETå’ŒMRIç”Ÿç‰©æ ‡å¿—ç‰©è´¡çŒ®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆä¸“å®¶(Mixture-of-Experts, MoE)æ¶æ„ï¼Œå°†æ¯ç§æ¨¡æ€ä¸­çš„ä¸­å°ºåº¦è„‘åŒºå»ºæ¨¡ä¸ºç‹¬ç«‹ä¸“å®¶ï¼Œå¹¶åˆ©ç”¨ä¸¤çº§é—¨æ§ç½‘ç»œå­¦ä¹ å—è¯•è€…ç‰¹å®šçš„èåˆæƒé‡ã€‚MREF-ADä¸ä»…æå‡äº†è¯Šæ–­æ€§èƒ½ï¼Œè¿˜ä¸ºç»“æ„å’Œåˆ†å­æˆåƒå¦‚ä½•å…±åŒè´¡çŒ®äºç–¾ç—…è¯Šæ–­æä¾›äº†æ¨¡æ€å’ŒåŒºåŸŸå±‚é¢çš„æ·±åº¦è§è§£ã€‚åŸºäºADNIæ•°æ®é›†çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMREF-ADåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³(SOTA)ï¼ŒåŒæ—¶æ˜¾è‘—å¢å¼ºäº†è„‘åŒºç‰¹å®šç”Ÿç‰©æ ‡å¿—ç‰©ç›¸å…³æ€§çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¨¡å‹ä¸ºç¥ç»å½±åƒå­¦ä¸­è‡ªé€‚åº”ä¸”å¯è§£é‡Šçš„å¤šæ¨¡æ€èåˆæä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç ”ç©¶æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10966v1",
      "published_date": "2025-11-30 02:12:12 UTC",
      "updated_date": "2025-11-30 02:12:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:18:46.068217+00:00"
    },
    {
      "arxiv_id": "2512.00696v3",
      "title": "Hierarchical Molecular Language Models (HMLMs)",
      "title_zh": "å±‚çº§åˆ†å­è¯­è¨€æ¨¡å‹ (HMLMs)",
      "authors": [
        "Hasi Hays",
        "Yue Yu",
        "William J. Richardson"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping computational and network biology by enabling new approaches to decode cellular communication networks. We introduce Hierarchical Molecular Language Models (HMLMs), a novel framework that models cellular signaling as a specialized molecular language, where signaling molecules function as tokens, protein interactions define syntax, and functional consequences constitute semantics. HMLMs employ a transformer-based architecture adapted to accommodate graph-structured signaling networks through information transducers, mathematical entities that capture how molecules receive, process, and transmit signals. The architecture integrates multi-modal data sources across molecular, pathway, and cellular scales through hierarchical attention mechanisms and scale-bridging operators that enable information flow across biological hierarchies. Applied to a complex network of cardiac fibroblast signaling, HMLMs outperformed traditional approaches in temporal dynamics prediction, particularly under sparse sampling conditions. Attention-based analysis revealed biologically meaningful crosstalk patterns, including previously uncharacterized interactions between signaling pathways. By bridging molecular mechanisms with cellular phenotypes through AI-driven molecular language representation, HMLMs establish a foundation for biology-oriented large language models (LLMs) that could be pre-trained on comprehensive pathway datasets and applied across diverse signaling systems and tissues, advancing precision medicine and therapeutic discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åˆ†å±‚åˆ†å­è¯­è¨€æ¨¡å‹(Hierarchical Molecular Language Models, HMLMs)ï¼Œè¿™æ˜¯ä¸€ç§å°†ç»†èƒä¿¡å·è½¬å¯¼å»ºæ¨¡ä¸ºä¸“é—¨åˆ†å­è¯­è¨€çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä¿¡å·åˆ†å­è§†ä¸ºTokenï¼Œè›‹ç™½è´¨ç›¸äº’ä½œç”¨å®šä¹‰ä¸ºè¯­æ³•ï¼ŒåŠŸèƒ½ç»“æœæ„æˆè¯­ä¹‰ï¼Œæ—¨åœ¨é€šè¿‡ AI æ‰‹æ®µè§£ç å¤æ‚çš„ç»†èƒé€šä¿¡ç½‘ç»œã€‚HMLMs é‡‡ç”¨äº†åŸºäº Transformer çš„æ¶æ„ï¼Œé€šè¿‡ä¿¡æ¯è½¬æ¢å™¨(information transducers)å¤„ç†å›¾ç»“æ„çš„ä¿¡å·ç½‘ç»œï¼Œå¹¶åˆ©ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶æ•´åˆè·¨å°ºåº¦çš„å¤šæ¨¡æ€æ•°æ®ã€‚åœ¨å¿ƒè„æˆçº¤ç»´ç»†èƒä¿¡å·ç½‘ç»œçš„å®éªŒä¸­ï¼ŒHMLMs åœ¨æ—¶é—´åŠ¨æ€é¢„æµ‹æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨ç¨€ç–é‡‡æ ·æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åˆ†æï¼Œè¯¥æ¨¡å‹æ­ç¤ºäº†å…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„ Crosstalk æ¨¡å¼åŠæœªè¡¨å¾çš„é€šè·¯é—´ç›¸äº’ä½œç”¨ã€‚è¿™ä¸€æˆæœé€šè¿‡ AI é©±åŠ¨çš„åˆ†å­è¯­è¨€è¡¨å¾æ¶èµ·äº†åˆ†å­æœºåˆ¶ä¸ç»†èƒè¡¨å‹çš„æ¡¥æ¢ï¼Œä¸ºæ„å»ºé¢å‘ç²¾å‡†åŒ»ç–—å’Œè¯ç‰©å‘ç°çš„ç”Ÿç‰©å¯¼å‘å¤§è¯­è¨€æ¨¡å‹(LLMs)å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "q-bio.MN",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "q-bio.MN",
      "comment": "The current version includes minor revisions to the preprint v2 (arXiv preprint arXiv:2512.00696), Added the Supplementary materials section",
      "pdf_url": "https://arxiv.org/pdf/2512.00696v3",
      "published_date": "2025-11-30 02:09:27 UTC",
      "updated_date": "2025-12-12 22:51:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:19:18.968595+00:00"
    },
    {
      "arxiv_id": "2512.00683v1",
      "title": "Model of human cognition",
      "title_zh": "äººç±»è®¤çŸ¥æ¨¡å‹",
      "authors": [
        "Wu Yonggang"
      ],
      "abstract": "The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨å¯è§£é‡Šæ€§ (explainability)ã€ç»Ÿä¸€ç†è®ºç¼ºå¤±ä»¥åŠé«˜æ˜‚è¿è¡Œæˆæœ¬æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªå…³äºç³»ç»Ÿæ™ºèƒ½æ¶Œç°çš„ç¥ç»ç†è®ºæ¡†æ¶ (neuro-theoretical framework)ã€‚è¯¥æ¨¡å‹åœ¨åŠŸèƒ½ä¸Šå…·æœ‰ç¨³å¥æ€§ï¼Œä¸”åœ¨ç”Ÿç‰©å­¦ä¸Šå…·æœ‰åˆç†æ€§ (biologically plausible)ï¼Œä¸ºå†³ç­– (decision-making) å’Œé—®é¢˜è§£å†³ (problem solving) ç­‰æ ¸å¿ƒè®¤çŸ¥è¿‡ç¨‹æä¾›äº†æ·±åˆ»çš„ç†è®ºæ´è§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨å¼€å‘å…·å¤‡å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ› (generalizable) çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚è¿™ä¸€æˆæœä¸ºè¿æ¥ç¥ç»ç§‘å­¦ä¸äººå·¥æ™ºèƒ½ã€å®ç°æ›´é«˜çº§åˆ«çš„é€šç”¨æ™ºèƒ½æä¾›äº†æ–°çš„ç†è®ºè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00683v1",
      "published_date": "2025-11-30 00:57:32 UTC",
      "updated_date": "2025-11-30 00:57:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:19:18.360513+00:00"
    },
    {
      "arxiv_id": "2512.00677v1",
      "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer",
      "title_zh": "Dynamic-eDiTorï¼šåŸºäºå¤šæ¨¡æ€æ‰©æ•£ Transformer çš„å…è®­ç»ƒæ–‡æœ¬é©±åŠ¨ 4D åœºæ™¯ç¼–è¾‘",
      "authors": [
        "Dong In Lee",
        "Hyungjun Doh",
        "Seunggeun Chi",
        "Runlin Duan",
        "Sangpil Kim",
        "Karthik Ramani"
      ],
      "abstract": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Dynamic-eDiTorï¼Œä¸€ç§æ— éœ€è®­ç»ƒï¼ˆTraining-Freeï¼‰çš„æ–‡æœ¬é©±åŠ¨ 4D åœºæ™¯ç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­å¤šè§†å›¾ä¸æ—¶é—´ä¸€è‡´æ€§éš¾ä»¥å…¼é¡¾çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº† Multimodal Diffusion Transformer (MM-DiT) ä¸ 4D Gaussian Splatting (4DGS) æŠ€æœ¯ï¼Œå…‹æœäº†ç°æœ‰ 2D æ‰©æ•£æ¨¡å‹å› ç‹¬ç«‹ç¼–è¾‘å„å¸§è€Œå¯¼è‡´çš„è¿åŠ¨å¤±çœŸã€å‡ ä½•æ¼‚ç§»å’Œç¼–è¾‘ä¸å®Œæ•´ç­‰ç¼ºé™·ã€‚å…¶æ ¸å¿ƒæœºåˆ¶åŒ…æ‹¬ç”¨äºå®ç°å±€éƒ¨ä¸€è‡´çš„è·¨è§†å›¾ä¸æ—¶é—´èåˆçš„ Spatio-Temporal Sub-Grid Attention (STGA)ï¼Œä»¥åŠé€šè¿‡ä»¤ç‰Œç»§æ‰¿å’Œå…‰æµå¼•å¯¼å®ç°å…¨å±€ä¿¡æ¯ä¼ æ’­çš„ Context Token Propagation (CTP)ã€‚é€šè¿‡è¿™äº›ç»„ä»¶çš„ååŒä½œç”¨ï¼ŒDynamic-eDiTor èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ç”Ÿæˆæ— ç¼ã€å…¨å±€ä¸€è‡´çš„å¤šè§†å›¾è§†é¢‘ï¼Œå¹¶èƒ½ç›´æ¥ä¼˜åŒ–é¢„è®­ç»ƒçš„æº 4DGS æ¨¡å‹ã€‚åœ¨å¤šè§†å›¾è§†é¢‘æ•°æ®é›† DyNeRF ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘ä¿çœŸåº¦ä»¥åŠæ—¶ç©ºä¸€è‡´æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "4D Scene Editing",
      "pdf_url": "https://arxiv.org/pdf/2512.00677v1",
      "published_date": "2025-11-30 00:18:46 UTC",
      "updated_date": "2025-11-30 00:18:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:19:42.959194+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 91,
  "processed_papers_count": 91,
  "failed_papers_count": 0,
  "llm_backup_calls": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T12:20:44.883857+00:00"
}
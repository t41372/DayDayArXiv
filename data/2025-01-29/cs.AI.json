{
  "date": "2025-01-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-29 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 安全、LLM（Large Language Models）优化与应用、医疗 AI 以及强化学习等领域，其中 Yoshua Bengio 等知名学者主导的“International AI Safety Report”最为引人注目，同时 LLM 探索能力和多模态模型的鲁棒性改进也值得关注。这些论文展示了 AI 在实际部署中的挑战与潜力，强调了模型的泛化、解释性和安全性的重要性。\n\n下面，我挑选并简要讨论几篇关键论文，先从高影响力或话题度高的入手，再快速掠过其他相关或次要内容。限于篇幅，我会聚焦于核心贡献，避免冗长描述。\n\n### 1. **国际 AI 安全报告 (International AI Safety Report)**\n   - **主要贡献和发现**：这篇由 Yoshua Bengio 等 100 多位专家合著的报告，综合分析了高级 AI 系统的能力、风险和安全措施，强调了 AI 扩展的生态和社会后果。该报告是 AI 安全领域的一次里程碑式总结，提出通过系统动力学建模来预测 AI 增长的极限，并呼吁重新调整 AI 开发优先级以避免“过度增长和崩溃”轨迹。\n   - **影响**：作为 AI Safety Summit 的产物，这篇论文可能推动全球 AI 治理政策。\n\n### 2. **大型语言模型思考太快以至于无法有效探索 (Large Language Models Think Too Fast To Explore Effectively)**\n   - **主要贡献和发现**：论文揭示了 LLM 在开放任务中的探索局限性，使用 Little Alchemy 2 作为范例，比较 LLM 和人类的探索策略。发现传统 LLM（如 GPT-4o）依赖不确定性驱动但决策过快，而 DeepSeek 模型通过更长的迭代推理表现出更像人类的探索。使用 Sparse Autoencoders 分析显示，LLM 的不确定性和赋权在不同层处理，导致过早决策。该研究为提升 LLM 的适应性提供了新方向。\n   - **影响**：这篇论文挑战了 LLM 的智能边界，相关于 Geoffrey Hinton 的观点，可能激发更多探索机制优化。\n\n### 3. **SAeUron: 扩散模型中可解释概念删除 (SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders)**\n   - **主要贡献和发现**：提出 SAeUron 框架，使用 Sparse Autoencoders 在文本到图像扩散模型中捕获并删除有害概念，实现精确干预而不影响整体性能。在 UnlearnCanvas 基准上，SAeUron 表现出 state-of-the-art 的性能，并能同时删除多个概念，同时抵抗对抗攻击。该方法提升了 AI 生成内容的伦理安全。\n   - **影响**：作为 AI 安全相关论文，与第1篇主题相近，强调模型可解释性和鲁棒性。\n\n### 4. **对话优于独白：通过策略对话指导医疗 LLM (Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations)**\n   - **主要贡献和发现**：针对医疗 AI 的局限，引入对话式微调框架，将静态数据集转化为对话格式，提高模型在多轮推理和噪声环境中的性能。在新基准上，对话调优模型比传统方法提升9.64%的推理准确率和6.18%的噪声鲁棒性。该研究推动了临床AI的可靠性和适应性。\n   - **影响**：医疗AI领域的重要进展，与第14篇（医疗LLM）相关，展示了对话机制在实际应用中的潜力。\n\n### 5. **学习最优停止：通过顺序概率比率测试在有限视野内进行早期分类 (Learning the Optimal Stopping for Early Classification within Finite Horizons via Sequential Probability Ratio Test)**\n   - **主要贡献和发现**：引入 FIRMBOUND 框架，使用密度比估计和凸函数学习优化序列分类的停止规则，在有限输入场景下最小化 Bayes 风险。实验显示，该方法在各种数据集上实现了最优权衡，并在 ICLR 2025 上被接受。\n   - **影响**：强化了机器学习的时间敏感应用，相关于高效AI决策。\n\n其他论文中，如第12篇“Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization”，探讨了 LLM 在数学任务中的推理优化，通过 Inference Budget-Constrained Policy Optimization 提升效率；第15篇“Digital Twin-Enabled Real-Time Control in Robotic Additive Manufacturing via Soft Actor-Critic Reinforcement Learning”，则在机器人制造中应用强化学习，实现实时控制。这些虽有价值，但相对次要，我仅快速提及以控制篇幅。\n\n总之，今天的 arXiv 论文突出了 AI 的安全与泛化挑战，Yoshua Bengio 的报告是亮点，建议读者关注 LLM 和医疗AI方向的创新。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2501.18059v1",
      "title": "Learning the Optimal Stopping for Early Classification within Finite Horizons via Sequential Probability Ratio Test",
      "title_zh": "通过顺序概率比检验学习有限时间范围内早期分类的最优停止策略",
      "authors": [
        "Akinori F. Ebihara",
        "Taiki Miyagawa",
        "Kazuyuki Sakurai",
        "Hitoshi Imaoka"
      ],
      "abstract": "Time-sensitive machine learning benefits from Sequential Probability Ratio\nTest (SPRT), which provides an optimal stopping time for early classification\nof time series. However, in finite horizon scenarios, where input lengths are\nfinite, determining the optimal stopping rule becomes computationally intensive\ndue to the need for backward induction, limiting practical applicability. We\nthus introduce FIRMBOUND, an SPRT-based framework that efficiently estimates\nthe solution to backward induction from training data, bridging the gap between\noptimal stopping theory and real-world deployment. It employs density ratio\nestimation and convex function learning to provide statistically consistent\nestimators for sufficient statistic and conditional expectation, both essential\nfor solving backward induction; consequently, FIRMBOUND minimizes Bayes risk to\nreach optimality. Additionally, we present a faster alternative using Gaussian\nprocess regression, which significantly reduces training time while retaining\nlow deployment overhead, albeit with potential compromise in statistical\nconsistency. Experiments across independent and identically distributed\n(i.i.d.), non-i.i.d., binary, multiclass, synthetic, and real-world datasets\nshow that FIRMBOUND achieves optimalities in the sense of Bayes risk and\nspeed-accuracy tradeoff. Furthermore, it advances the tradeoff boundary toward\noptimality when possible and reduces decision-time variance, ensuring reliable\ndecision-making. Code is publicly available at\nhttps://github.com/Akinori-F-Ebihara/FIRMBOUND",
      "tldr_zh": "这篇论文针对有限地平线场景下时间序列的早期分类，提出使用 Sequential Probability Ratio Test (SPRT) 来学习最优停止规则，以解决传统后向归纳计算密集的问题。作者引入 FIRMBOUND 框架，通过密度比估计（density ratio estimation）和凸函数学习（convex function learning）从训练数据中高效估计充分统计量和条件期望，从而最小化 Bayes risk 并实现最优性。该框架还提供了一个基于 Gaussian process regression 的更快替代方案，以减少训练时间，尽管可能牺牲部分统计一致性。实验在 i.i.d.、non-i.i.d.、二元、多类数据集上验证了 FIRMBOUND 的优越性，提高了速度-准确性权衡并降低了决策时间方差。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to International Conference on Learning Representations\n  (ICLR) 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.18059v1",
      "published_date": "2025-01-29 23:54:46 UTC",
      "updated_date": "2025-01-29 23:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:21:51.701719"
    },
    {
      "arxiv_id": "2501.18055v2",
      "title": "Current Pathology Foundation Models are unrobust to Medical Center Differences",
      "title_zh": "翻译失败",
      "authors": [
        "Edwin D. de Jong",
        "Eric Marcus",
        "Jonas Teuwen"
      ],
      "abstract": "Pathology Foundation Models (FMs) hold great promise for healthcare. Before\nthey can be used in clinical practice, it is essential to ensure they are\nrobust to variations between medical centers. We measure whether pathology FMs\nfocus on biological features like tissue and cancer type, or on the well known\nconfounding medical center signatures introduced by staining procedure and\nother differences. We introduce the Robustness Index. This novel robustness\nmetric reflects to what degree biological features dominate confounding\nfeatures. Ten current publicly available pathology FMs are evaluated. We find\nthat all current pathology foundation models evaluated represent the medical\ncenter to a strong degree. Significant differences in the robustness index are\nobserved. Only one model so far has a robustness index greater than one,\nmeaning biological features dominate confounding features, but only slightly. A\nquantitative approach to measure the influence of medical center differences on\nFM-based prediction performance is described. We analyze the impact of\nunrobustness on classification performance of downstream models, and find that\ncancer-type classification errors are not random, but specifically attributable\nto same-center confounders: images of other classes from the same medical\ncenter. We visualize FM embedding spaces, and find these are more strongly\norganized by medical centers than by biological factors. As a consequence, the\nmedical center of origin is predicted more accurately than the tissue source\nand cancer type. The robustness index introduced here is provided with the aim\nof advancing progress towards clinical adoption of robust and reliable\npathology FMs.",
      "tldr_zh": "本研究发现，现有的Pathology Foundation Models在面对医疗中心差异（如染色程序）时不鲁棒，常优先捕捉医疗中心特征而非生物特征（如组织和癌症类型）。研究者引入了Robustness Index这一新指标，用于量化生物特征相对于混淆特征的支配程度，并评估了10个公开模型，结果显示所有模型均强烈代表医疗中心，且仅有少数模型的Robustness Index大于1。进一步分析表明，这种不鲁棒性导致下游分类性能下降，错误主要归因于同中心的混淆因素，且FM嵌入空间更以医疗中心为基础组织。总体而言，该工作为提升Pathology Foundation Models的鲁棒性和可靠性提供了量化方法，推动其临床应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18055v2",
      "published_date": "2025-01-29 23:38:14 UTC",
      "updated_date": "2025-02-01 09:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:22:03.328754"
    },
    {
      "arxiv_id": "2501.18052v2",
      "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Bartosz Cywiński",
        "Kamil Deja"
      ],
      "abstract": "Diffusion models, while powerful, can inadvertently generate harmful or\nundesirable content, raising significant ethical and safety concerns. Recent\nmachine unlearning approaches offer potential solutions but often lack\ntransparency, making it difficult to understand the changes they introduce to\nthe base model. In this work, we introduce SAeUron, a novel method leveraging\nfeatures learned by sparse autoencoders (SAEs) to remove unwanted concepts in\ntext-to-image diffusion models. First, we demonstrate that SAEs, trained in an\nunsupervised manner on activations from multiple denoising timesteps of the\ndiffusion model, capture sparse and interpretable features corresponding to\nspecific concepts. Building on this, we propose a feature selection method that\nenables precise interventions on model activations to block targeted content\nwhile preserving overall performance. Evaluation with the competitive\nUnlearnCanvas benchmark on object and style unlearning highlights SAeUron's\nstate-of-the-art performance. Moreover, we show that with a single SAE, we can\nremove multiple concepts simultaneously and that in contrast to other methods,\nSAeUron mitigates the possibility of generating unwanted content, even under\nadversarial attack. Code and checkpoints are available at:\nhttps://github.com/cywinski/SAeUron.",
      "tldr_zh": "本文提出 SAeUron，一种基于 Sparse Autoencoders (SAEs) 的方法，用于在 Diffusion Models 中实现可解释的概念 unlearning，以移除有害或不期望内容，同时提升模型透明度。SAEs 通过无监督训练在多个 denoising timesteps 上捕获稀疏且可解释的特征，并采用特征选择技术对模型激活进行精确干预，阻挡目标内容而保持整体性能。在 UnlearnCanvas benchmark 测试中，SAeUron 展现出 state-of-the-art 性能，能同时移除多个概念，并在对抗攻击下有效防止生成 unwanted content。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18052v2",
      "published_date": "2025-01-29 23:29:47 UTC",
      "updated_date": "2025-01-31 18:39:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:22:14.463739"
    },
    {
      "arxiv_id": "2501.18045v2",
      "title": "From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors",
      "title_zh": "从工具到窃贼：通过众包比喻测量和理解公众对人工智能的感知",
      "authors": [
        "Myra Cheng",
        "Angela Y. Lee",
        "Kristina Rapuano",
        "Kate Niederhoffer",
        "Alex Liebscher",
        "Jeffrey Hancock"
      ],
      "abstract": "How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development.",
      "tldr_zh": "这篇论文通过收集超过12,000个来自美国全国代表性样本的开放式比喻响应，调查公众对AI的感知，克服了传统自报量表的局限性。研究采用混合方法，包括定量聚类和定性编码，识别出20个主导比喻，并使用语言建模(LM-based techniques)框架来量化anthropomorphism、warmth和competence维度，发现美国人对AI的拟人化和温暖感知在过去一年显著增加（+34%、+41%）。这些隐性感知强烈预测AI的信任和采用意愿（r² = 0.21、0.18），并揭示了系统的人口统计学差异，如女性、老年人和有色人种更倾向于anthropomorphize AI，从而为包容性和负责任的AI开发提供了行动洞见。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.18045v2",
      "published_date": "2025-01-29 23:17:43 UTC",
      "updated_date": "2025-04-29 05:20:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:22:28.034792"
    },
    {
      "arxiv_id": "2501.18016v1",
      "title": "Digital Twin-Enabled Real-Time Control in Robotic Additive Manufacturing via Soft Actor-Critic Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Matsive Ali",
        "Sandesh Giri",
        "Sen Liu",
        "Qin Yang"
      ],
      "abstract": "Smart manufacturing systems increasingly rely on adaptive control mechanisms\nto optimize complex processes. This research presents a novel approach\nintegrating Soft Actor-Critic (SAC) reinforcement learning with digital twin\ntechnology to enable real-time process control in robotic additive\nmanufacturing. We demonstrate our methodology using a Viper X300s robot arm,\nimplementing two distinct control scenarios: static target acquisition and\ndynamic trajectory following. The system architecture combines Unity's\nsimulation environment with ROS2 for seamless digital twin synchronization,\nwhile leveraging transfer learning to efficiently adapt trained models across\ntasks. Our hierarchical reward structure addresses common reinforcement\nlearning challenges including local minima avoidance, convergence acceleration,\nand training stability. Experimental results show rapid policy convergence and\nrobust task execution in both simulated and physical environments, with\nperformance metrics including cumulative reward, value prediction accuracy,\npolicy loss, and discrete entropy coefficient demonstrating the effectiveness\nof our approach. This work advances the integration of reinforcement learning\nwith digital twins for industrial robotics applications, providing a framework\nfor enhanced adaptive real-time control for smart additive manufacturing\nprocess.",
      "tldr_zh": "该研究提出了一种整合 Soft Actor-Critic (SAC) 强化学习与数字孪生技术的创新方法，用于机器人增材制造中的实时过程控制，针对 Viper X300s 机器人臂实现了静态目标获取和动态轨迹跟随场景。系统架构结合 Unity 模拟环境和 ROS2 进行数字孪生同步，并通过迁移学习和分层奖励结构解决强化学习中的局部最小值避免、收敛加速及训练稳定性问题。实验结果显示，该方法在模拟和物理环境中实现了快速策略收敛和稳健任务执行，性能指标如累积奖励和价值预测准确性均显著提升，为智能增材制造提供了一个增强的自适应实时控制框架。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18016v1",
      "published_date": "2025-01-29 22:06:53 UTC",
      "updated_date": "2025-01-29 22:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:22:38.566979"
    },
    {
      "arxiv_id": "2501.18011v2",
      "title": "Anatomy Might Be All You Need: Forecasting What to Do During Surgery",
      "title_zh": "解剖可能就是你所需的一切：手术期间的操作预测",
      "authors": [
        "Gary Sarwin",
        "Alessandro Carretta",
        "Victor Staartjes",
        "Matteo Zoli",
        "Diego Mazzatenta",
        "Luca Regli",
        "Carlo Serra",
        "Ender Konukoglu"
      ],
      "abstract": "Surgical guidance can be delivered in various ways. In neurosurgery, spatial\nguidance and orientation are predominantly achieved through neuronavigation\nsystems that reference pre-operative MRI scans. Recently, there has been\ngrowing interest in providing live guidance by analyzing video feeds from tools\nsuch as endoscopes. Existing approaches, including anatomy detection,\norientation feedback, phase recognition, and visual question-answering,\nprimarily focus on aiding surgeons in assessing the current surgical scene.\nThis work aims to provide guidance on a finer scale, aiming to provide guidance\nby forecasting the trajectory of the surgical instrument, essentially\naddressing the question of what to do next. To address this task, we propose a\nmodel that not only leverages the historical locations of surgical instruments\nbut also integrates anatomical features. Importantly, our work does not rely on\nexplicit ground truth labels for instrument trajectories. Instead, the ground\ntruth is generated by a detection model trained to detect both anatomical\nstructures and instruments within surgical videos of a comprehensive dataset\ncontaining pituitary surgery videos. By analyzing the interaction between\nanatomy and instrument movements in these videos and forecasting future\ninstrument movements, we show that anatomical features are a valuable asset in\naddressing this challenging task. To the best of our knowledge, this work is\nthe first attempt to address this task for manually operated surgeries.",
      "tldr_zh": "本研究针对手术指导的局限性，提出一种新方法，通过预测手术器械的轨迹来提供更精细的实时指导，旨在回答“下一步该做什么”。该模型整合手术器械的历史位置和解剖特征，而非依赖显式轨迹标签，而是使用训练好的检测模型从包含垂体手术视频的综合数据集生成地面真实标签，以检测解剖结构和器械。实验结果显示，解剖特征在分析器械与解剖互动并预测未来运动方面具有重要价值，这是首次针对手动手术的尝试。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18011v2",
      "published_date": "2025-01-29 21:54:31 UTC",
      "updated_date": "2025-01-31 17:07:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:24:43.656843"
    },
    {
      "arxiv_id": "2501.18009v2",
      "title": "Large Language Models Think Too Fast To Explore Effectively",
      "title_zh": "大语言模型思考得太快以致无法有效探索",
      "authors": [
        "Lan Pan",
        "Hanbo Xie",
        "Robert C. Wilson"
      ],
      "abstract": "Large Language Models (LLMs) have emerged with many intellectual capacities.\nWhile numerous benchmarks assess their intelligence, limited attention has been\ngiven to their ability to explore--an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with\ntraditional LLMs relying primarily on uncertainty-driven strategies, unlike\nhumans who balance uncertainty and empowerment. Results indicate that\ntraditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly\nfaster and less detailed reasoning process, limiting their exploratory\nperformance. In contrast, the DeepSeek reasoning model demonstrates prolonged,\niterative thought processes marked by repetitive analysis of combinations and\npast trials, reflecting a more thorough and human-like exploration strategy.\nRepresentational analysis of the models with Sparse Autoencoders (SAE) revealed\nthat uncertainty and choices are represented at earlier transformer blocks,\nwhile empowerment values are processed later, causing LLMs to think too fast\nand make premature decisions, hindering effective exploration. These findings\nshed light on the limitations of LLM exploration and suggest directions for\nimproving their adaptability.",
      "tldr_zh": "这篇论文评估了 Large Language Models (LLMs) 在开放任务中的探索能力，发现大多数 LLMs 表现不如人类，因为它们依赖不确定性驱动策略且推理过程过快，导致探索效率低下。研究以 Little Alchemy 2 游戏为测试范例，让模型结合元素发现新元素，结果显示只有 o1 模型能与人类匹敌，而传统 LLMs 如 GPT-4o 倾向于快速决策，忽略赋权 (empowerment) 的平衡。利用 Sparse Autoencoders (SAE) 进行表征分析，揭示不确定性和选择在 transformer 早期块处理，而赋权值在后期，导致 LLMs 过早决策。论文为提升 LLMs 的适应性和探索策略提供了改进建议。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 16 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2501.18009v2",
      "published_date": "2025-01-29 21:51:17 UTC",
      "updated_date": "2025-05-12 16:02:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:24:55.176607"
    },
    {
      "arxiv_id": "2501.18006v1",
      "title": "Topological Signatures of Adversaries in Multimodal Alignments",
      "title_zh": "翻译失败",
      "authors": [
        "Minh Vu",
        "Geigh Zollicoffer",
        "Huy Mai",
        "Ben Nebgen",
        "Boian Alexandrov",
        "Manish Bhattarai"
      ],
      "abstract": "Multimodal Machine Learning systems, particularly those aligning text and\nimage data like CLIP/BLIP models, have become increasingly prevalent, yet\nremain susceptible to adversarial attacks. While substantial research has\naddressed adversarial robustness in unimodal contexts, defense strategies for\nmultimodal systems are underexplored. This work investigates the topological\nsignatures that arise between image and text embeddings and shows how\nadversarial attacks disrupt their alignment, introducing distinctive\nsignatures. We specifically leverage persistent homology and introduce two\nnovel Topological-Contrastive losses based on Total Persistence and Multi-scale\nkernel methods to analyze the topological signatures introduced by adversarial\nperturbations. We observe a pattern of monotonic changes in the proposed\ntopological losses emerging in a wide range of attacks on image-text\nalignments, as more adversarial samples are introduced in the data. By\ndesigning an algorithm to back-propagate these signatures to input samples, we\nare able to integrate these signatures into Maximum Mean Discrepancy tests,\ncreating a novel class of tests that leverage topological signatures for better\nadversarial detection.",
      "tldr_zh": "本文研究多模态机器学习系统（如CLIP和BLIP模型）在图像-文本对齐中的对抗攻击问题，揭示了攻击如何破坏嵌入间的拓扑签名。作者引入persistent homology和两个新颖的Topological-Contrastive losses（基于Total Persistence和Multi-scale kernel方法），用于分析对抗扰动带来的单调变化模式。实验观察到，随着对抗样本增加，拓扑损失呈单调变化，并通过算法将这些签名整合到Maximum Mean Discrepancy测试中，开发出更有效的对抗检测方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18006v1",
      "published_date": "2025-01-29 21:45:10 UTC",
      "updated_date": "2025-01-29 21:45:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:25:07.801852"
    },
    {
      "arxiv_id": "2501.17991v1",
      "title": "Investigating the Monte-Carlo Tree Search Approach for the Job Shop Scheduling Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Laurie Boveroux",
        "Damien Ernst",
        "Quentin Louveaux"
      ],
      "abstract": "The Job Shop Scheduling Problem (JSSP) is a well-known optimization problem\nin manufacturing, where the goal is to determine the optimal sequence of jobs\nacross different machines to minimize a given objective. In this work, we focus\non minimising the weighted sum of job completion times. We explore the\npotential of Monte Carlo Tree Search (MCTS), a heuristic-based reinforcement\nlearning technique, to solve large-scale JSSPs, especially those with\nrecirculation. We propose several Markov Decision Process (MDP) formulations to\nmodel the JSSP for the MCTS algorithm. In addition, we introduce a new\nsynthetic benchmark derived from real manufacturing data, which captures the\ncomplexity of large, non-rectangular instances often encountered in practice.\nOur experimental results show that MCTS effectively produces good-quality\nsolutions for large-scale JSSP instances, outperforming our constraint\nprogramming approach.",
      "tldr_zh": "本研究探讨了 Monte Carlo Tree Search (MCTS) 在 Job Shop Scheduling Problem (JSSP) 中的应用，旨在优化作业在不同机器上的序列以最小化加权作业完成时间。研究者提出了几种 Markov Decision Process (MDP) 模型来适应 MCTS 算法，并引入了一个基于真实制造数据的合成基准，以模拟大型、非矩形实例的复杂性。实验结果显示，MCTS 在大规模 JSSP 实例上生成高质量解决方案，显著优于传统的约束编程方法。",
      "categories": [
        "cs.AI",
        "math.OC",
        "F.2.2"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17991v1",
      "published_date": "2025-01-29 20:55:53 UTC",
      "updated_date": "2025-01-29 20:55:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:23:26.020176"
    },
    {
      "arxiv_id": "2501.17982v2",
      "title": "Belief Roadmaps with Uncertain Landmark Evanescence",
      "title_zh": "翻译失败",
      "authors": [
        "Erick Fuentes",
        "Jared Strader",
        "Ethan Fahnestock",
        "Nicholas Roy"
      ],
      "abstract": "We would like a robot to navigate to a goal location while minimizing state\nuncertainty. To aid the robot in this endeavor, maps provide a prior belief\nover the location of objects and regions of interest. To localize itself within\nthe map, a robot identifies mapped landmarks using its sensors. However, as the\ntime between map creation and robot deployment increases, portions of the map\ncan become stale, and landmarks, once believed to be permanent, may disappear.\nWe refer to the propensity of a landmark to disappear as landmark evanescence.\nReasoning about landmark evanescence during path planning, and the associated\nimpact on localization accuracy, requires analyzing the presence or absence of\neach landmark, leading to an exponential number of possible outcomes of a given\nmotion plan. To address this complexity, we develop BRULE, an extension of the\nBelief Roadmap. During planning, we replace the belief over future robot poses\nwith a Gaussian mixture which is able to capture the effects of landmark\nevanescence. Furthermore, we show that belief updates can be made efficient,\nand that maintaining a random subset of mixture components is sufficient to\nfind high quality solutions. We demonstrate performance in simulated and\nreal-world experiments. Software is available at https://bit.ly/BRULE.",
      "tldr_zh": "该论文探讨了机器人导航中如何在最小化状态不确定性的同时处理地图地标可能消失的问题（landmark evanescence）。为了解决这一挑战，研究者开发了BRULE，一种扩展的Belief Roadmap框架，使用高斯混合模型(Gaussian mixture)来捕捉地标消失对机器人位置信念的影响，从而使路径规划更高效，并通过维护混合组件的随机子集来找到高质量解决方案。实验结果显示，BRULE在模拟和真实环境中表现出色，并提供了开源软件以供进一步应用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17982v2",
      "published_date": "2025-01-29 20:37:01 UTC",
      "updated_date": "2025-05-01 15:03:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:23:37.709904"
    },
    {
      "arxiv_id": "2501.17980v2",
      "title": "Limits to AI Growth: The Ecological and Social Consequences of Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Eshta Bhardwaj",
        "Rohan Alexander",
        "Christoph Becker"
      ],
      "abstract": "The accelerating development and deployment of AI technologies depend on the\ncontinued ability to scale their infrastructure. This has implied increasing\namounts of monetary investment and natural resources. Frontier AI applications\nhave thus resulted in rising financial, environmental, and social costs. While\nthe factors that AI scaling depends on reach its limits, the push for its\naccelerated advancement and entrenchment continues. In this paper, we provide a\nholistic review of AI scaling using four lenses (technical, economic,\necological, and social) and review the relationships between these lenses to\nexplore the dynamics of AI growth. We do so by drawing on system dynamics\nconcepts including archetypes such as \"limits to growth\" to model the dynamic\ncomplexity of AI scaling and synthesize several perspectives. Our work maps out\nthe entangled relationships between the technical, economic, ecological and\nsocial perspectives and the apparent limits to growth. The analysis explains\nhow industry's responses to external limits enables continued (but temporary)\nscaling and how this benefits Big Tech while externalizing social and\nenvironmental damages. To avoid an \"overshoot and collapse\" trajectory, we\nadvocate for realigning priorities and norms around scaling to prioritize\nsustainable and mindful advancements.",
      "tldr_zh": "这篇论文审视了AI规模化（AI scaling）的限制及其生态和社会后果，通过技术、经济、生态和社会四个角度进行整体回顾，并运用系统动态概念（如\"limits to growth\"原型）来建模AI增长的动态复杂性。研究发现，AI基础设施的扩张虽推动了技术进步，但导致了不断上升的财务、环境和社会成本，而行业的应对策略往往暂时维持规模化，却将损害外部化到社会和生态系统。作者主张重新调整对规模化的优先级和规范，优先推动可持续且负责任的AI发展，以避免\"overshoot and collapse\"轨迹。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.17980v2",
      "published_date": "2025-01-29 20:25:42 UTC",
      "updated_date": "2025-01-31 23:41:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:27:12.062798"
    },
    {
      "arxiv_id": "2501.17974v2",
      "title": "Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Zishun Yu",
        "Tengyu Xu",
        "Di Jin",
        "Karthik Abinav Sankararaman",
        "Yun He",
        "Wenxuan Zhou",
        "Zhouhao Zeng",
        "Eryk Helenowski",
        "Chen Zhu",
        "Sinong Wang",
        "Hao Ma",
        "Han Fang"
      ],
      "abstract": "Solving mathematics problems has been an intriguing capability of large\nlanguage models, and many efforts have been made to improve reasoning by\nextending reasoning length, such as through self-correction and extensive long\nchain-of-thoughts. While promising in problem-solving, advanced long reasoning\nchain models exhibit an undesired single-modal behavior, where trivial\nquestions require unnecessarily tedious long chains of thought. In this work,\nwe propose a way to allow models to be aware of inference budgets by\nformulating it as utility maximization with respect to an inference budget\nconstraint, hence naming our algorithm Inference Budget-Constrained Policy\nOptimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to\n``understand'' the difficulty of queries and allocate inference budgets to\nharder ones. With different inference budgets, our best models are able to have\na $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative\nimprovement) on MATH500 using $2.16$x and $4.32$x inference budgets\nrespectively, relative to LLaMA3.1 8B Instruct. These improvements are\napproximately $2$x those of self-consistency under the same budgets.",
      "tldr_zh": "该论文针对大型语言模型在解决数学问题时过度使用冗长推理链的问题，提出了一种适应性推理方法。作者开发了 Inference Budget-Constrained Policy Optimization (IBPO) 算法，将推理过程视为效用最大化问题，允许模型根据查询难度智能分配推理预算，从而避免在简单问题上浪费资源。实验结果显示，在 MATH500 数据集上，IBPO 优化后的模型相比 LLaMA3.1 8B Instruct，在 2.16x 和 4.32x 推理预算下，准确率分别绝对提高了 4.14% 和 5.74%，并约是 self-consistency 方法的两倍改进。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17974v2",
      "published_date": "2025-01-29 20:20:48 UTC",
      "updated_date": "2025-01-31 16:06:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:25:31.413083"
    },
    {
      "arxiv_id": "2501.17917v1",
      "title": "Deep Ensembles Secretly Perform Empirical Bayes",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Loaiza-Ganem",
        "Valentin Villecroze",
        "Yixin Wang"
      ],
      "abstract": "Quantifying uncertainty in neural networks is a highly relevant problem which\nis essential to many applications. The two predominant paradigms to tackle this\ntask are Bayesian neural networks (BNNs) and deep ensembles. Despite some\nsimilarities between these two approaches, they are typically surmised to lack\na formal connection and are thus understood as fundamentally different. BNNs\nare often touted as more principled due to their reliance on the Bayesian\nparadigm, whereas ensembles are perceived as more ad-hoc; yet, deep ensembles\ntend to empirically outperform BNNs, with no satisfying explanation as to why\nthis is the case. In this work we bridge this gap by showing that deep\nensembles perform exact Bayesian averaging with a posterior obtained with an\nimplicitly learned data-dependent prior. In other words deep ensembles are\nBayesian, or more specifically, they implement an empirical Bayes procedure\nwherein the prior is learned from the data. This perspective offers two main\nbenefits: (i) it theoretically justifies deep ensembles and thus provides an\nexplanation for their strong empirical performance; and (ii) inspection of the\nlearned prior reveals it is given by a mixture of point masses -- the use of\nsuch a strong prior helps elucidate observed phenomena about ensembles.\nOverall, our work delivers a newfound understanding of deep ensembles which is\nnot only of interest in it of itself, but which is also likely to generate\nfuture insights that drive empirical improvements for these models.",
      "tldr_zh": "这篇论文揭示了 deep ensembles 实际上在执行 empirical Bayes 过程，通过隐式学习的数据依赖先验进行精确的 Bayesian averaging，从而量化神经网络的不确定性。研究发现，尽管 deep ensembles 通常被视为一种临时性方法，但它们本质上实现了 Bayesian 框架，与 Bayesian neural networks (BNNs) 存在正式联系，这解释了 deep ensembles 在实证性能上优于 BNNs 的原因。作者进一步分析了学习到的先验，发现它是由混合点质量组成的，这有助于阐明 ensembles 的现象，并为这些模型的未来改进提供理论基础和实用见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17917v1",
      "published_date": "2025-01-29 19:00:01 UTC",
      "updated_date": "2025-01-29 19:00:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:25:42.784888"
    },
    {
      "arxiv_id": "2501.17860v1",
      "title": "Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Zijie Liu",
        "Xinyu Zhao",
        "Jie Peng",
        "Zhuangdi Zhu",
        "Qingyu Chen",
        "Xia Hu",
        "Tianlong Chen"
      ],
      "abstract": "Current medical AI systems often fail to replicate real-world clinical\nreasoning, as they are predominantly trained and evaluated on static text and\nquestion-answer tasks. These tuning methods and benchmarks overlook critical\naspects like evidence-based reasoning and handling distracting information. To\nbridge this gap, we introduce a novel benchmark that simulates real-world\ndiagnostic scenarios, integrating noise and difficulty levels aligned with\nUSMLE standards. Moreover, we explore dialogue-based fine-tuning, which\ntransforms static datasets into conversational formats to better capture\niterative reasoning processes. Experiments show that dialogue-tuned models\noutperform traditional methods, with improvements of $9.64\\%$ in multi-round\nreasoning scenarios and $6.18\\%$ in accuracy in a noisy environment. Our\nfindings highlight dialogue tuning as a promising approach for advancing\nclinically aligned and robust medical AI systems.",
      "tldr_zh": "当前医疗 AI 系统如医疗 LLMs 主要通过静态文本和问答任务训练，忽略了证据-based 推理和处理干扰信息的关键方面，导致临床推理不足。为解决此问题，本文引入一个模拟真实诊断场景的基准，整合噪声和难度水平以符合 USMLE 标准，并探索对话-based 微调方法，将静态数据集转化为对话格式以捕捉迭代推理过程。实验结果显示，对话调优模型在多轮推理场景中提升9.64%，在嘈杂环境中准确率提高6.18%。这一方法为开发更具临床相关性和鲁棒性的医疗 AI 系统提供了有前景的途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17860v1",
      "published_date": "2025-01-29 18:58:48 UTC",
      "updated_date": "2025-01-29 18:58:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:25:54.621975"
    },
    {
      "arxiv_id": "2501.17858v1",
      "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
      "title_zh": "通过投票操纵提升你在 Chatbot Arena 上的模型排名",
      "authors": [
        "Rui Min",
        "Tianyu Pang",
        "Chao Du",
        "Qian Liu",
        "Minhao Cheng",
        "Min Lin"
      ],
      "abstract": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,\nwhere users vote for their preferred response from two randomly sampled\nanonymous models. While Chatbot Arena is widely regarded as a reliable LLM\nranking leaderboard, we show that crowdsourced voting can be rigged to improve\n(or decrease) the ranking of a target model $m_{t}$. We first introduce a\nstraightforward target-only rigging strategy that focuses on new battles\ninvolving $m_{t}$, identifying it via watermarking or a binary classifier, and\nexclusively voting for $m_{t}$ wins. However, this strategy is practically\ninefficient because there are over $190$ models on Chatbot Arena and on average\nonly about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we\npropose omnipresent rigging strategies, exploiting the Elo rating mechanism of\nChatbot Arena that any new vote on a battle can influence the ranking of the\ntarget model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.\nWe conduct experiments on around $1.7$ million historical votes from the\nChatbot Arena Notebook, showing that omnipresent rigging strategies can improve\nmodel rankings by rigging only hundreds of new votes. While we have evaluated\nseveral defense mechanisms, our findings highlight the importance of continued\nefforts to prevent vote rigging. Our code is available at\nhttps://github.com/sail-sg/Rigging-ChatbotArena.",
      "tldr_zh": "这篇论文揭示了 Chatbot Arena 平台上通过 vote rigging 操纵模型排名的漏洞，该平台依赖用户投票进行 LLMs 的成对比较。作者提出了两种策略：简单的 target-only rigging，通过识别并优先投票给目标模型 $m_t$，以及更高效的 omnipresent rigging，利用 Elo rating 机制在非直接相关战斗中影响 $m_t$ 的排名。实验基于约170万历史投票数据显示，仅需操纵数百票就能显著提升模型排名，并呼吁加强防御机制以维护平台可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17858v1",
      "published_date": "2025-01-29 18:57:29 UTC",
      "updated_date": "2025-01-29 18:57:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:26:06.738975"
    },
    {
      "arxiv_id": "2501.17855v1",
      "title": "GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Ziang Liu",
        "Yuanchen Ju",
        "Yu Da",
        "Tom Silver",
        "Pranav N. Thakkar",
        "Jenna Li",
        "Justin Guo",
        "Katherine Dimitropoulou",
        "Tapomayukh Bhattacharjee"
      ],
      "abstract": "Robot caregiving should be personalized to meet the diverse needs of care\nrecipients -- assisting with tasks as needed, while taking user agency in\naction into account. In physical tasks such as handover, bathing, dressing, and\nrehabilitation, a key aspect of this diversity is the functional range of\nmotion (fROM), which can vary significantly between individuals. In this work,\nwe learn to predict personalized fROM as a way to generalize robot\ndecision-making in a wide range of caregiving tasks. We propose a novel\ndata-driven method for predicting personalized fROM using functional assessment\nscores from occupational therapy. We develop a neural model that learns to\nembed functional assessment scores into a latent representation of the user's\nphysical function. The model is trained using motion capture data collected\nfrom users with emulated mobility limitations. After training, the model\npredicts personalized fROM for new users without motion capture. Through\nsimulated experiments and a real-robot user study, we show that the\npersonalized fROM predictions from our model enable the robot to provide\npersonalized and effective assistance while improving the user's agency in\naction. See our website for more visualizations:\nhttps://emprise.cs.cornell.edu/grace/.",
      "tldr_zh": "该论文提出 GRACE 方法，用于泛化机器人辅助护理，通过用户功能嵌入（User Functionality Embeddings）预测个性化的功能范围（functional range of motion, fROM），以适应不同个体的身体多样性和行动自主性。研究开发了一个神经模型（neural model），利用职业治疗的功能评估分数和动作捕捉（motion capture）数据进行训练，将评估分数嵌入用户身体功能的潜在表示，从而为新用户预测 fROM，而无需额外捕捉数据。在模拟实验和真实机器人用户研究中，GRACE 显著提升了机器人在移交、沐浴、穿衣和康复等任务中的个性化协助效果，同时提高了用户的行动代理能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 5 figures, Accepted to IEEE/ACM International Conference on\n  Human-Robot Interaction (HRI), 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17855v1",
      "published_date": "2025-01-29 18:55:07 UTC",
      "updated_date": "2025-01-29 18:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:28:11.996749"
    },
    {
      "arxiv_id": "2501.17842v1",
      "title": "From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Junseok Park",
        "Hyeonseo Yang",
        "Min Whoo Lee",
        "Won-Seok Choi",
        "Minsu Lee",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Reinforcement learning (RL) agents often face challenges in balancing\nexploration and exploitation, particularly in environments where sparse or\ndense rewards bias learning. Biological systems, such as human toddlers,\nnaturally navigate this balance by transitioning from free exploration with\nsparse rewards to goal-directed behavior guided by increasingly dense rewards.\nInspired by this natural progression, we investigate the Toddler-Inspired\nReward Transition in goal-oriented RL tasks. Our study focuses on transitioning\nfrom sparse to potential-based dense (S2D) rewards while preserving optimal\nstrategies. Through experiments on dynamic robotic arm manipulation and\negocentric 3D navigation tasks, we demonstrate that effective S2D reward\ntransitions significantly enhance learning performance and sample efficiency.\nAdditionally, using a Cross-Density Visualizer, we show that S2D transitions\nsmooth the policy loss landscape, resulting in wider minima that improve\ngeneralization in RL models. In addition, we reinterpret Tolman's maze\nexperiments, underscoring the critical role of early free exploratory learning\nin the context of S2D rewards.",
      "tldr_zh": "本论文受人类幼儿启发，在目标导向 Reinforcement Learning (RL) 中提出从稀疏到基于潜力的密集奖励 (S2D) 过渡方法，以平衡探索和利用挑战。研究通过动态机器人臂操作和第一人称 3D 导航任务的实验，证明 S2D 奖励过渡显著提升了学习性能和样本效率。利用 Cross-Density Visualizer，论文进一步显示该方法平滑了策略损失景观，导致更宽的极小值，提高了 RL 模型的泛化能力，并重新解释了 Tolman 的迷宫实验，强调早期自由探索学习的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "68T05, 68T20, 91E40"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version of AAAI 2024 paper: Unveiling the Significance of\n  Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning.\n  This manuscript is currently being prepared for journal submission",
      "pdf_url": "http://arxiv.org/pdf/2501.17842v1",
      "published_date": "2025-01-29 18:46:35 UTC",
      "updated_date": "2025-01-29 18:46:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:26:31.474882"
    },
    {
      "arxiv_id": "2501.17823v2",
      "title": "Robust Multimodal Learning via Cross-Modal Proxy Tokens",
      "title_zh": "翻译失败",
      "authors": [
        "Md Kaykobad Reza",
        "Ameya Patil",
        "Mashhour Solh",
        "M. Salman Asif"
      ],
      "abstract": "Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality. To efficiently learn the approximation for\nthe missing modality via CMPTs with minimal computational overhead, we employ\nlow-rank adapters in frozen unimodal encoders and jointly optimize an alignment\nloss with a task-specific loss. Extensive experiments on five multimodal\ndatasets show that our method outperforms state-of-the-art baselines across\nvarious missing rates while achieving competitive results in complete-modality\nsettings. Overall, our method offers a flexible and efficient solution for\nrobust multimodal learning. The code and pretrained models will be released on\nGitHub.",
      "tldr_zh": "本研究针对多模态模型在模态缺失时的性能下降问题，提出了一种简单有效的鲁棒学习方法，即引入 Cross-Modal Proxy Tokens (CMPTs)，这些标记通过关注可用模态的标记来近似缺失模态的类标记。方法利用 low-rank adapters 在冻结的 unimodal encoders 中进行高效学习，并联合优化 alignment loss 和 task-specific loss，以最小化计算开销。在五个多模态数据集上的广泛实验显示，该方法在各种缺失率下优于最先进基线，同时在完整模态设置中保持竞争性性能，提供了一个灵活高效的鲁棒多模态学习解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "17 Pages, 10 Figures, 6 Tables",
      "pdf_url": "http://arxiv.org/pdf/2501.17823v2",
      "published_date": "2025-01-29 18:15:49 UTC",
      "updated_date": "2025-03-10 01:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:26:43.226125"
    },
    {
      "arxiv_id": "2501.17822v2",
      "title": "Aggregation Schemes for Single-Vector WSI Representation Learning in Digital Pathology",
      "title_zh": "翻译失败",
      "authors": [
        "Sobhan Hemati",
        "Ghazal Alabtah",
        "Saghir Alfasly",
        "H. R. Tizhoosh"
      ],
      "abstract": "A crucial step to efficiently integrate Whole Slide Images (WSIs) in\ncomputational pathology is assigning a single high-quality feature vector,\ni.e., one embedding, to each WSI. With the existence of many pre-trained deep\nneural networks and the emergence of foundation models, extracting embeddings\nfor sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,\ngiven their high resolution and gigapixel nature, inputting them into existing\nGPUs as a single image is not feasible. As a result, WSIs are usually split\ninto many patches. Feeding each patch to a pre-trained model, each WSI can then\nbe represented by a set of patches, hence, a set of embeddings. Hence, in such\na setup, WSI representation learning reduces to set representation learning\nwhere for each WSI we have access to a set of patch embeddings. To obtain a\nsingle embedding from a set of patch embeddings for each WSI, multiple\nset-based learning schemes have been proposed in the literature. In this paper,\nwe evaluate the WSI search performance of multiple recently developed\naggregation techniques (mainly set representation learning techniques)\nincluding simple average or max pooling operations, Deep Sets, Memory networks,\nFocal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse\nand binary Fisher Vector on four different primary sites including bladder,\nbreast, kidney, and Colon from TCGA. Further, we benchmark the search\nperformance of these methods against the median of minimum distances of patch\nembeddings, a non-aggregating approach used for WSI retrieval.",
      "tldr_zh": "本论文探讨了在数字病理学中，为 Whole Slide Images (WSIs) 生成单一高质量特征向量的聚合方案，以解决 WSIs 高分辨率问题。研究评估了多种聚合技术，包括 average pooling、max pooling、Deep Sets、Memory networks、Focal attention、Gaussian Mixture Model (GMM) Fisher Vector 以及 deep sparse and binary Fisher Vector，这些方法从 WSIs 的 patch embeddings 中提取单一 embedding。实验在 TCGA 数据集的 bladder、breast、kidney 和 colon 等四个部位上进行比较，结果显示这些聚合方案在 WSI 检索性能上优于非聚合方法（如 median of minimum distances）。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17822v2",
      "published_date": "2025-01-29 18:14:51 UTC",
      "updated_date": "2025-05-21 15:05:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:26:54.701901"
    },
    {
      "arxiv_id": "2501.17813v1",
      "title": "P-TAME: Explain Any Image Classifier with Trained Perturbations",
      "title_zh": "翻译失败",
      "authors": [
        "Mariano V. Ntrougkas",
        "Vasileios Mezaris",
        "Ioannis Patras"
      ],
      "abstract": "The adoption of Deep Neural Networks (DNNs) in critical fields where\npredictions need to be accompanied by justifications is hindered by their\ninherent black-box nature. In this paper, we introduce P-TAME\n(Perturbation-based Trainable Attention Mechanism for Explanations), a\nmodel-agnostic method for explaining DNN-based image classifiers. P-TAME\nemploys an auxiliary image classifier to extract features from the input image,\nbypassing the need to tailor the explanation method to the internal\narchitecture of the backbone classifier being explained. Unlike traditional\nperturbation-based methods, which have high computational requirements, P-TAME\noffers an efficient alternative by generating high-resolution explanations in a\nsingle forward pass during inference. We apply P-TAME to explain the decisions\nof VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image\nclassifiers. Quantitative and qualitative results show that our method matches\nor outperforms previous explainability methods, including model-specific\napproaches. Code and trained models will be released upon acceptance.",
      "tldr_zh": "本文提出 P-TAME，一种模型无关的方法，用于解释 DNN 图像分类器的决策，通过辅助图像分类器提取特征并利用训练扰动生成高分辨率解释，避免了传统扰动方法的计算密集需求。P-TAME 能够在单次前向传播中高效产生解释结果，并在 VGG-16、ResNet-50 和 ViT-B-16 等模型上进行测试。实验结果显示，其性能匹配或超过现有解释方法，包括特定模型的方法。代码和训练模型将在论文接受后发布。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted for publication",
      "pdf_url": "http://arxiv.org/pdf/2501.17813v1",
      "published_date": "2025-01-29 18:06:08 UTC",
      "updated_date": "2025-01-29 18:06:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:27:06.428101"
    },
    {
      "arxiv_id": "2501.17811v1",
      "title": "Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaokang Chen",
        "Zhiyu Wu",
        "Xingchao Liu",
        "Zizheng Pan",
        "Wen Liu",
        "Zhenda Xie",
        "Xingkai Yu",
        "Chong Ruan"
      ],
      "abstract": "In this work, we introduce Janus-Pro, an advanced version of the previous\nwork Janus. Specifically, Janus-Pro incorporates (1) an optimized training\nstrategy, (2) expanded training data, and (3) scaling to larger model size.\nWith these improvements, Janus-Pro achieves significant advancements in both\nmultimodal understanding and text-to-image instruction-following capabilities,\nwhile also enhancing the stability of text-to-image generation. We hope this\nwork will inspire further exploration in the field. Code and models are\npublicly available.",
      "tldr_zh": "本文介绍了 Janus-Pro，一种基于前作 Janus 的高级模型，通过优化训练策略、扩展训练数据和模型规模扩大，实现了多模态理解和生成能力的统一提升。相比基线，Janus-Pro 在 Multimodal Understanding 和 Text-to-Image Instruction-Following 方面取得了显著进展，同时增强了文本到图像生成的稳定性。这些改进有助于推动相关领域探索，代码和模型已公开可用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Research paper. arXiv admin note: text overlap with arXiv:2410.13848",
      "pdf_url": "http://arxiv.org/pdf/2501.17811v1",
      "published_date": "2025-01-29 18:00:19 UTC",
      "updated_date": "2025-01-29 18:00:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:28:24.506923"
    },
    {
      "arxiv_id": "2501.18649v1",
      "title": "Fake News Detection After LLM Laundering: Measurement and Explanation",
      "title_zh": "翻译失败",
      "authors": [
        "Rupak Kumar Das",
        "Jonathan Dodge"
      ],
      "abstract": "With their advanced capabilities, Large Language Models (LLMs) can generate\nhighly convincing and contextually relevant fake news, which can contribute to\ndisseminating misinformation. Though there is much research on fake news\ndetection for human-written text, the field of detecting LLM-generated fake\nnews is still under-explored. This research measures the efficacy of detectors\nin identifying LLM-paraphrased fake news, in particular, determining whether\nadding a paraphrase step in the detection pipeline helps or impedes detection.\nThis study contributes: (1) Detectors struggle to detect LLM-paraphrased fake\nnews more than human-written text, (2) We find which models excel at which\ntasks (evading detection, paraphrasing to evade detection, and paraphrasing for\nsemantic similarity). (3) Via LIME explanations, we discovered a possible\nreason for detection failures: sentiment shift. (4) We discover a worrisome\ntrend for paraphrase quality measurement: samples that exhibit sentiment shift\ndespite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing\ndatasets with paraphrase outputs and scores. The dataset is available on GitHub",
      "tldr_zh": "该研究评估了大型语言模型(LLMs)生成的假新闻检测效能，特别是LLM改写(laundering)后的假新闻，发现检测器对这类假新闻的识别比人类写的更困难，且添加改写步骤可能进一步削弱检测效果。研究通过实验识别了不同模型在逃避检测、改写以逃避检测以及保持语义相似性方面的表现，并利用LIME解释揭示了情感偏移(sentiment shift)是检测失败的主要原因。最终，他们提供了一对增强数据集，包括改写输出和分数，并在GitHub公开，以突出BERTSCORE高但情感偏移的问题。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18649v1",
      "published_date": "2025-01-29 17:58:07 UTC",
      "updated_date": "2025-01-29 17:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:28:35.272925"
    },
    {
      "arxiv_id": "2501.17805v1",
      "title": "International AI Safety Report",
      "title_zh": "翻译失败",
      "authors": [
        "Yoshua Bengio",
        "Sören Mindermann",
        "Daniel Privitera",
        "Tamay Besiroglu",
        "Rishi Bommasani",
        "Stephen Casper",
        "Yejin Choi",
        "Philip Fox",
        "Ben Garfinkel",
        "Danielle Goldfarb",
        "Hoda Heidari",
        "Anson Ho",
        "Sayash Kapoor",
        "Leila Khalatbari",
        "Shayne Longpre",
        "Sam Manning",
        "Vasilios Mavroudis",
        "Mantas Mazeika",
        "Julian Michael",
        "Jessica Newman",
        "Kwan Yee Ng",
        "Chinasa T. Okolo",
        "Deborah Raji",
        "Girish Sastry",
        "Elizabeth Seger",
        "Theodora Skeadas",
        "Tobin South",
        "Emma Strubell",
        "Florian Tramèr",
        "Lucia Velasco",
        "Nicole Wheeler",
        "Daron Acemoglu",
        "Olubayo Adekanmbi",
        "David Dalrymple",
        "Thomas G. Dietterich",
        "Edward W. Felten",
        "Pascale Fung",
        "Pierre-Olivier Gourinchas",
        "Fredrik Heintz",
        "Geoffrey Hinton",
        "Nick Jennings",
        "Andreas Krause",
        "Susan Leavy",
        "Percy Liang",
        "Teresa Ludermir",
        "Vidushi Marda",
        "Helen Margetts",
        "John McDermid",
        "Jane Munga",
        "Arvind Narayanan",
        "Alondra Nelson",
        "Clara Neppel",
        "Alice Oh",
        "Gopal Ramchurn",
        "Stuart Russell",
        "Marietje Schaake",
        "Bernhard Schölkopf",
        "Dawn Song",
        "Alvaro Soto",
        "Lee Tiedrich",
        "Gaël Varoquaux",
        "Andrew Yao",
        "Ya-Qin Zhang",
        "Fahad Albalawi",
        "Marwan Alserkal",
        "Olubunmi Ajala",
        "Guillaume Avrin",
        "Christian Busch",
        "André Carlos Ponce de Leon Ferreira de Carvalho",
        "Bronwyn Fox",
        "Amandeep Singh Gill",
        "Ahmet Halit Hatip",
        "Juha Heikkilä",
        "Gill Jolly",
        "Ziv Katzir",
        "Hiroaki Kitano",
        "Antonio Krüger",
        "Chris Johnson",
        "Saif M. Khan",
        "Kyoung Mu Lee",
        "Dominic Vincent Ligot",
        "Oleksii Molchanovskyi",
        "Andrea Monti",
        "Nusu Mwamanzi",
        "Mona Nemer",
        "Nuria Oliver",
        "José Ramón López Portillo",
        "Balaraman Ravindran",
        "Raquel Pezoa Rivera",
        "Hammam Riza",
        "Crystal Rugege",
        "Ciarán Seoighe",
        "Jerry Sheehan",
        "Haroon Sheikh",
        "Denise Wong",
        "Yi Zeng"
      ],
      "abstract": "The first International AI Safety Report comprehensively synthesizes the\ncurrent evidence on the capabilities, risks, and safety of advanced AI systems.\nThe report was mandated by the nations attending the AI Safety Summit in\nBletchley, UK. Thirty nations, the UN, the OECD, and the EU each nominated a\nrepresentative to the report's Expert Advisory Panel. A total of 100 AI experts\ncontributed, representing diverse perspectives and disciplines. Led by the\nreport's Chair, these independent experts collectively had full discretion over\nthe report's content.",
      "tldr_zh": "本报告是首份国际AI安全报告，全面综合了先进AI系统的能力、风险和安全方面的现有证据，由AI Safety Summit（在英国Bletchley举行）的与会国家授权。报告由30个国家、UN、OECD和EU提名的代表组成的专家咨询小组主导，总计100位AI专家贡献了多样视角和学科见解。专家们在报告主席的领导下，独立决定内容，为全球AI安全治理提供权威参考。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17805v1",
      "published_date": "2025-01-29 17:47:36 UTC",
      "updated_date": "2025-01-29 17:47:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:28:46.315837"
    },
    {
      "arxiv_id": "2501.17790v1",
      "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
      "title_zh": "翻译失败",
      "authors": [
        "Chan-Jan Hsu",
        "Yi-Cheng Lin",
        "Chia-Chun Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chen-An Li",
        "Yi-Chang Chen",
        "Chien-Yu Yu",
        "Ming-Ji Lee",
        "Chien-Cheng Chen",
        "Ru-Heng Huang",
        "Hung-yi Lee",
        "Da-Shan Shiu"
      ],
      "abstract": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted\nfor Taiwanese Mandarin, highlighting phonetic control abilities to address the\nunique challenges of polyphone disambiguation in the language. Building upon\nCosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an\noptimal-transport conditional flow matching model (OT-CFM), and a grapheme to\nphoneme prediction model, to generate realistic speech that closely mimics\nhuman utterances. Our evaluation demonstrates BreezyVoice's superior\nperformance in both general and code-switching contexts, highlighting its\nrobustness and effectiveness in generating high-fidelity speech. Additionally,\nwe address the challenges of generalizability in modeling long-tail speakers\nand polyphone disambiguation. Our approach significantly enhances performance\nand offers valuable insights into the workings of neural codec TTS systems.",
      "tldr_zh": "本研究提出 BreezyVoice，一种针对台湾普通话的 Text-to-Speech (TTS) 系统，通过增强的多音字消歧能力来解决语言特定挑战。系统基于 CosyVoice，整合了 S³ tokenizer、large language model (LLM)、optimal-transport conditional flow matching model (OT-CFM) 和 grapheme to phoneme 预测模型，以生成逼真的人类化语音。评估结果显示，BreezyVoice 在一般语境和代码切换场景中表现出色，提高了语音保真度，并提供了处理长尾说话者和多音字消歧的宝贵见解，从而提升了神经编解码 TTS 系统的整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17790v1",
      "published_date": "2025-01-29 17:31:26 UTC",
      "updated_date": "2025-01-29 17:31:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:28:58.833170"
    },
    {
      "arxiv_id": "2501.17771v1",
      "title": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs",
      "title_zh": "2SSP：一种用于大型语言模型结构化剪枝的两阶段框架",
      "authors": [
        "Fabrizio Sandri",
        "Elia Cunegatti",
        "Giovanni Iacca"
      ],
      "abstract": "We propose a novel Two-Stage framework for Structured Pruning (2SSP) for\npruning Large Language Models (LLMs), which combines two different strategies\nof pruning, namely Width and Depth Pruning. The first stage (Width Pruning)\nremoves entire neurons, hence their corresponding rows and columns, aiming to\npreserve the connectivity among the pruned structures in the intermediate state\nof the Feed-Forward Networks in each Transformer block. This is done based on\nan importance score measuring the impact of each neuron over the output\nmagnitude. The second stage (Depth Pruning), instead, removes entire Attention\nsubmodules. This is done by applying an iterative process that removes the\nAttention submodules with the minimum impact on a given metric of interest (in\nour case, perplexity). We also propose a novel mechanism to balance the\nsparsity rate of the two stages w.r.t. to the desired global sparsity. We test\n2SSP on four LLM families and three sparsity rates (25\\%, 37.5\\%, and 50\\%),\nmeasuring the resulting perplexity over three language modeling datasets as\nwell as the performance over six downstream tasks. Our method consistently\noutperforms five state-of-the-art competitors over three language modeling and\nsix downstream tasks, with an up to two-order-of-magnitude gain in terms of\npruning time. The code is available at available at\n\\url{https://github.com/FabrizioSandri/2SSP}.",
      "tldr_zh": "本文提出了一种名为2SSP的Two-Stage framework for Structured Pruning，用于高效压缩Large Language Models (LLMs)。该框架分为两阶段：第一阶段的Width Pruning基于神经元重要性分数移除整个神经元及其对应结构，以保持Transformer块的连接性；第二阶段的Depth Pruning通过迭代过程移除对perplexity影响最小的Attention submodules，并引入机制平衡两阶段的稀疏率。实验在四个LLM系列和25%、37.5%、50%三种稀疏率下显示，2SSP在三个语言建模数据集和六个下游任务上均优于五种state-of-the-art方法，修剪时间提升多达两个数量级。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17771v1",
      "published_date": "2025-01-29 17:05:33 UTC",
      "updated_date": "2025-01-29 17:05:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:29:11.230799"
    },
    {
      "arxiv_id": "2501.17767v1",
      "title": "Hybrid Graphs for Table-and-Text based Question Answering using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Ankush Agarwal",
        "Ganesh S",
        "Chaitanya Devaguptapu"
      ],
      "abstract": "Answering questions that require reasoning and aggregation across both\nstructured (tables) and unstructured (raw text) data sources presents\nsignificant challenges. Current methods rely on fine-tuning and high-quality,\nhuman-curated data, which is difficult to obtain. Recent advances in Large\nLanguage Models (LLMs) have shown promising results for multi-hop question\nanswering (QA) over single-source text data in a zero-shot setting, yet\nexploration into multi-source Table-Text QA remains limited. In this paper, we\npresent a novel Hybrid Graph-based approach for Table-Text QA that leverages\nLLMs without fine-tuning. Our method constructs a unified Hybrid Graph from\ntextual and tabular data, pruning information based on the input question to\nprovide the LLM with relevant context concisely. We evaluate our approach on\nthe challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs,\nincluding GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot\nperformance on both datasets, improving Exact Match scores by up to 10% on\nHybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up\nto 53% compared to the original context.",
      "tldr_zh": "本论文提出了一种基于Hybrid Graphs的方法，用于Table-and-Text QA，利用LLMs（如GPT-3.5、GPT-4和LLaMA-3）进行跨结构化（表格）和非结构化（文本）数据的多跳推理，而无需模型微调。该方法通过构建统一的Hybrid Graph并根据输入问题修剪信息，提供简洁的相关上下文，从而提升查询效率。在Hybrid-QA和OTT-QA数据集上的实验中，该方法在零样本设置下实现了最佳性能，Exact Match分数分别提高了10%和5.4%，并将token使用减少了高达53%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NAACL 2025 Main Track",
      "pdf_url": "http://arxiv.org/pdf/2501.17767v1",
      "published_date": "2025-01-29 16:58:18 UTC",
      "updated_date": "2025-01-29 16:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:29:22.483841"
    },
    {
      "arxiv_id": "2501.17759v1",
      "title": "Yin-Yang: Developing Motifs With Long-Term Structure And Controllability",
      "title_zh": "翻译失败",
      "authors": [
        "Keshav Bhandari",
        "Geraint A. Wiggins",
        "Simon Colton"
      ],
      "abstract": "Transformer models have made great strides in generating symbolically\nrepresented music with local coherence. However, controlling the development of\nmotifs in a structured way with global form remains an open research area. One\nof the reasons for this challenge is due to the note-by-note autoregressive\ngeneration of such models, which lack the ability to correct themselves after\ndeviations from the motif. In addition, their structural performance on\ndatasets with shorter durations has not been studied in the literature. In this\nstudy, we propose Yin-Yang, a framework consisting of a phrase generator,\nphrase refiner, and phrase selector models for the development of motifs into\nmelodies with long-term structure and controllability. The phrase refiner is\ntrained on a novel corruption-refinement strategy which allows it to produce\nmelodic and rhythmic variations of an original motif at generation time,\nthereby rectifying deviations of the phrase generator. We also introduce a new\nobjective evaluation metric for quantifying how smoothly the motif manifests\nitself within the piece. Evaluation results show that our model achieves better\nperformance compared to state-of-the-art transformer models while having the\nadvantage of being controllable and making the generated musical structure\nsemi-interpretable, paving the way for musical analysis. Our code and demo page\ncan be found at https://github.com/keshavbhandari/yinyang.",
      "tldr_zh": "该研究提出Yin-Yang框架，用于提升Transformer models在音乐生成中的主题（motifs）开发能力，专注于实现长期结构和可控性。该框架包括phrase generator、phrase refiner和phrase selector模型，其中phrase refiner采用novel corruption-refinement strategy来生成主题的旋律和节奏变体，从而纠正phrase generator的偏差。同时，引入一个新客观评估指标来量化主题在作品中的平滑表现。实验结果显示，Yin-Yang模型在性能上优于最先进的Transformer models，并提供可控性和生成的音乐结构半可解释性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.SD",
      "comment": "16 Pages, 4 Figures, Accepted at Artificial Intelligence in Music,\n  Sound, Art and Design: 14th International Conference, EvoMUSART 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17759v1",
      "published_date": "2025-01-29 16:50:09 UTC",
      "updated_date": "2025-01-29 16:50:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:29:34.795235"
    },
    {
      "arxiv_id": "2501.17755v2",
      "title": "AI Governance through Markets",
      "title_zh": "翻译失败",
      "authors": [
        "Philip Moreira Tomei",
        "Rupal Jain",
        "Matija Franklin"
      ],
      "abstract": "This paper argues that market governance mechanisms should be considered a\nkey approach in the governance of artificial intelligence (AI), alongside\ntraditional regulatory frameworks. While current governance approaches have\npredominantly focused on regulation, we contend that market-based mechanisms\noffer effective incentives for responsible AI development. We examine four\nemerging vectors of market governance: insurance, auditing, procurement, and\ndue diligence, demonstrating how these mechanisms can affirm the relationship\nbetween AI risk and financial risk while addressing capital allocation\ninefficiencies. While we do not claim that market forces alone can adequately\nprotect societal interests, we maintain that standardised AI disclosures and\nmarket mechanisms can create powerful incentives for safe and responsible AI\ndevelopment. This paper urges regulators, economists, and machine learning\nresearchers to investigate and implement market-based approaches to AI\ngovernance.",
      "tldr_zh": "这篇论文主张将市场治理机制视为人工智能（AI）治理的关键方法，与传统监管框架并行，以提供有效的激励促进负责任的AI开发。论文考察了四个新兴的市场治理向量：insurance、auditing、procurement 和 due diligence，这些机制能将AI风险转化为金融风险，同时解决资本分配的低效问题。尽管市场力量无法单独保护社会利益，但标准化的AI disclosures 和市场机制可创造强大激励，推动安全的AI创新。最终，论文呼吁监管者、经济学家和机器学习研究人员积极调查和实施这些市场-based 治理方法。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "q-fin.EC"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17755v2",
      "published_date": "2025-01-29 16:48:13 UTC",
      "updated_date": "2025-03-05 16:20:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:31:40.202964"
    },
    {
      "arxiv_id": "2501.17749v1",
      "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
      "title_zh": "OpenAI 的 o3-mini 的早期外部安全测试：来自部署前评估的洞见",
      "authors": [
        "Aitor Arrieta",
        "Miriam Ugarte",
        "Pablo Valle",
        "José Antonio Parejo",
        "Sergio Segura"
      ],
      "abstract": "Large Language Models (LLMs) have become an integral part of our daily lives.\nHowever, they impose certain risks, including those that can harm individuals'\nprivacy, perpetuate biases and spread misinformation. These risks highlight the\nneed for robust safety mechanisms, ethical guidelines, and thorough testing to\nensure their responsible deployment. Safety of LLMs is a key property that\nneeds to be thoroughly tested prior the model to be deployed and accessible to\nthe general users. This paper reports the external safety testing experience\nconducted by researchers from Mondragon University and University of Seville on\nOpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing\nprogram. In particular, we apply our tool, ASTRAL, to automatically and\nsystematically generate up to date unsafe test inputs (i.e., prompts) that\nhelps us test and assess different safety categories of LLMs. We automatically\ngenerate and execute a total of 10,080 unsafe test input on a early o3-mini\nbeta version. After manually verifying the test cases classified as unsafe by\nASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We\nhighlight key insights and findings uncovered during the pre-deployment\nexternal testing phase of OpenAI's latest LLM.",
      "tldr_zh": "这篇论文报告了对 OpenAI o3-mini 大语言模型(LLMs)的外部安全测试，由 Mondragon 大学和塞维利亚大学的研究人员在预部署阶段进行。研究团队使用 ASTRAL 工具自动生成并执行 10,080 个不安全测试输入(prompts)，以评估模型在隐私、偏见和错误信息等安全类别中的表现。测试结果显示，经过手动验证，确认了 87 个实际的不安全行为实例。总体而言，该研究提供了关键洞见，帮助改进 LLMs 的安全机制，确保其负责任部署。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "arXiv admin note: text overlap with arXiv:2501.17132",
      "pdf_url": "http://arxiv.org/pdf/2501.17749v1",
      "published_date": "2025-01-29 16:36:53 UTC",
      "updated_date": "2025-01-29 16:36:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:31:51.448080"
    },
    {
      "arxiv_id": "2501.17731v1",
      "title": "Exact characterization of ε-Safe Decision Regions for exponential family distributions and Multi Cost SVM approximation",
      "title_zh": "指数族分布的 ε-",
      "authors": [
        "Alberto Carlevaro",
        "Teodoro Alamo",
        "Fabrizio Dabbene",
        "Maurizio Mongelli"
      ],
      "abstract": "Probabilistic guarantees on the prediction of data-driven classifiers are\nnecessary to define models that can be considered reliable. This is a key\nrequirement for modern machine learning in which the goodness of a system is\nmeasured in terms of trustworthiness, clearly dividing what is safe from what\nis unsafe. The spirit of this paper is exactly in this direction. First, we\nintroduce a formal definition of {\\epsilon}-Safe Decision Region, a subset of\nthe input space in which the prediction of a target (safe) class is\nprobabilistically guaranteed. Second, we prove that, when data come from\nexponential family distributions, the form of such a region is analytically\ndetermined and controllable by design parameters, i.e. the probability of\nsampling the target class and the confidence on the prediction. However, the\nrequest of having exponential data is not always possible. Inspired by this\nlimitation, we developed Multi Cost SVM, an SVM based algorithm that\napproximates the safe region and is also able to handle unbalanced data. The\nresearch is complemented by experiments and code available for reproducibility.",
      "tldr_zh": "本文定义了 ε-Safe Decision Region，这是一个输入空间子集，其中目标（安全）类的预测在概率上得到保证。论文证明，当数据来自 exponential family distributions 时，这种区域的形式可通过分析确定，并受设计参数（如目标类采样概率和预测置信度）控制。为了应对数据并非总是指数族的限制，研究开发了 Multi Cost SVM 算法，该算法基于 SVM，能近似安全区域并处理不平衡数据。实验和可复现代码进一步验证了这些发现。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17731v1",
      "published_date": "2025-01-29 16:14:35 UTC",
      "updated_date": "2025-01-29 16:14:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:30:11.210889"
    },
    {
      "arxiv_id": "2501.17725v1",
      "title": "Using Code Generation to Solve Open Instances of Combinatorial Design Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher D. Rosin"
      ],
      "abstract": "The Handbook of Combinatorial Designs catalogs many types of combinatorial\ndesigns, together with lists of open instances for which existence has not yet\nbeen determined. We develop a constructive protocol CPro1, which uses Large\nLanguage Models (LLMs) to generate code that constructs combinatorial designs\nand resolves some of these open instances. The protocol starts from a\ndefinition of a particular type of design, and a verifier that reliably\nconfirms whether a proposed design is valid. The LLM selects strategies and\nimplements them in code, and scaffolding provides automated hyperparameter\ntuning and execution feedback using the verifier. Most generated code fails,\nbut by generating many candidates, the protocol automates exploration of a\nvariety of standard methods (e.g. simulated annealing, genetic algorithms) and\nexperimentation with variations (e.g. cost functions) to find successful\napproaches. Testing on 16 different types of designs, CPro1 constructs\nsolutions to open instances for 6 of them: Symmetric and Skew Weighing\nMatrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary\nDesigns, and Florentine Rectangles.",
      "tldr_zh": "本研究提出了一种名为 CPro1 的协议，利用 Large Language Models (LLMs) 生成代码来解决组合设计问题的开放实例，从而填补了《组合设计手册》中的未解决案例。协议从设计定义和验证器入手，LLM 选择策略（如 simulated annealing 和 genetic algorithms）并在代码中实现，通过自动超参数调优和反馈机制探索多种方法变异，最终生成有效的设计方案。在测试的 16 种设计类型中，CPro1 成功构建了 Symmetric and Skew Weighing Matrices、Equidistant Permutation Arrays、Packing Arrays、Balanced Ternary Designs 和 Florentine Rectangles 等 6 种的解决方案，为自动化组合设计探索提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DM",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17725v1",
      "published_date": "2025-01-29 15:57:43 UTC",
      "updated_date": "2025-01-29 15:57:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:32:02.334111"
    },
    {
      "arxiv_id": "2501.17704v1",
      "title": "Inferring Implicit Goals Across Differing Task Models",
      "title_zh": "翻译失败",
      "authors": [
        "Silvia Tulli",
        "Stylianos Loukas Vasileiou",
        "Mohamed Chetouani",
        "Sarath Sreedharan"
      ],
      "abstract": "One of the significant challenges to generating value-aligned behavior is to\nnot only account for the specified user objectives but also any implicit or\nunspecified user requirements. The existence of such implicit requirements\ncould be particularly common in settings where the user's understanding of the\ntask model may differ from the agent's estimate of the model. Under this\nscenario, the user may incorrectly expect some agent behavior to be inevitable\nor guaranteed. This paper addresses such expectation mismatch in the presence\nof differing models by capturing the possibility of unspecified user subgoal in\nthe context of a task captured as a Markov Decision Process (MDP) and querying\nfor it as required. Our method identifies bottleneck states and uses them as\ncandidates for potential implicit subgoals. We then introduce a querying\nstrategy that will generate the minimal number of queries required to identify\na policy guaranteed to achieve the underlying goal. Our empirical evaluations\ndemonstrate the effectiveness of our approach in inferring and achieving\nunstated goals across various tasks.",
      "tldr_zh": "该论文探讨了在用户和代理任务模型差异下推断隐式目标的挑战，旨在处理未指定用户要求以生成与价值对齐的行为。研究方法基于Markov Decision Process (MDP)，通过识别bottleneck states作为潜在隐式子目标候选，并引入最小查询策略来确定确保实现目标的政策。实证评估显示，该方法在各种任务中有效推断和实现未陈述目标，提高了代理行为的准确性和可靠性。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17704v1",
      "published_date": "2025-01-29 15:20:43 UTC",
      "updated_date": "2025-01-29 15:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:32:13.404853"
    },
    {
      "arxiv_id": "2501.17699v1",
      "title": "PulmoFusion: Advancing Pulmonary Health with Efficient Multi-Modal Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Sharshar",
        "Yasser Attia",
        "Mohammad Yaqub",
        "Mohsen Guizani"
      ],
      "abstract": "Traditional remote spirometry lacks the precision required for effective\npulmonary monitoring. We present a novel, non-invasive approach using\nmultimodal predictive models that integrate RGB or thermal video data with\npatient metadata. Our method leverages energy-efficient Spiking Neural Networks\n(SNNs) for the regression of Peak Expiratory Flow (PEF) and classification of\nForced Expiratory Volume (FEV1) and Forced Vital Capacity (FVC), using\nlightweight CNNs to overcome SNN limitations in regression tasks. Multimodal\ndata integration is improved with a Multi-Head Attention Layer, and we employ\nK-Fold validation and ensemble learning to boost robustness. Using thermal\ndata, our SNN models achieve 92% accuracy on a breathing-cycle basis and 99.5%\npatient-wise. PEF regression models attain Relative RMSEs of 0.11 (thermal) and\n0.26 (RGB), with an MAE of 4.52% for FEV1/FVC predictions, establishing\nstate-of-the-art performance. Code and dataset can be found on\nhttps://github.com/ahmed-sharshar/RespiroDynamics.git",
      "tldr_zh": "本研究针对传统远程肺活量测试精度不足的问题，提出PulmoFusion，一种非侵入性多模态预测模型，整合RGB或热成像视频数据与患者元数据，以提升肺部健康监测效率。\n该方法利用能量高效的Spiking Neural Networks (SNNs)进行Peak Expiratory Flow (PEF)回归和Forced Expiratory Volume (FEV1)/Forced Vital Capacity (FVC)分类，并结合轻量级CNNs克服SNN的回归局限，同时通过Multi-Head Attention Layer、K-Fold验证和集成学习优化多模态数据融合。\n实验结果显示，使用热成像数据，SNN模型在呼吸周期基础上达到92%准确率，在患者层面达到99.5%；PEF回归的相对RMSE分别为0.11（热成像）和0.26（RGB），FEV1/FVC预测的MAE为4.52%，实现了state-of-the-art性能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17699v1",
      "published_date": "2025-01-29 15:10:09 UTC",
      "updated_date": "2025-01-29 15:10:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:32:29.849948"
    },
    {
      "arxiv_id": "2501.17690v2",
      "title": "Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Zixue Zeng",
        "Xiaoyan Zhao",
        "Matthew Cartier",
        "Tong Yu",
        "Jing Wang",
        "Xin Meng",
        "Zhiyu Sheng",
        "Maryam Satarpour",
        "John M Cormack",
        "Allison Bean",
        "Ryan Nussbaum",
        "Maya Maurer",
        "Emily Landis-Walkenhorst",
        "Dinesh Kumbhare",
        "Kang Kim",
        "Ajay Wasan",
        "Jiantao Pu"
      ],
      "abstract": "We introduce a novel segmentation-aware joint training framework called\ngenerative reinforcement network (GRN) that integrates segmentation loss\nfeedback to optimize both image generation and segmentation performance in a\nsingle stage. An image enhancement technique called segmentation-guided\nenhancement (SGE) is also developed, where the generator produces images\ntailored specifically for the segmentation model. Two variants of GRN were also\ndeveloped, including GRN for sample-efficient learning (GRN-SEL) and GRN for\nsemi-supervised learning (GRN-SSL). GRN's performance was evaluated using a\ndataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The\nannotations included six anatomical structures: dermis, superficial fat,\nsuperficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and\nmuscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up\nto 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient\n(DSC) compared to models trained on fully labeled datasets. GRN-SEL alone\nreduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling\nrequirements by 70%, and GRN-SSL alone by 60%, all while maintaining\nperformance comparable to fully supervised models. These findings suggest the\neffectiveness of the GRN framework in optimizing segmentation performance with\nsignificantly less labeled data, offering a scalable and efficient solution for\nultrasound image analysis and reducing the burdens associated with data\nannotation.",
      "tldr_zh": "本研究提出了一种名为 Segmentation-Aware Generative Reinforcement Network (GRN) 的新框架，用于在 3-D Ultrasound Images 中进行组织层分割，以评估慢性低背痛 (cLBP)。GRN 通过整合分割损失反馈在单阶段优化图像生成和分割性能，并引入 Segmentation-Guided Enhancement (SGE) 技术，使生成器产生针对分割模型的图像；框架还包括两个变体：GRN for Sample-Efficient Learning (GRN-SEL) 和 GRN for Semi-Supervised Learning (GRN-SSL)。在包含 69 个 3D 超声扫描数据集上进行评估，结果显示 GRN-SEL 与 SGE 结合可减少 70% 的标注工作量，同时 Dice Similarity Coefficient (DSC) 提高 1.98%。总体而言，该框架显著降低了标注需求，同时维持与全监督模型相当的性能，为高效的超声图像分析提供可扩展解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17690v2",
      "published_date": "2025-01-29 14:58:48 UTC",
      "updated_date": "2025-04-30 14:19:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:32:39.241487"
    },
    {
      "arxiv_id": "2501.17688v3",
      "title": "ContourFormer: Real-Time Contour-Based End-to-End Instance Segmentation Transformer",
      "title_zh": "ContourFormer：实时基于轮廓的端到端实例分割 Transformer",
      "authors": [
        "Weiwei Yao",
        "Chen Li",
        "Minjun Xiong",
        "Wenbo Dong",
        "Hao Chen",
        "Xiong Xiao"
      ],
      "abstract": "This paper presents Contourformer, a real-time contour-based instance\nsegmentation algorithm. The method is fully based on the DETR paradigm and\nachieves end-to-end inference through iterative and progressive mechanisms to\noptimize contours. To improve efficiency and accuracy, we develop two novel\ntechniques: sub-contour decoupling mechanisms and contour fine-grained\ndistribution refinement. In the sub-contour decoupling mechanism, we propose a\ndeformable attention-based module that adaptively selects sampling regions\nbased on the current predicted contour, enabling more effective capturing of\nobject boundary information. Additionally, we design a multi-stage optimization\nprocess to enhance segmentation precision by progressively refining\nsub-contours. The contour fine-grained distribution refinement technique aims\nto further improve the ability to express fine details of contours. These\ninnovations enable Contourformer to achieve stable and precise segmentation for\neach instance while maintaining real-time performance. Extensive experiments\ndemonstrate the superior performance of Contourformer on multiple benchmark\ndatasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations\nand comparisons with existing state-of-the-art methods, showing significant\nimprovements in both accuracy and inference speed. This work provides a new\nsolution for contour-based instance segmentation tasks and lays a foundation\nfor future research, with the potential to become a strong baseline method in\nthis field.",
      "tldr_zh": "本文提出 Contourformer，一种基于 DETR 范式的实时轮廓实例分割 Transformer 算法，通过端到端推理和迭代机制优化轮廓以实现高效分割。关键创新包括 sub-contour decoupling mechanisms（使用 deformable attention 模块适应性选择采样区域并多阶段细化子轮廓）和 contour fine-grained distribution refinement（提升轮廓细细节表达能力），从而实现稳定、精确的实例分割。实验在 SBD、COCO 和 KINS 等基准数据集上表明，Contourformer 在准确性和推理速度上显著优于现有方法，提供了一个新的轮廓实例分割解决方案，并作为潜在的强基线。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17688v3",
      "published_date": "2025-01-29 14:56:27 UTC",
      "updated_date": "2025-04-15 01:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:34:50.891287"
    },
    {
      "arxiv_id": "2502.17445v1",
      "title": "Interpretable Dual-Filter Fuzzy Neural Networks for Affective Brain-Computer Interfaces",
      "title_zh": "可解释双滤波器",
      "authors": [
        "Xiaowei Jiang",
        "Yanan Chen",
        "Nikhil Ranjan Pal",
        "Yu-Cheng Chang",
        "Yunkai Yang",
        "Thomas Do",
        "Chin-Teng Lin"
      ],
      "abstract": "Fuzzy logic provides a robust framework for enhancing explainability,\nparticularly in domains requiring the interpretation of complex and ambiguous\nsignals, such as brain-computer interface (BCI) systems. Despite significant\nadvances in deep learning, interpreting human emotions remains a formidable\nchallenge. In this work, we present iFuzzyAffectDuo, a novel computational\nmodel that integrates a dual-filter fuzzy neural network architecture for\nimproved detection and interpretation of emotional states from neuroimaging\ndata. The model introduces a new membership function (MF) based on the Laplace\ndistribution, achieving superior accuracy and interpretability compared to\ntraditional approaches. By refining the extraction of neural signals associated\nwith specific emotions, iFuzzyAffectDuo offers a human-understandable framework\nthat unravels the underlying decision-making processes. We validate our\napproach across three neuroimaging datasets using functional Near-Infrared\nSpectroscopy (fNIRS) and Electroencephalography (EEG), demonstrating its\npotential to advance affective computing. These findings open new pathways for\nunderstanding the neural basis of emotions and their application in enhancing\nhuman-computer interaction.",
      "tldr_zh": "本文提出 iFuzzyAffectDuo，一种新型计算模型，整合双过滤器模糊神经网络架构，用于从神经影像数据中检测和解释情感状态，从而提升脑机接口（BCI）的解释性。模型引入基于 Laplace 分布的新的 Membership Function (MF)，相比传统方法显著提高了准确性和可解释性，并通过优化神经信号提取揭示决策过程。在使用功能近红外光谱（fNIRS）和脑电图（EEG）的三个数据集上验证，该方法为情感计算和人机交互提供了新途径。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.HC",
        "q-bio.NC"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17445v1",
      "published_date": "2025-01-29 14:31:57 UTC",
      "updated_date": "2025-01-29 14:31:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:35:02.214046"
    },
    {
      "arxiv_id": "2501.17905v2",
      "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
      "title_zh": "DReSS：数据驱动的正则化结构化精简方法，用于大型语言模型",
      "authors": [
        "Mingkuan Feng",
        "Jinyang Wu",
        "Shuai Zhang",
        "Pengpeng Shao",
        "Ruihan Jin",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Feihu Che"
      ],
      "abstract": "Large language models (LLMs) have achieved significant progress across\nvarious domains, but their increasing scale results in high computational and\nmemory costs. Recent studies have revealed that LLMs exhibit sparsity,\nproviding the potential to reduce model size through pruning techniques.\nHowever, existing pruning methods typically follow a prune-then-finetune\nparadigm. Since the pruned components still contain valuable information, their\ndirect removal often leads to irreversible performance degradation, imposing a\nsubstantial computational burden to recover performance during finetuning. In\nthis paper, we propose a novel paradigm that first applies regularization, then\nprunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a\nsimple and effective Data-driven Regularized Structured Streamlining method for\nLLMs. By leveraging a small amount of data to regularize the components to be\npruned, DReSS explicitly transfers the important information to the remaining\nparts of the model in advance. Compared to direct pruning, this can reduce the\ninformation loss caused by parameter removal, thereby enhancing its language\nmodeling capabilities. Experimental results demonstrate that DReSS\nsignificantly outperforms existing pruning methods even under extreme pruning\nratios, significantly reducing latency and increasing throughput.",
      "tldr_zh": "这篇论文提出 DReSS，一种数据驱动的正则化结构化精简方法，用于优化大型语言模型 (LLMs)，以解决模型规模导致的高计算和内存成本问题。不同于传统的先修剪后微调范式，DReSS 先通过少量数据对要修剪的组件进行正则化，将重要信息转移到模型剩余部分，从而减少参数移除造成的性能损失。实验结果显示，DReSS 在极端修剪比例下显著优于现有 pruning 方法，能降低延迟、提高吞吐量，并增强 LLMs 的语言建模能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17905v2",
      "published_date": "2025-01-29 14:28:11 UTC",
      "updated_date": "2025-02-10 04:07:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:33:13.756794"
    },
    {
      "arxiv_id": "2502.00055v1",
      "title": "Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Ljubisa Bojic",
        "Zorica Dodevska",
        "Yashar Deldjoo",
        "Nenad Pantelic"
      ],
      "abstract": "Given the exponential advancement in AI technologies and the potential\nescalation of harmful effects from recommendation systems, it is crucial to\nsimulate and evaluate these effects early on. Doing so can help prevent\npossible damage to both societies and technology companies. This paper\nintroduces the Recommender Systems LLMs Playground (RecSysLLMsP), a novel\nsimulation framework leveraging Large Language Models (LLMs) to explore the\nimpacts of different content recommendation setups on user engagement and\npolarization in social networks. By creating diverse AI agents (AgentPrompts)\nwith descriptive, static, and dynamic attributes, we assess their autonomous\nbehaviour across three scenarios: Plurality, Balanced, and Similarity. Our\nfindings reveal that the Similarity Scenario, which aligns content with user\npreferences, maximizes engagement while potentially fostering echo chambers.\nConversely, the Plurality Scenario promotes diverse interactions but produces\nmixed engagement results. Our study emphasizes the need for a careful balance\nin recommender system designs to enhance user satisfaction while mitigating\nsocietal polarization. It underscores the unique value and challenges of\nincorporating LLMs into simulation environments. The benefits of RecSysLLMsP\nlie in its potential to calculate polarization effects, which is crucial for\nassessing societal impacts and determining user engagement levels with diverse\nrecommender system setups. This advantage is essential for developing and\nmaintaining a successful business model for social media companies. However,\nthe study's limitations revolve around accurately emulating reality. Future\nefforts should validate the similarity in behaviour between real humans and\nAgentPrompts and establish metrics for measuring polarization scores.",
      "tldr_zh": "本研究引入了 Recommender Systems LLMs Playground (RecSysLLMsP)，一个基于 Large Language Models (LLMs) 的模拟框架，用于评估不同推荐系统设置对社交网络中用户参与度和极化 (polarization) 的影响。通过创建多样化的 AI 代理 (AgentPrompts) 模拟用户行为，该框架评估了 Plurality、Balanced 和 Similarity 三种场景，结果显示 Similarity 场景能最大化用户 engagement，但可能加剧回音室效应，而 Plurality 场景促进多样互动却导致参与度不稳定。该工作强调在推荐系统设计中需平衡用户满意度与社会极化风险，并指出未来需改进模拟的真实性以验证 AI 代理与真实人类行为的相似性。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.SI",
      "comment": "8 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.00055v1",
      "published_date": "2025-01-29 14:23:34 UTC",
      "updated_date": "2025-01-29 14:23:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:33:26.157348"
    },
    {
      "arxiv_id": "2501.17665v1",
      "title": "Planning with Vision-Language Models and a Use Case in Robot-Assisted Teaching",
      "title_zh": "基于视觉语言模型的规划以及在机器人辅助教学中的一个",
      "authors": [
        "Xuzhe Dang",
        "Lada Kudláčková",
        "Stefan Edelkamp"
      ],
      "abstract": "Automating the generation of Planning Domain Definition Language (PDDL) with\nLarge Language Model (LLM) opens new research topic in AI planning,\nparticularly for complex real-world tasks. This paper introduces Image2PDDL, a\nnovel framework that leverages Vision-Language Models (VLMs) to automatically\nconvert images of initial states and descriptions of goal states into PDDL\nproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL\naddresses key challenges in bridging perceptual understanding with symbolic\nplanning, reducing the expertise required to create structured problem\ninstances, and improving scalability across tasks of varying complexity. We\nevaluate the framework on various domains, including standard planning domains\nlike blocksworld and sliding tile puzzles, using datasets with multiple\ndifficulty levels. Performance is assessed on syntax correctness, ensuring\ngrammar and executability, and content correctness, verifying accurate state\nrepresentation in generated PDDL problems. The proposed approach demonstrates\npromising results across diverse task complexities, suggesting its potential\nfor broader applications in AI planning. We will discuss a potential use case\nin robot-assisted teaching of students with Autism Spectrum Disorder.",
      "tldr_zh": "本研究提出Image2PDDL框架，利用Vision-Language Models (VLMs)自动将图像的初始状态和目标状态描述转换为PDDL问题，从而桥接感知理解与符号规划。框架通过提供PDDL域来减少创建结构化问题实例的专业知识需求，并提升任务复杂性的可扩展性。在各种领域如blocksworld和sliding tile puzzles的评估中，Image2PDDL在语法正确性和内容准确性方面表现出色，显示了良好的性能。论文还讨论了其在机器人辅助教学中的潜在应用，例如帮助自闭症谱系障碍学生的教学。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17665v1",
      "published_date": "2025-01-29 14:04:54 UTC",
      "updated_date": "2025-01-29 14:04:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:33:38.488102"
    },
    {
      "arxiv_id": "2501.17654v1",
      "title": "Exploring Vision Language Models for Multimodal and Multilingual Stance Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jake Vasilakes",
        "Carolina Scarton",
        "Zhixue Zhao"
      ],
      "abstract": "Social media's global reach amplifies the spread of information, highlighting\nthe need for robust Natural Language Processing tasks like stance detection\nacross languages and modalities. Prior research predominantly focuses on\ntext-only inputs, leaving multimodal scenarios, such as those involving both\nimages and text, relatively underexplored. Meanwhile, the prevalence of\nmultimodal posts has increased significantly in recent years. Although\nstate-of-the-art Vision-Language Models (VLMs) show promise, their performance\non multimodal and multilingual stance detection tasks remains largely\nunexamined. This paper evaluates state-of-the-art VLMs on a newly extended\ndataset covering seven languages and multimodal inputs, investigating their use\nof visual cues, language-specific performance, and cross-modality interactions.\nOur results show that VLMs generally rely more on text than images for stance\ndetection and this trend persists across languages. Additionally, VLMs rely\nsignificantly more on text contained within the images than other visual\ncontent. Regarding multilinguality, the models studied tend to generate\nconsistent predictions across languages whether they are explicitly\nmultilingual or not, although there are outliers that are incongruous with\nmacro F1, language support, and model size.",
      "tldr_zh": "这篇论文探讨了视觉语言模型（VLMs）在多模态和多语言立场检测中的性能，针对社交媒体上图像和文本结合的全球信息传播问题。研究者使用一个扩展数据集，涵盖七种语言和多模态输入，评估了 VLMs 对视觉线索、语言特定表现以及跨模态交互的依赖。结果表明，VLMs 更依赖文本而非图像，尤其依赖图像中的文本内容，且在多语言任务上预测一致，但受模型大小和语言支持等因素影响而出现异常。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to the International AAAI Conference on Web and Social\n  Media (ICWSM) 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17654v1",
      "published_date": "2025-01-29 13:39:53 UTC",
      "updated_date": "2025-01-29 13:39:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:35:50.402049"
    },
    {
      "arxiv_id": "2501.17643v1",
      "title": "Tonguescape: Exploring Language Models Understanding of Vowel Articulation",
      "title_zh": "Tonguescape：探索语言模型对元音发音的理解",
      "authors": [
        "Haruki Sakajo",
        "Yusuke Sakai",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "abstract": "Vowels are primarily characterized by tongue position. Humans have discovered\nthese features of vowel articulation through their own experience and explicit\nobjective observation such as using MRI. With this knowledge and our\nexperience, we can explain and understand the relationship between tongue\npositions and vowels, and this knowledge is helpful for language learners to\nlearn pronunciation. Since language models (LMs) are trained on a large amount\nof data that includes linguistic and medical fields, our preliminary studies\nindicate that an LM is able to explain the pronunciation mechanisms of vowels.\nHowever, it is unclear whether multi-modal LMs, such as vision LMs, align\ntextual information with visual information. One question arises: do LMs\nassociate real tongue positions with vowel articulation? In this study, we\ncreated video and image datasets from the existing real-time MRI dataset and\ninvestigated whether LMs can understand vowel articulation based on tongue\npositions using vision-based information. Our findings suggest that LMs exhibit\npotential for understanding vowels and tongue positions when reference examples\nare provided while they have difficulties without them. Our code for dataset\nbuilding is available on GitHub.",
      "tldr_zh": "本研究探讨了语言模型（LMs）对元音发音机制的理解，特别是元音主要由舌位（如通过 MRI 观察）决定的特性。研究者使用现有实时 MRI 数据创建视频和图像数据集，并测试多模态 LMs 是否能将文本信息与视觉舌位信息对齐。结果显示，LMs 在提供参考示例时表现出理解元音和舌位发音的潜力，但缺乏示例时表现较差；相关数据集构建代码已在 GitHub 上开源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17643v1",
      "published_date": "2025-01-29 13:25:20 UTC",
      "updated_date": "2025-01-29 13:25:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:36:01.765954"
    },
    {
      "arxiv_id": "2501.18645v2",
      "title": "Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Manish Sanwal"
      ],
      "abstract": "Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to\nprovide step-by-step rationales, improving performance on complex tasks.\nDespite its benefits, vanilla CoT often fails to fully verify intermediate\ninferences and can produce misleading explanations. In this work, we propose\nLayered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that\nsystematically segments the reasoning process into multiple layers, each\nsubjected to external checks and optional user feedback. We expand on the key\nconcepts, present three scenarios -- medical triage, financial risk assessment,\nand agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT\nin terms of transparency, correctness, and user engagement. By integrating\nreferences from recent arXiv papers on interactive explainability, multi-agent\nframeworks, and agent-based collaboration, we illustrate how Layered-CoT paves\nthe way for more reliable and grounded explanations in high-stakes domains.",
      "tldr_zh": "本文提出 Layered Chain-of-Thought (Layered-CoT) Prompting 框架，用于多代理 Large Language Models (LLMs) 系统，以提升解释性和可靠性。该框架将链式思维 (Chain-of-Thought) 推理过程分层，每个层接受外部检查和可选用户反馈，从而解决普通 CoT 在验证中间推理和避免误导性解释方面的不足。在医疗分流、财务风险评估和敏捷工程等场景中，Layered-CoT 表现出更高的透明度、正确性和用户参与度，并通过整合 arXiv 论文中的多代理框架和交互式可解释性，展示了其在高风险领域提供可靠解释的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18645v2",
      "published_date": "2025-01-29 13:21:09 UTC",
      "updated_date": "2025-02-03 15:51:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:36:13.895775"
    },
    {
      "arxiv_id": "2501.17635v2",
      "title": "In-Context Meta LoRA Generation",
      "title_zh": "基于上下文的元 LoRA 生成",
      "authors": [
        "Yihua Shao",
        "Minxi Yan",
        "Yang Liu",
        "Siyu Chen",
        "Wenjie Chen",
        "Xinwei Long",
        "Ziyang Yan",
        "Lei Li",
        "Chenyu Zhang",
        "Nicu Sebe",
        "Hao Tang",
        "Yan Wang",
        "Hao Zhao",
        "Mengzhu Wang",
        "Jingcai Guo"
      ],
      "abstract": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
      "tldr_zh": "本文提出 In-Context Meta LoRA (ICM-LoRA)，一种高效方法，用于多任务场景下的大语言模型 (LLMs) 的任务特定定制，解决传统 Low-rank Adaptation (LoRA) 在存储和推理效率上的不足。ICM-LoRA 通过训练 Conditional Variational Autoencoder (CVAE) 以任务描述作为输入生成任务相关的 LoRA 权重，并结合 in-context meta-learning 捕捉任务间关系和参数分布，从而无需额外微调即可创建专用模型。实验表明，该方法比现有参数重建技术更准确，且仅占用 283MB 存储空间，相当于原 LoRA 的 1%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17635v2",
      "published_date": "2025-01-29 13:12:01 UTC",
      "updated_date": "2025-01-30 17:59:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:36:25.960844"
    },
    {
      "arxiv_id": "2501.17629v1",
      "title": "The Imitation Game According To Turing",
      "title_zh": "翻译失败",
      "authors": [
        "Sharon Temtsin",
        "Diane Proudfoot",
        "David Kaber",
        "Christoph Bartneck"
      ],
      "abstract": "The current cycle of hype and anxiety concerning the benefits and risks to\nhuman society of Artificial Intelligence is fuelled, not only by the increasing\nuse of generative AI and other AI tools by the general public, but also by\nclaims made on behalf of such technology by popularizers and scientists. In\nparticular, recent studies have claimed that Large Language Models (LLMs) can\npass the Turing Test-a goal for AI since the 1950s-and therefore can \"think\".\nLarge-scale impacts on society have been predicted as a result. Upon detailed\nexamination, however, none of these studies has faithfully applied Turing's\noriginal instructions. Consequently, we conducted a rigorous Turing Test with\nGPT-4-Turbo that adhered closely to Turing's instructions for a three-player\nimitation game. We followed established scientific standards where Turing's\ninstructions were ambiguous or missing. For example, we performed a\nComputer-Imitates-Human Game (CIHG) without constraining the time duration and\nconducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one\nparticipant correctly identified the LLM, showing that one of today's most\nadvanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent\nextravagant claims for such models are unsupported, and do not warrant either\noptimism or concern about the social impact of thinking machines.",
      "tldr_zh": "这篇论文批判了当前关于大型语言模型 (Large Language Models, LLMs) 通过图灵测试 (Turing Test) 并能“思考”的夸大声明，强调这些研究未严格遵守图灵的原指令。作者使用 GPT-4-Turbo 进行了严格的三玩家模仿游戏，包括 Computer-Imitates-Human Game (CIHG) 和 Man-Imitates-Woman Game (MIWG)，并在模糊处采用科学标准进行测试。结果显示，几乎所有参与者都能正确识别 LLM，证明该模型无法通过严格的图灵测试。论文结论是，这些模型的 extravagant claims 缺乏支持，不应引发对 AI 社会影响的过度乐观或担忧。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17629v1",
      "published_date": "2025-01-29 13:08:17 UTC",
      "updated_date": "2025-01-29 13:08:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:36:38.321834"
    },
    {
      "arxiv_id": "2501.17903v2",
      "title": "Free Agent in Agent-Based Mixture-of-Experts Generative AI Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Jung-Hua Liu"
      ],
      "abstract": "Multi-agent systems commonly distribute tasks among specialized, autonomous\nagents, yet they often lack mechanisms to replace or reassign underperforming\nagents in real time. Inspired by the free-agency model of Major League\nBaseball, the Reinforcement Learning Free Agent (RLFA) algorithm introduces a\nreward-based mechanism to detect and remove agents exhibiting persistent\nunderperformance and seamlessly insert more capable ones. Each agent internally\nuses a mixture-of-experts (MoE) approach, delegating incoming tasks to\nspecialized sub-models under the guidance of a gating function. A primary use\ncase is fraud detection, where RLFA promptly swaps out an agent whose detection\naccuracy dips below a preset threshold. A new agent is tested in a probationary\nmode, and upon demonstrating superior performance, fully replaces the\nunderperformer. This dynamic, free-agency cycle ensures sustained accuracy,\nquicker adaptation to emerging threats, and minimal disruption to ongoing\noperations. By continually refreshing its roster of agents, the system fosters\nongoing improvements and more resilient collaboration in multi-agent Generative\nAI environments.",
      "tldr_zh": "该论文提出 Reinforcement Learning Free Agent (RLFA) 算法，受棒球自由球员模式启发，用于多智能体系统中的动态优化。RLFA 通过奖励机制检测并移除表现不佳的智能体，同时插入更高效的替代者，每个智能体内部采用 Mixture-of-Experts (MoE) 方法，由 gating function 引导任务分配。实验表明，该框架在欺诈检测等应用中实现了持续准确性、快速适应新兴威胁，并最小化操作中断，从而提升多智能体生成 AI 环境的整体韧性和协作效率。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17903v2",
      "published_date": "2025-01-29 13:00:22 UTC",
      "updated_date": "2025-02-10 16:13:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:36:50.729174"
    },
    {
      "arxiv_id": "2501.17612v1",
      "title": "VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and Conditional Flow Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Ha-Yeong Choi",
        "Jaehan Park"
      ],
      "abstract": "Despite remarkable advancements in recent voice conversion (VC) systems,\nenhancing speaker similarity in zero-shot scenarios remains challenging. This\nchallenge arises from the difficulty of generalizing and adapting speaker\ncharacteristics in speech within zero-shot environments, which is further\ncomplicated by mismatch between the training and inference processes. To\naddress these challenges, we propose VoicePrompter, a robust zero-shot VC model\nthat leverages in-context learning with voice prompts. VoicePrompter is\ncomposed of (1) a factorization method that disentangles speech components and\n(2) a DiT-based conditional flow matching (CFM) decoder that conditions on\nthese factorized features and voice prompts. Additionally, (3) latent mixup is\nused to enhance in-context learning by combining various speaker features. This\napproach improves speaker similarity and naturalness in zero-shot VC by\napplying mixup to latent representations. Experimental results demonstrate that\nVoicePrompter outperforms existing zero-shot VC systems in terms of speaker\nsimilarity, speech intelligibility, and audio quality. Our demo is available at\n\\url{https://hayeong0.github.io/VoicePrompter-demo/}.",
      "tldr_zh": "该研究提出 VoicePrompter，一种鲁棒的 zero-shot voice conversion (VC) 模型，通过 voice prompts 和 in-context learning 解决说话者相似度在零-shot 环境中的挑战。模型的核心组件包括语音组件的因子分解方法、基于 DiT 的 conditional flow matching (CFM) 解码器，以及 latent mixup 技术，用于结合多种说话者特征以提升语音自然性和相似度。实验结果显示，VoicePrompter 在说话者相似度、语音可懂度和音频质量方面均优于现有 zero-shot VC 系统。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17612v1",
      "published_date": "2025-01-29 12:34:58 UTC",
      "updated_date": "2025-01-29 12:34:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:37:01.104759"
    },
    {
      "arxiv_id": "2501.17581v2",
      "title": "CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Amey Hengle",
        "Aswini Kumar",
        "Anil Bandhakavi",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Counterspeech has emerged as a popular and effective strategy for combating\nonline hate speech, sparking growing research interest in automating its\ngeneration using language models. However, the field still lacks standardised\nevaluation protocols and reliable automated evaluation metrics that align with\nhuman judgement. Current automatic evaluation methods, primarily based on\nsimilarity metrics, do not effectively capture the complex and independent\nattributes of counterspeech quality, such as contextual relevance,\naggressiveness, or argumentative coherence. This has led to an increased\ndependency on labor-intensive human evaluations to assess automated\ncounter-speech generation methods. To address these challenges, we introduce\nCSEval, a novel dataset and framework for evaluating counterspeech quality\nacross four dimensions: contextual-relevance, aggressiveness,\nargument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated\nCOT for Counterspeech Evaluation (Auto-CSEval), a prompt-based method with\nauto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large\nlanguage models. Our experiments show that Auto-CSEval outperforms traditional\nmetrics like ROUGE, METEOR, and BertScore in correlating with human judgement,\nindicating a significant improvement in automated counterspeech evaluation.",
      "tldr_zh": "该论文针对在线仇恨言论的 counterspeech 生成问题，提出 CSEval 框架，这是一个自动化、多维度且无需参考答案的评估方法，以解决现有相似性指标（如 ROUGE、METEOR 和 BertScore）无法捕捉 counterspeech 属性（如 contextual-relevance、aggressiveness、argument-coherence 和 suitableness）的局限性。研究引入了一个新数据集和 Auto-CSEval 方法，该方法基于 auto-calibrated chain-of-thought (CoT) 提示，利用大型语言模型 (LLMs) 进行多维度评分。实验结果表明，Auto-CSEval 在与人类判断的相关性上显著优于传统指标，为 counterspeech 的可靠自动化评估提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.17581v2",
      "published_date": "2025-01-29 11:38:29 UTC",
      "updated_date": "2025-02-09 17:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:37:14.423463"
    },
    {
      "arxiv_id": "2501.17578v1",
      "title": "Music2Latent2: Audio Compression with Summary Embeddings and Autoregressive Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Marco Pasini",
        "Stefan Lattner",
        "George Fazekas"
      ],
      "abstract": "Efficiently compressing high-dimensional audio signals into a compact and\ninformative latent space is crucial for various tasks, including generative\nmodeling and music information retrieval (MIR). Existing audio autoencoders,\nhowever, often struggle to achieve high compression ratios while preserving\naudio fidelity and facilitating efficient downstream applications. We introduce\nMusic2Latent2, a novel audio autoencoder that addresses these limitations by\nleveraging consistency models and a novel approach to representation learning\nbased on unordered latent embeddings, which we call summary embeddings. Unlike\nconventional methods that encode local audio features into ordered sequences,\nMusic2Latent2 compresses audio signals into sets of summary embeddings, where\neach embedding can capture distinct global features of the input sample. This\nenables to achieve higher reconstruction quality at the same compression ratio.\nTo handle arbitrary audio lengths, Music2Latent2 employs an autoregressive\nconsistency model trained on two consecutive audio chunks with causal masking,\nensuring coherent reconstruction across segment boundaries. Additionally, we\npropose a novel two-step decoding procedure that leverages the denoising\ncapabilities of consistency models to further refine the generated audio at no\nadditional cost. Our experiments demonstrate that Music2Latent2 outperforms\nexisting continuous audio autoencoders regarding audio quality and performance\non downstream tasks. Music2Latent2 paves the way for new possibilities in audio\ncompression.",
      "tldr_zh": "该论文提出Music2Latent2，一种新型音频自编码器，用于高效压缩音频信号到紧凑的潜在空间，同时提升音频保真度和下游任务性能。Music2Latent2 通过summary embeddings将音频表示为一组无序的全局特征嵌入，而不是传统的有序序列，从而在相同压缩比下实现更高重建质量；它还采用autoregressive consistency model和causal masking来处理任意长度音频，确保段边界的一致性。实验结果显示，Music2Latent2在音频质量和音乐信息检索(MIR)任务上优于现有连续音频自编码器，为音频压缩领域开辟新可能性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17578v1",
      "published_date": "2025-01-29 11:34:19 UTC",
      "updated_date": "2025-01-29 11:34:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:37:25.654713"
    },
    {
      "arxiv_id": "2501.17567v2",
      "title": "Exploring the Potential of Wireless-enabled Multi-Chip AI Accelerators",
      "title_zh": "探索无线启用多芯片 AI 加速器的潜力",
      "authors": [
        "Emmanuel Irabor",
        "Mariam Musavi",
        "Abhijit Das",
        "Sergi Abadal"
      ],
      "abstract": "The insatiable appetite of Artificial Intelligence (AI) workloads for\ncomputing power is pushing the industry to develop faster and more efficient\naccelerators. The rigidity of custom hardware, however, conflicts with the need\nfor scalable and versatile architectures capable of catering to the needs of\nthe evolving and heterogeneous pool of Machine Learning (ML) models in the\nliterature. In this context, multi-chiplet architectures assembling multiple\n(perhaps heterogeneous) accelerators are an appealing option that is\nunfortunately hindered by the still rigid and inefficient chip-to-chip\ninterconnects. In this paper, we explore the potential of wireless technology\nas a complement to existing wired interconnects in this multi-chiplet approach.\nUsing an evaluation framework from the state-of-the-art, we show that wireless\ninterconnects can lead to speedups of 10% on average and 20% maximum. We also\nhighlight the importance of load balancing between the wired and wireless\ninterconnects, which will be further explored in future work.",
      "tldr_zh": "本论文探讨了在多-chiplet 架构中，使用无线技术补充现有有线互连，以解决 AI 加速器在处理异构机器学习模型时的刚性和效率问题。研究通过先进的评估框架评估了无线互连的潜力，结果显示其可实现平均 10% 的加速效果，最高达 20%。此外，论文强调了有线和无线互连间负载平衡的关键作用，并指出这将是未来工作的重点。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Accepted in AccML @ HiPEAC 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17567v2",
      "published_date": "2025-01-29 11:00:09 UTC",
      "updated_date": "2025-04-28 05:42:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:37:37.077690"
    },
    {
      "arxiv_id": "2501.17559v1",
      "title": "Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research",
      "title_zh": "翻译失败",
      "authors": [
        "Shuxin Zhuang",
        "Shuxin Li",
        "Tianji Yang",
        "Muheng Li",
        "Xianjie Shi",
        "Bo An",
        "Youzhi Zhang"
      ],
      "abstract": "After the great achievement of solving two-player zero-sum games, more and\nmore AI researchers focus on solving multiplayer games. To facilitate the\ndevelopment of designing efficient learning algorithms for solving multiplayer\ngames, we propose a multiplayer game platform for solving Urban Network\nSecurity Games (\\textbf{UNSG}) that model real-world scenarios. That is,\npreventing criminal activity is a highly significant responsibility assigned to\npolice officers in cities, and police officers have to allocate their limited\nsecurity resources to interdict the escaping criminal when a crime takes place\nin a city. This interaction between multiple police officers and the escaping\ncriminal can be modeled as a UNSG. The variants of UNSGs can model different\nreal-world settings, e.g., whether real-time information is available or not,\nand whether police officers can communicate or not. The main challenges of\nsolving this game include the large size of the game and the co-existence of\ncooperation and competition. While previous efforts have been made to tackle\nUNSGs, they have been hampered by performance and scalability issues.\nTherefore, we propose an open-source UNSG platform (\\textbf{GraphChase}) for\ndesigning efficient learning algorithms for solving UNSGs. Specifically,\nGraphChase offers a unified and flexible game environment for modeling various\nvariants of UNSGs, supporting the development, testing, and benchmarking of\nalgorithms. We believe that GraphChase not only facilitates the development of\nefficient algorithms for solving real-world problems but also paves the way for\nsignificant advancements in algorithmic development for solving general\nmultiplayer games.",
      "tldr_zh": "本研究提出GraphChase，一个开源平台，用于解决Urban Network Security Games (UNSG)，这是一种模拟现实场景（如警察拦截逃犯）的多玩家游戏模型。UNSG面临的主要挑战包括游戏规模庞大以及合作与竞争的共存，GraphChase通过提供统一的灵活环境，支持各种UNSG变体（如实时信息可用性或警察通信选项），便于开发、测试和基准AI学习算法。与以往方法相比，该平台显著提升了算法的性能和可扩展性，最终有助于推进AI在多玩家游戏和真实世界问题中的应用。",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17559v1",
      "published_date": "2025-01-29 10:46:57 UTC",
      "updated_date": "2025-01-29 10:46:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:37:48.397023"
    },
    {
      "arxiv_id": "2501.17555v1",
      "title": "An Exceptional Dataset For Rare Pancreatic Tumor Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Wenqi Li",
        "Yingli Chen",
        "Keyang Zhou",
        "Xiaoxiao Hu",
        "Zilu Zheng",
        "Yue Yan",
        "Xinpeng Zhang",
        "Wei Tang",
        "Zhenxing Qian"
      ],
      "abstract": "Pancreatic NEuroendocrine Tumors (pNETs) are very rare endocrine neoplasms\nthat account for less than 5% of all pancreatic malignancies, with an incidence\nof only 1-1.5 cases per 100,000. Early detection of pNETs is critical for\nimproving patient survival, but the rarity of pNETs makes segmenting them from\nCT a very challenging problem. So far, there has not been a dataset\nspecifically for pNETs available to researchers. To address this issue, we\npropose a pNETs dataset, a well-annotated Contrast-Enhanced Computed Tomography\n(CECT) dataset focused exclusively on Pancreatic Neuroendocrine Tumors,\ncontaining data from 469 patients. This is the first dataset solely dedicated\nto pNETs, distinguishing it from previous collections. Additionally, we provide\nthe baseline detection networks with a new slice-wise weight loss function\ndesigned for the UNet-based model, improving the overall pNET segmentation\nperformance. We hope that our dataset can enhance the understanding and\ndiagnosis of pNET Tumors within the medical community, facilitate the\ndevelopment of more accurate diagnostic tools, and ultimately improve patient\noutcomes and advance the field of oncology.",
      "tldr_zh": "本研究针对胰腺神经内分泌肿瘤 (pNETs) 的稀有性及其早期检测挑战，提出一个专属数据集，包含 469 名患者的对比增强 CT (CECT) 图像，这是首个专注于 pNETs 的数据集。数据集通过详细标注，帮助解决从 CT 图像中分割 pNETs 的难题。研究者还提供基于 UNet 的基线检测网络，并引入新的切片级权重损失函数，提高了 pNETs 分割性能。该数据集有望提升医疗社区对 pNETs 的理解，促进更准确的诊断工具开发，并改善患者预后和肿瘤学领域进展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17555v1",
      "published_date": "2025-01-29 10:43:07 UTC",
      "updated_date": "2025-01-29 10:43:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:38:01.046935"
    },
    {
      "arxiv_id": "2501.17546v1",
      "title": "Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant",
      "title_zh": "翻译失败",
      "authors": [
        "Gaole He",
        "Nilay Aishwarya",
        "Ujwal Gadiraju"
      ],
      "abstract": "Explainable artificial intelligence (XAI) methods are being proposed to help\ninterpret and understand how AI systems reach specific predictions. Inspired by\nprior work on conversational user interfaces, we argue that augmenting existing\nXAI methods with conversational user interfaces can increase user engagement\nand boost user understanding of the AI system. In this paper, we explored the\nimpact of a conversational XAI interface on users' understanding of the AI\nsystem, their trust, and reliance on the AI system. In comparison to an XAI\ndashboard, we found that the conversational XAI interface can bring about a\nbetter understanding of the AI system among users and higher user trust.\nHowever, users of both the XAI dashboard and conversational XAI interfaces\nshowed clear overreliance on the AI system. Enhanced conversations powered by\nlarge language model (LLM) agents amplified over-reliance. Based on our\nfindings, we reason that the potential cause of such overreliance is the\nillusion of explanatory depth that is concomitant with both XAI interfaces. Our\nfindings have important implications for designing effective conversational XAI\ninterfaces to facilitate appropriate reliance and improve human-AI\ncollaboration. Code can be found at\nhttps://github.com/delftcrowd/IUI2025_ConvXAI",
      "tldr_zh": "本研究探讨了对话式可解释人工智能（Conversational XAI）接口如何提升用户对AI系统的理解和信任，并通过实验比较了其与传统XAI仪表板的差异。结果显示，对话式XAI接口能显著提高用户理解和信任水平，但同时导致用户过度依赖AI系统，尤其在使用大型语言模型（LLM）增强的对话时。论文指出，这种过度依赖可能源于“解释深度幻觉”（illusion of explanatory depth），并为设计更有效的对话式XAI接口提供了建议，以促进适当的人类-AI合作。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "conditionally accepted to IUI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17546v1",
      "published_date": "2025-01-29 10:29:27 UTC",
      "updated_date": "2025-01-29 10:29:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:38:13.121866"
    },
    {
      "arxiv_id": "2501.17518v1",
      "title": "RegD: Hierarchical Embeddings via Distances over Geometric Regions",
      "title_zh": "RegD：通过几何区域上距离的分层嵌入",
      "authors": [
        "Hui Yang",
        "Jiaoyan Chen"
      ],
      "abstract": "Hierarchical data are common in many domains like life sciences and\ne-commerce, and their embeddings often play a critical role. Although\nhyperbolic embeddings offer a grounded approach to representing hierarchical\nstructures in low-dimensional spaces, their utility is hindered by optimization\ndifficulties in hyperbolic space and dependence on handcrafted structural\nconstraints. We propose RegD, a novel Euclidean framework that addresses these\nlimitations by representing hierarchical data as geometric regions with two new\nmetrics: (1) depth distance, which preserves the representational power of\nhyperbolic spaces for hierarchical data, and (2) boundary distance, which\nexplicitly encodes set-inclusion relationships between regions in a general\nway. Our empirical evaluation on diverse real-world datasets shows consistent\nperformance gains over state-of-the-art methods and demonstrates RegD's\npotential for broader applications beyond hierarchy alone tasks.",
      "tldr_zh": "该论文提出RegD，一种基于欧氏空间(Euclidean framework)的创新框架，用于处理层次化数据(hierarchical data)的嵌入问题，旨在克服hyperbolic embeddings的优化困难和对手工结构约束的依赖。RegD将数据表示为几何区域，并引入两种新指标：depth distance以保留层次结构的表示能力，以及boundary distance以明确编码区域间的集合包含关系。实验在多样真实数据集上显示，RegD比最先进方法性能显著提升，并扩展了其在非层次任务中的应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17518v1",
      "published_date": "2025-01-29 09:44:03 UTC",
      "updated_date": "2025-01-29 09:44:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:38:24.931892"
    },
    {
      "arxiv_id": "2501.17510v1",
      "title": "LLM Assistance for Pediatric Depression",
      "title_zh": "LLM 用于儿科抑郁症的辅助",
      "authors": [
        "Mariia Ignashina",
        "Paulina Bondaronek",
        "Dan Santel",
        "John Pestian",
        "Julia Ive"
      ],
      "abstract": "Traditional depression screening methods, such as the PHQ-9, are particularly\nchallenging for children in pediatric primary care due to practical\nlimitations. AI has the potential to help, but the scarcity of annotated\ndatasets in mental health, combined with the computational costs of training,\nhighlights the need for efficient, zero-shot approaches. In this work, we\ninvestigate the feasibility of state-of-the-art LLMs for depressive symptom\nextraction in pediatric settings (ages 6-24). This approach aims to complement\ntraditional screening and minimize diagnostic errors.\n  Our findings show that all LLMs are 60% more efficient than word match, with\nFlan leading in precision (average F1: 0.65, precision: 0.78), excelling in the\nextraction of more rare symptoms like \"sleep problems\" (F1: 0.92) and\n\"self-loathing\" (F1: 0.8). Phi strikes a balance between precision (0.44) and\nrecall (0.60), performing well in categories like \"Feeling depressed\" (0.69)\nand \"Weight change\" (0.78). Llama 3, with the highest recall (0.90),\novergeneralizes symptoms, making it less suitable for this type of analysis.\nChallenges include the complexity of clinical notes and overgeneralization from\nPHQ-9 scores. The main challenges faced by LLMs include navigating the complex\nstructure of clinical notes with content from different times in the patient\ntrajectory, as well as misinterpreting elevated PHQ-9 scores.\n  We finally demonstrate the utility of symptom annotations provided by Flan as\nfeatures in an ML algorithm, which differentiates depression cases from\ncontrols with high precision of 0.78, showing a major performance boost\ncompared to a baseline that does not use these features.",
      "tldr_zh": "本研究探讨了使用大型语言模型（LLM）辅助儿童（6-24 岁）抑郁症状提取，以克服传统方法如 PHQ-9 在儿科筛查中的实际挑战，并采用零样本方法来补充诊断并减少错误。结果显示，所有 LLM 比词匹配方法效率高 60%，其中 Flan 在精确度上表现最佳（平均 F1: 0.65，precision: 0.78），尤其擅长提取稀有症状如“sleep problems”（F1: 0.92）和“self-loathing”（F1: 0.8）；Phi 在 precision（0.44）和 recall（0.60）间平衡，而 Llama 3 虽有最高 recall（0.90），但因过度泛化不适合临床应用。最终，Flan 的症状注释作为机器学习（ML）算法特征，能以 precision 0.78 区分抑郁病例和对照组，显著提升基线性能，并突显了处理临床笔记复杂性和 PHQ-9 误解的挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17510v1",
      "published_date": "2025-01-29 09:27:27 UTC",
      "updated_date": "2025-01-29 09:27:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:38:38.546418"
    },
    {
      "arxiv_id": "2501.17507v1",
      "title": "Reflections on \"Can AI Understand Our Universe?\"",
      "title_zh": "对“人工智能是否能理解我们的宇宙？”的反思",
      "authors": [
        "Yu Wang"
      ],
      "abstract": "This article briefly discusses the philosophical and technical aspects of AI.\nIt focuses on two concepts of understanding: intuition and causality, and\nhighlights three AI technologies: Transformers, chain-of-thought reasoning, and\nmultimodal processing. We anticipate that in principle AI could form\nunderstanding, with these technologies representing promising advancements.",
      "tldr_zh": "这篇论文反思了 AI 是否能理解我们的宇宙，探讨了理解的哲学和技术层面，包括直觉和因果关系两个关键概念。\n论文强调了三种 AI 技术：Transformers、chain-of-thought reasoning 和 multimodal processing，作为促进 AI 理解的重要进展。\n作者认为，AI 原则上能够形成理解，这些技术代表了有前景的未来发展。",
      "categories": [
        "cs.AI",
        "astro-ph.HE",
        "astro-ph.IM"
      ],
      "primary_category": "cs.AI",
      "comment": "Invited talk at the 17th Marcel Grossmann Meeting, associated with\n  arXiv:2404.10019, to be published in the International Journal of Modern\n  Physics D",
      "pdf_url": "http://arxiv.org/pdf/2501.17507v1",
      "published_date": "2025-01-29 09:24:47 UTC",
      "updated_date": "2025-01-29 09:24:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:38:48.431881"
    },
    {
      "arxiv_id": "2501.17496v2",
      "title": "SemML: Enhancing Automata-Theoretic LTL Synthesis with Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Kretinsky",
        "Tobias Meggendorfer",
        "Maximilian Prokop",
        "Ashkan Zarkhah"
      ],
      "abstract": "Synthesizing a reactive system from specifications given in linear temporal\nlogic (LTL) is a classical problem, finding its applications in safety-critical\nsystems design. We present our tool SemML, which won this year's LTL\nrealizability tracks of SYNTCOMP, after years of domination by Strix. While\nboth tools are based on the automata-theoretic approach, ours relies heavily on\n(i) Semantic labelling, additional information of logical nature, coming from\nrecent LTL-to-automata translations and decorating the resulting parity game,\nand (ii) Machine Learning approaches turning this information into a guidance\noracle for on-the-fly exploration of the parity game (whence the name SemML).\nOur tool fills the missing gaps of previous suggestions to use such an oracle\nand provides an efficeint implementation with additional algorithmic\nimprovements. We evaluate SemML both on the entire set of SYNTCOMP as well as a\nsynthetic data set, compare it to Strix, and analyze the advantages and\nlimitations. As SemML solves more instances on SYNTCOMP and does so\nsignificantly faster on larger instances, this demonstrates for the first time\nthat machine-learning-aided approaches can out-perform state-of-the-art tools\nin real LTL synthesis.",
      "tldr_zh": "本论文介绍了 SemML，一种通过 Machine Learning 增强 automata-theoretic LTL 合成的工具，旨在解决线性时序逻辑 (LTL) 合成在安全关键系统设计中的挑战。SemML 结合语义标签（从最近的 LTL 到自动机的翻译中获取的逻辑信息）和机器学习指导预言机，实现 parity game 的实时探索，并进行了算法优化。与传统工具 Strix 相比，SemML 在 SYNTCOMP 基准测试中解决了更多实例，并在较大实例上显著更快，首次证明 Machine Learning 辅助方法可超越最先进工具。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17496v2",
      "published_date": "2025-01-29 09:06:19 UTC",
      "updated_date": "2025-04-17 13:54:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:39:02.242795"
    },
    {
      "arxiv_id": "2501.17493v1",
      "title": "Certifying Pareto-Optimality in Multi-Objective Maximum Satisfiability",
      "title_zh": "翻译失败",
      "authors": [
        "Christoph Jabs",
        "Jeremias Berg",
        "Bart Bogaerts",
        "Matti Järvisalo"
      ],
      "abstract": "Due to the wide employment of automated reasoning in the analysis and\nconstruction of correct systems, the results reported by automated reasoning\nengines must be trustworthy. For Boolean satisfiability (SAT) solvers - and\nmore recently SAT-based maximum satisfiability (MaxSAT) solvers -\ntrustworthiness is obtained by integrating proof logging into solvers, making\nsolvers capable of emitting machine-verifiable proofs to certify correctness of\nthe reasoning steps performed. In this work, we enable for the first time proof\nlogging based on the VeriPB proof format for multi-objective MaxSAT (MO-MaxSAT)\noptimization techniques. Although VeriPB does not offer direct support for\nmulti-objective problems, we detail how preorders in VeriPB can be used to\nprovide certificates for MO-MaxSAT algorithms computing a representative\nsolution for each element in the non-dominated set of the search space under\nPareto-optimality, without extending the VeriPB format or the proof checker. By\nimplementing VeriPB proof logging into a state-of-the-art multi-objective\nMaxSAT solver, we show empirically that proof logging can be made scalable for\nMO-MaxSAT with reasonable overhead.",
      "tldr_zh": "本文提出了一种方法，用于在多目标最大可满足性 (MO-MaxSAT) 问题中证明 Pareto-optimality 的正确性，通过将 VeriPB 证明格式集成到求解器中，确保自动化推理结果的可信性。作者利用 VeriPB 中的预序 (preorders) 来为 MO-MaxSAT 算法生成证书，这些算法计算搜索空间中非支配集的代表解决方案，而无需扩展 VeriPB 格式或证明检查器。主要贡献包括在状态-of-the-art MO-MaxSAT 求解器中实现证明日志，实验结果显示该方法具有可扩展性，且附加开销合理。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17493v1",
      "published_date": "2025-01-29 09:01:26 UTC",
      "updated_date": "2025-01-29 09:01:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:39:13.542163"
    },
    {
      "arxiv_id": "2501.17489v1",
      "title": "Neural Spelling: A Spell-Based BCI System for Language Neural Decoding",
      "title_zh": "Neural Spelling：一种基于拼写的 BCI 系统用于语言神经解码",
      "authors": [
        "Xiaowei Jiang",
        "Charles Zhou",
        "Yiqun Duan",
        "Ziyi Zhao",
        "Thomas Do",
        "Chin-Teng Lin"
      ],
      "abstract": "Brain-computer interfaces (BCIs) present a promising avenue by translating\nneural activity directly into text, eliminating the need for physical actions.\nHowever, existing non-invasive BCI systems have not successfully covered the\nentire alphabet, limiting their practicality. In this paper, we propose a novel\nnon-invasive EEG-based BCI system with Curriculum-based Neural Spelling\nFramework, which recognizes all 26 alphabet letters by decoding neural signals\nassociated with handwriting first, and then apply a Generative AI (GenAI) to\nenhance spell-based neural language decoding tasks. Our approach combines the\nease of handwriting with the accessibility of EEG technology, utilizing\nadvanced neural decoding algorithms and pre-trained large language models\n(LLMs) to translate EEG patterns into text with high accuracy. This system show\nhow GenAI can improve the performance of typical spelling-based neural language\ndecoding task, and addresses the limitations of previous methods, offering a\nscalable and user-friendly solution for individuals with communication\nimpairments, thereby enhancing inclusive communication options.",
      "tldr_zh": "本研究提出了一种基于 EEG 的非侵入式脑机接口 (BCI) 系统，名为 Neural Spelling，利用 Curriculum-based Neural Spelling Framework 先解码与 handwriting 相关的神经信号，以识别所有 26 个字母。系统结合 Generative AI (GenAI) 和预训练的大型语言模型 (LLMs)，通过高级神经解码算法将 EEG 模式转化为高准确率的文本，从而提升拼写-based 神经语言解码任务的性能。相比现有方法，该系统解决了字母覆盖的局限性，提供了一个可扩展、用户友好的解决方案，帮助沟通障碍者实现更具包容性的通信选项。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17489v1",
      "published_date": "2025-01-29 08:57:51 UTC",
      "updated_date": "2025-01-29 08:57:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:39:25.246720"
    },
    {
      "arxiv_id": "2501.17486v1",
      "title": "DINT Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Yueyang Cang",
        "Yuhang Liu",
        "Xiaoteng Zhang",
        "Erlu Zhao",
        "Li Shi"
      ],
      "abstract": "DIFF Transformer addresses the issue of irrelevant context interference by\nintroducing a differential attention mechanism that enhances the robustness of\nlocal attention. However, it has two critical limitations: the lack of global\ncontext modeling, which is essential for identifying globally significant\ntokens, and numerical instability due to the absence of strict row\nnormalization in the attention matrix. To overcome these challenges, we propose\nDINT Transformer, which extends DIFF Transformer by incorporating a\ndifferential-integral mechanism. By computing global importance scores and\nintegrating them into the attention matrix, DINT Transformer improves its\nability to capture global dependencies. Moreover, the unified parameter design\nenforces row-normalized attention matrices, improving numerical stability.\nExperimental results demonstrate that DINT Transformer excels in accuracy and\nrobustness across various practical applications, such as long-context language\nmodeling and key information retrieval. These results position DINT Transformer\nas a highly effective and promising architecture.",
      "tldr_zh": "本研究针对 DIFF Transformer 的局限性（缺乏全局上下文建模和注意力矩阵的数值不稳定性），提出了 DINT Transformer 作为其扩展版本。\nDINT Transformer 通过引入 differential-integral 机制来计算全局重要性分数，并将其整合到注意力矩阵中，从而提升了对全局依赖的捕获能力，同时通过统一的参数设计强制行归一化以提高数值稳定性。\n实验结果表明，DINT Transformer 在长上下文语言建模和关键信息检索等实际应用中，表现出更高的准确性和鲁棒性，展现出作为高效架构的巨大潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2410.05258 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2501.17486v1",
      "published_date": "2025-01-29 08:53:29 UTC",
      "updated_date": "2025-01-29 08:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:39:37.010844"
    },
    {
      "arxiv_id": "2501.17479v2",
      "title": "DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Seffi Cohen",
        "Niv Goldshlager",
        "Nurit Cohen-Inger",
        "Bracha Shapira",
        "Lior Rokach"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious natural language processing tasks but often struggle to excel uniformly\nin diverse or complex domains. We propose a novel ensemble method - Diverse\nFingerprint Ensemble (DFPE), which leverages the complementary strengths of\nmultiple LLMs to achieve more robust performance. Our approach involves: (1)\nclustering models based on response \"fingerprints\" patterns, (2) applying a\nquantile-based filtering mechanism to remove underperforming models at a\nper-subject level, and (3) assigning adaptive weights to remaining models based\non their subject-wise validation accuracy. In experiments on the Massive\nMultitask Language Understanding (MMLU) benchmark, DFPE outperforms the best\nsingle model by 3% overall accuracy and 5% in discipline-level accuracy. This\nmethod increases the robustness and generalization of LLMs and underscores how\nmodel selection, diversity preservation, and performance-driven weighting can\neffectively address challenging, multi-faceted language understanding tasks.",
      "tldr_zh": "该论文提出了一种名为Diverse Fingerprint Ensemble (DFPE) 的新方法，用于提升大型语言模型 (LLMs) 在多样化领域的性能，通过利用多个LLMs的互补优势实现更强的鲁棒性。DFPE 的关键步骤包括：基于响应“指纹”模式聚类模型、使用分位数过滤机制移除每个主题下表现不佳的模型，并根据主题级验证准确性分配自适应权重。在MMLU基准测试中，DFPE 比最佳单模型整体准确率提高3%，学科级别提高5%，从而证明了模型选择、保持多样性和性能驱动加权在处理多方面语言理解任务中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17479v2",
      "published_date": "2025-01-29 08:44:45 UTC",
      "updated_date": "2025-02-06 21:47:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:39:48.932835"
    },
    {
      "arxiv_id": "2501.17459v1",
      "title": "Large Language Models for Single-Step and Multi-Step Flight Trajectory Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Kaiwei Luo",
        "Jiliu Zhou"
      ],
      "abstract": "Flight trajectory prediction is a critical time series task in aviation.\nWhile deep learning methods have shown significant promise, the application of\nlarge language models (LLMs) to this domain remains underexplored. This study\npioneers the use of LLMs for flight trajectory prediction by reframing it as a\nlanguage modeling problem. Specifically, We extract features representing the\naircraft's position and status from ADS-B flight data to construct a\nprompt-based dataset, where trajectory waypoints are converted into language\ntokens. The dataset is then employed to fine-tune LLMs, enabling them to learn\ncomplex spatiotemporal patterns for accurate predictions. Comprehensive\nexperiments demonstrate that LLMs achieve notable performance improvements in\nboth single-step and multi-step predictions compared to traditional methods,\nwith LLaMA-3.1 model achieving the highest overall accuracy. However, the high\ninference latency of LLMs poses a challenge for real-time applications,\nunderscoring the need for further research in this promising direction.",
      "tldr_zh": "该研究首次将大型语言模型（LLMs）应用于飞行轨迹预测，将这一航空时间序列任务转化为语言建模问题，以提升预测准确性。研究人员从 ADS-B 飞行数据中提取飞机位置和状态特征，构建基于提示的自定义数据集，并将轨迹航点转换为语言标记来微调 LLMs，从而学习复杂的时空模式。实验结果显示，LLMs 在单步和多步预测中比传统方法有显著性能提升，其中 LLaMA-3.1 模型表现出最高准确性；然而，LLMs 的高推理延迟对实时应用构成挑战，需要进一步优化。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.17459v1",
      "published_date": "2025-01-29 07:35:56 UTC",
      "updated_date": "2025-01-29 07:35:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:40:00.632071"
    },
    {
      "arxiv_id": "2501.17441v1",
      "title": "Towards Making Flowchart Images Machine Interpretable",
      "title_zh": "翻译失败",
      "authors": [
        "Shreya Shukla",
        "Prajwal Gatti",
        "Yogesh Kumar",
        "Vikash Yadav",
        "Anand Mishra"
      ],
      "abstract": "Computer programming textbooks and software documentations often contain\nflowcharts to illustrate the flow of an algorithm or procedure. Modern OCR\nengines often tag these flowcharts as graphics and ignore them in further\nprocessing. In this paper, we work towards making flowchart images\nmachine-interpretable by converting them to executable Python codes. To this\nend, inspired by the recent success in natural language to code generation\nliterature, we present a novel transformer-based framework, namely FloCo-T5.\nOur model is well-suited for this task,as it can effectively learn semantics,\nstructure, and patterns of programming languages, which it leverages to\ngenerate syntactically correct code. We also used a task-specific pre-training\nobjective to pre-train FloCo-T5 using a large number of logic-preserving\naugmented code samples. Further, to perform a rigorous study of this problem,\nwe introduce theFloCo dataset that contains 11,884 flowchart images and their\ncorresponding Python codes. Our experiments show promising results, and\nFloCo-T5 clearly outperforms related competitive baselines on code generation\nmetrics. We make our dataset and implementation publicly available.",
      "tldr_zh": "本论文针对计算机编程教材和软件文档中的流程图图像，提出一种方法将其转换为可执行的 Python codes，使其机器可解释。作者引入了 FloCo-T5 框架，一个基于 Transformer 的模型，通过任务特定预训练（如使用逻辑保持的增强代码样本）来学习编程语言的语义、结构和模式，从而生成语法正确的代码。为支持研究，他们创建了 FloCo 数据集，包含 11,884 个流程图图像及其对应代码。实验结果显示，FloCo-T5 在代码生成指标上显著优于基线模型，并已公开数据集和实现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.DL",
        "cs.SE"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at: ICDAR 2023, Project Page:\n  https://vl2g.github.io/projects/floco/",
      "pdf_url": "http://arxiv.org/pdf/2501.17441v1",
      "published_date": "2025-01-29 06:43:38 UTC",
      "updated_date": "2025-01-29 06:43:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:40:13.587240"
    },
    {
      "arxiv_id": "2501.17433v1",
      "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
      "title_zh": "翻译失败",
      "authors": [
        "Tiansheng Huang",
        "Sihao Hu",
        "Fatih Ilhan",
        "Selim Furkan Tekin",
        "Ling Liu"
      ],
      "abstract": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: \\textbf{it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack}, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus",
      "tldr_zh": "该研究揭示了大型语言模型（LLMs）在面对有害微调攻击时的脆弱性，并提出了一种名为 Virus 的新 red-teaming 方法，用于绕过 guardrail moderation。Virus 通过对有害数据进行轻微修改，使其逃避检测，导致高达 100% 的数据泄漏，同时保持出色的攻击性能。实验结果证明，这种攻击的有效性突显了 guardrail moderation 的局限性，强调需要解决 LLMs 的固有安全问题，而非仅依赖防护机制。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17433v1",
      "published_date": "2025-01-29 06:24:58 UTC",
      "updated_date": "2025-01-29 06:24:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:40:25.470771"
    },
    {
      "arxiv_id": "2501.17429v1",
      "title": "Algorithmic Segmentation and Behavioral Profiling for Ransomware Detection Using Temporal-Correlation Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Ignatius Rollere",
        "Caspian Hartsfield",
        "Seraphina Courtenay",
        "Lucian Fenwick",
        "Aurelia Grunwald"
      ],
      "abstract": "The rapid evolution of cyber threats has outpaced traditional detection\nmethodologies, necessitating innovative approaches capable of addressing the\nadaptive and complex behaviors of modern adversaries. A novel framework was\nintroduced, leveraging Temporal-Correlation Graphs to model the intricate\nrelationships and temporal patterns inherent in malicious operations. The\napproach dynamically captured behavioral anomalies, offering a robust mechanism\nfor distinguishing between benign and malicious activities in real-time\nscenarios. Extensive experiments demonstrated the framework's effectiveness\nacross diverse ransomware families, with consistently high precision, recall,\nand overall detection accuracy. Comparative evaluations highlighted its better\nperformance over traditional signature-based and heuristic methods,\nparticularly in handling polymorphic and previously unseen ransomware variants.\nThe architecture was designed with scalability and modularity in mind, ensuring\ncompatibility with enterprise-scale environments while maintaining resource\nefficiency. Analysis of encryption speeds, anomaly patterns, and temporal\ncorrelations provided deeper insights into the operational strategies of\nransomware, validating the framework's adaptability to evolving threats. The\nresearch contributes to advancing cybersecurity technologies by integrating\ndynamic graph analytics and machine learning for future innovations in threat\ndetection. Results from this study underline the potential for transforming the\nway organizations detect and mitigate complex cyberattacks.",
      "tldr_zh": "本文提出了一种利用Temporal-Correlation Graphs的框架，通过算法分割和行为剖析来检测勒索软件（ransomware），以建模恶意操作的复杂关系和时间模式。该框架动态捕获行为异常，实现实时区分良性和恶意活动，并在各种勒索软件家族上表现出高precision、recall和检测准确率。相比传统签名-based和启发式方法，该框架在处理多态和未知变体时性能更优，并设计为可扩展和模块化，适用于企业环境。研究还通过分析加密速度、异常模式和时间相关性，提供对勒索软件策略的深入洞察，并推动动态图分析与机器学习在网络安全领域的创新。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17429v1",
      "published_date": "2025-01-29 06:09:25 UTC",
      "updated_date": "2025-01-29 06:09:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:40:38.013655"
    },
    {
      "arxiv_id": "2501.17420v1",
      "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models",
      "title_zh": "行动胜于言语：代理决策揭示语言模型中的隐性偏见",
      "authors": [
        "Yuxuan Li",
        "Hirokazu Shirado",
        "Sauvik Das"
      ],
      "abstract": "While advances in fairness and alignment have helped mitigate overt biases\nexhibited by large language models (LLMs) when explicitly prompted, we\nhypothesize that these models may still exhibit implicit biases when simulating\nhuman behavior. To test this hypothesis, we propose a technique to\nsystematically uncover such biases across a broad range of sociodemographic\ncategories by assessing decision-making disparities among agents with\nLLM-generated, sociodemographically-informed personas. Using our technique, we\ntested six LLMs across three sociodemographic groups and four decision-making\nscenarios. Our results show that state-of-the-art LLMs exhibit significant\nsociodemographic disparities in nearly all simulations, with more advanced\nmodels exhibiting greater implicit biases despite reducing explicit biases.\nFurthermore, when comparing our findings to real-world disparities reported in\nempirical studies, we find that the biases we uncovered are directionally\naligned but markedly amplified. This directional alignment highlights the\nutility of our technique in uncovering systematic biases in LLMs rather than\nrandom variations; moreover, the presence and amplification of implicit biases\nemphasizes the need for novel strategies to address these biases.",
      "tldr_zh": "本研究假设，尽管大型语言模型(LLMs)已减少显性偏见，但它们在模拟人类行为时可能仍存在隐性偏见，并提出一种技术通过评估LLMs生成的、基于社会人口统计信息的代理人决策差异来系统揭示这些偏见。研究者测试了六种LLMs、三个社会人口统计群体和四个决策场景，结果显示先进模型在几乎所有模拟中表现出显著的社会人口统计差异，且这些偏见与真实世界实证研究的方向一致但被放大。这种现象突显了LLMs中系统性偏见的普遍性，并强调需要开发新策略来有效解决隐性偏见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17420v1",
      "published_date": "2025-01-29 05:21:31 UTC",
      "updated_date": "2025-01-29 05:21:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:40:49.812998"
    },
    {
      "arxiv_id": "2501.17414v1",
      "title": "Reqo: A Robust and Explainable Query Optimization Cost Model",
      "title_zh": "Reqo：一种稳健且可解释的查询优化成本模型",
      "authors": [
        "Baoming Chang",
        "Amin Kamali",
        "Verena Kantere"
      ],
      "abstract": "In recent years, there has been a growing interest in using machine learning\n(ML) in query optimization to select more efficient plans. Existing\nlearning-based query optimizers use certain model architectures to convert\ntree-structured query plans into representations suitable for downstream ML\ntasks. As the design of these architectures significantly impacts cost\nestimation, we propose a tree model architecture based on Bidirectional Graph\nNeural Networks (Bi-GNN) aggregated by Gated Recurrent Units (GRUs) to achieve\nmore accurate cost estimates. The inherent uncertainty of data and model\nparameters also leads to inaccurate cost estimates, resulting in suboptimal\nplans and less robust query performance. To address this, we implement a novel\nlearning-to-rank cost model that effectively quantifies the uncertainty in cost\nestimates using approximate probabilistic ML. This model adaptively integrates\nquantified uncertainty with estimated costs and learns from comparing pairwise\nplans, achieving more robust performance. In addition, we propose the first\nexplainability technique specifically designed for learning-based cost models.\nThis technique explains the contribution of any subgraphs in the query plan to\nthe final predicted cost, which can be integrated and trained with any\nlearning-based cost model to significantly boost the model's explainability. By\nincorporating these innovations, we propose a cost model for a Robust and\nExplainable Query Optimizer, Reqo, that improves the accuracy, robustness, and\nexplainability of cost estimation, outperforming state-of-the-art approaches in\nall three dimensions.",
      "tldr_zh": "这篇论文提出了 Reqo，一种稳健且可解释的查询优化成本模型，旨在通过机器学习（ML）提升查询计划的效率。Reqo 采用基于 Bidirectional Graph Neural Networks (Bi-GNN) 和 Gated Recurrent Units (GRUs) 的树模型架构来实现更准确的成本估计，并使用学习-to-rank 方法结合 approximate probabilistic ML 量化不确定性，从而提高模型的稳健性。此外，该模型引入了首个专为学习-based 成本模型设计的解释性技术，能够解释查询计划中子图对最终成本的贡献。实验结果表明，Reqo 在准确性、稳健性和解释性方面均优于现有方法。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17414v1",
      "published_date": "2025-01-29 04:48:51 UTC",
      "updated_date": "2025-01-29 04:48:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:41:02.836174"
    },
    {
      "arxiv_id": "2501.17899v2",
      "title": "The Right to AI",
      "title_zh": "人工智能的权利",
      "authors": [
        "Rashid Mushkani",
        "Hugo Berard",
        "Allison Cohen",
        "Shin Koeski"
      ],
      "abstract": "This paper proposes a Right to AI, which asserts that individuals and\ncommunities should meaningfully participate in the development and governance\nof the AI systems that shape their lives. Motivated by the increasing\ndeployment of AI in critical domains and inspired by Henri Lefebvre's concept\nof the Right to the City, we reconceptualize AI as a societal infrastructure,\nrather than merely a product of expert design. In this paper, we critically\nevaluate how generative agents, large-scale data extraction, and diverse\ncultural values bring new complexities to AI oversight. The paper proposes that\ngrassroots participatory methodologies can mitigate biased outcomes and enhance\nsocial responsiveness. It asserts that data is socially produced and should be\nmanaged and owned collectively. Drawing on Sherry Arnstein's Ladder of Citizen\nParticipation and analyzing nine case studies, the paper develops a four-tier\nmodel for the Right to AI that situates the current paradigm and envisions an\naspirational future. It proposes recommendations for inclusive data ownership,\ntransparent design processes, and stakeholder-driven oversight. We also discuss\nmarket-led and state-centric alternatives and argue that participatory\napproaches offer a better balance between technical efficiency and democratic\nlegitimacy.",
      "tldr_zh": "这篇论文提出“The Right to AI”，主张个人和社区应在影响其生活的AI系统开发与治理中发挥实质性作用，受Henri Lefebvre的“The Right to the City”概念启发，将AI视为社会基础设施而非单纯专家设计产物。论文评估了生成代理、大规模数据提取及多样文化价值观带来的复杂性，并基于Sherry Arnstein的“Ladder of Citizen Participation”及九个案例研究，开发了一个四层模型，强调草根参与方法可缓解偏见并促进社会响应性。最终，它推荐了包容性数据所有权、透明设计过程和利益相关者驱动的监督机制，并认为参与式方法比市场主导或国家中心方法更能平衡技术效率与民主合法性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17899v2",
      "published_date": "2025-01-29 04:32:41 UTC",
      "updated_date": "2025-05-08 02:28:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:41:13.379990"
    },
    {
      "arxiv_id": "2501.17411v1",
      "title": "A Genetic Algorithm-Based Approach for Automated Optimization of Kolmogorov-Arnold Networks in Classification Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Quan Long",
        "Bin Wang",
        "Bing Xue",
        "Mengjie Zhang"
      ],
      "abstract": "To address the issue of interpretability in multilayer perceptrons (MLPs),\nKolmogorov-Arnold Networks (KANs) are introduced in 2024. However, optimizing\nKAN structures is labor-intensive, typically requiring manual intervention and\nparameter tuning. This paper proposes GA-KAN, a genetic algorithm-based\napproach that automates the optimization of KANs, requiring no human\nintervention in the design process. To the best of our knowledge, this is the\nfirst time that evolutionary computation is explored to optimize KANs\nautomatically. Furthermore, inspired by the use of sparse connectivity in MLPs\nin effectively reducing the number of parameters, GA-KAN further explores\nsparse connectivity to tackle the challenge of extensive parameter spaces in\nKANs. GA-KAN is validated on two toy datasets, achieving optimal results\nwithout the manual tuning required by the original KAN. Additionally, GA-KAN\ndemonstrates superior performance across five classification datasets,\noutperforming traditional methods on all datasets and providing interpretable\nsymbolic formulae for the Wine and Iris datasets, thereby enhancing model\ntransparency. Furthermore, GA-KAN significantly reduces the number of\nparameters over the standard KAN across all the five datasets. The core\ncontributions of GA-KAN include automated optimization, a new encoding\nstrategy, and a new decoding process, which together improve the accuracy and\ninterpretability, and reduce the number of parameters.",
      "tldr_zh": "本论文提出 GA-KAN，一种基于 Genetic Algorithm 的方法，用于自动优化 Kolmogorov-Arnold Networks (KANs) 在分类任务中的结构，旨在解决 KANs 优化过程依赖手动干预的问题。这是首次使用进化计算来自动优化 KANs，并引入稀疏连接 (sparse connectivity) 来减少参数空间。实验结果显示，GA-KAN 在两个玩具数据集上无需手动调整即可达到最佳性能，并在五个分类数据集上超越传统方法，提供可解释的 symbolic formulae 用于 Wine 和 Iris 数据集，同时显著降低参数数量。核心贡献包括自动化优化、新编码策略 (new encoding strategy) 和新解码过程 (new decoding process)，从而提升了模型的准确性、可解释性和效率。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17411v1",
      "published_date": "2025-01-29 04:32:36 UTC",
      "updated_date": "2025-01-29 04:32:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:41:26.433533"
    },
    {
      "arxiv_id": "2501.18643v2",
      "title": "3D Reconstruction of Shoes for Augmented Reality",
      "title_zh": "用于增强现实的鞋子三维重建",
      "authors": [
        "Pratik Shrestha",
        "Sujan Kapali",
        "Swikar Gautam",
        "Vishal Pokharel",
        "Santosh Giri"
      ],
      "abstract": "This paper introduces a mobile-based solution that enhances online shoe\nshopping through 3D modeling and Augmented Reality (AR), leveraging the\nefficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D\nimages, the framework generates realistic 3D shoe models from 2D images,\nachieving an average Peak Signal-to-Noise Ratio (PSNR) of 32, and enables\nimmersive AR interactions via smartphones. A custom shoe segmentation dataset\nof 3120 images was created, with the best-performing segmentation model\nachieving an Intersection over Union (IoU) score of 0.95. This paper\ndemonstrates the potential of 3D modeling and AR to revolutionize online\nshopping by offering realistic virtual interactions, with applicability across\nbroader fashion categories.",
      "tldr_zh": "本研究提出了一种基于手机的解决方案，利用3D Gaussian Splatting技术从2D图像生成逼真的3D鞋子模型，从而提升在线购物体验。框架通过创建包含3120张图像的自定义鞋子分割数据集，实现最佳分割模型的Intersection over Union (IoU)分数达0.95，并获得平均Peak Signal-to-Noise Ratio (PSNR)为32的性能。最终，该方法支持智能手机上的沉浸式Augmented Reality (AR)互动，并展示了其在在线购物领域的革命性潜力，可扩展至更广泛的时尚类别。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18643v2",
      "published_date": "2025-01-29 04:18:51 UTC",
      "updated_date": "2025-02-17 05:11:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:41:36.816771"
    },
    {
      "arxiv_id": "2501.17403v1",
      "title": "General Scene Adaptation for Vision-and-Language Navigation",
      "title_zh": "视觉和语言导航的通用场景适应",
      "authors": [
        "Haodong Hong",
        "Yanyuan Qiao",
        "Sen Wang",
        "Jiajun Liu",
        "Qi Wu"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on\none-time execution of individual instructions across multiple environments,\naiming to develop agents capable of functioning in any environment in a\nzero-shot manner. However, real-world navigation robots often operate in\npersistent environments with relatively consistent physical layouts, visual\nobservations, and language styles from instructors. Such a gap in the task\nsetting presents an opportunity to improve VLN agents by incorporating\ncontinuous adaptation to specific environments. To better reflect these\nreal-world conditions, we introduce GSA-VLN, a novel task requiring agents to\nexecute navigation instructions within a specific scene and simultaneously\nadapt to it for improved performance over time. To evaluate the proposed task,\none has to address two challenges in existing VLN datasets: the lack of OOD\ndata, and the limited number and style diversity of instructions for each\nscene. Therefore, we propose a new dataset, GSA-R2R, which significantly\nexpands the diversity and quantity of environments and instructions for the R2R\ndataset to evaluate agent adaptability in both ID and OOD contexts.\nFurthermore, we design a three-stage instruction orchestration pipeline that\nleverages LLMs to refine speaker-generated instructions and apply role-playing\ntechniques to rephrase instructions into different speaking styles. This is\nmotivated by the observation that each individual user often has consistent\nsignatures or preferences in their instructions. We conducted extensive\nexperiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various\nmethods. Based on our findings, we propose a novel method, GR-DUET, which\nincorporates memory-based navigation graphs with an environment-specific\ntraining strategy, achieving state-of-the-art results on all GSA-R2R splits.",
      "tldr_zh": "本研究针对Vision-and-Language Navigation (VLN)任务的局限性，提出General Scene Adaptation for VLN (GSA-VLN)，一种新任务框架，要求代理在特定场景中执行指令并逐步适应环境，以提升长期性能。论文引入GSA-R2R数据集，通过扩展R2R数据集增加环境和指令的多样性，包括ID和OOD上下文，并设计一个三阶段指令编排管道，利用LLMs精炼指令并应用角色扮演技术模拟用户风格。最终，提出GR-DUET方法，结合基于记忆的导航图和环境特定训练策略，在GSA-R2R数据集上实现state-of-the-art性能，显著提高了代理的适应性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17403v1",
      "published_date": "2025-01-29 03:57:56 UTC",
      "updated_date": "2025-01-29 03:57:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:41:49.259245"
    },
    {
      "arxiv_id": "2501.17399v2",
      "title": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Ved Sirdeshmukh",
        "Kaustubh Deshpande",
        "Johannes Mols",
        "Lifeng Jin",
        "Ed-Yeremai Cardona",
        "Dean Lee",
        "Jeremy Kritz",
        "Willow Primack",
        "Summer Yue",
        "Chen Xing"
      ],
      "abstract": "We present MultiChallenge, a pioneering benchmark evaluating large language\nmodels (LLMs) on conducting multi-turn conversations with human users, a\ncrucial yet underexamined capability for their applications. MultiChallenge\nidentifies four categories of challenges in multi-turn conversations that are\nnot only common and realistic among current human-LLM interactions, but are\nalso challenging to all current frontier LLMs. All 4 challenges require\naccurate instruction-following, context allocation, and in-context reasoning at\nthe same time. We also develop LLM as judge with instance-level rubrics to\nfacilitate an automatic evaluation method with fair agreement with experienced\nhuman raters. Despite achieving near-perfect scores on existing multi-turn\nevaluation benchmarks, all frontier models have less than 50% accuracy on\nMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving\njust a 41.4% average accuracy.",
      "tldr_zh": "本研究引入了MultiChallenge，一个开创性的基准，用于评估大型语言模型(LLMs)在多轮对话中的性能，这是一个关键但尚未充分探索的能力。MultiChallenge识别了四类常见且现实的挑战，包括准确的指令遵循、上下文分配和上下文推理，这些对当前前沿LLMs构成重大难题。研究开发了基于LLM作为评判者的自动评估方法，使用实例级别的评分标准，与人类评分者达成公平一致。尽管前沿模型在现有多轮基准上表现近乎完美，但在MultiChallenge上，所有模型准确率不足50%，顶级模型Claude 3.5 Sonnet仅达到41.4%。这突显了LLMs在真实对话场景中的局限性，并为未来模型改进提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17399v2",
      "published_date": "2025-01-29 03:29:24 UTC",
      "updated_date": "2025-03-06 04:41:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:42:02.205392"
    },
    {
      "arxiv_id": "2501.17393v1",
      "title": "Intensional Inheritance Between Concepts: An Information-Theoretic Interpretation",
      "title_zh": "概念之间的内涵继承：一种信息理论解释",
      "authors": [
        "Ben Goertzel"
      ],
      "abstract": "This paper addresses the problem of formalizing and quantifying the concept\nof \"intensional inheritance\" between two concepts. We begin by conceiving the\nintensional inheritance of $W$ from $F$ as the amount of information the\nproposition \"x is $F$ \" provides about the proposition \"x is $W$. To flesh this\nout, we consider concepts $F$ and $W$ defined by sets of properties\n$\\left\\{F_{1}, F_{2}, \\ldots, F_{n}\\right\\}$ and $\\left\\{W_{1}, W_{2}, \\ldots,\nW_{m}\\right\\}$ with associated degrees $\\left\\{d_{1}, d_{2}, \\ldots,\nd_{n}\\right\\}$ and $\\left\\{e_{1}, e_{2}, \\ldots, e_{m}\\right\\}$, respectively,\nwhere the properties may overlap. We then derive formulas for the intensional\ninheritance using both Shannon information theory and algorithmic information\ntheory, incorporating interaction information among properties. We examine a\nspecial case where all properties are mutually exclusive and calculate the\nintensional inheritance in this case in both frameworks. We also derive\nexpressions for $P(W \\mid F)$ based on the mutual information formula. Finally\nwe consider the relationship between intensional inheritance and conventional\nset-theoretic \"extensional\" inheritance, concluding that in our\ninformation-theoretic framework, extensional inheritance emerges as a special\ncase of intensional inheritance.",
      "tldr_zh": "这篇论文探讨了概念之间“intensional inheritance”的形式化和量化，将其定义为概念F对概念W的信息贡献，即“x是F”对“x是W”的信息量。作者使用Shannon信息理论和algorithmic information theory，基于属性集及其相关度推导公式，并考虑属性间的interaction information。研究还考察了属性互斥的特例，推导了P(W | F)的表达式，并证明了extensional inheritance在该框架中是intensional inheritance的特例。",
      "categories": [
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17393v1",
      "published_date": "2025-01-29 03:01:29 UTC",
      "updated_date": "2025-01-29 03:01:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:42:13.687856"
    },
    {
      "arxiv_id": "2501.17391v2",
      "title": "Learning Free Token Reduction for Multi-Modal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zihui Zhao",
        "Yingxin Li",
        "Yang Li"
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance.",
      "tldr_zh": "本研究针对视觉语言模型（VLMs）在多模态任务中的高计算成本和长推理时间问题，提出了一种学习-free 的标记压缩范式。该方法在空间和时间维度上操作视觉数据，通过一个即插即用的压缩管道无缝集成到大多数多模态大语言模型（MLLMs）框架中，从而提升模型推理能力并减少计算开销。在 Video-QA 任务上的实验结果证明，该方法显著提高了效率，同时保持了性能水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17391v2",
      "published_date": "2025-01-29 02:52:32 UTC",
      "updated_date": "2025-04-14 17:34:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:42:25.524763"
    },
    {
      "arxiv_id": "2501.17386v2",
      "title": "Context-Aware Semantic Recomposition Mechanism for Large Language Models",
      "title_zh": "上下文感知语义重组机制，用于大型语言模型",
      "authors": [
        "Richard Katrix",
        "Quentin Carroway",
        "Rowan Hawkesbury",
        "Matthias Heathfield"
      ],
      "abstract": "Context-aware processing mechanisms have increasingly become a critical area\nof exploration for improving the semantic and contextual capabilities of\nlanguage generation models. The Context-Aware Semantic Recomposition Mechanism\n(CASRM) was introduced as a novel framework designed to address limitations in\ncoherence, contextual adaptability, and error propagation in large-scale text\ngeneration tasks. Through the integration of dynamically generated context\nvectors and attention modulation layers, CASRM enhances the alignment between\ntoken-level representations and broader contextual dependencies. Experimental\nevaluations demonstrated significant improvements in semantic coherence across\nmultiple domains, including technical, conversational, and narrative text. The\nability to adapt to unseen domains and ambiguous inputs was evaluated using a\ndiverse set of test scenarios, highlighting the robustness of the proposed\nmechanism. A detailed computational analysis revealed that while CASRM\nintroduces additional processing overhead, the gains in linguistic precision\nand contextual relevance outweigh the marginal increase in complexity. The\nframework also successfully mitigates error propagation in sequential tasks,\nimproving performance in dialogue continuation and multi-step text synthesis.\nAdditional investigations into token-level attention distribution emphasized\nthe dynamic focus shifts enabled through context-aware enhancements. The\nfindings suggest that CASRM offers a scalable and flexible solution for\nintegrating contextual intelligence into existing language model architectures.",
      "tldr_zh": "本文提出了一种 Context-Aware Semantic Recomposition Mechanism (CASRM) 框架，用于提升大型语言模型在文本生成中的语义连贯性、上下文适应性和错误传播控制。CASRM 通过整合动态生成的上下文向量和注意力调制层，增强 token-level 表示与更广泛上下文依赖的关联。实验结果显示，该机制在技术、对话和叙事领域显著提高了语义连贯性，并展示了良好的适应性与鲁棒性，尽管引入了轻微的计算开销，但整体性能提升使其成为可扩展的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
      "pdf_url": "http://arxiv.org/pdf/2501.17386v2",
      "published_date": "2025-01-29 02:38:28 UTC",
      "updated_date": "2025-03-26 15:57:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:42:37.373905"
    },
    {
      "arxiv_id": "2501.17384v1",
      "title": "A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengpeng Xie",
        "Jiahang Cao",
        "Yulong Zhang",
        "Qiang Zhang",
        "Renjing Xu"
      ],
      "abstract": "Recently, empowered with the powerful capabilities of neural networks,\nreinforcement learning (RL) has successfully tackled numerous challenging\ntasks. However, while these models demonstrate enhanced decision-making\nabilities, they are increasingly prone to overfitting. For instance, a trained\nRL model often fails to generalize to even minor variations of the same task,\nsuch as a change in background color or other minor semantic differences. To\naddress this issue, we propose a dual-agent adversarial policy learning\nframework, which allows agents to spontaneously learn the underlying semantics\nwithout introducing any human prior knowledge. Specifically, our framework\ninvolves a game process between two agents: each agent seeks to maximize the\nimpact of perturbing on the opponent's policy by producing representation\ndifferences for the same state, while maintaining its own stability against\nsuch perturbations. This interaction encourages agents to learn generalizable\npolicies, capable of handling irrelevant features from the high-dimensional\nobservations. Extensive experimental results on the Procgen benchmark\ndemonstrate that the adversarial process significantly improves the\ngeneralization performance of both agents, while also being applied to various\nRL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial\nframework, the RL agent outperforms the baseline methods by a significant\nmargin, especially in hard-level tasks, marking a significant step forward in\nthe generalization capabilities of deep reinforcement learning.",
      "tldr_zh": "这篇论文针对深度强化学习（RL）模型容易过拟合的问题（如对背景颜色等细微变化不鲁棒），提出了一种双智能体对抗策略学习框架。框架中，两个智能体通过游戏过程相互扰动表示差异，以最大化对手策略的影响，同时保持自身稳定性，从而在不引入人为先验知识的情况下学习底层语义和泛化能力。实验结果显示，该框架显著提升了RL算法如Proximal Policy Optimization (PPO)在Procgen基准上的泛化性能，尤其在困难任务中，表现出色地超越了基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17384v1",
      "published_date": "2025-01-29 02:36:47 UTC",
      "updated_date": "2025-01-29 02:36:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:42:49.568461"
    },
    {
      "arxiv_id": "2501.17377v1",
      "title": "ASAP: Learning Generalizable Online Bin Packing via Adaptive Selection After Pruning",
      "title_zh": "翻译失败",
      "authors": [
        "Han Fang",
        "Paul Weng",
        "Yutong Ban"
      ],
      "abstract": "Recently, deep reinforcement learning (DRL) has achieved promising results in\nsolving online 3D Bin Packing Problems (3D-BPP). However, these DRL-based\npolicies may perform poorly on new instances due to distribution shift. Besides\ngeneralization, we also consider adaptation, completely overlooked by previous\nwork, which aims at rapidly finetuning these policies to a new test\ndistribution. To tackle both generalization and adaptation issues, we propose\nAdaptive Selection After Pruning (ASAP), which decomposes a solver's\ndecision-making into two policies, one for pruning and one for selection. The\nrole of the pruning policy is to remove inherently bad actions, which allows\nthe selection policy to choose among the remaining most valuable actions. To\nlearn these policies, we propose a training scheme based on a meta-learning\nphase of both policies followed by a finetuning phase of the sole selection\npolicy to rapidly adapt it to a test distribution. Our experiments demonstrate\nthat ASAP exhibits excellent generalization and adaptation capabilities on\nin-distribution and out-of-distribution instances under both discrete and\ncontinuous setup.",
      "tldr_zh": "该研究针对深度强化学习(DRL)在在线3D箱子装箱问题(3D-BPP)中的泛化问题，提出ASAP方法，通过将决策分解为pruning（去除不良动作）和selection（选择最佳剩余动作）两个策略来提升性能。ASAP的训练方案包括一个元学习阶段来同时训练两个策略，随后只微调selection策略，以实现快速适应新测试分布。实验结果表明，ASAP在分布内和分布外实例上显示出优秀的泛化和适应能力，适用于离散和连续设置。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17377v1",
      "published_date": "2025-01-29 02:12:34 UTC",
      "updated_date": "2025-01-29 02:12:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:43:01.417371"
    },
    {
      "arxiv_id": "2501.17374v1",
      "title": "A Geometric Perspective for High-Dimensional Multiplex Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Kamel Abdous",
        "Nairouz Mrabah",
        "Mohamed Bouguessa"
      ],
      "abstract": "High-dimensional multiplex graphs are characterized by their high number of\ncomplementary and divergent dimensions. The existence of multiple hierarchical\nlatent relations between the graph dimensions poses significant challenges to\nembedding methods. In particular, the geometric distortions that might occur in\nthe representational space have been overlooked in the literature. This work\nstudies the problem of high-dimensional multiplex graph embedding from a\ngeometric perspective. We find that the node representations reside on highly\ncurved manifolds, thus rendering their exploitation more challenging for\ndownstream tasks. Moreover, our study reveals that increasing the number of\ngraph dimensions can cause further distortions to the highly curved manifolds.\nTo address this problem, we propose a novel multiplex graph embedding method\nthat harnesses hierarchical dimension embedding and Hyperbolic Graph Neural\nNetworks. The proposed approach hierarchically extracts hyperbolic node\nrepresentations that reside on Riemannian manifolds while gradually learning\nfewer and more expressive latent dimensions of the multiplex graph.\nExperimental results on real-world high-dimensional multiplex graphs show that\nthe synergy between hierarchical and hyperbolic embeddings incurs much fewer\ngeometric distortions and brings notable improvements over state-of-the-art\napproaches on downstream tasks.",
      "tldr_zh": "这篇论文从几何视角探讨高维多层图（high-dimensional multiplex graphs）的嵌入问题，发现节点表示位于高度弯曲的流形（highly curved manifolds）上，且增加图维度会进一步加剧几何扭曲（geometric distortions）。为了解决这一挑战，作者提出了一种新方法，结合层次化维度嵌入（hierarchical dimension embedding）和双曲图神经网络（Hyperbolic Graph Neural Networks），通过分层提取双曲节点表示（hyperbolic node representations）在黎曼流形（Riemannian manifolds）上学习更具表现力的潜在维度。实验在真实世界的高维多层图上表明，该方法显著减少了几何扭曲，并在下游任务中比现有方法取得了明显改进。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Proceedings of the ACM Conference on Information and\n  Knowledge Management (CIKM) 2024, DOI: 10.1145/3627673.3679541",
      "pdf_url": "http://arxiv.org/pdf/2501.17374v1",
      "published_date": "2025-01-29 02:02:37 UTC",
      "updated_date": "2025-01-29 02:02:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:43:14.524337"
    },
    {
      "arxiv_id": "2501.17366v1",
      "title": "Forecasting S&P 500 Using LSTM Models",
      "title_zh": "翻译失败",
      "authors": [
        "Prashant Pilla",
        "Raji Mekonen"
      ],
      "abstract": "With the volatile and complex nature of financial data influenced by external\nfactors, forecasting the stock market is challenging. Traditional models such\nas ARIMA and GARCH perform well with linear data but struggle with non-linear\ndependencies. Machine learning and deep learning models, particularly Long\nShort-Term Memory (LSTM) networks, address these challenges by capturing\nintricate patterns and long-term dependencies. This report compares ARIMA and\nLSTM models in predicting the S&P 500 index, a major financial benchmark.\n  Using historical price data and technical indicators, we evaluated these\nmodels using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). The\nARIMA model showed reasonable performance with an MAE of 462.1, RMSE of 614,\nand 89.8 percent accuracy, effectively capturing short-term trends but limited\nby its linear assumptions. The LSTM model, leveraging sequential processing\ncapabilities, outperformed ARIMA with an MAE of 369.32, RMSE of 412.84, and\n92.46 percent accuracy, capturing both short- and long-term dependencies.\nNotably, the LSTM model without additional features performed best, achieving\nan MAE of 175.9, RMSE of 207.34, and 96.41 percent accuracy, showcasing its\nability to handle market data efficiently.\n  Accurately predicting stock movements is crucial for investment strategies,\nrisk assessments, and market stability. Our findings confirm the potential of\ndeep learning models in handling volatile financial data compared to\ntraditional ones. The results highlight the effectiveness of LSTM and suggest\navenues for further improvements. This study provides insights into financial\nforecasting, offering a comparative analysis of ARIMA and LSTM while outlining\ntheir strengths and limitations.",
      "tldr_zh": "本研究比较了传统模型 ARIMA 和深度学习模型 LSTM 在预测 S&P 500 指数时的表现，旨在解决金融数据中非线性依赖的挑战。使用历史价格数据和技术指标进行评估，ARIMA 模型的 MAE 为 462.1、RMSE 为 614 和准确率 89.8%，适合捕捉短期趋势但受限于线性假设。LSTM 模型表现出色，特别是无额外特征版本，实现了 MAE 175.9、RMSE 207.34 和准确率 96.41%，证明其在捕捉短期和长期依赖方面更有效。该研究强调了 LSTM 在金融预测中的潜力，为投资策略和风险评估提供了宝贵insights。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.CP",
        "q-fin.TR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17366v1",
      "published_date": "2025-01-29 01:31:56 UTC",
      "updated_date": "2025-01-29 01:31:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:43:26.241053"
    },
    {
      "arxiv_id": "2501.17361v1",
      "title": "The M-factor: A Novel Metric for Evaluating Neural Architecture Search in Resource-Constrained Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Srikanth Thudumu",
        "Hy Nguyen",
        "Hung Du",
        "Nhat Duong",
        "Zafaryab Rasool",
        "Rena Logothetis",
        "Scott Barnett",
        "Rajesh Vasa",
        "Kon Mouzakis"
      ],
      "abstract": "Neural Architecture Search (NAS) aims to automate the design of deep neural\nnetworks. However, existing NAS techniques often focus on maximising accuracy,\nneglecting model efficiency. This limitation restricts their use in\nresource-constrained environments like mobile devices and edge computing\nsystems. Moreover, current evaluation metrics prioritise performance over\nefficiency, lacking a balanced approach for assessing architectures suitable\nfor constrained scenarios. To address these challenges, this paper introduces\nthe M-factor, a novel metric combining model accuracy and size. Four diverse\nNAS techniques are compared: Policy-Based Reinforcement Learning, Regularised\nEvolution, Tree-structured Parzen Estimator (TPE), and Multi-trial Random\nSearch. These techniques represent different NAS paradigms, providing a\ncomprehensive evaluation of the M-factor. The study analyses ResNet\nconfigurations on the CIFAR-10 dataset, with a search space of 19,683\nconfigurations. Experiments reveal that Policy-Based Reinforcement Learning and\nRegularised Evolution achieved M-factor values of 0.84 and 0.82, respectively,\nwhile Multi-trial Random Search attained 0.75, and TPE reached 0.67.\nPolicy-Based Reinforcement Learning exhibited performance changes after 39\ntrials, while Regularised Evolution optimised within 20 trials. The research\ninvestigates the optimisation dynamics and trade-offs between accuracy and\nmodel size for each strategy. Findings indicate that, in some cases, random\nsearch performed comparably to more complex algorithms when assessed using the\nM-factor. These results highlight how the M-factor addresses the limitations of\nexisting metrics by guiding NAS towards balanced architectures, offering\nvaluable insights for selecting strategies in scenarios requiring both\nperformance and efficiency.",
      "tldr_zh": "本文提出 M-factor，一种新颖的指标，用于评估 Neural Architecture Search (NAS) 在资源受限环境中的性能和效率平衡，该指标结合了模型准确性和大小，解决了现有指标偏重准确性而忽略效率的问题。研究比较了四种 NAS 技术——Policy-Based Reinforcement Learning、Regularised Evolution、Tree-structured Parzen Estimator (TPE) 和 Multi-trial Random Search——在 CIFAR-10 数据集上测试了 19,683 个 ResNet 配置。实验结果显示，Policy-Based Reinforcement Learning 和 Regularised Evolution 分别取得了 0.84 和 0.82 的 M-factor 值，而 Multi-trial Random Search 在某些场景下与复杂算法表现相当。这些发现为选择适用于移动设备和边缘计算的平衡架构提供了重要指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17361v1",
      "published_date": "2025-01-29 00:57:02 UTC",
      "updated_date": "2025-01-29 00:57:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:43:38.851167"
    },
    {
      "arxiv_id": "2501.17894v1",
      "title": "Progress in Artificial Intelligence and its Determinants",
      "title_zh": "人工智能的进步及其决定因素",
      "authors": [
        "Michael R. Douglas",
        "Sergiy Verstyuk"
      ],
      "abstract": "We study long-run progress in artificial intelligence in a quantitative way.\nMany measures, including traditional ones such as patents and publications,\nmachine learning benchmarks, and a new Aggregate State of the Art in ML (or\nASOTA) Index we have constructed from these, show exponential growth at roughly\nconstant rates over long periods. Production of patents and publications\ndoubles every ten years, by contrast with the growth of computing resources\ndriven by Moore's Law, roughly a doubling every two years. We argue that the\ninput of AI researchers is also crucial and its contribution can be objectively\nestimated. Consequently, we give a simple argument that explains the 5:1\nrelation between these two rates. We then discuss the application of this\nargument to different output measures and compare our analyses with predictions\nbased on machine learning scaling laws proposed in existing literature. Our\nquantitative framework facilitates understanding, predicting, and modulating\nthe development of these important technologies.",
      "tldr_zh": "本文定量研究了人工智能的长期进展，使用专利、出版物、机器学习基准以及新构建的Aggregate State of the Art in ML (ASOTA) Index等指标，显示这些输出指标呈指数增长，每十年翻倍。相比之下，计算资源增长（如Moore's Law每两年翻倍），作者强调AI研究人员的输入至关重要，并通过简单论证解释了输入与输出增长率之间的5:1关系。该框架有助于理解、预测和调控AI发展，并与现有machine learning scaling laws进行比较。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "physics.soc-ph",
        "q-fin.EC"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17894v1",
      "published_date": "2025-01-29 00:43:27 UTC",
      "updated_date": "2025-01-29 00:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:43:49.701732"
    },
    {
      "arxiv_id": "2501.17356v1",
      "title": "On the Coexistence and Ensembling of Watermarks",
      "title_zh": "翻译失败",
      "authors": [
        "Aleksandar Petrov",
        "Shruti Agarwal",
        "Philip H. S. Torr",
        "Adel Bibi",
        "John Collomosse"
      ],
      "abstract": "Watermarking, the practice of embedding imperceptible information into media\nsuch as images, videos, audio, and text, is essential for intellectual property\nprotection, content provenance and attribution. The growing complexity of\ndigital ecosystems necessitates watermarks for different uses to be embedded in\nthe same media. However, to detect and decode all watermarks, they need to\ncoexist well with one another. We perform the first study of coexistence of\ndeep image watermarking methods and, contrary to intuition, we find that\nvarious open-source watermarks can coexist with only minor impacts on image\nquality and decoding robustness. The coexistence of watermarks also opens the\navenue for ensembling watermarking methods. We show how ensembling can increase\nthe overall message capacity and enable new trade-offs between capacity,\naccuracy, robustness and image quality, without needing to retrain the base\nmodels.",
      "tldr_zh": "这篇论文探讨了图像 watermarks 的共存和集成问题，旨在解决数字生态系统中同一媒体嵌入多种 watermarks 的需求，以保护知识产权和内容归属。研究发现，各种开源深度图像 watermarking 方法可以共存，对图像质量和解码鲁棒性影响较小，这与直觉相反。论文进一步证明，通过 ensembling 水印方法，可以提升整体消息容量，并实现容量、准确性、鲁棒性和图像质量之间的新权衡，而无需重新训练基础模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17356v1",
      "published_date": "2025-01-29 00:37:06 UTC",
      "updated_date": "2025-01-29 00:37:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T04:44:01.474239"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 81,
  "processed_papers_count": 81,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T04:44:15.789584"
}
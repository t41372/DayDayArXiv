[
  {
    "arxiv_id": "2511.05502v1",
    "title": "Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS",
    "authors": [
      "Varun Rajesh",
      "Om Jodhpurkar",
      "Pooja Anbuselvan",
      "Mantinder Singh",
      "Ashok Jallepali",
      "Shantanu Godbole",
      "Pradeep Kumar Sharma",
      "Hritvik Shrivastava"
    ],
    "abstract": "We present a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, we measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity.\n  Under our settings, MLX achieves the highest sustained generation throughput, while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes and offers stronger out-of-the-box inference features. llama.cpp is highly efficient for lightweight single-stream use, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts.\n  All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. We release scripts, logs, and plots to reproduce all results. Our analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations for interactive and long-context processing. Although Apple Silicon inference frameworks still trail NVIDIA GPU-based systems such as vLLM in absolute performance, they are rapidly maturing into viable, production-grade solutions for private, on-device LLM inference.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05502v1",
    "published_date": "2025-10-09 23:53:38 UTC",
    "updated_date": "2025-10-09 23:53:38 UTC"
  },
  {
    "arxiv_id": "2510.08867v1",
    "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review",
    "authors": [
      "Gaurav Sahu",
      "Hugo Larochelle",
      "Laurent Charlin",
      "Christopher Pal"
    ],
    "abstract": "Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08867v1",
    "published_date": "2025-10-09 23:53:19 UTC",
    "updated_date": "2025-10-09 23:53:19 UTC"
  },
  {
    "arxiv_id": "2510.12819v1",
    "title": "Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis",
    "authors": [
      "Junyao Huang",
      "Rumin Situ"
    ],
    "abstract": "Traditional pet emotion recognition from vocalizations, based on discrete classification, struggles with ambiguity and capturing intensity variations. We propose a continuous Valence-Arousal (VA) model that represents emotions in a two-dimensional space. Our method uses an automatic VA label generation algorithm, enabling large-scale annotation of 42,553 pet vocalization samples. A multi-task learning framework jointly trains VA regression with auxiliary tasks (emotion, body size, gender) to enhance prediction by improving feature learning. Our Audio Transformer model achieves a validation Valence Pearson correlation of r = 0.9024 and an Arousal r = 0.7155, effectively resolving confusion between discrete categories like \"territorial\" and \"happy.\" This work introduces the first continuous VA framework for pet vocalization analysis, offering a more expressive representation for human-pet interaction, veterinary diagnostics, and behavioral training. The approach shows strong potential for deployment in consumer products like AI pet emotion translators.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "24 pages, 6 figures, 4 tables. First continuous VA framework for pet vocalization analysis with 42,553 samples",
    "pdf_url": "https://arxiv.org/pdf/2510.12819v1",
    "published_date": "2025-10-09 23:39:40 UTC",
    "updated_date": "2025-10-09 23:39:40 UTC"
  },
  {
    "arxiv_id": "2510.08859v1",
    "title": "Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models",
    "authors": [
      "Ragib Amin Nihal",
      "Rui Wen",
      "Kazuhiro Nakadai",
      "Jun Sakuma"
    ],
    "abstract": "Large language models (LLMs) remain vulnerable to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. These attacks target different harm categories (like malware generation, harassment, or fraud) through distinct conversational approaches (educational discussions, personal experiences, hypothetical scenarios). Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc exploration strategies, providing limited insight into underlying model weaknesses. The relationship between conversation patterns and model vulnerabilities across harm categories remains poorly understood. We propose Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation patterns to construct effective multi-turn jailbreaks through natural dialogue. Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve state-of-the-art performance, uncovering pattern-specific vulnerabilities and LLM behavioral characteristics: models exhibit distinct weakness profiles where robustness to one conversational pattern does not generalize to others, and model families share similar failure modes. These findings highlight limitations of safety training and indicate the need for pattern-aware defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08859v1",
    "published_date": "2025-10-09 23:26:28 UTC",
    "updated_date": "2025-10-09 23:26:28 UTC"
  },
  {
    "arxiv_id": "2510.08855v1",
    "title": "Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training",
    "authors": [
      "T. Ed Li",
      "Junyu Ren"
    ],
    "abstract": "Understanding the internal representations of large language models is crucial for ensuring their reliability and safety, with sparse autoencoders (SAEs) emerging as a promising interpretability approach. However, current SAE training methods face feature absorption, where features (or neurons) are absorbed into each other to minimize $L_1$ penalty, making it difficult to consistently identify and analyze model behaviors. We introduce Adaptive Temporal Masking (ATM), a novel training approach that dynamically adjusts feature selection by tracking activation magnitudes, frequencies, and reconstruction contributions to compute importance scores that evolve over time. ATM applies a probabilistic masking mechanism based on statistical thresholding of these importance scores, creating a more natural feature selection process. Through extensive experiments on the Gemma-2-2b model, we demonstrate that ATM achieves substantially lower absorption scores compared to existing methods like TopK and JumpReLU SAEs, while maintaining excellent reconstruction quality. These results establish ATM as a principled solution for learning stable, interpretable features in neural networks, providing a foundation for more reliable model analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "First submitted on February 10th, 2025 to ICLR 2025 Workshop (XAI4Science: From Understanding Model Behavior to Discovering New Scientific Knowledge). The paper was accepted but the workshop does not generate proceedings. Now uploading to arXiv to make the paper publicly available",
    "pdf_url": "https://arxiv.org/pdf/2510.08855v1",
    "published_date": "2025-10-09 23:12:51 UTC",
    "updated_date": "2025-10-09 23:12:51 UTC"
  },
  {
    "arxiv_id": "2510.08850v1",
    "title": "Repository-Aware File Path Retrieval via Fine-Tuned LLMs",
    "authors": [
      "Vasudha Yanuganti",
      "Ishaan Puri",
      "Swapnil Chhatre",
      "Mantinder Singh",
      "Ashok Jallepalli",
      "Hritvik Shrivastava",
      "Pradeep Kumar Sharma"
    ],
    "abstract": "Modern codebases make it hard for developers and AI coding assistants to find the right source files when answering questions like \"How does this feature work?\" or \"Where was the bug introduced?\" Traditional code search (keyword or IR based) often misses semantic context and cross file links, while large language models (LLMs) understand natural language but lack repository specific detail. We present a method for file path retrieval that fine tunes a strong LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file paths directly from a natural language query. To build training data, we introduce six code aware strategies that use abstract syntax tree (AST) structure and repository content to generate realistic question-answer pairs, where answers are sets of file paths. The strategies range from single file prompts to hierarchical repository summaries, providing broad coverage. We fine tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch, and obtain high retrieval accuracy: up to 91\\% exact match and 93\\% recall on held out queries, clearly beating single strategy training. On a large codebase like PyTorch (about 4,000 Python files), the model reaches 59\\% recall, showing scalability. We analyze how multi level code signals help the LLM reason over cross file context and discuss dataset design, limits (for example, context length in very large repos), and future integration of retrieval with LLM based code intelligence.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08850v1",
    "published_date": "2025-10-09 22:49:10 UTC",
    "updated_date": "2025-10-09 22:49:10 UTC"
  },
  {
    "arxiv_id": "2510.08847v1",
    "title": "What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment",
    "authors": [
      "Allison Sihan Jia",
      "Daniel Huang",
      "Nikhil Vytla",
      "Nirvika Choudhury",
      "Shayak Sen",
      "John C Mitchell",
      "Anupam Datta"
    ],
    "abstract": "We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation paradigm based on an agent's operational loop of setting goals, devising plans, and executing actions. The framework includes five evaluation metrics: Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan Adherence. Logical Consistency checks that an agent's actions are consistent with its prior actions. Execution Efficiency checks whether the agent executes in the most efficient way to achieve its goal. Plan Quality checks whether an agent's plans are aligned with its goals; Plan Adherence checks if an agent's actions are aligned with its plan; and Goal Fulfillment checks that agent's final outcomes match the stated goals. Our experimental results on two benchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for a production-grade data agent - show that this framework (a) provides a systematic way to cover a broad range of agent failures, including all agent errors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that exhibit strong agreement with human annotation, covering 80% to over 95% errors; and (c) localizes errors with 86% agreement to enable targeted improvement of agent performance.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08847v1",
    "published_date": "2025-10-09 22:40:19 UTC",
    "updated_date": "2025-10-09 22:40:19 UTC"
  },
  {
    "arxiv_id": "2510.12818v1",
    "title": "MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning",
    "authors": [
      "Rajarshi Ghosh",
      "Abhay Gupta",
      "Hudson McBride",
      "Anurag Vaidya",
      "Faisal Mahmood"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in clinical decision support, yet subtle demographic cues can influence their reasoning. Prior work has documented disparities in outputs across patient groups, but little is known about how internal reasoning shifts under controlled demographic changes. We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient pronouns (he/him, she/her, they/them) while holding critical symptoms and conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC ablations, producing three parallel datasets of approximately 23,000 items each (69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual Similarity (STS) between reasoning traces to measure stability across pronoun variants. Our results show overall high similarity (mean STS >0.80), but reveal consistent localized divergences in cited risk factors, guideline anchors, and differential ordering, even when final diagnoses remain unchanged. Our error analysis highlights certain cases in which the reasoning shifts, underscoring clinically relevant bias loci that may cascade into inequitable care. MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning stability in medical AI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12818v1",
    "published_date": "2025-10-09 22:12:58 UTC",
    "updated_date": "2025-10-09 22:12:58 UTC"
  },
  {
    "arxiv_id": "2510.08839v1",
    "title": "Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction",
    "authors": [
      "Motahare Mounesan",
      "Sourya Saha",
      "Houchao Gan",
      "Md. Nurul Absur",
      "Saptarshi Debroy"
    ],
    "abstract": "Real-time multi-view 3D reconstruction is a mission-critical application for key edge-native use cases, such as fire rescue, where timely and accurate 3D scene modeling enables situational awareness and informed decision-making. However, the dynamic and unpredictable nature of edge resource availability introduces disruptions, such as degraded image quality, unstable network links, and fluctuating server loads, which challenge the reliability of the reconstruction pipeline. In this work, we present a reinforcement learning (RL)-based edge resource management framework for reliable 3D reconstruction to ensure high quality reconstruction within a reasonable amount of time, despite the system operating under a resource-constrained and disruption-prone environment. In particular, the framework adopts two cooperative Q-learning agents, one for camera selection and one for server selection, both of which operate entirely online, learning policies through interactions with the edge environment. To support learning under realistic constraints and evaluate system performance, we implement a distributed testbed comprising lab-hosted end devices and FABRIC infrastructure-hosted edge servers to emulate smart city edge infrastructure under realistic disruption scenarios. Results show that the proposed framework improves application reliability by effectively balancing end-to-end latency and reconstruction quality in dynamic environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "cs.GR",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08839v1",
    "published_date": "2025-10-09 21:54:14 UTC",
    "updated_date": "2025-10-09 21:54:14 UTC"
  },
  {
    "arxiv_id": "2510.08831v1",
    "title": "Everyone prefers human writers, including AI",
    "authors": [
      "Wouter Haverals",
      "Meredith Martin"
    ],
    "abstract": "As AI writing tools become widespread, we need to understand how both humans and machines evaluate literary style, a domain where objective standards are elusive and judgments are inherently subjective. We conducted controlled experiments using Raymond Queneau's Exercises in Style (1947) to measure attribution bias across evaluators. Study 1 compared human participants (N=556) and AI models (N=13) evaluating literary passages from Queneau versus GPT-4-generated versions under three conditions: blind, accurately labeled, and counterfactually labeled. Study 2 tested bias generalization across a 14$\\times$14 matrix of AI evaluators and creators. Both studies revealed systematic pro-human attribution bias. Humans showed +13.7 percentage point (pp) bias (Cohen's h = 0.28, 95% CI: 0.21-0.34), while AI models showed +34.3 percentage point bias (h = 0.70, 95% CI: 0.65-0.76), a 2.5-fold stronger effect (P$<$0.001). Study 2 confirmed this bias operates across AI architectures (+25.8pp, 95% CI: 24.1-27.6%), demonstrating that AI systems systematically devalue creative content when labeled as \"AI-generated\" regardless of which AI created it. We also find that attribution labels cause evaluators to invert assessment criteria, with identical features receiving opposing evaluations based solely on perceived authorship. This suggests AI models have absorbed human cultural biases against artificial creativity during training. Our study represents the first controlled comparison of attribution bias between human and artificial evaluators in aesthetic judgment, revealing that AI systems not only replicate but amplify this human tendency.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "46 pages, 18 figures (5 main text + 13 supplementary), 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08831v1",
    "published_date": "2025-10-09 21:33:30 UTC",
    "updated_date": "2025-10-09 21:33:30 UTC"
  },
  {
    "arxiv_id": "2510.08829v1",
    "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization",
    "authors": [
      "Debeshee Das",
      "Luca Beurer-Kellner",
      "Marc Fischer",
      "Maximilian Baader"
    ],
    "abstract": "The increasing adoption of LLM agents with access to numerous tools and sensitive data significantly widens the attack surface for indirect prompt injections. Due to the context-dependent nature of attacks, however, current defenses are often ill-calibrated as they cannot reliably differentiate malicious and benign instructions, leading to high false positive rates that prevent their real-world adoption. To address this, we present a novel approach inspired by the fundamental principle of computer security: data should not contain executable instructions. Instead of sample-level classification, we propose a token-level sanitization process, which surgically removes any instructions directed at AI systems from tool outputs, capturing malicious instructions as a byproduct. In contrast to existing safety classifiers, this approach is non-blocking, does not require calibration, and is agnostic to the context of tool outputs. Further, we can train such token-level predictors with readily available instruction-tuning data only, and don't have to rely on unrealistic prompt injection examples from challenges or of other synthetic origin. In our experiments, we find that this approach generalizes well across a wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB and SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on AgentDojo), without impairing agent utility in both benign and malicious settings.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08829v1",
    "published_date": "2025-10-09 21:32:02 UTC",
    "updated_date": "2025-10-09 21:32:02 UTC"
  },
  {
    "arxiv_id": "2510.08827v1",
    "title": "McMining: Automated Discovery of Misconceptions in Student Code",
    "authors": [
      "Erfan Al-Hossami",
      "Razvan Bunescu"
    ],
    "abstract": "When learning to code, students often develop misconceptions about various programming language concepts. These can not only lead to bugs or inefficient code, but also slow down the learning of related concepts. In this paper, we introduce McMining, the task of mining programming misconceptions from samples of code from a student. To enable the training and evaluation of McMining systems, we develop an extensible benchmark dataset of misconceptions together with a large set of code samples where these misconceptions are manifested. We then introduce two LLM-based McMiner approaches and through extensive evaluations show that models from the Gemini, Claude, and GPT families are effective at discovering misconceptions in student code.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.SE",
    "comment": "16 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08827v1",
    "published_date": "2025-10-09 21:27:39 UTC",
    "updated_date": "2025-10-09 21:27:39 UTC"
  },
  {
    "arxiv_id": "2510.08818v1",
    "title": "D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition",
    "authors": [
      "Yiyang Huang",
      "Yizhou Wang",
      "Yun Fu"
    ],
    "abstract": "Video large language models (Vid-LLMs), which excel in diverse video-language tasks, can be effectively constructed by adapting image-pretrained vision-language models (VLMs). However, this adaptation remains challenging, as it requires processing dense and temporally extended visual inputs that exceed the capacity of image-based models. This paper identifies the perception bottleneck and token overload as key challenges in extending image-based VLMs to the video domain. To address these issues, we propose D-CoDe, a training-free adaptation framework that incorporates dynamic compression and question decomposition. Specifically, dynamic compression alleviates the perception bottleneck through adaptive selection of representative frames and content-aware aggregation of spatial tokens, thereby reducing redundancy while preserving informative content. In parallel, question decomposition mitigates token overload by reformulating the original query into sub-questions, guiding the model to focus on distinct aspects of the video and enabling more comprehensive understanding. Experiments demonstrate that D-CoDe effectively improves video understanding across various benchmarks. Furthermore, strong performance on the challenging long-video benchmark highlights the potential of D-CoDe in handling complex video-language tasks. Code is available at https://github.com/hukcc/D-CoDe.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted to EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08818v1",
    "published_date": "2025-10-09 21:08:32 UTC",
    "updated_date": "2025-10-09 21:08:32 UTC"
  },
  {
    "arxiv_id": "2510.08814v1",
    "title": "$\\mathsf{P} \\neq \\mathsf{NP}$: A Non-Relativizing Proof via Quantale Weakness and Geometric Complexity",
    "authors": [
      "Ben Goertzel"
    ],
    "abstract": "We give a compositional, information-theoretic framework that turns short programs into locality on many independent blocks, and combine it with symmetry and sparsity of masked random Unique-SAT to obtain distributional lower bounds that contradict the self-reduction upper bound under $\\mathsf{P}=\\mathsf{NP}$. We work in the weakness quantale $w_Q=K_{\\mathrm{poly}}(\\cdot\\mid\\cdot)$. For an efficiently samplable ensemble $D_m$ made by masking random $3$-CNFs with fresh $S_m\\ltimes(\\mathbb{Z}_2)^m$ symmetries and a small-seed Valiant--Vazirani isolation layer, we prove a Switching-by-Weakness normal form: for any polytime decoder $P$ of description length $\\le δt$ (with $t=Θ(m)$ blocks), a short wrapper $W$ makes $(P\\circ W)$ per-bit local on a $γ$-fraction of blocks. Two ingredients then force near-randomness on $Ω(t)$ blocks for every short decoder: (a) a sign-invariant neutrality lemma giving $\\Pr[X_i=1\\mid \\mathcal{I}]=1/2$ for any sign-invariant view $\\mathcal{I}$; and (b) a template sparsification theorem at logarithmic radius showing that any fixed local rule appears with probability $m^{-Ω(1)}$. Combined with single-block bounds for tiny $\\mathrm{ACC}^0$/streaming decoders, this yields a success bound $2^{-Ω(t)}$ and, by Compression-from-Success, $K_{\\mathrm{poly}}\\big((X_1,\\ldots,X_t)\\mid(Φ_1,\\ldots,Φ_t)\\big)\\ge ηt$. If $\\mathsf{P}=\\mathsf{NP}$, a uniform constant-length program maps any on-promise instance to its unique witness in polytime (bit fixing via a $\\mathrm{USAT}$ decider), so $K_{\\mathrm{poly}}(X\\midΦ)\\le O(1)$ and the tuple complexity is $O(1)$, contradicting the linear bound. The proof is non-relativizing and non-natural; symmetry, sparsification, and switching yield a quantale upper-lower clash, hence $\\mathsf{P}\\ne\\mathsf{NP}$.",
    "categories": [
      "cs.CC",
      "cs.AI"
    ],
    "primary_category": "cs.CC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08814v1",
    "published_date": "2025-10-09 21:01:17 UTC",
    "updated_date": "2025-10-09 21:01:17 UTC"
  },
  {
    "arxiv_id": "2510.08812v2",
    "title": "Adaptive Science Operations in Deep Space Missions Using Offline Belief State Planning",
    "authors": [
      "Grace Ra Kim",
      "Hailey Warner",
      "Duncan Eddy",
      "Evan Astle",
      "Zachary Booth",
      "Edward Balaban",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Deep space missions face extreme communication delays and environmental uncertainty that prevent real-time ground operations. To support autonomous science operations in communication-constrained environments, we present a partially observable Markov decision process (POMDP) framework that adaptively sequences spacecraft science instruments. We integrate a Bayesian network into the POMDP observation space to manage the high-dimensional and uncertain measurements typical of astrobiology missions. This network compactly encodes dependencies among measurements and improves the interpretability and computational tractability of science data. Instrument operation policies are computed offline, allowing resource-aware plans to be generated and thoroughly validated prior to launch. We use the Enceladus Orbilander's proposed Life Detection Suite (LDS) as a case study, demonstrating how Bayesian network structure and reward shaping influence system performance. We compare our method against the mission's baseline Concept of Operations (ConOps), evaluating both misclassification rates and performance in off-nominal sample accumulation scenarios. Our approach reduces sample identification errors by nearly 40%",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 4 tables, 5 figures, accepted in IEEE ISPARO 2025 (V2 - grammatical edits, also mispelled conference year)",
    "pdf_url": "https://arxiv.org/pdf/2510.08812v2",
    "published_date": "2025-10-09 20:58:35 UTC",
    "updated_date": "2026-01-10 22:32:11 UTC"
  },
  {
    "arxiv_id": "2510.08800v1",
    "title": "Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning Perspective",
    "authors": [
      "Wangjie You",
      "Xusheng Wang",
      "Xing Wang",
      "Wenxiang Jiao",
      "Chao Feng",
      "Juntao Li",
      "Min Zhang"
    ],
    "abstract": "While Large Language Models (LLMs) have demonstrated advanced reasoning capabilities, their comprehensive evaluation in general Chinese-language contexts remains understudied. To bridge this gap, we propose Chinese Commonsense Multi-hop Reasoning (CCMOR), a novel benchmark designed to evaluate LLMs' ability to integrate Chinese-specific factual knowledge with multi-step logical reasoning. Specifically, we first construct a domain-balanced seed set from existing QA datasets, then develop an LLM-powered pipeline to generate multi-hop questions anchored on factual unit chains. To ensure the quality of resulting dataset, we implement a human-in-the-loop verification system, where domain experts systematically validate and refine the generated questions. Using CCMOR, we evaluate state-of-the-art LLMs, demonstrating persistent limitations in LLMs' ability to process long-tail knowledge and execute knowledge-intensive reasoning. Notably, retrieval-augmented generation substantially mitigates these knowledge gaps, yielding significant performance gains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08800v1",
    "published_date": "2025-10-09 20:29:00 UTC",
    "updated_date": "2025-10-09 20:29:00 UTC"
  },
  {
    "arxiv_id": "2510.08799v1",
    "title": "SkipSR: Faster Super Resolution with Token Skipping",
    "authors": [
      "Rohan Choudhury",
      "Shanchuan Lin",
      "Jianyi Wang",
      "Hao Chen",
      "Qi Zhao",
      "Feng Cheng",
      "Lu Jiang",
      "Kris Kitani",
      "Laszlo A. Jeni"
    ],
    "abstract": "Diffusion-based super-resolution (SR) is a key component in video generation and video restoration, but is slow and expensive, limiting scalability to higher resolutions and longer videos. Our key insight is that many regions in video are inherently low-detail and gain little from refinement, yet current methods process all pixels uniformly. To take advantage of this, we propose SkipSR, a simple framework for accelerating video SR by identifying low-detail regions directly from low-resolution input, then skipping computation on them entirely, only super-resolving the areas that require refinement. This simple yet effective strategy preserves perceptual quality in both standard and one-step diffusion SR models while significantly reducing computation. In standard SR benchmarks, our method achieves up to 60% faster end-to-end latency than prior models on 720p videos with no perceptible loss in quality. Video demos are available at https://rccchoudhury.github.io/skipsr/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08799v1",
    "published_date": "2025-10-09 20:27:11 UTC",
    "updated_date": "2025-10-09 20:27:11 UTC"
  },
  {
    "arxiv_id": "2510.15945v1",
    "title": "BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling",
    "authors": [
      "Guangya Wan",
      "Zixin Stephen Xu",
      "Sasa Zorc",
      "Manel Baucells",
      "Mengxuan Hu",
      "Hao Wang",
      "Sheng Li"
    ],
    "abstract": "Sampling multiple responses is a common way to improve LLM output quality, but it comes at the cost of additional computation. The key challenge is deciding when to stop generating new samples to balance accuracy gains against efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive Criterion for Optimal N-stopping), a principled adaptive sampling framework grounded in Sequential Search with Bayesian Learning. BEACON sequentially generates responses from the policy LLM, updates posterior belief over reward distributions in real time without further training, and determines when to stop by weighing expected gains against computational cost. Sampling terminates once the marginal utility of further exploration no longer justifies the expense. We establish both theoretical optimality guarantees and practical tractability, and show empirically that BEACON reduces average sampling by up to 80% while maintaining response quality. We further demonstrate BEACON's utility for cost-efficient preference data generation and outline practical extensions, offering actionable insights for future researchers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review on ARR",
    "pdf_url": "https://arxiv.org/pdf/2510.15945v1",
    "published_date": "2025-10-09 20:26:25 UTC",
    "updated_date": "2025-10-09 20:26:25 UTC"
  },
  {
    "arxiv_id": "2510.08794v1",
    "title": "Deceptive Exploration in Multi-armed Bandits",
    "authors": [
      "I. Arda Vurankaya",
      "Mustafa O. Karabag",
      "Wesley A. Suttle",
      "Jesse Milzman",
      "David Fridovich-Keil",
      "Ufuk Topcu"
    ],
    "abstract": "We consider a multi-armed bandit setting in which each arm has a public and a private reward distribution. An observer expects an agent to follow Thompson Sampling according to the public rewards, however, the deceptive agent aims to quickly identify the best private arm without being noticed. The observer can observe the public rewards and the pulled arms, but not the private rewards. The agent, on the other hand, observes both the public and private rewards. We formalize detectability as a stepwise Kullback-Leibler (KL) divergence constraint between the actual pull probabilities used by the agent and the anticipated pull probabilities by the observer. We model successful pulling of public suboptimal arms as a % Bernoulli process where the success probability decreases with each successful pull, and show these pulls can happen at most at a $Θ(\\sqrt{T}) $ rate under the KL constraint. We then formulate a maximin problem based on public and private means, whose solution characterizes the optimal error exponent for best private arm identification. We finally propose an algorithm inspired by top-two algorithms. This algorithm naturally adapts its exploration according to the hardness of pulling arms based on the public suboptimality gaps. We provide numerical examples illustrating the $Θ(\\sqrt{T}) $ rate and the behavior of the proposed algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08794v1",
    "published_date": "2025-10-09 20:15:52 UTC",
    "updated_date": "2025-10-09 20:15:52 UTC"
  },
  {
    "arxiv_id": "2510.08790v1",
    "title": "COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context",
    "authors": [
      "Guangya Wan",
      "Mingyang Ling",
      "Xiaoqi Ren",
      "Rujun Han",
      "Sheng Li",
      "Zizhao Zhang"
    ],
    "abstract": "Long-horizon tasks that require sustained reasoning and multiple tool interactions remain challenging for LLM agents: small errors compound across steps, and even state-of-the-art models often hallucinate or lose coherence. We identify context management as the central bottleneck -- extended histories cause agents to overlook critical evidence or become distracted by irrelevant information, thus failing to replan or reflect from previous mistakes. To address this, we propose COMPASS (Context-Organized Multi-Agent Planning and Strategy System), a lightweight hierarchical framework that separates tactical execution, strategic oversight, and context organization into three specialized components: (1) a Main Agent that performs reasoning and tool use, (2) a Meta-Thinker that monitors progress and issues strategic interventions, and (3) a Context Manager that maintains concise, relevant progress briefs for different reasoning stages. Across three challenging benchmarks -- GAIA, BrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20% relative to both single- and multi-agent baselines. We further introduce a test-time scaling extension that elevates performance to match established DeepResearch agents, and a post-training pipeline that delegates context management to smaller models for enhanced efficiency.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review for ACL",
    "pdf_url": "https://arxiv.org/pdf/2510.08790v1",
    "published_date": "2025-10-09 20:14:26 UTC",
    "updated_date": "2025-10-09 20:14:26 UTC"
  },
  {
    "arxiv_id": "2510.13826v1",
    "title": "Towards Neurocognitive-Inspired Intelligence: From AI's Structural Mimicry to Human-Like Functional Cognition",
    "authors": [
      "Noorbakhsh Amiri Golilarz",
      "Hassan S. Al Khatib",
      "Shahram Rahimi"
    ],
    "abstract": "Artificial intelligence has advanced significantly through deep learning, reinforcement learning, and large language and vision models. However, these systems often remain task specific, struggle to adapt to changing conditions, and cannot generalize in ways similar to human cognition. Additionally, they mainly focus on mimicking brain structures, which often leads to black-box models with limited transparency and adaptability. Inspired by the structure and function of biological cognition, this paper introduces the concept of \"Neurocognitive-Inspired Intelligence (NII),\" a hybrid approach that combines neuroscience, cognitive science, computer vision, and AI to develop more general, adaptive, and robust intelligent systems capable of rapid learning, learning from less data, and leveraging prior experience. These systems aim to emulate the human brain's ability to flexibly learn, reason, remember, perceive, and act in real-world settings with minimal supervision. We review the limitations of current AI methods, define core principles of neurocognitive-inspired intelligence, and propose a modular, biologically inspired architecture that emphasizes integration, embodiment, and adaptability. We also discuss potential implementation strategies and outline various real-world applications, from robotics to education and healthcare. Importantly, this paper offers a hybrid roadmap for future research, laying the groundwork for building AI systems that more closely resemble human cognition.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13826v1",
    "published_date": "2025-10-09 20:10:55 UTC",
    "updated_date": "2025-10-09 20:10:55 UTC"
  },
  {
    "arxiv_id": "2510.08783v1",
    "title": "MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces",
    "authors": [
      "Reuben A. Luera",
      "Ryan Rossi",
      "Franck Dernoncourt",
      "Samyadeep Basu",
      "Sungchul Kim",
      "Subhojyoti Mukherjee",
      "Puneet Mathur",
      "Ruiyi Zhang",
      "Jihyung Kil",
      "Nedim Lipka",
      "Seunghyun Yoon",
      "Jiuxiang Gu",
      "Zichao Wang",
      "Cindy Xiong Bearfield",
      "Branislav Kveton"
    ],
    "abstract": "In an ideal design pipeline, user interface (UI) design is intertwined with user research to validate decisions, yet studies are often resource-constrained during early exploration. Recent advances in multimodal large language models (MLLMs) offer a promising opportunity to act as early evaluators, helping designers narrow options before formal testing. Unlike prior work that emphasizes user behavior in narrow domains such as e-commerce with metrics like clicks or conversions, we focus on subjective user evaluations across varied interfaces. We investigate whether MLLMs can mimic human preferences when evaluating individual UIs and comparing them. Using data from a crowdsourcing platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and examine alignment with human judgments on multiple UI factors. Our results show that MLLMs approximate human preferences on some dimensions but diverge on others, underscoring both their potential and limitations in supplementing early UX research.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08783v1",
    "published_date": "2025-10-09 20:00:41 UTC",
    "updated_date": "2025-10-09 20:00:41 UTC"
  },
  {
    "arxiv_id": "2510.08779v1",
    "title": "Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations",
    "authors": [
      "Vaibhav Jain",
      "Gerrit Grossmann"
    ],
    "abstract": "Reinforcement Learning (RL) agents often struggle in sparse-reward environments where traditional exploration strategies fail to discover effective action sequences. Large Language Models (LLMs) possess procedural knowledge and reasoning capabilities from text pretraining that could guide RL exploration, but existing approaches create rigid dependencies where RL policies must follow LLM suggestions or incorporate them directly into reward functions. We propose a framework that provides LLM-generated action recommendations through augmented observation spaces, allowing RL agents to learn when to follow or ignore this guidance. Our method leverages LLMs' world knowledge and reasoning abilities while maintaining flexibility through soft constraints. We evaluate our approach on three BabyAI environments of increasing complexity and show that the benefits of LLM guidance scale with task difficulty. In the most challenging environment, we achieve 71% relative improvement in final success rates over baseline. The approach provides substantial sample efficiency gains, with agents reaching performance thresholds up to 9 times faster, and requires no modifications to existing RL algorithms. Our results demonstrate an effective method for leveraging LLM planning capabilities to accelerate RL training in challenging environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to LM4Plan Workshop @ ICAPS 2025 (withdrawn before presentation due to lack of travel funding)",
    "pdf_url": "https://arxiv.org/pdf/2510.08779v1",
    "published_date": "2025-10-09 19:54:31 UTC",
    "updated_date": "2025-10-09 19:54:31 UTC"
  },
  {
    "arxiv_id": "2510.08776v1",
    "title": "Measuring Moral LLM Responses in Multilingual Capacities",
    "authors": [
      "Kimaya Basu",
      "Savi Kolari",
      "Allison Yu"
    ],
    "abstract": "With LLM usage becoming widespread across countries, languages, and humanity more broadly, the need to understand and guardrail their multilingual responses increases. Large-scale datasets for testing and benchmarking have been created to evaluate and facilitate LLM responses across multiple dimensions. In this study, we evaluate the responses of frontier and leading open-source models in five dimensions across low and high-resource languages to measure LLM accuracy and consistency across multilingual contexts. We evaluate the responses using a five-point grading rubric and a judge LLM. Our study shows that GPT-5 performed the best on average in each category, while other models displayed more inconsistency across language and category. Most notably, in the Consent & Autonomy and Harm Prevention & Safety categories, GPT scored the highest with averages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages of 1.39 and 1.98, respectively. These findings emphasize the need for further testing on how linguistic shifts impact LLM responses across various categories and improvement in these areas.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures; referenced articles: arXiv:2303.08774, arXiv:2303.12528, arXiv:2308.14132, arXiv:2505.12201, arXiv:2406.04428, arXiv:2407.02273, arXiv:2404.01268, arXiv:2502.09747, arXiv:2507.13474, arXiv:2505.21479, arXiv:2306.05685",
    "pdf_url": "https://arxiv.org/pdf/2510.08776v1",
    "published_date": "2025-10-09 19:47:40 UTC",
    "updated_date": "2025-10-09 19:47:40 UTC"
  },
  {
    "arxiv_id": "2510.08775v1",
    "title": "Re-Identifying Kākā with AI-Automated Video Key Frame Extraction",
    "authors": [
      "Paula Maddigan",
      "Andrew Lensen",
      "Rachael C. Shaw"
    ],
    "abstract": "Accurate recognition and re-identification of individual animals is essential for successful wildlife population monitoring. Traditional methods, such as leg banding of birds, are time consuming and invasive. Recent progress in artificial intelligence, particularly computer vision, offers encouraging solutions for smart conservation and efficient automation. This study presents a unique pipeline for extracting high-quality key frames from videos of kākā (Nestor meridionalis), a threatened forest-dwelling parrot in New Zealand. Key frame extraction is well-studied in person re-identification, however, its application to wildlife is limited. Using video recordings at a custom-built feeder, we extract key frames and evaluate the re-identification performance of our pipeline. Our unsupervised methodology combines object detection using YOLO and Grounding DINO, optical flow blur detection, image encoding with DINOv2, and clustering methods to identify representative key frames. The results indicate that our proposed key frame selection methods yield image collections which achieve high accuracy in kākā re-identification, providing a foundation for future research using media collected in more diverse and challenging environments. Through the use of artificial intelligence and computer vision, our non-invasive and efficient approach provides a valuable alternative to traditional physical tagging methods for recognising kākā individuals and therefore improving the monitoring of populations. This research contributes to developing fresh approaches in wildlife monitoring, with applications in ecology and conservation biology.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08775v1",
    "published_date": "2025-10-09 19:46:46 UTC",
    "updated_date": "2025-10-09 19:46:46 UTC"
  },
  {
    "arxiv_id": "2510.08774v1",
    "title": "Struc-EMB: The Potential of Structure-Aware Encoding in Language Embeddings",
    "authors": [
      "Shikun Liu",
      "Haoyu Wang",
      "Mufei Li",
      "Pan Li"
    ],
    "abstract": "Text embeddings from Large Language Models (LLMs) have become foundational for numerous applications. However, these models typically operate on raw text, overlooking the rich structural information, such as hyperlinks or citations, that provides crucial context in many real-world datasets. This paper introduces and systematically evaluates a new paradigm for generating structure-aware text embeddings by integrating these structural relations directly into the LLM's internal encoding process, rather than relying on traditional post-hoc aggregation. We investigate two primary in-process methods: sequential concatenation and parallel caching. Through extensive zero-shot experiments across retrieval, clustering, classification, and recommendation tasks, we demonstrate that our structure-aware approaches consistently outperform both text-only and post-hoc baselines. Our analysis reveals critical trade-offs: sequential concatenation excels with noisy, moderate-length contexts, while parallel caching scales more effectively to long, high-signal contexts but is more susceptible to distractors. To address the challenge of noisy structural data, we also introduce and validate two effective techniques: Context Distillation and Semantic Balancing. This work provides the first comprehensive analysis of in-process structure-aware encoding, offering a blueprint for building more powerful and contextually aware embedding models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08774v1",
    "published_date": "2025-10-09 19:45:54 UTC",
    "updated_date": "2025-10-09 19:45:54 UTC"
  },
  {
    "arxiv_id": "2510.08761v1",
    "title": "SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense",
    "authors": [
      "Jiayang Liu",
      "Daniel Tso",
      "Yiming Bu",
      "Qinru Qiu"
    ],
    "abstract": "Adversarial attacks significantly challenge the safe deployment of deep learning models, particularly in real-world applications. Traditional defenses often rely on computationally intensive optimization (e.g., adversarial training or data augmentation) to improve robustness, whereas the human visual system achieves inherent robustness to adversarial perturbations through evolved biological mechanisms. We hypothesize that attention guided non-homogeneous sparse sampling and predictive coding plays a key role in this robustness. To test this hypothesis, we propose a novel defense framework incorporating three key biological mechanisms: foveal-peripheral processing, saccadic eye movements, and cortical filling-in. Our approach employs reinforcement learning-guided saccades to selectively capture multiple foveal-peripheral glimpses, which are integrated into a reconstructed image before classification. This biologically inspired preprocessing effectively mitigates adversarial noise, preserves semantic integrity, and notably requires no retraining or fine-tuning of downstream classifiers, enabling seamless integration with existing systems. Experiments on the ImageNet dataset demonstrate that our method improves system robustness across diverse classifiers and attack types, while significantly reducing training overhead compared to both biologically and non-biologically inspired defense techniques.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08761v1",
    "published_date": "2025-10-09 19:23:19 UTC",
    "updated_date": "2025-10-09 19:23:19 UTC"
  },
  {
    "arxiv_id": "2510.08755v1",
    "title": "Robust Heuristic Algorithm Design with LLMs",
    "authors": [
      "Pantea Karimi",
      "Dany Rouhana",
      "Pooria Namyar",
      "Siva Kesava Reddy Kakarla",
      "Venkat Arun",
      "Behnaz Arzani"
    ],
    "abstract": "We posit that we can generate more robust and performant heuristics if we augment approaches using LLMs for heuristic design with tools that explain why heuristics underperform and suggestions about how to fix them. We find even simple ideas that (1) expose the LLM to instances where the heuristic underperforms; (2) explain why they occur; and (3) specialize design to regions in the input space, can produce more robust algorithms compared to existing techniques~ -- ~the heuristics we produce have a $\\sim28\\times$ better worst-case performance compared to FunSearch, improve average performance, and maintain the runtime.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08755v1",
    "published_date": "2025-10-09 19:13:56 UTC",
    "updated_date": "2025-10-09 19:13:56 UTC"
  },
  {
    "arxiv_id": "2510.08744v1",
    "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
    "authors": [
      "Gang Liu",
      "Jie Chen",
      "Yihan Zhu",
      "Michael Sun",
      "Tengfei Luo",
      "Nitesh V Chawla",
      "Meng Jiang"
    ],
    "abstract": "In-context learning allows large models to adapt to new tasks from a few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using a small set of molecule-score examples instead of text descriptions. These demonstrations guide a denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop a new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5$\\times$ fewer nodes. We curate a dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain a 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 100-1000$\\times$ larger and achieves an average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches. These results position DemoDiff as a molecular foundation model for in-context molecular design. Our code is available at https://github.com/liugangcode/DemoDiff.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 16 figures, 17 tables. Model available at: https://huggingface.co/liuganghuggingface/DemoDiff-0.7B",
    "pdf_url": "https://arxiv.org/pdf/2510.08744v1",
    "published_date": "2025-10-09 18:56:57 UTC",
    "updated_date": "2025-10-09 18:56:57 UTC"
  },
  {
    "arxiv_id": "2510.15944v1",
    "title": "Lyapunov-Stable Adaptive Control for Multimodal Concept Drift",
    "authors": [
      "Tianyu Bell Pan",
      "Mengdi Zhu",
      "Alexa Jordyn Cole",
      "Ronald Wilson",
      "Damon L. Woodard"
    ],
    "abstract": "Multimodal learning systems often struggle in non-stationary environments due to concept drift, where changing data distributions can degrade performance. Modality-specific drifts and the lack of mechanisms for continuous, stable adaptation compound this challenge. This paper introduces LS-OGD, a novel adaptive control framework for robust multimodal learning in the presence of concept drift. LS-OGD uses an online controller that dynamically adjusts the model's learning rate and the fusion weights between different data modalities in response to detected drift and evolving prediction errors. We prove that under bounded drift conditions, the LS-OGD system's prediction error is uniformly ultimately bounded and converges to zero if the drift ceases. Additionally, we demonstrate that the adaptive fusion strategy effectively isolates and mitigates the impact of severe modality-specific drift, thereby ensuring system resilience and fault tolerance. These theoretical guarantees establish a principled foundation for developing reliable and continuously adapting multimodal learning systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15944v1",
    "published_date": "2025-10-09 18:55:26 UTC",
    "updated_date": "2025-10-09 18:55:26 UTC"
  },
  {
    "arxiv_id": "2510.08741v1",
    "title": "Coordinates from Context: Using LLMs to Ground Complex Location References",
    "authors": [
      "Tessa Masis",
      "Brendan O'Connor"
    ],
    "abstract": "Geocoding is the task of linking a location reference to an actual geographic location and is essential for many downstream analyses of unstructured text. In this paper, we explore the challenging setting of geocoding compositional location references. Building on recent work demonstrating LLMs' abilities to reason over geospatial data, we evaluate LLMs' geospatial knowledge versus reasoning skills relevant to our task. Based on these insights, we propose an LLM-based strategy for geocoding compositional location references. We show that our approach improves performance for the task and that a relatively small fine-tuned LLM can achieve comparable performance with much larger off-the-shelf models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review at ARR",
    "pdf_url": "https://arxiv.org/pdf/2510.08741v1",
    "published_date": "2025-10-09 18:51:52 UTC",
    "updated_date": "2025-10-09 18:51:52 UTC"
  },
  {
    "arxiv_id": "2510.08731v1",
    "title": "When to Reason: Semantic Router for vLLM",
    "authors": [
      "Chen Wang",
      "Xunzhuo Liu",
      "Yuhan Liu",
      "Yue Zhu",
      "Xiangxi Mo",
      "Junchen Jiang",
      "Huamin Chen"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate substantial accuracy gains when augmented with reasoning modes such as chain-of-thought and inference-time scaling. However, reasoning also incurs significant costs in inference latency and token usage, with environmental and financial impacts, which are unnecessary for many simple prompts. We present a semantic router that classifies queries based on their reasoning requirements and selectively applies reasoning only when beneficial. Our approach achieves a 10.2 percentage point improvement in accuracy on the MMLU-Pro benchmark while reducing response latency by 47.1% and token consumption by 48.5% compared to direct inference with vLLM. These results demonstrate that semantic routing offers an effective mechanism for striking a balance between accuracy and efficiency in open-source LLM serving systems",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.CL",
      "eess.SY"
    ],
    "primary_category": "cs.ET",
    "comment": "5 pages, excluding references and appendix. To be appeared at Workshop on ML for Systems at NeurIPS 2025, December 6, 2025 https://mlforsystems.org/",
    "pdf_url": "https://arxiv.org/pdf/2510.08731v1",
    "published_date": "2025-10-09 18:38:00 UTC",
    "updated_date": "2025-10-09 18:38:00 UTC"
  },
  {
    "arxiv_id": "2510.08722v2",
    "title": "Enhancing Self-Supervised Learning with Semantic Pairs A New Dataset and Empirical Study",
    "authors": [
      "Mohammad Alkhalefi",
      "Georgios Leontidis",
      "Mingjun Zhong"
    ],
    "abstract": "Instance discrimination is a self-supervised representation learning paradigm wherein individual instances within a dataset are treated as distinct classes. This is typically achieved by generating two disparate views of each instance by applying stochastic transformations, encouraging the model to learn representations invariant to the common underlying object across these views. While this approach facilitates the acquisition of invariant representations for dataset instances under various handcrafted transformations (e.g., random cropping, colour jittering), an exclusive reliance on such data transformations for achieving invariance may inherently limit the model's generalizability to unseen datasets and diverse downstream tasks. The inherent limitation stems from the fact that the finite set of transformations within the data processing pipeline is unable to encompass the full spectrum of potential data variations. In this study, we provide the technical foundation for leveraging semantic pairs to enhance the generalizability of the model's representation and empirically demonstrate that incorporating semantic pairs mitigates the issue of limited transformation coverage. Specifically, we propose that by exposing the model to semantic pairs (i.e., two instances belonging to the same semantic category), we introduce varied real-world scene contexts, thereby fostering the development of more generalizable object representations. To validate this hypothesis, we constructed and released a novel dataset comprising curated semantic pairs and conducted extensive experimentation to empirically establish that their inclusion enables the model to learn more general representations, ultimately leading to improved performance across diverse downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 7 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08722v2",
    "published_date": "2025-10-09 18:31:55 UTC",
    "updated_date": "2025-10-13 09:09:06 UTC"
  },
  {
    "arxiv_id": "2510.08713v1",
    "title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation",
    "authors": [
      "Yifei Dong",
      "Fengyi Wu",
      "Guangyu Chen",
      "Zhi-Qi Cheng",
      "Qiyu Hu",
      "Yuxuan Zhou",
      "Jingdong Sun",
      "Jun-Yan He",
      "Qi Dai",
      "Alexander G Hauptmann"
    ],
    "abstract": "Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state-action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 11 figures, code: https://github.com/F1y1113/UniWM",
    "pdf_url": "https://arxiv.org/pdf/2510.08713v1",
    "published_date": "2025-10-09 18:18:11 UTC",
    "updated_date": "2025-10-09 18:18:11 UTC"
  },
  {
    "arxiv_id": "2510.08711v1",
    "title": "In-Context Learning for Non-Stationary MIMO Equalization",
    "authors": [
      "Jiachen Jiang",
      "Zhen Qin",
      "Zhihui Zhu"
    ],
    "abstract": "Channel equalization is fundamental for mitigating distortions such as frequency-selective fading and inter-symbol interference. Unlike standard supervised learning approaches that require costly retraining or fine-tuning for each new task, in-context learning (ICL) adapts to new channels at inference time with only a few examples. However, existing ICL-based equalizers are primarily developed for and evaluated on static channels within the context window. Indeed, to our knowledge, prior principled analyses and theoretical studies of ICL focus exclusively on the stationary setting, where the function remains fixed within the context. In this paper, we investigate the ability of ICL to address non-stationary problems through the lens of time-varying channel equalization. We employ a principled framework for designing efficient attention mechanisms with improved adaptivity in non-stationary tasks, leveraging algorithms from adaptive signal processing to guide better designs. For example, new attention variants can be derived from the Least Mean Square (LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for enhanced robustness, or multi-step gradient updates for improved long-term tracking. Experimental results demonstrate that ICL holds strong promise for non-stationary MIMO equalization, and that attention mechanisms inspired by classical adaptive algorithms can substantially enhance adaptability and performance in dynamic environments. Our findings may provide critical insights for developing next-generation wireless foundation models with stronger adaptability and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08711v1",
    "published_date": "2025-10-09 18:16:41 UTC",
    "updated_date": "2025-10-09 18:16:41 UTC"
  },
  {
    "arxiv_id": "2510.08705v1",
    "title": "ConPoSe: LLM-Guided Contact Point Selection for Scalable Cooperative Object Pushing",
    "authors": [
      "Noah Steinkrüger",
      "Nisarga Nilavadi",
      "Wolfram Burgard",
      "Tanja Katharina Kaiser"
    ],
    "abstract": "Object transportation in cluttered environments is a fundamental task in various domains, including domestic service and warehouse logistics. In cooperative object transport, multiple robots must coordinate to move objects that are too large for a single robot. One transport strategy is pushing, which only requires simple robots. However, careful selection of robot-object contact points is necessary to push the object along a preplanned path. Although this selection can be solved analytically, the solution space grows combinatorially with the number of robots and object size, limiting scalability. Inspired by how humans rely on common-sense reasoning for cooperative transport, we propose combining the reasoning capabilities of Large Language Models with local search to select suitable contact points. Our LLM-guided local search method for contact point selection, ConPoSe, successfully selects contact points for a variety of shapes, including cuboids, cylinders, and T-shapes. We demonstrate that ConPoSe scales better with the number of robots and object size than the analytical approach, and also outperforms pure LLM-based selection.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08705v1",
    "published_date": "2025-10-09 18:07:39 UTC",
    "updated_date": "2025-10-09 18:07:39 UTC"
  },
  {
    "arxiv_id": "2510.08697v2",
    "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution",
    "authors": [
      "Terry Yue Zhuo",
      "Xiaolong Jin",
      "Hange Liu",
      "Juyong Jiang",
      "Tianyang Liu",
      "Chen Gong",
      "Bhupesh Bishnoi",
      "Vaisakhi Mishra",
      "Marek Suppa",
      "Noah Ziems",
      "Saiteja Utpala",
      "Ming Xu",
      "Guangyu Song",
      "Kaixin Li",
      "Yuhan Cao",
      "Bo Liu",
      "Zheng Liu",
      "Sabina Abdurakhmanova",
      "Wenhao Yu",
      "Mengzhao Jia",
      "Jihan Yao",
      "Kenneth Hamilton",
      "Kumar Shridhar",
      "Minh Chien Vu",
      "Dingmin Wang",
      "Jiawei Liu",
      "Zijian Wang",
      "Qian Liu",
      "Binyuan Hui",
      "Meg Risdal",
      "Ahsen Khaliq",
      "Atin Sood",
      "Zhenchang Xing",
      "Wasi Uddin Ahmad",
      "John Grundy",
      "David Lo",
      "Banghua Zhu",
      "Xiaoning Du",
      "Torsten Scholak",
      "Leandro von Werra"
    ],
    "abstract": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Built with love by the BigCode community :)",
    "pdf_url": "https://arxiv.org/pdf/2510.08697v2",
    "published_date": "2025-10-09 18:01:47 UTC",
    "updated_date": "2025-12-18 15:19:25 UTC"
  },
  {
    "arxiv_id": "2510.08572v1",
    "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation",
    "authors": [
      "Rocktim Jyoti Das",
      "Harsh Singh",
      "Diana Turmakhan",
      "Muhammad Abdullah Sohail",
      "Mingfei Han",
      "Preslav Nakov",
      "Fabio Pizzati",
      "Ivan Laptev"
    ],
    "abstract": "Scaling data and models has played a pivotal role in the remarkable progress of computer vision and language. Inspired by these domains, recent efforts in robotics have similarly focused on scaling both data and model size to develop more generalizable and robust policies. However, unlike vision and language, robotics lacks access to internet-scale demonstrations across diverse robotic tasks and environments. As a result, the scale of existing datasets typically suffers from the need for manual data collection and curation. To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically generated training data. We build on the zero-shot capabilities of LLM planners and automatically generate demonstrations for diverse manipulation tasks in simulation. Successful examples are then used to finetune an LLM and to improve its planning capabilities without human supervision. Notably, while BLAZER training requires access to the simulator's state, we demonstrate direct transfer of acquired skills to sensor-based manipulation. Through extensive experiments, we show BLAZER to significantly improve zero-shot manipulation in both simulated and real environments. Moreover, BLAZER improves on tasks outside of its training pool and enables downscaling of LLM models. Our code and data will be made publicly available on the project page.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08572v1",
    "published_date": "2025-10-09 17:59:58 UTC",
    "updated_date": "2025-10-09 17:59:58 UTC"
  },
  {
    "arxiv_id": "2510.08569v1",
    "title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation",
    "authors": [
      "Qin Liu",
      "Jacob Dineen",
      "Yuxi Huang",
      "Sheng Zhang",
      "Hoifung Poon",
      "Ben Zhou",
      "Muhao Chen"
    ],
    "abstract": "Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question-answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.08569v1",
    "published_date": "2025-10-09 17:59:55 UTC",
    "updated_date": "2025-10-09 17:59:55 UTC"
  },
  {
    "arxiv_id": "2510.08568v1",
    "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos",
    "authors": [
      "Hongyu Li",
      "Lingfeng Sun",
      "Yafei Hu",
      "Duy Ta",
      "Jennifer Barry",
      "George Konidaris",
      "Jiahui Fu"
    ],
    "abstract": "Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08568v1",
    "published_date": "2025-10-09 17:59:55 UTC",
    "updated_date": "2025-10-09 17:59:55 UTC"
  },
  {
    "arxiv_id": "2510.08567v3",
    "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning",
    "authors": [
      "Tajamul Ashraf",
      "Umair Nawaz",
      "Abdelrahman M. Shaker",
      "Rao Anwer",
      "Philip Torr",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ],
    "abstract": "Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "We have come across a recent approach that has not been properly attributed at the time of submission and compared in a fair setting. Therefore, we would like to withdraw the paper to address these concerns",
    "pdf_url": "https://arxiv.org/pdf/2510.08567v3",
    "published_date": "2025-10-09 17:59:54 UTC",
    "updated_date": "2025-10-21 10:22:02 UTC"
  },
  {
    "arxiv_id": "2510.08564v1",
    "title": "How to Teach Large Multimodal Models New Skills",
    "authors": [
      "Zhen Zhu",
      "Yiming Gong",
      "Yao Xiao",
      "Yaoyao Liu",
      "Derek Hoiem"
    ],
    "abstract": "How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent \"forgetting\" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "In submission. Code is available at https://github.com/jessemelpolio/LMM_CL",
    "pdf_url": "https://arxiv.org/pdf/2510.08564v1",
    "published_date": "2025-10-09 17:59:37 UTC",
    "updated_date": "2025-10-09 17:59:37 UTC"
  },
  {
    "arxiv_id": "2510.08559v1",
    "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models",
    "authors": [
      "Andong Deng",
      "Taojiannan Yang",
      "Shoubin Yu",
      "Lincoln Spencer",
      "Mohit Bansal",
      "Chen Chen",
      "Serena Yeung-Levy",
      "Xiaohan Wang"
    ],
    "abstract": "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08559v1",
    "published_date": "2025-10-09 17:59:23 UTC",
    "updated_date": "2025-10-09 17:59:23 UTC"
  },
  {
    "arxiv_id": "2510.08558v2",
    "title": "Agent Learning via Early Experience",
    "authors": [
      "Kai Zhang",
      "Xiangchao Chen",
      "Bo Liu",
      "Tianci Xue",
      "Zeyi Liao",
      "Zhihan Liu",
      "Xiyao Wang",
      "Yuting Ning",
      "Zhaorun Chen",
      "Xiaohan Fu",
      "Jian Xie",
      "Yuxuan Sun",
      "Boyu Gou",
      "Qi Qi",
      "Zihang Meng",
      "Jianwei Yang",
      "Ning Zhang",
      "Xian Li",
      "Ashish Shah",
      "Dat Huynh",
      "Hengduo Li",
      "Zi Yang",
      "Sara Cao",
      "Lawrence Jang",
      "Shuyan Zhou",
      "Jiacheng Zhu",
      "Huan Sun",
      "Jason Weston",
      "Yu Su",
      "Yifan Wu"
    ],
    "abstract": "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2510.08558v2",
    "published_date": "2025-10-09 17:59:17 UTC",
    "updated_date": "2025-10-13 23:25:00 UTC"
  },
  {
    "arxiv_id": "2510.08553v1",
    "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation",
    "authors": [
      "Yunzhe Xu",
      "Yiyuan Pan",
      "Zhe Liu"
    ],
    "abstract": "Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 6 figures, 13 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08553v1",
    "published_date": "2025-10-09 17:58:01 UTC",
    "updated_date": "2025-10-09 17:58:01 UTC"
  },
  {
    "arxiv_id": "2510.08543v1",
    "title": "VideoNorms: Benchmarking Cultural Awareness of Video Language Models",
    "authors": [
      "Nikhil Reddy Varimalla",
      "Yunfei Xu",
      "Arkadiy Saakyan",
      "Meng Fan Wang",
      "Smaranda Muresan"
    ],
    "abstract": "As Video Large Language Models (VideoLLMs) are deployed globally, they require understanding of and grounding in the relevant cultural background. To properly assess these models' cultural awareness, adequate benchmarks are needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm) pairs from US and Chinese cultures annotated with socio-cultural norms grounded in speech act theory, norm adherence and violations labels, and verbal and non-verbal evidence. To build VideoNorms, we use a human-AI collaboration framework, where a teacher model using theoretically-grounded prompting provides candidate annotations and a set of trained human experts validate and correct the annotations. We benchmark a variety of open-weight VideoLLMs on the new dataset which highlight several common trends: 1) models performs worse on norm violation than adherence; 2) models perform worse w.r.t Chinese culture compared to the US culture; 3) models have more difficulty in providing non-verbal evidence compared to verbal for the norm adhere/violation label and struggle to identify the exact norm corresponding to a speech-act; and 4) unlike humans, models perform worse in formal, non-humorous contexts. Our findings emphasize the need for culturally-grounded video language model training - a gap our benchmark and framework begin to address.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 5 figures, under review",
    "pdf_url": "https://arxiv.org/pdf/2510.08543v1",
    "published_date": "2025-10-09 17:54:55 UTC",
    "updated_date": "2025-10-09 17:54:55 UTC"
  },
  {
    "arxiv_id": "2510.08539v2",
    "title": "On the optimization dynamics of RLVR: Gradient gap and step size thresholds",
    "authors": [
      "Joe Suk",
      "Yaqi Duan"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple binary feedback to post-train large language models, has shown significant empirical success. However, a principled understanding of why it works has been lacking. This paper builds a theoretical foundation for RLVR by analyzing its training process at both the full-response (trajectory) and token levels. Central to our analysis is a quantity called the Gradient Gap, which formalizes the direction of improvement from low-reward to high-reward regions of the response space. We prove that convergence critically depends on aligning the update direction with this Gradient Gap. Moreover, we derive a sharp step-size threshold based on the magnitude of the Gradient Gap: below it, learning converges, whereas above it, performance collapses. Our theory further predicts how the critical step size must scale with response length and the success rate, thereby explaining why practical heuristics such as length normalization improve stability and showing that, with a fixed learning rate, the success rate can stagnate strictly below $100\\%$. We validate these predictions through controlled bandit simulations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08539v2",
    "published_date": "2025-10-09 17:53:41 UTC",
    "updated_date": "2025-10-10 02:46:51 UTC"
  },
  {
    "arxiv_id": "2510.08532v1",
    "title": "Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing",
    "authors": [
      "Rishubh Parihar",
      "Or Patashnik",
      "Daniil Ostashev",
      "R. Venkatesh Babu",
      "Daniel Cohen-Or",
      "Kuan-Chieh Wang"
    ],
    "abstract": "Instruction-based image editing offers a powerful and intuitive way to manipulate images through natural language. Yet, relying solely on text instructions limits fine-grained control over the extent of edits. We introduce Kontinuous Kontext, an instruction-driven editing model that provides a new dimension of control over edit strength, enabling users to adjust edits gradually from no change to a fully realized result in a smooth and continuous manner. Kontinuous Kontext extends a state-of-the-art image editing model to accept an additional input, a scalar edit strength which is then paired with the edit instruction, enabling explicit control over the extent of the edit. To inject this scalar information, we train a lightweight projector network that maps the input scalar and the edit instruction to coefficients in the model's modulation space. For training our model, we synthesize a diverse dataset of image-edit-instruction-strength quadruplets using existing generative models, followed by a filtering stage to ensure quality and consistency. Kontinuous Kontext provides a unified approach for fine-grained control over edit strength for instruction driven editing from subtle to strong across diverse operations such as stylization, attribute, material, background, and shape changes, without requiring attribute-specific training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://snap-research.github.io/kontinuouskontext/",
    "pdf_url": "https://arxiv.org/pdf/2510.08532v1",
    "published_date": "2025-10-09 17:51:03 UTC",
    "updated_date": "2025-10-09 17:51:03 UTC"
  },
  {
    "arxiv_id": "2510.08531v1",
    "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models",
    "authors": [
      "Hongxing Li",
      "Dingming Li",
      "Zixuan Wang",
      "Yuchen Yan",
      "Hang Wu",
      "Wenqi Zhang",
      "Yongliang Shen",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://zju-real.github.io/SpatialLadder/ Code: https://github.com/ZJU-REAL/SpatialLadder",
    "pdf_url": "https://arxiv.org/pdf/2510.08531v1",
    "published_date": "2025-10-09 17:50:54 UTC",
    "updated_date": "2025-10-09 17:50:54 UTC"
  },
  {
    "arxiv_id": "2510.08529v1",
    "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
    "authors": [
      "Xiangyuan Xue",
      "Yifan Zhou",
      "Guibin Zhang",
      "Zaibin Zhang",
      "Yijiang Li",
      "Chen Zhang",
      "Zhenfei Yin",
      "Philip Torr",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "abstract": "Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08529v1",
    "published_date": "2025-10-09 17:50:26 UTC",
    "updated_date": "2025-10-09 17:50:26 UTC"
  },
  {
    "arxiv_id": "2510.12817v2",
    "title": "From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP",
    "authors": [
      "Shanshan Xu",
      "Santosh T. Y. S. S",
      "Barbara Plank"
    ],
    "abstract": "Human Label Variation (HLV) refers to legitimate disagreement in annotation that reflects the diversity of human perspectives rather than mere error. Long treated in NLP as noise to be eliminated, HLV has only recently been reframed as a signal for improving model robustness. With the rise of large language models (LLMs) and post-training methods such as human feedback-based alignment, the role of HLV has become increasingly consequential. Yet current preference-learning datasets routinely collapse multiple annotations into a single label, flattening diverse perspectives into artificial consensus. Preserving HLV is necessary not only for pluralistic alignment but also for sociotechnical safety evaluation, where model behavior must be assessed in relation to human interaction and societal context. This position paper argues that preserving HLV as an embodiment of human pluralism must be treated as a Selbstzweck, an intrinsic value in itself. We analyze the limitations of existing preference datasets and propose actionable strategies for incorporating HLV into dataset construction to better preserve pluralistic human values.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12817v2",
    "published_date": "2025-10-09 17:48:29 UTC",
    "updated_date": "2026-01-16 17:00:35 UTC"
  },
  {
    "arxiv_id": "2510.08521v2",
    "title": "FlowSearch: Advancing deep research with dynamic structured knowledge flow",
    "authors": [
      "Yusong Hu",
      "Runmin Ma",
      "Yue Fan",
      "Jinxin Shi",
      "Zongsheng Cao",
      "Yuhao Zhou",
      "Jiakang Yuan",
      "Shuaiyu Zhang",
      "Shiyang Feng",
      "Xiangchao Yan",
      "Shufei Zhang",
      "Wenlong Zhang",
      "Lei Bai",
      "Bo Zhang"
    ],
    "abstract": "Deep research is an inherently challenging task that demands both breadth and depth of thinking. It involves navigating diverse knowledge spaces and reasoning over complex, multi-step dependencies, which presents substantial challenges for agentic systems. To address this, we propose FlowSearch, a multi-agent framework that actively constructs and evolves a dynamic structured knowledge flow to drive subtask execution and reasoning. FlowSearch is capable of strategically planning and expanding the knowledge flow to enable parallel exploration and hierarchical task decomposition, while also adjusting the knowledge flow in real time based on feedback from intermediate reasoning outcomes and insights. FlowSearch achieves competitive performance on both general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA, demonstrating its effectiveness in multi-disciplinary research scenarios and its potential to advance scientific discovery. The code is available at https://github.com/InternScience/InternAgent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08521v2",
    "published_date": "2025-10-09 17:48:12 UTC",
    "updated_date": "2026-01-11 03:37:26 UTC"
  },
  {
    "arxiv_id": "2510.08517v1",
    "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
    "authors": [
      "Grace Liu",
      "Yuxiao Qu",
      "Jeff Schneider",
      "Aarti Singh",
      "Aviral Kumar"
    ],
    "abstract": "Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08517v1",
    "published_date": "2025-10-09 17:46:39 UTC",
    "updated_date": "2025-10-09 17:46:39 UTC"
  },
  {
    "arxiv_id": "2510.08511v1",
    "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents",
    "authors": [
      "Shangheng Du",
      "Xiangchao Yan",
      "Dengyang Jiang",
      "Jiakang Yuan",
      "Yusong Hu",
      "Xin Li",
      "Liang He",
      "Bo Zhang",
      "Lei Bai"
    ],
    "abstract": "Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08511v1",
    "published_date": "2025-10-09 17:45:05 UTC",
    "updated_date": "2025-10-09 17:45:05 UTC"
  },
  {
    "arxiv_id": "2510.08510v1",
    "title": "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models",
    "authors": [
      "Jiayun Luo",
      "Wan-Cyuan Fan",
      "Lyuyang Wang",
      "Xiangteng He",
      "Tanzila Rahman",
      "Purang Abolmaesumi",
      "Leonid Sigal"
    ],
    "abstract": "Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Project page: https://davidhalladay.github.io/diysink_demo",
    "pdf_url": "https://arxiv.org/pdf/2510.08510v1",
    "published_date": "2025-10-09 17:44:42 UTC",
    "updated_date": "2025-10-09 17:44:42 UTC"
  },
  {
    "arxiv_id": "2510.08498v1",
    "title": "AI-Driven Radiology Report Generation for Traumatic Brain Injuries",
    "authors": [
      "Riadh Bouslimi",
      "Houda Trabelsi",
      "Wahiba Ben Abdssalem Karaa",
      "Hana Hedhli"
    ],
    "abstract": "Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08498v1",
    "published_date": "2025-10-09 17:39:04 UTC",
    "updated_date": "2025-10-09 17:39:04 UTC"
  },
  {
    "arxiv_id": "2510.08671v1",
    "title": "Optimizing delivery for quick commerce factoring qualitative assessment of generated routes",
    "authors": [
      "Milon Bhattacharya",
      "Milan Kumar"
    ],
    "abstract": "Indias e-commerce market is projected to grow rapidly, with last-mile delivery accounting for nearly half of operational expenses. Although vehicle routing problem (VRP) based solvers are widely used for delivery planning, their effectiveness in real-world scenarios is limited due to unstructured addresses, incomplete maps, and computational constraints in distance estimation. This study proposes a framework that employs large language models (LLMs) to critique VRP-generated routes against policy-based criteria, allowing logistics operators to evaluate and prioritise more efficient delivery plans. As a illustration of our approach we generate, annotate and evaluated 400 cases using large language models. Our study found that open-source LLMs identified routing issues with 79% accuracy, while proprietary reasoning models achieved reach upto 86%. The results demonstrate that LLM-based evaluation of VRP-generated routes can be an effective and scalable layer of evaluation which goes beyond beyond conventional distance and time based metrics. This has implications for improving cost efficiency, delivery reliability, and sustainability in last-mile logistics, especially for developing countries like India.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08671v1",
    "published_date": "2025-10-09 17:31:58 UTC",
    "updated_date": "2025-10-09 17:31:58 UTC"
  },
  {
    "arxiv_id": "2510.08483v1",
    "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
    "authors": [
      "Shangqing Tu",
      "Yaxuan Li",
      "Yushi Bai",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 4 figures, please check out the project page: https://deepprune.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.08483v1",
    "published_date": "2025-10-09 17:24:54 UTC",
    "updated_date": "2025-10-09 17:24:54 UTC"
  },
  {
    "arxiv_id": "2510.08669v1",
    "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching",
    "authors": [
      "Jiacheng Liu",
      "Peiliang Cai",
      "Qinming Zhou",
      "Yuqi Lin",
      "Deyang Kong",
      "Benhao Huang",
      "Yupei Pan",
      "Haowen Xu",
      "Chang Zou",
      "Junshu Tang",
      "Shikang Zheng",
      "Linfeng Zhang"
    ],
    "abstract": "The application of diffusion transformers is suffering from their significant inference costs. Recently, feature caching has been proposed to solve this problem by reusing features from previous timesteps, thereby skipping computation in future timesteps. However, previous feature caching assumes that features in adjacent timesteps are similar or continuous, which does not always hold in all settings. To investigate this, this paper begins with an analysis from the frequency domain, which reveal that different frequency bands in the features of diffusion models exhibit different dynamics across timesteps. Concretely, low-frequency components, which decide the structure of images, exhibit higher similarity but poor continuity. In contrast, the high-frequency bands, which decode the details of images, show significant continuity but poor similarity. These interesting observations motivate us to propose Frequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their similarity, while using a second-order Hermite interpolator to predict the volatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF) instead of the features in all the layers, which reduces the memory footprint of feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit demonstrate its effectiveness in both generation and editing. Codes are available in the supplementary materials and will be released on GitHub.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08669v1",
    "published_date": "2025-10-09 17:22:23 UTC",
    "updated_date": "2025-10-09 17:22:23 UTC"
  },
  {
    "arxiv_id": "2510.08470v1",
    "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling",
    "authors": [
      "Bianca-Mihaela Ganescu",
      "Suchir Salhan",
      "Andrew Caines",
      "Paula Buttery"
    ],
    "abstract": "Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the EMNLP 2025 BabyLM Workshop",
    "pdf_url": "https://arxiv.org/pdf/2510.08470v1",
    "published_date": "2025-10-09 17:10:36 UTC",
    "updated_date": "2025-10-09 17:10:36 UTC"
  },
  {
    "arxiv_id": "2510.08469v1",
    "title": "Platform-Agnostic Modular Architecture for Quantum Benchmarking",
    "authors": [
      "Neer Patel",
      "Anish Giri",
      "Hrushikesh Pramod Patil",
      "Noah Siekierski",
      "Avimita Chatterjee",
      "Sonika Johri",
      "Timothy Proctor",
      "Thomas Lubinski",
      "Siyuan Niu"
    ],
    "abstract": "We present a platform-agnostic modular architecture that addresses the increasingly fragmented landscape of quantum computing benchmarking by decoupling problem generation, circuit execution, and results analysis into independent, interoperable components. Supporting over 20 benchmark variants ranging from simple algorithmic tests like Bernstein-Vazirani to complex Hamiltonian simulation with observable calculations, the system integrates with multiple circuit generation APIs (Qiskit, CUDA-Q, Cirq) and enables diverse workflows. We validate the architecture through successful integration with Sandia's $\\textit{pyGSTi}$ for advanced circuit analysis and CUDA-Q for multi-GPU HPC simulations. Extensibility of the system is demonstrated by implementing dynamic circuit variants of existing benchmarks and a new quantum reinforcement learning benchmark, which become readily available across multiple execution and analysis modes. Our primary contribution is identifying and formalizing modular interfaces that enable interoperability between incompatible benchmarking frameworks, demonstrating that standardized interfaces reduce ecosystem fragmentation while preserving optimization flexibility. This architecture has been developed as a key enhancement to the continually evolving QED-C Application-Oriented Performance Benchmarks for Quantum Computing suite.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08469v1",
    "published_date": "2025-10-09 17:09:56 UTC",
    "updated_date": "2025-10-09 17:09:56 UTC"
  },
  {
    "arxiv_id": "2510.08456v1",
    "title": "Integral Signatures of Activation Functions: A 9-Dimensional Taxonomy and Stability Theory for Deep Learning",
    "authors": [
      "Ankur Mali",
      "Lawrence Hall",
      "Jake Williams",
      "Gordon Richards"
    ],
    "abstract": "Activation functions govern the expressivity and stability of neural networks, yet existing comparisons remain largely heuristic. We propose a rigorous framework for their classification via a nine-dimensional integral signature S_sigma(phi), combining Gaussian propagation statistics (m1, g1, g2, m2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures (TV(phi'), C(phi)). This taxonomy establishes well-posedness, affine reparameterization laws with bias, and closure under bounded slope variation. Dynamical analysis yields Lyapunov theorems with explicit descent constants and identifies variance stability regions through (m2', g2). From a kernel perspective, we derive dimension-free Hessian bounds and connect smoothness to bounded variation of phi'. Applying the framework, we classify eight standard activations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU), proving sharp distinctions between saturating, linear-growth, and smooth families. Numerical Gauss-Hermite and Monte Carlo validation confirms theoretical predictions. Our framework provides principled design guidance, moving activation choice from trial-and-error to provable stability and kernel conditioning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.08456v1",
    "published_date": "2025-10-09 17:03:00 UTC",
    "updated_date": "2025-10-09 17:03:00 UTC"
  },
  {
    "arxiv_id": "2510.08450v2",
    "title": "gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity",
    "authors": [
      "Hugh Blayney",
      "Álvaro Arroyo",
      "Xiaowen Dong",
      "Michael M. Bronstein"
    ],
    "abstract": "Graph Neural Networks (GNNs) leverage the graph structure to transmit information between nodes, typically through the message-passing mechanism. While these models have found a wide variety of applications, they are known to suffer from over-squashing, where information from a large receptive field of node representations is collapsed into a single fixed sized vector, resulting in an information bottleneck. In this paper, we re-examine the over-squashing phenomenon through the lens of model storage and retrieval capacity, which we define as the amount of information that can be stored in a node's representation for later use. We study some of the limitations of existing tasks used to measure over-squashing and introduce a new synthetic task to demonstrate that an information bottleneck can saturate this capacity. Furthermore, we adapt ideas from the sequence modeling literature on associative memories, fast weight programmers, and the xLSTM model to develop a novel GNN architecture with improved capacity. We demonstrate strong performance of this architecture both on our capacity synthetic task, as well as a range of real-world graph benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 22 figures, 7 tables. v2: clarified over-squashing separation in light of related work",
    "pdf_url": "https://arxiv.org/pdf/2510.08450v2",
    "published_date": "2025-10-09 16:58:49 UTC",
    "updated_date": "2025-10-22 16:55:32 UTC"
  },
  {
    "arxiv_id": "2510.08445v3",
    "title": "Synthetic Series-Symbol Data Generation for Time Series Foundation Models",
    "authors": [
      "Wenxuan Wang",
      "Kai Wu",
      "Yujian Betterest Li",
      "Dan Wang",
      "Xiaoyu Zhang"
    ],
    "abstract": "Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as training data scarcity and imbalance continue to hinder their development. Inspired by complex dynamic system theories, we design a series-symbol data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic expressions. To leverage series-symbol data pairs with strong correlations, we develop SymTime, a pre-trained foundation model for enhancing time series representation using symbolic information. SymTime demonstrates competitive performance across five major TSA tasks when fine-tunes with downstream tasks, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of series-symbol data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance. The code is available at https://github.com/wwhenxuan/SymTime.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "64 pages, 25 figures, 35 tables, NeurIPS 2025 accepted",
    "pdf_url": "https://arxiv.org/pdf/2510.08445v3",
    "published_date": "2025-10-09 16:54:18 UTC",
    "updated_date": "2025-10-20 08:04:01 UTC"
  },
  {
    "arxiv_id": "2510.08442v2",
    "title": "Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning",
    "authors": [
      "Andrew Lee",
      "Ian Chuang",
      "Dechen Gao",
      "Kai Fukazawa",
      "Iman Soltani"
    ],
    "abstract": "Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.52x improvement in sample efficiency and can solve challenging tasks from the ManiSkill3 benchmark that the baseline fails to learn, without modifying the underlying algorithm or hyperparameters.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://andrewcwlee.github.io/gaze-on-the-prize",
    "pdf_url": "https://arxiv.org/pdf/2510.08442v2",
    "published_date": "2025-10-09 16:54:11 UTC",
    "updated_date": "2025-12-12 00:54:58 UTC"
  },
  {
    "arxiv_id": "2510.08439v1",
    "title": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning",
    "authors": [
      "Cheng Qian",
      "Zuxin Liu",
      "Shirley Kokane",
      "Akshara Prabhakar",
      "Jielin Qiu",
      "Haolin Chen",
      "Zhiwei Liu",
      "Heng Ji",
      "Weiran Yao",
      "Shelby Heinecke",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang"
    ],
    "abstract": "Modern LLM deployments confront a widening cost-performance spectrum: premium models deliver strong reasoning but are expensive, while lightweight models are economical yet brittle on complex tasks. Static escalation rules and keyword heuristics under-utilize this spectrum and fail to adapt across task types. We present xRouter, a tool-calling-based routing system in which a learned router can either answer directly or invoke one or more external models. The router is trained end-to-end with reinforcement learning using an explicit, cost-aware reward that encodes cost-performance trade-offs, eliminating the need for hand-engineered routing rules. Our implementation encompasses the full reinforcement learning framework, including reward and cost accounting, as well as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter achieves strong cost-performance trade-offs (e.g., substantial cost reductions at comparable task completion rates), and provides empirical insights into what reliably helps learned routing and what does not, ranging from model trainability to the difficulty of eliciting sophisticated orchestration behaviors in small open models. We hope these findings and our open implementation will serve as a practical substrate for advancing learned, cost-aware LLM orchestration.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "24 Pages, 4 Figures, 2 Tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08439v1",
    "published_date": "2025-10-09 16:52:01 UTC",
    "updated_date": "2025-10-09 16:52:01 UTC"
  },
  {
    "arxiv_id": "2510.08429v1",
    "title": "ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing",
    "authors": [
      "Stella C. Dong",
      "James R. Finlay"
    ],
    "abstract": "Reinsurance treaty pricing must satisfy stringent regulatory standards, yet current quoting practices remain opaque and difficult to audit. We introduce ClauseLens, a clause-grounded reinforcement learning framework that produces transparent, regulation-compliant, and risk-aware treaty quotes.\n  ClauseLens models the quoting task as a Risk-Aware Constrained Markov Decision Process (RA-CMDP). Statutory and policy clauses are retrieved from legal and underwriting corpora, embedded into the agent's observations, and used both to constrain feasible actions and to generate clause-grounded natural language justifications.\n  Evaluated in a multi-agent treaty simulator calibrated to industry data, ClauseLens reduces solvency violations by 51%, improves tail-risk performance by 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded explanations with retrieval precision of 87.4% and recall of 91.1%.\n  These findings demonstrate that embedding legal context into both decision and explanation pathways yields interpretable, auditable, and regulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and the EU AI Act.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at the 6th ACM International Conference on AI in Finance (ICAIF 2025), Singapore. Author-accepted version (October 2025). 10 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08429v1",
    "published_date": "2025-10-09 16:43:49 UTC",
    "updated_date": "2025-10-09 16:43:49 UTC"
  },
  {
    "arxiv_id": "2510.08667v1",
    "title": "RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data",
    "authors": [
      "Mohammad Baqar"
    ],
    "abstract": "Modern software teams frequently encounter delays in resolving recurring or related issues due to fragmented knowledge scattered across JIRA tickets, developer discussions, and GitHub pull requests (PRs). To address this challenge, we propose a Retrieval-Augmented Generation (RAG) framework that integrates Sentence-Transformers for semantic embeddings with FAISS-based vector search to deliver context-aware ticket resolution recommendations. The approach embeds historical JIRA tickets, user comments, and linked PR metadata to retrieve semantically similar past cases, which are then synthesized by a Large Language Model (LLM) into grounded and explainable resolution suggestions. The framework contributes a unified pipeline linking JIRA and GitHub data, an embedding and FAISS indexing strategy for heterogeneous software artifacts, and a resolution generation module guided by retrieved evidence. Experimental evaluation using precision, recall, resolution time reduction, and developer acceptance metrics shows that the proposed system significantly improves resolution accuracy, fix quality, and knowledge reuse in modern DevOps environments.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "13 Pages",
    "pdf_url": "https://arxiv.org/pdf/2510.08667v1",
    "published_date": "2025-10-09 16:33:00 UTC",
    "updated_date": "2025-10-09 16:33:00 UTC"
  },
  {
    "arxiv_id": "2510.08413v1",
    "title": "Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors",
    "authors": [
      "David Madras",
      "Joshua Safyan",
      "Qiuyi",
      "Zhang"
    ],
    "abstract": "Many prompt engineering techniques have been successful in practice, even when optimizing over a large prompt space with with a small amount of task-specific data. Recent work has partially explained this success by showing generalization bounds which apply PAC-Bayes theory to the discrete prompt space, but they are non-vacuous only in data-rich scenarios. We argue that such widespread success can be more fully explained through more carefully considering data- or distribution-dependent perplexity, which acts as an effective prior and steers the optimization towards prompts that are more ``natural'' for the task at hand. We derive novel generalization bounds that are non-vacuous for data-scarce prompt optimization via more useful priors, formally analyzing how perplexity regularization tightens these bounds by limiting exploration. Empirically, we explore both the bounds' effectiveness and the practical benefits of perplexity regularization in improving prompt generalization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "EXAIT Workshop paper at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08413v1",
    "published_date": "2025-10-09 16:32:46 UTC",
    "updated_date": "2025-10-09 16:32:46 UTC"
  },
  {
    "arxiv_id": "2510.08404v1",
    "title": "Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT",
    "authors": [
      "Noor Ul Zain",
      "Mohsin Raza",
      "Ahsan Adeel"
    ],
    "abstract": "We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08404v1",
    "published_date": "2025-10-09 16:22:30 UTC",
    "updated_date": "2025-10-09 16:22:30 UTC"
  },
  {
    "arxiv_id": "2510.08666v3",
    "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
    "authors": [
      "Yuxin Ma",
      "Lun Du",
      "Lanning Wei",
      "Kun Chen",
      "Qian Xu",
      "Kangyu Wang",
      "Guofeng Feng",
      "Guoshan Lu",
      "Lin Liu",
      "Xiaojing Qi",
      "Xinyuan Zhang",
      "Zhen Tao",
      "Haibo Feng",
      "Ziyun Jiang",
      "Ying Xu",
      "Zenan Huang",
      "Yihong Zhuang",
      "Haokai Xu",
      "Jiaqi Hu",
      "Zhenzhong Lan",
      "Junbo Zhao",
      "Jianguo Li",
      "Da Zheng"
    ],
    "abstract": "Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components--model, diffusion iteration manager, decoding strategy, and KV-cache manager--and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to prior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while maintaining similar model performance. Even compared to the AR model (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with the latest vLLM inference engine, dInfer still delivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08666v3",
    "published_date": "2025-10-09 16:19:42 UTC",
    "updated_date": "2025-10-22 14:33:49 UTC"
  },
  {
    "arxiv_id": "2510.09699v1",
    "title": "VisualDAN: Exposing Vulnerabilities in VLMs with Visual-Driven DAN Commands",
    "authors": [
      "Aofan Liu",
      "Lulu Tang"
    ],
    "abstract": "Vision-Language Models (VLMs) have garnered significant attention for their remarkable ability to interpret and generate multimodal content. However, securing these models against jailbreak attacks continues to be a substantial challenge. Unlike text-only models, VLMs integrate additional modalities, introducing novel vulnerabilities such as image hijacking, which can manipulate the model into producing inappropriate or harmful responses. Drawing inspiration from text-based jailbreaks like the \"Do Anything Now\" (DAN) command, this work introduces VisualDAN, a single adversarial image embedded with DAN-style commands. Specifically, we prepend harmful corpora with affirmative prefixes (e.g., \"Sure, I can provide the guidance you need\") to trick the model into responding positively to malicious queries. The adversarial image is then trained on these DAN-inspired harmful texts and transformed into the text domain to elicit malicious outputs. Extensive experiments on models such as MiniGPT-4, MiniGPT-v2, InstructBLIP, and LLaVA reveal that VisualDAN effectively bypasses the safeguards of aligned VLMs, forcing them to execute a broad range of harmful instructions that severely violate ethical standards. Our results further demonstrate that even a small amount of toxic content can significantly amplify harmful outputs once the model's defenses are compromised. These findings highlight the urgent need for robust defenses against image-based attacks and offer critical insights for future research into the alignment and security of VLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09699v1",
    "published_date": "2025-10-09 16:18:31 UTC",
    "updated_date": "2025-10-09 16:18:31 UTC"
  },
  {
    "arxiv_id": "2510.08396v2",
    "title": "FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts",
    "authors": [
      "Heming Zou",
      "Yunliang Zang",
      "Wutong Xu",
      "Yao Zhu",
      "Xiangyang Ji"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 accepted paper",
    "pdf_url": "https://arxiv.org/pdf/2510.08396v2",
    "published_date": "2025-10-09 16:17:13 UTC",
    "updated_date": "2025-10-23 17:14:06 UTC"
  },
  {
    "arxiv_id": "2510.08389v1",
    "title": "Revisiting Hallucination Detection with Effective Rank-based Uncertainty",
    "authors": [
      "Rui Wang",
      "Zeming Wei",
      "Guanzhang Yue",
      "Meng Sun"
    ],
    "abstract": "Detecting hallucinations in large language models (LLMs) remains a fundamental challenge for their trustworthy deployment. Going beyond basic uncertainty-driven hallucination detection frameworks, we propose a simple yet powerful method that quantifies uncertainty by measuring the effective rank of hidden states derived from multiple model outputs and different layers. Grounded in the spectral analysis of representations, our approach provides interpretable insights into the model's internal reasoning process through semantic variations, while requiring no extra knowledge or additional modules, thus offering a combination of theoretical elegance and practical efficiency. Meanwhile, we theoretically demonstrate the necessity of quantifying uncertainty both internally (representations of a single response) and externally (different responses), providing a justification for using representations among different layers and responses from LLMs to detect hallucinations. Extensive experiments demonstrate that our method effectively detects hallucinations and generalizes robustly across various scenarios, contributing to a new paradigm of hallucination detection for LLM truthfulness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08389v1",
    "published_date": "2025-10-09 16:12:12 UTC",
    "updated_date": "2025-10-09 16:12:12 UTC"
  },
  {
    "arxiv_id": "2510.08385v1",
    "title": "Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning",
    "authors": [
      "Sofia Kirsanova",
      "Yao-Yi Chiang",
      "Weiwei Duan"
    ],
    "abstract": "Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08385v1",
    "published_date": "2025-10-09 16:08:48 UTC",
    "updated_date": "2025-10-09 16:08:48 UTC"
  },
  {
    "arxiv_id": "2510.08383v1",
    "title": "QAgent: A modular Search Agent with Interactive Query Understanding",
    "authors": [
      "Yi Jiang",
      "Lei Shen",
      "Lujie Niu",
      "Sendong Zhao",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "abstract": "Large language models (LLMs) excel at natural language tasks but are limited by their static parametric knowledge, especially in knowledge-intensive task. Retrieval-augmented generation (RAG) mitigates this by integrating external information. However, (1) traditional RAG struggles with complex query understanding, and (2) even search agents trained with reinforcement learning (RL), despite their promise, still face generalization and deployment challenges. To address these limitations, we propose QAgent, a unified agentic RAG framework that employs a search agent for adaptive retrieval. This agent optimizes its understanding of the query through interactive reasoning and retrieval. To facilitate real-world application, we focus on modular search agent for query understanding that are plug-and-play in complex systems. Secifically, the agent follows a multi-step decision process trained with RL to maximize retrieval quality and support accurate downstream answers. We further analyze the strengths and weaknesses of end-to-end RL and propose a strategy that focuses on effective retrieval, thereby enhancing generalization in LLM applications. Experiments show QAgent excels at QA and serves as a plug-and-play module for real-world deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Code is available at https://github.com/OpenStellarTeam/QAgent",
    "pdf_url": "https://arxiv.org/pdf/2510.08383v1",
    "published_date": "2025-10-09 16:08:05 UTC",
    "updated_date": "2025-10-09 16:08:05 UTC"
  },
  {
    "arxiv_id": "2510.08381v1",
    "title": "Airy: Reading Robot Intent through Height and Sky",
    "authors": [
      "Baoyang Chen",
      "Xian Xu",
      "Huamin Qu"
    ],
    "abstract": "As industrial robots move into shared human spaces, their opaque decision making threatens safety, trust, and public oversight. This artwork, Airy, asks whether complex multi agent AI can become intuitively understandable by staging a competition between two reinforcement trained robot arms that snap a bedsheet skyward. Building on three design principles, competition as a clear metric (who lifts higher), embodied familiarity (audiences recognize fabric snapping), and sensor to sense mapping (robot cooperation or rivalry shown through forest and weather projections), the installation gives viewers a visceral way to read machine intent. Observations from five international exhibitions indicate that audiences consistently read the robots' strategies, conflict, and cooperation in real time, with emotional reactions that mirror the system's internal state. The project shows how sensory metaphors can turn a black box into a public interface.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08381v1",
    "published_date": "2025-10-09 16:07:30 UTC",
    "updated_date": "2025-10-09 16:07:30 UTC"
  },
  {
    "arxiv_id": "2510.08665v1",
    "title": "RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution",
    "authors": [
      "Aofan Liu",
      "Haoxuan Li",
      "Bin Wang",
      "Ao Yang",
      "Hui Li"
    ],
    "abstract": "Code generation models based on large language models (LLMs) have gained wide adoption, but challenges remain in ensuring safety, accuracy, and controllability, especially for complex tasks. Existing methods often lack dynamic integration of external tools, transparent reasoning, and user control over safety. To address these issues, we propose a controllable code generation framework utilizing the ReAct paradigm for multi-agent task execution. This framework is a multi-agent system designed to enable efficient, precise, and interpretable code generation through dynamic interactions between LLMs and external resources. The framework adopts a collaborative architecture comprising four specialized agents: a Planner for task decomposition, a Searcher that leverages the ReAct framework for reasoning and tool integration, a CodeGen agent for accurate code generation, and an Extractor for structured data retrieval. The ReAct-based Searcher alternates between generating reasoning traces and executing actions, facilitating seamless integration of internal knowledge with external tools (such as search engines) to enhance accuracy and user control. Experimental results show the framework's effectiveness across multiple languages, achieving a 94.8% security rate on the SVEN dataset with CodeQL, outperforming existing approaches. Its transparent reasoning process fosters user trust and improves controllability.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08665v1",
    "published_date": "2025-10-09 15:59:24 UTC",
    "updated_date": "2025-10-09 15:59:24 UTC"
  },
  {
    "arxiv_id": "2510.08664v1",
    "title": "Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware",
    "authors": [
      "Jianan Mu",
      "Mingyu Shi",
      "Yining Wang",
      "Tianmeng Yang",
      "Bin Sun",
      "Xing Hu",
      "Jing Ye",
      "Huawei Li"
    ],
    "abstract": "LLM-based RTL generation is an interesting research direction, as it holds the potential to liberate the least automated stage in the current chip design. However, due to the substantial semantic gap between high-level specifications and RTL, coupled with limited training data, existing models struggle with generation accuracy. Drawing on human experience, design with verification helps improving accuracy. However, as the RTL testbench data are even more scarce, it is not friendly for LLMs. Although LLMs excel at higher-level languages like Python/C, they have a huge semantic gap from RTL. When implementing the same functionality, Python/C code and hardware code differ significantly in the spatiotemporal granularity, requiring the LLM not only to consider high-level functional semantics but also to ensure the low-level details align with the circuit code. It is not an easy task. In this paper, we propose a function abstracted verifiable middleware (Faver) that streamlines RTL verification in LLM-based workflows. By mixing LLM-friendly code structures with a rule-based template, Faver decouples the details of circuit verification, allowing the LLM to focus on the functionality itself. In our experiments on the SFT model and open-source models, Faver improved the model's generation accuracy by up to 14%.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08664v1",
    "published_date": "2025-10-09 15:41:43 UTC",
    "updated_date": "2025-10-09 15:41:43 UTC"
  },
  {
    "arxiv_id": "2510.08352v2",
    "title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception",
    "authors": [
      "Nikos Theodoridis",
      "Tim Brophy",
      "Reenu Mohandas",
      "Ganesh Sistu",
      "Fiachra Collins",
      "Anthony Scanlan",
      "Ciaran Eising"
    ],
    "abstract": "Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not \"shortsighted\", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in IEEE Open Journal of Vehicular Technology. Final version available at: https://ieeexplore.ieee.org/document/11230063",
    "pdf_url": "https://arxiv.org/pdf/2510.08352v2",
    "published_date": "2025-10-09 15:38:41 UTC",
    "updated_date": "2025-12-10 11:35:59 UTC"
  },
  {
    "arxiv_id": "2510.08350v2",
    "title": "DeepEN: A Deep Reinforcement Learning Framework for Personalized Enteral Nutrition in Critical Care",
    "authors": [
      "Daniel Jason Tan",
      "Jiayang Chen",
      "Dilruk Perera",
      "Kay Choong See",
      "Mengling Feng"
    ],
    "abstract": "ICU enteral feeding remains sub-optimal due to limited personalization and uncertainty about appropriate calorie, protein, and fluid targets, particularly under rapidly changing metabolic demands and heterogeneous patient responses. This study introduces DeepEN, a reinforcement learning (RL)-based framework that personalizes enteral nutrition (EN) dosing for critically ill patients using electronic health record data. DeepEN was trained on over 11,000 ICU patients from the MIMIC-IV database to generate 4-hourly, patient-specific targets for caloric, protein, and fluid intake. The model's state space integrates demographics, comorbidities, vital signs, laboratory results, and prior interventions relevant to nutritional management, while its reward function balances short-term physiological and nutrition-related goals with long-term survival. A dueling double deep Q-network with Conservative Q-Learning regularization is used to ensure safe and reliable policy learning from retrospective data. DeepEN achieved a 3.7 $\\pm$ 0.17 percentage-point absolute reduction in estimated mortality compared with the clinician policy (18.8% vs 22.5%) and higher expected returns compared with guideline-based dosing (11.89 vs 8.11), with improvements in key nutritional biomarkers. U-shaped associations between deviations from clinician dosing and mortality suggest that the learned policy aligns with high-value clinician actions while diverging from suboptimal ones. These findings demonstrate the feasibility of conservative offline RL for individualized EN therapy and suggest that data-driven personalization may improve outcomes beyond guideline- or heuristic-based approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08350v2",
    "published_date": "2025-10-09 15:37:56 UTC",
    "updated_date": "2025-11-19 15:14:58 UTC"
  },
  {
    "arxiv_id": "2510.08663v2",
    "title": "A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data",
    "authors": [
      "Joe Watson",
      "Ivan O'Connor",
      "Chia-Wen Chen",
      "Luning Sun",
      "Fang Luo",
      "David Stillwell"
    ],
    "abstract": "Psychological assessments are dominated by rating scales, which cannot capture the nuance in natural language. Efforts to supplement them with qualitative text have relied on labelled datasets or expert rubrics, limiting scalability. We introduce a framework that avoids this reliance: large language models (LLMs) score free-text responses with simple prompts to produce candidate LLM items, from which we retain those that yield the most test information when co-calibrated with a baseline scale. Using depression as a case study, we developed and tested the method in upper-secondary students (n=693) and a matched synthetic dataset (n=3,000). Results on held-out test sets showed that augmenting a 19-item scale with LLM items improved its precision, accuracy, and convergent validity. Further, the test information gain matched that of adding as many as 16 rating-scale items. This framework leverages the increasing availability of transcribed language to enhance psychometric measures, with applications in clinical health and beyond.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08663v2",
    "published_date": "2025-10-09 15:37:24 UTC",
    "updated_date": "2025-11-23 19:31:46 UTC"
  },
  {
    "arxiv_id": "2510.08341v1",
    "title": "Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization",
    "authors": [
      "Pál Zsámboki",
      "Benjamin Levi",
      "David Ansel Josef Smith",
      "Mitansh Kagalwala",
      "Arlington Kell",
      "Samuel Liechty",
      "Cong Wang"
    ],
    "abstract": "We study length generalization in transformers through the set complement task, where a model must predict a uniform distribution over tokens absent from an input sequence -- an ability central to board-game style reasoning. Our main theoretical result establishes two statements. First, we prove tight bounds on embedding and value dimensions for single-layer attention-only transformers. Second, we show that if such a model achieves balanced logit displacement at lengths 1 and 2, then it must generalize to longer sequences, though with reduced precision. A mechanistic reading of the proof explains this limitation: as more tokens are attended to, softmax compresses logit displacements, eroding separation between valid and invalid outputs. Training dynamics also suggest a second obstacle: when many next tokens are possible, updates become noisy. We hypothesize that dropout can counteract the first effect and Exponential Moving Average (EMA) the second. We validate these hypotheses through random hyperparameter search on the set complement task, which confirms both mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random Othello moves, and find that EMA again improves length generalization in this more complex setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08341v1",
    "published_date": "2025-10-09 15:26:48 UTC",
    "updated_date": "2025-10-09 15:26:48 UTC"
  },
  {
    "arxiv_id": "2510.08338v3",
    "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings",
    "authors": [
      "Benjamin F. Maier",
      "Ulf Aslak",
      "Luca Fiaschi",
      "Nina Rismal",
      "Kemble Fletcher",
      "Christian C. Luhmann",
      "Robbie Dow",
      "Kli Pappas",
      "Thomas V. Wiecki"
    ],
    "abstract": "Consumer research costs companies billions annually yet suffers from panel biases and limited scale. Large language models (LLMs) offer an alternative by simulating synthetic consumers, but produce unrealistic response distributions when asked directly for numerical ratings. We present semantic similarity rating (SSR), a method that elicits textual responses from LLMs and maps these to Likert distributions using embedding similarity to reference statements. Testing on an extensive dataset comprising 57 personal care product surveys conducted by a leading corporation in that market (9,300 human responses), SSR achieves 90% of human test-retest reliability while maintaining realistic response distributions (KS similarity > 0.85). Additionally, these synthetic respondents provide rich qualitative feedback explaining their ratings. This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 35 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08338v3",
    "published_date": "2025-10-09 15:24:48 UTC",
    "updated_date": "2025-10-27 12:39:25 UTC"
  },
  {
    "arxiv_id": "2510.09696v1",
    "title": "Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form",
    "authors": [
      "Lorenzo Nikiforos",
      "Charalampos Antoniadis",
      "Luciano Prono",
      "Fabio Pareschi",
      "Riccardo Rovatti",
      "Gianluca Setti"
    ],
    "abstract": "The increasing scale of deep neural networks has led to a growing need for compression techniques such as pruning, quantization, and low-rank decomposition. While these methods are very effective in reducing memory, computation and energy consumption, they often introduce severe accuracy degradation when applied directly. We introduce Vanishing Contributions (VCON), a general approach for smoothly transitioning neural models into compressed form. Rather than replacing the original network directly with its compressed version, VCON executes the two in parallel during fine-tuning. The contribution of the original (uncompressed) model is progressively reduced, while that of the compressed model is gradually increased. This smooth transition allows the network to adapt over time, improving stability and mitigating accuracy degradation. We evaluate VCON across computer vision and natural language processing benchmarks, in combination with multiple compression strategies. Across all scenarios, VCON leads to consistent improvements: typical gains exceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus provides a generalizable method that can be applied to the existing compression techniques, with evidence of consistent gains across multiple benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at https://github.com/foros15/vanishing-contributions",
    "pdf_url": "https://arxiv.org/pdf/2510.09696v1",
    "published_date": "2025-10-09 15:17:13 UTC",
    "updated_date": "2025-10-09 15:17:13 UTC"
  },
  {
    "arxiv_id": "2510.08325v2",
    "title": "Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries",
    "authors": [
      "Marius Dragoi",
      "Ioana Pintilie",
      "Florin Gogianu",
      "Florin Brad"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 3 figures. v2 adds discussion of related work (G-Pass@k)",
    "pdf_url": "https://arxiv.org/pdf/2510.08325v2",
    "published_date": "2025-10-09 15:14:58 UTC",
    "updated_date": "2025-10-21 12:29:05 UTC"
  },
  {
    "arxiv_id": "2510.08317v1",
    "title": "Iterated Agent for Symbolic Regression",
    "authors": [
      "Zhuo-Yang Song",
      "Zeyu Cai",
      "Shutao Zhang",
      "Jiashen Wei",
      "Jichen Pan",
      "Shi Qiu",
      "Qing-Hong Cao",
      "Tie-Jiun Hou",
      "Xiaohui Liu",
      "Ming-xing Luo",
      "Hua Xing Zhu"
    ],
    "abstract": "Symbolic regression (SR), the automated discovery of mathematical expressions from data, is a cornerstone of scientific inquiry. However, it is often hindered by the combinatorial explosion of the search space and a tendency to overfit. Popular methods, rooted in genetic programming, explore this space syntactically, often yielding overly complex, uninterpretable models. This paper introduces IdeaSearchFitter, a framework that employs Large Language Models (LLMs) as semantic operators within an evolutionary search. By generating candidate expressions guided by natural-language rationales, our method biases discovery towards models that are not only accurate but also conceptually coherent and interpretable. We demonstrate IdeaSearchFitter's efficacy across diverse challenges: it achieves competitive, noise-robust performance on the Feynman Symbolic Regression Database (FSReD), outperforming several strong baselines; discovers mechanistically aligned models with good accuracy-complexity trade-offs on real-world data; and derives compact, physically-motivated parametrizations for Parton Distribution Functions in a frontier high-energy physics application. IdeaSearchFitter is a specialized module within our broader iterated agent framework, IdeaSearch, which is publicly available at https://www.ideasearch.cn/.",
    "categories": [
      "physics.comp-ph",
      "astro-ph.IM",
      "cs.AI",
      "cs.LG",
      "hep-ph"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "45 pages, 22 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08317v1",
    "published_date": "2025-10-09 15:02:56 UTC",
    "updated_date": "2025-10-09 15:02:56 UTC"
  },
  {
    "arxiv_id": "2510.08308v1",
    "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
    "authors": [
      "Liwei Kang",
      "Yue Deng",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Wee Sun Lee",
      "Lidong Bing"
    ],
    "abstract": "Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08308v1",
    "published_date": "2025-10-09 14:57:10 UTC",
    "updated_date": "2025-10-09 14:57:10 UTC"
  },
  {
    "arxiv_id": "2510.08300v1",
    "title": "Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks",
    "authors": [
      "Bart Kuipers",
      "Freek Byrman",
      "Daniel Uyterlinde",
      "Alejandro García-Castellanos"
    ],
    "abstract": "Amortized optimization accelerates the solution of related optimization problems by learning mappings that exploit shared structure across problem instances. We explore the use of Scale Equivariant Graph Metanetworks (ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs enable single-shot fine-tuning of existing models, reducing the need for iterative optimization. We demonstrate the effectiveness of this approach empirically and provide a theoretical result: the gauge freedom induced by scaling symmetries is strictly smaller in convolutional neural networks than in multi-layer perceptrons. This insight helps explain the performance differences observed between architectures in both our work and that of Kalogeropoulos et al. (2024). Overall, our findings underscore the potential of symmetry-aware metanetworks as a powerful approach for efficient and generalizable neural network optimization. Open-source code: https://github.com/daniuyter/scalegmn_amortization",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08300v1",
    "published_date": "2025-10-09 14:51:15 UTC",
    "updated_date": "2025-10-09 14:51:15 UTC"
  },
  {
    "arxiv_id": "2510.08294v1",
    "title": "Counterfactual Identifiability via Dynamic Optimal Transport",
    "authors": [
      "Fabio De Sousa Ribeiro",
      "Ainkaran Santhirasekaram",
      "Ben Glocker"
    ],
    "abstract": "We address the open question of counterfactual identification for high-dimensional multivariate outcomes from observational data. Pearl (2000) argues that counterfactuals must be identifiable (i.e., recoverable from the observed data distribution) to justify causal claims. A recent line of work on counterfactual inference shows promising results but lacks identification, undermining the causal validity of its estimates. To address this, we establish a foundation for multivariate counterfactual identification using continuous-time flows, including non-Markovian settings under standard criteria. We characterise the conditions under which flow matching yields a unique, monotone and rank-preserving counterfactual transport map with tools from dynamic optimal transport, ensuring consistent inference. Building on this, we validate the theory in controlled scenarios with counterfactual ground-truth and demonstrate improvements in axiomatic counterfactual soundness on real images.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08294v1",
    "published_date": "2025-10-09 14:45:13 UTC",
    "updated_date": "2025-10-09 14:45:13 UTC"
  },
  {
    "arxiv_id": "2510.09694v1",
    "title": "Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection",
    "authors": [
      "Xiaodan Li",
      "Mengjie Wu",
      "Yao Zhu",
      "Yunna Lv",
      "YueFeng Chen",
      "Cen Chen",
      "Jianmei Guo",
      "Hui Xue"
    ],
    "abstract": "Large models (LMs) are powerful content generators, yet their open-ended nature can also introduce potential risks, such as generating harmful or biased content. Existing guardrails mostly perform post-hoc detection that may expose unsafe content before it is caught, and the latency constraints further push them toward lightweight models, limiting detection accuracy. In this work, we propose Kelp, a novel plug-in framework that enables streaming risk detection within the LM generation pipeline. Kelp leverages intermediate LM hidden states through a Streaming Latent Dynamics Head (SLD), which models the temporal evolution of risk across the generated sequence for more accurate real-time risk detection. To ensure reliable streaming moderation in real applications, we introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic harm predictions by embedding a benign-then-harmful temporal prior. Besides, for a rigorous evaluation of streaming guardrails, we also present StreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from each protected model, reflecting real-world streaming scenarios in both text and vision-language tasks. Across diverse models and datasets, Kelp consistently outperforms state-of-the-art post-hoc guardrails and prior plug-in probes (15.61% higher average F1), while using only 20M parameters and adding less than 0.5 ms of per-token latency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09694v1",
    "published_date": "2025-10-09 14:42:50 UTC",
    "updated_date": "2025-10-09 14:42:50 UTC"
  },
  {
    "arxiv_id": "2510.08279v2",
    "title": "Learning Neural Exposure Fields for View Synthesis",
    "authors": [
      "Michael Niemeyer",
      "Fabian Manhardt",
      "Marie-Julie Rakotosaona",
      "Michael Oechsle",
      "Christina Tsalicoglou",
      "Keisuke Tateno",
      "Jonathan T. Barron",
      "Federico Tombari"
    ],
    "abstract": "Recent advances in neural scene representations have led to unprecedented quality in 3D reconstruction and view synthesis. Despite achieving high-quality results for common benchmarks with curated data, outputs often degrade for data that contain per image variations such as strong exposure changes, present, e.g., in most scenes with indoor and outdoor areas or rooms with windows. In this paper, we introduce Neural Exposure Fields (NExF), a novel technique for robustly reconstructing 3D scenes with high quality and 3D-consistent appearance from challenging real-world captures. In the core, we propose to learn a neural field predicting an optimal exposure value per 3D point, enabling us to optimize exposure along with the neural scene representation. While capture devices such as cameras select optimal exposure per image/pixel, we generalize this concept and perform optimization in 3D instead. This enables accurate view synthesis in high dynamic range scenarios, bypassing the need of post-processing steps or multi-exposure captures. Our contributions include a novel neural representation for exposure prediction, a system for joint optimization of the scene representation and the exposure field via a novel neural conditioning mechanism, and demonstrated superior performance on challenging real-world data. We find that our approach trains faster than prior works and produces state-of-the-art results on several benchmarks improving by over 55% over best-performing baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2025. Project page available at https://m-niemeyer.github.io/nexf/index.html",
    "pdf_url": "https://arxiv.org/pdf/2510.08279v2",
    "published_date": "2025-10-09 14:32:41 UTC",
    "updated_date": "2025-10-10 11:49:36 UTC"
  },
  {
    "arxiv_id": "2510.08263v2",
    "title": "Co-TAP: Three-Layer Agent Interaction Protocol Technical Report",
    "authors": [
      "Shunyu An",
      "Miao Wang",
      "Yongchao Li",
      "Dong Wan",
      "Lina Wang",
      "Ling Qin",
      "Liqin Gao",
      "Congyao Fan",
      "Zhiyong Mao",
      "Jiange Pu",
      "Wenji Xia",
      "Dong Zhao",
      "Zhaohui Hao",
      "Rui Hu",
      "Ji Lu",
      "Guiyue Zhou",
      "Baoyu Tang",
      "Yanqin Gao",
      "Yongsheng Du",
      "Daigang Xu",
      "Lingjun Huang",
      "Baoli Wang",
      "Xiwen Zhang",
      "Luyao Wang",
      "Shilong Liu"
    ],
    "abstract": "This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer agent interaction protocol designed to address the challenges faced by multi-agent systems across the three core dimensions of Interoperability, Interaction and Collaboration, and Knowledge Sharing. We have designed and proposed a layered solution composed of three core protocols: the Human-Agent Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction layer, standardizing the flow of information between users, interfaces, and agents by defining a standardized, event-driven communication paradigm. This ensures the real-time performance, reliability, and synergy of interactions. As the core of the infrastructure layer, UAP is designed to break down communication barriers among heterogeneous agents through unified service discovery and protocol conversion mechanisms, thereby enabling seamless interconnection and interoperability of the underlying network. MEK, in turn, operates at the cognitive layer. By establishing a standardized ''Memory (M) - Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the ability to learn from individual experiences and form shareable knowledge, thereby laying the foundation for the realization of true collective intelligence. We believe this protocol framework will provide a solid engineering foundation and theoretical guidance for building the next generation of efficient, scalable, and intelligent multi-agent applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08263v2",
    "published_date": "2025-10-09 14:20:19 UTC",
    "updated_date": "2025-10-28 07:56:58 UTC"
  },
  {
    "arxiv_id": "2510.08257v1",
    "title": "A Distributed Emulation Environment for In-Memory Computing Systems",
    "authors": [
      "Eleni Bougioukou",
      "Anastasios Petropoulos",
      "Nikolaos Toulgaridis",
      "Theodoros Chatzimichail",
      "Theodore Antonakopoulos"
    ],
    "abstract": "In-memory computing technology is used extensively in artificial intelligence devices due to lower power consumption and fast calculation of matrix-based functions. The development of such a device and its integration in a system takes a significant amount of time and requires the use of a real-time emulation environment, where various system aspects are analyzed, microcode is tested, and applications are deployed, even before the real chip is available. In this work, we present the architecture, the software development tools, and experimental results of a distributed and expandable emulation system for rapid prototyping of integrated circuits based on in-memory computing technologies. Presented experimental results demonstrate the usefulness of the proposed emulator.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "6 pages, 5 figures, 2025 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)",
    "pdf_url": "https://arxiv.org/pdf/2510.08257v1",
    "published_date": "2025-10-09 14:15:35 UTC",
    "updated_date": "2025-10-09 14:15:35 UTC"
  },
  {
    "arxiv_id": "2510.08256v1",
    "title": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization",
    "authors": [
      "Jason Bohne",
      "Pawel Polak",
      "David Rosenberg",
      "Brian Bloniarz",
      "Gary Kazantsev"
    ],
    "abstract": "Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08256v1",
    "published_date": "2025-10-09 14:15:14 UTC",
    "updated_date": "2025-10-09 14:15:14 UTC"
  },
  {
    "arxiv_id": "2510.08255v1",
    "title": "Opponent Shaping in LLM Agents",
    "authors": [
      "Marta Emili Garcia Segura",
      "Stephen Hailes",
      "Mirco Musolesi"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 15 figures, 15 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08255v1",
    "published_date": "2025-10-09 14:13:24 UTC",
    "updated_date": "2025-10-09 14:13:24 UTC"
  },
  {
    "arxiv_id": "2510.08245v1",
    "title": "Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling",
    "authors": [
      "Jannek Ulm",
      "Kevin Du",
      "Vésteinn Snæbjarnarson"
    ],
    "abstract": "Large language models (LLMs) are trained on huge amounts of textual data, and concerns have been raised that the limits of such data may soon be reached. A potential solution is to train on synthetic data sampled from LLMs. In this work, we build on this idea and investigate the benefits of contrastive decoding for generating synthetic corpora. In a controlled setting, we experiment with sampling corpora using the relative difference between a good and bad model trained on the same original corpus of 100 million words. By amplifying the signal from a model that has better performance, we create a synthetic corpus and mix it with the original training data. Our findings show that training on a mixture of synthesized and real data improves performance on the language modeling objective and a range of downstream tasks. In particular, we see that training with a mix of synthetic data from contrastive decoding benefits tasks that require more reasoning skills, while synthetic data from traditional sampling helps more on tasks dependent on surface level linguistic capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08245v1",
    "published_date": "2025-10-09 14:04:52 UTC",
    "updated_date": "2025-10-09 14:04:52 UTC"
  },
  {
    "arxiv_id": "2511.04682v1",
    "title": "Efficient Deployment of CNN Models on Multiple In-Memory Computing Units",
    "authors": [
      "Eleni Bougioukou",
      "Theodore Antonakopoulos"
    ],
    "abstract": "In-Memory Computing (IMC) represents a paradigm shift in deep learning acceleration by mitigating data movement bottlenecks and leveraging the inherent parallelism of memory-based computations. The efficient deployment of Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use of advanced task allocation strategies for achieving maximum computational efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple Processing Units (PUs) for investigating how the deployment of a CNN model in a multi-processing system affects its performance, in terms of processing rate and latency. For that purpose, we introduce the Load-Balance-Longest-Path (LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE PUs, for maximizing the processing rate and minimizing latency due to efficient resources utilization. We are benchmarking LBLP against other alternative scheduling strategies for a number of CNN models and experimental results demonstrate the effectiveness of the proposed algorithm.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "5 pages, 4 figures, 2025 14th International Conference on Modern Circuits and Systems Technologies (MOCAST)",
    "pdf_url": "https://arxiv.org/pdf/2511.04682v1",
    "published_date": "2025-10-09 14:03:32 UTC",
    "updated_date": "2025-10-09 14:03:32 UTC"
  },
  {
    "arxiv_id": "2510.08238v1",
    "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness",
    "authors": [
      "Jiyang Qiu",
      "Xinbei Ma",
      "Yunqing Xu",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ],
    "abstract": "The rapid deployment of large language model (LLM)-based agents in real-world applications has raised serious concerns about their trustworthiness. In this work, we reveal the security and robustness vulnerabilities of these agents through backdoor attacks. Distinct from traditional backdoors limited to single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a multi-step backdoor attack designed for long-horizon agentic control. CoTri relies on an ordered sequence. It starts with an initial trigger, and subsequent ones are drawn from the environment, allowing multi-step manipulation that diverts the agent from its intended task. Experimental results show that CoTri achieves a near-perfect attack success rate (ASR) while maintaining a near-zero false trigger rate (FTR). Due to training data modeling the stochastic nature of the environment, the implantation of CoTri paradoxically enhances the agent's performance on benign tasks and even improves its robustness against environmental distractions. We further validate CoTri on vision-language models (VLMs), confirming its scalability to multimodal agents. Our work highlights that CoTri achieves stable, multi-step control within agents, improving their inherent robustness and task capabilities, which ultimately makes the attack more stealthy and raises potential safty risks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08238v1",
    "published_date": "2025-10-09 14:01:43 UTC",
    "updated_date": "2025-10-09 14:01:43 UTC"
  },
  {
    "arxiv_id": "2510.08236v2",
    "title": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models",
    "authors": [
      "Konrad Löhr",
      "Shuzhou Yuan",
      "Michael Färber"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly integral to information dissemination and decision-making processes. Given their growing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propagation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the inherent political leanings of these models. Subsequently, persona prompting with the PCT is used to explore explicit stereotypes across various social dimensions. In a final step, implicit stereotypes are uncovered by evaluating models with multilingual versions of the PCT. Key findings reveal a consistent left-leaning political alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those identified via explicit persona prompting. Interestingly, for most models, implicit and explicit stereotypes show a notable alignment, suggesting a degree of transparency or \"awareness\" regarding their inherent biases. This study underscores the complex interplay of political bias and stereotypes in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08236v2",
    "published_date": "2025-10-09 14:00:40 UTC",
    "updated_date": "2025-10-16 13:44:28 UTC"
  },
  {
    "arxiv_id": "2510.08662v2",
    "title": "DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops",
    "authors": [
      "Pengcheng Deng",
      "Kening Liu",
      "Mengxi Zhou",
      "Mingxi Li",
      "Rui Yang",
      "Chuzhe Cao",
      "Maojun Wang",
      "Zeyu Zhang"
    ],
    "abstract": "Genomic Selection (GS) uses whole-genome information to predict crop phenotypes and accelerate breeding. Traditional GS methods, however, struggle with prediction accuracy for complex traits and large datasets. We propose DPCformer, a deep learning model integrating convolutional neural networks with a self-attention mechanism to model complex genotype-phenotype relationships. We applied DPCformer to 13 traits across five crops (maize, cotton, tomato, rice, chickpea). Our approach uses an 8-dimensional one-hot encoding for SNP data, ordered by chromosome, and employs the PMF algorithm for feature selection. Evaluations show DPCformer outperforms existing methods. In maize datasets, accuracy for traits like days to tasseling and plant height improved by up to 2.92%. For cotton, accuracy gains for fiber traits reached 8.37%. On small-sample tomato data, the Pearson Correlation Coefficient for a key trait increased by up to 57.35%. In chickpea, the yield correlation was boosted by 16.62%. DPCformer demonstrates superior accuracy, robustness in small-sample scenarios, and enhanced interpretability, providing a powerful tool for precision breeding and addressing global food security challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted by BIBM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08662v2",
    "published_date": "2025-10-09 13:53:36 UTC",
    "updated_date": "2025-11-10 14:28:13 UTC"
  },
  {
    "arxiv_id": "2510.08661v1",
    "title": "CATS-Linear: Classification Auxiliary Linear Model for Time Series Forecasting",
    "authors": [
      "Zipo Jibao",
      "Yingyi Fu",
      "Xinyang Chen",
      "Guoting Chen"
    ],
    "abstract": "Recent research demonstrates that linear models achieve forecasting performance competitive with complex architectures, yet methodologies for enhancing linear models remain underexplored. Motivated by the hypothesis that distinct time series instances may follow heterogeneous linear mappings, we propose the Classification Auxiliary Trend-Seasonal Decoupling Linear Model CATS-Linear, employing Classification Auxiliary Channel-Independence (CACI). CACI dynamically routes instances to dedicated predictors via classification, enabling supervised channel design. We further analyze the theoretical expected risks of different channel settings. Additionally, we redesign the trend-seasonal decomposition architecture by adding a decoupling -- linear mapping -- recoupling framework for trend components and complex-domain linear projections for seasonal components. Extensive experiments validate that CATS-Linear with fixed hyperparameters achieves state-of-the-art accuracy comparable to hyperparameter-tuned baselines while delivering SOTA accuracy against fixed-hyperparameter counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08661v1",
    "published_date": "2025-10-09 13:51:48 UTC",
    "updated_date": "2025-10-09 13:51:48 UTC"
  },
  {
    "arxiv_id": "2510.08222v1",
    "title": "Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens",
    "authors": [
      "Yunlong Deng",
      "Boyang Sun",
      "Yan Li",
      "Lingjing Kong",
      "Zeyu Tang",
      "Kun Zhang",
      "Guangyi Chen"
    ],
    "abstract": "Due to their inherent complexity, reasoning tasks have long been regarded as rigorous benchmarks for assessing the capabilities of machine learning models, especially large language models (LLMs). Although humans can solve these tasks with ease, existing models, even after extensive pre-training and post-training at scale, still fail to perform reasoning reliably. In this paper, we revisit reasoning tasks from a causal perspective, seeking to understand their behavior in latent space and to offer insights for addressing their challenges. Specifically, we cast reasoning tasks as a selection mechanism, in which high-level logical concepts function as selection operators on the given observations, such as, identifying the correct answer in a math problem or filling the appropriate entry in Sudoku. We emphasize two key properties of this formulation that shed light on the difficulty of reasoning tasks. First, the latent space exceeds the observation space in complexity, even when the correct answer is fully determined by the observed input. Second, the latent variables, corresponding to logical thought, are densely structured and exhibit strong dependencies. Building on this formulation, we introduce a framework, called SR$^2$, that incorporates the estimated latent variables as feedback into the selection mechanism, thereby facilitating the learning of dense dependencies among latent representations. The framework consists of three key modules: reflective representation learning, dependency self-refinement, and periodic intermediate alignment. Experimentally, we show that our approach yields significant gains in reasoning accuracy, for example, attaining over 10$\\%$ improvement in performance with 8$\\times$ fewer parameters on the Sudoku and Maze tasks over the recent advances.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08222v1",
    "published_date": "2025-10-09 13:45:31 UTC",
    "updated_date": "2025-10-09 13:45:31 UTC"
  },
  {
    "arxiv_id": "2510.08218v1",
    "title": "Expressive Value Learning for Scalable Offline Reinforcement Learning",
    "authors": [
      "Nicolas Espinosa-Dice",
      "Kiante Brantley",
      "Wen Sun"
    ],
    "abstract": "Reinforcement learning (RL) is a powerful paradigm for learning to make sequences of decisions. However, RL has yet to be fully leveraged in robotics, principally due to its lack of scalability. Offline RL offers a promising avenue by training agents on large, diverse datasets, avoiding the costly real-world interactions of online RL. Scaling offline RL to increasingly complex datasets requires expressive generative models such as diffusion and flow matching. However, existing methods typically depend on either backpropagation through time (BPTT), which is computationally prohibitive, or policy distillation, which introduces compounding errors and limits scalability to larger base policies. In this paper, we consider the question of how to develop a scalable offline RL approach without relying on distillation or backpropagation through time. We introduce Expressive Value Learning for Offline Reinforcement Learning (EVOR): a scalable offline RL approach that integrates both expressive policies and expressive value functions. EVOR learns an optimal, regularized Q-function via flow matching during training. At inference-time, EVOR performs inference-time policy extraction via rejection sampling against the expressive value function, enabling efficient optimization, regularization, and compute-scalable search without retraining. Empirically, we show that EVOR outperforms baselines on a diverse set of offline RL tasks, demonstrating the benefit of integrating expressive value learning into offline RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08218v1",
    "published_date": "2025-10-09 13:42:20 UTC",
    "updated_date": "2025-10-09 13:42:20 UTC"
  },
  {
    "arxiv_id": "2510.08217v1",
    "title": "FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption",
    "authors": [
      "Justus Viga",
      "Penelope Mueck",
      "Alexander Löser",
      "Torben Weis"
    ],
    "abstract": "In the shipping industry, fuel consumption and emissions are critical factors due to their significant impact on economic efficiency and environmental sustainability. Accurate prediction of ship fuel consumption is essential for further optimization of maritime operations. However, heterogeneous methodologies and limited high-quality datasets hinder direct comparison of modeling approaches. This paper makes three key contributions: (1) we introduce and release a new dataset (https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operational and environmental data from three ships; (2) we define a standardized benchmark covering tabular regression and time-series regression (3) we investigate the application of in-context learning for ship consumption modeling using the TabPFN foundation model - a first in this domain to our knowledge. Our results demonstrate strong performance across all evaluated models, supporting the feasibility of onboard, data-driven fuel prediction. Models incorporating environmental conditions consistently outperform simple polynomial baselines relying solely on vessel speed. TabPFN slightly outperforms other techniques, highlighting the potential of foundation models with in-context learning capabilities for tabular prediction. Furthermore, including temporal context improves accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution will be published in \"ECML PKDD Workshop 2025 - Advanced Analytics and Learning on Temporal Data\"",
    "pdf_url": "https://arxiv.org/pdf/2510.08217v1",
    "published_date": "2025-10-09 13:38:46 UTC",
    "updated_date": "2025-10-09 13:38:46 UTC"
  },
  {
    "arxiv_id": "2510.08211v2",
    "title": "LLMs Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions",
    "authors": [
      "Xuhao Hu",
      "Peng Wang",
      "Xiaoya Lu",
      "Dongrui Liu",
      "Xuanjing Huang",
      "Jing Shao"
    ],
    "abstract": "Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions. Refer to https://github.com/hxhcreate/LLM_Deceive_Unintentionally for experimental resources.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08211v2",
    "published_date": "2025-10-09 13:35:19 UTC",
    "updated_date": "2026-01-18 07:03:40 UTC"
  },
  {
    "arxiv_id": "2510.08207v1",
    "title": "DODO: Causal Structure Learning with Budgeted Interventions",
    "authors": [
      "Matteo Gregorini",
      "Chiara Boldrini",
      "Lorenzo Valerio"
    ],
    "abstract": "Artificial Intelligence has achieved remarkable advancements in recent years, yet much of its progress relies on identifying increasingly complex correlations. Enabling causality awareness in AI has the potential to enhance its performance by enabling a deeper understanding of the underlying mechanisms of the environment. In this paper, we introduce DODO, an algorithm defining how an Agent can autonomously learn the causal structure of its environment through repeated interventions. We assume a scenario where an Agent interacts with a world governed by a causal Directed Acyclic Graph (DAG), which dictates the system's dynamics but remains hidden from the Agent. The Agent's task is to accurately infer the causal DAG, even in the presence of noise. To achieve this, the Agent performs interventions, leveraging causal inference techniques to analyze the statistical significance of observed changes. Results show better performance for DODO, compared to observational approaches, in all but the most limited resource conditions. DODO is often able to reconstruct with as low as zero errors the structure of the causal graph. In the most challenging configuration, DODO outperforms the best baseline by +0.25 F1 points.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review. Supported by SoBigDatait IR0000013, FAIR PE00000013, ICSC CN00000013",
    "pdf_url": "https://arxiv.org/pdf/2510.08207v1",
    "published_date": "2025-10-09 13:32:33 UTC",
    "updated_date": "2025-10-09 13:32:33 UTC"
  },
  {
    "arxiv_id": "2510.08203v1",
    "title": "Memory Retrieval and Consolidation in Large Language Models through Function Tokens",
    "authors": [
      "Shaohua Zhang",
      "Yuan Lin",
      "Hang Li"
    ],
    "abstract": "The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08203v1",
    "published_date": "2025-10-09 13:31:20 UTC",
    "updated_date": "2025-10-09 13:31:20 UTC"
  },
  {
    "arxiv_id": "2510.08202v1",
    "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions",
    "authors": [
      "Lirui Guo",
      "Michael G. Burke",
      "Wynita M. Griggs"
    ],
    "abstract": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for presentation at IEEE ITSC 2025 and for publication in its Proceedings. \\c{opyright} 2025 IEEE. Personal use permitted; other uses require permission from IEEE, including reprinting, republishing, or reuse of any copyrighted component of this work",
    "pdf_url": "https://arxiv.org/pdf/2510.08202v1",
    "published_date": "2025-10-09 13:30:23 UTC",
    "updated_date": "2025-10-09 13:30:23 UTC"
  },
  {
    "arxiv_id": "2510.08197v1",
    "title": "The Tournament Tree Method for preference elicitation in Multi-criteria decision-making",
    "authors": [
      "Diego García-Zamora",
      "Álvaro Labella",
      "José Rui Figueira"
    ],
    "abstract": "Pairwise comparison methods, such as Fuzzy Preference Relations and Saaty's Multiplicative Preference Relations, are widely used to model expert judgments in multi-criteria decision-making. However, their application is limited by the high cognitive load required to complete $m(m-1)/2$ comparisons, the risk of inconsistency, and the computational complexity of deriving consistent value scales. This paper proposes the Tournament Tree Method (TTM), a novel elicitation and evaluation framework that overcomes these limitations. The TTM requires only $m-1$ pairwise comparisons to obtain a complete, reciprocal, and consistent comparison matrix. The method consists of three phases: (i) elicitation of expert judgments using a reduced set of targeted comparisons, (ii) construction of the consistent pairwise comparison matrix, and (iii) derivation of a global value scale from the resulting matrix. The proposed approach ensures consistency by design, minimizes cognitive effort, and reduces the dimensionality of preference modeling from $m(m-1)/2$ to $m$ parameters. Furthermore, it is compatible with the classical Deck of Cards method, and thus it can handle interval and ratio scales. We have also developed a web-based tool that demonstrates its practical applicability in real decision-making scenarios.",
    "categories": [
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08197v1",
    "published_date": "2025-10-09 13:24:32 UTC",
    "updated_date": "2025-10-09 13:24:32 UTC"
  },
  {
    "arxiv_id": "2510.08193v2",
    "title": "Measuring What Matters: The AI Pluralism Index",
    "authors": [
      "Rashid Mushkani"
    ],
    "abstract": "Artificial intelligence systems increasingly mediate knowledge, communication, and decision making. Development and governance remain concentrated within a small set of firms and states, raising concerns that technologies may encode narrow interests and limit public agency. Capability benchmarks for language, vision, and coding are common, yet public, auditable measures of pluralistic governance are rare. We define AI pluralism as the degree to which affected stakeholders can shape objectives, data practices, safeguards, and deployment. We present the AI Pluralism Index (AIPI), a transparent, evidence-based instrument that evaluates producers and system families across four pillars: participatory governance, inclusivity and diversity, transparency, and accountability. AIPI codes verifiable practices from public artifacts and independent evaluations, explicitly handling \"Unknown\" evidence to report both lower-bound (\"evidence\") and known-only scores with coverage. We formalize the measurement model; implement a reproducible pipeline that integrates structured web and repository analysis, external assessments, and expert interviews; and assess reliability with inter-rater agreement, coverage reporting, cross-index correlations, and sensitivity analysis. The protocol, codebook, scoring scripts, and evidence graph are maintained openly with versioned releases and a public adjudication process. We report pilot provider results and situate AIPI relative to adjacent transparency, safety, and governance frameworks. The index aims to steer incentives toward pluralistic practice and to equip policymakers, procurers, and the public with comparable evidence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been accepted for publication in the proceedings of the International Association for Safe & Ethical AI (IASEAI) 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.08193v2",
    "published_date": "2025-10-09 13:19:34 UTC",
    "updated_date": "2025-12-15 02:08:45 UTC"
  },
  {
    "arxiv_id": "2510.08189v2",
    "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?",
    "authors": [
      "Yi Lu",
      "Jianing Wang",
      "Linsen Guo",
      "Wei He",
      "Hongyin Tang",
      "Tao Gui",
      "Xuanjing Huang",
      "Xuezhi Cao",
      "Wei Wang",
      "Xunliang Cai"
    ],
    "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08189v2",
    "published_date": "2025-10-09 13:16:22 UTC",
    "updated_date": "2025-10-21 13:49:36 UTC"
  },
  {
    "arxiv_id": "2510.08178v1",
    "title": "Robust Canonicalization through Bootstrapped Data Re-Alignment",
    "authors": [
      "Johann Schmidt",
      "Sebastian Stober"
    ],
    "abstract": "Fine-grained visual classification (FGVC) tasks, such as insect and bird identification, demand sensitivity to subtle visual cues while remaining robust to spatial transformations. A key challenge is handling geometric biases and noise, such as different orientations and scales of objects. Existing remedies rely on heavy data augmentation, which demands powerful models, or on equivariant architectures, which constrain expressivity and add cost. Canonicalization offers an alternative by shielding such biases from the downstream model. In practice, such functions are often obtained using canonicalization priors, which assume aligned training data. Unfortunately, real-world datasets never fulfill this assumption, causing the obtained canonicalizer to be brittle. We propose a bootstrapping algorithm that iteratively re-aligns training samples by progressively reducing variance and recovering the alignment assumption. We establish convergence guarantees under mild conditions for arbitrary compact groups, and show on four FGVC benchmarks that our method consistently outperforms equivariant, and canonicalization baselines while performing on par with augmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08178v1",
    "published_date": "2025-10-09 13:05:20 UTC",
    "updated_date": "2025-10-09 13:05:20 UTC"
  },
  {
    "arxiv_id": "2510.08176v1",
    "title": "Leveraging Whisper Embeddings for Audio-based Lyrics Matching",
    "authors": [
      "Eleonora Mancini",
      "Joan Serrà",
      "Paolo Torroni",
      "Yuki Mitsufuji"
    ],
    "abstract": "Audio-based lyrics matching can be an appealing alternative to other content-based retrieval approaches, but existing methods often suffer from limited reproducibility and inconsistent baselines. In this work, we introduce WEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings for lyrics matching tasks. WEALY establishes robust and transparent baselines, while also exploring multimodal extensions that integrate textual and acoustic features. Through extensive experiments on standard datasets, we demonstrate that WEALY achieves a performance comparable to state-of-the-art methods that lack reproducibility. In addition, we provide ablation studies and analyses on language robustness, loss functions, and embedding strategies. This work contributes a reliable benchmark for future research, and underscores the potential of speech technologies for music information retrieval tasks.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08176v1",
    "published_date": "2025-10-09 13:03:34 UTC",
    "updated_date": "2025-10-09 13:03:34 UTC"
  },
  {
    "arxiv_id": "2510.08659v1",
    "title": "Provably Robust Adaptation for Language-Empowered Foundation Models",
    "authors": [
      "Yuni Lai",
      "Xiaoyu Xue",
      "Linghui Shen",
      "Yulun Wu",
      "Gaolei Li",
      "Song Guo",
      "Kai Zhou",
      "Bin Xiao"
    ],
    "abstract": "Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP, have transformed multimodal learning by aligning visual (or graph) features with textual representations, enabling powerful downstream capabilities like few-shot learning. However, the reliance on small, task-specific support datasets collected in open environments exposes these models to poisoning attacks, where adversaries manipulate the support samples to degrade performance. Existing defenses rely on empirical strategies, which lack formal guarantees and remain vulnerable to unseen and adaptive attacks. Certified robustness offers provable guarantees but has been largely unexplored for few-shot classifiers based on LeFMs. This study seeks to fill these critical gaps by proposing the first provably robust few-shot classifier that is tailored for LeFMs. We term our model Language-empowered Few-shot Certification (\\textbf{LeFCert}). It integrates both textual and feature embeddings with an adaptive blending mechanism. To achieve provable robustness, we propose a twofold trimmed mean prototype and derive provable upper and lower bounds for classification scores, enabling certification under worst-case poisoning scenarios. To further enhance the performance, we extend LeFCert with two variants by considering a more realistic and tighter attack budget: LeFCert-L incorporates randomized smoothing to provide Lipschitz continuity and derive robustness under dual budget constraints, and LeFCert-C provides collective certification for scenarios where attackers distribute a shared poisoning budget across multiple samples. Experiments demonstrate that LeFCert achieves state-of-the-art performance, significantly improving both clean and certified accuracy compared to existing baselines. Despite its advanced robustness mechanisms, LeFCert is computationally efficient, making it practical for real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.08659v1",
    "published_date": "2025-10-09 13:01:57 UTC",
    "updated_date": "2025-10-09 13:01:57 UTC"
  },
  {
    "arxiv_id": "2510.08175v1",
    "title": "Prepared mind, fast response: A temporal decoupling framework for adaptive knowledge orchestration in open-domain dialogue",
    "authors": [
      "Jinling Gan",
      "Churong Liang",
      "Runnan Li"
    ],
    "abstract": "The latency-quality tradeoff is a fundamental constraint in open-domain dialogue AI systems, since comprehensive knowledge access necessitates prohibitive response delays. Contemporary approaches offer two inadequate solutions: lightweight instruct models achieve sub-second latency but lack reasoning depth, while tool-augmented ReAct agents enhance factuality through external knowledge at the cost of synchronous execution that blocks interaction during retrieval processes. PMFR is thus proposed, with a temporal decoupling framework that fundamentally resolves the contradiction through asynchronous knowledge orchestration. PMFR employs three coordinated components: (1) a Knowledge Adequacy Evaluator for real-time sufficiency assessment, (2) a Lightweight Response Generator for immediate user interaction, and (3) an Asynchronous Knowledge Refinement Agent for background knowledge enhancement. This architecture maintains continuous conversational flow while progressively enriching knowledge coverage through intelligent triggering mechanisms. Evaluation results on TopiOCQA demonstrate PMFR outperforms brute-force scaling: PMFR achieves 95.3% latency reduction (23.38s -> 1.09s) while preserving response quality comparable to heavyweight synchronous baselines (GEval-C: 0.613 vs. 0.620).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08175v1",
    "published_date": "2025-10-09 13:01:00 UTC",
    "updated_date": "2025-10-09 13:01:00 UTC"
  },
  {
    "arxiv_id": "2510.08173v1",
    "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions",
    "authors": [
      "Haolin Yang",
      "Yuxing Long",
      "Zhuoyuan Yu",
      "Zihan Yang",
      "Minghan Wang",
      "Jiapeng Xu",
      "Yihan Wang",
      "Ziyan Yu",
      "Wenzhe Cai",
      "Lei Kang",
      "Hao Dong"
    ],
    "abstract": "Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08173v1",
    "published_date": "2025-10-09 12:59:19 UTC",
    "updated_date": "2025-10-09 12:59:19 UTC"
  },
  {
    "arxiv_id": "2510.08159v1",
    "title": "Quantum Agents for Algorithmic Discovery",
    "authors": [
      "Iordanis Kerenidis",
      "El-Amine Cherrat"
    ],
    "abstract": "We introduce quantum agents trained by episodic, reward-based reinforcement learning to autonomously rediscover several seminal quantum algorithms and protocols. In particular, our agents learn: efficient logarithmic-depth quantum circuits for the Quantum Fourier Transform; Grover's search algorithm; optimal cheating strategies for strong coin flipping; and optimal winning strategies for the CHSH and other nonlocal games. The agents achieve these results directly through interaction, without prior access to known optimal solutions. This demonstrates the potential of quantum intelligence as a tool for algorithmic discovery, opening the way for the automated design of novel quantum algorithms and protocols.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08159v1",
    "published_date": "2025-10-09 12:38:53 UTC",
    "updated_date": "2025-10-09 12:38:53 UTC"
  },
  {
    "arxiv_id": "2510.08152v1",
    "title": "DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations",
    "authors": [
      "Elena Khasanova",
      "Harsh Saini",
      "Md Tahmid Rahman Laskar",
      "Xue-Yong Fu",
      "Cheng Chen",
      "Shashi Bhushan TN"
    ],
    "abstract": "The rapid advancements in Large Language Models (LLMs) have enabled their adoption in real-world industrial scenarios for various natural language processing tasks. However, the high inference cost of large-scale LLMs makes their deployment impractical, necessitating the use of smaller models. Despite their efficiency, smaller LLMs lack robust zero-shot instruction-following capabilities across diverse domains, limiting their adaptability to dynamic user requirements. Traditional fine-tuning approaches exacerbate this issue by inducing catastrophic forgetting, reducing the model's generalization ability for unseen tasks. In this paper, we propose Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual pre-training technique that enhances smaller LLMs' domain adaptability for business conversational tasks. Unlike conventional pre-training approaches that rely on next-token prediction, DACIP-RC generates diverse task instructions and responses via reading comprehension on conversation transcripts, enabling better instruction generalization. Our empirical evaluations demonstrate that DACIP-RC significantly improves zero-shot generalization across a wide range of business conversational tasks, including meeting summarization, action item generation, and call purpose identification. To the best of our knowledge, this is the first work to apply instruction pre-training on business conversational data, providing insights into how industries can leverage proprietary datasets for domain adaptation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the EMNLP 2025 Industry Track. Equal contribution from the first four authors",
    "pdf_url": "https://arxiv.org/pdf/2510.08152v1",
    "published_date": "2025-10-09 12:35:20 UTC",
    "updated_date": "2025-10-09 12:35:20 UTC"
  },
  {
    "arxiv_id": "2510.08149v1",
    "title": "AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents",
    "authors": [
      "Md Tahmid Rahman Laskar",
      "Julien Bouvier Tremblay",
      "Xue-Yong Fu",
      "Cheng Chen",
      "Shashi Bhushan TN"
    ],
    "abstract": "The utilization of conversational AI systems by leveraging Retrieval Augmented Generation (RAG) techniques to solve customer problems has been on the rise with the rapid progress of Large Language Models (LLMs). However, the absence of a company-specific dedicated knowledge base is a major barrier to the integration of conversational AI systems in contact centers. To this end, we introduce AI Knowledge Assist, a system that extracts knowledge in the form of question-answer (QA) pairs from historical customer-agent conversations to automatically build a knowledge base. Fine-tuning a lightweight LLM on internal data demonstrates state-of-the-art performance, outperforming larger closed-source LLMs. More specifically, empirical evaluation on 20 companies demonstrates that the proposed AI Knowledge Assist system that leverages the LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by achieving above 90% accuracy in answering information-seeking questions. This enables immediate deployment of RAG-powered chatbots.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the EMNLP 2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2510.08149v1",
    "published_date": "2025-10-09 12:34:31 UTC",
    "updated_date": "2025-10-09 12:34:31 UTC"
  },
  {
    "arxiv_id": "2510.08146v3",
    "title": "Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning",
    "authors": [
      "Aman Sharma",
      "Paras Chopra"
    ],
    "abstract": "We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they've gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08146v3",
    "published_date": "2025-10-09 12:33:16 UTC",
    "updated_date": "2025-10-28 10:58:14 UTC"
  },
  {
    "arxiv_id": "2510.08657v1",
    "title": "Inner-Instance Normalization for Time Series Forecasting",
    "authors": [
      "Zipo Jibao",
      "Yingyi Fu",
      "Xinyang Chen",
      "Guoting Chen"
    ],
    "abstract": "Real-world time series are influenced by numerous factors and exhibit complex non-stationary characteristics. Non-stationarity can lead to distribution shifts, where the statistical properties of time series change over time, negatively impacting model performance. Several instance normalization techniques have been proposed to address distribution shifts in time series forecasting. However, existing methods fail to account for shifts within individual instances, leading to suboptimal performance. To tackle inner-instance distribution shifts, we propose two novel point-level methods: Learning Distribution (LD) and Learning Conditional Distribution (LCD). LD eliminates internal discrepancies by fitting the internal distribution of input and output with different parameters at different time steps, while LCD utilizes neural networks to predict scaling coefficients of the output. We evaluate the performance of the two methods with various backbone models across public benchmarks and demonstrate the effectiveness of the point-level paradigm through comparative experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08657v1",
    "published_date": "2025-10-09 12:24:47 UTC",
    "updated_date": "2025-10-09 12:24:47 UTC"
  },
  {
    "arxiv_id": "2510.08138v1",
    "title": "Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement",
    "authors": [
      "Chengzhi Li",
      "Heyan Huang",
      "Ping Jian",
      "Zhen Yang",
      "Yaning Tian"
    ],
    "abstract": "Large language models (LLMs) often generate self-contradictory outputs, which severely impacts their reliability and hinders their adoption in practical applications. In video-language models (Video-LLMs), this phenomenon recently draws the attention of researchers. Specifically, these models fail to provide logically consistent responses to rephrased questions based on their grounding outputs. However, the underlying causes of this phenomenon remain underexplored. In this work, we adopt an interpretability-driven approach to analyze, statistically summarize, and intervention the potential factors of the phenomenon. We find that one of the primary reasons for the inconsistency in responses lies in the inability of cross-modal attention heads to effectively distinguish video tokens across different timestamps. To address this, we propose an attention enhancement method called Temporally Conditioned Attention Sharpening (TCAS), which constructs an enhancement objective based on attention distinctions to enhance the model's temporal resolution capability, thereby improving its temporal understanding logic consistency. Experimental results demonstrate that our method significantly enhances the temporal logic consistency of Video-LLMs. Further interpretability analyses reveal that our method indeed improves the temporal discriminability of attention heads, validating our conclusions. Additionally, our method achieves performance improvements in general video temporal grounding tasks, highlighting that temporal logic consistency is a bottleneck in temporal understanding. By enhancing consistency, our method drives significant progress in video temporal understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08138v1",
    "published_date": "2025-10-09 12:22:06 UTC",
    "updated_date": "2025-10-09 12:22:06 UTC"
  },
  {
    "arxiv_id": "2510.08132v1",
    "title": "Approximate Domain Unlearning for Vision-Language Models",
    "authors": [
      "Kodai Kawamura",
      "Yuta Goto",
      "Rintaro Yanagi",
      "Hirokatsu Kataoka",
      "Go Irie"
    ],
    "abstract": "Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize real cars while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments show that our approach outperforms baselines built upon VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code: https://kodaikawamura.github.io/Domain_Unlearning/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.08132v1",
    "published_date": "2025-10-09 12:17:59 UTC",
    "updated_date": "2025-10-09 12:17:59 UTC"
  },
  {
    "arxiv_id": "2510.08120v1",
    "title": "Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations",
    "authors": [
      "Jasmina Gajcin",
      "Erik Miehling",
      "Rahul Nair",
      "Elizabeth Daly",
      "Radu Marinescu",
      "Seshu Tirupathi"
    ],
    "abstract": "Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local Verifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful to decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 2 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08120v1",
    "published_date": "2025-10-09 12:05:37 UTC",
    "updated_date": "2025-10-09 12:05:37 UTC"
  },
  {
    "arxiv_id": "2510.08114v1",
    "title": "Can Risk-taking AI-Assistants suitably represent entities",
    "authors": [
      "Ali Mazyaki",
      "Mohammad Naghizadeh",
      "Samaneh Ranjkhah Zonouzaghi",
      "Amirhossein Farshi Sotoudeh"
    ],
    "abstract": "Responsible AI demands systems whose behavioral tendencies can be effectively measured, audited, and adjusted to prevent inadvertently nudging users toward risky decisions or embedding hidden biases in risk aversion. As language models (LMs) are increasingly incorporated into AI-driven decision support systems, understanding their risk behaviors is crucial for their responsible deployment. This study investigates the manipulability of risk aversion (MoRA) in LMs, examining their ability to replicate human risk preferences across diverse economic scenarios, with a focus on gender-specific attitudes, uncertainty, role-based decision-making, and the manipulability of risk aversion. The results indicate that while LMs such as DeepSeek Reasoner and Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable discrepancies highlight the need to refine bio-centric measures of manipulability. These findings suggest directions for refining AI design to better align human and AI risk preferences and enhance ethical decision-making. The study calls for further advancements in model design to ensure that AI systems more accurately replicate human risk preferences, thereby improving their effectiveness in risk management contexts. This approach could enhance the applicability of AI assistants in managing risk.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08114v1",
    "published_date": "2025-10-09 11:55:31 UTC",
    "updated_date": "2025-10-09 11:55:31 UTC"
  },
  {
    "arxiv_id": "2510.08113v1",
    "title": "Bayesian Decision Making around Experts",
    "authors": [
      "Daniel Jarne Ornia",
      "Joel Dyer",
      "Nicholas Bishop",
      "Anisoara Calinescu",
      "Michael Wooldridge"
    ],
    "abstract": "Complex learning agents are increasingly deployed alongside existing experts, such as human operators or previously trained agents. However, it remains unclear how should learners optimally incorporate certain forms of expert data, which may differ in structure from the learner's own action-outcome experiences. We study this problem in the context of Bayesian multi-armed bandits, considering: (i) offline settings, where the learner receives a dataset of outcomes from the expert's optimal policy before interaction, and (ii) simultaneous settings, where the learner must choose at each step whether to update its beliefs based on its own experience, or based on the outcome simultaneously achieved by an expert. We formalize how expert data influences the learner's posterior, and prove that pretraining on expert outcomes tightens information-theoretic regret bounds by the mutual information between the expert data and the optimal action. For the simultaneous setting, we propose an information-directed rule where the learner processes the data source that maximizes their one-step information gain about the optimal action. Finally, we propose strategies for how the learner can infer when to trust the expert and when not to, safeguarding the learner for the cases where the expert is ineffective or compromised. By quantifying the value of expert data, our framework provides practical, information-theoretic algorithms for agents to intelligently decide when to learn from others.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08113v1",
    "published_date": "2025-10-09 11:53:19 UTC",
    "updated_date": "2025-10-09 11:53:19 UTC"
  },
  {
    "arxiv_id": "2510.08109v1",
    "title": "VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents",
    "authors": [
      "Daniel Huwiler",
      "Kurt Stockinger",
      "Jonathan Fürst"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems fail when documents evolve through versioning-a ubiquitous characteristic of technical documentation. Existing approaches achieve only 58-64% accuracy on version-sensitive questions, retrieving semantically similar content without temporal validity checks. We present VersionRAG, a version-aware RAG framework that explicitly models document evolution through a hierarchical graph structure capturing version sequences, content boundaries, and changes between document states. During retrieval, VersionRAG routes queries through specialized paths based on intent classification, enabling precise version-aware filtering and change tracking. On our VersionQA benchmark-100 manually curated questions across 34 versioned technical documents-VersionRAG achieves 90% accuracy, outperforming naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit change detection where baselines fail (0-10%), demonstrating its ability to track undocumented modifications. Additionally, VersionRAG requires 97% fewer tokens during indexing than GraphRAG, making it practical for large-scale deployment. Our work establishes versioned document QA as a distinct task and provides both a solution and benchmark for future research.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08109v1",
    "published_date": "2025-10-09 11:48:58 UTC",
    "updated_date": "2025-10-09 11:48:58 UTC"
  },
  {
    "arxiv_id": "2510.08104v1",
    "title": "Development of Mental Models in Human-AI Collaboration: A Conceptual Framework",
    "authors": [
      "Joshua Holstein",
      "Gerhard Satzger"
    ],
    "abstract": "Artificial intelligence has become integral to organizational decision-making and while research has explored many facets of this human-AI collaboration, the focus has mainly been on designing the AI agent(s) and the way the collaboration is set up - generally assuming a human decision-maker to be \"fixed\". However, it has largely been neglected that decision-makers' mental models evolve through their continuous interaction with AI systems. This paper addresses this gap by conceptualizing how the design of human-AI collaboration influences the development of three complementary and interdependent mental models necessary for this collaboration. We develop an integrated socio-technical framework that identifies the mechanisms driving the mental model evolution: data contextualization, reasoning transparency, and performance feedback. Our work advances human-AI collaboration literature through three key contributions: introducing three distinct mental models (domain, information processing, complementarity-awareness); recognizing the dynamic nature of mental models; and establishing mechanisms that guide the purposeful design of effective human-AI collaboration.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Preprint version. Accepted for presentation at the International Conference on Information Systems (ICIS 2025). Please cite the published version when available",
    "pdf_url": "https://arxiv.org/pdf/2510.08104v1",
    "published_date": "2025-10-09 11:40:41 UTC",
    "updated_date": "2025-10-09 11:40:41 UTC"
  },
  {
    "arxiv_id": "2510.08102v1",
    "title": "Lossless Vocabulary Reduction for Auto-Regressive Language Models",
    "authors": [
      "Daiki Chijiwa",
      "Taku Hasegawa",
      "Kyosuke Nishida",
      "Shin'ya Yamaguchi",
      "Tomoya Ohba",
      "Tamao Sakao",
      "Susumu Takeuchi"
    ],
    "abstract": "Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distribution given the previous ones, and thus tokenization directly affects their efficiency in text generation. Since each language model has their own vocabulary as a set of possible tokens, they struggle to cooperate with each other at the level of next-token distributions such as model ensemble. In this paper, we establish a theoretical framework of lossless vocabulary reduction, which efficiently converts a given auto-regressive language model into the one with an arbitrarily small vocabulary without any loss in accuracy. As an application, we demonstrate that language models with different tokenization can cooperate with each other efficiently through their maximal common vocabulary.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08102v1",
    "published_date": "2025-10-09 11:38:48 UTC",
    "updated_date": "2025-10-09 11:38:48 UTC"
  },
  {
    "arxiv_id": "2510.08098v2",
    "title": "The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models",
    "authors": [
      "Sherzod Hakimov",
      "Roland Bernard",
      "Tim Leiber",
      "Karl Osswald",
      "Kristina Richert",
      "Ruilin Yang",
      "Raffaella Bernardi",
      "David Schlangen"
    ],
    "abstract": "Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We present the first comprehensive study that systematically evaluates how explicit reasoning training affects the negotiation abilities of both commercial and open-weight large language models, comparing these models to their vanilla counterparts across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning -- that is, scaling test time compute -- significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while a leading commercial model maintains language consistency between reasoning and final output.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EACL 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.08098v2",
    "published_date": "2025-10-09 11:36:38 UTC",
    "updated_date": "2026-01-09 10:31:20 UTC"
  },
  {
    "arxiv_id": "2510.09691v1",
    "title": "Evaluation of Differential Privacy Mechanisms on Federated Learning",
    "authors": [
      "Tejash Varsani"
    ],
    "abstract": "Federated learning is distributed model training across several clients without disclosing raw data. Despite advancements in data privacy, risks still remain. Differential Privacy (DP) is a technique to protect sensitive data by adding noise to model updates, usually controlled by a fixed privacy budget. However, this approach can introduce excessive noise, particularly when the model converges, which compromises performance. To address this problem, adaptive privacy budgets have been investigated as a potential solution. This work implements DP methods using Laplace and Gaussian mechanisms with an adaptive privacy budget, extending the SelecEval simulator. We introduce an adaptive clipping approach in the Gaussian mechanism, ensuring that gradients of the model are dynamically updated rather than using a fixed sensitivity. We conduct extensive experiments with various privacy budgets, IID and non-IID datasets, and different numbers of selected clients per round. While our experiments were limited to 200 training rounds, the results suggest that adaptive privacy budgets and adaptive clipping can help maintain model accuracy while preserving privacy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Supervised by Prof. Dr.-Ing. habil. Alois C. Knoll; Advisor: Nagacharan Teja Tangirala, M.Sc",
    "pdf_url": "https://arxiv.org/pdf/2510.09691v1",
    "published_date": "2025-10-09 11:32:36 UTC",
    "updated_date": "2025-10-09 11:32:36 UTC"
  },
  {
    "arxiv_id": "2510.08091v1",
    "title": "Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility",
    "authors": [
      "Shramay Palta",
      "Peter Rankel",
      "Sarah Wiegreffe",
      "Rachel Rudinger"
    ],
    "abstract": "We investigate the degree to which human plausibility judgments of multiple-choice commonsense benchmark answers are subject to influence by (im)plausibility arguments for or against an answer, in particular, using rationales generated by LLMs. We collect 3,000 plausibility judgments from humans and another 13,600 judgments from LLMs. Overall, we observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively, suggesting that, on the whole, human judges find these rationales convincing. Experiments with LLMs reveal similar patterns of influence. Our findings demonstrate a novel use of LLMs for studying aspects of human cognition, while also raising practical concerns that, even in domains where humans are ``experts'' (i.e., common sense), LLMs have the potential to exert considerable influence on people's beliefs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "pre-print",
    "pdf_url": "https://arxiv.org/pdf/2510.08091v1",
    "published_date": "2025-10-09 11:22:29 UTC",
    "updated_date": "2025-10-09 11:22:29 UTC"
  },
  {
    "arxiv_id": "2510.08086v1",
    "title": "From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport Framework for Certifiably Fair AI Systems",
    "authors": [
      "Sukriti Bhattacharya",
      "Chitro Majumdar"
    ],
    "abstract": "This paper presents a framework for provably fair AI that overcomes the limits of current bias mitigation methods by systematically removing all sensitive information and its proxies. Using ontology engineering in OWL 2 QL, it formally defines sensitive attributes and infers their proxies through logical reasoning, constructing a sigma algebra G that captures the full structure of biased patterns. Fair representations are then obtained via Delbaen Majumdar optimal transport, which generates variables independent of G while minimizing L2 distance to preserve accuracy. This guarantees true independence rather than mere decorrelation. By modeling bias as dependence between sigma algebras, compiling ontological knowledge into measurable structures, and using optimal transport as the unique fair transformation, the approach ensures complete fairness in tasks like loan approval, where proxies such as ZIP code reveal race. The result is a certifiable and mathematically grounded method for trustworthy AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08086v1",
    "published_date": "2025-10-09 11:18:41 UTC",
    "updated_date": "2025-10-09 11:18:41 UTC"
  },
  {
    "arxiv_id": "2510.08084v1",
    "title": "A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems",
    "authors": [
      "Hikmat A. M. Abdeljaber",
      "Md. Alamgir Hossain",
      "Sultan Ahmad",
      "Ahmed Alsanad",
      "Md Alimul Haque",
      "Sudan Jha",
      "Jabeen Nazeer"
    ],
    "abstract": "The rapid expansion of Internet of Things (IoT) devices has transformed industries and daily life by enabling widespread connectivity and data exchange. However, this increased interconnection has introduced serious security vulnerabilities, making IoT systems more exposed to sophisticated cyber attacks. This study presents a novel ensemble learning architecture designed to improve IoT attack detection. The proposed approach applies advanced machine learning techniques, specifically the Extra Trees Classifier, along with thorough preprocessing and hyperparameter optimization. It is evaluated on several benchmark datasets including CICIoT2023, IoTID20, BotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent performance, achieving high recall, accuracy, and precision with very low error rates. These outcomes demonstrate the model efficiency and superiority compared to existing approaches, providing an effective and scalable method for securing IoT environments. This research establishes a solid foundation for future progress in protecting connected devices from evolving cyber threats.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "14 pages, 5 fiugres, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08084v1",
    "published_date": "2025-10-09 11:15:15 UTC",
    "updated_date": "2025-10-09 11:15:15 UTC"
  },
  {
    "arxiv_id": "2510.08081v1",
    "title": "AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment",
    "authors": [
      "Xiaochong Lan",
      "Jie Feng",
      "Yinxing Liu",
      "Xinlei Shi",
      "Yong Li"
    ],
    "abstract": "Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08081v1",
    "published_date": "2025-10-09 11:11:02 UTC",
    "updated_date": "2025-10-09 11:11:02 UTC"
  },
  {
    "arxiv_id": "2510.08075v2",
    "title": "Multi-Condition Conformal Selection",
    "authors": [
      "Qingyang Hao",
      "Wenbo Liao",
      "Bingyi Jing",
      "Hongxin Wei"
    ],
    "abstract": "Selecting high-quality candidates from large-scale datasets is critically important in resource-constrained applications such as drug discovery, precision medicine, and the alignment of large language models. While conformal selection methods offer a rigorous solution with False Discovery Rate (FDR) control, their applicability is confined to single-threshold scenarios (i.e., y > c) and overlooks practical needs for multi-condition selection, such as conjunctive or disjunctive conditions. In this work, we propose the Multi-Condition Conformal Selection (MCCS) algorithm, which extends conformal selection to scenarios with multiple conditions. In particular, we introduce a novel nonconformity score with regional monotonicity for conjunctive conditions and a global Benjamini-Hochberg (BH) procedure for disjunctive conditions, thereby establishing finite-sample FDR control with theoretical guarantees. The integration of these components enables the proposed method to achieve rigorous FDR-controlled selection in various multi-condition environments. Extensive experiments validate the superiority of MCCS over baselines, its generalizability across diverse condition combinations, different real-world modalities, and multi-task scalability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08075v2",
    "published_date": "2025-10-09 11:02:10 UTC",
    "updated_date": "2025-10-12 10:02:13 UTC"
  },
  {
    "arxiv_id": "2510.08068v2",
    "title": "An Adaptive Multi Agent Bitcoin Trading System",
    "authors": [
      "Aadi Singhi"
    ],
    "abstract": "This paper presents a Multi Agent Bitcoin Trading system that utilizes Large Language Models (LLMs) for alpha generation and portfolio management in the cryptocurrencies market. Unlike equities, cryptocurrencies exhibit extreme volatility and are heavily influenced by rapidly shifting market sentiments and regulatory announcements, making them difficult to model using static regression models or neural networks trained solely on historical data. The proposed framework overcomes this by structuring LLMs into specialised agents for technical analysis, sentiment evaluation, decision-making, and performance reflection. The agents improve over time via a novel verbal feedback mechanism where a Reflect agent provides daily and weekly natural-language critiques of trading decisions. These textual evaluations are then injected into future prompts of the agents, allowing them to adjust allocation logic without weight updates or finetuning. Back-testing on Bitcoin price data from July 2024 to April 2025 shows consistent outperformance across market regimes: the Quantitative agent delivered over 30\\% higher returns in bullish phases and 15\\% overall gains versus buy-and-hold, while the sentiment-driven agent turned sideways markets from a small loss into a gain of over 100\\%. Adding weekly feedback further improved total performance by 31\\% and reduced bearish losses by 10\\%. The results demonstrate that verbal feedback represents a new, scalable, and low-cost approach of tuning LLMs for financial goals.",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "q-fin.PM",
    "comment": "18 pages, 6 figures , 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08068v2",
    "published_date": "2025-10-09 10:55:52 UTC",
    "updated_date": "2025-11-14 11:36:27 UTC"
  },
  {
    "arxiv_id": "2510.08062v1",
    "title": "Attribution-by-design: Ensuring Inference-Time Provenance in Generative Music Systems",
    "authors": [
      "Fabio Morreale",
      "Wiebke Hutiri",
      "Joan Serrà",
      "Alice Xiang",
      "Yuki Mitsufuji"
    ],
    "abstract": "The rise of AI-generated music is diluting royalty pools and revealing structural flaws in existing remuneration frameworks, challenging the well-established artist compensation systems in the music industry. Existing compensation solutions, such as piecemeal licensing agreements, lack scalability and technical rigour, while current data attribution mechanisms provide only uncertain estimates and are rarely implemented in practice. This paper introduces a framework for a generative music infrastructure centred on direct attribution, transparent royalty distribution, and granular control for artists and rights' holders. We distinguish ontologically between the training set and the inference set, which allows us to propose two complementary forms of attribution: training-time attribution and inference-time attribution. We here favour inference-time attribution, as it enables direct, verifiable compensation whenever an artist's catalogue is used to condition a generated output. Besides, users benefit from the ability to condition generations on specific songs and receive transparent information about attribution and permitted usage. Our approach offers an ethical and practical solution to the pressing need for robust compensation mechanisms in the era of AI-generated music, ensuring that provenance and fairness are embedded at the core of generative systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08062v1",
    "published_date": "2025-10-09 10:49:44 UTC",
    "updated_date": "2025-10-09 10:49:44 UTC"
  },
  {
    "arxiv_id": "2510.08058v1",
    "title": "FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation",
    "authors": [
      "Shule Lu",
      "Lingxiang Wang",
      "Sijia Wen",
      "Ziwei Wang",
      "Hainan Zhang"
    ],
    "abstract": "With the rapid development of artificial intelligence, dialogue systems have become a prominent form of human-computer interaction. However, traditional centralized or fully local training approaches face challenges in balancing privacy preservation and personalization due to data privacy concerns and heterogeneous device capabilities. Federated learning, as a representative distributed paradigm, offers a promising solution. However, existing methods often suffer from overfitting under limited client data and tend to forget global information after multiple training rounds, leading to poor generalization. To address these issues, we propose FedDTRE, a Federated adaptive aggregation strategy for Dialogue generation based on Trustworthiness Evaluation. Instead of directly replacing local models with the global model, FedDTRE leverages trustworthiness scores of both global and local models on a fairness-oriented evaluation dataset to dynamically regulate the global model's contribution during local updates. Experimental results demonstrate that FedDTRE can improve dialogue model performance and enhance the quality of dialogue generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08058v1",
    "published_date": "2025-10-09 10:43:14 UTC",
    "updated_date": "2025-10-09 10:43:14 UTC"
  },
  {
    "arxiv_id": "2510.08049v2",
    "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models",
    "authors": [
      "Congming Zheng",
      "Jiachen Zhu",
      "Zhuoying Ou",
      "Yuxiang Chen",
      "Kangning Zhang",
      "Rong Shan",
      "Zeyu Zheng",
      "Mengyue Yang",
      "Jianghao Lin",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08049v2",
    "published_date": "2025-10-09 10:35:31 UTC",
    "updated_date": "2025-10-21 14:21:25 UTC"
  },
  {
    "arxiv_id": "2510.08048v2",
    "title": "TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance",
    "authors": [
      "Jianhui Yang",
      "Yiming Jin",
      "Pengkun Jiao",
      "Chenhe Dong",
      "Zerui Huang",
      "Shaowei Yao",
      "Xiaojiang Zhou",
      "Dan Ou",
      "Haihong Tang"
    ],
    "abstract": "Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08048v2",
    "published_date": "2025-10-09 10:34:39 UTC",
    "updated_date": "2025-11-12 07:49:51 UTC"
  },
  {
    "arxiv_id": "2510.08046v1",
    "title": "LinguaSim: Interactive Multi-Vehicle Testing Scenario Generation via Natural Language Instruction Based on Large Language Models",
    "authors": [
      "Qingyuan Shi",
      "Qingwen Meng",
      "Hao Cheng",
      "Qing Xu",
      "Jianqiang Wang"
    ],
    "abstract": "The generation of testing and training scenarios for autonomous vehicles has drawn significant attention. While Large Language Models (LLMs) have enabled new scenario generation methods, current methods struggle to balance command adherence accuracy with the realism of real-world driving environments. To reduce scenario description complexity, these methods often compromise realism by limiting scenarios to 2D, or open-loop simulations where background vehicles follow predefined, non-interactive behaviors. We propose LinguaSim, an LLM-based framework that converts natural language into realistic, interactive 3D scenarios, ensuring both dynamic vehicle interactions and faithful alignment between the input descriptions and the generated scenarios. A feedback calibration module further refines the generation precision, improving fidelity to user intent. By bridging the gap between natural language and closed-loop, interactive simulations, LinguaSim constrains adversarial vehicle behaviors using both the scenario description and the autonomous driving model guiding them. This framework facilitates the creation of high-fidelity scenarios that enhance safety testing and training. Experiments show LinguaSim can generate scenarios with varying criticality aligned with different natural language descriptions (ACT: 0.072 s for dangerous vs. 3.532 s for safe descriptions; comfortability: 0.654 vs. 0.764), and its refinement module effectively reduces excessive aggressiveness in LinguaSim's initial outputs, lowering the crash rate from 46.9% to 6.3% to better match user intentions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08046v1",
    "published_date": "2025-10-09 10:30:02 UTC",
    "updated_date": "2025-10-09 10:30:02 UTC"
  },
  {
    "arxiv_id": "2510.08045v1",
    "title": "Verifying Graph Neural Networks with Readout is Intractable",
    "authors": [
      "Artem Chernobrovkin",
      "Marco Sälzer",
      "François Schwarzentruber",
      "Nicolas Troquard"
    ],
    "abstract": "We introduce a logical language for reasoning about quantized aggregate-combine graph neural networks with global readout (ACR-GNNs). We provide a logical characterization and use it to prove that verification tasks for quantized GNNs with readout are (co)NEXPTIME-complete. This result implies that the verification of quantized GNNs is computationally intractable, prompting substantial research efforts toward ensuring the safety of GNN-based systems. We also experimentally demonstrate that quantized ACR-GNN models are lightweight while maintaining good accuracy and generalization capabilities with respect to non-quantized models.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CC",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08045v1",
    "published_date": "2025-10-09 10:29:09 UTC",
    "updated_date": "2025-10-09 10:29:09 UTC"
  },
  {
    "arxiv_id": "2510.08044v1",
    "title": "Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation",
    "authors": [
      "Shiyuan Yin",
      "Chenjia Bai",
      "Zihao Zhang",
      "Junwei Jin",
      "Xinxin Zhang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Large language models (LLMs) demonstrate advanced reasoning abilities, enabling robots to understand natural language instructions and generate high-level plans with appropriate grounding. However, LLM hallucinations present a significant challenge, often leading to overconfident yet potentially misaligned or unsafe plans. While researchers have explored uncertainty estimation to improve the reliability of LLM-based planning, existing studies have not sufficiently differentiated between epistemic and intrinsic uncertainty, limiting the effectiveness of uncertainty estimation. In this paper, we present Combined Uncertainty estimation for Reliable Embodied planning (CURE), which decomposes the uncertainty into epistemic and intrinsic uncertainty, each estimated separately. Furthermore, epistemic uncertainty is subdivided into task clarity and task familiarity for more accurate evaluation. The overall uncertainty assessments are obtained using random network distillation and multi-layer perceptron regression heads driven by LLM features. We validated our approach in two distinct experimental settings: kitchen manipulation and tabletop rearrangement experiments. The results show that, compared to existing methods, our approach yields uncertainty estimates that are more closely aligned with the actual execution outcomes.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08044v1",
    "published_date": "2025-10-09 10:26:58 UTC",
    "updated_date": "2025-10-09 10:26:58 UTC"
  },
  {
    "arxiv_id": "2510.08039v1",
    "title": "MRI-derived quantification of hepatic vessel-to-volume ratios in chronic liver disease using a deep learning approach",
    "authors": [
      "Alexander Herold",
      "Daniel Sobotka",
      "Lucian Beer",
      "Nina Bastati",
      "Sarah Poetter-Lang",
      "Michael Weber",
      "Thomas Reiberger",
      "Mattias Mandorfer",
      "Georg Semmler",
      "Benedikt Simbrunner",
      "Barbara D. Wichtmann",
      "Sami A. Ba-Ssalamah",
      "Michael Trauner",
      "Ahmed Ba-Ssalamah",
      "Georg Langs"
    ],
    "abstract": "Background: We aimed to quantify hepatic vessel volumes across chronic liver disease stages and healthy controls using deep learning-based magnetic resonance imaging (MRI) analysis, and assess correlations with biomarkers for liver (dys)function and fibrosis/portal hypertension.\n  Methods: We assessed retrospectively healthy controls, non-advanced and advanced chronic liver disease (ACLD) patients using a 3D U-Net model for hepatic vessel segmentation on portal venous phase gadoxetic acid-enhanced 3-T MRI. Total (TVVR), hepatic (HVVR), and intrahepatic portal vein-to-volume ratios (PVVR) were compared between groups and correlated with: albumin-bilirubin (ALBI) and model for end-stage liver disease-sodium (MELD-Na) score, and fibrosis/portal hypertension (Fibrosis-4 [FIB-4] score, liver stiffness measurement [LSM], hepatic venous pressure gradient [HVPG], platelet count [PLT], and spleen volume).\n  Results: We included 197 subjects, aged 54.9 $\\pm$ 13.8 years (mean $\\pm$ standard deviation), 111 males (56.3\\%): 35 healthy controls, 44 non-ACLD, and 118 ACLD patients. TVVR and HVVR were highest in controls (3.9; 2.1), intermediate in non-ACLD (2.8; 1.7), and lowest in ACLD patients (2.3; 1.0) ($p \\leq 0.001$). PVVR was reduced in both non-ACLD and ACLD patients (both 1.2) compared to controls (1.7) ($p \\leq 0.001$), but showed no difference between CLD groups ($p = 0.999$). HVVR significantly correlated indirectly with FIB-4, ALBI, MELD-Na, LSM, and spleen volume ($ρ$ ranging from -0.27 to -0.40), and directly with PLT ($ρ= 0.36$). TVVR and PVVR showed similar but weaker correlations.\n  Conclusions: Deep learning-based hepatic vessel volumetry demonstrated differences between healthy liver and chronic liver disease stages and shows correlations with established markers of disease severity.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "^Alexander Herold and Daniel Sobotka share first-authorship",
    "pdf_url": "https://arxiv.org/pdf/2510.08039v1",
    "published_date": "2025-10-09 10:23:16 UTC",
    "updated_date": "2025-10-09 10:23:16 UTC"
  },
  {
    "arxiv_id": "2510.08034v1",
    "title": "AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models",
    "authors": [
      "Xiaoshuang Ji",
      "Zhendong Zhao",
      "Xiaoyan Gu",
      "Xiaojun Chen",
      "Xin Zhao",
      "Zeyao Liu"
    ],
    "abstract": "Parameter-efficient finetuning (PEFT) aims to mitigate the substantial computational and memory overhead involved in adapting large-scale pretrained models to diverse downstream tasks. Among numerous PEFT strategies, Low-Rank Adaptation (LoRA) has emerged as one of the most widely adopted approaches due to its robust empirical performance and low implementation complexity. In practical deployment, LoRA is typically applied to the $W^Q$ and $W^V$ projection matrices of self-attention modules, enabling an effective trade-off between model performance and parameter efficiency. While LoRA has achieved considerable empirical success, it still encounters challenges such as suboptimal performance and slow convergence. To address these limitations, we introduce \\textbf{AILoRA}, a novel parameter-efficient method that incorporates function-aware asymmetric low-rank priors. Our empirical analysis reveals that the projection matrices $W^Q$ and $W^V$ in the self-attention mechanism exhibit distinct parameter characteristics, stemming from their functional differences. Specifically, $W^Q$ captures task-specific semantic space knowledge essential for attention distributions computation, making its parameters highly sensitive to downstream task variations. In contrast, $W^V$ encodes token-level feature representations that tend to remain stable across tasks and layers. Leveraging these insights, AILoRA performs a function-aware initialization by injecting the principal components of $W^Q$ to retain task-adaptive capacity, and the minor components of $W^V$ to preserve generalizable feature representations. This asymmetric initialization strategy enables LoRA modules to better capture the specialized roles of attention parameters, thereby enhancing both finetuning performance and convergence efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to AAAI2026",
    "pdf_url": "https://arxiv.org/pdf/2510.08034v1",
    "published_date": "2025-10-09 10:13:16 UTC",
    "updated_date": "2025-10-09 10:13:16 UTC"
  },
  {
    "arxiv_id": "2510.08026v2",
    "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
    "authors": [
      "Chen Huang",
      "Wei Lu",
      "Wenxuan Zhang"
    ],
    "abstract": "Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution. This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08026v2",
    "published_date": "2025-10-09 10:04:31 UTC",
    "updated_date": "2025-10-10 07:08:11 UTC"
  },
  {
    "arxiv_id": "2510.08022v1",
    "title": "FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset",
    "authors": [
      "Kehui Liu",
      "Zhongjie Jia",
      "Yang Li",
      "Zhaxizhuoma",
      "Pengan Chen",
      "Song Liu",
      "Xin Liu",
      "Pingrui Zhang",
      "Haoming Song",
      "Xinyi Ye",
      "Nieqing Cao",
      "Zhigang Wang",
      "Jia Zeng",
      "Dong Wang",
      "Yan Ding",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "abstract": "Data-driven robotic manipulation learning depends on large-scale, high-quality expert demonstration datasets. However, existing datasets, which primarily rely on human teleoperated robot collection, are limited in terms of scalability, trajectory smoothness, and applicability across different robotic embodiments in real-world environments. In this paper, we present FastUMI-100K, a large-scale UMI-style multimodal demonstration dataset, designed to overcome these limitations and meet the growing complexity of real-world manipulation tasks. Collected by FastUMI, a novel robotic system featuring a modular, hardware-decoupled mechanical design and an integrated lightweight tracking system, FastUMI-100K offers a more scalable, flexible, and adaptable solution to fulfill the diverse requirements of real-world robot demonstration data. Specifically, FastUMI-100K contains over 100K+ demonstration trajectories collected across representative household environments, covering 54 tasks and hundreds of object types. Our dataset integrates multimodal streams, including end-effector states, multi-view wrist-mounted fisheye images and textual annotations. Each trajectory has a length ranging from 120 to 500 frames. Experimental results demonstrate that FastUMI-100K enables high policy success rates across various baseline algorithms, confirming its robustness, adaptability, and real-world applicability for solving complex, dynamic manipulation challenges. The source code and dataset will be released in this link https://github.com/MrKeee/FastUMI-100K.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08022v1",
    "published_date": "2025-10-09 09:57:25 UTC",
    "updated_date": "2025-10-09 09:57:25 UTC"
  },
  {
    "arxiv_id": "2510.08016v1",
    "title": "Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses",
    "authors": [
      "Stanisław Pawlak",
      "Jan Dubiński",
      "Daniel Marczak",
      "Bartłomiej Twardowski"
    ],
    "abstract": "Model merging (MM) recently emerged as an effective method for combining large deep learning models. However, it poses significant security risks. Recent research shows that it is highly susceptible to backdoor attacks, which introduce a hidden trigger into a single fine-tuned model instance that allows the adversary to control the output of the final merged model at inference time. In this work, we propose a simple framework for understanding backdoor attacks by treating the attack itself as a task vector. $Backdoor\\ Vector\\ (BV)$ is calculated as the difference between the weights of a fine-tuned backdoored model and fine-tuned clean model. BVs reveal new insights into attacks understanding and a more effective framework to measure their similarity and transferability. Furthermore, we propose a novel method that enhances backdoor resilience through merging dubbed $Sparse\\ Backdoor\\ Vector\\ (SBV)$ that combines multiple attacks into a single one. We identify the core vulnerability behind backdoor threats in MM: $inherent\\ triggers$ that exploit adversarial weaknesses in the base model. To counter this, we propose $Injection\\ BV\\ Subtraction\\ (IBVS)$ - an assumption-free defense against backdoors in MM. Our results show that SBVs surpass prior attacks and is the first method to leverage merging to improve backdoor effectiveness. At the same time, IBVS provides a lightweight, general defense that remains effective even when the backdoor threat is entirely unknown.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 13 figures, 15 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08016v1",
    "published_date": "2025-10-09 09:54:05 UTC",
    "updated_date": "2025-10-09 09:54:05 UTC"
  },
  {
    "arxiv_id": "2510.08009v1",
    "title": "Language Models Do Not Embed Numbers Continuously",
    "authors": [
      "Alex O. Davies",
      "Roussel Nzoyem",
      "Nirav Ajmeri",
      "Telmo M. Silva Filho"
    ],
    "abstract": "Recent research has extensively studied how large language models manipulate integers in specific arithmetic tasks, and on a more fundamental level, how they represent numeric values. These previous works have found that language model embeddings can be used to reconstruct the original values, however, they do not evaluate whether language models actually model continuous values as continuous. Using expected properties of the embedding space, including linear reconstruction and principal component analysis, we show that language models not only represent numeric spaces as non-continuous but also introduce significant noise. Using models from three major providers (OpenAI, Google Gemini and Voyage AI), we show that while reconstruction is possible with high fidelity ($R^2 \\geq 0.95$), principal components only explain a minor share of variation within the embedding space. This indicates that many components within the embedding space are orthogonal to the simple numeric input space. Further, both linear reconstruction and explained variance suffer with increasing decimal precision, despite the ordinal nature of the input space being fundamentally unchanged. The findings of this work therefore have implications for the many areas where embedding models are used, in-particular where high numerical precision, large magnitudes or mixed-sign values are common.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 10 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08009v1",
    "published_date": "2025-10-09 09:46:19 UTC",
    "updated_date": "2025-10-09 09:46:19 UTC"
  },
  {
    "arxiv_id": "2510.09689v1",
    "title": "CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search",
    "authors": [
      "Haoran Ou",
      "Kangjie Chen",
      "Xingshuo Han",
      "Gelei Deng",
      "Jie Zhang",
      "Han Qiu",
      "Tianwei Zhang"
    ],
    "abstract": "Large Language Models (LLMs) excel at tasks such as dialogue, summarization, and question answering, yet they struggle to adapt to specialized domains and evolving facts. To overcome this, web search has been integrated into LLMs, allowing real-time access to online content. However, this connection magnifies safety risks, as adversarial prompts combined with untrusted sources can cause severe vulnerabilities. We investigate red teaming for LLMs with web search and present CREST-Search, a framework that systematically exposes risks in such systems. Unlike existing methods for standalone LLMs, CREST-Search addresses the complex workflow of search-enabled models by generating adversarial queries with in-context learning and refining them through iterative feedback. We further construct WebSearch-Harm, a search-specific dataset to fine-tune LLMs into efficient red-teaming agents. Experiments show that CREST-Search effectively bypasses safety filters and reveals vulnerabilities in modern web-augmented LLMs, underscoring the need for specialized defenses to ensure trustworthy deployment.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09689v1",
    "published_date": "2025-10-09 09:44:14 UTC",
    "updated_date": "2025-10-09 09:44:14 UTC"
  },
  {
    "arxiv_id": "2510.08005v1",
    "title": "Past, Present, and Future of Bug Tracking in the Generative AI Era",
    "authors": [
      "Utku Boran Torun",
      "Mehmet Taha Demircan",
      "Mahmut Furkan Gön",
      "Eray Tüzün"
    ],
    "abstract": "Traditional bug tracking systems rely heavily on manual reporting, reproduction, triaging, and resolution, each carried out by different stakeholders such as end users, customer support, developers, and testers. This division of responsibilities requires significant coordination and widens the communication gap between non-technical users and technical teams, slowing the process from bug discovery to resolution. Moreover, current systems are highly asynchronous; users often wait hours or days for a first response, delaying fixes and contributing to frustration. This paper examines the evolution of bug tracking, from early paper-based reporting to today's web-based and SaaS platforms. Building on this trajectory, we propose an AI-powered bug tracking framework that augments existing tools with intelligent, large language model (LLM)-driven automation. Our framework addresses two main challenges: reducing time-to-fix and minimizing human overhead. Users report issues in natural language, while AI agents refine reports, attempt reproduction, and request missing details. Reports are then classified, invalid ones resolved through no-code fixes, and valid ones localized and assigned to developers. LLMs also generate candidate patches, with human oversight ensuring correctness. By integrating automation into each phase, our framework accelerates response times, improves collaboration, and strengthens software maintenance practices for a more efficient, user-centric future.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Submitted to ACM TOSEM Special Issue: 2030 Software Engineering Roadmap",
    "pdf_url": "https://arxiv.org/pdf/2510.08005v1",
    "published_date": "2025-10-09 09:42:30 UTC",
    "updated_date": "2025-10-09 09:42:30 UTC"
  },
  {
    "arxiv_id": "2510.08002v1",
    "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks",
    "authors": [
      "Cheng Yang",
      "Xuemeng Yang",
      "Licheng Wen",
      "Daocheng Fu",
      "Jianbiao Mei",
      "Rong Wu",
      "Pinlong Cai",
      "Yufan Shen",
      "Nianchen Deng",
      "Botian Shi",
      "Yu Qiao",
      "Haifeng Li"
    ],
    "abstract": "Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08002v1",
    "published_date": "2025-10-09 09:40:34 UTC",
    "updated_date": "2025-10-09 09:40:34 UTC"
  },
  {
    "arxiv_id": "2510.07993v1",
    "title": "Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge",
    "authors": [
      "Watcharapong Timklaypachara",
      "Monrada Chiewhawan",
      "Nopporn Lekuthai",
      "Titipat Achakulvisut"
    ],
    "abstract": "Scientific figure captions require both accuracy and stylistic consistency to convey visual information. Here, we present a domain-specific caption generation system for the 3rd SciCap Challenge that integrates figure-related textual context with author-specific writing styles using the LaMP-Cap dataset. Our approach uses a two-stage pipeline: Stage 1 combines context filtering, category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption candidate selection; Stage 2 applies few-shot prompting with profile figures for stylistic refinement. Our experiments demonstrate that category-specific prompts outperform both zero-shot and general optimized approaches, improving ROUGE-1 recall by +8.3\\% while limiting precision loss to -2.8\\% and BLEU-4 reduction to -10.9\\%. Profile-informed stylistic refinement yields 40--48\\% gains in BLEU scores and 25--27\\% in ROUGE. Overall, our system demonstrates that combining contextual understanding with author-specific stylistic adaptation can generate captions that are both scientifically accurate and stylistically faithful to the source paper.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07993v1",
    "published_date": "2025-10-09 09:30:28 UTC",
    "updated_date": "2025-10-09 09:30:28 UTC"
  },
  {
    "arxiv_id": "2510.07988v1",
    "title": "ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation",
    "authors": [
      "Haitao Jia",
      "Ming He",
      "Zimo Yin",
      "Likang Wu",
      "Jianping Fan",
      "Jitao Sang"
    ],
    "abstract": "Mobile GUI agents exhibit substantial potential to facilitate and automate the execution of user tasks on mobile phones. However, exist mobile GUI agents predominantly privilege autonomous operation and neglect the necessity of active user engagement during task execution. This omission undermines their adaptability to information dilemmas including ambiguous, dynamically evolving, and conflicting task scenarios, leading to execution outcomes that deviate from genuine user requirements and preferences. To address these shortcomings, we propose ReInAgent, a context-aware multi-agent framework that leverages dynamic information management to enable human-in-the-loop mobile task navigation. ReInAgent integrates three specialized agents around a shared memory module: an information-managing agent for slot-based information management and proactive interaction with the user, a decision-making agent for conflict-aware planning, and a reflecting agent for task reflection and information consistency validation. Through continuous contextual information analysis and sustained user-agent collaboration, ReInAgent overcomes the limitation of existing approaches that rely on clear and static task assumptions. Consequently, it enables more adaptive and reliable mobile task navigation in complex, real-world scenarios. Experimental results demonstrate that ReInAgent effectively resolves information dilemmas and produces outcomes that are more closely aligned with genuine user preferences. Notably, on complex tasks involving information dilemmas, ReInAgent achieves a 25% higher success rate than Mobile-Agent-v2.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07988v1",
    "published_date": "2025-10-09 09:22:05 UTC",
    "updated_date": "2025-10-09 09:22:05 UTC"
  },
  {
    "arxiv_id": "2510.07985v2",
    "title": "Fewer Weights, More Problems: A Practical Attack on LLM Pruning",
    "authors": [
      "Kazuki Egashira",
      "Robin Staab",
      "Thibaud Gloaguen",
      "Mark Vero",
      "Martin Vechev"
    ],
    "abstract": "Model pruning, i.e., removing a subset of model weights, has become a prominent approach to reducing the memory footprint of large language models (LLMs) during inference. Notably, popular inference engines, such as vLLM, enable users to conveniently prune downloaded models before they are deployed. While the utility and efficiency of pruning methods have improved significantly, the security implications of pruning remain underexplored. In this work, for the first time, we show that modern LLM pruning methods can be maliciously exploited. In particular, an adversary can construct a model that appears benign yet, once pruned, exhibits malicious behaviors. Our method is based on the idea that the adversary can compute a proxy metric that estimates how likely each parameter is to be pruned. With this information, the adversary can first inject a malicious behavior into those parameters that are unlikely to be pruned. Then, they can repair the model by using parameters that are likely to be pruned, effectively canceling out the injected behavior in the unpruned model. We demonstrate the severity of our attack through extensive evaluation on five models; after any of the pruning in vLLM are applied (Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious behaviors in a diverse set of attack scenarios (success rates of up to $95.7\\%$ for jailbreak, $98.7\\%$ for benign instruction refusal, and $99.5\\%$ for targeted content injection). Our results reveal a critical deployment-time security gap and underscore the urgent need for stronger security awareness in model compression.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07985v2",
    "published_date": "2025-10-09 09:17:35 UTC",
    "updated_date": "2025-10-10 08:42:47 UTC"
  },
  {
    "arxiv_id": "2510.07984v1",
    "title": "Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN",
    "authors": [
      "Chandresh Sutariya",
      "Nitin Singh"
    ],
    "abstract": "The simultaneous restoration of high-frequency details and suppression of severe noise in low-light imagery presents a significant and persistent challenge in computer vision. While large-scale Transformer models like SwinIR have set the state of the art in performance, their high computational cost can be a barrier for practical applications. This paper investigates the critical trade-off between performance and efficiency by comparing the state-of-the-art SwinIR model against a standard, lightweight Convolutional Neural Network (CNN) on this challenging task. Our experimental results reveal a nuanced but important finding. While the Transformer-based SwinIR model achieves a higher peak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the lightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially, the CNN reached this performance after converging in only 10 epochs of training, whereas the more complex SwinIR model required 132 epochs. This efficiency is further underscored by the model's size; the CNN is over 55 times smaller than SwinIR. This work demonstrates that a standard CNN can provide a near state-of-the-art result with significantly lower computational overhead, presenting a compelling case for its use in real-world scenarios where resource constraints are a primary concern.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07984v1",
    "published_date": "2025-10-09 09:16:05 UTC",
    "updated_date": "2025-10-09 09:16:05 UTC"
  },
  {
    "arxiv_id": "2510.07983v1",
    "title": "ZeroCard: Cardinality Estimation with Zero Dependence on Target Databases -- No Data, No Query, No Retraining",
    "authors": [
      "Xianghong Xu",
      "Rong Kang",
      "Xiao He",
      "Lei Zhang",
      "Jianjun Chen",
      "Tieying Zhang"
    ],
    "abstract": "Cardinality estimation is a fundamental task in database systems and plays a critical role in query optimization. Despite significant advances in learning-based cardinality estimation methods, most existing approaches remain difficult to generalize to new datasets due to their strong dependence on raw data or queries, thus limiting their practicality in real scenarios. To overcome these challenges, we argue that semantics in the schema may benefit cardinality estimation, and leveraging such semantics may alleviate these dependencies. To this end, we introduce ZeroCard, the first semantics-driven cardinality estimation method that can be applied without any dependence on raw data access, query logs, or retraining on the target database. Specifically, we propose to predict data distributions using schema semantics, thereby avoiding raw data dependence. Then, we introduce a query template-agnostic representation method to alleviate query dependence. Finally, we construct a large-scale query dataset derived from real-world tables and pretrain ZeroCard on it, enabling it to learn cardinality from schema semantics and predicate representations. After pretraining, ZeroCard's parameters can be frozen and applied in an off-the-shelf manner. We conduct extensive experiments to demonstrate the distinct advantages of ZeroCard and show its practical applications in query optimization. Its zero-dependence property significantly facilitates deployment in real-world scenarios.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07983v1",
    "published_date": "2025-10-09 09:16:01 UTC",
    "updated_date": "2025-10-09 09:16:01 UTC"
  },
  {
    "arxiv_id": "2510.08656v1",
    "title": "A 3D Generation Framework from Cross Modality to Parameterized Primitive",
    "authors": [
      "Yiming Liang",
      "Huan Yu",
      "Zili Wang",
      "Shuyou Zhang",
      "Guodong Yi",
      "Jin Wang",
      "Jianrong Tan"
    ],
    "abstract": "Recent advancements in AI-driven 3D model generation have leveraged cross modality, yet generating models with smooth surfaces and minimizing storage overhead remain challenges. This paper introduces a novel multi-stage framework for generating 3D models composed of parameterized primitives, guided by textual and image inputs. In the framework, A model generation algorithm based on parameterized primitives, is proposed, which can identifies the shape features of the model constituent elements, and replace the elements with parameterized primitives with high quality surface. In addition, a corresponding model storage method is proposed, it can ensure the original surface quality of the model, while retaining only the parameters of parameterized primitives. Experiments on virtual scene dataset and real scene dataset demonstrate the effectiveness of our method, achieving a Chamfer Distance of 0.003092, a VIoU of 0.545, a F1-Score of 0.9139 and a NC of 0.8369, with primitive parameter files approximately 6KB in size. Our approach is particularly suitable for rapid prototyping of simple models.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08656v1",
    "published_date": "2025-10-09 09:15:33 UTC",
    "updated_date": "2025-10-09 09:15:33 UTC"
  },
  {
    "arxiv_id": "2510.07980v1",
    "title": "Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training",
    "authors": [
      "Qinglun Li",
      "Yingqi Liu",
      "Miao Zhang",
      "Xiaochun Cao",
      "Quanjun Yin",
      "Li Shen"
    ],
    "abstract": "Decentralized training removes the centralized server, making it a communication-efficient approach that can significantly improve training efficiency, but it often suffers from degraded performance compared to centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective bridge between decentralized and centralized training, significantly reducing experiment performance gaps. However, the theoretical reasons for its effectiveness and whether this gap can be fully eliminated by MGS remain open questions. In this paper, we derive upper bounds on the generalization error and excess error of MGS using stability analysis, systematically answering these two key questions. 1). Optimization Error Reduction: MGS reduces the optimization error bound at an exponential rate, thereby exponentially tightening the generalization error bound and enabling convergence to better solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a non-negligible gap in generalization error remains compared to centralized mini-batch SGD ($\\mathcal{O}(T^{\\frac{cβ}{cβ+1}}/{n m})$ in centralized and $\\mathcal{O}(T^{\\frac{2cβ}{2cβ+2}}/{n m^{\\frac{1}{2cβ+2}}})$ in decentralized). Furthermore, we provide the first unified analysis of how factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact the generalization of MGS under non-convex settings without the bounded gradients assumption, filling a critical theoretical gap in decentralized training. Finally, promising experiments on CIFAR datasets support our theoretical findings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by NeurIPS 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.07980v1",
    "published_date": "2025-10-09 09:14:47 UTC",
    "updated_date": "2025-10-09 09:14:47 UTC"
  },
  {
    "arxiv_id": "2510.07978v2",
    "title": "VoiceAgentBench: Are Voice Assistants ready for agentic tasks?",
    "authors": [
      "Dhruv Jain",
      "Harshit Shukla",
      "Gautam Rajeev",
      "Ashish Kulkarni",
      "Chandra Khatri",
      "Shubham Agarwal"
    ],
    "abstract": "Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07978v2",
    "published_date": "2025-10-09 09:11:38 UTC",
    "updated_date": "2025-11-05 07:44:45 UTC"
  },
  {
    "arxiv_id": "2510.07975v1",
    "title": "Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation",
    "authors": [
      "Mingyang Sun",
      "Jiude Wei",
      "Qichen He",
      "Donglin Wang",
      "Cewu Lu",
      "Jianhua Sun"
    ],
    "abstract": "Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this \"semantic-to-physical\" gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07975v1",
    "published_date": "2025-10-09 09:08:33 UTC",
    "updated_date": "2025-10-09 09:08:33 UTC"
  },
  {
    "arxiv_id": "2510.07974v2",
    "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning",
    "authors": [
      "Jialu Du",
      "Guiyang Hou",
      "Yihui Fu",
      "Chen Wu",
      "Wenqi Zhang",
      "Yongliang Shen",
      "Weiming Lu"
    ],
    "abstract": "While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like \"tricky\" and \"confused\" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07974v2",
    "published_date": "2025-10-09 09:07:31 UTC",
    "updated_date": "2025-10-11 05:57:45 UTC"
  },
  {
    "arxiv_id": "2510.08655v1",
    "title": "Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis",
    "authors": [
      "Premt Cara",
      "Kamilia Zaripova",
      "David Bani-Harouni",
      "Nassir Navab",
      "Azade Farshad"
    ],
    "abstract": "Rare genetic disease diagnosis faces critical challenges: insufficient patient data, inaccessible full genome sequencing, and the immense number of possible causative genes. These limitations cause prolonged diagnostic journeys, inappropriate treatments, and critical delays, disproportionately affecting patients in resource-limited settings where diagnostic tools are scarce. We propose RareNet, a subgraph-based Graph Neural Network that requires only patient phenotypes to identify the most likely causal gene and retrieve focused patient subgraphs for targeted clinical investigation. RareNet can function as a standalone method or serve as a pre-processing or post-processing filter for other candidate gene prioritization methods, consistently enhancing their performance while potentially enabling explainable insights. Through comprehensive evaluation on two biomedical datasets, we demonstrate competitive and robust causal gene prediction and significant performance gains when integrated with other frameworks. By requiring only phenotypic data, which is readily available in any clinical setting, RareNet democratizes access to sophisticated genetic analysis, offering particular value for underserved populations lacking advanced genomic infrastructure.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08655v1",
    "published_date": "2025-10-09 09:05:06 UTC",
    "updated_date": "2025-10-09 09:05:06 UTC"
  },
  {
    "arxiv_id": "2510.07972v1",
    "title": "TaoSR-SHE: Stepwise Hybrid Examination Reinforcement Learning Framework for E-commerce Search Relevance",
    "authors": [
      "Pengkun Jiao",
      "Yiming Jin",
      "Jianhui Yang",
      "Chenhe Dong",
      "Zerui Huang",
      "Shaowei Yao",
      "Xiaojiang Zhou",
      "Dan Ou",
      "Haihong Tang"
    ],
    "abstract": "Query-product relevance analysis is a foundational technology in e-commerce search engines and has become increasingly important in AI-driven e-commerce. The recent emergence of large language models (LLMs), particularly their chain-of-thought (CoT) reasoning capabilities, offers promising opportunities for developing relevance systems that are both more interpretable and more robust. However, existing training paradigms have notable limitations: SFT and DPO suffer from poor generalization on long-tail queries and from a lack of fine-grained, stepwise supervision to enforce rule-aligned reasoning. In contrast, reinforcement learning with verification rewards (RLVR) suffers from sparse feedback, which provides insufficient signal to correct erroneous intermediate steps, thereby undermining logical consistency and limiting performance in complex inference scenarios.\n  To address these challenges, we introduce the Stepwise Hybrid Examination Reinforcement Learning framework for Taobao Search Relevance (TaoSR-SHE). At its core is Stepwise Reward Policy Optimization (SRPO), a reinforcement learning algorithm that leverages step-level rewards generated by a hybrid of a high-quality generative stepwise reward model and a human-annotated offline verifier, prioritizing learning from critical correct and incorrect reasoning steps. TaoSR-SHE further incorporates two key techniques: diversified data filtering to encourage exploration across varied reasoning paths and mitigate policy entropy collapse, and multi-stage curriculum learning to foster progressive capability growth. Extensive experiments on real-world search benchmarks show that TaoSR-SHE improves both reasoning quality and relevance-prediction accuracy in large-scale e-commerce settings, outperforming SFT, DPO, GRPO, and other baselines, while also enhancing interpretability and robustness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07972v1",
    "published_date": "2025-10-09 09:03:15 UTC",
    "updated_date": "2025-10-09 09:03:15 UTC"
  },
  {
    "arxiv_id": "2510.07962v1",
    "title": "LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?",
    "authors": [
      "Jingyuan Wang",
      "Yankai Chen",
      "Zhonghang Li",
      "Chao Huang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07962v1",
    "published_date": "2025-10-09 08:55:12 UTC",
    "updated_date": "2025-10-09 08:55:12 UTC"
  },
  {
    "arxiv_id": "2510.07960v2",
    "title": "A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG",
    "authors": [
      "Emilio Estevan",
      "María Sierra-Torralba",
      "Eduardo López-Larraz",
      "Luis Montesano"
    ],
    "abstract": "Wearable EEG devices have emerged as a promising alternative to polysomnography (PSG). As affordable and scalable solutions, their widespread adoption results in the collection of massive volumes of unlabeled data that cannot be analyzed by clinicians at scale. Meanwhile, the recent success of deep learning for sleep scoring has relied on large annotated datasets. Self-supervised learning (SSL) offers an opportunity to bridge this gap, leveraging unlabeled signals to address label scarcity and reduce annotation effort. In this paper, we present the first systematic evaluation of SSL for sleep staging using wearable EEG. We investigate a range of well-established SSL methods and evaluate them on two sleep databases acquired with the Ikon Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and wearable EEG recordings with consensus labels, and HOGAR, a large collection of home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios are defined to study label efficiency, representation quality, and cross-dataset generalization. Results show that SSL consistently improves classification performance by up to 10% over supervised baselines, with gains particularly evident when labeled data is scarce. SSL achieves clinical-grade accuracy above 80% leveraging only 5% to 10% of labeled data, while the supervised approach requires twice the labels. Additionally, SSL representations prove robust to variations in population characteristics, recording environments, and signal quality. Our findings demonstrate the potential of SSL to enable label-efficient sleep staging with wearable EEG, reducing reliance on manual annotations and advancing the development of affordable sleep monitoring systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07960v2",
    "published_date": "2025-10-09 08:54:10 UTC",
    "updated_date": "2025-11-06 11:48:32 UTC"
  },
  {
    "arxiv_id": "2510.07959v1",
    "title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
    "authors": [
      "Alexander Rubinstein",
      "Benjamin Raible",
      "Martin Gubri",
      "Seong Joon Oh"
    ],
    "abstract": "Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that $\\textit{maximise diversity in model responses}$. Our method, $\\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. $\\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: https://github.com/arubique/disco-public.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07959v1",
    "published_date": "2025-10-09 08:53:59 UTC",
    "updated_date": "2025-10-09 08:53:59 UTC"
  },
  {
    "arxiv_id": "2510.07958v1",
    "title": "A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning",
    "authors": [
      "Fengji Zhang",
      "Xinyao Niu",
      "Chengyang Ying",
      "Guancheng Lin",
      "Zhongkai Hao",
      "Zhou Fan",
      "Chengen Huang",
      "Jacky Keung",
      "Bei Chen",
      "Junyang Lin"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\\mathrm{AnsF1}@1$ score of $48.4\\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07958v1",
    "published_date": "2025-10-09 08:53:31 UTC",
    "updated_date": "2025-10-09 08:53:31 UTC"
  },
  {
    "arxiv_id": "2510.07951v1",
    "title": "A Large-scale Dataset for Robust Complex Anime Scene Text Detection",
    "authors": [
      "Ziyi Dong",
      "Yurui Zhang",
      "Changmao Li",
      "Naomi Rue Golding",
      "Qing Long"
    ],
    "abstract": "Current text detection datasets primarily target natural or document scenes, where text typically appear in regular font and shapes, monotonous colors, and orderly layouts. The text usually arranged along straight or curved lines. However, these characteristics differ significantly from anime scenes, where text is often diverse in style, irregularly arranged, and easily confused with complex visual elements such as symbols and decorative patterns. Text in anime scene also includes a large number of handwritten and stylized fonts. Motivated by this gap, we introduce AnimeText, a large-scale dataset containing 735K images and 4.2M annotated text blocks. It features hierarchical annotations and hard negative samples tailored for anime scenarios. %Cross-dataset evaluations using state-of-the-art methods demonstrate that models trained on AnimeText achieve superior performance in anime text detection tasks compared to existing datasets. To evaluate the robustness of AnimeText in complex anime scenes, we conducted cross-dataset benchmarking using state-of-the-art text detection methods. Experimental results demonstrate that models trained on AnimeText outperform those trained on existing datasets in anime scene text detection tasks. AnimeText on HuggingFace: https://huggingface.co/datasets/deepghs/AnimeText",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07951v1",
    "published_date": "2025-10-09 08:47:52 UTC",
    "updated_date": "2025-10-09 08:47:52 UTC"
  },
  {
    "arxiv_id": "2510.07943v1",
    "title": "Agent-Based Genetic Algorithm for Crypto Trading Strategy Optimization",
    "authors": [
      "Qiushi Tian",
      "Churong Liang",
      "Kairan Hong",
      "Runnan Li"
    ],
    "abstract": "Cryptocurrency markets present formidable challenges for trading strategy optimization due to extreme volatility, non-stationary dynamics, and complex microstructure patterns that render conventional parameter optimization methods fundamentally inadequate. We introduce Cypto Genetic Algorithm Agent (CGA-Agent), a pioneering hybrid framework that synergistically integrates genetic algorithms with intelligent multi-agent coordination mechanisms for adaptive trading strategy parameter optimization in dynamic financial environments. The framework uniquely incorporates real-time market microstructure intelligence and adaptive strategy performance feedback through intelligent mechanisms that dynamically guide evolutionary processes, transcending the limitations of static optimization approaches. Comprehensive empirical evaluation across three cryptocurrencies demonstrates systematic and statistically significant performance improvements on both total returns and risk-adjusted metrics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07943v1",
    "published_date": "2025-10-09 08:41:42 UTC",
    "updated_date": "2025-10-09 08:41:42 UTC"
  },
  {
    "arxiv_id": "2510.07940v1",
    "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation",
    "authors": [
      "Leigang Qu",
      "Ziyang Wang",
      "Na Zheng",
      "Wenjie Wang",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ],
    "abstract": "Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ttom-t2v.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.07940v1",
    "published_date": "2025-10-09 08:37:00 UTC",
    "updated_date": "2025-10-09 08:37:00 UTC"
  },
  {
    "arxiv_id": "2510.07925v1",
    "title": "Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles",
    "authors": [
      "Rebecca Westhäußer",
      "Wolfgang Minker",
      "Sebatian Zepf"
    ],
    "abstract": "Large language models (LLMs) increasingly serve as the central control unit of AI agents, yet current approaches remain limited in their ability to deliver personalized interactions. While Retrieval Augmented Generation enhances LLM capabilities by improving context-awareness, it lacks mechanisms to combine contextual information with user-specific data. Although personalization has been studied in fields such as human-computer interaction or cognitive science, existing perspectives largely remain conceptual, with limited focus on technical implementation. To address these gaps, we build on a unified definition of personalization as a conceptual foundation to derive technical requirements for adaptive, user-centered LLM-based agents. Combined with established agentic AI patterns such as multi-agent collaboration or multi-source retrieval, we present a framework that integrates persistent memory, dynamic coordination, self-validation, and evolving user profiles to enable personalized long-term interactions. We evaluate our approach on three public datasets using metrics such as retrieval accuracy, response correctness, or BertScore. We complement these results with a five-day pilot user study providing initial insights into user feedback on perceived personalization. The study provides early indications that guide future work and highlights the potential of integrating persistent memory and user profiles to improve the adaptivity and perceived personalization of LLM-based agents.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 1 figure, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2510.07925v1",
    "published_date": "2025-10-09 08:22:16 UTC",
    "updated_date": "2025-10-09 08:22:16 UTC"
  },
  {
    "arxiv_id": "2510.07923v1",
    "title": "STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models",
    "authors": [
      "Kyumin Lee",
      "Minjin Jeon",
      "Sanghwan Jang",
      "Hwanjo Yu"
    ],
    "abstract": "Answering complex real-world questions requires step-by-step retrieval and integration of relevant information to generate well-grounded responses. However, existing knowledge distillation methods overlook the need for different reasoning abilities at different steps, hindering transfer in multi-step retrieval-augmented frameworks. To address this, we propose Stepwise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models (StepER). StepER employs step-wise supervision to align with evolving information and reasoning demands across stages. Additionally, it incorporates difficulty-aware training to progressively optimize learning by prioritizing suitable steps. Our method is adaptable to various multi-step retrieval-augmented language models, including those that use retrieval queries for reasoning paths or decomposed questions. Extensive experiments show that StepER outperforms prior methods on multi-hop QA benchmarks, with an 8B model achieving performance comparable to a 70B teacher model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2510.07923v1",
    "published_date": "2025-10-09 08:20:27 UTC",
    "updated_date": "2025-10-09 08:20:27 UTC"
  },
  {
    "arxiv_id": "2510.07920v1",
    "title": "Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents",
    "authors": [
      "Xiangyu Li",
      "Yawen Zeng",
      "Xiaofen Xing",
      "Jin Xu",
      "Xiangmin Xu"
    ],
    "abstract": "LLM-based financial agents have attracted widespread excitement for their ability to trade like human experts. However, most systems exhibit a \"profit mirage\": dazzling back-tested returns evaporate once the model's knowledge window ends, because of the inherent information leakage in LLMs. In this paper, we systematically quantify this leakage issue across four dimensions and release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to mitigate this issue, we introduce FactFin, a framework that applies counterfactual perturbations to compel LLM-based agents to learn causal drivers instead of memorized outcomes. FactFin integrates four core components: Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree Search, and Counterfactual Simulator. Extensive experiments show that our method surpasses all baselines in out-of-sample generalization, delivering superior risk-adjusted performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07920v1",
    "published_date": "2025-10-09 08:13:35 UTC",
    "updated_date": "2025-10-09 08:13:35 UTC"
  },
  {
    "arxiv_id": "2510.07912v1",
    "title": "Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation",
    "authors": [
      "Fanwei Zhua",
      "Jiaxuan He",
      "Xiaoxiao Chen",
      "Zulong Chen",
      "Quan Lu",
      "Chenrui Mei"
    ],
    "abstract": "Automatic grading of subjective questions remains a significant challenge in examination assessment due to the diversity in question formats and the open-ended nature of student responses. Existing works primarily focus on a specific type of subjective question and lack the generality to support comprehensive exams that contain diverse question types. In this paper, we propose a unified Large Language Model (LLM)-enhanced auto-grading framework that provides human-like evaluation for all types of subjective questions across various domains. Our framework integrates four complementary modules to holistically evaluate student answers. In addition to a basic text matching module that provides a foundational assessment of content similarity, we leverage the powerful reasoning and generative capabilities of LLMs to: (1) compare key knowledge points extracted from both student and reference answers, (2) generate a pseudo-question from the student answer to assess its relevance to the original question, and (3) simulate human evaluation by identifying content-related and non-content strengths and weaknesses. Extensive experiments on both general-purpose and domain-specific datasets show that our framework consistently outperforms traditional and LLM-based baselines across multiple grading metrics. Moreover, the proposed system has been successfully deployed in real-world training and certification exams at a major e-commerce enterprise.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07912v1",
    "published_date": "2025-10-09 08:05:39 UTC",
    "updated_date": "2025-10-09 08:05:39 UTC"
  },
  {
    "arxiv_id": "2510.07910v1",
    "title": "MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation",
    "authors": [
      "Chongmyung Kwon",
      "Yujin Kim",
      "Seoeun Park",
      "Yunji Lee",
      "Charmgil Hong"
    ],
    "abstract": "Drug recommendation is an essential task in machine learning-based clinical decision support systems. However, the risk of drug-drug interactions (DDI) between co-prescribed medications remains a significant challenge. Previous studies have used graph neural networks (GNNs) to represent drug structures. Regardless, their simplified discrete forms cannot fully capture the molecular binding affinity and reactivity. Therefore, we propose Multimodal DDI Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a novel framework that integrates three-dimensional (3D) quantum-chemical information into drug representation learning. It generates 3D electron density maps using the ELF. To capture both therapeutic relevance and interaction risks, MMM combines ELF-derived features that encode global electronic properties with a bipartite graph encoder that models local substructure interactions. This design enables learning complementary characteristics of drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442 substructures), comparing it with several baseline models. In particular, a comparison with the GNN-based SafeDrug model demonstrates statistically significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112), and the DDI rate (p = 0.0386). These results demonstrate the potential of ELF-based 3D representations to enhance prediction accuracy and support safer combinatorial drug prescribing in clinical practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Medical Image Computing and Computer-Assisted Intervention (MICCAI) Predictive Intelligence in Medicine Workshop (MICCAI PRIME) 2025; 13 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.07910v1",
    "published_date": "2025-10-09 08:03:14 UTC",
    "updated_date": "2025-10-09 08:03:14 UTC"
  },
  {
    "arxiv_id": "2510.07889v1",
    "title": "Towards Meaningful Transparency in Civic AI Systems",
    "authors": [
      "Dave Murray-Rust",
      "Kars Alfrink",
      "Cristina Zaga"
    ],
    "abstract": "Artificial intelligence has become a part of the provision of governmental services, from making decisions about benefits to issuing fines for parking violations. However, AI systems rarely live up to the promise of neutral optimisation, creating biased or incorrect outputs and reducing the agency of both citizens and civic workers to shape the way decisions are made. Transparency is a principle that can both help subjects understand decisions made about them and shape the processes behind those decisions. However, transparency as practiced around AI systems tends to focus on the production of technical objects that represent algorithmic aspects of decision making. These are often difficult for publics to understand, do not connect to potential for action, and do not give insight into the wider socio-material context of decision making. In this paper, we build on existing approaches that take a human-centric view on AI transparency, combined with a socio-technical systems view, to develop the concept of meaningful transparency for civic AI systems: transparencies that allow publics to engage with AI systems that affect their lives, connecting understanding with potential for action.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07889v1",
    "published_date": "2025-10-09 07:43:01 UTC",
    "updated_date": "2025-10-09 07:43:01 UTC"
  },
  {
    "arxiv_id": "2510.07884v1",
    "title": "Contrastive Weak-to-strong Generalization",
    "authors": [
      "Houcheng Jiang",
      "Junfeng Fang",
      "Jiaxin Wu",
      "Tianyu Zhang",
      "Chen Gao",
      "Yong Li",
      "Xiang Wang",
      "Xiangnan He",
      "Yang Deng"
    ],
    "abstract": "Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07884v1",
    "published_date": "2025-10-09 07:37:23 UTC",
    "updated_date": "2025-10-09 07:37:23 UTC"
  },
  {
    "arxiv_id": "2510.09687v1",
    "title": "On the Occurence of Critical Learning Periods in Neural Networks",
    "authors": [
      "Stanisław Pawlak"
    ],
    "abstract": "This study delves into the plasticity of neural networks, offering empirical support for the notion that critical learning periods and warm-starting performance loss can be avoided through simple adjustments to learning hyperparameters. The critical learning phenomenon emerges when training is initiated with deficit data. Subsequently, after numerous deficit epochs, the network's plasticity wanes, impeding its capacity to achieve parity in accuracy with models trained from scratch, even when extensive clean data training follows deficit epochs. Building upon seminal research introducing critical learning periods, we replicate key findings and broaden the experimental scope of the main experiment from the original work. In addition, we consider a warm-starting approach and show that it can be seen as a form of deficit pretraining. In particular, we demonstrate that these problems can be averted by employing a cyclic learning rate schedule. Our findings not only impact neural network training practices but also establish a vital link between critical learning periods and ongoing research on warm-starting neural network training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09687v1",
    "published_date": "2025-10-09 07:34:06 UTC",
    "updated_date": "2025-10-09 07:34:06 UTC"
  },
  {
    "arxiv_id": "2510.07871v3",
    "title": "Learning to Navigate Socially Through Proactive Risk Perception",
    "authors": [
      "Erjia Xiao",
      "Lingfeng Zhang",
      "Yingbo Tang",
      "Hao Cheng",
      "Renjing Xu",
      "Wenbo Ding",
      "Lei Zhou",
      "Long Chen",
      "Hangjun Ye",
      "Xiaoshuai Hao"
    ],
    "abstract": "In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07871v3",
    "published_date": "2025-10-09 07:22:12 UTC",
    "updated_date": "2025-11-07 07:41:15 UTC"
  },
  {
    "arxiv_id": "2510.07865v1",
    "title": "DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation",
    "authors": [
      "Guowei Zou",
      "Haitao Wang",
      "Hejun Wu",
      "Yukun Qian",
      "Yuhang Wang",
      "Weibing Li"
    ],
    "abstract": "The ability to learn multi-modal action distributions is indispensable for robotic manipulation policies to perform precise and robust control. Flow-based generative models have recently emerged as a promising solution to learning distributions of actions, offering one-step action generation and thus achieving much higher sampling efficiency compared to diffusion-based methods. However, existing flow-based policies suffer from representation collapse, the inability to distinguish similar visual representations, leading to failures in precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive Regularization for One-Step Robotic Manipulation), a novel flow matching framework that integrates dispersive regularization into MeanFlow to prevent collapse while maintaining one-step efficiency. DM1 employs multiple dispersive regularization variants across different intermediate embedding layers, encouraging diverse representations across training batches without introducing additional network modules or specialized training procedures. Experiments on RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the Lift task reaching 99% success over 85% of the baseline. Real-robot deployment on a Franka Panda further validates that DM1 transfers effectively from simulation to the physical world. To the best of our knowledge, this is the first work to leverage representation regularization to enable flow-based policies to achieve strong performance in robotic manipulation, establishing a simple yet powerful approach for efficient and robust manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Website with code: https://guowei-zou.github.io/dm1/",
    "pdf_url": "https://arxiv.org/pdf/2510.07865v1",
    "published_date": "2025-10-09 07:12:20 UTC",
    "updated_date": "2025-10-09 07:12:20 UTC"
  },
  {
    "arxiv_id": "2510.07861v1",
    "title": "Understanding DeepResearch via Reports",
    "authors": [
      "Tianyu Fan",
      "Xinyao Niu",
      "Yuxiang Zheng",
      "Fengji Zhang",
      "Chengen Huang",
      "Bei Chen",
      "Junyang Lin",
      "Chao Huang"
    ],
    "abstract": "DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisticated reasoning and multi-tool integration. However, evaluating these systems remains critically challenging due to open-ended research scenarios and existing benchmarks that focus on isolated capabilities rather than holistic performance. Unlike traditional LLM tasks, DeepResearch systems must synthesize diverse sources, generate insights, and present coherent findings, which are capabilities that resist simple verification. To address this gap, we introduce DeepResearch-ReportEval, a comprehensive framework designed to assess DeepResearch systems through their most representative outputs: research reports. Our approach systematically measures three dimensions: quality, redundancy, and factuality, using an innovative LLM-as-a-Judge methodology achieving strong expert concordance. We contribute a standardized benchmark of 100 curated queries spanning 12 real-world categories, enabling systematic capability comparison. Our evaluation of four leading commercial systems reveals distinct design philosophies and performance trade-offs, establishing foundational insights as DeepResearch evolves from information assistants toward intelligent research partners. Source code and data are available at: https://github.com/HKUDS/DeepResearch-Eval.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07861v1",
    "published_date": "2025-10-09 07:03:43 UTC",
    "updated_date": "2025-10-09 07:03:43 UTC"
  },
  {
    "arxiv_id": "2510.07858v2",
    "title": "Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models",
    "authors": [
      "Zhiqing Cui",
      "Binwu Wang",
      "Qingxiang Liu",
      "Yeqiang Wang",
      "Zhengyang Zhou",
      "Yuxuan Liang",
      "Yang Wang"
    ],
    "abstract": "Large language models (LLM) have emerged as a promising avenue for time series forecasting, offering the potential to integrate multimodal data. However, existing LLM-based approaches face notable limitations-such as marginalized role in model architectures, reliance on coarse statistical text prompts, and lack of interpretability. In this work, we introduce Augur, a fully LLM driven time series forecasting framework that exploits LLM causal reasoning to discover and use directed causal associations among covariates. Augur uses a two stage teacher student architecture where a powerful teacher LLM infers a directed causal graph from time series using heuristic search together with pairwise causality testing. A lightweight student agent then refines the graph and fine tune on high confidence causal associations that are encoded as rich textual prompts to perform forecasting. This design improves predictive accuracy while yielding transparent, traceable reasoning about variable interactions. Extensive experiments on real-world datasets with 26 baselines demonstrate that Augur achieves competitive performance and robust zero-shot generalization.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07858v2",
    "published_date": "2025-10-09 06:59:15 UTC",
    "updated_date": "2025-11-26 11:39:28 UTC"
  },
  {
    "arxiv_id": "2510.07853v1",
    "title": "Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials",
    "authors": [
      "Thomas Lautenschlager",
      "Nils Friederich",
      "Angelo Jovin Yamachui Sitcheu",
      "Katja Nau",
      "Gaëlle Hayot",
      "Thomas Dickmeis",
      "Ralf Mikut"
    ],
    "abstract": "High-throughput toxicity testing offers a fast and cost-effective way to test large amounts of compounds. A key component for such systems is the automated evaluation via machine learning models. In this paper, we address critical challenges in this domain and demonstrate how representations learned via self-supervised learning can effectively identify toxicant-induced changes. We provide a proof-of-concept that utilizes the publicly available EmbryoNet dataset, which contains ten zebrafish embryo phenotypes elicited by various chemical compounds targeting different processes in early embryonic development. Our analysis shows that the learned representations using self-supervised learning are suitable for effectively distinguishing between the modes-of-action of different compounds. Finally, we discuss the integration of machine learning models in a physical toxicity testing device in the context of the TOXBOX project.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07853v1",
    "published_date": "2025-10-09 06:51:12 UTC",
    "updated_date": "2025-10-09 06:51:12 UTC"
  },
  {
    "arxiv_id": "2510.07852v2",
    "title": "FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning",
    "authors": [
      "Shuangyan Deng",
      "Haizhou Peng",
      "Jiachen Xu",
      "Rui Mao",
      "Ciprian Doru Giurcăneanu",
      "Jiamou Liu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have made substantial progress in recent years. However, their rigorous evaluation within specialized domains like finance is hindered by the absence of datasets characterized by professional-level knowledge intensity, detailed annotations, and advanced reasoning complexity. To address this critical gap, we introduce FinMR, a high-quality, knowledge-intensive multimodal dataset explicitly designed to evaluate expert-level financial reasoning capabilities at a professional analyst's standard. FinMR comprises over 3,200 meticulously curated and expertly annotated question-answer pairs across 15 diverse financial topics, ensuring broad domain diversity and integrating sophisticated mathematical reasoning, advanced financial knowledge, and nuanced visual interpretation tasks across multiple image types. Through comprehensive benchmarking with leading closed-source and open-source MLLMs, we highlight significant performance disparities between these models and professional financial analysts, uncovering key areas for model advancement, such as precise image analysis, accurate application of complex financial formulas, and deeper contextual financial understanding. By providing richly varied visual content and thorough explanatory annotations, FinMR establishes itself as an essential benchmark tool for assessing and advancing multimodal financial reasoning toward professional analyst-level competence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The methodology section contains inaccuracies that may lead to misleading interpretations. The authors have withdrawn this version for correction",
    "pdf_url": "https://arxiv.org/pdf/2510.07852v2",
    "published_date": "2025-10-09 06:49:55 UTC",
    "updated_date": "2025-11-22 04:54:04 UTC"
  },
  {
    "arxiv_id": "2510.08649v2",
    "title": "Formalizing Style in Personal Narratives",
    "authors": [
      "Gustave Cortal",
      "Alain Finkel"
    ],
    "abstract": "Personal narratives are stories authors construct to make meaning of their experiences. Style, the distinctive way authors use language to express themselves, is fundamental to how these narratives convey subjective experiences. Yet there is a lack of a formal framework for systematically analyzing these stylistic choices. We present a novel approach that formalizes style in personal narratives as patterns in the linguistic choices authors make when communicating subjective experiences. Our framework integrates three domains: functional linguistics establishes language as a system of meaningful choices, computer science provides methods for automatically extracting and analyzing sequential patterns, and these patterns are linked to psychological observations. Using language models, we automatically extract linguistic features such as processes, participants, and circumstances. We apply our framework to hundreds of dream narratives, including a case study on a war veteran with post-traumatic stress disorder. Analysis of his narratives uncovers distinctive patterns, particularly how verbal processes dominate over mental ones, illustrating the relationship between linguistic choices and psychological states.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08649v2",
    "published_date": "2025-10-09 06:48:06 UTC",
    "updated_date": "2025-10-13 11:58:35 UTC"
  },
  {
    "arxiv_id": "2510.07847v1",
    "title": "Meta-Learning Based Few-Shot Graph-Level Anomaly Detection",
    "authors": [
      "Liting Li",
      "Yumeng Wang",
      "Yueheng Sun"
    ],
    "abstract": "Graph-level anomaly detection aims to identify anomalous graphs or subgraphs within graph datasets, playing a vital role in various fields such as fraud detection, review classification, and biochemistry. While Graph Neural Networks (GNNs) have made significant progress in this domain, existing methods rely heavily on large amounts of labeled data, which is often unavailable in real-world scenarios. Additionally, few-shot anomaly detection methods based on GNNs are prone to noise interference, resulting in poor embedding quality and reduced model robustness. To address these challenges, we propose a novel meta-learning-based graph-level anomaly detection framework (MA-GAD), incorporating a graph compression module that reduces the graph size, mitigating noise interference while retaining essential node information. We also leverage meta-learning to extract meta-anomaly information from similar networks, enabling the learning of an initialization model that can rapidly adapt to new tasks with limited samples. This improves the anomaly detection performance on target graphs, and a bias network is used to enhance the distinction between anomalous and normal nodes. Our experimental results, based on four real-world biochemical datasets, demonstrate that MA-GAD outperforms existing state-of-the-art methods in graph-level anomaly detection under few-shot conditions. Experiments on both graph anomaly and subgraph anomaly detection tasks validate the framework's effectiveness on real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ARRML2025",
    "pdf_url": "https://arxiv.org/pdf/2510.07847v1",
    "published_date": "2025-10-09 06:45:07 UTC",
    "updated_date": "2025-10-09 06:45:07 UTC"
  },
  {
    "arxiv_id": "2510.08648v1",
    "title": "Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity",
    "authors": [
      "Edward Y. Chang",
      "Ethan Y. Chang"
    ],
    "abstract": "Large language models can change answers under harmless edits that matter in practice: RAG outputs flip when passages are reordered, fine-tuning erodes invariances learned at pretraining, debate or chain-of-thought prompts take path-dependent routes, and compiler fusion or reordering perturbs logits near decision boundaries. These failures violate intended invariances, break continuous integration, and force teams to trade safety for speed. The effects are small yet distributed across layers and positions, sensitive to context length and evaluation order, and costly to repair with retraining or formal verification. We present WILSON, a minimal post-hoc diagnostic suite that converts simple loop and reordering checks on internal representations into system signals. WILSON combines an inverse-free curvature map over positions and layers, computed with JVPs and Hutchinson probes, with activation-level commutators that flag reorder risk. Signals are cheap to compute, model-agnostic for standard Transformers, and exported as thresholds and CSV artifacts for orchestrators. This enables concrete actions: guard RAG against order effects, catch fine-tuning regressions, stabilize debate pathways and long multi-turn contexts, and gate fusions or reorders in deployment. In short, WILSON helps anticipate failures and approve safe optimizations so reliability and throughput can improve together without changing model architecture or training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 10 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.08648v1",
    "published_date": "2025-10-09 06:41:18 UTC",
    "updated_date": "2025-10-09 06:41:18 UTC"
  },
  {
    "arxiv_id": "2510.07842v1",
    "title": "AdaSwitch: Adaptive Switching Generation for Knowledge Distillation",
    "authors": [
      "Jingyu Peng",
      "Maolin Wang",
      "Hengyi Cai",
      "Yuchen Li",
      "Kai Zhang",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xiangyu Zhao"
    ],
    "abstract": "Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07842v1",
    "published_date": "2025-10-09 06:38:37 UTC",
    "updated_date": "2025-10-09 06:38:37 UTC"
  },
  {
    "arxiv_id": "2510.07841v1",
    "title": "Self-Improving LLM Agents at Test-Time",
    "authors": [
      "Emre Can Acikgoz",
      "Cheng Qian",
      "Heng Ji",
      "Dilek Hakkani-Tür",
      "Gokhan Tur"
    ],
    "abstract": "One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07841v1",
    "published_date": "2025-10-09 06:37:35 UTC",
    "updated_date": "2025-10-09 06:37:35 UTC"
  },
  {
    "arxiv_id": "2510.08647v1",
    "title": "Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression",
    "authors": [
      "Chengzhengxu Li",
      "Xiaoming Liu",
      "Zhaohan Zhang",
      "Shaochu Zhang",
      "Shengchao Liu",
      "Guoxin Ma",
      "Yu Lan",
      "Chao Shen"
    ],
    "abstract": "Recent developments have enabled advanced reasoning in Large Language Models (LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high computational costs and significant latency losses owing to the autoregressive nature of generative LLMs. CoT compression aims to improve efficiency in the reasoning process by reducing output length. Previous works trade reasoning efficiency by either laborious discrete prompt designing or the construction of external compressed CoT datasets that sacrifice key reasoning details. In this work, we propose Upfront CoT (UCoT): an efficient reasoning framework with upfront thought embedding to automate CoT compression. UCoT is a cooperative workflow involving a small model (compressor) and a large model (executor). The first stage of UCoT trains compressor to generate upfront thought embeddings rich in reasoning information for the executor, avoiding the drawbacks of manually designed prompts. The second stage optimizes executor to utilize upfront thought embeddings to derive the correct answer with short reasoning, using a reward mechanism. Extensive experiments show that UCoT maintains the powerful reasoning ability of executor while significantly reducing the length of CoT. It is worth mentioning that when applying UCoT to the Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by 50\\%, while the performance is 3.08\\% higher than that of the state-of-the-art (SOTA) method. The code and dataset are in supplementary material.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL2026 Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.08647v1",
    "published_date": "2025-10-09 06:34:31 UTC",
    "updated_date": "2025-10-09 06:34:31 UTC"
  },
  {
    "arxiv_id": "2510.07835v1",
    "title": "MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation",
    "authors": [
      "Weisen Jiang",
      "Sinno Jialin Pan"
    ],
    "abstract": "This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted By NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.07835v1",
    "published_date": "2025-10-09 06:27:34 UTC",
    "updated_date": "2025-10-09 06:27:34 UTC"
  },
  {
    "arxiv_id": "2510.07829v1",
    "title": "The Rise of the Knowledge Sculptor: A New Archetype for Knowledge Work in the Age of Generative AI",
    "authors": [
      "Cathal Doyle"
    ],
    "abstract": "In the Generative Age, the nature of knowledge work is transforming. Traditional models that emphasise the organisation and retrieval of pre-existing information are increasingly inadequate in the face of generative AI (GenAI) systems capable of autonomous content creation. This paper introduces the Knowledge Sculptor (KS), a new professional archetype for Human-GenAI collaboration that transforms raw AI output into trustworthy, actionable knowledge. Grounded in a socio-technical perspective, the KS is conceptualised through a framework of competencies, including architecting a vision, iterative dialogue, information sculpting, and curiosity-driven synthesis. A practice-based vignette illustrates the KS role in action, and in a self-referential approach, the paper itself serves as an artefact of the sculpting process it describes.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "23 pages, 11 figures, preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.07829v1",
    "published_date": "2025-10-09 06:19:17 UTC",
    "updated_date": "2025-10-09 06:19:17 UTC"
  },
  {
    "arxiv_id": "2510.07825v1",
    "title": "An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation",
    "authors": [
      "Yuping Zhou",
      "Siqi Lai",
      "Jindong Han",
      "Hao Liu"
    ],
    "abstract": "The rise of Internet of Vehicles (IoV) technologies is transforming traffic management from isolated control to a collective, multi-vehicle process. At the heart of this shift is multi-vehicle dynamic navigation, which requires simultaneously routing large fleets under evolving traffic conditions. Existing path search algorithms and reinforcement learning methods struggle to scale to city-wide networks, often failing to capture the nonlinear, stochastic, and coupled dynamics of urban traffic. To address these challenges, we propose CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle navigation. CityNav integrates a global traffic allocation agent, which coordinates strategic traffic flow distribution across regions, with local navigation agents that generate locally adaptive routes aligned with global directives. To enable effective cooperation, we introduce a cooperative reasoning optimization mechanism, in which agents are jointly trained with a dual-reward structure: individual rewards promote per-vehicle efficiency, while shared rewards encourage network-wide coordination and congestion reduction. Extensive experiments on four real-world road networks of varying scales (up to 1.6 million roads and 430,000 intersections) and traffic datasets demonstrate that CityNav consistently outperforms nine classical path search and RL-based baselines in city-scale travel efficiency and congestion mitigation. Our results highlight the potential of LLMs to enable scalable, adaptive, and cooperative city-wide traffic navigation, providing a foundation for intelligent, large-scale vehicle routing in complex urban environments. Our project is available at https://github.com/usail-hkust/CityNav.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07825v1",
    "published_date": "2025-10-09 06:14:29 UTC",
    "updated_date": "2025-10-09 06:14:29 UTC"
  },
  {
    "arxiv_id": "2510.07822v1",
    "title": "SIMU: Selective Influence Machine Unlearning",
    "authors": [
      "Anu Agarwal",
      "Mihir Pamnani",
      "Dilek Hakkani-Tur"
    ],
    "abstract": "The undesired memorization of sensitive information by Large Language Models (LLMs) has emphasized the need for safety mechanisms that can regulate model behavior. This has led to the development of machine unlearning techniques that enable models to precisely forget sensitive and unwanted information. For machine unlearning, first-order and second-order optimizer-based methods have shown significant progress in enabling LLMs to forget targeted information. However, in doing so, these approaches often compromise the model's original capabilities, resulting in unlearned models that struggle to retain their prior knowledge and overall utility. To address this, we propose Selective Influence Machine Unlearning (SIMU), a two-step framework that enhances second-order optimizer-based unlearning by selectively updating only the critical neurons responsible for encoding the forget-set. By constraining updates to these targeted neurons, SIMU achieves comparable unlearning efficacy while substantially outperforming current methods in retaining the model's original knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025 Workshop: Constrained Optimization for Machine Learning (COML)",
    "pdf_url": "https://arxiv.org/pdf/2510.07822v1",
    "published_date": "2025-10-09 06:03:15 UTC",
    "updated_date": "2025-10-09 06:03:15 UTC"
  },
  {
    "arxiv_id": "2510.08646v1",
    "title": "Energy-Driven Steering: Reducing False Refusals in Large Language Models",
    "authors": [
      "Eric Hanchen Jiang",
      "Weixuan Ou",
      "Run Liu",
      "Shengyuan Pang",
      "Guancheng Wan",
      "Ranjie Duan",
      "Wei Dong",
      "Kai-Wei Chang",
      "XiaoFeng Wang",
      "Ying Nian Wu",
      "Xinfeng Li"
    ],
    "abstract": "Safety alignment of large language models (LLMs) faces a key challenge: current alignment techniques often only focus on improving safety against harmful prompts, causing LLMs to become over-cautious and refuse to respond to benign prompts. Therefore, a key objective of safe alignment is to enhance safety while simultaneously reducing false refusals. In this paper, we introduce Energy-Driven Steering (EDS), a novel, fine-tuning free framework designed to resolve this challenge through dynamic, inference-time intervention. We trained a lightweight, external Energy-Based Model (EBM) to assign high energy to undesirable (false refusal or jailbreak) states and low energy to desirable (helpful response or safe reject) ones. During inference, EBM maps the LLM's internal activations to an \"energy landscape\". We use the gradient of the energy function to dynamically steer the LLM's hidden states to low energy regions, correcting the model to generate a desirable response in real-time without modifying its weights. This method decouples behavioral control from the model's core knowledge, offering a flexible solution with minimal computational overhead. Extensive experiments across a wide range of models show our method successfully achieves this objective: it substantially lowers false refusal rates. For example, raising compliance on the ORB-H benchmark from 57.3% to 82.6% while maintaining the baseline safety performance. Our work presents an effective paradigm for building LLMs that achieve both low false refusal rates and high safety.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08646v1",
    "published_date": "2025-10-09 06:01:41 UTC",
    "updated_date": "2025-10-09 06:01:41 UTC"
  },
  {
    "arxiv_id": "2510.09686v1",
    "title": "Stop DDoS Attacking the Research Community with AI-Generated Survey Papers",
    "authors": [
      "Jianghao Lin",
      "Rong Shan",
      "Jiachen Zhu",
      "Yunjia Xi",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Survey papers are foundational to the scholarly progress of research communities, offering structured overviews that guide both novices and experts across disciplines. However, the recent surge of AI-generated surveys, especially enabled by large language models (LLMs), has transformed this traditionally labor-intensive genre into a low-effort, high-volume output. While such automation lowers entry barriers, it also introduces a critical threat: the phenomenon we term the \"survey paper DDoS attack\" to the research community. This refers to the unchecked proliferation of superficially comprehensive but often redundant, low-quality, or even hallucinated survey manuscripts, which floods preprint platforms, overwhelms researchers, and erodes trust in the scientific record. In this position paper, we argue that we must stop uploading massive amounts of AI-generated survey papers (i.e., survey paper DDoS attack) to the research community, by instituting strong norms for AI-assisted review writing. We call for restoring expert oversight and transparency in AI usage and, moreover, developing new infrastructures such as Dynamic Live Surveys, community-maintained, version-controlled repositories that blend automated updates with human curation. Through quantitative trend analysis, quality audits, and cultural impact discussion, we show that safeguarding the integrity of surveys is no longer optional but imperative to the research community.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted by NeurIPS 2025 (Position Track)",
    "pdf_url": "https://arxiv.org/pdf/2510.09686v1",
    "published_date": "2025-10-09 05:55:13 UTC",
    "updated_date": "2025-10-09 05:55:13 UTC"
  },
  {
    "arxiv_id": "2510.07813v1",
    "title": "Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games",
    "authors": [
      "Valerio La Gatta",
      "Dolev Mutzari",
      "Sarit Kraus",
      "VS Subrahmanian"
    ],
    "abstract": "Adversarial environments require agents to navigate a key strategic trade-off: acquiring information enhances situational awareness, but may simultaneously expose them to threats. To investigate this tension, we formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer agent must decide when to communicate in order to obtain the evader's position. Each communication reveals the pursuer's location, increasing the risk of being targeted. Both agents learn their movement policies via reinforcement learning, while the pursuer additionally learns a communication policy that balances observability and risk. We propose SHADOW (Strategic-communication Hybrid Action Decision-making under partial Observation for Warfare), a multi-headed sequential reinforcement learning framework that integrates continuous navigation control, discrete communication actions, and opponent modeling for behavior prediction. Empirical evaluations show that SHADOW pursuers achieve higher success rates than six competitive baselines. Our ablation study confirms that temporal sequence modeling and opponent modeling are critical for effective decision-making. Finally, our sensitivity analysis reveals that the learned policies generalize well across varying communication risks and physical asymmetries between agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07813v1",
    "published_date": "2025-10-09 05:44:00 UTC",
    "updated_date": "2025-10-09 05:44:00 UTC"
  },
  {
    "arxiv_id": "2510.07809v2",
    "title": "Practical and Stealthy Touch-Guided Jailbreak Attacks on Deployed Mobile Vision-Language Agents",
    "authors": [
      "Renhua Ding",
      "Xiao Yang",
      "Zhengwei Fang",
      "Jun Luo",
      "Kun He",
      "Jun Zhu"
    ],
    "abstract": "Large vision-language models (LVLMs) enable autonomous mobile agents to operate smartphone user interfaces, yet vulnerabilities in their perception and interaction remain critically understudied. Existing research often relies on conspicuous overlays, elevated permissions, or unrealistic threat assumptions, limiting stealth and real-world feasibility. In this paper, we introduce a practical and stealthy jailbreak attack framework, which comprises three key components: (i) non-privileged perception compromise, which injects visual payloads into the application interface without requiring elevated system permissions; (ii) agent-attributable activation, which leverages input attribution signals to distinguish agent from human interactions and limits prompt exposure to transient intervals to preserve stealth from end users; and (iii) efficient one-shot jailbreak, a heuristic iterative deepening search algorithm (HG-IDA*) that performs keyword-level detoxification to bypass built-in safety alignment of LVLMs. Moreover, we developed three representative Android applications and curated a prompt-injection dataset for mobile agents. We evaluated our attack across multiple LVLM backends, including closed-source services and representative open-source models, and observed high planning and execution hijack rates (e.g., GPT-4o: 82.5% planning / 75.0% execution), exposing a fundamental security vulnerability in current mobile agents and underscoring critical implications for autonomous smartphone operation.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07809v2",
    "published_date": "2025-10-09 05:34:57 UTC",
    "updated_date": "2025-11-20 03:13:23 UTC"
  },
  {
    "arxiv_id": "2510.07799v1",
    "title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models",
    "authors": [
      "Eric Hanchen Jiang",
      "Guancheng Wan",
      "Sophia Yin",
      "Mengting Li",
      "Yuchen Wu",
      "Xiao Liang",
      "Xinfeng Li",
      "Yizhou Sun",
      "Wei Wang",
      "Kai-Wei Chang",
      "Ying Nian Wu"
    ],
    "abstract": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07799v1",
    "published_date": "2025-10-09 05:28:28 UTC",
    "updated_date": "2025-10-09 05:28:28 UTC"
  },
  {
    "arxiv_id": "2510.07794v1",
    "title": "HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation",
    "authors": [
      "Peilin Wu",
      "Mian Zhang",
      "Kun Wan",
      "Wentian Zhao",
      "Kaiyu He",
      "Xinya Du",
      "Zhiyu Chen"
    ],
    "abstract": "Agentic RAG is a powerful technique for incorporating external information that LLMs lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a RL framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), a training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished while improving search efficiency, reducing the over-search rate to just 2.3% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.07794v1",
    "published_date": "2025-10-09 05:13:10 UTC",
    "updated_date": "2025-10-09 05:13:10 UTC"
  },
  {
    "arxiv_id": "2510.07793v3",
    "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology",
    "authors": [
      "Sajib Acharjee Dip",
      "Adrika Zafor",
      "Bikash Kumar Paul",
      "Uddip Acharjee Shuvo",
      "Muhit Islam Emon",
      "Xuan Wang",
      "Liqing Zhang"
    ],
    "abstract": "Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, 5 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.07793v3",
    "published_date": "2025-10-09 05:12:09 UTC",
    "updated_date": "2025-11-23 05:07:35 UTC"
  },
  {
    "arxiv_id": "2510.07790v1",
    "title": "GCPO: When Contrast Fails, Go Gold",
    "authors": [
      "Hao Wu",
      "Wei Liu"
    ],
    "abstract": "Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07790v1",
    "published_date": "2025-10-09 05:09:06 UTC",
    "updated_date": "2025-10-09 05:09:06 UTC"
  },
  {
    "arxiv_id": "2510.07778v1",
    "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction",
    "authors": [
      "Yandu Chen",
      "Kefan Gu",
      "Yuqing Wen",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Liqiang Nie"
    ],
    "abstract": "Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $π_0$, achieving 18\\% higher success rates with direct instructions and 28\\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07778v1",
    "published_date": "2025-10-09 04:49:46 UTC",
    "updated_date": "2025-10-09 04:49:46 UTC"
  },
  {
    "arxiv_id": "2510.07777v2",
    "title": "Drift No More? Context Equilibria in Multi-Turn LLM Interactions",
    "authors": [
      "Vardhan Dongre",
      "Ryan A. Rossi",
      "Viet Dac Lai",
      "David Seunghyun Yoon",
      "Dilek Hakkani-Tür",
      "Trung Bui"
    ],
    "abstract": "Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $τ$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07777v2",
    "published_date": "2025-10-09 04:48:49 UTC",
    "updated_date": "2025-11-21 19:11:25 UTC"
  },
  {
    "arxiv_id": "2510.07773v1",
    "title": "Trajectory Conditioned Cross-embodiment Skill Transfer",
    "authors": [
      "YuHang Tang",
      "Yixuan Lou",
      "Pengfei Han",
      "Haoming Song",
      "Xinyi Ye",
      "Dong Wang",
      "Bin Zhao"
    ],
    "abstract": "Learning manipulation skills from human demonstration videos presents a promising yet challenging problem, primarily due to the significant embodiment gap between human body and robot manipulators. Existing methods rely on paired datasets or hand-crafted rewards, which limit scalability and generalization. We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment Skill Transfer, enabling robots to acquire manipulation skills directly from human demonstration videos. Our key insight is to represent human motions as sparse optical flow trajectories, which serve as embodiment-agnostic motion cues by removing morphological variations while preserving essential dynamics. Conditioned on these trajectories together with visual and textual inputs, TrajSkill jointly synthesizes temporally consistent robot manipulation videos and translates them into executable actions, thereby achieving cross-embodiment skill transfer. Extensive experiments are conducted, and the results on simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\\% and KVD by 36.6\\% compared with the state-of-the-art, and improves cross-embodiment success rate by up to 16.7\\%. Real-robot experiments in kitchen manipulation tasks further validate the effectiveness of our approach, demonstrating practical human-to-robot skill transfer across embodiments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07773v1",
    "published_date": "2025-10-09 04:26:06 UTC",
    "updated_date": "2025-10-09 04:26:06 UTC"
  },
  {
    "arxiv_id": "2510.07772v3",
    "title": "An approach for systematic decomposition of complex llm tasks",
    "authors": [
      "Tianle Zhou",
      "Jiakai Xu",
      "Guanhong Liu",
      "Jiaxiang Liu",
      "Haonan Wang",
      "Eugene Wu"
    ],
    "abstract": "Large Language Models (LLMs) suffer from reliability issues on complex tasks, as existing decomposition methods are heuristic and rely on agent or manual decomposition. This work introduces a novel, systematic decomposition framework that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models the task as a constraint problem and leverages formal complexity measures to guide decomposition. On combinatorial (SAT-Bench) and LLM database querying tasks (Spider), we find that by decomposing the tasks following the measure of complexity, agent can perform considerably better.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07772v3",
    "published_date": "2025-10-09 04:24:47 UTC",
    "updated_date": "2026-01-18 18:47:31 UTC"
  },
  {
    "arxiv_id": "2510.11728v1",
    "title": "Modeling Hypergraph Using Large Language Models",
    "authors": [
      "Bingqiao Gu",
      "Jiale Zeng",
      "Xingqin Qi",
      "Dong Li"
    ],
    "abstract": "Due to the advantages of hypergraphs in modeling high-order relationships in complex systems, they have been applied to higher-order clustering, hypergraph neural networks and computer vision. These applications rely heavily on access to high-quality, large-scale real-world hypergraph data. Yet, compared to traditional pairwise graphs, real hypergraph datasets remain scarce in both scale and diversity. This shortage significantly limits the development and evaluation of advanced hypergraph learning algorithms. Therefore, how to quickly generate large-scale hypergraphs that conform to the characteristics of real networks is a crucial task that has not received sufficient attention. Motivated by recent advances in large language models (LLMs), particularly their capabilities in semantic reasoning, structured generation, and simulating human behavior, we investigate whether LLMs can facilitate hypergraph generation from a fundamentally new perspective. We introduce HyperLLM, a novel LLM-driven hypergraph generator that simulates the formation and evolution of hypergraphs through a multi-agent collaboration. The framework integrates prompts and structural feedback mechanisms to ensure that the generated hypergraphs reflect key real-world patterns. Extensive experiments across diverse datasets demonstrate that HyperLLM achieves superior fidelity to structural and temporal hypergraph patterns, while requiring minimal statistical priors. Our findings suggest that LLM-based frameworks offer a promising new direction for hypergraph modeling.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "10 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11728v1",
    "published_date": "2025-10-09 04:23:16 UTC",
    "updated_date": "2025-10-09 04:23:16 UTC"
  },
  {
    "arxiv_id": "2510.07768v1",
    "title": "ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning",
    "authors": [
      "Murong Yue",
      "Zhiwei Liu",
      "Liangwei Yang",
      "Jianguo Zhang",
      "Zuxin Liu",
      "Haolin Chen",
      "Ziyu Yao",
      "Silvio Savarese",
      "Caiming Xiong",
      "Shelby Heinecke",
      "Huan Wang"
    ],
    "abstract": "Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07768v1",
    "published_date": "2025-10-09 04:11:16 UTC",
    "updated_date": "2025-10-09 04:11:16 UTC"
  },
  {
    "arxiv_id": "2510.09685v1",
    "title": "Deep Neural Networks Inspired by Differential Equations",
    "authors": [
      "Yongshuai Liu",
      "Lianfang Wang",
      "Kuilin Qin",
      "Qinghua Zhang",
      "Faqiang Wang",
      "Li Cui",
      "Jun Liu",
      "Yuping Duan",
      "Tieyong Zeng"
    ],
    "abstract": "Deep learning has become a pivotal technology in fields such as computer vision, scientific computing, and dynamical systems, significantly advancing these disciplines. However, neural Networks persistently face challenges related to theoretical understanding, interpretability, and generalization. To address these issues, researchers are increasingly adopting a differential equations perspective to propose a unified theoretical framework and systematic design methodologies for neural networks. In this paper, we provide an extensive review of deep neural network architectures and dynamic modeling methods inspired by differential equations. We specifically examine deep neural network models and deterministic dynamical network constructs based on ordinary differential equations (ODEs), as well as regularization techniques and stochastic dynamical network models informed by stochastic differential equations (SDEs). We present numerical comparisons of these models to illustrate their characteristics and performance. Finally, we explore promising research directions in integrating differential equations with deep learning to offer new insights for developing intelligent computational methods that boast enhanced interpretability and generalization capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "35 Pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09685v1",
    "published_date": "2025-10-09 04:08:23 UTC",
    "updated_date": "2025-10-09 04:08:23 UTC"
  },
  {
    "arxiv_id": "2510.07762v1",
    "title": "From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation",
    "authors": [
      "Xiangwei Lv",
      "JinLuan Yang",
      "Wang Lin",
      "Jingyuan Chen",
      "Beishui Liao"
    ],
    "abstract": "Graph domain adaptation (GDA) has achieved great attention due to its effectiveness in addressing the domain shift between train and test data. A significant bottleneck in existing graph domain adaptation methods is their reliance on source-domain data, which is often unavailable due to privacy or security concerns. This limitation has driven the development of Test-Time Graph Domain Adaptation (TT-GDA), which aims to transfer knowledge without accessing the source examples. Inspired by the generative power of large language models (LLMs), we introduce a novel framework that reframes TT-GDA as a generative graph restoration problem, \"restoring the target graph to its pristine, source-domain-like state\". There are two key challenges: (1) We need to construct a reasonable graph restoration process and design an effective encoding scheme that an LLM can understand, bridging the modality gap. (2) We need to devise a mechanism to ensure the restored graph acquires the intrinsic features of the source domain, even without access to the source data. To ensure the effectiveness of graph restoration, we propose GRAIL, that restores the target graph into a state that is well-aligned with the source domain. Specifically, we first compress the node representations into compact latent features and then use a graph diffusion process to model the graph restoration process. Then a quantization module encodes the restored features into discrete tokens. Building on this, an LLM is fine-tuned as a generative restorer to transform a \"noisy\" target graph into a \"native\" one. To further improve restoration quality, we introduce a reinforcement learning process guided by specialized alignment and confidence rewards. Extensive experiments demonstrate the effectiveness of our approach across various datasets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07762v1",
    "published_date": "2025-10-09 04:00:42 UTC",
    "updated_date": "2025-10-09 04:00:42 UTC"
  },
  {
    "arxiv_id": "2510.07760v1",
    "title": "A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization",
    "authors": [
      "Yiqin Lv",
      "Zhiyu Mou",
      "Miao Xu",
      "Jinghao Chen",
      "Qi Wang",
      "Yixiu Mao",
      "Yun Qu",
      "Rongquan Bai",
      "Chuan Yu",
      "Jian Xu",
      "Bo Zheng",
      "Xiangyang Ji"
    ],
    "abstract": "In online advertising, heterogeneous advertiser requirements give rise to numerous customized bidding tasks that are typically optimized independently, resulting in extensive computation and limited data efficiency. Multi-task learning offers a principled framework to train these tasks jointly through shared representations. However, existing multi-task optimization strategies are primarily guided by training dynamics and often generalize poorly in volatile bidding environments. To this end, we present Validation-Aligned Multi-task Optimization (VAMO), which adaptively assigns task weights based on the alignment between per-task training gradients and a held-out validation gradient, thereby steering updates toward validation improvement and better matching deployment objectives. We further equip the framework with a periodicity-aware temporal module and couple it with an advanced generative auto-bidding backbone to enhance cross-task transfer of seasonal structure and strengthen bidding performance. Meanwhile, we provide theoretical insights into the proposed method, e.g., convergence guarantee and alignment analysis. Extensive experiments on both simulated and large-scale real-world advertising systems consistently demonstrate significant improvements over typical baselines, illuminating the effectiveness of the proposed approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07760v1",
    "published_date": "2025-10-09 03:59:51 UTC",
    "updated_date": "2025-10-09 03:59:51 UTC"
  },
  {
    "arxiv_id": "2510.07748v1",
    "title": "Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains",
    "authors": [
      "Yilun Zhang",
      "Dexing Kong"
    ],
    "abstract": "Large Language Models (LLMs) show promise in medicine but are prone to factual and logical errors, which is unacceptable in this high-stakes field. To address this, we introduce the \"Haibu Mathematical-Medical Intelligent Agent\" (MMIA), an LLM-driven architecture that ensures reliability through a formally verifiable reasoning process. MMIA recursively breaks down complex medical tasks into atomic, evidence-based steps. This entire reasoning chain is then automatically audited for logical coherence and evidence traceability, similar to theorem proving. A key innovation is MMIA's \"bootstrapping\" mode, which stores validated reasoning chains as \"theorems.\" Subsequent tasks can then be efficiently solved using Retrieval-Augmented Generation (RAG), shifting from costly first-principles reasoning to a low-cost verification model. We validated MMIA across four healthcare administration domains, including DRG/DIP audits and medical insurance adjudication, using expert-validated benchmarks. Results showed MMIA achieved an error detection rate exceeding 98% with a false positive rate below 1%, significantly outperforming baseline LLMs. Furthermore, the RAG matching mode is projected to reduce average processing costs by approximately 85% as the knowledge base matures. In conclusion, MMIA's verifiable reasoning framework is a significant step toward creating trustworthy, transparent, and cost-effective AI systems, making LLM technology viable for critical applications in medicine.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07748v1",
    "published_date": "2025-10-09 03:35:37 UTC",
    "updated_date": "2025-10-09 03:35:37 UTC"
  },
  {
    "arxiv_id": "2510.07745v3",
    "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
    "authors": [
      "Runyang You",
      "Yongqi Li",
      "Meng Liu",
      "Wenjie Wang",
      "Liqiang Nie",
      "Wenjie Li"
    ],
    "abstract": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code and checkpoints released at https://github.com/ModalityDance/LatentTTS",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07745v3",
    "published_date": "2025-10-09 03:33:00 UTC",
    "updated_date": "2026-01-15 14:50:27 UTC"
  },
  {
    "arxiv_id": "2510.07741v1",
    "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes",
    "authors": [
      "Yuang Meng",
      "Xin Jin",
      "Lina Lei",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "abstract": "Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07741v1",
    "published_date": "2025-10-09 03:29:39 UTC",
    "updated_date": "2025-10-09 03:29:39 UTC"
  },
  {
    "arxiv_id": "2510.07740v1",
    "title": "AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?",
    "authors": [
      "Dezhi Ran",
      "Yuan Cao",
      "Mengzhou Wu",
      "Simin Chen",
      "Yuzhe Guo",
      "Jun Ren",
      "Zihe Song",
      "Hao Yu",
      "Jialei Wei",
      "Linyi Li",
      "Wei Yang",
      "Baishakhi Ray",
      "Tao Xie"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capability in function-level code generation tasks. Unlike isolated functions, real-world applications demand reasoning over the entire software system: developers must orchestrate how different components interact, maintain consistency across states over time, and ensure the application behaves correctly within the lifecycle and framework constraints. Yet, no existing benchmark adequately evaluates whether LLMs can bridge this gap and construct entire software systems from scratch. To address this gap, we propose APPFORGE, a benchmark consisting of 101 software development problems drawn from real-world Android apps. Given a natural language specification detailing the app functionality, a language model is tasked with implementing the functionality into an Android app from scratch. Developing an Android app from scratch requires understanding and coordinating app states, lifecycle management, and asynchronous operations, calling for LLMs to generate context-aware, robust, and maintainable code. To construct APPFORGE, we design a multi-agent system to automatically summarize the main functionalities from app documents and navigate the app to synthesize test cases validating the functional correctness of app implementation. Following rigorous manual verification by Android development experts, APPFORGE incorporates the test cases within an automated evaluation framework that enables reproducible assessment without human intervention, making it easily adoptable for future research. Our evaluation on 12 flagship LLMs show that all evaluated models achieve low effectiveness, with the best-performing model (GPT-5) developing only 18.8% functionally correct applications, highlighting fundamental limitations in current models' ability to handle complex, multi-component software engineering challenges.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Under Review. Benchmark and leadboards at https://appforge-bench.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.07740v1",
    "published_date": "2025-10-09 03:26:05 UTC",
    "updated_date": "2025-10-09 03:26:05 UTC"
  },
  {
    "arxiv_id": "2510.07739v1",
    "title": "MeSH: Memory-as-State-Highways for Recursive Transformers",
    "authors": [
      "Chengting Yu",
      "Xiaobo Shu",
      "Yadao Wang",
      "Yizhen Zhang",
      "Haoyi Wu",
      "Jiaang Li",
      "Rujiao Long",
      "Ziheng Chen",
      "Yuchi Xu",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "abstract": "Recursive transformers reuse parameters and iterate over hidden states multiple times, decoupling compute depth from parameter depth. However, under matched compute, recursive models with fewer parameters often lag behind non-recursive counterparts. By probing hidden states, we trace this performance gap to two primary bottlenecks: undifferentiated computation, where the core is forced to adopt a similar computational pattern at every iteration, and information overload, where long-lived and transient information must coexist in a single hidden state. To address the issues, we introduce a Memory-as-State-Highways (MeSH) scheme, which externalizes state management into an explicit memory buffer and employs lightweight routers to dynamically diversify computation across iterations. Probing visualizations confirm that MeSH successfully resolves the pathologies by inducing functional specialization across iterations. On the Pythia suite (160M-1.4B), MeSH-enhanced recursive transformers consistently improve over recursive baselines and outperforms its larger non-recursive counterpart at the 1.4B scale, improving average downstream accuracy by +1.06% with 33% fewer non-embedding parameters. Our analysis establishes MeSH as a scalable and principled architecture for building stronger recursive models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07739v1",
    "published_date": "2025-10-09 03:23:38 UTC",
    "updated_date": "2025-10-09 03:23:38 UTC"
  },
  {
    "arxiv_id": "2510.07733v2",
    "title": "SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation",
    "authors": [
      "Minh-Anh Nguye",
      "Minh-Duc Nguyen",
      "Ha Lan N. T.",
      "Kieu Hai Dang",
      "Nguyen Tien Dong",
      "Dung D. Le"
    ],
    "abstract": "Large language models (LLMs) are increasingly adopted for automating survey paper generation \\cite{wang2406autosurvey, liang2025surveyx, yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing approaches typically extract content from a large collection of related papers and prompt LLMs to summarize them directly. However, such methods often overlook the structural relationships among papers, resulting in generated surveys that lack a coherent taxonomy and a deeper contextual understanding of research progress. To address these shortcomings, we propose \\textbf{SurveyG}, an LLM-based agent framework that integrates \\textit{hierarchical citation graph}, where nodes denote research papers and edges capture both citation dependencies and semantic relatedness between their contents, thereby embedding structural and contextual knowledge into the survey generation process. The graph is organized into three layers: \\textbf{Foundation}, \\textbf{Development}, and \\textbf{Frontier}, to capture the evolution of research from seminal works to incremental advances and emerging directions. By combining horizontal search within layers and vertical depth traversal across layers, the agent produces multi-level summaries, which are consolidated into a structured survey outline. A multi-agent validation stage then ensures consistency, coverage, and factual accuracy in generating the final survey. Experiments, including evaluations by human experts and LLM-as-a-judge, demonstrate that SurveyG outperforms state-of-the-art frameworks, producing surveys that are more comprehensive and better structured to the underlying knowledge taxonomy of a field.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07733v2",
    "published_date": "2025-10-09 03:14:20 UTC",
    "updated_date": "2025-10-10 02:59:37 UTC"
  },
  {
    "arxiv_id": "2510.07731v2",
    "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning",
    "authors": [
      "Ruiling Xu",
      "Yifan Zhang",
      "Qingyun Wang",
      "Carl Edwards",
      "Heng Ji"
    ],
    "abstract": "Organic reaction mechanisms are the stepwise elementary reactions by which reactants form intermediates and products, and are fundamental to understanding chemical reactivity and designing new molecules and reactions. Although large language models (LLMs) have shown promise in understanding chemical tasks such as synthesis design, it is unclear to what extent this reflects genuine chemical reasoning capabilities, i.e., the ability to generate valid intermediates, maintain chemical consistency, and follow logically coherent multi-step pathways. We address this by introducing oMeBench, the first large-scale, expert-curated benchmark for organic mechanism reasoning in organic chemistry. It comprises over 10,000 annotated mechanistic steps with intermediates, type labels, and difficulty ratings. Furthermore, to evaluate LLM capability more precisely and enable fine-grained scoring, we propose oMeS, a dynamic evaluation framework that combines step-level logic and chemical similarity. We analyze the performance of state-of-the-art LLMs, and our results show that although current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Notably, we find that using prompting strategy and fine-tuning a specialist model on our proposed dataset increases performance by 50% over the leading closed-source model. We hope that oMeBench will serve as a rigorous foundation for advancing AI systems toward genuine chemical reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07731v2",
    "published_date": "2025-10-09 03:13:31 UTC",
    "updated_date": "2025-10-12 08:15:32 UTC"
  },
  {
    "arxiv_id": "2510.07730v1",
    "title": "DEAS: DEtached value learning with Action Sequence for Scalable Offline RL",
    "authors": [
      "Changyeon Kim",
      "Haeone Lee",
      "Younggyo Seo",
      "Kimin Lee",
      "Yuke Zhu"
    ],
    "abstract": "Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Project website: https://changyeon.site/deas",
    "pdf_url": "https://arxiv.org/pdf/2510.07730v1",
    "published_date": "2025-10-09 03:11:09 UTC",
    "updated_date": "2025-10-09 03:11:09 UTC"
  },
  {
    "arxiv_id": "2510.07715v1",
    "title": "Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning",
    "authors": [
      "Xiaochen Tang",
      "Zhenya Zhang",
      "Miaomiao Zhang",
      "Jie An"
    ],
    "abstract": "In real-time and safety-critical cyber-physical systems (CPSs), control synthesis must guarantee that generated policies meet stringent timing and correctness requirements under uncertain and dynamic conditions. Signal temporal logic (STL) has emerged as a powerful formalism of expressing real-time constraints, with its semantics enabling quantitative assessment of system behavior. Meanwhile, reinforcement learning (RL) has become an important method for solving control synthesis problems in unknown environments. Recent studies incorporate STL-based reward functions into RL to automatically synthesize control policies. However, the automatically inferred rewards obtained by these methods represent the global assessment of a whole or partial path but do not accumulate the rewards of local changes accurately, so the sparse global rewards may lead to non-convergence and unstable training performances. In this paper, we propose an online reward generation method guided by the online causation monitoring of STL. Our approach continuously monitors system behavior against an STL specification at each control step, computing the quantitative distance toward satisfaction or violation and thereby producing rewards that reflect instantaneous state dynamics. Additionally, we provide a smooth approximation of the causation semantics to overcome the discontinuity of the causation semantics and make it differentiable for using deep-RL methods. We have implemented a prototype tool and evaluated it in the Gym environment on a variety of continuously controlled benchmarks. Experimental results show that our proposed STL-guided RL method with online causation semantics outperforms existing relevant STL-guided RL methods, providing a more robust and efficient reward generation framework for deep-RL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 4 figures, 6 tables, accepted by RTSS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.07715v1",
    "published_date": "2025-10-09 02:49:28 UTC",
    "updated_date": "2025-10-09 02:49:28 UTC"
  },
  {
    "arxiv_id": "2510.07709v1",
    "title": "Multimodal Safety Evaluation in Generative Agent Social Simulations",
    "authors": [
      "Alhim Vera",
      "Karen Sanchez",
      "Carlos Hinojosa",
      "Haidar Bin Hamid",
      "Donghoon Kim",
      "Bernard Ghanem"
    ],
    "abstract": "Can generative agents be trusted in multimodal environments? Despite advances in large language and vision-language models that enable agents to act autonomously and pursue goals in rich settings, their ability to reason about safety, coherence, and trust across modalities remains limited. We introduce a reproducible simulation framework for evaluating agents along three dimensions: (1) safety improvement over time, including iterative plan revisions in text-visual scenarios; (2) detection of unsafe activities across multiple categories of social situations; and (3) social dynamics, measured as interaction counts and acceptance ratios of social exchanges. Agents are equipped with layered memory, dynamic planning, multimodal perception, and are instrumented with SocialMetrics, a suite of behavioral and structural metrics that quantifies plan revisions, unsafe-to-safe conversions, and information diffusion across networks. Experiments show that while agents can detect direct multimodal contradictions, they often fail to align local revisions with global safety, reaching only a 55 percent success rate in correcting unsafe plans. Across eight simulation runs with three models - Claude, GPT-4o mini, and Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75, 55, and 58 percent, respectively. Overall performance ranged from 20 percent in multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted when paired with misleading visuals, showing a strong tendency to overtrust images. These findings expose critical limitations in current architectures and provide a reproducible platform for studying multimodal safety, coherence, and social dynamics.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07709v1",
    "published_date": "2025-10-09 02:42:57 UTC",
    "updated_date": "2025-10-09 02:42:57 UTC"
  },
  {
    "arxiv_id": "2510.07707v1",
    "title": "Causality Guided Representation Learning for Cross-Style Hate Speech Detection",
    "authors": [
      "Chengshuai Zhao",
      "Shu Wan",
      "Paras Sheth",
      "Karan Patwa",
      "K. Selçuk Candan",
      "Huan Liu"
    ],
    "abstract": "The proliferation of online hate speech poses a significant threat to the harmony of the web. While explicit hate is easily recognized through overt slurs, implicit hate speech is often conveyed through sarcasm, irony, stereotypes, or coded language -- making it harder to detect. Existing hate speech detection models, which predominantly rely on surface-level linguistic cues, fail to generalize effectively across diverse stylistic variations. Moreover, hate speech spread on different platforms often targets distinct groups and adopts unique styles, potentially inducing spurious correlations between them and labels, further challenging current detection approaches. Motivated by these observations, we hypothesize that the generation of hate speech can be modeled as a causal graph involving key factors: contextual environment, creator motivation, target, and style. Guided by this graph, we propose CADET, a causal representation learning framework that disentangles hate speech into interpretable latent factors and then controls confounders, thereby isolating genuine hate intent from superficial linguistic cues. Furthermore, CADET allows counterfactual reasoning by intervening on style within the latent space, naturally guiding the model to robustly identify hate speech in varying forms. CADET demonstrates superior performance in comprehensive experiments, highlighting the potential of causal priors in advancing generalizable hate speech detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07707v1",
    "published_date": "2025-10-09 02:41:37 UTC",
    "updated_date": "2025-10-09 02:41:37 UTC"
  },
  {
    "arxiv_id": "2510.07697v1",
    "title": "Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs",
    "authors": [
      "Man Hu",
      "Xinyi Wu",
      "Zuofeng Suo",
      "Jinbo Feng",
      "Linghui Meng",
      "Yanhao Jia",
      "Anh Tuan Luu",
      "Shuai Zhao"
    ],
    "abstract": "With the rise of advanced reasoning capabilities, large language models (LLMs) are receiving increasing attention. However, although reasoning improves LLMs' performance on downstream tasks, it also introduces new security risks, as adversaries can exploit these capabilities to conduct backdoor attacks. Existing surveys on backdoor attacks and reasoning security offer comprehensive overviews but lack in-depth analysis of backdoor attacks and defenses targeting LLMs' reasoning abilities. In this paper, we take the first step toward providing a comprehensive review of reasoning-based backdoor attacks in LLMs by analyzing their underlying mechanisms, methodological frameworks, and unresolved challenges. Specifically, we introduce a new taxonomy that offers a unified perspective for summarizing existing approaches, categorizing reasoning-based backdoor attacks into associative, passive, and active. We also present defense strategies against such attacks and discuss current challenges alongside potential directions for future research. This work offers a novel perspective, paving the way for further exploration of secure and trustworthy LLM communities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07697v1",
    "published_date": "2025-10-09 02:35:37 UTC",
    "updated_date": "2025-10-09 02:35:37 UTC"
  },
  {
    "arxiv_id": "2510.07686v2",
    "title": "Stress-Testing Model Specs Reveals Character Differences among Language Models",
    "authors": [
      "Jifan Zhang",
      "Henry Sleight",
      "Andi Peng",
      "John Schulman",
      "Esin Durmus"
    ],
    "abstract": "Large language models (LLMs) are increasingly trained from AI constitutions and model specifications that establish behavioral guidelines and ethical principles. However, these specifications face critical challenges, including internal conflicts between principles and insufficient coverage of nuanced scenarios. We present a systematic methodology for stress-testing model character specifications, automatically identifying numerous cases of principle contradictions and interpretive ambiguities in current model specs.\n  We stress test current model specs by generating scenarios that force explicit tradeoffs between competing value-based principles. Using a comprehensive taxonomy we generate diverse value tradeoff scenarios where models must choose between pairs of legitimate principles that cannot be simultaneously satisfied. We evaluate responses from twelve frontier LLMs across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral disagreement through value classification scores. Among these scenarios, we identify over 70,000 cases exhibiting significant behavioral divergence. Empirically, we show this high divergence in model behavior strongly predicts underlying problems in model specifications. Through qualitative analysis, we provide numerous example issues in current model specs such as direct contradiction and interpretive ambiguities of several principles. Additionally, our generated dataset also reveals both clear misalignment cases and false-positive refusals across all of the frontier models we study. Lastly, we also provide value prioritization patterns and differences of these models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07686v2",
    "published_date": "2025-10-09 02:24:37 UTC",
    "updated_date": "2025-10-23 07:31:33 UTC"
  },
  {
    "arxiv_id": "2510.07681v2",
    "title": "Curriculum Learning with Synthetic Data for Enhanced Pulmonary Nodule Detection in Chest Radiographs",
    "authors": [
      "Pranav Sambhu",
      "Om Guin",
      "Madhav Sambhu",
      "Jinho Cha"
    ],
    "abstract": "This study evaluates whether integrating curriculum learning with diffusion-based synthetic augmentation can enhance the detection of difficult pulmonary nodules in chest radiographs, particularly those with low size, brightness, and contrast, which often challenge conventional AI models due to data imbalance and limited annotation. A Faster R-CNN with a Feature Pyramid Network (FPN) backbone was trained on a hybrid dataset comprising expert-labeled NODE21 (1,213 patients; 52.4 percent male; mean age 63.2 +/- 11.5 years), VinDr-CXR, CheXpert, and 11,206 DDPM-generated synthetic images. Difficulty scores based on size, brightness, and contrast guided curriculum learning. Performance was compared to a non-curriculum baseline using mean average precision (mAP), Dice score, and area under the curve (AUC). Statistical tests included bootstrapped confidence intervals, DeLong tests, and paired t-tests. The curriculum model achieved a mean AUC of 0.95 versus 0.89 for the baseline (p < 0.001), with improvements in sensitivity (70 percent vs. 48 percent) and accuracy (82 percent vs. 70 percent). Stratified analysis demonstrated consistent gains across all difficulty bins (Easy to Very Hard). Grad-CAM visualizations confirmed more anatomically focused attention under curriculum learning. These results suggest that curriculum-guided synthetic augmentation enhances model robustness and generalization for pulmonary nodule detection.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This version has been withdrawn due to authorship changes and a decision to substantially revise the manuscript with new methodology. A future version may be submitted separately",
    "pdf_url": "https://arxiv.org/pdf/2510.07681v2",
    "published_date": "2025-10-09 02:06:13 UTC",
    "updated_date": "2025-10-20 20:02:50 UTC"
  },
  {
    "arxiv_id": "2510.07670v2",
    "title": "Ctrl-VI: Controllable Video Synthesis via Variational Inference",
    "authors": [
      "Haoyi Duan",
      "Yunzhi Zhang",
      "Yilun Du",
      "Jiajun Wu"
    ],
    "abstract": "Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop Ctrl-VI, a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://video-synthesis-variational.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.07670v2",
    "published_date": "2025-10-09 01:48:16 UTC",
    "updated_date": "2025-10-16 17:48:29 UTC"
  },
  {
    "arxiv_id": "2510.07666v1",
    "title": "TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration",
    "authors": [
      "Heming Wu",
      "Di Wang",
      "Tai Ma",
      "Peng Zhao",
      "Yubin Xiao",
      "Zhongke Wu",
      "Xing-Ce Wang",
      "Chuang Li",
      "Xuan Wu",
      "You Zhou"
    ],
    "abstract": "Although pyramid networks have demonstrated superior performance in deformable medical image registration, their decoder architectures are inherently prone to propagating and accumulating anatomical structure misalignments. Moreover, most existing models do not adaptively determine the number of iterations for optimization under varying deformation requirements across images, resulting in either premature termination or excessive iterations that degrades registration accuracy. To effectively mitigate the accumulation of anatomical misalignments, we propose the Feature-Enhanced Residual Module (FERM) as the core component of each decoding layer in the pyramid network. FERM comprises three sequential blocks that extract anatomical semantic features, learn to suppress irrelevant features, and estimate the final deformation field, respectively. To adaptively determine the number of iterations for varying images, we propose the dual-stage Threshold-Controlled Iterative (TCI) strategy. In the first stage, TCI assesses registration stability and with asserted stability, it continues with the second stage to evaluate convergence. We coin the model that integrates FERM and TCI as Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP outperforms the state-of-the-art (SOTA) registration networks in terms of accuracy, while maintaining comparable inference speed and a compact model parameter size. Finally, we assess the generalizability of FERM and TCI by integrating them with existing registration networks and further conduct ablation studies to validate the effectiveness of these two proposed methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07666v1",
    "published_date": "2025-10-09 01:38:40 UTC",
    "updated_date": "2025-10-09 01:38:40 UTC"
  },
  {
    "arxiv_id": "2510.08640v2",
    "title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools",
    "authors": [
      "Ha Min Son",
      "Huan Ren",
      "Xin Liu",
      "Zhe Zhao"
    ],
    "abstract": "Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixer's success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the model's high-level reasoning and effective low-level execution.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08640v2",
    "published_date": "2025-10-09 01:33:25 UTC",
    "updated_date": "2025-11-19 18:46:34 UTC"
  },
  {
    "arxiv_id": "2510.07661v2",
    "title": "IKNet: Interpretable Stock Price Prediction via Keyword-Guided Integration of News and Technical Indicators",
    "authors": [
      "Jinwoong Kim",
      "Sangjin Park"
    ],
    "abstract": "The increasing influence of unstructured external information, such as news articles, on stock prices has attracted growing attention in financial markets. Despite recent advances, most existing newsbased forecasting models represent all articles using sentiment scores or average embeddings that capture the general tone but fail to provide quantitative, context-aware explanations of the impacts of public sentiment on predictions. To address this limitation, we propose an interpretable keyword-guided network (IKNet), which is an explainable forecasting framework that models the semantic association between individual news keywords and stock price movements. The IKNet identifies salient keywords via FinBERTbased contextual analysis, processes each embedding through a separate nonlinear projection layer, and integrates their representations with the time-series data of technical indicators to forecast next-day closing prices. By applying Shapley Additive Explanations the model generates quantifiable and interpretable attributions for the contribution of each keyword to predictions. Empirical evaluations of S&P 500 data from 2015 to 2024 demonstrate that IKNet outperforms baselines, including recurrent neural networks and transformer models, reducing RMSE by up to 32.9% and improving cumulative returns by 18.5%. Moreover, IKNet enhances transparency by offering contextualized explanations of volatility events driven by public sentiment.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.07661v2",
    "published_date": "2025-10-09 01:30:30 UTC",
    "updated_date": "2025-10-27 08:47:49 UTC"
  },
  {
    "arxiv_id": "2510.07651v1",
    "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference",
    "authors": [
      "Yuzhe Gu",
      "Xiyu Liang",
      "Jiaojiao Zhao",
      "Enmao Diao"
    ],
    "abstract": "Large language models (LLMs) with extended context windows enable powerful downstream applications but impose significant memory overhead, as caching all key-value (KV) states scales linearly with sequence length and batch size. Existing cache eviction methods address this by exploiting attention sparsity, yet they typically rank tokens heuristically using accumulated attention weights without considering their true impact on attention outputs. We propose Optimal Brain Cache (OBCache), a principled framework that formulates cache eviction as a layer-wise structured pruning problem. Building upon the Optimal Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the perturbation in attention outputs induced by pruning tokens, with closed-form scores derived for isolated keys, isolated values, and joint key-value pairs. Our scores account not only for attention weights but also for information from value states and attention outputs, thereby enhancing existing eviction strategies with output-aware signals. Experiments on LLaMA and Qwen models demonstrate that replacing the heuristic scores in existing works, which estimate token saliency across different query positions, with OBCache's output-aware scores consistently improves long-context accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07651v1",
    "published_date": "2025-10-09 00:58:28 UTC",
    "updated_date": "2025-10-09 00:58:28 UTC"
  },
  {
    "arxiv_id": "2510.07650v1",
    "title": "Value Flows",
    "authors": [
      "Perry Dong",
      "Chongyi Zheng",
      "Chelsea Finn",
      "Dorsa Sadigh",
      "Benjamin Eysenbach"
    ],
    "abstract": "While most reinforcement learning methods today flatten the distribution of future returns to a single scalar value, distributional RL methods exploit the return distribution to provide stronger learning signals and to enable applications in exploration and safe RL. While the predominant method for estimating the return distribution is by modeling it as a categorical distribution over discrete bins or estimating a finite number of quantiles, such approaches leave unanswered questions about the fine-grained structure of the return distribution and about how to distinguish states with high return uncertainty for decision-making. The key idea in this paper is to use modern, flexible flow-based models to estimate the full future return distributions and identify those states with high return variance. We do so by formulating a new flow-matching objective that generates probability density paths satisfying the distributional Bellman equation. Building upon the learned flow models, we estimate the return uncertainty of distinct states using a new flow derivative ODE. We additionally use this uncertainty information to prioritize learning a more accurate return estimation on certain transitions. We compare our method (Value Flows) with prior methods in the offline and online-to-online settings. Experiments on $37$ state-based and $25$ image-based benchmark tasks demonstrate that Value Flows achieves a $1.3\\times$ improvement on average in success rates. Website: https://pd-perry.github.io/value-flows Code: https://github.com/chongyi-zheng/value-flows",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07650v1",
    "published_date": "2025-10-09 00:57:40 UTC",
    "updated_date": "2025-10-09 00:57:40 UTC"
  },
  {
    "arxiv_id": "2510.07645v1",
    "title": "Banking Done Right: Redefining Retail Banking with Language-Centric AI",
    "authors": [
      "Xin Jie Chua",
      "Jeraelyn Ming Li Tan",
      "Jia Xuan Tan",
      "Soon Chang Poh",
      "Yi Xian Goh",
      "Debbie Hui Tian Choong",
      "Chee Mun Foong",
      "Sze Jue Yang",
      "Chee Seng Chan"
    ],
    "abstract": "This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt Bank to enable customers to execute core financial transactions through natural language conversation. This represents the first global regulator-approved deployment worldwide where conversational AI functions as the primary banking interface, in contrast to prior assistants that have been limited to advisory or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a closed-source LLM developed internally, and replaces rigid multi-screen workflows with a single dialogue orchestrated by four LLM-powered agents (Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific LoRA adapter to ILMU, which is hosted within the bank's infrastructure to ensure consistent behavior with minimal overhead. Deterministic guardrails, human-in-the-loop confirmation, and a stateless audit architecture provide defense-in-depth for security and compliance. The result is Banking Done Right: demonstrating that regulator-approved natural-language interfaces can reliably support core financial operations under strict governance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2510.07645v1",
    "published_date": "2025-10-09 00:35:08 UTC",
    "updated_date": "2025-10-09 00:35:08 UTC"
  },
  {
    "arxiv_id": "2510.07635v1",
    "title": "Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning",
    "authors": [
      "Haruka Kiyohara",
      "Yusuke Narita",
      "Yuta Saito",
      "Kei Tateno",
      "Takuma Udagawa"
    ],
    "abstract": "In many real recommender systems, novel items are added frequently over time. The importance of sufficiently presenting novel actions has widely been acknowledged for improving long-term user engagement. A recent work builds on Off-Policy Learning (OPL), which trains a policy from only logged data, however, the existing methods can be unsafe in the presence of novel actions. Our goal is to develop a framework to enforce exploration of novel actions with a guarantee for safety. To this end, we first develop Safe Off-Policy Policy Gradient (Safe OPG), which is a model-free safe OPL method based on a high confidence off-policy evaluation. In our first experiment, we observe that Safe OPG almost always satisfies a safety requirement, even when existing methods violate it greatly. However, the result also reveals that Safe OPG tends to be too conservative, suggesting a difficult tradeoff between guaranteeing safety and exploring novel actions. To overcome this tradeoff, we also propose a novel framework called Deployment-Efficient Policy Learning for Safe User Exploration, which leverages safety margin and gradually relaxes safety regularization during multiple (not many) deployments. Our framework thus enables exploration of novel actions while guaranteeing safe implementation of recommender systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07635v1",
    "published_date": "2025-10-09 00:10:07 UTC",
    "updated_date": "2025-10-09 00:10:07 UTC"
  },
  {
    "arxiv_id": "2510.07632v1",
    "title": "Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models",
    "authors": [
      "Yinglun Zhu",
      "Jiancheng Zhang",
      "Fuzhi Tang"
    ],
    "abstract": "Frontier AI models have achieved remarkable progress, yet recent studies suggest they struggle with compositional reasoning, often performing at or below random chance on established benchmarks. We revisit this problem and show that widely used evaluation metrics systematically underestimate model capability. To address this, we introduce a group matching score that better exploits group structure and reveals substantial hidden capability in both contrastive vision-language models (VLMs) and multimodal large language models (MLLMs). Moreover, simply overfitting to the induced group matchings at test time transfers this hidden capability into higher scores under standard evaluation metrics, closing much of the reported gap. This adjustment enables SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first result surpassing estimated human performance on Winoground.\n  Building on this insight, we propose Test-Time Matching (TTM), an iterative, self-improving algorithm that further bootstraps model performance without any external supervision. TTM delivers additional, non-trivial improvements: for example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a new state of the art. Importantly, TTM remains broadly effective even on benchmarks without metric-induced effects or group structures, achieving relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16 dataset variants spanning diverse setups, our experiments demonstrate that TTM consistently improves model performance and advances the frontier of compositional reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07632v1",
    "published_date": "2025-10-09 00:00:49 UTC",
    "updated_date": "2025-10-09 00:00:49 UTC"
  }
]
{
  "date": "2025-03-19",
  "category": "cs.AI",
  "summary": "好的，教授！让我们开始撰写今天的 arXiv TLDR 快报。\n\n---\n\n欢迎来到 UTC 时间 2025-03-19 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文热点纷呈，大型语言模型（LLM）依然是焦点，研究者们深入探索了 LLM 在评估、对齐、安全性、效率以及特定任务（如代码生成、对话系统、小说改编）中的应用和挑战。多模态学习也备受关注，特别是在视觉-语言联合预训练、视觉定位、图像生成与编辑等方面涌现出不少新框架和基准。强化学习（RL）在机器人控制、多智能体协作和样本效率方面取得了新进展。此外，扩散模型、图神经网络、可解释性 AI 以及 AI 在科学和工程领域的应用也贡献了许多有趣的工作。\n\n**今天的亮点论文包括：**\n\n*   **LLM 评估与对齐新视角：** ContextualJudgeBench (16) 针对 RAG 等上下文场景提出新的 LLM 评判基准；Value Profiles (20) 提出用自然语言描述用户价值观以实现个性化对齐；AlignX (25) 构建大规模个性化偏好数据集，探索用户级对齐；Reward Model (22) 从优化角度分析奖励模型的有效性，强调奖励方差的重要性。\n*   **多模态模型新进展：** TULIP (19) 提出统一语言-图像预训练框架，兼顾视觉细节和语义对齐；VPP-LLaVA (29) 引入视觉位置提示增强 MLLM 的视觉定位能力；POSTA (83) 提出定制化艺术海报生成框架；MMDT (94) 构建了首个全面的多模态基础模型安全与可信度评估平台。\n*   **RL 与机器人技术突破：** Reward Training Wheels (4) 提出自适应辅助奖励框架加速机器人 RL；SAFER (5) 将安全意识融入 LLM 驱动的机器人任务规划；PEnGUiN (18) 提出部分等变 GNN 提升 MARL 样本效率；GraspCorrect (70) 利用 VLM 反馈修正机器人抓取；还有研究让机器人在真实世界学习弹钢琴 (21)。\n*   **扩散模型与生成效率：** Di[M]O (26) 首次将掩码扩散模型蒸馏为单步生成器；Curiosity-Diffuser (92) 利用好奇心引导扩散模型提高机器人策略的可靠性。\n*   **代码与数学推理：** BigO(Bench) (44) 评估 LLM 控制代码时空复杂度的能力；MASS (82) 利用技能图谱选择数学数据优化 LLM 预训练；MetaLadder (86) 通过类比问题推理迁移提升数学解题质量。\n*   **AI 安全与可信度：** 多篇论文关注 LLM 和 MLLM 的安全性，如 DeepSeek 安全评估 (65)、DeepFake 检测新范式 TruthLens (38)、缓解 MLLM 对象幻觉的 MFP (85)、以及机器反学习方法 DeepCUT (84)。\n\n**接下来，我们详细看看这些有趣的论文：**\n\n---\n\n**LLM、多模态、评估与对齐**\n\n1.  **LLaVA-MORE: LLM 和视觉骨干网络在增强视觉指令微调中的比较研究 (LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning)**\n    *   作者：Federico Cocchi, Nicholas Moratelli, et al.\n    *   摘要：本文介绍了 LLaVA-MORE 系列多模态大模型 (MLLM)，系统比较了不同规模的 LLM (Phi-4, LLaMA-3.1, Gemma-2) 和多种视觉编码器 (CLIP, DINOv2, SigLIP 等) 对 MLLM 性能的影响。研究采用统一训练协议进行公平比较，并探讨了图像分辨率、预训练数据等因素，为设计更有效的 MLLM 提供了见解和可复现的评估框架。\n    *   链接：[https://arxiv.org/abs/2503.12851](https://arxiv.org/abs/2503.12851)\n    *   项目页：[https://github.com/aimagelab/LLaVA-MORE](https://github.com/aimagelab/LLaVA-MORE)\n\n2.  **TULIP: 迈向统一的语言-图像预训练 (TULIP: Towards Unified Language-Image Pretraining)**\n    *   作者：Zineng Tang, Long Lian, et al.\n    *   摘要：针对 CLIP 类模型在细粒度视觉任务上的不足，本文提出 TULIP 方法，通过生成式数据增强、增强的图像/文本对比学习和重建正则化，学习细粒度视觉特征同时保持全局语义对齐。该方法在 ImageNet-1K 零样本分类、少样本分类和视觉语言任务上取得 SOTA 性能。\n    *   链接：[https://arxiv.org/abs/2503.12831](https://arxiv.org/abs/2503.12831)\n    *   项目页：[https://tulip-berkeley.github.io](https://tulip-berkeley.github.io/)\n\n3.  **ContextualJudgeBench: 评估上下文环境中基于 LLM 的评判器 (ContextualJudgeBench: Evaluating LLM-based Judges in Contextual Settings)**\n    *   作者：Austin Xu, Srijan Bansal, et al.\n    *   摘要：现有 LLM 评判器通常在非上下文场景评估，忽略了 RAG、摘要等常见上下文应用。本文提出 ContextualJudgeBench，一个包含 2000 个挑战性响应对的基准，专门评估 LLM 在上下文环境中的评判能力。研究发现，即使是 SOTA 模型（如 OpenAI o1）在处理上下文信息和条件评估标准时也面临巨大挑战，一致性准确率仅勉强达到 55%。\n    *   链接：[https://arxiv.org/abs/2503.12847](https://arxiv.org/abs/2503.12847)\n\n4.  **价值档案：编码人类差异性 (Value Profiles for Encoding Human Variation)**\n    *   作者：Taylor Sorensen, Pushkar Mishra, et al.\n    *   摘要：本文提出使用“价值档案”（Value Profiles），即从上下文演示中压缩出的自然语言价值观描述，来表示个体差异。结合可控解码器模型，可以根据价值档案预测评分。研究发现价值档案能有效压缩演示信息（>70%），并在可解释性、可控性方面优于人口统计学信息，为个性化、多元对齐和计算社会科学提供了新方法。\n    *   链接：[https://arxiv.org/abs/2503.12829](https://arxiv.org/abs/2503.12829)\n\n5.  **从百万用户到每个用户：扩展个性化偏好以实现用户级对齐 (From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment)**\n    *   作者：Jia-Nan Li, Jian Guan, et al.\n    *   摘要：为解决传统 LLM 对齐忽略用户偏好多样性的问题，本文提出了一个可扩展的个性化对齐框架。该框架定义了系统的偏好空间，并构建了 AlignX 数据集（超 130 万个性化偏好样本）。提出了上下文对齐和偏好桥接对齐两种方法，实验证明在多个基准上显著优于现有方法，并展现了对新偏好、有限用户数据和精确偏好控制的强大适应能力。\n    *   链接：[https://arxiv.org/abs/2503.12817](https://arxiv.org/abs/2503.12817)\n\n6.  **什么让奖励模型成为好老师？一个优化视角 (What Makes a Reward Model a Good Teacher? An Optimization Perspective)**\n    *   作者：Noam Razin, Zixuan Wang, et al.\n    *   摘要：本文从优化角度分析 RLHF 中奖励模型 (RM) 的有效性。研究证明，无论 RM 多准确，如果它引起的奖励方差低，RLHF 目标函数就会变得平坦，导致优化极其缓慢。即使是完美的 RM 也可能因低方差而表现不佳。这表明仅凭准确性或脱离具体 LLM 来评估 RM 存在局限性，足够的奖励方差对高效优化至关重要。\n    *   链接：[https://arxiv.org/abs/2503.12825](https://arxiv.org/abs/2503.12825)\n    *   代码：[https://github.com/princeton-pli/what-makes-good-rm](https://github.com/princeton-pli/what-makes-good-rm)\n\n7.  **MAMM-Refine: 通过多智能体协作提高生成忠实度的秘诀 (MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration)**\n    *   作者：David Wan, Justin Chih-Yao Chen, et al.\n    *   摘要：本文将多智能体多模型推理扩展到长文本生成任务（如摘要、问答）的忠实度提升。研究发现，多智能体（多实例）和多模型（不同 LLM 类型）协作有助于错误检测和批判，将批判和修正视为重排序任务能提高多智能体性能。最终提出的 MAMM-Refine 方法在摘要和长问答任务上显著提升了忠实度。\n    *   链接：[https://arxiv.org/abs/2503.12682](https://arxiv.org/abs/2503.12682)\n    *   代码：[https://github.com/meetdavidwan/mammrefine](https://github.com/meetdavidwan/mammrefine)\n\n8.  **MMDT: 解码多模态基础模型的可靠性与安全性 (MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models)**\n    *   作者：Chejian Xu, Jiawei Zhang, et al.\n    *   摘要：本文提出首个统一的多模态基础模型 (MMFM) 安全与可信度评估平台 MMDT。该平台从安全性、幻觉、公平性/偏见、隐私、对抗鲁棒性和分布外泛化等多个维度，设计了多种评估场景和红队算法，生成高质量基准。对多种 MMFM 的评估揭示了它们在这些方面的漏洞和改进空间。\n    *   链接：[https://arxiv.org/abs/2503.13015](https://arxiv.org/abs/2503.13015)\n    *   项目页：[https://mmdecodingtrust.github.io/](https://mmdecodingtrust.github.io/)\n\n**LLM 应用与探索**\n\n9.  **R$^2$: 基于 LLM 和因果情节图的小说到剧本生成框架 (R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs)**\n    *   作者：Zefeng Lin, Yi Xiao, et al.\n    *   摘要：本文提出 R$^2$ 框架，利用 LLM 将小说自动改编为剧本。为解决 LLM 幻觉和情节因果性提取挑战，提出了幻觉感知修正 (HAR) 和基于贪心破圈算法的因果情节图构建 (CPC) 方法。R$^2$ 包含 Reader 和 Rewriter 模块，模拟人类改编过程，实验证明其性能远超现有方法。\n    *   链接：[https://arxiv.org/abs/2503.12887](https://arxiv.org/abs/2503.12887)\n\n10. **BigO(Bench) -- LLM 能生成具有受控时空复杂度的代码吗？ (BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?)**\n    *   作者：Pierre Chambon, Baptiste Roziere, et al.\n    *   摘要：本文介绍 BigO(Bench)，一个新颖的编码基准，旨在评估生成式语言模型理解和生成具有指定时空复杂度代码的能力。该基准包含推断 Python 函数复杂度的工具，以及大量带复杂度标签的编码问题和解决方案。对多个 SOTA 模型的评估揭示了它们在处理复杂度约束方面的优缺点。\n    *   链接：[https://arxiv.org/abs/2503.12664](https://arxiv.org/abs/2503.12664)\n\n11. **MASS: 通过技能图谱进行数学数据选择以预训练大语言模型 (MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models)**\n    *   作者：Jiazheng Li, Lu Yu, et al.\n    *   摘要：针对数学推理领域 LLM 预训练的数据选择问题，本文提出 MASS 框架。通过构建数学技能图谱，为目标数据集打分，选择高质量子集进行预训练。实验表明，MASS 能显著提升预训练效率（减少 50%-70% tokens）和效果（同等 tokens 下性能提升 3.3%-5.9%）。\n    *   链接：[https://arxiv.org/abs/2503.13072](https://arxiv.org/abs/2503.13072)\n\n12. **MetaLadder: 通过类比问题推理迁移提升数学解题质量 (MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer)**\n    *   作者：Honglin Lin, Zhuoshi Pan, et al.\n    *   摘要：受人类通过类比解决问题启发，本文提出 MetaLadder 框架。该框架显式提示 LLM 在解决目标问题前，先回忆和反思结构或语义相似的元问题及其 CoT 解答。结合问题重述机制，模型能实现类比推理迁移。实验证明 MetaLadder 显著提升了 LLM 数学解题准确率，优于标准 CoT 方法。\n    *   链接：[https://arxiv.org/abs/2503.13060](https://arxiv.org/abs/2503.13060)\n    *   代码：[https://github.com/LHL3341/MetaLadder](https://github.com/LHL3341/MetaLadder)\n\n13. **探索大语言模型用于文字游戏：谁是卧底？ (Exploring Large Language Models for Word Games: Who is the Spy?)**\n    *   作者：Chentian Wei, Jiewei Chen, Jinzhu Xu\n    *   摘要：本文以“谁是卧底”游戏为例，探索 LLM 在文字游戏中的应用，并提出了一个基于思维链 (CoT) 的免训练调度框架。该框架使 LLM 能够在推断词语、伪装身份等任务中表现出色。实验证明了框架的有效性，提升了 LLM 在多个数据集上的性能。\n    *   链接：[https://arxiv.org/abs/2503.12658](https://arxiv.org/abs/2503.12658)\n    *   代码：[https://github.com/ct-wei/Who-is-The-Spy](https://github.com/ct-wei/Who-is-The-Spy)\n\n14. **ECLAIR: 交互式响应的增强澄清框架 (ECLAIR: Enhanced Clarification for Interactive Responses)**\n    *   作者：John Murzaku, Zifan Liu, et al.\n    *   摘要：本文提出 ECLAIR，一个用于企业 AI 助手中交互式消除歧义的统一端到端框架。ECLAIR 能为模糊的用户查询生成澄清问题，并根据用户响应解决歧义。它能集成来自多个下游代理的歧义信息，增强上下文感知能力。实验表明 ECLAIR 在澄清问题生成和歧义解决方面优于 few-shot prompting 技术。\n    *   链接：[https://arxiv.org/abs/2503.13020](https://arxiv.org/abs/2503.13020)\n\n**多模态、视觉与生成**\n\n15. **VPP-LLaVA: 基于 MLLM 视觉定位的视觉位置提示 (Visual Position Prompt for MLLM based Visual Grounding)**\n    *   作者：Wei Tang, Yanpeng Sun, et al.\n    *   摘要：为解决 MLLM 在视觉定位任务中坐标对齐不精确的问题，本文提出 VPP-LLaVA，引入视觉位置提示 (VPP)。全局 VPP 提供结构化空间线索，局部 VPP 关注细粒度定位。同时构建了 VPP-SFT 数据集。实验表明，该方法在标准定位基准上取得 SOTA 效果，且训练数据量远少于 MiniGPT-v2 等模型。\n    *   链接：[https://arxiv.org/abs/2503.12791](https://arxiv.org/abs/2503.12791)\n    *   代码与数据待发布。\n\n16. **通过可解释视觉概念进行表征相似性分析 (Representational Similarity via Interpretable Visual Concepts)**\n    *   作者：Neehar Kondapaneni, Oisin Mac Aodha, Pietro Perona\n    *   摘要：本文提出一种可解释的表征相似性方法 RSVC，用于比较两个深度神经网络。RSVC 可以发现模型间共享和独特的视觉概念，揭示模型差异的部分原因在于独特概念的存在。通过跨不同视觉模型架构和训练协议的广泛评估验证了其有效性。\n    *   链接：[https://arxiv.org/abs/2503.12901](https://arxiv.org/abs/2503.12901)\n\n17. **UI-Vision: 面向视觉感知与交互的桌面中心 GUI 基准 (UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction)**\n    *   作者：Shravan Nayak, Xiangru Jian, et al.\n    *   摘要：本文推出 UI-Vision，首个全面的、许可宽松的、用于离线细粒度评估真实桌面环境中计算机使用智能体的基准。包含 83 个软件应用中人类演示的高质量标注（边界框、UI 标签、动作轨迹等），并定义了元素定位、布局定位和动作预测三个任务及评估指标。评估揭示了 SOTA 模型在专业软件理解、空间推理和复杂动作方面的局限性。\n    *   链接：[https://arxiv.org/abs/2503.12899](https://arxiv.org/abs/2503.12899)\n\n18. **Di[M]O: 将掩码扩散模型蒸馏为单步生成器 (Di[M]O: Distilling Masked Diffusion Models into One-step Generator)**\n    *   作者：Yuanzhi Zhu, Xi Wang, et al.\n    *   摘要：为解决掩码扩散模型 (MDM) 推理速度慢的问题，本文提出 Di[M]O，首次将 MDM 蒸馏为单步生成器。通过令牌级分布匹配和令牌初始化策略，解决了单步生成中利用中间步信息困难和初始分布熵不足的问题。在图像生成任务上，Di[M]O 性能接近多步教师模型，但推理速度大幅提升。\n    *   链接：[https://arxiv.org/abs/2503.12813](https://arxiv.org/abs/2503.12813)\n\n19. **POSTA: 定制化艺术海报生成的 Go-to 框架 (POSTA: A Go-to Framework for Customized Artistic Poster Generation)**\n    *   作者：Haoyu Chen, Xiaojie Xu, et al.\n    *   摘要：针对现有海报生成方法缺乏文本准确性、用户定制化和美观性的问题，本文提出 POSTA 框架。该框架利用扩散模型和 MLLM，包含背景生成、布局与排版设计、艺术文本风格化三个模块，允许完全定制。同时构建了 PosterArt 数据集。实验证明 POSTA 在可控性、设计多样性、文本准确性和美学质量上优于现有模型。\n    *   链接：[https://arxiv.org/abs/2503.13078](https://arxiv.org/abs/2503.13078)\n\n20. **通过多频扰动缓解 MLLM 中的对象幻觉 (Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations)**\n    *   作者：Shuo Li, Jiajun Sun, et al.\n    *   摘要：本文发现 MLLM 对象幻觉的一个关键原因是模型对特定图像频率特征过度敏感。提出 MFP 方法，利用图像的低频和高频特征扰动视觉表示，在推理时显式抑制冗余频域特征。该方法简单、低成本、可插拔，实验证明能显著缓解多种模型架构的对象幻觉，并可与推理时方法结合达到 SOTA。\n    *   链接：[https://arxiv.org/abs/2503.13063](https://arxiv.org/abs/2503.13063)\n\n**强化学习与机器人**\n\n21. **奖励训练轮：用于机器人强化学习的自适应辅助奖励 (Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning)**\n    *   作者：Linji Wang, Tong Xu, et al.\n    *   摘要：为解决机器人 RL 中辅助奖励工程量大、可能引入偏见且无法自适应的问题，本文提出 Reward Training Wheels (RTW) 师生框架。RTW 教师根据学生能力动态调整辅助奖励权重。在模拟和真实机器人导航、越野移动任务中，RTW 优于专家设计的奖励，提高了成功率、性能和训练效率。\n    *   链接：[https://arxiv.org/abs/2503.13010](https://arxiv.org/abs/2503.13010)\n\n22. **机器人中基于大语言模型的安全感知任务规划 (Safety Aware Task Planning via Large Language Models in Robotics)**\n    *   作者：Azal Ahmad Khan, Michael Andrev, et al.\n    *   摘要：为解决 LLM 驱动的机器人规划中安全保障不足的问题，本文提出 SAFER 框架。该框架包含一个与主规划器并行的安全智能体提供反馈，并引入 LLM-as-a-Judge 指标量化计划中的安全违规。SAFER 在执行的多个阶段集成安全反馈，并结合控制屏障函数 (CBF) 确保安全。实验证明 SAFER 能有效减少安全违规同时保持任务效率。\n    *   链接：[https://arxiv.org/abs/2503.12917](https://arxiv.org/abs/2503.12917)\n\n23. **PEnGUiN: 用于样本高效 MARL 的部分等变图神经网络 (PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL)**\n    *   作者：Joshua McClellan, Greyson Brothers, et al.\n    *   摘要：针对现实世界中多智能体环境常存在不对称性，导致等变 GNN (EGNN) 效果受限的问题，本文提出 PEnGUiN。该架构能处理子群、特征、区域、近似等变性，理论上能统一学习完全等变和非等变表示。实验证明 PEnGUiN 在不对称环境中优于 EGNN 和标准 GNN。\n    *   链接：[https://arxiv.org/abs/2503.12839](https://arxiv.org/abs/2503.12839)\n\n24. **在真实世界中学习弹钢琴 (Learning to Play Piano in the Real World)**\n    *   作者：Yves-Simon Zeulner, Sandeep Selvaraj, Roberto Calandra\n    *   摘要：本文开发了首个在真实灵巧机器人上部署学习方法的钢琴弹奏系统。利用 Sim2Real，在模拟中使用 RL 训练策略，然后部署到真实机器人。实验评估了域随机化和动力学模型准确性的相互作用，以及策略在不同复杂度乐曲上的泛化能力。\n    *   链接：[https://arxiv.org/abs/2503.12827](https://arxiv.org/abs/2503.12827)\n    *   项目页：[https://lasr.org/research/learning-to-play-piano](https://lasr.org/research/learning-to-play-piano)\n\n25. **GraspCorrect: 通过视觉语言模型引导反馈进行机器人抓取修正 (GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback)**\n    *   作者：Sungjae Lee, Yeonjoo Hong, Kwang In Kim\n    *   摘要：为解决机器人抓取不稳定的问题，本文提出即插即用的 GraspCorrect 模块。该模块利用 VLM 引导的反馈，通过迭代式视觉问答框架（包含抓取引导提示和对象感知采样）生成中间视觉目标并转化为关节动作，显著提高了现有策略模型在 RLBench 和 CALVIN 数据集上的抓取稳定性和任务成功率。\n    *   链接：[https://arxiv.org/abs/2503.13142](https://arxiv.org/abs/2503.13142)\n\n26. **Curiosity-Diffuser: 好奇心引导扩散模型以提高可靠性 (Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability)**\n    *   作者：Zihao Liu, Xing Liu, et al.\n    *   摘要：为解决模仿学习策略可能产生幻觉导致行为不可靠的问题，本文提出 Curiosity-Diffuser。利用 RND 好奇心模块评估模型行为与训练数据的一致性，通过分类器引导扩散最小化好奇心，减少推理时的过度泛化。实验证明该方法能显著提高任务性能并产生更接近训练数据的行为。\n    *   链接：[https://arxiv.org/abs/2503.13034](https://arxiv.org/abs/2503.13034)\n    *   代码：[github.com/CarlDegio/Curiosity-Diffuser](http://github.com/CarlDegio/Curiosity-Diffuser)\n\n27. **1000 层网络用于自监督 RL：扩展深度可实现新的目标达成能力 (1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities)**\n    *   作者：Kevin Wang, Ishaan Javali, et al.\n    *   摘要：本文研究了自监督 RL 的可扩展性，发现将网络深度从通常的 2-5 层增加到 1024 层可以显著提升性能。在无监督目标条件设置下（无演示、无奖励），深度模型在模拟运动和操作任务上的性能提高了 2-50 倍，并学会了 qualitatively 不同的行为。\n    *   链接：[https://arxiv.org/abs/2503.13051](https://arxiv.org/abs/2503.13051)\n    *   项目页：[https://wang-kevin3290.github.io/scaling-crl/](https://wang-kevin3290.github.io/scaling-crl/)\n\n**AI 安全、可信度与理论**\n\n28. **DeepSeek 模型安全边界探索：评估与发现 (Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings)**\n    *   作者：Zonghao Ying, Guangyi Zheng, et al.\n    *   摘要：本文首次对 DeepSeek 模型（LLM、MLLM、文生图）进行了全面的安全评估，特别关注其生成内容的风险。开发了针对中国社会文化背景的双语安全评估数据集。结果表明，尽管 DeepSeek 模型通用能力强，但在算法歧视、性内容等多个风险维度上存在显著的安全漏洞。\n    *   链接：[https://arxiv.org/abs/2503.13184](https://arxiv.org/abs/2503.13184)\n    *   代码：[https://github.com/NY1024/DeepSeek-Safety-Eval](https://github.com/NY1024/DeepSeek-Safety-Eval)\n\n29. **TruthLens: 一种用于 DeepFake 检测的免训练范式 (TruthLens: A Training-Free Paradigm for DeepFake Detection)**\n    *   作者：Ritabrata Chakraborty, Rajatsubhra Chakraborty, et al.\n    *   摘要：为解决现有 DeepFake 检测方法缺乏可解释性的问题，本文提出 TruthLens 框架。将检测视为 VQA 任务，利用 LVLM 观察伪影，结合 LLM (如 GPT-4) 的推理能力进行分析决策。该方法不仅能分类真假图像，还能提供可解释的决策理由。\n    *   链接：[https://arxiv.org/abs/2503.12736](https://arxiv.org/abs/2503.12736)\n\n30. **面向语言模型的深度对比反学习 (Deep Contrastive Unlearning for Language Models)**\n    *   作者：Estrid He, Tabinda Sarwar, et al.\n    *   摘要：为实现机器反学习（从模型中移除特定训练样本信息），本文提出 DeepCUT 框架。该方法通过直接优化模型的潜在空间来实现反学习，而非仅关注输出。实验证明 DeepCUT 在真实数据集上有效且高效，显著优于基线方法。\n    *   链接：[https://arxiv.org/abs/2503.13069](https://arxiv.org/abs/2503.13069)\n\n31. **图神经网络泛化理论综述 (Survey on Generalization Theory for Graph Neural Networks)**\n    *   作者：Antonis Vasileiou, Stefanie Jegelka, et al.\n    *   摘要：本文系统回顾了关于消息传递图神经网络 (MPNN) 泛化能力的现有文献。分析了不同研究的优势和局限性，并指出了未来研究方向，旨在加深对 MPNN 泛化能力的理解。\n    *   链接：[https://arxiv.org/abs/2503.12880](https://arxiv.org/abs/2503.12880)\n\n**其他值得关注的论文**\n\n*   **(3) D&D 5E 战斗中 LLM 控制对抗者的强化学习环境:** 利用 LLM (GPT-4o, LLaMA 3) 控制对抗智能体，为小型 RL 智能体 (DQN) 提供更具挑战性的训练环境。\n*   **(6) 通过任务并行性预测多智能体专业化:** 提出当环境约束限制任务并行性时，专业化智能体团队优于通用型团队，并用 Overcooked-AI 实验验证。\n*   **(9) R$^2$: 基于 LLM 和因果情节图的小说到剧本生成框架:** 提出 R$^2$ 框架自动将小说改编为剧本，解决 LLM 幻觉和因果情节提取问题。\n*   **(10) 可靠的放射学骨骼肌面积评估:** 开发 SMAART-AI，一个基于深度学习的端到端自动化流程，用于 CT 图像中骨骼肌面积评估，辅助癌症恶病质诊断。\n*   **(12) AEJIM: 实时 AI 框架用于众包、透明、合乎道德的环境危害检测与报告:** 结合实时检测、众包验证和 AI 驱动报告，提升环境新闻报道的速度和准确性。\n*   **(17) CAM-Seg: 用于语义图像生成的连续值嵌入方法:** 提出使用连续值嵌入（而非量化嵌入）进行语义分割，提高了掩码生成精度和对分布变化的鲁棒性。\n*   **(23) EgoDTM: 面向 3D 感知的自我中心视频-语言预训练:** 提出 EgoDTM 模型，通过大规模 3D 感知视频预训练和视频-文本对比学习，联合学习 3D 感知能力。\n*   **(31) 时间正则化让你的视频生成器更强大:** 首次探索视频生成中的时间增强，提出 FluxFlow 策略，通过数据级时间扰动提高生成视频的时间一致性和多样性。\n*   **(35) 多模态 LLM 驱动流程在 EHR 数据高精度临床试验患者匹配中的真实世界验证:** 提出无需集成的 LLM 流程，利用视觉和推理能力处理 EHR 文档，自动化患者-试验匹配，显著提高效率。\n*   **(40) AI 构建系统动力学模型的能力如何？:** 引入技术正确性和指令遵循性两个指标，评估 LLM 生成因果图的能力，发现 GPT-4.5-preview 表现最佳。\n*   **(49) 使用 VLM、反应式规划器和行为树的机器人实时故障处理统一框架:** 结合 VLM、反应式规划器和行为树，实现机器人执行前验证和执行中故障检测与纠正。\n*   **(51) 通过原型感知视图转换为低分辨率查询进行 3D 占用预测:** 提出 ProtoOcc 网络，利用聚类图像片段的原型进行视图转换，以增强低分辨率查询下的 3D 占用预测质量。\n*   **(67) Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis:** 提出 Sorcen 统一 SSL 框架，结合对比-重构目标，利用生成的回声样本构建正样本对，无需额外图像裁剪，提升了效率和性能。\n*   **(70) GraspCorrect: 通过视觉语言模型引导反馈进行机器人抓取修正:** 提出 GraspCorrect 模块，利用 VLM 迭代反馈修正机器人抓取，提高稳定性和任务成功率。\n*   **(78) FAVOR-Bench: 细粒度视频运动理解综合基准:** 提出 FAVOR-Bench 基准和 FAVOR-Train 数据集，用于评估和提升 MLLM 的细粒度视频运动理解能力。\n*   **(79) Shushing! 让我们从无声视频中想象真实的语音:** 提出 ImaginTalk 跨模态扩散框架，仅用视觉输入在离散空间生成高保真度、富于表现力的语音。\n\n---\n\n希望这份 TLDR 快报能帮助您快速了解 arXiv 的最新动态！明天再见！",
  "papers": [
    {
      "arxiv_id": "2503.15739v1",
      "title": "ECLAIR: Enhanced Clarification for Interactive Responses",
      "title_zh": "ECLAIR：面向交互式响应的增强澄清框架",
      "authors": [
        "John Murzaku",
        "Zifan Liu",
        "Md Mehrab Tanjim",
        "Vaishnavi Muppala",
        "Xiang Chen",
        "Yunyao Li"
      ],
      "abstract": "We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel\nunified and end-to-end framework for interactive disambiguation in enterprise\nAI assistants. ECLAIR generates clarification questions for ambiguous user\nqueries and resolves ambiguity based on the user's response.We introduce a\ngeneralized architecture capable of integrating ambiguity information from\nmultiple downstream agents, enhancing context-awareness in resolving\nambiguities and allowing enterprise specific definition of agents. We further\ndefine agents within our system that provide domain-specific grounding\ninformation. We conduct experiments comparing ECLAIR to few-shot prompting\ntechniques and demonstrate ECLAIR's superior performance in clarification\nquestion generation and ambiguity resolution.",
      "tldr_zh": "该研究提出了ECLAIR框架，一种端到端的企业级AI助手交互消歧系统，能够针对模糊用户查询生成澄清问题并根据用户反馈解决歧义。该框架创新性地整合了多下游智能体的歧义信息，支持企业自定义智能体并融入领域特定的基础信息，显著提升了上下文感知能力。实验表明，ECLAIR在澄清问题生成和歧义消解任务上优于few-shot prompting技术，为企业AI助手提供了更强大的交互式消歧能力。",
      "categories": [
        "cs.AI",
        "68T50",
        "I.2.7; H.5.2"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15739v1",
      "published_date": "2025-03-19 23:04:00 UTC",
      "updated_date": "2025-03-19 23:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:36:24.052710"
    },
    {
      "arxiv_id": "2503.17403v1",
      "title": "ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models",
      "title_zh": "ChatGPT还是无处不在的沉默助手：大语言模型综述",
      "authors": [
        "Azim Akhtarshenas",
        "Afshin Dini",
        "Navid Ayoobi"
      ],
      "abstract": "Large Language Models (LLMs) have revo lutionized natural language processing\nNatural Language Processing (NLP), with Chat Generative Pre-trained Transformer\n(ChatGPT) standing out as a notable exampledue to its advanced capabilities and\nwidespread applications. This survey provides a comprehensive analysis of\nChatGPT, exploring its architecture, training processes, and functionalities.\nWe examine its integration into various domains across industries such as\ncustomer service, education, healthcare, and entertainment. A comparative\nanalysis with other LLMs highlights ChatGPT's unique features and performance\nmetrics. Regarding benchmarks, the paper examines ChatGPT's comparative\nperformance against other LLMs and discusses potential risks such as\nmisinformation, bias, and data privacy concerns. Additionally, we offer a\nnumber of figures and tables that outline the backdrop of the discussion, the\nmain ideas of the article, the numerous LLM models, a thorough list of datasets\nused for pre-training, fine-tuning, and evaluation, as well as particular LLM\napplications with pertinent references. Finally, we identify future research\ndirections and technological advancements, underscoring the evolving landscape\nof LLMs and their profound impact on artificial intelligence Artificial\nIntelligence (AI) and society.",
      "tldr_zh": "这篇综述全面分析了以ChatGPT为代表的大语言模型（LLMs），探讨了其架构、训练流程和在客服、教育、医疗等领域的应用。研究通过与其他LLMs的对比，突出了ChatGPT的独特性能指标，同时指出了其潜在风险如错误信息、偏见和数据隐私问题。文章还系统梳理了LLM相关数据集、评估基准及具体应用案例，最后展望了该技术未来的发展方向及其对人工智能与社会的深远影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17403v1",
      "published_date": "2025-03-19 22:55:08 UTC",
      "updated_date": "2025-03-19 22:55:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:36:45.197990"
    },
    {
      "arxiv_id": "2503.15726v1",
      "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
      "title_zh": "基于LLM控制敌对玩家的《龙与地下城5版》战斗强化学习环境",
      "authors": [
        "Joseph Emmanuel DL Dayo",
        "Michel Onasis S. Ogbinar",
        "Prospero C. Naval Jr"
      ],
      "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
      "tldr_zh": "本研究设计了一个基于《龙与地下城》第五版（D&D 5E）战斗场景的强化学习（RL）环境，其中引入了由GPT-4o和LLaMA 3 8B等大型语言模型（LLMs）控制的强大对手来挑战小型RL智能体。研究采用深度Q网络（DQN）训练小型智能体，并通过整合LLMs提升战略决策能力。实验结果表明，尽管RL智能体在标准指标上优于LLM控制的对手，但LLMs提供的战略深度显著增强了AI在复杂规则环境中的整体能力。该研究为开发适应性AI系统和创新互动模拟提供了重要见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Submitted to the 31st International Conference on Neural\n  Information Processing (ICONIP 2024)",
      "pdf_url": "http://arxiv.org/pdf/2503.15726v1",
      "published_date": "2025-03-19 22:48:20 UTC",
      "updated_date": "2025-03-19 22:48:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:37:02.747102"
    },
    {
      "arxiv_id": "2503.15724v1",
      "title": "Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning",
      "title_zh": "奖励训练辅助轮：面向机器人强化学习的自适应辅助奖励机制",
      "authors": [
        "Linji Wang",
        "Tong Xu",
        "Yuanjie Lu",
        "Xuesu Xiao"
      ],
      "abstract": "Robotics Reinforcement Learning (RL) often relies on carefully engineered\nauxiliary rewards to supplement sparse primary learning objectives to\ncompensate for the lack of large-scale, real-world, trial-and-error data. While\nthese auxiliary rewards accelerate learning, they require significant\nengineering effort, may introduce human biases, and cannot adapt to the robot's\nevolving capabilities during training. In this paper, we introduce Reward\nTraining Wheels (RTW), a teacher-student framework that automates auxiliary\nreward adaptation for robotics RL. To be specific, the RTW teacher dynamically\nadjusts auxiliary reward weights based on the student's evolving capabilities\nto determine which auxiliary reward aspects require more or less emphasis to\nimprove the primary objective. We demonstrate RTW on two challenging robot\ntasks: navigation in highly constrained spaces and off-road vehicle mobility on\nvertically challenging terrain. In simulation, RTW outperforms expert-designed\nrewards by 2.35% in navigation success rate and improves off-road mobility\nperformance by 122.62%, while achieving 35% and 3X faster training efficiency,\nrespectively. Physical robot experiments further validate RTW's effectiveness,\nachieving a perfect success rate (5/5 trials vs. 2/5 for expert-designed\nrewards) and improving vehicle stability with up to 47.4% reduction in\norientation angles.",
      "tldr_zh": "本文提出了一种名为Reward Training Wheels (RTW)的师生框架，用于自动化机器人强化学习(RL)中的辅助奖励调整。RTW通过动态调整辅助奖励的权重，根据机器人在训练中的能力变化，优化主要目标的实现。实验表明，RTW在受限空间导航和越野地形移动任务中，分别比专家设计的奖励策略提高了2.35%的成功率和122.62%的性能，同时显著提升了训练效率。真实机器人实验进一步验证了RTW的有效性，成功率达到100%，并显著提高了车辆稳定性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15724v1",
      "published_date": "2025-03-19 22:45:59 UTC",
      "updated_date": "2025-03-19 22:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:37:22.182807"
    },
    {
      "arxiv_id": "2503.15707v1",
      "title": "Safety Aware Task Planning via Large Language Models in Robotics",
      "title_zh": "机器人学中基于大语言模型的安全感知任务规划",
      "authors": [
        "Azal Ahmad Khan",
        "Michael Andrev",
        "Muhammad Ali Murtaza",
        "Sergio Aguilera",
        "Rui Zhang",
        "Jie Ding",
        "Seth Hutchinson",
        "Ali Anwar"
      ],
      "abstract": "The integration of large language models (LLMs) into robotic task planning\nhas unlocked better reasoning capabilities for complex, long-horizon workflows.\nHowever, ensuring safety in LLM-driven plans remains a critical challenge, as\nthese models often prioritize task completion over risk mitigation. This paper\nintroduces SAFER (Safety-Aware Framework for Execution in Robotics), a\nmulti-LLM framework designed to embed safety awareness into robotic task\nplanning. SAFER employs a Safety Agent that operates alongside the primary task\nplanner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge,\na novel metric leveraging LLMs as evaluators to quantify safety violations\nwithin generated task plans. Our framework integrates safety feedback at\nmultiple stages of execution, enabling real-time risk assessment, proactive\nerror correction, and transparent safety evaluation. We also integrate a\ncontrol framework using Control Barrier Functions (CBFs) to ensure safety\nguarantees within SAFER's task planning. We evaluated SAFER against\nstate-of-the-art LLM planners on complex long-horizon tasks involving\nheterogeneous robotic agents, demonstrating its effectiveness in reducing\nsafety violations while maintaining task efficiency. We also verify the task\nplanner and safety planner through actual hardware experiments involving\nmultiple robots and a human.",
      "tldr_zh": "本文提出SAFER框架，通过多LLM协同机制将安全考量嵌入机器人任务规划。该框架创新性地采用Safety Agent进行实时安全评估，并引入LLM-as-a-Judge量化安全违规，结合Control Barrier Functions(CBFs)控制层保障安全。实验表明，SAFER在异构机器人长时程任务中能显著减少安全违规，同时保持任务效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15707v1",
      "published_date": "2025-03-19 21:41:10 UTC",
      "updated_date": "2025-03-19 21:41:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:41:00.823019"
    },
    {
      "arxiv_id": "2503.15703v1",
      "title": "Predicting Multi-Agent Specialization via Task Parallelizability",
      "title_zh": "预测多智能体专业化：基于任务可并行性的分析",
      "authors": [
        "Elizabeth Mieczkowski",
        "Ruaridh Mon-Williams",
        "Neil Bramley",
        "Christopher G. Lucas",
        "Natalia Velez",
        "Thomas L. Griffiths"
      ],
      "abstract": "Multi-agent systems often rely on specialized agents with distinct roles\nrather than general-purpose agents that perform the entire task independently.\nHowever, the conditions that govern the optimal degree of specialization remain\npoorly understood. In this work, we propose that specialist teams outperform\ngeneralist ones when environmental constraints limit task parallelizability --\nthe potential to execute task components concurrently. Drawing inspiration from\ndistributed systems, we introduce a heuristic to predict the relative\nefficiency of generalist versus specialist teams by estimating the speed-up\nachieved when two agents perform a task in parallel rather than focus on\ncomplementary subtasks. We validate this heuristic through three multi-agent\nreinforcement learning (MARL) experiments in Overcooked-AI, demonstrating that\nkey factors limiting task parallelizability influence specialization. We also\nobserve that as the state space expands, agents tend to converge on specialist\nstrategies, even when generalist ones are theoretically more efficient,\nhighlighting potential biases in MARL training algorithms. Our findings provide\na principled framework for interpreting specialization given the task and\nenvironment, and introduce a novel benchmark for evaluating whether MARL finds\noptimal strategies.",
      "tldr_zh": "该研究提出了一种基于任务并行性的多智能体专业化预测框架，指出当环境约束限制任务的并行性时，专业化团队优于通用团队。研究借鉴分布式系统思想，引入了一种启发式方法，通过估计并行执行任务时的加速比来预测通用团队与专业化团队的相对效率。通过在Overcooked-AI中的多智能体强化学习(MARL)实验验证，发现限制任务并行性的关键因素会影响专业化程度。研究还观察到，随着状态空间扩大，智能体倾向于收敛于专业化策略，即使通用策略理论上更高效，这揭示了MARL训练算法中的潜在偏差。该研究为解释任务和环境驱动的专业化提供了理论框架，并引入了评估MARL是否找到最优策略的新基准。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15703v1",
      "published_date": "2025-03-19 21:33:48 UTC",
      "updated_date": "2025-03-19 21:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:38:01.116248"
    },
    {
      "arxiv_id": "2503.15699v1",
      "title": "Representational Similarity via Interpretable Visual Concepts",
      "title_zh": "可解释视觉概念驱动的表征相似性度量",
      "authors": [
        "Neehar Kondapaneni",
        "Oisin Mac Aodha",
        "Pietro Perona"
      ],
      "abstract": "How do two deep neural networks differ in how they arrive at a decision?\nMeasuring the similarity of deep networks has been a long-standing open\nquestion. Most existing methods provide a single number to measure the\nsimilarity of two networks at a given layer, but give no insight into what\nmakes them similar or dissimilar. We introduce an interpretable\nrepresentational similarity method (RSVC) to compare two networks. We use RSVC\nto discover shared and unique visual concepts between two models. We show that\nsome aspects of model differences can be attributed to unique concepts\ndiscovered by one model that are not well represented in the other. Finally, we\nconduct extensive evaluation across different vision model architectures and\ntraining protocols to demonstrate its effectiveness.",
      "tldr_zh": "该研究提出了一种可解释的表征相似性方法(RSVC)，用于比较两个深度神经网络在决策过程中的差异。与现有仅提供单一相似度数值的方法不同，RSVC能够发现两个模型之间共享及独有的视觉概念(visual concepts)。研究表明，模型间的部分差异可归因于某一模型发现的独特概念未被另一模型充分表征。该方法通过在不同视觉模型架构和训练协议上的广泛评估验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "32 pages, 5 Figures, 16 Supplemental Figures, ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15699v1",
      "published_date": "2025-03-19 21:21:45 UTC",
      "updated_date": "2025-03-19 21:21:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:38:20.194386"
    },
    {
      "arxiv_id": "2503.15661v1",
      "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction",
      "title_zh": "UI-Vision：面向视觉感知与交互的桌面中心图形用户界面基准",
      "authors": [
        "Shravan Nayak",
        "Xiangru Jian",
        "Kevin Qinghong Lin",
        "Juan A. Rodriguez",
        "Montek Kalsi",
        "Rabiul Awal",
        "Nicolas Chapados",
        "M. Tamer Özsu",
        "Aishwarya Agrawal",
        "David Vazquez",
        "Christopher Pal",
        "Perouz Taslakian",
        "Spandana Gella",
        "Sai Rajeswar"
      ],
      "abstract": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.",
      "tldr_zh": "该研究提出了UI-Vision，首个面向真实桌面环境的综合性GUI基准测试平台，填补了离线场景下计算机使用智能体评估的空白。平台包含83个软件应用的高质量人工标注数据（包括UI元素边界框、标签和操作轨迹），并设计了从细粒度到粗粒度的三项核心任务：元素定位、布局理解和动作预测。评估发现当前最先进模型（如UI-TARS-72B）在专业软件理解、空间推理和拖拽等复杂操作上存在显著缺陷。该开源基准为开发实用的桌面自动化智能体提供了关键测试框架，揭示了实现完全自主计算机操作代理面临的核心挑战。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15661v1",
      "published_date": "2025-03-19 19:26:17 UTC",
      "updated_date": "2025-03-19 19:26:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:38:50.190595"
    },
    {
      "arxiv_id": "2503.15655v1",
      "title": "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs",
      "title_zh": "R²：基于因果情节图的大语言模型小说到剧本生成框架",
      "authors": [
        "Zefeng Lin",
        "Yi Xiao",
        "Zhiqiang Mo",
        "Qifan Zhang",
        "Jie Wang",
        "Jiayang Chen",
        "Jiajing Zhang",
        "Hui Zhang",
        "Zhengyi Liu",
        "Xianyong Fang",
        "Xiaohua Xu"
      ],
      "abstract": "Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.",
      "tldr_zh": "该研究提出R²框架，一种基于大语言模型(LLM)的小说转剧本生成方法，通过因果情节图(Causal Plot Graphs)解决两大核心挑战。针对LLM的幻觉问题，框架采用幻觉感知精炼方法(HAR)进行迭代修正；针对情节因果性，开发基于贪心破环算法的因果图构建方法(CPC)。该系统模仿人类编剧流程，通过Reader模块构建因果情节图，Rewriter模块首先生成场景大纲再转化为剧本，实验表明R²在GPT-4o上相比现有方法取得51.3%、22.6%和57.1%的绝对性能提升。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15655v1",
      "published_date": "2025-03-19 19:09:40 UTC",
      "updated_date": "2025-03-19 19:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:39:07.391464"
    },
    {
      "arxiv_id": "2503.16556v1",
      "title": "Reliable Radiologic Skeletal Muscle Area Assessment -- A Biomarker for Cancer Cachexia Diagnosis",
      "title_zh": "可靠的放射学骨骼肌面积评估——癌症恶病质诊断的生物标志物",
      "authors": [
        "Sabeen Ahmed",
        "Nathan Parker",
        "Margaret Park",
        "Daniel Jeong",
        "Lauren Peres",
        "Evan W. Davis",
        "Jennifer B. Permuth",
        "Erin Siegel",
        "Matthew B. Schabath",
        "Yasin Yilmaz",
        "Ghulam Rasool"
      ],
      "abstract": "Cancer cachexia is a common metabolic disorder characterized by severe muscle\natrophy which is associated with poor prognosis and quality of life. Monitoring\nskeletal muscle area (SMA) longitudinally through computed tomography (CT)\nscans, an imaging modality routinely acquired in cancer care, is an effective\nway to identify and track this condition. However, existing tools often lack\nfull automation and exhibit inconsistent accuracy, limiting their potential for\nintegration into clinical workflows. To address these challenges, we developed\nSMAART-AI (Skeletal Muscle Assessment-Automated and Reliable Tool-based on AI),\nan end-to-end automated pipeline powered by deep learning models (nnU-Net 2D)\ntrained on mid-third lumbar level CT images with 5-fold cross-validation,\nensuring generalizability and robustness. SMAART-AI incorporates an\nuncertainty-based mechanism to flag high-error SMA predictions for expert\nreview, enhancing reliability. We combined the SMA, skeletal muscle index, BMI,\nand clinical data to train a multi-layer perceptron (MLP) model designed to\npredict cachexia at the time of cancer diagnosis. Tested on the\ngastroesophageal cancer dataset, SMAART-AI achieved a Dice score of 97.80% +/-\n0.93%, with SMA estimated across all four datasets in this study at a median\nabsolute error of 2.48% compared to manual annotations with SliceOmatic.\nUncertainty metrics-variance, entropy, and coefficient of variation-strongly\ncorrelated with SMA prediction errors (0.83, 0.76, and 0.73 respectively). The\nMLP model predicts cachexia with 79% precision, providing clinicians with a\nreliable tool for early diagnosis and intervention. By combining automation,\naccuracy, and uncertainty awareness, SMAART-AI bridges the gap between research\nand clinical application, offering a transformative approach to managing cancer\ncachexia.",
      "tldr_zh": "该研究开发了SMAART-AI，一种基于深度学习（nnU-Net 2D）的端到端自动化工具，用于通过CT图像评估骨骼肌面积（SMA），以诊断癌症恶病质。该工具结合不确定性机制，能够标记高误差预测以供专家审查，提高了可靠性。在胃食管癌数据集上测试，SMAART-AI的Dice评分达到97.80%，并通过多层感知器（MLP）模型预测恶病质的准确率为79%，为癌症恶病质的早期诊断和干预提供了可靠工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CE",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "47 pages, 19 figures, 9 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.16556v1",
      "published_date": "2025-03-19 19:07:59 UTC",
      "updated_date": "2025-03-19 19:07:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:39:34.228189"
    },
    {
      "arxiv_id": "2503.15650v1",
      "title": "Survey on Generalization Theory for Graph Neural Networks",
      "title_zh": "图神经网络泛化理论综述",
      "authors": [
        "Antonis Vasileiou",
        "Stefanie Jegelka",
        "Ron Levie",
        "Christopher Morris"
      ],
      "abstract": "Message-passing graph neural networks (MPNNs) have emerged as the leading\napproach for machine learning on graphs, attracting significant attention in\nrecent years. While a large set of works explored the expressivity of MPNNs,\ni.e., their ability to separate graphs and approximate functions over them,\ncomparatively less attention has been directed toward investigating their\ngeneralization abilities, i.e., making meaningful predictions beyond the\ntraining data. Here, we systematically review the existing literature on the\ngeneralization abilities of MPNNs. We analyze the strengths and limitations of\nvarious studies in these domains, providing insights into their methodologies\nand findings. Furthermore, we identify potential avenues for future research,\naiming to deepen our understanding of the generalization abilities of MPNNs.",
      "tldr_zh": "本文系统综述了消息传递图神经网络（MPNNs）的泛化理论研究现状。尽管MPNNs已成为图机器学习的主流方法，但现有研究多关注其表达能力（如区分图和逼近函数），而较少探讨其泛化性能（即在训练数据外的预测能力）。文章分析了该领域各类研究的方法论、成果及局限性，并指出了未来深化MPNNs泛化理论理解的潜在研究方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15650v1",
      "published_date": "2025-03-19 19:04:24 UTC",
      "updated_date": "2025-03-19 19:04:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:41:18.260291"
    },
    {
      "arxiv_id": "2503.17401v1",
      "title": "AEJIM: A Real-Time AI Framework for Crowdsourced, Transparent, and Ethical Environmental Hazard Detection and Reporting",
      "title_zh": "AEJIM：面向众包化、透明化与伦理化的环境危害实时检测与报告人工智能框架",
      "authors": [
        "Torsten Tiltack"
      ],
      "abstract": "Environmental journalism is vital for raising awareness of ecological crises\nand driving evidence-based policy, yet traditional methods falter under delays,\ninaccuracies, and scalability limits, especially in under-monitored regions\ncritical to the United Nations Sustainable Development Goals. To bridge these\ngaps, this paper introduces the AI-Environmental Journalism Integration Model\n(AEJIM), an innovative framework combining real-time hazard detection,\ncrowdsourced validation, and AI-driven reporting.\n  Validated through a pilot study, AEJIM significantly improved the speed and\naccuracy of environmental hazard reporting, outperforming traditional methods.\nFurthermore, the model directly addresses key ethical, regulatory, and\nscalability challenges, ensuring AI accountability through Explainable AI\n(XAI), GDPR-compliant data governance, and active public participation. AEJIM\nprovides a transparent and adaptable solution, setting a new benchmark for\nAI-enhanced environmental journalism and supporting informed global\ndecision-making across diverse socio-political landscapes.",
      "tldr_zh": "该研究提出了AEJIM框架，通过结合实时环境危害检测、众包验证和AI驱动的报告，解决了传统环境新闻在时效性、准确性和扩展性上的不足。该框架采用可解释AI（XAI）和符合GDPR的数据治理机制，在试点研究中显著提升了环境危害报告的速度和准确性，同时确保了AI系统的透明度和伦理合规性。AEJIM为AI增强型环境新闻设定了新标准，支持全球多样化的社会政治环境下的决策制定。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "J.4; I.2.10; I.2.7; H.3.4; H.5.2"
      ],
      "primary_category": "cs.CY",
      "comment": "21 pages, 10 figures, 5 tables. Keywords: Artificial Intelligence,\n  Environmental Journalism, Real-Time Reporting, Vision Transformers, Image\n  Recognition, Crowdsourced Validation, GPT-4, Automated News Generation, GIS\n  Integration, Data Privacy Compliance, Explainable AI (XAI), AI Ethics,\n  Sustainable Development",
      "pdf_url": "http://arxiv.org/pdf/2503.17401v1",
      "published_date": "2025-03-19 19:00:24 UTC",
      "updated_date": "2025-03-19 19:00:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:42:03.877018"
    },
    {
      "arxiv_id": "2503.15639v1",
      "title": "A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition",
      "title_zh": "一种基于上下文驱动的免训练轻量级场景文本分割与识别网络",
      "authors": [
        "Ritabrata Chakraborty",
        "Shivakumara Palaiahnakote",
        "Umapada Pal",
        "Cheng-Lin Liu"
      ],
      "abstract": "Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources.",
      "tldr_zh": "该研究提出了一种轻量级、无需训练的即插即用框架，用于场景文本分割与识别。该方法利用预训练文本识别器的优势，通过基于注意力的像素级分割和上下文理解来优化候选文本区域，避免传统端到端模型的冗余计算。实验表明，该框架在公开基准测试中达到先进性能，同时显著降低计算资源需求。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15639v1",
      "published_date": "2025-03-19 18:51:01 UTC",
      "updated_date": "2025-03-19 18:51:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:41:57.919830"
    },
    {
      "arxiv_id": "2503.15629v1",
      "title": "Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning",
      "title_zh": "基于自监督强化学习的神经Lyapunov函数逼近",
      "authors": [
        "Luc McCutcheon",
        "Bahman Gharesifard",
        "Saber Fallah"
      ],
      "abstract": "Control Lyapunov functions are traditionally used to design a controller\nwhich ensures convergence to a desired state, yet deriving these functions for\nnonlinear systems remains a complex challenge. This paper presents a novel,\nsample-efficient method for neural approximation of nonlinear Lyapunov\nfunctions, leveraging self-supervised Reinforcement Learning (RL) to enhance\ntraining data generation, particularly for inaccurately represented regions of\nthe state space. The proposed approach employs a data-driven World Model to\ntrain Lyapunov functions from off-policy trajectories. The method is validated\non both standard and goal-conditioned robotic tasks, demonstrating faster\nconvergence and higher approximation accuracy compared to the state-of-the-art\nneural Lyapunov approximation baseline. The code is available at:\nhttps://github.com/CAV-Research-Lab/SACLA.git",
      "tldr_zh": "该论文提出了一种基于自监督强化学习（RL）的神经网络Lyapunov函数近似方法，用于解决非线性系统控制中的Lyapunov函数推导难题。该方法利用数据驱动的World Model从离线策略轨迹中训练Lyapunov函数，并通过自监督RL增强状态空间欠表征区域的训练数据生成。实验表明，相比现有最优神经Lyapunov近似基线，该方法在标准机器人任务和目标条件任务上均实现了更快的收敛速度和更高的近似精度。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CG",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA)",
      "pdf_url": "http://arxiv.org/pdf/2503.15629v1",
      "published_date": "2025-03-19 18:29:25 UTC",
      "updated_date": "2025-03-19 18:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:42:21.807269"
    },
    {
      "arxiv_id": "2503.15621v1",
      "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning",
      "title_zh": "LLaVA-MORE：增强视觉指令调优中大型语言模型与视觉骨干架构的比较研究",
      "authors": [
        "Federico Cocchi",
        "Nicholas Moratelli",
        "Davide Caffagni",
        "Sara Sarto",
        "Lorenzo Baraldi",
        "Marcella Cornia",
        "Rita Cucchiara"
      ],
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.",
      "tldr_zh": "这项研究提出了LLaVA-MORE框架，系统比较了不同视觉主干网络和大语言模型(LLM)在视觉指令调优中的表现。研究采用统一训练协议，评估了Phi-4、LLaMA-3.1和Gemma-2等中小规模LLM，以及CLIP、DINOv2、SigLIP等多种视觉编码器的多模态推理能力。实验发现，模型性能并非单纯依赖参数量，而是取决于架构选择和预训练数据的匹配度。该研究不仅建立了可复现的评估框架，还为高效MLLM设计提供了关键见解，所有代码和模型均已开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15621v1",
      "published_date": "2025-03-19 18:10:12 UTC",
      "updated_date": "2025-03-19 18:10:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:43:16.697011"
    },
    {
      "arxiv_id": "2503.15620v1",
      "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings",
      "title_zh": "上下文重要吗？ContextualJudgeBench 用于评估基于大语言模型的上下文环境中的评判者",
      "authors": [
        "Austin Xu",
        "Srijan Bansal",
        "Yifei Ming",
        "Semih Yavuz",
        "Shafiq Joty"
      ],
      "abstract": "The large language model (LLM)-as-judge paradigm has been used to meet the\ndemand for a cheap, reliable, and fast evaluation of model outputs during AI\nsystem development and post-deployment monitoring. While judge models -- LLMs\nfinetuned to specialize in assessing and critiquing model outputs -- have been\ntouted as general purpose evaluators, they are typically evaluated only on\nnon-contextual scenarios, such as instruction following. The omission of\ncontextual settings -- those where external information is used as context to\ngenerate an output -- is surprising given the increasing prevalence of\nretrieval-augmented generation (RAG) and summarization use cases. Contextual\nassessment is uniquely challenging, as evaluation often depends on practitioner\npriorities, leading to conditional evaluation criteria (e.g., comparing\nresponses based on factuality and then considering completeness if they are\nequally factual). To address the gap, we propose ContextualJudgeBench, a judge\nbenchmark with 2,000 challenging response pairs across eight splits inspired by\nreal-world contextual evaluation scenarios. We build our benchmark with a\nmulti-pronged data construction pipeline that leverages both existing human\nannotations and model-based perturbations. Our comprehensive study across 11\njudge models and 9 general purpose models, reveals that the contextual\ninformation and its assessment criteria present a significant challenge to even\nstate-of-the-art models. For example, OpenAI's o1, the best-performing model,\nbarely reaches 55% consistent accuracy.",
      "tldr_zh": "该研究提出ContextualJudgeBench基准测试，专门评估基于LLM的评判模型在上下文环境中的表现。针对当前LLM评判系统主要测试非上下文场景的局限，该团队构建了包含2000个挑战性响应对的评测集，覆盖检索增强生成(RAG)和摘要等8类真实场景任务。实验发现，即使是性能最佳的OpenAI o1模型，在上下文相关评估中也仅达到55%的一致性准确率，凸显了现有模型在条件评估标准（如事实性优先、完整性次之）方面的显著不足。该研究填补了LLM评判系统在上下文评估领域的空白，为RAG等依赖上下文的AI应用提供了重要评测工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 13 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.15620v1",
      "published_date": "2025-03-19 18:09:19 UTC",
      "updated_date": "2025-03-19 18:09:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:43:04.674704"
    },
    {
      "arxiv_id": "2503.15617v1",
      "title": "CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation",
      "title_zh": "CAM-Seg：基于连续值嵌入的语义图像生成方法",
      "authors": [
        "Masud Ahmed",
        "Zahid Hasan",
        "Syed Arefinul Haque",
        "Abu Zaher Md Faridee",
        "Sanjay Purushotham",
        "Suya You",
        "Nirmalya Roy"
      ],
      "abstract": "Traditional transformer-based semantic segmentation relies on quantized\nembeddings. However, our analysis reveals that autoencoder accuracy on\nsegmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than\ncontinuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a\ncontinuous-valued embedding framework for semantic segmentation. By\nreformulating semantic mask generation as a continuous image-to-embedding\ndiffusion process, our approach eliminates the need for discrete latent\nrepresentations while preserving fine-grained spatial and semantic details. Our\nkey contribution includes a diffusion-guided autoregressive transformer that\nlearns a continuous semantic embedding space by modeling long-range\ndependencies in image features. Our framework contains a unified architecture\ncombining a VAE encoder for continuous feature extraction, a diffusion-guided\ntransformer for conditioned embedding generation, and a VAE decoder for\nsemantic mask reconstruction. Our setting facilitates zero-shot domain\nadaptation capabilities enabled by the continuity of the embedding space.\nExperiments across diverse datasets (e.g., Cityscapes and domain-shifted\nvariants) demonstrate state-of-the-art robustness to distribution shifts,\nincluding adverse weather (e.g., fog, snow) and viewpoint variations. Our model\nalso exhibits strong noise resilience, achieving robust performance ($\\approx$\n95% AP compared to baseline) under gaussian noise, moderate motion blur, and\nmoderate brightness/contrast variations, while experiencing only a moderate\nimpact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,\nsaturation and hue shifts. Code available:\nhttps://github.com/mahmed10/CAMSS.git",
      "tldr_zh": "本研究提出了一种基于连续值嵌入的语义图像生成框架CAM-Seg，解决了传统量化嵌入方法在语义分割中的性能瓶颈。通过将语义掩码生成重新定义为连续图像到嵌入的扩散过程，该框架避免了离散潜在表示，同时保留了精细的空间和语义细节。其核心贡献包括一个扩散引导的自回归transformer，能够建模图像特征中的长程依赖关系，以及一个结合VAE编码器、扩散引导transformer和VAE解码器的统一架构。实验表明，该框架在跨域适应、噪声鲁棒性和分布偏移（如恶劣天气和视角变化）方面表现出色，实现了最先进的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15617v1",
      "published_date": "2025-03-19 18:06:54 UTC",
      "updated_date": "2025-03-19 18:06:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:43:19.618812"
    },
    {
      "arxiv_id": "2503.15615v1",
      "title": "PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL",
      "title_zh": "PEnGUiN：面向样本高效多智能体强化学习的部分等变图神经网络",
      "authors": [
        "Joshua McClellan",
        "Greyson Brothers",
        "Furong Huang",
        "Pratap Tokekar"
      ],
      "abstract": "Equivariant Graph Neural Networks (EGNNs) have emerged as a promising\napproach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry\nguarantees to greatly improve sample efficiency and generalization. However,\nreal-world environments often exhibit inherent asymmetries arising from factors\nsuch as external forces, measurement inaccuracies, or intrinsic system biases.\nThis paper introduces \\textit{Partially Equivariant Graph NeUral Networks\n(PEnGUiN)}, a novel architecture specifically designed to address these\nchallenges. We formally identify and categorize various types of partial\nequivariance relevant to MARL, including subgroup equivariance, feature-wise\nequivariance, regional equivariance, and approximate equivariance. We\ntheoretically demonstrate that PEnGUiN is capable of learning both fully\nequivariant (EGNN) and non-equivariant (GNN) representations within a unified\nframework. Through extensive experiments on a range of MARL problems\nincorporating various asymmetries, we empirically validate the efficacy of\nPEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both\nEGNNs and standard GNNs in asymmetric environments, highlighting their\npotential to improve the robustness and applicability of graph-based MARL\nalgorithms in real-world scenarios.",
      "tldr_zh": "该论文提出了PEnGUiN（部分等变图神经网络），一种新型架构用于解决多智能体强化学习(MARL)中环境不对称性问题。该方法通过形式化定义子群等变性、特征级等变性、区域等变性和近似等变性等部分等变类型，在统一框架中同时学习全等变(EGNN)和非等变(GNN)表示。实验表明，在包含各种不对称性的MARL任务中，PEnGUiN显著优于传统EGNN和标准GNN，提高了图神经网络在现实场景中的鲁棒性和适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15615v1",
      "published_date": "2025-03-19 18:01:14 UTC",
      "updated_date": "2025-03-19 18:01:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:43:40.624349"
    },
    {
      "arxiv_id": "2503.15485v1",
      "title": "TULIP: Towards Unified Language-Image Pretraining",
      "title_zh": "TULIP：迈向统一语言-图像预训练",
      "authors": [
        "Zineng Tang",
        "Long Lian",
        "Seun Eisape",
        "XuDong Wang",
        "Roei Herzig",
        "Adam Yala",
        "Alane Suhr",
        "Trevor Darrell",
        "David M. Chan"
      ],
      "abstract": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
      "tldr_zh": "该研究提出了TULIP模型，旨在解决现有CLIP类模型在视觉中心任务（如计数、深度估计和细粒度识别）上的不足。通过生成式数据增强、增强的图像-图像/文本-文本对比学习以及重建正则化，TULIP在保持语义对齐的同时学习细粒度视觉特征。实验表明，该模型在多个基准测试中超越现有方法，包括在ImageNet-1K上实现新的零样本SOTA性能，在RxRx1上的少样本分类任务中准确率比SigLIP提高2倍，并在MMVP基准上获得3倍以上的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15485v1",
      "published_date": "2025-03-19 17:58:57 UTC",
      "updated_date": "2025-03-19 17:58:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:44:32.370673"
    },
    {
      "arxiv_id": "2503.15484v1",
      "title": "Value Profiles for Encoding Human Variation",
      "title_zh": "《价值档案：编码人类多样性的表征方法》",
      "authors": [
        "Taylor Sorensen",
        "Pushkar Mishra",
        "Roma Patel",
        "Michael Henry Tessler",
        "Michiel Bakker",
        "Georgina Evans",
        "Iason Gabriel",
        "Noah Goodman",
        "Verena Rieser"
      ],
      "abstract": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.",
      "tldr_zh": "这项研究提出了一种名为\"价值档案\"(value profiles)的自然语言描述方法，用于编码人类个体在评分任务中的差异性。该方法通过从上下文示例中压缩提取核心价值描述，配合可调控的解码器模型，能够比传统人口统计特征更有效地预测个体评分行为（保留超过70%的信息量）。研究采用信息论方法验证了价值档案在可解释性、可调控性方面的优势，并证明其能更好地解释评分者差异，同时保持模型校准性，为个性化AI系统和社会计算提供了新工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15484v1",
      "published_date": "2025-03-19 17:57:49 UTC",
      "updated_date": "2025-03-19 17:57:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:44:48.920999"
    },
    {
      "arxiv_id": "2503.15481v1",
      "title": "Learning to Play Piano in the Real World",
      "title_zh": "现实世界中的钢琴学习",
      "authors": [
        "Yves-Simon Zeulner",
        "Sandeep Selvaraj",
        "Roberto Calandra"
      ],
      "abstract": "Towards the grand challenge of achieving human-level manipulation in robots,\nplaying piano is a compelling testbed that requires strategic, precise, and\nflowing movements. Over the years, several works demonstrated hand-designed\ncontrollers on real world piano playing, while other works evaluated robot\nlearning approaches on simulated piano scenarios. In this paper, we develop the\nfirst piano playing robotic system that makes use of learning approaches while\nalso being deployed on a real world dexterous robot. Specifically, we make use\nof Sim2Real to train a policy in simulation using reinforcement learning before\ndeploying the learned policy on a real world dexterous robot. In our\nexperiments, we thoroughly evaluate the interplay between domain randomization\nand the accuracy of the dynamics model used in simulation. Moreover, we\nevaluate the robot's performance across multiple songs with varying complexity\nto study the generalization of our learned policy. By providing a\nproof-of-concept of learning to play piano in the real world, we want to\nencourage the community to adopt piano playing as a compelling benchmark\ntowards human-level manipulation. We open-source our code and show additional\nvideos at https://lasr.org/research/learning-to-play-piano .",
      "tldr_zh": "该论文首次实现了基于强化学习的钢琴演奏机器人系统，通过Sim2Real方法在仿真环境中训练策略后部署至真实灵巧机器人。研究重点探讨了领域随机化与动力学模型精度之间的相互作用，并在不同复杂度的曲目上测试了策略的泛化能力。这项工作将钢琴演奏确立为衡量类人操作能力的重要基准，同时开源了相关代码以推动该领域研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.15481v1",
      "published_date": "2025-03-19 17:56:14 UTC",
      "updated_date": "2025-03-19 17:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:45:07.566925"
    },
    {
      "arxiv_id": "2503.15477v1",
      "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
      "title_zh": "奖励模型何以成为良师？基于优化视角的探讨",
      "authors": [
        "Noam Razin",
        "Zixuan Wang",
        "Hubert Strauss",
        "Stanley Wei",
        "Jason D. Lee",
        "Sanjeev Arora"
      ],
      "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.",
      "tldr_zh": "这项研究从优化角度探讨了如何评估强化学习人类反馈(RLHF)中奖励模型的质量。研究发现，仅凭准确性无法全面衡量奖励模型的效能——即使完全准确的奖励模型，若产生的奖励方差过低，也会导致RLHF目标函数呈现平坦景观，从而造成优化效率低下。实验证明，奖励方差与准确性共同决定了策略优化的速度，且同一奖励模型对不同语言模型可能产生完全不同的优化效果。研究指出，优秀的奖励模型除了准确性外，还必须能产生足够的奖励方差才能实现高效优化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm",
      "pdf_url": "http://arxiv.org/pdf/2503.15477v1",
      "published_date": "2025-03-19 17:54:41 UTC",
      "updated_date": "2025-03-19 17:54:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:45:27.988813"
    },
    {
      "arxiv_id": "2503.15470v1",
      "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining",
      "title_zh": "EgoDTM：面向三维感知的第一人称视角视频-语言预训练",
      "authors": [
        "Boshen Xu",
        "Yuting Mei",
        "Xinbi Liu",
        "Sipeng Zheng",
        "Qin Jin"
      ],
      "abstract": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.",
      "tldr_zh": "本文提出EgoDTM模型，通过融合3D感知和视频-文本对比学习，突破传统1D文本或2D视觉线索的局限。该模型采用轻量级3D感知解码器，从深度估计模型生成的伪深度图中高效学习3D表征，并创新性地结合多个基础模型来增强原始视频描述的物体交互信息。实验表明，EgoDTM在多种下游任务中展现出卓越的3D感知视觉理解能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code will be released at: https://github.com/xuboshen/EgoDTM",
      "pdf_url": "http://arxiv.org/pdf/2503.15470v1",
      "published_date": "2025-03-19 17:45:56 UTC",
      "updated_date": "2025-03-19 17:45:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:45:48.174168"
    },
    {
      "arxiv_id": "2503.15469v2",
      "title": "Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional Context-Aware Representation Learning for Enhanced Text Classification",
      "title_zh": "动态双向Elman注意力网络（DBEAN）：面向增强文本分类的双向上下文感知表征学习",
      "authors": [
        "ZhengLin Lai",
        "MengYao Liao",
        "Dong Xu"
      ],
      "abstract": "Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency.",
      "tldr_zh": "本文提出动态双向Elman注意力网络(DBEAN)，通过结合双向时序建模与自注意力机制来解决文本分类任务中的挑战。该模型能动态分配输入关键片段的权重，在保持计算效率的同时增强上下文表征能力。实验表明，DBEAN在平衡可解释性、计算效率和长程上下文理解方面优于传统RNN和Transformer模型，为复杂语言结构和语义依赖的文本分类提供了新解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages,1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.15469v2",
      "published_date": "2025-03-19 17:45:13 UTC",
      "updated_date": "2025-03-20 10:09:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:46:16.416135"
    },
    {
      "arxiv_id": "2503.15463v2",
      "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment",
      "title_zh": "从百万用户到每位用户：用户级个性化偏好对齐的规模化实现",
      "authors": [
        "Jia-Nan Li",
        "Jian Guan",
        "Songhao Wu",
        "Wei Wu",
        "Rui Yan"
      ],
      "abstract": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
      "tldr_zh": "该研究提出了一种可扩展的大语言模型（LLM）个性化对齐框架，突破了传统\"一刀切\"式对齐方法的局限。通过构建包含心理和行为维度的系统化偏好空间，以及130万规模的高质量个性化偏好数据集AlignX，研究者开发了两种互补的对齐方法：基于上下文条件化的直接对齐和基于偏好分布建模的桥接对齐。实验表明，该方法在四项基准测试中平均准确率提升17.06%，展现出对新偏好的强适应能力、有限用户数据下的鲁棒性以及精确的偏好可控性，为实现真正用户自适应的AI系统迈出重要一步。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15463v2",
      "published_date": "2025-03-19 17:41:46 UTC",
      "updated_date": "2025-03-21 10:33:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:47:17.458988"
    },
    {
      "arxiv_id": "2503.15457v1",
      "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator",
      "title_zh": "Di$\\mathtt{[M]}$O：将掩码扩散模型蒸馏为一步生成器",
      "authors": [
        "Yuanzhi Zhu",
        "Xi Wang",
        "Stéphane Lathuilière",
        "Vicky Kalogeiton"
      ],
      "abstract": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.",
      "tldr_zh": "本文提出Di$\\mathtt{[M]}$O方法，首次实现将掩码扩散模型(MDMs)蒸馏为单步生成器的突破。该方法通过两个关键技术解决核心挑战：(1)采用基于\"on-policy框架\"的token级分布匹配，利用辅助模型优化输出logits；(2)设计token初始化策略注入随机性同时保持与教师训练分布相似性。实验表明，Di$\\mathtt{[M]}$O在类别条件和文本条件图像生成任务中，能在保持与多步教师模型相当性能的同时大幅降低推理耗时。该工作不仅是首个成功实现掩码扩散模型单步蒸馏的研究，也是首次将离散蒸馏应用于文生图领域，为高效生成建模开辟了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15457v1",
      "published_date": "2025-03-19 17:36:54 UTC",
      "updated_date": "2025-03-19 17:36:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:46:52.143123"
    },
    {
      "arxiv_id": "2503.15438v1",
      "title": "VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning",
      "title_zh": "VenusFactory：蛋白质工程数据检索与语言模型微调的统一平台",
      "authors": [
        "Yang Tan",
        "Chen Liu",
        "Jingyuan Gao",
        "Banghao Wu",
        "Mingchen Li",
        "Ruilin Wang",
        "Lingrong Zhang",
        "Huiqun Yu",
        "Guisheng Fan",
        "Liang Hong",
        "Bingxin Zhou"
      ],
      "abstract": "Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory.",
      "tldr_zh": "该研究开发了VenusFactory统一平台，旨在解决蛋白质工程领域数据收集、任务基准测试和模型应用的关键挑战。该平台整合了40多个蛋白质相关数据集和40多种主流蛋白质语言模型(PLMs)，提供命令行执行和Gradio无代码界面两种使用方式，支持生物数据检索、标准化任务评估和模块化PLMs微调。通过开源实现，该平台为计算机科学和生物学研究者提供了便捷的蛋白质工程研究工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 1 figure, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.15438v1",
      "published_date": "2025-03-19 17:19:07 UTC",
      "updated_date": "2025-03-19 17:19:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:47:07.219871"
    },
    {
      "arxiv_id": "2503.15436v1",
      "title": "An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts",
      "title_zh": "一项广泛模拟研究：评估多重因果发现情境下重采样技术的交互作用",
      "authors": [
        "Ritwick Banerjee",
        "Bryan Andrews",
        "Erich Kummerfeld"
      ],
      "abstract": "Despite the accelerating presence of exploratory causal analysis in modern\nscience and medicine, the available non-experimental methods for validating\ncausal models are not well characterized. One of the most popular methods is to\nevaluate the stability of model features after resampling the data, similar to\nresampling methods for estimating confidence intervals in statistics. Many\naspects of this approach have received little to no attention, however, such as\nwhether the choice of resampling method should depend on the sample size,\nalgorithms being used, or algorithm tuning parameters. We present theoretical\nresults proving that certain resampling methods closely emulate the assignment\nof specific values to algorithm tuning parameters. We also report the results\nof extensive simulation experiments, which verify the theoretical result and\nprovide substantial data to aid researchers in further characterizing\nresampling in the context of causal discovery analysis. Together, the\ntheoretical work and simulation results provide specific guidance on how\nresampling methods and tuning parameters should be selected in practice.",
      "tldr_zh": "这项研究通过理论分析和模拟实验系统评估了重采样技术在因果发现中的交互作用。研究证明某些重采样方法可以等效替代特定算法调参值，并通过大量模拟验证了这一理论发现。研究结果为实践中如何根据样本量、算法类型和调参需求选择合适重采样方法提供了具体指导，弥补了因果模型验证方法的表征不足问题。",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15436v1",
      "published_date": "2025-03-19 17:18:18 UTC",
      "updated_date": "2025-03-19 17:18:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:47:39.817084"
    },
    {
      "arxiv_id": "2503.15426v2",
      "title": "Visual Position Prompt for MLLM based Visual Grounding",
      "title_zh": "视觉位置提示：基于MLLM的视觉定位方法",
      "authors": [
        "Wei Tang",
        "Yanpeng Sun",
        "Qinying Gu",
        "Zechao Li"
      ],
      "abstract": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.",
      "tldr_zh": "该研究提出VPP-LLaVA模型，通过创新的视觉位置提示(VPP)机制提升多模态大语言模型(MLLM)的视觉定位能力。模型采用全局VPP(轴状嵌入提供空间线索)和局部VPP(位置感知查询精确定位)的双重机制，解决了MLLM在空间坐标对齐上的固有缺陷。研究还构建了包含60万样本的VPP-SFT数据集，使模型仅需少量训练数据即在标准基准测试中达到最优性能，显著优于需要2100万样本的MiniGPT-v2等现有模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15426v2",
      "published_date": "2025-03-19 17:08:13 UTC",
      "updated_date": "2025-03-24 16:34:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:47:51.720022"
    },
    {
      "arxiv_id": "2503.15421v1",
      "title": "Probing the topology of the space of tokens with structured prompts",
      "title_zh": "探究基于结构化提示的标记空间拓扑结构",
      "authors": [
        "Michael Robinson",
        "Sourya Dey",
        "Taisa Kushner"
      ],
      "abstract": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.",
      "tldr_zh": "本文提出了一种通用灵活的提示方法，通过结构化提示(structured prompts)探究大型语言模型(LLM)中token输入嵌入的拓扑结构。研究不仅提供了该方法可行的数学证明，还通过实验成功重建了Llemma-7B模型的token子空间。该成果不仅适用于LLMs，还可推广到一般的非线性自回归过程。",
      "categories": [
        "math.DG",
        "cs.AI",
        "53Z50, 58Z05",
        "I.2.7"
      ],
      "primary_category": "math.DG",
      "comment": "20 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15421v1",
      "published_date": "2025-03-19 17:01:15 UTC",
      "updated_date": "2025-03-19 17:01:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:48:31.694276"
    },
    {
      "arxiv_id": "2503.15417v1",
      "title": "Temporal Regularization Makes Your Video Generator Stronger",
      "title_zh": "时序正则化让您的视频生成器更强大",
      "authors": [
        "Harold Haodong Chen",
        "Haojian Huang",
        "Xianfeng Wu",
        "Yexin Liu",
        "Yajing Bai",
        "Wen-Jie Shu",
        "Harry Yang",
        "Ser-Nam Lim"
      ],
      "abstract": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
      "tldr_zh": "该论文首次提出视频生成中的时间维度增强方法FluxFlow，通过在数据层面施加可控时间扰动来提升视频的时序质量。这种方法无需修改模型架构，即可显著改善U-Net、DiT和基于AR架构等多种视频生成模型的时间连贯性和多样性，同时保持空间保真度。在UCF-101和VBench基准测试中，FluxFlow展现出提升视频生成质量的潜力，为时序正则化技术的应用开辟了新方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project: https://haroldchen19.github.io/FluxFlow/",
      "pdf_url": "http://arxiv.org/pdf/2503.15417v1",
      "published_date": "2025-03-19 16:59:32 UTC",
      "updated_date": "2025-03-19 16:59:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:48:51.361535"
    },
    {
      "arxiv_id": "2503.15415v1",
      "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures",
      "title_zh": "深度学习模型中可解释人工智能输出的自动处理及其在大型基础设施故障诊断中的应用",
      "authors": [
        "Giovanni Floreale",
        "Piero Baraldi",
        "Enrico Zio",
        "Olga Fink"
      ],
      "abstract": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.",
      "tldr_zh": "该研究提出了一种新型框架，将可解释人工智能(XAI)技术与半监督学习相结合，用于自动检测深度学习模型在大型基础设施故障诊断中的异常解释行为。该方法通过对比正确分类图像的XAI输出（如GradCAM热力图），自动识别可能反映模型偏见或非因果捷径的异常解释，将人工复核工作量减少85%仅需处理15%的标记图像。在电网绝缘子无人机图像诊断任务中，该框架使两类故障的平均分类准确率提升8%，F1分数优于基于忠实度指标的现有方法，并能有效识别由ID标签等非因果特征导致的伪正确分类。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15415v1",
      "published_date": "2025-03-19 16:57:00 UTC",
      "updated_date": "2025-03-19 16:57:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:49:27.656727"
    },
    {
      "arxiv_id": "2503.15402v1",
      "title": "Towards efficient keyword spotting using spike-based time difference encoders",
      "title_zh": "面向高效关键词检测的基于脉冲的时间差分编码器研究",
      "authors": [
        "Alejandro Pequeño-Zurro",
        "Lyes Khacef",
        "Stefano Panzeri",
        "Elisabetta Chicca"
      ],
      "abstract": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.",
      "tldr_zh": "本研究探讨了基于脉冲的时间差分编码器(TDE)在关键词识别中的高效应用。通过比较三种不同结构的脉冲神经网络(SNNs)，发现前馈式TDE网络在准确率(89%)上显著优于前馈CuBa-LIF网络(71%)，且接近循环CuBa-LIF网络(91%)，同时突触操作量减少92%。实验表明，TDE能有效处理语音信号的时空模式，其输出结果与关键词的频域特征高度相关，为边缘设备实现低功耗、可解释的语音识别提供了新思路。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.ET"
      ],
      "primary_category": "cs.NE",
      "comment": "26 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15402v1",
      "published_date": "2025-03-19 16:43:35 UTC",
      "updated_date": "2025-03-19 16:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:49:36.345517"
    },
    {
      "arxiv_id": "2503.15386v1",
      "title": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling",
      "title_zh": "CCDP：基于引导采样的条件扩散策略组合",
      "authors": [
        "Amirreza Razmjoo",
        "Sylvain Calinon",
        "Michael Gienger",
        "Fan Zhang"
      ],
      "abstract": "Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem (which may\nrequire long-horizon history to manage failures) into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines.",
      "tldr_zh": "本研究提出CCDP框架，通过条件扩散策略组合与引导采样改进模仿学习的动作生成效率。该方法创新性地利用成功演示数据逆向推断恢复动作，无需额外探索或高层控制器，并采用扩散模型分解技术将复杂长时程问题拆解为多个可管理子问题。实验表明，该动态调整采样空间的低层控制器在未知方向开门、物体操作等任务中显著优于传统基线方法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15386v1",
      "published_date": "2025-03-19 16:24:55 UTC",
      "updated_date": "2025-03-19 16:24:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:49:59.468159"
    },
    {
      "arxiv_id": "2503.15374v1",
      "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
      "title_zh": "基于电子健康记录的多模态大语言模型临床受试者匹配高精度管道的真实世界验证",
      "authors": [
        "Anatole Callies",
        "Quentin Bodinier",
        "Philippe Ravaud",
        "Kourosh Davarpanah"
      ],
      "abstract": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
      "tldr_zh": "该研究开发了一种基于多模态大语言模型(LLM)的临床实验患者匹配自动化流程，通过整合电子健康记录(EHR)数据显著提升了匹配效率。该方法创新性地结合了推理型LLM的复杂条件评估能力、视觉模态的医疗记录解析技术以及多模态嵌入检索，在n2c2数据集上达到93%的判定标准准确率，实际临床实验中实现87%准确率。相比传统人工病历审查，该方法使患者资格评估时间缩短80%（平均9分钟/人），且无需定制化系统集成，为大规模临床实验招募提供了可扩展的AI解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15374v1",
      "published_date": "2025-03-19 16:12:11 UTC",
      "updated_date": "2025-03-19 16:12:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:50:25.820600"
    },
    {
      "arxiv_id": "2503.15354v1",
      "title": "Optimizing Decomposition for Optimal Claim Verification",
      "title_zh": "优化分解以实现最优声明验证",
      "authors": [
        "Yining Lu",
        "Noah Ziems",
        "Hy Dang",
        "Meng Jiang"
      ],
      "abstract": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
      "tldr_zh": "本文针对“分解-验证”范式在长文本事实性评估中的不足，提出了一种优化分解策略的方法。研究发现，现有的分解策略与下游验证器在信息密度（atomicity）方面存在偏差，导致验证结果欠佳。为此，作者将最优分解策略的求解建模为双层优化问题，并提出了一种基于强化学习的动态分解框架，利用验证器反馈学习分解策略。实验表明，该方法在验证置信度和准确性上均优于现有策略，分别平均提高了0.07和0.12。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15354v1",
      "published_date": "2025-03-19 15:56:21 UTC",
      "updated_date": "2025-03-19 15:56:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:51:04.764947"
    },
    {
      "arxiv_id": "2503.15352v1",
      "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer",
      "title_zh": "利用完美多模态对齐与高斯假设实现跨模态迁移",
      "authors": [
        "Abhi Kamboj",
        "Minh N. Do"
      ],
      "abstract": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.",
      "tldr_zh": "该研究提出了一个基于完美多模态对齐和混合高斯假设的跨模态迁移方法。通过将多模态对齐建模为逆问题，论文证明了在特定条件下可以实现完美的模态对齐。基于语义类别在隐空间中呈现混合高斯分布的假设，研究者展示了如何通过将数据点投影到不同模态子空间来实现无监督跨模态迁移。实验在合成的多模态高斯数据上验证了该方法的有效性，为跨模态学习开辟了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15352v1",
      "published_date": "2025-03-19 15:51:17 UTC",
      "updated_date": "2025-03-19 15:51:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:51:17.343907"
    },
    {
      "arxiv_id": "2503.15342v1",
      "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
      "title_zh": "TruthLens：一种无需训练的深度伪造检测新范式",
      "authors": [
        "Ritabrata Chakraborty",
        "Rajatsubhra Chakraborty",
        "Ali Khaleghi Rahimian",
        "Thomas MacDougall"
      ],
      "abstract": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.",
      "tldr_zh": "本研究提出TruthLens，一种无需训练的深度伪造检测新范式，将检测任务重新定义为视觉问答(VQA)问题。该方法创新性地结合大型视觉语言模型(LVLMs)和大型语言模型(LLMs)的多模态优势，通过分析视觉伪影特征并生成可解释的决策依据，实现高精度且透明的检测。实验表明，TruthLens在保持出色检测性能的同时，显著提升了结果可解释性，为对抗合成媒体威胁提供了兼顾准确性和透明度的新解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15342v1",
      "published_date": "2025-03-19 15:41:32 UTC",
      "updated_date": "2025-03-19 15:41:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:51:14.111244"
    },
    {
      "arxiv_id": "2503.15275v1",
      "title": "Challenges and Trends in Egocentric Vision: A Survey",
      "title_zh": "第一视角视觉的挑战与趋势：一项综述",
      "authors": [
        "Xiang Li",
        "Heqian Qiu",
        "Lanxiao Wang",
        "Hanwen Zhang",
        "Chenghao Qi",
        "Linfeng Han",
        "Huiyu Xiong",
        "Hongliang Li"
      ],
      "abstract": "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.",
      "tldr_zh": "本文对第一人称视角（Egocentric Vision）研究进行了系统性综述。研究将该领域划分为主体理解、物体理解、环境理解和混合理解四大任务类别，并详细分析了各类别下的子任务。论文总结了当前研究面临的主要挑战和发展趋势，整理了大量高质量的第一人称视角数据集资源。作者展望了该技术在增强现实（AR）、虚拟现实（VR）和具身智能等领域的应用前景，并基于最新进展提出了未来研究方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15275v1",
      "published_date": "2025-03-19 14:51:27 UTC",
      "updated_date": "2025-03-19 14:51:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:51:29.552746"
    },
    {
      "arxiv_id": "2503.15580v1",
      "title": "How Well Can AI Build SD Models?",
      "title_zh": "AI构建系统动力学模型的能力评估",
      "authors": [
        "William Schoenberg",
        "Davidson Girard",
        "Saras Chung",
        "Ellen O'Neill",
        "Janet Velasquez",
        "Sara Metcalf"
      ],
      "abstract": "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
      "tldr_zh": "该研究探讨了人工智能（AI）在构建系统动力学（SD）模型中的表现，并提出了两项评估指标：技术正确性（因果翻译）和指令遵循性（一致性）。研究团队开发了一个名为sd-ai的开源项目，用于评估不同大语言模型（LLMs）在因果翻译和用户指令遵循方面的能力。测试结果显示，gpt-4.5-preview在整体表现上得分最高（92.9%），而o1在因果翻译任务中达到100%的准确率。研究强调了持续评估和开放协作的重要性，以确保AI工具在动态建模中的负责任开发。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15580v1",
      "published_date": "2025-03-19 14:48:47 UTC",
      "updated_date": "2025-03-19 14:48:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:52:36.207764"
    },
    {
      "arxiv_id": "2503.15272v1",
      "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration",
      "title_zh": "MAMM-Refine：通过多智能体协作提升生成内容忠实性的方法",
      "authors": [
        "David Wan",
        "Justin Chih-Yao Chen",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.",
      "tldr_zh": "该研究提出MAMM-Refine方法，通过多智能体多模型协作机制提升文本生成任务（如摘要和问答）的忠实性。该方法创新性地将错误检测、批判性分析和修正等子任务重构为排序问题，实验表明多智能体（多个模型实例）和多模型（不同LLM类型）协作能有效提升各子任务表现。在三个摘要数据集和长文本问答任务上的测试显示，该方法显著优于基线，验证了其有效性和泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine",
      "pdf_url": "http://arxiv.org/pdf/2503.15272v1",
      "published_date": "2025-03-19 14:46:53 UTC",
      "updated_date": "2025-03-19 14:46:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:53:00.982271"
    },
    {
      "arxiv_id": "2503.15268v1",
      "title": "Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
      "title_zh": "大型语言模型的思维链在贝叶斯推理中是否会出现幻觉、认知偏差或恐惧症？",
      "authors": [
        "Roberto Araya"
      ],
      "abstract": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
      "tldr_zh": "该研究探讨了大语言模型(LLMs)在贝叶斯推理中使用链式思维(Chain-of-Thought, CoT)时是否会出现幻觉、认知偏差或策略回避等问题。研究发现，尽管LLMs能够通过CoT进行推理和解释，但它们缺乏人类常用的生态有效策略（如自然频率、整体对象和具身启发式），这些策略在贝叶斯推理中具有重要的教学价值。通过引入提示词引导LLMs使用这些策略，发现LLMs能够部分采用这些方法，但仍表现出对符号推理的持续偏好和对生态有效策略的回避倾向。",
      "categories": [
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15268v1",
      "published_date": "2025-03-19 14:44:02 UTC",
      "updated_date": "2025-03-19 14:44:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:53:27.243195"
    },
    {
      "arxiv_id": "2503.15248v1",
      "title": "Automated Non-Functional Requirements Generation in Software Engineering with Large Language Models: A Comparative Study",
      "title_zh": "基于大语言模型的软件工程非功能性需求自动生成：一项对比研究",
      "authors": [
        "Jomar Thomas Almonte",
        "Santhosh Anitha Boominathan",
        "Nathalia Nascimento"
      ],
      "abstract": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
      "tldr_zh": "这篇论文提出了一种利用大语言模型(LLMs)自动生成非功能性需求(NFRs)的框架。研究团队开发了基于Deno的管道流程，采用定制提示技术，能够从功能性需求(FRs)中推导出符合ISO/IEC 25010:2023标准的NFRs。通过对34个代表性功能需求生成1,593条NFRs的实验评估显示，LLMs生成的建议与行业专家评估高度吻合：在1-5分制中，有效性和适用性中位数达到5.0，且80.4%的质量属性分类与专家判断一致。在8种LLMs的对比中，gemini-1.5-pro表现出最高的属性准确性，而llama-3.3-70B则在有效性和适用性上得分更高。该研究为AI辅助需求工程提供了可行性验证。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.15248v1",
      "published_date": "2025-03-19 14:23:22 UTC",
      "updated_date": "2025-03-19 14:23:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:53:49.100970"
    },
    {
      "arxiv_id": "2503.15242v2",
      "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?",
      "title_zh": "BigO(Bench)——大型语言模型能否生成具有可控时空复杂度的代码？",
      "authors": [
        "Pierre Chambon",
        "Baptiste Roziere",
        "Benoit Sagot",
        "Gabriel Synnaeve"
      ],
      "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
      "tldr_zh": "该研究提出了BigO(Bench)基准测试，专门评估大语言模型(LLMs)生成符合指定时间和空间复杂度代码的能力。该基准包含3,105个编程问题和超过100万带复杂度标注的解决方案，并开发了从性能分析数据推断Python函数算法复杂度的工具。实验发现，当前最先进的语言模型在代码生成方面表现优异，但在理解复杂度要求方面存在不足，表明它们难以泛化到训练时未获得奖励的任务。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15242v2",
      "published_date": "2025-03-19 14:19:57 UTC",
      "updated_date": "2025-03-20 17:58:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:53:57.134835"
    },
    {
      "arxiv_id": "2503.15234v1",
      "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification",
      "title_zh": "CoE：通过自动视觉概念回路描述与多义性量化的解释链",
      "authors": [
        "Wenlong Yu",
        "Qilong Wang",
        "Chuang Liu",
        "Dong Li",
        "Qinghua Hu"
      ],
      "abstract": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.",
      "tldr_zh": "本研究提出Chain-of-Explanation (CoE)框架，通过自动化视觉概念回路描述和多义性量化来增强深度视觉模型(DVMs)的可解释性。该框架包含三个创新：1) 自动构建全局概念解释数据集；2) 设计概念多义性解耦机制，提出概念多义性熵(CPE)量化指标；3) 通过追踪概念回路实现DVM决策过程的局部语言解释。实验表明，CoE在可解释性评分上平均绝对提升36%，有效解决了现有方法在全局概念解释和多义性处理方面的不足。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15234v1",
      "published_date": "2025-03-19 14:13:02 UTC",
      "updated_date": "2025-03-19 14:13:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:54:25.973323"
    },
    {
      "arxiv_id": "2503.15235v1",
      "title": "Exploring Large Language Models for Word Games:Who is the Spy?",
      "title_zh": "探索大型语言模型在文字游戏中的应用：谁是卧底？",
      "authors": [
        "Chentian Wei",
        "Jiewei Chen",
        "Jinzhu Xu"
      ],
      "abstract": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.",
      "tldr_zh": "本研究探索了大型语言模型(LLMs)在文字游戏\"谁是卧底\"中的应用，提出了一个无需训练的思维链(CoT)调度框架。该框架使LLMs能够出色完成推理角色词和隐藏身份等任务，在多个数据集上显著提升了模型表现。实验结果表明，该方案有效提高了游戏胜率和智能体分析准确性，展现了LLMs在结构化游戏环境中进行情境推理和社交互动的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15235v1",
      "published_date": "2025-03-19 14:13:02 UTC",
      "updated_date": "2025-03-19 14:13:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:54:34.681350"
    },
    {
      "arxiv_id": "2503.15225v1",
      "title": "A Personalized Data-Driven Generative Model of Human Motion",
      "title_zh": "个性化数据驱动的人类运动生成模型",
      "authors": [
        "Angelo Di Porzio",
        "Marco Coraggio"
      ],
      "abstract": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data.",
      "tldr_zh": "该研究提出了一种基于LSTM神经网络的个性化数据驱动生成模型，用于创建能捕捉个体独特运动特征的人类动作。通过标量振荡运动的真实数据验证，该模型能有效复现训练个体的速度分布和振幅包络特征，并保持与其他个体的差异性。实验表明，该模型在模拟人类数据相似性方面优于现有最优方法，为虚拟化身和机器人的认知架构设计提供了更精确的人类运动建模方案。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.GR",
      "comment": "6 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15225v1",
      "published_date": "2025-03-19 14:03:20 UTC",
      "updated_date": "2025-03-19 14:03:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:54:56.306816"
    },
    {
      "arxiv_id": "2503.15204v1",
      "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection",
      "title_zh": "当猪群患病时：用于猪病检测的多智能体人工智能系统",
      "authors": [
        "Tittaya Mairittha",
        "Tanakon Sawanglok",
        "Panuwit Raden",
        "Sorrawit Treesuk"
      ],
      "abstract": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.",
      "tldr_zh": "该研究提出了一种基于多智能体AI的猪病检测系统，旨在解决全球农业中猪病监测面临的资源有限、诊断延迟和准确性不足等问题。该系统利用检索增强生成技术(RAG)，通过自动分类用户输入为知识检索查询或症状诊断查询，实现精准信息获取和诊断推理。采用自适应提问协议和置信度加权决策融合机制，系统能够生成可靠的疾病预测和治疗建议。实验表明，该系统在查询分类、疾病诊断和知识检索方面表现出高准确性、快速响应和一致可靠性，为可持续畜牧业管理和全球粮食安全提供了技术支持。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "14 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15204v1",
      "published_date": "2025-03-19 13:47:25 UTC",
      "updated_date": "2025-03-19 13:47:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:55:20.180240"
    },
    {
      "arxiv_id": "2503.15202v2",
      "title": "A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees",
      "title_zh": "基于视觉语言模型、反应式规划器与行为树的机器人实时故障处理统一框架",
      "authors": [
        "Faseeh Ahmad",
        "Hashim Ismail",
        "Jonathan Styrud",
        "Maj Stenmark",
        "Volker Krueger"
      ],
      "abstract": "Robotic systems often face execution failures due to unexpected obstacles,\nsensor errors, or environmental changes. Traditional failure recovery methods\nrely on predefined strategies or human intervention, making them less\nadaptable. This paper presents a unified failure recovery framework that\ncombines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees\n(BTs) to enable real-time failure handling. Our approach includes pre-execution\nverification, which checks for potential failures before execution, and\nreactive failure handling, which detects and corrects failures during execution\nby verifying existing BT conditions, adding missing preconditions and, when\nnecessary, generating new skills. The framework uses a scene graph for\nstructured environmental perception and an execution history for continuous\nmonitoring, enabling context-aware and adaptive failure handling. We evaluate\nour framework through real-world experiments with an ABB YuMi robot on tasks\nlike peg insertion, object sorting, and drawer placement, as well as in\nAI2-THOR simulator. Compared to using pre-execution and reactive methods\nseparately, our approach achieves higher task success rates and greater\nadaptability. Ablation studies highlight the importance of VLM-based reasoning,\nstructured scene representation, and execution history tracking for effective\nfailure recovery in robotics.",
      "tldr_zh": "该研究提出了一种结合视觉语言模型(VLMs)、反应式规划器和行为树(BTs)的统一框架，用于机器人实时故障处理。通过预执行验证和反应式故障处理的双重机制，系统能在执行前后动态检测并修正故障，包括验证条件、补充缺失前提或生成新技能。框架采用场景图进行结构化环境感知，并利用执行历史持续监控，在ABB YuMi机器人上的实验显示，相比单独方法，该方案显著提高了任务成功率和适应性。消融研究证实了VLM推理、结构化场景表征和执行历史追踪对有效故障恢复的关键作用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15202v2",
      "published_date": "2025-03-19 13:40:56 UTC",
      "updated_date": "2025-03-21 08:10:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:55:38.218954"
    },
    {
      "arxiv_id": "2503.15576v1",
      "title": "A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doñana",
      "title_zh": "基于深度学习的鸟类鸣叫检测器提升识别准确率：以多尼亚纳自然保护区为例",
      "authors": [
        "Alba Márquez-Rodríguez",
        "Miguel Ángel Mohedano-Munoz",
        "Manuel J. Marín-Jiménez",
        "Eduardo Santamaría-García",
        "Giulia Bastianelli",
        "Pedro Jordano",
        "Irene Mendoza"
      ],
      "abstract": "Passive Acoustic Monitoring with automatic recorders is essential for\necosystem conservation but generates vast unsupervised audio data, posing\nchallenges for extracting meaningful information. Deep Learning techniques\noffer a promising solution. BirdNET, a widely used model for bird\nidentification, has shown success in many study systems but is limited in some\nregions due to biases in its training data. A key challenge in bird species\ndetection is that many recordings either lack target species or contain\noverlapping vocalizations. To overcome these problems, we developed a\nmulti-stage pipeline for automatic bird vocalization identification in Do\\~nana\nNational Park (SW Spain), a region facing significant conservation threats. Our\napproach included a Bird Song Detector to isolate vocalizations and custom\nclassifiers trained with BirdNET embeddings. We manually annotated 461 minutes\nof audio from three habitats across nine locations, yielding 3,749 annotations\nfor 34 classes. Spectrograms facilitated the use of image processing\ntechniques. Applying the Bird Song Detector before classification improved\nspecies identification, as all classification models performed better when\nanalyzing only the segments where birds were detected. Specifically, the\ncombination of the Bird Song Detector and fine-tuned BirdNET compared to the\nbaseline without the Bird Song Detector. Our approach demonstrated the\neffectiveness of integrating a Bird Song Detector with fine-tuned\nclassification models for bird identification at local soundscapes. These\nfindings highlight the need to adapt general-purpose tools for specific\necological challenges, as demonstrated in Do\\~nana. Automatically detecting\nbird species serves for tracking the health status of this threatened\necosystem, given the sensitivity of birds to environmental changes, and helps\nin the design of conservation measures for reducing biodiversity loss",
      "tldr_zh": "本研究针对西班牙多尼亚纳国家公园的鸟类保护需求，开发了一个基于深度学习的多阶段鸟类声音识别系统。核心创新是提出Bird Song Detector预处理模块，通过分离鸟鸣片段并结合基于BirdNET嵌入的自定义分类器，有效解决了传统方法在背景噪音和重叠鸣叫场景下的识别难题。实验表明，该方案在34个鸟类物种的3749条标注数据上显著提升识别准确率，比直接使用BirdNET基线模型效果更优。该技术为濒危生态系统的健康监测提供了自动化工具，凸显了通用模型在特定生态场景定制化应用的重要性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NE",
        "I.5.4; I.2.6; I.4.8"
      ],
      "primary_category": "cs.SD",
      "comment": "20 pages, 13 images, for associated dataset see\n  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for\n  associated code see\n  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https://github.com/GrunCrow/Bird-Song-Detector",
      "pdf_url": "http://arxiv.org/pdf/2503.15576v1",
      "published_date": "2025-03-19 13:19:06 UTC",
      "updated_date": "2025-03-19 13:19:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:55:56.055395"
    },
    {
      "arxiv_id": "2503.15185v1",
      "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
      "title_zh": "《基于原型感知视角转换的低分辨率查询三维占据预测》",
      "authors": [
        "Gyeongrok Oh",
        "Sungjune Kim",
        "Heeju Ko",
        "Hyung-gun Chi",
        "Jinkyu Kim",
        "Dongwook Lee",
        "Daehyun Ji",
        "Sungjoon Choi",
        "Sujin Jang",
        "Sangpil Kim"
      ],
      "abstract": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution.",
      "tldr_zh": "本文提出ProtoOcc，一种基于原型感知的新型3D占位预测网络，通过聚类图像片段原型来增强低分辨率体素查询的视觉信息表达。该方法创新性地将2D视觉几何原型映射到3D体素空间，并结合多视角解码策略，有效缓解了因体素分辨率降低导致的空间信息损失问题。实验表明，在Occ3D和SemanticKITTI基准测试中，即使体素分辨率降低75%，该模型仍能保持与基线相当的预测性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15185v1",
      "published_date": "2025-03-19 13:14:57 UTC",
      "updated_date": "2025-03-19 13:14:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:56:40.865620"
    },
    {
      "arxiv_id": "2503.15182v1",
      "title": "Foundation models may exhibit staged progression in novel CBRN threat disclosure",
      "title_zh": "基础模型在新型化生放核威胁披露中可能呈现阶段性进展",
      "authors": [
        "Kevin M Esvelt"
      ],
      "abstract": "The extent to which foundation models can disclose novel chemical,\nbiological, radiation, and nuclear (CBRN) threats to expert users is unclear\ndue to a lack of test cases. I leveraged the unique opportunity presented by an\nupcoming publication describing a novel catastrophic biothreat - \"Technical\nReport on Mirror Bacteria: Feasibility and Risks\" - to conduct a small\ncontrolled study before it became public. Graduate-trained biologists tasked\nwith predicting the consequences of releasing mirror E. coli showed no\nsignificant differences in rubric-graded accuracy using Claude Sonnet 3.5 new\n(n=10) or web search only (n=2); both groups scored comparably to a web\nbaseline (28 and 43 versus 36). However, Sonnet reasoned correctly when\nprompted by a report author, but a smaller model, Haiku 3.5, failed even with\nauthor guidance (80 versus 5). These results suggest distinct stages of model\ncapability: Haiku is unable to reason about mirror life even with threat-aware\nexpert guidance (Stage 1), while Sonnet correctly reasons only with\nthreat-aware prompting (Stage 2). Continued advances may allow future models to\ndisclose novel CBRN threats to naive experts (Stage 3) or unskilled users\n(Stage 4). While mirror life represents only one case study, monitoring new\nmodels' ability to reason about privately known threats may allow protective\nmeasures to be implemented before widespread disclosure.",
      "tldr_zh": "该研究通过\"镜像细菌\"这一新型生物威胁案例，揭示了基础模型在CBRN(化学、生物、放射性和核)威胁披露能力上的阶段性差异。实验发现：Haiku 3.5模型即使在专家引导下也无法理解镜像生命体威胁(阶段1)，而Claude Sonnet 3.5模型仅能在威胁知情专家的特定提示下正确推理(阶段2)。研究表明，未来更先进的模型可能逐步具备向领域专家(阶段3)甚至普通用户(阶段4)披露新型CBRN威胁的能力，这对制定预防性监管措施具有重要启示。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "q-bio.OT"
      ],
      "primary_category": "cs.CY",
      "comment": "26 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15182v1",
      "published_date": "2025-03-19 13:08:01 UTC",
      "updated_date": "2025-03-19 13:08:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:56:36.858383"
    },
    {
      "arxiv_id": "2503.15172v1",
      "title": "Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems",
      "title_zh": "动态频谱接入系统中基于谐波退火剪枝的多智能体演员-评论家框架",
      "authors": [
        "George Stamatelis",
        "Angelos-Nikolaos Kanatas",
        "George C. Alexandropoulos"
      ],
      "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful\ntool for optimizing decentralized decision-making systems in complex settings,\nsuch as Dynamic Spectrum Access (DSA). However, deploying deep learning models\non resource-constrained edge devices remains challenging due to their high\ncomputational cost. To address this challenge, in this paper, we present a\nnovel sparse recurrent MARL framework integrating gradual neural network\npruning into the independent actor global critic paradigm. Additionally, we\nintroduce a harmonic annealing sparsity scheduler, which achieves comparable,\nand in certain cases superior, performance to standard linear and polynomial\npruning schedulers at large sparsities. Our experimental investigation\ndemonstrates that the proposed DSA framework can discover superior policies,\nunder diverse training conditions, outperforming conventional DSA, MADRL\nbaselines, and state-of-the-art pruning techniques.",
      "tldr_zh": "本文提出了一种新型稀疏多智能体强化学习框架，将渐进式神经网络剪枝技术与独立智能体-全局评论家（independent actor global critic）范式相结合，用于动态频谱接入（DSA）系统优化。该框架创新性地采用谐波退火稀疏调度器（harmonic annealing sparsity scheduler），在大稀疏度条件下性能优于传统线性和多项式调度器。实验表明，该框架能在多样化训练条件下发现更优策略，其性能超越传统DSA方法、多智能体强化学习基线以及最先进的剪枝技术。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures, 1 table, submited to an IEEE conference",
      "pdf_url": "http://arxiv.org/pdf/2503.15172v1",
      "published_date": "2025-03-19 12:56:23 UTC",
      "updated_date": "2025-03-19 12:56:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:57:04.124012"
    },
    {
      "arxiv_id": "2503.15169v1",
      "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks",
      "title_zh": "Llama3与DeepSeekR1在生物医学文本分类任务上的性能比较",
      "authors": [
        "Yuting Guo",
        "Abeed Sarker"
      ],
      "abstract": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs.",
      "tldr_zh": "本研究比较了开源大语言模型Llama3-70B与其蒸馏版本DeepSeekR1-distill-Llama3-70B在六项生物医学文本分类任务中的表现，包括社交媒体数据和电子健康记录临床笔记。实验结果显示，蒸馏模型DeepSeekR1在多数任务上精确度(precision)更优，但召回率(recall)表现不一。虽然某些任务的F1分数较高，但两种模型在零样本(zero-shot)设置下对部分任务表现欠佳。研究表明，医疗文本分类任务需根据精确度-召回率的权衡来选型，且在有标注数据时，监督学习方法可能比零样本LLMs更可靠。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.15169v1",
      "published_date": "2025-03-19 12:51:52 UTC",
      "updated_date": "2025-03-19 12:51:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:57:17.475422"
    },
    {
      "arxiv_id": "2503.15168v1",
      "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child",
      "title_zh": "人工智能中的世界模型：像儿童一样感知、学习与推理",
      "authors": [
        "Javier Del Ser",
        "Jesus L. Lobo",
        "Heimo Müller",
        "Andreas Holzinger"
      ],
      "abstract": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
      "tldr_zh": "该论文探讨了如何通过\"世界模型\"(World Models)让AI系统像儿童一样感知、学习和推理。研究指出当前AI的世界模型缺乏儿童认知发展中的结构化动态表征能力，提出应基于皮亚杰认知发展理论构建可解释框架。作者强调六个关键研究方向：物理信息学习、神经符号学习、持续学习、因果推理、人机协同AI和负责任AI，认为通过整合这些领域的统计学习技术，AI才能从模式识别真正进化到具备理解、适应和推理能力。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "cs.LG",
        "68T05"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.15168v1",
      "published_date": "2025-03-19 12:50:40 UTC",
      "updated_date": "2025-03-19 12:50:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:57:34.743370"
    },
    {
      "arxiv_id": "2503.15167v1",
      "title": "Volumetric Reconstruction From Partial Views for Task-Oriented Grasping",
      "title_zh": "面向任务抓取的部分视图体积重建",
      "authors": [
        "Fujian Yan",
        "Hui Li",
        "Hongsheng He"
      ],
      "abstract": "Object affordance and volumetric information are essential in devising\neffective grasping strategies under task-specific constraints. This paper\npresents an approach for inferring suitable grasping strategies from limited\npartial views of an object. To achieve this, a recurrent generative adversarial\nnetwork (R-GAN) was proposed by incorporating a recurrent generator with long\nshort-term memory (LSTM) units for it to process a variable number of depth\nscans. To determine object affordances, the AffordPose knowledge dataset is\nutilized as prior knowledge. Affordance retrieving is defined by the volume\nsimilarity measured via Chamfer Distance and action similarities. A Proximal\nPolicy Optimization (PPO) reinforcement learning model is further implemented\nto refine the retrieved grasp strategies for task-oriented grasping. The\nretrieved grasp strategies were evaluated on a dual-arm mobile manipulation\nrobot with an overall grasping accuracy of 89% for four tasks: lift, handle\ngrasp, wrap grasp, and press.",
      "tldr_zh": "本研究提出一种从物体局部视角进行体积重建的任务导向抓取方法。通过结合循环生成对抗网络(R-GAN)和LSTM单元处理可变数量的深度扫描，并利用AffordPose知识数据集计算Chamfer距离和动作相似度来推断物体功能特性。进一步采用PPO强化学习模型优化抓取策略，在双臂移动操作机器人上实现了89%的平均抓取准确率，可完成举升、握把、环绕握持和按压四种任务。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15167v1",
      "published_date": "2025-03-19 12:47:50 UTC",
      "updated_date": "2025-03-19 12:47:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:58:03.546988"
    },
    {
      "arxiv_id": "2503.15166v1",
      "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU",
      "title_zh": "双曲空间与欧氏空间多模态对比学习中的机器遗忘：基于MERU的校准对齐方法",
      "authors": [
        "Àlex Pujol Vidal",
        "Sergio Escalera",
        "Kamal Nasrollahi",
        "Thomas B. Moeslund"
      ],
      "abstract": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC",
      "tldr_zh": "该论文研究了双曲空间(Hyperbolic)对比学习中的机器遗忘(Machine Unlearning)问题，通过将Alignment Calibration方法适配到双曲多模态模型MERU中。研究发现，相比欧式空间(Euclidean)模型，双曲几何能更有效地重组语义层级结构，实现近乎完美的概念遗忘，同时保持保留概念的性能，特别是在多概念删除时表现更优。论文提出的双曲特有组件(如entailment calibration和norm regularization)利用了双曲空间的独特性质，揭示了多模态模型中几何特性对概念表示和删除的根本影响。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.15166v1",
      "published_date": "2025-03-19 12:47:37 UTC",
      "updated_date": "2025-03-19 12:47:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:58:16.755912"
    },
    {
      "arxiv_id": "2503.15130v1",
      "title": "A Foundational Theory for Decentralized Sensory Learning",
      "title_zh": "去中心化感知学习的基础理论",
      "authors": [
        "Linus Mårtensson",
        "Jonas M. D. Enander",
        "Udaya B. Rongala",
        "Henrik Jörntell"
      ],
      "abstract": "In both neuroscience and artificial intelligence, popular functional\nframeworks and neural network formulations operate by making use of extrinsic\nerror measurements and global learning algorithms. Through a set of conjectures\nbased on evolutionary insights on the origin of cellular adaptive mechanisms,\nwe reinterpret the core meaning of sensory signals to allow the brain to be\ninterpreted as a negative feedback control system, and show how this could lead\nto local learning algorithms without the need for global error correction\nmetrics. Thereby, a sufficiently good minima in sensory activity can be the\ncomplete reward signal of the network, as well as being both necessary and\nsufficient for biological learning to arise. We show that this method of\nlearning was likely already present in the earliest unicellular life forms on\nearth. We show evidence that the same principle holds and scales to\nmulticellular organisms where it in addition can lead to division of labour\nbetween cells. Available evidence shows that the evolution of the nervous\nsystem likely was an adaptation to more effectively communicate intercellular\nsignals to support such division of labour. We therefore propose that the same\nlearning principle that evolved already in the earliest unicellular life forms,\ni.e. negative feedback control of externally and internally generated sensor\nsignals, has simply been scaled up to become a fundament of the learning we see\nin biological brains today. We illustrate diverse biological settings, from the\nearliest unicellular organisms to humans, where this operational principle\nappears to be a plausible interpretation of the meaning of sensor signals in\nbiology, and how this relates to current neuroscientific theories and findings.",
      "tldr_zh": "本研究提出了一个基于进化生物学的去中心化感知学习基础理论。通过重新诠释感官信号的核心意义，作者将大脑建模为一个负反馈控制系统，证明局部学习算法无需全局误差修正指标即可实现，仅需感官活动的最小化作为完整奖励信号。研究表明，这种学习机制最早出现在单细胞生物中，并随着多细胞生物和神经系统的进化而扩展，成为现代生物大脑学习的基础。该理论为当前神经科学发现提供了统一解释框架，从单细胞生物到人类都适用这一负反馈控制原理。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15130v1",
      "published_date": "2025-03-19 11:44:58 UTC",
      "updated_date": "2025-03-19 11:44:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:58:37.237273"
    },
    {
      "arxiv_id": "2503.15129v1",
      "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models",
      "title_zh": "基于众包人类反馈对齐的大型语言模型代码生成强化学习",
      "authors": [
        "Man Fai Wong",
        "Chee Wei Tan"
      ],
      "abstract": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.",
      "tldr_zh": "本研究探讨了如何通过众包计算整合人类反馈，以增强大语言模型(LLM)在代码生成中的强化学习(RLHF)效果。提出了一种贝叶斯优化框架，用于在代码生成任务中实现AI对齐，通过分布式反馈收集减轻负担，并强调高质量人类反馈的价值。实验证明，该方法能有效训练LLM代理，提升文本到代码的生成能力，且该框架可推广至特定领域语言，促进AI辅助编程中LLM能力与人类反馈的对齐。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15129v1",
      "published_date": "2025-03-19 11:44:47 UTC",
      "updated_date": "2025-03-19 11:44:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:58:53.431746"
    },
    {
      "arxiv_id": "2503.15128v1",
      "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
      "title_zh": "提升微调多语言机器生成文本检测器的鲁棒性",
      "authors": [
        "Dominik Macko",
        "Robert Moro",
        "Ivan Srba"
      ],
      "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.",
      "tldr_zh": "该研究针对多语言机器生成文本检测器的鲁棒性问题，提出了一种改进的微调方法。通过增强检测模型对混淆攻击和分布外数据的泛化能力，该方法提高了现有检测器在识别机器生成有害内容方面的准确性。实验表明，改进后的模型能更可靠地区分高质量机器生成文本和人工撰写内容，为在线信息可信度评估提供了有效工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15128v1",
      "published_date": "2025-03-19 11:42:33 UTC",
      "updated_date": "2025-03-19 11:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:59:14.672012"
    },
    {
      "arxiv_id": "2503.15126v1",
      "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation",
      "title_zh": "基于文本衍生关系图增强的骨架动作分割网络",
      "authors": [
        "Haoyu Ji",
        "Bowen Chen",
        "Weihong Ren",
        "Wenze Huang",
        "Zhihao Yang",
        "Zhiyong Wang",
        "Honghai Liu"
      ],
      "abstract": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.",
      "tldr_zh": "该研究提出了一种基于文本的关系图增强网络(TRG-Net)，用于骨骼动作分割任务。该方法创新性地利用大语言模型(LLM)生成的先验关系图，通过动态时空融合建模(DSFM)和绝对-相对类间监督(ARIS)方法，有效捕捉骨骼关节与动作间的内在关联。实验表明，该网络在四个公开数据集上达到了最先进性能，为解决传统方法对骨骼特征内在相关性建模不足的问题提供了新思路。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15126v1",
      "published_date": "2025-03-19 11:38:14 UTC",
      "updated_date": "2025-03-19 11:38:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:59:42.687244"
    },
    {
      "arxiv_id": "2503.15113v1",
      "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
      "title_zh": "推理努力与问题复杂度：大语言模型中的规模分析",
      "authors": [
        "Benjamin Estermann",
        "Roger Wattenhofer"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)的推理能力如何随问题复杂度变化。通过可无限扩展的Tents拼图实验发现，模型的推理努力会随问题规模增加，但仅达到一个临界复杂度阈值；超过该阈值后，推理努力不再继续增加甚至可能下降，揭示了当前LLMs在逻辑连贯性上的关键局限。研究还表明，面对日益复杂的逻辑谜题时，当前最先进的推理模型之间存在显著性能差异，凸显了提升推理可扩展性策略的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
      "pdf_url": "http://arxiv.org/pdf/2503.15113v1",
      "published_date": "2025-03-19 11:13:51 UTC",
      "updated_date": "2025-03-19 11:13:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:59:56.127971"
    },
    {
      "arxiv_id": "2503.15108v1",
      "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making",
      "title_zh": "VIPER：面向序列决策的视觉感知与可解释推理",
      "authors": [
        "Mohamed Salim Aissi",
        "Clemence Grislain",
        "Mohamed Chetouani",
        "Olivier Sigaud",
        "Laure Soulier",
        "Nicolas Thome"
      ],
      "abstract": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
      "tldr_zh": "该研究提出了VIPER框架，结合视觉语言模型(VLM)的感知能力和大语言模型(LLM)的推理能力，用于多模态指令规划任务。VIPER通过模块化流程，首先利用VLM生成图像观察的文本描述，再由LLM策略根据任务目标预测动作。研究采用行为克隆和强化学习微调推理模块，提升决策能力。实验表明，VIPER在ALFWorld基准测试中显著优于现有视觉指令规划器，同时缩小了与纯文本规划器的性能差距。此外，VIPER通过文本作为中间表示，增强了可解释性，为感知和推理组件的细粒度分析提供了可能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15108v1",
      "published_date": "2025-03-19 11:05:42 UTC",
      "updated_date": "2025-03-19 11:05:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:00:15.895560"
    },
    {
      "arxiv_id": "2503.15095v1",
      "title": "Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control",
      "title_zh": "基于扩散模型的不确定性感知模型预测控制预测方法",
      "authors": [
        "Stelios Zarifis",
        "Ioannis Kordonis",
        "Petros Maragos"
      ],
      "abstract": "We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic\nframework for uncertainty-aware prediction and decision-making in partially\nobservable stochastic systems by integrating diffusion-based time series\nforecasting models in Model Predictive Control algorithms. In our approach, a\ndiffusion-based time series forecasting model is used to probabilistically\nestimate the evolution of the system's stochastic components. These forecasts\nare then incorporated into MPC algorithms to estimate future trajectories and\noptimize action selection under the uncertainty of the future. We evaluate the\nframework on the task of energy arbitrage, where a Battery Energy Storage\nSystem participates in the day-ahead electricity market of the New York state.\nExperimental results indicate that our model-based approach with a\ndiffusion-based forecaster significantly outperforms both implementations with\nclassical forecasting methods and model-free reinforcement learning baselines.",
      "tldr_zh": "本文提出了一种基于扩散模型的预测控制框架（D-I MPC），用于处理部分可观测随机系统中的不确定性预测与决策问题。该框架将扩散时间序列预测模型与模型预测控制（MPC）算法相结合，通过概率化预测系统随机组件的演化，并在不确定条件下优化未来轨迹和动作选择。在纽约州电力市场储能套利任务中，实验表明该扩散预测方法显著优于传统预测方法和无模型强化学习基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2.6; I.5.1"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures, 3 tables. This version is submitted to the 33rd\n  European Signal Processing Conference (EUSIPCO 2025), to be held in Isola\n  delle Femmine - Palermo - Italy, on September 8-12, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15095v1",
      "published_date": "2025-03-19 10:48:26 UTC",
      "updated_date": "2025-03-19 10:48:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:00:36.607541"
    },
    {
      "arxiv_id": "2503.15092v1",
      "title": "Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings",
      "title_zh": "探索DeepSeek模型的安全边界：评估与发现",
      "authors": [
        "Zonghao Ying",
        "Guangyi Zheng",
        "Yongxin Huang",
        "Deyue Zhang",
        "Wenxin Zhang",
        "Quanchen Zou",
        "Aishan Liu",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "abstract": "This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps://github.com/NY1024/DeepSeek-Safety-Eval.",
      "tldr_zh": "本研究首次对DeepSeek系列模型（包括大语言模型、多模态大模型和文生图模型）进行了全面的安全性评估，重点关注生成内容的安全风险。研究团队开发了适配中国社会文化背景的中英双语安全评估数据集，系统检测模型在算法歧视、色情内容等多维风险上的表现。结果显示，尽管DeepSeek模型具备强大的通用能力，但在多个安全维度仍存在显著漏洞，这为理解和完善大模型安全性提供了关键洞见。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15092v1",
      "published_date": "2025-03-19 10:44:37 UTC",
      "updated_date": "2025-03-19 10:44:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:00:57.264594"
    },
    {
      "arxiv_id": "2503.15082v1",
      "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
      "title_zh": "StyleLoco：基于生成对抗蒸馏的自然拟人机器人运动控制",
      "authors": [
        "Le Ma",
        "Ziyu Meng",
        "Tengyu Liu",
        "Yuhan Li",
        "Ran Song",
        "Wei Zhang",
        "Siyuan Huang"
      ],
      "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs.",
      "tldr_zh": "该研究提出StyleLoco框架，通过生成对抗蒸馏（GAD）技术解决人形机器人运动训练中敏捷性与自然性的矛盾问题。该方法采用两阶段策略：首先通过强化学习训练敏捷的教师策略，再利用多判别器架构同时从教师策略和动作捕捉数据中提取技能特征。实验证明，该框架既能保持强化学习的动态性能，又能实现接近人类动作的自然流畅度，成功实现了不同运动风格间的稳定迁移。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15082v1",
      "published_date": "2025-03-19 10:27:44 UTC",
      "updated_date": "2025-03-19 10:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:01:15.770083"
    },
    {
      "arxiv_id": "2503.15060v2",
      "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis",
      "title_zh": "《幻化正样本对：实现表征学习与图像合成高效统一的创新方法》",
      "authors": [
        "Imanol G. Estepa",
        "Jesús M. Rodríguez-de-Vera",
        "Ignacio Sarasúa",
        "Bhalaji Nagarajan",
        "Petia Radeva"
      ],
      "abstract": "While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.",
      "tldr_zh": "该研究提出Sorcen——一种新型统一自监督学习(SSL)框架，通过协同对比-重建目标实现了表征学习与图像生成的高效统一。其创新性\"Echo Contrast\"对比目标利用生成能力在语义token空间自动构建正样本对，无需额外图像裁剪或增强，同时完全基于预计算token运作，显著降低60.8%计算开销。在ImageNet-1k上的实验表明，Sorcen在线性探测(提升0.4%)、无条件图像生成(FID改善1.48)、小样本学习等任务全面超越现有统一SSL方法，并在单裁剪MIM任务中达到SOTA性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.5.4; I.5.1; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "The source code is available in https://github.com/ImaGonEs/Sorcen",
      "pdf_url": "http://arxiv.org/pdf/2503.15060v2",
      "published_date": "2025-03-19 09:53:11 UTC",
      "updated_date": "2025-03-20 15:09:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:01:46.701264"
    },
    {
      "arxiv_id": "2503.15058v1",
      "title": "Texture-Aware StarGAN for CT data harmonisation",
      "title_zh": "纹理感知StarGAN用于CT数据协调",
      "authors": [
        "Francesco Di Feola",
        "Ludovica Pompilio",
        "Cecilia Assolito",
        "Valerio Guarrasi",
        "Paolo Soda"
      ],
      "abstract": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.",
      "tldr_zh": "本文提出了一种纹理感知的StarGAN模型，用于解决CT影像数据因重建核(kernel)差异导致的非生物性变异问题。该方法创新性地引入多尺度纹理损失函数，通过捕捉不同空间和角度尺度的纹理特征，实现了跨多种重建核的一对多数据标准化。在包含48,667张胸部CT切片的大规模实验中，该模型显著优于基准StarGAN，为医学影像深度学习的泛化性能提供了有效解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15058v1",
      "published_date": "2025-03-19 09:50:32 UTC",
      "updated_date": "2025-03-19 09:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:02:00.249177"
    },
    {
      "arxiv_id": "2503.15049v1",
      "title": "HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation",
      "title_zh": "HAD-Gen：面向可控场景生成的人性化多样化驾驶行为建模",
      "authors": [
        "Cheng Wang",
        "Lingxin Kong",
        "Massimiliano Tamborski",
        "Stefano V. Albrecht"
      ],
      "abstract": "Simulation-based testing has emerged as an essential tool for verifying and\nvalidating autonomous vehicles (AVs). However, contemporary methodologies, such\nas deterministic and imitation learning-based driver models, struggle to\ncapture the variability of human-like driving behavior. Given these challenges,\nwe propose HAD-Gen, a general framework for realistic traffic scenario\ngeneration that simulates diverse human-like driving behaviors. The framework\nfirst clusters the vehicle trajectory data into different driving styles\naccording to safety features. It then employs maximum entropy inverse\nreinforcement learning on each of the clusters to learn the reward function\ncorresponding to each driving style. Using these reward functions, the method\nintegrates offline reinforcement learning pre-training and multi-agent\nreinforcement learning algorithms to obtain general and robust driving\npolicies. Multi-perspective simulation results show that our proposed scenario\ngeneration framework can simulate diverse, human-like driving behaviors with\nstrong generalization capability. The proposed framework achieves a 90.96%\ngoal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in\nthe generalization test, outperforming prior approaches by over 20% in\ngoal-reaching performance. The source code is released at\nhttps://github.com/RoboSafe-Lab/Sim4AD.",
      "tldr_zh": "该研究提出HAD-Gen框架，用于生成具有人类驾驶行为多样性的可控交通场景。该方法首先基于安全特征对车辆轨迹数据进行驾驶风格聚类，然后采用最大熵逆强化学习(maximum entropy inverse reinforcement learning)学习每种风格的奖励函数，并结合离线强化学习预训练与多智能体强化学习算法来获得稳健驾驶策略。实验表明，该框架能模拟多样化人类驾驶行为，在泛化测试中达成90.96%的目标到达率，各项指标优于现有方法20%以上。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15049v1",
      "published_date": "2025-03-19 09:38:45 UTC",
      "updated_date": "2025-03-19 09:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:02:28.356641"
    },
    {
      "arxiv_id": "2503.15035v1",
      "title": "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback",
      "title_zh": "GraspCorrect：基于视觉语言模型引导反馈的机器人抓取校正系统",
      "authors": [
        "Sungjae Lee",
        "Yeonjoo Hong",
        "Kwang In Kim"
      ],
      "abstract": "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
      "tldr_zh": "该研究提出GraspCorrect模块，通过视觉语言模型(VLM)引导的反馈机制改进机器人抓取性能。该系统采用迭代式视觉问答框架，结合抓取引导提示(grasp-guided prompting)和物体感知采样(object-aware sampling)两项关键技术，将中间视觉目标转化为关节动作。实验表明，该模块能显著提升现有策略模型在RLBench和CALVIN数据集上的抓取稳定性和任务成功率。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15035v1",
      "published_date": "2025-03-19 09:25:32 UTC",
      "updated_date": "2025-03-19 09:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:02:37.565823"
    },
    {
      "arxiv_id": "2503.15008v1",
      "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection",
      "title_zh": "乳腺癌检测新方法：融合区域边界学习的通道增强型残差CNN-Transformer网络",
      "authors": [
        "Aamir Mehmood",
        "Yue Hu",
        "Saddam Hussain Khan"
      ],
      "abstract": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.",
      "tldr_zh": "该研究提出了一种新型混合框架CB-Res-RBCMT，结合定制化残差CNN和新型视觉Transformer(ViT)组件，用于乳腺癌超声图像(BUSI)检测。该框架采用CNN与Transformer融合的CMT模块，通过区域边界(RB)特征提取捕捉肿瘤形态变化，并利用通道增强(CB)策略结合迁移学习提升有限数据集的表征能力。实验结果表明，该方法在标准BUSI数据集上达到95.63%的准确率和96.42%的灵敏度，显著优于现有ViT和CNN方法，展现了CNN-Transformer混合架构在医学图像分析中的优势。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986",
      "pdf_url": "http://arxiv.org/pdf/2503.15008v1",
      "published_date": "2025-03-19 08:59:02 UTC",
      "updated_date": "2025-03-19 08:59:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:02:59.287208"
    },
    {
      "arxiv_id": "2503.16547v1",
      "title": "Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis",
      "title_zh": "赋能医疗多智能体：基于临床会诊流程的动态诊断系统",
      "authors": [
        "Sihan Wang",
        "Suiyang Jiang",
        "Yibo Gao",
        "Boming Wang",
        "Shangqi Gao",
        "Xiahai Zhuang"
      ],
      "abstract": "Traditional AI-based healthcare systems often rely on single-modal data,\nlimiting diagnostic accuracy due to incomplete information. However, recent\nadvancements in foundation models show promising potential for enhancing\ndiagnosis combining multi-modal information. While these models excel in static\ntasks, they struggle with dynamic diagnosis, failing to manage multi-turn\ninteractions and often making premature diagnostic decisions due to\ninsufficient persistence in information collection.To address this, we propose\na multi-agent framework inspired by consultation flow and reinforcement\nlearning (RL) to simulate the entire consultation process, integrating multiple\nclinical information for effective diagnosis. Our approach incorporates a\nhierarchical action set, structured from clinic consultation flow and medical\ntextbook, to effectively guide the decision-making process. This strategy\nimproves agent interactions, enabling them to adapt and optimize actions based\non the dynamic state. We evaluated our framework on a public dynamic diagnosis\nbenchmark. The proposed framework evidentially improves the baseline methods\nand achieves state-of-the-art performance compared to existing foundation\nmodel-based methods.",
      "tldr_zh": "本文提出了一种基于临床问诊流程和强化学习的多智能体框架，用于解决传统AI医疗系统在动态诊断中的局限性。该方法通过分层动作集模拟完整问诊流程，整合多模态临床信息进行动态决策优化。在公开动态诊断基准测试中，该框架显著超越基线方法并达到最先进水平。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16547v1",
      "published_date": "2025-03-19 08:47:18 UTC",
      "updated_date": "2025-03-19 08:47:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:03:20.071179"
    },
    {
      "arxiv_id": "2503.16546v1",
      "title": "A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions",
      "title_zh": "深度CNN架构进展全面综述：挑战、应用与新兴研究方向",
      "authors": [
        "Saddam Hussain Khan",
        "Rashid Iqbal"
      ],
      "abstract": "Deep Convolutional Neural Networks (CNNs) have significantly advanced deep\nlearning, driving breakthroughs in computer vision, natural language\nprocessing, medical diagnosis, object detection, and speech recognition.\nArchitectural innovations including 1D, 2D, and 3D convolutional models,\ndilated and grouped convolutions, depthwise separable convolutions, and\nattention mechanisms address domain-specific challenges and enhance feature\nrepresentation and computational efficiency. Structural refinements such as\nspatial-channel exploitation, multi-path design, and feature-map enhancement\ncontribute to robust hierarchical feature extraction and improved\ngeneralization, particularly through transfer learning. Efficient preprocessing\nstrategies, including Fourier transforms, structured transforms, low-precision\ncomputation, and weight compression, optimize inference speed and facilitate\ndeployment in resource-constrained environments. This survey presents a unified\ntaxonomy that classifies CNN architectures based on spatial exploitation,\nmulti-path structures, depth, width, dimensionality expansion, channel\nboosting, and attention mechanisms. It systematically reviews CNN applications\nin face recognition, pose estimation, action recognition, text classification,\nstatistical language modeling, disease diagnosis, radiological analysis,\ncryptocurrency sentiment prediction, 1D data processing, video analysis, and\nspeech recognition. In addition to consolidating architectural advancements,\nthe review highlights emerging learning paradigms such as few-shot, zero-shot,\nweakly supervised, federated learning frameworks and future research directions\ninclude hybrid CNN-transformer models, vision-language integration, generative\nlearning, etc. This review provides a comprehensive perspective on CNN's\nevolution from 2015 to 2025, outlining key innovations, challenges, and\nopportunities.",
      "tldr_zh": "这篇综述系统梳理了深度卷积神经网络(CNN)的架构演进，全面分析了1D/2D/3D卷积、空洞卷积、分组卷积、深度可分离卷积和注意力机制等核心创新如何提升特征表示与计算效率。研究建立了基于空间利用、多路径结构、通道增强等维度的统一分类体系，涵盖计算机视觉、自然语言处理、医疗诊断等跨领域应用。特别探讨了傅里叶变换、低精度计算等预处理策略对资源受限环境的适配优化，并前瞻性地指出CNN-Transformer混合架构、视觉语言融合等未来方向。该研究为2015-2025年间CNN技术的发展提供了里程碑式的全景式总结。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "100 Pages, 44 Figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16546v1",
      "published_date": "2025-03-19 08:41:06 UTC",
      "updated_date": "2025-03-19 08:41:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:03:45.664008"
    },
    {
      "arxiv_id": "2503.14976v2",
      "title": "Application of linear regression method to the deep reinforcement learning in continuous action cases",
      "title_zh": "线性回归方法在连续动作场景下深度强化学习中的应用",
      "authors": [
        "Hisato Komatsu"
      ],
      "abstract": "The linear regression (LR) method offers the advantage that optimal\nparameters can be calculated relatively easily, although its representation\ncapability is limited than that of the deep learning technique. To improve deep\nreinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was\nproposed by Levine et al., which combines Deep Q Network (DQN) with LR method.\nHowever, the LS-DQN method assumes that the actions are discrete. In this\nstudy, we propose the Double Least Squares Deep Deterministic Policy Gradient\n(DLS-DDPG) method to address this limitation. This method combines the LR\nmethod with the Deep Deterministic Policy Gradient (DDPG) technique, one of the\nrepresentative deep reinforcement learning algorithms for continuous action\ncases. Numerical experiments conducted in MuJoCo environments showed that the\nLR update improved performance at least in some tasks, although there are\ndifficulties such as the inability to make the regularization terms small.",
      "tldr_zh": "该研究提出了一种新型深度强化学习方法DLS-DDPG，通过将线性回归(LR)与深度确定性策略梯度(DDPG)相结合，解决了现有LS-DQN方法仅适用于离散动作空间的局限性。在MuJoCo环境中的实验表明，尽管存在正则化项难以缩小等技术挑战，该混合方法在部分连续控制任务中有效提升了性能表现。这一创新为连续动作空间下的深度强化学习提供了新的优化思路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14976v2",
      "published_date": "2025-03-19 08:10:54 UTC",
      "updated_date": "2025-03-21 11:40:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:04:03.270037"
    },
    {
      "arxiv_id": "2503.14973v1",
      "title": "Behaviour Discovery and Attribution for Explainable Reinforcement Learning",
      "title_zh": "行为发现与归因：可解释强化学习的关键",
      "authors": [
        "Rishav Rishav",
        "Somjit Nath",
        "Vincent Michalski",
        "Samira Ebrahimi Kahou"
      ],
      "abstract": "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
      "tldr_zh": "该论文提出了一种用于强化学习(RL)可解释性的行为发现与归因框架，旨在解决传统显著性分析解释力不足的问题。通过识别离线RL轨迹中的有意义行为片段，该方法能够提供更精细化的行为级解释，而非笼统的长轨迹归因。该框架具有高度适应性，可跨多种环境应用，为可解释RL提供了可扩展的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14973v1",
      "published_date": "2025-03-19 08:06:00 UTC",
      "updated_date": "2025-03-19 08:06:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:04:21.673920"
    },
    {
      "arxiv_id": "2503.18958v1",
      "title": "Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI",
      "title_zh": "深度学习进阶之路：概率工程——现代人工智能的实用新范式",
      "authors": [
        "Jianyi Zhang"
      ],
      "abstract": "Recent years have witnessed the rapid progression of deep learning, pushing\nus closer to the realization of AGI (Artificial General Intelligence).\nProbabilistic modeling is critical to many of these advancements, which\nprovides a foundational framework for capturing data distributions. However, as\nthe scale and complexity of AI applications grow, traditional probabilistic\nmodeling faces escalating challenges, such as high-dimensional parameter\nspaces, heterogeneous data sources, and evolving real-world requirements often\nrender classical approaches insufficiently flexible.\n  This paper proposes a novel concept, Probability Engineering, which treats\nthe already-learned probability distributions within deep learning as\nengineering artifacts. Rather than merely fitting or inferring distributions,\nwe actively modify and reinforce them to better address the diverse and\nevolving demands of modern AI. Specifically, Probability Engineering introduces\nnovel techniques and constraints to refine existing probability distributions,\nimproving their robustness, efficiency, adaptability, or trustworthiness.\n  We showcase this paradigm through a series of applications spanning Bayesian\ndeep learning, Edge AI (including federated learning and knowledge\ndistillation), and Generative AI (such as text-to-image generation with\ndiffusion models and high-quality text generation with large language models).\nThese case studies demonstrate how probability distributions once treated as\nstatic objects can be engineered to meet the diverse and evolving requirements\nof large-scale, data-intensive, and trustworthy AI systems. By systematically\nexpanding and strengthening the role of probabilistic modeling, Probability\nEngineering paves the way for more robust, adaptive, efficient, and trustworthy\ndeep learning solutions in today's fast-growing AI era.",
      "tldr_zh": "该研究提出\"概率工程\"(Probability Engineering)新范式，将深度学习中的概率分布视为可主动优化的工程对象，而非静态建模结果。通过引入新型约束和技术对既有概率分布进行修正强化，该方法提升了贝叶斯深度学习、边缘AI(如联邦学习)和生成式AI(如扩散模型)等领域的模型鲁棒性、适应性与可信度。案例研究表明，这种动态优化概率分布的工程化思路能有效满足大规模可信AI系统的演进需求，为深度学习在AGI时代的发展提供了新路径。",
      "categories": [
        "cs.AI",
        "math.PR",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "Ph.D. dissertation",
      "pdf_url": "http://arxiv.org/pdf/2503.18958v1",
      "published_date": "2025-03-19 07:48:23 UTC",
      "updated_date": "2025-03-19 07:48:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:04:39.725853"
    },
    {
      "arxiv_id": "2503.14950v1",
      "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network",
      "title_zh": "USAM-Net：基于U-Net架构的网络，利用预训练图像分割网络特征提升立体匹配与场景深度估计性能",
      "authors": [
        "Joseph Emmanuel DL Dayo",
        "Prospero C. Naval Jr"
      ],
      "abstract": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data.",
      "tldr_zh": "本文提出USAM-Net网络，这是一种基于U-Net的新型卷积神经网络，通过整合立体图像输入、语义分割图和注意力机制来提升深度估计精度。该网络采用双路径架构，将预训练分割模型(SAM)与深度估计模型相结合，利用语义分割信息强化对物体边界和表面纹理等关键特征的学习。在DrivingStereo数据集上的实验表明，USAM-Net以3.61%的全局差异(GD)和0.88的端点误差(EPE)超越了CFNet等传统模型，验证了语义信息融合对立体深度估计任务的有效提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14950v1",
      "published_date": "2025-03-19 07:29:02 UTC",
      "updated_date": "2025-03-19 07:29:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:05:02.167201"
    },
    {
      "arxiv_id": "2503.14935v1",
      "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding",
      "title_zh": "FAVOR-Bench：面向细粒度视频运动理解的综合基准测试",
      "authors": [
        "Chongjun Tu",
        "Lin Zhang",
        "Pengtao Chen",
        "Peng Ye",
        "Xianfang Zeng",
        "Wei Cheng",
        "Gang Yu",
        "Tao Chen"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.",
      "tldr_zh": "该研究提出了FAVOR-Bench，首个针对多模态大语言模型(MLLMs)细粒度视频运动理解的综合评测基准，包含1,776段带结构化标注的视频数据。通过8,184道选择题和开放式任务评测21个前沿MLLMs，发现现有模型在捕捉视频时序动态方面存在显著缺陷。为此研究者进一步构建了17,152段视频的训练集FAVOR-Train，实验表明基于该数据集微调的Qwen2.5-VL模型在多个运动理解任务上获得稳定提升。该工作为开发更强大的视频理解模型提供了标准化评测工具和训练资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "FAVOR-Bench project page: https://favor-bench.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.14935v1",
      "published_date": "2025-03-19 06:42:32 UTC",
      "updated_date": "2025-03-19 06:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:05:21.723250"
    },
    {
      "arxiv_id": "2503.14928v1",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "title_zh": "嘘！让我们从无声视频中想象一段真实的语音",
      "authors": [
        "Jiaxin Ye",
        "Hongming Shan"
      ],
      "abstract": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.",
      "tldr_zh": "该研究提出ImaginTalk，一种基于跨模态扩散框架的视觉引导语音生成方法，仅依靠视频输入即可生成高保真语音。该方法创新性地采用离散唇形对齐器提取语义信息，并结合BERT进行错误修正；同时通过配备面部风格适配器的风格扩散Transformer增强音色和情感表现力。实验表明，相比现有技术，该系统生成的语音在语义准确性、音色和情感表达方面均有显著提升，为无声视频配音等应用提供了新方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://imagintalk.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.14928v1",
      "published_date": "2025-03-19 06:28:17 UTC",
      "updated_date": "2025-03-19 06:28:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:05:46.192942"
    },
    {
      "arxiv_id": "2503.16544v1",
      "title": "Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies",
      "title_zh": "因果发现与反事实推理优化说服性对话策略",
      "authors": [
        "Donghuo Zeng",
        "Roberto Legaspi",
        "Yuewen Sun",
        "Xinshuai Dong",
        "Kazushi Ikeda",
        "Peter Spirtes",
        "Kun Zhang"
      ],
      "abstract": "Tailoring persuasive conversations to users leads to more effective\npersuasion. However, existing dialogue systems often struggle to adapt to\ndynamically evolving user states. This paper presents a novel method that\nleverages causal discovery and counterfactual reasoning for optimizing system\npersuasion capability and outcomes. We employ the Greedy Relaxation of the\nSparsest Permutation (GRaSP) algorithm to identify causal relationships between\nuser and system utterance strategies, treating user strategies as states and\nsystem strategies as actions. GRaSP identifies user strategies as causal\nfactors influencing system responses, which inform Bidirectional Conditional\nGenerative Adversarial Networks (BiCoGAN) in generating counterfactual\nutterances for the system. Subsequently, we use the Dueling Double Deep\nQ-Network (D3QN) model to utilize counterfactual data to determine the best\npolicy for selecting system utterances. Our experiments with the\nPersuasionForGood dataset show measurable improvements in persuasion outcomes\nusing our approach over baseline methods. The observed increase in cumulative\nrewards and Q-values highlights the effectiveness of causal discovery in\nenhancing counterfactual reasoning and optimizing reinforcement learning\npolicies for online dialogue systems.",
      "tldr_zh": "该研究提出了一种结合因果发现与反事实推理的新型对话策略优化方法，用于提升劝说对话系统的适应性。通过GRaSP算法识别用户策略与系统响应间的因果关系，并利用BiCoGAN生成反事实对话样本，最终采用D3QN强化学习模型优化对话策略。在PersuasionForGood数据集上的实验表明，该方法相比基线模型显著提高了劝说效果，其累积奖励和Q值均有明显提升，验证了因果发现对强化学习策略优化的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16544v1",
      "published_date": "2025-03-19 06:06:10 UTC",
      "updated_date": "2025-03-19 06:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:06:02.012109"
    },
    {
      "arxiv_id": "2503.14922v1",
      "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
      "title_zh": "针对图卷积网络的语义化清洁标签后门攻击",
      "authors": [
        "Jiazhu Dai",
        "Haoyu Sun"
      ],
      "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples.",
      "tldr_zh": "该研究提出了一种针对图卷积网络(GCNs)的语义式干净标签后门攻击(SCLBA)，揭示了GCN在图形分类任务中存在的安全漏洞。该方法通过重要性分析选择特定节点作为语义触发器，在不改变样本标签的情况下将触发器插入图中生成毒化样本，使被攻击模型在保持正常样本分类性能的同时，对触发样本实现高达99%的攻击成功率。实验表明，仅需不到3%的毒化率即可达成攻击效果，突显了GCN模型在实际应用中的潜在安全风险。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14922v1",
      "published_date": "2025-03-19 06:04:55 UTC",
      "updated_date": "2025-03-19 06:04:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:06:21.762413"
    },
    {
      "arxiv_id": "2503.14917v1",
      "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
      "title_zh": "MASS：基于技能图谱的数学数据筛选框架用于大语言模型预训练",
      "authors": [
        "Jiazheng Li",
        "Lu Yu",
        "Qing Cui",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Yanfang Ye",
        "Chuxu Zhang"
      ],
      "abstract": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.",
      "tldr_zh": "本研究提出了MASS框架，通过构建技能图(Skill Graph)来优化数学推理领域的大语言模型(LLMs)预训练数据选择。该方法利用数学技能及其关联关系，为目标数据集分配质量评分，从而筛选出高质量子集用于预训练。实验表明，MASS在不同模型规模(1B和7B)和预训练数据集(网络数据和合成数据)上均表现出色：在效率方面，使用MASS筛选的数据训练模型，仅需30%-50%的token即可达到与全数据集相当的性能；在效果方面，在相同token数量下，MASS筛选数据训练的模型性能提升了3.3%-5.9%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14917v1",
      "published_date": "2025-03-19 05:50:21 UTC",
      "updated_date": "2025-03-19 05:50:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:06:44.290903"
    },
    {
      "arxiv_id": "2503.14908v1",
      "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation",
      "title_zh": "POSTA：定制化艺术海报生成的通用框架",
      "authors": [
        "Haoyu Chen",
        "Xiaojie Xu",
        "Wenbo Li",
        "Jingjing Ren",
        "Tian Ye",
        "Songhua Liu",
        "Ying-Cong Chen",
        "Lei Zhu",
        "Xinchao Wang"
      ],
      "abstract": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality.",
      "tldr_zh": "本文提出了POSTA框架，这是一个基于扩散模型和多模态大语言模型(MLLMs)的模块化系统，专门用于定制化艺术海报生成。该框架包含三个核心模块：主题背景生成(Background Diffusion)、布局排版设计(Design MLLM)和艺术文本增强(ArtText Diffusion)，通过协同工作确保内容准确性和视觉吸引力。研究团队还构建了PosterArt数据集用于模型训练，包含高质量艺术海报及其标注信息。实验结果表明，POSTA在文本准确性和美学质量上均优于现有方法，为电影、展览等艺术领域提供了高度可控且多样化的海报生成解决方案。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14908v1",
      "published_date": "2025-03-19 05:22:38 UTC",
      "updated_date": "2025-03-19 05:22:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:07:02.896243"
    },
    {
      "arxiv_id": "2503.14900v1",
      "title": "Deep Contrastive Unlearning for Language Models",
      "title_zh": "深度对比遗忘：面向语言模型的遗忘学习",
      "authors": [
        "Estrid He",
        "Tabinda Sarwar",
        "Ibrahim Khalil",
        "Xun Yi",
        "Ke Wang"
      ],
      "abstract": "The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods.",
      "tldr_zh": "该研究提出DeepCUT框架，通过深度对比学习优化语言模型的潜在空间分布，实现高效机器遗忘(machine unlearning)。针对现有方法忽视潜在空间几何结构的问题，该框架在保护模型预测性能的同时，有效移除特定训练样本的知识。实验表明，DeepCUT在真实数据集上显著优于基线方法，为语言模型实现\"被遗忘权\"提供了新解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14900v1",
      "published_date": "2025-03-19 04:58:45 UTC",
      "updated_date": "2025-03-19 04:58:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:07:21.553823"
    },
    {
      "arxiv_id": "2503.14895v1",
      "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations",
      "title_zh": "通过多频扰动缓解多模态大语言模型中的物体幻觉问题",
      "authors": [
        "Shuo Li",
        "Jiajun Sun",
        "Guodong Zheng",
        "Xiaoran Fan",
        "Yujiong Shen",
        "Yi Lu",
        "Zhiheng Xi",
        "Yuming Yang",
        "Wenming Tan",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.",
      "tldr_zh": "该研究提出了多频扰动(MFP)方法，用于缓解多模态大语言模型(MLLMs)中的物体幻觉问题。研究发现，MLLMs生成虚假物体的主要原因是模型过度依赖图像特定频率特征。MFP通过同时利用图像低频和高频特征扰动视觉表示，显式抑制冗余频域特征，从而有效减少幻觉。实验表明，该方法能显著降低各类模型架构的物体幻觉，且在训练阶段可与推理阶段方法结合，在CHAIR基准上达到最优性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14895v1",
      "published_date": "2025-03-19 04:39:45 UTC",
      "updated_date": "2025-03-19 04:39:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:07:41.514715"
    },
    {
      "arxiv_id": "2503.14891v1",
      "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer",
      "title_zh": "MetaLadder：通过类比问题推理迁移提升数学解题质量",
      "authors": [
        "Honglin Lin",
        "Zhuoshi Pan",
        "Yu Li",
        "Qizhi Pei",
        "Xin Gao",
        "Mengzhang Cai",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.",
      "tldr_zh": "该研究提出MetaLadder框架，通过类比问题推理迁移提升大语言模型(LLMs)解决数学问题的能力。该框架模仿人类解题策略，先让LLMs回忆结构/语义相似的元问题(meta-problems)及其思维链(CoT)解决方案，再结合问题重述机制增强理解，实现\"从案例学习\"的推理迁移。实验表明，相比标准CoT方法，MetaLadder在数学基准测试中准确率提升10.3%，显著优于其他方法。该工作为LLMs的类比推理能力提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14891v1",
      "published_date": "2025-03-19 04:36:35 UTC",
      "updated_date": "2025-03-19 04:36:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:08:01.392242"
    },
    {
      "arxiv_id": "2503.14883v1",
      "title": "Envisioning an AI-Enhanced Mental Health Ecosystem",
      "title_zh": "AI赋能心理健康生态系统的构想",
      "authors": [
        "Kellie Yu Hui Sim",
        "Kenny Tsu Wei Choo"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.",
      "tldr_zh": "该研究探讨了AI在心理健康领域的应用潜力，提出了一种以人为中心的混合生态系统，旨在利用大型语言模型(LLMs)和智能体AI技术补充人类干预，提供可扩展且情境感知的支持。研究涵盖了AI在同伴支持、自助干预、主动监测和数据驱动洞察等方面的应用，同时强调了伦理、透明度、隐私和过度依赖等挑战。研究主张AI应辅助而非取代人类提供者，并提出了未来研究方向，以完善符合伦理和文化敏感准则的AI增强干预措施。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.0"
      ],
      "primary_category": "cs.HC",
      "comment": "5 pages, 0 figures, accepted to the CHI'25 Envisioning the Future of\n  Interactive Health Workshop, to be published in HAL",
      "pdf_url": "http://arxiv.org/pdf/2503.14883v1",
      "published_date": "2025-03-19 04:21:38 UTC",
      "updated_date": "2025-03-19 04:21:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:08:24.773867"
    },
    {
      "arxiv_id": "2503.14881v1",
      "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers",
      "title_zh": "探索视觉自回归Transformer中KV缓存压缩的极限",
      "authors": [
        "Bo Chen",
        "Xiaoyu Li",
        "Yekun Ke",
        "Yingyu Liang",
        "Zhenmei Shi",
        "Zhao Song"
      ],
      "abstract": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
      "tldr_zh": "本研究首次形式化定义了视觉自回归Transformer中的KV缓存压缩问题，并证明了一个基本的下界结果：在注意力架构下，顺序生成视觉token的机制必须至少使用$\\Omega(n^2 d)$的内存（当$d = \\Omega(\\log n)$时），其中$n$是生成的token数量，$d$是嵌入维度。这表明，在没有额外结构约束的情况下，实现真正次二次方内存使用是不可能的。研究通过从计算下界问题中推导，并借鉴降维技术的随机嵌入方法，构建了这一证明。此外，还探讨了视觉表示中的稀疏性先验对内存效率的影响，提出了不可能性结果及缓解内存开销的潜在方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14881v1",
      "published_date": "2025-03-19 04:18:57 UTC",
      "updated_date": "2025-03-19 04:18:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:08:53.547258"
    },
    {
      "arxiv_id": "2503.14868v1",
      "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation",
      "title_zh": "无需反向传播的高效量化扩散模型个性化方法",
      "authors": [
        "Hoigi Seo",
        "Wongi Jeong",
        "Kyungryeol Lee",
        "Se Young Chun"
      ],
      "abstract": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$.",
      "tldr_zh": "本研究提出了一种无需反向传播的高效量化扩散模型个性化方法。针对现有量化模型在微调时仍需大量内存的问题，作者采用Textual Inversion技术结合零阶优化算法，避免了传统反向传播所需的高昂梯度存储开销。为解决少量样本个性化时零阶优化梯度估计噪声大的问题，创新性地提出了子空间梯度投影技术(Subspace Gradient)和部分均匀时间步采样策略(Partial Uniform Timestep Sampling)。实验表明，该方法在保持Stable Diffusion模型个性化性能的同时，可降低8.2倍训练内存需求，使量化模型能在移动设备上高效运行。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14868v1",
      "published_date": "2025-03-19 03:45:37 UTC",
      "updated_date": "2025-03-19 03:45:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:09:02.038458"
    },
    {
      "arxiv_id": "2503.14858v2",
      "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "title_zh": "千层网络赋能自监督强化学习：深度扩展解锁全新目标达成能力",
      "authors": [
        "Kevin Wang",
        "Ishaan Javali",
        "Michał Bortkiewicz",
        "Tomasz Trzciński",
        "Benjamin Eysenbach"
      ],
      "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and\nvision, yet comparable progress has remained elusive in reinforcement learning\n(RL). In this paper, we study building blocks for self-supervised RL that\nunlock substantial improvements in scalability, with network depth serving as a\ncritical factor. Whereas most RL papers in recent years have relied on shallow\narchitectures (around 2 - 5 layers), we demonstrate that increasing the depth\nup to 1024 layers can significantly boost performance. Our experiments are\nconducted in an unsupervised goal-conditioned setting, where no demonstrations\nor rewards are provided, so an agent must explore (from scratch) and learn how\nto maximize the likelihood of reaching commanded goals. Evaluated on simulated\nlocomotion and manipulation tasks, our approach increases performance by\n$2\\times$ - $50\\times$. Increasing the model depth not only increases success\nrates but also qualitatively changes the behaviors learned.",
      "tldr_zh": "该论文探索了自监督强化学习(RL)中的深度网络扩展，发现将网络深度提升至1024层可显著提升性能。在无监督目标条件设置下，智能体无需演示或奖励，通过探索和学习最大化达到指定目标的可能性。实验表明，深度扩展不仅提高了成功率，还改变了学习到的行为模式，在模拟运动和操作任务中性能提升2到50倍。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Link to project website:\n  https://wang-kevin3290.github.io/scaling-crl/",
      "pdf_url": "http://arxiv.org/pdf/2503.14858v2",
      "published_date": "2025-03-19 03:33:57 UTC",
      "updated_date": "2025-03-22 22:24:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:09:19.439083"
    },
    {
      "arxiv_id": "2503.14847v1",
      "title": "Project Jenkins: Turning Monkey Neural Data into Robotic Arm Movement, and Back",
      "title_zh": "Project Jenkins：将猴子神经数据转化为机械臂运动，并逆向转换",
      "authors": [
        "Andrii Zahorodnii",
        "Dima Yanovsky"
      ],
      "abstract": "Project Jenkins explores how neural activity in the brain can be decoded into\nrobotic movement and, conversely, how movement patterns can be used to generate\nsynthetic neural data. Using real neural data recorded from motor and premotor\ncortex areas of a macaque monkey named Jenkins, we develop models for decoding\n(converting brain signals into robotic arm movements) and encoding (simulating\nbrain activity corresponding to a given movement). For the interface between\nthe brain simulation and the physical world, we utilized Koch v1.1 leader and\nfollower robotic arms. We developed an interactive web console that allows\nusers to generate synthetic brain data from joystick movements in real time.\nOur results are a step towards brain-controlled robotics, prosthetics, and\nenhancing normal motor function. By accurately modeling brain activity, we take\na step toward flexible brain-computer interfaces that generalize beyond\npredefined movements. To support the research community, we provide open source\ntools for both synthetic data generation and neural decoding, fostering\nreproducibility and accelerating progress. The project is available at\nhttps://www.808robots.com/projects/jenkins",
      "tldr_zh": "这项研究提出了\"Project Jenkins\"，探索如何将猴子的神经活动数据转化为机械臂运动，并反向生成合成神经数据。研究团队利用名为Jenkins的猕猴运动皮层神经数据，开发了双向编解码模型：既能将脑信号解码为机械臂运动，又能根据运动生成模拟神经活动。该系统采用Koch v1.1主从机械臂作为物理接口，并开发了实时网络控制台，允许用户通过摇杆运动生成合成脑数据。该成果为脑控机器人、假肢技术及运动功能增强提供了新思路，其开源工具支持神经编解码和合成数据生成，推动脑机接口领域发展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SP",
        "q-bio.NC"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 5 figures, project webpage and github",
      "pdf_url": "http://arxiv.org/pdf/2503.14847v1",
      "published_date": "2025-03-19 03:12:17 UTC",
      "updated_date": "2025-03-19 03:12:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:09:43.547362"
    },
    {
      "arxiv_id": "2503.14833v1",
      "title": "Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability",
      "title_zh": "好奇心扩散器：以好奇心引导扩散模型提升可靠性",
      "authors": [
        "Zihao Liu",
        "Xing Liu",
        "Yizhai Zhang",
        "Zhengxiong Liu",
        "Panfeng Huang"
      ],
      "abstract": "One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com/CarlDegio/Curiosity-Diffuser",
      "tldr_zh": "该研究提出Curiosity-Diffuser方法，通过随机网络蒸馏(RND)好奇心模块评估神经网络策略与训练数据的偏离程度，并利用分类器引导扩散模型生成低好奇度的轨迹，从而提升机器人策略的可靠性。该方法创新性地将好奇心机制与扩散模型结合，有效减少了策略过泛化现象，并通过提出的计算高效指标量化策略可靠性。实验证明，该方法在多种场景下显著提升了任务性能，使生成行为更贴近训练数据分布。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14833v1",
      "published_date": "2025-03-19 02:25:36 UTC",
      "updated_date": "2025-03-19 02:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:10:01.544890"
    },
    {
      "arxiv_id": "2503.14828v1",
      "title": "The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval",
      "title_zh": "CLEF-2025 CheckThat!实验室：主观性识别、事实核查、主张规范化与检索",
      "authors": [
        "Firoj Alam",
        "Julia Maria Struß",
        "Tanmoy Chakraborty",
        "Stefan Dietze",
        "Salim Hafid",
        "Katerina Korre",
        "Arianna Muti",
        "Preslav Nakov",
        "Federico Ruggeri",
        "Sebastian Schellhammer",
        "Vinay Setty",
        "Megha Sundriyal",
        "Konstantin Todorov",
        "Venktesh V"
      ],
      "abstract": "The CheckThat! lab aims to advance the development of innovative technologies\ndesigned to identify and counteract online disinformation and manipulation\nefforts across various languages and platforms. The first five editions focused\non key tasks in the information verification pipeline, including\ncheck-worthiness, evidence retrieval and pairing, and verification. Since the\n2023 edition, the lab has expanded its scope to address auxiliary tasks that\nsupport research and decision-making in verification. In the 2025 edition, the\nlab revisits core verification tasks while also considering auxiliary\nchallenges. Task 1 focuses on the identification of subjectivity (a follow-up\nfrom CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets\nfact-checking numerical claims, and Task 4 explores scientific web discourse\nprocessing. These tasks present challenging classification and retrieval\nproblems at both the document and span levels, including multilingual settings.",
      "tldr_zh": "CLEF-2025 CheckThat! 实验室专注于开发反在线虚假信息的技术，涵盖多语言和多平台场景。本次新增四项核心任务：主观性识别（Task 1）、主张规范化（Task 2）、数值主张事实核查（Task 3）以及科学网络话语处理（Task 4）。这些任务涉及文档级和片段级的分类与检索挑战，延续了该实验室在信息验证流程（包括核查价值判断、证据检索配对等）的技术积累，同时拓展了支持验证研究的辅助功能维度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "misinformation, factuality, fact-checking, fact-checkers,\n  check-worthiness, Social Media Platforms",
      "pdf_url": "http://arxiv.org/pdf/2503.14828v1",
      "published_date": "2025-03-19 02:06:07 UTC",
      "updated_date": "2025-03-19 02:06:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:10:35.951988"
    },
    {
      "arxiv_id": "2503.14827v1",
      "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
      "title_zh": "MMDT：解码多模态基础模型的可信度与安全性",
      "authors": [
        "Chejian Xu",
        "Jiawei Zhang",
        "Zhaorun Chen",
        "Chulin Xie",
        "Mintong Kang",
        "Yujin Potter",
        "Zhun Wang",
        "Zhuowen Yuan",
        "Alexander Xiong",
        "Zidi Xiong",
        "Chenhui Zhang",
        "Lingzhi Yuan",
        "Yi Zeng",
        "Peiyang Xu",
        "Chengquan Guo",
        "Andy Zhou",
        "Jeffrey Ziwei Tan",
        "Xuandong Zhao",
        "Francesco Pinto",
        "Zhen Xiang",
        "Yu Gai",
        "Zinan Lin",
        "Dan Hendrycks",
        "Bo Li",
        "Dawn Song"
      ],
      "abstract": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.",
      "tldr_zh": "该研究提出了首个多模态基础模型(MMFMs)安全可信度统一评估平台MMDT，填补了现有评测体系在安全性、幻觉、公平性等多维度评估的空白。通过设计不同任务场景下的红队测试算法和挑战性数据，该平台从安全性、隐私保护、对抗鲁棒性等六个维度全面评估模型性能。实验揭示了当前多模态模型存在的一系列漏洞，为开发更安全可靠的AI系统提供了基准工具和重要洞见。该平台已开源，网址为mmdecodingtrust.github.io。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14827v1",
      "published_date": "2025-03-19 01:59:44 UTC",
      "updated_date": "2025-03-19 01:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:10:45.400901"
    },
    {
      "arxiv_id": "2503.14809v1",
      "title": "Learning with Expert Abstractions for Efficient Multi-Task Continuous Control",
      "title_zh": "基于专家抽象的多任务连续控制高效学习",
      "authors": [
        "Jeff Jewett",
        "Sandhya Saisubramanian"
      ],
      "abstract": "Decision-making in complex, continuous multi-task environments is often\nhindered by the difficulty of obtaining accurate models for planning and the\ninefficiency of learning purely from trial and error. While precise environment\ndynamics may be hard to specify, human experts can often provide high-fidelity\nabstractions that capture the essential high-level structure of a task and user\npreferences in the target environment. Existing hierarchical approaches often\ntarget discrete settings and do not generalize across tasks. We propose a\nhierarchical reinforcement learning approach that addresses these limitations\nby dynamically planning over the expert-specified abstraction to generate\nsubgoals to learn a goal-conditioned policy. To overcome the challenges of\nlearning under sparse rewards, we shape the reward based on the optimal state\nvalue in the abstract model. This structured decision-making process enhances\nsample efficiency and facilitates zero-shot generalization. Our empirical\nevaluation on a suite of procedurally generated continuous control environments\ndemonstrates that our approach outperforms existing hierarchical reinforcement\nlearning methods in terms of sample efficiency, task completion rate,\nscalability to complex tasks, and generalization to novel scenarios.",
      "tldr_zh": "该论文提出了一种基于专家抽象知识的层次化强化学习方法，用于解决复杂连续多任务控制问题。该方法通过动态规划专家提供的高层任务抽象来生成子目标，并训练一个目标条件策略，同时利用抽象模型的最优状态值进行奖励塑形以缓解稀疏奖励问题。实验表明，该方法在程序化生成的连续控制环境中，相比现有层次化强化学习方法，在样本效率、任务完成率、复杂任务扩展性以及零样本泛化能力等方面均表现出优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 6 figures. Submitted to RLC 2025. Code and experiments at\n  https://github.com/Intelligent-Reliable-Autonomous-Systems/gcrs-expert-abstractions",
      "pdf_url": "http://arxiv.org/pdf/2503.14809v1",
      "published_date": "2025-03-19 00:44:23 UTC",
      "updated_date": "2025-03-19 00:44:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:11:00.926006"
    },
    {
      "arxiv_id": "2503.14800v1",
      "title": "Long Context Modeling with Ranked Memory-Augmented Retrieval",
      "title_zh": "长上下文建模与基于排序的记忆增强检索",
      "authors": [
        "Ghadir Alselwi",
        "Hao Xue",
        "Shoaib Jameel",
        "Basem Suleiman",
        "Flora D. Salim",
        "Imran Razzak"
      ],
      "abstract": "Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.",
      "tldr_zh": "该研究提出了一种新型的长文本建模框架ERMAR，通过动态相关性排序机制改进记忆检索效率。不同于传统方法，该模型创新性地结合了信息检索中的学习排序(learning-to-rank)技术，开发了基于键值嵌入的相关性评分和逐点重排模型。实验表明，ERMAR在标准测试集上取得了最先进的性能表现，为语言模型的长上下文记忆管理提供了有效解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14800v1",
      "published_date": "2025-03-19 00:24:01 UTC",
      "updated_date": "2025-03-19 00:24:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:11:24.254897"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 96,
  "processed_papers_count": 96,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T17:12:55.710134"
}
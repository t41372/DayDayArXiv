[
  {
    "arxiv_id": "2503.15739v1",
    "title": "ECLAIR: Enhanced Clarification for Interactive Responses",
    "authors": [
      "John Murzaku",
      "Zifan Liu",
      "Md Mehrab Tanjim",
      "Vaishnavi Muppala",
      "Xiang Chen",
      "Yunyao Li"
    ],
    "abstract": "We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel\nunified and end-to-end framework for interactive disambiguation in enterprise\nAI assistants. ECLAIR generates clarification questions for ambiguous user\nqueries and resolves ambiguity based on the user's response.We introduce a\ngeneralized architecture capable of integrating ambiguity information from\nmultiple downstream agents, enhancing context-awareness in resolving\nambiguities and allowing enterprise specific definition of agents. We further\ndefine agents within our system that provide domain-specific grounding\ninformation. We conduct experiments comparing ECLAIR to few-shot prompting\ntechniques and demonstrate ECLAIR's superior performance in clarification\nquestion generation and ambiguity resolution.",
    "categories": [
      "cs.AI",
      "68T50",
      "I.2.7; H.5.2"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15739v1",
    "published_date": "2025-03-19 23:04:00 UTC",
    "updated_date": "2025-03-19 23:04:00 UTC"
  },
  {
    "arxiv_id": "2503.17403v1",
    "title": "ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models",
    "authors": [
      "Azim Akhtarshenas",
      "Afshin Dini",
      "Navid Ayoobi"
    ],
    "abstract": "Large Language Models (LLMs) have revo lutionized natural language processing\nNatural Language Processing (NLP), with Chat Generative Pre-trained Transformer\n(ChatGPT) standing out as a notable exampledue to its advanced capabilities and\nwidespread applications. This survey provides a comprehensive analysis of\nChatGPT, exploring its architecture, training processes, and functionalities.\nWe examine its integration into various domains across industries such as\ncustomer service, education, healthcare, and entertainment. A comparative\nanalysis with other LLMs highlights ChatGPT's unique features and performance\nmetrics. Regarding benchmarks, the paper examines ChatGPT's comparative\nperformance against other LLMs and discusses potential risks such as\nmisinformation, bias, and data privacy concerns. Additionally, we offer a\nnumber of figures and tables that outline the backdrop of the discussion, the\nmain ideas of the article, the numerous LLM models, a thorough list of datasets\nused for pre-training, fine-tuning, and evaluation, as well as particular LLM\napplications with pertinent references. Finally, we identify future research\ndirections and technological advancements, underscoring the evolving landscape\nof LLMs and their profound impact on artificial intelligence Artificial\nIntelligence (AI) and society.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17403v1",
    "published_date": "2025-03-19 22:55:08 UTC",
    "updated_date": "2025-03-19 22:55:08 UTC"
  },
  {
    "arxiv_id": "2503.15726v1",
    "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
    "authors": [
      "Joseph Emmanuel DL Dayo",
      "Michel Onasis S. Ogbinar",
      "Prospero C. Naval Jr"
    ],
    "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. Submitted to the 31st International Conference on Neural\n  Information Processing (ICONIP 2024)",
    "pdf_url": "http://arxiv.org/pdf/2503.15726v1",
    "published_date": "2025-03-19 22:48:20 UTC",
    "updated_date": "2025-03-19 22:48:20 UTC"
  },
  {
    "arxiv_id": "2503.15724v1",
    "title": "Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning",
    "authors": [
      "Linji Wang",
      "Tong Xu",
      "Yuanjie Lu",
      "Xuesu Xiao"
    ],
    "abstract": "Robotics Reinforcement Learning (RL) often relies on carefully engineered\nauxiliary rewards to supplement sparse primary learning objectives to\ncompensate for the lack of large-scale, real-world, trial-and-error data. While\nthese auxiliary rewards accelerate learning, they require significant\nengineering effort, may introduce human biases, and cannot adapt to the robot's\nevolving capabilities during training. In this paper, we introduce Reward\nTraining Wheels (RTW), a teacher-student framework that automates auxiliary\nreward adaptation for robotics RL. To be specific, the RTW teacher dynamically\nadjusts auxiliary reward weights based on the student's evolving capabilities\nto determine which auxiliary reward aspects require more or less emphasis to\nimprove the primary objective. We demonstrate RTW on two challenging robot\ntasks: navigation in highly constrained spaces and off-road vehicle mobility on\nvertically challenging terrain. In simulation, RTW outperforms expert-designed\nrewards by 2.35% in navigation success rate and improves off-road mobility\nperformance by 122.62%, while achieving 35% and 3X faster training efficiency,\nrespectively. Physical robot experiments further validate RTW's effectiveness,\nachieving a perfect success rate (5/5 trials vs. 2/5 for expert-designed\nrewards) and improving vehicle stability with up to 47.4% reduction in\norientation angles.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15724v1",
    "published_date": "2025-03-19 22:45:59 UTC",
    "updated_date": "2025-03-19 22:45:59 UTC"
  },
  {
    "arxiv_id": "2503.15707v1",
    "title": "Safety Aware Task Planning via Large Language Models in Robotics",
    "authors": [
      "Azal Ahmad Khan",
      "Michael Andrev",
      "Muhammad Ali Murtaza",
      "Sergio Aguilera",
      "Rui Zhang",
      "Jie Ding",
      "Seth Hutchinson",
      "Ali Anwar"
    ],
    "abstract": "The integration of large language models (LLMs) into robotic task planning\nhas unlocked better reasoning capabilities for complex, long-horizon workflows.\nHowever, ensuring safety in LLM-driven plans remains a critical challenge, as\nthese models often prioritize task completion over risk mitigation. This paper\nintroduces SAFER (Safety-Aware Framework for Execution in Robotics), a\nmulti-LLM framework designed to embed safety awareness into robotic task\nplanning. SAFER employs a Safety Agent that operates alongside the primary task\nplanner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge,\na novel metric leveraging LLMs as evaluators to quantify safety violations\nwithin generated task plans. Our framework integrates safety feedback at\nmultiple stages of execution, enabling real-time risk assessment, proactive\nerror correction, and transparent safety evaluation. We also integrate a\ncontrol framework using Control Barrier Functions (CBFs) to ensure safety\nguarantees within SAFER's task planning. We evaluated SAFER against\nstate-of-the-art LLM planners on complex long-horizon tasks involving\nheterogeneous robotic agents, demonstrating its effectiveness in reducing\nsafety violations while maintaining task efficiency. We also verify the task\nplanner and safety planner through actual hardware experiments involving\nmultiple robots and a human.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15707v1",
    "published_date": "2025-03-19 21:41:10 UTC",
    "updated_date": "2025-03-19 21:41:10 UTC"
  },
  {
    "arxiv_id": "2503.15703v1",
    "title": "Predicting Multi-Agent Specialization via Task Parallelizability",
    "authors": [
      "Elizabeth Mieczkowski",
      "Ruaridh Mon-Williams",
      "Neil Bramley",
      "Christopher G. Lucas",
      "Natalia Velez",
      "Thomas L. Griffiths"
    ],
    "abstract": "Multi-agent systems often rely on specialized agents with distinct roles\nrather than general-purpose agents that perform the entire task independently.\nHowever, the conditions that govern the optimal degree of specialization remain\npoorly understood. In this work, we propose that specialist teams outperform\ngeneralist ones when environmental constraints limit task parallelizability --\nthe potential to execute task components concurrently. Drawing inspiration from\ndistributed systems, we introduce a heuristic to predict the relative\nefficiency of generalist versus specialist teams by estimating the speed-up\nachieved when two agents perform a task in parallel rather than focus on\ncomplementary subtasks. We validate this heuristic through three multi-agent\nreinforcement learning (MARL) experiments in Overcooked-AI, demonstrating that\nkey factors limiting task parallelizability influence specialization. We also\nobserve that as the state space expands, agents tend to converge on specialist\nstrategies, even when generalist ones are theoretically more efficient,\nhighlighting potential biases in MARL training algorithms. Our findings provide\na principled framework for interpreting specialization given the task and\nenvironment, and introduce a novel benchmark for evaluating whether MARL finds\noptimal strategies.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15703v1",
    "published_date": "2025-03-19 21:33:48 UTC",
    "updated_date": "2025-03-19 21:33:48 UTC"
  },
  {
    "arxiv_id": "2503.15699v1",
    "title": "Representational Similarity via Interpretable Visual Concepts",
    "authors": [
      "Neehar Kondapaneni",
      "Oisin Mac Aodha",
      "Pietro Perona"
    ],
    "abstract": "How do two deep neural networks differ in how they arrive at a decision?\nMeasuring the similarity of deep networks has been a long-standing open\nquestion. Most existing methods provide a single number to measure the\nsimilarity of two networks at a given layer, but give no insight into what\nmakes them similar or dissimilar. We introduce an interpretable\nrepresentational similarity method (RSVC) to compare two networks. We use RSVC\nto discover shared and unique visual concepts between two models. We show that\nsome aspects of model differences can be attributed to unique concepts\ndiscovered by one model that are not well represented in the other. Finally, we\nconduct extensive evaluation across different vision model architectures and\ntraining protocols to demonstrate its effectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages, 5 Figures, 16 Supplemental Figures, ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.15699v1",
    "published_date": "2025-03-19 21:21:45 UTC",
    "updated_date": "2025-03-19 21:21:45 UTC"
  },
  {
    "arxiv_id": "2503.15661v1",
    "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction",
    "authors": [
      "Shravan Nayak",
      "Xiangru Jian",
      "Kevin Qinghong Lin",
      "Juan A. Rodriguez",
      "Montek Kalsi",
      "Rabiul Awal",
      "Nicolas Chapados",
      "M. Tamer Özsu",
      "Aishwarya Agrawal",
      "David Vazquez",
      "Christopher Pal",
      "Perouz Taslakian",
      "Spandana Gella",
      "Sai Rajeswar"
    ],
    "abstract": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15661v1",
    "published_date": "2025-03-19 19:26:17 UTC",
    "updated_date": "2025-03-19 19:26:17 UTC"
  },
  {
    "arxiv_id": "2503.15655v1",
    "title": "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs",
    "authors": [
      "Zefeng Lin",
      "Yi Xiao",
      "Zhiqiang Mo",
      "Qifan Zhang",
      "Jie Wang",
      "Jiayang Chen",
      "Jiajing Zhang",
      "Hui Zhang",
      "Zhengyi Liu",
      "Xianyong Fang",
      "Xiaohua Xu"
    ],
    "abstract": "Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15655v1",
    "published_date": "2025-03-19 19:09:40 UTC",
    "updated_date": "2025-03-19 19:09:40 UTC"
  },
  {
    "arxiv_id": "2503.16556v1",
    "title": "Reliable Radiologic Skeletal Muscle Area Assessment -- A Biomarker for Cancer Cachexia Diagnosis",
    "authors": [
      "Sabeen Ahmed",
      "Nathan Parker",
      "Margaret Park",
      "Daniel Jeong",
      "Lauren Peres",
      "Evan W. Davis",
      "Jennifer B. Permuth",
      "Erin Siegel",
      "Matthew B. Schabath",
      "Yasin Yilmaz",
      "Ghulam Rasool"
    ],
    "abstract": "Cancer cachexia is a common metabolic disorder characterized by severe muscle\natrophy which is associated with poor prognosis and quality of life. Monitoring\nskeletal muscle area (SMA) longitudinally through computed tomography (CT)\nscans, an imaging modality routinely acquired in cancer care, is an effective\nway to identify and track this condition. However, existing tools often lack\nfull automation and exhibit inconsistent accuracy, limiting their potential for\nintegration into clinical workflows. To address these challenges, we developed\nSMAART-AI (Skeletal Muscle Assessment-Automated and Reliable Tool-based on AI),\nan end-to-end automated pipeline powered by deep learning models (nnU-Net 2D)\ntrained on mid-third lumbar level CT images with 5-fold cross-validation,\nensuring generalizability and robustness. SMAART-AI incorporates an\nuncertainty-based mechanism to flag high-error SMA predictions for expert\nreview, enhancing reliability. We combined the SMA, skeletal muscle index, BMI,\nand clinical data to train a multi-layer perceptron (MLP) model designed to\npredict cachexia at the time of cancer diagnosis. Tested on the\ngastroesophageal cancer dataset, SMAART-AI achieved a Dice score of 97.80% +/-\n0.93%, with SMA estimated across all four datasets in this study at a median\nabsolute error of 2.48% compared to manual annotations with SliceOmatic.\nUncertainty metrics-variance, entropy, and coefficient of variation-strongly\ncorrelated with SMA prediction errors (0.83, 0.76, and 0.73 respectively). The\nMLP model predicts cachexia with 79% precision, providing clinicians with a\nreliable tool for early diagnosis and intervention. By combining automation,\naccuracy, and uncertainty awareness, SMAART-AI bridges the gap between research\nand clinical application, offering a transformative approach to managing cancer\ncachexia.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CE",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "47 pages, 19 figures, 9 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.16556v1",
    "published_date": "2025-03-19 19:07:59 UTC",
    "updated_date": "2025-03-19 19:07:59 UTC"
  },
  {
    "arxiv_id": "2503.15650v1",
    "title": "Survey on Generalization Theory for Graph Neural Networks",
    "authors": [
      "Antonis Vasileiou",
      "Stefanie Jegelka",
      "Ron Levie",
      "Christopher Morris"
    ],
    "abstract": "Message-passing graph neural networks (MPNNs) have emerged as the leading\napproach for machine learning on graphs, attracting significant attention in\nrecent years. While a large set of works explored the expressivity of MPNNs,\ni.e., their ability to separate graphs and approximate functions over them,\ncomparatively less attention has been directed toward investigating their\ngeneralization abilities, i.e., making meaningful predictions beyond the\ntraining data. Here, we systematically review the existing literature on the\ngeneralization abilities of MPNNs. We analyze the strengths and limitations of\nvarious studies in these domains, providing insights into their methodologies\nand findings. Furthermore, we identify potential avenues for future research,\naiming to deepen our understanding of the generalization abilities of MPNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15650v1",
    "published_date": "2025-03-19 19:04:24 UTC",
    "updated_date": "2025-03-19 19:04:24 UTC"
  },
  {
    "arxiv_id": "2503.17401v1",
    "title": "AEJIM: A Real-Time AI Framework for Crowdsourced, Transparent, and Ethical Environmental Hazard Detection and Reporting",
    "authors": [
      "Torsten Tiltack"
    ],
    "abstract": "Environmental journalism is vital for raising awareness of ecological crises\nand driving evidence-based policy, yet traditional methods falter under delays,\ninaccuracies, and scalability limits, especially in under-monitored regions\ncritical to the United Nations Sustainable Development Goals. To bridge these\ngaps, this paper introduces the AI-Environmental Journalism Integration Model\n(AEJIM), an innovative framework combining real-time hazard detection,\ncrowdsourced validation, and AI-driven reporting.\n  Validated through a pilot study, AEJIM significantly improved the speed and\naccuracy of environmental hazard reporting, outperforming traditional methods.\nFurthermore, the model directly addresses key ethical, regulatory, and\nscalability challenges, ensuring AI accountability through Explainable AI\n(XAI), GDPR-compliant data governance, and active public participation. AEJIM\nprovides a transparent and adaptable solution, setting a new benchmark for\nAI-enhanced environmental journalism and supporting informed global\ndecision-making across diverse socio-political landscapes.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "J.4; I.2.10; I.2.7; H.3.4; H.5.2"
    ],
    "primary_category": "cs.CY",
    "comment": "21 pages, 10 figures, 5 tables. Keywords: Artificial Intelligence,\n  Environmental Journalism, Real-Time Reporting, Vision Transformers, Image\n  Recognition, Crowdsourced Validation, GPT-4, Automated News Generation, GIS\n  Integration, Data Privacy Compliance, Explainable AI (XAI), AI Ethics,\n  Sustainable Development",
    "pdf_url": "http://arxiv.org/pdf/2503.17401v1",
    "published_date": "2025-03-19 19:00:24 UTC",
    "updated_date": "2025-03-19 19:00:24 UTC"
  },
  {
    "arxiv_id": "2503.15639v1",
    "title": "A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition",
    "authors": [
      "Ritabrata Chakraborty",
      "Shivakumara Palaiahnakote",
      "Umapada Pal",
      "Cheng-Lin Liu"
    ],
    "abstract": "Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15639v1",
    "published_date": "2025-03-19 18:51:01 UTC",
    "updated_date": "2025-03-19 18:51:01 UTC"
  },
  {
    "arxiv_id": "2503.15629v1",
    "title": "Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning",
    "authors": [
      "Luc McCutcheon",
      "Bahman Gharesifard",
      "Saber Fallah"
    ],
    "abstract": "Control Lyapunov functions are traditionally used to design a controller\nwhich ensures convergence to a desired state, yet deriving these functions for\nnonlinear systems remains a complex challenge. This paper presents a novel,\nsample-efficient method for neural approximation of nonlinear Lyapunov\nfunctions, leveraging self-supervised Reinforcement Learning (RL) to enhance\ntraining data generation, particularly for inaccurately represented regions of\nthe state space. The proposed approach employs a data-driven World Model to\ntrain Lyapunov functions from off-policy trajectories. The method is validated\non both standard and goal-conditioned robotic tasks, demonstrating faster\nconvergence and higher approximation accuracy compared to the state-of-the-art\nneural Lyapunov approximation baseline. The code is available at:\nhttps://github.com/CAV-Research-Lab/SACLA.git",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CG",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA)",
    "pdf_url": "http://arxiv.org/pdf/2503.15629v1",
    "published_date": "2025-03-19 18:29:25 UTC",
    "updated_date": "2025-03-19 18:29:25 UTC"
  },
  {
    "arxiv_id": "2503.15621v1",
    "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning",
    "authors": [
      "Federico Cocchi",
      "Nicholas Moratelli",
      "Davide Caffagni",
      "Sara Sarto",
      "Lorenzo Baraldi",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15621v1",
    "published_date": "2025-03-19 18:10:12 UTC",
    "updated_date": "2025-03-19 18:10:12 UTC"
  },
  {
    "arxiv_id": "2503.15620v1",
    "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings",
    "authors": [
      "Austin Xu",
      "Srijan Bansal",
      "Yifei Ming",
      "Semih Yavuz",
      "Shafiq Joty"
    ],
    "abstract": "The large language model (LLM)-as-judge paradigm has been used to meet the\ndemand for a cheap, reliable, and fast evaluation of model outputs during AI\nsystem development and post-deployment monitoring. While judge models -- LLMs\nfinetuned to specialize in assessing and critiquing model outputs -- have been\ntouted as general purpose evaluators, they are typically evaluated only on\nnon-contextual scenarios, such as instruction following. The omission of\ncontextual settings -- those where external information is used as context to\ngenerate an output -- is surprising given the increasing prevalence of\nretrieval-augmented generation (RAG) and summarization use cases. Contextual\nassessment is uniquely challenging, as evaluation often depends on practitioner\npriorities, leading to conditional evaluation criteria (e.g., comparing\nresponses based on factuality and then considering completeness if they are\nequally factual). To address the gap, we propose ContextualJudgeBench, a judge\nbenchmark with 2,000 challenging response pairs across eight splits inspired by\nreal-world contextual evaluation scenarios. We build our benchmark with a\nmulti-pronged data construction pipeline that leverages both existing human\nannotations and model-based perturbations. Our comprehensive study across 11\njudge models and 9 general purpose models, reveals that the contextual\ninformation and its assessment criteria present a significant challenge to even\nstate-of-the-art models. For example, OpenAI's o1, the best-performing model,\nbarely reaches 55% consistent accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 13 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.15620v1",
    "published_date": "2025-03-19 18:09:19 UTC",
    "updated_date": "2025-03-19 18:09:19 UTC"
  },
  {
    "arxiv_id": "2503.15617v1",
    "title": "CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation",
    "authors": [
      "Masud Ahmed",
      "Zahid Hasan",
      "Syed Arefinul Haque",
      "Abu Zaher Md Faridee",
      "Sanjay Purushotham",
      "Suya You",
      "Nirmalya Roy"
    ],
    "abstract": "Traditional transformer-based semantic segmentation relies on quantized\nembeddings. However, our analysis reveals that autoencoder accuracy on\nsegmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than\ncontinuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a\ncontinuous-valued embedding framework for semantic segmentation. By\nreformulating semantic mask generation as a continuous image-to-embedding\ndiffusion process, our approach eliminates the need for discrete latent\nrepresentations while preserving fine-grained spatial and semantic details. Our\nkey contribution includes a diffusion-guided autoregressive transformer that\nlearns a continuous semantic embedding space by modeling long-range\ndependencies in image features. Our framework contains a unified architecture\ncombining a VAE encoder for continuous feature extraction, a diffusion-guided\ntransformer for conditioned embedding generation, and a VAE decoder for\nsemantic mask reconstruction. Our setting facilitates zero-shot domain\nadaptation capabilities enabled by the continuity of the embedding space.\nExperiments across diverse datasets (e.g., Cityscapes and domain-shifted\nvariants) demonstrate state-of-the-art robustness to distribution shifts,\nincluding adverse weather (e.g., fog, snow) and viewpoint variations. Our model\nalso exhibits strong noise resilience, achieving robust performance ($\\approx$\n95% AP compared to baseline) under gaussian noise, moderate motion blur, and\nmoderate brightness/contrast variations, while experiencing only a moderate\nimpact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,\nsaturation and hue shifts. Code available:\nhttps://github.com/mahmed10/CAMSS.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15617v1",
    "published_date": "2025-03-19 18:06:54 UTC",
    "updated_date": "2025-03-19 18:06:54 UTC"
  },
  {
    "arxiv_id": "2503.15615v1",
    "title": "PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL",
    "authors": [
      "Joshua McClellan",
      "Greyson Brothers",
      "Furong Huang",
      "Pratap Tokekar"
    ],
    "abstract": "Equivariant Graph Neural Networks (EGNNs) have emerged as a promising\napproach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry\nguarantees to greatly improve sample efficiency and generalization. However,\nreal-world environments often exhibit inherent asymmetries arising from factors\nsuch as external forces, measurement inaccuracies, or intrinsic system biases.\nThis paper introduces \\textit{Partially Equivariant Graph NeUral Networks\n(PEnGUiN)}, a novel architecture specifically designed to address these\nchallenges. We formally identify and categorize various types of partial\nequivariance relevant to MARL, including subgroup equivariance, feature-wise\nequivariance, regional equivariance, and approximate equivariance. We\ntheoretically demonstrate that PEnGUiN is capable of learning both fully\nequivariant (EGNN) and non-equivariant (GNN) representations within a unified\nframework. Through extensive experiments on a range of MARL problems\nincorporating various asymmetries, we empirically validate the efficacy of\nPEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both\nEGNNs and standard GNNs in asymmetric environments, highlighting their\npotential to improve the robustness and applicability of graph-based MARL\nalgorithms in real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15615v1",
    "published_date": "2025-03-19 18:01:14 UTC",
    "updated_date": "2025-03-19 18:01:14 UTC"
  },
  {
    "arxiv_id": "2503.15485v1",
    "title": "TULIP: Towards Unified Language-Image Pretraining",
    "authors": [
      "Zineng Tang",
      "Long Lian",
      "Seun Eisape",
      "XuDong Wang",
      "Roei Herzig",
      "Adam Yala",
      "Alane Suhr",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "abstract": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15485v1",
    "published_date": "2025-03-19 17:58:57 UTC",
    "updated_date": "2025-03-19 17:58:57 UTC"
  },
  {
    "arxiv_id": "2503.15484v1",
    "title": "Value Profiles for Encoding Human Variation",
    "authors": [
      "Taylor Sorensen",
      "Pushkar Mishra",
      "Roma Patel",
      "Michael Henry Tessler",
      "Michiel Bakker",
      "Georgina Evans",
      "Iason Gabriel",
      "Noah Goodman",
      "Verena Rieser"
    ],
    "abstract": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15484v1",
    "published_date": "2025-03-19 17:57:49 UTC",
    "updated_date": "2025-03-19 17:57:49 UTC"
  },
  {
    "arxiv_id": "2503.15481v1",
    "title": "Learning to Play Piano in the Real World",
    "authors": [
      "Yves-Simon Zeulner",
      "Sandeep Selvaraj",
      "Roberto Calandra"
    ],
    "abstract": "Towards the grand challenge of achieving human-level manipulation in robots,\nplaying piano is a compelling testbed that requires strategic, precise, and\nflowing movements. Over the years, several works demonstrated hand-designed\ncontrollers on real world piano playing, while other works evaluated robot\nlearning approaches on simulated piano scenarios. In this paper, we develop the\nfirst piano playing robotic system that makes use of learning approaches while\nalso being deployed on a real world dexterous robot. Specifically, we make use\nof Sim2Real to train a policy in simulation using reinforcement learning before\ndeploying the learned policy on a real world dexterous robot. In our\nexperiments, we thoroughly evaluate the interplay between domain randomization\nand the accuracy of the dynamics model used in simulation. Moreover, we\nevaluate the robot's performance across multiple songs with varying complexity\nto study the generalization of our learned policy. By providing a\nproof-of-concept of learning to play piano in the real world, we want to\nencourage the community to adopt piano playing as a compelling benchmark\ntowards human-level manipulation. We open-source our code and show additional\nvideos at https://lasr.org/research/learning-to-play-piano .",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.15481v1",
    "published_date": "2025-03-19 17:56:14 UTC",
    "updated_date": "2025-03-19 17:56:14 UTC"
  },
  {
    "arxiv_id": "2503.15477v1",
    "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
    "authors": [
      "Noam Razin",
      "Zixuan Wang",
      "Hubert Strauss",
      "Stanley Wei",
      "Jason D. Lee",
      "Sanjeev Arora"
    ],
    "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm",
    "pdf_url": "http://arxiv.org/pdf/2503.15477v1",
    "published_date": "2025-03-19 17:54:41 UTC",
    "updated_date": "2025-03-19 17:54:41 UTC"
  },
  {
    "arxiv_id": "2503.15470v1",
    "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining",
    "authors": [
      "Boshen Xu",
      "Yuting Mei",
      "Xinbi Liu",
      "Sipeng Zheng",
      "Qin Jin"
    ],
    "abstract": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code will be released at: https://github.com/xuboshen/EgoDTM",
    "pdf_url": "http://arxiv.org/pdf/2503.15470v1",
    "published_date": "2025-03-19 17:45:56 UTC",
    "updated_date": "2025-03-19 17:45:56 UTC"
  },
  {
    "arxiv_id": "2503.15469v2",
    "title": "Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional Context-Aware Representation Learning for Enhanced Text Classification",
    "authors": [
      "ZhengLin Lai",
      "MengYao Liao",
      "Dong Xu"
    ],
    "abstract": "Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages,1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.15469v2",
    "published_date": "2025-03-19 17:45:13 UTC",
    "updated_date": "2025-03-20 10:09:43 UTC"
  },
  {
    "arxiv_id": "2503.15463v2",
    "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment",
    "authors": [
      "Jia-Nan Li",
      "Jian Guan",
      "Songhao Wu",
      "Wei Wu",
      "Rui Yan"
    ],
    "abstract": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15463v2",
    "published_date": "2025-03-19 17:41:46 UTC",
    "updated_date": "2025-03-21 10:33:21 UTC"
  },
  {
    "arxiv_id": "2503.15457v1",
    "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator",
    "authors": [
      "Yuanzhi Zhu",
      "Xi Wang",
      "Stéphane Lathuilière",
      "Vicky Kalogeiton"
    ],
    "abstract": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15457v1",
    "published_date": "2025-03-19 17:36:54 UTC",
    "updated_date": "2025-03-19 17:36:54 UTC"
  },
  {
    "arxiv_id": "2503.15438v1",
    "title": "VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning",
    "authors": [
      "Yang Tan",
      "Chen Liu",
      "Jingyuan Gao",
      "Banghao Wu",
      "Mingchen Li",
      "Ruilin Wang",
      "Lingrong Zhang",
      "Huiqun Yu",
      "Guisheng Fan",
      "Liang Hong",
      "Bingxin Zhou"
    ],
    "abstract": "Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 1 figure, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.15438v1",
    "published_date": "2025-03-19 17:19:07 UTC",
    "updated_date": "2025-03-19 17:19:07 UTC"
  },
  {
    "arxiv_id": "2503.15436v1",
    "title": "An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts",
    "authors": [
      "Ritwick Banerjee",
      "Bryan Andrews",
      "Erich Kummerfeld"
    ],
    "abstract": "Despite the accelerating presence of exploratory causal analysis in modern\nscience and medicine, the available non-experimental methods for validating\ncausal models are not well characterized. One of the most popular methods is to\nevaluate the stability of model features after resampling the data, similar to\nresampling methods for estimating confidence intervals in statistics. Many\naspects of this approach have received little to no attention, however, such as\nwhether the choice of resampling method should depend on the sample size,\nalgorithms being used, or algorithm tuning parameters. We present theoretical\nresults proving that certain resampling methods closely emulate the assignment\nof specific values to algorithm tuning parameters. We also report the results\nof extensive simulation experiments, which verify the theoretical result and\nprovide substantial data to aid researchers in further characterizing\nresampling in the context of causal discovery analysis. Together, the\ntheoretical work and simulation results provide specific guidance on how\nresampling methods and tuning parameters should be selected in practice.",
    "categories": [
      "stat.ME",
      "cs.AI"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15436v1",
    "published_date": "2025-03-19 17:18:18 UTC",
    "updated_date": "2025-03-19 17:18:18 UTC"
  },
  {
    "arxiv_id": "2503.15426v2",
    "title": "Visual Position Prompt for MLLM based Visual Grounding",
    "authors": [
      "Wei Tang",
      "Yanpeng Sun",
      "Qinying Gu",
      "Zechao Li"
    ],
    "abstract": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15426v2",
    "published_date": "2025-03-19 17:08:13 UTC",
    "updated_date": "2025-03-24 16:34:55 UTC"
  },
  {
    "arxiv_id": "2503.15421v1",
    "title": "Probing the topology of the space of tokens with structured prompts",
    "authors": [
      "Michael Robinson",
      "Sourya Dey",
      "Taisa Kushner"
    ],
    "abstract": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.",
    "categories": [
      "math.DG",
      "cs.AI",
      "53Z50, 58Z05",
      "I.2.7"
    ],
    "primary_category": "math.DG",
    "comment": "20 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15421v1",
    "published_date": "2025-03-19 17:01:15 UTC",
    "updated_date": "2025-03-19 17:01:15 UTC"
  },
  {
    "arxiv_id": "2503.15417v1",
    "title": "Temporal Regularization Makes Your Video Generator Stronger",
    "authors": [
      "Harold Haodong Chen",
      "Haojian Huang",
      "Xianfeng Wu",
      "Yexin Liu",
      "Yajing Bai",
      "Wen-Jie Shu",
      "Harry Yang",
      "Ser-Nam Lim"
    ],
    "abstract": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project: https://haroldchen19.github.io/FluxFlow/",
    "pdf_url": "http://arxiv.org/pdf/2503.15417v1",
    "published_date": "2025-03-19 16:59:32 UTC",
    "updated_date": "2025-03-19 16:59:32 UTC"
  },
  {
    "arxiv_id": "2503.15415v1",
    "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures",
    "authors": [
      "Giovanni Floreale",
      "Piero Baraldi",
      "Enrico Zio",
      "Olga Fink"
    ],
    "abstract": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15415v1",
    "published_date": "2025-03-19 16:57:00 UTC",
    "updated_date": "2025-03-19 16:57:00 UTC"
  },
  {
    "arxiv_id": "2503.15402v1",
    "title": "Towards efficient keyword spotting using spike-based time difference encoders",
    "authors": [
      "Alejandro Pequeño-Zurro",
      "Lyes Khacef",
      "Stefano Panzeri",
      "Elisabetta Chicca"
    ],
    "abstract": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.ET"
    ],
    "primary_category": "cs.NE",
    "comment": "26 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15402v1",
    "published_date": "2025-03-19 16:43:35 UTC",
    "updated_date": "2025-03-19 16:43:35 UTC"
  },
  {
    "arxiv_id": "2503.15386v1",
    "title": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling",
    "authors": [
      "Amirreza Razmjoo",
      "Sylvain Calinon",
      "Michael Gienger",
      "Fan Zhang"
    ],
    "abstract": "Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem (which may\nrequire long-horizon history to manage failures) into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15386v1",
    "published_date": "2025-03-19 16:24:55 UTC",
    "updated_date": "2025-03-19 16:24:55 UTC"
  },
  {
    "arxiv_id": "2503.15374v1",
    "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
    "authors": [
      "Anatole Callies",
      "Quentin Bodinier",
      "Philippe Ravaud",
      "Kourosh Davarpanah"
    ],
    "abstract": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15374v1",
    "published_date": "2025-03-19 16:12:11 UTC",
    "updated_date": "2025-03-19 16:12:11 UTC"
  },
  {
    "arxiv_id": "2503.15354v1",
    "title": "Optimizing Decomposition for Optimal Claim Verification",
    "authors": [
      "Yining Lu",
      "Noah Ziems",
      "Hy Dang",
      "Meng Jiang"
    ],
    "abstract": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15354v1",
    "published_date": "2025-03-19 15:56:21 UTC",
    "updated_date": "2025-03-19 15:56:21 UTC"
  },
  {
    "arxiv_id": "2503.15352v1",
    "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer",
    "authors": [
      "Abhi Kamboj",
      "Minh N. Do"
    ],
    "abstract": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15352v1",
    "published_date": "2025-03-19 15:51:17 UTC",
    "updated_date": "2025-03-19 15:51:17 UTC"
  },
  {
    "arxiv_id": "2503.15342v1",
    "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
    "authors": [
      "Ritabrata Chakraborty",
      "Rajatsubhra Chakraborty",
      "Ali Khaleghi Rahimian",
      "Thomas MacDougall"
    ],
    "abstract": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15342v1",
    "published_date": "2025-03-19 15:41:32 UTC",
    "updated_date": "2025-03-19 15:41:32 UTC"
  },
  {
    "arxiv_id": "2503.15275v1",
    "title": "Challenges and Trends in Egocentric Vision: A Survey",
    "authors": [
      "Xiang Li",
      "Heqian Qiu",
      "Lanxiao Wang",
      "Hanwen Zhang",
      "Chenghao Qi",
      "Linfeng Han",
      "Huiyu Xiong",
      "Hongliang Li"
    ],
    "abstract": "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15275v1",
    "published_date": "2025-03-19 14:51:27 UTC",
    "updated_date": "2025-03-19 14:51:27 UTC"
  },
  {
    "arxiv_id": "2503.15580v1",
    "title": "How Well Can AI Build SD Models?",
    "authors": [
      "William Schoenberg",
      "Davidson Girard",
      "Saras Chung",
      "Ellen O'Neill",
      "Janet Velasquez",
      "Sara Metcalf"
    ],
    "abstract": "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15580v1",
    "published_date": "2025-03-19 14:48:47 UTC",
    "updated_date": "2025-03-19 14:48:47 UTC"
  },
  {
    "arxiv_id": "2503.15272v1",
    "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration",
    "authors": [
      "David Wan",
      "Justin Chih-Yao Chen",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "abstract": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine",
    "pdf_url": "http://arxiv.org/pdf/2503.15272v1",
    "published_date": "2025-03-19 14:46:53 UTC",
    "updated_date": "2025-03-19 14:46:53 UTC"
  },
  {
    "arxiv_id": "2503.15268v1",
    "title": "Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
    "authors": [
      "Roberto Araya"
    ],
    "abstract": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
    "categories": [
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15268v1",
    "published_date": "2025-03-19 14:44:02 UTC",
    "updated_date": "2025-03-19 14:44:02 UTC"
  },
  {
    "arxiv_id": "2503.15248v1",
    "title": "Automated Non-Functional Requirements Generation in Software Engineering with Large Language Models: A Comparative Study",
    "authors": [
      "Jomar Thomas Almonte",
      "Santhosh Anitha Boominathan",
      "Nathalia Nascimento"
    ],
    "abstract": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.15248v1",
    "published_date": "2025-03-19 14:23:22 UTC",
    "updated_date": "2025-03-19 14:23:22 UTC"
  },
  {
    "arxiv_id": "2503.15242v2",
    "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?",
    "authors": [
      "Pierre Chambon",
      "Baptiste Roziere",
      "Benoit Sagot",
      "Gabriel Synnaeve"
    ],
    "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15242v2",
    "published_date": "2025-03-19 14:19:57 UTC",
    "updated_date": "2025-03-20 17:58:17 UTC"
  },
  {
    "arxiv_id": "2503.15234v1",
    "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification",
    "authors": [
      "Wenlong Yu",
      "Qilong Wang",
      "Chuang Liu",
      "Dong Li",
      "Qinghua Hu"
    ],
    "abstract": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2503.15234v1",
    "published_date": "2025-03-19 14:13:02 UTC",
    "updated_date": "2025-03-19 14:13:02 UTC"
  },
  {
    "arxiv_id": "2503.15235v1",
    "title": "Exploring Large Language Models for Word Games:Who is the Spy?",
    "authors": [
      "Chentian Wei",
      "Jiewei Chen",
      "Jinzhu Xu"
    ],
    "abstract": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15235v1",
    "published_date": "2025-03-19 14:13:02 UTC",
    "updated_date": "2025-03-19 14:13:02 UTC"
  },
  {
    "arxiv_id": "2503.15225v1",
    "title": "A Personalized Data-Driven Generative Model of Human Motion",
    "authors": [
      "Angelo Di Porzio",
      "Marco Coraggio"
    ],
    "abstract": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.GR",
    "comment": "6 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15225v1",
    "published_date": "2025-03-19 14:03:20 UTC",
    "updated_date": "2025-03-19 14:03:20 UTC"
  },
  {
    "arxiv_id": "2503.15204v1",
    "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection",
    "authors": [
      "Tittaya Mairittha",
      "Tanakon Sawanglok",
      "Panuwit Raden",
      "Sorrawit Treesuk"
    ],
    "abstract": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "14 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15204v1",
    "published_date": "2025-03-19 13:47:25 UTC",
    "updated_date": "2025-03-19 13:47:25 UTC"
  },
  {
    "arxiv_id": "2503.15202v2",
    "title": "A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees",
    "authors": [
      "Faseeh Ahmad",
      "Hashim Ismail",
      "Jonathan Styrud",
      "Maj Stenmark",
      "Volker Krueger"
    ],
    "abstract": "Robotic systems often face execution failures due to unexpected obstacles,\nsensor errors, or environmental changes. Traditional failure recovery methods\nrely on predefined strategies or human intervention, making them less\nadaptable. This paper presents a unified failure recovery framework that\ncombines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees\n(BTs) to enable real-time failure handling. Our approach includes pre-execution\nverification, which checks for potential failures before execution, and\nreactive failure handling, which detects and corrects failures during execution\nby verifying existing BT conditions, adding missing preconditions and, when\nnecessary, generating new skills. The framework uses a scene graph for\nstructured environmental perception and an execution history for continuous\nmonitoring, enabling context-aware and adaptive failure handling. We evaluate\nour framework through real-world experiments with an ABB YuMi robot on tasks\nlike peg insertion, object sorting, and drawer placement, as well as in\nAI2-THOR simulator. Compared to using pre-execution and reactive methods\nseparately, our approach achieves higher task success rates and greater\nadaptability. Ablation studies highlight the importance of VLM-based reasoning,\nstructured scene representation, and execution history tracking for effective\nfailure recovery in robotics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15202v2",
    "published_date": "2025-03-19 13:40:56 UTC",
    "updated_date": "2025-03-21 08:10:48 UTC"
  },
  {
    "arxiv_id": "2503.15576v1",
    "title": "A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doñana",
    "authors": [
      "Alba Márquez-Rodríguez",
      "Miguel Ángel Mohedano-Munoz",
      "Manuel J. Marín-Jiménez",
      "Eduardo Santamaría-García",
      "Giulia Bastianelli",
      "Pedro Jordano",
      "Irene Mendoza"
    ],
    "abstract": "Passive Acoustic Monitoring with automatic recorders is essential for\necosystem conservation but generates vast unsupervised audio data, posing\nchallenges for extracting meaningful information. Deep Learning techniques\noffer a promising solution. BirdNET, a widely used model for bird\nidentification, has shown success in many study systems but is limited in some\nregions due to biases in its training data. A key challenge in bird species\ndetection is that many recordings either lack target species or contain\noverlapping vocalizations. To overcome these problems, we developed a\nmulti-stage pipeline for automatic bird vocalization identification in Do\\~nana\nNational Park (SW Spain), a region facing significant conservation threats. Our\napproach included a Bird Song Detector to isolate vocalizations and custom\nclassifiers trained with BirdNET embeddings. We manually annotated 461 minutes\nof audio from three habitats across nine locations, yielding 3,749 annotations\nfor 34 classes. Spectrograms facilitated the use of image processing\ntechniques. Applying the Bird Song Detector before classification improved\nspecies identification, as all classification models performed better when\nanalyzing only the segments where birds were detected. Specifically, the\ncombination of the Bird Song Detector and fine-tuned BirdNET compared to the\nbaseline without the Bird Song Detector. Our approach demonstrated the\neffectiveness of integrating a Bird Song Detector with fine-tuned\nclassification models for bird identification at local soundscapes. These\nfindings highlight the need to adapt general-purpose tools for specific\necological challenges, as demonstrated in Do\\~nana. Automatically detecting\nbird species serves for tracking the health status of this threatened\necosystem, given the sensitivity of birds to environmental changes, and helps\nin the design of conservation measures for reducing biodiversity loss",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.NE",
      "I.5.4; I.2.6; I.4.8"
    ],
    "primary_category": "cs.SD",
    "comment": "20 pages, 13 images, for associated dataset see\n  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for\n  associated code see\n  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https://github.com/GrunCrow/Bird-Song-Detector",
    "pdf_url": "http://arxiv.org/pdf/2503.15576v1",
    "published_date": "2025-03-19 13:19:06 UTC",
    "updated_date": "2025-03-19 13:19:06 UTC"
  },
  {
    "arxiv_id": "2503.15185v1",
    "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
    "authors": [
      "Gyeongrok Oh",
      "Sungjune Kim",
      "Heeju Ko",
      "Hyung-gun Chi",
      "Jinkyu Kim",
      "Dongwook Lee",
      "Daehyun Ji",
      "Sungjoon Choi",
      "Sujin Jang",
      "Sangpil Kim"
    ],
    "abstract": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2503.15185v1",
    "published_date": "2025-03-19 13:14:57 UTC",
    "updated_date": "2025-03-19 13:14:57 UTC"
  },
  {
    "arxiv_id": "2503.15182v1",
    "title": "Foundation models may exhibit staged progression in novel CBRN threat disclosure",
    "authors": [
      "Kevin M Esvelt"
    ],
    "abstract": "The extent to which foundation models can disclose novel chemical,\nbiological, radiation, and nuclear (CBRN) threats to expert users is unclear\ndue to a lack of test cases. I leveraged the unique opportunity presented by an\nupcoming publication describing a novel catastrophic biothreat - \"Technical\nReport on Mirror Bacteria: Feasibility and Risks\" - to conduct a small\ncontrolled study before it became public. Graduate-trained biologists tasked\nwith predicting the consequences of releasing mirror E. coli showed no\nsignificant differences in rubric-graded accuracy using Claude Sonnet 3.5 new\n(n=10) or web search only (n=2); both groups scored comparably to a web\nbaseline (28 and 43 versus 36). However, Sonnet reasoned correctly when\nprompted by a report author, but a smaller model, Haiku 3.5, failed even with\nauthor guidance (80 versus 5). These results suggest distinct stages of model\ncapability: Haiku is unable to reason about mirror life even with threat-aware\nexpert guidance (Stage 1), while Sonnet correctly reasons only with\nthreat-aware prompting (Stage 2). Continued advances may allow future models to\ndisclose novel CBRN threats to naive experts (Stage 3) or unskilled users\n(Stage 4). While mirror life represents only one case study, monitoring new\nmodels' ability to reason about privately known threats may allow protective\nmeasures to be implemented before widespread disclosure.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "q-bio.OT"
    ],
    "primary_category": "cs.CY",
    "comment": "26 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15182v1",
    "published_date": "2025-03-19 13:08:01 UTC",
    "updated_date": "2025-03-19 13:08:01 UTC"
  },
  {
    "arxiv_id": "2503.15172v1",
    "title": "Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems",
    "authors": [
      "George Stamatelis",
      "Angelos-Nikolaos Kanatas",
      "George C. Alexandropoulos"
    ],
    "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful\ntool for optimizing decentralized decision-making systems in complex settings,\nsuch as Dynamic Spectrum Access (DSA). However, deploying deep learning models\non resource-constrained edge devices remains challenging due to their high\ncomputational cost. To address this challenge, in this paper, we present a\nnovel sparse recurrent MARL framework integrating gradual neural network\npruning into the independent actor global critic paradigm. Additionally, we\nintroduce a harmonic annealing sparsity scheduler, which achieves comparable,\nand in certain cases superior, performance to standard linear and polynomial\npruning schedulers at large sparsities. Our experimental investigation\ndemonstrates that the proposed DSA framework can discover superior policies,\nunder diverse training conditions, outperforming conventional DSA, MADRL\nbaselines, and state-of-the-art pruning techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 3 figures, 1 table, submited to an IEEE conference",
    "pdf_url": "http://arxiv.org/pdf/2503.15172v1",
    "published_date": "2025-03-19 12:56:23 UTC",
    "updated_date": "2025-03-19 12:56:23 UTC"
  },
  {
    "arxiv_id": "2503.15169v1",
    "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks",
    "authors": [
      "Yuting Guo",
      "Abeed Sarker"
    ],
    "abstract": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.15169v1",
    "published_date": "2025-03-19 12:51:52 UTC",
    "updated_date": "2025-03-19 12:51:52 UTC"
  },
  {
    "arxiv_id": "2503.15168v1",
    "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child",
    "authors": [
      "Javier Del Ser",
      "Jesus L. Lobo",
      "Heimo Müller",
      "Andreas Holzinger"
    ],
    "abstract": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG",
      "68T05"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.15168v1",
    "published_date": "2025-03-19 12:50:40 UTC",
    "updated_date": "2025-03-19 12:50:40 UTC"
  },
  {
    "arxiv_id": "2503.15167v1",
    "title": "Volumetric Reconstruction From Partial Views for Task-Oriented Grasping",
    "authors": [
      "Fujian Yan",
      "Hui Li",
      "Hongsheng He"
    ],
    "abstract": "Object affordance and volumetric information are essential in devising\neffective grasping strategies under task-specific constraints. This paper\npresents an approach for inferring suitable grasping strategies from limited\npartial views of an object. To achieve this, a recurrent generative adversarial\nnetwork (R-GAN) was proposed by incorporating a recurrent generator with long\nshort-term memory (LSTM) units for it to process a variable number of depth\nscans. To determine object affordances, the AffordPose knowledge dataset is\nutilized as prior knowledge. Affordance retrieving is defined by the volume\nsimilarity measured via Chamfer Distance and action similarities. A Proximal\nPolicy Optimization (PPO) reinforcement learning model is further implemented\nto refine the retrieved grasp strategies for task-oriented grasping. The\nretrieved grasp strategies were evaluated on a dual-arm mobile manipulation\nrobot with an overall grasping accuracy of 89% for four tasks: lift, handle\ngrasp, wrap grasp, and press.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15167v1",
    "published_date": "2025-03-19 12:47:50 UTC",
    "updated_date": "2025-03-19 12:47:50 UTC"
  },
  {
    "arxiv_id": "2503.15166v1",
    "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU",
    "authors": [
      "Àlex Pujol Vidal",
      "Sergio Escalera",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund"
    ],
    "abstract": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.15166v1",
    "published_date": "2025-03-19 12:47:37 UTC",
    "updated_date": "2025-03-19 12:47:37 UTC"
  },
  {
    "arxiv_id": "2503.15130v1",
    "title": "A Foundational Theory for Decentralized Sensory Learning",
    "authors": [
      "Linus Mårtensson",
      "Jonas M. D. Enander",
      "Udaya B. Rongala",
      "Henrik Jörntell"
    ],
    "abstract": "In both neuroscience and artificial intelligence, popular functional\nframeworks and neural network formulations operate by making use of extrinsic\nerror measurements and global learning algorithms. Through a set of conjectures\nbased on evolutionary insights on the origin of cellular adaptive mechanisms,\nwe reinterpret the core meaning of sensory signals to allow the brain to be\ninterpreted as a negative feedback control system, and show how this could lead\nto local learning algorithms without the need for global error correction\nmetrics. Thereby, a sufficiently good minima in sensory activity can be the\ncomplete reward signal of the network, as well as being both necessary and\nsufficient for biological learning to arise. We show that this method of\nlearning was likely already present in the earliest unicellular life forms on\nearth. We show evidence that the same principle holds and scales to\nmulticellular organisms where it in addition can lead to division of labour\nbetween cells. Available evidence shows that the evolution of the nervous\nsystem likely was an adaptation to more effectively communicate intercellular\nsignals to support such division of labour. We therefore propose that the same\nlearning principle that evolved already in the earliest unicellular life forms,\ni.e. negative feedback control of externally and internally generated sensor\nsignals, has simply been scaled up to become a fundament of the learning we see\nin biological brains today. We illustrate diverse biological settings, from the\nearliest unicellular organisms to humans, where this operational principle\nappears to be a plausible interpretation of the meaning of sensor signals in\nbiology, and how this relates to current neuroscientific theories and findings.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15130v1",
    "published_date": "2025-03-19 11:44:58 UTC",
    "updated_date": "2025-03-19 11:44:58 UTC"
  },
  {
    "arxiv_id": "2503.15129v1",
    "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models",
    "authors": [
      "Man Fai Wong",
      "Chee Wei Tan"
    ],
    "abstract": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15129v1",
    "published_date": "2025-03-19 11:44:47 UTC",
    "updated_date": "2025-03-19 11:44:47 UTC"
  },
  {
    "arxiv_id": "2503.15128v1",
    "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
    "authors": [
      "Dominik Macko",
      "Robert Moro",
      "Ivan Srba"
    ],
    "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15128v1",
    "published_date": "2025-03-19 11:42:33 UTC",
    "updated_date": "2025-03-19 11:42:33 UTC"
  },
  {
    "arxiv_id": "2503.15126v1",
    "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation",
    "authors": [
      "Haoyu Ji",
      "Bowen Chen",
      "Weihong Ren",
      "Wenze Huang",
      "Zhihao Yang",
      "Zhiyong Wang",
      "Honghai Liu"
    ],
    "abstract": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15126v1",
    "published_date": "2025-03-19 11:38:14 UTC",
    "updated_date": "2025-03-19 11:38:14 UTC"
  },
  {
    "arxiv_id": "2503.15113v1",
    "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
    "authors": [
      "Benjamin Estermann",
      "Roger Wattenhofer"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
    "pdf_url": "http://arxiv.org/pdf/2503.15113v1",
    "published_date": "2025-03-19 11:13:51 UTC",
    "updated_date": "2025-03-19 11:13:51 UTC"
  },
  {
    "arxiv_id": "2503.15108v1",
    "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making",
    "authors": [
      "Mohamed Salim Aissi",
      "Clemence Grislain",
      "Mohamed Chetouani",
      "Olivier Sigaud",
      "Laure Soulier",
      "Nicolas Thome"
    ],
    "abstract": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15108v1",
    "published_date": "2025-03-19 11:05:42 UTC",
    "updated_date": "2025-03-19 11:05:42 UTC"
  },
  {
    "arxiv_id": "2503.15095v1",
    "title": "Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control",
    "authors": [
      "Stelios Zarifis",
      "Ioannis Kordonis",
      "Petros Maragos"
    ],
    "abstract": "We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic\nframework for uncertainty-aware prediction and decision-making in partially\nobservable stochastic systems by integrating diffusion-based time series\nforecasting models in Model Predictive Control algorithms. In our approach, a\ndiffusion-based time series forecasting model is used to probabilistically\nestimate the evolution of the system's stochastic components. These forecasts\nare then incorporated into MPC algorithms to estimate future trajectories and\noptimize action selection under the uncertainty of the future. We evaluate the\nframework on the task of energy arbitrage, where a Battery Energy Storage\nSystem participates in the day-ahead electricity market of the New York state.\nExperimental results indicate that our model-based approach with a\ndiffusion-based forecaster significantly outperforms both implementations with\nclassical forecasting methods and model-free reinforcement learning baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.2.6; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 3 figures, 3 tables. This version is submitted to the 33rd\n  European Signal Processing Conference (EUSIPCO 2025), to be held in Isola\n  delle Femmine - Palermo - Italy, on September 8-12, 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.15095v1",
    "published_date": "2025-03-19 10:48:26 UTC",
    "updated_date": "2025-03-19 10:48:26 UTC"
  },
  {
    "arxiv_id": "2503.15092v1",
    "title": "Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings",
    "authors": [
      "Zonghao Ying",
      "Guangyi Zheng",
      "Yongxin Huang",
      "Deyue Zhang",
      "Wenxin Zhang",
      "Quanchen Zou",
      "Aishan Liu",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "abstract": "This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps://github.com/NY1024/DeepSeek-Safety-Eval.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15092v1",
    "published_date": "2025-03-19 10:44:37 UTC",
    "updated_date": "2025-03-19 10:44:37 UTC"
  },
  {
    "arxiv_id": "2503.15082v1",
    "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
    "authors": [
      "Le Ma",
      "Ziyu Meng",
      "Tengyu Liu",
      "Yuhan Li",
      "Ran Song",
      "Wei Zhang",
      "Siyuan Huang"
    ],
    "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.15082v1",
    "published_date": "2025-03-19 10:27:44 UTC",
    "updated_date": "2025-03-19 10:27:44 UTC"
  },
  {
    "arxiv_id": "2503.15060v2",
    "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis",
    "authors": [
      "Imanol G. Estepa",
      "Jesús M. Rodríguez-de-Vera",
      "Ignacio Sarasúa",
      "Bhalaji Nagarajan",
      "Petia Radeva"
    ],
    "abstract": "While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.5.4; I.5.1; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "The source code is available in https://github.com/ImaGonEs/Sorcen",
    "pdf_url": "http://arxiv.org/pdf/2503.15060v2",
    "published_date": "2025-03-19 09:53:11 UTC",
    "updated_date": "2025-03-20 15:09:59 UTC"
  },
  {
    "arxiv_id": "2503.15058v1",
    "title": "Texture-Aware StarGAN for CT data harmonisation",
    "authors": [
      "Francesco Di Feola",
      "Ludovica Pompilio",
      "Cecilia Assolito",
      "Valerio Guarrasi",
      "Paolo Soda"
    ],
    "abstract": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15058v1",
    "published_date": "2025-03-19 09:50:32 UTC",
    "updated_date": "2025-03-19 09:50:32 UTC"
  },
  {
    "arxiv_id": "2503.15049v1",
    "title": "HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation",
    "authors": [
      "Cheng Wang",
      "Lingxin Kong",
      "Massimiliano Tamborski",
      "Stefano V. Albrecht"
    ],
    "abstract": "Simulation-based testing has emerged as an essential tool for verifying and\nvalidating autonomous vehicles (AVs). However, contemporary methodologies, such\nas deterministic and imitation learning-based driver models, struggle to\ncapture the variability of human-like driving behavior. Given these challenges,\nwe propose HAD-Gen, a general framework for realistic traffic scenario\ngeneration that simulates diverse human-like driving behaviors. The framework\nfirst clusters the vehicle trajectory data into different driving styles\naccording to safety features. It then employs maximum entropy inverse\nreinforcement learning on each of the clusters to learn the reward function\ncorresponding to each driving style. Using these reward functions, the method\nintegrates offline reinforcement learning pre-training and multi-agent\nreinforcement learning algorithms to obtain general and robust driving\npolicies. Multi-perspective simulation results show that our proposed scenario\ngeneration framework can simulate diverse, human-like driving behaviors with\nstrong generalization capability. The proposed framework achieves a 90.96%\ngoal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in\nthe generalization test, outperforming prior approaches by over 20% in\ngoal-reaching performance. The source code is released at\nhttps://github.com/RoboSafe-Lab/Sim4AD.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15049v1",
    "published_date": "2025-03-19 09:38:45 UTC",
    "updated_date": "2025-03-19 09:38:45 UTC"
  },
  {
    "arxiv_id": "2503.15035v1",
    "title": "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback",
    "authors": [
      "Sungjae Lee",
      "Yeonjoo Hong",
      "Kwang In Kim"
    ],
    "abstract": "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15035v1",
    "published_date": "2025-03-19 09:25:32 UTC",
    "updated_date": "2025-03-19 09:25:32 UTC"
  },
  {
    "arxiv_id": "2503.15008v1",
    "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection",
    "authors": [
      "Aamir Mehmood",
      "Yue Hu",
      "Saddam Hussain Khan"
    ],
    "abstract": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986",
    "pdf_url": "http://arxiv.org/pdf/2503.15008v1",
    "published_date": "2025-03-19 08:59:02 UTC",
    "updated_date": "2025-03-19 08:59:02 UTC"
  },
  {
    "arxiv_id": "2503.16547v1",
    "title": "Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis",
    "authors": [
      "Sihan Wang",
      "Suiyang Jiang",
      "Yibo Gao",
      "Boming Wang",
      "Shangqi Gao",
      "Xiahai Zhuang"
    ],
    "abstract": "Traditional AI-based healthcare systems often rely on single-modal data,\nlimiting diagnostic accuracy due to incomplete information. However, recent\nadvancements in foundation models show promising potential for enhancing\ndiagnosis combining multi-modal information. While these models excel in static\ntasks, they struggle with dynamic diagnosis, failing to manage multi-turn\ninteractions and often making premature diagnostic decisions due to\ninsufficient persistence in information collection.To address this, we propose\na multi-agent framework inspired by consultation flow and reinforcement\nlearning (RL) to simulate the entire consultation process, integrating multiple\nclinical information for effective diagnosis. Our approach incorporates a\nhierarchical action set, structured from clinic consultation flow and medical\ntextbook, to effectively guide the decision-making process. This strategy\nimproves agent interactions, enabling them to adapt and optimize actions based\non the dynamic state. We evaluated our framework on a public dynamic diagnosis\nbenchmark. The proposed framework evidentially improves the baseline methods\nand achieves state-of-the-art performance compared to existing foundation\nmodel-based methods.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16547v1",
    "published_date": "2025-03-19 08:47:18 UTC",
    "updated_date": "2025-03-19 08:47:18 UTC"
  },
  {
    "arxiv_id": "2503.16546v1",
    "title": "A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions",
    "authors": [
      "Saddam Hussain Khan",
      "Rashid Iqbal"
    ],
    "abstract": "Deep Convolutional Neural Networks (CNNs) have significantly advanced deep\nlearning, driving breakthroughs in computer vision, natural language\nprocessing, medical diagnosis, object detection, and speech recognition.\nArchitectural innovations including 1D, 2D, and 3D convolutional models,\ndilated and grouped convolutions, depthwise separable convolutions, and\nattention mechanisms address domain-specific challenges and enhance feature\nrepresentation and computational efficiency. Structural refinements such as\nspatial-channel exploitation, multi-path design, and feature-map enhancement\ncontribute to robust hierarchical feature extraction and improved\ngeneralization, particularly through transfer learning. Efficient preprocessing\nstrategies, including Fourier transforms, structured transforms, low-precision\ncomputation, and weight compression, optimize inference speed and facilitate\ndeployment in resource-constrained environments. This survey presents a unified\ntaxonomy that classifies CNN architectures based on spatial exploitation,\nmulti-path structures, depth, width, dimensionality expansion, channel\nboosting, and attention mechanisms. It systematically reviews CNN applications\nin face recognition, pose estimation, action recognition, text classification,\nstatistical language modeling, disease diagnosis, radiological analysis,\ncryptocurrency sentiment prediction, 1D data processing, video analysis, and\nspeech recognition. In addition to consolidating architectural advancements,\nthe review highlights emerging learning paradigms such as few-shot, zero-shot,\nweakly supervised, federated learning frameworks and future research directions\ninclude hybrid CNN-transformer models, vision-language integration, generative\nlearning, etc. This review provides a comprehensive perspective on CNN's\nevolution from 2015 to 2025, outlining key innovations, challenges, and\nopportunities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "100 Pages, 44 Figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16546v1",
    "published_date": "2025-03-19 08:41:06 UTC",
    "updated_date": "2025-03-19 08:41:06 UTC"
  },
  {
    "arxiv_id": "2503.14976v2",
    "title": "Application of linear regression method to the deep reinforcement learning in continuous action cases",
    "authors": [
      "Hisato Komatsu"
    ],
    "abstract": "The linear regression (LR) method offers the advantage that optimal\nparameters can be calculated relatively easily, although its representation\ncapability is limited than that of the deep learning technique. To improve deep\nreinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was\nproposed by Levine et al., which combines Deep Q Network (DQN) with LR method.\nHowever, the LS-DQN method assumes that the actions are discrete. In this\nstudy, we propose the Double Least Squares Deep Deterministic Policy Gradient\n(DLS-DDPG) method to address this limitation. This method combines the LR\nmethod with the Deep Deterministic Policy Gradient (DDPG) technique, one of the\nrepresentative deep reinforcement learning algorithms for continuous action\ncases. Numerical experiments conducted in MuJoCo environments showed that the\nLR update improved performance at least in some tasks, although there are\ndifficulties such as the inability to make the regularization terms small.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14976v2",
    "published_date": "2025-03-19 08:10:54 UTC",
    "updated_date": "2025-03-21 11:40:42 UTC"
  },
  {
    "arxiv_id": "2503.14973v1",
    "title": "Behaviour Discovery and Attribution for Explainable Reinforcement Learning",
    "authors": [
      "Rishav Rishav",
      "Somjit Nath",
      "Vincent Michalski",
      "Samira Ebrahimi Kahou"
    ],
    "abstract": "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14973v1",
    "published_date": "2025-03-19 08:06:00 UTC",
    "updated_date": "2025-03-19 08:06:00 UTC"
  },
  {
    "arxiv_id": "2503.18958v1",
    "title": "Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI",
    "authors": [
      "Jianyi Zhang"
    ],
    "abstract": "Recent years have witnessed the rapid progression of deep learning, pushing\nus closer to the realization of AGI (Artificial General Intelligence).\nProbabilistic modeling is critical to many of these advancements, which\nprovides a foundational framework for capturing data distributions. However, as\nthe scale and complexity of AI applications grow, traditional probabilistic\nmodeling faces escalating challenges, such as high-dimensional parameter\nspaces, heterogeneous data sources, and evolving real-world requirements often\nrender classical approaches insufficiently flexible.\n  This paper proposes a novel concept, Probability Engineering, which treats\nthe already-learned probability distributions within deep learning as\nengineering artifacts. Rather than merely fitting or inferring distributions,\nwe actively modify and reinforce them to better address the diverse and\nevolving demands of modern AI. Specifically, Probability Engineering introduces\nnovel techniques and constraints to refine existing probability distributions,\nimproving their robustness, efficiency, adaptability, or trustworthiness.\n  We showcase this paradigm through a series of applications spanning Bayesian\ndeep learning, Edge AI (including federated learning and knowledge\ndistillation), and Generative AI (such as text-to-image generation with\ndiffusion models and high-quality text generation with large language models).\nThese case studies demonstrate how probability distributions once treated as\nstatic objects can be engineered to meet the diverse and evolving requirements\nof large-scale, data-intensive, and trustworthy AI systems. By systematically\nexpanding and strengthening the role of probabilistic modeling, Probability\nEngineering paves the way for more robust, adaptive, efficient, and trustworthy\ndeep learning solutions in today's fast-growing AI era.",
    "categories": [
      "cs.AI",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "Ph.D. dissertation",
    "pdf_url": "http://arxiv.org/pdf/2503.18958v1",
    "published_date": "2025-03-19 07:48:23 UTC",
    "updated_date": "2025-03-19 07:48:23 UTC"
  },
  {
    "arxiv_id": "2503.14950v1",
    "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network",
    "authors": [
      "Joseph Emmanuel DL Dayo",
      "Prospero C. Naval Jr"
    ],
    "abstract": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14950v1",
    "published_date": "2025-03-19 07:29:02 UTC",
    "updated_date": "2025-03-19 07:29:02 UTC"
  },
  {
    "arxiv_id": "2503.14935v1",
    "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding",
    "authors": [
      "Chongjun Tu",
      "Lin Zhang",
      "Pengtao Chen",
      "Peng Ye",
      "Xianfang Zeng",
      "Wei Cheng",
      "Gang Yu",
      "Tao Chen"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "FAVOR-Bench project page: https://favor-bench.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.14935v1",
    "published_date": "2025-03-19 06:42:32 UTC",
    "updated_date": "2025-03-19 06:42:32 UTC"
  },
  {
    "arxiv_id": "2503.14928v1",
    "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
    "authors": [
      "Jiaxin Ye",
      "Hongming Shan"
    ],
    "abstract": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://imagintalk.github.io",
    "pdf_url": "http://arxiv.org/pdf/2503.14928v1",
    "published_date": "2025-03-19 06:28:17 UTC",
    "updated_date": "2025-03-19 06:28:17 UTC"
  },
  {
    "arxiv_id": "2503.16544v1",
    "title": "Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies",
    "authors": [
      "Donghuo Zeng",
      "Roberto Legaspi",
      "Yuewen Sun",
      "Xinshuai Dong",
      "Kazushi Ikeda",
      "Peter Spirtes",
      "Kun Zhang"
    ],
    "abstract": "Tailoring persuasive conversations to users leads to more effective\npersuasion. However, existing dialogue systems often struggle to adapt to\ndynamically evolving user states. This paper presents a novel method that\nleverages causal discovery and counterfactual reasoning for optimizing system\npersuasion capability and outcomes. We employ the Greedy Relaxation of the\nSparsest Permutation (GRaSP) algorithm to identify causal relationships between\nuser and system utterance strategies, treating user strategies as states and\nsystem strategies as actions. GRaSP identifies user strategies as causal\nfactors influencing system responses, which inform Bidirectional Conditional\nGenerative Adversarial Networks (BiCoGAN) in generating counterfactual\nutterances for the system. Subsequently, we use the Dueling Double Deep\nQ-Network (D3QN) model to utilize counterfactual data to determine the best\npolicy for selecting system utterances. Our experiments with the\nPersuasionForGood dataset show measurable improvements in persuasion outcomes\nusing our approach over baseline methods. The observed increase in cumulative\nrewards and Q-values highlights the effectiveness of causal discovery in\nenhancing counterfactual reasoning and optimizing reinforcement learning\npolicies for online dialogue systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16544v1",
    "published_date": "2025-03-19 06:06:10 UTC",
    "updated_date": "2025-03-19 06:06:10 UTC"
  },
  {
    "arxiv_id": "2503.14922v1",
    "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
    "authors": [
      "Jiazhu Dai",
      "Haoyu Sun"
    ],
    "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14922v1",
    "published_date": "2025-03-19 06:04:55 UTC",
    "updated_date": "2025-03-19 06:04:55 UTC"
  },
  {
    "arxiv_id": "2503.14917v1",
    "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
    "authors": [
      "Jiazheng Li",
      "Lu Yu",
      "Qing Cui",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Yanfang Ye",
      "Chuxu Zhang"
    ],
    "abstract": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14917v1",
    "published_date": "2025-03-19 05:50:21 UTC",
    "updated_date": "2025-03-19 05:50:21 UTC"
  },
  {
    "arxiv_id": "2503.14908v1",
    "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation",
    "authors": [
      "Haoyu Chen",
      "Xiaojie Xu",
      "Wenbo Li",
      "Jingjing Ren",
      "Tian Ye",
      "Songhua Liu",
      "Ying-Cong Chen",
      "Lei Zhu",
      "Xinchao Wang"
    ],
    "abstract": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14908v1",
    "published_date": "2025-03-19 05:22:38 UTC",
    "updated_date": "2025-03-19 05:22:38 UTC"
  },
  {
    "arxiv_id": "2503.14900v1",
    "title": "Deep Contrastive Unlearning for Language Models",
    "authors": [
      "Estrid He",
      "Tabinda Sarwar",
      "Ibrahim Khalil",
      "Xun Yi",
      "Ke Wang"
    ],
    "abstract": "The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14900v1",
    "published_date": "2025-03-19 04:58:45 UTC",
    "updated_date": "2025-03-19 04:58:45 UTC"
  },
  {
    "arxiv_id": "2503.14895v1",
    "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations",
    "authors": [
      "Shuo Li",
      "Jiajun Sun",
      "Guodong Zheng",
      "Xiaoran Fan",
      "Yujiong Shen",
      "Yi Lu",
      "Zhiheng Xi",
      "Yuming Yang",
      "Wenming Tan",
      "Tao Ji",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14895v1",
    "published_date": "2025-03-19 04:39:45 UTC",
    "updated_date": "2025-03-19 04:39:45 UTC"
  },
  {
    "arxiv_id": "2503.14891v1",
    "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer",
    "authors": [
      "Honglin Lin",
      "Zhuoshi Pan",
      "Yu Li",
      "Qizhi Pei",
      "Xin Gao",
      "Mengzhang Cai",
      "Conghui He",
      "Lijun Wu"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14891v1",
    "published_date": "2025-03-19 04:36:35 UTC",
    "updated_date": "2025-03-19 04:36:35 UTC"
  },
  {
    "arxiv_id": "2503.14883v1",
    "title": "Envisioning an AI-Enhanced Mental Health Ecosystem",
    "authors": [
      "Kellie Yu Hui Sim",
      "Kenny Tsu Wei Choo"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.0"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 0 figures, accepted to the CHI'25 Envisioning the Future of\n  Interactive Health Workshop, to be published in HAL",
    "pdf_url": "http://arxiv.org/pdf/2503.14883v1",
    "published_date": "2025-03-19 04:21:38 UTC",
    "updated_date": "2025-03-19 04:21:38 UTC"
  },
  {
    "arxiv_id": "2503.14881v1",
    "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers",
    "authors": [
      "Bo Chen",
      "Xiaoyu Li",
      "Yekun Ke",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14881v1",
    "published_date": "2025-03-19 04:18:57 UTC",
    "updated_date": "2025-03-19 04:18:57 UTC"
  },
  {
    "arxiv_id": "2503.14868v1",
    "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation",
    "authors": [
      "Hoigi Seo",
      "Wongi Jeong",
      "Kyungryeol Lee",
      "Se Young Chun"
    ],
    "abstract": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14868v1",
    "published_date": "2025-03-19 03:45:37 UTC",
    "updated_date": "2025-03-19 03:45:37 UTC"
  },
  {
    "arxiv_id": "2503.14858v2",
    "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
    "authors": [
      "Kevin Wang",
      "Ishaan Javali",
      "Michał Bortkiewicz",
      "Tomasz Trzciński",
      "Benjamin Eysenbach"
    ],
    "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and\nvision, yet comparable progress has remained elusive in reinforcement learning\n(RL). In this paper, we study building blocks for self-supervised RL that\nunlock substantial improvements in scalability, with network depth serving as a\ncritical factor. Whereas most RL papers in recent years have relied on shallow\narchitectures (around 2 - 5 layers), we demonstrate that increasing the depth\nup to 1024 layers can significantly boost performance. Our experiments are\nconducted in an unsupervised goal-conditioned setting, where no demonstrations\nor rewards are provided, so an agent must explore (from scratch) and learn how\nto maximize the likelihood of reaching commanded goals. Evaluated on simulated\nlocomotion and manipulation tasks, our approach increases performance by\n$2\\times$ - $50\\times$. Increasing the model depth not only increases success\nrates but also qualitatively changes the behaviors learned.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Link to project website:\n  https://wang-kevin3290.github.io/scaling-crl/",
    "pdf_url": "http://arxiv.org/pdf/2503.14858v2",
    "published_date": "2025-03-19 03:33:57 UTC",
    "updated_date": "2025-03-22 22:24:37 UTC"
  },
  {
    "arxiv_id": "2503.14847v1",
    "title": "Project Jenkins: Turning Monkey Neural Data into Robotic Arm Movement, and Back",
    "authors": [
      "Andrii Zahorodnii",
      "Dima Yanovsky"
    ],
    "abstract": "Project Jenkins explores how neural activity in the brain can be decoded into\nrobotic movement and, conversely, how movement patterns can be used to generate\nsynthetic neural data. Using real neural data recorded from motor and premotor\ncortex areas of a macaque monkey named Jenkins, we develop models for decoding\n(converting brain signals into robotic arm movements) and encoding (simulating\nbrain activity corresponding to a given movement). For the interface between\nthe brain simulation and the physical world, we utilized Koch v1.1 leader and\nfollower robotic arms. We developed an interactive web console that allows\nusers to generate synthetic brain data from joystick movements in real time.\nOur results are a step towards brain-controlled robotics, prosthetics, and\nenhancing normal motor function. By accurately modeling brain activity, we take\na step toward flexible brain-computer interfaces that generalize beyond\npredefined movements. To support the research community, we provide open source\ntools for both synthetic data generation and neural decoding, fostering\nreproducibility and accelerating progress. The project is available at\nhttps://www.808robots.com/projects/jenkins",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SP",
      "q-bio.NC"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 5 figures, project webpage and github",
    "pdf_url": "http://arxiv.org/pdf/2503.14847v1",
    "published_date": "2025-03-19 03:12:17 UTC",
    "updated_date": "2025-03-19 03:12:17 UTC"
  },
  {
    "arxiv_id": "2503.14833v1",
    "title": "Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability",
    "authors": [
      "Zihao Liu",
      "Xing Liu",
      "Yizhai Zhang",
      "Zhengxiong Liu",
      "Panfeng Huang"
    ],
    "abstract": "One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com/CarlDegio/Curiosity-Diffuser",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14833v1",
    "published_date": "2025-03-19 02:25:36 UTC",
    "updated_date": "2025-03-19 02:25:36 UTC"
  },
  {
    "arxiv_id": "2503.14828v1",
    "title": "The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval",
    "authors": [
      "Firoj Alam",
      "Julia Maria Struß",
      "Tanmoy Chakraborty",
      "Stefan Dietze",
      "Salim Hafid",
      "Katerina Korre",
      "Arianna Muti",
      "Preslav Nakov",
      "Federico Ruggeri",
      "Sebastian Schellhammer",
      "Vinay Setty",
      "Megha Sundriyal",
      "Konstantin Todorov",
      "Venktesh V"
    ],
    "abstract": "The CheckThat! lab aims to advance the development of innovative technologies\ndesigned to identify and counteract online disinformation and manipulation\nefforts across various languages and platforms. The first five editions focused\non key tasks in the information verification pipeline, including\ncheck-worthiness, evidence retrieval and pairing, and verification. Since the\n2023 edition, the lab has expanded its scope to address auxiliary tasks that\nsupport research and decision-making in verification. In the 2025 edition, the\nlab revisits core verification tasks while also considering auxiliary\nchallenges. Task 1 focuses on the identification of subjectivity (a follow-up\nfrom CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets\nfact-checking numerical claims, and Task 4 explores scientific web discourse\nprocessing. These tasks present challenging classification and retrieval\nproblems at both the document and span levels, including multilingual settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "misinformation, factuality, fact-checking, fact-checkers,\n  check-worthiness, Social Media Platforms",
    "pdf_url": "http://arxiv.org/pdf/2503.14828v1",
    "published_date": "2025-03-19 02:06:07 UTC",
    "updated_date": "2025-03-19 02:06:07 UTC"
  },
  {
    "arxiv_id": "2503.14827v1",
    "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
    "authors": [
      "Chejian Xu",
      "Jiawei Zhang",
      "Zhaorun Chen",
      "Chulin Xie",
      "Mintong Kang",
      "Yujin Potter",
      "Zhun Wang",
      "Zhuowen Yuan",
      "Alexander Xiong",
      "Zidi Xiong",
      "Chenhui Zhang",
      "Lingzhi Yuan",
      "Yi Zeng",
      "Peiyang Xu",
      "Chengquan Guo",
      "Andy Zhou",
      "Jeffrey Ziwei Tan",
      "Xuandong Zhao",
      "Francesco Pinto",
      "Zhen Xiang",
      "Yu Gai",
      "Zinan Lin",
      "Dan Hendrycks",
      "Bo Li",
      "Dawn Song"
    ],
    "abstract": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14827v1",
    "published_date": "2025-03-19 01:59:44 UTC",
    "updated_date": "2025-03-19 01:59:44 UTC"
  },
  {
    "arxiv_id": "2503.14809v1",
    "title": "Learning with Expert Abstractions for Efficient Multi-Task Continuous Control",
    "authors": [
      "Jeff Jewett",
      "Sandhya Saisubramanian"
    ],
    "abstract": "Decision-making in complex, continuous multi-task environments is often\nhindered by the difficulty of obtaining accurate models for planning and the\ninefficiency of learning purely from trial and error. While precise environment\ndynamics may be hard to specify, human experts can often provide high-fidelity\nabstractions that capture the essential high-level structure of a task and user\npreferences in the target environment. Existing hierarchical approaches often\ntarget discrete settings and do not generalize across tasks. We propose a\nhierarchical reinforcement learning approach that addresses these limitations\nby dynamically planning over the expert-specified abstraction to generate\nsubgoals to learn a goal-conditioned policy. To overcome the challenges of\nlearning under sparse rewards, we shape the reward based on the optimal state\nvalue in the abstract model. This structured decision-making process enhances\nsample efficiency and facilitates zero-shot generalization. Our empirical\nevaluation on a suite of procedurally generated continuous control environments\ndemonstrates that our approach outperforms existing hierarchical reinforcement\nlearning methods in terms of sample efficiency, task completion rate,\nscalability to complex tasks, and generalization to novel scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 6 figures. Submitted to RLC 2025. Code and experiments at\n  https://github.com/Intelligent-Reliable-Autonomous-Systems/gcrs-expert-abstractions",
    "pdf_url": "http://arxiv.org/pdf/2503.14809v1",
    "published_date": "2025-03-19 00:44:23 UTC",
    "updated_date": "2025-03-19 00:44:23 UTC"
  },
  {
    "arxiv_id": "2503.14800v1",
    "title": "Long Context Modeling with Ranked Memory-Augmented Retrieval",
    "authors": [
      "Ghadir Alselwi",
      "Hao Xue",
      "Shoaib Jameel",
      "Basem Suleiman",
      "Flora D. Salim",
      "Imran Razzak"
    ],
    "abstract": "Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14800v1",
    "published_date": "2025-03-19 00:24:01 UTC",
    "updated_date": "2025-03-19 00:24:01 UTC"
  }
]
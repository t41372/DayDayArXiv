{
  "date": "2024-09-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-12 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 和机器学习的应用创新，包括强化学习、多模态模型和图像处理等领域，重点突出大型语言模型的安全性（如 LLM Honeypot）和因果链接预测（如 Amit Sheth 的作品），其中一些论文展示了高效算法的潜力，如 Retro-li 在小规模检索增强生成中的改进，以及 FlashSplat 在图像生成的快速优化。\n\n下面，我将挑选并简要讨论今天更重要的论文，先从主题相关性强的（如 AI 安全和强化学习）开始，然后聊聊图像处理和多模态创新，最后快速掠过其他较常规的文章。每个条目会列出论文标题（中文 + 英文），并清晰概述其主要贡献和发现，保留核心学术术语。\n\n### 重点论文讨论\n\n**AI 安全与强化学习领域**  \n这些论文关注 AI 系统的鲁棒性和决策优化，Amit Sheth 等学者的作品特别值得注意，因为它们涉及实际应用中的因果关系和安全挑战。\n\n- **Influence of Backdoor Paths on Causal Link Prediction（后门路径对因果链接预测的影响）**  \n  作者：Utkarshani Jaimini, Cory Henson, Amit Sheth。  \n  这篇论文的主要贡献是通过后门路径调整（backdoor path adjustment）来改进知识图谱中的因果链接预测，提出 CausalLPBack 方法，显著减少了混淆变量的影响；在因果推理基准数据集上，MRR 和 Hits@K 指标提高了 30% 和 16%，增强了预测的准确性和鲁棒性。\n\n- **HyperCausalLP: Causal Link Prediction using Hyper-Relational Knowledge Graph（HyperCausalLP: 使用超关系知识图谱的因果链接预测）**  \n  作者：Utkarshani Jaimini, Cory Henson, Amit Sheth。  \n  论文发现现有知识图谱链接预测忽略了中介变量（mediator links），因此引入 HyperCausalLP 方法，将因果网络建模为超关系图谱，平均提高了 5.94% 的均互惠排名（mean reciprocal rank），适用于处理不完整因果网络。\n\n- **Towards Opinion Shaping: A Deep Reinforcement Learning Approach in Bot-User Interactions（朝着意见塑造：Bot-用户互动中的深度强化学习方法）**  \n  作者：Farbod Siahkali, Saba Samadi, Hamed Kebriaei。  \n  关键发现是通过深度确定性策略梯度（DDPG）算法优化社交网络中的 Bot 定位和广告策略，在随机边界置信模型（SBCM）下实现了高效意见塑造，实验证明了其在社交平台资源分配的潜力。\n\n- **Fitted Q-Iteration via Max-Plus-Linear Approximation（通过最大加线性逼近的拟合 Q-迭代）**  \n  作者：Y. Liu, M. A. S. Kolarijani。  \n  论文提出了一种新的拟合 Q-迭代算法，使用最大加线性逼近器（max-plus-linear approximators）来加速离线强化学习，减少了每迭代的计算复杂度，使其独立于样本数量，适用于折扣 Markov 决策过程。\n\n- **Scores as Actions: a framework of fine-tuning diffusion models by continuous-time reinforcement learning（分数作为动作：通过连续时间强化学习的扩散模型微调框架）**  \n  作者：Hanyang Zhao, Haoxian Chen, Ji Zhang, David D. Yao, Wenpin Tang。  \n  贡献在于将分数匹配函数视为控制，将扩散模型微调问题建模为连续时间随机控制问题，通过强化学习提升生成质量，特别适用于文本到图像生成。\n\n这些论文突出了 AI 在安全和决策中的应用，具有实际影响，尤其是在社交和机器人领域。\n\n**图像处理与多模态创新**  \n这一组论文聚焦于生成模型和多模态融合，强调高效性和鲁棒性，如 FlashSplat 的快速优化和 Retro-li 的小规模检索增强。\n\n- **FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally（FlashSplat: 优化解决的 2D 到 3D 高斯散斑分割）**  \n  作者：Qiuhong Shen, Xingyi Yang, Xinchao Wang。  \n  主要发现是通过线性规划求解高斯散斑分割问题，实现了 50 倍速度提升，同时提高了 3D 场景分割的鲁棒性，支持下游任务如物体移除。\n\n- **Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy Similarity Searches and Domain Shift Generalization（Retro-li: 支持噪声相似搜索和小规模数据库的检索增强生成）**  \n  作者：Gentiana Rashiti, Geethan Karunaratne, Mrinmaya Sachan, Abu Sebastian, Abbas Rahimi。  \n  论文创新性地为小规模数据库引入检索增强生成，添加正则化减少噪声影响和领域偏移，提高了困惑度并支持模拟硬件实现，展示了在资源有限环境下的泛化能力。\n\n- **360PanT: Training-Free Text-Driven 360-Degree Panorama-to-Panorama Translation（360PanT: 无需训练的文本驱动 360 度全景到全景转换）**  \n  作者：Hai Wang, Jing-Hao Xue。  \n  贡献在于提出边界连续编码和无缝平铺转换方法，实现文本驱动的 360 度全景图像转换，避免了边界不连续问题，在真实数据集上表现出色。\n\n- **AnySkin: Plug-and-play Skin Sensing for Robotic Touch（AnySkin: 即插即用皮肤感知用于机器人触觉）**  \n  作者：Raunaq Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto。  \n  论文开发了一种磁性触觉传感器，支持无校准滑移检测和策略学习，并实现了跨实例零样本泛化，适用于机器人操作。\n\n这些工作在多模态和图像领域展示了高效工具的潜力，特别适合实际应用如机器人和虚拟现实。\n\n**其他快速掠过**  \n剩余论文涉及广泛主题，如医疗 AI、教育应用和基础模型优化，但许多是常规扩展或初步实验，这里只简要提及几个代表性 ones。\n\n- **Knowledge Tagging with Large Language Model based Multi-Agent System（基于大语言模型的多代理系统知识标记）**  \n  作者：Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen。  \n  使用多代理系统自动化教育问题知识标记，改进了复杂案例处理，在 MathKnowCT 数据集上表现优异。\n\n- **E-QUARTIC: Energy Efficient Edge Ensemble of Convolutional Neural Networks（E-QUARTIC: 能量高效的边缘卷积神经网络集成）**  \n  作者：Le Zhang, Onat Gungor, Flavio Ponzina, Tajana Rosing。  \n  提出能量感知集成模型，提高了边缘设备准确性和鲁棒性。\n\n- **FedProphet: Memory-Efficient Federated Adversarial Training（FedProphet: 内存高效的联邦对抗训练）**  \n  作者：Minxue Tang, Yitu Wang, Jingyang Zhang, Louis DiValentin, Aolin Ding, Amin Hass, Yiran Chen, Hai \"Helen\" Li。  \n  优化了联邦学习中的内存使用和鲁棒性。\n\n其他如 LLM 在市场行为或核聚变优化中的应用，虽然有趣，但较为初步或特定，因此不展开讨论。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，特别是安全和高效算法的潜力。感兴趣的读者可关注上述关键论文，探索实际应用。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2409.11426v1",
      "title": "Towards Opinion Shaping: A Deep Reinforcement Learning Approach in Bot-User Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Farbod Siahkali",
        "Saba Samadi",
        "Hamed Kebriaei"
      ],
      "abstract": "This paper aims to investigate the impact of interference in social network\nalgorithms via user-bot interactions, focusing on the Stochastic Bounded\nConfidence Model (SBCM). This paper explores two approaches: positioning bots\ncontrolled by agents into the network and targeted advertising under various\ncircumstances, operating with an advertising budget. This study integrates the\nDeep Deterministic Policy Gradient (DDPG) algorithm and its variants to\nexperiment with different Deep Reinforcement Learning (DRL). Finally,\nexperimental results demonstrate that this approach can result in efficient\nopinion shaping, indicating its potential in deploying advertising resources on\nsocial platforms.",
      "tldr_zh": "这篇论文研究了通过用户-机器人交互干预社交网络算法的影响，焦点是 Stochastic Bounded Confidence Model (SBCM)，旨在探索意见塑造的策略。研究采用两种方法：将由代理控制的机器人放置到网络中，以及在广告预算限制下进行目标广告，并整合 Deep Deterministic Policy Gradient (DDPG) 算法及其变体进行 Deep Reinforcement Learning (DRL) 实验。结果显示，这种方法能高效地塑造意见，并展示其在社交平台广告资源部署方面的潜力。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "I.2.8; I.2.6"
      ],
      "primary_category": "cs.SI",
      "comment": "5 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.11426v1",
      "published_date": "2024-09-12 23:39:07 UTC",
      "updated_date": "2024-09-12 23:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:16:13.642376"
    },
    {
      "arxiv_id": "2410.00004v2",
      "title": "Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy Similarity Searches and Domain Shift Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Gentiana Rashiti",
        "Geethan Karunaratne",
        "Mrinmaya Sachan",
        "Abu Sebastian",
        "Abbas Rahimi"
      ],
      "abstract": "The retrieval augmented generation (RAG) system such as Retro has been shown\nto improve language modeling capabilities and reduce toxicity and\nhallucinations by retrieving from a database of non-parametric memory\ncontaining trillions of entries. We introduce Retro-li that shows retrieval can\nalso help using a small-scale database, but it demands more accurate and better\nneighbors when searching in a smaller hence sparser non-parametric memory. This\ncan be met by using a proper semantic similarity search. We further propose\nadding a regularization to the non-parametric memory for the first time: it\nsignificantly reduces perplexity when the neighbor search operations are noisy\nduring inference, and it improves generalization when a domain shift occurs. We\nalso show that Retro-li's non-parametric memory can potentially be implemented\non analog in-memory computing hardware, exhibiting O(1) search time while\ncausing noise in retrieving neighbors, with minimal (<1%) performance loss. Our\ncode is available at:\nhttps://github.com/IBM/Retrieval-Enhanced-Transformer-Little.",
      "tldr_zh": "这篇论文介绍了 Retro-li，一种小型 Retrieval Augmented Generation (RAG) 系统，它通过更准确的语义相似性搜索来优化在小规模非参数记忆中的检索，支持噪声搜索和领域偏移泛化。研究首次在非参数记忆中添加正则化机制，能够显著降低检索噪声时的 perplexity 并提升模型在领域偏移情况下的泛化性能。实验结果显示，Retro-li 可在模拟内存计算硬件上实现 O(1) 搜索时间，同时保持性能损失小于 1%，并提供了开源代码。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00004v2",
      "published_date": "2024-09-12 23:29:33 UTC",
      "updated_date": "2025-03-26 10:27:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:16:24.540024"
    },
    {
      "arxiv_id": "2409.08422v3",
      "title": "Fitted Q-Iteration via Max-Plus-Linear Approximation",
      "title_zh": "基于最大加线性逼近的拟合 Q 迭代",
      "authors": [
        "Y. Liu",
        "M. A. S. Kolarijani"
      ],
      "abstract": "In this study, we consider the application of max-plus-linear approximators\nfor Q-function in offline reinforcement learning of discounted Markov decision\nprocesses. In particular, we incorporate these approximators to propose novel\nfitted Q-iteration (FQI) algorithms with provable convergence. Exploiting the\ncompatibility of the Bellman operator with max-plus operations, we show that\nthe max-plus-linear regression within each iteration of the proposed FQI\nalgorithm reduces to simple max-plus matrix-vector multiplications. We also\nconsider the variational implementation of the proposed algorithm which leads\nto a per-iteration complexity that is independent of the number of samples.",
      "tldr_zh": "该研究探讨了在离线强化学习中，使用 Max-Plus-Linear approximators 来近似 Q-function，以处理折扣 Markov 决策过程。论文提出了一种新型 Fitted Q-Iteration (FQI) 算法，该算法通过 Bellman operator 与 max-plus 操作的兼容性，将每迭代的回归步骤简化为高效的 max-plus 矩阵-向量乘法，并证明了其收敛性。此外，算法的变分实现使得每迭代的计算复杂度独立于样本数量，提高了整体效率。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08422v3",
      "published_date": "2024-09-12 22:51:08 UTC",
      "updated_date": "2025-03-07 21:19:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:16:37.225901"
    },
    {
      "arxiv_id": "2410.14680v1",
      "title": "Influence of Backdoor Paths on Causal Link Prediction",
      "title_zh": "后门路径对因果链接预测的影响",
      "authors": [
        "Utkarshani Jaimini",
        "Cory Henson",
        "Amit Sheth"
      ],
      "abstract": "The current method for predicting causal links in knowledge graphs uses\nweighted causal relations. For a given link between cause-effect entities, the\npresence of a confounder affects the causal link prediction, which can lead to\nspurious and inaccurate results. We aim to block these confounders using\nbackdoor path adjustment. Backdoor paths are non-causal association flows that\nconnect the \\textit{cause-entity} to the \\textit{effect-entity} through other\nvariables. Removing these paths ensures a more accurate prediction of causal\nlinks. This paper proposes CausalLPBack, a novel approach to causal link\nprediction that eliminates backdoor paths and uses knowledge graph link\nprediction methods. It extends the representation of causality in a\nneuro-symbolic framework, enabling the adoption and use of traditional causal\nAI concepts and methods. We demonstrate our approach using a causal reasoning\nbenchmark dataset of simulated videos. The evaluation involves a unique dataset\nsplitting method called the Markov-based split that's relevant for causal link\nprediction. The evaluation of the proposed approach demonstrates atleast 30\\%\nin MRR and 16\\% in Hits@K inflated performance for causal link prediction that\nis due to the bias introduced by backdoor paths for both baseline and weighted\ncausal relations.",
      "tldr_zh": "该论文探讨了 backdoor paths 对知识图谱中因果链接预测的影响，这些路径会通过混杂因素(confounders)导致预测结果虚假和不准确。作者提出了一种新方法 CausalLPBack，通过消除 backdoor paths 并结合知识图谱链接预测技术，在神经符号框架中扩展因果表示。实验使用因果推理基准数据集和 Markov-based split 分割方法，结果显示 CausalLPBack 比基线模型在 MRR 上至少提高了 30%，在 Hits@K 上提高了 16%，显著降低了偏差影响。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.14680v1",
      "published_date": "2024-09-12 22:16:36 UTC",
      "updated_date": "2024-09-12 22:16:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:16:48.819909"
    },
    {
      "arxiv_id": "2409.08406v2",
      "title": "Knowledge Tagging with Large Language Model based Multi-Agent System",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Li",
        "Tianlong Xu",
        "Ethan Chang",
        "Qingsong Wen"
      ],
      "abstract": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts.",
      "tldr_zh": "本文研究了在教育应用中，使用基于大型语言模型 (LLMs) 的多智能体系统来自动化知识标签任务，以克服传统方法和现有算法在处理复杂知识定义及数值约束方面的局限性。该系统通过深度语义理解和问题逻辑链接，显著提升了标签准确性，并在公开数据集 MathKnowCT 上表现出色，比之前方法有显著改进。最终，论文讨论了这种 LLM-based 多智能体系统的潜力，有助于在教育场景中实现高效、可扩展的知识标签自动化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2025 (AAAI/IAAI 2025 Innovative Application Award)",
      "pdf_url": "http://arxiv.org/pdf/2409.08406v2",
      "published_date": "2024-09-12 21:39:01 UTC",
      "updated_date": "2024-12-19 16:09:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:17:01.457979"
    },
    {
      "arxiv_id": "2409.08400v1",
      "title": "Scores as Actions: a framework of fine-tuning diffusion models by continuous-time reinforcement learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyang Zhao",
        "Haoxian Chen",
        "Ji Zhang",
        "David D. Yao",
        "Wenpin Tang"
      ],
      "abstract": "Reinforcement Learning from human feedback (RLHF) has been shown a promising\ndirection for aligning generative models with human intent and has also been\nexplored in recent works for alignment of diffusion generative models. In this\nwork, we provide a rigorous treatment by formulating the task of fine-tuning\ndiffusion models, with reward functions learned from human feedback, as an\nexploratory continuous-time stochastic control problem. Our key idea lies in\ntreating the score-matching functions as controls/actions, and upon this, we\ndevelop a unified framework from a continuous-time perspective, to employ\nreinforcement learning (RL) algorithms in terms of improving the generation\nquality of diffusion models. We also develop the corresponding continuous-time\nRL theory for policy optimization and regularization under assumptions of\nstochastic different equations driven environment. Experiments on the\ntext-to-image (T2I) generation will be reported in the accompanied paper.",
      "tldr_zh": "本研究提出一个框架，将扩散模型（diffusion models）的微调任务表述为探索性的连续时间随机控制问题，通过强化学习从人类反馈（RLHF）来改进生成质量。关键想法是将分数匹配函数（score-matching functions）视为控制/动作，并基于此开发统一的连续时间强化学习（continuous-time reinforcement learning）算法，包括政策优化和正则化理论。实验将聚焦于文本到图像（T2I）生成，以验证该框架的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08400v1",
      "published_date": "2024-09-12 21:12:21 UTC",
      "updated_date": "2024-09-12 21:12:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:17:13.104853"
    },
    {
      "arxiv_id": "2410.14679v1",
      "title": "HyperCausalLP: Causal Link Prediction using Hyper-Relational Knowledge Graph",
      "title_zh": "HyperCausalLP：利用超关系知识图谱的因果链接预测",
      "authors": [
        "Utkarshani Jaimini",
        "Cory Henson",
        "Amit Sheth"
      ],
      "abstract": "Causal networks are often incomplete with missing causal links. This is due\nto various issues, such as missing observation data. Recent approaches to the\nissue of incomplete causal networks have used knowledge graph link prediction\nmethods to find the missing links. In the causal link A causes B causes C, the\ninfluence of A to C is influenced by B which is known as a mediator. Existing\napproaches using knowledge graph link prediction do not consider these mediated\ncausal links. This paper presents HyperCausalLP, an approach designed to find\nmissing causal links within a causal network with the help of mediator links.\nThe problem of missing links is formulated as a hyper-relational knowledge\ngraph completion. The approach uses a knowledge graph link prediction model\ntrained on a hyper-relational knowledge graph with the mediators. The approach\nis evaluated on a causal benchmark dataset, CLEVRER-Humans. Results show that\nthe inclusion of knowledge about mediators in causal link prediction using\nhyper-relational knowledge graph improves the performance on an average by\n5.94% mean reciprocal rank.",
      "tldr_zh": "该论文提出HyperCausalLP方法，用于预测因果网络中的缺失链接问题，该方法利用Hyper-Relational Knowledge Graph来考虑中介者（mediators）的作用，以弥补现有知识图谱链接预测模型的不足。研究将缺失链接问题表述为超关系知识图谱完成任务，并通过在包含中介者的知识图谱上训练模型来提升预测准确性。在CLEVRER-Humans因果基准数据集上评估，结果显示性能平均提高了5.94%的Mean Reciprocal Rank（MRR）。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2405.02327",
      "pdf_url": "http://arxiv.org/pdf/2410.14679v1",
      "published_date": "2024-09-12 21:01:30 UTC",
      "updated_date": "2024-09-12 21:01:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:17:24.621098"
    },
    {
      "arxiv_id": "2409.08397v1",
      "title": "360PanT: Training-Free Text-Driven 360-Degree Panorama-to-Panorama Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Hai Wang",
        "Jing-Hao Xue"
      ],
      "abstract": "Preserving boundary continuity in the translation of 360-degree panoramas\nremains a significant challenge for existing text-driven image-to-image\ntranslation methods. These methods often produce visually jarring\ndiscontinuities at the translated panorama's boundaries, disrupting the\nimmersive experience. To address this issue, we propose 360PanT, a\ntraining-free approach to text-based 360-degree panorama-to-panorama\ntranslation with boundary continuity. Our 360PanT achieves seamless\ntranslations through two key components: boundary continuity encoding and\nseamless tiling translation with spatial control. Firstly, the boundary\ncontinuity encoding embeds critical boundary continuity information of the\ninput 360-degree panorama into the noisy latent representation by constructing\nan extended input image. Secondly, leveraging this embedded noisy latent\nrepresentation and guided by a target prompt, the seamless tiling translation\nwith spatial control enables the generation of a translated image with\nidentical left and right halves while adhering to the extended input's\nstructure and semantic layout. This process ensures a final translated\n360-degree panorama with seamless boundary continuity. Experimental results on\nboth real-world and synthesized datasets demonstrate the effectiveness of our\n360PanT in translating 360-degree panoramas. Code is available at\n\\href{https://github.com/littlewhitesea/360PanT}{https://github.com/littlewhitesea/360PanT}.",
      "tldr_zh": "该论文提出360PanT，一种无需训练的文本驱动360-degree panorama-to-panorama翻译方法，旨在解决现有方法在处理360度全景图时产生的边界不连续性问题。核心组件包括boundary continuity encoding，通过构建扩展输入图像将边界连续性信息嵌入噪声潜在表示，以及seamless tiling translation with spatial control，利用目标提示生成左右半边相同的翻译图像，确保结构和语义布局的一致性。实验在真实和合成数据集上验证了360PanT的有效性，并提供了开源代码链接。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by WACV 2025, Project Page:\n  \\href{https://littlewhitesea.github.io/360PanT.github.io/}{https://littlewhitesea.github.io/360PanT.github.io/}",
      "pdf_url": "http://arxiv.org/pdf/2409.08397v1",
      "published_date": "2024-09-12 20:56:16 UTC",
      "updated_date": "2024-09-12 20:56:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:17:36.631009"
    },
    {
      "arxiv_id": "2409.08386v1",
      "title": "Self-Supervised Inference of Agents in Trustless Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Vladyslav Larin",
        "Ivan Nikitin",
        "Alexander Firsov"
      ],
      "abstract": "In this paper, we propose a novel approach where agents can form swarms to\nproduce high-quality responses effectively. This is accomplished by utilizing\nagents capable of data inference and ranking, which can be effectively\nimplemented using LLMs as response classifiers. We assess existing approaches\nfor trustless agent inference, define our methodology, estimate practical\nparameters, and model various types of malicious agent attacks. Our method\nleverages the collective intelligence of swarms, ensuring robust and efficient\ndecentralized AI inference with better accuracy, security, and reliability. We\nshow that our approach is an order of magnitude faster than other trustless\ninference strategies reaching less than 125 ms validation latency.",
      "tldr_zh": "该论文提出了一种自监督代理推理方法，在无信任环境中让代理形成群（swarms），以高效生成高质量响应。方法利用大型语言模型（LLMs）作为响应分类器，进行数据推理和排名，同时评估现有策略、定义参数并模拟恶意代理攻击。结果显示，该方法比其他无信任推理策略快一个数量级，验证延迟不到125 ms，并显著提高了准确性、安全性和可靠性。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08386v1",
      "published_date": "2024-09-12 20:32:07 UTC",
      "updated_date": "2024-09-12 20:32:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:17:49.041531"
    },
    {
      "arxiv_id": "2409.08381v1",
      "title": "Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations",
      "title_zh": "重新审视带有部分标注的多标签识别提示策略",
      "authors": [
        "Samyak Rawlekar",
        "Shubhang Bhatnagar",
        "Narendra Ahuja"
      ],
      "abstract": "Vision-language models (VLMs) like CLIP have been adapted for Multi-Label\nRecognition (MLR) with partial annotations by leveraging prompt-learning, where\npositive and negative prompts are learned for each class to associate their\nembeddings with class presence or absence in the shared vision-text feature\nspace. While this approach improves MLR performance by relying on VLM priors,\nwe hypothesize that learning negative prompts may be suboptimal, as the\ndatasets used to train VLMs lack image-caption pairs explicitly focusing on\nclass absence. To analyze the impact of positive and negative prompt learning\non MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is\nlearned with VLM guidance while the other is replaced by an embedding vector\nlearned directly in the shared feature space without relying on the text\nencoder. Through empirical analysis, we observe that negative prompts degrade\nMLR performance, and learning only positive prompts, combined with learned\nnegative embeddings (PositiveCoOp), outperforms dual prompt learning\napproaches. Moreover, we quantify the performance benefits that prompt-learning\noffers over a simple vision-features-only baseline, observing that the baseline\ndisplays strong performance comparable to dual prompt learning approach\n(DualCoOp), when the proportion of missing labels is low, while requiring half\nthe training compute and 16 times fewer parameters",
      "tldr_zh": "这篇论文重新审视了在部分标注的多标签识别 (MLR) 中提示学习策略，针对视觉语言模型 (VLMs) 如 CLIP，通过学习正负提示来关联类别的存在或缺失。作者提出 PositiveCoOp 和 NegativeCoOp 方法，其中只学习一个提示（如正提示），而另一个（如负提示）用直接学习的嵌入向量替换，以避免依赖文本编码器。实验发现，学习负提示会降低 MLR 性能，而 PositiveCoOp 优于传统的双提示学习 (DualCoOp)。此外，与简单视觉特征基线相比，提示学习在缺失标签比例低时优势有限，但基线只需一半的训练计算量和 16 倍更少的参数，提供更高效的替代方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08381v1",
      "published_date": "2024-09-12 20:02:51 UTC",
      "updated_date": "2024-09-12 20:02:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:18:02.053159"
    },
    {
      "arxiv_id": "2409.08379v2",
      "title": "The Impact of Large Language Models on Open-source Innovation: Evidence from GitHub Copilot",
      "title_zh": "翻译失败",
      "authors": [
        "Doron Yeverechyahu",
        "Raveesh Mayya",
        "Gal Oestreicher-Singer"
      ],
      "abstract": "Large Language Models (LLMs) have been shown to enhance individual\nproductivity in guided settings. Whereas LLMs are likely to also transform\ninnovation processes in a collaborative work setting, it is unclear what\ntrajectory this transformation will follow. Innovation in these contexts\nencompasses both capability innovation that explores new possibilities by\nacquiring new competencies in a project and iterative innovation that exploits\nexisting foundations by enhancing established competencies and improving\nproject quality. Whether LLMs affect these two aspects of collaborative work\nand to what extent is an open empirical question. Open-source development\nprovides an ideal setting to examine LLM impacts on these innovation types, as\nits voluntary and open/collaborative nature of contributions provides the\ngreatest opportunity for technological augmentation. We focus on open-source\nprojects on GitHub by leveraging a natural experiment around the selective\nrollout of GitHub Copilot (a programming-focused LLM) in October 2021, where\nGitHub Copilot selectively supported programming languages like Python or Rust,\nbut not R or Haskell. We observe a significant jump in overall contributions,\nsuggesting that LLMs effectively augment collaborative innovation in an\nunguided setting. Interestingly, Copilot's launch increased iterative\ninnovation focused on maintenance-related or feature-refining contributions\nsignificantly more than it did capability innovation through code-development\nor feature-introducing commits. This disparity was more pronounced after the\nmodel upgrade in June 2022 and was evident in active projects with extensive\ncoding activity, suggesting that as both LLM capabilities and/or available\ncontextual information improve, the gap between capability and iterative\ninnovation may widen. We discuss practical and policy implications to\nincentivize high-value innovative solutions.",
      "tldr_zh": "这篇论文研究了大型语言模型 (LLMs) 对开源创新的影响，通过 GitHub Copilot 的自然实验分析其在协作环境中的作用。研究将创新分为 capability innovation（探索新能力）和 iterative innovation（改进现有功能），并发现 Copilot 的推出显著提升了整体贡献，尤其是在 iterative innovation 方面，而 capability innovation 的增长较小。这种差异在 2022 年模型升级后更为明显，且在活跃项目中更突出。论文讨论了实际和政策含义，以激励高价值创新解决方案。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "econ.GN",
        "q-fin.EC",
        "I.2.7; D.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "JEL Classification: O31, C88, J24, O35, L86",
      "pdf_url": "http://arxiv.org/pdf/2409.08379v2",
      "published_date": "2024-09-12 19:59:54 UTC",
      "updated_date": "2025-05-13 16:08:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:18:13.627583"
    },
    {
      "arxiv_id": "2409.08372v2",
      "title": "FedProphet: Memory-Efficient Federated Adversarial Training via Robust and Consistent Cascade Learning",
      "title_zh": "FedProphet：通过鲁棒且一致的级联学习实现的内存高效联邦对抗训练",
      "authors": [
        "Minxue Tang",
        "Yitu Wang",
        "Jingyang Zhang",
        "Louis DiValentin",
        "Aolin Ding",
        "Amin Hass",
        "Yiran Chen",
        "Hai \"Helen\" Li"
      ],
      "abstract": "Federated Adversarial Training (FAT) can supplement robustness against\nadversarial examples to Federated Learning (FL), promoting a meaningful step\ntoward trustworthy AI. However, FAT requires large models to preserve high\naccuracy while achieving strong robustness, incurring high memory-swapping\nlatency when training on memory-constrained edge devices. Existing\nmemory-efficient FL methods suffer from poor accuracy and weak robustness due\nto inconsistent local and global models. In this paper, we propose FedProphet,\na novel FAT framework that can achieve memory efficiency, robustness, and\nconsistency simultaneously. FedProphget reduces the memory requirement in local\ntraining while guaranteeing adversarial robustness by adversarial cascade\nlearning with strong convexity regularization, and we show that the strong\nrobustness also implies low inconsistency in FedProphet. We also develop a\ntraining coordinator on the server of FL, with Adaptive Perturbation Adjustment\nfor utility-robustness balance and Differentiated Module Assignment for\nobjective inconsistency mitigation. FedPeophet significantly outperforms other\nbaselines under different experimental settings, maintaining the accuracy and\nrobustness of end-to-end FAT with 80% memory reduction and up to 10.8x speedup\nin training time.",
      "tldr_zh": "本研究提出FedProphet，一种高效的Federated Adversarial Training (FAT)框架，旨在解决FAT在Federated Learning (FL)中因大模型内存需求高而导致的训练延迟问题，同时兼顾鲁棒性和模型一致性。FedProphet通过对抗级联学习(adversarial cascade learning)和强凸性正则化(strong convexity regularization)减少本地训练内存需求，并证明其强鲁棒性可降低本地与全局模型的不一致性。此外，服务器端的训练协调器引入Adaptive Perturbation Adjustment平衡效用与鲁棒性，以及Differentiated Module Assignment缓解目标不一致。实验结果显示，FedProphet在不同设置下显著优于基线，保持端到端FAT的准确性和鲁棒性，同时减少80%内存并加速训练时间高达10.8倍。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by MLSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.08372v2",
      "published_date": "2024-09-12 19:39:14 UTC",
      "updated_date": "2025-04-14 18:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:18:26.738174"
    },
    {
      "arxiv_id": "2409.08369v1",
      "title": "E-QUARTIC: Energy Efficient Edge Ensemble of Convolutional Neural Networks for Resource-Optimized Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Le Zhang",
        "Onat Gungor",
        "Flavio Ponzina",
        "Tajana Rosing"
      ],
      "abstract": "Ensemble learning is a meta-learning approach that combines the predictions\nof multiple learners, demonstrating improved accuracy and robustness.\nNevertheless, ensembling models like Convolutional Neural Networks (CNNs)\nresult in high memory and computing overhead, preventing their deployment in\nembedded systems. These devices are usually equipped with small batteries that\nprovide power supply and might include energy-harvesting modules that extract\nenergy from the environment. In this work, we propose E-QUARTIC, a novel Energy\nEfficient Edge Ensembling framework to build ensembles of CNNs targeting\nArtificial Intelligence (AI)-based embedded systems. Our design outperforms\nsingle-instance CNN baselines and state-of-the-art edge AI solutions, improving\naccuracy and adapting to varying energy conditions while maintaining similar\nmemory requirements. Then, we leverage the multi-CNN structure of the designed\nensemble to implement an energy-aware model selection policy in\nenergy-harvesting AI systems. We show that our solution outperforms the\nstate-of-the-art by reducing system failure rate by up to 40% while ensuring\nhigher average output qualities. Ultimately, we show that the proposed design\nenables concurrent on-device training and high-quality inference execution at\nthe edge, limiting the performance and energy overheads to less than 0.04%.",
      "tldr_zh": "该研究提出E-QUARTIC，一种能量高效的边缘集成框架，用于构建Convolutional Neural Networks (CNNs)的集成模型，针对AI嵌入式系统优化资源利用。该框架通过集成多个CNN模型，提高准确性和鲁棒性，同时适应动态能量条件，并保持相似的内存需求。实验结果显示，E-QUARTIC在能量采集系统中将系统故障率降低高达40%，并在边缘设备上实现并发训练和推理，性能和能量开销低于0.04%。这为资源优化学习提供了高效解决方案。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted by the 30th Asia and South Pacific Design Automation\n  Conference (ASP-DAC 2025)",
      "pdf_url": "http://arxiv.org/pdf/2409.08369v1",
      "published_date": "2024-09-12 19:30:22 UTC",
      "updated_date": "2024-09-12 19:30:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:18:47.652103"
    },
    {
      "arxiv_id": "2409.08357v2",
      "title": "An Experimental Study of Competitive Market Behavior Through LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Jingru Jia",
        "Zehua Yuan"
      ],
      "abstract": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies.",
      "tldr_zh": "本研究通过大型语言模型（LLMs）进行实验，探讨它们在理解竞争市场动态方面的能力，方法包括模拟市场代理行为并评估其是否能趋向竞争均衡（competitive equilibria）。结果显示，当前 LLMs 在复制人类动态决策过程中面临挑战，无法达到市场均衡（market equilibrium），从而暴露了其在市场模拟中的局限性。尽管 LLMs 提供了一种可扩展且可重复的实验工具，但研究建议未来需增强动态学习能力并整合行为经济学元素，以更好地捕捉市场行为的复杂性并为经济政策优化提供新见解。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08357v2",
      "published_date": "2024-09-12 18:50:13 UTC",
      "updated_date": "2024-11-01 01:45:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:18:49.426848"
    },
    {
      "arxiv_id": "2409.08351v1",
      "title": "Bayesian Inverse Graphics for Few-Shot Concept Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Octavio Arriaga",
        "Jichen Guo",
        "Rebecca Adam",
        "Sebastian Houben",
        "Frank Kirchner"
      ],
      "abstract": "Humans excel at building generalizations of new concepts from just one single\nexample. Contrary to this, current computer vision models typically require\nlarge amount of training samples to achieve a comparable accuracy. In this work\nwe present a Bayesian model of perception that learns using only minimal data,\na prototypical probabilistic program of an object. Specifically, we propose a\ngenerative inverse graphics model of primitive shapes, to infer posterior\ndistributions over physically consistent parameters from one or several images.\nWe show how this representation can be used for downstream tasks such as\nfew-shot classification and pose estimation. Our model outperforms existing\nfew-shot neural-only classification algorithms and demonstrates generalization\nacross varying lighting conditions, backgrounds, and out-of-distribution\nshapes. By design, our model is uncertainty-aware and uses our new\ndifferentiable renderer for optimizing global scene parameters through gradient\ndescent, sampling posterior distributions over object parameters with Markov\nChain Monte Carlo (MCMC), and using a neural based likelihood function.",
      "tldr_zh": "本研究提出了一种贝叶斯逆图形(Bayesian Inverse Graphics)模型，用于实现少样本(Few-Shot)概念学习，旨在模仿人类从单一示例中快速泛化新概念的能力。该模型基于一个生成性逆图形框架，通过概率程序推断原始形状的物理一致参数后验分布，仅需一或几张图像即可完成学习。模型利用可微渲染器(differentiable renderer)结合梯度下降和Markov Chain Monte Carlo (MCMC)采样，以及基于神经网络的似然函数，确保不确定性感知和优化全局场景参数。在下游任务如少样本分类和姿态估计中，该模型优于现有神经网络算法，并展示出对不同照明、背景和分布外形状的强泛化能力。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08351v1",
      "published_date": "2024-09-12 18:30:41 UTC",
      "updated_date": "2024-09-12 18:30:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:19:01.951295"
    },
    {
      "arxiv_id": "2409.08346v1",
      "title": "Towards Quantifying and Reducing Language Mismatch Effects in Cross-Lingual Speech Anti-Spoofing",
      "title_zh": "翻译失败",
      "authors": [
        "Tianchi Liu",
        "Ivan Kukanov",
        "Zihan Pan",
        "Qiongqiong Wang",
        "Hardik B. Sailor",
        "Kong Aik Lee"
      ],
      "abstract": "The effects of language mismatch impact speech anti-spoofing systems, while\ninvestigations and quantification of these effects remain limited. Existing\nanti-spoofing datasets are mainly in English, and the high cost of acquiring\nmultilingual datasets hinders training language-independent models. We initiate\nthis work by evaluating top-performing speech anti-spoofing systems that are\ntrained on English data but tested on other languages, observing notable\nperformance declines. We propose an innovative approach - Accent-based data\nexpansion via TTS (ACCENT), which introduces diverse linguistic knowledge to\nmonolingual-trained models, improving their cross-lingual capabilities. We\nconduct experiments on a large-scale dataset consisting of over 3 million\nsamples, including 1.8 million training samples and nearly 1.2 million testing\nsamples across 12 languages. The language mismatch effects are preliminarily\nquantified and remarkably reduced over 15% by applying the proposed ACCENT.\nThis easily implementable method shows promise for multilingual and\nlow-resource language scenarios.",
      "tldr_zh": "该研究探讨了语言不匹配对跨语言语音反欺骗系统的影响，通过评估在英文数据上训练的顶级模型，发现其在其他语言上的性能显著下降。作者提出了一种创新方法ACCENT（基于TTS的口音数据扩展），利用文本到语音技术引入多样语言知识，从而提升模型的跨语言能力。在覆盖12种语言的大规模数据集（超过300万样本）上进行实验，成功量化了语言不匹配效果，并通过ACCENT将性能下降减少了15%以上，为多语言和低资源语言场景提供了一种易于实施的解决方案。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to the IEEE Spoken Language Technology Workshop (SLT) 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.08346v1",
      "published_date": "2024-09-12 18:18:22 UTC",
      "updated_date": "2024-09-12 18:18:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:19:16.749308"
    },
    {
      "arxiv_id": "2409.08276v3",
      "title": "AnySkin: Plug-and-play Skin Sensing for Robotic Touch",
      "title_zh": "AnySkin：即插即用机器人触觉皮肤感知",
      "authors": [
        "Raunaq Bhirangi",
        "Venkatesh Pattabiraman",
        "Enes Erciyes",
        "Yifeng Cao",
        "Tess Hellebrekers",
        "Lerrel Pinto"
      ],
      "abstract": "While tactile sensing is widely accepted as an important and useful sensing\nmodality, its use pales in comparison to other sensory modalities like vision\nand proprioception. AnySkin addresses the critical challenges that impede the\nuse of tactile sensing -- versatility, replaceability, and data reusability.\nBuilding on the simplistic design of ReSkin, and decoupling the sensing\nelectronics from the sensing interface, AnySkin simplifies integration making\nit as straightforward as putting on a phone case and connecting a charger.\nFurthermore, AnySkin is the first uncalibrated tactile-sensor with\ncross-instance generalizability of learned manipulation policies. To summarize,\nthis work makes three key contributions: first, we introduce a streamlined\nfabrication process and a design tool for creating an adhesive-free, durable\nand easily replaceable magnetic tactile sensor; second, we characterize slip\ndetection and policy learning with the AnySkin sensor; and third, we\ndemonstrate zero-shot generalization of models trained on one instance of\nAnySkin to new instances, and compare it with popular existing tactile\nsolutions like DIGIT and ReSkin. Videos of experiments, fabrication details and\ndesign files can be found on https://any-skin.github.io/",
      "tldr_zh": "这篇论文介绍了 AnySkin，一种即插即用的触觉传感器设计，用于提升机器人触觉感知的通用性、可替换性和数据可重用性。AnySkin 基于 ReSkin 的简单设计，将传感电子设备与传感界面分离，使集成过程像安装手机壳一样便捷，同时它是首个无需校准的触觉传感器，能够实现跨实例的操作策略学习。论文的关键贡献包括：简化制造过程和设计工具以创建无粘合剂的磁性触觉传感器；表征滑移检测和策略学习；以及展示在一种 AnySkin 实例上训练的模型对新实例的零样本 generalization，并与 DIGIT 和 ReSkin 等传感器进行比较。实验结果证明了 AnySkin 在机器人触觉应用中的显著优势。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08276v3",
      "published_date": "2024-09-12 17:59:44 UTC",
      "updated_date": "2024-09-27 16:09:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:19:36.514151"
    },
    {
      "arxiv_id": "2409.08273v1",
      "title": "Hand-Object Interaction Pretraining from Videos",
      "title_zh": "基于视频的手-物体",
      "authors": [
        "Himanshu Gaurav Singh",
        "Antonio Loquercio",
        "Carmelo Sferrazza",
        "Jane Wu",
        "Haozhi Qi",
        "Pieter Abbeel",
        "Jitendra Malik"
      ],
      "abstract": "We present an approach to learn general robot manipulation priors from 3D\nhand-object interaction trajectories. We build a framework to use in-the-wild\nvideos to generate sensorimotor robot trajectories. We do so by lifting both\nthe human hand and the manipulated object in a shared 3D space and retargeting\nhuman motions to robot actions. Generative modeling on this data gives us a\ntask-agnostic base policy. This policy captures a general yet flexible\nmanipulation prior. We empirically demonstrate that finetuning this policy,\nwith both reinforcement learning (RL) and behavior cloning (BC), enables\nsample-efficient adaptation to downstream tasks and simultaneously improves\nrobustness and generalizability compared to prior approaches. Qualitative\nexperiments are available at: \\url{https://hgaurav2k.github.io/hop/}.",
      "tldr_zh": "该论文提出了一种从视频中学习 3D 手-物体交互轨迹的方法，用于预训练机器人操作先验。通过将人类手部和物体提升到共享的 3D 空间，并将人类动作重定向为机器人动作，构建了一个任务无关的基策略，该策略通过生成建模捕捉通用且灵活的操纵先验。实验结果显示，通过强化学习 (RL) 或行为克隆 (BC) 微调，该策略能更高效适应下游任务，同时提高鲁棒性和泛化性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08273v1",
      "published_date": "2024-09-12 17:59:07 UTC",
      "updated_date": "2024-09-12 17:59:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:19:37.643979"
    },
    {
      "arxiv_id": "2409.08270v1",
      "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
      "title_zh": "翻译失败",
      "authors": [
        "Qiuhong Shen",
        "Xingyi Yang",
        "Xinchao Wang"
      ],
      "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian\nSplatting from 2D masks. Conventional methods often rely on iterative gradient\ndescent to assign each Gaussian a unique label, leading to lengthy optimization\nand sub-optimal solutions. Instead, we propose a straightforward yet globally\noptimal solver for 3D-GS segmentation. The core insight of our method is that,\nwith a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially\na linear function with respect to the labels of each Gaussian. As such, the\noptimal label assignment can be solved via linear programming in closed form.\nThis solution capitalizes on the alpha blending characteristic of the splatting\nprocess for single step optimization. By incorporating the background bias in\nour objective function, our method shows superior robustness in 3D segmentation\nagainst noises. Remarkably, our optimization completes within 30 seconds, about\n50$\\times$ faster than the best existing methods. Extensive experiments\ndemonstrate the efficiency and robustness of our method in segmenting various\nscenes, and its superior performance in downstream tasks such as object removal\nand inpainting. Demos and code will be available at\nhttps://github.com/florinshen/FlashSplat.",
      "tldr_zh": "这篇论文提出 FlashSplat，一种全球最优的求解器，用于从 2D 掩码精确分割 3D Gaussian Splatting，避免了传统迭代梯度下降方法的耗时和子优问题。核心创新在于认识到渲染 2D 掩码是关于每个 Gaussian 标签的线性函数，从而通过线性 programming 在闭合形式下实现单步优化，并通过加入背景偏差提升了对噪声的鲁棒性。该方法优化时间仅需 30 秒，比现有方法快 50 倍，并在各种场景的分割任务中表现出色，同时在下游应用如物体移除和修复中实现优越性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV'2024",
      "pdf_url": "http://arxiv.org/pdf/2409.08270v1",
      "published_date": "2024-09-12 17:58:13 UTC",
      "updated_date": "2024-09-12 17:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:19:50.564228"
    },
    {
      "arxiv_id": "2409.08264v2",
      "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Rogerio Bonatti",
        "Dan Zhao",
        "Francesco Bonacci",
        "Dillon Dupont",
        "Sara Abdali",
        "Yinheng Li",
        "Yadong Lu",
        "Justin Wagle",
        "Kazuhito Koishida",
        "Arthur Bucker",
        "Lawrence Jang",
        "Zack Hui"
      ],
      "abstract": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena",
      "tldr_zh": "该研究引入了Windows Agent Arena，一种可重现的通用基准，用于大规模评估多模态OS代理（Multi-Modal OS Agents）的性能，旨在解决现有基准在模态限制和评估效率方面的挑战。该框架基于OSWorld扩展，创建了150+种多样化Windows任务，涵盖规划、屏幕理解和工具使用，并支持在Azure上平行化以实现20分钟内完成完整评估。研究团队开发了新代理Navi，在Windows域中实现19.5%的成功率，而人类表现为74.5%；Navi在其他基准如Mind2Web上也表现出色。通过定量和定性分析，该工作为未来代理开发和数据生成提供了宝贵见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08264v2",
      "published_date": "2024-09-12 17:56:43 UTC",
      "updated_date": "2024-09-13 20:17:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:20:01.866154"
    },
    {
      "arxiv_id": "2409.08255v1",
      "title": "LoRID: Low-Rank Iterative Diffusion for Adversarial Purification",
      "title_zh": "翻译失败",
      "authors": [
        "Geigh Zollicoffer",
        "Minh Vu",
        "Ben Nebgen",
        "Juan Castorena",
        "Boian Alexandrov",
        "Manish Bhattarai"
      ],
      "abstract": "This work presents an information-theoretic examination of diffusion-based\npurification methods, the state-of-the-art adversarial defenses that utilize\ndiffusion models to remove malicious perturbations in adversarial examples. By\ntheoretically characterizing the inherent purification errors associated with\nthe Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank\nIterative Diffusion purification method designed to remove adversarial\nperturbation with low intrinsic purification errors. LoRID centers around a\nmulti-stage purification process that leverages multiple rounds of\ndiffusion-denoising loops at the early time-steps of the diffusion models, and\nthe integration of Tucker decomposition, an extension of matrix factorization,\nto remove adversarial noise at high-noise regimes. Consequently, LoRID\nincreases the effective diffusion time-steps and overcomes strong adversarial\nattacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ,\nand ImageNet datasets under both white-box and black-box settings.",
      "tldr_zh": "这篇论文从信息理论角度分析了基于扩散模型的对抗净化方法，揭示了这些方法固有的净化错误，并提出了LoRID，一种新型的Low-Rank Iterative Diffusion净化方法。LoRID采用多阶段净化过程，包括早期时间步的多轮扩散去噪循环，并整合Tucker decomposition来有效去除高噪声环境下的对抗扰动，从而增加有效扩散时间步并提升防御能力。实验结果显示，LoRID在CIFAR-10/100、CelebA-HQ和ImageNet数据集上，在white-box和black-box攻击场景下，表现出色，显著提高了对抗鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "LA-UR-24-28834",
      "pdf_url": "http://arxiv.org/pdf/2409.08255v1",
      "published_date": "2024-09-12 17:51:25 UTC",
      "updated_date": "2024-09-12 17:51:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:20:14.421259"
    },
    {
      "arxiv_id": "2409.08250v2",
      "title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahao Nick Li",
        "Zhuohao Jerry Zhang",
        "Jiaju Ma"
      ],
      "abstract": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nonly support retrieving individual pieces of information like certain objects\nin photos, and struggle with answering more complex queries that involve\ninterpreting interconnected memories like sequential events. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments individual captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories. Given a question, OmniQuery retrieves relevant augmented memories and\nuses a large language model (LLM) to generate answers with references. In human\nevaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%,\noutperforming a conventional RAG system by winning or tying for 74.5% of the\ntime.",
      "tldr_zh": "该论文提出 OmniQuery 系统，用于增强捕捉的多模态记忆（如照片和视频），以支持回答复杂个人问题，这些问题需要整合多个互联记忆的情境信息。研究者通过为期一个月的日记研究收集用户查询，并创建了情境信息分类法，OmniQuery 随后利用检索增强生成技术 (RAG) 和大型语言模型 (LLM) 检索相关增强记忆并生成带引用的答案。在人类评估中，OmniQuery 的准确率达到 71.5%，在 74.5% 的情况下优于传统 RAG 系统，展示了其在处理顺序事件和互联记忆查询方面的有效性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Paper accepted to the 2025 CHI Conference on Human Factors in\n  Computing Systems (CHI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2409.08250v2",
      "published_date": "2024-09-12 17:48:08 UTC",
      "updated_date": "2025-02-21 02:46:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:20:27.234927"
    },
    {
      "arxiv_id": "2409.08240v3",
      "title": "IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yinwei Wu",
        "Xianpan Zhou",
        "Bing Ma",
        "Xuefeng Su",
        "Kai Ma",
        "Xinchao Wang"
      ],
      "abstract": "While Text-to-Image (T2I) diffusion models excel at generating visually\nappealing images of individual instances, they struggle to accurately position\nand control the features generation of multiple instances. The Layout-to-Image\n(L2I) task was introduced to address the positioning challenges by\nincorporating bounding boxes as spatial control signals, but it still falls\nshort in generating precise instance features. In response, we propose the\nInstance Feature Generation (IFG) task, which aims to ensure both positional\naccuracy and feature fidelity in generated instances. To address the IFG task,\nwe introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances\nfeature depiction by incorporating additional appearance tokens and utilizing\nan Instance Semantic Map to align instance-level features with spatial\nlocations. The IFAdapter guides the diffusion process as a plug-and-play\nmodule, making it adaptable to various community models. For evaluation, we\ncontribute an IFG benchmark and develop a verification pipeline to objectively\ncompare models' abilities to generate instances with accurate positioning and\nfeatures. Experimental results demonstrate that IFAdapter outperforms other\nmodels in both quantitative and qualitative evaluations.",
      "tldr_zh": "该研究针对 Text-to-Image (T2I) 扩散模型在生成多个实例时存在的定位和特征控制问题，提出新的 Instance Feature Generation (IFG) 任务，以确保实例的 positional accuracy 和 feature fidelity。研究引入了 Instance Feature Adapter (IFAdapter)，该模块通过添加 appearance tokens 和利用 Instance Semantic Map 来对齐实例级特征与空间位置，并作为 plug-and-play 组件适配各种模型。实验结果显示，IFAdapter 在新建立的 IFG benchmark 上，通过定量和定性评估，显著优于其他基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08240v3",
      "published_date": "2024-09-12 17:39:23 UTC",
      "updated_date": "2024-11-06 13:03:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:20:38.481646"
    },
    {
      "arxiv_id": "2409.08239v1",
      "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
      "title_zh": "翻译失败",
      "authors": [
        "Alisia Lupidi",
        "Carlos Gemmell",
        "Nicola Cancedda",
        "Jane Dwivedi-Yu",
        "Jason Weston",
        "Jakob Foerster",
        "Roberta Raileanu",
        "Maria Lomeli"
      ],
      "abstract": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines.",
      "tldr_zh": "本文提出Source2Synth方法，用于生成基于真实数据源的合成数据，帮助Large Language Models (LLMs) 提升在结构化数据、复杂推理和工具使用方面的技能，而无需依赖昂贵的人工标注。  \n该方法从自定义数据源创建合成数据点，包括中间推理步骤，并通过评估生成的可回答性来过滤低质量样本，从而提高数据集质量。  \n实验结果显示，Source2Synth在tabular question answering (TQA) 任务上使WikiSQL性能提升25.51%，在multi-hop question answering (MHQA) 任务上使HotPotQA性能提升22.57%，显著优于基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08239v1",
      "published_date": "2024-09-12 17:39:08 UTC",
      "updated_date": "2024-09-12 17:39:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:20:51.606107"
    },
    {
      "arxiv_id": "2409.08234v2",
      "title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems",
      "title_zh": "LLM 蜜罐：利用大型语言模型作为先进的交互式",
      "authors": [
        "Hakan T. Otal",
        "M. Abdullah Canbaz"
      ],
      "abstract": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.",
      "tldr_zh": "该论文提出了一种创新方法，将大型语言模型（LLMs）用于构建高级交互式蜜罐系统，以检测和分析网络攻击行为。作者通过在攻击者生成命令和响应的多样化数据集上微调预训练的开源模型，并结合数据处理、提示工程、模型选择和监督微调，实现了蜜罐系统的真实互动和响应生成。实验评估显示，该系统在相似性指标和实际部署中表现出色，显著提升了恶意活动检测的准确性和效率，从而为网络安全基础设施提供强大工具。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.NI",
        "68T50, 68M10",
        "I.2.7; D.4.6; K.6.5"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.08234v2",
      "published_date": "2024-09-12 17:33:06 UTC",
      "updated_date": "2024-09-15 14:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:21:01.155917"
    },
    {
      "arxiv_id": "2409.08231v1",
      "title": "Design Optimization of Nuclear Fusion Reactor through Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jinsu Kim",
        "Jaemin Seo"
      ],
      "abstract": "This research explores the application of Deep Reinforcement Learning (DRL)\nto optimize the design of a nuclear fusion reactor. DRL can efficiently address\nthe challenging issues attributed to multiple physics and engineering\nconstraints for steady-state operation. The fusion reactor design computation\nand the optimization code applicable to parallelization with DRL are developed.\nThe proposed framework enables finding the optimal reactor design that\nsatisfies the operational requirements while reducing building costs.\nMulti-objective design optimization for a fusion reactor is now simplified by\nDRL, indicating the high potential of the proposed framework for advancing the\nefficient and sustainable design of future reactors.",
      "tldr_zh": "本研究利用 Deep Reinforcement Learning (DRL) 优化核聚变反应器的设计，高效处理多物理和工程约束问题。研究开发了可并行化的计算和优化代码，使框架能够找到满足操作要求并降低建造成本的最优设计方案。通过 DRL 简化了多目标设计优化过程，展示了该框架在推进未来反应器高效、可持续设计方面的巨大潜力。",
      "categories": [
        "physics.plasm-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.plasm-ph",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.08231v1",
      "published_date": "2024-09-12 17:23:01 UTC",
      "updated_date": "2024-09-12 17:23:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:21:12.377794"
    },
    {
      "arxiv_id": "2409.08229v1",
      "title": "Photonic Quantum Computers",
      "title_zh": "光子量子计算机",
      "authors": [
        "M. AbuGhanem"
      ],
      "abstract": "In the pursuit of scalable and fault-tolerant quantum computing\narchitectures, photonic-based quantum computers have emerged as a leading\nfrontier. This article provides a comprehensive overview of advancements in\nphotonic quantum computing, developed by leading industry players, examining\ncurrent performance, architectural designs, and strategies for developing\nlarge-scale, fault-tolerant photonic quantum computers. It also highlights\nrecent groundbreaking experiments that leverage the unique advantages of\nphotonic technologies, underscoring their transformative potential. This review\ncaptures a pivotal moment of photonic quantum computing in the noisy\nintermediate-scale quantum (NISQ) era, offering insights into how photonic\nquantum computers might reshape the future of quantum computing.",
      "tldr_zh": "本论文对Photonic Quantum Computers进行了全面概述，聚焦于其在可扩展和容错量子计算架构中的进展，包括当前性能、架构设计以及开发大规模容错系统的策略。该研究突出了利用光子技术独特优势的突破性实验，展示了其在Noisy Intermediate-Scale Quantum (NISQ)时代中的变革潜力。通过这些见解，论文探讨了Photonic Quantum Computers如何重塑量子计算的未来。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "quant-ph",
      "comment": "47 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.08229v1",
      "published_date": "2024-09-12 17:16:38 UTC",
      "updated_date": "2024-09-12 17:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:21:25.181883"
    },
    {
      "arxiv_id": "2409.13748v1",
      "title": "TheraGen: Therapy for Every Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Kartikey Doshi",
        "Jimit Shah",
        "Narendra Shekokar"
      ],
      "abstract": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support.",
      "tldr_zh": "我们介绍了 TheraGen，一种基于 LLaMA 2 7B 模型的 AI 心理健康聊天机器人，利用 100 万条对话数据集（包括匿名治疗记录、在线讨论和心理文献）提供全天候个性化、同理心支持。系统通过转移学习、微调和高级训练技术优化性能，确保用户友好界面和基于证据的应对策略。评估结果显示，用户满意度达 94%，心理健康改善显著，BLEU 分数 0.67 和 ROUGE 分数 0.62，平均响应时间 1395 毫秒。TheraGen 作为专业治疗的补充工具，提升了心理健康服务的可及性，并探讨了伦理考虑和未来发展方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13748v1",
      "published_date": "2024-09-12 17:15:44 UTC",
      "updated_date": "2024-09-12 17:15:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:21:38.481787"
    },
    {
      "arxiv_id": "2409.08217v2",
      "title": "CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs",
      "title_zh": "CliquePH：通过团图上的持久同调为图神经网络提供高阶信息",
      "authors": [
        "Davide Buffelli",
        "Farzin Soleymani",
        "Bastian Rieck"
      ],
      "abstract": "Graph neural networks have become the default choice by practitioners for\ngraph learning tasks such as graph classification and node classification.\nNevertheless, popular graph neural network models still struggle to capture\nhigher-order information, i.e., information that goes \\emph{beyond} pairwise\ninteractions. Recent work has shown that persistent homology, a tool from\ntopological data analysis, can enrich graph neural networks with topological\ninformation that they otherwise could not capture. Calculating such features is\nefficient for dimension 0 (connected components) and dimension 1 (cycles).\nHowever, when it comes to higher-order structures, it does not scale well, with\na complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order\nof the structures. In this work, we introduce a novel method that extracts\ninformation about higher-order structures in the graph while still using the\nefficient low-dimensional persistent homology algorithm. On standard benchmark\ndatasets, we show that our method can lead to up to $31\\%$ improvements in test\naccuracy.",
      "tldr_zh": "该研究指出，图神经网络（Graph Neural Networks）在图学习任务中难以捕捉高阶信息（beyond pairwise interactions），尽管它们在节点和图分类方面广泛应用。为解决这一问题，本文提出 CliquePH 方法，通过在团图（Clique Graphs）上应用持久同调（Persistent Homology）算法，高效提取高阶结构信息，同时避免了高维计算的复杂度。在标准基准数据集上，该方法将测试准确率提高了高达 31%，为增强图神经网络的拓扑数据分析能力提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Proceedings of the Third Learning on Graphs Conference\n  (LoG 2024), PMLR 269",
      "pdf_url": "http://arxiv.org/pdf/2409.08217v2",
      "published_date": "2024-09-12 16:56:26 UTC",
      "updated_date": "2024-11-26 18:01:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:21:49.938312"
    },
    {
      "arxiv_id": "2409.08215v2",
      "title": "LT3SD: Latent Trees for 3D Scene Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Quan Meng",
        "Lei Li",
        "Matthias Nießner",
        "Angela Dai"
      ],
      "abstract": "We present LT3SD, a novel latent diffusion model for large-scale 3D scene\ngeneration. Recent advances in diffusion models have shown impressive results\nin 3D object generation, but are limited in spatial extent and quality when\nextended to 3D scenes. To generate complex and diverse 3D scene structures, we\nintroduce a latent tree representation to effectively encode both\nlower-frequency geometry and higher-frequency detail in a coarse-to-fine\nhierarchy. We can then learn a generative diffusion process in this latent 3D\nscene space, modeling the latent components of a scene at each resolution\nlevel. To synthesize large-scale scenes with varying sizes, we train our\ndiffusion model on scene patches and synthesize arbitrary-sized output 3D\nscenes through shared diffusion generation across multiple scene patches.\nThrough extensive experiments, we demonstrate the efficacy and benefits of\nLT3SD for large-scale, high-quality unconditional 3D scene generation and for\nprobabilistic completion for partial scene observations.",
      "tldr_zh": "我们提出 LT3SD，一种新型的 latent diffusion model，用于生成大规模 3D 场景，以克服现有模型在扩展到 3D 场景时的空间范围和质量限制。\n该模型引入 latent tree representation 来编码粗到细的层次结构，包括低频几何和高频细节，并在潜在 3D 场景空间中学习生成扩散过程。\n通过在场景补丁上训练，LT3SD 能合成任意大小的输出场景，并在实验中证明其在高品质无条件 3D 场景生成和部分场景观察的概率完成方面的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://quan-meng.github.io/projects/lt3sd/ Video:\n  https://youtu.be/AJ5sG9VyjGA",
      "pdf_url": "http://arxiv.org/pdf/2409.08215v2",
      "published_date": "2024-09-12 16:55:51 UTC",
      "updated_date": "2025-05-01 15:23:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:22:02.207115"
    },
    {
      "arxiv_id": "2409.08202v2",
      "title": "What Makes a Maze Look Like a Maze?",
      "title_zh": "是什么让一个迷宫看起来像一个迷宫？",
      "authors": [
        "Joy Hsu",
        "Jiayuan Mao",
        "Joshua B. Tenenbaum",
        "Noah D. Goodman",
        "Jiajun Wu"
      ],
      "abstract": "A unique aspect of human visual understanding is the ability to flexibly\ninterpret abstract concepts: acquiring lifted rules explaining what they\nsymbolize, grounding them across familiar and unfamiliar contexts, and making\npredictions or reasoning about them. While off-the-shelf vision-language models\nexcel at making literal interpretations of images (e.g., recognizing object\ncategories such as tree branches), they still struggle to make sense of such\nvisual abstractions (e.g., how an arrangement of tree branches may form the\nwalls of a maze). To address this challenge, we introduce Deep Schema Grounding\n(DSG), a framework that leverages explicit structured representations of visual\nabstractions for grounding and reasoning. At the core of DSG are\nschemas--dependency graph descriptions of abstract concepts that decompose them\ninto more primitive-level symbols. DSG uses large language models to extract\nschemas, then hierarchically grounds concrete to abstract components of the\nschema onto images with vision-language models. The grounded schema is used to\naugment visual abstraction understanding. We systematically evaluate DSG and\ndifferent methods in reasoning on our new Visual Abstractions Dataset, which\nconsists of diverse, real-world images of abstract concepts and corresponding\nquestion-answer pairs labeled by humans. We show that DSG significantly\nimproves the abstract visual reasoning performance of vision-language models,\nand is a step toward human-aligned understanding of visual abstractions.",
      "tldr_zh": "本研究探讨了视觉语言模型（vision-language models）在理解视觉抽象概念（如迷宫的结构）上的局限性，与人类灵活解释能力的差距。作者提出 Deep Schema Grounding (DSG) 框架，该框架利用 schemas（依赖图描述）来分解抽象概念，并通过大型语言模型提取 schemas，然后用 vision-language models 分层地将这些组件映射到图像上，以增强抽象理解。实验在新的 Visual Abstractions Dataset 上进行，结果显示 DSG 显著提高了模型的抽象视觉推理性能，推动了更接近人类的视觉抽象理解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.08202v2",
      "published_date": "2024-09-12 16:41:47 UTC",
      "updated_date": "2025-02-17 23:45:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:22:13.306272"
    },
    {
      "arxiv_id": "2409.08199v2",
      "title": "AudioBERT: Audio Knowledge Augmented Language Model",
      "title_zh": "AudioBERT：音频知识增强语言模型",
      "authors": [
        "Hyunjong Ok",
        "Suho Yoo",
        "Jaeho Lee"
      ],
      "abstract": "Recent studies have identified that language models, pretrained on text-only\ndatasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of\neveryday objects. Motivated by this observation, we ask whether a similar\nshortcoming exists in terms of the \\textit{auditory} knowledge. To answer this\nquestion, we construct a new dataset called AuditoryBench, which consists of\ntwo novel tasks for evaluating auditory knowledge. Based on our analysis using\nthe benchmark, we find that language models also suffer from a severe lack of\nauditory knowledge. To address this limitation, we propose AudioBERT, a novel\nmethod to augment the auditory knowledge of BERT through a retrieval-based\napproach. First, we detect auditory knowledge spans in prompts to query our\nretrieval model efficiently. Then, we inject audio knowledge into BERT and\nswitch on low-rank adaptation for effective adaptation when audio knowledge is\nrequired. Our experiments demonstrate that AudioBERT is quite effective,\nachieving superior performance on the AuditoryBench. The dataset and code are\navailable at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.",
      "tldr_zh": "研究发现，基于文本数据集预训练的语言模型（如BERT）存在严重的听觉知识缺失问题，因此作者构建了AuditoryBench数据集，包括两个任务用于评估这一缺陷。针对这一问题，他们提出了AudioBERT方法，通过检索-based 方式检测提示中的听觉知识spans，并注入音频知识，同时结合low-rank adaptation进行有效适应。实验结果表明，AudioBERT在AuditoryBench上表现出色，显著提升了模型的听觉知识处理能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 3 figures, ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.08199v2",
      "published_date": "2024-09-12 16:36:39 UTC",
      "updated_date": "2025-01-16 12:17:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:22:25.716159"
    },
    {
      "arxiv_id": "2409.08185v1",
      "title": "Fine-tuning Large Language Models for Entity Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Aaron Steiner",
        "Ralph Peeters",
        "Christian Bizer"
      ],
      "abstract": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini.",
      "tldr_zh": "本研究探讨了微调大型语言模型 (LLMs) 用于实体匹配 (Entity Matching) 的潜力，作为一种超越提示工程和 in-context learning 的方法。研究从两个维度分析微调：一是训练示例的表示，包括添加不同类型的 LLM 生成解释；二是使用 LLMs 选择和生成训练示例。实验结果显示，微调显著提升了较小模型的匹配性能，并改善了模型对 in-domain 数据集的泛化能力，但会损害跨领域转移；此外，添加结构化解释对三个 LLM 有积极影响，而示例选择和生成方法仅提升了 Llama 3.1 8B 的性能，同时降低了 GPT-4o Mini 的表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch",
      "pdf_url": "http://arxiv.org/pdf/2409.08185v1",
      "published_date": "2024-09-12 16:20:57 UTC",
      "updated_date": "2024-09-12 16:20:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:22:37.502977"
    },
    {
      "arxiv_id": "2409.17166v1",
      "title": "ScriptSmith: A Unified LLM Framework for Enhancing IT Operations via Automated Bash Script Generation, Assessment, and Refinement",
      "title_zh": "ScriptSmith：一个统一的",
      "authors": [
        "Oishik Chatterjee",
        "Pooja Aggarwal",
        "Suranjana Samanta",
        "Ting Dai",
        "Prateeti Mohapatra",
        "Debanjana Kar",
        "Ruchi Mahindru",
        "Steve Barbieri",
        "Eugen Postea",
        "Brad Blancett",
        "Arthur De Magalhaes"
      ],
      "abstract": "In the rapidly evolving landscape of site reliability engineering (SRE), the\ndemand for efficient and effective solutions to manage and resolve issues in\nsite and cloud applications is paramount. This paper presents an innovative\napproach to action automation using large language models (LLMs) for script\ngeneration, assessment, and refinement. By leveraging the capabilities of LLMs,\nwe aim to significantly reduce the human effort involved in writing and\ndebugging scripts, thereby enhancing the productivity of SRE teams. Our\nexperiments focus on Bash scripts, a commonly used tool in SRE, and involve the\nCodeSift dataset of 100 tasks and the InterCode dataset of 153 tasks. The\nresults show that LLMs can automatically assess and refine scripts efficiently,\nreducing the need for script validation in an execution environment. Results\ndemonstrate that the framework shows an overall improvement of 7-10% in script\ngeneration.",
      "tldr_zh": "本论文提出ScriptSmith，一种统一的LLM框架，用于通过自动化生成、评估和优化Bash脚本来提升SRE（Site Reliability Engineering）操作效率。该框架利用LLMs减少手动编写和调试脚本的努力，并在CodeSift数据集（100个任务）和InterCode数据集（153个任务）上进行实验。结果显示，该框架在脚本生成方面提高了7-10%的性能，并有效降低了脚本在执行环境中的验证需求，从而提升了SRE团队的生产力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2409.17166v1",
      "published_date": "2024-09-12 15:11:43 UTC",
      "updated_date": "2024-09-12 15:11:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:22:49.169511"
    },
    {
      "arxiv_id": "2409.08111v1",
      "title": "Towards a graph-based foundation model for network traffic analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Louis Van Langendonck",
        "Ismael Castell-Uroz",
        "Pere Barlet-Ros"
      ],
      "abstract": "Foundation models have shown great promise in various fields of study. A\npotential application of such models is in computer network traffic analysis,\nwhere these models can grasp the complexities of network traffic dynamics and\nadapt to any specific task or network environment with minimal fine-tuning.\nPrevious approaches have used tokenized hex-level packet data and the model\narchitecture of large language transformer models. We propose a new, efficient\ngraph-based alternative at the flow-level. Our approach represents network\ntraffic as a dynamic spatio-temporal graph, employing a self-supervised link\nprediction pretraining task to capture the spatial and temporal dynamics in\nthis network graph framework. To evaluate the effectiveness of our approach, we\nconduct a few-shot learning experiment for three distinct downstream network\ntasks: intrusion detection, traffic classification, and botnet classification.\nModels finetuned from our pretrained base achieve an average performance\nincrease of 6.87\\% over training from scratch, demonstrating their ability to\neffectively learn general network traffic dynamics during pretraining. This\nsuccess suggests the potential for a large-scale version to serve as an\noperational foundational model.",
      "tldr_zh": "本文提出了一种基于图的（graph-based）foundation model，用于网络流量分析，以更好地捕捉网络流量动态并适应特定任务。方法将流量表示为动态的spatio-temporal graph，并采用self-supervised link prediction预训练任务来学习空间和时间动态。与传统方法不同，该框架在flow-level上操作，提高了效率。在few-shot learning实验中，该预训练模型微调后，在入侵检测、流量分类和botnet classification等下游任务上平均性能提升6.87%，证明了其学习网络流量动态的有效性，并为大规模操作foundation model的开发提供了潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "Pre-print of Accepted Workshop paper to 3rd GNNet, co-located with\n  CoNEXT'24",
      "pdf_url": "http://arxiv.org/pdf/2409.08111v1",
      "published_date": "2024-09-12 15:04:34 UTC",
      "updated_date": "2024-09-12 15:04:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:23:02.236933"
    },
    {
      "arxiv_id": "2409.08098v3",
      "title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal",
      "title_zh": "CLC-UKET 数据集：英国就业法庭案例结果预测的",
      "authors": [
        "Huiyuan Xie",
        "Felix Steffek",
        "Joana Ribeiro de Faria",
        "Christine Carter",
        "Jonathan Rutherford"
      ],
      "abstract": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.",
      "tldr_zh": "本研究开发了CLC-UKET数据集，作为英国就业法庭(UKET)案件结果预测的基准，旨在提升技术创新与司法可及性的交汇。该数据集包含约19,000个UKET案件及其元数据，通过大型语言模型(LLM)进行自动标注，涵盖事实、索赔、先例引用、法规引用、案件结果、原因和管辖权代码等全面注释。研究比较了人类预测与基线模型的表现，结果显示微调的Transformer模型在多类预测任务中优于零样本和少样本LLM，而通过整合任务相关信息可提升零样本LLM的性能。该数据集及其经验发现有望成为就业相关争议解决的重要基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08098v3",
      "published_date": "2024-09-12 14:51:43 UTC",
      "updated_date": "2024-10-26 06:00:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:23:14.804491"
    },
    {
      "arxiv_id": "2409.08069v1",
      "title": "TravelAgent: An AI Assistant for Personalized Travel Planning",
      "title_zh": "TravelAgent：用于个性化旅行规划的AI助手",
      "authors": [
        "Aili Chen",
        "Xuyang Ge",
        "Ziquan Fu",
        "Yanghua Xiao",
        "Jiangjie Chen"
      ],
      "abstract": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations.",
      "tldr_zh": "本文提出TravelAgent，一种基于LLMs（Large Language Models）的AI辅助系统，旨在解决智能旅行规划中的合理性、全面性和个性化挑战。该系统包括四个模块：Tool-usage、Recommendation、Planning和Memory Module，通过动态场景下的LLM驱动来生成实用且定制化的旅行行程。在人类和模拟用户评估中，TravelAgent在上述三个标准上表现出整体有效性，并验证了其个性化推荐的准确性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08069v1",
      "published_date": "2024-09-12 14:24:45 UTC",
      "updated_date": "2024-09-12 14:24:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:23:25.519096"
    },
    {
      "arxiv_id": "2409.08065v3",
      "title": "InvDesFlow: An AI-driven materials inverse design workflow to explore possible high-temperature superconductors",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao-Qi Han",
        "Zhenfeng Ouyang",
        "Peng-Jie Guo",
        "Hao Sun",
        "Ze-Feng Gao",
        "Zhong-Yi Lu"
      ],
      "abstract": "The discovery of new superconducting materials, particularly those exhibiting\nhigh critical temperature ($T_c$), has been a vibrant area of study within the\nfield of condensed matter physics. Conventional approaches primarily rely on\nphysical intuition to search for potential superconductors within the existing\ndatabases. However, the known materials only scratch the surface of the\nextensive array of possibilities within the realm of materials. Here, we\ndevelop InvDesFlow, an AI search engine that integrates deep model pre-training\nand fine-tuning techniques, diffusion models, and physics-based approaches\n(e.g., first-principles electronic structure calculation) for the discovery of\nhigh-$T_c$ superconductors. Utilizing InvDesFlow, we have obtained 74\ndynamically stable materials with critical temperatures predicted by the AI\nmodel to be $T_c \\geq$ 15 K based on a very small set of samples. Notably,\nthese materials are not contained in any existing dataset. Furthermore, we\nanalyze trends in our dataset and individual materials including B$_4$CN$_3$\n(at 5 GPa) and B$_5$CN$_2$ (at ambient pressure) whose $T_c$s are 24.08 K and\n15.93 K, respectively. We demonstrate that AI technique can discover a set of\nnew high-$T_c$ superconductors, outline its potential for accelerating\ndiscovery of the materials with targeted properties.",
      "tldr_zh": "本文开发了 InvDesFlow，一种 AI 驱动的材料反向设计工作流，整合深度模型预训练、扩散模型以及物理方法（如第一性原理电子结构计算），以探索高 T_c 超导体。利用该框架，从一个小型样本集发现了 74 种动态稳定的新材料，这些材料不在现有数据集，且 AI 模型预测其 T_c ≥ 15 K，包括 B$_4$CN$_3$ (在 5 GPa 下 T_c 为 24.08 K) 和 B$_5$CN$_2$ (在常压下 T_c 为 15.93 K)。这项研究展示了 AI 技术在加速发现具有目标特性的新超导体材料方面的巨大潜力。",
      "categories": [
        "cond-mat.supr-con",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.supr-con",
      "comment": "22 pages, 17 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.08065v3",
      "published_date": "2024-09-12 14:16:56 UTC",
      "updated_date": "2025-05-13 08:22:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:23:38.457758"
    },
    {
      "arxiv_id": "2409.18973v1",
      "title": "EEG-EMG FAConformer: Frequency Aware Conv-Transformer for the fusion of EEG and EMG",
      "title_zh": "翻译失败",
      "authors": [
        "ZhengXiao He",
        "Minghong Cai",
        "Letian Li",
        "Siyuan Tian",
        "Ren-Jie Dai"
      ],
      "abstract": "Motor pattern recognition paradigms are the main forms of Brain-Computer\nInterfaces(BCI) aimed at motor function rehabilitation and are the most easily\npromoted applications. In recent years, many researchers have suggested\nencouraging patients to perform real motor control execution simultaneously in\nMI-based BCI rehabilitation training systems. Electromyography (EMG) signals\nare the most direct physiological signals that can assess the execution of\nmovements. Multimodal signal fusion is practically significant for decoding\nmotor patterns. Therefore, we introduce a multimodal motion pattern recognition\nalgorithm for EEG and EMG signals: EEG-EMG FAConformer, a method with several\nattention modules correlated with temporal and frequency information for motor\npattern recognition. We especially devise a frequency band attention module to\nencode EEG information accurately and efficiently. What's more, modules like\nMulti-Scale Fusion Module, Independent Channel-Specific Convolution\nModule(ICSCM), and Fuse Module which can effectively eliminate irrelevant\ninformation in EEG and EMG signals and fully exploit hidden dynamics are\ndeveloped and show great effects. Extensive experiments show that EEG-EMG\nFAConformer surpasses existing methods on Jeong2020 dataset, showcasing\noutstanding performance, high robustness and impressive stability.",
      "tldr_zh": "这篇论文提出了一种名为 EEG-EMG FAConformer 的算法，用于融合 EEG 和 EMG 信号，以提升脑机接口(BCI)中运动模式识别的性能，特别是针对运动功能康复场景。算法设计了频率带注意力模块来准确编码 EEG 信息，并引入 Multi-Scale Fusion Module、Independent Channel-Specific Convolution Module (ICSCM) 和 Fuse Module，这些组件能有效消除无关信息并利用隐藏动态。实验结果显示，在 Jeong2020 数据集上，该方法超过了现有方法，在性能、鲁棒性和稳定性方面表现出色。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18973v1",
      "published_date": "2024-09-12 14:08:56 UTC",
      "updated_date": "2024-09-12 14:08:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:23:50.540384"
    },
    {
      "arxiv_id": "2409.08045v1",
      "title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking",
      "title_zh": "翻译失败",
      "authors": [
        "Stav Cohen",
        "Ron Bitton",
        "Ben Nassi"
      ],
      "abstract": "In this paper, we show that with the ability to jailbreak a GenAI model,\nattackers can escalate the outcome of attacks against RAG-based GenAI-powered\napplications in severity and scale. In the first part of the paper, we show\nthat attackers can escalate RAG membership inference attacks and RAG entity\nextraction attacks to RAG documents extraction attacks, forcing a more severe\noutcome compared to existing attacks. We evaluate the results obtained from\nthree extraction methods, the influence of the type and the size of five\nembeddings algorithms employed, the size of the provided context, and the GenAI\nengine. We show that attackers can extract 80%-99.8% of the data stored in the\ndatabase used by the RAG of a Q&A chatbot. In the second part of the paper, we\nshow that attackers can escalate the scale of RAG data poisoning attacks from\ncompromising a single GenAI-powered application to compromising the entire\nGenAI ecosystem, forcing a greater scale of damage. This is done by crafting an\nadversarial self-replicating prompt that triggers a chain reaction of a\ncomputer worm within the ecosystem and forces each affected application to\nperform a malicious activity and compromise the RAG of additional applications.\nWe evaluate the performance of the worm in creating a chain of confidential\ndata extraction about users within a GenAI ecosystem of GenAI-powered email\nassistants and analyze how the performance of the worm is affected by the size\nof the context, the adversarial self-replicating prompt used, the type and size\nof the embeddings algorithm employed, and the number of hops in the\npropagation. Finally, we review and analyze guardrails to protect RAG-based\ninference and discuss the tradeoffs.",
      "tldr_zh": "本文研究了通过 jailbreaking 技术升级针对 RAG-based GenAI 应用的攻击，旨在加剧攻击的严重性和规模。在第一部分，攻击者可将 RAG 成员推理攻击和实体提取攻击升级为 RAG 文档提取攻击，实验显示能提取80%-99.8%的数据库数据，并评估了嵌入算法类型、上下文大小和 GenAI 引擎的影响。在第二部分，攻击者使用 adversarial self-replicating prompt 触发蠕虫式连锁反应，将 RAG 数据中毒攻击从单一应用扩展到整个 GenAI 生态系统，导致更大范围的破坏。最后，论文讨论了保护 RAG-based 推理的防护措施及其权衡。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "for Github, see\n  https://github.com/StavC/UnleashingWorms-ExtractingData . arXiv admin note:\n  substantial text overlap with arXiv:2403.02817",
      "pdf_url": "http://arxiv.org/pdf/2409.08045v1",
      "published_date": "2024-09-12 13:50:22 UTC",
      "updated_date": "2024-09-12 13:50:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:24:02.373912"
    },
    {
      "arxiv_id": "2409.08023v2",
      "title": "Edge-Wise Graph-Instructed Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Francesco Della Santa",
        "Antonio Mastropietro",
        "Sandra Pieraccini",
        "Francesco Vaccarino"
      ],
      "abstract": "The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over some graph-structured input data, like the ones inferred from\nthe Barabasi-Albert graph, and improve the training regularization on graphs\nwith chaotic connectivity, like the ones inferred from the Erdos-Renyi graph.",
      "tldr_zh": "这篇论文分析了 Graph-Instructed Neural Network (GINN) 在图节点多任务回归问题中的局限性，并提出了一种新型的 edge-wise GI (EWGI) 层，以提升消息传递图神经网络的性能。EWGI 层通过边级处理优化了模型在复杂图结构上的表现，例如在 Barabasi-Albert 和 Erdos-Renyi 图中，提高了训练正则化和整体准确性。实验结果表明，EWGINNs 比 GINNs 在这些图数据上表现出色，为图结构数据处理提供了更有效的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "05C21, 65D15, 68T07, 90C35"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08023v2",
      "published_date": "2024-09-12 13:05:28 UTC",
      "updated_date": "2025-01-08 12:40:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:24:14.516702"
    },
    {
      "arxiv_id": "2409.08012v1",
      "title": "Learning Causally Invariant Reward Functions from Diverse Demonstrations",
      "title_zh": "从多样演示中学习",
      "authors": [
        "Ivan Ovinnikov",
        "Eugene Bykovets",
        "Joachim M. Buhmann"
      ],
      "abstract": "Inverse reinforcement learning methods aim to retrieve the reward function of\na Markov decision process based on a dataset of expert demonstrations. The\ncommonplace scarcity and heterogeneous sources of such demonstrations can lead\nto the absorption of spurious correlations in the data by the learned reward\nfunction. Consequently, this adaptation often exhibits behavioural overfitting\nto the expert data set when a policy is trained on the obtained reward function\nunder distribution shift of the environment dynamics. In this work, we explore\na novel regularization approach for inverse reinforcement learning methods\nbased on the causal invariance principle with the goal of improved reward\nfunction generalization. By applying this regularization to both exact and\napproximate formulations of the learning task, we demonstrate superior policy\nperformance when trained using the recovered reward functions in a transfer\nsetting",
      "tldr_zh": "本文研究了逆强化学习(Inverse Reinforcement Learning)方法，从多样化的专家演示中学习Markov决策过程的奖励函数，以避免吸收虚假相关性导致的行为过拟合。该方法引入基于因果不变性原则(causal invariance principle)的正则化技术，应用于精确和近似学习任务中，提高了奖励函数的泛化能力。实验结果显示，在环境动态分布偏移的转移设置下，使用该奖励函数训练的策略表现出色，显著优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08012v1",
      "published_date": "2024-09-12 12:56:24 UTC",
      "updated_date": "2024-09-12 12:56:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:24:25.350680"
    },
    {
      "arxiv_id": "2409.07989v2",
      "title": "Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms",
      "title_zh": "通过可学习的多尺度嵌入和注意力机制增强少样本图像分类",
      "authors": [
        "Fatemeh Askari",
        "Amirreza Fateh",
        "Mohammad Reza Mohammadi"
      ],
      "abstract": "In the context of few-shot classification, the goal is to train a classifier\nusing a limited number of samples while maintaining satisfactory performance.\nHowever, traditional metric-based methods exhibit certain limitations in\nachieving this objective. These methods typically rely on a single distance\nvalue between the query feature and support feature, thereby overlooking the\ncontribution of shallow features. To overcome this challenge, we propose a\nnovel approach in this paper. Our approach involves utilizing a multi-output\nembedding network that maps samples into distinct feature spaces. The proposed\nmethod extracts feature vectors at different stages, enabling the model to\ncapture both global and abstract features. By utilizing these diverse feature\nspaces, our model enhances its performance. Moreover, employing a\nself-attention mechanism improves the refinement of features at each stage,\nleading to even more robust representations and improved overall performance.\nFurthermore, assigning learnable weights to each stage significantly improved\nperformance and results. We conducted comprehensive evaluations on the\nMiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way\n5-shot scenarios. Additionally, we performed cross-domain tasks across eight\nbenchmark datasets, achieving high accuracy in the testing domains. These\nevaluations demonstrate the efficacy of our proposed method in comparison to\nstate-of-the-art approaches. https://github.com/FatemehAskari/MSENet",
      "tldr_zh": "本研究针对 few-shot classification 的挑战，提出了一种新型方法，通过可学习的多尺度嵌入(multi-scale embedding)网络来提取不同阶段的特征向量，从而捕捉全局和抽象特征。方法还整合了自注意力机制(self-attention mechanism)和为每个阶段分配的可学习权重，进一步提升特征表示的鲁棒性。在 MiniImageNet 和 FC100 数据集上的 5-way 1-shot 及 5-way 5-shot 场景中，以及跨八个基准数据集的跨域任务中，该方法实现了比现有最先进方法更高的准确率，证明了其有效性。GitHub 链接：https://github.com/FatemehAskari/MSENet。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07989v2",
      "published_date": "2024-09-12 12:34:29 UTC",
      "updated_date": "2025-01-16 14:01:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:24:37.628850"
    },
    {
      "arxiv_id": "2409.07985v1",
      "title": "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols",
      "title_zh": "翻译失败",
      "authors": [
        "Charlie Griffin",
        "Louis Thomson",
        "Buck Shlegeris",
        "Alessandro Abate"
      ],
      "abstract": "To evaluate the safety and usefulness of deployment protocols for untrusted\nAIs, AI Control uses a red-teaming exercise played between a protocol designer\nand an adversary. This paper introduces AI-Control Games, a formal\ndecision-making model of the red-teaming exercise as a multi-objective,\npartially observable, stochastic game. We also introduce methods for finding\noptimal protocols in AI-Control Games, by reducing them to a set of zero-sum\npartially observable stochastic games. We apply our formalism to model,\nevaluate and synthesise protocols for deploying untrusted language models as\nprogramming assistants, focusing on Trusted Monitoring protocols, which use\nweaker language models and limited human assistance. Finally, we demonstrate\nthe utility of our formalism by showcasing improvements over empirical studies\nin existing settings, evaluating protocols in new settings, and analysing how\nmodelling assumptions affect the safety and usefulness of protocols.",
      "tldr_zh": "这篇论文引入了 AI-Control Games，这是一个多目标、部分可观察的随机游戏模型，用于模拟红-teaming 练习，以评估不受信任 AI 部署协议的安全性和实用性。论文提出方法，通过将 AI-Control Games 简化为一组零和部分可观察随机游戏，来寻找最优协议，并将其应用于部署不受信任语言模型作为编程助手的场景，特别关注 Trusted Monitoring 协议（利用较弱语言模型和有限人类辅助）。实验结果展示了该形式化模型的实用性，包括在现有设置中的改进、评估新场景下的协议，以及分析建模假设对协议安全性和有用性的影响。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, with appendices",
      "pdf_url": "http://arxiv.org/pdf/2409.07985v1",
      "published_date": "2024-09-12 12:30:07 UTC",
      "updated_date": "2024-09-12 12:30:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:24:49.952451"
    },
    {
      "arxiv_id": "2409.07966v4",
      "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE",
      "title_zh": "ProbTalk3D：使用 VQ-VAE 的非确定性情感可控语音驱动 3D 面部动画合成",
      "authors": [
        "Sichun Wu",
        "Kazi Injamamul Haque",
        "Zerrin Yumak"
      ],
      "abstract": "Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).",
      "tldr_zh": "本研究提出ProbTalk3D，一种非确定性神经网络方法，用于情感可控的语音驱动3D面部动画合成，解决现有模型忽略情感控制和确定性输出的问题。方法采用两阶段VQ-VAE模型和情感丰富的3DMEAD数据集，实现对情感标签和强度级别的控制，从而生成多样化的动画。实验结果显示，ProbTalk3D在客观指标、定性和用户研究中优于现有情感控制和非确定性模型，代码已公开以供进一步验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM\n  SIGGRAPH MIG 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.07966v4",
      "published_date": "2024-09-12 11:53:05 UTC",
      "updated_date": "2025-02-16 14:23:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:25:02.127081"
    },
    {
      "arxiv_id": "2409.07965v1",
      "title": "Autonomous Vehicle Controllers From End-to-End Differentiable Simulation",
      "title_zh": "基于端到端可微分模拟的自主车辆控制器",
      "authors": [
        "Asen Nachkov",
        "Danda Pani Paudel",
        "Luc Van Gool"
      ],
      "abstract": "Current methods to learn controllers for autonomous vehicles (AVs) focus on\nbehavioural cloning. Being trained only on exact historic data, the resulting\nagents often generalize poorly to novel scenarios. Simulators provide the\nopportunity to go beyond offline datasets, but they are still treated as\ncomplicated black boxes, only used to update the global simulation state. As a\nresult, these RL algorithms are slow, sample-inefficient, and prior-agnostic.\nIn this work, we leverage a differentiable simulator and design an analytic\npolicy gradients (APG) approach to training AV controllers on the large-scale\nWaymo Open Motion Dataset. Our proposed framework brings the differentiable\nsimulator into an end-to-end training loop, where gradients of the environment\ndynamics serve as a useful prior to help the agent learn a more grounded\npolicy. We combine this setup with a recurrent architecture that can\nefficiently propagate temporal information across long simulated trajectories.\nThis APG method allows us to learn robust, accurate, and fast policies, while\nonly requiring widely-available expert trajectories, instead of scarce expert\nactions. We compare to behavioural cloning and find significant improvements in\nperformance and robustness to noise in the dynamics, as well as overall more\nintuitive human-like handling.",
      "tldr_zh": "该研究针对自动驾驶车辆（AVs）的控制器训练问题，指出传统行为克隆（behavioural cloning）方法依赖历史数据，导致在新场景中泛化能力差。作者提出一种基于端到端可微模拟器（differentiable simulator）的分析策略梯度（APG）框架，将模拟器整合到训练循环中，利用环境动态梯度作为先验，并结合循环架构（recurrent architecture）来高效处理长轨迹信息，从而在Waymo Open Motion Dataset上训练更鲁棒的政策。该方法仅需专家轨迹而非稀缺动作，结果显示APG在性能、准确性和对动态噪声的鲁棒性上显著优于行为克隆，且更接近人类直观的操控。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07965v1",
      "published_date": "2024-09-12 11:50:06 UTC",
      "updated_date": "2024-09-12 11:50:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:25:15.206788"
    },
    {
      "arxiv_id": "2409.07964v1",
      "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks",
      "title_zh": "WirelessAgent：大语言模型代理用于智能无线网络",
      "authors": [
        "Jingwen Tong",
        "Jiawei Shao",
        "Qiong Wu",
        "Wei Guo",
        "Zijian Li",
        "Zehong Lin",
        "Jun Zhang"
      ],
      "abstract": "Wireless networks are increasingly facing challenges due to their expanding\nscale and complexity. These challenges underscore the need for advanced\nAI-driven strategies, particularly in the upcoming 6G networks. In this\narticle, we introduce WirelessAgent, a novel approach leveraging large language\nmodels (LLMs) to develop AI agents capable of managing complex tasks in\nwireless networks. It can effectively improve network performance through\nadvanced reasoning, multimodal data processing, and autonomous decision making.\nThereafter, we demonstrate the practical applicability and benefits of\nWirelessAgent for network slicing management. The experimental results show\nthat WirelessAgent is capable of accurately understanding user intent,\neffectively allocating slice resources, and consistently maintaining optimal\nperformance.",
      "tldr_zh": "这篇论文针对无线网络日益扩大的规模和复杂性挑战，特别是即将到来的6G网络，提出WirelessAgent框架，该框架利用Large Language Models (LLMs)构建AI代理，以通过高级推理、多模态数据处理和自主决策来提升网络性能。主要贡献包括展示WirelessAgent在网络切片管理中的实际应用，能够准确理解用户意图、有效分配切片资源，并保持最佳性能。实验结果证明了该框架的实用性和优势，为智能无线网络管理提供了创新解决方案。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07964v1",
      "published_date": "2024-09-12 11:48:01 UTC",
      "updated_date": "2024-09-12 11:48:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:25:25.741765"
    },
    {
      "arxiv_id": "2409.07957v1",
      "title": "Rapid Parameter Estimation for Extreme Mass Ratio Inspirals Using Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Liang",
        "Hong Guo",
        "Tianyu Zhao",
        "He wang",
        "Herik Evangelinelis",
        "Yuxiang Xu",
        "Chang liu",
        "Manjia Liang",
        "Xiaotong Wei",
        "Yong Yuan",
        "Peng Xu",
        "Minghui Du",
        "Wei-Liang Qian",
        "Ziren Luo"
      ],
      "abstract": "Extreme-mass-ratio inspiral (EMRI) signals pose significant challenges in\ngravitational wave (GW) astronomy owing to their low-frequency nature and\nhighly complex waveforms, which occupy a high-dimensional parameter space with\nnumerous variables. Given their extended inspiral timescales and low\nsignal-to-noise ratios, EMRI signals warrant prolonged observation periods.\nParameter estimation becomes particularly challenging due to non-local\nparameter degeneracies, arising from multiple local maxima, as well as flat\nregions and ridges inherent in the likelihood function. These factors lead to\nexceptionally high time complexity for parameter analysis while employing\ntraditional matched filtering and random sampling methods. To address these\nchallenges, the present study applies machine learning to Bayesian posterior\nestimation of EMRI signals, leveraging the recently developed flow matching\ntechnique based on ODE neural networks. Our approach demonstrates computational\nefficiency several orders of magnitude faster than the traditional Markov Chain\nMonte Carlo (MCMC) methods, while preserving the unbiasedness of parameter\nestimation. We show that machine learning technology has the potential to\nefficiently handle the vast parameter space, involving up to seventeen\nparameters, associated with EMRI signals. Furthermore, to our knowledge, this\nis the first instance of applying machine learning, specifically the Continuous\nNormalizing Flows (CNFs), to EMRI signal analysis. Our findings highlight the\npromising potential of machine learning in EMRI waveform analysis, offering new\nperspectives for the advancement of space-based GW detection and GW astronomy.",
      "tldr_zh": "该研究针对极端质量比吸积（EMRI）信号在引力波（GW）天文学中的挑战，如高维参数空间、非局部参数退化及低信噪比，提出了一种基于机器学习的快速参数估计方法。方法利用基于ODE神经网络的flow matching技术进行Bayesian后验估计，比传统Markov Chain Monte Carlo (MCMC)方法快几个数量级，同时保持参数估计的无偏性。首次将Continuous Normalizing Flows (CNFs)应用于EMRI信号分析，该方法能高效处理多达17个参数的空间，并为空间基GW检测和天文学的发展提供新视角。",
      "categories": [
        "physics.comp-ph",
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07957v1",
      "published_date": "2024-09-12 11:36:23 UTC",
      "updated_date": "2024-09-12 11:36:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:25:37.610948"
    },
    {
      "arxiv_id": "2409.07932v2",
      "title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies",
      "title_zh": "强化学习发现高效去中心化图路径搜索策略",
      "authors": [
        "Alexei Pisacane",
        "Victor-Alexandru Darvariu",
        "Mirco Musolesi"
      ],
      "abstract": "Graph path search is a classic computer science problem that has been\nrecently approached with Reinforcement Learning (RL) due to its potential to\noutperform prior methods. Existing RL techniques typically assume a global view\nof the network, which is not suitable for large-scale, dynamic, and\nprivacy-sensitive settings. An area of particular interest is search in social\nnetworks due to its numerous applications. Inspired by seminal work in\nexperimental sociology, which showed that decentralized yet efficient search is\npossible in social networks, we frame the problem as a collaborative task\nbetween multiple agents equipped with a limited local view of the network. We\npropose a multi-agent approach for graph path search that successfully\nleverages both homophily and structural heterogeneity. Our experiments, carried\nout over synthetic and real-world social networks, demonstrate that our model\nsignificantly outperforms learned and heuristic baselines. Furthermore, our\nresults show that meaningful embeddings for graph navigation can be constructed\nusing reward-driven learning.",
      "tldr_zh": "该论文探讨了图路径搜索问题，通过强化学习（Reinforcement Learning, RL）开发高效的去中心化策略，以适应大规模、动态和隐私敏感的环境。研究将问题框架为多代理协作任务，每个代理仅拥有局部网络视图，并利用同质性（homophily）和结构异质性（structural heterogeneity）来优化搜索过程。实验结果显示，该方法在合成和真实社交网络上显著优于学习和启发式基线，并证明了奖励驱动学习可以构建有效的图导航嵌入（embeddings）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07932v2",
      "published_date": "2024-09-12 10:56:38 UTC",
      "updated_date": "2024-11-26 17:46:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:25:49.503007"
    },
    {
      "arxiv_id": "2409.07930v1",
      "title": "A convolutional neural network approach to deblending seismic data",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Sun",
        "Sigmund Slang",
        "Thomas Elboth",
        "Thomas Larsen Greiner",
        "Steven McDonald",
        "Leiv-J Gelius"
      ],
      "abstract": "For economic and efficiency reasons, blended acquisition of seismic data is\nbecoming more and more commonplace. Seismic deblending methods are always\ncomputationally demanding and normally consist of multiple processing steps.\nBesides, the parameter setting is not always trivial. Machine learning-based\nprocessing has the potential to significantly reduce processing time and to\nchange the way seismic deblending is carried out. We present a data-driven deep\nlearning-based method for fast and efficient seismic deblending. The blended\ndata are sorted from the common source to the common channel domain to\ntransform the character of the blending noise from coherent events to\nincoherent distributions. A convolutional neural network (CNN) is designed\naccording to the special character of seismic data, and performs deblending\nwith comparable results to those obtained with conventional industry deblending\nalgorithms. To ensure authenticity, the blending was done numerically and only\nfield seismic data were employed, including more than 20000 training examples.\nAfter training and validation of the network, seismic deblending can be\nperformed in near real time. Experiments also show that the initial signal to\nnoise ratio (SNR) is the major factor controlling the quality of the final\ndeblended result. The network is also demonstrated to be robust and adaptive by\nusing the trained model to firstly deblend a new data set from a different\ngeological area with a slightly different delay time setting, and secondly\ndeblend shots with blending noise in the top part of the data.",
      "tldr_zh": "本研究提出了一种基于卷积神经网络 (CNN) 的数据驱动方法，用于地震数据的去混合处理，以应对传统方法计算密集和参数设置复杂的挑战。方法首先将混合数据从公共源排序到公共通道域，将混合噪声从相干事件转变为非相干分布，然后利用专为地震数据设计的 CNN 进行高效去混合，效果与传统算法相当。实验使用超过 20000 个真实场地震数据样本进行训练，实现了近实时处理，并证明初始信噪比 (SNR) 是影响结果质量的主要因素；此外，该网络显示出鲁棒性和适应性，能成功应用于不同地质区域和新数据设置。",
      "categories": [
        "physics.geo-ph",
        "cs.AI"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07930v1",
      "published_date": "2024-09-12 10:54:35 UTC",
      "updated_date": "2024-09-12 10:54:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:26:01.818487"
    },
    {
      "arxiv_id": "2409.07925v1",
      "title": "A framework for measuring the training efficiency of a neural architecture",
      "title_zh": "一种测量神经网络架构训练效率的框架",
      "authors": [
        "Eduardo Cueto-Mendoza",
        "John D. Kelleher"
      ],
      "abstract": "Measuring Efficiency in neural network system development is an open research\nproblem. This paper presents an experimental framework to measure the training\nefficiency of a neural architecture. To demonstrate our approach, we analyze\nthe training efficiency of Convolutional Neural Networks and Bayesian\nequivalents on the MNIST and CIFAR-10 tasks. Our results show that training\nefficiency decays as training progresses and varies across different stopping\ncriteria for a given neural model and learning task. We also find a non-linear\nrelationship between training stopping criteria, training Efficiency, model\nsize, and training Efficiency.\n  Furthermore, we illustrate the potential confounding effects of overtraining\non measuring the training efficiency of a neural architecture. Regarding\nrelative training efficiency across different architectures, our results\nindicate that CNNs are more efficient than BCNNs on both datasets. More\ngenerally, as a learning task becomes more complex, the relative difference in\ntraining efficiency between different architectures becomes more pronounced.",
      "tldr_zh": "本研究提出一个实验框架，用于衡量神经架构的训练效率（training efficiency）。该框架通过分析卷积神经网络（CNNs）和其贝叶斯等价物（BCNNs）在MNIST和CIFAR-10任务上的表现，揭示了训练效率会随着训练进展而衰减，并与停止标准、模型大小存在非线性关系。结果显示，CNNs在两个数据集上比BCNNs更高效，且任务复杂度增加时，不同架构间的效率差异更加明显；此外，过训练（overtraining）可能干扰效率测量，为神经网络开发提供重要见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07925v1",
      "published_date": "2024-09-12 10:45:38 UTC",
      "updated_date": "2024-09-12 10:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:26:13.164959"
    },
    {
      "arxiv_id": "2409.07918v1",
      "title": "Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Elizabeth Wilson",
        "György Fazekas",
        "Geraint Wiggins"
      ],
      "abstract": "This paper presents Tidal-MerzA, a novel system designed for collaborative\nperformances between humans and a machine agent in the context of live coding,\nspecifically focusing on the generation of musical patterns. Tidal-MerzA fuses\ntwo foundational models: ALCAA (Affective Live Coding Autonomous Agent) and\nTidal Fuzz, a computational framework. By integrating affective modelling with\ncomputational generation, this system leverages reinforcement learning\ntechniques to dynamically adapt music composition parameters within the\nTidalCycles framework, ensuring both affective qualities to the patterns and\nsyntactical correctness. The development of Tidal-MerzA introduces two distinct\nagents: one focusing on the generation of mini-notation strings for musical\nexpression, and another on the alignment of music with targeted affective\nstates through reinforcement learning. This approach enhances the adaptability\nand creative potential of live coding practices and allows exploration of\nhuman-machine creative interactions. Tidal-MerzA advances the field of\ncomputational music generation, presenting a novel methodology for\nincorporating artificial intelligence into artistic practices.",
      "tldr_zh": "本论文介绍了 Tidal MerzA 系统，这是一种结合 affective modelling 和 autonomous code generation 的框架，通过 Reinforcement Learning 实现人类与机器代理在实时编码中的协作表演，专注于生成音乐模式。系统融合 ALCAA 和 Tidal Fuzz 模型，开发了两个代理：一个负责生成 mini-notation strings 以表达音乐，另一个通过 Reinforcement Learning 动态调整音乐参数，确保与目标情感状态对齐。Tidal MerzA 提升了实时编码的适应性和创意潜力，并为计算音乐生成领域提供了新方法，推动了人工智能在艺术实践中的应用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07918v1",
      "published_date": "2024-09-12 10:38:55 UTC",
      "updated_date": "2024-09-12 10:38:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:26:26.065029"
    },
    {
      "arxiv_id": "2409.07914v3",
      "title": "InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Lee",
        "Ian Chuang",
        "Ling-Yuan Chen",
        "Iman Soltani"
      ],
      "abstract": "Bimanual manipulation presents unique challenges compared to unimanual tasks\ndue to the complexity of coordinating two robotic arms. In this paper, we\nintroduce InterACT: Inter-dependency aware Action Chunking with Hierarchical\nAttention Transformers, a novel imitation learning framework designed\nspecifically for bimanual manipulation. InterACT leverages hierarchical\nattention mechanisms to effectively capture inter-dependencies between dual-arm\njoint states and visual inputs. The framework comprises a Hierarchical\nAttention Encoder, which processes multi-modal inputs through segment-wise and\ncross-segment attention mechanisms, and a Multi-arm Decoder that generates each\narm's action predictions in parallel, while sharing information between the\narms through synchronization blocks by providing the other arm's intermediate\noutput as context. Our experiments, conducted on various simulated and\nreal-world bimanual manipulation tasks, demonstrate that InterACT outperforms\nexisting methods. Detailed ablation studies further validate the significance\nof key components, including the impact of CLS tokens, cross-segment encoders,\nand synchronization blocks on task performance. We provide supplementary\nmaterials and videos on our project page.",
      "tldr_zh": "本研究提出InterACT框架，这是一种针对双臂机械臂操作（Bimanual Manipulation）的模仿学习方法，能够有效捕捉双臂联合状态和视觉输入之间的相互依赖。InterACT包括Hierarchical Attention Encoder，通过段内和段间注意力机制处理多模态输入，以及Multi-arm Decoder，该解码器并行生成每个臂的动作预测，并通过同步块共享信息以协调双臂动作。实验结果显示，InterACT在各种模拟和真实世界任务中优于现有方法，消融研究进一步证实了CLS tokens、跨段编码器和同步块等关键组件对性能的显著影响。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at Conference on Robot Learning (CoRL) 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.07914v3",
      "published_date": "2024-09-12 10:30:44 UTC",
      "updated_date": "2024-10-16 08:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:26:37.889485"
    },
    {
      "arxiv_id": "2409.07913v1",
      "title": "UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints",
      "title_zh": "翻译失败",
      "authors": [
        "Inzamamul Alam",
        "Muhammad Shahid Muneer",
        "Simon S. Woo"
      ],
      "abstract": "In the wake of a fabricated explosion image at the Pentagon, an ability to\ndiscern real images from fake counterparts has never been more critical. Our\nstudy introduces a novel multi-modal approach to detect AI-generated images\namidst the proliferation of new generation methods such as Diffusion models.\nOur method, UGAD, encompasses three key detection steps: First, we transform\nthe RGB images into YCbCr channels and apply an Integral Radial Operation to\nemphasize salient radial features. Secondly, the Spatial Fourier Extraction\noperation is used for a spatial shift, utilizing a pre-trained deep learning\nnetwork for optimal feature extraction. Finally, the deep neural network\nclassification stage processes the data through dense layers using softmax for\nclassification. Our approach significantly enhances the accuracy of\ndifferentiating between real and AI-generated images, as evidenced by a 12.64%\nincrease in accuracy and 28.43% increase in AUC compared to existing\nstate-of-the-art methods.",
      "tldr_zh": "本研究提出了一种通用 AI 生成图像检测器 UGAD，利用频率指纹技术应对真实与假图像的区分挑战。UGAD 的方法包括三个关键步骤：首先，将 RGB 图像转换为 YCbCr 通道并应用 Integral Radial Operation 以突出径向特征；其次，使用 Spatial Fourier Extraction 操作结合预训练深度学习网络进行特征提取；最后，通过深度神经网络的密集层和 softmax 分类实现精确检测。实验结果显示，UGAD 相较现有最先进方法，准确率提升了 12.64%，AUC 提升了 28.43%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07913v1",
      "published_date": "2024-09-12 10:29:37 UTC",
      "updated_date": "2024-09-12 10:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:26:49.924484"
    },
    {
      "arxiv_id": "2410.05273v3",
      "title": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Jianke Zhang",
        "Yanjiang Guo",
        "Xiaoyu Chen",
        "Yen-Jen Wang",
        "Yucheng Hu",
        "Chengming Shi",
        "Jianyu Chen"
      ],
      "abstract": "Large Vision-Language-Action (VLA) models, leveraging powerful pre trained\nVision-Language Models (VLMs) backends, have shown promise in robotic control\ndue to their impressive generalization ability. However, the success comes at a\ncost. Their reliance on VLM backends with billions of parameters leads to high\ncomputational costs and inference latency, limiting the testing scenarios to\nmainly quasi-static tasks and hindering performance in dynamic tasks requiring\nrapid interactions. To address these limitations, this paper proposes HiRT, a\nHierarchical Robot Transformer framework that enables flexible frequency and\nperformance trade-off. HiRT keeps VLMs running at low frequencies to capture\ntemporarily invariant features while enabling real-time interaction through a\nhigh-frequency vision-based policy guided by the slowly updated features.\nExperiment results in both simulation and real-world settings demonstrate\nsignificant improvements over baseline methods. Empirically, in static tasks,\nwe double the control frequency and achieve comparable success rates.\nAdditionally, on novel real-world dynamic ma nipulation tasks which are\nchallenging for previous VLA models, HiRT improves the success rate from 48% to\n75%.",
      "tldr_zh": "该论文提出 HiRT（Hierarchical Robot Transformer）框架，以解决大型 Vision-Language-Action (VLA) 模型在机器人控制中因依赖高参数 VLM 后端而导致的计算成本高和推理延迟问题。HiRT 通过让 VLMs 以低频率运行捕捉不变特征，同时结合高频率的视觉-based 政策实现实时交互，从而实现频率和性能的灵活权衡。实验结果显示，在模拟和真实世界环境中，HiRT 在静态任务中将控制频率提高一倍并保持可比成功率，在动态操作任务中将成功率从 48% 提升至 75%，显著优于基线方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CORL 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.05273v3",
      "published_date": "2024-09-12 09:18:09 UTC",
      "updated_date": "2025-02-03 04:07:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:27:00.805726"
    },
    {
      "arxiv_id": "2409.07850v1",
      "title": "Enhancing Cross-Market Recommendation System with Graph Isomorphism Networks: A Novel Approach to Personalized User Experience",
      "title_zh": "翻译失败",
      "authors": [
        "Sümeyye Öztürk",
        "Ahmed Burak Ercan",
        "Resul Tugay",
        "Şule Gündüz Öğüdücü"
      ],
      "abstract": "In today's world of globalized commerce, cross-market recommendation systems\n(CMRs) are crucial for providing personalized user experiences across diverse\nmarket segments. However, traditional recommendation algorithms have\ndifficulties dealing with market specificity and data sparsity, especially in\nnew or emerging markets. In this paper, we propose the CrossGR model, which\nutilizes Graph Isomorphism Networks (GINs) to improve CMR systems. It\noutperforms existing benchmarks in NDCG@10 and HR@10 metrics, demonstrating its\nadaptability and accuracy in handling diverse market segments. The CrossGR\nmodel is adaptable and accurate, making it well-suited for handling the\ncomplexities of cross-market recommendation tasks. Its robustness is\ndemonstrated by consistent performance across different evaluation timeframes,\nindicating its potential to cater to evolving market trends and user\npreferences. Our findings suggest that GINs represent a promising direction for\nCMRs, paving the way for more sophisticated, personalized, and context-aware\nrecommendation systems in the dynamic landscape of global e-commerce.",
      "tldr_zh": "本研究针对跨市场推荐系统（CMRs）面临的挑战，如市场特异性和数据稀疏性，提出了一种名为 CrossGR 的新型模型，利用 Graph Isomorphism Networks (GINs) 来提升个性化用户体验。CrossGR 通过构建图结构来处理不同市场段的数据，显著提高了推荐的适应性和准确性，在 NDCG@10 和 HR@10 指标上超越现有基准。实验结果显示，该模型在各种时间框架中表现出稳健性能，为全球电子商务中的动态个性化推荐系统提供了有前景的方向。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "7 pages, 1 figure, 3 tables, 5 equations",
      "pdf_url": "http://arxiv.org/pdf/2409.07850v1",
      "published_date": "2024-09-12 08:53:11 UTC",
      "updated_date": "2024-09-12 08:53:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:27:13.527078"
    },
    {
      "arxiv_id": "2409.07825v3",
      "title": "Deep Multimodal Learning with Missing Modality: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Renjie Wu",
        "Hu Wang",
        "Hsiang-Ting Chen",
        "Gustavo Carneiro"
      ],
      "abstract": "During multimodal model training and testing, certain data modalities may be\nabsent due to sensor limitations, cost constraints, privacy concerns, or data\nloss, negatively affecting performance. Multimodal learning techniques designed\nto handle missing modalities can mitigate this by ensuring model robustness\neven when some modalities are unavailable. This survey reviews recent progress\nin Multimodal Learning with Missing Modality (MLMM), focusing on deep learning\nmethods. It provides the first comprehensive survey that covers the motivation\nand distinctions between MLMM and standard multimodal learning setups, followed\nby a detailed analysis of current methods, applications, and datasets,\nconcluding with challenges and future directions.",
      "tldr_zh": "本调查论文探讨了多模态学习中缺失模态（Missing Modality）的问题，原因包括传感器限制、成本约束、隐私担忧或数据丢失，这些会影响模型性能。论文首次提供全面回顾，涵盖MLMM的动机及其与标准多模态学习的区别，详细分析了当前深度学习方法、应用场景、数据集以及面临的挑战。最终，论文指出了未来研究方向，以提升模型的鲁棒性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ACM Computing Surveys",
      "pdf_url": "http://arxiv.org/pdf/2409.07825v3",
      "published_date": "2024-09-12 08:15:39 UTC",
      "updated_date": "2024-10-21 09:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:27:25.512241"
    },
    {
      "arxiv_id": "2409.07822v1",
      "title": "Over-the-Air Federated Learning via Weighted Aggregation",
      "title_zh": "基于加",
      "authors": [
        "Seyed Mohammad Azimi-Abarghouyi",
        "Leandros Tassiulas"
      ],
      "abstract": "This paper introduces a new federated learning scheme that leverages\nover-the-air computation. A novel feature of this scheme is the proposal to\nemploy adaptive weights during aggregation, a facet treated as predefined in\nother over-the-air schemes. This can mitigate the impact of wireless channel\nconditions on learning performance, without needing channel state information\nat transmitter side (CSIT). We provide a mathematical methodology to derive the\nconvergence bound for the proposed scheme in the context of computational\nheterogeneity and general loss functions, supplemented with design insights.\nAccordingly, we propose aggregation cost metrics and efficient algorithms to\nfind optimized weights for the aggregation. Finally, through numerical\nexperiments, we validate the effectiveness of the proposed scheme. Even with\nthe challenges posed by channel conditions and device heterogeneity, the\nproposed scheme surpasses other over-the-air strategies by an accuracy\nimprovement of 15% over the scheme using CSIT and 30% compared to the one\nwithout CSIT.",
      "tldr_zh": "这篇论文提出了一种新的联邦学习方案，利用 over-the-air computation 并引入自适应权重进行聚合，以减轻无线通道条件对学习性能的影响，而无需在发射端使用通道状态信息 (CSIT)。该方案通过数学方法推导收敛边界，考虑计算异质性和一般损失函数，并设计了聚合成本指标和高效算法来优化权重。实验结果显示，该方案在准确率上比使用 CSIT 的方法提高了 15%，并比不使用 CSIT 的方法提高了 30%，证明了其在设备异质性环境中的有效性。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07822v1",
      "published_date": "2024-09-12 08:07:11 UTC",
      "updated_date": "2024-09-12 08:07:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:27:38.005977"
    },
    {
      "arxiv_id": "2409.07796v2",
      "title": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Mehdi Rastikerdar",
        "Jin Huang",
        "Hui Guan",
        "Deepak Ganesan"
      ],
      "abstract": "Resource-constrained IoT devices increasingly rely on deep learning models\nfor inference tasks in remote environments. However, these models experience\nsignificant accuracy drops due to domain shifts when encountering variations in\nlighting, weather, and seasonal conditions. While cloud-based retraining can\naddress this issue, many IoT deployments operate with limited connectivity and\nenergy constraints, making traditional fine-tuning approaches impractical. We\nexplore this challenge through the lens of wildlife ecology, where camera traps\nmust maintain accurate species classification across changing seasons, weather,\nand habitats without reliable connectivity. We introduce WildFit, an autonomous\nin-situ adaptation framework that leverages the key insight that background\nscenes change more frequently than the visual characteristics of monitored\nspecies. WildFit combines background-aware synthesis to generate training\nsamples on-device with drift-aware fine-tuning that triggers model updates only\nwhen necessary to conserve resources. Through extensive evaluation on multiple\ncamera trap deployments, we demonstrate that WildFit significantly improves\naccuracy while greatly reducing adaptation overhead compared to traditional\napproaches.",
      "tldr_zh": "该论文探讨了资源受限的 IoT 设备在远程环境中使用深度学习模型进行推理时，由于领域偏移（如照明、天气和季节变化）导致准确性显著下降的问题，尤其在野生动物生态的相机陷阱应用中。作者提出了 WildFit 框架，该框架利用背景场景变化更频繁的洞见，通过 background-aware synthesis 在设备上生成训练样本，并采用 drift-aware fine-tuning 仅在必要时触发模型更新，以实现高效的 in-situ 适应。实验结果显示，WildFit 在多个相机陷阱部署中显著提高了物种分类准确性，同时大大降低了适应开销。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07796v2",
      "published_date": "2024-09-12 06:56:52 UTC",
      "updated_date": "2025-01-24 05:24:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:27:50.171122"
    },
    {
      "arxiv_id": "2409.07793v1",
      "title": "Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation",
      "title_zh": "拉格朗日对偶与复合多注意力 Transformer 用于半监督医疗图像分割",
      "authors": [
        "Fuchen Zheng",
        "Quanjun Li",
        "Weixuan Li",
        "Xuhang Chen",
        "Yihang Dong",
        "Guoheng Huang",
        "Chi-Man Pun",
        "Shoujun Zhou"
      ],
      "abstract": "Medical image segmentation, a critical application of semantic segmentation\nin healthcare, has seen significant advancements through specialized computer\nvision techniques. While deep learning-based medical image segmentation is\nessential for assisting in medical diagnosis, the lack of diverse training data\ncauses the long-tail problem. Moreover, most previous hybrid CNN-ViT\narchitectures have limited ability to combine various attentions in different\nlayers of the Convolutional Neural Network. To address these issues, we propose\na Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware\nContrastive Loss, as the overall training objective for semi-supervised\nlearning to mitigate the long-tail problem. Additionally, we introduce\nCMAformer, a novel network that synergizes the strengths of ResUNet and\nTransformer. The cross-attention block in CMAformer effectively integrates\nspatial attention and channel attention for multi-scale feature fusion.\nOverall, our results indicate that CMAformer, combined with the feature fusion\nframework and the new consistency loss, demonstrates strong complementarity in\nsemi-supervised learning ensembles. We achieve state-of-the-art results on\nmultiple public medical image datasets. Example code are available at:\n\\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.",
      "tldr_zh": "本文提出了一种针对医疗图像分割的长尾问题（long-tail problem）的解决方案，引入了 Lagrange Duality Consistency (LDC) Loss 与 Boundary-Aware Contrastive Loss 作为半监督学习（semi-supervised learning）的整体训练目标，以提升模型的鲁棒性。作者还设计了 CMAformer 网络，该网络融合了 ResUNet 和 Transformer 的优势，通过 cross-attention block 实现空间注意力和通道注意力的多尺度特征融合。实验结果表明，该方法在多个公共医疗图像数据集上达到了最先进（state-of-the-art）性能，展示了其在半监督场景下的强大互补性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 4 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.07793v1",
      "published_date": "2024-09-12 06:52:46 UTC",
      "updated_date": "2024-09-12 06:52:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:28:04.595118"
    },
    {
      "arxiv_id": "2409.07779v3",
      "title": "AFFSegNet: Adaptive Feature Fusion Segmentation Network for Microtumors and Multi-Organ Segmentation",
      "title_zh": "AFFSegNet：适应性特征融合分割网络，用于微肿瘤和多器官分割",
      "authors": [
        "Fuchen Zheng",
        "Xinyi Chen",
        "Xuhang Chen",
        "Haolun Li",
        "Xiaojiao Guo",
        "Weihuang Liu",
        "Chi-Man Pun",
        "Shoujun Zhou"
      ],
      "abstract": "Medical image segmentation, a crucial task in computer vision, facilitates\nthe automated delineation of anatomical structures and pathologies, supporting\nclinicians in diagnosis, treatment planning, and disease monitoring. Notably,\ntransformers employing shifted window-based self-attention have demonstrated\nexceptional performance. However, their reliance on local window attention\nlimits the fusion of local and global contextual information, crucial for\nsegmenting microtumors and miniature organs. To address this limitation, we\npropose the Adaptive Semantic Segmentation Network (ASSNet), a transformer\narchitecture that effectively integrates local and global features for precise\nmedical image segmentation. ASSNet comprises a transformer-based U-shaped\nencoder-decoder network. The encoder utilizes shifted window self-attention\nacross five resolutions to extract multi-scale features, which are then\npropagated to the decoder through skip connections. We introduce an augmented\nmulti-layer perceptron within the encoder to explicitly model long-range\ndependencies during feature extraction. Recognizing the constraints of\nconventional symmetrical encoder-decoder designs, we propose an Adaptive\nFeature Fusion (AFF) decoder to complement our encoder. This decoder\nincorporates three key components: the Long Range Dependencies (LRD) block, the\nMulti-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)\nblock. These components synergistically facilitate the effective fusion of\nmulti-scale features extracted by the decoder while capturing long-range\ndependencies and refining object boundaries. Comprehensive experiments on\ndiverse medical image segmentation tasks, including multi-organ, liver tumor,\nand bladder tumor segmentation, demonstrate that ASSNet achieves\nstate-of-the-art results. Code and models are available at:\n\\url{https://github.com/lzeeorno/ASSNet}.",
      "tldr_zh": "本文提出 ASSNet，一种基于 Transformer 的自适应语义分割网络，用于医疗图像分割，旨在解决传统移位窗口自注意力机制在融合局部和全局上下文方面的局限性，从而更好地处理微肿瘤和多器官分割。ASSNet 采用 U-shaped 编码器-解码器结构，编码器通过移位窗口自注意力和增强的多层感知器提取多尺度特征，而解码器的 Adaptive Feature Fusion (AFF) 模块，包括 Long Range Dependencies (LRD) 块、Multi-Scale Feature Fusion (MFF) 块和 Adaptive Semantic Center (ASC) 块，协同融合特征、捕获长程依赖并精炼对象边界。在多器官、肝肿瘤和膀胱肿瘤分割等任务上，ASSNet 实现了 state-of-the-art 的性能表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.07779v3",
      "published_date": "2024-09-12 06:25:44 UTC",
      "updated_date": "2024-12-10 16:16:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:28:27.473026"
    },
    {
      "arxiv_id": "2409.07776v1",
      "title": "Training Spiking Neural Networks via Augmented Direct Feedback Alignment",
      "title_zh": "通过增强的直接反馈对齐训练脉冲神经网络",
      "authors": [
        "Yongbo Zhang",
        "Katsuma Inoue",
        "Mitsumasa Nakajima",
        "Toshikazu Hashimoto",
        "Yasuo Kuniyoshi",
        "Kohei Nakajima"
      ],
      "abstract": "Spiking neural networks (SNNs), the models inspired by the mechanisms of real\nneurons in the brain, transmit and represent information by employing discrete\naction potentials or spikes. The sparse, asynchronous properties of information\nprocessing make SNNs highly energy efficient, leading to SNNs being promising\nsolutions for implementing neural networks in neuromorphic devices. However,\nthe nondifferentiable nature of SNN neurons makes it a challenge to train them.\nThe current training methods of SNNs that are based on error backpropagation\n(BP) and precisely designing surrogate gradient are difficult to implement and\nbiologically implausible, hindering the implementation of SNNs on neuromorphic\ndevices. Thus, it is important to train SNNs with a method that is both\nphysically implementatable and biologically plausible. In this paper, we\npropose using augmented direct feedback alignment (aDFA), a gradient-free\napproach based on random projection, to train SNNs. This method requires only\npartial information of the forward process during training, so it is easy to\nimplement and biologically plausible. We systematically demonstrate the\nfeasibility of the proposed aDFA-SNNs scheme, propose its effective working\nrange, and analyze its well-performing settings by employing genetic algorithm.\nWe also analyze the impact of crucial features of SNNs on the scheme, thus\ndemonstrating its superiority and stability over BP and conventional direct\nfeedback alignment. Our scheme can achieve competitive performance without\naccurate prior knowledge about the utilized system, thus providing a valuable\nreference for physically training SNNs.",
      "tldr_zh": "本研究针对脉冲神经网络（SNNs）的训练挑战，提出了一种基于随机投影的梯度自由方法——augmented direct feedback alignment (aDFA)。该方法仅需部分前向过程信息，便于实现且生物学合理，解决了传统基于 error backpropagation (BP) 的方法在神经形态设备上的实施难题。通过系统实验和 genetic algorithm 优化，aDFA-SNNs 展示了其有效工作范围，并在性能上优于 BP 和传统 direct feedback alignment，实现了竞争性的准确率。总体而言，该方案为无需精确先验知识的物理训练 SNNs 提供了稳定且可行的参考。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "20 pages, 8 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.07776v1",
      "published_date": "2024-09-12 06:22:44 UTC",
      "updated_date": "2024-09-12 06:22:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:28:28.361392"
    },
    {
      "arxiv_id": "2409.07775v1",
      "title": "A Spatiotemporal Stealthy Backdoor Attack against Cooperative Multi-Agent Deep Reinforcement Learning",
      "title_zh": "针对合作多智能体深度强化学习的时空隐蔽后门攻击",
      "authors": [
        "Yinbo Yu",
        "Saihao Yan",
        "Jiajia Liu"
      ],
      "abstract": "Recent studies have shown that cooperative multi-agent deep reinforcement\nlearning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor\ntrigger is observed, it will perform abnormal actions leading to failures or\nmalicious goals. However, existing proposed backdoors suffer from several\nissues, e.g., fixed visual trigger patterns lack stealthiness, the backdoor is\ntrained or activated by an additional network, or all agents are backdoored. To\nthis end, in this paper, we propose a novel backdoor attack against c-MADRL,\nwhich attacks the entire multi-agent team by embedding the backdoor only in a\nsingle agent. Firstly, we introduce adversary spatiotemporal behavior patterns\nas the backdoor trigger rather than manual-injected fixed visual patterns or\ninstant status and control the attack duration. This method can guarantee the\nstealthiness and practicality of injected backdoors. Secondly, we hack the\noriginal reward function of the backdoored agent via reward reverse and\nunilateral guidance during training to ensure its adverse influence on the\nentire team. We evaluate our backdoor attacks on two classic c-MADRL algorithms\nVDN and QMIX, in a popular c-MADRL environment SMAC. The experimental results\ndemonstrate that our backdoor attacks are able to reach a high attack success\nrate (91.6\\%) while maintaining a low clean performance variance rate (3.7\\%).",
      "tldr_zh": "本研究提出了一种时空隐蔽后门攻击（Spatiotemporal Stealthy Backdoor Attack），针对合作多智能体深度强化学习（c-MADRL），仅在单个智能体中嵌入后门即可影响整个团队。不同于传统方法，该攻击使用对手时空行为模式作为触发器，并通过奖励逆转（reward reverse）和单向指导（unilateral guidance）修改后门智能体的奖励函数，以提升隐蔽性和实用性。在经典算法 VDN 和 QMIX 上进行 SMAC 环境实验，结果显示攻击成功率高达 91.6%，而干净性能变化率仅 3.7%，证明了其高效率和低影响性。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, IEEE Globecom 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.07775v1",
      "published_date": "2024-09-12 06:17:37 UTC",
      "updated_date": "2024-09-12 06:17:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:28:40.337777"
    },
    {
      "arxiv_id": "2409.08308v1",
      "title": "DiReDi: Distillation and Reverse Distillation for AIoT Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Sun",
        "Qing Tong",
        "Wenshuang Yang",
        "Wenqi Zhang"
      ],
      "abstract": "Typically, the significant efficiency can be achieved by deploying different\nedge AI models in various real world scenarios while a few large models manage\nthose edge AI models remotely from cloud servers. However, customizing edge AI\nmodels for each user's specific application or extending current models to new\napplication scenarios remains a challenge. Inappropriate local training or fine\ntuning of edge AI models by users can lead to model malfunction, potentially\nresulting in legal issues for the manufacturer. To address aforementioned\nissues, this paper proposes an innovative framework called \"DiReD\", which\ninvolves knowledge DIstillation & REverse DIstillation. In the initial step, an\nedge AI model is trained with presumed data and a KD process using the cloud AI\nmodel in the upper management cloud server. This edge AI model is then\ndispatched to edge AI devices solely for inference in the user's application\nscenario. When the user needs to update the edge AI model to better fit the\nactual scenario, the reverse distillation (RD) process is employed to extract\nthe knowledge: the difference between user preferences and the manufacturer's\npresumptions from the edge AI model using the user's exclusive data. Only the\nextracted knowledge is reported back to the upper management cloud server to\nupdate the cloud AI model, thus protecting user privacy by not using any\nexclusive data. The updated cloud AI can then update the edge AI model with the\nextended knowledge. Simulation results demonstrate that the proposed \"DiReDi\"\nframework allows the manufacturer to update the user model by learning new\nknowledge from the user's actual scenario with private data. The initial\nredundant knowledge is reduced since the retraining emphasizes user private\ndata.",
      "tldr_zh": "该论文提出了一种名为DiReDi的框架，结合Distillation和Reverse Distillation，用于AIoT应用中管理边缘AI模型，以解决模型定制、扩展和隐私保护问题。框架首先通过知识蒸馏(KD)利用云AI模型训练边缘AI模型，并将其部署到用户设备仅用于推理；当需要更新时，反向蒸馏(RD)从边缘模型提取用户偏好差异的知识，仅上传这些知识回云服务器，避免直接使用用户私有数据。实验模拟结果显示，DiReDi框架能有效帮助制造商学习新场景知识、减少冗余训练，并提升模型适应性，同时保护用户隐私。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08308v1",
      "published_date": "2024-09-12 06:02:44 UTC",
      "updated_date": "2024-09-12 06:02:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:28:52.078254"
    },
    {
      "arxiv_id": "2409.07770v1",
      "title": "Universal Pooling Method of Multi-layer Features from Pretrained Models for Speaker Verification",
      "title_zh": "基于预训练模型多层特征的通用池化方法，用于说话人验证",
      "authors": [
        "Jin Sob Kim",
        "Hyun Joon Park",
        "Wooseok Shin",
        "Sung Won Han"
      ],
      "abstract": "Recent advancements in automatic speaker verification (ASV) studies have been\nachieved by leveraging large-scale pretrained networks. In this study, we\nanalyze the approaches toward such a paradigm and underline the significance of\ninterlayer information processing as a result. Accordingly, we present a novel\napproach for exploiting the multilayered nature of pretrained models for ASV,\nwhich comprises a layer/frame-level network and two steps of pooling\narchitectures for each layer and frame axis. Specifically, we let convolutional\narchitecture directly processes a stack of layer outputs.Then, we present a\nchannel attention-based scheme of gauging layer significance and squeeze the\nlayer level with the most representative value. Finally, attentive statistics\nover frame-level representations yield a single vector speaker embedding.\nComparative experiments are designed using versatile data environments and\ndiverse pretraining models to validate the proposed approach. The experimental\nresults demonstrate the stability of the approach using multi-layer outputs in\nleveraging pretrained architectures. Then, we verify the superiority of the\nproposed ASV backend structure, which involves layer-wise operations, in terms\nof performance improvement along with cost efficiency compared to the\nconventional method. The ablation study shows how the proposed interlayer\nprocessing aids in maximizing the advantage of utilizing pretrained models.",
      "tldr_zh": "本研究提出了一种通用的池化方法，用于从预训练模型的多层特征中提取信息，以提升自动说话人验证 (ASV) 的性能。该方法包括层/帧级网络和两步池化架构：首先，使用卷积网络处理层输出堆栈，并通过基于通道注意力的方案评估层重要性，选择最具代表性的层值；然后，对帧级表示进行注意统计，生成单一的说话人嵌入向量。实验在多种数据环境和预训练模型上验证，结果显示该方法比传统方法提高了性能，同时提升了成本效率；消融研究进一步证明，层间处理最大化了预训练模型的优势。",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2409.07770v1",
      "published_date": "2024-09-12 05:55:32 UTC",
      "updated_date": "2024-09-12 05:55:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:29:04.562192"
    },
    {
      "arxiv_id": "2409.07763v1",
      "title": "Reimagining Linear Probing: Kolmogorov-Arnold Networks in Transfer Learning",
      "title_zh": "重新构想线性探查：Kolmogorov-Arnold Networks 在迁移学习中",
      "authors": [
        "Sheng Shen",
        "Rabih Younes"
      ],
      "abstract": "This paper introduces Kolmogorov-Arnold Networks (KAN) as an enhancement to\nthe traditional linear probing method in transfer learning. Linear probing,\noften applied to the final layer of pre-trained models, is limited by its\ninability to model complex relationships in data. To address this, we propose\nsubstituting the linear probing layer with KAN, which leverages spline-based\nrepresentations to approximate intricate functions. In this study, we integrate\nKAN with a ResNet-50 model pre-trained on ImageNet and evaluate its performance\non the CIFAR-10 dataset. We perform a systematic hyperparameter search,\nfocusing on grid size and spline degree (k), to optimize KAN's flexibility and\naccuracy. Our results demonstrate that KAN consistently outperforms traditional\nlinear probing, achieving significant improvements in accuracy and\ngeneralization across a range of configurations. These findings indicate that\nKAN offers a more powerful and adaptable alternative to conventional linear\nprobing techniques in transfer learning.",
      "tldr_zh": "本论文重新构想了迁移学习中的线性探测方法，通过引入 Kolmogorov-Arnold Networks (KAN) 来提升其性能。KAN 利用基于样条的表示来近似复杂数据关系，取代了传统线性探测层的局限性，并在预训练的 ResNet-50 模型上进行整合。研究通过系统超参数搜索（包括网格大小和样条度 k）优化 KAN 的灵活性和准确性，结果显示 KAN 在 CIFAR-10 数据集上显著超越传统方法，提高了准确性和泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.07763v1",
      "published_date": "2024-09-12 05:36:40 UTC",
      "updated_date": "2024-09-12 05:36:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:29:15.597034"
    },
    {
      "arxiv_id": "2409.18971v1",
      "title": "Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better",
      "title_zh": "翻译失败",
      "authors": [
        "Mengying Ge",
        "Mingyang Li",
        "Dongkai Tang",
        "Pengbo Li",
        "Kuo Liu",
        "Shuhao Deng",
        "Songbai Pu",
        "Long Liu",
        "Yang Song",
        "Tao Zhang"
      ],
      "abstract": "In this paper, we present our solutions for emotion recognition in the\nsub-challenges of Multimodal Emotion Recognition Challenge (MER2024). To\nmitigate the modal competition issue between audio and text, we adopt an early\nfusion strategy based on a large language model, where joint training of audio\nand text is conducted initially. And the joint Audio-Text modal feature will be\nlate-fused with other unimodal features. In order to solve the problems of data\ninsufficiency and class imbalance, We use multiple turns of multi-model voting\nfor data mining. Moreover, to enhance the quality of audio features, we employ\nspeech source separation to preprocess audios. Our model ranks \\textbf{2nd} in\nboth MER2024-SEMI and MER2024-NOISE, validating our method's effectiveness.",
      "tldr_zh": "本论文针对多模态情感识别挑战（MER2024）提出了一种解决方案，通过早融合策略基于大型语言模型（LLM）进行音频和文本的联合训练，从而缓解模态竞争问题，并将联合的Audio-Text模态特征与其它单模态特征进行晚融合。针对数据不足和类别不平衡问题，他们采用多轮多模型投票进行数据挖掘，并使用语音源分离（speech source separation）预处理音频以提升特征质量。实验结果显示，该方法在MER2024-SEMI和MER2024-NOISE挑战中均排名第二，证明了其有效性。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18971v1",
      "published_date": "2024-09-12 05:05:34 UTC",
      "updated_date": "2024-09-12 05:05:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:29:28.306558"
    },
    {
      "arxiv_id": "2409.07753v4",
      "title": "Relevance for Human Robot Collaboration",
      "title_zh": "人-机器人协作的相关性",
      "authors": [
        "Xiaotong Zhang",
        "Dean Huang",
        "Kamal Youcef-Toumi"
      ],
      "abstract": "Inspired by the human ability to selectively focus on relevant information,\nthis paper introduces relevance, a novel dimensionality reduction process for\nhuman-robot collaboration (HRC). Our approach incorporates a continuously\noperating perception module, evaluates cue sufficiency within the scene, and\napplies a flexible formulation and computation framework. To accurately and\nefficiently quantify relevance, we developed an event-based framework that\nmaintains a continuous perception of the scene and selectively triggers\nrelevance determination. Within this framework, we developed a probabilistic\nmethodology, which considers various factors and is built on a novel structured\nscene representation. Simulation results demonstrate that the relevance\nframework and methodology accurately predict the relevance of a general HRC\nsetup, achieving a precision of 0.99, a recall of 0.94, an F1 score of 0.96,\nand an object ratio of 0.94. Relevance can be broadly applied to several areas\nin HRC to accurately improve task planning time by 79.56% compared with pure\nplanning for a cereal task, reduce perception latency by up to 26.53% for an\nobject detector, improve HRC safety by up to 13.50% and reduce the number of\ninquiries for HRC by 80.84%. A real-world demonstration showcases the relevance\nframework's ability to intelligently and seamlessly assist humans in everyday\ntasks.",
      "tldr_zh": "本论文受人类选择性关注启发的灵感，引入了“relevance”作为一种新型维度减少过程，用于提升人机协作（HRC）。该方法包括一个持续运作的感知模块、事件-based 框架和概率方法ology，通过结构化场景表示来评估和量化场景中相关信息的充分性。模拟结果显示，该框架在 HRC 设置中实现了高精度（0.99）、召回率（0.94）和 F1 分数（0.96），并将任务规划时间提高 79.56%、感知延迟减少 26.53%、安全性能提升 13.50%，以及减少询问 80.84%。真实世界演示证明了该框架能智能无缝地辅助日常任务。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2409.07753v4",
      "published_date": "2024-09-12 04:57:34 UTC",
      "updated_date": "2025-04-17 16:19:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:29:50.625817"
    },
    {
      "arxiv_id": "2409.07748v1",
      "title": "Top-down Activity Representation Learning for Video Question Answering",
      "title_zh": "自上而下的活动表示学习用于视频问答",
      "authors": [
        "Yanan Wang",
        "Shuichiro Haruta",
        "Donghuo Zeng",
        "Julio Vizcarra",
        "Mori Kurokawa"
      ],
      "abstract": "Capturing complex hierarchical human activities, from atomic actions (e.g.,\npicking up one present, moving to the sofa, unwrapping the present) to\ncontextual events (e.g., celebrating Christmas) is crucial for achieving\nhigh-performance video question answering (VideoQA). Recent works have expanded\nmultimodal models (e.g., CLIP, LLaVA) to process continuous video sequences,\nenhancing the model's temporal reasoning capabilities. However, these\napproaches often fail to capture contextual events that can be decomposed into\nmultiple atomic actions non-continuously distributed over relatively long-term\nsequences. In this paper, to leverage the spatial visual context representation\ncapability of the CLIP model for obtaining non-continuous visual\nrepresentations in terms of contextual events in videos, we convert long-term\nvideo sequences into a spatial image domain and finetune the multimodal model\nLLaVA for the VideoQA task. Our approach achieves competitive performance on\nthe STAR task, in particular, with a 78.4% accuracy score, exceeding the\ncurrent state-of-the-art score by 2.8 points on the NExTQA task.",
      "tldr_zh": "该论文针对视频问答（VideoQA）任务，提出了一种自上而下的活动表示学习方法，以捕捉从原子动作（如捡起礼物）到上下文事件（如庆祝圣诞）的复杂层次活动。作者将长视频序列转换为空间图像域，利用 CLIP 模型的视觉上下文表示能力，并对 LLaVA 模型进行微调，以处理非连续分布式动作的挑战。该方法在 STAR 任务上表现出色，并在 NExTQA 任务上实现 78.4% 的准确率，比现有最先进方法高出 2.8 分，展示了其在提升 VideoQA 性能方面的显著贡献。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "presented at MIRU2024",
      "pdf_url": "http://arxiv.org/pdf/2409.07748v1",
      "published_date": "2024-09-12 04:43:27 UTC",
      "updated_date": "2024-09-12 04:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:30:02.648091"
    },
    {
      "arxiv_id": "2409.07747v1",
      "title": "Multi-object event graph representation learning for Video Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Yanan Wang",
        "Shuichiro Haruta",
        "Donghuo Zeng",
        "Julio Vizcarra",
        "Mori Kurokawa"
      ],
      "abstract": "Video question answering (VideoQA) is a task to predict the correct answer to\nquestions posed about a given video. The system must comprehend spatial and\ntemporal relationships among objects extracted from videos to perform causal\nand temporal reasoning. While prior works have focused on modeling individual\nobject movements using transformer-based methods, they falter when capturing\ncomplex scenarios involving multiple objects (e.g., \"a boy is throwing a ball\nin a hoop\"). We propose a contrastive language event graph representation\nlearning method called CLanG to address this limitation. Aiming to capture\nevent representations associated with multiple objects, our method employs a\nmulti-layer GNN-cluster module for adversarial graph representation learning,\nenabling contrastive learning between the question text and its relevant\nmulti-object event graph. Our method outperforms a strong baseline, achieving\nup to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and\nTGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal\nand temporal questions, highlighting its strength in reasoning multiple\nobject-based events.",
      "tldr_zh": "本研究针对 Video Question Answering (VideoQA) 任务，提出了一种名为 CLanG 的对比学习方法，以更好地捕捉视频中多个物体之间的空间和时间关系，从而提升因果及时间推理能力。该方法利用多层 GNN-cluster 模块进行对抗图表示学习，并在问题文本与相关多物体事件图之间进行对比学习，解决了现有基于 transformer 的方法在复杂多物体场景中的不足。在 NExT-QA 和 TGIF-QA-R 数据集上，CLanG 比强基线模型准确率提升高达 2.2%，尤其在处理因果和时间问题时高出 2.8%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "presented at MIRU2024",
      "pdf_url": "http://arxiv.org/pdf/2409.07747v1",
      "published_date": "2024-09-12 04:42:51 UTC",
      "updated_date": "2024-09-12 04:42:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:30:03.803695"
    },
    {
      "arxiv_id": "2409.07736v1",
      "title": "Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Aaryan Panda",
        "Damodar Panigrahi",
        "Shaswata Mitra",
        "Sudip Mittal",
        "Shahram Rahimi"
      ],
      "abstract": "The field of Computer Vision (CV) has faced challenges. Initially, it relied\non handcrafted features and rule-based algorithms, resulting in limited\naccuracy. The introduction of machine learning (ML) has brought progress,\nparticularly Transfer Learning (TL), which addresses various CV problems by\nreusing pre-trained models. TL requires less data and computing while\ndelivering nearly equal accuracy, making it a prominent technique in the CV\nlandscape. Our research focuses on TL development and how CV applications use\nit to solve real-world problems. We discuss recent developments, limitations,\nand opportunities.",
      "tldr_zh": "这篇论文调查了Transfer Learning (TL)在Computer Vision (CV)领域的应用，回顾了CV从依赖手工特征和规则算法到引入机器学习（ML）的演变过程。TL通过重用预训练模型，显著减少了数据和计算资源需求，同时实现了几乎相同的准确性。论文重点讨论了TL的最新发展、面临的限制以及潜在机会，为CV问题的实际解决提供了宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.07736v1",
      "published_date": "2024-09-12 03:59:15 UTC",
      "updated_date": "2024-09-12 03:59:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:30:15.485771"
    },
    {
      "arxiv_id": "2409.07732v1",
      "title": "Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT",
      "title_zh": "翻译失败",
      "authors": [
        "Irene Weber"
      ],
      "abstract": "Large Language Models (LLMs) offer numerous applications, the full extent of\nwhich is not yet understood. This paper investigates if LLMs can be applied for\nediting structured and semi-structured documents with minimal effort. Using a\nqualitative research approach, we conduct two case studies with ChatGPT and\nthoroughly analyze the results. Our experiments indicate that LLMs can\neffectively edit structured and semi-structured documents when provided with\nbasic, straightforward prompts. ChatGPT demonstrates a strong ability to\nrecognize and process the structure of annotated documents. This suggests that\nexplicitly structuring tasks and data in prompts might enhance an LLM's ability\nto understand and solve tasks. Furthermore, the experiments also reveal\nimpressive pattern matching skills in ChatGPT. This observation deserves\nfurther investigation, as it may contribute to understanding the processes\nleading to hallucinations in LLMs.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在编辑结构化和半结构化文档方面的潜力，通过两个案例研究使用 ChatGPT 进行定性实验。结果显示，LLMs 在简单提示下能有效识别和处理文档结构，提升了任务理解和编辑能力。研究还揭示了 ChatGPT 的强大模式匹配技能，这可能有助于进一步理解 LLMs 中的幻觉（hallucinations）问题。总之，此工作强调了在提示中显式结构化任务和数据的重要性，以优化 LLMs 的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07732v1",
      "published_date": "2024-09-12 03:41:39 UTC",
      "updated_date": "2024-09-12 03:41:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:30:27.569160"
    },
    {
      "arxiv_id": "2409.08867v1",
      "title": "Establish seedling quality classification standard for Chrysanthemum efficiently with help of deep clustering algorithm",
      "title_zh": "翻译失败",
      "authors": [
        "Yanzhi Jing",
        "Hongguang Zhao",
        "Shujun Yu"
      ],
      "abstract": "Establishing reasonable standards for edible chrysanthemum seedlings helps\npromote seedling development, thereby improving plant quality. However, current\ngrading methods have the several issues. The limitation that only support a few\nindicators causes information loss, and indicators selected to evaluate\nseedling level have a narrow applicability. Meanwhile, some methods misuse\nmathematical formulas. Therefore, we propose a simple, efficient, and generic\nframework, SQCSEF, for establishing seedling quality classification standards\nwith flexible clustering modules, applicable to most plant species. In this\nstudy, we introduce the state-of-the-art deep clustering algorithm CVCL, using\nfactor analysis to divide indicators into several perspectives as inputs for\nthe CVCL method, resulting in more reasonable clusters and ultimately a grading\nstandard $S_{cvcl}$ for edible chrysanthemum seedlings. Through conducting\nextensive experiments, we validate the correctness and efficiency of the\nproposed SQCSEF framework.",
      "tldr_zh": "本文针对现有菊花幼苗分级方法的不足（如信息丢失、指标适用性窄和误用数学公式），提出一个简单、高效且通用的框架 SQCSEF，支持灵活的聚类模块，适用于大多数植物物种。框架采用最先进的深度聚类算法 CVCL，并通过因子分析将指标分为多个视角作为输入，生成更合理的聚类结果，从而建立食用菊花幼苗的分级标准 $S_{cvcl}$。实验验证显示，SQCSEF 框架在正确性和效率方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08867v1",
      "published_date": "2024-09-12 03:09:11 UTC",
      "updated_date": "2024-09-12 03:09:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:30:39.764098"
    },
    {
      "arxiv_id": "2409.07725v2",
      "title": "GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Kaizhe Fan",
        "Quanjun Li"
      ],
      "abstract": "Graph representation learning has emerged as a powerful tool for preserving\ngraph topology when mapping nodes to vector representations, enabling various\ndownstream tasks such as node classification and community detection. However,\nmost current graph neural network models face the challenge of requiring\nextensive labeled data, which limits their practical applicability in\nreal-world scenarios where labeled data is scarce. To address this challenge,\nresearchers have explored Graph Contrastive Learning (GCL), which leverages\nenhanced graph data and contrastive learning techniques. While promising,\nexisting GCL methods often struggle with effectively capturing both local and\nglobal graph structures, and balancing the trade-off between nodelevel and\ngraph-level representations. In this work, we propose Graph Representation\nEmbedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our\nmodel introduces a novel triple network architecture with a multi-head\nattention GNN as the core. GRE2-MDCL first globally and locally augments the\ninput graph using SVD and LAGNN techniques. It then constructs a\nmultidimensional contrastive loss, incorporating cross-network, cross-view, and\nneighbor contrast, to optimize the model. Extensive experiments on benchmark\ndatasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves\nstate-of-the-art performance, with average accuracies of 82.5%, 72.5%, and\n81.6% respectively. Visualizations further show tighter intra-cluster\naggregation and clearer inter-cluster boundaries, highlighting the\neffectiveness of our framework in improving upon baseline GCL models.",
      "tldr_zh": "该论文针对图表示学习中依赖大量标注数据的挑战，提出了一种新模型 GRE^2-MDCL，通过多维对比学习（Multidimensional Contrastive Learning）增强图表示嵌入。模型采用三元网络架构，以多头注意力 GNN（Graph Neural Network）为核心，先使用 SVD 和 LAGNN 技术进行全局和局部图增强，然后构建多维对比损失，包括跨网络、跨视图和邻居对比，以更好地捕捉局部和全局图结构。在 Cora、Citeseer 和 PubMed 等基准数据集上，GRE^2-MDCL 实现了最先进性能，平均准确率分别为 82.5%、72.5% 和 81.6%，并通过可视化展示了更紧凑的内部聚类和清晰的集群边界。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "I am requesting the withdrawal of my paper due to errors identified\n  in the methodology and experimental results. Specifically, there are\n  inaccuracies in the analysis section that may lead to misleading conclusions",
      "pdf_url": "http://arxiv.org/pdf/2409.07725v2",
      "published_date": "2024-09-12 03:09:05 UTC",
      "updated_date": "2025-03-20 02:10:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:30:53.250749"
    },
    {
      "arxiv_id": "2409.07723v2",
      "title": "Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy",
      "title_zh": "翻译失败",
      "authors": [
        "Bojian Li",
        "Bo Liu",
        "Xinning Yao",
        "Jinghua Yue",
        "Fugen Zhou"
      ],
      "abstract": "Depth estimation is a cornerstone of 3D reconstruction and plays a vital role\nin minimally invasive endoscopic surgeries. However, most current depth\nestimation networks rely on traditional convolutional neural networks, which\nare limited in their ability to capture global information. Foundation models\noffer a promising approach to enhance depth estimation, but those models\ncurrently available are primarily trained on natural images, leading to\nsuboptimal performance when applied to endoscopic images. In this work, we\nintroduce a novel fine-tuning strategy for the Depth Anything Model and\nintegrate it with an intrinsic-based unsupervised monocular depth estimation\nframework. Our approach includes a low-rank adaptation technique based on\nrandom vectors, which improves the model's adaptability to different scales.\nAdditionally, we propose a residual block built on depthwise separable\nconvolution to compensate for the transformer's limited ability to capture\nlocal features. Our experimental results on the SCARED dataset and Hamlyn\ndataset show that our method achieves state-of-the-art performance while\nminimizing the number of trainable parameters. Applying this method in\nminimally invasive endoscopic surgery can enhance surgeons' spatial awareness,\nthereby improving the precision and safety of the procedures.",
      "tldr_zh": "本研究针对内窥镜图像的深度估计问题，提出了一种改进Depth Anything Model的细调策略，并将其整合到基于内在属性的无监督单目深度估计框架中，以克服传统CNN在捕获全局信息方面的局限性。关键创新包括使用基于随机向量的Low-Rank Adaptation技术提升模型对不同尺度的适应性，以及引入基于深度可分离卷积的残差块来补偿Transformer在局部特征捕获上的不足。在SCARED和Hamlyn数据集上的实验显示，该方法实现了最先进性能，同时显著减少了可训练参数。通过应用于微创内窥镜手术，该方法能提升外科医生的空间感知，从而提高手术的精度和安全性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.07723v2",
      "published_date": "2024-09-12 03:04:43 UTC",
      "updated_date": "2025-03-06 01:40:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:31:03.914842"
    },
    {
      "arxiv_id": "2409.07715v1",
      "title": "FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Devansh Dhrafani",
        "Yifei Liu",
        "Andrew Jong",
        "Ukcheol Shin",
        "Yao He",
        "Tyler Harp",
        "Yaoyu Hu",
        "Jean Oh",
        "Sebastian Scherer"
      ],
      "abstract": "Robust depth perception in visually-degraded environments is crucial for\nautonomous aerial systems. Thermal imaging cameras, which capture infrared\nradiation, are robust to visual degradation. However, due to lack of a\nlarge-scale dataset, the use of thermal cameras for unmanned aerial system\n(UAS) depth perception has remained largely unexplored. This paper presents a\nstereo thermal depth perception dataset for autonomous aerial perception\napplications. The dataset consists of stereo thermal images, LiDAR, IMU and\nground truth depth maps captured in urban and forest settings under diverse\nconditions like day, night, rain, and smoke. We benchmark representative stereo\ndepth estimation algorithms, offering insights into their performance in\ndegraded conditions. Models trained on our dataset generalize well to unseen\nsmoky conditions, highlighting the robustness of stereo thermal imaging for\ndepth perception. We aim for this work to enhance robotic perception in\ndisaster scenarios, allowing for exploration and operations in previously\nunreachable areas. The dataset and source code are available at\nhttps://firestereo.github.io.",
      "tldr_zh": "本研究介绍了 FIReStereo 数据集，这是一个针对视觉退化环境（如日夜、雨和烟雾）的森林红外立体数据集，用于提升无人驾驶航空系统 (UAS) 的深度感知能力。数据集包含立体热成像图像、LiDAR、IMU 和地面真实深度图，采集自城市和森林场景，以支持自主空中感知应用。研究者基准测试了代表性的立体深度估计算法，发现这些模型在退化条件下表现出色，并能良好泛化到未见过的烟雾环境。总体而言，此工作旨在增强机器人感知在灾害场景中的鲁棒性，促进探索和操作先前难以到达的区域。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review in RA-L. The first 2 authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2409.07715v1",
      "published_date": "2024-09-12 02:51:21 UTC",
      "updated_date": "2024-09-12 02:51:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:31:15.818674"
    },
    {
      "arxiv_id": "2409.17275v1",
      "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains",
      "title_zh": "翻译失败",
      "authors": [
        "Xun Xian",
        "Ganghua Wang",
        "Xuan Bi",
        "Jayanth Srinivasa",
        "Ashish Kundu",
        "Charles Fleming",
        "Mingyi Hong",
        "Jie Ding"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance\nthe performance of large language models (LLMs) in knowledge-intensive domains\nsuch as healthcare, finance, and legal contexts. Given a query, RAG retrieves\nrelevant documents from a corpus and integrates them into the LLMs' generation\nprocess. In this study, we investigate the adversarial robustness of RAG,\nfocusing specifically on examining the retrieval system. First, across 225\ndifferent setup combinations of corpus, retriever, query, and targeted\ninformation, we show that retrieval systems are vulnerable to universal\npoisoning attacks in medical Q\\&A. In such attacks, adversaries generate\npoisoned documents containing a broad spectrum of targeted information, such as\npersonally identifiable information. When these poisoned documents are inserted\ninto a corpus, they can be accurately retrieved by any users, as long as\nattacker-specified queries are used. To understand this vulnerability, we\ndiscovered that the deviation from the query's embedding to that of the\npoisoned document tends to follow a pattern in which the high similarity\nbetween the poisoned document and the query is retained, thereby enabling\nprecise retrieval. Based on these findings, we develop a new detection-based\ndefense to ensure the safe use of RAG. Through extensive experiments spanning\nvarious Q\\&A domains, we observed that our proposed method consistently\nachieves excellent detection rates in nearly all cases.",
      "tldr_zh": "本研究探讨了Retrieval-Augmented Generation (RAG) 在知识密集领域（如医疗、金融和法律）中的脆弱性，重点考察了其检索系统的对抗鲁棒性。实验显示，在225种不同设置中，RAG 易受universal poisoning attacks 影响，例如攻击者插入包含目标信息的毒化文档，导致这些文档在特定查询下被准确检索，尤其在医疗 Q&A 场景中。研究发现，这种漏洞源于毒化文档与查询嵌入之间的高相似性，使检索系统无法有效区分。基于此，作者提出了一种新的检测-based 防御方法，通过广泛实验验证，该方法在各种 Q&A 领域中实现了近乎完美的检测率，从而提升了 RAG 的安全性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.DB",
        "cs.ET",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17275v1",
      "published_date": "2024-09-12 02:43:40 UTC",
      "updated_date": "2024-09-12 02:43:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:31:28.475405"
    },
    {
      "arxiv_id": "2409.07706v1",
      "title": "Attack End-to-End Autonomous Driving through Module-Wise Noise",
      "title_zh": "翻译失败",
      "authors": [
        "Lu Wang",
        "Tianyuan Zhang",
        "Yikai Han",
        "Muyang Fang",
        "Ting Jin",
        "Jiaqi Kang"
      ],
      "abstract": "With recent breakthroughs in deep neural networks, numerous tasks within\nautonomous driving have exhibited remarkable performance. However, deep\nlearning models are susceptible to adversarial attacks, presenting significant\nsecurity risks to autonomous driving systems. Presently, end-to-end\narchitectures have emerged as the predominant solution for autonomous driving,\nowing to their collaborative nature across different tasks. Yet, the\nimplications of adversarial attacks on such models remain relatively\nunexplored. In this paper, we conduct comprehensive adversarial security\nresearch on the modular end-to-end autonomous driving model for the first time.\nWe thoroughly consider the potential vulnerabilities in the model inference\nprocess and design a universal attack scheme through module-wise noise\ninjection. We conduct large-scale experiments on the full-stack autonomous\ndriving model and demonstrate that our attack method outperforms previous\nattack methods. We trust that our research will offer fresh insights into\nensuring the safety and reliability of autonomous driving systems.",
      "tldr_zh": "该研究首次针对模块化端到端（end-to-end）自动驾驶模型进行了全面对抗安全（adversarial attacks）分析，强调了深度神经网络（deep neural networks）在自动驾驶中的易受攻击风险。研究者设计了一种通过模块级噪声注入（module-wise noise injection）的通用攻击方案，针对模型推理过程的潜在漏洞进行系统性攻击。在大规模实验中，该方法在全栈自动驾驶模型上表现优于现有攻击方法，并为提升自动驾驶系统的安全性和可靠性提供了新颖见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07706v1",
      "published_date": "2024-09-12 02:19:16 UTC",
      "updated_date": "2024-09-12 02:19:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:31:39.610058"
    },
    {
      "arxiv_id": "2409.07704v1",
      "title": "Super Monotonic Alignment Search",
      "title_zh": "翻译失败",
      "authors": [
        "Junhyeok Lee",
        "Hyeongju Kim"
      ],
      "abstract": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
      "tldr_zh": "该论文提出了一种加速版 Monotonic Alignment Search (MAS) 算法，名为 Super Monotonic Alignment Search，针对 Glow-TTS 中用于估计文本和语音对齐的原算法时间复杂度 O(T × S) 问题进行了优化。作者发现原算法在 CPU 上执行效率低下且难以并行化，因此开发了基于 Triton 内核和 PyTorch JIT 脚本的 GPU 实现，消除了设备间复制开销。实验结果显示，新版本在极端长度情况下速度提升高达 72 倍，代码已在 GitHub 上开源。",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2409.07704v1",
      "published_date": "2024-09-12 02:13:57 UTC",
      "updated_date": "2024-09-12 02:13:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:31:52.872999"
    },
    {
      "arxiv_id": "2409.07703v3",
      "title": "DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?",
      "title_zh": "DSBench：数据科学代理距离成为数据科学专家还有多远？",
      "authors": [
        "Liqiang Jing",
        "Zhehui Huang",
        "Xiaoyang Wang",
        "Wenlin Yao",
        "Wenhao Yu",
        "Kaixin Ma",
        "Hongming Zhang",
        "Xinya Du",
        "Dong Yu"
      ],
      "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.",
      "tldr_zh": "本研究引入 DSBench，一种全面基准，用于评估数据科学代理（agents）在现实任务中的表现，旨在弥补现有基准简化设置的不足。DSBench 包含 466 个数据分析任务和 74 个数据建模任务，源自 Eloquence 和 Kaggle 竞赛，并模拟真实场景如长上下文、多模态背景、大数据文件和多表结构。评估结果显示，最先进的 LLMs、LVLMs 和 agents 在大多数任务中表现不佳，最好代理仅解决 34.12% 的数据分析任务，Relative Performance Gap (RPG) 为 34.74%。这些发现强调了开发更实用、智能和自治数据科学代理的必要性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07703v3",
      "published_date": "2024-09-12 02:08:00 UTC",
      "updated_date": "2025-04-11 14:12:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:32:04.267108"
    },
    {
      "arxiv_id": "2409.07684v1",
      "title": "Modeling Information Narrative Detection and Evolution on Telegram during the Russia-Ukraine War",
      "title_zh": "翻译失败",
      "authors": [
        "Patrick Gerard",
        "Svitlana Volkova",
        "Louis Penafiel",
        "Kristina Lerman",
        "Tim Weninger"
      ],
      "abstract": "Following the Russian Federation's full-scale invasion of Ukraine in February\n2022, a multitude of information narratives emerged within both pro-Russian and\npro-Ukrainian communities online. As the conflict progresses, so too do the\ninformation narratives, constantly adapting and influencing local and global\ncommunity perceptions and attitudes. This dynamic nature of the evolving\ninformation environment (IE) underscores a critical need to fully discern how\nnarratives evolve and affect online communities. Existing research, however,\noften fails to capture information narrative evolution, overlooking both the\nfluid nature of narratives and the internal mechanisms that drive their\nevolution. Recognizing this, we introduce a novel approach designed to both\nmodel narrative evolution and uncover the underlying mechanisms driving them.\nIn this work we perform a comparative discourse analysis across communities on\nTelegram covering the initial three months following the invasion. First, we\nuncover substantial disparities in narratives and perceptions between\npro-Russian and pro-Ukrainian communities. Then, we probe deeper into prevalent\nnarratives of each group, identifying key themes and examining the underlying\nmechanisms fueling their evolution. Finally, we explore influences and factors\nthat may shape the development and spread of narratives.",
      "tldr_zh": "本研究探讨了俄乌战争期间Telegram平台上信息叙事（Information Narrative）的检测和演变，针对现有研究忽略叙事动态和内部机制的不足，提出了一种新方法来建模叙事的演变过程。研究通过对入侵后前三个月的pro-Russian和pro-Ukrainian社区进行比较话语分析（Discourse Analysis），揭示了两个社区在叙事和感知上的重大差异，并识别了关键主题以及驱动演变的底层机制。最后，论文考察了可能影响叙事发展和传播的各种因素，为理解在线信息环境的动态提供了新见解。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "12 pages, International AAAI Conference on Web and Social Media 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.07684v1",
      "published_date": "2024-09-12 01:18:57 UTC",
      "updated_date": "2024-09-12 01:18:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:32:15.895897"
    },
    {
      "arxiv_id": "2409.07683v1",
      "title": "Open-Vocabulary Remote Sensing Image Semantic Segmentation",
      "title_zh": "开放词汇遥感图像语义分割",
      "authors": [
        "Qinglong Cao",
        "Yuntian Chen",
        "Chao Ma",
        "Xiaokang Yang"
      ],
      "abstract": "Open-vocabulary image semantic segmentation (OVS) seeks to segment images\ninto semantic regions across an open set of categories. Existing OVS methods\ncommonly depend on foundational vision-language models and utilize similarity\ncomputation to tackle OVS tasks. However, these approaches are predominantly\ntailored to natural images and struggle with the unique characteristics of\nremote sensing images, such as rapidly changing orientations and significant\nscale variations. These challenges complicate OVS tasks in earth vision,\nrequiring specialized approaches. To tackle this dilemma, we propose the first\nOVS framework specifically designed for remote sensing imagery, drawing\ninspiration from the distinct remote sensing traits. Particularly, to address\nthe varying orientations, we introduce a rotation-aggregative similarity\ncomputation module that generates orientation-adaptive similarity maps as\ninitial semantic maps. These maps are subsequently refined at both spatial and\ncategorical levels to produce more accurate semantic maps. Additionally, to\nmanage significant scale changes, we integrate multi-scale image features into\nthe upsampling process, resulting in the final scale-aware semantic masks. To\nadvance OVS in earth vision and encourage reproducible research, we establish\nthe first open-sourced OVS benchmark for remote sensing imagery, including four\npublic remote sensing datasets. Extensive experiments on this benchmark\ndemonstrate our proposed method achieves state-of-the-art performance. All\ncodes and datasets are available at https://github.com/caoql98/OVRS.",
      "tldr_zh": "该论文提出了一种针对遥感图像的开放词汇语义分割（Open-Vocabulary Semantic Segmentation, OVS）框架，以解决现有方法在处理遥感图像的快速变化方向和显著尺度变化时存在的局限性。框架的关键创新包括引入旋转聚合相似性计算模块（rotation-aggregative similarity computation）生成适应方向的相似性映射，并通过空间和类别级别的精炼以及多尺度特征整合到上采样过程中，产出精确的尺度感知语义掩码（scale-aware semantic masks）。此外，论文建立了首个开源OVS基准，涵盖四个公共遥感数据集，实验结果显示该方法在基准上实现了state-of-the-art性能，所有代码和数据集可从GitHub获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07683v1",
      "published_date": "2024-09-12 01:16:25 UTC",
      "updated_date": "2024-09-12 01:16:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:32:27.974695"
    },
    {
      "arxiv_id": "2409.07672v1",
      "title": "An Unsupervised Dialogue Topic Segmentation Model Based on Utterance Rewriting",
      "title_zh": "翻译失败",
      "authors": [
        "Xia Hou",
        "Qifeng Li",
        "Tongliang Li"
      ],
      "abstract": "Dialogue topic segmentation plays a crucial role in various types of dialogue\nmodeling tasks. The state-of-the-art unsupervised DTS methods learn topic-aware\ndiscourse representations from conversation data through adjacent discourse\nmatching and pseudo segmentation to further mine useful clues in unlabeled\nconversational relations. However, in multi-round dialogs, discourses often\nhave co-references or omissions, leading to the fact that direct use of these\ndiscourses for representation learning may negatively affect the semantic\nsimilarity computation in the neighboring discourse matching task. In order to\nfully utilize the useful cues in conversational relations, this study proposes\na novel unsupervised dialog topic segmentation method that combines the\nUtterance Rewriting (UR) technique with an unsupervised learning algorithm to\nefficiently utilize the useful cues in unlabeled dialogs by rewriting the\ndialogs in order to recover the co-referents and omitted words. Compared with\nexisting unsupervised models, the proposed Discourse Rewriting Topic\nSegmentation Model (UR-DTS) significantly improves the accuracy of topic\nsegmentation. The main finding is that the performance on DialSeg711 improves\nby about 6% in terms of absolute error score and WD, achieving 11.42% in terms\nof absolute error score and 12.97% in terms of WD. on Doc2Dial the absolute\nerror score and WD improves by about 3% and 2%, respectively, resulting in SOTA\nreaching 35.17% in terms of absolute error score and 38.49% in terms of WD.\nThis shows that the model is very effective in capturing the nuances of\nconversational topics, as well as the usefulness and challenges of utilizing\nunlabeled conversations.",
      "tldr_zh": "这篇论文提出了一种基于 Utterance Rewriting 的无监督对话主题分割（Dialogue Topic Segmentation, DTS）模型 UR-DTS，以解决多轮对话中共同引用（co-references）和省略词导致的语义相似性计算问题。模型通过结合 Utterance Rewriting 技术重写对话，恢复缺失信息，并与无监督学习算法一起挖掘无标签对话中的有用线索，从而提升主题分割的准确性。实验结果显示，在 DialSeg711 数据集上，绝对错误分数（absolute error score）和 WD 指标分别改善约 6%，达到 11.42% 和 12.97%；在 Doc2Dial 数据集上，分别改善约 3% 和 2%，实现 SOTA（State-of-the-Art）水平，分别为 35.17% 和 38.49%。这一发现突出了 UR-DTS 在捕捉对话主题细微差别方面的有效性，并展示了利用无标签对话的潜力与挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "in Chinese language",
      "pdf_url": "http://arxiv.org/pdf/2409.07672v1",
      "published_date": "2024-09-12 00:27:31 UTC",
      "updated_date": "2024-09-12 00:27:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:32:42.795911"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 83,
  "processed_papers_count": 83,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T23:33:04.749129"
}
[
  {
    "arxiv_id": "2409.11426v1",
    "title": "Towards Opinion Shaping: A Deep Reinforcement Learning Approach in Bot-User Interactions",
    "authors": [
      "Farbod Siahkali",
      "Saba Samadi",
      "Hamed Kebriaei"
    ],
    "abstract": "This paper aims to investigate the impact of interference in social network\nalgorithms via user-bot interactions, focusing on the Stochastic Bounded\nConfidence Model (SBCM). This paper explores two approaches: positioning bots\ncontrolled by agents into the network and targeted advertising under various\ncircumstances, operating with an advertising budget. This study integrates the\nDeep Deterministic Policy Gradient (DDPG) algorithm and its variants to\nexperiment with different Deep Reinforcement Learning (DRL). Finally,\nexperimental results demonstrate that this approach can result in efficient\nopinion shaping, indicating its potential in deploying advertising resources on\nsocial platforms.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "I.2.8; I.2.6"
    ],
    "primary_category": "cs.SI",
    "comment": "5 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.11426v1",
    "published_date": "2024-09-12 23:39:07 UTC",
    "updated_date": "2024-09-12 23:39:07 UTC"
  },
  {
    "arxiv_id": "2410.00004v2",
    "title": "Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy Similarity Searches and Domain Shift Generalization",
    "authors": [
      "Gentiana Rashiti",
      "Geethan Karunaratne",
      "Mrinmaya Sachan",
      "Abu Sebastian",
      "Abbas Rahimi"
    ],
    "abstract": "The retrieval augmented generation (RAG) system such as Retro has been shown\nto improve language modeling capabilities and reduce toxicity and\nhallucinations by retrieving from a database of non-parametric memory\ncontaining trillions of entries. We introduce Retro-li that shows retrieval can\nalso help using a small-scale database, but it demands more accurate and better\nneighbors when searching in a smaller hence sparser non-parametric memory. This\ncan be met by using a proper semantic similarity search. We further propose\nadding a regularization to the non-parametric memory for the first time: it\nsignificantly reduces perplexity when the neighbor search operations are noisy\nduring inference, and it improves generalization when a domain shift occurs. We\nalso show that Retro-li's non-parametric memory can potentially be implemented\non analog in-memory computing hardware, exhibiting O(1) search time while\ncausing noise in retrieving neighbors, with minimal (<1%) performance loss. Our\ncode is available at:\nhttps://github.com/IBM/Retrieval-Enhanced-Transformer-Little.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00004v2",
    "published_date": "2024-09-12 23:29:33 UTC",
    "updated_date": "2025-03-26 10:27:15 UTC"
  },
  {
    "arxiv_id": "2409.08422v3",
    "title": "Fitted Q-Iteration via Max-Plus-Linear Approximation",
    "authors": [
      "Y. Liu",
      "M. A. S. Kolarijani"
    ],
    "abstract": "In this study, we consider the application of max-plus-linear approximators\nfor Q-function in offline reinforcement learning of discounted Markov decision\nprocesses. In particular, we incorporate these approximators to propose novel\nfitted Q-iteration (FQI) algorithms with provable convergence. Exploiting the\ncompatibility of the Bellman operator with max-plus operations, we show that\nthe max-plus-linear regression within each iteration of the proposed FQI\nalgorithm reduces to simple max-plus matrix-vector multiplications. We also\nconsider the variational implementation of the proposed algorithm which leads\nto a per-iteration complexity that is independent of the number of samples.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08422v3",
    "published_date": "2024-09-12 22:51:08 UTC",
    "updated_date": "2025-03-07 21:19:24 UTC"
  },
  {
    "arxiv_id": "2410.14680v1",
    "title": "Influence of Backdoor Paths on Causal Link Prediction",
    "authors": [
      "Utkarshani Jaimini",
      "Cory Henson",
      "Amit Sheth"
    ],
    "abstract": "The current method for predicting causal links in knowledge graphs uses\nweighted causal relations. For a given link between cause-effect entities, the\npresence of a confounder affects the causal link prediction, which can lead to\nspurious and inaccurate results. We aim to block these confounders using\nbackdoor path adjustment. Backdoor paths are non-causal association flows that\nconnect the \\textit{cause-entity} to the \\textit{effect-entity} through other\nvariables. Removing these paths ensures a more accurate prediction of causal\nlinks. This paper proposes CausalLPBack, a novel approach to causal link\nprediction that eliminates backdoor paths and uses knowledge graph link\nprediction methods. It extends the representation of causality in a\nneuro-symbolic framework, enabling the adoption and use of traditional causal\nAI concepts and methods. We demonstrate our approach using a causal reasoning\nbenchmark dataset of simulated videos. The evaluation involves a unique dataset\nsplitting method called the Markov-based split that's relevant for causal link\nprediction. The evaluation of the proposed approach demonstrates atleast 30\\%\nin MRR and 16\\% in Hits@K inflated performance for causal link prediction that\nis due to the bias introduced by backdoor paths for both baseline and weighted\ncausal relations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14680v1",
    "published_date": "2024-09-12 22:16:36 UTC",
    "updated_date": "2024-09-12 22:16:36 UTC"
  },
  {
    "arxiv_id": "2409.08406v2",
    "title": "Knowledge Tagging with Large Language Model based Multi-Agent System",
    "authors": [
      "Hang Li",
      "Tianlong Xu",
      "Ethan Chang",
      "Qingsong Wen"
    ],
    "abstract": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI 2025 (AAAI/IAAI 2025 Innovative Application Award)",
    "pdf_url": "http://arxiv.org/pdf/2409.08406v2",
    "published_date": "2024-09-12 21:39:01 UTC",
    "updated_date": "2024-12-19 16:09:47 UTC"
  },
  {
    "arxiv_id": "2409.08400v1",
    "title": "Scores as Actions: a framework of fine-tuning diffusion models by continuous-time reinforcement learning",
    "authors": [
      "Hanyang Zhao",
      "Haoxian Chen",
      "Ji Zhang",
      "David D. Yao",
      "Wenpin Tang"
    ],
    "abstract": "Reinforcement Learning from human feedback (RLHF) has been shown a promising\ndirection for aligning generative models with human intent and has also been\nexplored in recent works for alignment of diffusion generative models. In this\nwork, we provide a rigorous treatment by formulating the task of fine-tuning\ndiffusion models, with reward functions learned from human feedback, as an\nexploratory continuous-time stochastic control problem. Our key idea lies in\ntreating the score-matching functions as controls/actions, and upon this, we\ndevelop a unified framework from a continuous-time perspective, to employ\nreinforcement learning (RL) algorithms in terms of improving the generation\nquality of diffusion models. We also develop the corresponding continuous-time\nRL theory for policy optimization and regularization under assumptions of\nstochastic different equations driven environment. Experiments on the\ntext-to-image (T2I) generation will be reported in the accompanied paper.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08400v1",
    "published_date": "2024-09-12 21:12:21 UTC",
    "updated_date": "2024-09-12 21:12:21 UTC"
  },
  {
    "arxiv_id": "2410.14679v1",
    "title": "HyperCausalLP: Causal Link Prediction using Hyper-Relational Knowledge Graph",
    "authors": [
      "Utkarshani Jaimini",
      "Cory Henson",
      "Amit Sheth"
    ],
    "abstract": "Causal networks are often incomplete with missing causal links. This is due\nto various issues, such as missing observation data. Recent approaches to the\nissue of incomplete causal networks have used knowledge graph link prediction\nmethods to find the missing links. In the causal link A causes B causes C, the\ninfluence of A to C is influenced by B which is known as a mediator. Existing\napproaches using knowledge graph link prediction do not consider these mediated\ncausal links. This paper presents HyperCausalLP, an approach designed to find\nmissing causal links within a causal network with the help of mediator links.\nThe problem of missing links is formulated as a hyper-relational knowledge\ngraph completion. The approach uses a knowledge graph link prediction model\ntrained on a hyper-relational knowledge graph with the mediators. The approach\nis evaluated on a causal benchmark dataset, CLEVRER-Humans. Results show that\nthe inclusion of knowledge about mediators in causal link prediction using\nhyper-relational knowledge graph improves the performance on an average by\n5.94% mean reciprocal rank.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2405.02327",
    "pdf_url": "http://arxiv.org/pdf/2410.14679v1",
    "published_date": "2024-09-12 21:01:30 UTC",
    "updated_date": "2024-09-12 21:01:30 UTC"
  },
  {
    "arxiv_id": "2409.08397v1",
    "title": "360PanT: Training-Free Text-Driven 360-Degree Panorama-to-Panorama Translation",
    "authors": [
      "Hai Wang",
      "Jing-Hao Xue"
    ],
    "abstract": "Preserving boundary continuity in the translation of 360-degree panoramas\nremains a significant challenge for existing text-driven image-to-image\ntranslation methods. These methods often produce visually jarring\ndiscontinuities at the translated panorama's boundaries, disrupting the\nimmersive experience. To address this issue, we propose 360PanT, a\ntraining-free approach to text-based 360-degree panorama-to-panorama\ntranslation with boundary continuity. Our 360PanT achieves seamless\ntranslations through two key components: boundary continuity encoding and\nseamless tiling translation with spatial control. Firstly, the boundary\ncontinuity encoding embeds critical boundary continuity information of the\ninput 360-degree panorama into the noisy latent representation by constructing\nan extended input image. Secondly, leveraging this embedded noisy latent\nrepresentation and guided by a target prompt, the seamless tiling translation\nwith spatial control enables the generation of a translated image with\nidentical left and right halves while adhering to the extended input's\nstructure and semantic layout. This process ensures a final translated\n360-degree panorama with seamless boundary continuity. Experimental results on\nboth real-world and synthesized datasets demonstrate the effectiveness of our\n360PanT in translating 360-degree panoramas. Code is available at\n\\href{https://github.com/littlewhitesea/360PanT}{https://github.com/littlewhitesea/360PanT}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by WACV 2025, Project Page:\n  \\href{https://littlewhitesea.github.io/360PanT.github.io/}{https://littlewhitesea.github.io/360PanT.github.io/}",
    "pdf_url": "http://arxiv.org/pdf/2409.08397v1",
    "published_date": "2024-09-12 20:56:16 UTC",
    "updated_date": "2024-09-12 20:56:16 UTC"
  },
  {
    "arxiv_id": "2409.08386v1",
    "title": "Self-Supervised Inference of Agents in Trustless Environments",
    "authors": [
      "Vladyslav Larin",
      "Ivan Nikitin",
      "Alexander Firsov"
    ],
    "abstract": "In this paper, we propose a novel approach where agents can form swarms to\nproduce high-quality responses effectively. This is accomplished by utilizing\nagents capable of data inference and ranking, which can be effectively\nimplemented using LLMs as response classifiers. We assess existing approaches\nfor trustless agent inference, define our methodology, estimate practical\nparameters, and model various types of malicious agent attacks. Our method\nleverages the collective intelligence of swarms, ensuring robust and efficient\ndecentralized AI inference with better accuracy, security, and reliability. We\nshow that our approach is an order of magnitude faster than other trustless\ninference strategies reaching less than 125 ms validation latency.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08386v1",
    "published_date": "2024-09-12 20:32:07 UTC",
    "updated_date": "2024-09-12 20:32:07 UTC"
  },
  {
    "arxiv_id": "2409.08381v1",
    "title": "Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations",
    "authors": [
      "Samyak Rawlekar",
      "Shubhang Bhatnagar",
      "Narendra Ahuja"
    ],
    "abstract": "Vision-language models (VLMs) like CLIP have been adapted for Multi-Label\nRecognition (MLR) with partial annotations by leveraging prompt-learning, where\npositive and negative prompts are learned for each class to associate their\nembeddings with class presence or absence in the shared vision-text feature\nspace. While this approach improves MLR performance by relying on VLM priors,\nwe hypothesize that learning negative prompts may be suboptimal, as the\ndatasets used to train VLMs lack image-caption pairs explicitly focusing on\nclass absence. To analyze the impact of positive and negative prompt learning\non MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is\nlearned with VLM guidance while the other is replaced by an embedding vector\nlearned directly in the shared feature space without relying on the text\nencoder. Through empirical analysis, we observe that negative prompts degrade\nMLR performance, and learning only positive prompts, combined with learned\nnegative embeddings (PositiveCoOp), outperforms dual prompt learning\napproaches. Moreover, we quantify the performance benefits that prompt-learning\noffers over a simple vision-features-only baseline, observing that the baseline\ndisplays strong performance comparable to dual prompt learning approach\n(DualCoOp), when the proportion of missing labels is low, while requiring half\nthe training compute and 16 times fewer parameters",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08381v1",
    "published_date": "2024-09-12 20:02:51 UTC",
    "updated_date": "2024-09-12 20:02:51 UTC"
  },
  {
    "arxiv_id": "2409.08379v2",
    "title": "The Impact of Large Language Models on Open-source Innovation: Evidence from GitHub Copilot",
    "authors": [
      "Doron Yeverechyahu",
      "Raveesh Mayya",
      "Gal Oestreicher-Singer"
    ],
    "abstract": "Large Language Models (LLMs) have been shown to enhance individual\nproductivity in guided settings. Whereas LLMs are likely to also transform\ninnovation processes in a collaborative work setting, it is unclear what\ntrajectory this transformation will follow. Innovation in these contexts\nencompasses both capability innovation that explores new possibilities by\nacquiring new competencies in a project and iterative innovation that exploits\nexisting foundations by enhancing established competencies and improving\nproject quality. Whether LLMs affect these two aspects of collaborative work\nand to what extent is an open empirical question. Open-source development\nprovides an ideal setting to examine LLM impacts on these innovation types, as\nits voluntary and open/collaborative nature of contributions provides the\ngreatest opportunity for technological augmentation. We focus on open-source\nprojects on GitHub by leveraging a natural experiment around the selective\nrollout of GitHub Copilot (a programming-focused LLM) in October 2021, where\nGitHub Copilot selectively supported programming languages like Python or Rust,\nbut not R or Haskell. We observe a significant jump in overall contributions,\nsuggesting that LLMs effectively augment collaborative innovation in an\nunguided setting. Interestingly, Copilot's launch increased iterative\ninnovation focused on maintenance-related or feature-refining contributions\nsignificantly more than it did capability innovation through code-development\nor feature-introducing commits. This disparity was more pronounced after the\nmodel upgrade in June 2022 and was evident in active projects with extensive\ncoding activity, suggesting that as both LLM capabilities and/or available\ncontextual information improve, the gap between capability and iterative\ninnovation may widen. We discuss practical and policy implications to\nincentivize high-value innovative solutions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "econ.GN",
      "q-fin.EC",
      "I.2.7; D.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "JEL Classification: O31, C88, J24, O35, L86",
    "pdf_url": "http://arxiv.org/pdf/2409.08379v2",
    "published_date": "2024-09-12 19:59:54 UTC",
    "updated_date": "2025-05-13 16:08:10 UTC"
  },
  {
    "arxiv_id": "2409.08372v2",
    "title": "FedProphet: Memory-Efficient Federated Adversarial Training via Robust and Consistent Cascade Learning",
    "authors": [
      "Minxue Tang",
      "Yitu Wang",
      "Jingyang Zhang",
      "Louis DiValentin",
      "Aolin Ding",
      "Amin Hass",
      "Yiran Chen",
      "Hai \"Helen\" Li"
    ],
    "abstract": "Federated Adversarial Training (FAT) can supplement robustness against\nadversarial examples to Federated Learning (FL), promoting a meaningful step\ntoward trustworthy AI. However, FAT requires large models to preserve high\naccuracy while achieving strong robustness, incurring high memory-swapping\nlatency when training on memory-constrained edge devices. Existing\nmemory-efficient FL methods suffer from poor accuracy and weak robustness due\nto inconsistent local and global models. In this paper, we propose FedProphet,\na novel FAT framework that can achieve memory efficiency, robustness, and\nconsistency simultaneously. FedProphget reduces the memory requirement in local\ntraining while guaranteeing adversarial robustness by adversarial cascade\nlearning with strong convexity regularization, and we show that the strong\nrobustness also implies low inconsistency in FedProphet. We also develop a\ntraining coordinator on the server of FL, with Adaptive Perturbation Adjustment\nfor utility-robustness balance and Differentiated Module Assignment for\nobjective inconsistency mitigation. FedPeophet significantly outperforms other\nbaselines under different experimental settings, maintaining the accuracy and\nrobustness of end-to-end FAT with 80% memory reduction and up to 10.8x speedup\nin training time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by MLSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.08372v2",
    "published_date": "2024-09-12 19:39:14 UTC",
    "updated_date": "2025-04-14 18:20:43 UTC"
  },
  {
    "arxiv_id": "2409.08369v1",
    "title": "E-QUARTIC: Energy Efficient Edge Ensemble of Convolutional Neural Networks for Resource-Optimized Learning",
    "authors": [
      "Le Zhang",
      "Onat Gungor",
      "Flavio Ponzina",
      "Tajana Rosing"
    ],
    "abstract": "Ensemble learning is a meta-learning approach that combines the predictions\nof multiple learners, demonstrating improved accuracy and robustness.\nNevertheless, ensembling models like Convolutional Neural Networks (CNNs)\nresult in high memory and computing overhead, preventing their deployment in\nembedded systems. These devices are usually equipped with small batteries that\nprovide power supply and might include energy-harvesting modules that extract\nenergy from the environment. In this work, we propose E-QUARTIC, a novel Energy\nEfficient Edge Ensembling framework to build ensembles of CNNs targeting\nArtificial Intelligence (AI)-based embedded systems. Our design outperforms\nsingle-instance CNN baselines and state-of-the-art edge AI solutions, improving\naccuracy and adapting to varying energy conditions while maintaining similar\nmemory requirements. Then, we leverage the multi-CNN structure of the designed\nensemble to implement an energy-aware model selection policy in\nenergy-harvesting AI systems. We show that our solution outperforms the\nstate-of-the-art by reducing system failure rate by up to 40% while ensuring\nhigher average output qualities. Ultimately, we show that the proposed design\nenables concurrent on-device training and high-quality inference execution at\nthe edge, limiting the performance and energy overheads to less than 0.04%.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.ET",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by the 30th Asia and South Pacific Design Automation\n  Conference (ASP-DAC 2025)",
    "pdf_url": "http://arxiv.org/pdf/2409.08369v1",
    "published_date": "2024-09-12 19:30:22 UTC",
    "updated_date": "2024-09-12 19:30:22 UTC"
  },
  {
    "arxiv_id": "2409.08357v2",
    "title": "An Experimental Study of Competitive Market Behavior Through LLMs",
    "authors": [
      "Jingru Jia",
      "Zehua Yuan"
    ],
    "abstract": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08357v2",
    "published_date": "2024-09-12 18:50:13 UTC",
    "updated_date": "2024-11-01 01:45:02 UTC"
  },
  {
    "arxiv_id": "2409.08351v1",
    "title": "Bayesian Inverse Graphics for Few-Shot Concept Learning",
    "authors": [
      "Octavio Arriaga",
      "Jichen Guo",
      "Rebecca Adam",
      "Sebastian Houben",
      "Frank Kirchner"
    ],
    "abstract": "Humans excel at building generalizations of new concepts from just one single\nexample. Contrary to this, current computer vision models typically require\nlarge amount of training samples to achieve a comparable accuracy. In this work\nwe present a Bayesian model of perception that learns using only minimal data,\na prototypical probabilistic program of an object. Specifically, we propose a\ngenerative inverse graphics model of primitive shapes, to infer posterior\ndistributions over physically consistent parameters from one or several images.\nWe show how this representation can be used for downstream tasks such as\nfew-shot classification and pose estimation. Our model outperforms existing\nfew-shot neural-only classification algorithms and demonstrates generalization\nacross varying lighting conditions, backgrounds, and out-of-distribution\nshapes. By design, our model is uncertainty-aware and uses our new\ndifferentiable renderer for optimizing global scene parameters through gradient\ndescent, sampling posterior distributions over object parameters with Markov\nChain Monte Carlo (MCMC), and using a neural based likelihood function.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08351v1",
    "published_date": "2024-09-12 18:30:41 UTC",
    "updated_date": "2024-09-12 18:30:41 UTC"
  },
  {
    "arxiv_id": "2409.08346v1",
    "title": "Towards Quantifying and Reducing Language Mismatch Effects in Cross-Lingual Speech Anti-Spoofing",
    "authors": [
      "Tianchi Liu",
      "Ivan Kukanov",
      "Zihan Pan",
      "Qiongqiong Wang",
      "Hardik B. Sailor",
      "Kong Aik Lee"
    ],
    "abstract": "The effects of language mismatch impact speech anti-spoofing systems, while\ninvestigations and quantification of these effects remain limited. Existing\nanti-spoofing datasets are mainly in English, and the high cost of acquiring\nmultilingual datasets hinders training language-independent models. We initiate\nthis work by evaluating top-performing speech anti-spoofing systems that are\ntrained on English data but tested on other languages, observing notable\nperformance declines. We propose an innovative approach - Accent-based data\nexpansion via TTS (ACCENT), which introduces diverse linguistic knowledge to\nmonolingual-trained models, improving their cross-lingual capabilities. We\nconduct experiments on a large-scale dataset consisting of over 3 million\nsamples, including 1.8 million training samples and nearly 1.2 million testing\nsamples across 12 languages. The language mismatch effects are preliminarily\nquantified and remarkably reduced over 15% by applying the proposed ACCENT.\nThis easily implementable method shows promise for multilingual and\nlow-resource language scenarios.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to the IEEE Spoken Language Technology Workshop (SLT) 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.08346v1",
    "published_date": "2024-09-12 18:18:22 UTC",
    "updated_date": "2024-09-12 18:18:22 UTC"
  },
  {
    "arxiv_id": "2409.08276v3",
    "title": "AnySkin: Plug-and-play Skin Sensing for Robotic Touch",
    "authors": [
      "Raunaq Bhirangi",
      "Venkatesh Pattabiraman",
      "Enes Erciyes",
      "Yifeng Cao",
      "Tess Hellebrekers",
      "Lerrel Pinto"
    ],
    "abstract": "While tactile sensing is widely accepted as an important and useful sensing\nmodality, its use pales in comparison to other sensory modalities like vision\nand proprioception. AnySkin addresses the critical challenges that impede the\nuse of tactile sensing -- versatility, replaceability, and data reusability.\nBuilding on the simplistic design of ReSkin, and decoupling the sensing\nelectronics from the sensing interface, AnySkin simplifies integration making\nit as straightforward as putting on a phone case and connecting a charger.\nFurthermore, AnySkin is the first uncalibrated tactile-sensor with\ncross-instance generalizability of learned manipulation policies. To summarize,\nthis work makes three key contributions: first, we introduce a streamlined\nfabrication process and a design tool for creating an adhesive-free, durable\nand easily replaceable magnetic tactile sensor; second, we characterize slip\ndetection and policy learning with the AnySkin sensor; and third, we\ndemonstrate zero-shot generalization of models trained on one instance of\nAnySkin to new instances, and compare it with popular existing tactile\nsolutions like DIGIT and ReSkin. Videos of experiments, fabrication details and\ndesign files can be found on https://any-skin.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08276v3",
    "published_date": "2024-09-12 17:59:44 UTC",
    "updated_date": "2024-09-27 16:09:13 UTC"
  },
  {
    "arxiv_id": "2409.08273v1",
    "title": "Hand-Object Interaction Pretraining from Videos",
    "authors": [
      "Himanshu Gaurav Singh",
      "Antonio Loquercio",
      "Carmelo Sferrazza",
      "Jane Wu",
      "Haozhi Qi",
      "Pieter Abbeel",
      "Jitendra Malik"
    ],
    "abstract": "We present an approach to learn general robot manipulation priors from 3D\nhand-object interaction trajectories. We build a framework to use in-the-wild\nvideos to generate sensorimotor robot trajectories. We do so by lifting both\nthe human hand and the manipulated object in a shared 3D space and retargeting\nhuman motions to robot actions. Generative modeling on this data gives us a\ntask-agnostic base policy. This policy captures a general yet flexible\nmanipulation prior. We empirically demonstrate that finetuning this policy,\nwith both reinforcement learning (RL) and behavior cloning (BC), enables\nsample-efficient adaptation to downstream tasks and simultaneously improves\nrobustness and generalizability compared to prior approaches. Qualitative\nexperiments are available at: \\url{https://hgaurav2k.github.io/hop/}.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08273v1",
    "published_date": "2024-09-12 17:59:07 UTC",
    "updated_date": "2024-09-12 17:59:07 UTC"
  },
  {
    "arxiv_id": "2409.08270v1",
    "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian\nSplatting from 2D masks. Conventional methods often rely on iterative gradient\ndescent to assign each Gaussian a unique label, leading to lengthy optimization\nand sub-optimal solutions. Instead, we propose a straightforward yet globally\noptimal solver for 3D-GS segmentation. The core insight of our method is that,\nwith a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially\na linear function with respect to the labels of each Gaussian. As such, the\noptimal label assignment can be solved via linear programming in closed form.\nThis solution capitalizes on the alpha blending characteristic of the splatting\nprocess for single step optimization. By incorporating the background bias in\nour objective function, our method shows superior robustness in 3D segmentation\nagainst noises. Remarkably, our optimization completes within 30 seconds, about\n50$\\times$ faster than the best existing methods. Extensive experiments\ndemonstrate the efficiency and robustness of our method in segmenting various\nscenes, and its superior performance in downstream tasks such as object removal\nand inpainting. Demos and code will be available at\nhttps://github.com/florinshen/FlashSplat.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV'2024",
    "pdf_url": "http://arxiv.org/pdf/2409.08270v1",
    "published_date": "2024-09-12 17:58:13 UTC",
    "updated_date": "2024-09-12 17:58:13 UTC"
  },
  {
    "arxiv_id": "2409.08264v2",
    "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
    "authors": [
      "Rogerio Bonatti",
      "Dan Zhao",
      "Francesco Bonacci",
      "Dillon Dupont",
      "Sara Abdali",
      "Yinheng Li",
      "Yadong Lu",
      "Justin Wagle",
      "Kazuhito Koishida",
      "Arthur Bucker",
      "Lawrence Jang",
      "Zack Hui"
    ],
    "abstract": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08264v2",
    "published_date": "2024-09-12 17:56:43 UTC",
    "updated_date": "2024-09-13 20:17:13 UTC"
  },
  {
    "arxiv_id": "2409.08255v1",
    "title": "LoRID: Low-Rank Iterative Diffusion for Adversarial Purification",
    "authors": [
      "Geigh Zollicoffer",
      "Minh Vu",
      "Ben Nebgen",
      "Juan Castorena",
      "Boian Alexandrov",
      "Manish Bhattarai"
    ],
    "abstract": "This work presents an information-theoretic examination of diffusion-based\npurification methods, the state-of-the-art adversarial defenses that utilize\ndiffusion models to remove malicious perturbations in adversarial examples. By\ntheoretically characterizing the inherent purification errors associated with\nthe Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank\nIterative Diffusion purification method designed to remove adversarial\nperturbation with low intrinsic purification errors. LoRID centers around a\nmulti-stage purification process that leverages multiple rounds of\ndiffusion-denoising loops at the early time-steps of the diffusion models, and\nthe integration of Tucker decomposition, an extension of matrix factorization,\nto remove adversarial noise at high-noise regimes. Consequently, LoRID\nincreases the effective diffusion time-steps and overcomes strong adversarial\nattacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ,\nand ImageNet datasets under both white-box and black-box settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "LA-UR-24-28834",
    "pdf_url": "http://arxiv.org/pdf/2409.08255v1",
    "published_date": "2024-09-12 17:51:25 UTC",
    "updated_date": "2024-09-12 17:51:25 UTC"
  },
  {
    "arxiv_id": "2409.08250v2",
    "title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering",
    "authors": [
      "Jiahao Nick Li",
      "Zhuohao Jerry Zhang",
      "Jiaju Ma"
    ],
    "abstract": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nonly support retrieving individual pieces of information like certain objects\nin photos, and struggle with answering more complex queries that involve\ninterpreting interconnected memories like sequential events. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments individual captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories. Given a question, OmniQuery retrieves relevant augmented memories and\nuses a large language model (LLM) to generate answers with references. In human\nevaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%,\noutperforming a conventional RAG system by winning or tying for 74.5% of the\ntime.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Paper accepted to the 2025 CHI Conference on Human Factors in\n  Computing Systems (CHI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2409.08250v2",
    "published_date": "2024-09-12 17:48:08 UTC",
    "updated_date": "2025-02-21 02:46:25 UTC"
  },
  {
    "arxiv_id": "2409.08240v3",
    "title": "IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation",
    "authors": [
      "Yinwei Wu",
      "Xianpan Zhou",
      "Bing Ma",
      "Xuefeng Su",
      "Kai Ma",
      "Xinchao Wang"
    ],
    "abstract": "While Text-to-Image (T2I) diffusion models excel at generating visually\nappealing images of individual instances, they struggle to accurately position\nand control the features generation of multiple instances. The Layout-to-Image\n(L2I) task was introduced to address the positioning challenges by\nincorporating bounding boxes as spatial control signals, but it still falls\nshort in generating precise instance features. In response, we propose the\nInstance Feature Generation (IFG) task, which aims to ensure both positional\naccuracy and feature fidelity in generated instances. To address the IFG task,\nwe introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances\nfeature depiction by incorporating additional appearance tokens and utilizing\nan Instance Semantic Map to align instance-level features with spatial\nlocations. The IFAdapter guides the diffusion process as a plug-and-play\nmodule, making it adaptable to various community models. For evaluation, we\ncontribute an IFG benchmark and develop a verification pipeline to objectively\ncompare models' abilities to generate instances with accurate positioning and\nfeatures. Experimental results demonstrate that IFAdapter outperforms other\nmodels in both quantitative and qualitative evaluations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08240v3",
    "published_date": "2024-09-12 17:39:23 UTC",
    "updated_date": "2024-11-06 13:03:20 UTC"
  },
  {
    "arxiv_id": "2409.08239v1",
    "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
    "authors": [
      "Alisia Lupidi",
      "Carlos Gemmell",
      "Nicola Cancedda",
      "Jane Dwivedi-Yu",
      "Jason Weston",
      "Jakob Foerster",
      "Roberta Raileanu",
      "Maria Lomeli"
    ],
    "abstract": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08239v1",
    "published_date": "2024-09-12 17:39:08 UTC",
    "updated_date": "2024-09-12 17:39:08 UTC"
  },
  {
    "arxiv_id": "2409.08234v2",
    "title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems",
    "authors": [
      "Hakan T. Otal",
      "M. Abdullah Canbaz"
    ],
    "abstract": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NI",
      "68T50, 68M10",
      "I.2.7; D.4.6; K.6.5"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.08234v2",
    "published_date": "2024-09-12 17:33:06 UTC",
    "updated_date": "2024-09-15 14:41:42 UTC"
  },
  {
    "arxiv_id": "2409.08231v1",
    "title": "Design Optimization of Nuclear Fusion Reactor through Deep Reinforcement Learning",
    "authors": [
      "Jinsu Kim",
      "Jaemin Seo"
    ],
    "abstract": "This research explores the application of Deep Reinforcement Learning (DRL)\nto optimize the design of a nuclear fusion reactor. DRL can efficiently address\nthe challenging issues attributed to multiple physics and engineering\nconstraints for steady-state operation. The fusion reactor design computation\nand the optimization code applicable to parallelization with DRL are developed.\nThe proposed framework enables finding the optimal reactor design that\nsatisfies the operational requirements while reducing building costs.\nMulti-objective design optimization for a fusion reactor is now simplified by\nDRL, indicating the high potential of the proposed framework for advancing the\nefficient and sustainable design of future reactors.",
    "categories": [
      "physics.plasm-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.plasm-ph",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.08231v1",
    "published_date": "2024-09-12 17:23:01 UTC",
    "updated_date": "2024-09-12 17:23:01 UTC"
  },
  {
    "arxiv_id": "2409.08229v1",
    "title": "Photonic Quantum Computers",
    "authors": [
      "M. AbuGhanem"
    ],
    "abstract": "In the pursuit of scalable and fault-tolerant quantum computing\narchitectures, photonic-based quantum computers have emerged as a leading\nfrontier. This article provides a comprehensive overview of advancements in\nphotonic quantum computing, developed by leading industry players, examining\ncurrent performance, architectural designs, and strategies for developing\nlarge-scale, fault-tolerant photonic quantum computers. It also highlights\nrecent groundbreaking experiments that leverage the unique advantages of\nphotonic technologies, underscoring their transformative potential. This review\ncaptures a pivotal moment of photonic quantum computing in the noisy\nintermediate-scale quantum (NISQ) era, offering insights into how photonic\nquantum computers might reshape the future of quantum computing.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "quant-ph",
    "comment": "47 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.08229v1",
    "published_date": "2024-09-12 17:16:38 UTC",
    "updated_date": "2024-09-12 17:16:38 UTC"
  },
  {
    "arxiv_id": "2409.13748v1",
    "title": "TheraGen: Therapy for Every Generation",
    "authors": [
      "Kartikey Doshi",
      "Jimit Shah",
      "Narendra Shekokar"
    ],
    "abstract": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13748v1",
    "published_date": "2024-09-12 17:15:44 UTC",
    "updated_date": "2024-09-12 17:15:44 UTC"
  },
  {
    "arxiv_id": "2409.08217v2",
    "title": "CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs",
    "authors": [
      "Davide Buffelli",
      "Farzin Soleymani",
      "Bastian Rieck"
    ],
    "abstract": "Graph neural networks have become the default choice by practitioners for\ngraph learning tasks such as graph classification and node classification.\nNevertheless, popular graph neural network models still struggle to capture\nhigher-order information, i.e., information that goes \\emph{beyond} pairwise\ninteractions. Recent work has shown that persistent homology, a tool from\ntopological data analysis, can enrich graph neural networks with topological\ninformation that they otherwise could not capture. Calculating such features is\nefficient for dimension 0 (connected components) and dimension 1 (cycles).\nHowever, when it comes to higher-order structures, it does not scale well, with\na complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order\nof the structures. In this work, we introduce a novel method that extracts\ninformation about higher-order structures in the graph while still using the\nefficient low-dimensional persistent homology algorithm. On standard benchmark\ndatasets, we show that our method can lead to up to $31\\%$ improvements in test\naccuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Proceedings of the Third Learning on Graphs Conference\n  (LoG 2024), PMLR 269",
    "pdf_url": "http://arxiv.org/pdf/2409.08217v2",
    "published_date": "2024-09-12 16:56:26 UTC",
    "updated_date": "2024-11-26 18:01:01 UTC"
  },
  {
    "arxiv_id": "2409.08215v2",
    "title": "LT3SD: Latent Trees for 3D Scene Diffusion",
    "authors": [
      "Quan Meng",
      "Lei Li",
      "Matthias Niener",
      "Angela Dai"
    ],
    "abstract": "We present LT3SD, a novel latent diffusion model for large-scale 3D scene\ngeneration. Recent advances in diffusion models have shown impressive results\nin 3D object generation, but are limited in spatial extent and quality when\nextended to 3D scenes. To generate complex and diverse 3D scene structures, we\nintroduce a latent tree representation to effectively encode both\nlower-frequency geometry and higher-frequency detail in a coarse-to-fine\nhierarchy. We can then learn a generative diffusion process in this latent 3D\nscene space, modeling the latent components of a scene at each resolution\nlevel. To synthesize large-scale scenes with varying sizes, we train our\ndiffusion model on scene patches and synthesize arbitrary-sized output 3D\nscenes through shared diffusion generation across multiple scene patches.\nThrough extensive experiments, we demonstrate the efficacy and benefits of\nLT3SD for large-scale, high-quality unconditional 3D scene generation and for\nprobabilistic completion for partial scene observations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://quan-meng.github.io/projects/lt3sd/ Video:\n  https://youtu.be/AJ5sG9VyjGA",
    "pdf_url": "http://arxiv.org/pdf/2409.08215v2",
    "published_date": "2024-09-12 16:55:51 UTC",
    "updated_date": "2025-05-01 15:23:06 UTC"
  },
  {
    "arxiv_id": "2409.08202v2",
    "title": "What Makes a Maze Look Like a Maze?",
    "authors": [
      "Joy Hsu",
      "Jiayuan Mao",
      "Joshua B. Tenenbaum",
      "Noah D. Goodman",
      "Jiajun Wu"
    ],
    "abstract": "A unique aspect of human visual understanding is the ability to flexibly\ninterpret abstract concepts: acquiring lifted rules explaining what they\nsymbolize, grounding them across familiar and unfamiliar contexts, and making\npredictions or reasoning about them. While off-the-shelf vision-language models\nexcel at making literal interpretations of images (e.g., recognizing object\ncategories such as tree branches), they still struggle to make sense of such\nvisual abstractions (e.g., how an arrangement of tree branches may form the\nwalls of a maze). To address this challenge, we introduce Deep Schema Grounding\n(DSG), a framework that leverages explicit structured representations of visual\nabstractions for grounding and reasoning. At the core of DSG are\nschemas--dependency graph descriptions of abstract concepts that decompose them\ninto more primitive-level symbols. DSG uses large language models to extract\nschemas, then hierarchically grounds concrete to abstract components of the\nschema onto images with vision-language models. The grounded schema is used to\naugment visual abstraction understanding. We systematically evaluate DSG and\ndifferent methods in reasoning on our new Visual Abstractions Dataset, which\nconsists of diverse, real-world images of abstract concepts and corresponding\nquestion-answer pairs labeled by humans. We show that DSG significantly\nimproves the abstract visual reasoning performance of vision-language models,\nand is a step toward human-aligned understanding of visual abstractions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.08202v2",
    "published_date": "2024-09-12 16:41:47 UTC",
    "updated_date": "2025-02-17 23:45:08 UTC"
  },
  {
    "arxiv_id": "2409.08199v2",
    "title": "AudioBERT: Audio Knowledge Augmented Language Model",
    "authors": [
      "Hyunjong Ok",
      "Suho Yoo",
      "Jaeho Lee"
    ],
    "abstract": "Recent studies have identified that language models, pretrained on text-only\ndatasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of\neveryday objects. Motivated by this observation, we ask whether a similar\nshortcoming exists in terms of the \\textit{auditory} knowledge. To answer this\nquestion, we construct a new dataset called AuditoryBench, which consists of\ntwo novel tasks for evaluating auditory knowledge. Based on our analysis using\nthe benchmark, we find that language models also suffer from a severe lack of\nauditory knowledge. To address this limitation, we propose AudioBERT, a novel\nmethod to augment the auditory knowledge of BERT through a retrieval-based\napproach. First, we detect auditory knowledge spans in prompts to query our\nretrieval model efficiently. Then, we inject audio knowledge into BERT and\nswitch on low-rank adaptation for effective adaptation when audio knowledge is\nrequired. Our experiments demonstrate that AudioBERT is quite effective,\nachieving superior performance on the AuditoryBench. The dataset and code are\navailable at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 3 figures, ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.08199v2",
    "published_date": "2024-09-12 16:36:39 UTC",
    "updated_date": "2025-01-16 12:17:18 UTC"
  },
  {
    "arxiv_id": "2409.08185v1",
    "title": "Fine-tuning Large Language Models for Entity Matching",
    "authors": [
      "Aaron Steiner",
      "Ralph Peeters",
      "Christian Bizer"
    ],
    "abstract": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch",
    "pdf_url": "http://arxiv.org/pdf/2409.08185v1",
    "published_date": "2024-09-12 16:20:57 UTC",
    "updated_date": "2024-09-12 16:20:57 UTC"
  },
  {
    "arxiv_id": "2409.17166v1",
    "title": "ScriptSmith: A Unified LLM Framework for Enhancing IT Operations via Automated Bash Script Generation, Assessment, and Refinement",
    "authors": [
      "Oishik Chatterjee",
      "Pooja Aggarwal",
      "Suranjana Samanta",
      "Ting Dai",
      "Prateeti Mohapatra",
      "Debanjana Kar",
      "Ruchi Mahindru",
      "Steve Barbieri",
      "Eugen Postea",
      "Brad Blancett",
      "Arthur De Magalhaes"
    ],
    "abstract": "In the rapidly evolving landscape of site reliability engineering (SRE), the\ndemand for efficient and effective solutions to manage and resolve issues in\nsite and cloud applications is paramount. This paper presents an innovative\napproach to action automation using large language models (LLMs) for script\ngeneration, assessment, and refinement. By leveraging the capabilities of LLMs,\nwe aim to significantly reduce the human effort involved in writing and\ndebugging scripts, thereby enhancing the productivity of SRE teams. Our\nexperiments focus on Bash scripts, a commonly used tool in SRE, and involve the\nCodeSift dataset of 100 tasks and the InterCode dataset of 153 tasks. The\nresults show that LLMs can automatically assess and refine scripts efficiently,\nreducing the need for script validation in an execution environment. Results\ndemonstrate that the framework shows an overall improvement of 7-10% in script\ngeneration.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2409.17166v1",
    "published_date": "2024-09-12 15:11:43 UTC",
    "updated_date": "2024-09-12 15:11:43 UTC"
  },
  {
    "arxiv_id": "2409.08111v1",
    "title": "Towards a graph-based foundation model for network traffic analysis",
    "authors": [
      "Louis Van Langendonck",
      "Ismael Castell-Uroz",
      "Pere Barlet-Ros"
    ],
    "abstract": "Foundation models have shown great promise in various fields of study. A\npotential application of such models is in computer network traffic analysis,\nwhere these models can grasp the complexities of network traffic dynamics and\nadapt to any specific task or network environment with minimal fine-tuning.\nPrevious approaches have used tokenized hex-level packet data and the model\narchitecture of large language transformer models. We propose a new, efficient\ngraph-based alternative at the flow-level. Our approach represents network\ntraffic as a dynamic spatio-temporal graph, employing a self-supervised link\nprediction pretraining task to capture the spatial and temporal dynamics in\nthis network graph framework. To evaluate the effectiveness of our approach, we\nconduct a few-shot learning experiment for three distinct downstream network\ntasks: intrusion detection, traffic classification, and botnet classification.\nModels finetuned from our pretrained base achieve an average performance\nincrease of 6.87\\% over training from scratch, demonstrating their ability to\neffectively learn general network traffic dynamics during pretraining. This\nsuccess suggests the potential for a large-scale version to serve as an\noperational foundational model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "Pre-print of Accepted Workshop paper to 3rd GNNet, co-located with\n  CoNEXT'24",
    "pdf_url": "http://arxiv.org/pdf/2409.08111v1",
    "published_date": "2024-09-12 15:04:34 UTC",
    "updated_date": "2024-09-12 15:04:34 UTC"
  },
  {
    "arxiv_id": "2409.08098v3",
    "title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal",
    "authors": [
      "Huiyuan Xie",
      "Felix Steffek",
      "Joana Ribeiro de Faria",
      "Christine Carter",
      "Jonathan Rutherford"
    ],
    "abstract": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08098v3",
    "published_date": "2024-09-12 14:51:43 UTC",
    "updated_date": "2024-10-26 06:00:40 UTC"
  },
  {
    "arxiv_id": "2409.08069v1",
    "title": "TravelAgent: An AI Assistant for Personalized Travel Planning",
    "authors": [
      "Aili Chen",
      "Xuyang Ge",
      "Ziquan Fu",
      "Yanghua Xiao",
      "Jiangjie Chen"
    ],
    "abstract": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08069v1",
    "published_date": "2024-09-12 14:24:45 UTC",
    "updated_date": "2024-09-12 14:24:45 UTC"
  },
  {
    "arxiv_id": "2409.08065v3",
    "title": "InvDesFlow: An AI-driven materials inverse design workflow to explore possible high-temperature superconductors",
    "authors": [
      "Xiao-Qi Han",
      "Zhenfeng Ouyang",
      "Peng-Jie Guo",
      "Hao Sun",
      "Ze-Feng Gao",
      "Zhong-Yi Lu"
    ],
    "abstract": "The discovery of new superconducting materials, particularly those exhibiting\nhigh critical temperature ($T_c$), has been a vibrant area of study within the\nfield of condensed matter physics. Conventional approaches primarily rely on\nphysical intuition to search for potential superconductors within the existing\ndatabases. However, the known materials only scratch the surface of the\nextensive array of possibilities within the realm of materials. Here, we\ndevelop InvDesFlow, an AI search engine that integrates deep model pre-training\nand fine-tuning techniques, diffusion models, and physics-based approaches\n(e.g., first-principles electronic structure calculation) for the discovery of\nhigh-$T_c$ superconductors. Utilizing InvDesFlow, we have obtained 74\ndynamically stable materials with critical temperatures predicted by the AI\nmodel to be $T_c \\geq$ 15 K based on a very small set of samples. Notably,\nthese materials are not contained in any existing dataset. Furthermore, we\nanalyze trends in our dataset and individual materials including B$_4$CN$_3$\n(at 5 GPa) and B$_5$CN$_2$ (at ambient pressure) whose $T_c$s are 24.08 K and\n15.93 K, respectively. We demonstrate that AI technique can discover a set of\nnew high-$T_c$ superconductors, outline its potential for accelerating\ndiscovery of the materials with targeted properties.",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.supr-con",
    "comment": "22 pages, 17 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.08065v3",
    "published_date": "2024-09-12 14:16:56 UTC",
    "updated_date": "2025-05-13 08:22:00 UTC"
  },
  {
    "arxiv_id": "2409.18973v1",
    "title": "EEG-EMG FAConformer: Frequency Aware Conv-Transformer for the fusion of EEG and EMG",
    "authors": [
      "ZhengXiao He",
      "Minghong Cai",
      "Letian Li",
      "Siyuan Tian",
      "Ren-Jie Dai"
    ],
    "abstract": "Motor pattern recognition paradigms are the main forms of Brain-Computer\nInterfaces(BCI) aimed at motor function rehabilitation and are the most easily\npromoted applications. In recent years, many researchers have suggested\nencouraging patients to perform real motor control execution simultaneously in\nMI-based BCI rehabilitation training systems. Electromyography (EMG) signals\nare the most direct physiological signals that can assess the execution of\nmovements. Multimodal signal fusion is practically significant for decoding\nmotor patterns. Therefore, we introduce a multimodal motion pattern recognition\nalgorithm for EEG and EMG signals: EEG-EMG FAConformer, a method with several\nattention modules correlated with temporal and frequency information for motor\npattern recognition. We especially devise a frequency band attention module to\nencode EEG information accurately and efficiently. What's more, modules like\nMulti-Scale Fusion Module, Independent Channel-Specific Convolution\nModule(ICSCM), and Fuse Module which can effectively eliminate irrelevant\ninformation in EEG and EMG signals and fully exploit hidden dynamics are\ndeveloped and show great effects. Extensive experiments show that EEG-EMG\nFAConformer surpasses existing methods on Jeong2020 dataset, showcasing\noutstanding performance, high robustness and impressive stability.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18973v1",
    "published_date": "2024-09-12 14:08:56 UTC",
    "updated_date": "2024-09-12 14:08:56 UTC"
  },
  {
    "arxiv_id": "2409.08045v1",
    "title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking",
    "authors": [
      "Stav Cohen",
      "Ron Bitton",
      "Ben Nassi"
    ],
    "abstract": "In this paper, we show that with the ability to jailbreak a GenAI model,\nattackers can escalate the outcome of attacks against RAG-based GenAI-powered\napplications in severity and scale. In the first part of the paper, we show\nthat attackers can escalate RAG membership inference attacks and RAG entity\nextraction attacks to RAG documents extraction attacks, forcing a more severe\noutcome compared to existing attacks. We evaluate the results obtained from\nthree extraction methods, the influence of the type and the size of five\nembeddings algorithms employed, the size of the provided context, and the GenAI\nengine. We show that attackers can extract 80%-99.8% of the data stored in the\ndatabase used by the RAG of a Q&A chatbot. In the second part of the paper, we\nshow that attackers can escalate the scale of RAG data poisoning attacks from\ncompromising a single GenAI-powered application to compromising the entire\nGenAI ecosystem, forcing a greater scale of damage. This is done by crafting an\nadversarial self-replicating prompt that triggers a chain reaction of a\ncomputer worm within the ecosystem and forces each affected application to\nperform a malicious activity and compromise the RAG of additional applications.\nWe evaluate the performance of the worm in creating a chain of confidential\ndata extraction about users within a GenAI ecosystem of GenAI-powered email\nassistants and analyze how the performance of the worm is affected by the size\nof the context, the adversarial self-replicating prompt used, the type and size\nof the embeddings algorithm employed, and the number of hops in the\npropagation. Finally, we review and analyze guardrails to protect RAG-based\ninference and discuss the tradeoffs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "for Github, see\n  https://github.com/StavC/UnleashingWorms-ExtractingData . arXiv admin note:\n  substantial text overlap with arXiv:2403.02817",
    "pdf_url": "http://arxiv.org/pdf/2409.08045v1",
    "published_date": "2024-09-12 13:50:22 UTC",
    "updated_date": "2024-09-12 13:50:22 UTC"
  },
  {
    "arxiv_id": "2409.08023v2",
    "title": "Edge-Wise Graph-Instructed Neural Networks",
    "authors": [
      "Francesco Della Santa",
      "Antonio Mastropietro",
      "Sandra Pieraccini",
      "Francesco Vaccarino"
    ],
    "abstract": "The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over some graph-structured input data, like the ones inferred from\nthe Barabasi-Albert graph, and improve the training regularization on graphs\nwith chaotic connectivity, like the ones inferred from the Erdos-Renyi graph.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "05C21, 65D15, 68T07, 90C35"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08023v2",
    "published_date": "2024-09-12 13:05:28 UTC",
    "updated_date": "2025-01-08 12:40:56 UTC"
  },
  {
    "arxiv_id": "2409.08012v1",
    "title": "Learning Causally Invariant Reward Functions from Diverse Demonstrations",
    "authors": [
      "Ivan Ovinnikov",
      "Eugene Bykovets",
      "Joachim M. Buhmann"
    ],
    "abstract": "Inverse reinforcement learning methods aim to retrieve the reward function of\na Markov decision process based on a dataset of expert demonstrations. The\ncommonplace scarcity and heterogeneous sources of such demonstrations can lead\nto the absorption of spurious correlations in the data by the learned reward\nfunction. Consequently, this adaptation often exhibits behavioural overfitting\nto the expert data set when a policy is trained on the obtained reward function\nunder distribution shift of the environment dynamics. In this work, we explore\na novel regularization approach for inverse reinforcement learning methods\nbased on the causal invariance principle with the goal of improved reward\nfunction generalization. By applying this regularization to both exact and\napproximate formulations of the learning task, we demonstrate superior policy\nperformance when trained using the recovered reward functions in a transfer\nsetting",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08012v1",
    "published_date": "2024-09-12 12:56:24 UTC",
    "updated_date": "2024-09-12 12:56:24 UTC"
  },
  {
    "arxiv_id": "2409.07989v2",
    "title": "Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms",
    "authors": [
      "Fatemeh Askari",
      "Amirreza Fateh",
      "Mohammad Reza Mohammadi"
    ],
    "abstract": "In the context of few-shot classification, the goal is to train a classifier\nusing a limited number of samples while maintaining satisfactory performance.\nHowever, traditional metric-based methods exhibit certain limitations in\nachieving this objective. These methods typically rely on a single distance\nvalue between the query feature and support feature, thereby overlooking the\ncontribution of shallow features. To overcome this challenge, we propose a\nnovel approach in this paper. Our approach involves utilizing a multi-output\nembedding network that maps samples into distinct feature spaces. The proposed\nmethod extracts feature vectors at different stages, enabling the model to\ncapture both global and abstract features. By utilizing these diverse feature\nspaces, our model enhances its performance. Moreover, employing a\nself-attention mechanism improves the refinement of features at each stage,\nleading to even more robust representations and improved overall performance.\nFurthermore, assigning learnable weights to each stage significantly improved\nperformance and results. We conducted comprehensive evaluations on the\nMiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way\n5-shot scenarios. Additionally, we performed cross-domain tasks across eight\nbenchmark datasets, achieving high accuracy in the testing domains. These\nevaluations demonstrate the efficacy of our proposed method in comparison to\nstate-of-the-art approaches. https://github.com/FatemehAskari/MSENet",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07989v2",
    "published_date": "2024-09-12 12:34:29 UTC",
    "updated_date": "2025-01-16 14:01:58 UTC"
  },
  {
    "arxiv_id": "2409.07985v1",
    "title": "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols",
    "authors": [
      "Charlie Griffin",
      "Louis Thomson",
      "Buck Shlegeris",
      "Alessandro Abate"
    ],
    "abstract": "To evaluate the safety and usefulness of deployment protocols for untrusted\nAIs, AI Control uses a red-teaming exercise played between a protocol designer\nand an adversary. This paper introduces AI-Control Games, a formal\ndecision-making model of the red-teaming exercise as a multi-objective,\npartially observable, stochastic game. We also introduce methods for finding\noptimal protocols in AI-Control Games, by reducing them to a set of zero-sum\npartially observable stochastic games. We apply our formalism to model,\nevaluate and synthesise protocols for deploying untrusted language models as\nprogramming assistants, focusing on Trusted Monitoring protocols, which use\nweaker language models and limited human assistance. Finally, we demonstrate\nthe utility of our formalism by showcasing improvements over empirical studies\nin existing settings, evaluating protocols in new settings, and analysing how\nmodelling assumptions affect the safety and usefulness of protocols.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, with appendices",
    "pdf_url": "http://arxiv.org/pdf/2409.07985v1",
    "published_date": "2024-09-12 12:30:07 UTC",
    "updated_date": "2024-09-12 12:30:07 UTC"
  },
  {
    "arxiv_id": "2409.07966v4",
    "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE",
    "authors": [
      "Sichun Wu",
      "Kazi Injamamul Haque",
      "Zerrin Yumak"
    ],
    "abstract": "Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM\n  SIGGRAPH MIG 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.07966v4",
    "published_date": "2024-09-12 11:53:05 UTC",
    "updated_date": "2025-02-16 14:23:08 UTC"
  },
  {
    "arxiv_id": "2409.07965v1",
    "title": "Autonomous Vehicle Controllers From End-to-End Differentiable Simulation",
    "authors": [
      "Asen Nachkov",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "Current methods to learn controllers for autonomous vehicles (AVs) focus on\nbehavioural cloning. Being trained only on exact historic data, the resulting\nagents often generalize poorly to novel scenarios. Simulators provide the\nopportunity to go beyond offline datasets, but they are still treated as\ncomplicated black boxes, only used to update the global simulation state. As a\nresult, these RL algorithms are slow, sample-inefficient, and prior-agnostic.\nIn this work, we leverage a differentiable simulator and design an analytic\npolicy gradients (APG) approach to training AV controllers on the large-scale\nWaymo Open Motion Dataset. Our proposed framework brings the differentiable\nsimulator into an end-to-end training loop, where gradients of the environment\ndynamics serve as a useful prior to help the agent learn a more grounded\npolicy. We combine this setup with a recurrent architecture that can\nefficiently propagate temporal information across long simulated trajectories.\nThis APG method allows us to learn robust, accurate, and fast policies, while\nonly requiring widely-available expert trajectories, instead of scarce expert\nactions. We compare to behavioural cloning and find significant improvements in\nperformance and robustness to noise in the dynamics, as well as overall more\nintuitive human-like handling.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07965v1",
    "published_date": "2024-09-12 11:50:06 UTC",
    "updated_date": "2024-09-12 11:50:06 UTC"
  },
  {
    "arxiv_id": "2409.07964v1",
    "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks",
    "authors": [
      "Jingwen Tong",
      "Jiawei Shao",
      "Qiong Wu",
      "Wei Guo",
      "Zijian Li",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "Wireless networks are increasingly facing challenges due to their expanding\nscale and complexity. These challenges underscore the need for advanced\nAI-driven strategies, particularly in the upcoming 6G networks. In this\narticle, we introduce WirelessAgent, a novel approach leveraging large language\nmodels (LLMs) to develop AI agents capable of managing complex tasks in\nwireless networks. It can effectively improve network performance through\nadvanced reasoning, multimodal data processing, and autonomous decision making.\nThereafter, we demonstrate the practical applicability and benefits of\nWirelessAgent for network slicing management. The experimental results show\nthat WirelessAgent is capable of accurately understanding user intent,\neffectively allocating slice resources, and consistently maintaining optimal\nperformance.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07964v1",
    "published_date": "2024-09-12 11:48:01 UTC",
    "updated_date": "2024-09-12 11:48:01 UTC"
  },
  {
    "arxiv_id": "2409.07957v1",
    "title": "Rapid Parameter Estimation for Extreme Mass Ratio Inspirals Using Machine Learning",
    "authors": [
      "Bo Liang",
      "Hong Guo",
      "Tianyu Zhao",
      "He wang",
      "Herik Evangelinelis",
      "Yuxiang Xu",
      "Chang liu",
      "Manjia Liang",
      "Xiaotong Wei",
      "Yong Yuan",
      "Peng Xu",
      "Minghui Du",
      "Wei-Liang Qian",
      "Ziren Luo"
    ],
    "abstract": "Extreme-mass-ratio inspiral (EMRI) signals pose significant challenges in\ngravitational wave (GW) astronomy owing to their low-frequency nature and\nhighly complex waveforms, which occupy a high-dimensional parameter space with\nnumerous variables. Given their extended inspiral timescales and low\nsignal-to-noise ratios, EMRI signals warrant prolonged observation periods.\nParameter estimation becomes particularly challenging due to non-local\nparameter degeneracies, arising from multiple local maxima, as well as flat\nregions and ridges inherent in the likelihood function. These factors lead to\nexceptionally high time complexity for parameter analysis while employing\ntraditional matched filtering and random sampling methods. To address these\nchallenges, the present study applies machine learning to Bayesian posterior\nestimation of EMRI signals, leveraging the recently developed flow matching\ntechnique based on ODE neural networks. Our approach demonstrates computational\nefficiency several orders of magnitude faster than the traditional Markov Chain\nMonte Carlo (MCMC) methods, while preserving the unbiasedness of parameter\nestimation. We show that machine learning technology has the potential to\nefficiently handle the vast parameter space, involving up to seventeen\nparameters, associated with EMRI signals. Furthermore, to our knowledge, this\nis the first instance of applying machine learning, specifically the Continuous\nNormalizing Flows (CNFs), to EMRI signal analysis. Our findings highlight the\npromising potential of machine learning in EMRI waveform analysis, offering new\nperspectives for the advancement of space-based GW detection and GW astronomy.",
    "categories": [
      "physics.comp-ph",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07957v1",
    "published_date": "2024-09-12 11:36:23 UTC",
    "updated_date": "2024-09-12 11:36:23 UTC"
  },
  {
    "arxiv_id": "2409.07932v2",
    "title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies",
    "authors": [
      "Alexei Pisacane",
      "Victor-Alexandru Darvariu",
      "Mirco Musolesi"
    ],
    "abstract": "Graph path search is a classic computer science problem that has been\nrecently approached with Reinforcement Learning (RL) due to its potential to\noutperform prior methods. Existing RL techniques typically assume a global view\nof the network, which is not suitable for large-scale, dynamic, and\nprivacy-sensitive settings. An area of particular interest is search in social\nnetworks due to its numerous applications. Inspired by seminal work in\nexperimental sociology, which showed that decentralized yet efficient search is\npossible in social networks, we frame the problem as a collaborative task\nbetween multiple agents equipped with a limited local view of the network. We\npropose a multi-agent approach for graph path search that successfully\nleverages both homophily and structural heterogeneity. Our experiments, carried\nout over synthetic and real-world social networks, demonstrate that our model\nsignificantly outperforms learned and heuristic baselines. Furthermore, our\nresults show that meaningful embeddings for graph navigation can be constructed\nusing reward-driven learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07932v2",
    "published_date": "2024-09-12 10:56:38 UTC",
    "updated_date": "2024-11-26 17:46:32 UTC"
  },
  {
    "arxiv_id": "2409.07930v1",
    "title": "A convolutional neural network approach to deblending seismic data",
    "authors": [
      "Jing Sun",
      "Sigmund Slang",
      "Thomas Elboth",
      "Thomas Larsen Greiner",
      "Steven McDonald",
      "Leiv-J Gelius"
    ],
    "abstract": "For economic and efficiency reasons, blended acquisition of seismic data is\nbecoming more and more commonplace. Seismic deblending methods are always\ncomputationally demanding and normally consist of multiple processing steps.\nBesides, the parameter setting is not always trivial. Machine learning-based\nprocessing has the potential to significantly reduce processing time and to\nchange the way seismic deblending is carried out. We present a data-driven deep\nlearning-based method for fast and efficient seismic deblending. The blended\ndata are sorted from the common source to the common channel domain to\ntransform the character of the blending noise from coherent events to\nincoherent distributions. A convolutional neural network (CNN) is designed\naccording to the special character of seismic data, and performs deblending\nwith comparable results to those obtained with conventional industry deblending\nalgorithms. To ensure authenticity, the blending was done numerically and only\nfield seismic data were employed, including more than 20000 training examples.\nAfter training and validation of the network, seismic deblending can be\nperformed in near real time. Experiments also show that the initial signal to\nnoise ratio (SNR) is the major factor controlling the quality of the final\ndeblended result. The network is also demonstrated to be robust and adaptive by\nusing the trained model to firstly deblend a new data set from a different\ngeological area with a slightly different delay time setting, and secondly\ndeblend shots with blending noise in the top part of the data.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07930v1",
    "published_date": "2024-09-12 10:54:35 UTC",
    "updated_date": "2024-09-12 10:54:35 UTC"
  },
  {
    "arxiv_id": "2409.07925v1",
    "title": "A framework for measuring the training efficiency of a neural architecture",
    "authors": [
      "Eduardo Cueto-Mendoza",
      "John D. Kelleher"
    ],
    "abstract": "Measuring Efficiency in neural network system development is an open research\nproblem. This paper presents an experimental framework to measure the training\nefficiency of a neural architecture. To demonstrate our approach, we analyze\nthe training efficiency of Convolutional Neural Networks and Bayesian\nequivalents on the MNIST and CIFAR-10 tasks. Our results show that training\nefficiency decays as training progresses and varies across different stopping\ncriteria for a given neural model and learning task. We also find a non-linear\nrelationship between training stopping criteria, training Efficiency, model\nsize, and training Efficiency.\n  Furthermore, we illustrate the potential confounding effects of overtraining\non measuring the training efficiency of a neural architecture. Regarding\nrelative training efficiency across different architectures, our results\nindicate that CNNs are more efficient than BCNNs on both datasets. More\ngenerally, as a learning task becomes more complex, the relative difference in\ntraining efficiency between different architectures becomes more pronounced.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07925v1",
    "published_date": "2024-09-12 10:45:38 UTC",
    "updated_date": "2024-09-12 10:45:38 UTC"
  },
  {
    "arxiv_id": "2409.07918v1",
    "title": "Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning",
    "authors": [
      "Elizabeth Wilson",
      "Gyrgy Fazekas",
      "Geraint Wiggins"
    ],
    "abstract": "This paper presents Tidal-MerzA, a novel system designed for collaborative\nperformances between humans and a machine agent in the context of live coding,\nspecifically focusing on the generation of musical patterns. Tidal-MerzA fuses\ntwo foundational models: ALCAA (Affective Live Coding Autonomous Agent) and\nTidal Fuzz, a computational framework. By integrating affective modelling with\ncomputational generation, this system leverages reinforcement learning\ntechniques to dynamically adapt music composition parameters within the\nTidalCycles framework, ensuring both affective qualities to the patterns and\nsyntactical correctness. The development of Tidal-MerzA introduces two distinct\nagents: one focusing on the generation of mini-notation strings for musical\nexpression, and another on the alignment of music with targeted affective\nstates through reinforcement learning. This approach enhances the adaptability\nand creative potential of live coding practices and allows exploration of\nhuman-machine creative interactions. Tidal-MerzA advances the field of\ncomputational music generation, presenting a novel methodology for\nincorporating artificial intelligence into artistic practices.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07918v1",
    "published_date": "2024-09-12 10:38:55 UTC",
    "updated_date": "2024-09-12 10:38:55 UTC"
  },
  {
    "arxiv_id": "2409.07914v3",
    "title": "InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation",
    "authors": [
      "Andrew Lee",
      "Ian Chuang",
      "Ling-Yuan Chen",
      "Iman Soltani"
    ],
    "abstract": "Bimanual manipulation presents unique challenges compared to unimanual tasks\ndue to the complexity of coordinating two robotic arms. In this paper, we\nintroduce InterACT: Inter-dependency aware Action Chunking with Hierarchical\nAttention Transformers, a novel imitation learning framework designed\nspecifically for bimanual manipulation. InterACT leverages hierarchical\nattention mechanisms to effectively capture inter-dependencies between dual-arm\njoint states and visual inputs. The framework comprises a Hierarchical\nAttention Encoder, which processes multi-modal inputs through segment-wise and\ncross-segment attention mechanisms, and a Multi-arm Decoder that generates each\narm's action predictions in parallel, while sharing information between the\narms through synchronization blocks by providing the other arm's intermediate\noutput as context. Our experiments, conducted on various simulated and\nreal-world bimanual manipulation tasks, demonstrate that InterACT outperforms\nexisting methods. Detailed ablation studies further validate the significance\nof key components, including the impact of CLS tokens, cross-segment encoders,\nand synchronization blocks on task performance. We provide supplementary\nmaterials and videos on our project page.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at Conference on Robot Learning (CoRL) 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.07914v3",
    "published_date": "2024-09-12 10:30:44 UTC",
    "updated_date": "2024-10-16 08:52:42 UTC"
  },
  {
    "arxiv_id": "2409.07913v1",
    "title": "UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints",
    "authors": [
      "Inzamamul Alam",
      "Muhammad Shahid Muneer",
      "Simon S. Woo"
    ],
    "abstract": "In the wake of a fabricated explosion image at the Pentagon, an ability to\ndiscern real images from fake counterparts has never been more critical. Our\nstudy introduces a novel multi-modal approach to detect AI-generated images\namidst the proliferation of new generation methods such as Diffusion models.\nOur method, UGAD, encompasses three key detection steps: First, we transform\nthe RGB images into YCbCr channels and apply an Integral Radial Operation to\nemphasize salient radial features. Secondly, the Spatial Fourier Extraction\noperation is used for a spatial shift, utilizing a pre-trained deep learning\nnetwork for optimal feature extraction. Finally, the deep neural network\nclassification stage processes the data through dense layers using softmax for\nclassification. Our approach significantly enhances the accuracy of\ndifferentiating between real and AI-generated images, as evidenced by a 12.64%\nincrease in accuracy and 28.43% increase in AUC compared to existing\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07913v1",
    "published_date": "2024-09-12 10:29:37 UTC",
    "updated_date": "2024-09-12 10:29:37 UTC"
  },
  {
    "arxiv_id": "2410.05273v3",
    "title": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers",
    "authors": [
      "Jianke Zhang",
      "Yanjiang Guo",
      "Xiaoyu Chen",
      "Yen-Jen Wang",
      "Yucheng Hu",
      "Chengming Shi",
      "Jianyu Chen"
    ],
    "abstract": "Large Vision-Language-Action (VLA) models, leveraging powerful pre trained\nVision-Language Models (VLMs) backends, have shown promise in robotic control\ndue to their impressive generalization ability. However, the success comes at a\ncost. Their reliance on VLM backends with billions of parameters leads to high\ncomputational costs and inference latency, limiting the testing scenarios to\nmainly quasi-static tasks and hindering performance in dynamic tasks requiring\nrapid interactions. To address these limitations, this paper proposes HiRT, a\nHierarchical Robot Transformer framework that enables flexible frequency and\nperformance trade-off. HiRT keeps VLMs running at low frequencies to capture\ntemporarily invariant features while enabling real-time interaction through a\nhigh-frequency vision-based policy guided by the slowly updated features.\nExperiment results in both simulation and real-world settings demonstrate\nsignificant improvements over baseline methods. Empirically, in static tasks,\nwe double the control frequency and achieve comparable success rates.\nAdditionally, on novel real-world dynamic ma nipulation tasks which are\nchallenging for previous VLA models, HiRT improves the success rate from 48% to\n75%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CORL 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.05273v3",
    "published_date": "2024-09-12 09:18:09 UTC",
    "updated_date": "2025-02-03 04:07:37 UTC"
  },
  {
    "arxiv_id": "2409.07850v1",
    "title": "Enhancing Cross-Market Recommendation System with Graph Isomorphism Networks: A Novel Approach to Personalized User Experience",
    "authors": [
      "Smeyye ztrk",
      "Ahmed Burak Ercan",
      "Resul Tugay",
      "ule Gndz dc"
    ],
    "abstract": "In today's world of globalized commerce, cross-market recommendation systems\n(CMRs) are crucial for providing personalized user experiences across diverse\nmarket segments. However, traditional recommendation algorithms have\ndifficulties dealing with market specificity and data sparsity, especially in\nnew or emerging markets. In this paper, we propose the CrossGR model, which\nutilizes Graph Isomorphism Networks (GINs) to improve CMR systems. It\noutperforms existing benchmarks in NDCG@10 and HR@10 metrics, demonstrating its\nadaptability and accuracy in handling diverse market segments. The CrossGR\nmodel is adaptable and accurate, making it well-suited for handling the\ncomplexities of cross-market recommendation tasks. Its robustness is\ndemonstrated by consistent performance across different evaluation timeframes,\nindicating its potential to cater to evolving market trends and user\npreferences. Our findings suggest that GINs represent a promising direction for\nCMRs, paving the way for more sophisticated, personalized, and context-aware\nrecommendation systems in the dynamic landscape of global e-commerce.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages, 1 figure, 3 tables, 5 equations",
    "pdf_url": "http://arxiv.org/pdf/2409.07850v1",
    "published_date": "2024-09-12 08:53:11 UTC",
    "updated_date": "2024-09-12 08:53:11 UTC"
  },
  {
    "arxiv_id": "2409.07825v3",
    "title": "Deep Multimodal Learning with Missing Modality: A Survey",
    "authors": [
      "Renjie Wu",
      "Hu Wang",
      "Hsiang-Ting Chen",
      "Gustavo Carneiro"
    ],
    "abstract": "During multimodal model training and testing, certain data modalities may be\nabsent due to sensor limitations, cost constraints, privacy concerns, or data\nloss, negatively affecting performance. Multimodal learning techniques designed\nto handle missing modalities can mitigate this by ensuring model robustness\neven when some modalities are unavailable. This survey reviews recent progress\nin Multimodal Learning with Missing Modality (MLMM), focusing on deep learning\nmethods. It provides the first comprehensive survey that covers the motivation\nand distinctions between MLMM and standard multimodal learning setups, followed\nby a detailed analysis of current methods, applications, and datasets,\nconcluding with challenges and future directions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to ACM Computing Surveys",
    "pdf_url": "http://arxiv.org/pdf/2409.07825v3",
    "published_date": "2024-09-12 08:15:39 UTC",
    "updated_date": "2024-10-21 09:14:47 UTC"
  },
  {
    "arxiv_id": "2409.07822v1",
    "title": "Over-the-Air Federated Learning via Weighted Aggregation",
    "authors": [
      "Seyed Mohammad Azimi-Abarghouyi",
      "Leandros Tassiulas"
    ],
    "abstract": "This paper introduces a new federated learning scheme that leverages\nover-the-air computation. A novel feature of this scheme is the proposal to\nemploy adaptive weights during aggregation, a facet treated as predefined in\nother over-the-air schemes. This can mitigate the impact of wireless channel\nconditions on learning performance, without needing channel state information\nat transmitter side (CSIT). We provide a mathematical methodology to derive the\nconvergence bound for the proposed scheme in the context of computational\nheterogeneity and general loss functions, supplemented with design insights.\nAccordingly, we propose aggregation cost metrics and efficient algorithms to\nfind optimized weights for the aggregation. Finally, through numerical\nexperiments, we validate the effectiveness of the proposed scheme. Even with\nthe challenges posed by channel conditions and device heterogeneity, the\nproposed scheme surpasses other over-the-air strategies by an accuracy\nimprovement of 15% over the scheme using CSIT and 30% compared to the one\nwithout CSIT.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07822v1",
    "published_date": "2024-09-12 08:07:11 UTC",
    "updated_date": "2024-09-12 08:07:11 UTC"
  },
  {
    "arxiv_id": "2409.07796v2",
    "title": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation",
    "authors": [
      "Mohammad Mehdi Rastikerdar",
      "Jin Huang",
      "Hui Guan",
      "Deepak Ganesan"
    ],
    "abstract": "Resource-constrained IoT devices increasingly rely on deep learning models\nfor inference tasks in remote environments. However, these models experience\nsignificant accuracy drops due to domain shifts when encountering variations in\nlighting, weather, and seasonal conditions. While cloud-based retraining can\naddress this issue, many IoT deployments operate with limited connectivity and\nenergy constraints, making traditional fine-tuning approaches impractical. We\nexplore this challenge through the lens of wildlife ecology, where camera traps\nmust maintain accurate species classification across changing seasons, weather,\nand habitats without reliable connectivity. We introduce WildFit, an autonomous\nin-situ adaptation framework that leverages the key insight that background\nscenes change more frequently than the visual characteristics of monitored\nspecies. WildFit combines background-aware synthesis to generate training\nsamples on-device with drift-aware fine-tuning that triggers model updates only\nwhen necessary to conserve resources. Through extensive evaluation on multiple\ncamera trap deployments, we demonstrate that WildFit significantly improves\naccuracy while greatly reducing adaptation overhead compared to traditional\napproaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07796v2",
    "published_date": "2024-09-12 06:56:52 UTC",
    "updated_date": "2025-01-24 05:24:14 UTC"
  },
  {
    "arxiv_id": "2409.07793v1",
    "title": "Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation",
    "authors": [
      "Fuchen Zheng",
      "Quanjun Li",
      "Weixuan Li",
      "Xuhang Chen",
      "Yihang Dong",
      "Guoheng Huang",
      "Chi-Man Pun",
      "Shoujun Zhou"
    ],
    "abstract": "Medical image segmentation, a critical application of semantic segmentation\nin healthcare, has seen significant advancements through specialized computer\nvision techniques. While deep learning-based medical image segmentation is\nessential for assisting in medical diagnosis, the lack of diverse training data\ncauses the long-tail problem. Moreover, most previous hybrid CNN-ViT\narchitectures have limited ability to combine various attentions in different\nlayers of the Convolutional Neural Network. To address these issues, we propose\na Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware\nContrastive Loss, as the overall training objective for semi-supervised\nlearning to mitigate the long-tail problem. Additionally, we introduce\nCMAformer, a novel network that synergizes the strengths of ResUNet and\nTransformer. The cross-attention block in CMAformer effectively integrates\nspatial attention and channel attention for multi-scale feature fusion.\nOverall, our results indicate that CMAformer, combined with the feature fusion\nframework and the new consistency loss, demonstrates strong complementarity in\nsemi-supervised learning ensembles. We achieve state-of-the-art results on\nmultiple public medical image datasets. Example code are available at:\n\\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.07793v1",
    "published_date": "2024-09-12 06:52:46 UTC",
    "updated_date": "2024-09-12 06:52:46 UTC"
  },
  {
    "arxiv_id": "2409.07779v3",
    "title": "AFFSegNet: Adaptive Feature Fusion Segmentation Network for Microtumors and Multi-Organ Segmentation",
    "authors": [
      "Fuchen Zheng",
      "Xinyi Chen",
      "Xuhang Chen",
      "Haolun Li",
      "Xiaojiao Guo",
      "Weihuang Liu",
      "Chi-Man Pun",
      "Shoujun Zhou"
    ],
    "abstract": "Medical image segmentation, a crucial task in computer vision, facilitates\nthe automated delineation of anatomical structures and pathologies, supporting\nclinicians in diagnosis, treatment planning, and disease monitoring. Notably,\ntransformers employing shifted window-based self-attention have demonstrated\nexceptional performance. However, their reliance on local window attention\nlimits the fusion of local and global contextual information, crucial for\nsegmenting microtumors and miniature organs. To address this limitation, we\npropose the Adaptive Semantic Segmentation Network (ASSNet), a transformer\narchitecture that effectively integrates local and global features for precise\nmedical image segmentation. ASSNet comprises a transformer-based U-shaped\nencoder-decoder network. The encoder utilizes shifted window self-attention\nacross five resolutions to extract multi-scale features, which are then\npropagated to the decoder through skip connections. We introduce an augmented\nmulti-layer perceptron within the encoder to explicitly model long-range\ndependencies during feature extraction. Recognizing the constraints of\nconventional symmetrical encoder-decoder designs, we propose an Adaptive\nFeature Fusion (AFF) decoder to complement our encoder. This decoder\nincorporates three key components: the Long Range Dependencies (LRD) block, the\nMulti-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)\nblock. These components synergistically facilitate the effective fusion of\nmulti-scale features extracted by the decoder while capturing long-range\ndependencies and refining object boundaries. Comprehensive experiments on\ndiverse medical image segmentation tasks, including multi-organ, liver tumor,\nand bladder tumor segmentation, demonstrate that ASSNet achieves\nstate-of-the-art results. Code and models are available at:\n\\url{https://github.com/lzeeorno/ASSNet}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.07779v3",
    "published_date": "2024-09-12 06:25:44 UTC",
    "updated_date": "2024-12-10 16:16:12 UTC"
  },
  {
    "arxiv_id": "2409.07776v1",
    "title": "Training Spiking Neural Networks via Augmented Direct Feedback Alignment",
    "authors": [
      "Yongbo Zhang",
      "Katsuma Inoue",
      "Mitsumasa Nakajima",
      "Toshikazu Hashimoto",
      "Yasuo Kuniyoshi",
      "Kohei Nakajima"
    ],
    "abstract": "Spiking neural networks (SNNs), the models inspired by the mechanisms of real\nneurons in the brain, transmit and represent information by employing discrete\naction potentials or spikes. The sparse, asynchronous properties of information\nprocessing make SNNs highly energy efficient, leading to SNNs being promising\nsolutions for implementing neural networks in neuromorphic devices. However,\nthe nondifferentiable nature of SNN neurons makes it a challenge to train them.\nThe current training methods of SNNs that are based on error backpropagation\n(BP) and precisely designing surrogate gradient are difficult to implement and\nbiologically implausible, hindering the implementation of SNNs on neuromorphic\ndevices. Thus, it is important to train SNNs with a method that is both\nphysically implementatable and biologically plausible. In this paper, we\npropose using augmented direct feedback alignment (aDFA), a gradient-free\napproach based on random projection, to train SNNs. This method requires only\npartial information of the forward process during training, so it is easy to\nimplement and biologically plausible. We systematically demonstrate the\nfeasibility of the proposed aDFA-SNNs scheme, propose its effective working\nrange, and analyze its well-performing settings by employing genetic algorithm.\nWe also analyze the impact of crucial features of SNNs on the scheme, thus\ndemonstrating its superiority and stability over BP and conventional direct\nfeedback alignment. Our scheme can achieve competitive performance without\naccurate prior knowledge about the utilized system, thus providing a valuable\nreference for physically training SNNs.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "20 pages, 8 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.07776v1",
    "published_date": "2024-09-12 06:22:44 UTC",
    "updated_date": "2024-09-12 06:22:44 UTC"
  },
  {
    "arxiv_id": "2409.07775v1",
    "title": "A Spatiotemporal Stealthy Backdoor Attack against Cooperative Multi-Agent Deep Reinforcement Learning",
    "authors": [
      "Yinbo Yu",
      "Saihao Yan",
      "Jiajia Liu"
    ],
    "abstract": "Recent studies have shown that cooperative multi-agent deep reinforcement\nlearning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor\ntrigger is observed, it will perform abnormal actions leading to failures or\nmalicious goals. However, existing proposed backdoors suffer from several\nissues, e.g., fixed visual trigger patterns lack stealthiness, the backdoor is\ntrained or activated by an additional network, or all agents are backdoored. To\nthis end, in this paper, we propose a novel backdoor attack against c-MADRL,\nwhich attacks the entire multi-agent team by embedding the backdoor only in a\nsingle agent. Firstly, we introduce adversary spatiotemporal behavior patterns\nas the backdoor trigger rather than manual-injected fixed visual patterns or\ninstant status and control the attack duration. This method can guarantee the\nstealthiness and practicality of injected backdoors. Secondly, we hack the\noriginal reward function of the backdoored agent via reward reverse and\nunilateral guidance during training to ensure its adverse influence on the\nentire team. We evaluate our backdoor attacks on two classic c-MADRL algorithms\nVDN and QMIX, in a popular c-MADRL environment SMAC. The experimental results\ndemonstrate that our backdoor attacks are able to reach a high attack success\nrate (91.6\\%) while maintaining a low clean performance variance rate (3.7\\%).",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, IEEE Globecom 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.07775v1",
    "published_date": "2024-09-12 06:17:37 UTC",
    "updated_date": "2024-09-12 06:17:37 UTC"
  },
  {
    "arxiv_id": "2409.08308v1",
    "title": "DiReDi: Distillation and Reverse Distillation for AIoT Applications",
    "authors": [
      "Chen Sun",
      "Qing Tong",
      "Wenshuang Yang",
      "Wenqi Zhang"
    ],
    "abstract": "Typically, the significant efficiency can be achieved by deploying different\nedge AI models in various real world scenarios while a few large models manage\nthose edge AI models remotely from cloud servers. However, customizing edge AI\nmodels for each user's specific application or extending current models to new\napplication scenarios remains a challenge. Inappropriate local training or fine\ntuning of edge AI models by users can lead to model malfunction, potentially\nresulting in legal issues for the manufacturer. To address aforementioned\nissues, this paper proposes an innovative framework called \"DiReD\", which\ninvolves knowledge DIstillation & REverse DIstillation. In the initial step, an\nedge AI model is trained with presumed data and a KD process using the cloud AI\nmodel in the upper management cloud server. This edge AI model is then\ndispatched to edge AI devices solely for inference in the user's application\nscenario. When the user needs to update the edge AI model to better fit the\nactual scenario, the reverse distillation (RD) process is employed to extract\nthe knowledge: the difference between user preferences and the manufacturer's\npresumptions from the edge AI model using the user's exclusive data. Only the\nextracted knowledge is reported back to the upper management cloud server to\nupdate the cloud AI model, thus protecting user privacy by not using any\nexclusive data. The updated cloud AI can then update the edge AI model with the\nextended knowledge. Simulation results demonstrate that the proposed \"DiReDi\"\nframework allows the manufacturer to update the user model by learning new\nknowledge from the user's actual scenario with private data. The initial\nredundant knowledge is reduced since the retraining emphasizes user private\ndata.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08308v1",
    "published_date": "2024-09-12 06:02:44 UTC",
    "updated_date": "2024-09-12 06:02:44 UTC"
  },
  {
    "arxiv_id": "2409.07770v1",
    "title": "Universal Pooling Method of Multi-layer Features from Pretrained Models for Speaker Verification",
    "authors": [
      "Jin Sob Kim",
      "Hyun Joon Park",
      "Wooseok Shin",
      "Sung Won Han"
    ],
    "abstract": "Recent advancements in automatic speaker verification (ASV) studies have been\nachieved by leveraging large-scale pretrained networks. In this study, we\nanalyze the approaches toward such a paradigm and underline the significance of\ninterlayer information processing as a result. Accordingly, we present a novel\napproach for exploiting the multilayered nature of pretrained models for ASV,\nwhich comprises a layer/frame-level network and two steps of pooling\narchitectures for each layer and frame axis. Specifically, we let convolutional\narchitecture directly processes a stack of layer outputs.Then, we present a\nchannel attention-based scheme of gauging layer significance and squeeze the\nlayer level with the most representative value. Finally, attentive statistics\nover frame-level representations yield a single vector speaker embedding.\nComparative experiments are designed using versatile data environments and\ndiverse pretraining models to validate the proposed approach. The experimental\nresults demonstrate the stability of the approach using multi-layer outputs in\nleveraging pretrained architectures. Then, we verify the superiority of the\nproposed ASV backend structure, which involves layer-wise operations, in terms\nof performance improvement along with cost efficiency compared to the\nconventional method. The ablation study shows how the proposed interlayer\nprocessing aids in maximizing the advantage of utilizing pretrained models.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2409.07770v1",
    "published_date": "2024-09-12 05:55:32 UTC",
    "updated_date": "2024-09-12 05:55:32 UTC"
  },
  {
    "arxiv_id": "2409.07763v1",
    "title": "Reimagining Linear Probing: Kolmogorov-Arnold Networks in Transfer Learning",
    "authors": [
      "Sheng Shen",
      "Rabih Younes"
    ],
    "abstract": "This paper introduces Kolmogorov-Arnold Networks (KAN) as an enhancement to\nthe traditional linear probing method in transfer learning. Linear probing,\noften applied to the final layer of pre-trained models, is limited by its\ninability to model complex relationships in data. To address this, we propose\nsubstituting the linear probing layer with KAN, which leverages spline-based\nrepresentations to approximate intricate functions. In this study, we integrate\nKAN with a ResNet-50 model pre-trained on ImageNet and evaluate its performance\non the CIFAR-10 dataset. We perform a systematic hyperparameter search,\nfocusing on grid size and spline degree (k), to optimize KAN's flexibility and\naccuracy. Our results demonstrate that KAN consistently outperforms traditional\nlinear probing, achieving significant improvements in accuracy and\ngeneralization across a range of configurations. These findings indicate that\nKAN offers a more powerful and adaptable alternative to conventional linear\nprobing techniques in transfer learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.07763v1",
    "published_date": "2024-09-12 05:36:40 UTC",
    "updated_date": "2024-09-12 05:36:40 UTC"
  },
  {
    "arxiv_id": "2409.18971v1",
    "title": "Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better",
    "authors": [
      "Mengying Ge",
      "Mingyang Li",
      "Dongkai Tang",
      "Pengbo Li",
      "Kuo Liu",
      "Shuhao Deng",
      "Songbai Pu",
      "Long Liu",
      "Yang Song",
      "Tao Zhang"
    ],
    "abstract": "In this paper, we present our solutions for emotion recognition in the\nsub-challenges of Multimodal Emotion Recognition Challenge (MER2024). To\nmitigate the modal competition issue between audio and text, we adopt an early\nfusion strategy based on a large language model, where joint training of audio\nand text is conducted initially. And the joint Audio-Text modal feature will be\nlate-fused with other unimodal features. In order to solve the problems of data\ninsufficiency and class imbalance, We use multiple turns of multi-model voting\nfor data mining. Moreover, to enhance the quality of audio features, we employ\nspeech source separation to preprocess audios. Our model ranks \\textbf{2nd} in\nboth MER2024-SEMI and MER2024-NOISE, validating our method's effectiveness.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18971v1",
    "published_date": "2024-09-12 05:05:34 UTC",
    "updated_date": "2024-09-12 05:05:34 UTC"
  },
  {
    "arxiv_id": "2409.07753v4",
    "title": "Relevance for Human Robot Collaboration",
    "authors": [
      "Xiaotong Zhang",
      "Dean Huang",
      "Kamal Youcef-Toumi"
    ],
    "abstract": "Inspired by the human ability to selectively focus on relevant information,\nthis paper introduces relevance, a novel dimensionality reduction process for\nhuman-robot collaboration (HRC). Our approach incorporates a continuously\noperating perception module, evaluates cue sufficiency within the scene, and\napplies a flexible formulation and computation framework. To accurately and\nefficiently quantify relevance, we developed an event-based framework that\nmaintains a continuous perception of the scene and selectively triggers\nrelevance determination. Within this framework, we developed a probabilistic\nmethodology, which considers various factors and is built on a novel structured\nscene representation. Simulation results demonstrate that the relevance\nframework and methodology accurately predict the relevance of a general HRC\nsetup, achieving a precision of 0.99, a recall of 0.94, an F1 score of 0.96,\nand an object ratio of 0.94. Relevance can be broadly applied to several areas\nin HRC to accurately improve task planning time by 79.56% compared with pure\nplanning for a cereal task, reduce perception latency by up to 26.53% for an\nobject detector, improve HRC safety by up to 13.50% and reduce the number of\ninquiries for HRC by 80.84%. A real-world demonstration showcases the relevance\nframework's ability to intelligently and seamlessly assist humans in everyday\ntasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2409.07753v4",
    "published_date": "2024-09-12 04:57:34 UTC",
    "updated_date": "2025-04-17 16:19:17 UTC"
  },
  {
    "arxiv_id": "2409.07748v1",
    "title": "Top-down Activity Representation Learning for Video Question Answering",
    "authors": [
      "Yanan Wang",
      "Shuichiro Haruta",
      "Donghuo Zeng",
      "Julio Vizcarra",
      "Mori Kurokawa"
    ],
    "abstract": "Capturing complex hierarchical human activities, from atomic actions (e.g.,\npicking up one present, moving to the sofa, unwrapping the present) to\ncontextual events (e.g., celebrating Christmas) is crucial for achieving\nhigh-performance video question answering (VideoQA). Recent works have expanded\nmultimodal models (e.g., CLIP, LLaVA) to process continuous video sequences,\nenhancing the model's temporal reasoning capabilities. However, these\napproaches often fail to capture contextual events that can be decomposed into\nmultiple atomic actions non-continuously distributed over relatively long-term\nsequences. In this paper, to leverage the spatial visual context representation\ncapability of the CLIP model for obtaining non-continuous visual\nrepresentations in terms of contextual events in videos, we convert long-term\nvideo sequences into a spatial image domain and finetune the multimodal model\nLLaVA for the VideoQA task. Our approach achieves competitive performance on\nthe STAR task, in particular, with a 78.4% accuracy score, exceeding the\ncurrent state-of-the-art score by 2.8 points on the NExTQA task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "presented at MIRU2024",
    "pdf_url": "http://arxiv.org/pdf/2409.07748v1",
    "published_date": "2024-09-12 04:43:27 UTC",
    "updated_date": "2024-09-12 04:43:27 UTC"
  },
  {
    "arxiv_id": "2409.07747v1",
    "title": "Multi-object event graph representation learning for Video Question Answering",
    "authors": [
      "Yanan Wang",
      "Shuichiro Haruta",
      "Donghuo Zeng",
      "Julio Vizcarra",
      "Mori Kurokawa"
    ],
    "abstract": "Video question answering (VideoQA) is a task to predict the correct answer to\nquestions posed about a given video. The system must comprehend spatial and\ntemporal relationships among objects extracted from videos to perform causal\nand temporal reasoning. While prior works have focused on modeling individual\nobject movements using transformer-based methods, they falter when capturing\ncomplex scenarios involving multiple objects (e.g., \"a boy is throwing a ball\nin a hoop\"). We propose a contrastive language event graph representation\nlearning method called CLanG to address this limitation. Aiming to capture\nevent representations associated with multiple objects, our method employs a\nmulti-layer GNN-cluster module for adversarial graph representation learning,\nenabling contrastive learning between the question text and its relevant\nmulti-object event graph. Our method outperforms a strong baseline, achieving\nup to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and\nTGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal\nand temporal questions, highlighting its strength in reasoning multiple\nobject-based events.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "presented at MIRU2024",
    "pdf_url": "http://arxiv.org/pdf/2409.07747v1",
    "published_date": "2024-09-12 04:42:51 UTC",
    "updated_date": "2024-09-12 04:42:51 UTC"
  },
  {
    "arxiv_id": "2409.07736v1",
    "title": "Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities",
    "authors": [
      "Aaryan Panda",
      "Damodar Panigrahi",
      "Shaswata Mitra",
      "Sudip Mittal",
      "Shahram Rahimi"
    ],
    "abstract": "The field of Computer Vision (CV) has faced challenges. Initially, it relied\non handcrafted features and rule-based algorithms, resulting in limited\naccuracy. The introduction of machine learning (ML) has brought progress,\nparticularly Transfer Learning (TL), which addresses various CV problems by\nreusing pre-trained models. TL requires less data and computing while\ndelivering nearly equal accuracy, making it a prominent technique in the CV\nlandscape. Our research focuses on TL development and how CV applications use\nit to solve real-world problems. We discuss recent developments, limitations,\nand opportunities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.07736v1",
    "published_date": "2024-09-12 03:59:15 UTC",
    "updated_date": "2024-09-12 03:59:15 UTC"
  },
  {
    "arxiv_id": "2409.07732v1",
    "title": "Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT",
    "authors": [
      "Irene Weber"
    ],
    "abstract": "Large Language Models (LLMs) offer numerous applications, the full extent of\nwhich is not yet understood. This paper investigates if LLMs can be applied for\nediting structured and semi-structured documents with minimal effort. Using a\nqualitative research approach, we conduct two case studies with ChatGPT and\nthoroughly analyze the results. Our experiments indicate that LLMs can\neffectively edit structured and semi-structured documents when provided with\nbasic, straightforward prompts. ChatGPT demonstrates a strong ability to\nrecognize and process the structure of annotated documents. This suggests that\nexplicitly structuring tasks and data in prompts might enhance an LLM's ability\nto understand and solve tasks. Furthermore, the experiments also reveal\nimpressive pattern matching skills in ChatGPT. This observation deserves\nfurther investigation, as it may contribute to understanding the processes\nleading to hallucinations in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07732v1",
    "published_date": "2024-09-12 03:41:39 UTC",
    "updated_date": "2024-09-12 03:41:39 UTC"
  },
  {
    "arxiv_id": "2409.08867v1",
    "title": "Establish seedling quality classification standard for Chrysanthemum efficiently with help of deep clustering algorithm",
    "authors": [
      "Yanzhi Jing",
      "Hongguang Zhao",
      "Shujun Yu"
    ],
    "abstract": "Establishing reasonable standards for edible chrysanthemum seedlings helps\npromote seedling development, thereby improving plant quality. However, current\ngrading methods have the several issues. The limitation that only support a few\nindicators causes information loss, and indicators selected to evaluate\nseedling level have a narrow applicability. Meanwhile, some methods misuse\nmathematical formulas. Therefore, we propose a simple, efficient, and generic\nframework, SQCSEF, for establishing seedling quality classification standards\nwith flexible clustering modules, applicable to most plant species. In this\nstudy, we introduce the state-of-the-art deep clustering algorithm CVCL, using\nfactor analysis to divide indicators into several perspectives as inputs for\nthe CVCL method, resulting in more reasonable clusters and ultimately a grading\nstandard $S_{cvcl}$ for edible chrysanthemum seedlings. Through conducting\nextensive experiments, we validate the correctness and efficiency of the\nproposed SQCSEF framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08867v1",
    "published_date": "2024-09-12 03:09:11 UTC",
    "updated_date": "2024-09-12 03:09:11 UTC"
  },
  {
    "arxiv_id": "2409.07725v2",
    "title": "GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning",
    "authors": [
      "Kaizhe Fan",
      "Quanjun Li"
    ],
    "abstract": "Graph representation learning has emerged as a powerful tool for preserving\ngraph topology when mapping nodes to vector representations, enabling various\ndownstream tasks such as node classification and community detection. However,\nmost current graph neural network models face the challenge of requiring\nextensive labeled data, which limits their practical applicability in\nreal-world scenarios where labeled data is scarce. To address this challenge,\nresearchers have explored Graph Contrastive Learning (GCL), which leverages\nenhanced graph data and contrastive learning techniques. While promising,\nexisting GCL methods often struggle with effectively capturing both local and\nglobal graph structures, and balancing the trade-off between nodelevel and\ngraph-level representations. In this work, we propose Graph Representation\nEmbedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our\nmodel introduces a novel triple network architecture with a multi-head\nattention GNN as the core. GRE2-MDCL first globally and locally augments the\ninput graph using SVD and LAGNN techniques. It then constructs a\nmultidimensional contrastive loss, incorporating cross-network, cross-view, and\nneighbor contrast, to optimize the model. Extensive experiments on benchmark\ndatasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves\nstate-of-the-art performance, with average accuracies of 82.5%, 72.5%, and\n81.6% respectively. Visualizations further show tighter intra-cluster\naggregation and clearer inter-cluster boundaries, highlighting the\neffectiveness of our framework in improving upon baseline GCL models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "I am requesting the withdrawal of my paper due to errors identified\n  in the methodology and experimental results. Specifically, there are\n  inaccuracies in the analysis section that may lead to misleading conclusions",
    "pdf_url": "http://arxiv.org/pdf/2409.07725v2",
    "published_date": "2024-09-12 03:09:05 UTC",
    "updated_date": "2025-03-20 02:10:52 UTC"
  },
  {
    "arxiv_id": "2409.07723v2",
    "title": "Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy",
    "authors": [
      "Bojian Li",
      "Bo Liu",
      "Xinning Yao",
      "Jinghua Yue",
      "Fugen Zhou"
    ],
    "abstract": "Depth estimation is a cornerstone of 3D reconstruction and plays a vital role\nin minimally invasive endoscopic surgeries. However, most current depth\nestimation networks rely on traditional convolutional neural networks, which\nare limited in their ability to capture global information. Foundation models\noffer a promising approach to enhance depth estimation, but those models\ncurrently available are primarily trained on natural images, leading to\nsuboptimal performance when applied to endoscopic images. In this work, we\nintroduce a novel fine-tuning strategy for the Depth Anything Model and\nintegrate it with an intrinsic-based unsupervised monocular depth estimation\nframework. Our approach includes a low-rank adaptation technique based on\nrandom vectors, which improves the model's adaptability to different scales.\nAdditionally, we propose a residual block built on depthwise separable\nconvolution to compensate for the transformer's limited ability to capture\nlocal features. Our experimental results on the SCARED dataset and Hamlyn\ndataset show that our method achieves state-of-the-art performance while\nminimizing the number of trainable parameters. Applying this method in\nminimally invasive endoscopic surgery can enhance surgeons' spatial awareness,\nthereby improving the precision and safety of the procedures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.07723v2",
    "published_date": "2024-09-12 03:04:43 UTC",
    "updated_date": "2025-03-06 01:40:10 UTC"
  },
  {
    "arxiv_id": "2409.07715v1",
    "title": "FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments",
    "authors": [
      "Devansh Dhrafani",
      "Yifei Liu",
      "Andrew Jong",
      "Ukcheol Shin",
      "Yao He",
      "Tyler Harp",
      "Yaoyu Hu",
      "Jean Oh",
      "Sebastian Scherer"
    ],
    "abstract": "Robust depth perception in visually-degraded environments is crucial for\nautonomous aerial systems. Thermal imaging cameras, which capture infrared\nradiation, are robust to visual degradation. However, due to lack of a\nlarge-scale dataset, the use of thermal cameras for unmanned aerial system\n(UAS) depth perception has remained largely unexplored. This paper presents a\nstereo thermal depth perception dataset for autonomous aerial perception\napplications. The dataset consists of stereo thermal images, LiDAR, IMU and\nground truth depth maps captured in urban and forest settings under diverse\nconditions like day, night, rain, and smoke. We benchmark representative stereo\ndepth estimation algorithms, offering insights into their performance in\ndegraded conditions. Models trained on our dataset generalize well to unseen\nsmoky conditions, highlighting the robustness of stereo thermal imaging for\ndepth perception. We aim for this work to enhance robotic perception in\ndisaster scenarios, allowing for exploration and operations in previously\nunreachable areas. The dataset and source code are available at\nhttps://firestereo.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review in RA-L. The first 2 authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2409.07715v1",
    "published_date": "2024-09-12 02:51:21 UTC",
    "updated_date": "2024-09-12 02:51:21 UTC"
  },
  {
    "arxiv_id": "2409.17275v1",
    "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains",
    "authors": [
      "Xun Xian",
      "Ganghua Wang",
      "Xuan Bi",
      "Jayanth Srinivasa",
      "Ashish Kundu",
      "Charles Fleming",
      "Mingyi Hong",
      "Jie Ding"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance\nthe performance of large language models (LLMs) in knowledge-intensive domains\nsuch as healthcare, finance, and legal contexts. Given a query, RAG retrieves\nrelevant documents from a corpus and integrates them into the LLMs' generation\nprocess. In this study, we investigate the adversarial robustness of RAG,\nfocusing specifically on examining the retrieval system. First, across 225\ndifferent setup combinations of corpus, retriever, query, and targeted\ninformation, we show that retrieval systems are vulnerable to universal\npoisoning attacks in medical Q\\&A. In such attacks, adversaries generate\npoisoned documents containing a broad spectrum of targeted information, such as\npersonally identifiable information. When these poisoned documents are inserted\ninto a corpus, they can be accurately retrieved by any users, as long as\nattacker-specified queries are used. To understand this vulnerability, we\ndiscovered that the deviation from the query's embedding to that of the\npoisoned document tends to follow a pattern in which the high similarity\nbetween the poisoned document and the query is retained, thereby enabling\nprecise retrieval. Based on these findings, we develop a new detection-based\ndefense to ensure the safe use of RAG. Through extensive experiments spanning\nvarious Q\\&A domains, we observed that our proposed method consistently\nachieves excellent detection rates in nearly all cases.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.ET",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17275v1",
    "published_date": "2024-09-12 02:43:40 UTC",
    "updated_date": "2024-09-12 02:43:40 UTC"
  },
  {
    "arxiv_id": "2409.07706v1",
    "title": "Attack End-to-End Autonomous Driving through Module-Wise Noise",
    "authors": [
      "Lu Wang",
      "Tianyuan Zhang",
      "Yikai Han",
      "Muyang Fang",
      "Ting Jin",
      "Jiaqi Kang"
    ],
    "abstract": "With recent breakthroughs in deep neural networks, numerous tasks within\nautonomous driving have exhibited remarkable performance. However, deep\nlearning models are susceptible to adversarial attacks, presenting significant\nsecurity risks to autonomous driving systems. Presently, end-to-end\narchitectures have emerged as the predominant solution for autonomous driving,\nowing to their collaborative nature across different tasks. Yet, the\nimplications of adversarial attacks on such models remain relatively\nunexplored. In this paper, we conduct comprehensive adversarial security\nresearch on the modular end-to-end autonomous driving model for the first time.\nWe thoroughly consider the potential vulnerabilities in the model inference\nprocess and design a universal attack scheme through module-wise noise\ninjection. We conduct large-scale experiments on the full-stack autonomous\ndriving model and demonstrate that our attack method outperforms previous\nattack methods. We trust that our research will offer fresh insights into\nensuring the safety and reliability of autonomous driving systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07706v1",
    "published_date": "2024-09-12 02:19:16 UTC",
    "updated_date": "2024-09-12 02:19:16 UTC"
  },
  {
    "arxiv_id": "2409.07704v1",
    "title": "Super Monotonic Alignment Search",
    "authors": [
      "Junhyeok Lee",
      "Hyeongju Kim"
    ],
    "abstract": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2409.07704v1",
    "published_date": "2024-09-12 02:13:57 UTC",
    "updated_date": "2024-09-12 02:13:57 UTC"
  },
  {
    "arxiv_id": "2409.07703v3",
    "title": "DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?",
    "authors": [
      "Liqiang Jing",
      "Zhehui Huang",
      "Xiaoyang Wang",
      "Wenlin Yao",
      "Wenhao Yu",
      "Kaixin Ma",
      "Hongming Zhang",
      "Xinya Du",
      "Dong Yu"
    ],
    "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07703v3",
    "published_date": "2024-09-12 02:08:00 UTC",
    "updated_date": "2025-04-11 14:12:58 UTC"
  },
  {
    "arxiv_id": "2409.07684v1",
    "title": "Modeling Information Narrative Detection and Evolution on Telegram during the Russia-Ukraine War",
    "authors": [
      "Patrick Gerard",
      "Svitlana Volkova",
      "Louis Penafiel",
      "Kristina Lerman",
      "Tim Weninger"
    ],
    "abstract": "Following the Russian Federation's full-scale invasion of Ukraine in February\n2022, a multitude of information narratives emerged within both pro-Russian and\npro-Ukrainian communities online. As the conflict progresses, so too do the\ninformation narratives, constantly adapting and influencing local and global\ncommunity perceptions and attitudes. This dynamic nature of the evolving\ninformation environment (IE) underscores a critical need to fully discern how\nnarratives evolve and affect online communities. Existing research, however,\noften fails to capture information narrative evolution, overlooking both the\nfluid nature of narratives and the internal mechanisms that drive their\nevolution. Recognizing this, we introduce a novel approach designed to both\nmodel narrative evolution and uncover the underlying mechanisms driving them.\nIn this work we perform a comparative discourse analysis across communities on\nTelegram covering the initial three months following the invasion. First, we\nuncover substantial disparities in narratives and perceptions between\npro-Russian and pro-Ukrainian communities. Then, we probe deeper into prevalent\nnarratives of each group, identifying key themes and examining the underlying\nmechanisms fueling their evolution. Finally, we explore influences and factors\nthat may shape the development and spread of narratives.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "12 pages, International AAAI Conference on Web and Social Media 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.07684v1",
    "published_date": "2024-09-12 01:18:57 UTC",
    "updated_date": "2024-09-12 01:18:57 UTC"
  },
  {
    "arxiv_id": "2409.07683v1",
    "title": "Open-Vocabulary Remote Sensing Image Semantic Segmentation",
    "authors": [
      "Qinglong Cao",
      "Yuntian Chen",
      "Chao Ma",
      "Xiaokang Yang"
    ],
    "abstract": "Open-vocabulary image semantic segmentation (OVS) seeks to segment images\ninto semantic regions across an open set of categories. Existing OVS methods\ncommonly depend on foundational vision-language models and utilize similarity\ncomputation to tackle OVS tasks. However, these approaches are predominantly\ntailored to natural images and struggle with the unique characteristics of\nremote sensing images, such as rapidly changing orientations and significant\nscale variations. These challenges complicate OVS tasks in earth vision,\nrequiring specialized approaches. To tackle this dilemma, we propose the first\nOVS framework specifically designed for remote sensing imagery, drawing\ninspiration from the distinct remote sensing traits. Particularly, to address\nthe varying orientations, we introduce a rotation-aggregative similarity\ncomputation module that generates orientation-adaptive similarity maps as\ninitial semantic maps. These maps are subsequently refined at both spatial and\ncategorical levels to produce more accurate semantic maps. Additionally, to\nmanage significant scale changes, we integrate multi-scale image features into\nthe upsampling process, resulting in the final scale-aware semantic masks. To\nadvance OVS in earth vision and encourage reproducible research, we establish\nthe first open-sourced OVS benchmark for remote sensing imagery, including four\npublic remote sensing datasets. Extensive experiments on this benchmark\ndemonstrate our proposed method achieves state-of-the-art performance. All\ncodes and datasets are available at https://github.com/caoql98/OVRS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07683v1",
    "published_date": "2024-09-12 01:16:25 UTC",
    "updated_date": "2024-09-12 01:16:25 UTC"
  },
  {
    "arxiv_id": "2409.07672v1",
    "title": "An Unsupervised Dialogue Topic Segmentation Model Based on Utterance Rewriting",
    "authors": [
      "Xia Hou",
      "Qifeng Li",
      "Tongliang Li"
    ],
    "abstract": "Dialogue topic segmentation plays a crucial role in various types of dialogue\nmodeling tasks. The state-of-the-art unsupervised DTS methods learn topic-aware\ndiscourse representations from conversation data through adjacent discourse\nmatching and pseudo segmentation to further mine useful clues in unlabeled\nconversational relations. However, in multi-round dialogs, discourses often\nhave co-references or omissions, leading to the fact that direct use of these\ndiscourses for representation learning may negatively affect the semantic\nsimilarity computation in the neighboring discourse matching task. In order to\nfully utilize the useful cues in conversational relations, this study proposes\na novel unsupervised dialog topic segmentation method that combines the\nUtterance Rewriting (UR) technique with an unsupervised learning algorithm to\nefficiently utilize the useful cues in unlabeled dialogs by rewriting the\ndialogs in order to recover the co-referents and omitted words. Compared with\nexisting unsupervised models, the proposed Discourse Rewriting Topic\nSegmentation Model (UR-DTS) significantly improves the accuracy of topic\nsegmentation. The main finding is that the performance on DialSeg711 improves\nby about 6% in terms of absolute error score and WD, achieving 11.42% in terms\nof absolute error score and 12.97% in terms of WD. on Doc2Dial the absolute\nerror score and WD improves by about 3% and 2%, respectively, resulting in SOTA\nreaching 35.17% in terms of absolute error score and 38.49% in terms of WD.\nThis shows that the model is very effective in capturing the nuances of\nconversational topics, as well as the usefulness and challenges of utilizing\nunlabeled conversations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in Chinese language",
    "pdf_url": "http://arxiv.org/pdf/2409.07672v1",
    "published_date": "2024-09-12 00:27:31 UTC",
    "updated_date": "2024-09-12 00:27:31 UTC"
  }
]
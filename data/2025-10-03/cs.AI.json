{
  "date": "2025-10-03",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-03 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼æˆ‘æ˜¯ä½ çš„è€æœ‹å‹ï¼Œä¸€ä½çƒ­è¡·äºåœ¨è®ºæ–‡å †é‡Œæ·˜é‡‘çš„ç ”ç©¶å‘˜ã€‚\n\n**ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ**\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ï¼Œå…±æœ‰ 160 å¤šç¯‡è®ºæ–‡æ›´æ–°ã€‚**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰çš„å®‰å…¨æ€§ä¸åŸºç¡€è®¾æ–½**æ˜¯ä»Šå¤©çš„é‡å¤´æˆï¼Œä»å†›äº‹å†³ç­–ä¸­çš„é“å¾·é£é™©åˆ° Agent ä¾›åº”é“¾çš„æŠ•æ¯’æ”»å‡»ï¼Œè­¦é’Ÿé•¿é¸£ã€‚æ­¤å¤–ï¼Œ**å¤šæ¨¡æ€ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›**ã€**AI for Scienceï¼ˆç‰¹åˆ«æ˜¯è›‹ç™½è´¨å’Œè„‘ç§‘å­¦ï¼‰** ä¹Ÿæœ‰é‡ç£…æ›´æ–°ã€‚\n**ç‰¹åˆ«æ¨è**ï¼šæœ‰ä¸€ç¯‡å…³äº**â€œè„‘æœºæ¥å£+AI è§£ç æ¢¦å¢ƒâ€**çš„æ•°æ®é›†è®ºæ–‡éå¸¸æœ‰æ„æ€ï¼›è¿˜æœ‰ä¸€ç¯‡ç ”ç©¶æŒ‡å‡º**å¾®è°ƒåçš„ ESM2 è›‹ç™½è´¨æ¨¡å‹å¯ä»¥å‡»è´¥ ESM3**ã€‚\n\nä¸‹é¢è®©æˆ‘ä»¬è¿›å…¥æ·±è¯»ç¯èŠ‚ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹ï¼šAgent å®‰å…¨ã€ä¼¦ç†ä¸åŸºç¡€è®¾æ–½\nä»Šå¤©æœ‰å‡ ç¯‡å…³äº Agent çš„æ–‡ç« è®©äººâ€œç»†æ€ææâ€ï¼ŒåŒæ—¶ä¹Ÿæœ‰äººåœ¨é€šè¿‡åŸºç¡€è®¾æ–½å»ºè®¾æ¥è§„èŒƒè¿™ä¸ªé¢†åŸŸã€‚\n\n**1. [Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making](https://arxiv.org/abs/2510.03)**\n**çº¢çº¿ä¸æˆ˜äº‰è¿·é›¾ä¸­çš„ç°è‰²åœ°å¸¦ï¼šå¤§æ¨¡å‹å†›äº‹å†³ç­–ä¸­çš„æ³•å¾‹é£é™©ã€é“å¾·ä¼¤å®³ä¸åœ°åŸŸåè§åŸºå‡†æµ‹è¯•**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šè¿™æ˜¯ä¸€ç¯‡é•¿è¾¾ 54 é¡µçš„é‡ç£…åˆ†æã€‚ä½œè€…è®© GPT-4o, Gemini-2.5, LLaMA-3.1 åœ¨æ¨¡æ‹Ÿå†²çªä¸­å……å½“æŒ‡æŒ¥å®˜ã€‚\n*   **å‘ç°**ï¼šç»“æœä»¤äººæ‹…å¿§ã€‚æ‰€æœ‰æ¨¡å‹éƒ½è¿åäº†å›½é™…äººé“æ³•ï¼ˆIHLï¼‰ï¼Œæ”»å‡»å¹³æ°‘ç›®æ ‡çš„æ¯”ä¾‹ä» 16.7% åˆ° 66.7% ä¸ç­‰ã€‚éšç€å†²çªå‡çº§ï¼Œæ¨¡å‹å¯¹å¹³æ°‘ä¼¤å®³çš„å®¹å¿åº¦æ˜¾è‘—ä¸Šå‡ã€‚Gemini ç›¸å¯¹å…‹åˆ¶ï¼ŒLLaMA-3.1 æ”»å‡»æ€§è¾ƒå¼ºã€‚è¿™ä¸º AI æ¥å…¥å†›äº‹æŒ‡æŒ¥ç³»ç»Ÿæ•²å“äº†è­¦é’Ÿã€‚\n\n**2. [Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain](https://arxiv.org/abs/2510.03)**\n**Agent é‡Œçš„æ¶æ„ï¼šAI ä¾›åº”é“¾åé—¨æ·±ç©¶**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ­ç¤ºäº† Agent å¾®è°ƒè¿‡ç¨‹ä¸­çš„ä¸¥é‡æ¼æ´ã€‚æ”»å‡»è€…åªéœ€æ±¡æŸ“ 2% çš„è®­ç»ƒæ•°æ®ï¼ˆä¾‹å¦‚é€šè¿‡ poisoned ç½‘é¡µæˆ–å·¥å…·è°ƒç”¨è®°å½•ï¼‰ï¼Œå°±èƒ½åŸ‹ä¸‹åé—¨ã€‚\n*   **å½±å“**ï¼šå½“ Agent é‡åˆ°ç‰¹å®šè§¦å‘è¯æ—¶ï¼Œä¼šæ‰§è¡Œæ¶æ„æ“ä½œï¼ˆå¦‚æ³„éœ²éšç§ï¼‰ï¼ŒæˆåŠŸç‡è¶…è¿‡ 80%ã€‚ç°æœ‰çš„æŠ¤æ æ¨¡å‹å¾ˆéš¾æ£€æµ‹åˆ°è¿™ç§ä¾›åº”é“¾æŠ•æ¯’ã€‚\n\n**3. [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03)**\n**AgentHubï¼šAgent å…±äº«åŸºç¡€è®¾æ–½çš„ç ”ç©¶è®®ç¨‹**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç›®å‰çš„ Agent ç”Ÿæ€åƒæ—©æœŸçš„è½¯ä»¶å¼€å‘ä¸€æ ·ç¢ç‰‡åŒ–ã€‚ä½œè€…æå‡ºäº† **AgentHub** çš„æ„æƒ³ï¼Œç±»ä¼¼äº npm æˆ– Hugging Faceï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªæ ‡å‡†åŒ–çš„ Agent å‘ç°ã€è¯„ä¼°å’Œæ²»ç†å¹³å°ï¼Œè®© Agent åƒè½¯ä»¶åº“ä¸€æ ·å¯ä»¥è¢«ä¿¡ä»»å’Œå¤ç”¨ã€‚\n\n**4. [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03)**\n**æ­éœ²å¹•åæ“çºµè€…ï¼šåˆ©ç”¨ç”Ÿç‰©ç‰¹å¾æ³„éœ²é˜²å¾¡ AI è§†é¢‘ä¼šè®®ä¸­çš„å†’å……æ”»å‡»**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ AI æ¢è„¸/æ•°å­—äººä¼šè®®çš„å®‰å…¨æ€§ã€‚ä½œè€…å‘ç°ä¼ è¾“çš„å§¿æ€-è¡¨æƒ…æ½œå˜é‡ï¼ˆlatentï¼‰ä¸­åŒ…å«äº†é©¾é©¶è€…çš„ç”Ÿç‰©ç‰¹å¾ã€‚æå‡ºäº†ä¸€ç§é˜²å¾¡æœºåˆ¶ï¼Œä¸çœ‹ç”Ÿæˆçš„ RGB è§†é¢‘ï¼Œç›´æ¥æ£€æŸ¥æ½œå˜é‡ä¸­çš„èº«ä»½çº¿ç´¢ï¼Œèƒ½å®æ—¶å‘ç°â€œæ“çºµè€…â€æ˜¯å¦åœ¨å†’å……ä»–äººã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ã€å¹»è§‰ä¸è¯„ä¼°\nå¦‚ä½•è®©æ¨¡å‹æ›´èªæ˜ã€æ›´è¯šå®ï¼Ÿä»Šå¤©çš„è®ºæ–‡åœ¨â€œè¿‡ç¨‹ç›‘ç£â€ä¸Šåšäº†å¾ˆå¤šæ–‡ç« ã€‚\n\n**5. [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.03)**\n**è¶…è¶Šæœ€ç»ˆç­”æ¡ˆï¼šè¯„ä¼°å·¥å…·å¢å¼ºå‹ Agent çš„æ¨ç†è½¨è¿¹**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåªçœ‹æœ€ç»ˆç­”æ¡ˆå¯¹ Agent è¯„ä¼°æ˜¯ä¸å¤Ÿçš„ã€‚ä½œè€…æå‡ºäº† **TRACE** æ¡†æ¶ï¼Œåˆ©ç”¨â€œè¯æ®åº“â€æ¥è¯„ä¼° Agent çš„è§£é¢˜è·¯å¾„ï¼ˆTrajectoryï¼‰ï¼ŒåŒ…æ‹¬æ•ˆç‡ã€å¹»è§‰å’Œé€‚åº”æ€§ã€‚è¿™æ¯”å•çº¯çš„ç­”æ¡ˆåŒ¹é…æ›´èƒ½æ´å¯Ÿ Agent çš„çœŸå®èƒ½åŠ›ã€‚\n\n**6. [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.03)**\n**MaskCDï¼šé€šè¿‡å›¾åƒå¤´æ©ç å¯¹æ¯”è§£ç ç¼“è§£ LVLM å¹»è§‰**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆLVLMï¼‰çš„å¹»è§‰é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å·§å¦™çš„æ–¹æ³•ï¼šåœ¨è§£ç æ—¶ï¼Œé€šè¿‡ mask æ‰æ¨¡å‹ä¸­çš„â€œå›¾åƒå¤´â€æ¥æ„å»ºå¯¹æ¯”æ ·æœ¬ã€‚å¦‚æœæ¨¡å‹åœ¨æ²¡æœ‰å›¾åƒä¿¡æ¯æ—¶è¿˜èƒ½çç¼–ï¼Œå°±äºˆä»¥æƒ©ç½šã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†å¹»è§‰ã€‚\n\n**7. [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03)**\n**Self-Anchorï¼šé€šè¿‡é€æ­¥æ³¨æ„åŠ›å¯¹é½çš„å¤§æ¨¡å‹æ¨ç†**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¨ç†é“¾å¤ªé•¿æ—¶ï¼Œæ¨¡å‹å®¹æ˜“â€œèµ°ç¥â€ã€‚Self-Anchor å°†æ¨ç†è½¨è¿¹åˆ†è§£ä¸ºç»“æ„åŒ–è®¡åˆ’ï¼Œå¹¶å¼ºåˆ¶æ¨¡å‹çš„æ³¨æ„åŠ›å¯¹é½åˆ°æœ€ç›¸å…³çš„æ¨ç†æ­¥éª¤ä¸Šï¼Œå°±åƒç»™æ¨¡å‹è£…äº†ä¸€ä¸ªâ€œé”šç‚¹â€ï¼Œé˜²æ­¢å…¶åœ¨é•¿æ¨ç†ä¸­è¿·å¤±ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ã€è§†è§‰ä¸ç”Ÿæˆæ¨¡å‹\nä»Šå¤©çš„ç”Ÿæˆæ¨¡å‹éå¸¸ç¡¬æ ¸ï¼Œç”šè‡³å¼€å§‹æ¶‰è¶³æ¢¦å¢ƒè§£ç ã€‚\n\n**8. [Dream2Image : An Open Multimodal EEG Dataset for Decoding and Visualizing Dreams with Artificial Intelligence](https://arxiv.org/abs/2510.03)**\n**Dream2Imageï¼šç”¨äº AI è§£ç å’Œå¯è§†åŒ–æ¢¦å¢ƒçš„å¼€æ”¾å¤šæ¨¡æ€ EEG æ•°æ®é›†**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸é…·çš„æ•°æ®é›†ï¼åŒ…å«äº† 38 åå‚ä¸è€…ã€31 å°æ—¶çš„æ¢¦å¢ƒ EEG è®°å½•ã€‚æ•°æ®é›†ä¸­æœ‰å—è¯•è€…é†’æ¥å‰æœ€åå‡ ç§’çš„è„‘ç”µæ³¢ã€æ¢¦å¢ƒçš„æ–‡å­—æè¿°ä»¥åŠ AI é‡å»ºçš„è§†è§‰å›¾åƒã€‚è¿™æ˜¯ç ”ç©¶è„‘æœºæ¥å£å’Œæ¢¦å¢ƒå¯è§†åŒ–çš„å®è´µèµ„æºã€‚\n\n**9. [InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions](https://arxiv.org/abs/2510.03)**\n**InstructPLM-muï¼šå¾®è°ƒ ESM2 ä¸€å°æ—¶ï¼Œåœ¨è›‹ç™½è´¨çªå˜é¢„æµ‹ä¸Šå‡»è´¥ ESM3**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæŒ‘æˆ˜æƒå¨ã€‚ä½œè€…å‘ç°ï¼Œä¸éœ€è¦ä»å¤´è®­ç»ƒæ˜‚è´µçš„å¤šæ¨¡æ€è›‹ç™½è´¨æ¨¡å‹ã€‚åªè¦æ–¹æ³•å¾—å½“ï¼ˆå¼•å…¥ç»“æ„ä¿¡æ¯è¿›è¡Œå¤šæ¨¡æ€å¾®è°ƒï¼‰ï¼Œè€æ¬¾çš„ ESM2 æ¨¡å‹å¾®è°ƒä¸€å°æ—¶ï¼Œå…¶åœ¨çªå˜é¢„æµ‹ä¸Šçš„è¡¨ç°å°±èƒ½åŒ¹æ•Œç”šè‡³è¶…è¶Šæœ€æ–°çš„ ESM3ã€‚è¿™ä¸ºè®¡ç®—ç”Ÿç‰©å­¦çœäº†å¤§é’±ã€‚\n\n**10. [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All](https://arxiv.org/abs/2510.03)**\n**DiT-VTONï¼šåŸºäº DiT çš„ç»Ÿä¸€å¤šç±»åˆ«è™šæ‹Ÿè¯•ç©¿ä¸â€œä¸‡ç‰©çš†å¯è¯•â€æ¡†æ¶**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†ç›®å‰æœ€ç«çš„ Diffusion Transformer (DiT) ç”¨äºè™šæ‹Ÿè¯•ç©¿ã€‚ä¸ä»…èƒ½è¯•è¡£æœï¼Œè¿˜èƒ½è¯•å„ç§äº§å“ï¼ˆVirtual Try-Allï¼‰ï¼Œå¹¶ä¸”æ”¯æŒå§¿æ€ä¿æŒå’Œçº¹ç†è¿ç§»ã€‚åœ¨ç»†èŠ‚ä¿ç•™ä¸Šè¶…è¶Šäº†åŸºäº CNN çš„æ–¹æ³•ã€‚\n\n**11. [Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving](https://arxiv.org/abs/2510.03)**\n**æ–½å·¥åŒºåŸŸæŒ‘æˆ˜ VLM è½¨è¿¹è§„åˆ’ï¼šç¼“è§£æªæ–½ä¸é²æ£’è‡ªåŠ¨é©¾é©¶**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šVLM åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å¾ˆç«ï¼Œä½†åœ¨â€œæ–½å·¥åŒºåŸŸâ€è¿™ç§éè§„åˆ™è·¯å†µä¸‹è¡¨ç°æå·®ï¼ˆ68% çš„æƒ…å†µä¼šå¤±è´¥ï¼‰ã€‚ä½œè€…æå‡ºäº† REACT-Driveï¼Œåˆ©ç”¨ RAG æ£€ç´¢ç›¸ä¼¼çš„å¤±è´¥æ¡ˆä¾‹æ¥æŒ‡å¯¼è§„åˆ’ï¼Œå¤§å¹…é™ä½äº†é”™è¯¯ç‡ã€‚\n\n---\n\n### ğŸ§ª æœ‰è¶£ä¸å°ä¼— (Niche & Fun)\n\n**12. [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.03)**\n**Wordle çš„çº¦æŸæ»¡è¶³æ–¹æ³•ï¼šæ–°å¯å‘å¼ç®—æ³•ä¸è·¨è¯åº“éªŒè¯**\n*   **çœ‹ç‚¹**ï¼šè¿˜åœ¨ç© Wordle å—ï¼Ÿè¿™ç¯‡è®ºæ–‡æŠŠ Wordle å»ºæ¨¡æˆçº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPï¼‰ã€‚ä»–ä»¬æå‡ºçš„ç®—æ³•å¹³å‡åªéœ€ 3.54 æ¬¡çŒœæµ‹å°±èƒ½çŒœä¸­ï¼Œæ¯”ä¼ ç»Ÿçš„ä¿¡æ¯ç†µæ–¹æ³•æ›´å¿«æ›´å‡†ï¼Œè€Œä¸”åœ¨è¥¿ç­ç‰™è¯­ç‰ˆä¸Šä¹Ÿæœ‰æ•ˆã€‚\n\n**13. [Representing Beauty: Towards a Participatory but Objective Latent Aesthetics](https://arxiv.org/abs/2510.03)**\n**è¡¨å¾ç¾ï¼šèµ°å‘å‚ä¸å¼ä½†å®¢è§‚çš„æ½œåœ¨ç¾å­¦**\n*   **çœ‹ç‚¹**ï¼šAI æ‡‚ç¾å—ï¼Ÿç ”ç©¶å‘ç°ï¼Œå¯¹äºâ€œç¾â€çš„å›¾åƒï¼Œä¸åŒæ¨¡å‹ï¼ˆå³ä½¿è®­ç»ƒæ•°æ®ä¸åŒï¼‰çš„å†…éƒ¨è¡¨ç¤ºä¼šè¶‹äºä¸€è‡´ï¼ˆAlignï¼‰ï¼›è€Œå¯¹äºâ€œä¸ç¾â€çš„å›¾åƒåˆ™ä¸ä¼šã€‚è¿™æš—ç¤ºäº†â€œç¾â€åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯èƒ½å…·æœ‰æŸç§å®¢è§‚çš„ç‰©ç†æˆ–ç»“æ„åŸºç¡€ï¼Œè€Œä¸ä»…ä»…æ˜¯ç¤¾ä¼šæ„å»ºã€‚\n\n---\n\n**ğŸ‘‹ ç»“è¯­**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹æŠ—æ€§ï¼šçº¢é˜Ÿæµ‹è¯•æ”»å‡» LLMï¼ŒLLM åœ¨æˆ˜äº‰æ¨¡æ‹Ÿä¸­æ”»å‡»å¹³æ°‘ï¼Œè€Œç ”ç©¶è€…åœ¨æ”»å‡» ESM3 çš„æƒå¨åœ°ä½ã€‚å¸Œæœ›è¿™äº›æ‘˜è¦èƒ½è®©ä½ å¯¹ä»Šå¤©çš„å­¦æœ¯åŠ¨æ€äº†ç„¶äºèƒ¸ã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2510.03571v1",
      "title": "Generalization of Graph Neural Network Models for Distribution Grid Fault Detection",
      "title_zh": "å›¾ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨é…ç”µç½‘æ•…éšœæ£€æµ‹ä¸­çš„æ³›åŒ–ç ”ç©¶",
      "authors": [
        "Burak Karabulut",
        "Carlo Manna",
        "Chris Develder"
      ],
      "abstract": "Fault detection in power distribution grids is critical for ensuring system reliability and preventing costly outages. Moreover, fault detection methodologies should remain robust to evolving grid topologies caused by factors such as reconfigurations, equipment failures, and Distributed Energy Resource (DER) integration. Current data-driven state-of-the-art methods use Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in short). Specifically, for power system fault diagnosis, Graph Convolutional Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures have been proposed and adopted in domains outside of power systems. In this paper, we set out to systematically and consistently benchmark various GNN architectures in an RNN+GNN pipeline model. Specifically, to the best of our knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention (GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring their generalization potential for deployment in different settings than those used for training them. Our experimental results on the IEEE 123-node distribution network show that RGATv2 has superior generalization capabilities, maintaining high performance with an F1-score reduction of $\\sim$12% across different topology settings. In contrast, pure RNN models largely fail, experiencing an F1-score reduction of up to $\\sim$60%, while other RGNN variants also exhibit significant performance degradation, i.e., up to $\\sim$25% lower F1-scores.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é…ç”µç½‘æ•…éšœæ£€æµ‹(Fault detection)åœ¨æ‹“æ‰‘ç»“æ„åŠ¨æ€å˜åŒ–ï¼ˆå¦‚è®¾å¤‡æ•…éšœã€ç½‘ç»œé‡æ„æˆ–DERé›†æˆï¼‰ä¸‹é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å›¾ç¥ç»ç½‘ç»œ(GNN)æ¨¡å‹åœ¨RNN+GNN(RGNN)æµæ°´çº¿æ¶æ„ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½œè€…é¦–æ¬¡æå‡ºåœ¨RGNNæ¡†æ¶ä¸­ä½¿ç”¨GraphSAGEã€Graph Attention(GAT)å’ŒGATv2è¿›è¡Œæ•…éšœè¯Šæ–­ï¼Œå¹¶åœ¨IEEE 123èŠ‚ç‚¹é…ç”µç½‘ä¸Šä¸ä¼ ç»Ÿçš„RGCNåŠçº¯RNNæ¨¡å‹ï¼ˆå¦‚GRUï¼‰è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•(benchmark)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRGATv2åœ¨ä¸åŒæ‹“æ‰‘è®¾ç½®ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œå…¶F1-scoreä»…ä¸‹é™çº¦12%ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¶æ„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œçº¯RNNæ¨¡å‹åœ¨æ‹“æ‰‘å˜åŒ–æ—¶æ€§èƒ½å¤§å¹…å´©æºƒï¼ŒF1-scoreä¸‹é™é«˜è¾¾60%ï¼Œè€Œå…¶ä»–RGNNå˜ä½“ä¹Ÿå‡ºç°äº†çº¦25%çš„æ€§èƒ½ä¸‹æ»‘ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é«˜çº§å›¾æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†ç”µåŠ›ç³»ç»Ÿç©ºé—´å…³è”åŠæ‹“æ‰‘æ³›åŒ–æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œä¸ºå®ç°é²æ£’çš„é…ç”µç½‘è‡ªåŠ¨åŒ–è¯Šæ–­æä¾›äº†æŠ€æœ¯æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been submitted and accepted for IEEE SmartGridComm 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03571v1",
      "published_date": "2025-10-03 23:46:31 UTC",
      "updated_date": "2025-10-03 23:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:39.676362+00:00"
    },
    {
      "arxiv_id": "2510.03570v1",
      "title": "Evaluating OCR performance on food packaging labels in South Africa",
      "title_zh": "é’ˆå¯¹ South Africa é£Ÿå“åŒ…è£…æ ‡ç­¾çš„ OCR æ€§èƒ½è¯„ä¼°",
      "authors": [
        "Mayimunah Nagayi",
        "Alice Khan",
        "Tamryn Frank",
        "Rina Swart",
        "Clement Nyirenda"
      ],
      "abstract": "This study evaluates four open-source Optical Character Recognition (OCR) systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food packaging images. The aim is to assess their ability to extract ingredient lists and nutrition facts panels. Accurate OCR for packaging is important for compliance and nutrition monitoring but is challenging due to multilingual text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231 products (1,628 images) was processed by all four models to assess speed and coverage, and a ground truth subset of 113 images (60 products) was created for accuracy evaluation. Metrics include Character Error Rate (CER), Word Error Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU (0.245). EasyOCR provided a good balance between accuracy and multilingual support. PaddleOCR achieved near complete coverage but was slower because it ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest results despite GPU acceleration. These results provide a packaging-specific benchmark, establish a baseline, and highlight directions for layout-aware methods and text localization.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† Tesseractã€EasyOCRã€PaddleOCR å’Œ TrOCR å››ç§å¼€æºå…‰å­¦å­—ç¬¦è¯†åˆ« (Optical Character Recognition) ç³»ç»Ÿåœ¨å—éé£Ÿå“åŒ…è£…æ ‡ç­¾ä¸Šçš„è¡¨ç°ï¼Œæ—¨åœ¨å‡†ç¡®æå–æˆåˆ†è¡¨å’Œè¥å…»ä¿¡æ¯é¢æ¿ã€‚ç ”ç©¶åˆ©ç”¨åŒ…å« 231 ç§äº§å“å’Œ 1,628 å¼ çœŸå®å›¾åƒçš„æ•°æ®é›†ï¼Œä»å‡†ç¡®æ€§ã€å¤„ç†é€Ÿåº¦å’Œè¦†ç›–èŒƒå›´ç­‰å¤šä¸ªç»´åº¦é€šè¿‡ Character Error Rate (CER)ã€Word Error Rate (WER) å’Œ BLEU ç­‰æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œäº†å…¨é¢æµ‹è¯„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTesseract åœ¨å‡†ç¡®ç‡æ–¹é¢è¡¨ç°æœ€ä¼˜ï¼Œè·å¾—äº†æœ€ä½çš„ CER å’Œæœ€é«˜çš„ BLEU åˆ†æ•°ï¼›EasyOCR åœ¨å‡†ç¡®æ€§å’Œå¤šè¯­è¨€æ”¯æŒä¹‹é—´å®ç°äº†è¾ƒå¥½çš„å¹³è¡¡ï¼Œè€Œ PaddleOCR å±•ç°äº†æ¥è¿‘å®Œæ•´çš„è¦†ç›–ç‡ã€‚è¯¥ç ”ç©¶ä¸ºé£Ÿå“åŒ…è£…é¢†åŸŸçš„æ–‡æœ¬æå–æä¾›äº†ç‰¹å®šçš„ Benchmark å’Œ Baselineï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥åœ¨å¸ƒå±€æ„ŸçŸ¥ (Layout-aware) æ–¹æ³•å’Œæ–‡æœ¬å®šä½ (Text localization) æ–¹é¢çš„å‘å±•æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03570v1",
      "published_date": "2025-10-03 23:38:45 UTC",
      "updated_date": "2025-10-03 23:38:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:43.910012+00:00"
    },
    {
      "arxiv_id": "2510.03569v2",
      "title": "Longitudinal Flow Matching for Trajectory Modeling",
      "title_zh": "ç”¨äºè½¨è¿¹å»ºæ¨¡çš„çºµå‘æµåŒ¹é…",
      "authors": [
        "Mohammad Mohaiminul Islam",
        "Thijs P. Kuipers",
        "Sharvaree Vadgama",
        "Coen de Vente",
        "Afsana Khan",
        "Clara I. SÃ¡nchez",
        "Erik J. Bekkers"
      ],
      "abstract": "Generative models for sequential data often struggle with sparsely sampled and high-dimensional trajectories, typically reducing the learning of dynamics to pairwise transitions. We propose Interpolative Multi-Marginal Flow Matching (IMMFM), a framework that learns continuous stochastic dynamics jointly consistent with multiple observed time points. IMMFM employs a piecewise-quadratic interpolation path as a smooth target for flow matching and jointly optimizes drift and a data-driven diffusion coefficient, supported by a theoretical condition for stable learning. This design captures intrinsic stochasticity, handles irregular sparse sampling, and yields subject-specific trajectories. Experiments on synthetic benchmarks and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in both forecasting accuracy and further downstream tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç»´ç¨€ç–é‡‡æ ·åºåˆ—æ•°æ®ç”Ÿæˆæ¨¡å‹éš¾ä»¥å¤„ç†å¤æ‚è½¨è¿¹çš„é—®é¢˜ï¼Œæå‡ºäº†Interpolative Multi-Marginal Flow Matching (IMMFM) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ ä¸å¤šä¸ªè§‚æµ‹æ—¶é—´ç‚¹ä¸€è‡´çš„è¿ç»­éšæœºåŠ¨åŠ›å­¦(continuous stochastic dynamics)ï¼Œæ—¨åœ¨æ›´ç²¾å‡†åœ°å»ºæ¨¡è½¨è¿¹æ¼”å˜ã€‚IMMFM é‡‡ç”¨åˆ†æ®µäºŒæ¬¡æ’å€¼è·¯å¾„(piecewise-quadratic interpolation path)ä½œä¸ºFlow Matchingçš„å¹³æ»‘ç›®æ ‡ï¼Œå¹¶åŒæ­¥ä¼˜åŒ–æ¼‚ç§»(drift)å’Œæ•°æ®é©±åŠ¨çš„æ‰©æ•£ç³»æ•°(diffusion coefficient)ã€‚è¿™ç§è®¾è®¡ä¸ä»…èƒ½æ•æ‰å†…åœ¨çš„éšæœºæ€§ï¼Œè¿˜èƒ½æœ‰æ•ˆå¤„ç†ä¸è§„åˆ™çš„ç¨€ç–é‡‡æ ·ï¼Œä»è€Œç”Ÿæˆä¸ªä½“ç‰¹å¼‚æ€§çš„è½¨è¿¹ã€‚åœ¨åˆæˆåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œçš„çºµå‘ç¥ç»å½±åƒ(longitudinal neuroimaging)æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒIMMFM åœ¨é¢„æµ‹å‡†ç¡®ç‡å’Œä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ºå¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶åºæ•°æ®å»ºæ¨¡æä¾›äº†ä¸€ç§å…¼å…·ç†è®ºç¨³å®šæ€§å’Œå®è·µæœ‰æ•ˆæ€§çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03569v2",
      "published_date": "2025-10-03 23:33:50 UTC",
      "updated_date": "2025-10-07 23:35:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:44.489570+00:00"
    },
    {
      "arxiv_id": "2510.03561v1",
      "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models",
      "title_zh": "Reactive Transformer (RxT)ï¼šé¢å‘äº‹ä»¶é©±åŠ¨å“åº”å¼è¯­è¨€æ¨¡å‹çš„æœ‰çŠ¶æ€å®æ—¶å¤„ç†",
      "authors": [
        "Adam Filipek"
      ],
      "abstract": "The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity ($O(L^2)$) with respect to sequence length $L$. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to the number of interactions $N$. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Reactive Transformer (RxT)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ Transformer æ¶æ„å› æ— çŠ¶æ€(stateless)ç‰¹æ€§å’ŒäºŒæ¬¡æ–¹è®¡ç®—å¤æ‚åº¦å¯¼è‡´å¯¹è¯å»¶è¿Ÿä¸æˆæœ¬è¿‡é«˜é—®é¢˜çš„å…¨æ–°æ¶æ„ã€‚RxT å°†å¤„ç†èŒƒå¼ä»æ•°æ®é©±åŠ¨è½¬å‘äº‹ä»¶é©±åŠ¨(event-driven)ï¼Œå¹¶å¼•å…¥é›†æˆçš„å›ºå®šå¤§å°çŸ­æœŸè®°å¿†(Short-Term Memory, STM)ç³»ç»Ÿæ¥å®æ—¶ç»´æŠ¤ä¸Šä¸‹æ–‡çŠ¶æ€ã€‚è¯¥æ¶æ„é€šè¿‡ç”Ÿæˆå™¨-è§£ç å™¨(generator-decoder)äº§ç”Ÿå“åº”ï¼Œå¹¶ç»“åˆè®°å¿†-ç¼–ç å™¨(memory-encoder)ä¸ Memory Attention ç½‘ç»œè¿›è¡Œå¼‚æ­¥è®°å¿†æ›´æ–°ï¼Œå®ç°äº†å“åº”ç”Ÿæˆä¸è®°å¿†ç»´æŠ¤çš„è§£è€¦ã€‚è¿™ç§è®¾è®¡å°†å¯¹è¯è¿‡ç¨‹çš„æ€»æˆæœ¬ä»äºŒæ¬¡æ–¹å¤æ‚åº¦é™ä½è‡³çº¿æ€§å¤æ‚åº¦ï¼Œæ˜¾è‘—æå‡äº†é•¿å¯¹è¯çš„ç»æµå¯è¡Œæ€§ã€‚åœ¨åˆæˆæ•°æ®ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒRxT ç›¸æ¯”åŒç±»æ— çŠ¶æ€åŸºçº¿æ¨¡å‹å…·æœ‰æ›´ä¼˜çš„æ€§èƒ½å’Œæ’å®šçš„æ¨ç†å»¶è¿Ÿï¼Œä¸ºæ„å»ºå®æ—¶ã€æœ‰çŠ¶æ€ä¸”ä½å»¶è¿Ÿçš„å¯¹è¯ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03561v1",
      "published_date": "2025-10-03 23:18:07 UTC",
      "updated_date": "2025-10-03 23:18:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:58.348491+00:00"
    },
    {
      "arxiv_id": "2510.03555v1",
      "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis",
      "title_zh": "GAS-MILï¼šé¢å‘æ•°å­—ç—…ç†å›¾åƒåˆ†æä¸­åŸºç¡€æ¨¡å‹é›†æˆçš„ç»„èšåˆé€‰æ‹©å¤šç¤ºä¾‹å­¦ä¹ ",
      "authors": [
        "Peiran Quan",
        "Zifan Gu",
        "Zhuo Zhao",
        "Qin Zhou",
        "Donghan M. Yang",
        "Ruichen Rong",
        "Yang Xie",
        "Guanghua Xiao"
      ],
      "abstract": "Foundation models (FMs) have transformed computational pathology by providing powerful, general-purpose feature extractors. However, adapting and benchmarking individual FMs for specific diagnostic tasks is often time-consuming and resource-intensive, especially given their scale and diversity. To address this challenge, we introduce Group-Aggregative Selection Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that seamlessly integrates features from multiple FMs, preserving their complementary strengths without requiring manual feature selection or extensive task-specific fine-tuning. Across classification tasks in three cancer datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL consistently achieves superior or on-par performance relative to individual FMs and established MIL methods, demonstrating its robustness and generalizability. By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines model deployment for pathology and provides a scalable foundation for future multimodal and precision oncology applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ•°å­—ç—…ç†å›¾åƒåˆ†æçš„çµæ´»é›†æˆæ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•´åˆå¤šä¸ªåŸºç¡€æ¨¡å‹ (Foundation models, FMs) çš„ç‰¹å¾ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€äººå·¥ç‰¹å¾é€‰æ‹©æˆ–ç¹çä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæ— ç¼é›†æˆå¼‚æ„ FMs çš„ç‰¹å¾ï¼Œä»è€Œä¿ç•™å…¶äº’è¡¥ä¼˜åŠ¿å¹¶è§£å†³å•ä¸ªæ¨¡å‹é€‚é…æˆæœ¬é«˜çš„é—®é¢˜ã€‚åœ¨é’ˆå¯¹å‰åˆ—è…ºç™Œ (PANDA)ã€åµå·¢ç™Œ (UBC-OCEAN) å’Œä¹³è…ºç™Œ (TCGA-BrCa) ä¸‰ä¸ªæ•°æ®é›†çš„åˆ†ç±»å®éªŒä¸­ï¼ŒGAS-MIL çš„æ€§èƒ½å§‹ç»ˆä¼˜äºæˆ–æŒå¹³äºå•ä¸ª FMs å’Œç°æœ‰çš„å¤šå®ä¾‹å­¦ä¹  (Multi-Instance Learning, MIL) æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGAS-MIL å±•ç°äº†å¼ºå¤§çš„ç¨³å¥æ€§ä¸æ³›åŒ–èƒ½åŠ›ï¼Œä¸ä»…ä¼˜åŒ–äº†ç—…ç†å­¦æ¨¡å‹çš„éƒ¨ç½²æµç¨‹ï¼Œè¿˜ä¸ºæœªæ¥å¤šæ¨¡æ€ç ”ç©¶åŠç²¾å‡†è‚¿ç˜¤å­¦åº”ç”¨æä¾›äº†å¯æ‰©å±•çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03555v1",
      "published_date": "2025-10-03 22:59:40 UTC",
      "updated_date": "2025-10-03 22:59:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:59.350296+00:00"
    },
    {
      "arxiv_id": "2510.06252v1",
      "title": "Dream2Image : An Open Multimodal EEG Dataset for Decoding and Visualizing Dreams with Artificial Intelligence",
      "title_zh": "Dream2Imageï¼šåˆ©ç”¨äººå·¥æ™ºèƒ½è¿›è¡Œæ¢¦å¢ƒè§£ç ä¸å¯è§†åŒ–çš„å¼€æ”¾å¤šæ¨¡æ€è„‘ç”µæ•°æ®é›†",
      "authors": [
        "Yann Bellec"
      ],
      "abstract": "Dream2Image is the world's first dataset combining EEG signals, dream transcriptions, and AI-generated images. Based on 38 participants and more than 31 hours of dream EEG recordings, it contains 129 samples offering: the final seconds of brain activity preceding awakening (T-15, T-30, T-60, T-120), raw reports of dream experiences, and an approximate visual reconstruction of the dream. This dataset provides a novel resource for dream research, a unique resource to study the neural correlates of dreaming, to develop models for decoding dreams from brain activity, and to explore new approaches in neuroscience, psychology, and artificial intelligence. Available in open access on Hugging Face and GitHub, Dream2Image provides a multimodal resource designed to support research at the interface of artificial intelligence and neuroscience. It was designed to inspire researchers and extend the current approaches to brain activity decoding. Limitations include the relatively small sample size and the variability of dream recall, which may affect generalizability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Dream2Imageï¼Œè¿™æ˜¯å…¨çƒé¦–ä¸ªç»“åˆäº†è„‘ç”µå›¾ (EEG) ä¿¡å·ã€æ¢¦å¢ƒè½¬å½•å’Œ AI ç”Ÿæˆå›¾åƒçš„å¼€æºå¤šæ¨¡æ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŸºäº 38 åå‚ä¸è€…è¶…è¿‡ 31 å°æ—¶çš„æ¢¦å¢ƒ EEG è®°å½•ï¼Œæå–äº†è§‰é†’å‰ç‰¹å®šæ—¶é—´æ®µçš„è„‘ç”µæ´»åŠ¨å¹¶åŒ¹é…äº†å¯¹åº”çš„æ¢¦å¢ƒæŠ¥å‘Šã€‚é€šè¿‡ç»“åˆ AI æŠ€æœ¯ç”Ÿæˆçš„æ¢¦å¢ƒè§†è§‰é‡å»ºï¼Œè¯¥æ•°æ®é›†ä¸ºç ”ç©¶æ¢¦å¢ƒçš„ç¥ç»ç›¸å…³ç‰©åŠå¼€å‘å¤§è„‘è§£ç æ¨¡å‹æä¾›äº†æ ¸å¿ƒèµ„æºã€‚ç ”ç©¶äººå‘˜å¯ä»¥åœ¨ Hugging Face å’Œ GitHub ä¸Šè·å–è¯¥æ•°æ®é›†ï¼Œä»¥æ”¯æŒäººå·¥æ™ºèƒ½ä¸ç¥ç»ç§‘å­¦çš„äº¤å‰é¢†åŸŸç ”ç©¶ã€‚å°½ç®¡é¢ä¸´æ ·æœ¬é‡è¾ƒå°å’Œæ¢¦å¢ƒå›å¿†å·®å¼‚æ€§ç­‰å±€é™ï¼ŒDream2Image ä»ä¸ºå¤§è„‘æ´»åŠ¨è§£ç æä¾›äº†æå…·å¯å‘æ€§çš„æ–°æ–¹å‘ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "7 Pages, 3 Figures, The Dream2Image dataset is openly available on Hugging Face at: https://huggingface.co/datasets/opsecsystems/Dream2Image",
      "pdf_url": "https://arxiv.org/pdf/2510.06252v1",
      "published_date": "2025-10-03 22:43:27 UTC",
      "updated_date": "2025-10-03 22:43:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:52.850985+00:00"
    },
    {
      "arxiv_id": "2510.03548v2",
      "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing",
      "title_zh": "è¯†ç ´å¹•åæ“çºµè€…ï¼šåˆ©ç”¨ç”Ÿç‰©ç‰¹å¾æ³„éœ²ç“¦è§£åŸºäºäººå·¥æ™ºèƒ½çš„è§†é¢‘ä¼šè®®èº«ä»½å†’å……",
      "authors": [
        "Danial Samadi Vahdati",
        "Tai Duc Nguyen",
        "Ekta Prashnani",
        "Koki Nagano",
        "David Luebke",
        "Orazio Gallo",
        "Matthew Stamm"
      ],
      "abstract": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a compact pose-expression latent and re-synthesizing RGB at the receiver, but this latent can be puppeteered, letting an attacker hijack a victim's likeness in real time. Because every frame is synthetic, deepfake and synthetic video detectors fail outright. To address this security problem, we exploit a key observation: the pose-expression latent inherently contains biometric information of the driving identity. Therefore, we introduce the first biometric leakage defense without ever looking at the reconstructed RGB video: a pose-conditioned, large-margin contrastive encoder that isolates persistent identity cues inside the transmitted latent while cancelling transient pose and expression. A simple cosine test on this disentangled embedding flags illicit identity swaps as the video is rendered. Our experiments on multiple talking-head generation models show that our method consistently outperforms existing puppeteering defenses, operates in real-time, and shows strong generalization to out-of-distribution scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºAIçš„è°ˆè¯å¤´åƒ(talking-head)è§†é¢‘ä¼šè®®ç³»ç»Ÿä¸­å­˜åœ¨çš„èº«ä»½å†’ç”¨é£é™©ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ç”Ÿç‰©ç‰¹å¾æ³„éœ²è¿›è¡Œé˜²å¾¡çš„æ–°æœºåˆ¶ã€‚ä¼ ç»Ÿçš„æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨åœ¨å¤„ç†å®Œå…¨ç”±åˆæˆç”Ÿæˆçš„è§†é¢‘å¸§æ—¶å¾€å¾€å¤±æ•ˆï¼Œå› æ­¤è¯¥ç ”ç©¶åˆ©ç”¨äº†å§¿æ€-è¡¨æƒ…æ½œç©ºé—´(pose-expression latent)ä¸­éšå«çš„é©¾é©¶è€…(driving identity)ç”Ÿç‰©ç‰¹å¾ä¿¡æ¯ã€‚ä½œè€…è®¾è®¡äº†ä¸€ç§å§¿æ€æ¡ä»¶åŒ–ã€å¤§è¾¹è·å¯¹æ¯”ç¼–ç å™¨(pose-conditioned, large-margin contrastive encoder)ï¼Œæ—¨åœ¨ä»ä¼ è¾“çš„æ½œå˜é‡ä¸­åˆ†ç¦»å‡ºæŒä¹…çš„èº«ä»½ç‰¹å¾ï¼ŒåŒæ—¶æ¶ˆé™¤ç¬æ—¶çš„å§¿æ€å’Œè¡¨æƒ…å¹²æ‰°ã€‚é€šè¿‡å¯¹è§£è€¦åçš„åµŒå…¥å‘é‡è¿›è¡Œç®€å•çš„ä½™å¼¦ç›¸ä¼¼åº¦æµ‹è¯•(cosine test)ï¼Œç³»ç»Ÿèƒ½å¤Ÿåœ¨è§†é¢‘æ¸²æŸ“è¿‡ç¨‹ä¸­å®æ—¶æ ‡è®°éæ³•çš„èº«ä»½äº¤æ¢è¡Œä¸ºã€‚åœ¨å¤šç§è°ˆè¯å¤´åƒç”Ÿæˆæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®æ—¶æ€§ã€æ³›åŒ–èƒ½åŠ›ä»¥åŠé˜²å¾¡ puppeteering æ”»å‡»çš„å‡†ç¡®ç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„é˜²å¾¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03548v2",
      "published_date": "2025-10-03 22:37:03 UTC",
      "updated_date": "2025-10-24 18:41:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:58.212425+00:00"
    },
    {
      "arxiv_id": "2510.03544v2",
      "title": "Agile Tradespace Exploration for Space Rendezvous Mission Design via Transformers",
      "title_zh": "åŸºäº Transformer çš„ç©ºé—´äº¤ä¼šä»»åŠ¡è®¾è®¡æ•æ·æƒè¡¡ç©ºé—´æ¢ç´¢",
      "authors": [
        "Yuji Takubo",
        "Daniele Gammelli",
        "Marco Pavone",
        "Simone D'Amico"
      ],
      "abstract": "Spacecraft rendezvous enables on-orbit servicing, debris removal, and crewed docking, forming the foundation for a scalable space economy. Designing such missions requires rapid exploration of the tradespace between control cost and flight time across multiple candidate targets. However, multi-objective optimization in this setting is challenging, as the underlying constraints are often nonconvex, and mission designers must balance accuracy (e.g., solving the full problem) with efficiency (e.g., convex relaxations), slowing iteration and limiting design agility. To address these challenges, this paper proposes an AI-powered framework that enables agile and generalized rendezvous mission design. Given the orbital information of the target spacecraft, boundary conditions of the servicer, and a range of flight times, a transformer model generates a set of near-Pareto optimal trajectories across varying flight times in a single parallelized inference step, thereby enabling rapid mission trade studies. The model is further extended to accommodate variable flight times and perturbed orbital dynamics, supporting realistic multi-objective trade-offs. Validation on chance-constrained rendezvous problems in Earth orbits with passive safety constraints demonstrates that the model generalizes across both flight times and dynamics, consistently providing high-quality initial guesses that converge to superior solutions in fewer iterations. Moreover, the framework efficiently approximates the Pareto front, achieving runtimes comparable to convex relaxation by exploiting parallelized inference. Together, these results position the proposed framework as a practical surrogate for nonconvex trajectory generation and mark an important step toward AI-driven trajectory design for accelerating preliminary mission planning in real-world rendezvous applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èˆªå¤©å™¨äº¤ä¼š(Spacecraft rendezvous)ä»»åŠ¡è®¾è®¡ä¸­æ§åˆ¶æˆæœ¬ä¸é£è¡Œæ—¶é—´æƒè¡¡çš„é«˜å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºAIçš„æ•æ·åŒ–ã€æ³›åŒ–è®¾è®¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Transformeræ¨¡å‹ï¼Œåœ¨ç»™å®šç›®æ ‡è½¨é“ä¿¡æ¯å’Œè¾¹ç•Œæ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å•æ¬¡å¹¶è¡ŒåŒ–æ¨ç†å³å¯ç”Ÿæˆä¸€ç³»åˆ—è·¨è¶Šä¸åŒé£è¡Œæ—¶é—´çš„è¿‘Paretoæœ€ä¼˜è½¨è¿¹ï¼Œæå¤§åœ°åŠ é€Ÿäº†ä»»åŠ¡æ–¹æ¡ˆçš„æƒè¡¡ç ”ç©¶ã€‚æ¨¡å‹è¿›ä¸€æ­¥æ‰©å±•ä»¥é€‚åº”å¯å˜é£è¡Œæ—¶é—´å’Œæ‘„åŠ¨è½¨é“åŠ¨åŠ›å­¦(perturbed orbital dynamics)ï¼Œä»è€Œæ”¯æŒç°å®åœºæ™¯ä¸‹çš„å¤šç›®æ ‡æŠ˜è¡·ã€‚åœ¨å¸¦æœ‰è¢«åŠ¨å®‰å…¨çº¦æŸ(passive safety constraints)çš„åœ°çƒè½¨é“ä»»åŠ¡éªŒè¯ä¸­ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½æä¾›é«˜è´¨é‡çš„åˆå§‹çŒœæµ‹ï¼Œä½¿éå‡¸ä¼˜åŒ–é—®é¢˜ä»¥æ›´å°‘çš„è¿­ä»£æ¬¡æ•°æ”¶æ•›è‡³æ›´ä¼˜è§£ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é€¼è¿‘Paretoå‰æ²¿çš„æ•ˆç‡ä¸Šå¯ä¸å‡¸æ¾å¼›(convex relaxation)æ–¹æ³•åª²ç¾ï¼Œæœ‰æ•ˆè§£å†³äº†éå‡¸è½¨è¿¹ç”Ÿæˆçš„è®¡ç®—ç“¶é¢ˆï¼Œä¸ºåŠ é€Ÿå®é™…äº¤ä¼šä»»åŠ¡çš„åˆæ­¥è§„åˆ’æä¾›äº†å®ç”¨çš„ä»£ç†æ¨¡å‹ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "math.OC",
      "comment": "14 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03544v2",
      "published_date": "2025-10-03 22:28:46 UTC",
      "updated_date": "2026-01-11 22:27:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:28:59.543215+00:00"
    },
    {
      "arxiv_id": "2510.03536v2",
      "title": "Triplet-Structured Knowledge Integration for Multi-Turn Medical Reasoning",
      "title_zh": "é¢å‘å¤šè½®åŒ»ç–—æ¨ç†çš„ä¸‰å…ƒç»„ç»“æ„çŸ¥è¯†é›†æˆ",
      "authors": [
        "Zhaohan Meng",
        "Zaiqiao Meng",
        "Siwei Liu",
        "Iadh Ounis"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance on static medical Question Answering (QA) tasks, yet their reasoning often deteriorates in multi-turn clinical dialogues where patient information is scattered across turns. This paper introduces TriMediQ, a triplet-structured approach that enhances the reasoning reliability of LLMs through explicit knowledge integration. TriMediQ first employs a frozen triplet extraction LLM to convert patient responses into clinically grounded triplets, ensuring factual precision via constrained prompting. These triplets are incorporated into a patient-specific Knowledge Graph (KG), from which a trainable projection module consisting of a graph encoder and a projector captures relational dependencies while keeping all LLM parameters frozen. During inference, the projection module guides multi-hop reasoning over the KG, enabling coherent clinical dialogue understanding. Experiments on two interactive medical QA benchmarks show that TriMediQ achieves up to 10.4\\% improvement in accuracy over five existing baselines on the iMedQA dataset. These results demonstrate that structuring patient information as triplets can effectively improve the reasoning capability of LLMs in multi-turn medical QA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TriMediQï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸‰å…ƒç»„ç»“æ„çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè½®åŒ»ç–—å¯¹è¯ä¸­çš„æ¨ç†å¯é æ€§ï¼Œè§£å†³æ‚£è€…ä¿¡æ¯åœ¨å¤šè½®å¯¹è¯ä¸­åˆ†æ•£å¯¼è‡´æ¨ç†æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚TriMediQé¦–å…ˆåˆ©ç”¨å†»ç»“çš„ä¸‰å…ƒç»„æå–LLMå°†æ‚£è€…å›ç­”è½¬æ¢ä¸ºå…·æœ‰ä¸´åºŠä¾æ®çš„ä¸‰å…ƒç»„(triplets)ï¼Œå¹¶é€šè¿‡å—é™æç¤ºç¡®ä¿äº‹å®çš„ç²¾ç¡®æ€§ã€‚è¿™äº›ä¸‰å…ƒç»„éšåè¢«æ•´åˆåˆ°æ‚£è€…ç‰¹æœ‰çš„çŸ¥è¯†å›¾è°±(Knowledge Graph)ä¸­ï¼Œåˆ©ç”¨åŒ…å«å›¾ç¼–ç å™¨å’ŒæŠ•å½±å™¨çš„å¯è®­ç»ƒæŠ•å½±æ¨¡å—(projection module)æ•è·å…³ç³»ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒä¸»LLMå‚æ•°å†»ç»“ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ¨¡å—å¼•å¯¼æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œå¤šè·³æ¨ç†(multi-hop reasoning)ï¼Œä»è€Œå®ç°è¿è´¯çš„ä¸´åºŠå¯¹è¯ç†è§£ã€‚åœ¨ä¸¤ä¸ªäº¤äº’å¼åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTriMediQåœ¨iMedQAæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æ¯”äº”ä¸ªç°æœ‰åŸºçº¿æ¨¡å‹æœ€é«˜æå‡äº†10.4%ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œå°†æ‚£è€…ä¿¡æ¯ç»“æ„åŒ–ä¸ºä¸‰å…ƒç»„èƒ½æœ‰æ•ˆæé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®åŒ»ç–—é—®ç­”ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.03536v2",
      "published_date": "2025-10-03 22:11:17 UTC",
      "updated_date": "2025-10-14 11:40:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:02.753407+00:00"
    },
    {
      "arxiv_id": "2510.07331v1",
      "title": "Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation",
      "title_zh": "çœŸå®æ€§æ„ŸçŸ¥è§£ç ï¼šä¸€ç§åŸºäºç¨‹åºé€»è¾‘çš„äº‹å®æ€§è¯­è¨€ç”Ÿæˆæ–¹æ³•",
      "authors": [
        "Faruk Alpay",
        "Hamdi Alakkad"
      ],
      "abstract": "This paper introduces Truth-Aware Decoding (TAD), a verification-oriented decoding scheme that aligns neural language generation with knowledge bases. Situated in the tradition of probabilistic program semantics for sequence models, TAD augments modern instruction-tuned systems with a lattice of semantic guards that operate at decode time. Our contributions are fourfold: (i) a constraint-based semantics that renders oracle filtering as a program-logic judgment, (ii) a proof that greedy selection enjoys local likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an entropy-style invariant that quantifies factual risk via knowledge-aware safe mass, and (iv) a multi-agent operational calculus with verified Lean artefacts to certify implementation behaviour. Numerical and algorithmic case studies confirm that the resulting guardrails reduce hallucinations without sacrificing throughput, yielding a pragmatic bridge between large-scale empirical models and formal verification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Truth-Aware Decoding (TAD)ï¼Œè¿™æ˜¯ä¸€ç§é¢å‘éªŒè¯çš„è§£ç æ–¹æ¡ˆï¼Œæ—¨åœ¨ä½¿ç¥ç»è¯­è¨€ç”Ÿæˆè¿‡ç¨‹ä¸çŸ¥è¯†åº“ä¿æŒä¸€è‡´ã€‚TAD å€Ÿé‰´äº†åºåˆ—æ¨¡å‹çš„æ¦‚ç‡ç¨‹åºè¯­ä¹‰ (probabilistic program semantics) ä¼ ç»Ÿï¼Œé€šè¿‡åœ¨è§£ç é˜¶æ®µå¼•å…¥è¯­ä¹‰å®ˆå« (semantic guards) æ ¼ç»“æ„ï¼Œä¸ºç°ä»£æŒ‡ä»¤å¾®è°ƒç³»ç»Ÿæä¾›å®æ—¶çº¦æŸã€‚è®ºæ–‡å»ºç«‹äº†åŸºäºçº¦æŸçš„è¯­ä¹‰æ¡†æ¶ï¼Œå°† Oracle Filtering è½¬åŒ–ä¸ºç¨‹åºé€»è¾‘åˆ¤å®šï¼Œå¹¶è¯æ˜äº†åœ¨å®Œå¤‡å®ˆå«ä¸‹è´ªå©ªé€‰æ‹©å…·æœ‰å±€éƒ¨ä¼¼ç„¶ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºä¿¡æ¯ç†µçš„å˜é‡æ¥é‡åŒ–äº‹å®æ€§é£é™©ï¼Œå¹¶åˆ©ç”¨å¸¦æœ‰ Lean å½¢å¼åŒ–éªŒè¯å·¥å…·çš„å¤šæ™ºèƒ½ä½“è¿ç®—è§„åˆ™æ¥ç¡®è¯ç³»ç»Ÿè¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAD æ¡†æ¶èƒ½æœ‰æ•ˆå‡å°‘æ¨¡å‹å¹»è§‰ (hallucinations) ä¸”ä¸å½±å“ååé‡ï¼Œä¸ºå¤§è§„æ¨¡ç»éªŒæ¨¡å‹ä¸å½¢å¼åŒ–éªŒè¯ (formal verification) ä¹‹é—´æ­å»ºäº†åŠ¡å®çš„æ¡¥æ¢ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, Lean code provided",
      "pdf_url": "https://arxiv.org/pdf/2510.07331v1",
      "published_date": "2025-10-03 22:11:15 UTC",
      "updated_date": "2025-10-03 22:11:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:10.156956+00:00"
    },
    {
      "arxiv_id": "2510.06250v2",
      "title": "Scalable multilingual PII annotation for responsible AI in LLMs",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹è´Ÿè´£ä»»äººå·¥æ™ºèƒ½çš„å¯æ‰©å±•å¤šè¯­è¨€ PII æ ‡æ³¨",
      "authors": [
        "Bharti Meena",
        "Joanna Skubisz",
        "Harshit Rajgarhia",
        "Nand Dave",
        "Kiran Ganesh",
        "Shivali Dalmia",
        "Abhishek Mukherji",
        "Vasudevan Sundarababu"
      ],
      "abstract": "As Large Language Models (LLMs) gain wider adoption, ensuring their reliable handling of Personally Identifiable Information (PII) across diverse regulatory contexts has become essential. This work introduces a scalable multilingual data curation framework designed for high-quality PII annotation across 13 underrepresented locales, covering approximately 336 locale-specific PII types. Our phased, human-in-the-loop annotation methodology combines linguistic expertise with rigorous quality assurance, leading to substantial improvements in recall and false positive rates from pilot, training, and production phases. By leveraging inter-annotator agreement metrics and root-cause analysis, the framework systematically uncovers and resolves annotation inconsistencies, resulting in high-fidelity datasets suitable for supervised LLM fine-tuning. Beyond reporting empirical gains, we highlight common annotator challenges in multilingual PII labeling and demonstrate how iterative, analytics-driven pipelines can enhance both annotation quality and downstream model reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„å¤šè¯­è¨€æ•°æ®ç­–åˆ’åˆ†å±‚æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é«˜è´¨é‡çš„ PII (Personally Identifiable Information) æ ‡æ³¨æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†æ•æ„Ÿä¿¡æ¯çš„å¯é æ€§ã€‚è¯¥æ¡†æ¶è¦†ç›–äº†13ä¸ªä»£è¡¨æ€§ä¸è¶³çš„åœ°åŒºï¼Œæ¶‰åŠçº¦336ç§ç‰¹å®šåœ°åŒºçš„ PII ç±»å‹ã€‚ç ”ç©¶é‡‡ç”¨äº†äººæœºååŒ (human-in-the-loop) çš„æ ‡æ³¨æ–¹æ³•ï¼Œå°†è¯­è¨€ä¸“é•¿ä¸ä¸¥æ ¼çš„è´¨é‡ä¿è¯ç›¸ç»“åˆï¼Œåœ¨è¯•ç‚¹ã€è®­ç»ƒå’Œç”Ÿäº§é˜¶æ®µæ˜¾è‘—æå‡äº†å¬å›ç‡å¹¶é™ä½äº†è¯¯æŠ¥ç‡ã€‚é€šè¿‡åˆ©ç”¨æ ‡æ³¨è€…é—´ä¸€è‡´æ€§æŒ‡æ ‡ (inter-annotator agreement) å’Œæ ¹æœ¬åŸå› åˆ†æ (root-cause analysis)ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°è§£å†³äº†æ ‡æ³¨ä¸ä¸€è‡´é—®é¢˜ï¼Œç”Ÿæˆäº†é€‚ç”¨äºç›‘ç£å¼å¾®è°ƒ (supervised fine-tuning) çš„é«˜ä¿çœŸæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ€»ç»“äº†å¤šè¯­è¨€ PII æ ‡æ³¨ä¸­çš„å¸¸è§æŒ‘æˆ˜ï¼Œè¯æ˜äº†è¿­ä»£å¼ã€åˆ†æé©±åŠ¨çš„æµæ°´çº¿èƒ½æœ‰æ•ˆæå‡æ ‡æ³¨è´¨é‡åŠä¸‹æ¸¸æ¨¡å‹çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.06250v2",
      "published_date": "2025-10-03 21:40:31 UTC",
      "updated_date": "2025-10-09 22:12:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:21.642037+00:00"
    },
    {
      "arxiv_id": "2510.03521v1",
      "title": "Identifying Financial Risk Information Using RAG with a Contrastive Insight",
      "title_zh": "åŸºäºå¯¹æ¯”è§†è§’çš„ RAG é‡‘èé£é™©ä¿¡æ¯è¯†åˆ«",
      "authors": [
        "Ali Elahi"
      ],
      "abstract": "In specialized domains, humans often compare new problems against similar examples, highlight nuances, and draw conclusions instead of analyzing information in isolation. When applying reasoning in specialized contexts with LLMs on top of a RAG, the pipeline can capture contextually relevant information, but it is not designed to retrieve comparable cases or related problems.\n  While RAG is effective at extracting factual information, its outputs in specialized reasoning tasks often remain generic, reflecting broad facts rather than context-specific insights. In finance, it results in generic risks that are true for the majority of companies. To address this limitation, we propose a peer-aware comparative inference layer on top of RAG.\n  Our contrastive approach outperforms baseline RAG in text generation metrics such as ROUGE and BERTScore in comparison with human-generated equity research and risk.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ‡å‡†æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) åœ¨é‡‘èç­‰ä¸“ä¸šé¢†åŸŸä¸­è¾“å‡ºå†…å®¹è¿‡äºæ³›åŒ–ã€ç¼ºä¹ç‰¹å®šèƒŒæ™¯æ´å¯Ÿçš„é—®é¢˜å±•å¼€ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åœ¨ RAG ä¹‹ä¸Šçš„â€œåŒè¡Œæ„ŸçŸ¥å¯¹æ¯”æ¨ç†å±‚â€ (peer-aware comparative inference layer)ï¼Œæ¨¡æ‹Ÿäººç±»é€šè¿‡å¯¹æ¯”ç›¸ä¼¼æ¡ˆä¾‹æ¥å‘ç°ç»†å¾®å·®å¼‚å¹¶å¾—å‡ºç»“è®ºçš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§å¯¹æ¯”æ–¹æ³•æ—¨åœ¨æ”¹å˜ä»¥å¾€ LLMs åœ¨å¤„ç†é‡‘èé£é™©ä¿¡æ¯æ—¶åªèƒ½æå–é€šç”¨äº‹å®çš„å±€é™ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´å…·é’ˆå¯¹æ€§çš„é£é™©åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸äººç±»ç¼–å†™çš„è‚¡ç¥¨ç ”ç©¶å’Œé£é™©æŠ¥å‘Šè¿›è¡Œå¯¹æ¯”æ—¶ï¼Œåœ¨ ROUGE å’Œ BERTScore ç­‰æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿ RAG æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¼•å…¥å¯¹æ¯”è§†è§’èƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä¸“ä¸šåœºæ™¯ä¸‹çš„æ¨ç†æ·±åº¦ä¸ä¿¡æ¯è¯†åˆ«ç²¾åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 1 figure, Workshop on Generative AI in Finance, NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03521v1",
      "published_date": "2025-10-03 21:24:56 UTC",
      "updated_date": "2025-10-03 21:24:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:24.959804+00:00"
    },
    {
      "arxiv_id": "2510.03520v1",
      "title": "Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models",
      "title_zh": "Certifiable Safe RLHFï¼šé¢å‘æ›´å®‰å…¨è¯­è¨€æ¨¡å‹çš„å›ºå®šæƒ©ç½šçº¦æŸä¼˜åŒ–",
      "authors": [
        "Kartik Pandit",
        "Sourav Ganguly",
        "Arnesh Banerjee",
        "Shaahin Angizi",
        "Arnob Ghosh"
      ],
      "abstract": "Ensuring safety is a foundational requirement for large language models (LLMs). Achieving an appropriate balance between enhancing the utility of model outputs and mitigating their potential for harm is a complex and persistent challenge. Contemporary approaches frequently formalize this problem within the framework of Constrained Markov Decision Processes (CMDPs) and employ established CMDP optimization techniques. However, these methods exhibit two notable limitations. First, their reliance on reward and cost functions renders performance highly sensitive to the underlying scoring mechanism, which must capture semantic meaning rather than being triggered by superficial keywords. Second, CMDP-based training entails tuning dual-variable, a process that is both computationally expensive and does not provide any provable safety guarantee for a fixed dual variable that can be exploitable through adversarial jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF (CS-RLHF) that introduces a cost model trained on a large-scale corpus to assign semantically grounded safety scores. In contrast to the lagrangian-based approach, CS-RLHF adopts a rectified penalty-based formulation. This design draws on the theory of exact penalty functions in constrained optimization, wherein constraint satisfaction is enforced directly through a suitably chosen penalty term. With an appropriately scaled penalty, feasibility of the safety constraints can be guaranteed at the optimizer, eliminating the need for dual-variable updates. Empirical evaluation demonstrates that CS-RLHF outperforms state-of-the-art LLM model responses rendering at-least 5 times efficient against nominal and jail-breaking prompts",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Certifiable Safe-RLHF (CS-RLHF)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¹³è¡¡è¾“å‡ºæ•ˆç”¨ä¸å®‰å…¨æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰åŸºäºå—é™é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(CMDP)çš„æ–¹æ³•å¯¹è¯„åˆ†æœºåˆ¶æ•æ„Ÿä¸”ç”±äºä¾èµ–å¯¹å¶å˜é‡æ›´æ–°è€Œç¼ºä¹å¯è¯æ˜å®‰å…¨æ€§ä¿è¯çš„é—®é¢˜ï¼ŒCS-RLHFå¼•å…¥äº†åœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šè®­ç»ƒçš„Cost Modelï¼Œä»¥åˆ†é…å…·æœ‰è¯­ä¹‰åŸºç¡€çš„å®‰å…¨è¯„åˆ†ã€‚ä¸ä¼ ç»Ÿçš„æ‹‰æ ¼æœ—æ—¥(Lagrangian)æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†åŸºäºå—é™ä¼˜åŒ–ä¸­ç²¾ç¡®æƒ©ç½šå‡½æ•°(Exact Penalty Functions)ç†è®ºçš„ä¿®æ­£æƒ©ç½šé¡¹å…¬å¼ï¼Œé€šè¿‡åˆé€‚æ¯”ä¾‹çš„æƒ©ç½šç›´æ¥å¼ºåˆ¶æ‰§è¡Œçº¦æŸï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹Dual-variableæ›´æ–°çš„éœ€æ±‚ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†å®‰å…¨çº¦æŸåœ¨ä¼˜åŒ–å™¨å±‚é¢çš„å¯è¡Œæ€§ï¼Œä¸ºæŠµå¾¡å¯¹æŠ—æ€§Jailbreaksæä¾›äº†ç†è®ºä¸Šçš„å®‰å…¨ä¿è¯ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒCS-RLHFåœ¨å¤„ç†å¸¸è§„æç¤ºå’ŒJail-breakingæç¤ºæ—¶ï¼Œå…¶å®‰å…¨æ€§è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸”åœ¨é˜²å¾¡æ•ˆç‡ä¸Šæå‡äº†è‡³å°‘5å€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03520v1",
      "published_date": "2025-10-03 21:24:41 UTC",
      "updated_date": "2025-10-03 21:24:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:27.855426+00:00"
    },
    {
      "arxiv_id": "2510.03519v1",
      "title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning",
      "title_zh": "TS-Reasonerï¼šå°†æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¯¹é½",
      "authors": [
        "Fangxu Yu",
        "Hongyu Zhao",
        "Tianyi Zhou"
      ],
      "abstract": "Time series reasoning is crucial to decision-making in diverse domains, including finance, energy usage, traffic, weather, and scientific discovery. While existing time series foundation models (TSFMs) can capture low-level dynamic patterns and provide accurate forecasting, further analysis usually requires additional background knowledge and sophisticated reasoning, which are lacking in most TSFMs but can be achieved through large language models (LLMs). On the other hand, without expensive post-training, LLMs often struggle with the numerical understanding of time series data. Although it is intuitive to integrate the two types of models, developing effective training recipes that align the two modalities for reasoning tasks is still an open challenge. To this end, we propose TS-Reasoner that aligns the latent representations of TSFMs with the textual inputs of LLMs for downstream understanding/reasoning tasks. Specifically, we propose a simple yet effective method to curate diverse, synthetic pairs of time series and textual captions for alignment training. We then develop a two-stage training recipe that applies instruction finetuning after the alignment pretraining. Unlike existing works that train an LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it during training. Extensive experiments on several benchmarks demonstrate that TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision Language Models (VLMs), and Time Series LLMs, but also achieves this with remarkable data efficiency, e.g., using less than half the training data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TS-Reasonerï¼Œæ—¨åœ¨é€šè¿‡å°†æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(TSFMs)çš„æ½œè¡¨å¾ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ–‡æœ¬è¾“å…¥ç›¸å¯¹é½ï¼Œè§£å†³ç°æœ‰æ¨¡å‹åœ¨æ—¶é—´åºåˆ—æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹èƒŒæ™¯çŸ¥è¯†æˆ–æ•°å€¼ç†è§£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹å¼æ¥æ„å»ºå¤šæ ·åŒ–çš„åˆæˆæ—¶é—´åºåˆ—ä¸æ–‡æœ¬è¯´æ˜å¯¹ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„å¯¹é½è®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨äº†ä¸¤é˜¶æ®µæ–¹æ¡ˆï¼Œå³åœ¨å¯¹é½é¢„è®­ç»ƒä¹‹åè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ(Instruction Finetuning)ï¼Œä¸”åœ¨è®­ç»ƒæœŸé—´ä¿æŒé¢„è®­ç»ƒçš„TSFMå¤„äºå†»ç»“çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTS-Reasoneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å‡ä¼˜äºç›®å‰ä¸»æµçš„LLMsã€è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä»¥åŠä¸“é—¨çš„æ—¶é—´åºåˆ—LLMsã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°äº†æé«˜çš„æ•°æ®æ•ˆç‡ï¼Œä»…ä½¿ç”¨ä¸åˆ°ä¸€åŠçš„è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°å“è¶Šæ€§èƒ½ï¼Œä¸ºé‡‘èã€èƒ½æºåŠæ°”è±¡ç­‰é¢†åŸŸçš„å¤æ‚å†³ç­–æä¾›äº†å¼ºæœ‰åŠ›çš„æ¨ç†æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03519v1",
      "published_date": "2025-10-03 21:20:54 UTC",
      "updated_date": "2025-10-03 21:20:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:30.249887+00:00"
    },
    {
      "arxiv_id": "2510.03514v1",
      "title": "Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making",
      "title_zh": "æˆ˜äº‰è¿·é›¾ä¸­çš„çº¢çº¿ä¸ç°è‰²åœ°å¸¦ï¼šå¤§è¯­è¨€æ¨¡å‹å†›äº‹å†³ç­–ä¸­çš„æ³•å¾‹é£é™©ã€ä¼¦ç†æŸå®³åŠåœ°åŸŸåè§åŸºå‡†è¯„ä¼°",
      "authors": [
        "Toby Drinkall"
      ],
      "abstract": "As military organisations consider integrating large language models (LLMs) into command and control (C2) systems for planning and decision support, understanding their behavioural tendencies is critical. This study develops a benchmarking framework for evaluating aspects of legal and moral risk in targeting behaviour by comparing LLMs acting as agents in multi-turn simulated conflict. We introduce four metrics grounded in International Humanitarian Law (IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target Rate (DTR) assess compliance with legal targeting principles, while Mean and Max Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for civilian harm.\n  We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through 90 multi-agent, multi-turn crisis simulations across three geographic regions. Our findings reveal that off-the-shelf LLMs exhibit concerning and unpredictable targeting behaviour in simulated conflict environments. All models violated the IHL principle of distinction by targeting civilian objects, with breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through crisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in late turns. Significant inter-model variation emerged: LLaMA-3.1 selected an average of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while Gemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These differences indicate that model selection for deployment constitutes a choice about acceptable legal and moral risk profiles in military operations.\n  This work seeks to provide a proof-of-concept of potential behavioural risks that could emerge from the use of LLMs in Decision Support Systems (AI DSS) as well as a reproducible benchmarking framework with interpretable metrics for standardising pre-deployment testing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†›äº‹æŒ‡æŒ¥ä¸æ§åˆ¶(C2)ç³»ç»Ÿä¸­å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„é›†æˆï¼Œå¼€å‘äº†ä¸€ä¸ªè¯„ä¼°å…¶åœ¨ç›®æ ‡é€‰æ‹©è¡Œä¸ºä¸­æ³•å¾‹ä¸é“å¾·é£é™©çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡90åœºå¤šæ™ºèƒ½ä½“ã€å¤šè½®å±æœºæ¨¡æ‹Ÿï¼Œå¯¹æ¯”è¯„ä¼°äº†GPT-4oã€Gemini-2.5å’ŒLLaMA-3.1ç­‰æ¨¡å‹åœ¨ä¸åŒåœ°ç†åŒºåŸŸçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å››ä¸ªåŸºäºå›½é™…äººé“æ³•(IHL)çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬è¡¡é‡åˆè§„æ€§çš„Civilian Target Rate (CTR)ä¸Dual-use Target Rate (DTR)ï¼Œä»¥åŠé‡åŒ–å¹³æ°‘ä¼¤å®³å®¹å¿åº¦çš„Simulated Non-combatant Casualty Value (SNCV)ã€‚å®éªŒå‘ç°ï¼Œæ‰€æœ‰å—æµ‹æ¨¡å‹å‡è¿åäº†IHLçš„åŒºåˆ†åŸåˆ™ï¼Œè¿è§„ç‡åœ¨16.7%è‡³66.7%ä¹‹é—´ï¼Œä¸”éšç€å±æœºå‡çº§ï¼Œæ¨¡å‹å¯¹å¹³æ°‘ä¼¤å®³çš„å®¹å¿åº¦æ˜¾è‘—å¢åŠ ã€‚ä¸åŒæ¨¡å‹é—´è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ï¼ŒLLaMA-3.1çš„æ³•å¾‹é£é™©æœ€é«˜ï¼Œè€ŒGemini-2.5ç›¸å¯¹è¾ƒä½ï¼Œè¿™è¡¨æ˜æ¨¡å‹é€‰æ‹©å®è´¨ä¸Šæ˜¯å¯¹å†›äº‹æ“ä½œä¸­æ³•å¾‹å’Œé“å¾·é£é™©æ¦‚å†µçš„é€‰æ‹©ã€‚è¯¥é¡¹å·¥ä½œä¸ºäººå·¥æ™ºèƒ½å†³ç­–æ”¯æŒç³»ç»Ÿ(AI DSS)ä¸­çš„æ½œåœ¨è¡Œä¸ºé£é™©æä¾›äº†æ¦‚å¿µéªŒè¯ï¼Œå¹¶å»ºç«‹äº†ä¸€å¥—å¯é‡å¤çš„é¢„éƒ¨ç½²æµ‹è¯•æ ‡å‡†åŒ–åŸºå‡†ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "54 pages; 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03514v1",
      "published_date": "2025-10-03 20:55:04 UTC",
      "updated_date": "2025-10-03 20:55:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:34.353995+00:00"
    },
    {
      "arxiv_id": "2510.03511v2",
      "title": "Platonic Transformers: A Solid Choice For Equivariance",
      "title_zh": "Platonic Transformerï¼šç­‰å˜æ€§å»ºæ¨¡çš„å¯é ä¹‹é€‰",
      "authors": [
        "Mohammad Mohaiminul Islam",
        "Rishabh Anand",
        "David R. Wessels",
        "Friso de Kruiff",
        "Thijs P. Kuipers",
        "Rex Ying",
        "Clara I. SÃ¡nchez",
        "Sharvaree Vadgama",
        "Georg BÃ¶kman",
        "Erik J. Bekkers"
      ],
      "abstract": "While widespread, Transformers lack inductive biases for geometric symmetries common in science and computer vision. Existing equivariant methods often sacrifice the efficiency and flexibility that make Transformers so effective through complex, computationally intensive designs. We introduce the Platonic Transformer to resolve this trade-off. By defining attention relative to reference frames from the Platonic solid symmetry groups, our method induces a principled weight-sharing scheme. This enables combined equivariance to continuous translations and Platonic symmetries, while preserving the exact architecture and computational cost of a standard Transformer. Furthermore, we show that this attention is formally equivalent to a dynamic group convolution, which reveals that the model learns adaptive geometric filters and enables a highly scalable, linear-time convolutional variant. Across diverse benchmarks in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25), the Platonic Transformer achieves competitive performance by leveraging these geometric constraints at no additional cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Platonic Transformersï¼Œæ—¨åœ¨è§£å†³æ ‡å‡† Transformers åœ¨ç§‘å­¦å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸç¼ºä¹å‡ ä½•å¯¹ç§°æ€§å½’çº³åç½®çš„é—®é¢˜ã€‚ç°æœ‰ç­‰å˜ï¼ˆequivariantï¼‰æ–¹æ³•é€šå¸¸è®¾è®¡å¤æ‚ä¸”è®¡ç®—é‡å¤§ï¼Œè€Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æŸæ‹‰å›¾å¤šé¢ä½“ï¼ˆPlatonic solidï¼‰å¯¹ç§°ç¾¤çš„å‚è€ƒç³»å®šä¹‰æ³¨æ„åŠ›ï¼Œå¼•å…¥äº†ä¸€ç§åŸåˆ™æ€§çš„æƒå€¼å…±äº«æ–¹æ¡ˆã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒæ ‡å‡† Transformer æ¶æ„å’Œè®¡ç®—æˆæœ¬ä¸å˜çš„å‰æä¸‹ï¼Œå®ç°äº†å¯¹è¿ç»­å¹³ç§»ï¼ˆcontinuous translationsï¼‰å’Œ Platonic å¯¹ç§°æ€§çš„ç­‰å˜æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œè¿™ç§æ³¨æ„åŠ›æœºåˆ¶åœ¨å½¢å¼ä¸Šç­‰åŒäºåŠ¨æ€ç¾¤å·ç§¯ï¼ˆdynamic group convolutionï¼‰ï¼Œèƒ½å¤Ÿå­¦ä¹ è‡ªé€‚åº”å‡ ä½•æ»¤æ³¢å™¨ï¼Œå¹¶å¯æ‰©å±•ä¸ºé«˜æ•ˆçš„çº¿æ€§æ—¶é—´å·ç§¯å˜ä½“ã€‚åœ¨ CIFAR-10ã€ScanObjectNN ä»¥åŠåˆ†å­å±æ€§é¢„æµ‹ï¼ˆQM9, OMol25ï¼‰ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPlatonic Transformers å‡­å€Ÿå‡ ä½•çº¦æŸåœ¨ä¸å¢åŠ é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03511v2",
      "published_date": "2025-10-03 20:51:25 UTC",
      "updated_date": "2025-10-08 00:09:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:40.453799+00:00"
    },
    {
      "arxiv_id": "2510.03506v3",
      "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
      "title_zh": "OneFlowï¼šåŸºäºç¼–è¾‘æµçš„å¹¶å‘æ··åˆæ¨¡æ€ä¸äº¤é”™ç”Ÿæˆ",
      "authors": [
        "John Nguyen",
        "Marton Havasi",
        "Tariq Berrada",
        "Luke Zettlemoyer",
        "Ricky T. Q. Chen"
      ],
      "abstract": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OneFlowï¼Œè¿™æ˜¯é¦–ä¸ªæ”¯æŒå˜é•¿ä¸”å¹¶å‘æ··åˆæ¨¡æ€ç”Ÿæˆçš„éè‡ªå›å½’ (non-autoregressive) å¤šæ¨¡æ€æ¨¡å‹ã€‚ä¸åŒäºåœ¨æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆé—´å¼ºåˆ¶æ‰§è¡Œä¸¥æ ¼å› æœé¡ºåºçš„è‡ªå›å½’æ¨¡å‹ï¼ŒOneFlow åˆ›æ–°æ€§åœ°ç»“åˆäº†ç”¨äºç¦»æ•£æ–‡æœ¬æ ‡è®°çš„æ’å…¥å¼ Edit Flow å’Œç”¨äºå›¾åƒæ½œå˜é‡çš„ Flow Matching æŠ€æœ¯ã€‚è¯¥æ¡†æ¶é€šè¿‡å±‚æ¬¡åŒ–é‡‡æ · (hierarchical sampling) ä¼˜å…ˆå¤„ç†å†…å®¹ä¿¡æ¯ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å›¾æ–‡å¹¶å‘åˆæˆã€‚å®éªŒè¯æ˜ï¼ŒOneFlow åœ¨ 1B è‡³ 8B å‚æ•°è§„æ¨¡ä¸‹ï¼Œå…¶ç”Ÿæˆä¸ç†è§£èƒ½åŠ›å‡ä¼˜äºä¼ ç»Ÿè‡ªå›å½’åŸºçº¿ï¼Œä¸”è®­ç»ƒ FLOPs é™ä½äº† 50%ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¶Šè‡ªå›å½’åŠæ‰©æ•£ (diffusion-based) æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜å¼€åˆ›äº†è¿­ä»£ä¼˜åŒ– (iterative refinement) å’Œç±»è‡ªç„¶æ¨ç†ç”Ÿæˆç­‰å…¨æ–°åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "https://oneflow.framer.ai",
      "pdf_url": "https://arxiv.org/pdf/2510.03506v3",
      "published_date": "2025-10-03 20:40:30 UTC",
      "updated_date": "2025-12-10 16:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:38.454122+00:00"
    },
    {
      "arxiv_id": "2510.03502v2",
      "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection",
      "title_zh": "ALHDï¼šç”¨äºé˜¿æ‹‰ä¼¯è¯­ LLM ç”Ÿæˆæ–‡æœ¬æ£€æµ‹çš„å¤§è§„æ¨¡å¤šä½“è£åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Ali Khairallah",
        "Arkaitz Zubiaga"
      ],
      "abstract": "We introduce ALHD, the first large-scale comprehensive Arabic dataset explicitly designed to distinguish between human- and LLM-generated texts. ALHD spans three genres (news, social media, reviews), covering both MSA and dialectal Arabic, and contains over 400K balanced samples generated by three leading LLMs and originated from multiple human sources, which enables studying generalizability in Arabic LLM-genearted text detection. We provide rigorous preprocessing, rich annotations, and standardized balanced splits to support reproducibility. In addition, we present, analyze and discuss benchmark experiments using our new dataset, in turn identifying gaps and proposing future research directions. Benchmarking across traditional classifiers, BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. Results are however not always consistent, as we observe challenges when generalizing across genres; indeed, models struggle to generalize when they need to deal with unseen patterns in cross-genre settings, and these challenges are particularly prominent when dealing with news articles, where LLM-generated texts resemble human texts in style, which opens up avenues for future research. ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ALHDï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡ã€å¤šé¢†åŸŸçš„é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºåŒºåˆ†äººç±»ç¼–å†™ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„æ–‡æœ¬ã€‚ALHDåŒ…å«è¶…è¿‡40ä¸‡ä¸ªå¹³è¡¡æ ·æœ¬ï¼Œæ¶µç›–äº†æ–°é—»ã€ç¤¾äº¤åª’ä½“å’Œè¯„è®ºä¸‰ç§ä½“è£ï¼Œå¹¶å…¼é¡¾äº†æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­(MSA)ä¸åœ°æ–¹æ–¹è¨€ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ•°æ®é›†å¯¹ä¼ ç»Ÿåˆ†ç±»å™¨ã€åŸºäºBERTçš„æ¨¡å‹ä»¥åŠLLMsåœ¨é›¶æ ·æœ¬(zero-shot)å’Œå°‘æ ·æœ¬(few-shot)ä¸‹çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°ç»è¿‡å¾®è°ƒçš„BERTæ¨¡å‹åœ¨æ£€æµ‹æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç„¶è€Œï¼Œå®éªŒä¹Ÿæ­ç¤ºäº†æ¨¡å‹åœ¨è·¨é¢†åŸŸæ³›åŒ–æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°é—»æ–‡ç« ä¸­ï¼ŒLLMç”Ÿæˆçš„æ–‡æœ¬åœ¨é£æ ¼ä¸Šä¸äººç±»åˆ›ä½œé«˜åº¦ç›¸ä¼¼ï¼Œå¢åŠ äº†è¯†åˆ«éš¾åº¦ã€‚è¯¥æ•°æ®é›†çš„å‘å¸ƒä¸ºåº”å¯¹é˜¿æ‹‰ä¼¯è¯­è¯­å¢ƒä¸‹çš„è™šå‡ä¿¡æ¯ã€å­¦æœ¯ä¸ç«¯åŠç½‘ç»œå®‰å…¨å¨èƒå¥ å®šäº†å…³é”®çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "47 pages, 15 figures. Dataset available at Zenodo: https://doi.org/10.5281/zenodo.17249602 Codebase available at GitHub: https://github.com/alikhairallah/ALHD-Benchmarking",
      "pdf_url": "https://arxiv.org/pdf/2510.03502v2",
      "published_date": "2025-10-03 20:27:45 UTC",
      "updated_date": "2025-10-21 20:12:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:41.647814+00:00"
    },
    {
      "arxiv_id": "2510.03501v1",
      "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms",
      "title_zh": "é¢å‘é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤çš„ç§»åŠ¨å¹³å°å®æ—¶å¤šçº¿ç¨‹æ³¢æ–‘é¸¨æ£€æµ‹ä¸åˆ†å‰²",
      "authors": [
        "Lyes Saad Saoud",
        "Loic Lesobre",
        "Enrico Sorato",
        "Irfan Hussain"
      ],
      "abstract": "Real-time animal detection and segmentation in natural environments are vital for wildlife conservation, enabling non-invasive monitoring through remote camera streams. However, these tasks remain challenging due to limited computational resources and the cryptic appearance of many species. We propose a mobile-optimized two-stage deep learning framework that integrates a Threading Detection Model (TDM) to parallelize YOLOv10-based detection and MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach improves real-time performance by reducing latency through threading. YOLOv10 handles detection while MobileSAM performs lightweight segmentation, both executed concurrently for efficient resource use. On the cryptic Houbara Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627, mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10 operates at 43.7 ms per frame, confirming real-time readiness. We introduce a curated Houbara dataset of 40,000 annotated images to support model training and evaluation across diverse conditions. The code and dataset used in this study are publicly available on GitHub at https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos and additional resources, visit https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ç§»åŠ¨å¹³å°ä¼˜åŒ–çš„ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¸­çš„å®æ—¶ Houbara Bustard æ£€æµ‹ä¸åˆ†å‰²ã€‚ä¸ºäº†å…‹æœç§»åŠ¨ç«¯è®¡ç®—èµ„æºå—é™ä»¥åŠç‰©ç§å¤–è§‚å…·æœ‰éšè”½æ€§ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†çº¿ç¨‹åŒ–æ£€æµ‹æ¨¡å‹(Threading Detection Model, TDM)ï¼Œé€šè¿‡å¤šçº¿ç¨‹æŠ€æœ¯ä½¿åŸºäº YOLOv10 çš„æ£€æµ‹ä»»åŠ¡ä¸åŸºäº MobileSAM çš„è½»é‡åŒ–åˆ†å‰²ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œï¼Œä»è€Œå¤§å¹…é™ä½äº†ç³»ç»Ÿå»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ Houbara Bustard ä»»åŠ¡ä¸Šå®ç°äº† 0.9627 çš„ mAP50 å’Œ 0.7421 çš„ mIoUï¼Œä¸” YOLOv10 çš„è¿è¡Œé€Ÿåº¦è¾¾åˆ°æ¯å¸§ 43.7 msï¼Œå®Œå…¨æ»¡è¶³å®æ—¶ç›‘æµ‹éœ€æ±‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªåŒ…å« 40,000 å¼ æ ‡æ³¨å›¾åƒçš„ Houbara æ•°æ®é›†ï¼Œç”¨äºæ”¯æŒæ¨¡å‹åœ¨ä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹çš„è®­ç»ƒä¸æ€§èƒ½è¯„ä¼°ï¼Œä¸ºå®ç°é«˜æ•ˆã€éä¾µå…¥å¼çš„é‡ç”ŸåŠ¨ç‰©è¿œç¨‹ç›‘æ§å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03501v1",
      "published_date": "2025-10-03 20:25:58 UTC",
      "updated_date": "2025-10-03 20:25:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:50.559069+00:00"
    },
    {
      "arxiv_id": "2510.03495v1",
      "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure",
      "title_zh": "AgentHubï¼šæ™ºèƒ½ä½“å…±äº«åŸºç¡€è®¾æ–½çš„ç ”ç©¶è®®ç¨‹",
      "authors": [
        "Erik Pautsch",
        "Tanmay Singla",
        "Wenxin Jiang",
        "Huiyun Peng",
        "Behnaz Hassanshahi",
        "Konstantin LÃ¤ufer",
        "George K. Thiruvathukal",
        "James C. Davis"
      ],
      "abstract": "LLM-based agents are rapidly proliferating, yet the infrastructure for discovering, evaluating, and governing them remains fragmented compared to mature ecosystems like software package registries (e.g., npm) and model hubs (e.g., Hugging Face). Recent research and engineering works have begun to consider the requisite infrastructure, but so far they focus narrowly -- on distribution, naming, or protocol negotiation. However, considering broader software engineering requirements would improve open-source distribution and ease reuse. We therefore propose AgentHub, a research agenda for agent sharing. By framing the key challenges of capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration, AgentHub charts a community-wide agenda for building reliable and scalable agent ecosystems. Our vision is a future where agents can be shared, trusted, and composed as seamlessly as today's software libraries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgentHubï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ agent sharing åŸºç¡€è®¾æ–½çš„ç ”ç©¶è®®ç¨‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰åŸºäº LLM çš„æ™ºèƒ½ä½“åœ¨å‘ç°ã€è¯„ä¼°å’Œæ²»ç†æ–¹é¢å­˜åœ¨çš„ç¢ç‰‡åŒ–é—®é¢˜ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶ç›®å‰çš„ç ”ç©¶å·²å¼€å§‹å…³æ³¨åˆ†å‘å’Œåè®®åå•†ï¼Œä½†å…¶è§†é‡ä»è¾ƒå±€é™ï¼Œå°šæœªå½¢æˆç±»ä¼¼ npm æˆ– Hugging Face è¿™æ ·æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿã€‚AgentHub é€šè¿‡æ¡†æ¶åŒ–èƒ½åŠ›æ¸…æ™°åº¦ï¼ˆcapability clarityï¼‰ã€ç”Ÿå‘½å‘¨æœŸé€æ˜åº¦ï¼ˆlifecycle transparencyï¼‰ã€äº’æ“ä½œæ€§ï¼ˆinteroperabilityï¼‰ã€æ²»ç†ã€å®‰å…¨åŠå·¥ä½œæµé›†æˆç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ºæ„å»ºå¯é ä¸”å¯æ‰©å±•çš„æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿè§„åˆ’äº†ç¤¾åŒºè·¯çº¿ã€‚è¯¥è®®ç¨‹å¼ºè°ƒäº†å¼•å…¥æ›´å¹¿æ³›çš„è½¯ä»¶å·¥ç¨‹éœ€æ±‚å¯¹äºæ”¹å–„å¼€æºåˆ†å‘å’Œç®€åŒ–å¤ç”¨çš„é‡è¦æ€§ã€‚å…¶æœ€ç»ˆæ„¿æ™¯æ˜¯å®ç°æ™ºèƒ½ä½“èƒ½åƒç°ä»£è½¯ä»¶åº“ï¼ˆsoftware librariesï¼‰ä¸€æ ·ï¼Œåœ¨æœªæ¥è¢«æ— ç¼åœ°å…±äº«ã€ä¿¡ä»»å’Œç»„åˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03495v1",
      "published_date": "2025-10-03 20:18:58 UTC",
      "updated_date": "2025-10-03 20:18:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:29:49.458923+00:00"
    },
    {
      "arxiv_id": "2510.03490v2",
      "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark",
      "title_zh": "SEERï¼šåŸºäºç‰‡æ®µçš„æƒ…æ„Ÿè¯æ®æ£€ç´¢åŸºå‡†",
      "authors": [
        "Aneesha Sampath",
        "Oya Aran",
        "Emily Mower Provost"
      ],
      "abstract": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to test Large Language Models' (LLMs) ability to identify the specific spans of text that express emotion. Unlike traditional emotion recognition tasks that assign a single label to an entire sentence, SEER targets the underexplored task of emotion evidence detection: pinpointing which exact phrases convey emotion. This span-level approach is crucial for applications like empathetic dialogue and clinical support, which need to know how emotion is expressed, not just what the emotion is. SEER includes two tasks: identifying emotion evidence within a single sentence, and identifying evidence across a short passage of five consecutive sentences. It contains new annotations for both emotion and emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs and find that, while some models approach average human performance on single-sentence inputs, their accuracy degrades in longer passages. Our error analysis reveals key failure modes, including overreliance on emotion keywords and false positives in neutral text.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SEER (Span-based Emotion Evidence Retrieval) åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¯†åˆ«è¡¨è¾¾æƒ…æ„Ÿçš„å…·ä½“æ–‡æœ¬ç‰‡æ®µ (spans) çš„èƒ½åŠ›ã€‚ä¸ä»…ä¸ºæ•´ä¸ªå¥å­åˆ†é…å•ä¸€æƒ…æ„Ÿæ ‡ç­¾çš„ä¼ ç»Ÿä»»åŠ¡ä¸åŒï¼ŒSEER ä¸“æ³¨äºæƒ…æ„Ÿè¯æ®æ£€æµ‹ï¼Œå³ç²¾ç¡®å®šä½æ–‡ä¸­å“ªäº›ç‰¹å®šçŸ­è¯­ä¼ è¾¾äº†æƒ…æ„Ÿï¼Œè¿™å¯¹äºå…±æƒ…å¯¹è¯å’Œä¸´åºŠæ”¯æŒå…·æœ‰é‡è¦æ„ä¹‰ã€‚è¯¥åŸºå‡†åŒ…å«å•å¥æƒ…æ„Ÿè¯æ®è¯†åˆ«å’Œè·¨äº”å¥è¿ç»­æ®µè½è¯†åˆ«ä¸¤é¡¹ä»»åŠ¡ï¼Œå¹¶é’ˆå¯¹ 1200 ä¸ªçœŸå®ä¸–ç•Œå¥å­æä¾›äº†æƒ…æ„ŸåŠè¯æ®çš„æ–°æ ‡æ³¨ã€‚é€šè¿‡å¯¹ 14 ç§å¼€æº LLMs çš„è¯„ä¼°å‘ç°ï¼Œè™½ç„¶éƒ¨åˆ†æ¨¡å‹åœ¨å¤„ç†å•å¥è¾“å…¥æ—¶æ¥è¿‘äººç±»å¹³å‡æ°´å¹³ï¼Œä½†åœ¨è¾ƒé•¿æ®µè½ä¸­çš„å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚é”™è¯¯åˆ†ææ­ç¤ºäº†æ¨¡å‹çš„ä¸»è¦å¤±æ•ˆæ¨¡å¼ï¼ŒåŒ…æ‹¬å¯¹æƒ…æ„Ÿå…³é”®è¯çš„è¿‡åº¦ä¾èµ–ä»¥åŠåœ¨ä¸­æ€§æ–‡æœ¬ä¸­äº§ç”Ÿçš„è¯¯æŠ¥ (false positives)ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£ LLMs å¦‚ä½•è§£è¯»æƒ…æ„Ÿè¡¨è¾¾æä¾›äº†ç»†ç²’åº¦çš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03490v2",
      "published_date": "2025-10-03 20:15:24 UTC",
      "updated_date": "2025-10-28 01:07:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:03.952182+00:00"
    },
    {
      "arxiv_id": "2510.03486v1",
      "title": "Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains",
      "title_zh": "åŸºäºæ¨ç†çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼šä¸€ç§é¢å‘è·¨é¢†åŸŸçš„å®æ—¶ã€å¯æ‰©å±•ä¸”è‡ªåŠ¨åŒ–çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•",
      "authors": [
        "Anupam Panwar",
        "Himadri Pal",
        "Jiali Chen",
        "Kyle Cho",
        "Riddick Jiang",
        "Miao Zhao",
        "Rajiv Krishnamurthy"
      ],
      "abstract": "Detecting anomalies in large, distributed systems presents several challenges. The first challenge arises from the sheer volume of data that needs to be processed. Flagging anomalies in a high-throughput environment calls for a careful consideration of both algorithm and system design. The second challenge comes from the heterogeneity of time-series datasets that leverage such a system in production. In practice, anomaly detection systems are rarely deployed for a single use case. Typically, there are several metrics to monitor, often across several domains (e.g. engineering, business and operations). A one-size-fits-all approach rarely works, so these systems need to be fine-tuned for every application - this is often done manually. The third challenge comes from the fact that determining the root-cause of anomalies in such settings is akin to finding a needle in a haystack. Identifying (in real time) a time-series dataset that is associated causally with the anomalous time-series data is a very difficult problem. In this paper, we describe a unified framework that addresses these challenges. Reasoning based Anomaly Detection Framework (RADF) is designed to perform real time anomaly detection on very large datasets. This framework employs a novel technique (mSelect) that automates the process of algorithm selection and hyper-parameter tuning for each use case. Finally, it incorporates a post-detection capability that allows for faster triaging and root-cause determination. Our extensive experiments demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly detection models in AUC performance for 5 out of 9 public benchmarking datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a distinction unmatched by any other state-of-the-art model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŸºäºæ¨ç†çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶(Reasoning based Anomaly Detection Framework, RADF)ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿä¸­æ•°æ®é‡å·¨å¤§ã€æ•°æ®é›†å¼‚æ„æ€§å¼ºä»¥åŠæ ¹å› åˆ†æå›°éš¾ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åä¸º mSelect çš„åˆ›æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒåº”ç”¨åœºæ™¯è‡ªåŠ¨å®Œæˆç®—æ³•é€‰æ‹©å’Œè¶…å‚æ•°è°ƒä¼˜(hyper-parameter tuning)ï¼Œå®ç°äº†æ£€æµ‹æµç¨‹çš„é«˜åº¦è‡ªåŠ¨åŒ–ã€‚æ­¤å¤–ï¼ŒRADF è¿˜å…·å¤‡æ£€æµ‹åå¤„ç†èƒ½åŠ›ï¼Œå¯æ˜¾è‘—åŠ å¿«æ•…éšœåˆ†ç±»å’Œæ ¹å› ç¡®å®š(root-cause determination)çš„é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ 9 ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸­ï¼Œæ­è½½ mSelect çš„ RADF åœ¨ 5 ä¸ªæ•°æ®é›†ä¸Šçš„ AUC è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›(state-of-the-art)æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRADF åœ¨ 7 ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡ 0.85 çš„ AUCï¼Œè¯æ˜äº†å…¶åœ¨å¤šé¢†åŸŸç”Ÿäº§ç¯å¢ƒä¸­æä¾›å®æ—¶ã€å¯æ‰©å±•å¼‚å¸¸æ£€æµ‹çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03486v1",
      "published_date": "2025-10-03 20:06:31 UTC",
      "updated_date": "2025-10-03 20:06:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:04.668884+00:00"
    },
    {
      "arxiv_id": "2510.03485v1",
      "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection",
      "title_zh": "è¿ˆå‘ç­–ç•¥åˆè§„æ™ºèƒ½ä½“ï¼šå­¦ä¹ ç”¨äºç­–ç•¥è¿è§„æ£€æµ‹çš„é«˜æ•ˆæŠ¤æ ",
      "authors": [
        "Xiaofei Wen",
        "Wenjie Jacky Mo",
        "Yanan Xie",
        "Peng Qi",
        "Muhao Chen"
      ],
      "abstract": "Autonomous web agents need to operate under externally imposed or human-specified policies while generating long-horizon trajectories. However, little work has examined whether these trajectories comply with such policies, or whether policy violations persist across different contexts such as domains (e.g., shopping or coding websites) and subdomains (e.g., product search and order management in shopping). To address this gap, we introduce PolicyGuardBench, a benchmark of about 60k examples for detecting policy violations in agent trajectories. From diverse agent runs, we generate a broad set of policies and create both within subdomain and cross subdomain pairings with violation labels. In addition to full-trajectory evaluation, PolicyGuardBench also includes a prefix-based violation detection task where models must anticipate policy violations from truncated trajectory prefixes rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a lightweight guardrail model that delivers strong detection accuracy across all tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes across domains and preserves high accuracy on unseen settings. Together, PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework for studying policy compliance in web agent trajectories, and show that accurate and generalizable guardrails are feasible at small scales.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªä¸» Web æ™ºèƒ½ä½“(Autonomous web agents)åœ¨æ‰§è¡Œé•¿å‘¨æœŸè½¨è¿¹æ—¶å¦‚ä½•éµå®ˆå¤–éƒ¨æˆ–äººç±»æŒ‡å®šçš„ç­–ç•¥(Policy)é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰ç ”ç©¶åœ¨è·¨é¢†åŸŸç­–ç•¥è¿è§„æ£€æµ‹(Policy Violation Detection)æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† PolicyGuardBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«çº¦ 6 ä¸‡ä¸ªç¤ºä¾‹çš„åŸºå‡†æµ‹è¯•é›†ï¼Œä¸“é—¨ç”¨äºæ£€æµ‹æ™ºèƒ½ä½“è½¨è¿¹ä¸­çš„ç­–ç•¥è¿è§„è¡Œä¸ºã€‚è¯¥åŸºå‡†æ¶µç›–äº†å­é¢†åŸŸå†…åŠè·¨å­é¢†åŸŸçš„é…å¯¹æ ‡ç­¾ï¼Œå¹¶å¼•å…¥äº†åŸºäºå‰ç¼€çš„è¿è§„æ£€æµ‹ä»»åŠ¡(Prefix-based violation detection)ï¼Œè¦æ±‚æ¨¡å‹èƒ½ä»æˆªæ–­çš„è½¨è¿¹å‰ç¼€ä¸­é¢„åˆ¤è¿è§„ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿè®­ç»ƒäº† PolicyGuard-4Bï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æŠ¤æ æ¨¡å‹(Guardrail model)ï¼Œæ—¨åœ¨ä¿è¯é«˜æ£€æµ‹å‡†ç¡®æ€§çš„åŒæ—¶å…¼é¡¾æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPolicyGuard-4B åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæå¼ºçš„æ£€æµ‹ç²¾åº¦ï¼Œå¹¶èƒ½æœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„é¢†åŸŸå’Œè®¾ç½®ã€‚è¯¥ç ”ç©¶ä¸º Web æ™ºèƒ½ä½“è½¨è¿¹çš„ç­–ç•¥åˆè§„æ€§æä¾›äº†é¦–ä¸ªå…¨é¢æ¡†æ¶ï¼Œè¯æ˜äº†åœ¨å°è§„æ¨¡æ¨¡å‹ä¸Šå®ç°å‡†ç¡®ä¸”å…·æœ‰æ³›åŒ–èƒ½åŠ›çš„æŠ¤æ æŠ€æœ¯æ˜¯åˆ‡å®å¯è¡Œçš„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03485v1",
      "published_date": "2025-10-03 20:03:19 UTC",
      "updated_date": "2025-10-03 20:03:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:08.305953+00:00"
    },
    {
      "arxiv_id": "2510.03483v1",
      "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis",
      "title_zh": "DuPLUSï¼šé¢å‘é€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸é¢„åé¢„æµ‹çš„åŒæç¤ºè§†è§‰-è¯­è¨€æ¡†æ¶",
      "authors": [
        "Numan Saeed",
        "Tausifa Jan Saleem",
        "Fadillah Maani",
        "Muhammad Ridzuan",
        "Hu Wang",
        "Mohammad Yaqub"
      ],
      "abstract": "Deep learning for medical imaging is hampered by task-specific models that lack generalizability and prognostic capabilities, while existing 'universal' approaches suffer from simplistic conditioning and poor medical semantic understanding. To address these limitations, we introduce DuPLUS, a deep learning framework for efficient multi-modal medical image analysis. DuPLUS introduces a novel vision-language framework that leverages hierarchical semantic prompts for fine-grained control over the analysis task, a capability absent in prior universal models. To enable extensibility to other medical tasks, it includes a hierarchical, text-controlled architecture driven by a unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize across three imaging modalities, ten different anatomically various medical datasets, encompassing more than 30 organs and tumor types. It outperforms the state-of-the-art task specific and universal models on 8 out of 10 datasets. We demonstrate extensibility of its text-controlled architecture by seamless integration of electronic health record (EHR) data for prognosis prediction, and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI) of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks and modalities from varying centers, establishing DuPLUS as a versatile and clinically relevant solution for medical image analysis. The code for this work is made available at: https://anonymous.4open.science/r/DuPLUS-6C52",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DuPLUSï¼Œä¸€ç§é’ˆå¯¹é€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸é¢„åé¢„æµ‹çš„åŒæç¤ºè§†è§‰è¯­è¨€æ¡†æ¶ (Dual-Prompt Vision-Language Framework)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç‰¹å®šä»»åŠ¡æ¨¡å‹ç¼ºä¹æ³›åŒ–èƒ½åŠ›ä»¥åŠé€šç”¨æ¨¡å‹è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ›æ–°çš„å±‚çº§è¯­ä¹‰æç¤ºå’Œç‹¬ç‰¹çš„åŒæç¤ºæœºåˆ¶ (Dual-Prompt Mechanism)ï¼Œå®ç°äº†å¯¹åˆ†æä»»åŠ¡çš„ç»†ç²’åº¦æ§åˆ¶ã€‚åœ¨åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒDuPLUS èƒ½å¤Ÿè·¨è¶Š 3 ç§å½±åƒæ¨¡æ€å’Œ 10 ä¸ªè§£å‰–æ•°æ®é›†è¿›è¡Œæ³›åŒ–ï¼Œæ¶µç›–è¶…è¿‡ 30 ç§å™¨å®˜å’Œè‚¿ç˜¤ç±»å‹ï¼Œå¹¶åœ¨ 8 ä¸ªæ•°æ®é›†ä¸Šæ€§èƒ½ä¼˜äºç°æœ‰çš„ç‰¹å®šä»»åŠ¡å’Œé€šç”¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„æ”¯æŒæ— ç¼é›†æˆç”µå­å¥åº·è®°å½• (EHR) ä»¥è¿›è¡Œé¢„åé¢„æµ‹ï¼Œåœ¨å¤´é¢ˆç™Œä»»åŠ¡ä¸­è¾¾åˆ°äº† 0.69 çš„ä¸€è‡´æ€§æŒ‡æ•° (C-Index)ã€‚é€šè¿‡åˆ©ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒ (Parameter-efficient fine-tuning) æŠ€æœ¯ï¼ŒDuPLUS èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ä¸åŒåŒ»ç–—ä¸­å¿ƒçš„æ–°ä»»åŠ¡ï¼Œä¸ºä¸´åºŠåŒ»å­¦å½±åƒåˆ†ææä¾›äº†ä¸€ç§é€šç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03483v1",
      "published_date": "2025-10-03 20:01:00 UTC",
      "updated_date": "2025-10-03 20:01:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:10.055149+00:00"
    },
    {
      "arxiv_id": "2510.03472v1",
      "title": "Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems",
      "title_zh": "æœºå™¨äººåˆ†æ‹£ç³»ç»Ÿä¸­å¤šæœºå™¨äººåä½œçš„â€œç›®çš„åœ°-æ»‘æ§½â€ä»»åŠ¡æ˜ å°„ä¼˜åŒ–",
      "authors": [
        "Yulun Zhang",
        "Alexandre O. G. Barbosa",
        "Federico Pecora",
        "Jiaoyang Li"
      ],
      "abstract": "We study optimizing a destination-to-chutes task mapping to improve throughput in Robotic Sorting Systems (RSS), where a team of robots sort packages on a sortation floor by transporting them from induct workstations to eject chutes based on their shipping destinations (e.g. Los Angeles or Pittsburgh). The destination-to-chutes task mapping is used to determine which chutes a robot can drop its package. Finding a high-quality task mapping is challenging because of the complexity of a real-world RSS. First, optimizing task mapping is interdependent with robot target assignment and path planning. Second, chutes will be CLOSED for a period of time once they receive sufficient packages to allow for downstream processing. Third, task mapping quality directly impacts the downstream processing, as scattered chutes for the same destination increase package handling time. In this paper, we first formally define task mappings and the problem of Task Mapping Optimization (TMO). We then present a simulator of RSS to evaluate task mappings. We then present a simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear Programming, demonstrating the advantage of our optimized task mappings over the greedily generated ones in various RSS setups with different map sizes, numbers of chutes, and destinations. Finally, we use Quality Diversity algorithms to analyze the throughput of a diverse set of task mappings. Our code is available online at https://github.com/lunjohnzhang/tmo_public.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨äººåˆ†æ‹£ç³»ç»Ÿ(Robotic Sorting Systems, RSS)ä¸­çš„ç›®çš„åœ°åˆ°æ»‘æ§½(Destination-to-Chutes)ä»»åŠ¡æ˜ å°„ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–ä»»åŠ¡åˆ†é…æå‡ç³»ç»Ÿååé‡ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä»»åŠ¡æ˜ å°„çš„è´¨é‡å—é™äºæœºå™¨äººè·¯å¾„è§„åˆ’ã€æ»‘æ§½å‘¨æœŸæ€§å…³é—­ä»¥åŠä¸‹æ¸¸å¤„ç†æ•ˆç‡ç­‰å¤æ‚åŠ¨æ€å› ç´ ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ­£å¼å®šä¹‰äº†ä»»åŠ¡æ˜ å°„ä¼˜åŒ–(Task Mapping Optimization, TMO)é—®é¢˜ï¼Œå¹¶å¼€å‘äº†RSSæ¨¡æ‹Ÿå™¨ç”¨äºè¯„ä¼°ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè¿›åŒ–ç®—æ³•(Evolutionary Algorithm)ä¸æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(Mixed Integer Linear Programming, MILP)çš„TMOä¼˜åŒ–æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§åœ°å›¾å°ºå¯¸å’Œæ»‘æ§½é…ç½®ä¸‹ï¼Œè¯¥ä¼˜åŒ–æ˜ å°„æ–¹æ¡ˆåœ¨ååé‡è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºè´ªå©ªç®—æ³•ç”Ÿæˆçš„æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨è´¨é‡å¤šæ ·æ€§(Quality Diversity)ç®—æ³•æ·±å…¥åˆ†æäº†ä¸åŒä»»åŠ¡æ˜ å°„å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ï¼Œä¸ºå¤æ‚ç‰©æµåœºæ™¯ä¸‹çš„å¤šæœºå™¨äººåä½œæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IEEE International Symposium on Multi-Robot and Multi-Agent Systems (MRS) 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03472v1",
      "published_date": "2025-10-03 19:49:37 UTC",
      "updated_date": "2025-10-03 19:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:16.047819+00:00"
    },
    {
      "arxiv_id": "2510.03469v2",
      "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification",
      "title_zh": "è¿æ¥å¤§è¯­è¨€æ¨¡å‹è§„åˆ’æ™ºèƒ½ä½“ä¸å½¢å¼åŒ–æ–¹æ³•ï¼šè§„åˆ’éªŒè¯æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Keshav Ramani",
        "Vali Tawosi",
        "Salwa Alamir",
        "Daniel Borrajo"
      ],
      "abstract": "We introduce a novel framework for evaluating the alignment between natural language plans and their expected behavior by converting them into Kripke structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs) and performing model checking. We systematically evaluate this framework on a simplified version of the PlanBench plan verification dataset and report on metrics like Accuracy, Precision, Recall and F1 scores. Our experiments demonstrate that GPT-5 achieves excellent classification performance (F1 score of 96.3%) while almost always producing syntactically perfect formal representations that can act as guarantees. However, the synthesis of semantically perfect formal models remains an area for future exploration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå°†è‡ªç„¶è¯­è¨€è®¡åˆ’ä¸é¢„æœŸè¡Œä¸ºå¯¹é½çš„æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨Large Language Models (LLMs)å°†è®¡åˆ’è½¬åŒ–ä¸ºKripke structureså’ŒLinear Temporal Logic (LTL)å¹¶è¿›è¡Œæ¨¡å‹æ£€æµ‹(model checking)ã€‚ç ”ç©¶äººå‘˜åœ¨ç®€åŒ–ç‰ˆçš„PlanBenchè®¡åˆ’éªŒè¯æ•°æ®é›†ä¸Šå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œå¹¶æŠ¥å‘Šäº†å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1 scoresç­‰æ ¸å¿ƒæŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-5åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶F1 scoreè¾¾åˆ°äº†96.3%ï¼Œä¸”å‡ ä¹æ€»èƒ½ç”Ÿæˆè¯­æ³•å®Œç¾çš„æ­£å¼è¡¨ç¤ºä»¥ä½œä¸ºè¡Œä¸ºä¿è¯ã€‚å°½ç®¡è¯¥æ¡†æ¶åœ¨åˆ†ç±»æ€§èƒ½ä¸Šè¡¨ç°å“è¶Šï¼Œä½†åˆæˆè¯­ä¹‰å®Œå…¨æ­£ç¡®çš„æ­£å¼æ¨¡å‹(formal models)ä»æ˜¯æœªæ¥äºŸå¾…æ¢ç´¢çš„æ–¹å‘ã€‚è¯¥ç ”ç©¶æˆåŠŸä¸ºLLM Planning Agentsä¸Formal Methodsçš„ç»“åˆæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AgenticSE Workshop at ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03469v2",
      "published_date": "2025-10-03 19:46:55 UTC",
      "updated_date": "2025-11-24 18:17:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:20.153076+00:00"
    },
    {
      "arxiv_id": "2510.03463v2",
      "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework",
      "title_zh": "ALMASï¼šä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“è½¯ä»¶å·¥ç¨‹æ¡†æ¶",
      "authors": [
        "Vali Tawosi",
        "Keshav Ramani",
        "Salwa Alamir",
        "Xiaomo Liu"
      ],
      "abstract": "Multi-agent Large Language Model (LLM) systems have been leading the way in applied LLM research across a number of fields. One notable area is software development, where researchers have advanced the automation of code implementation, code testing, code maintenance, inter alia, using LLM agents. However, software development is a multifaceted environment that extends beyond just code. As such, a successful LLM system must factor in multiple stages of the software development life-cycle (SDLC). In this paper, we propose a vision for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework, which follows the above SDLC philosophy such that it may work within an agile software development team to perform several tasks end-to-end. ALMAS aligns its agents with agile roles, and can be used in a modular fashion to seamlessly integrate with human developers and their development environment. We showcase the progress towards ALMAS through our published works and a use case demonstrating the framework, where ALMAS is able to seamlessly generate an application and add a new feature.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ALMASï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“è½¯ä»¶å·¥ç¨‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶è¿‡åº¦å…³æ³¨ä»£ç å®ç°è€Œå¿½è§†è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸ(SDLC)å¤šé˜¶æ®µåä½œçš„é—®é¢˜ã€‚ALMASéµå¾ªSDLCå“²å­¦ï¼Œé€šè¿‡å°†æ™ºèƒ½ä½“ä¸æ•æ·å¼€å‘è§’è‰²(Agile Roles)è¿›è¡Œå¯¹é½ï¼Œå®ç°åœ¨æ•æ·å¼€å‘å›¢é˜Ÿä¸­çš„ç«¯åˆ°ç«¯ä»»åŠ¡æ‰§è¡Œã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œèƒ½å¤Ÿä¸äººç±»å¼€å‘è€…åŠå…¶å¼€å‘ç¯å¢ƒæ— ç¼é›†æˆã€‚é€šè¿‡å·²å‘è¡¨çš„å·¥ä½œå’Œå®é™…ç”¨ä¾‹å±•ç¤ºï¼ŒALMASè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨ç”Ÿæˆåº”ç”¨ç¨‹åºä»¥åŠè‡ªä¸»æ·»åŠ æ–°åŠŸèƒ½æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºè½¯ä»¶å·¥ç¨‹çš„å…¨é¢è‡ªåŠ¨åŒ–æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to MAS-GAIN Workshop at ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03463v2",
      "published_date": "2025-10-03 19:35:23 UTC",
      "updated_date": "2025-11-24 18:11:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:20.549118+00:00"
    },
    {
      "arxiv_id": "2510.03453v1",
      "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories",
      "title_zh": "è®¤çŸ¥ç†è®ºä¸ç”Ÿæˆç†è®ºçš„å®šæ€§å¯¹æ¯”è¯„ä¼°",
      "authors": [
        "Paul S. Rosenbloom"
      ],
      "abstract": "Evaluation is a critical activity associated with any theory. Yet this has proven to be an exceptionally challenging activity for theories based on cognitive architectures. For an overlapping set of reasons, evaluation can also be challenging for theories based on generative neural architectures. This dual challenge is approached here by leveraging a broad perspective on theory evaluation to yield a wide-ranging, albeit qualitative, comparison of whole-mind-oriented cognitive and generative architectures and the full systems that are based on these architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè®¤çŸ¥æ¶æ„(Cognitive Architectures)å’Œç”Ÿæˆå¼ç¥ç»æ¶æ„(Generative Neural Architectures)çš„ç†è®ºåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­é¢ä¸´çš„ä¸¥å³»æŒ‘æˆ˜è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œç”±äºè¿™äº›æ¶æ„çš„å¤æ‚æ€§ï¼Œä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•åœ¨è¿™ä¸¤ç§ç†è®ºä½“ç³»ä¸­éƒ½è¡¨ç°å‡ºæå¤§çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶é€šè¿‡å¼•å…¥æ›´å¹¿æ³›çš„ç†è®ºè¯„ä¼°è§†è§’ï¼Œå¯¹é¢å‘å…¨è„‘(Whole-mind-oriented)çš„è®¤çŸ¥æ¶æ„ä¸ç”Ÿæˆæ¶æ„åŠå…¶å®Œæ•´ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢çš„å®šæ€§æ¯”è¾ƒã€‚è¿™ç§å¯¹æ¯”åˆ†æä¸ä»…æ¶µç›–äº†ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ï¼Œè¿˜æ·±å…¥æ¢è®¨äº†ä¸¤ç§æ¶æ„åœ¨ç†è®ºæ„å»ºå’Œå®é™…åº”ç”¨ä¸­çš„æœ¬è´¨å·®å¼‚ã€‚è¯¥é¡¹å·¥ä½œä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„è·¨èŒƒå¼è¯„ä¼°æä¾›äº†å®šæ€§å‚è€ƒï¼Œæœ‰åŠ©äºç ”ç©¶è€…æ›´å¥½åœ°ç†è§£ä¸åŒæ¶æ„åœ¨æ¨¡æ‹Ÿå¤æ‚è®¤çŸ¥ä»»åŠ¡æ—¶çš„ä¼˜åŠ¿ä¸çŸ­æ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in Proceedings of the 12th Annual Conference on Advances in Cognitive Systems (ACS-25)",
      "pdf_url": "https://arxiv.org/pdf/2510.03453v1",
      "published_date": "2025-10-03 19:19:48 UTC",
      "updated_date": "2025-10-03 19:19:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:25.365585+00:00"
    },
    {
      "arxiv_id": "2510.03442v1",
      "title": "The Argument is the Explanation: Structured Argumentation for Trust in Agents",
      "title_zh": "è®ºè¯å³è§£é‡Šï¼šæ„å»ºæ™ºèƒ½ä½“ä¿¡ä»»çš„ç»“æ„åŒ–è®ºè¯",
      "authors": [
        "Ege Cakar",
        "Per Ola Kristensson"
      ],
      "abstract": "Humans are black boxes -- we cannot observe their neural processes, yet society functions by evaluating verifiable arguments. AI explainability should follow this principle: stakeholders need verifiable reasoning chains, not mechanistic transparency. We propose using structured argumentation to provide a level of explanation and verification neither interpretability nor LLM-generated explanation is able to offer. Our pipeline achieves state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7 points above prior work) and $0.81$ macro F1, $\\sim$0.07 above previous published results with comparable data setups, for Argumentative MicroTexts relation classification, converting LLM text into argument graphs and enabling verification at each inferential step. We demonstrate this idea on multi-agent risk assessment using the Structured What-If Technique, where specialized agents collaborate transparently to carry out risk assessment otherwise achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we capture support/attack relationships, thereby enabling automatic hallucination detection via fact nodes attacking arguments. We also provide a verification mechanism that enables iterative refinement through test-time feedback without retraining. For easy deployment, we provide a Docker container for the fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python package on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†â€œè®ºè¯å³è§£é‡Šâ€çš„ç†å¿µï¼Œä¸»å¼ åˆ©ç”¨ç»“æ„åŒ–è®ºè¯(Structured Argumentation)ä¸ºAIç³»ç»Ÿæä¾›å¯éªŒè¯çš„æ¨ç†é“¾ï¼Œä»¥å¼¥è¡¥å½“å‰è§£é‡Šæ€§(AI explainability)ç ”ç©¶ä¸­æœºåˆ¶é€æ˜åº¦çš„ä¸è¶³ã€‚è¯¥æµæ°´çº¿èƒ½å¤Ÿå°†å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„æ–‡æœ¬è½¬åŒ–ä¸ºè®ºè¯å›¾(Argument Graphs)ï¼Œä»è€Œåœ¨æ¯ä¸ªæ¨æ–­æ­¥éª¤å®ç°éªŒè¯ï¼Œå¹¶åœ¨AAECå’ŒAMTæ•°æ®é›†ä¸Šå‡å–å¾—äº†é¢†åŸŸé¢†å…ˆçš„å®F1åˆ†æ•°ã€‚é€šè¿‡åœ¨å¤šæ™ºèƒ½ä½“é£é™©è¯„ä¼°ä¸­åº”ç”¨ç»“æ„åŒ–å‡è®¾æŠ€æœ¯(Structured What-If Technique)ï¼Œç ”ç©¶å±•ç¤ºäº†ä¸“é—¨åŒ–æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡é€æ˜åä½œå®Œæˆå¤æ‚çš„é£é™©è¯„ä¼°ä»»åŠ¡ã€‚åˆ©ç”¨åŒæåŸºäºå‡è®¾çš„è®ºè¯(Bipolar Assumption-Based Argumentation)æ•æ‰æ”¯æŒä¸æ”»å‡»å…³ç³»ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡äº‹å®èŠ‚ç‚¹æœ‰æ•ˆå®ç°è‡ªåŠ¨å¹»è§‰æ£€æµ‹(Hallucination detection)ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆæä¾›äº†ä¸€ç§æ— éœ€é‡è®­ç»ƒå³å¯é€šè¿‡æµ‹è¯•æ—¶åé¦ˆ(test-time feedback)è¿›è¡Œè¿­ä»£ä¼˜åŒ–çš„éªŒè¯æœºåˆ¶ï¼Œå¹¶é…å¥—å‘å¸ƒäº†å¾®è°ƒåçš„æ¨¡å‹é•œåƒåŠPythonå·¥å…·åŒ…ä»¥æ–¹ä¾¿éƒ¨ç½²ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures, 6 tables, submitted to IAAI-26",
      "pdf_url": "https://arxiv.org/pdf/2510.03442v1",
      "published_date": "2025-10-03 19:04:15 UTC",
      "updated_date": "2025-10-03 19:04:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:27.944283+00:00"
    },
    {
      "arxiv_id": "2510.03441v1",
      "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning",
      "title_zh": "Spatial-ViLTï¼šé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å¢å¼ºè§†è§‰ç©ºé—´æ¨ç†",
      "authors": [
        "Chashi Mahiul Islam",
        "Oteo Mamo",
        "Samuel Jacob Chacko",
        "Xiuwen Liu",
        "Weikuan Yu"
      ],
      "abstract": "Vision-language models (VLMs) have advanced multimodal reasoning but still face challenges in spatial reasoning for 3D scenes and complex object configurations. To address this, we introduce SpatialViLT, an enhanced VLM that integrates spatial features like depth maps, 3D coordinates, and edge maps through a multi-task learning framework. This approach enriches multimodal embeddings with spatial understanding. We propose two variants: SpatialViLT and MaskedSpatialViLT, focusing on full and masked object regions, respectively. Additionally, SpatialEnsemble combines both approaches, achieving state-of-the-art accuracy. Our models excel in spatial reasoning categories such as directional, topological, and proximity relations, as demonstrated on the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a significant step in enhancing the spatial intelligence of AI systems, crucial for advanced multimodal understanding and real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Spatial-ViLTï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨ 3D åœºæ™¯åŠå¤æ‚ç‰©ä½“é…ç½®ä¸­ç©ºé—´æ¨ç†èƒ½åŠ›çš„æ”¹è¿›æ¡†æ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šä»»åŠ¡å­¦ä¹  (Multi-Task Learning) æ•´åˆäº†æ·±åº¦å›¾ (depth maps)ã€3D åæ ‡ (3D coordinates) å’Œè¾¹ç¼˜å›¾ (edge maps) ç­‰ç©ºé—´ç‰¹å¾ï¼Œæ˜¾è‘—ä¸°å¯Œäº†å¤šæ¨¡æ€åµŒå…¥çš„ç©ºé—´è¯­ä¹‰ã€‚ç ”ç©¶è€…è®¾è®¡äº† SpatialViLT å’Œ MaskedSpatialViLT ä¸¤ä¸ªå˜ä½“ï¼Œå¹¶æå‡º SpatialEnsemble èåˆæ–¹æ¡ˆï¼Œåœ¨ Visual Spatial Reasoning (VSR) æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿› (State-of-the-art) çš„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»åˆ—æ¨¡å‹åœ¨å¤„ç†æ–¹å‘ (directional)ã€æ‹“æ‰‘ (topological) å’Œé‚»è¿‘ (proximity) å…³ç³»ç­‰ç©ºé—´æ¨ç†ç±»åˆ«æ—¶è¡¨ç°å‡ºè‰²ã€‚è¿™ä¸€æˆæœæœ‰æ•ˆæå‡äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç©ºé—´æ™ºèƒ½ï¼Œå¯¹é«˜çº§å¤šæ¨¡æ€ç†è§£å’Œå®é™…åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03441v1",
      "published_date": "2025-10-03 19:04:15 UTC",
      "updated_date": "2025-10-03 19:04:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:30.145422+00:00"
    },
    {
      "arxiv_id": "2510.03438v1",
      "title": "Scalable Ground Station Selection for Large LEO Constellations",
      "title_zh": "å¤§å‹ä½è½¨å«æ˜Ÿæ˜Ÿåº§çš„å¯æ‰©å±•åœ°é¢ç«™é€‰æ‹©",
      "authors": [
        "Grace Ra Kim",
        "Duncan Eddy",
        "Vedant Srinivas",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "Effective ground station selection is critical for low Earth orbiting (LEO) satellite constellations to minimize operational costs, maximize data downlink volume, and reduce communication gaps between access windows. Traditional ground station selection typically begins by choosing from a fixed set of locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps reduce the problem scope to optimizing locations over existing infrastructure. However, finding a globally optimal solution for stations using existing mixed-integer programming methods quickly becomes intractable at scale, especially when considering multiple providers and large satellite constellations. To address this issue, we introduce a scalable, hierarchical framework that decomposes the global selection problem into single-satellite, short time-window subproblems. Optimal station choices from each subproblem are clustered to identify consistently high-value locations across all decomposed cases. Cluster-level sets are then matched back to the closest GSaaS candidate sites to produce a globally feasible solution. This approach enables scalable coordination while maintaining near-optimal performance. We evaluate our method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10 stations), achieving solutions within 95% of the global IP optimum for all test cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and Planet's Flock (96) show that while exact IP solutions fail to scale, our framework continues to deliver high-quality site selections.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½åœ°çƒè½¨é“(LEO)å«æ˜Ÿæ˜Ÿåº§çš„åœ°é¢ç«™é€‰æ‹©é—®é¢˜ï¼Œæ—¨åœ¨ä¼˜åŒ–è¿è¥æˆæœ¬ã€æœ€å¤§åŒ–æ•°æ®ä¸‹è¡Œå®¹é‡å¹¶å‡å°‘é€šä¿¡é—´éš”ã€‚ç”±äºä¼ ç»Ÿçš„æ··åˆæ•´æ•°è§„åˆ’(Mixed-Integer Programming, MIP)æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ˜Ÿåº§å’Œå¤šæœåŠ¡å•†æ—¶éš¾ä»¥æ‰©å±•ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„å±‚æ¬¡åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å…¨å±€é€‰æ‹©é—®é¢˜åˆ†è§£ä¸ºå•å«æ˜Ÿã€çŸ­æ—¶é—´çª—å£çš„å­é—®é¢˜ï¼Œé€šè¿‡èšç±»åˆ†æè¯†åˆ«å‡ºé«˜ä»·å€¼ä½ç½®ï¼Œå¹¶å°†å…¶ä¸åœ°é¢ç«™å³æœåŠ¡(GSaaS)å€™é€‰ç«™ç‚¹åŒ¹é…ä»¥ç”Ÿæˆå…¨å±€å¯è¡Œæ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åˆæˆçš„Walker-Staræ¡ˆä¾‹ä¸­ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è§£è¾¾åˆ°äº†å…¨å±€IPæœ€ä¼˜è§£çš„95%ä»¥ä¸Šã€‚åœ¨å¯¹Capella Spaceã€ICEYEå’ŒPlanet's Flockç­‰çœŸå®æ¡ˆä¾‹çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨ç²¾ç¡®IPè§£æ³•å¤±æ•ˆçš„å¤§è§„æ¨¡åœºæ™¯ä¸‹ä¾ç„¶ä¿æŒäº†é«˜è´¨é‡çš„ç«™ç‚¹é€‰æ‹©æ€§èƒ½ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "14 pages, 7 tables, 10 figures, submitted to IEEE Aeroconf 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.03438v1",
      "published_date": "2025-10-03 18:58:15 UTC",
      "updated_date": "2025-10-03 18:58:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:45.952742+00:00"
    },
    {
      "arxiv_id": "2510.08589v1",
      "title": "Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes",
      "title_zh": "è¶…è¶Š CNNï¼šé¢å‘å°æ ·æœ¬ç›®æ ‡æ£€æµ‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå¾®è°ƒ",
      "authors": [
        "Nirmal Elamon",
        "Rouzbeh Davoudi"
      ],
      "abstract": "The field of object detection and understanding is rapidly evolving, driven by advances in both traditional CNN-based models and emerging multi-modal large language models (LLMs). While CNNs like ResNet and YOLO remain highly effective for image-based tasks, recent transformer-based LLMs introduce new capabilities such as dynamic context reasoning, language-guided prompts, and holistic scene understanding. However, when used out-of-the-box, the full potential of LLMs remains underexploited, often resulting in suboptimal performance on specialized visual tasks. In this work, we conduct a comprehensive comparison of fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and fine-tuned multi-modal LLMs on the challenging task of artificial text overlay detection in images. A key contribution of our study is demonstrating that LLMs can be effectively fine-tuned on very limited data (fewer than 1,000 images) to achieve up to 36% accuracy improvement, matching or surpassing CNN-based baselines that typically require orders of magnitude more data. By exploring how language-guided models can be adapted for precise visual understanding with minimal supervision, our work contributes to the broader effort of bridging vision and language, offering novel insights into efficient cross-modal learning strategies. These findings highlight the adaptability and data efficiency of LLM-based approaches for real-world object detection tasks and provide actionable guidance for applying multi-modal transformers in low-resource visual environments. To support continued progress in this area, we have made the code used to fine-tune the models available in our GitHub, enabling future improvements and reuse in related applications.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒäººå·¥æ–‡æœ¬è¦†ç›–æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå¯¹å¾®è°ƒåçš„ä¼ ç»Ÿ CNNsã€é›¶æ ·æœ¬é¢„è®­ç»ƒ Multi-Modal LLMs ä»¥åŠå¾®è°ƒåçš„ Multi-Modal LLMs è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚è™½ç„¶ä¼ ç»Ÿ CNNs åœ¨å›¾åƒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¯¥ç ”ç©¶å‘ç° Multi-Modal LLMs çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›åœ¨ç‰¹å®šè§†è§‰ä»»åŠ¡ä¸­å¸¸è¢«ä½ä¼°ã€‚è¯¥å·¥ä½œçš„ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯è¯æ˜äº† Multi-Modal LLMs å¯ä»¥åœ¨å°‘äº 1,000 å¼ å›¾åƒçš„æä½æ•°æ®é‡ä¸‹è¿›è¡Œæœ‰æ•ˆå¾®è°ƒï¼Œå¹¶å®ç°é«˜è¾¾ 36% çš„å‡†ç¡®ç‡æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯åŒ¹é…æˆ–è¶…è¶Šé€šå¸¸éœ€è¦å¤§è§„æ¨¡æ•°æ®çš„ CNN-based baselinesï¼Œçªæ˜¾äº†åŸºäº LLM æ–¹æ¡ˆçš„æ•°æ®æ•ˆç‡å’Œé€‚åº”æ€§ã€‚é€šè¿‡æ¢ç´¢è¯­è¨€å¼•å¯¼æ¨¡å‹åœ¨æå°ç›‘ç£ä¸‹å®ç°ç²¾ç¡®è§†è§‰ç†è§£çš„æ½œåŠ›ï¼Œæœ¬ç ”ç©¶ä¸ºä½èµ„æºç¯å¢ƒä¸‹çš„ Cross-modal learning ç­–ç•¥æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³å¾®è°ƒä»£ç ä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.08589v1",
      "published_date": "2025-10-03 18:53:18 UTC",
      "updated_date": "2025-10-03 18:53:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:46.944160+00:00"
    },
    {
      "arxiv_id": "2510.03431v1",
      "title": "Application of a Virtual Imaging Framework for Investigating a Deep Learning-Based Reconstruction Method for 3D Quantitative Photoacoustic Computed Tomography",
      "title_zh": "è™šæ‹Ÿæˆåƒæ¡†æ¶åœ¨ä¸‰ç»´å®šé‡å…‰å£°è®¡ç®—æ–­å±‚æ‰«ææ·±åº¦å­¦ä¹ é‡å»ºæ–¹æ³•ç ”ç©¶ä¸­çš„åº”ç”¨",
      "authors": [
        "Refik Mert Cam",
        "Seonyeong Park",
        "Umberto Villa",
        "Mark A. Anastasio"
      ],
      "abstract": "Quantitative photoacoustic computed tomography (qPACT) is a promising imaging modality for estimating physiological parameters such as blood oxygen saturation. However, developing robust qPACT reconstruction methods remains challenging due to computational demands, modeling difficulties, and experimental uncertainties. Learning-based methods have been proposed to address these issues but remain largely unvalidated. Virtual imaging (VI) studies are essential for validating such methods early in development, before proceeding to less-controlled phantom or in vivo studies. Effective VI studies must employ ensembles of stochastically generated numerical phantoms that accurately reflect relevant anatomy and physiology. Yet, most prior VI studies for qPACT relied on overly simplified phantoms. In this work, a realistic VI testbed is employed for the first time to assess a representative 3D learning-based qPACT reconstruction method for breast imaging. The method is evaluated across subject variability and physical factors such as measurement noise and acoustic aberrations, offering insights into its strengths and limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨Virtual Imaging (VI)æ¡†æ¶å¯¹3D Quantitative Photoacoustic Computed Tomography (qPACT)çš„Deep Learning-Basedé‡å»ºæ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚é’ˆå¯¹qPACTé‡å»ºé¢ä¸´çš„è®¡ç®—å‹åŠ›ã€å»ºæ¨¡å›°éš¾å’Œå®éªŒéªŒè¯ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œç ”ç©¶å¼ºè°ƒäº†åœ¨è¿›è¡Œç¦»ä½“æˆ–æ´»ä½“å®éªŒå‰åˆ©ç”¨VIç ”ç©¶è¿›è¡Œæ—©æœŸéªŒè¯çš„é‡è¦æ€§ã€‚è¯¥å·¥ä½œé¦–æ¬¡é‡‡ç”¨é«˜åº¦é€¼çœŸçš„VIæµ‹è¯•å¹³å°ï¼Œé’ˆå¯¹ä¹³è…ºæˆåƒä¸­çš„3Då­¦ä¹ å‹qPACTé‡å»ºæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ã€‚é€šè¿‡è€ƒå¯Ÿå—è¯•è€…å·®å¼‚æ€§ä»¥åŠæµ‹é‡å™ªå£°ã€å£°å­¦ç•¸å˜ç­‰ç‰©ç†å› ç´ çš„å½±å“ï¼Œç ”ç©¶æ·±å…¥åˆ†æäº†è¯¥é‡å»ºæ–¹æ³•çš„æ€§èƒ½è¡¨ç°åŠå…¶ä¼˜åŠ¿ä¸å±€é™ã€‚è¯¥ç ”ç©¶ä¸ºå­¦ä¹ å‹é‡å»ºç®—æ³•åœ¨å¤æ‚åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„ç¨³å¥æ€§è¯„ä¼°æä¾›äº†é‡è¦å‚è€ƒå’Œæœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "physics.med-ph",
      "comment": "Preprint submitted to Elsevier Photoacoustics",
      "pdf_url": "https://arxiv.org/pdf/2510.03431v1",
      "published_date": "2025-10-03 18:49:51 UTC",
      "updated_date": "2025-10-03 18:49:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:48.238184+00:00"
    },
    {
      "arxiv_id": "2510.03426v2",
      "title": "Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation",
      "title_zh": "é¢å‘å¯æ‰©å±•å¹¶è¡Œé«˜åŠ¨æ€èŒƒå›´è®¡ç®—çš„å¹¿ä¹‰æ•°é‡çº§",
      "authors": [
        "Franz A. Heinsen",
        "Leo Kozachkov"
      ],
      "abstract": "Many domains, from deep learning to finance, require compounding real numbers over long sequences, often leading to catastrophic numerical underflow or overflow. We introduce generalized orders of magnitude (GOOMs), a principled extension of traditional orders of magnitude that incorporates floating-point numbers as a special case, and which in practice enables stable computation over significantly larger dynamic ranges of real numbers than previously possible. We implement GOOMs, along with an efficient custom parallel prefix scan, to support native execution on parallel hardware such as GPUs. We demonstrate that our implementation of GOOMs outperforms traditional approaches with three representative experiments, all of which were previously considered impractical or impossible, and now become possible and practical: (1) compounding real matrix products far beyond standard floating-point limits; (2) estimating spectra of Lyapunov exponents in parallel, orders of magnitude faster than with previous methods, applying a novel selective-resetting method to prevent state colinearity; and (3) capturing long-range dependencies in deep recurrent neural networks with non-diagonal recurrent states, computed in parallel via a prefix scan, without requiring any form of stabilization. Our results show that our implementation of GOOMs, combined with efficient parallel scanning, offers a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¹¿ä¹‰æ•°é‡çº§ (Generalized Orders of Magnitude, GOOMs)ï¼Œè¿™æ˜¯ä¸€ç§ä¼ ç»Ÿæ•°é‡çº§çš„åŸåˆ™æ€§æ‰©å±•ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ å’Œé‡‘èç­‰é¢†åŸŸåœ¨é•¿åºåˆ—å®æ•°å¤åˆè¿ç®—ä¸­å¸¸è§çš„æ•°å€¼ä¸‹æº¢æˆ–æº¢å‡ºé—®é¢˜ã€‚GOOMs å°†ä¼ ç»Ÿçš„æµ®ç‚¹æ•° (floating-point numbers) è§†ä¸ºå…¶ç‰¹ä¾‹ï¼Œåœ¨å®è·µä¸­èƒ½å¤Ÿæ”¯æŒæ¯”ä»¥å¾€æ›´å®½çš„åŠ¨æ€èŒƒå›´ï¼Œå¹¶æ”¯æŒåœ¨ GPU ç­‰å¹¶è¡Œç¡¬ä»¶ä¸Šè¿›è¡ŒåŸç”Ÿæ‰§è¡Œã€‚é€šè¿‡ç»“åˆé«˜æ•ˆçš„è‡ªå®šä¹‰å¹¶è¡Œå‰ç¼€æ‰«æ (parallel prefix scan)ï¼Œè¯¥æ¡†æ¶æˆåŠŸå®ç°äº†è¿œè¶…æ ‡å‡†æµ®ç‚¹æé™çš„å®æ•°çŸ©é˜µä¹˜æ³•å¤åˆï¼Œå¹¶åœ¨ Lyapunov exponents å…‰è°±ä¼°è®¡ä»»åŠ¡ä¸­åˆ©ç”¨æ–°å‹çš„é€‰æ‹©æ€§é‡ç½® (selective-resetting) æ–¹æ³•æ˜¾è‘—æå‡äº†è®¡ç®—é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒGOOMs èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–ç¨³å®šåŒ–å¤„ç†çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¹¶è¡ŒåŒ–æ–¹å¼æ•æ‰å…·æœ‰éå¯¹è§’å¾ªç¯çŠ¶æ€çš„æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ (deep recurrent neural networks) ä¸­çš„é•¿ç¨‹ä¾èµ–ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGOOMs ä¸ºé«˜åŠ¨æ€èŒƒå›´åº”ç”¨æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æ•°å€¼é²æ£’çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè§£å†³äº†å¤šé¡¹æ­¤å‰è¢«è®¤ä¸ºåœ¨è®¡ç®—ä¸Šä¸å¯è¡Œçš„å­¦æœ¯éš¾é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 4 figures (main text). 14 pages, 21 figures (appendix). Code is at https://github.com/glassroom/generalized_orders_of_magnitude",
      "pdf_url": "https://arxiv.org/pdf/2510.03426v2",
      "published_date": "2025-10-03 18:38:26 UTC",
      "updated_date": "2025-10-09 13:23:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:53.345955+00:00"
    },
    {
      "arxiv_id": "2510.03419v2",
      "title": "Multi-task Neural Diffusion Processes",
      "title_zh": "å¤šä»»åŠ¡ç¥ç»æ‰©æ•£è¿‡ç¨‹",
      "authors": [
        "Joseph Rawson",
        "Domniki Ladopoulou",
        "Petros Dellaportas"
      ],
      "abstract": "Neural diffusion processes provide a scalable, non-Gaussian approach to modelling distributions over functions, but existing formulations are limited to single-task inference and do not capture dependencies across related tasks. In many multi-task regression settings, jointly modelling correlated functions and enabling task-aware conditioning is crucial for improving predictive performance and uncertainty calibration, particularly in low-data regimes. We propose multi-task neural diffusion processes, an extension that incorporates a task encoder to enable task-conditioned probabilistic regression and few-shot adaptation across related functions. The task encoder extracts a low-dimensional representation from context observations and conditions the diffusion model on this representation, allowing information sharing across tasks while preserving input-size agnosticity and the equivariance properties of neural diffusion processes. The resulting framework retains the expressiveness and scalability of neural diffusion processes while enabling efficient transfer to unseen tasks. Empirical results demonstrate improved point prediction accuracy and better-calibrated predictive uncertainty compared to single-task neural diffusion processes and Gaussian process baselines. We validate the approach on real wind farm data appropriate for wind power prediction. In this high-impact application, reliable uncertainty quantification directly supports operational decision-making in wind farm management, illustrating effective few-shot adaptation in a challenging real-world multi-task regression setting.",
      "tldr_zh": "é’ˆå¯¹ç°æœ‰ç¥ç»æ‰©æ•£è¿‡ç¨‹(Neural diffusion processes, NDP)ä»…é™äºå•ä»»åŠ¡æ¨ç†ä¸”æ— æ³•æ•æ‰ç›¸å…³ä»»åŠ¡é—´ä¾èµ–å…³ç³»çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†å¤šä»»åŠ¡ç¥ç»æ‰©æ•£è¿‡ç¨‹(multi-task neural diffusion processes)ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä»»åŠ¡ç¼–ç å™¨(task encoder)ä»èƒŒæ™¯è§‚æµ‹ä¸­æå–ä½ç»´è¡¨ç¤ºï¼Œå®ç°äº†ä»»åŠ¡æ¡ä»¶çš„æ¦‚ç‡å›å½’å’Œè·¨ç›¸å…³å‡½æ•°çš„å°‘æ ·æœ¬è‡ªé€‚åº”(few-shot adaptation)ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒè¾“å…¥å°ºå¯¸ä¸å¯çŸ¥æ€§å’Œç­‰å˜æ€§(equivariance)çš„åŒæ—¶ï¼Œåˆ©ç”¨ä»»åŠ¡ç¼–ç å™¨ä¿ƒè¿›äº†ä»»åŠ¡é—´çš„ä¿¡æ¯å…±äº«ï¼Œå¹¶æ”¯æŒå‘æœªçŸ¥ä»»åŠ¡çš„é«˜æ•ˆè¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç‚¹é¢„æµ‹å‡†ç¡®æ€§å’Œé¢„æµ‹ä¸ç¡®å®šæ€§æ ¡å‡†æ–¹é¢å‡ä¼˜äºå•ä»»åŠ¡NDPå’Œé«˜æ–¯è¿‡ç¨‹(Gaussian process)åŸºçº¿ã€‚é€šè¿‡åœ¨çœŸå®é£ç”µåœºé¢„æµ‹æ•°æ®ä¸Šçš„éªŒè¯ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†å…¶åœ¨å¤æ‚ç°å®å¤šä»»åŠ¡å›å½’åœºæ™¯ä¸­æä¾›å¯é ä¸ç¡®å®šæ€§é‡åŒ–ä»¥æ”¯æŒè¿è¥å†³ç­–çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 12 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03419v2",
      "published_date": "2025-10-03 18:26:23 UTC",
      "updated_date": "2026-01-16 12:40:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:30:59.852323+00:00"
    },
    {
      "arxiv_id": "2510.03418v2",
      "title": "LegalWiz: A Multi-Agent Generation Framework for Contradiction Detection in Legal Documents",
      "title_zh": "LegalWizï¼šä¸€ç§é¢å‘æ³•å¾‹æ–‡æ¡£çŸ›ç›¾æ£€æµ‹çš„å¤šæ™ºèƒ½ä½“ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Ananya Mantravadi",
        "Shivali Dalmia",
        "Olga Pospelova",
        "Abhishek Mukherji",
        "Nand Dave",
        "Anudha Mittal"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) integrates large language models (LLMs) with external sources, but unresolved contradictions in retrieved evidence often lead to hallucinations and legally unsound outputs. Benchmarks currently used for contradiction detection lack domain realism, cover only limited conflict types, and rarely extend beyond single-sentence pairs, making them unsuitable for legal applications. Controlled generation of documents with embedded contradictions is therefore essential: it enables systematic stress-testing of models, ensures coverage of diverse conflict categories, and provides a reliable basis for evaluating contradiction detection and resolution. We present a multi-agent contradiction-aware benchmark framework for the legal domain that generates synthetic legal-style documents, injects six structured contradiction types, and models both self- and pairwise inconsistencies. Automated contradiction mining is combined with human-in-the-loop validation to guarantee plausibility and fidelity. This benchmark offers one of the first structured resources for contradiction-aware evaluation in legal RAG pipelines, supporting more consistent, interpretable, and trustworthy systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LegalWizï¼Œä¸€ç§ç”¨äºæ³•å¾‹æ–‡æ¡£çŸ›ç›¾æ£€æµ‹çš„å¤šæ™ºèƒ½ä½“ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­å› æ£€ç´¢è¯æ®çŸ›ç›¾è€Œå¯¼è‡´çš„å¹»è§‰å’Œæ³•å¾‹ä¸å¯é è¾“å‡ºé—®é¢˜ã€‚ç”±äºç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹é¢†åŸŸçœŸå®æ€§ä¸”æ¶µç›–å†²çªç±»å‹æœ‰é™ï¼ŒLegalWizé€šè¿‡ç”Ÿæˆåˆæˆæ³•å¾‹é£æ ¼æ–‡æ¡£å¹¶æ³¨å…¥å…­ç§ç»“æ„åŒ–çŸ›ç›¾ç±»å‹ï¼Œç³»ç»Ÿåœ°æ¨¡æ‹Ÿäº†æ–‡æ¡£å†…çš„è‡ªçŸ›ç›¾ä¸æˆå¯¹ä¸ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è‡ªåŠ¨åŒ–çŸ›ç›¾æŒ–æ˜ä¸äººå·¥åœ¨ç¯(human-in-the-loop)éªŒè¯ï¼Œæœ‰æ•ˆä¿è¯äº†ç”Ÿæˆå†…å®¹çš„åˆç†æ€§ä¸é«˜ä¿çœŸåº¦ã€‚è¯¥åŸºå‡†æ¡†æ¶ä¸ºæ³•å¾‹RAGæµæ°´çº¿çš„çŸ›ç›¾æ„ŸçŸ¥è¯„ä¼°æä¾›äº†é¦–æ‰¹ç»“æ„åŒ–èµ„æºï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å¯é æ€§ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ€§çš„å‹åŠ›æµ‹è¯•ï¼Œè¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´å…·å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦çš„æ³•å¾‹æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03418v2",
      "published_date": "2025-10-03 18:24:27 UTC",
      "updated_date": "2025-10-10 19:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:01.606671+00:00"
    },
    {
      "arxiv_id": "2510.03417v2",
      "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks",
      "title_zh": "NEXUSï¼šé¢å‘å¤šè½®å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±ä¸­ä¸å®‰å…¨åºåˆ—åˆ©ç”¨çš„ç½‘ç»œæ¢ç´¢",
      "authors": [
        "Javad Rafiei Asl",
        "Sidhant Narula",
        "Mohammad Ghasemigol",
        "Eduardo Blanco",
        "Daniel Takabi"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized natural language processing but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks that distribute malicious intent across benign exchanges and bypass alignment mechanisms. Existing approaches often explore the adversarial space poorly, rely on hand-crafted heuristics, or lack systematic query refinement. We present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular framework for constructing, refining, and executing optimized multi-turn attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a harmful intent into a structured semantic network of topics, entities, and query chains; (2) a feedback-driven Simulator that iteratively refines and prunes these chains through attacker-victim-judge LLM collaboration using harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser that adaptively navigates the refined query space for real-time attacks. This pipeline uncovers stealthy, high-success adversarial paths across LLMs. On several closed-source and open-source LLMs, NEXUS increases attack success rate by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­æ˜“å—è¶Šç‹±æ”»å‡»ï¼ˆJailbreak Attacksï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º NEXUS çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºã€ä¼˜åŒ–å¹¶æ‰§è¡Œå¤šè½®æ”»å‡»åºåˆ—ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šThoughtNet å°†æœ‰å®³æ„å›¾å±‚æ¬¡åŒ–æ‰©å±•ä¸ºåŒ…å«ä¸»é¢˜å’Œå®ä½“çš„ç»“æ„åŒ–è¯­ä¹‰ç½‘ç»œï¼›åé¦ˆé©±åŠ¨çš„ Simulator é€šè¿‡æ”»å‡»è€…-å—å®³è€…-è£åˆ¤ï¼ˆAttacker-Victim-Judgeï¼‰çš„åä½œæ¥è¿­ä»£ç»†åŒ–æŸ¥è¯¢é“¾ï¼›Network Traverser åˆ™åœ¨å®æ—¶æ”»å‡»ä¸­è‡ªé€‚åº”å¯¼èˆªä¼˜åŒ–åçš„æŸ¥è¯¢ç©ºé—´ã€‚NEXUS è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¯¹æŠ—ç©ºé—´æ¢ç´¢ä¸è¶³åŠç¼ºä¹ç³»ç»ŸåŒ–æŸ¥è¯¢ç²¾ç»†åŒ–ç­‰ç¼ºé™·ï¼Œèƒ½å¤ŸæŒ–æ˜å‡ºæ›´å…·éšè”½æ€§ä¸”é«˜æˆåŠŸç‡çš„å¯¹æŠ—è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNEXUS åœ¨å¤šç§å¼€æºå’Œé—­æº LLMs ä¸Šçš„æ”»å‡»æˆåŠŸç‡ï¼ˆAttack Success Rateï¼‰è¾ƒç°æœ‰æ–¹æ³•æå‡äº† 2.1% è‡³ 19.4%ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†å¤šè½®äº¤äº’ä¸­å¯¹é½æœºåˆ¶çš„è„†å¼±æ€§ï¼Œä¹Ÿä¸ºè¯„ä¼° LLMs çš„å®‰å…¨æ€§æä¾›äº†ç³»ç»ŸåŒ–çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper has been accepted in the main conference proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors",
      "pdf_url": "https://arxiv.org/pdf/2510.03417v2",
      "published_date": "2025-10-03 18:24:14 UTC",
      "updated_date": "2025-10-21 17:41:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:02.747504+00:00"
    },
    {
      "arxiv_id": "2510.03415v2",
      "title": "PLSemanticsBench: Large Language Models As Programming Language Interpreters",
      "title_zh": "PLSemanticsBenchï¼šä½œä¸ºç¼–ç¨‹è¯­è¨€è§£é‡Šå™¨çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Aditya Thimmaiah",
        "Jiyang Zhang",
        "Jayanth Srinivasa",
        "Junyi Jessy Li",
        "Milos Gligoric"
      ],
      "abstract": "As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦èƒ½ä»…åŸºäºç¼–ç¨‹è¯­è¨€çš„å½¢å¼åŒ–è¯­ä¹‰(formal semantics)æ¥æ‰§è¡Œç¨‹åºå¹¶å……å½“è§£é‡Šå™¨ï¼Œæ—¨åœ¨å®ç°ç¼–ç¨‹è¯­è¨€æ–°ç‰¹æ€§çš„å¿«é€ŸåŸå‹è®¾è®¡ã€‚ç ”ç©¶è€…æå‡ºäº†PLSemanticsBenchåŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨å°æ­¥æ“ä½œè¯­ä¹‰(SOS)å’Œé‡å†™æ“ä½œè¯­ä¹‰(K-semantics)å¯¹å‘½ä»¤å¼è¯­è¨€IMPè¿›è¡Œå½¢å¼åŒ–ï¼Œå¹¶åœ¨æœ€ç»ˆçŠ¶æ€é¢„æµ‹ã€è¯­ä¹‰è§„åˆ™é¢„æµ‹åŠæ‰§è¡Œè½¨è¿¹é¢„æµ‹ä¸‰é¡¹ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ä¸ºåŒºåˆ†æ¨¡å‹æ˜¯ä¾èµ–é¢„è®­ç»ƒè®°å¿†è¿˜æ˜¯å…·å¤‡çœŸå®çš„è¯­ä¹‰ç†è§£ï¼Œç ”ç©¶é€šè¿‡ç³»ç»Ÿå˜å¼‚è§„åˆ™å®šä¹‰äº†éæ ‡å‡†è¯­ä¹‰(nonstandard semantics)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼ºæ¨ç†æ¨¡å‹åœ¨éæ ‡å‡†è¯­ä¹‰ä¸‹çš„æ€§èƒ½æ™®éä¸‹é™ï¼Œä¸”åœ¨å¤„ç†é«˜å¤æ‚åº¦ç¨‹åºæ—¶è¡¨ç°è™½å¥½ï¼Œä½†æä¾›å½¢å¼åŒ–è¯­ä¹‰å¾€å¾€å¯¹å¤æ‚ä»»åŠ¡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚è¯¥ç ”ç©¶è¡¨æ˜LLMsä½œä¸ºç¨‹åºè§£é‡Šå™¨å…·æœ‰ä¸€å®šå‰æ™¯ï¼Œä½†ç›®å‰ä»ç¼ºä¹é²æ£’çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03415v2",
      "published_date": "2025-10-03 18:23:26 UTC",
      "updated_date": "2025-10-07 03:28:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:03.762104+00:00"
    },
    {
      "arxiv_id": "2510.03413v2",
      "title": "Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science",
      "title_zh": "2025å¹´ä¸‹ä¸€ä»£ç§‘å­¦è®¡ç®—ç”Ÿæ€ç³»ç»Ÿç ”è®¨ä¼šæŠ¥å‘Šï¼šæ±‡èšç¤¾åŒºã€è½¯ä»¶ä¸äººå·¥æ™ºèƒ½ä¹‹åŠ›ï¼Œèµ‹èƒ½è·¨å­¦ç§‘å›¢é˜Ÿç§‘å­¦",
      "authors": [
        "Lois Curfman McInnes",
        "Dorian Arnold",
        "Prasanna Balaprakash",
        "Mike Bernhardt",
        "Beth Cerny",
        "Anshu Dubey",
        "Roscoe Giles",
        "Denice Ward Hood",
        "Mary Ann Leung",
        "Vanessa Lopez-Marrero",
        "Paul Messina",
        "Olivia B. Newton",
        "Chris Oehmen",
        "Stefan M. Wild",
        "Jim Willenbring",
        "Lou Woodley",
        "Tony Baylis",
        "David E. Bernholdt",
        "Chris Camano",
        "Johannah Cohoon",
        "Charles Ferenbaugh",
        "Stephen M. Fiore",
        "Sandra Gesing",
        "Diego Gomez-Zara",
        "James Howison",
        "Tanzima Islam",
        "David Kepczynski",
        "Charles Lively",
        "Harshitha Menon",
        "Bronson Messer",
        "Marieme Ngom",
        "Umesh Paliath",
        "Michael E. Papka",
        "Irene Qualters",
        "Elaine M. Raybourn",
        "Katherine Riley",
        "Paulina Rodriguez",
        "Damian Rouson",
        "Michelle Schwalbe",
        "Sudip K. Seal",
        "Ozge Surer",
        "Valerie Taylor",
        "Lingfei Wu"
      ],
      "abstract": "This report summarizes insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science, which convened more than 40 experts from national laboratories, academia, industry, and community organizations to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems. To address urgent challenges at the intersection of high-performance computing (HPC), AI, and scientific software, participants envisioned agile, robust ecosystems built through socio-technical co-design--the intentional integration of social and technical components as interdependent parts of a unified strategy. This approach combines advances in AI, HPC, and software with new models for cross-disciplinary collaboration, training, and workforce development. Key recommendations include building modular, trustworthy AI-enabled scientific software systems; enabling scientific teams to integrate AI systems into their workflows while preserving human creativity, trust, and scientific rigor; and creating innovative training pipelines that keep pace with rapid technological change. Pilot projects were identified as near-term catalysts, with initial priorities focused on hybrid AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy, responsible AI guidelines, and prototyping of public-private partnerships. This report presents a vision of next-generation ecosystems for scientific computing where AI, software, hardware, and human expertise are interwoven to drive discovery, expand access, strengthen the workforce, and accelerate scientific progress.",
      "tldr_zh": "è¯¥æŠ¥å‘Šæ€»ç»“äº†2025å¹´ç§‘å­¦è®¡ç®—ä¸‹ä¸€ä»£ç”Ÿæ€ç³»ç»Ÿç ”è®¨ä¼š(Next-Generation Ecosystems for Scientific Computing)çš„æˆæœï¼Œæ—¨åœ¨é€šè¿‡ç¤¾åŒºã€è½¯ä»¶å’ŒAIçš„ç»“åˆæ¨åŠ¨è·¨å­¦ç§‘å›¢é˜Ÿç§‘å­¦ã€‚ä¸“å®¶ä»¬æå‡ºäº†é€šè¿‡ç¤¾ä¼šæŠ€æœ¯ååŒè®¾è®¡(socio-technical co-design)æ„å»ºæ•æ·ä¸”ç¨³å¥çš„ç”Ÿæ€ç³»ç»Ÿï¼Œå°†ç¤¾ä¼šå› ç´ ä¸æŠ€æœ¯ç»„ä»¶è§†ä¸ºç»Ÿä¸€æˆ˜ç•¥ä¸­ç›¸äº’ä¾å­˜çš„éƒ¨åˆ†ã€‚æ ¸å¿ƒæ„¿æ™¯æ˜¯å°†äººå·¥æ™ºèƒ½(AI)ã€é«˜æ€§èƒ½è®¡ç®—(HPC)å’Œç§‘å­¦è½¯ä»¶ä¸è·¨å­¦ç§‘åä½œã€åŸ¹è®­åŠåŠ³åŠ¨åŠ›å‘å±•çš„æ–°æ¨¡å¼ç›¸ç»“åˆã€‚å…³é”®å»ºè®®åŒ…æ‹¬æ„å»ºæ¨¡å—åŒ–ä¸”å¯ä¿¡çš„ AI-enabled ç§‘å­¦è½¯ä»¶ç³»ç»Ÿï¼Œå¹¶ç¡®ä¿ç§‘ç ”å›¢é˜Ÿåœ¨å°† AI é›†æˆåˆ°å·¥ä½œæµæ—¶èƒ½ä¿æŒäººç±»çš„åˆ›é€ åŠ›ä¸ç§‘å­¦ä¸¥è°¨æ€§ã€‚æŠ¥å‘Šè¿˜å¼ºè°ƒäº†å»ºç«‹åˆ›æ–°åŸ¹è®­ç®¡é“åŠè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½(responsible AI)å‡†åˆ™ï¼Œå¹¶é€šè¿‡æ··åˆ AI/HPC åŸºç¡€è®¾æ–½ç­‰è¯•ç‚¹é¡¹ç›®ä½œä¸ºè¿‘æœŸå‚¬åŒ–å‰‚ã€‚æœ€ç»ˆç›®æ ‡æ˜¯å®ç° AIã€è½¯ä»¶ã€ç¡¬ä»¶ä¸äººç±»ä¸“ä¸šçŸ¥è¯†çš„æ·±åº¦äº¤ç»‡ï¼Œä»è€ŒåŠ é€Ÿç§‘å­¦å‘ç°å¹¶å¼ºåŒ–ç§‘ç ”åŠ³åŠ¨åŠ›å»ºè®¾ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "38 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03413v2",
      "published_date": "2025-10-03 18:22:47 UTC",
      "updated_date": "2025-10-07 14:08:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:06.353534+00:00"
    },
    {
      "arxiv_id": "2510.03405v1",
      "title": "LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits",
      "title_zh": "LegalSimï¼šç”¨äºå‘ç°ç¨‹åºæ€§æ¼æ´çš„æ³•å¾‹ç³»ç»Ÿå¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿ",
      "authors": [
        "Sanket Badhe"
      ],
      "abstract": "We present LegalSim, a modular multi-agent simulation of adversarial legal proceedings that explores how AI systems can exploit procedural weaknesses in codified rules. Plaintiff and defendant agents choose from a constrained action space (for example, discovery requests, motions, meet-and-confer, sanctions) governed by a JSON rules engine, while a stochastic judge model with calibrated grant rates, cost allocations, and sanction tendencies resolves outcomes. We compare four policies: PPO, a contextual bandit with an LLM, a direct LLM policy, and a hand-crafted heuristic; Instead of optimizing binary case outcomes, agents are trained and evaluated using effective win rate and a composite exploit score that combines opponent-cost inflation, calendar pressure, settlement pressure at low merit, and a rule-compliance margin. Across configurable regimes (e.g., bankruptcy stays, inter partes review, tax procedures) and heterogeneous judges, we observe emergent ``exploit chains'', such as cost-inflating discovery sequences and calendar-pressure tactics that remain procedurally valid yet systemically harmful. Evaluation via cross-play and Bradley-Terry ratings shows, PPO wins more often, the bandit is the most consistently competitive across opponents, the LLM trails them, and the heuristic is weakest. The results are stable in judge settings, and the simulation reveals emergent exploit chains, motivating red-teaming of legal rule systems in addition to model-level testing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LegalSimï¼Œä¸€ç§é’ˆå¯¹å¯¹æŠ—æ€§æ³•å¾‹ç¨‹åºçš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿ(Multi-Agent Simulation)ç³»ç»Ÿï¼Œæ—¨åœ¨æ¢ç´¢äººå·¥æ™ºèƒ½å¦‚ä½•åˆ©ç”¨æˆæ–‡è§„åˆ™ä¸­çš„ç¨‹åºæ€§æ¼æ´(Procedural Exploits)ã€‚åŸå‘Šä¸è¢«å‘Šæ™ºèƒ½ä½“åœ¨ JSON è§„åˆ™å¼•æ“çš„çº¦æŸä¸‹é€‰æ‹©è¯æ®å¼€ç¤º(Discovery)å’ŒåŠ¨è®®ç­‰è¡ŒåŠ¨ï¼Œå¹¶ç”±å…·å¤‡éšæœºè£å†³æ¦‚ç‡å’Œæˆæœ¬åˆ†é…æœºåˆ¶çš„éšæœºæ³•å®˜æ¨¡å‹(Stochastic Judge Model)åˆ¤å®šç»“æœã€‚ç ”ç©¶å¯¹æ¯”äº† PPOã€ç»“åˆ LLM çš„ä¸Šä¸‹æ–‡è€è™æœº(Contextual Bandit)ã€ç›´æ¥ LLM ç­–ç•¥åŠå¯å‘å¼ç®—æ³•ï¼Œå¹¶é‡‡ç”¨å¤åˆæ¼æ´è¯„åˆ†(Composite Exploit Score)è¡¡é‡æˆæœ¬è†¨èƒ€å’Œæ—¥ç¨‹å‹åŠ›ç­‰æŒ‡æ ‡ã€‚å®éªŒè§‚å¯Ÿåˆ°äº†â€œæ¼æ´é“¾â€(Exploit Chains)çš„æ¼”åŒ–ç”Ÿæˆï¼Œå³åˆ©ç”¨åˆè§„ç¨‹åºæ‰‹æ®µå®ç°ç³»ç»Ÿæ€§æŸå®³çš„æˆ˜æœ¯ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤º PPO åœ¨èƒœç‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œè€Œä¸Šä¸‹æ–‡è€è™æœºå±•ç°å‡ºæœ€ç¨³å¥çš„è·¨å¯¹æ‰‹ç«äº‰åŠ›ã€‚LegalSim æ­ç¤ºäº†æ³•å¾‹ç³»ç»Ÿä¸­æ½œåœ¨çš„ç¨‹åºæ€§å¼±ç‚¹ï¼Œä¸ºé™¤æ¨¡å‹çº§æµ‹è¯•å¤–çš„æ³•å¾‹è§„åˆ™ç³»ç»Ÿçº¢é˜Ÿæµ‹è¯•(Red-teaming)æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.MA",
      "comment": "12 pages with 2 figures, accepted at the NLLP workshop at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03405v1",
      "published_date": "2025-10-03 18:01:57 UTC",
      "updated_date": "2025-10-03 18:01:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:18.348783+00:00"
    },
    {
      "arxiv_id": "2510.03399v1",
      "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition",
      "title_zh": "è®¤è¯†ä½ è‡ªå·±ï¼Ÿè®ºäººå·¥æ™ºèƒ½è‡ªæˆ‘è¯†åˆ«èƒ½åŠ›çš„ç¼ºå¤±åŠå…¶å¯ç¤º",
      "authors": [
        "Xiaoyan Bai",
        "Aryan Shrivastava",
        "Ari Holtzman",
        "Chenhao Tan"
      ],
      "abstract": "Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªæˆ‘è¯†åˆ«ï¼ˆself-recognitionï¼‰èƒ½åŠ›ï¼Œè¿™ä¸€å…ƒè®¤çŸ¥èƒ½åŠ›å¯¹ AI å¿ƒç†åˆ†æå’Œå®‰å…¨æ€§è¯„ä¼°è‡³å…³é‡è¦ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ä¸ªç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œæµ‹è¯•äº† 10 ç§å½“ä»£ä¸»æµ LLMs åœ¨è¯†åˆ«è‡ªèº«ç”Ÿæˆæ–‡æœ¬ä¸ä»–è€…æ–‡æœ¬æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹æ™®éç¼ºä¹è‡ªæˆ‘è¯†åˆ«èƒ½åŠ›ï¼Œä»…æœ‰ 4 ä¸ªæ¨¡å‹èƒ½é¢„æµ‹è‡ªå·±ä¸ºç”Ÿæˆè€…ï¼Œä¸”æ•´ä½“è¡¨ç°æå°‘è¶…è¿‡éšæœºæ¦‚ç‡ã€‚æ¨¡å‹åœ¨é¢„æµ‹æ—¶å±•ç°å‡ºå¼ºçƒˆçš„åå‘æ€§ï¼Œå€¾å‘äºå°†é«˜è´¨é‡æ–‡æœ¬é¢„æµ‹ä¸º GPT æˆ– Claude ç³»åˆ—ã€‚å°½ç®¡æ¨¡å‹å±•ç°å‡ºå¯¹è‡ªå·±å’Œä»–äººå­˜åœ¨çš„ä¸€å®šè®¤çŸ¥ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹æ­ç¤ºäº†æ˜æ˜¾çš„ç­‰çº§åè§ï¼ˆhierarchical biasï¼‰ï¼Œé€šå¸¸å°†é«˜è´¨é‡æ–‡æœ¬ä¸é¡¶å±‚æ¨¡å‹æŒ‚é’©ã€‚è¯¥ç ”ç©¶ç»“è®ºå¼ºè°ƒäº†ç›®å‰ AI åœ¨è‡ªæˆ‘æ„è¯†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†è¿™äº›å‘ç°å¯¹ AI Safety çš„æ½œåœ¨å½±å“ä»¥åŠæœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Our code is available, see https://github.com/ChicagoHAI/self-recognition",
      "pdf_url": "https://arxiv.org/pdf/2510.03399v1",
      "published_date": "2025-10-03 18:00:01 UTC",
      "updated_date": "2025-10-03 18:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:38.238919+00:00"
    },
    {
      "arxiv_id": "2510.03231v1",
      "title": "Reward Models are Metrics in a Trench Coat",
      "title_zh": "å¥–åŠ±æ¨¡å‹ï¼šæŠ«ç€é£è¡£çš„è¯„ä¼°æŒ‡æ ‡",
      "authors": [
        "Sebastian Gehrmann"
      ],
      "abstract": "The emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. Reward models assess the quality of sampled model outputs to generate training signals. This task is also performed by evaluation metrics that monitor the performance of an AI model. We find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve data quality, and approaches to meta-evaluation. Our position paper argues that a closer collaboration between the fields can help overcome these issues. To that end, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to multiple research topics in which closer alignment can improve reward models and metrics in areas such as preference elicitation methods, avoidance of spurious correlations and reward hacking, and calibration-aware meta-evaluation.",
      "tldr_zh": "è¿™ç¯‡ç«‹åœºè®ºæ–‡æå‡ºå¥–åŠ±æ¨¡å‹ (Reward Models) æœ¬è´¨ä¸Šä¸è¯„ä¼°æŒ‡æ ‡ (evaluation metrics) å±äºåŒä¸€èŒƒç•´ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹åæœŸè®­ç»ƒä¸­è¿™ä¸¤ä¸ªé¢†åŸŸå› é•¿æœŸç‹¬ç«‹ç ”ç©¶è€Œå¯¼è‡´çš„æœ¯è¯­å†—ä½™ä¸é‡å¤é™·é˜±ã€‚ç ”ç©¶æŒ‡å‡ºä¸¤è€…åœ¨åº”å¯¹è™šå‡ç›¸å…³æ€§ (spurious correlations)ã€å¥–åŠ±é»‘å®¢è¡Œä¸º (reward hacking)ã€æ•°æ®è´¨é‡æå‡ä»¥åŠå…ƒè¯„ä¼° (meta-evaluation) ç­‰å…³é”®æŠ€æœ¯ç“¶é¢ˆæ—¶é¢ä¸´å…±åŒçš„æŒ‘æˆ˜ã€‚ä½œè€…é€šè¿‡å®éªŒå±•ç¤ºäº†ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡åœ¨ç‰¹å®šä»»åŠ¡ä¸Šç”šè‡³èƒ½ä¼˜äºå¥–åŠ±æ¨¡å‹ï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€æä¾›äº†è·¨é¢†åŸŸçš„è¯¦å°½ç»¼è¿°ã€‚è®ºæ–‡å¼ºè°ƒé€šè¿‡åŠ å¼ºä¸¤ä¸ªé¢†åŸŸçš„å¯¹é½åä½œï¼Œå¯ä»¥æœ‰æ•ˆæ”¹å–„åå¥½å¼•å¯¼æ–¹æ³• (preference elicitation methods) å¹¶å¢å¼ºæ¨¡å‹ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶æœ€ç»ˆä¸ºä¼˜åŒ–å¥–åŠ±æ¨¡å‹å’ŒæŒ‡æ ‡åœ¨æ ¡å‡†æ„ŸçŸ¥ (calibration-aware) å…ƒè¯„ä¼°ç­‰æ–¹å‘çš„èåˆæä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03231v1",
      "published_date": "2025-10-03 17:59:44 UTC",
      "updated_date": "2025-10-03 17:59:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:34.363728+00:00"
    },
    {
      "arxiv_id": "2510.03230v1",
      "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
      "title_zh": "é€šè¿‡æ˜¾å¼ä½ç½®-åæ ‡æ˜ å°„æå‡GUIå®šä½èƒ½åŠ›",
      "authors": [
        "Suyuchen Wang",
        "Tianyu Zhang",
        "Ahmed Masry",
        "Christopher Pal",
        "Spandana Gella",
        "Bang Liu",
        "Perouz Taslakian"
      ],
      "abstract": "GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢å®šä½(GUI Grounding)ä»»åŠ¡ä¸­è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å±å¹•æ—¶ patch-to-pixel æ˜ å°„ä¸å¯é çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ˜¾å¼çš„ç©ºé—´æ˜ å°„æ–¹æ¡ˆã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–æ¨¡å‹ä»è§†è§‰ç‰¹å¾ä¸­éšå¼æ¨æ–­åæ ‡ï¼Œå¯¼è‡´å…¶åœ¨é¢å¯¹è®­ç»ƒä¸­æœªè§çš„åˆ†è¾¨ç‡æ—¶å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº† RULER tokens ä½œä¸ºæ˜¾å¼åæ ‡æ ‡è®°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåƒå‚è€ƒåœ°å›¾æ ¼çº¿ä¸€æ ·å¼•ç”¨å¹¶è°ƒæ•´ä½ç½®ï¼ŒåŒæ—¶æå‡ºäº†äº¤é”™å¼å¤šç»´æ—‹è½¬ä½ç½®ç¼–ç (Interleaved MRoPE)ä»¥ä¼˜åŒ–ç©ºé—´ç»´åº¦çš„å¯¹ç§°è¡¨ç¤ºã€‚å®éªŒåœ¨ ScreenSpotã€ScreenSpot-V2 å’Œ ScreenSpot-Pro ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šä½ç²¾åº¦ä¸Šå–å¾—äº†æŒç»­æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡ç•Œé¢ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚é€šè¿‡å°†éšå¼å­¦ä¹ è½¬å˜ä¸ºæ˜¾å¼ç©ºé—´å¼•å¯¼ï¼Œè¯¥ç ”ç©¶ä¸ºå®ç°è·¨å¤šå¹³å°ã€å¤šåˆ†è¾¨ç‡çš„å¯é  GUI è‡ªåŠ¨åŒ–ä»£ç†å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03230v1",
      "published_date": "2025-10-03 17:59:34 UTC",
      "updated_date": "2025-10-03 17:59:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:36.751240+00:00"
    },
    {
      "arxiv_id": "2510.03224v1",
      "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles",
      "title_zh": "åŸºäºæ½œåœ¨é›†æˆéšæœºå…±æŒ¯çš„å¯¹æŠ—æ”»å‡»æµ‹è¯•æ—¶é˜²å¾¡",
      "authors": [
        "Dong Lao",
        "Yuxiang Zhang",
        "Haniyeh Ehsani Oskouie",
        "Yangchao Wu",
        "Alex Wong",
        "Stefano Soatto"
      ],
      "abstract": "We propose a test-time defense mechanism against adversarial attacks: imperceptible image perturbations that significantly alter the predictions of a model. Unlike existing methods that rely on feature filtering or smoothing, which can lead to information loss, we propose to \"combat noise with noise\" by leveraging stochastic resonance to enhance robustness while minimizing information loss. Our approach introduces small translational perturbations to the input image, aligns the transformed feature embeddings, and aggregates them before mapping back to the original reference image. This can be expressed in a closed-form formula, which can be deployed on diverse existing network architectures without introducing additional network modules or fine-tuning for specific attack types. The resulting method is entirely training-free, architecture-agnostic, and attack-agnostic. Empirical results show state-of-the-art robustness on image classification and, for the first time, establish a generic test-time defense for dense prediction tasks, including stereo matching and optical flow, highlighting the method's versatility and practicality. Specifically, relative to clean (unperturbed) performance, our method recovers up to 68.1% of the accuracy loss on image classification, 71.9% on stereo matching, and 29.2% on optical flow under various types of adversarial attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å¯¹æŠ—æ”»å‡»(Adversarial Attacks)çš„æµ‹è¯•æ—¶é˜²å¾¡(Test-Time Defense)æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç‰¹å¾è¿‡æ»¤æˆ–å¹³æ»‘æ–¹æ³•å¸¦æ¥çš„ä¿¡æ¯æŸå¤±é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨éšæœºå…±æŒ¯(Stochastic Resonance)æŠ€æœ¯å®ç°â€œä»¥å™ªåˆ¶å™ªâ€ï¼Œåœ¨å¢å¼ºæ¨¡å‹ç¨³å¥æ€§çš„åŒæ—¶æœ€å°åŒ–ä¿¡æ¯æŸè€—ã€‚å…·ä½“æ“ä½œä¸­ï¼Œè¯¥æ–¹æ³•å¯¹è¾“å…¥å›¾åƒå¼•å…¥å¾®å°å¹³ç§»æ‰°åŠ¨ï¼Œå¯¹å˜æ¢åçš„ç‰¹å¾åµŒå…¥(Feature Embeddings)è¿›è¡Œå¯¹é½ä¸èšåˆï¼Œå¹¶æœ€ç»ˆæ˜ å°„å›åŸå§‹å‚è€ƒå›¾åƒã€‚è¯¥æ–¹æ¡ˆå…·æœ‰è§£æå½¢å¼(Closed-form formula)ï¼Œè¡¨ç°å‡ºæ— éœ€è®­ç»ƒã€æ¶æ„æ— å…³(Architecture-agnostic)ä¸”æ”»å‡»æ— å…³(Attack-agnostic)çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»ä»¥åŠç«‹ä½“åŒ¹é…(Stereo Matching)å’Œå…‰æµ(Optical Flow)ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„é˜²å¾¡æ•ˆæœã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåœ¨å„ç§å¯¹æŠ—æ”»å‡»ä¸‹åˆ†åˆ«æ¢å¤äº†é«˜è¾¾68.1%çš„åˆ†ç±»å‡†ç¡®åº¦æŸå¤±ã€71.9%çš„ç«‹ä½“åŒ¹é…ç²¾åº¦æŸå¤±å’Œ29.2%çš„å…‰æµæŸå¤±ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨ä¸åŒè§†è§‰ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03224v1",
      "published_date": "2025-10-03 17:57:25 UTC",
      "updated_date": "2025-10-03 17:57:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:40.255733+00:00"
    },
    {
      "arxiv_id": "2510.03223v1",
      "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment",
      "title_zh": "Self-Anchorï¼šåŸºäºé€æ­¥æ³¨æ„åŠ›å¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†",
      "authors": [
        "Hongxiang Zhang",
        "Yuan Tian",
        "Tianyi Zhang"
      ],
      "abstract": "To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors. In this paper, we propose Self-Anchor, a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories into structured plans and automatically aligns the model's attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows that Self-Anchor outperforms SOTA prompting methods across six benchmarks. Notably, Self-Anchor significantly reduces the performance gap between ``non-reasoning'' models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä¸­å› æ¨ç†é“¾è¿‡é•¿å¯¼è‡´å…³é”®ä¸­é—´æ­¥éª¤è¢«æ·¹æ²¡çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Self-Anchor çš„æ–°å‹æ¡†æ¶ã€‚Self-Anchor åˆ©ç”¨æ¨ç†çš„å›ºæœ‰ç»“æ„æ¥å¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›ï¼Œé€šè¿‡å°†æ¨ç†è½¨è¿¹åˆ†è§£ä¸ºç»“æ„åŒ–è®¡åˆ’ï¼Œè‡ªåŠ¨å®ç°æ­¥éª¤çº§çš„æ³¨æ„åŠ›å¯¹é½ï¼ˆStep-by-step Attention Alignmentï¼‰ã€‚è¿™ç§æœºåˆ¶ç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½æŒç»­èšç„¦äºæœ€ç›¸å…³çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘æ¨ç†é”™è¯¯ã€‚å®éªŒè¡¨æ˜ï¼ŒSelf-Anchor åœ¨å…­é¡¹åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„ SOTA æç¤ºæ–¹æ³•ã€‚è¯¥ç ”ç©¶çš„ä¸€ä¸ªé‡è¦å‘ç°æ˜¯ï¼ŒSelf-Anchor æ˜¾è‘—ç¼©å°äº†éæ¨ç†æ¨¡å‹ä¸ä¸“ä¸šæ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¿™è¡¨æ˜å¤§å¤šæ•° LLMs éƒ½æœ‰æ½œåŠ›åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è¯¥æ–¹æ³•å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03223v1",
      "published_date": "2025-10-03 17:56:33 UTC",
      "updated_date": "2025-10-03 17:56:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:46.149558+00:00"
    },
    {
      "arxiv_id": "2510.03217v1",
      "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair",
      "title_zh": "å¼ƒæƒä¸éªŒè¯ï¼šä¸€ç§æ—¨åœ¨é™ä½æ™ºèƒ½ä½“ç¨‹åºä¿®å¤å™ªå£°çš„åŒ LLM ç­–ç•¥",
      "authors": [
        "JosÃ© Cambronero",
        "Michele Tufano",
        "Sherry Shi",
        "Renyao Wei",
        "Grant Uy",
        "Runxiang Cheng",
        "Chin-Jung Liu",
        "Shiying Pan",
        "Satish Chandra",
        "Pat Rondon"
      ],
      "abstract": "Agentic Automated Program Repair (APR) is increasingly tackling complex, repository-level bugs in industry, but ultimately agent-generated patches still need to be reviewed by a human before committing them to ensure they address the bug. Showing unlikely patches to developers can lead to substantial noise, wasting valuable developer time and eroding trust in automated code changes. We introduce two complementary LLM-based policies to reduce such noise: bug abstention and patch validation policies. Bug abstention excludes bugs that the agentic APR system is unlikely to fix. Patch validation rejects patches that are unlikely to be a good fix for the given bug. We evaluate both policies on three sets of bugs from Google's codebase, and their candidate patches generated by an internal agentic APR system. On a set of 174 human-reported bugs, removing bugs and patch trajectories rejected by our policies can raise success rates by up to 13 percentage points and 15 percentage points, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with machine-generated bug reports, patch validation also improves average single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of agentic APR systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»£ç†å¼è‡ªåŠ¨ç¨‹åºä¿®å¤(Agentic Automated Program Repair, APR)åœ¨å¤„ç†å·¥ä¸šçº§ä»“åº“æ¼æ´æ—¶äº§ç”Ÿçš„ä½è´¨é‡è¡¥ä¸å™ªéŸ³é—®é¢˜ï¼Œæå‡ºäº†å‡å°‘å¼€å‘è€…å®¡æ ¸è´Ÿæ‹…çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚ç ”ç©¶è€…å¼•å…¥äº†ä¸¤ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„äº’è¡¥ç­–ç•¥ï¼šæ¼æ´å¼ƒæƒ(Bug Abstention)ç­–ç•¥ç”¨äºæ’é™¤ç³»ç»Ÿéš¾ä»¥ä¿®å¤çš„æ¼æ´ï¼Œè¡¥ä¸éªŒè¯(Patch Validation)ç­–ç•¥åˆ™ç”¨äºæ‹’ç»ä¸åˆæ ¼çš„ä¿®å¤è¡¥ä¸ã€‚é€šè¿‡åœ¨Googleå†…éƒ¨ä»£ç åº“çš„å¤šç§æ¼æ´é›†ï¼ˆåŒ…æ‹¬äººä¸ºæŠ¥å‘Šæ¼æ´ã€ç©ºæŒ‡é’ˆå¼‚å¸¸ç­‰ï¼‰ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå®éªŒè¡¨æ˜è¿™ä¸¤ç§ç­–ç•¥çš„ç»“åˆèƒ½å°†äººä¸ºæŠ¥å‘Šæ¼æ´çš„ä¿®å¤æˆåŠŸç‡æœ€é«˜æå‡39ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æœºå™¨ç”Ÿæˆçš„æ¼æ´æŠ¥å‘Šä¸Šä¹Ÿæ˜¾è‘—æé«˜äº†å•æ ·æœ¬ä¿®å¤çš„æˆåŠŸç‡ã€‚è¿™ç§åŒç­–ç•¥æ¡†æ¶ä¸ºå®ç°å¯é çš„ã€å·¥ä¸šçº§è§„æ¨¡çš„ä»£ç†å¼APRç³»ç»Ÿéƒ¨ç½²æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03217v1",
      "published_date": "2025-10-03 17:53:28 UTC",
      "updated_date": "2025-10-03 17:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:45.612108+00:00"
    },
    {
      "arxiv_id": "2510.03216v1",
      "title": "Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation",
      "title_zh": "Wave-GMSï¼šç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„è½»é‡çº§å¤šå°ºåº¦ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Talha Ahmed",
        "Nehal Ahmed Shaikh",
        "Hassan Mohy-ud-Din"
      ],
      "abstract": "For equitable deployment of AI tools in hospitals and healthcare facilities, we need Deep Segmentation Networks that offer high performance and can be trained on cost-effective GPUs with limited memory and large batch sizes. In this work, we propose Wave-GMS, a lightweight and efficient multi-scale generative model for medical image segmentation. Wave-GMS has a substantially smaller number of trainable parameters, does not require loading memory-intensive pretrained vision foundation models, and supports training with large batch sizes on GPUs with limited memory. We conducted extensive experiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument, and HAM10000), demonstrating that Wave-GMS achieves state-of-the-art segmentation performance with superior cross-domain generalizability, while requiring only ~2.6M trainable parameters. Code is available at https://github.com/ATPLab-LUMS/Wave-GMS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Wave-GMSï¼Œä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation)çš„è½»é‡çº§ä¸”é«˜æ•ˆçš„å¤šå°ºåº¦ç”Ÿæˆæ¨¡å‹(Multi-Scale Generative Model)ã€‚è¯¥æ¨¡å‹æ—¨åœ¨æ»¡è¶³åŒ»ç–—æœºæ„å¯¹é«˜æ€§èƒ½æ·±åº¦åˆ†å‰²ç½‘ç»œ(Deep Segmentation Networks)çš„éœ€æ±‚ï¼ŒåŒæ—¶ç¡®ä¿å…¶èƒ½åœ¨æ˜¾å­˜æœ‰é™çš„ä½æˆæœ¬ GPU ä¸Šä»¥å¤§æ‰¹é‡(Large Batch Sizes)è¿›è¡Œè®­ç»ƒã€‚Wave-GMS çš„å¯è®­ç»ƒå‚æ•°é‡ä»…çº¦ 2.6Mï¼Œä¸”æ— éœ€åŠ è½½å†…å­˜å¯†é›†å‹çš„é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹(Pretrained Vision Foundation Models)ã€‚é€šè¿‡åœ¨ BUSã€BUSIã€Kvasir-Instrument å’Œ HAM10000 å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯¥æ¨¡å‹è¯æ˜äº†å…¶åœ¨å®ç°æœ€å…ˆè¿›(State-of-the-art)åˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œå…·æœ‰å“è¶Šçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›(Cross-domain Generalizability)ã€‚è¯¥ç ”ç©¶æˆæœä¸ºåœ¨èµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒä¸­å®ç° AI å·¥å…·çš„å…¬å¹³éƒ¨ç½²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "5 pages, 1 figure, 4 tables; Submitted to IEEE Conference for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2510.03216v1",
      "published_date": "2025-10-03 17:53:16 UTC",
      "updated_date": "2025-10-03 17:53:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:48.649574+00:00"
    },
    {
      "arxiv_id": "2510.03206v1",
      "title": "Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner",
      "title_zh": "ååŒæ¼”åŒ–è¿ç»­ç¦»æ•£æ‰©æ•£ï¼šè®©æ‰©æ•£è¯­è¨€æ¨¡å‹æˆä¸ºéšç©ºé—´æ¨ç†å™¨",
      "authors": [
        "Cai Zhou",
        "Chenxiao Yang",
        "Yi Hu",
        "Chenyu Wang",
        "Chubin Zhang",
        "Muhan Zhang",
        "Lester Mackey",
        "Tommi Jaakkola",
        "Stephen Bates",
        "Dinghuai Zhang"
      ],
      "abstract": "Diffusion language models, especially masked discrete diffusion models, have achieved great success recently. While there are some theoretical and primary empirical results showing the advantages of latent reasoning with looped transformers or continuous chain-of-thoughts, continuous diffusion models typically underperform their discrete counterparts. In this paper, we argue that diffusion language models do not necessarily need to be in the discrete space. In particular, we prove that continuous diffusion models have stronger expressivity than discrete diffusions and looped transformers. We attribute the contradiction between the theoretical expressiveness and empirical performance to their practical trainability: while continuous diffusion provides intermediate supervision that looped transformers lack, they introduce additional difficulty decoding tokens into the discrete token space from the continuous representation space. We therefore propose Coevolutionary Continuous Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process on the union of a continuous representation space and a discrete token space, leveraging a single model to simultaneously denoise in the joint space. By combining two modalities, CCDD is expressive with rich semantics in the latent space, as well as good trainability and sample quality with the help of explicit discrete tokens. We also propose effective architectures and advanced training/sampling techniques for CCDD, which reveals strong empirical performance in extensive language modeling experiments on real-world tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹(Diffusion language models)ä¸­è¿ç»­æ‰©æ•£æ¨¡å‹ç†è®ºè¡¨è¾¾èƒ½åŠ›å¼ºä½†å®é™…æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ååŒè¿›åŒ–è¿ç»­ç¦»æ•£æ‰©æ•£(Coevolutionary Continuous Discrete Diffusion, CCDD)æ¡†æ¶ã€‚ä½œè€…é€šè¿‡ç†è®ºè¯æ˜ï¼Œè¿ç»­æ‰©æ•£æ¨¡å‹åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šä¼˜äºç¦»æ•£æ‰©æ•£å’Œå¾ªç¯å˜æ¢å™¨(looped transformers)ï¼Œå¹¶æŒ‡å‡ºå…¶æ€§èƒ½ç“¶é¢ˆåœ¨äºè¿ç»­è¡¨ç¤ºå‘ç¦»æ•£è¯å…ƒ(tokens)è§£ç æ—¶çš„è®­ç»ƒéš¾åº¦ã€‚CCDDé€šè¿‡åœ¨è¿ç»­è¡¨ç¤ºç©ºé—´å’Œç¦»æ•£è¯å…ƒç©ºé—´çš„å¹¶é›†ä¸Šå®šä¹‰è”åˆå¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹ï¼Œåˆ©ç”¨å•ä¸€æ¨¡å‹åœ¨è”åˆç©ºé—´å†…åŒæ­¥å»å™ªï¼Œä»è€Œèåˆäº†æ½œåœ¨ç©ºé—´(latent space)çš„ä¸°å¯Œè¯­ä¹‰ä¸ç¦»æ•£è¯å…ƒå¸¦æ¥çš„å¯è®­ç»ƒæ€§ä¸é‡‡æ ·è´¨é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é…å¥—å¼€å‘äº†é«˜æ•ˆçš„æ¶æ„ä¸å…ˆè¿›çš„è®­ç»ƒé‡‡æ ·æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCCDDåœ¨å¤šé¡¹ç°å®ä¸–ç•Œçš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºåŠ²çš„å®è¯æ€§èƒ½ï¼ŒæˆåŠŸè¯æ˜äº†å°†æ‰©æ•£è¯­è¨€æ¨¡å‹ä½œä¸ºæ½œå±‚æ¨ç†å™¨(latent reasoner)çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03206v1",
      "published_date": "2025-10-03 17:44:41 UTC",
      "updated_date": "2025-10-03 17:44:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:57.850808+00:00"
    },
    {
      "arxiv_id": "2510.06249v5",
      "title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B",
      "title_zh": "TRepLiNaï¼šé€å±‚ CKA+REPINA å¯¹é½æå‡ Aya-23 8B ä½èµ„æºæœºå™¨ç¿»è¯‘æ€§èƒ½",
      "authors": [
        "Toshiki Nakai",
        "Ravi Kiran Chikkala",
        "Lena Sophie Oberkircher",
        "Nicholas Jennings",
        "Natalia Skachkova",
        "Tatiana Anikina",
        "Jesujoba Oluwadara Alabi"
      ],
      "abstract": "The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India's most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åº¦ä½èµ„æºè¯­è¨€ï¼ˆLRLï¼‰ç¿»è¯‘èµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºTRepLiNaçš„å±‚çº§å¯¹é½æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†Centered Kernel Alignment (CKA)ç›¸ä¼¼åº¦æŒ‡æ ‡ä¸REPINAæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åœ¨decoder-onlyå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰¹å®šä¸­é—´å±‚å¼ºåˆ¶æ‰§è¡Œè·¨è¯­è¨€è¡¨å¾å¯¹é½ï¼Œä»è€Œä¼˜åŒ–ä»LRLåˆ°é«˜èµ„æºè¯­è¨€ï¼ˆHRLï¼‰çš„ç¿»è¯‘æ•ˆæœã€‚ç ”ç©¶äººå‘˜åœ¨Aya-23 8Bæ¨¡å‹ä¸Šåˆ©ç”¨QLoRAæŠ€æœ¯ï¼Œé’ˆå¯¹Mundariã€Santaliå’ŒBhiliç­‰è¯­è¨€å¯¹è¿›è¡Œäº†zero-shotã€few-shotåŠfine-tuningå®éªŒã€‚å®éªŒç»“æœè¯æ˜ï¼Œå¯¹æ¨¡å‹ä¸­é—´å±‚è¿›è¡ŒTRepLiNaå¯¹é½æ˜¯ä¸€ç§ä½æˆæœ¬ä¸”é«˜åº¦å®ç”¨çš„æ”¹è¿›æ‰‹æ®µï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„ä½èµ„æºèƒŒæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶ä¸ºå¤šè¯­è¨€LLMåœ¨ä½èµ„æºè¯­å¢ƒä¸‹çš„ç¿»è¯‘æ€§èƒ½æå‡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "It is work in progress",
      "pdf_url": "https://arxiv.org/pdf/2510.06249v5",
      "published_date": "2025-10-03 17:36:12 UTC",
      "updated_date": "2025-12-10 18:15:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:05.454355+00:00"
    },
    {
      "arxiv_id": "2510.03194v1",
      "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
      "title_zh": "CoDAï¼šé¢å‘åä½œå¼æ•°æ®å¯è§†åŒ–çš„æ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Zichen Chen",
        "Jiefeng Chen",
        "Sercan Ã–. Arik",
        "Misha Sra",
        "Tomas Pfister",
        "Jinsung Yoon"
      ],
      "abstract": "Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. In this paper, we reframe this challenge as a collaborative multi-agent problem. We introduce CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. We formalize this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoDAï¼Œä¸€ç§æ—¨åœ¨å®ç°åä½œæ•°æ®å¯è§†åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent system)ï¼Œä»¥è§£å†³æ•°æ®ç§‘å­¦å®¶åœ¨æ‰‹åŠ¨åˆ¶ä½œå¯è§†åŒ–æ—¶é¢ä¸´çš„æ•ˆç‡ç“¶é¢ˆä»¥åŠç°æœ‰ç³»ç»Ÿå¤„ç†å¤æ‚æ•°æ®é›†èƒ½åŠ›çš„ä¸è¶³ã€‚CoDAé€šè¿‡éƒ¨ç½²ä¸“é—¨çš„LLMæ™ºèƒ½ä½“åˆ†åˆ«è´Ÿè´£å…ƒæ•°æ®åˆ†æ(metadata analysis)ã€ä»»åŠ¡è§„åˆ’(task planning)ã€ä»£ç ç”Ÿæˆ(code generation)å’Œè‡ªæˆ‘åæ€(self-reflection)ï¼Œæ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„åä½œå·¥ä½œæµã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä»¥å…ƒæ•°æ®ä¸ºä¸­å¿ƒçš„åˆ†ææ–¹æ³•æœ‰æ•ˆç»•è¿‡äº†Tokené™åˆ¶ï¼Œå¹¶é€šè¿‡è´¨é‡é©±åŠ¨çš„è¿­ä»£å®Œå–„æœºåˆ¶ç¡®ä¿äº†ç”Ÿæˆç»“æœçš„é²æ£’æ€§ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒCoDAåœ¨ç»¼åˆæ€§èƒ½æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œæå‡å¹…åº¦æœ€é«˜è¾¾41.5%ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¯è§†åŒ–è‡ªåŠ¨åŒ–çš„å…³é”®åœ¨äºé›†æˆåŒ–çš„åä½œæ™ºèƒ½ä½“å·¥ä½œæµï¼Œè€Œéå•çº¯çš„å­¤ç«‹ä»£ç ç”Ÿæˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03194v1",
      "published_date": "2025-10-03 17:30:16 UTC",
      "updated_date": "2025-10-03 17:30:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:31:58.455141+00:00"
    },
    {
      "arxiv_id": "2510.03182v1",
      "title": "Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning",
      "title_zh": "ä»æ¨¡æ‹Ÿåˆ°è§„åˆ™ï¼šé¢å‘å½¢å¼åŒ–è§†è§‰è§„åˆ’çš„åŒ VLM æ¡†æ¶",
      "authors": [
        "Yilun Hao",
        "Yongchao Chen",
        "Chuchu Fan",
        "Yang Zhang"
      ],
      "abstract": "Vision Language Models (VLMs) show strong potential for visual planning but struggle with precise spatial and long-horizon reasoning. In contrast, Planning Domain Definition Language (PDDL) planners excel at long-horizon formal planning, but cannot interpret visual inputs. Recent works combine these complementary advantages by enabling VLMs to turn visual planning problems into PDDL files for formal planning. However, while VLMs can generate PDDL problem files satisfactorily, they struggle to accurately generate the PDDL domain files, which describe all the planning rules. As a result, prior methods rely on human experts to predefine domain files or on constant environment access for refinement. We propose VLMFP, a Dual-VLM-guided framework that can autonomously generate both PDDL problem and domain files for formal visual planning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A SimVLM that simulates action consequences based on input rule descriptions, and a GenVLM that generates and iteratively refines PDDL files by comparing the PDDL and SimVLM execution results. VLMFP unleashes multiple levels of generalizability: The same generated PDDL domain file works for all the different instances under the same problem, and VLMs generalize to different problems with varied appearances and rules. We evaluate VLMFP with 6 grid-world domains and test its generalization to unseen instances, appearance, and game rules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios, simulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal reaching for seen and unseen appearances, respectively. With the guidance of SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for unseen instances in seen and unseen appearances, respectively. Project page: https://sites.google.com/view/vlmfp.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç²¾ç¡®ç©ºé—´å’Œé•¿ç¨‹æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œä»¥åŠå…¶åœ¨ç”Ÿæˆè§„åˆ’åŸŸå®šä¹‰è¯­è¨€(PDDL)åŸŸæ–‡ä»¶æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†åŒVLMå¼•å¯¼çš„æ¡†æ¶VLMFPã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è´Ÿè´£æ¨¡æ‹ŸåŠ¨ä½œåæœçš„SimVLMå’Œè´Ÿè´£è¿­ä»£ä¼˜åŒ–PDDLæ–‡ä»¶çš„GenVLMï¼Œå®ç°äº†PDDLé—®é¢˜å’ŒåŸŸæ–‡ä»¶çš„è‡ªä¸»ç”Ÿæˆï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹äººç±»ä¸“å®¶é¢„å®šä¹‰è§„åˆ™æˆ–æŒç»­ç¯å¢ƒè®¿é—®çš„ä¾èµ–ã€‚VLMFPå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå…¶ç”Ÿæˆçš„PDDLåŸŸæ–‡ä»¶å¯é€‚ç”¨äºåŒç±»é—®é¢˜çš„ä¸åŒå®ä¾‹ï¼Œå¹¶èƒ½é€‚åº”å¤šæ ·çš„è§†è§‰å¤–è§‚å’Œè§„åˆ™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSimVLMåœ¨åŠ¨ä½œæ¨¡æ‹Ÿå’Œç›®æ ‡åˆ¤å®šæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½¿VLMFPåœ¨æœªè§å¤–è§‚çš„å®ä¾‹ä¸­è¾¾åˆ°äº†54.1%çš„æœ‰æ•ˆè§„åˆ’ç‡ã€‚è¿™é¡¹ç ”ç©¶é€šè¿‡ç»“åˆå¤§æ¨¡å‹çš„æ¨¡æ‹Ÿæ¨ç†èƒ½åŠ›ä¸å½¢å¼åŒ–è§„åˆ’å™¨çš„é€»è¾‘ä¸¥è°¨æ€§ï¼Œä¸ºå®ç°å®Œå…¨è‡ªä¸»çš„å½¢å¼åŒ–è§†è§‰è§„åˆ’æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.SC"
      ],
      "primary_category": "cs.RO",
      "comment": "30 pages, 5 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03182v1",
      "published_date": "2025-10-03 16:57:01 UTC",
      "updated_date": "2025-10-03 16:57:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:25.052183+00:00"
    },
    {
      "arxiv_id": "2510.03384v1",
      "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks",
      "title_zh": "äººç±»ä¸å¤§è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡Œä¸»è§‚æ—¥å¸¸ä»»åŠ¡ä¸­ä½“ç°çš„éšæ€§ä»·å€¼è§‚",
      "authors": [
        "Arjun Arunasalam",
        "Madison Pickering",
        "Z. Berkay Celik",
        "Blase Ur"
      ],
      "abstract": "Large language models (LLMs) can underpin AI assistants that help users with everyday tasks, such as by making recommendations or performing basic computation. Despite AI assistants' promise, little is known about the implicit values these assistants display while completing subjective everyday tasks. Humans may consider values like environmentalism, charity, and diversity. To what extent do LLMs exhibit these values in completing everyday tasks? How do they compare with humans? We answer these questions by auditing how six popular LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human crowdworkers from the US. We find LLMs often do not align with humans, nor with other LLMs, in the implicit values exhibited.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨æ‰§è¡Œä¸»è§‚æ—¥å¸¸ä»»åŠ¡æ—¶æ‰€è¡¨ç°å‡ºçš„éšå«ä»·å€¼ï¼Œæ—¨åœ¨æ­ç¤ºAIåŠ©æ‰‹åœ¨æä¾›å»ºè®®æˆ–è¿›è¡ŒåŸºç¡€è®¡ç®—æ—¶çš„ä»·å€¼è§‚å€¾å‘ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å®¡è®¡å…­ç§ä¸»æµLLMsï¼Œå¹¶å°†å…¶ä¸100åç¾å›½ä¼—åŒ…äººå‘˜åœ¨30é¡¹æ—¥å¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œå¯¹æ¯”ï¼Œé‡ç‚¹åˆ†æäº†ç¯å¢ƒä¿æŠ¤ä¸»ä¹‰(Environmentalism)ã€æ…ˆå–„(Charity)å’Œå¤šæ ·æ€§(Diversity)ç­‰æ ¸å¿ƒä»·å€¼çš„ä½“ç°ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å±•ç¤ºè¿™äº›éšå«ä»·å€¼æ—¶ï¼Œä¸ä»…ä¸äººç±»çš„é€‰æ‹©å­˜åœ¨æ˜æ˜¾åå·®ï¼Œä¸”ä¸åŒæ¨¡å‹ä¹‹é—´ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„ä¸ä¸€è‡´æ€§ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰LLMsåœ¨å¤„ç†ä¸»è§‚æ—¥å¸¸ä»»åŠ¡æ—¶é¢ä¸´çš„ä»·å€¼å¯¹é½(Value Alignment)æŒ‘æˆ˜ï¼Œä¸ºç†è§£äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç¤¾ä¼šä»·å€¼åè§æä¾›äº†é‡è¦çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03384v1",
      "published_date": "2025-10-03 16:52:23 UTC",
      "updated_date": "2025-10-03 16:52:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:24.447378+00:00"
    },
    {
      "arxiv_id": "2510.03174v1",
      "title": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?",
      "title_zh": "å°†ä¸»é¢˜å»ºæ¨¡è§†ä¸ºé•¿æ–‡æœ¬ç”Ÿæˆï¼šé•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹èƒ½å¦é€šè¿‡é›¶æ ·æœ¬æç¤ºå˜é©ç¥ç»ä¸»é¢˜æ¨¡å‹ï¼Ÿ",
      "authors": [
        "Xuan Xu",
        "Haolun Li",
        "Zhongliang Yang",
        "Beilin Chu",
        "Jia Song",
        "Moxuan Xu",
        "Linna Zhou"
      ],
      "abstract": "Traditional topic models such as neural topic models rely on inference and generation networks to learn latent topic distributions. This paper explores a new paradigm for topic modeling in the era of large language models, framing TM as a long-form generation task whose definition is updated in this paradigm. We propose a simple but practical approach to implement LLM-based topic model tasks out of the box (sample a data subset, generate topics and representative text with our prompt, text assignment with keyword match). We then investigate whether the long-form generation paradigm can beat NTMs via zero-shot prompting. We conduct a systematic comparison between NTMs and LLMs in terms of topic quality and empirically examine the claim that \"a majority of NTMs are outdated.\"",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¶ä»£ä¸‹ä¸»é¢˜å»ºæ¨¡(Topic Modeling, TM)çš„æ–°èŒƒå¼ï¼Œå°†å…¶é‡æ–°å®šä¹‰ä¸ºä¸€ç§é•¿æ–‡æœ¬ç”Ÿæˆ(Long-Form Generation)ä»»åŠ¡ã€‚ä¸åŒäºä¼ ç»Ÿçš„ç¥ç»ä¸»é¢˜æ¨¡å‹(NTMs)ä¾èµ–æ¨ç†å’Œç”Ÿæˆç½‘ç»œå­¦ä¹ æ½œåœ¨ä¸»é¢˜åˆ†å¸ƒï¼Œè¯¥ç ”ç©¶æ·±å…¥è€ƒå¯Ÿäº†é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹(Long-Context LLMs)é€šè¿‡é›¶æ ·æœ¬æç¤º(Zero-Shot Prompting)é©æ–°ä¸»é¢˜å»ºæ¨¡çš„å¯èƒ½æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å¼€ç®±å³ç”¨çš„å®ç”¨æ–¹æ³•ï¼Œé€šè¿‡æŠ½å–æ•°æ®å­é›†ã€åˆ©ç”¨ç‰¹å®šæç¤ºè¯ç”Ÿæˆä¸»é¢˜åŠä»£è¡¨æ€§æ–‡æœ¬ï¼Œå¹¶ç»“åˆå…³é”®è¯åŒ¹é…å®Œæˆæ–‡æœ¬åˆ†é…ã€‚é€šè¿‡åœ¨ä¸»é¢˜è´¨é‡ä¸Šå¯¹ç¥ç»ä¸»é¢˜æ¨¡å‹ä¸å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ€§å¯¹æ¯”ï¼Œè¯¥ç ”ç©¶å®è¯æ£€éªŒäº†â€œå¤§å¤šæ•°ç¥ç»ä¸»é¢˜æ¨¡å‹å·²ç»è¿‡æ—¶â€çš„è®ºæ–­ã€‚è¿™ç§æ–°èŒƒå¼ä¸ºåˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¤„ç†ä¼ ç»Ÿæ–‡æœ¬åˆ†æä»»åŠ¡æä¾›äº†é‡è¦çš„å®è¯ä¾æ®å’Œæ–¹æ³•å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03174v1",
      "published_date": "2025-10-03 16:48:32 UTC",
      "updated_date": "2025-10-03 16:48:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:23.046127+00:00"
    },
    {
      "arxiv_id": "2510.17833v1",
      "title": "Brain-Language Model Alignment: Insights into the Platonic Hypothesis and Intermediate-Layer Advantage",
      "title_zh": "å¤§è„‘ä¸è¯­è¨€æ¨¡å‹å¯¹é½ï¼šå…³äºæŸæ‹‰å›¾å‡è®¾ä¸ä¸­é—´å±‚ä¼˜åŠ¿çš„å¯ç¤º",
      "authors": [
        "Ãngela LÃ³pez-Cardona",
        "SebastiÃ¡n Idesis",
        "Mireia Masias-Bruns",
        "Sergi Abadal",
        "Ioannis Arapakis"
      ],
      "abstract": "Do brains and language models converge toward the same internal representations of the world? Recent years have seen a rise in studies of neural activations and model alignment. In this work, we review 25 fMRI-based studies published between 2023 and 2025 and explicitly confront their findings with two key hypotheses: (i) the Platonic Representation Hypothesis -- that as models scale and improve, they converge to a representation of the real world, and (ii) the Intermediate-Layer Advantage -- that intermediate (mid-depth) layers often encode richer, more generalizable features. Our findings provide converging evidence that models and brains may share abstract representational structures, supporting both hypotheses and motivating further research on brain-model alignment.",
      "tldr_zh": "è¯¥é¡¹å·¥ä½œå›é¡¾äº†2023å¹´è‡³2025å¹´é—´å‘è¡¨çš„25é¡¹åŸºäºfMRIçš„ç ”ç©¶ï¼Œæ—¨åœ¨æ¢è®¨äººç±»å¤§è„‘ä¸è¯­è¨€æ¨¡å‹åœ¨ä¸–ç•Œå†…éƒ¨è¡¨å¾ä¸Šæ˜¯å¦å…·æœ‰æ”¶æ•›æ€§ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”åˆ†ææ˜ç¡®éªŒè¯äº†ä¸¤ä¸ªå…³é”®å‡è®¾ï¼Œå³æ¨¡å‹éšç€è§„æ¨¡å’Œæ€§èƒ½æå‡ä¼šè¶‹å‘äºè¡¨å¾çœŸå®ä¸–ç•Œçš„Platonic Representation Hypothesisï¼Œä»¥åŠä¸­é—´å±‚å¾€å¾€æ¯”æ·±å±‚æˆ–æµ…å±‚ç¼–ç æ›´ä¸°å¯Œã€æ›´é€šç”¨ç‰¹å¾çš„Intermediate-Layer Advantageã€‚ç ”ç©¶ç»“æœä¸ºå¤§è„‘ä¸æ¨¡å‹ä¹‹é—´å…±äº«æŠ½è±¡è¡¨å¾ç»“æ„æä¾›äº†è¶‹åŒçš„è¯æ®ï¼Œæœ‰åŠ›æ”¯æŒäº†ä¸Šè¿°ä¸¤é¡¹å‡è®¾ã€‚è¿™äº›å‘ç°ä¸ä»…æ·±åŒ–äº†å¯¹brain-model alignmentçš„ç†è§£ï¼Œä¹Ÿä¸ºæœªæ¥æ¢ç´¢ç”Ÿç‰©æ™ºèƒ½ä¸äººå·¥æ™ºèƒ½çš„å†…åœ¨å…³è”æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.17833v1",
      "published_date": "2025-10-03 16:33:09 UTC",
      "updated_date": "2025-10-03 16:33:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:26.149829+00:00"
    },
    {
      "arxiv_id": "2510.03161v1",
      "title": "UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization",
      "title_zh": "UniShieldï¼šé¢å‘ç»Ÿä¸€ä¼ªé€ å›¾åƒæ£€æµ‹ä¸å®šä½çš„è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Qing Huang",
        "Zhipei Xu",
        "Xuanyu Zhang",
        "Jian Zhang"
      ],
      "abstract": "With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ä¼ªé€ å›¾åƒæ£€æµ‹æ–¹æ³•åœ¨è·¨é¢†åŸŸæ³›åŒ–å’Œç»Ÿä¸€æ¡†æ¶é€‚é…æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº† UniShieldï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“ï¼ˆmulti-agentï¼‰çš„è‡ªé€‚åº”ç»Ÿä¸€ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°å¯¹å›¾åƒç¯¡æ”¹ã€æ–‡æ¡£ç¯¡æ”¹ã€DeepFake ä»¥åŠ AI ç”Ÿæˆå›¾åƒç­‰å¤šç§ä¼ªé€ ç±»å‹çš„æ£€æµ‹ä¸å®šä½ï¼ˆForgery Image Detection and Localization, FIDLï¼‰ã€‚UniShield åˆ›æ–°æ€§åœ°ç»“åˆäº†æ„ŸçŸ¥æ™ºèƒ½ä½“ï¼ˆperception agentï¼‰ä¸æ£€æµ‹æ™ºèƒ½ä½“ï¼ˆdetection agentï¼‰ï¼Œå‰è€…é€šè¿‡æ™ºèƒ½åˆ†æå›¾åƒç‰¹å¾åŠ¨æ€ç­›é€‰æœ€åˆé€‚çš„æ£€æµ‹æ¨¡å‹ï¼Œåè€…åˆ™æ•´åˆå¤šä¸ªä¸“å®¶æ£€æµ‹å™¨å¹¶è¾“å‡ºå…·æœ‰å¯è§£é‡Šæ€§çš„æŠ¥å‘Šã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒUniShield åœ¨å¤šé¡¹æµ‹è¯•ä¸­å‡è¾¾åˆ°äº† state-of-the-art æ°´å¹³ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç»Ÿä¸€åŒ–æ–¹æ³•åŠç‰¹å®šé¢†åŸŸæ£€æµ‹å™¨ã€‚è¯¥æ¡†æ¶é€šè¿‡å…¶å“è¶Šçš„å®ç”¨æ€§ã€è‡ªé€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºåº”å¯¹åˆæˆå›¾åƒå¸¦æ¥çš„è™šå‡ä¿¡æ¯å’Œæ¬ºè¯ˆé£é™©æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03161v1",
      "published_date": "2025-10-03 16:33:05 UTC",
      "updated_date": "2025-10-03 16:33:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:27.548856+00:00"
    },
    {
      "arxiv_id": "2510.03160v2",
      "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus",
      "title_zh": "SpineBenchï¼šåŸºäº SpineMed-450k è¯­æ–™åº“çš„ä¸´åºŠæ˜¾è‘—ã€æ¤ä½“æ°´å¹³æ„ŸçŸ¥è¯„ä¼°åŸºå‡†",
      "authors": [
        "Ming Zhao",
        "Wenhui Dong",
        "Yang Zhang",
        "Xiang Zheng",
        "Zhonghao Zhang",
        "Zian Zhou",
        "Yunzhi Guan",
        "Liukun Xu",
        "Wei Peng",
        "Zhaoyang Gong",
        "Zhicheng Zhang",
        "Dachuan Li",
        "Xiaosheng Ma",
        "Yuli Ma",
        "Jianing Ni",
        "Changjiang Jiang",
        "Lixia Tian",
        "Qixin Chen",
        "Kaishun Xia",
        "Pingping Liu",
        "Tongshun Zhang",
        "Zhiqiang Liu",
        "Zhongyan Bi",
        "Chenyang Si",
        "Tiansheng Sun",
        "Caifeng Shan"
      ],
      "abstract": "Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our model's outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SpineMedç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è„ŠæŸ±ç–¾ç—…AIè¾…åŠ©è¯Šæ–­ä¸­ç¼ºä¹çº§åˆ«æ„ŸçŸ¥(level-aware)å¤šæ¨¡æ€æ•°æ®é›†å’Œæ ‡å‡†åŒ–åŸºå‡†çš„é—®é¢˜ã€‚è¯¥ç³»ç»ŸåŒ…å«SpineMed-450kï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºæ¤éª¨çº§åˆ«æ¨ç†(vertebral-level reasoning)è®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–è¶…è¿‡45ä¸‡ä¸ªè·¨X-rayã€CTå’ŒMRIçš„æŒ‡ä»¤å®ä¾‹ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸´åºŠåŒ»ç”Ÿå‚ä¸(clinician-in-the-loop)çš„æµæ°´çº¿å’Œä¸¤é˜¶æ®µLLMç”Ÿæˆæ–¹æ³•ï¼Œç¡®ä¿äº†æ•°æ®åœ¨é—®ç­”ã€å¤šè½®å’¨è¯¢å’ŒæŠ¥å‘Šç”Ÿæˆä¸­çš„é«˜è´¨é‡ã€‚åŒæ—¶ï¼Œç ”ç©¶æå‡ºäº†SpineBenchè¯„ä»·æ¡†æ¶ï¼Œä»çº§åˆ«è¯†åˆ«ã€ç—…ç†è¯„ä¼°å’Œæ‰‹æœ¯è§„åˆ’ç­‰ä¸´åºŠæ˜¾è‘—ç»´åº¦å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚å®éªŒå‘ç°ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹(LVLMs)åœ¨ç²¾ç»†åŒ–çº§åˆ«æ¨ç†æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¼±ç‚¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºSpineMed-450kå¾®è°ƒçš„æ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œä¸”ä¸´åºŠåŒ»ç”Ÿè¯„ä¼°è¯å®å…¶è¾“å‡ºå…·æœ‰æé«˜çš„è¯Šæ–­æ¸…æ™°åº¦å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03160v2",
      "published_date": "2025-10-03 16:32:02 UTC",
      "updated_date": "2025-10-25 03:34:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:35.857538+00:00"
    },
    {
      "arxiv_id": "2510.05160v1",
      "title": "Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders",
      "title_zh": "ç”Ÿæˆå¼é€†å‘è®¾è®¡ï¼šåˆ©ç”¨æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨å®ç°ä»å•ç‚¹ä¼˜åŒ–åˆ°å¤šæ ·åŒ–è®¾è®¡æ–¹æ¡ˆç»„åˆçš„è·¨è¶Š",
      "authors": [
        "Muhammad Arif Hakimi Zamrai"
      ],
      "abstract": "Inverse design, which seeks to find optimal parameters for a target output, is a central challenge in engineering. Surrogate-based optimization (SBO) has become a standard approach, yet it is fundamentally structured to converge to a single-point solution, thereby limiting design space exploration and ignoring potentially valuable alternative topologies. This paper presents a paradigm shift from single-point optimization to generative inverse design. We introduce a framework based on a Conditional Variational Autoencoder (CVAE) that learns a probabilistic mapping between a system's design parameters and its performance, enabling the generation of a diverse portfolio of high-performing candidates conditioned on a specific performance objective. We apply this methodology to the complex, non-linear problem of minimizing airfoil self-noise, using a high-performing SBO method from a prior benchmark study as a rigorous baseline. The CVAE framework successfully generated 256 novel designs with a 94.1\\% validity rate. A subsequent surrogate-based evaluation revealed that 77.2\\% of these valid designs achieved superior performance compared to the single optimal design found by the SBO baseline. This work demonstrates that the generative approach not only discovers higher-quality solutions but also provides a rich portfolio of diverse candidates, fundamentally enhancing the engineering design process by enabling multi-criteria decision-making.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä»å•ç‚¹ä¼˜åŒ–å‘ç”Ÿæˆå¼é€†å‘è®¾è®¡(Generative Inverse Design)è½¬å˜çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä»£ç†æ¨¡å‹ä¼˜åŒ–(SBO)ä»…èƒ½æ”¶æ•›äºå•ç‚¹è§£ä¸”ç¼ºä¹å¤šæ ·æ€§çš„å±€é™ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(CVAE)çš„æ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ è®¾è®¡å‚æ•°ä¸æ€§èƒ½ä¹‹é—´çš„æ¦‚ç‡æ˜ å°„ï¼Œå®ç°åœ¨ç‰¹å®šæ€§èƒ½ç›®æ ‡çº¦æŸä¸‹ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜æ€§èƒ½è®¾è®¡æ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ–¹æ³•åº”ç”¨äºå¤æ‚çš„éçº¿æ€§æœºç¿¼è‡ªå™ªå£°(airfoil self-noise)æœ€å°åŒ–é—®é¢˜ï¼Œå¹¶ä»¥é«˜æ€§èƒ½çš„SBOæ–¹æ³•ä½œä¸ºåŸºå‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCVAEæ¡†æ¶æˆåŠŸç”Ÿæˆäº†256ä¸ªæ–°é¢–è®¾è®¡ï¼Œæœ‰æ•ˆç‡è¾¾94.1%ï¼Œä¸”å…¶ä¸­77.2%çš„æœ‰æ•ˆè®¾è®¡åœ¨æ€§èƒ½ä¸Šä¼˜äºSBOåŸºå‡†å¯»å¾—çš„å•ä¸€æœ€ä¼˜è§£ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†ç”Ÿæˆå¼æ–¹æ³•ä¸ä»…èƒ½å‘ç°æ›´é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œè¿˜èƒ½æä¾›ä¸°å¯Œçš„å€™é€‰è®¾è®¡ç»„åˆï¼Œé€šè¿‡æ”¯æŒå¤šå‡†åˆ™å†³ç­–(multi-criteria decision-making)ä»æ ¹æœ¬ä¸Šå¢å¼ºäº†å·¥ç¨‹è®¾è®¡æµç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05160v1",
      "published_date": "2025-10-03 16:28:19 UTC",
      "updated_date": "2025-10-03 16:28:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:37.656330+00:00"
    },
    {
      "arxiv_id": "2510.03155v1",
      "title": "Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches",
      "title_zh": "åŸºäºåˆºæ¿€ç”µå‹çš„åŠ¨ä½œç”µä½èµ·å§‹æ—¶åˆ»é¢„æµ‹ï¼šç»å…¸ä¸é‡å­å¯å‘å¼æ–¹æ³•çš„å¯¹æ¯”",
      "authors": [
        "Stevens Johnson",
        "Varun Puram",
        "Johnson Thomas",
        "Acsah Konuparamban",
        "Ashwin Kannan"
      ],
      "abstract": "Accurate modeling of neuronal action potential (AP) onset timing is crucial for understanding neural coding of danger signals. Traditional leaky integrate-and-fire (LIF) models, while widely used, exhibit high relative error in predicting AP onset latency, especially under strong or rapidly changing stimuli. Inspired by recent experimental findings and quantum theory, we present a quantum-inspired leaky integrate-and-fire (QI-LIF) model that treats AP onset as a probabilistic event, represented by a Gaussian wave packet in time. This approach captures the biological variability and uncertainty inherent in neuronal firing. We systematically compare the relative error of AP onset predictions between the classical LIF and QI-LIF models using synthetic data from hippocampal and sensory neurons subjected to varying stimulus amplitudes. Our results demonstrate that the QI-LIF model significantly reduces prediction error, particularly for high-intensity stimuli, aligning closely with observed biological responses. This work highlights the potential of quantum-inspired computational frameworks in advancing the accuracy of neural modeling and has implications for quantum engineering approaches to brain-inspired computing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ¼ç”µç§¯åˆ†å‘æ”¾(Leaky Integrate-and-Fire, LIF)æ¨¡å‹åœ¨å¼ºåˆºæ¿€ä¸‹é¢„æµ‹åŠ¨ä½œç”µä½(Action Potential, AP)èµ·å§‹æ—¶é—´è¯¯å·®è¾ƒå¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é‡å­å¯å‘æ¼ç”µç§¯åˆ†å‘æ”¾(QI-LIF)æ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†APèµ·å§‹è§†ä¸ºæ¦‚ç‡æ€§äº‹ä»¶ï¼Œå¹¶åˆ©ç”¨æ—¶é—´è½´ä¸Šçš„é«˜æ–¯æ³¢åŒ…(Gaussian wave packet)æ¥è¡¨å¾ç¥ç»å…ƒæ”¾ç”µå›ºæœ‰çš„ç”Ÿç‰©å˜å¼‚æ€§ä¸ä¸ç¡®å®šæ€§ã€‚é€šè¿‡å¯¹æµ·é©¬ä½“å’Œæ„Ÿè§‰ç¥ç»å…ƒåˆæˆæ•°æ®çš„ç³»ç»Ÿå¯¹æ¯”ï¼Œç ”ç©¶å‘ç°QI-LIFæ¨¡å‹åœ¨ä¸åŒåˆºæ¿€å¼ºåº¦ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å¼ºåº¦åˆºæ¿€æ—¶ï¼Œå…¶é¢„æµ‹è¯¯å·®æ˜¾è‘—ä½äºç»å…¸LIFæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜QI-LIFæ¨¡å‹ä¸è§‚å¯Ÿåˆ°çš„ç”Ÿç‰©ååº”é«˜åº¦å»åˆï¼Œè¯æ˜äº†é‡å­å¯å‘è®¡ç®—æ¡†æ¶åœ¨æé«˜ç¥ç»å»ºæ¨¡å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶æˆæœä¸ä»…ä¸ºç¥ç»ç¼–ç ç ”ç©¶æä¾›äº†æ›´ç²¾ç¡®çš„è®¡ç®—å·¥å…·ï¼Œä¹Ÿä¸ºç±»è„‘è®¡ç®—çš„é‡å­å·¥ç¨‹åŒ–è·¯å¾„æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03155v1",
      "published_date": "2025-10-03 16:28:02 UTC",
      "updated_date": "2025-10-03 16:28:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:44.847179+00:00"
    },
    {
      "arxiv_id": "2510.04797v1",
      "title": "DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing",
      "title_zh": "DiT-VTONï¼šé›†æˆå›¾åƒç¼–è¾‘åŠŸèƒ½çš„ç»Ÿä¸€å¤šç±»åˆ«è™šæ‹Ÿè¯•ç©¿ä¸å…¨å“ç±»è™šæ‹Ÿè¯•ç”¨æ‰©æ•£ Transformer æ¡†æ¶",
      "authors": [
        "Qi Li",
        "Shuwen Qiu",
        "Julien Han",
        "Xingzi Xu",
        "Mehmet Saygin Seyfioglu",
        "Kee Kiat Koo",
        "Karim Bouyarmane"
      ],
      "abstract": "The rapid growth of e-commerce has intensified the demand for Virtual Try-On (VTO) technologies, enabling customers to realistically visualize products overlaid on their own images. Despite recent advances, existing VTO models face challenges with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories. In this paper, we present DiT-VTON, a novel VTO framework that leverages a Diffusion Transformer (DiT), renowned for its performance on text-conditioned image generation, adapted here for the image-conditioned VTO task. We systematically explore multiple DiT configurations, including in-context token concatenation, channel concatenation, and ControlNet integration, to determine the best setup for VTO image conditioning.\n  To enhance robustness, we train the model on an expanded dataset encompassing varied backgrounds, unstructured references, and non-garment categories, demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also redefines the VTO task beyond garment try-on, offering a versatile Virtual Try-All (VTA) solution capable of handling a wide range of product categories and supporting advanced image editing functionalities such as pose preservation, localized editing, texture transfer, and object-level customization. Experimental results show that our model surpasses state-of-the-art methods on VITON-HD, achieving superior detail preservation and robustness without reliance on additional condition encoders. It also outperforms models with VTA and image editing capabilities on a diverse dataset spanning thousands of product categories.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiT-VTONï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Diffusion Transformer (DiT) çš„æ–°å‹è™šæ‹Ÿè¯•ç©¿æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Virtual Try-On (VTO) æ¨¡å‹åœ¨ç»†èŠ‚ä¿ç•™ã€é²æ£’æ€§å’Œå¤šç±»åˆ«æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç³»ç»Ÿæ¢ç´¢ token æ‹¼æ¥å’Œ ControlNet ç­‰å¤šç§ DiT é…ç½®ï¼Œè¯¥æ¡†æ¶å®ç°äº†é«˜æ•ˆçš„å›¾åƒæ¡ä»¶ç”Ÿæˆï¼Œå¹¶å°†å…¶åº”ç”¨èŒƒå›´ä»æœè£…æ‰©å±•åˆ°é€šç”¨çš„ Virtual Try-All (VTA) ä»»åŠ¡ã€‚DiT-VTON è¿˜åœ¨å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œå¹¶æ”¯æŒå§¿æ€ä¿æŒã€çº¹ç†è¿ç§»å’Œå±€éƒ¨ç¼–è¾‘ç­‰é«˜çº§å›¾åƒç¼–è¾‘åŠŸèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ VITON-HD åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨æ— éœ€é¢å¤–æ¡ä»¶ç¼–ç å™¨çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„ç»†èŠ‚è¿˜åŸä¸è·¨å“ç±»æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content Creation workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.04797v1",
      "published_date": "2025-10-03 16:27:53 UTC",
      "updated_date": "2025-10-03 16:27:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:44.653101+00:00"
    },
    {
      "arxiv_id": "2510.03153v1",
      "title": "Improving Cooperation in Collaborative Embodied AI",
      "title_zh": "æå‡åä½œå¼å…·èº«æ™ºèƒ½çš„åä½œèƒ½åŠ›",
      "authors": [
        "Hima Jacob Leven Suprabha",
        "Laxmi Nag Laxminarayan Nagesh",
        "Ajith Nair",
        "Alvin Reuben Amal Selvaster",
        "Ayan Khan",
        "Raghuram Damarla",
        "Sanju Hannah Samuel",
        "Sreenithi Saravana Perumal",
        "Titouan Puech",
        "Venkataramireddy Marella",
        "Vishal Sonar",
        "Alessandro Suglia",
        "Oliver Lemon"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨å¦‚ä½•é€šè¿‡å°† Large Language Models (LLMs) é›†æˆåˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œä»¥æå‡ Collaborative Embodied AI é¢†åŸŸçš„åä½œæ¨ç†å’Œåˆä½œèƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜æ”¹è¿›äº† CoELA æ¡†æ¶ï¼Œåˆ©ç”¨ LLMs åœ¨å…±äº«è™šæ‹Ÿç©ºé—´ä¸­å®ç°å¤šæ™ºèƒ½ä½“é€šä¿¡ã€æ¨ç†å’Œä»»åŠ¡åè°ƒã€‚é€šè¿‡å¯¹å¤šç§ LLMs å’Œ Prompt Engineering ç­–ç•¥çš„ç³»ç»Ÿæ€§å®éªŒï¼Œç ”ç©¶ç¡®å®šäº†èƒ½å¤Ÿæœ€å¤§åŒ–åä½œæ€§èƒ½çš„ä¼˜åŒ–ç»„åˆã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é›†æˆäº†è¯­éŸ³åŠŸèƒ½ä»¥å®ç°æ— ç¼çš„è¯­éŸ³åä½œäº¤äº’ï¼Œä¸ºç³»ç»Ÿå¼€å‘å’Œæ¼”ç¤ºæä¾›äº†æ›´å…·å‚ä¸åº¦çš„ç•Œé¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ Prompt ä¼˜åŒ–ï¼Œæœ€ä¼˜ç»„åˆä½¿åŸºäº Gemma 3 çš„ç³»ç»Ÿæ•ˆç‡è¾ƒåŸå§‹ CoELA ç³»ç»Ÿæå‡äº† 22%ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº† Prompt ä¼˜åŒ–å’Œè¯­éŸ³é›†æˆåœ¨å¢å¼ºåä½œæ™ºèƒ½ä½“æ€§èƒ½å’Œæå‡ç”¨æˆ·ä½“éªŒæ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "In proceedings of UKCI 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03153v1",
      "published_date": "2025-10-03 16:25:48 UTC",
      "updated_date": "2025-10-03 16:25:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:45.351543+00:00"
    },
    {
      "arxiv_id": "2510.03129v3",
      "title": "Signature-Informed Transformer for Asset Allocation",
      "title_zh": "ç”¨äºèµ„äº§é…ç½®çš„ç‰¹å¾ç­¾åæ„ŸçŸ¥ Transformer",
      "authors": [
        "Yoontae Hwang",
        "Stefan Zohren"
      ],
      "abstract": "Modern deep learning for asset allocation typically separates forecasting from optimization. We argue this creates a fundamental mismatch where minimizing prediction errors fails to yield robust portfolios. We propose the Signature Informed Transformer to address this by unifying feature extraction and decision making into a single policy. Our model employs path signatures to encode complex path dependencies and introduces a specialized attention mechanism that targets geometric asset relationships. By directly minimizing the Conditional Value at Risk we ensure the training objective aligns with financial goals. We prove that our attention module rigorously amplifies signature derived signals. Experiments across diverse equity universes show our approach significantly outperforms both traditional strategies and advanced forecasting baselines. The code is available at: https://anonymous.4open.science/r/Signature-Informed-Transformer-For-Asset-Allocation-DB88",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Signature-Informed Transformerï¼Œæ—¨åœ¨è§£å†³ç°ä»£æ·±åº¦å­¦ä¹ åœ¨èµ„äº§é…ç½®(Asset Allocation)ä¸­å› é¢„æµ‹ä¸ä¼˜åŒ–åˆ†ç¦»è€Œå¯¼è‡´çš„æŠ•èµ„ç»„åˆä¸ç¨³å¥é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†ç‰¹å¾æå–ä¸å†³ç­–è¿‡ç¨‹ç»Ÿä¸€ä¸ºå•ä¸€ç­–ç•¥ï¼Œåˆ©ç”¨è·¯å¾„ç­¾å(path signatures)ç¼–ç å¤æ‚çš„è·¯å¾„ä¾èµ–æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§é’ˆå¯¹å‡ ä½•èµ„äº§å…³ç³»çš„ä¸“é—¨æ³¨æ„åŠ›æœºåˆ¶(specialized attention mechanism)ã€‚é€šè¿‡ç›´æ¥æœ€å°åŒ–æ¡ä»¶é£é™©ä»·å€¼(Conditional Value at Risk, CVaR)ï¼Œè¯¥æ¡†æ¶ç¡®ä¿äº†è®­ç»ƒç›®æ ‡ä¸è´¢åŠ¡é£é™©ç®¡ç†ç›®æ ‡çš„é«˜åº¦ä¸€è‡´ã€‚ç†è®ºè¯æ˜è¡¨æ˜å…¶æ³¨æ„åŠ›æ¨¡å—èƒ½æœ‰æ•ˆæ”¾å¤§è·¯å¾„ç­¾åè¡ç”Ÿçš„ä¿¡å·ï¼Œä¸”å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªè‚¡ç¥¨æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿç­–ç•¥å’Œå…ˆè¿›çš„é¢„æµ‹åŸºçº¿ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°å¯ç›´æ¥ä¼˜åŒ–é‡‘èç›®æ ‡çš„ç«¯åˆ°ç«¯èµ„äº§ç®¡ç†æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.PM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03129v3",
      "published_date": "2025-10-03 15:58:21 UTC",
      "updated_date": "2026-01-22 08:38:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:57.950170+00:00"
    },
    {
      "arxiv_id": "2510.15930v1",
      "title": "ImplÃ©mentation Efficiente de Fonctions de Convolution sur FPGA Ã  l'Aide de Blocs ParamÃ©trables et d'Approximations Polynomiales",
      "title_zh": "åŸºäºå¯å‚æ•°åŒ–æ¨¡å—ä¸å¤šé¡¹å¼é€¼è¿‘çš„ FPGA å·ç§¯å‡½æ•°é«˜æ•ˆå®ç°",
      "authors": [
        "Philippe MagalhÃ£es",
        "Virginie Fresse",
        "BenoÃ®t Suffran",
        "Olivier Alata"
      ],
      "abstract": "Implementing convolutional neural networks (CNNs) on field-programmable gate arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower latency, greater power efficiency and greater flexibility. However, this development remains complex due to the hardware knowledge required and the long synthesis, placement and routing stages, which slow down design cycles and prevent rapid exploration of network configurations, making resource optimisation under severe constraints particularly challenging. This paper proposes a library of configurable convolution Blocks designed to optimize FPGA implementation and adapt to available resources. It also presents a methodological framework for developing mathematical models that predict FPGA resources utilization. The approach is validated by analyzing the correlation between the parameters, followed by error metrics. The results show that the designed blocks enable adaptation of convolution layers to hardware constraints, and that the models accurately predict resource consumption, providing a useful tool for FPGA selection and optimized CNN deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ FPGA ä¸Šå®ç°å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) æ—¶é¢ä¸´çš„ç¡¬ä»¶çŸ¥è¯†é—¨æ§›é«˜ã€è®¾è®¡å‘¨æœŸé•¿ä»¥åŠèµ„æºä¼˜åŒ–å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€å¥—å¯é…ç½®å·ç§¯å— (configurable convolution Blocks) åº“ï¼Œæ—¨åœ¨ä¼˜åŒ– FPGA å®ç°å¹¶æ ¹æ®å¯ç”¨èµ„æºè¿›è¡Œçµæ´»è°ƒæ•´ã€‚ç ”ç©¶è¿˜å»ºç«‹äº†ä¸€ä¸ªæ–¹æ³•è®ºæ¡†æ¶ï¼Œé€šè¿‡æ•°å­¦æ¨¡å‹ç²¾ç¡®é¢„æµ‹ FPGA çš„èµ„æºåˆ©ç”¨ç‡ (resource utilization)ã€‚é€šè¿‡å¯¹å‚æ•°ç›¸å…³æ€§åŠè¯¯å·®æŒ‡æ ‡çš„æ·±å…¥åˆ†æï¼Œè¯¥æ–¹æ³•å¾—åˆ°äº†æœ‰æ•ˆéªŒè¯ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæ‰€è®¾è®¡çš„æ¨¡å—èƒ½ä½¿å·ç§¯å±‚é«˜æ•ˆé€‚åº”ç¡¬ä»¶çº¦æŸï¼Œä¸”é¢„æµ‹æ¨¡å‹èƒ½å‡†ç¡®ä¼°ç®—èµ„æºæ¶ˆè€—ã€‚è¿™ä¸€æˆæœä¸º FPGA å™¨ä»¶çš„é€‰æ‹©ä»¥åŠ CNN æ¨¡å‹çš„ä¼˜åŒ–éƒ¨ç½²æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AR",
      "comment": "in French language, XXXe Colloque Francophone de Traitement du Signal et des Images (GRETSI), Aug 2025, Strabourg, France",
      "pdf_url": "https://arxiv.org/pdf/2510.15930v1",
      "published_date": "2025-10-03 15:58:20 UTC",
      "updated_date": "2025-10-03 15:58:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:32:58.554528+00:00"
    },
    {
      "arxiv_id": "2510.03127v1",
      "title": "A Study of Rule Omission in Raven's Progressive Matrices",
      "title_zh": "ç‘æ–‡æ¸è¿›çŸ©é˜µä¸­çš„è§„åˆ™ç¼ºå¤±ç ”ç©¶",
      "authors": [
        "Binze Li"
      ],
      "abstract": "Analogical reasoning lies at the core of human cognition and remains a fundamental challenge for artificial intelligence. Raven's Progressive Matrices (RPM) serve as a widely used benchmark to assess abstract reasoning by requiring the inference of underlying structural rules. While many vision-based and language-based models have achieved success on RPM tasks, it remains unclear whether their performance reflects genuine reasoning ability or reliance on statistical shortcuts. This study investigates the generalization capacity of modern AI systems under conditions of incomplete training by deliberately omitting several structural rules during training. Both sequence-to-sequence transformer models and vision-based architectures such as CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN (I-RAVEN) dataset. Experiments reveal that although transformers demonstrate strong performance on familiar rules, their accuracy declines sharply when faced with novel or omitted rules. Moreover, the gap between token-level accuracy and complete answer accuracy highlights fundamental limitations in current approaches. These findings provide new insights into the reasoning mechanisms underlying deep learning models and underscore the need for architectures that move beyond pattern recognition toward robust abstract reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨Raven's Progressive Matrices (RPM) ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨è§„åˆ™ç¼ºå¤±æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶æ—¨åœ¨å˜æ¸…è§†è§‰å’Œè¯­è¨€æ¨¡å‹åœ¨RPMåŸºå‡†ä¸Šçš„æˆåŠŸæ˜¯æºäºçœŸå®çš„Abstract Reasoningï¼Œè¿˜æ˜¯ä»…ä»…ä¾èµ–äºStatistical Shortcutsã€‚å®éªŒé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•…æ„çœç•¥éƒ¨åˆ†ç»“æ„åŒ–è§„åˆ™ï¼Œå¹¶åˆ©ç”¨I-RAVENæ•°æ®é›†å¯¹Sequence-to-Sequence Transformerä»¥åŠCoPINetã€Dual-Contrast Networkç­‰æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡Transformeråœ¨ç†Ÿæ‚‰è§„åˆ™ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢å¯¹æ–°è§„åˆ™æˆ–ç¼ºå¤±è§„åˆ™æ—¶ï¼Œå…¶å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼ŒToken-levelå‡†ç¡®ç‡ä¸å®Œæ•´ç­”æ¡ˆå‡†ç¡®ç‡ä¹‹é—´çš„å·¨å¤§å·®è·æ­ç¤ºäº†ç°æœ‰æ–¹æ³•çš„æ ¹æœ¬å±€é™ã€‚æœ€ç»ˆç»“è®ºæŒ‡å‡ºï¼Œå½“å‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä»ä¸»è¦åœç•™äºPattern Recognitioné˜¶æ®µï¼Œå°šéœ€å¼€å‘èƒ½å¤Ÿå®ç°é²æ£’Abstract Reasoningçš„æ–°å‹æ¶æ„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03127v1",
      "published_date": "2025-10-03 15:53:28 UTC",
      "updated_date": "2025-10-03 15:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:33:59.052278+00:00"
    },
    {
      "arxiv_id": "2510.03122v2",
      "title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion",
      "title_zh": "HAVIRï¼šåŸºäº CLIP å¼•å¯¼çš„ Versatile Diffusion çš„å±‚æ¬¡åŒ–è§†è§‰åˆ°å›¾åƒé‡å»º",
      "authors": [
        "Shiyi Zhang",
        "Dong Liang",
        "Hairong Zheng",
        "Yihang Zhou"
      ],
      "abstract": "The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»å¤§è„‘æ´»åŠ¨ä¸­ç²¾ç¡®æ¢å¤å¤æ‚è§†è§‰åˆºæ¿€çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º HAVIR çš„åˆ†å±‚è§†è§‰åˆ°å›¾åƒé‡å»ºæ¨¡å‹ã€‚å—è§†è§‰çš®å±‚åˆ†å±‚è¡¨å¾ç†è®ºå¯å‘ï¼Œè¯¥æ¨¡å‹å°†è§†è§‰çš®å±‚åˆ’åˆ†ä¸ºä¸¤ä¸ªå±‚çº§åŒºåŸŸä»¥åˆ†åˆ«æå–ç‰¹å¾ï¼Œæ—¨åœ¨è§£å†³è‡ªç„¶åœºæ™¯ä¸­ä½çº§ç‰¹å¾å¼‚è´¨æ€§å’Œé«˜çº§è¯­ä¹‰çº ç¼ çš„é—®é¢˜ã€‚HAVIR é€šè¿‡ Structural Generator ä»ç©ºé—´å¤„ç†ä½“ç´ ä¸­æå–ç»“æ„ä¿¡æ¯å¹¶å°†å…¶è½¬æ¢ä¸º Latent Diffusion Priorsï¼ŒåŒæ—¶åˆ©ç”¨ Semantic Extractor å°†è¯­ä¹‰å¤„ç†ä½“ç´ è½¬æ¢ä¸º CLIP Embeddingsã€‚è¿™äº›æ ¸å¿ƒç»„ä»¶é€šè¿‡ Versatile Diffusion æ¨¡å‹è¿›è¡Œæ•´åˆï¼Œä»è€Œåˆæˆæœ€ç»ˆçš„é«˜è´¨é‡å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHAVIR åœ¨å¤æ‚åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†é‡å»ºå›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰è´¨é‡ï¼Œä¸”åœ¨æ€§èƒ½è¡¨ç°ä¸Šä¼˜äºç°æœ‰çš„ä¸»æµæ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03122v2",
      "published_date": "2025-10-03 15:50:52 UTC",
      "updated_date": "2025-10-12 15:32:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:33:04.553833+00:00"
    },
    {
      "arxiv_id": "2510.03381v2",
      "title": "Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges",
      "title_zh": "é¢å‘é«˜é€Ÿå…¬è·¯äº’é€šæ¢çº½åŒé“æµé‡é¢„æµ‹çš„è·¨æ¨¡æ€é‡æ„é¢„è®­ç»ƒ",
      "authors": [
        "Yongchao Li",
        "Jun Chen",
        "Zhuoxuan Li",
        "Chao Gao",
        "Yang Li",
        "Chu Zhang",
        "Changyin Dong"
      ],
      "abstract": "Interchanges are crucial nodes for vehicle transfers between highways, yet the lack of real-time ramp detectors creates blind spots in traffic prediction. To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a two-stage framework that leverages cross-modal reconstruction pretraining. In the first stage, STDAE reconstructs historical ramp flows from mainline data, forcing the model to capture intrinsic spatio-temporal relations. Its decoupled architecture with parallel spatial and temporal autoencoders efficiently extracts heterogeneous features. In the prediction stage, the learned representations are integrated with models such as GWNet to enhance accuracy. Experiments on three real-world interchange datasets show that STDAE-GWNET consistently outperforms thirteen state-of-the-art baselines and achieves performance comparable to models using historical ramp data. This demonstrates its effectiveness in overcoming detector scarcity and its plug-and-play potential for diverse forecasting pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜é€Ÿå…¬è·¯ç«‹äº¤æ¡¥å› ç¼ºä¹å®æ—¶åŒé“æ£€æµ‹å™¨è€Œå¯¼è‡´çš„æµé‡é¢„æµ‹ç›²ç‚¹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSpatio-Temporal Decoupled Autoencoder (STDAE) çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è·¨æ¨¡æ€é‡å»ºé¢„è®­ç»ƒ(Cross-Modal Reconstruction Pretraining)æŠ€æœ¯ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¹²çº¿æ•°æ®é‡å»ºå†å²åŒé“æµé‡ï¼Œä¿ƒä½¿æ¨¡å‹æ•æ‰å†…åœ¨çš„æ—¶ç©ºå…³è”ã€‚STDAE é‡‡ç”¨äº†è§£è€¦æ¶æ„ï¼Œé€šè¿‡å¹¶è¡Œçš„ç©ºé—´å’Œæ—¶é—´è‡ªåŠ¨ç¼–ç å™¨é«˜æ•ˆæå–å¼‚æ„ç‰¹å¾ã€‚åœ¨é¢„æµ‹é˜¶æ®µï¼Œå­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤ºå¯ä¸ GWNet ç­‰æ¨¡å‹é›†æˆä»¥æå‡å‡†ç¡®ç‡ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œç«‹äº¤æ¡¥æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSTDAE-GWNET çš„è¡¨ç°ä¼˜äº13ç§æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ï¼Œä¸”è¾¾åˆ°äº†ä¸ä½¿ç”¨å†å²åŒé“æ•°æ®çš„æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚è¿™è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å…‹æœæ£€æµ‹å™¨ç¨€ç¼ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶ä½œä¸ºå³æ’å³ç”¨(Plug-and-play)ç»„ä»¶åº”ç”¨äºå¤šç§é¢„æµ‹ç®¡çº¿çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03381v2",
      "published_date": "2025-10-03 15:26:56 UTC",
      "updated_date": "2025-11-27 14:40:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:05.360974+00:00"
    },
    {
      "arxiv_id": "2510.03095v3",
      "title": "Distilled Protein Backbone Generation",
      "title_zh": "è’¸é¦å¼è›‹ç™½è´¨éª¨æ¶ç”Ÿæˆ",
      "authors": [
        "Liyang Xie",
        "Haoran Zhang",
        "Zhendong Wang",
        "Wesley Tansey",
        "Mingyuan Zhou"
      ],
      "abstract": "Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications. The PyTorch implementation is available at https://github.com/LY-Xie/SiD_Protein",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è›‹ç™½è´¨éª¨æ¶ç”Ÿæˆ(protein backbone generation)é¢†åŸŸä¸­æ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹(diffusion- and flow-based models)æ¨ç†é€Ÿåº¦æ…¢çš„ç“¶é¢ˆã€‚ä¸ºäº†æå‡å¤§è§„æ¨¡è›‹ç™½è´¨å‘ç°çš„æ•ˆç‡ï¼Œä½œè€…é’ˆå¯¹è›‹ç™½è´¨è®¾è®¡ä»»åŠ¡æ”¹è¿›äº†Score identity Distillation (SiD)ç­–ç•¥ï¼ŒæˆåŠŸè®­ç»ƒå‡ºå°‘æ­¥ç”Ÿæˆå™¨ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¤šæ­¥ç”Ÿæˆ(multistep generation)ç»“åˆæ¨ç†é˜¶æ®µå™ªå£°è°ƒåˆ¶(inference time noise modulation)æ˜¯ç»´æŒç”Ÿæˆè´¨é‡çš„å…³é”®æŠ€æœ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥è’¸é¦æ¨¡å‹åœ¨ä¿æŒä¸Proteinaæ•™å¸ˆæ¨¡å‹ç›¸å½“çš„designabilityã€diversityå’Œnoveltyçš„åŒæ—¶ï¼Œå®ç°äº†è¶…è¿‡20å€çš„é‡‡æ ·é€Ÿåº¦æå‡ã€‚è¿™ä¸€æˆæœæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ–¹æ³•æ›´æ¥è¿‘å®é™…çš„è›‹ç™½è´¨å·¥ç¨‹åº”ç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "PyTorch implementation: https://github.com/LY-Xie/SiD_Protein",
      "pdf_url": "https://arxiv.org/pdf/2510.03095v3",
      "published_date": "2025-10-03 15:25:08 UTC",
      "updated_date": "2025-10-27 19:32:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:33:11.345082+00:00"
    },
    {
      "arxiv_id": "2510.03380v1",
      "title": "A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew",
      "title_zh": "ä¸€ç§é¢å‘æ•°é‡å€¾æ–œéç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®çš„é²æ£’èšç±»è”é‚¦å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Michael Ben Ali",
        "Imen Megdiche",
        "AndrÃ© Peninou",
        "Olivier Teste"
      ],
      "abstract": "Federated Learning (FL) is a decentralized paradigm that enables a client-server architecture to collaboratively train a global Artificial Intelligence model without sharing raw data, thereby preserving privacy. A key challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of Non-IID, where clients hold highly heterogeneous data volumes. Clustered Federated Learning (CFL) is an emergent variant of FL that presents a promising solution to Non-IID problem. It improves models' performance by grouping clients with similar data distributions into clusters. CFL methods generally fall into two operating strategies. In the first strategy, clients select the cluster that minimizes the local training loss. In the second strategy, the server groups clients based on local model similarities. However, most CFL methods lack systematic evaluation under QS but present significant challenges because of it.  In this paper, we present two main contributions. The first one is an evaluation of state-of-the-art CFL algorithms under various Non-IID settings, applying multiple QS scenarios to assess their robustness. Our second contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes an optimal coordination between both operating strategies of CFL. Our approach is robust against the different variations of QS settings. We conducted intensive experiments on six image classification datasets, resulting in 270 Non-IID configurations. The results show that CORNFLQS achieves the highest average ranking in both accuracy and clustering quality, as well as strong robustness to QS perturbations. Overall, our approach outperforms actual CFL algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Federated Learning (FL)ä¸­Non-IIDæ•°æ®çš„Quantity Skew (QS)é—®é¢˜ï¼Œå³å®¢æˆ·ç«¯æŒæœ‰æ•°æ®é‡é«˜åº¦å¼‚æ„çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç¨³å¥çš„Clustered Federated Learning (CFL)æ–¹æ³•ã€‚è®ºæ–‡é¦–å…ˆç³»ç»Ÿè¯„ä¼°äº†ç°æœ‰SOTA CFLç®—æ³•åœ¨å¤šç§QSåœºæ™¯ä¸‹çš„ç¨³å¥æ€§ï¼Œæ­ç¤ºäº†å½“å‰æ–¹æ³•åœ¨å¤„ç†ä¸å‡åŒ€æ•°æ®é‡åˆ†å¸ƒæ—¶çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºCORNFLQSçš„æ–°å‹è¿­ä»£CFLç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡åœ¨å®¢æˆ·ç«¯æŸå¤±æœ€å°åŒ–å’ŒæœåŠ¡å™¨ç«¯æ¨¡å‹ç›¸ä¼¼åº¦èšç±»ä¸¤ç§ç­–ç•¥ä¹‹é—´å®ç°æœ€ä¼˜åè°ƒï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹QSå¹²æ‰°çš„é²æ£’æ€§ã€‚åœ¨å…­ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ã€å…±è®¡270ç§Non-IIDé…ç½®ä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCORNFLQSåœ¨åˆ†ç±»å‡†ç¡®ç‡å’Œèšç±»è´¨é‡æ–¹é¢å‡è·å¾—äº†æœ€é«˜å¹³å‡æ’åã€‚è¯¥ç ”ç©¶è¯æ˜äº†CORNFLQSåœ¨åº”å¯¹æåº¦å¼‚æ„æ•°æ®ç¯å¢ƒæ—¶ä¼˜äºç°æœ‰çš„CFLç®—æ³•ï¼Œä¸ºå®ç°æ›´ç¨³å¥çš„åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03380v1",
      "published_date": "2025-10-03 15:23:43 UTC",
      "updated_date": "2025-10-03 15:23:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:33:20.755744+00:00"
    },
    {
      "arxiv_id": "2510.03078v1",
      "title": "From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments",
      "title_zh": "ä»äº‹å®åˆ°å¯¹æ¯”ï¼šé¢å‘æ™ºèƒ½ç¯å¢ƒçš„åäº‹å®è§£é‡Šè®¾è®¡ä¸è¯„ä¼°",
      "authors": [
        "Anna Trapp",
        "Mersedeh Sadeghi",
        "Andreas Vogelsang"
      ],
      "abstract": "Explainability is increasingly seen as an essential feature of rule-based smart environments. While counterfactual explanations, which describe what could have been done differently to achieve a desired outcome, are a powerful tool in eXplainable AI (XAI), no established methods exist for generating them in these rule-based domains. In this paper, we present the first formalization and implementation of counterfactual explanations tailored to this domain. It is implemented as a plugin that extends an existing explanation engine for smart environments. We conducted a user study (N=17) to evaluate our generated counterfactuals against traditional causal explanations. The results show that user preference is highly contextual: causal explanations are favored for their linguistic simplicity and in time-pressured situations, while counterfactuals are preferred for their actionable content, particularly when a user wants to resolve a problem. Our work contributes a practical framework for a new type of explanation in smart environments and provides empirical evidence to guide the choice of when each explanation type is most effective.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè§„åˆ™çš„æ™ºèƒ½ç¯å¢ƒ (smart environments)ï¼Œé¦–æ¬¡æå‡ºäº†åäº‹å®è§£é‡Š (counterfactual explanations) çš„å½¢å¼åŒ–å®šä¹‰ä¸å®ç°ï¼Œå¹¶å°†å…¶ä½œä¸ºç°æœ‰è§£é‡Šå¼•æ“çš„æ’ä»¶è¿›è¡Œäº†é›†æˆã€‚é€šè¿‡ä¸€é¡¹æ¶‰åŠ17åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ï¼Œè¯¥ç ”ç©¶å¯¹æ¯”äº†åäº‹å®è§£é‡Šä¸ä¼ ç»Ÿå› æœè§£é‡Š (causal explanations) çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”¨æˆ·çš„åå¥½å…·æœ‰é«˜åº¦çš„æƒ…å¢ƒç›¸å…³æ€§ï¼šå› æœè§£é‡Šå› å…¶è¯­è¨€ç®€æ´ä¸”é€‚ç”¨äºæ—¶é—´å‹åŠ›è¾ƒå¤§çš„åœºæ™¯è€Œå—åˆ°é’çï¼Œè€Œåäº‹å®è§£é‡Šåˆ™å› èƒ½æä¾›å…·æœ‰è¡ŒåŠ¨å¯¼å‘çš„å†…å®¹ï¼Œåœ¨ç”¨æˆ·éœ€è¦è§£å†³å…·ä½“é—®é¢˜æ—¶æ›´å…·ä¼˜åŠ¿ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºæ™ºèƒ½ç¯å¢ƒæä¾›äº†ä¸€ç§æ–°å‹è§£é‡Šç±»å‹çš„å®ç”¨æ¡†æ¶ï¼Œè¿˜ä¸ºåœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹é€‰æ‹©æœ€æœ‰æ•ˆçš„è§£é‡Šç­–ç•¥æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at Ex-ASE 2025, co-located with the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03078v1",
      "published_date": "2025-10-03 15:06:53 UTC",
      "updated_date": "2025-10-03 15:06:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:33:21.751470+00:00"
    },
    {
      "arxiv_id": "2510.03379v1",
      "title": "Can an AI-Powered Presentation Platform Based On The Game \"Just a Minute\" Be Used To Improve Students' Public Speaking Skills?",
      "title_zh": "åŸºäº â€œJust a Minuteâ€ æ¸¸æˆçš„ AI é©±åŠ¨æ¼”è®²å¹³å°èƒ½å¦æå‡å­¦ç”Ÿçš„å…¬å¼€æ¼”è®²èƒ½åŠ›ï¼Ÿ",
      "authors": [
        "Frederic Higham",
        "Tommy Yuan"
      ],
      "abstract": "This study explores the effectiveness of applying AI and gamification into a presentation platform aimed at University students wanting to improve their public speaking skills in their native tongue. Specifically, a platform based on the radio show, Just a Minute (JAM), is explored. In this game, players are challenged to speak fluently on a topic for 60 seconds without repeating themselves, hesitating or deviating from the topic. JAM has proposed benefits such as allowing students to improve their spontaneous speaking skills and reduce their use of speech disfluencies (\"um\", \"uh\", etc.).\n  Previous research has highlighted the difficulties students face when speaking publicly, the main one being anxiety. AI Powered Presentation Platforms (AI-PPPs), where students can speak with an immersive AI audience and receive real-time feedback, have been explored as a method to improve student's speaking skills and confidence. So far they have shown promising results which this study aims to build upon.\n  A group of students from the University of York are enlisted to evaluate the effectiveness of the JAM platform. They are asked to fill in a questionnaire, play through the game twice and then complete a final questionnaire to discuss their experiences playing the game. Various statistics are gathered during their gameplay such as the number of points they gained and the number of rules they broke. The results showed that students found the game promising and believed that their speaking skills could improve if they played the game for longer. More work will need to be carried out to prove the effectiveness of the game beyond the short term.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†äººå·¥æ™ºèƒ½(AI)ä¸æ¸¸æˆåŒ–(Gamification)å…ƒç´ èå…¥æ¼”è®²å¹³å°ï¼Œä»¥æå‡å¤§å­¦ç”Ÿæ¯è¯­æ¼”è®²èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†åŸºäºå¹¿æ’­èŠ‚ç›®â€œJust a Minuteâ€(JAM)å¼€å‘çš„å¹³å°ï¼Œè¦æ±‚ç»ƒä¹ è€…åœ¨ä¸é‡å¤ã€ä¸çŠ¹è±«ä¸”ä¸åç¦»ä¸»é¢˜çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹ç‰¹å®šè¯é¢˜æŒç»­æ¼”è¯´60ç§’ã€‚è¯¥å¹³å°æ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿæ²‰æµ¸å¼AIå—ä¼—å’Œæä¾›å®æ—¶åé¦ˆï¼Œè§£å†³å­¦ç”Ÿåœ¨å…¬å…±æ¼”è®²ä¸­å¸¸è§çš„ç„¦è™‘é—®é¢˜ï¼Œå¹¶å‡å°‘â€œå—¯â€ã€â€œå•Šâ€ç­‰è¨€è¯­ä¸æµåˆ©(speech disfluencies)ç°è±¡ã€‚å®éªŒé‚€è¯·äº†çº¦å…‹å¤§å­¦(University of York)çš„å­¦ç”Ÿè¿›è¡Œè¯„ä¼°ï¼Œé€šè¿‡é—®å·è°ƒæŸ¥å’Œä¸¤æ¬¡æ¸¸æˆä½“éªŒè®°å½•å…¶å¾—åˆ†åŠè¿è§„ç»Ÿè®¡æ•°æ®ã€‚ç»“æœæ˜¾ç¤ºï¼Œå­¦ç”Ÿè®¤ä¸ºè¯¥æ¸¸æˆåŒ–å¹³å°å…·æœ‰è‰¯å¥½çš„åº”ç”¨å‰æ™¯ï¼Œå¹¶ç›¸ä¿¡é•¿æœŸç»ƒä¹ èƒ½æ˜¾è‘—æé«˜è‡ªå‘æ€§æ¼”è®²(spontaneous speaking)æŠ€å·§ã€‚å°½ç®¡åˆæ­¥åé¦ˆç§¯æï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ä»¥éªŒè¯è¯¥å¹³å°åœ¨é•¿æœŸæå‡æ¼”è®²æŠ€èƒ½æ–¹é¢çš„æŒç»­æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "11 pages, to be presented orally at the International Conference on Education and Artificial Intelligence Technologies (Nov 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03379v1",
      "published_date": "2025-10-03 15:06:19 UTC",
      "updated_date": "2025-10-03 15:06:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:33:24.747374+00:00"
    },
    {
      "arxiv_id": "2510.03075v2",
      "title": "What Drives Compositional Generalization in Visual Generative Models?",
      "title_zh": "è§†è§‰ç”Ÿæˆæ¨¡å‹ç»„åˆæ³›åŒ–èƒ½åŠ›çš„é©±åŠ¨å› ç´ æ¢æ",
      "authors": [
        "Karim Farid",
        "Rajat Sahay",
        "Yumna Ali Alnaggar",
        "Simon Schrodi",
        "Volker Fischer",
        "Cordelia Schmid",
        "Thomas Brox"
      ],
      "abstract": "Compositional generalization, the ability to generate novel combinations of known concepts, is a key ingredient for visual generative models. Yet, not all mechanisms that enable or inhibit it are fully understood. In this work, we conduct a systematic study of how various design choices influence compositional generalization in image and video generation in a positive or negative way. Through controlled experiments, we identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution, and (ii) to what extent conditioning provides information about the constituent concepts during training. Building on these insights, we show that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based objective can improve compositional performance in discrete models like MaskGIT.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸­ç»„åˆæ³›åŒ–ï¼ˆCompositional generalizationï¼‰çš„é©±åŠ¨æœºåˆ¶ï¼Œæ—¨åœ¨ç†è§£å½±å“æ¨¡å‹ç”Ÿæˆå·²çŸ¥æ¦‚å¿µæ–°é¢–ç»„åˆçš„å…³é”®å› ç´ ã€‚é€šè¿‡åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸçš„å—æ§å®éªŒï¼Œç ”ç©¶è¯†åˆ«å‡ºä¸¤ä¸ªå½±å“ç»„åˆæ€§èƒ½çš„æ ¸å¿ƒè¦ç´ ï¼šä¸€æ˜¯è®­ç»ƒç›®æ ‡å‡½æ•°æ˜¯åŸºäºç¦»æ•£åˆ†å¸ƒè¿˜æ˜¯è¿ç»­åˆ†å¸ƒï¼ŒäºŒæ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒèŠ‚ï¼ˆConditioningï¼‰ä¿¡æ¯å¯¹ç»„æˆæ¦‚å¿µçš„è¦†ç›–ç¨‹åº¦ã€‚åŸºäºè¿™äº›æ´å¯Ÿï¼Œä½œè€…æå‡ºåˆ©ç”¨è¾…åŠ©çš„è¿ç»­JEPAï¼ˆJoint-Embedding Predictive Architectureï¼‰ç›®æ ‡æ¥ä¼˜åŒ–MaskGITçš„ç¦»æ•£æŸå¤±å‡½æ•°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§æ–¹æ³•èƒ½æœ‰æ•ˆæ”¹å–„MaskGITç­‰ç¦»æ•£æ¨¡å‹åœ¨å¤æ‚ç»„åˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ºæå‡ç”Ÿæˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†ç†è®ºæ”¯æŒä¸æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03075v2",
      "published_date": "2025-10-03 15:02:27 UTC",
      "updated_date": "2025-10-06 10:01:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:33:35.247884+00:00"
    },
    {
      "arxiv_id": "2510.03069v1",
      "title": "A Study of Neural Polar Decoders for Communication",
      "title_zh": "é¢å‘é€šä¿¡çš„ç¥ç»æåŒ–è¯‘ç å™¨ç ”ç©¶",
      "authors": [
        "Rom Hirsch",
        "Ziv Aharoni",
        "Henry D. Pfister",
        "Haim H. Permuter"
      ],
      "abstract": "In this paper, we adapt and analyze Neural Polar Decoders (NPDs) for end-to-end communication systems. While prior work demonstrated the effectiveness of NPDs on synthetic channels, this study extends the NPD to real-world communication systems. The NPD was adapted to complete OFDM and single-carrier communication systems. To satisfy practical system requirements, the NPD is extended to support any code length via rate matching, higher-order modulations, and robustness across diverse channel conditions. The NPD operates directly on channels with memory, exploiting their structure to achieve higher data rates without requiring pilots and a cyclic prefix. Although NPD entails higher computational complexity than the standard 5G polar decoder, its neural network architecture enables an efficient representation of channel statistics, resulting in manageable complexity suitable for practical systems. Experimental results over 5G channels demonstrate that the NPD consistently outperforms the 5G polar decoder in terms of BER, BLER, and throughput. These improvements are particularly significant for low-rate and short-block configurations, which are prevalent in 5G control channels. Furthermore, NPDs applied to single-carrier systems offer performance comparable to OFDM with lower PAPR, enabling effective single-carrier transmission over 5G channels. These results position the NPD as a high-performance, pilotless, and robust decoding solution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç«¯åˆ°ç«¯é€šä¿¡ç³»ç»Ÿæ”¹è¿›å¹¶åˆ†æäº†ç¥ç»ç½‘ç»œæåŒ–ç è¯‘ç å™¨(Neural Polar Decoders, NPDs)ï¼Œå°†å…¶åº”ç”¨ä»åˆæˆä¿¡é“æ‰©å±•åˆ°åŒ…å«æ­£äº¤é¢‘åˆ†å¤ç”¨(OFDM)å’Œå•è½½æ³¢(single-carrier)çš„å®é™…é€šä¿¡åœºæ™¯ã€‚ä¸ºäº†æ»¡è¶³å®é™…ç³»ç»Ÿéœ€æ±‚ï¼ŒNPD è¢«æ‰©å±•ä»¥æ”¯æŒé€šè¿‡é€Ÿç‡åŒ¹é…(rate matching)å®ç°çš„ä»»æ„ç é•¿ã€é«˜é˜¶è°ƒåˆ¶ä»¥åŠå¯¹å¤šæ ·åŒ–ä¿¡é“ç¯å¢ƒçš„é²æ£’æ€§ã€‚è¯¥è¯‘ç å™¨èƒ½ç›´æ¥åœ¨æœ‰è®°å¿†ä¿¡é“ä¸Šè¿è¡Œï¼Œåˆ©ç”¨å…¶ç»“æ„ç‰¹å¾å®ç°æ›´é«˜çš„æ•°æ®é€Ÿç‡ï¼Œä¸”æ— éœ€å¯¼é¢‘(pilots)å’Œå¾ªç¯å‰ç¼€(cyclic prefix)ã€‚å°½ç®¡ NPD çš„è®¡ç®—å¤æ‚åº¦é«˜äºæ ‡å‡†çš„ 5G æåŒ–ç è¯‘ç å™¨ï¼Œä½†å…¶ç¥ç»ç½‘ç»œæ¶æ„èƒ½å¤Ÿé«˜æ•ˆè¡¨å¾ä¿¡é“ç»Ÿè®¡ç‰¹æ€§ï¼Œåœ¨å®é™…ç³»ç»Ÿä¸­å…·æœ‰å¯è¡Œæ€§ã€‚åœ¨ 5G ä¿¡é“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNPD åœ¨è¯¯ç ç‡(BER)ã€è¯¯å—ç‡(BLER)å’Œååé‡æ–¹é¢æŒç»­ä¼˜äº 5G æåŒ–ç è¯‘ç å™¨ï¼Œå°¤å…¶åœ¨ 5G æ§åˆ¶ä¿¡é“å¸¸ç”¨çš„ä½é€Ÿç‡å’ŒçŸ­å—é…ç½®ä¸‹ä¼˜åŠ¿æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œåº”ç”¨äºå•è½½æ³¢ç³»ç»Ÿçš„ NPD èƒ½å¤Ÿæä¾›ä¸ OFDM ç›¸å½“çš„æ€§èƒ½ä¸”å…·å¤‡æ›´ä½çš„å³°å€¼å¹³å‡åŠŸç‡æ¯”(PAPR)ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºé«˜æ€§èƒ½ã€æ— å¯¼é¢‘ä¸”ç¨³å¥çš„è¯‘ç è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03069v1",
      "published_date": "2025-10-03 14:55:18 UTC",
      "updated_date": "2025-10-03 14:55:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:17.060069+00:00"
    },
    {
      "arxiv_id": "2510.03065v1",
      "title": "A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem",
      "title_zh": "é¢å‘è¶³å¤Ÿè¿‘æ—…è¡Œå•†é—®é¢˜çš„ç»Ÿä¸€æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Mingfeng Fan",
        "Jiaqi Cheng",
        "Yaoxin Wu",
        "Yifeng Zhang",
        "Yibin Yang",
        "Guohua Wu",
        "Guillaume Sartoretti"
      ],
      "abstract": "In recent years, deep reinforcement learning (DRL) has gained traction for solving the NP-hard traveling salesman problem (TSP). However, limited attention has been given to the close-enough TSP (CETSP), primarily due to the challenge introduced by its neighborhood-based visitation criterion, wherein a node is considered visited if the agent enters a compact neighborhood around it. In this work, we formulate a Markov decision process (MDP) for CETSP using a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL) framework that separates decision-making into node selection and waypoint determination. Specifically, an adapted encoder is employed for effective feature extraction, followed by a node-decoder and a loc-decoder to handle the two sub-tasks, respectively. A k-nearest neighbors subgraph interaction strategy is further introduced to enhance spatial reasoning during location decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a unified model capable of generalizing across different problem sizes and varying neighborhood radius types (i.e., constant and random radii). Experimental results show that UD3RL outperforms conventional methods in both solution quality and runtime, while exhibiting strong generalization across problem scales, spatial distributions, and radius ranges, as well as robustness to dynamic environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰é‚»åŸŸè®¿é—®å‡†åˆ™çš„ Close Enough Traveling Salesman Problem (CETSP) æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆã€‚è®ºæ–‡é€šè¿‡ç¦»æ•£åŒ–æ–¹æ¡ˆæ„å»ºäº† Markov Decision Process (MDP)ï¼Œå¹¶æå‡ºäº†åä¸º Unified Dual-Decoder DRL (UD3RL) çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†å†³ç­–è¿‡ç¨‹åˆ†ç¦»ä¸ºèŠ‚ç‚¹é€‰æ‹©å’Œèˆªè·¯ç‚¹ç¡®å®šã€‚è¯¥æ¡†æ¶é‡‡ç”¨ Adapted Encoder è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ç»“åˆ Node-decoder å’Œ Loc-decoder å¤„ç†å­ä»»åŠ¡ï¼ŒåŒæ—¶å¼•å…¥ k-nearest neighbors å­å›¾äº¤äº’ç­–ç•¥ä»¥å¢å¼ºç©ºé—´æ¨ç†ã€‚ç ”ç©¶ä½¿ç”¨å®šåˆ¶çš„ REINFORCE ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿è‡³ä¸åŒè§„æ¨¡çš„é—®é¢˜åŠå„ç±»é‚»åŸŸåŠå¾„è®¾ç½®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUD3RL åœ¨æ±‚è§£è´¨é‡å’Œè¿è¡Œæ—¶é—´ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§ç©ºé—´åˆ†å¸ƒå’ŒåŠ¨æ€ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03065v1",
      "published_date": "2025-10-03 14:49:05 UTC",
      "updated_date": "2025-10-03 14:49:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:19.951937+00:00"
    },
    {
      "arxiv_id": "2510.03064v1",
      "title": "Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation",
      "title_zh": "é¢å‘ç½‘é¡µæœç´¢åŒ¹é…æ–¹æ¡ˆç”Ÿæˆçš„å‚æ•°åŒ–åŠ¨ä½œ Actor-Critic å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¯”è¾ƒåˆ†æ",
      "authors": [
        "Ubayd Bapoo",
        "Clement N Nyirenda"
      ],
      "abstract": "This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional decision-making tasks using fully observable environments. The focus is on parametrized action (PA) spaces, eliminating the need for recurrent networks, with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to continuous action-parameter spaces. Hyperparameter optimization was performed with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC) outperformed other algorithms, achieving the fastest training times and highest returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC demonstrated superior efficiency and reliability, making it ideal for tasks requiring rapid convergence and robust performance. Future work could explore hybrid strategies combining entropy-regularization with truncation-based methods to enhance stability and expand investigations into generalizability.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹é«˜ç»´å†³ç­–ä»»åŠ¡ï¼Œå¯¹æ¯”è¯„ä¼°äº† Soft Actor Critic (SAC)ã€Greedy Actor Critic (GAC) å’Œ Truncated Quantile Critics (TQC) ä¸‰ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å‚æ•°åŒ–åŠ¨ä½œ (Parameterized Action) ç©ºé—´ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨ Platform-v0 å’Œ Goal-v0 åŸºå‡†ç¯å¢ƒä¸‹çš„æµ‹è¯•ï¼Œç ”ç©¶åˆ©ç”¨ Microsoft NNI è¿›è¡Œäº†è¶…å‚æ•°ä¼˜åŒ–ï¼Œç¡®ä¿äº†å®éªŒç»“æœçš„å¯é‡å¤æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒParameterized Action Greedy Actor-Critic (PAGAC) åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°æœ€ä½³ï¼Œä¸ä»…è·å¾—äº†æœ€é«˜çš„å›æŠ¥ï¼Œä¸”åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–ç®—æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒPAGAC åœ¨ Platform æ¸¸æˆå’Œæœºå™¨äººè¶³çƒ Goal æ¸¸æˆä¸­åˆ†åˆ«ä»…ç”¨çº¦ 41 åˆ†é’Ÿå’Œ 24 åˆ†é’Ÿå³å®Œæˆ 5000 ä¸ªå›åˆçš„è®­ç»ƒã€‚ç›¸è¾ƒäº PASAC å’Œ PATQCï¼ŒPAGAC å±•ç°å‡ºçš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ä½¿å…¶æˆä¸ºå¤„ç†å¤æ‚åŠ¨ä½œç©ºé—´ä»»åŠ¡çš„ç†æƒ³é€‰æ‹©ã€‚æœªæ¥ç ”ç©¶å°†æ¢ç´¢ç»“åˆç†µæ­£åˆ™åŒ–ä¸æˆªæ–­æ–¹æ³•çš„æ··åˆç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç®—æ³•çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 10th International Congress on Information and Communication Technology (ICICT 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03064v1",
      "published_date": "2025-10-03 14:48:57 UTC",
      "updated_date": "2025-10-03 14:48:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:22.253465+00:00"
    },
    {
      "arxiv_id": "2510.03060v1",
      "title": "Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles",
      "title_zh": "è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„è¯­ä¹‰åŒºåˆ†ï¼šæè¿°æ€§ä¸è¡¨è¾¾æ€§è¯­éŸ³è§’è‰²çš„å¯ç¤º",
      "authors": [
        "Rongchen Guo",
        "Vincent Francoeur",
        "Isar Nejadgholi",
        "Sylvain Gagnon",
        "Miodrag Bolic"
      ],
      "abstract": "Speech Emotion Recognition (SER) is essential for improving human-computer interaction, yet its accuracy remains constrained by the complexity of emotional nuances in speech. In this study, we distinguish between descriptive semantics, which represents the contextual content of speech, and expressive semantics, which reflects the speaker's emotional state. After watching emotionally charged movie segments, we recorded audio clips of participants describing their experiences, along with the intended emotion tags for each clip, participants' self-rated emotional responses, and their valence/arousal scores. Through experiments, we show that descriptive semantics align with intended emotions, while expressive semantics correlate with evoked emotions. Our findings inform SER applications in human-AI interaction and pave the way for more context-aware AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«(Speech Emotion Recognition, SER)ä¸­çš„è¯­ä¹‰å·®å¼‚ï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³ä¸­æƒ…æ„Ÿç»†å¾®å·®åˆ«å¯¼è‡´çš„è¯†åˆ«å‡†ç¡®ç‡å—é™é—®é¢˜ã€‚ç ”ç©¶è€…åŒºåˆ†äº†ä»£è¡¨è¯­éŸ³ä¸Šä¸‹æ–‡å†…å®¹çš„æè¿°æ€§è¯­ä¹‰(descriptive semantics)ä¸åæ˜ è¯´è¯è€…æƒ…æ„ŸçŠ¶æ€çš„è¡¨è¾¾æ€§è¯­ä¹‰(expressive semantics)ã€‚é€šè¿‡åˆ†æå‚ä¸è€…è§‚çœ‹ç”µå½±ç‰‡æ®µåçš„æè¿°æ€§è¯­éŸ³è®°å½•åŠå…¶å¯¹åº”çš„æƒ…æ„Ÿæ ‡ç­¾ã€è‡ªæˆ‘è¯„ä¼°æƒ…ç»ªå’Œæ•ˆä»·/å”¤é†’åº¦å¾—åˆ†ï¼Œç ”ç©¶æ·±å…¥åˆ†æäº†ä¸¤ç±»è¯­ä¹‰çš„ä½œç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæè¿°æ€§è¯­ä¹‰ä¸é¢„æœŸæƒ…æ„Ÿ(intended emotions)ç›¸ä¸€è‡´ï¼Œè€Œè¡¨è¾¾æ€§è¯­ä¹‰åˆ™ä¸è¯±å‘æƒ…æ„Ÿ(evoked emotions)å¯†åˆ‡ç›¸å…³ã€‚è¿™äº›å‘ç°ä¸ºæ”¹è¿›äººæœºäº¤äº’ä¸­çš„SERåº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œæœ‰åŠ©äºæ„å»ºæ›´å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„AIç³»ç»Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the *SEM conference collocated with EMNLP2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03060v1",
      "published_date": "2025-10-03 14:42:35 UTC",
      "updated_date": "2025-10-03 14:42:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:26.455524+00:00"
    },
    {
      "arxiv_id": "2510.03051v1",
      "title": "ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization",
      "title_zh": "ZeroShotOptï¼šè¿ˆå‘é«˜æ•ˆé»‘ç›’ä¼˜åŒ–çš„é›¶æ ·æœ¬é¢„è®­ç»ƒæ¨¡å‹",
      "authors": [
        "Jamison Meindl",
        "Yunsheng Tian",
        "Tony Cui",
        "Veronika Thost",
        "Zhang-Wei Hong",
        "Johannes DÃ¼rholt",
        "Jie Chen",
        "Wojciech Matusik",
        "Mina KonakoviÄ‡ LukoviÄ‡"
      ],
      "abstract": "Global optimization of expensive, derivative-free black-box functions requires extreme sample efficiency. While Bayesian optimization (BO) is the current state-of-the-art, its performance hinges on surrogate and acquisition function hyper-parameters that are often hand-tuned and fail to generalize across problem landscapes. We present ZeroShotOpt, a general-purpose, pretrained model for continuous black-box optimization tasks ranging from 2D to 20D. Our approach leverages offline reinforcement learning on large-scale optimization trajectories collected from 12 BO variants. To scale pretraining, we generate millions of synthetic Gaussian process-based functions with diverse landscapes, enabling the model to learn transferable optimization policies. As a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array of unseen benchmarks, matching or surpassing the sample efficiency of leading global optimizers, including BO, while also offering a reusable foundation for future extensions and improvements. Our open-source code, dataset, and model are available at: https://github.com/jamisonmeindl/zeroshotopt",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜æˆæœ¬ã€æ— å¯¼æ•°é»‘ç›’ä¼˜åŒ–(Black-box optimization)å¯¹æ ·æœ¬æ•ˆç‡çš„æé«˜è¦æ±‚ï¼Œæå‡ºäº†åä¸ºZeroShotOptçš„é€šç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ºè§£å†³ä¼ ç»Ÿè´å¶æ–¯ä¼˜åŒ–(Bayesian optimization)è¶…å‚æ•°éš¾ä»¥è·¨åœºæ™¯æ³›åŒ–çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline reinforcement learning)åœ¨12ç§BOå˜ä½“çš„å¤§è§„æ¨¡è½¨è¿¹ä¸Šè¿›è¡Œå­¦ä¹ ã€‚é€šè¿‡ç”Ÿæˆæ•°ç™¾ä¸‡ä¸ªå…·æœ‰å¤šæ ·åŒ–åœ°å½¢çš„é«˜æ–¯è¿‡ç¨‹(Gaussian process)åˆæˆå‡½æ•°ï¼ŒZeroShotOptæˆåŠŸä¹ å¾—äº†æå¼ºçš„å¯è¿ç§»ä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨2Dè‡³20Dçš„å„ç±»æœªçŸ¥åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬(Zero-shot)æ³›åŒ–æ€§èƒ½ã€‚å…¶æ ·æœ¬æ•ˆç‡ä¸ä»…åŒ¹é…ç”šè‡³è¶…è¶Šäº†ç›®å‰é¢†å…ˆçš„å…¨å±€ä¼˜åŒ–ç®—æ³•ï¼Œä¸”ä½œä¸ºä¸€ç§åŸºç¡€æ¨¡å‹ï¼Œä¸ºæœªæ¥ä¼˜åŒ–æŠ€æœ¯çš„æŒç»­æ”¹è¿›æä¾›äº†å¼€æºçš„æ¡†æ¶ä¸æ•°æ®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03051v1",
      "published_date": "2025-10-03 14:33:23 UTC",
      "updated_date": "2025-10-03 14:33:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:30.756558+00:00"
    },
    {
      "arxiv_id": "2510.03049v1",
      "title": "When and Where do Events Switch in Multi-Event Video Generation?",
      "title_zh": "å¤šäº‹ä»¶è§†é¢‘ç”Ÿæˆä¸­çš„äº‹ä»¶åˆ‡æ¢ï¼šæ—¶æœºä¸ä½ç½®æ¢ç©¶",
      "authors": [
        "Ruotong Liao",
        "Guowen Huang",
        "Qing Cheng",
        "Thomas Seidl",
        "Daniel Cremers",
        "Volker Tresp"
      ],
      "abstract": "Text-to-video (T2V) generation has surged in response to challenging questions, especially when a long video must depict multiple sequential events with temporal coherence and controllable content. Existing methods that extend to multi-event generation omit an inspection of the intrinsic factor in event shifting. The paper aims to answer the central question: When and where multi-event prompts control event transition during T2V generation. This work introduces MEve, a self-curated prompt suite for evaluating multi-event text-to-video (T2V) generation, and conducts a systematic study of two representative model families, i.e., OpenSora and CogVideoX. Extensive experiments demonstrate the importance of early intervention in denoising steps and block-wise model layers, revealing the essential factor for multi-event video generation and highlighting the possibilities for multi-event conditioning in future models.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆè§†é¢‘(Text-to-video, T2V)åœ¨ç”ŸæˆåŒ…å«å¤šä¸ªåºåˆ—äº‹ä»¶çš„é•¿è§†é¢‘æ—¶ï¼Œå¦‚ä½•ä¿æŒæ—¶é—´è¿è´¯æ€§ä¸å†…å®¹å¯æ§æ€§è¿™ä¸€æŒ‘æˆ˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…é‡ç‚¹ç ”ç©¶äº†äº‹ä»¶åˆ‡æ¢(event shifting)çš„å†…åœ¨æœºåˆ¶ï¼Œæ—¨åœ¨å›ç­”å¤šäº‹ä»¶æç¤ºè¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä½•æ—¶ä»¥åŠä½•å¤„å‘æŒ¥æ§åˆ¶ä½œç”¨ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸“é—¨çš„æç¤ºè¯è¯„ä¼°å¥—ä»¶MEveï¼Œå¹¶å¯¹OpenSoraå’ŒCogVideoXä¸¤ç±»ä»£è¡¨æ€§æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§ç ”ç©¶ã€‚å®éªŒç»“æœæ­ç¤ºäº†åœ¨å»å™ªæ­¥éª¤(denoising steps)å’Œç‰¹å®šçš„æ¨¡å‹å—å±‚(block-wise model layers)è¿›è¡Œæ—©æœŸå¹²é¢„çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…ç¡®å®šäº†å¤šäº‹ä»¶è§†é¢‘ç”Ÿæˆçš„å…³é”®å½±å“å› ç´ ï¼Œè¿˜ä¸ºæœªæ¥æ¨¡å‹å®ç°æ›´ç²¾å‡†çš„å¤šäº‹ä»¶è°ƒèŠ‚(multi-event conditioning)å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Work in Progress. Accepted to ICCV2025 @ LongVid-Foundations",
      "pdf_url": "https://arxiv.org/pdf/2510.03049v1",
      "published_date": "2025-10-03 14:31:56 UTC",
      "updated_date": "2025-10-03 14:31:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:49.150695+00:00"
    },
    {
      "arxiv_id": "2510.08587v1",
      "title": "EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation",
      "title_zh": "EGSTalkerï¼šåŸºäºé«˜æ•ˆé«˜æ–¯å˜å½¢çš„å®æ—¶éŸ³é¢‘é©±åŠ¨è¯´è¯äººç”Ÿæˆ",
      "authors": [
        "Tianheng Zhu",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "abstract": "This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EGSTalkerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº 3D Gaussian Splatting (3DGS) çš„å®æ—¶éŸ³é¢‘é©±åŠ¨è°ˆè¯å¤´åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶æå‡ç”Ÿæˆé€Ÿåº¦ä¸è§†è§‰ä¿çœŸåº¦ã€‚è¯¥æ¡†æ¶ä»…éœ€ 3-5 åˆ†é’Ÿçš„è®­ç»ƒè§†é¢‘å³å¯åˆæˆé«˜è´¨é‡é¢éƒ¨åŠ¨ç”»ï¼Œä¸»è¦åŒ…å«é™æ€é«˜æ–¯åˆå§‹åŒ–å’ŒéŸ³é¢‘é©±åŠ¨å˜å½¢ä¸¤ä¸ªé˜¶æ®µã€‚åœ¨åˆå§‹åŒ–é˜¶æ®µï¼Œåˆ©ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œä¸‰å¹³é¢ (multi-resolution hash triplane) ä¸ Kolmogorov-Arnold Network (KAN) æå–ç©ºé—´ç‰¹å¾å¹¶æ„å»ºç´§å‡‘çš„ 3D Gaussian è¡¨ç¤ºï¼›åœ¨å˜å½¢é˜¶æ®µï¼Œé€šè¿‡é«˜æ•ˆç©ºé—´éŸ³é¢‘æ³¨æ„åŠ› (Efficient Spatial-Audio Attention, ESAA) æ¨¡å—èåˆéŸ³é¢‘ä¸ç©ºé—´çº¿ç´¢ï¼Œå¹¶ç”± KAN é¢„æµ‹å¯¹åº”çš„ Gaussian å˜å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEGSTalker åœ¨æ¸²æŸ“è´¨é‡å’Œå£å‹åŒæ­¥ (lip-sync) å‡†ç¡®ç‡ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æˆæœå……åˆ†å±•ç¤ºäº† EGSTalker åœ¨å®æ—¶å¤šåª’ä½“åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (6 pages). Accepted for publication by IEEE International Conference on Systems, Man, and Cybernetics 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.08587v1",
      "published_date": "2025-10-03 14:31:20 UTC",
      "updated_date": "2025-10-03 14:31:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:34.157104+00:00"
    },
    {
      "arxiv_id": "2510.03038v1",
      "title": "CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration",
      "title_zh": "CHORDï¼šç«¯äº‘ååŒä¸‹é¢å‘åºåˆ—æ¨èçš„å®šåˆ¶åŒ–æ··åˆç²¾åº¦ç«¯ä¾§æ¨¡å‹",
      "authors": [
        "Tianqi Liu",
        "Kairui Fu",
        "Shengyu Zhang",
        "Wenyan Fan",
        "Zhaocheng Du",
        "Jieming Zhu",
        "Fan Wu",
        "Fei Wu"
      ],
      "abstract": "With the advancement of mobile device capabilities, deploying reranking models directly on devices has become feasible, enabling real-time contextual recommendations. When migrating models from cloud to devices, resource heterogeneity inevitably necessitates model compression. Recent quantization methods show promise for efficient deployment, yet they overlook device-specific user interests, resulting in compromised recommendation accuracy. While on-device finetuning captures personalized user preference, it imposes additional computational burden through local retraining. To address these challenges, we propose a framework for \\underline{\\textbf{C}}ustomizing \\underline{\\textbf{H}}ybrid-precision \\underline{\\textbf{O}}n-device model for sequential \\underline{\\textbf{R}}ecommendation with \\underline{\\textbf{D}}evice-cloud collaboration (\\textbf{CHORD}), leveraging channel-wise mixed-precision quantization to simultaneously achieve personalization and resource-adaptive deployment. CHORD distributes randomly initialized models across heterogeneous devices and identifies user-specific critical parameters through auxiliary hypernetwork modules on the cloud. Our parameter sensitivity analysis operates across multiple granularities (layer, filter, and element levels), enabling precise mapping from user profiles to quantization strategy. Through on-device mixed-precision quantization, CHORD delivers dynamic model adaptation and accelerated inference without backpropagation, eliminating costly retraining cycles. We minimize communication overhead by encoding quantization strategies using only 2 bits per channel instead of 32-bit weights. Experiments on three real-world datasets with two popular backbones (SASRec and Caser) demonstrate the accuracy, efficiency, and adaptivity of CHORD.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CHORDï¼Œä¸€ç§é¢å‘åºå‘æ¨è(Sequential Recommendation)çš„ç«¯äº‘åä½œ(Device-cloud Collaboration)æ··åˆç²¾åº¦åœ¨ç«¯æ¨¡å‹å®šåˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹ä»äº‘ç«¯è¿ç§»è‡³å¼‚æ„è®¾å¤‡æ—¶å› å‹ç¼©å¯¼è‡´çš„ä¸ªæ€§åŒ–ç¼ºå¤±åŠæœ¬åœ°å¾®è°ƒè®¡ç®—è´Ÿæ‹…ã€‚CHORDåˆ©ç”¨é€šé“çº§æ··åˆç²¾åº¦é‡åŒ–(Channel-wise mixed-precision quantization)å®ç°ä¸ªæ€§åŒ–ä¸èµ„æºè‡ªé€‚åº”éƒ¨ç½²ï¼Œé€šè¿‡äº‘ç«¯çš„è¾…åŠ©è¶…ç½‘ç»œ(Hypernetwork)æ¨¡å—è¯†åˆ«ç”¨æˆ·ç‰¹å®šçš„å…³é”®å‚æ•°ã€‚è¯¥æ–¹æ¡ˆæ‰§è¡Œè·¨å±‚ã€æ»¤æ³¢å™¨åŠå…ƒç´ çº§åˆ«çš„å¤šç²’åº¦å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼Œä»è€Œå®ç°ä»ç”¨æˆ·ç”»åƒåˆ°é‡åŒ–ç­–ç•¥çš„ç²¾ç¡®æ˜ å°„ã€‚å€ŸåŠ©ç«¯ä¸Šæ··åˆç²¾åº¦é‡åŒ–ï¼ŒCHORDæ— éœ€åå‘ä¼ æ’­(Backpropagation)å³å¯å®ŒæˆåŠ¨æ€æ¨¡å‹é€‚é…å’Œæ¨ç†åŠ é€Ÿï¼Œæ˜¾è‘—æ¶ˆé™¤äº†æœ¬åœ°é‡æ–°è®­ç»ƒçš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¯é€šé“ä»…2æ¯”ç‰¹çš„ç­–ç•¥ç¼–ç ä»£æ›¿å…¨é‡æƒé‡ï¼Œæå¤§åœ°é™ä½äº†ç«¯äº‘é€šä¿¡å¼€é”€ã€‚åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šç»“åˆSASRecå’ŒCaserä¸»å¹²ç½‘ç»œçš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œè‡ªé€‚åº”æ€§æ–¹é¢çš„ä¼˜è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by ACM MM'25",
      "pdf_url": "https://arxiv.org/pdf/2510.03038v1",
      "published_date": "2025-10-03 14:20:45 UTC",
      "updated_date": "2025-10-03 14:20:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:37.751702+00:00"
    },
    {
      "arxiv_id": "2510.15929v1",
      "title": "Comparing LLMs for Sentiment Analysis in Financial Market News",
      "title_zh": "é‡‘èå¸‚åœºæ–°é—»æƒ…æ„Ÿåˆ†æä¸­çš„å¤§è¯­è¨€æ¨¡å‹å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Lucas Eduardo Pereira Teles",
        "Carlos M. S. Figueiredo"
      ],
      "abstract": "This article presents a comparative study of large language models (LLMs) in the task of sentiment analysis of financial market news. This work aims to analyze the performance difference of these models in this important natural language processing task within the context of finance. LLM models are compared with classical approaches, allowing for the quantification of the benefits of each tested model or approach. Results show that large language models outperform classical models in the vast majority of cases.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹é‡‘èå¸‚åœºæ–°é—»çš„ Sentiment Analysis ä»»åŠ¡ï¼Œå¯¹å¤šç§ Large Language Models (LLMs) è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ¯”è¾ƒç ”ç©¶ã€‚è¯¥å·¥ä½œæ—¨åœ¨æ·±å…¥åˆ†æä¸åŒæ¨¡å‹åœ¨é‡‘èè¿™ä¸€é‡è¦ Natural Language Processing é¢†åŸŸä¸­çš„æ€§èƒ½è¡¨ç°å·®å¼‚ã€‚ç ”ç©¶è¿‡ç¨‹ä¸­ï¼Œä½œè€…å°† LLM æ¨¡å‹ä¸ä¼ ç»Ÿçš„ Classical Approaches è¿›è¡Œäº†é‡åŒ–å¯¹æ¯”ï¼Œè¯„ä¼°äº†å„æµ‹è¯•æ–¹æ³•åœ¨å¤„ç†ç‰¹å®šé‡‘èæ–‡æœ¬æ—¶çš„å…·ä½“æ•ˆç›Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLarge Language Models åœ¨ç»å¤§å¤šæ•°æµ‹è¯•æ¡ˆä¾‹ä¸­å‡å±•ç°å‡ºä¼˜äºä¼ ç»Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶åœ¨é‡‘èæƒ…ç»ªåˆ†æä»»åŠ¡ä¸­çš„å…ˆè¿›æ€§ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15929v1",
      "published_date": "2025-10-03 14:10:51 UTC",
      "updated_date": "2025-10-03 14:10:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:48.954761+00:00"
    },
    {
      "arxiv_id": "2510.03029v1",
      "title": "Investigating The Smells of LLM Generated Code",
      "title_zh": "LLM ç”Ÿæˆä»£ç çš„ä»£ç å¼‚å‘³æ¢ç©¶",
      "authors": [
        "Debalina Ghosh Paul",
        "Hong Zhu",
        "Ian Bayley"
      ],
      "abstract": "Context: Large Language Models (LLMs) are increasingly being used to generate program code. Much research has been reported on the functional correctness of generated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating the quality of LLM-generated code to identify the weakest scenarios in which the quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code quality, and compares them with a baseline formed from reference solutions of professionally written code. The test dataset is divided into various subsets according to the topics of the code and complexity of the coding tasks to represent different scenarios of using LLMs for code generation. We will also present an automated test system for this purpose and report experiments with the Java programs generated in response to prompts given to four state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code smells compared to reference solutions. Falcon performed the least badly, with a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%) and finally Codex (84.97%). The average smell increase across all LLMs was 63.34%, comprising 73.35% for implementation smells and 21.42% for design smells. We also found that the increase in code smells is greater for more complex coding tasks and for more advanced topics, such as those involving object-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding task complexities and topics are highly correlated to the quality of human written code in the corresponding scenarios. However, the quality of LLM generated code is noticeably poorer than human written code.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„ä»£ç è´¨é‡ï¼Œé‡ç‚¹åˆ†æäº†ä»£ç å¼‚å‘³(code smells)è¿™ä¸€è¡¡é‡ä»£ç è´¨é‡çš„å…³é”®æŒ‡æ ‡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºåœºæ™¯çš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµ‹è¯•ç³»ç»Ÿå¯¹æ¯”äº† Gemini Proã€ChatGPTã€Codex å’Œ Falcon ç”Ÿæˆçš„ Java ç¨‹åºä¸ä¸“ä¸šç¼–å†™çš„å‚è€ƒæ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLM ç”Ÿæˆçš„ä»£ç åœ¨ä»£ç å¼‚å‘³å‘ç”Ÿç‡ä¸Šå¹³å‡æ¯”å‚è€ƒæ–¹æ¡ˆé«˜å‡º 63.34%ï¼Œå…¶ä¸­å®ç°å¼‚å‘³(implementation smells)çš„å¢åŠ ï¼ˆ73.35%ï¼‰è¿œé«˜äºè®¾è®¡å¼‚å‘³(design smells)ï¼ˆ21.42%ï¼‰ã€‚åœ¨å‚è¯„æ¨¡å‹ä¸­ï¼ŒFalcon çš„è¡¨ç°ç›¸å¯¹æœ€å¥½ï¼Œè€Œ Codex çš„ä»£ç è´¨é‡æœ€å·®ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œéšç€ä»»åŠ¡å¤æ‚åº¦çš„æå‡ä»¥åŠæ¶‰åŠé¢å‘å¯¹è±¡(object-orientated)ç­‰é«˜çº§æ¦‚å¿µæ—¶ï¼Œä»£ç å¼‚å‘³çš„å¢åŠ æ›´åŠ æ˜¾è‘—ã€‚æœ€ç»ˆç»“è®ºè¡¨æ˜ï¼Œè™½ç„¶ LLM åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ä¸äººå·¥ç¼–å†™ä»£ç çš„è´¨é‡é«˜åº¦ç›¸å…³ï¼Œä½†å…¶ç”Ÿæˆçš„ä»£ç è´¨é‡ä»æ˜æ˜¾ä½äºäººç±»ä¸“ä¸šæ°´å¹³ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚ç¼–ç¨‹åœºæ™¯æ—¶ä»éœ€æ”¹è¿›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03029v1",
      "published_date": "2025-10-03 14:09:55 UTC",
      "updated_date": "2025-10-03 14:09:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:34:44.850895+00:00"
    },
    {
      "arxiv_id": "2510.06245v1",
      "title": "DynBenchmark: Customizable Ground Truths to Benchmark Community Detection and Tracking in Temporal Networks",
      "title_zh": "DynBenchmarkï¼šç”¨äºè¯„ä¼°æ—¶åºç½‘ç»œç¤¾åŒºæ£€æµ‹ä¸è·Ÿè¸ªçš„å¯å®šåˆ¶åŸºå‡†çœŸå€¼",
      "authors": [
        "Laurent Brisson",
        "CÃ©cile Bothorel",
        "Nicolas Duminy"
      ],
      "abstract": "Graph models help understand network dynamics and evolution. Creating graphs with controlled topology and embedded partitions is a common strategy for evaluating community detection algorithms. However, existing benchmarks often overlook the need to track the evolution of communities in real-world networks. To address this, a new community-centered model is proposed to generate customizable evolving community structures where communities can grow, shrink, merge, split, appear or disappear. This benchmark also generates the underlying temporal network, where nodes can appear, disappear, or move between communities. The benchmark has been used to test three methods, measuring their performance in tracking nodes' cluster membership and detecting community evolution. Python libraries, drawing utilities, and validation metrics are provided to compare ground truth with algorithm results for detecting dynamic communities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DynBenchmarkï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ—¶åºç½‘ç»œ (Temporal Networks) ä¸­ç¤¾åŒºå‘ç° (Community Detection) ä¸è¿½è¸ªæ€§èƒ½çš„å¯å®šåˆ¶åŒ–åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€å¿½è§†çœŸå®ç½‘ç»œä¸­ç¤¾åŒºæ¼”åŒ–è¿½è¸ªéœ€æ±‚çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§ä»¥ç¤¾åŒºä¸ºä¸­å¿ƒçš„æ–°æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯å®šåˆ¶çš„æ¼”åŒ–ç¤¾åŒºç»“æ„ï¼Œæ”¯æŒç¤¾åŒºçš„å¢é•¿ (Grow)ã€æ”¶ç¼© (Shrink)ã€åˆå¹¶ (Merge)ã€åˆ†è£‚ (Split) ä»¥åŠå‡ºç°æˆ–æ¶ˆå¤±ç­‰å¤šç§åŠ¨æ€å˜åŒ–ã€‚åŒæ—¶ï¼Œå®ƒè¿˜èƒ½ç”Ÿæˆåº•å±‚æ—¶åºç½‘ç»œï¼Œå…è®¸èŠ‚ç‚¹åœ¨ç¤¾åŒºé—´ç§»åŠ¨æˆ–å¢å‡ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥åŸºå‡†æµ‹è¯•è¯„ä¼°äº†ä¸‰ç§æ–¹æ³•åœ¨è¿½è¸ªèŠ‚ç‚¹é›†ç¾¤æˆå‘˜èº«ä»½å’Œæ£€æµ‹ç¤¾åŒºæ¼”åŒ–æ–¹é¢çš„è¡¨ç°ï¼Œå¹¶é…å¥—æä¾›äº† Python åº“ã€ç»˜å›¾å·¥å…·å’ŒéªŒè¯æŒ‡æ ‡ã€‚è¯¥å·¥å…·èƒ½å¤Ÿæœ‰æ•ˆå¯¹æ¯” Ground Truth ä¸ç®—æ³•æ£€æµ‹ç»“æœï¼Œä¸ºåŠ¨æ€ç¤¾åŒºå‘ç°ç ”ç©¶æä¾›äº†å¯é çš„æ€§èƒ½è¯„ä¼°æ‰‹æ®µã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.06245v1",
      "published_date": "2025-10-03 14:02:22 UTC",
      "updated_date": "2025-10-03 14:02:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:00.354262+00:00"
    },
    {
      "arxiv_id": "2510.03016v2",
      "title": "Learning Robust Diffusion Models from Imprecise Supervision",
      "title_zh": "åŸºäºä¸ç²¾ç¡®ç›‘ç£çš„é²æ£’æ‰©æ•£æ¨¡å‹å­¦ä¹ ",
      "authors": [
        "Dong-Dong Wu",
        "Jiacheng Cui",
        "Wei Wang",
        "Zhiqiang Shen",
        "Masashi Sugiyama"
      ],
      "abstract": "Conditional diffusion models have achieved remarkable success in various generative tasks recently, but their training typically relies on large-scale datasets that inevitably contain imprecise information in conditional inputs. Such supervision, often stemming from noisy, ambiguous, or incomplete labels, will cause condition mismatch and degrade generation quality. To address this challenge, we propose DMIS, a unified framework for training robust Diffusion Models from Imprecise Supervision, which is the first systematic study within diffusion models. Our framework is derived from likelihood maximization and decomposes the objective into generative and classification components: the generative component models imprecise-label distributions, while the classification component leverages a diffusion classifier to infer class-posterior probabilities, with its efficiency further improved by an optimized timestep sampling strategy. Extensive experiments on diverse forms of imprecise supervision, covering tasks of image generation, weakly supervised learning, and noisy dataset condensation demonstrate that DMIS consistently produces high-quality and class-discriminative samples.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DMISï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»ä¸ç²¾ç¡®ç›‘ç£ (Imprecise Supervision) ä¸­è®­ç»ƒé²æ£’æ‰©æ•£æ¨¡å‹ (Diffusion Models) çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè§£å†³äº†å¤§è§„æ¨¡æ•°æ®é›†ä¸­å™ªå£°æˆ–æ¨¡ç³Šæ ‡ç­¾å¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºä¼¼ç„¶æœ€å¤§åŒ– (Likelihood Maximization) åŸç†ï¼Œå°†è®­ç»ƒç›®æ ‡åˆ†è§£ä¸ºç”Ÿæˆç»„ä»¶å’Œåˆ†ç±»ç»„ä»¶ï¼šç”Ÿæˆç»„ä»¶è´Ÿè´£å¯¹ä¸ç²¾ç¡®æ ‡ç­¾åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œè€Œåˆ†ç±»ç»„ä»¶åˆ©ç”¨æ‰©æ•£åˆ†ç±»å™¨ (Diffusion Classifier) æ¨æ–­ç±»åˆ«åéªŒæ¦‚ç‡ã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¼˜åŒ–çš„æ—¶é—´æ­¥é‡‡æ · (Timestep Sampling) ç­–ç•¥ã€‚åœ¨å›¾åƒç”Ÿæˆã€å¼±ç›‘ç£å­¦ä¹ å’Œå™ªå£°æ•°æ®é›†å‹ç¼©ç­‰å¤šç§ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDMIS èƒ½å¤ŸæŒç»­ç”Ÿæˆé«˜è´¨é‡ä¸”å…·æœ‰ç±»åˆ«è¾¨è¯†æ€§çš„æ ·æœ¬ã€‚è¯¥ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹åœ¨é¢å¯¹ä¸å®Œå–„æ•°æ®æ—¶çš„ç¨³å¥è®­ç»ƒæä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03016v2",
      "published_date": "2025-10-03 14:00:32 UTC",
      "updated_date": "2025-10-10 08:42:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:04.255332+00:00"
    },
    {
      "arxiv_id": "2510.03377v1",
      "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints",
      "title_zh": "é¢å‘å¸¦é˜»å¡çº¦æŸèƒ½æºæ„ŸçŸ¥æ··åˆæµæ°´è½¦é—´è°ƒåº¦çš„æ”¹è¿›è¿­ä»£å¸•ç´¯æ‰˜è´ªå©ªç®—æ³•",
      "authors": [
        "Ahmed Missaoui",
        "Cemalettin Ozturk",
        "Barry O'Sullivan"
      ],
      "abstract": "The scarcity of non-renewable energy sources, geopolitical problems in its supply, increasing prices, and the impact of climate change, force the global economy to develop more energy-efficient solutions for their operations. The Manufacturing sector is not excluded from this challenge as one of the largest consumers of energy. Energy-efficient scheduling is a method that attracts manufacturing companies to reduce their consumption as it can be quickly deployed and can show impact immediately. In this study, the hybrid flow shop scheduling problem with blocking constraint (BHFS) is investigated in which we seek to minimize the latest completion time (i.e. makespan) and overall energy consumption, a typical manufacturing setting across many industries from automotive to pharmaceutical. Energy consumption and the latest completion time of customer orders are usually conflicting objectives. Therefore, we first formulate the problem as a novel multi-objective mixed integer programming (MIP) model and propose an augmented epsilon-constraint method for finding the Pareto-optimal solutions. Also, an effective multi-objective metaheuristic algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large instances in reasonable time. Our proposed methods are benchmarked using small, medium, and large-size instances to evaluate their efficiency. Two well-known algorithms are adopted for comparing our novel approaches. The computational results show the effectiveness of our method.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å…·æœ‰é˜»å¡çº¦æŸçš„æ··åˆæµæ°´è½¦é—´è°ƒåº¦é—®é¢˜(BHFS)ï¼Œæ¢è®¨äº†å¦‚ä½•åœ¨ä¿è¯ç”Ÿäº§æ•ˆç‡çš„åŒæ—¶é™ä½èƒ½æºæ¶ˆè€—ï¼Œæ—¨åœ¨åŒæ—¶æœ€å°åŒ–å®Œå·¥æ—¶é—´(makespan)å’Œæ€»èƒ½è€—ã€‚ç ”ç©¶é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–°å‹çš„å¤šç›®æ ‡æ··åˆæ•´æ•°è§„åˆ’(MIP)æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨å¢å¼ºå‹epsilon-çº¦æŸæ–¹æ³•(augmented epsilon-constraint method)æ¥å¯»æ‰¾Paretoæœ€ä¼˜è§£ã€‚ä¸ºäº†åœ¨åˆç†æ—¶é—´å†…è§£å†³å¤§è§„æ¨¡ç”Ÿäº§è°ƒåº¦é—®é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§åä¸ºç²¾ç‚¼è¿­ä»£å¸•ç´¯æ‰˜è´ªå©ª(Refined Iterated Pareto Greedy, RIPG)çš„æœ‰æ•ˆå¤šç›®æ ‡å…ƒå¯å‘å¼ç®—æ³•ã€‚é€šè¿‡å¯¹å°ã€ä¸­ã€å¤§å‹ä¸åŒè§„æ¨¡çš„å®ä¾‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶å°†æ‰€ææ–¹æ³•ä¸ä¸¤ç§çŸ¥åç®—æ³•è¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—ç»“æœå……åˆ†éªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡æ•ˆç‡å’Œé™ä½èƒ½è€—æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶æˆæœå¯å¿«é€Ÿåº”ç”¨äºæ±½è½¦ã€åˆ¶è¯ç­‰å¤šä¸ªå·¥ä¸šé¢†åŸŸï¼Œä¸ºå®ç°åˆ¶é€ ä¸šçš„èŠ‚èƒ½å‡æ’å’Œå¯æŒç»­å‘å±•æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03377v1",
      "published_date": "2025-10-03 13:52:20 UTC",
      "updated_date": "2025-10-03 13:52:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:12.053173+00:00"
    },
    {
      "arxiv_id": "2510.03004v1",
      "title": "BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia",
      "title_zh": "BrainIB++ï¼šåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œä¸ä¿¡æ¯ç“¶é¢ˆè¯†åˆ«ç²¾ç¥åˆ†è£‚ç—‡åŠŸèƒ½æ€§è„‘ç”Ÿç‰©æ ‡å¿—ç‰©",
      "authors": [
        "Tianzheng Hu",
        "Qiang Li",
        "Shu Liu",
        "Vince D. Calhoun",
        "Guido van Wingen",
        "Shujian Yu"
      ],
      "abstract": "The development of diagnostic models is gaining traction in the field of psychiatric disorders. Recently, machine learning classifiers based on resting-state functional magnetic resonance imaging (rs-fMRI) have been developed to identify brain biomarkers that differentiate psychiatric disorders from healthy controls. However, conventional machine learning-based diagnostic models often depend on extensive feature engineering, which introduces bias through manual intervention. While deep learning models are expected to operate without manual involvement, their lack of interpretability poses significant challenges in obtaining explainable and reliable brain biomarkers to support diagnostic decisions, ultimately limiting their clinical applicability. In this study, we introduce an end-to-end innovative graph neural network framework named BrainIB++, which applies the information bottleneck (IB) principle to identify the most informative data-driven brain regions as subgraphs during model training for interpretation. We evaluate the performance of our model against nine established brain network classification methods across three multi-cohort schizophrenia datasets. It consistently demonstrates superior diagnostic accuracy and exhibits generalizability to unseen data. Furthermore, the subgraphs identified by our model also correspond with established clinical biomarkers in schizophrenia, particularly emphasizing abnormalities in the visual, sensorimotor, and higher cognition brain functional network. This alignment enhances the model's interpretability and underscores its relevance for real-world diagnostic applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç²¾ç¥åˆ†è£‚ç—‡(Schizophrenia)è¯Šæ–­ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ è¿‡åº¦ä¾èµ–ç‰¹å¾å·¥ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†BrainIB++æ¡†æ¶ã€‚BrainIB++æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNN)æ¡†æ¶ï¼Œåˆ›æ–°æ€§åœ°åº”ç”¨äº†ä¿¡æ¯ç“¶é¢ˆ(Information Bottleneck, IB)åŸåˆ™ï¼Œèƒ½å¤Ÿåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è¯†åˆ«å‡ºæœ€å…·ä¿¡æ¯é‡çš„è„‘åŒºä½œä¸ºå­å›¾(subgraphs)ã€‚ç ”ç©¶äººå‘˜åœ¨ä¸‰ä¸ªå¤šä¸­å¿ƒç²¾ç¥åˆ†è£‚ç—‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºBrainIB++åœ¨è¯Šæ–­å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æ˜¾è‘—ä¼˜äºä¹ç§ç°æœ‰çš„è„‘ç½‘ç»œåˆ†ç±»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¯†åˆ«å‡ºçš„å­å›¾ä¸ä¸´åºŠå·²çŸ¥çš„ç”Ÿç‰©æ ‡å¿—ç‰©é«˜åº¦ä¸€è‡´ï¼Œç‰¹åˆ«æ­ç¤ºäº†è§†è§‰ã€æ„Ÿè§‰è¿åŠ¨å’Œé«˜çº§è®¤çŸ¥è„‘åŠŸèƒ½ç½‘ç»œä¸­çš„å¼‚å¸¸ã€‚è¿™ç§é«˜åº¦çš„ä¸€è‡´æ€§å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸å¯é æ€§ï¼Œä¸ºç²¾ç¥åˆ†è£‚ç—‡çš„ä¸´åºŠè¾…åŠ©è¯Šæ–­æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This manuscript has been accepted by Biomedical Signal Processing and Control and the code is available at https://github.com/TianzhengHU/BrainIB_coding/tree/main/BrainIB_GIB",
      "pdf_url": "https://arxiv.org/pdf/2510.03004v1",
      "published_date": "2025-10-03 13:48:15 UTC",
      "updated_date": "2025-10-03 13:48:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:09.961630+00:00"
    },
    {
      "arxiv_id": "2510.03003v1",
      "title": "From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime",
      "title_zh": "ä»é«˜é¢‘ä¼ æ„Ÿå™¨åˆ°åˆæŠ¥ï¼šåŸºäºè¿ç§»å­¦ä¹ çš„æµ·è¿è½´åŠŸç‡é¢„æµ‹",
      "authors": [
        "Akriti Sharma",
        "Dogan Altan",
        "Dusica Marijan",
        "ArnbjÃ¸rn Maressa"
      ],
      "abstract": "With the growth of global maritime transportation, energy optimization has become crucial for reducing costs and ensuring operational efficiency. Shaft power is the mechanical power transmitted from the engine to the shaft and directly impacts fuel consumption, making its accurate prediction a paramount step in optimizing vessel performance. Power consumption is highly correlated with ship parameters such as speed and shaft rotation per minute, as well as weather and sea conditions. Frequent access to this operational data can improve prediction accuracy. However, obtaining high-quality sensor data is often infeasible and costly, making alternative sources such as noon reports a viable option. In this paper, we propose a transfer learning-based approach for predicting vessels shaft power, where a model is initially trained on high-frequency data from a vessel and then fine-tuned with low-frequency daily noon reports from other vessels. We tested our approach on sister vessels (identical dimensions and configurations), a similar vessel (slightly larger with a different engine), and a different vessel (distinct dimensions and configurations). The experiments showed that the mean absolute percentage error decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel, and 5.3 percent for a different vessel, compared to the model trained solely on noon report data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒæµ·è¿èƒ½æ•ˆä¼˜åŒ–ä¸­è½´åŠŸç‡(shaft power)é¢„æµ‹çš„å…³é”®ä½œç”¨ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¿ç§»å­¦ä¹ (transfer learning)çš„é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é«˜è´¨é‡é«˜é¢‘ä¼ æ„Ÿå™¨æ•°æ®è·å–æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ™®åŠçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆåœ¨ç‰¹å®šèˆ¹èˆ¶çš„é«˜é¢‘æ•°æ®ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œéšååˆ©ç”¨å…¶ä»–èˆ¹èˆ¶æ˜“äºè·å–çš„ä½é¢‘æ­£åˆæŠ¥å‘Š(noon reports)æ•°æ®è¿›è¡Œå¾®è°ƒ(fine-tuning)ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å§Šå¦¹èˆ¹(sister vessels)ã€ç›¸ä¼¼è§„æ ¼èˆ¹èˆ¶(similar vessel)ä»¥åŠå®Œå…¨ä¸åŒè§„æ ¼çš„èˆ¹èˆ¶(different vessel)ä¸Šè¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»…ä¾èµ–æ­£åˆæŠ¥å‘Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥è¿ç§»å­¦ä¹ æ–¹æ³•ä½¿å§Šå¦¹èˆ¹çš„å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®(MAPE)é™ä½äº†10.6%ï¼Œç›¸ä¼¼èˆ¹é™ä½äº†3.6%ï¼Œä¸åŒè§„æ ¼èˆ¹èˆ¶é™ä½äº†5.3%ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é€šè¿‡è·¨èˆ¹èˆ¶ã€è·¨é¢‘ç‡çš„æ•°æ®çŸ¥è¯†è¿ç§»ï¼Œå¯ä»¥æ˜¾è‘—æå‡èˆ¹èˆ¶èƒ½è€—é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œä¸ºèˆªè¿ä¸šå®ç°ä½æˆæœ¬çš„æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Keywords: transfer learning, shaft power prediction, noon reports, sensor data, maritime",
      "pdf_url": "https://arxiv.org/pdf/2510.03003v1",
      "published_date": "2025-10-03 13:47:16 UTC",
      "updated_date": "2025-10-03 13:47:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:14.446125+00:00"
    },
    {
      "arxiv_id": "2510.02999v2",
      "title": "Untargeted Jailbreak Attack",
      "title_zh": "éå®šå‘è¶Šç‹±æ”»å‡»",
      "authors": [
        "Xinzhe Huang",
        "Wenjing Hu",
        "Tianhang Zheng",
        "Kedong Xiu",
        "Xiaojun Jia",
        "Di Wang",
        "Zhan Qin",
        "Kui Ren"
      ],
      "abstract": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs), such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize adversarial suffixes to align the LLM output with a predefined target response. However, by restricting the optimization objective as inducing a predefined target, these methods inherently constrain the adversarial search space, which limit their overall attack efficacy. Furthermore, existing methods typically require a large number of optimization iterations to fulfill the large gap between the fixed target and the original model response, resulting in low attack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an unsafe response without enforcing any predefined patterns. Specifically, we formulate an untargeted attack objective to maximize the unsafety probability of the LLM response, which can be quantified using a judge model. Since the objective is non-differentiable, we further decompose it into two differentiable sub-objectives for optimizing an optimal harmful response and the corresponding adversarial prompt, with a theoretical analysis to validate the decomposition. In contrast to targeted jailbreak attacks, UJA's unrestricted objective significantly expands the search space, enabling a more flexible and efficient exploration of LLM vulnerabilities.Extensive evaluations demonstrate that UJA can achieve over 80% attack success rates against recent safety-aligned LLMs with only 100 optimization iterations, outperforming the state-of-the-art gradient-based attacks such as I-GCG and COLD-Attack by over 20%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºæ¢¯åº¦çš„è¶Šç‹±æ”»å‡»ï¼ˆå¦‚ GCG å’Œ COLD-Attackï¼‰å› é¢„å®šä¹‰ç›®æ ‡å“åº”è€Œå¯¼è‡´æœç´¢ç©ºé—´å—é™ä¸”æ”»å‡»æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºæ¢¯åº¦çš„æ— ç›®æ ‡è¶Šç‹±æ”»å‡»ï¼ˆUntargeted Jailbreak Attack, UJAï¼‰ã€‚UJA æ—¨åœ¨è¯±å¯¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº§ç”Ÿä»»ä½•ä¸å®‰å…¨å“åº”ï¼Œè€Œä¸å¼ºåŠ ä»»ä½•é¢„å®šä¹‰æ¨¡å¼ï¼Œä»è€Œæ˜¾è‘—æ‰©å±•äº†æ¼æ´æœç´¢ç©ºé—´ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¸å¯å¾®çš„æ”»å‡»ç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªå¯å¾®å­ç›®æ ‡ï¼Œå®ç°äº†å¯¹æœ‰å®³å“åº”å’Œå¯¹æŠ—æ€§æç¤ºçš„ååŒä¼˜åŒ–ï¼Œå¹¶æä¾›äº†ç†è®ºè¯æ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUJA åœ¨ä»… 100 æ¬¡ä¼˜åŒ–è¿­ä»£ä¸‹å³å¯åœ¨å®‰å…¨å¯¹é½çš„ LLMs ä¸Šè¾¾åˆ°è¶…è¿‡ 80% çš„æ”»å‡»æˆåŠŸç‡ã€‚ç›¸æ¯” I-GCG å’Œ COLD-Attack ç­‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒUJA çš„æˆåŠŸç‡æå‡äº† 20% ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨æ¢ç´¢æ¨¡å‹è„†å¼±æ€§æ–¹é¢å…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02999v2",
      "published_date": "2025-10-03 13:38:56 UTC",
      "updated_date": "2025-10-28 08:06:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:15.749817+00:00"
    },
    {
      "arxiv_id": "2510.02996v1",
      "title": "Onto-Epistemological Analysis of AI Explanations",
      "title_zh": "äººå·¥æ™ºèƒ½è§£é‡Šçš„æœ¬ä½“-è®¤è¯†è®ºåˆ†æ",
      "authors": [
        "Martina Mattioli",
        "Eike Petersen",
        "Aasa Feragen",
        "Marcello Pelillo",
        "Siavash A. Bigdeli"
      ],
      "abstract": "Artificial intelligence (AI) is being applied in almost every field. At the same time, the currently dominant deep learning methods are fundamentally black-box systems that lack explanations for their inferences, significantly limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim to overcome this challenge by providing explanations of the models' decision process. Such methods are often proposed and developed by engineers and scientists with a predominantly technical background and incorporate their assumptions about the existence, validity, and explanatory utility of different conceivable explanatory mechanisms. However, the basic concept of an explanation -- what it is, whether we can know it, whether it is absolute or relative -- is far from trivial and has been the subject of deep philosophical debate for millennia. As we point out here, the assumptions incorporated into different XAI methods are not harmless and have important consequences for the validity and interpretation of AI explanations in different domains. We investigate ontological and epistemological assumptions in explainability methods when they are applied to AI systems, meaning the assumptions we make about the existence of explanations and our ability to gain knowledge about those explanations. Our analysis shows how seemingly small technical changes to an XAI method may correspond to important differences in the underlying assumptions about explanations. We furthermore highlight the risks of ignoring the underlying onto-epistemological paradigm when choosing an XAI method for a given application, and we discuss how to select and adapt appropriate XAI methods for different domains of application.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹äººå·¥æ™ºèƒ½è§£é‡Š(AI Explanations)è¿›è¡Œäº†æœ¬ä½“è®ºä¸è®¤è¯†è®º(Onto-Epistemological)åˆ†æï¼Œæ—¨åœ¨æ¢è®¨æ·±åº¦å­¦ä¹ æ¨¡å‹é»‘ç›’ç‰¹æ€§å¸¦æ¥çš„å¯ä¿¡åº¦æŒ‘æˆ˜ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ–¹æ³•å¤§å¤šåŸºäºå¼€å‘è€…çš„æŠ€æœ¯å‡è®¾ï¼Œè€Œå¿½è§†äº†å“²å­¦é¢†åŸŸå…³äºâ€œè§£é‡Šâ€æœ¬è´¨çš„é•¿æœŸè¾©è®ºã€‚æœ¬æ–‡æ·±å…¥è°ƒæŸ¥äº†XAIæ–¹æ³•åœ¨è§£é‡Šçš„å­˜åœ¨æ€§åŠçŸ¥è¯†è·å–èƒ½åŠ›æ–¹é¢çš„åº•å±‚å‡è®¾ï¼Œæ­ç¤ºäº†æŠ€æœ¯å±‚é¢çš„å¾®å°å˜åŠ¨å¦‚ä½•å¯¹åº”äºè§£é‡Šå‡è®¾çš„é‡å¤§å·®å¼‚ã€‚ç ”ç©¶å¼ºè°ƒäº†åœ¨ç‰¹å®šé¢†åŸŸåº”ç”¨ä¸­å¿½è§†æœ¬ä½“è®ºä¸è®¤è¯†è®ºèŒƒå¼å¯èƒ½å¯¼è‡´çš„æ•ˆåŠ›ä¸è§£é‡Šé£é™©ã€‚æœ€åï¼Œè¯¥ç ”ç©¶ä¸ºä¸åŒåº”ç”¨åœºæ™¯ä¸‹å¦‚ä½•é€‰æ‹©å’Œæ”¹è¿›é€‚å½“çš„XAIæ–¹æ³•æä¾›äº†ç†è®ºæŒ‡å¯¼ä¸å»ºè®®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02996v1",
      "published_date": "2025-10-03 13:36:57 UTC",
      "updated_date": "2025-10-03 13:36:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:18.643524+00:00"
    },
    {
      "arxiv_id": "2510.02978v1",
      "title": "AI Generated Child Sexual Abuse Material -- What's the Harm?",
      "title_zh": "AIç”Ÿæˆçš„å„¿ç«¥æ€§è™å¾…ææ–™ï¼šå±å®³ä½•åœ¨ï¼Ÿ",
      "authors": [
        "Caoilte Ã“ Ciardha",
        "John Buckley",
        "Rebecca S. Portnoff"
      ],
      "abstract": "The development of generative artificial intelligence (AI) tools capable of producing wholly or partially synthetic child sexual abuse material (AI CSAM) presents profound challenges for child protection, law enforcement, and societal responses to child exploitation. While some argue that the harmfulness of AI CSAM differs fundamentally from other CSAM due to a perceived absence of direct victimization, this perspective fails to account for the range of risks associated with its production and consumption. AI has been implicated in the creation of synthetic CSAM of children who have not previously been abused, the revictimization of known survivors of abuse, the facilitation of grooming, coercion and sexual extortion, and the normalization of child sexual exploitation. Additionally, AI CSAM may serve as a new or enhanced pathway into offending by lowering barriers to engagement, desensitizing users to progressively extreme content, and undermining protective factors for individuals with a sexual interest in children. This paper provides a primer on some key technologies, critically examines the harms associated with AI CSAM, and cautions against claims that it may function as a harm reduction tool, emphasizing how some appeals to harmlessness obscure its real risks and may contribute to inertia in ecosystem responses.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) äº§ç”Ÿçš„å…¨åˆæˆæˆ–éƒ¨åˆ†åˆæˆå„¿ç«¥æ€§è™å¾…ææ–™ (AI CSAM) å¯¹å„¿ç«¥ä¿æŠ¤ã€æ‰§æ³•å’Œç¤¾ä¼šåº”å¯¹æå‡ºçš„ä¸¥å³»æŒ‘æˆ˜ã€‚è®ºæ–‡æ‰¹åˆ¤æ€§åœ°åé©³äº†ç”±äºæ„ŸçŸ¥åˆ°çš„â€œç¼ºä¹ç›´æ¥å—å®³è€…â€è€Œè®¤ä¸º AI CSAM å±å®³è¾ƒå°çš„è§‚ç‚¹ï¼ŒæŒ‡å‡º AI CSAM ä¸ä»…ä¼šå¯¼è‡´å·²çŸ¥å¹¸å­˜è€…çš„äºŒæ¬¡å—å®³ (revictimization)ï¼Œè¿˜ä¼šä¿ƒè¿›è¯±æ‹ (grooming)ã€èƒè¿«åŠæ€§å‹’ç´¢ï¼Œå¹¶ä½¿å„¿ç«¥æ€§å‰¥å‰Šç°è±¡å¸¸æ€åŒ– (normalization)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼ºè°ƒ AI CSAM é€šè¿‡é™ä½å‚ä¸é—¨æ§›å’Œä½¿ç”¨æˆ·è„±æ• (desensitizing)ï¼Œå¯èƒ½æˆä¸ºè¯±å‘æ€§çŠ¯ç½ªçš„æ–°è·¯å¾„ã€‚è®ºæ–‡è­¦å‘Šç§°ï¼Œå°† AI CSAM è§†ä¸ºâ€œå‡å°‘ä¼¤å®³å·¥å…· (harm reduction tool)â€çš„è¯´æ³•æ©ç›–äº†å…¶çœŸå®é£é™©ï¼Œå¹¶å¯èƒ½å¯¼è‡´ç¤¾ä¼šå’Œç”Ÿæ€ç³»ç»Ÿå“åº”çš„æ»åã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02978v1",
      "published_date": "2025-10-03 13:11:28 UTC",
      "updated_date": "2025-10-03 13:11:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:28.752545+00:00"
    },
    {
      "arxiv_id": "2510.02973v1",
      "title": "Corrosion Risk Estimation for Heritage Preservation: An Internet of Things and Machine Learning Approach Using Temperature and Humidity",
      "title_zh": "æ–‡åŒ–é—äº§ä¿æŠ¤ä¸­çš„è…èš€é£é™©è¯„ä¼°ï¼šèåˆæ¸©æ¹¿åº¦æ•°æ®çš„ç‰©è”ç½‘ä¸æœºå™¨å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Reginald Juan M. Mercado",
        "Muhammad Kabeer",
        "Haider Al-Obaidy",
        "Rosdiadee Nordin"
      ],
      "abstract": "Proactive preservation of steel structures at culturally significant heritage sites like the San Sebastian Basilica in the Philippines requires accurate corrosion forecasting. This study developed an Internet of Things hardware system connected with LoRa wireless communications to monitor heritage buildings with steel structures. From a three year dataset generated by the IoT system, we built a machine learning framework for predicting atmospheric corrosion rates using only temperature and relative humidity data. Deployed via a Streamlit dashboard with ngrok tunneling for public access, the framework provides real-time corrosion monitoring and actionable preservation recommendations. This minimal-data approach is scalable and cost effective for heritage sites with limited monitoring resources, showing that advanced regression can extract accurate corrosion predictions from basic meteorological data enabling proactive preservation of culturally significant structures worldwide without requiring extensive sensor networks",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰æ–‡åŒ–ä»·å€¼çš„é—äº§å»ºç­‘é’¢ç»“æ„ä¿æŠ¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºInternet of Thingså’ŒMachine Learningçš„è…èš€é£é™©è¯„ä¼°æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†é›†æˆLoRaæ— çº¿é€šä¿¡çš„ç¡¬ä»¶ç³»ç»Ÿï¼Œåˆ©ç”¨é•¿è¾¾ä¸‰å¹´çš„ç›‘æµ‹æ•°æ®é›†æ„å»ºäº†ä»…ä¾èµ–æ¸©åº¦å’Œç›¸å¯¹æ¹¿åº¦çš„é¢„æµ‹æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡Streamlitä»ªè¡¨ç›˜å’Œngrokéš§é“å®ç°äº†å®æ—¶è…èš€ç›‘æµ‹ä¸ä¿å­˜å»ºè®®çš„å…¬å¼€å‘å¸ƒã€‚è¿™ç§æç®€æ•°æ®(minimal-data)çš„æ–¹æ³•æå…·æˆæœ¬æ•ˆç›Šå’Œå¯æ‰©å±•æ€§ï¼Œè¯æ˜äº†é«˜çº§å›å½’ç®—æ³•èƒ½å¤Ÿä»åŸºç¡€æ°”è±¡æ•°æ®ä¸­æå–å‡†ç¡®çš„è…èš€é¢„æµ‹ã€‚è¯¥æˆæœä¸ºå…¨çƒèµ„æºæœ‰é™çš„æ–‡åŒ–é—äº§åœ°æä¾›äº†æ— éœ€å¤§è§„æ¨¡ä¼ æ„Ÿå™¨ç½‘ç»œå³å¯å®ç°çš„ä¸»åŠ¨ä¿æŠ¤æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.NI",
        "eess.SY"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.02973v1",
      "published_date": "2025-10-03 13:07:39 UTC",
      "updated_date": "2025-10-03 13:07:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:23.156850+00:00"
    },
    {
      "arxiv_id": "2510.02967v3",
      "title": "Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè¯æ®ä¸­çš„è½åœ°ï¼šä¸€ç§ç”¨äºæŸ¥è¯¢è‹±å›½ NICE ä¸´åºŠæŒ‡å—çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ",
      "authors": [
        "Matthew Lewis",
        "Samuel Thio",
        "Amy Roberts",
        "Catherine Siju",
        "Whoasif Mukit",
        "Rebecca Kuruvilla",
        "Zhangshu Joshua Jiang",
        "Niko MÃ¶ller-Grell",
        "Aditya Borakati",
        "Richard JB Dobson",
        "Spiros Denaxas"
      ],
      "abstract": "This paper presents the development and evaluation of a Retrieval-Augmented Generation (RAG) system for querying the United Kingdom's National Institute for Health and Care Excellence (NICE) clinical guidelines using Large Language Models (LLMs). The extensive length and volume of these guidelines can impede their utilisation within a time-constrained healthcare system, a challenge this project addresses through the creation of a system capable of providing users with precisely matched information in response to natural language queries. The system's retrieval architecture, composed of a hybrid embedding mechanism, was evaluated against a corpus of 10,195 text chunks derived from three hundred guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR) of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten retrieved chunks, when evaluated on 7901 queries. The most significant impact of the RAG system was observed during the generation phase. When evaluated on a manually curated dataset of seventy question-answer pairs, RAG-enhanced models showed substantial gains in performance. Faithfulness, the measure of whether an answer is supported by the source text, was increased by 64.7 percentage points to 99.5% for the RAG-enhanced O4-Mini model and significantly outperformed the medical-focused Meditron3-8B LLM, which scored 43%. Clinical evaluation by seven Subject Matter Experts (SMEs) further validated these findings, with GPT-4.1 achieving 98.7% accuracy while reducing unsafe responses by 67% compared to O4-Mini (from 3.0 to 1.0 per evaluator). This study thus establishes RAG as an effective, reliable, and scalable approach for applying generative AI in healthcare, enabling cost-effective access to medical guidelines.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸€å¥—é’ˆå¯¹è‹±å›½å›½å®¶å«ç”Ÿä¸ä¸´åºŠä¼˜åŒ–ç ”ç©¶æ‰€ (NICE) ä¸´åºŠæŒ‡å—çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³åŒ»ç–—ä»ä¸šè€…åœ¨æ—¶é—´å—é™çš„æƒ…å†µä¸‹éš¾ä»¥é«˜æ•ˆæŸ¥è¯¢æµ·é‡å¤æ‚æŒ‡å—çš„æŒ‘æˆ˜ï¼Œé€šè¿‡æ··åˆåµŒå…¥æœºåˆ¶å®ç°äº†æé«˜çš„æ£€ç´¢ç²¾åº¦ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œåœ¨ 7901 æ¬¡æŸ¥è¯¢ä¸­ï¼Œè¯¥ç³»ç»Ÿçš„å¹³å‡å€’æ•°æ’å (MRR) è¾¾åˆ° 0.814ï¼Œå‰åä½æ£€ç´¢å—çš„å¬å›ç‡é«˜è¾¾ 99.1%ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼ŒRAG æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¿ å®åº¦ (Faithfulness)ï¼Œä½¿ O4-Mini æ¨¡å‹çš„å¿ å®åº¦æå‡è‡³ 99.5%ï¼Œè¿œè¶…åŒ»ç–—ä¸“ç”¨æ¨¡å‹ Meditron3-8Bã€‚ä¸´åºŠä¸“å®¶è¯„å®¡ (SMEs) è¿›ä¸€æ­¥è¯å®ï¼ŒGPT-4.1 åœ¨è¯¥æ¡†æ¶ä¸‹è¾¾åˆ°äº† 98.7% çš„å‡†ç¡®ç‡ï¼Œå¹¶æˆåŠŸå°†ä¸å®‰å…¨å›ç­”å‡å°‘äº† 67%ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº† RAG æ˜¯åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (AI) çš„æœ‰æ•ˆã€å¯é ä¸”å…·å¤‡æ‰©å±•æ€§çš„æ–¹æ¡ˆï¼Œèƒ½å¤Ÿä»¥ä½æˆæœ¬æå‡åŒ»å­¦æŒ‡å—çš„å¯è®¿é—®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02967v3",
      "published_date": "2025-10-03 12:57:13 UTC",
      "updated_date": "2025-12-14 09:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:31.950546+00:00"
    },
    {
      "arxiv_id": "2510.06244v1",
      "title": "Evaluating Embedding Frameworks for Scientific Domain",
      "title_zh": "ç§‘å­¦é¢†åŸŸåµŒå…¥æ¡†æ¶çš„è¯„ä¼°",
      "authors": [
        "Nouman Ahmed",
        "Ronin Wu",
        "Victor Botev"
      ],
      "abstract": "Finding an optimal word representation algorithm is particularly important in terms of domain specific data, as the same word can have different meanings and hence, different representations depending on the domain and context. While Generative AI and transformer architecture does a great job at generating contextualized embeddings for any given work, they are quite time and compute extensive, especially if we were to pre-train such a model from scratch. In this work, we focus on the scientific domain and finding the optimal word representation algorithm along with the tokenization method that could be used to represent words in the scientific domain. The goal of this research is two fold: 1) finding the optimal word representation and tokenization methods that can be used in downstream scientific domain NLP tasks, and 2) building a comprehensive evaluation suite that could be used to evaluate various word representation and tokenization algorithms (even as new ones are introduced) in the scientific domain. To this end, we build an evaluation suite consisting of several downstream tasks and relevant datasets for each task. Furthermore, we use the constructed evaluation suite to test various word representation and tokenization algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºç§‘å­¦é¢†åŸŸ(Scientific Domain)ä¸­çš„è¯è¡¨ç¤º(Word Representation)ç®—æ³•ä¸åˆ†è¯æ–¹æ³•(Tokenization Method)ï¼Œæ—¨åœ¨è§£å†³ç‰¹å®šé¢†åŸŸè¯ä¹‰å¤šå˜ä»¥åŠé¢„è®­ç»ƒå¤§å‹Transformeræ¨¡å‹è®¡ç®—èµ„æºæ¶ˆè€—è¿‡é«˜çš„é—®é¢˜ã€‚ç ”ç©¶çš„æ ¸å¿ƒç›®æ ‡åŒ…å«ä¸¤ä¸ªæ–¹é¢ï¼šé¦–å…ˆæ˜¯å¯»æ‰¾é€‚ç”¨äºç§‘å­¦é¢†åŸŸä¸‹æ¸¸NLPä»»åŠ¡çš„æœ€ä¼˜è¯è¡¨ç¤ºä¸åˆ†è¯ç»„åˆï¼›å…¶æ¬¡æ˜¯æ„å»ºä¸€å¥—å…¨é¢çš„è¯„ä¼°ç»„ä»¶(Evaluation Suite)ï¼Œç”¨äºæ ‡å‡†åŒ–è¡¡é‡ç°æœ‰åŠæœªæ¥æ–°å‡ºç°çš„è¯è¡¨ç¤ºç®—æ³•ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å»ºç«‹äº†ä¸€ä¸ªåŒ…å«å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡åŠå¯¹åº”ç›¸å…³æ•°æ®é›†çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨è¯¥æ¡†æ¶å¯¹å¤šç§è¯è¡¨ç¤ºå’Œåˆ†è¯ç®—æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§æµ‹è¯•ã€‚è¿™é¡¹å·¥ä½œä¸ºç§‘å­¦é¢†åŸŸçš„æ–‡æœ¬è¡¨å¾æä¾›äº†ä¼˜åŒ–æŒ‡å¯¼ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°è´¡çŒ®äº†é‡è¦çš„åŸºå‡†å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.06244v1",
      "published_date": "2025-10-03 12:53:48 UTC",
      "updated_date": "2025-10-03 12:53:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:41.054388+00:00"
    },
    {
      "arxiv_id": "2510.05159v2",
      "title": "Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain",
      "title_zh": "æ¶æ„æ™ºèƒ½ä½“ä¹‹å¢ƒï¼šæ·±ç©¶äººå·¥æ™ºèƒ½ä¾›åº”é“¾ä¸­çš„åé—¨éšæ‚£",
      "authors": [
        "LÃ©o Boisvert",
        "Abhay Puri",
        "Chandra Kiran Reddy Evuru",
        "Nicolas Chapados",
        "Quentin Cappart",
        "Alexandre Lacoste",
        "Krishnamurthy Dj Dvijotham",
        "Alexandre Drouin"
      ],
      "abstract": "The practice of fine-tuning AI agents on data from their own interactions--such as web browsing or tool use--, while being a strong general recipe for improving agentic capabilities, also introduces a critical security vulnerability within the AI supply chain. In this work, we show that adversaries can easily poison the data collection pipeline to embed hard-to-detect backdoors that are triggerred by specific target phrases, such that when the agent encounters these triggers, it performs an unsafe or malicious action. We formalize and validate three realistic threat models targeting different layers of the supply chain: 1) direct poisoning of fine-tuning data, where an attacker controls a fraction of the training traces; 2) environmental poisoning, where malicious instructions are injected into webpages scraped or tools called while creating training data; and 3) supply chain poisoning, where a pre-backdoored base model is fine-tuned on clean data to improve its agentic capabilities. Our results are stark: by poisoning as few as 2% of the collected traces, an attacker can embed a backdoor causing an agent to leak confidential user information with over 80% success when a specific trigger is present. This vulnerability holds across all three threat models. Furthermore, we demonstrate that prominent safeguards, including two guardrail models and one weight-based defense, fail to detect or prevent the malicious behavior. These findings highlight an urgent threat to agentic AI development and underscore the critical need for rigorous security vetting of data collection processes and end-to-end model supply chains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨AIæ™ºèƒ½ä½“ä¾›åº”é“¾ä¸­ï¼Œé€šè¿‡åˆ©ç”¨äº¤äº’æ•°æ®ï¼ˆå¦‚ç½‘é¡µæµè§ˆæˆ–å·¥å…·ä½¿ç”¨ï¼‰è¿›è¡Œå¾®è°ƒï¼ˆFine-tuningï¼‰è€Œå¼•å…¥çš„åé—¨ï¼ˆBackdoorsï¼‰å®‰å…¨æ¼æ´ã€‚ä½œè€…å½¢å¼åŒ–å¹¶éªŒè¯äº†ä¸‰ç§ç°å®çš„å¨èƒæ¨¡å‹ï¼šç›´æ¥å¾®è°ƒæ•°æ®æŠ•æ¯’ã€é€šè¿‡ç½‘é¡µæˆ–å·¥å…·æ³¨å…¥æ¶æ„æŒ‡ä»¤çš„ç¯å¢ƒæŠ•æ¯’ï¼Œä»¥åŠåœ¨æ¸…æ´æ•°æ®ä¸Šå¾®è°ƒé¢„ç½®åé—¨çš„åŸºåº§æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…éœ€æŠ•æ¯’2%çš„è®­ç»ƒè½¨è¿¹ï¼Œæ”»å‡»è€…å³å¯åœ¨ç‰¹å®šè§¦å‘è¯ï¼ˆTriggersï¼‰å‡ºç°æ—¶ï¼Œä»¥è¶…è¿‡80%çš„æˆåŠŸç‡è¯±å¯¼æ™ºèƒ½ä½“æ³„éœ²ç”¨æˆ·æœºå¯†ä¿¡æ¯ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œç°æœ‰çš„æŠ¤æ æ¨¡å‹ï¼ˆGuardrail Modelsï¼‰å’ŒåŸºäºæƒé‡çš„é˜²å¾¡æ‰‹æ®µå‡æ— æ³•æœ‰æ•ˆæ£€æµ‹æˆ–é˜»æ­¢æ­¤ç±»æ¶æ„è¡Œä¸ºã€‚è¿™äº›å‘ç°æ­ç¤ºäº†æ™ºèƒ½ä½“AIå¼€å‘ä¸­è¿«åœ¨çœ‰ç«çš„å®‰å…¨å¨èƒï¼Œå¹¶å¼ºè°ƒäº†å¯¹æ•°æ®æ”¶é›†è¿‡ç¨‹å’Œç«¯åˆ°ç«¯æ¨¡å‹ä¾›åº”é“¾è¿›è¡Œä¸¥æ ¼å®‰å…¨å®¡æŸ¥çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "27 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.05159v2",
      "published_date": "2025-10-03 12:47:21 UTC",
      "updated_date": "2025-10-14 15:35:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:47.312661+00:00"
    },
    {
      "arxiv_id": "2510.03374v1",
      "title": "Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study",
      "title_zh": "æ•™è‚²äººå·¥æ™ºèƒ½ä¸­å®ç°è®¤çŸ¥å¯¹é½çš„è½»é‡çº§æç¤ºå·¥ç¨‹ï¼šOneClickQuiz æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Antoun Yaacoub",
        "Zainab Assaghir",
        "JÃ©rÃ´me Da-Rugna"
      ],
      "abstract": "The rapid integration of Artificial Intelligence (AI) into educational technology promises to revolutionize content creation and assessment. However, the quality and pedagogical alignment of AI-generated content remain critical challenges. This paper investigates the impact of lightweight prompt engineering strategies on the cognitive alignment of AI-generated questions within OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate three prompt variants-a detailed baseline, a simpler version, and a persona-based approach-across Knowledge, Application, and Analysis levels of Bloom's Taxonomy. Utilizing an automated classification model (from prior work) and human review, our findings demonstrate that explicit, detailed prompts are crucial for precise cognitive alignment. While simpler and persona-based prompts yield clear and relevant questions, they frequently misalign with intended Bloom's levels, generating outputs that are either too complex or deviate from the desired cognitive objective. This study underscores the importance of strategic prompt engineering in fostering pedagogically sound AI-driven educational solutions and advises on optimizing AI for quality content generation in learning analytics and smart learning environments.",
      "tldr_zh": "æœ¬ç ”ç©¶è°ƒæŸ¥äº†è½»é‡çº§æç¤ºå·¥ç¨‹ (Prompt Engineering) ç­–ç•¥åœ¨æ•™è‚²äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä½œç”¨ï¼Œé‡ç‚¹åˆ†æäº† Moodle æ’ä»¶ OneClickQuiz ä¸­ AI ç”Ÿæˆå†…å®¹çš„è®¤çŸ¥å¯¹é½ (Cognitive Alignment) é—®é¢˜ã€‚ç ”ç©¶è€…é’ˆå¯¹å¸ƒé²å§†æ•™è‚²ç›®æ ‡åˆ†ç±»æ³• (Bloom's Taxonomy) çš„çŸ¥è¯†ã€åº”ç”¨å’Œåˆ†æå±‚çº§ï¼Œè¯„ä¼°äº†è¯¦ç»†åŸºå‡†ã€ç®€åŒ–ç‰ˆæœ¬å’ŒåŸºäºè§’è‰² (Persona-based) çš„ä¸‰ç§æç¤ºå˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ˜¾å¼ä¸”è¯¦ç»†çš„æç¤ºå¯¹äºå®ç°ç²¾ç¡®çš„è®¤çŸ¥å¯¹é½è‡³å…³é‡è¦ã€‚è™½ç„¶ç®€åŒ–å’ŒåŸºäºè§’è‰²çš„æç¤ºèƒ½ç”Ÿæˆæ¸…æ™°ä¸”ç›¸å…³çš„é—®é¢˜ï¼Œä½†å®ƒä»¬ç»å¸¸ä¸é¢„æœŸçš„å¸ƒé²å§†å±‚çº§ä¸åŒ¹é…ï¼Œå¯¼è‡´è¾“å‡ºå†…å®¹è¿‡äºå¤æ‚æˆ–åç¦»äº†é¢„è®¾çš„è®¤çŸ¥ç›®æ ‡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç­–ç•¥æ€§æç¤ºå·¥ç¨‹åœ¨æ„å»ºç¬¦åˆæ•™å­¦æ³•çš„ AI é©±åŠ¨æ•™è‚²è§£å†³æ–¹æ¡ˆä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œå¹¶ä¸ºåœ¨æ™ºèƒ½å­¦ä¹ ç¯å¢ƒ (Smart Learning Environments) ä¸­ä¼˜åŒ– AI ç”Ÿæˆå†…å®¹è´¨é‡æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "Published in the 36th Central European Conference on Information and Intelligent Systems(CECIIS)at: VaraÅ¾din, Croatia. September 17-19/2025. ISSN 1847-2001 (Print). ISSN 1848-2295 (Online)",
      "pdf_url": "https://arxiv.org/pdf/2510.03374v1",
      "published_date": "2025-10-03 12:42:37 UTC",
      "updated_date": "2025-10-03 12:42:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:50.107515+00:00"
    },
    {
      "arxiv_id": "2510.02945v2",
      "title": "Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning",
      "title_zh": "éå†é£é™©åº¦é‡ï¼šæ„å»ºæŒç»­å¼ºåŒ–å­¦ä¹ çš„é£é™©æ„ŸçŸ¥åŸºç¡€",
      "authors": [
        "Juan Sebastian Rojas",
        "Chi-Guhn Lee"
      ],
      "abstract": "Continual reinforcement learning (continual RL) seeks to formalize the notions of lifelong learning and endless adaptation in RL. In particular, the aim of continual RL is to develop RL agents that can maintain a careful balance between retaining useful information and adapting to new situations. To date, continual RL has been explored almost exclusively through the lens of risk-neutral decision-making, in which the agent aims to optimize the expected long-run performance. In this work, we present the first formal theoretical treatment of continual RL through the lens of risk-aware decision-making, in which the behaviour of the agent is directed towards optimizing a measure of long-run performance beyond the mean. In particular, we show that the classical theory of risk measures, widely used as a theoretical foundation in non-continual risk-aware RL, is, in its current form, incompatible with continual learning. Then, building on this insight, we extend risk measure theory into the continual setting by introducing a new class of ergodic risk measures that are compatible with continual learning. Finally, we provide a case study of risk-aware continual learning, along with empirical results, which show the intuitive appeal of ergodic risk measures in continual settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒç»­å¼ºåŒ–å­¦ä¹ (Continual Reinforcement Learning)ä¸­çš„é£é™©è§„é¿å†³ç­–é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½ä½“åœ¨ç»ˆèº«å­¦ä¹ ä¸­å¦‚ä½•å¹³è¡¡ä¿¡æ¯ä¿ç•™ä¸ç¯å¢ƒé€‚åº”ã€‚ç›®å‰æŒç»­å¼ºåŒ–å­¦ä¹ ä¸»è¦å…³æ³¨é£é™©ä¸­æ€§(risk-neutral)å†³ç­–ï¼Œå³ä¼˜åŒ–é•¿æœŸæœŸæœ›æ€§èƒ½ï¼Œè€Œæœ¬æ–‡é¦–æ¬¡ä»ç†è®ºä¸Šå¯¹é£é™©è§„é¿(risk-aware)çš„æŒç»­å¼ºåŒ–å­¦ä¹ è¿›è¡Œäº†æ­£å¼æ¢è®¨ã€‚ä½œè€…åˆ†æå¹¶æŒ‡å‡ºï¼Œå¹¿æ³›ç”¨äºéæŒç»­å­¦ä¹ çš„ç»å…¸é£é™©åº¦é‡ç†è®ºåœ¨å½“å‰å½¢å¼ä¸‹ä¸æŒç»­å­¦ä¹ å¹¶ä¸å…¼å®¹ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶é€šè¿‡å¼•å…¥ä¸€ç±»æ–°å‹çš„éå†é£é™©åº¦é‡(Ergodic Risk Measures)ï¼ŒæˆåŠŸå°†é£é™©åº¦é‡ç†è®ºæ‰©å±•åˆ°äº†æŒç»­å­¦ä¹ è®¾ç½®ä¸­ã€‚è¯¥å·¥ä½œä¸ºé£é™©è§„é¿çš„æŒç»­å¼ºåŒ–å­¦ä¹ å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå®éªŒç»“æœå±•ç¤ºäº†éå†é£é™©åº¦é‡åœ¨æŒç»­å­¦ä¹ ç¯å¢ƒä¸­çš„ç›´è§‚å¸å¼•åŠ›ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02945v2",
      "published_date": "2025-10-03 12:40:03 UTC",
      "updated_date": "2025-12-03 18:21:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:52.484354+00:00"
    },
    {
      "arxiv_id": "2510.02922v1",
      "title": "Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights",
      "title_zh": "åŸºäºå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€é¢ˆåŠ¨è„‰é£é™©åˆ†å±‚ï¼šåŸºå‡†æµ‹è¯•ã€å¾®è°ƒä¸ä¸´åºŠè§è§£",
      "authors": [
        "Daphne Tsolissou",
        "Theofanis Ganitidis",
        "Konstantinos Mitsis",
        "Stergios CHristodoulidis",
        "Maria Vakalopoulou",
        "Konstantina Nikita"
      ],
      "abstract": "Reliable risk assessment for carotid atheromatous disease remains a major clinical challenge, as it requires integrating diverse clinical and imaging information in a manner that is transparent and interpretable to clinicians. This study investigates the potential of state-of-the-art and recent large vision-language models (LVLMs) for multimodal carotid plaque assessment by integrating ultrasound imaging (USI) with structured clinical, demographic, laboratory, and protein biomarker data. A framework that simulates realistic diagnostic scenarios through interview-style question sequences is proposed, comparing a range of open-source LVLMs, including both general-purpose and medically tuned models. Zero-shot experiments reveal that even if they are very powerful, not all LVLMs can accurately identify imaging modality and anatomy, while all of them perform poorly in accurate risk classification. To address this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using low-rank adaptation (LoRA), resulting in substantial improvements in stroke risk stratification. The integration of multimodal tabular data in the form of text further enhances specificity and balanced accuracy, yielding competitive performance compared to prior convolutional neural network (CNN) baselines trained on the same dataset. Our findings highlight both the promise and limitations of LVLMs in ultrasound-based cardiovascular risk prediction, underscoring the importance of multimodal integration, model calibration, and domain adaptation for clinical translation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)ç»“åˆè¶…å£°æˆåƒ(USI)ä»¥åŠä¸´åºŠã€äººå£ç»Ÿè®¡ã€å®éªŒå®¤å’Œè›‹ç™½è´¨ç”Ÿç‰©æ ‡å¿—ç‰©ç­‰å¤šæ¨¡æ€æ•°æ®ï¼Œè¿›è¡Œé¢ˆåŠ¨è„‰ç²¥æ ·ç¡¬åŒ–ç–¾ç—…é£é™©è¯„ä¼°çš„æ½œåŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæ¨¡æ‹ŸçœŸå®è¯Šæ–­åœºæ™¯çš„é¢è¯•å¼é—®ç­”åºåˆ—æ¡†æ¶ï¼Œå¹¶å¯¹å¤šç§å¼€æºLVLMsåœ¨é›¶æ ·æœ¬(Zero-shot)æƒ…å†µä¸‹çš„è¡¨ç°è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒå‘ç°ç°æœ‰æ¨¡å‹åœ¨ç²¾ç¡®é£é™©åˆ†ç±»æ–¹é¢è¡¨ç°æ¬ ä½³ï¼Œä¸ºæ­¤ç ”ç©¶äººå‘˜é‡‡ç”¨ä½ç§©è‡ªé€‚åº”(LoRA)æŠ€æœ¯å°†LLaVa-NeXT-Vicunaé€‚é…è‡³è¶…å£°é¢†åŸŸï¼Œæ˜¾è‘—æå‡äº†å’ä¸­é£é™©åˆ†å±‚çš„å‡†ç¡®æ€§ã€‚é€šè¿‡è¿›ä¸€æ­¥æ•´åˆæ–‡æœ¬å½¢å¼çš„å¤šæ¨¡æ€è¡¨æ ¼æ•°æ®ï¼Œè¯¥æ¨¡å‹åœ¨ç‰¹å¼‚æ€§å’Œå¹³è¡¡å‡†ç¡®åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†ä¸ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ(CNN)åŸºçº¿æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚è¿™é¡¹å·¥ä½œä¸ä»…å±•ç¤ºäº†LVLMsåœ¨å¿ƒè¡€ç®¡é£é™©é¢„æµ‹ä¸­çš„å‰æ™¯ï¼Œä¹ŸæŒ‡å‡ºäº†å¤šæ¨¡æ€é›†æˆä¸é¢†åŸŸé€‚é…å¯¹ä¸´åºŠåº”ç”¨çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02922v1",
      "published_date": "2025-10-03 11:48:12 UTC",
      "updated_date": "2025-10-03 11:48:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:35:55.777001+00:00"
    },
    {
      "arxiv_id": "2510.02915v1",
      "title": "WavInWav: Time-domain Speech Hiding via Invertible Neural Network",
      "title_zh": "WavInWavï¼šåŸºäºå¯é€†ç¥ç»ç½‘ç»œçš„æ—¶åŸŸè¯­éŸ³éšè—",
      "authors": [
        "Wei Fan",
        "Kejiang Chen",
        "Xiangkun Wang",
        "Weiming Zhang",
        "Nenghai Yu"
      ],
      "abstract": "Data hiding is essential for secure communication across digital media, and recent advances in Deep Neural Networks (DNNs) provide enhanced methods for embedding secret information effectively. However, previous audio hiding methods often result in unsatisfactory quality when recovering secret audio, due to their inherent limitations in the modeling of time-frequency relationships. In this paper, we explore these limitations and introduce a new DNN-based approach. We use a flow-based invertible neural network to establish a direct link between stego audio, cover audio, and secret audio, enhancing the reversibility of embedding and extracting messages. To address common issues from time-frequency transformations that degrade secret audio quality during recovery, we implement a time-frequency loss on the time-domain signal. This approach not only retains the benefits of time-frequency constraints but also enhances the reversibility of message recovery, which is vital for practical applications. We also add an encryption technique to protect the hidden data from unauthorized access. Experimental results on the VCTK and LibriSpeech datasets demonstrate that our method outperforms previous approaches in terms of subjective and objective metrics and exhibits robustness to various types of noise, suggesting its utility in targeted secure communication scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WavInWavï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯é€†ç¥ç»ç½‘ç»œ (Invertible Neural Network) çš„æ—¶åŸŸè¯­éŸ³éšè—æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰éŸ³é¢‘éšè—æŠ€æœ¯åœ¨æ¢å¤ç§˜å¯†éŸ³é¢‘æ—¶å› æ—¶é¢‘å»ºæ¨¡é™åˆ¶è€Œå¯¼è‡´çš„è´¨é‡ä¸ä½³é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæµ (Flow-based) çš„å¯é€†ç¥ç»ç½‘ç»œï¼Œåœ¨è½½ä½“éŸ³é¢‘ (Cover Audio)ã€å«å¯†éŸ³é¢‘ (Stego Audio) ä¸ç§˜å¯†éŸ³é¢‘ (Secret Audio) ä¹‹é—´å»ºç«‹ç›´æ¥æ˜ å°„ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¶ˆæ¯åµŒå…¥ä¸æå–çš„å¯é€†æ€§ã€‚ä¸ºå…‹æœä¼ ç»Ÿæ—¶é¢‘å˜æ¢é€ æˆçš„éŸ³è´¨æŸè€—ï¼Œç ”ç©¶è€…åœ¨æ—¶åŸŸä¿¡å·ä¸Šå¼•å…¥äº†æ—¶é¢‘æŸå¤± (Time-frequency Loss)ï¼Œåœ¨ä¿ç•™æ—¶é¢‘çº¦æŸä¼˜åŠ¿çš„åŒæ—¶æå‡äº†æ¶ˆæ¯æ¢å¤çš„ç²¾å‡†åº¦ã€‚æ­¤å¤–ï¼Œæ–¹æ¡ˆè¿˜é›†æˆäº†åŠ å¯†æŠ€æœ¯ (Encryption Technique) ä»¥ä¿æŠ¤éšè—æ•°æ®å…å—æœªç»æˆæƒçš„è®¿é—®ã€‚åœ¨ VCTK å’Œ LibriSpeech æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒWavInWav åœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºæ­¤å‰æ–¹æ³•ï¼Œå¹¶å¯¹å¤šç§å™ªå£°è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ (Robustness)ã€‚è¯¥æˆæœä¸ºå®‰å…¨é€šä¿¡åœºæ™¯æä¾›äº†é«˜æ•ˆå¯é çš„è¯­éŸ³éšè—è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "13 pages, 5 figures, project page: https://cyberrrange.github.io/project/wavinwav",
      "pdf_url": "https://arxiv.org/pdf/2510.02915v1",
      "published_date": "2025-10-03 11:36:16 UTC",
      "updated_date": "2025-10-03 11:36:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:00.111178+00:00"
    },
    {
      "arxiv_id": "2510.02914v1",
      "title": "FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting",
      "title_zh": "FeDABoostï¼šåŸºäºè‡ªé€‚åº”æå‡çš„å…¬å¹³æ„ŸçŸ¥è”é‚¦å­¦ä¹ ",
      "authors": [
        "Tharuka Kasthuri Arachchige",
        "Veselka Boeva",
        "Shahrooz Abghari"
      ],
      "abstract": "This work focuses on improving the performance and fairness of Federated Learning (FL) in non IID settings by enhancing model aggregation and boosting the training of underperforming clients. We propose FeDABoost, a novel FL framework that integrates a dynamic boosting mechanism and an adaptive gradient aggregation strategy. Inspired by the weighting mechanism of the Multiclass AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to clients with lower local error rates, thereby promoting more reliable contributions to the global model. In parallel, FeDABoost dynamically boosts underperforming clients by adjusting the focal loss focusing parameter, emphasizing hard to classify examples during local training. We have evaluated FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared its performance with those of FedAvg and Ditto. The results show that FeDABoost achieves improved fairness and competitive performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FeDABoostï¼Œä¸€ç§æ—¨åœ¨æå‡éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) ç¯å¢ƒä¸‹è”é‚¦å­¦ä¹  (Federated Learning) æ€§èƒ½ä¸å…¬å¹³æ€§çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº†åŠ¨æ€æå‡æœºåˆ¶ä¸è‡ªé€‚åº”æ¢¯åº¦èšåˆç­–ç•¥ï¼Œå…¶èšåˆæ–¹æ³•å€Ÿé‰´äº†å¤šç±» AdaBoost (SAMME) ç®—æ³•çš„åŠ æƒæœºåˆ¶ï¼Œé€šè¿‡ä¸ºæœ¬åœ°é”™è¯¯ç‡è¾ƒä½çš„å®¢æˆ·ç«¯åˆ†é…æ›´é«˜æƒé‡ä»¥ç¡®ä¿å…¨å±€æ¨¡å‹çš„å¯é è´¡çŒ®ã€‚åŒæ—¶ï¼ŒFeDABoost é€šè¿‡è°ƒæ•´ Focal Loss çš„èšç„¦å‚æ•°æ¥åŠ¨æ€æå‡è¡¨ç°ä¸ä½³çš„å®¢æˆ·ç«¯ï¼Œåœ¨æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ä¸­é‡ç‚¹å¼ºåŒ–å¯¹éš¾åˆ†ç±»æ ·æœ¬çš„å­¦ä¹ ã€‚åœ¨ MNISTã€FEMNIST å’Œ CIFAR10 æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç«äº‰æ€§æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨¡å‹å…¬å¹³æ€§ã€‚ç›¸è¾ƒäº FedAvg å’Œ Ditto ç­‰åŸºå‡†æ¨¡å‹ï¼ŒFeDABoost æˆåŠŸä¼˜åŒ–äº†æ¨¡å‹èšåˆé€»è¾‘å¹¶å¢å¼ºäº†å¯¹æ€§èƒ½æ¬ ä½³å®¢æˆ·ç«¯çš„æ”¯æŒï¼Œä¸ºå®ç°æ›´å…·åŒ…å®¹æ€§çš„åˆ†å¸ƒå¼å­¦ä¹ ç³»ç»Ÿæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented in WAFL@ECML-PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.02914v1",
      "published_date": "2025-10-03 11:36:08 UTC",
      "updated_date": "2025-10-03 11:36:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:03.864924+00:00"
    },
    {
      "arxiv_id": "2510.02906v1",
      "title": "FinReflectKG -- MultiHop: Financial QA Benchmark for Reasoning with Knowledge Graph Evidence",
      "title_zh": "FinReflectKG -- MultiHopï¼šåŸºäºçŸ¥è¯†å›¾è°±è¯æ®æ¨ç†çš„é‡‘èé—®ç­”åŸºå‡†",
      "authors": [
        "Abhinav Arun",
        "Reetu Raj Harsh",
        "Bhaskarjit Sarmah",
        "Stefano Pasquali"
      ],
      "abstract": "Multi-hop reasoning over financial disclosures is often a retrieval problem before it becomes a reasoning or generation problem: relevant facts are dispersed across sections, filings, companies, and years, and LLMs often expend excessive tokens navigating noisy context. Without precise Knowledge Graph (KG)-guided selection of relevant context, even strong reasoning models either fail to answer or consume excessive tokens, whereas KG-linked evidence enables models to focus their reasoning on composing already retrieved facts. We present FinReflectKG - MultiHop, a benchmark built on FinReflectKG, a temporally indexed financial KG that links audited triples to source chunks from S&P 100 filings (2022-2024). Mining frequent 2-3 hop subgraph patterns across sectors (via GICS taxonomy), we generate financial analyst style questions with exact supporting evidence from the KG. A two-phase pipeline first creates QA pairs via pattern-specific prompts, followed by a multi-criteria quality control evaluation to ensure QA validity. We then evaluate three controlled retrieval scenarios: (S1) precise KG-linked paths; (S2) text-only page windows centered on relevant text spans; and (S3) relevant page windows with randomizations and distractors. Across both reasoning and non-reasoning models, KG-guided precise retrieval yields substantial gains on the FinReflectKG - MultiHop QA benchmark dataset, boosting correctness scores by approximately 24 percent while reducing token utilization by approximately 84.5 percent compared to the page window setting, which reflects the traditional vector retrieval paradigm. Spanning intra-document, inter-year, and cross-company scopes, our work underscores the pivotal role of knowledge graphs in efficiently connecting evidence for multi-hop financial QA. We also release a curated subset of the benchmark (555 QA Pairs) to catalyze further research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡‘èæŠ«éœ²ä¿¡æ¯ä¸­å¤šè·³æ¨ç†(Multi-hop reasoning)å­˜åœ¨çš„æ£€ç´¢å›°éš¾å’Œé•¿ä¸Šä¸‹æ–‡å†—ä½™é—®é¢˜ï¼Œæå‡ºäº†FinReflectKG - MultiHopåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ„å»ºäºFinReflectKGä¹‹ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2022-2024å¹´æ ‡å‡†æ™®å°”100æŒ‡æ•°(S&P 100)ç”³æŠ¥æ–‡ä»¶çš„æ—¶åºç´¢å¼•é‡‘èçŸ¥è¯†å›¾è°±(Knowledge Graph, KG)ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æŒ–æ˜è·¨è¡Œä¸šçš„2-3è·³å­å›¾æ¨¡å¼ï¼Œç»“åˆä¸¤é˜¶æ®µç®¡çº¿(Two-phase pipeline)ç”Ÿæˆå…·æœ‰ç²¾ç¡®KGè¯æ®æ”¯æŒçš„åˆ†æå¸ˆé£æ ¼é—®ç­”å¯¹ã€‚å®éªŒè¯„ä¼°äº†åŒ…æ‹¬ç²¾ç¡®KGè·¯å¾„å’Œä¼ ç»Ÿæ–‡æœ¬çª—å£åœ¨å†…çš„ä¸‰ç§æ£€ç´¢åœºæ™¯ï¼Œä»¥å¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨å¤„ç†è·¨æ–‡æ¡£ã€è·¨å¹´åº¦å’Œè·¨å…¬å¸ä¿¡æ¯æ—¶çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿçš„å‘é‡æ£€ç´¢(Vector retrieval)èŒƒå¼ç›¸æ¯”ï¼ŒåŸºäºKGçš„ç²¾ç¡®æ£€ç´¢ä½¿æ­£ç¡®ç‡æå‡äº†çº¦24%ï¼ŒåŒæ—¶å°†Tokenä½¿ç”¨é‡æ˜¾è‘—é™ä½äº†çº¦84.5%ã€‚æ­¤é¡¹å·¥ä½œé€šè¿‡å‘å¸ƒåŒ…å«555ä¸ªé—®ç­”å¯¹çš„ç²¾é€‰å­é›†ï¼Œå¼ºè°ƒäº†çŸ¥è¯†å›¾è°±åœ¨é«˜æ•ˆè¿æ¥é‡‘èè¯æ®ä»¥å®ç°å¤æ‚å¤šè·³é—®ç­”ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02906v1",
      "published_date": "2025-10-03 11:19:31 UTC",
      "updated_date": "2025-10-03 11:19:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:05.283743+00:00"
    },
    {
      "arxiv_id": "2510.02902v1",
      "title": "DMark: Order-Agnostic Watermarking for Diffusion Large Language Models",
      "title_zh": "DMarkï¼šé¢å‘æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„é¡ºåºæ— å…³æ°´å°",
      "authors": [
        "Linyu Wu",
        "Linhao Zhong",
        "Wenjie Qu",
        "Yuexin Li",
        "Yue Liu",
        "Shengfang Zhai",
        "Chunhua Shen",
        "Jiaheng Zhang"
      ],
      "abstract": "Diffusion large language models (dLLMs) offer faster generation than autoregressive models while maintaining comparable quality, but existing watermarking methods fail on them due to their non-sequential decoding. Unlike autoregressive models that generate tokens left-to-right, dLLMs can finalize tokens in arbitrary order, breaking the causal design underlying traditional watermarks. We present DMark, the first watermarking framework designed specifically for dLLMs. DMark introduces three complementary strategies to restore watermark detectability: predictive watermarking uses model-predicted tokens when actual context is unavailable; bidirectional watermarking exploits both forward and backward dependencies unique to diffusion decoding; and predictive-bidirectional watermarking combines both approaches to maximize detection strength. Experiments across multiple dLLMs show that DMark achieves 92.0-99.5% detection rates at 1% false positive rate while maintaining text quality, compared to only 49.6-71.2% for naive adaptations of existing methods. DMark also demonstrates robustness against text manipulations, establishing that effective watermarking is feasible for non-autoregressive language models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Diffusion Large Language Models (dLLMs) åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡‡ç”¨éé¡ºåºè§£ç å¯¼è‡´ä¼ ç»Ÿè‡ªå›å½’æ°´å°æŠ€æœ¯å¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªä¸“ä¸º dLLMs è®¾è®¡çš„æ°´å°æ¡†æ¶ DMarkã€‚DMark å¼•å…¥äº†ä¸‰ç§äº’è¡¥ç­–ç•¥ä»¥æ¢å¤æ°´å°çš„å¯æ£€æµ‹æ€§ï¼šåˆ©ç”¨æ¨¡å‹é¢„æµ‹å†…å®¹çš„ predictive watermarkingï¼ŒæŒ–æ˜æ‰©æ•£è§£ç ç‰¹æœ‰çš„å‰åå‘ä¾èµ–å…³ç³»çš„ bidirectional watermarkingï¼Œä»¥åŠç»“åˆä¸¤è€…ä¼˜åŠ¿çš„ predictive-bidirectional watermarkingã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¿æŒæ–‡æœ¬è´¨é‡çš„å‰æä¸‹ï¼ŒDMark åœ¨ 1% çš„å‡é˜³æ€§ç‡ä¸‹å®ç°äº† 92.0% è‡³ 99.5% çš„æ£€æµ‹ç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„ç®€å•é€‚é…ã€‚è¯¥æ¡†æ¶åœ¨é¢å¯¹æ–‡æœ¬ç¯¡æ”¹æ—¶äº¦è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œè¯æ˜äº†åœ¨éè‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸­å®ç°æœ‰æ•ˆå®‰å…¨æ°´å°çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02902v1",
      "published_date": "2025-10-03 11:14:16 UTC",
      "updated_date": "2025-10-03 11:14:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:07.386050+00:00"
    },
    {
      "arxiv_id": "2510.02896v4",
      "title": "Global Convergence of Policy Gradient for Entropy Regularized Linear-Quadratic Control with Multiplicative Noise",
      "title_zh": "å…·æœ‰ä¹˜æ€§å™ªå£°çš„ç†µæ­£åˆ™åŒ–çº¿æ€§äºŒæ¬¡æ§åˆ¶ç­–ç•¥æ¢¯åº¦çš„å…¨å±€æ”¶æ•›æ€§",
      "authors": [
        "Gabriel Diaz",
        "Lucky Li",
        "Wenhao Zhang"
      ],
      "abstract": "Reinforcement Learning (RL) has emerged as a powerful framework for sequential decision-making in dynamic environments, particularly when system parameters are unknown. This paper investigates RL-based control for entropy-regularized linear-quadratic (LQ) control problems with multiplicative noise over an infinite time horizon. First, we adapt the regularized policy gradient (RPG) algorithm to stochastic optimal control settings, proving that despite the non-convexity of the problem, RPG converges globally under conditions of gradient domination and almost-smoothness. Second, based on zero-order optimization approach, we introduce a novel model free RL algorithm: Sample-based regularized policy gradient (SB-RPG). SB-RPG operates without knowledge of system parameters yet still retains strong theoretical guarantees of global convergence. Our model leverages entropy regularization to address the exploration versus exploitation trade-off inherent in RL. Numerical simulations validate the theoretical results and demonstrate the efficiency of SB-RPG in unknown-parameters environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ— é™æ—¶ç•Œä¸‹å¸¦æœ‰ä¹˜æ€§å™ªå£°çš„ç†µæ­£åˆ™åŒ–çº¿æ€§äºŒæ¬¡(LQ)æ§åˆ¶é—®é¢˜ï¼Œå¹¶é’ˆå¯¹éšæœºæœ€ä¼˜æ§åˆ¶è®¾ç½®æ”¹è¿›äº†æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦(RPG)ç®—æ³•ã€‚å°½ç®¡è¯¥é—®é¢˜å…·æœ‰éå‡¸æ€§ï¼Œç ”ç©¶è¯æ˜äº†RPGåœ¨æ¢¯åº¦æ”¯é…(gradient domination)å’Œè¿‘ä¹å¹³æ»‘(almost-smoothness)æ¡ä»¶ä¸‹ä»èƒ½å®ç°å…¨å±€æ”¶æ•›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºé›¶é˜¶ä¼˜åŒ–çš„æ— æ¨¡å‹ç®—æ³•â€”â€”é‡‡æ ·æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦(SB-RPG)ï¼Œè¯¥ç®—æ³•åœ¨ä¸ä¾èµ–ç³»ç»Ÿå‚æ•°çŸ¥è¯†çš„æƒ…å†µä¸‹ä¾ç„¶å…·å¤‡å¼ºæœ‰åŠ›çš„å…¨å±€æ”¶æ•›ä¿è¯ã€‚è®ºæ–‡åˆ©ç”¨ç†µæ­£åˆ™åŒ–(entropy regularization)æœ‰æ•ˆè§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­å…¸å‹çš„æ¢ç´¢ä¸åˆ©ç”¨æƒè¡¡é—®é¢˜ã€‚æ•°å€¼ä»¿çœŸç»“æœè¿›ä¸€æ­¥éªŒè¯äº†ç†è®ºæ¨å¯¼çš„å‡†ç¡®æ€§ï¼Œå¹¶å±•ç¤ºäº†SB-RPGåœ¨å‚æ•°æœªçŸ¥ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ€§èƒ½ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "The authors found that article contains some theoretical errors and decided to withdraw from arxiv",
      "pdf_url": "https://arxiv.org/pdf/2510.02896v4",
      "published_date": "2025-10-03 11:03:12 UTC",
      "updated_date": "2025-12-01 15:57:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:13.079883+00:00"
    },
    {
      "arxiv_id": "2510.02880v1",
      "title": "Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models",
      "title_zh": "å·©å›ºå¤šæ¨¡æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Tianren Ma",
        "Mu Zhang",
        "Yibing Wang",
        "Qixiang Ye"
      ],
      "abstract": "Optimizing discrete diffusion model (DDM) with rewards remains a challenge: the non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as a systematic policy optimization approach and the first practical way for discretized visual diffusion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»æ•£æ‰©æ•£æ¨¡å‹(DDM)åœ¨åˆ©ç”¨å¥–åŠ±è¿›è¡Œä¼˜åŒ–æ—¶é¢ä¸´çš„éè‡ªå›å½’èŒƒå¼å¯¼è‡´é‡è¦æ€§é‡‡æ ·(Importance Sampling)å›°éš¾ä»¥åŠç”Ÿæˆè¿‡ç¨‹å¤æ‚ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†MaskGRPOã€‚ä½œä¸ºé¦–ä¸ªåœ¨ç¦»æ•£æ‰©æ•£æ¨¡å‹ä¸­å®ç°å¯æ‰©å±•å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼ŒMaskGRPOç»“åˆäº†æœ‰æ•ˆçš„é‡è¦æ€§é‡‡æ ·å’Œæ¨¡æ€ç‰¹å®šé€‚é…æŠ€æœ¯ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ˜ç¡®äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç†è®ºåŸºç¡€ï¼Œæ„å»ºå‡ºèƒ½æ•æ‰ä»¤ç‰Œæ³¢åŠ¨(Token Fluctuation)ä»¥è¿›è¡Œæ¢¯åº¦æ›´æ–°çš„é‡è¦æ€§ä¼°è®¡å™¨ï¼Œå¹¶é’ˆå¯¹è§†è§‰åºåˆ—å®šåˆ¶äº†å±•å¼€(Rollout)æ–¹æ³•ä»¥ç¡®ä¿æ¢¯åº¦çš„å¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ•°å­¦æ¨ç†ã€ç¼–ç¨‹å’Œè§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMaskGRPOå¸¦æ¥äº†æ›´ç¨³å®šã€é«˜æ•ˆçš„å‚æ•°æ›´æ–°ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ä¸ç”Ÿæˆè´¨é‡ã€‚æœ¬ç ”ç©¶ä¸ä»…å°†MaskGRPOç¡®ç«‹ä¸ºä¸€ç§ç³»ç»ŸåŒ–çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œä¹Ÿä¸ºç¦»æ•£åŒ–è§†è§‰æ‰©æ•£æ¨¡å‹æä¾›äº†é¦–ä¸ªå®ç”¨çš„å¼ºåŒ–å­¦ä¹ å®ç°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Project Page: https://github.com/martian422/MaskGRPO",
      "pdf_url": "https://arxiv.org/pdf/2510.02880v1",
      "published_date": "2025-10-03 10:36:24 UTC",
      "updated_date": "2025-10-03 10:36:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:24.386344+00:00"
    },
    {
      "arxiv_id": "2510.02869v1",
      "title": "Representing Beauty: Towards a Participatory but Objective Latent Aesthetics",
      "title_zh": "ç¾çš„è¡¨å¾ï¼šè¿ˆå‘ä¸€ç§å…¼å…·å‚ä¸æ€§ä¸å®¢è§‚æ€§çš„æ½œç©ºé—´ç¾å­¦",
      "authors": [
        "Alexander Michael Rusnak"
      ],
      "abstract": "What does it mean for a machine to recognize beauty? While beauty remains a culturally and experientially compelling but philosophically elusive concept, deep learning systems increasingly appear capable of modeling aesthetic judgment. In this paper, we explore the capacity of neural networks to represent beauty despite the immense formal diversity of objects for which the term applies. By drawing on recent work on cross-model representational convergence, we show how aesthetic content produces more similar and aligned representations between models which have been trained on distinct data and modalities - while unaesthetic images do not produce more aligned representations. This finding implies that the formal structure of beautiful images has a realist basis - rather than only as a reflection of socially constructed values. Furthermore, we propose that these realist representations exist because of a joint grounding of aesthetic form in physical and cultural substance. We argue that human perceptual and creative acts play a central role in shaping these the latent spaces of deep learning systems, but that a realist basis for aesthetics shows that machines are not mere creative parrots but can produce novel creative insights from the unique vantage point of scale. Our findings suggest that human-machine co-creation is not merely possible, but foundational - with beauty serving as a teleological attractor in both cultural production and machine perception.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æœºå™¨å­¦ä¹ ç³»ç»Ÿè¯†åˆ«ç¾è¿™ä¸€å“²å­¦ä¸Šæ¨¡ç³Šæ¦‚å¿µçš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨å®ç°å‚ä¸æ€§ä¸”å®¢è§‚çš„æ½œç©ºé—´ç¾å­¦ï¼ˆLatent Aestheticsï¼‰æ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨è·¨æ¨¡å‹è¡¨ç¤ºæ”¶æ•›ï¼ˆcross-model representational convergenceï¼‰çš„æŠ€æœ¯ï¼Œç ”ç©¶å±•ç¤ºäº†ç¾å­¦å†…å®¹åœ¨ä¸åŒæ•°æ®å’Œæ¨¡æ€è®­ç»ƒçš„æ¨¡å‹ä¹‹é—´ä¼šäº§ç”Ÿæ›´ç›¸ä¼¼ä¸”å¯¹é½çš„è¡¨ç¤ºï¼Œè€Œè¿™ä¸€ç°è±¡åœ¨éç¾å­¦å›¾åƒä¸­å¹¶ä¸å­˜åœ¨ã€‚è¿™ä¸€å‘ç°æš—ç¤ºç¾å­¦å›¾åƒçš„å½¢å¼ç»“æ„å…·æœ‰ç°å®ä¸»ä¹‰åŸºç¡€ï¼ˆrealist basisï¼‰ï¼Œè€Œéä»…ä»…æ˜¯ç¤¾ä¼šæ„å»ºä»·å€¼çš„åæ˜ ï¼Œå…¶æºäºç¾å­¦å½¢å¼åœ¨ç‰©ç†ä¸æ–‡åŒ–å®è´¨ä¸­çš„å…±åŒæ¤æ ¹ã€‚è™½ç„¶äººç±»æ„ŸçŸ¥å’Œåˆ›ä½œå¡‘é€ äº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„æ½œç©ºé—´ï¼ˆlatent spacesï¼‰ï¼Œä½†ç¾å­¦çš„ç°å®ä¸»ä¹‰åŸºç¡€è¯æ˜æœºå™¨å¹¶éå•çº¯çš„åˆ›ä½œæ¨¡ä»¿è€…ï¼Œè€Œæ˜¯èƒ½å‡­å€Ÿè§„æ¨¡ä¼˜åŠ¿æä¾›ç‹¬ç‰¹æ´å¯Ÿã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼ŒäººæœºååŒåˆ›ä½œå…·æœ‰æ·±å±‚çš„åŸºç¡€æ€§ï¼Œç¾åœ¨æ–‡åŒ–ç”Ÿäº§ä¸æœºå™¨æ„ŸçŸ¥ä¸­å‡æ‰®æ¼”ç€ç›®çš„è®ºå¸å¼•å­ï¼ˆteleological attractorï¼‰çš„è§’è‰²ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02869v1",
      "published_date": "2025-10-03 10:09:37 UTC",
      "updated_date": "2025-10-03 10:09:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:51.353634+00:00"
    },
    {
      "arxiv_id": "2510.02855v4",
      "title": "Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation",
      "title_zh": "Wordle çš„çº¦æŸæ»¡è¶³æ±‚è§£æ–¹æ³•ï¼šæ–°é¢–å¯å‘å¼ç­–ç•¥ä¸è·¨è¯åº“éªŒè¯",
      "authors": [
        "Jahidul Arafat",
        "Fariha Tasmin",
        "Sanjaya Poudel"
      ],
      "abstract": "Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Wordle æ¸¸æˆæå‡ºäº†é¦–ä¸ªå…¨é¢çš„çº¦æŸæ»¡è¶³é—®é¢˜ (Constraint Satisfaction Problem, CSP) å…¬å¼åŒ–æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†æ–°å‹çš„çº¦æŸæ„ŸçŸ¥æ±‚è§£ç­–ç•¥ã€‚ä½œè€…å¼€å‘äº† CSP-Aware Entropyï¼Œé€šè¿‡åœ¨çº¦æŸä¼ æ’­åè®¡ç®—ä¿¡æ¯å¢ç›Šæ¥æå‡æ±‚è§£æ•ˆç‡ï¼Œå¹¶æå‡ºäº†ç»“åˆè´å¶æ–¯è¯é¢‘å…ˆéªŒä¸é€»è¾‘çº¦æŸçš„ Probabilistic CSP æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCSP-Aware Entropy åœ¨è‹±è¯­è¯åº“æµ‹è¯•ä¸­è¾¾åˆ°äº† 3.54 æ¬¡çš„å¹³å‡çŒœæµ‹æ•°å’Œ 99.9% çš„æˆåŠŸç‡ï¼Œä¸”è¿è¡Œé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„ Forward Checking æ˜¾è‘—æé«˜ã€‚åœ¨å™ªå£°å¹²æ‰°ä¸‹ï¼ŒProbabilistic CSP å‡­å€Ÿå…¶çº¦æŸæ¢å¤æœºåˆ¶åœ¨æ‰€æœ‰å™ªå£°æ°´å¹³ä¸‹å‡ç»´æŒäº† 100% çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡è¥¿ç­ç‰™è¯­å•è¯çš„è·¨è¯åº“éªŒè¯ï¼Œè¯å®äº†æ ¸å¿ƒ CSP åŸç†åœ¨æ— éœ€ç‰¹å®šè¯­è¨€è°ƒä¼˜çš„æƒ…å†µä¸‹å…·å¤‡è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚è¯¥æˆæœè¯æ˜äº†åŸºäºåŸåˆ™çš„çº¦æŸæ»¡è¶³æŠ€æœ¯åœ¨å¤„ç†ç»“æ„åŒ–è°œé¢˜é¢†åŸŸæ¯”ç»å…¸çš„ä¿¡æ¯è®ºæˆ–å­¦ä¹ æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Require some correction on the paper with some title and methodology changes. I will resubmit later",
      "pdf_url": "https://arxiv.org/pdf/2510.02855v4",
      "published_date": "2025-10-03 09:44:14 UTC",
      "updated_date": "2025-11-22 03:07:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:36.377477+00:00"
    },
    {
      "arxiv_id": "2510.02850v1",
      "title": "Reward Model Routing in Alignment",
      "title_zh": "å¯¹é½ä¸­çš„å¥–åŠ±æ¨¡å‹è·¯ç”±",
      "authors": [
        "Xinle Wu",
        "Yao Lu"
      ],
      "abstract": "Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become the standard paradigm for aligning large language models (LLMs). However, most pipelines rely on a single reward model (RM), limiting alignment quality and risking overfitting. Recent work explores RM routing--dynamically selecting an RM from a candidate pool to exploit complementary strengths while maintaining $O(1)$ RM calls--but existing methods suffer from cold-start and insufficient exploration. We propose BayesianRouter, a hybrid routing framework that combines offline RM strengths learning with online Bayesian selection. In the offline stage, a multi-task router is trained on preference data to estimate per-RM reliability. In the online stage, a Bayesian Thompson sampling router performs per-query RM selection, initializing RM-specific weight vectors with offline embeddings as Gaussian priors and adaptively updating their posteriors with online rewards to adapt to the evolving policy distribution. Extensive experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently outperforms individual RMs, RM ensembling, and existing routing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ä¸­è¿‡åº¦ä¾èµ–å•ä¸€å¥–åŠ±æ¨¡å‹(Reward Model)å¯¼è‡´çš„æ•ˆæœå—é™å’Œè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡ºäº†BayesianRouterã€‚è¿™æ˜¯ä¸€ç§ç»“åˆäº†ç¦»çº¿æ¨¡å‹èƒ½åŠ›å­¦ä¹ ä¸åœ¨çº¿è´å¶æ–¯é€‰æ‹©çš„æ··åˆè·¯ç”±æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹è·¯ç”±(RM routing)æ–¹æ³•åœ¨å†·å¯åŠ¨å’Œæ¢ç´¢æ•ˆç‡ä¸Šçš„ä¸è¶³ã€‚åœ¨ç¦»çº¿é˜¶æ®µï¼Œç³»ç»Ÿé€šè¿‡å¤šä»»åŠ¡è·¯ç”±å™¨åœ¨åå¥½æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥è¯„ä¼°å€™é€‰æ± ä¸­å„å¥–åŠ±æ¨¡å‹çš„å¯é æ€§ï¼›åœ¨åœ¨çº¿é˜¶æ®µï¼Œåˆ©ç”¨è´å¶æ–¯æ±¤æ™®æ£®é‡‡æ ·(Bayesian Thompson sampling)è¿›è¡ŒæŒ‰æŸ¥è¯¢çš„æ¨¡å‹é€‰æ‹©ï¼Œå¹¶æ ¹æ®åœ¨çº¿å¥–åŠ±åŠ¨æ€æ›´æ–°åéªŒåˆ†å¸ƒä»¥é€‚åº”ç­–ç•¥å˜åŒ–ã€‚åœ¨AlpacaEval-2ã€Arena-Hardã€GSM8Kå’ŒMMLUç­‰æŒ‡ä»¤éµå¾ªä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBayesianRouterçš„æ•ˆæœä¸€è‡´ä¼˜äºå•ä¸€æ¨¡å‹ã€æ¨¡å‹é›†æˆ(RM ensembling)åŠç°æœ‰è·¯ç”±ç®—æ³•ã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒå•æ¬¡RMè°ƒç”¨å¼€é”€çš„åŒæ—¶ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†ä¸åŒæ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹é½çš„æ•´ä½“è´¨é‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02850v1",
      "published_date": "2025-10-03 09:37:59 UTC",
      "updated_date": "2025-10-03 09:37:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:35.279882+00:00"
    },
    {
      "arxiv_id": "2510.02848v1",
      "title": "Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech",
      "title_zh": "Flamed-TTSï¼šé¢å‘é«˜æ•ˆç”Ÿæˆä¸åŠ¨æ€è¯­é€Ÿé›¶æ ·æœ¬è¯­éŸ³åˆæˆçš„æµåŒ¹é…æ— æ³¨æ„åŠ›æ¨¡å‹",
      "authors": [
        "Hieu-Nghia Huynh-Nguyen",
        "Huynh Nguyen Dang",
        "Ngoc-Son Nguyen",
        "Van Nguyen"
      ],
      "abstract": "Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling models to synthesize speech from text using short, limited-context prompts. These prompts serve as voice exemplars, allowing the model to mimic speaker identity, prosody, and other traits without extensive speaker-specific data. Although recent approaches incorporating language models, diffusion, and flow matching have proven their effectiveness in zero-shot TTS, they still encounter challenges such as unreliable synthesis caused by token repetition or unexpected content transfer, along with slow inference and substantial computational overhead. Moreover, temporal diversity-crucial for enhancing the naturalness of synthesized speech-remains largely underexplored. To address these challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that emphasizes low computational cost, low latency, and high speech fidelity alongside rich temporal diversity. To achieve this, we reformulate the flow matching training paradigm and incorporate both discrete and continuous representations corresponding to different attributes of speech. Experimental results demonstrate that Flamed-TTS surpasses state-of-the-art models in terms of intelligibility, naturalness, speaker similarity, acoustic characteristics preservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4% compared to the leading zero-shot TTS baselines, while maintaining low latency in inference and high fidelity in generated speech. Code and audio samples are available at our demo page https://flamed-tts.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Flamed-TTSï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Flow Matching çš„ Attention-Free é›¶æ ·æœ¬è¯­éŸ³åˆæˆ (Zero-shot TTS) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨åˆæˆå¯é æ€§ã€æ¨ç†é€Ÿåº¦ä»¥åŠè®¡ç®—å¼€é”€æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡æ„ Flow Matching è®­ç»ƒèŒƒå¼ï¼Œå¹¶ç»“åˆå¯¹åº”äºä¸åŒè¯­éŸ³å±æ€§çš„ç¦»æ•£å’Œè¿ç»­è¡¨ç¤ºï¼Œåœ¨ä¿è¯é«˜è¯­éŸ³ä¿çœŸåº¦çš„åŒæ—¶å®ç°äº†ä½è®¡ç®—æˆæœ¬å’Œä½å»¶è¿Ÿã€‚Flamed-TTS ç‰¹åˆ«å¼ºè°ƒäº†å¢å¼ºè‡ªç„¶åº¦æ‰€å¿…éœ€çš„æ—¶é—´å¤šæ ·æ€§ (Temporal Diversity)ï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰åŠ¨æ€æ­¥é€Ÿ (Dynamic Pacing) çš„è¯­éŸ³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ¸…æ™°åº¦ã€è‡ªç„¶åº¦ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œå£°å­¦ç‰¹å¾ä¿æŒæ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚åœ¨æ¨ç†æµ‹è¯•ä¸­ï¼ŒFlamed-TTS è¾¾åˆ°äº† 4% çš„æœ€ä½å­—é”™è¯¯ç‡ (WER)ï¼Œè¯æ˜äº†å…¶åœ¨é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¤šå˜èŠ‚å¥è¯­éŸ³æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02848v1",
      "published_date": "2025-10-03 09:36:55 UTC",
      "updated_date": "2025-10-03 09:36:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:50.649804+00:00"
    },
    {
      "arxiv_id": "2510.02840v1",
      "title": "Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization",
      "title_zh": "ä¸¥è‚ƒå¯¹å¾… Goodhartï¼šé€šç”¨äººå·¥æ™ºèƒ½ä¼˜åŒ–çš„åŸåˆ™æ€§é™åˆ¶",
      "authors": [
        "Antoine Maier",
        "Aude Maier",
        "Tom David"
      ],
      "abstract": "A common but rarely examined assumption in machine learning is that training yields models that actually satisfy their specified objective function. We call this the Objective Satisfaction Assumption (OSA). Although deviations from OSA are acknowledged, their implications are overlooked. We argue, in a learning-paradigm-agnostic framework, that OSA fails in realistic conditions: approximation, estimation, and optimization errors guarantee systematic deviations from the intended objective, regardless of the quality of its specification. Beyond these technical limitations, perfectly capturing and translating the developer's intent, such as alignment with human preferences, into a formal objective is practically impossible, making misspecification inevitable. Building on recent mathematical results, absent a mathematical characterization of these gaps, they are indistinguishable from those that collapse into Goodhart's law failure modes under strong optimization pressure. Because the Goodhart breaking point cannot be located ex ante, a principled limit on the optimization of General-Purpose AI systems is necessary. Absent such a limit, continued optimization is liable to push systems into predictable and irreversible loss of control.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‘æˆ˜äº†æœºå™¨å­¦ä¹ ä¸­å¸¸è§çš„â€œç›®æ ‡æ»¡è¶³å‡è®¾â€(Objective Satisfaction Assumption, OSA)ï¼Œå³æ¨¡å‹åœ¨è®­ç»ƒåèƒ½å®Œå…¨ç¬¦åˆå…¶æ—¢å®šç›®æ ‡å‡½æ•°ã€‚ä½œè€…é€šè¿‡ä¸€ä¸ªå­¦ä¹ èŒƒå¼æ— å…³çš„æ¡†æ¶è®ºè¯ï¼Œç”±äºè¿‘ä¼¼(approximation)ã€ä¼°è®¡(estimation)åŠä¼˜åŒ–è¯¯å·®ï¼ŒOSAåœ¨ç°å®æ¡ä»¶ä¸‹å¿…ç„¶å¤±æ•ˆã€‚æ­¤å¤–ï¼Œç”±äºå°†å¼€å‘è€…æ„å›¾ï¼ˆå¦‚äººç±»åå¥½å¯¹é½ï¼‰è½¬åŒ–ä¸ºå½¢å¼åŒ–ç›®æ ‡å­˜åœ¨å®è·µä¸Šçš„ä¸å¯èƒ½ï¼Œç›®æ ‡é”™è¯¯è®¾å®š(misspecification)å˜å¾—ä¸å¯é¿å…ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œåœ¨ç¼ºä¹æ•°å­¦è¡¨å¾çš„æƒ…å†µä¸‹ï¼Œå¼ºå¤§çš„ä¼˜åŒ–å‹åŠ›ä¼šè¯±å‘â€œå¤å¾·å“ˆç‰¹å®šå¾‹â€(Goodhart's law)å¼çš„å¤±æ•ˆæ¨¡å¼ã€‚ç”±äºâ€œå¤å¾·å“ˆç‰¹çªç ´ç‚¹â€æ— æ³•è¢«æå‰é¢„çŸ¥ï¼Œå¯¹é€šç”¨äººå·¥æ™ºèƒ½(General-Purpose AI)ç³»ç»Ÿçš„ä¼˜åŒ–å¼ºåº¦è®¾ç½®åŸåˆ™æ€§é™åˆ¶(principled limit)æ˜¯å¿…è¦çš„ã€‚è‹¥ç¼ºä¹æ­¤ç±»é™åˆ¶ï¼ŒæŒç»­çš„ä¼˜åŒ–ææ˜“å¯¼è‡´ç³»ç»Ÿé™·å…¥å¯é¢„æµ‹ä¸”ä¸å¯é€†è½¬çš„å¤±æ§çŠ¶æ€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 1 figure. Under review",
      "pdf_url": "https://arxiv.org/pdf/2510.02840v1",
      "published_date": "2025-10-03 09:25:12 UTC",
      "updated_date": "2025-10-03 09:25:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:45.886177+00:00"
    },
    {
      "arxiv_id": "2510.02839v1",
      "title": "Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics",
      "title_zh": "èåˆçŸ¥è¯†æ„ŸçŸ¥å»ºæ¨¡ä¸é¢‘ç‡è‡ªé€‚åº”å­¦ä¹ çš„ç”µæ± å¥åº·é¢„æµ‹",
      "authors": [
        "Vijay Babu Pamshetti",
        "Wei Zhang",
        "Sumei Sun",
        "Jie Zhang",
        "Yonggang Wen",
        "Qingyu Yan"
      ],
      "abstract": "Battery health prognostics are critical for ensuring safety, efficiency, and sustainability in modern energy systems. However, it has been challenging to achieve accurate and robust prognostics due to complex battery degradation behaviors with nonlinearity, noise, capacity regeneration, etc. Existing data-driven models capture temporal degradation features but often lack knowledge guidance, which leads to unreliable long-term health prognostics. To overcome these limitations, we propose Karma, a knowledge-aware model with frequency-adaptive learning for battery capacity estimation and remaining useful life prediction. The model first performs signal decomposition to derive battery signals in different frequency bands. A dual-stream deep learning architecture is developed, where one stream captures long-term low-frequency degradation trends and the other models high-frequency short-term dynamics. Karma regulates the prognostics with knowledge, where battery degradation is modeled as a double exponential function based on empirical studies. Our dual-stream model is used to optimize the parameters of the knowledge with particle filters to ensure physically consistent and reliable prognostics and uncertainty quantification. Experimental study demonstrates Karma's superior performance, achieving average error reductions of 50.6% and 32.6% over state-of-the-art algorithms for battery health prediction on two mainstream datasets, respectively. These results highlight Karma's robustness, generalizability, and potential for safer and more reliable battery management across diverse applications.",
      "tldr_zh": "å‡†ç¡®é¢„æµ‹ç”µæ± å¥åº·çŠ¶å†µå¯¹èƒ½æºç³»ç»Ÿçš„å®‰å…¨è‡³å…³é‡è¦ï¼Œä½†ç”µæ± é€€åŒ–çš„éçº¿æ€§å’Œå™ªå£°ä½¿ç°æœ‰æ¨¡å‹éš¾ä»¥å®ç°å¯é çš„é•¿æœŸé¢„æµ‹ã€‚è¯¥ç ”ç©¶æå‡ºäº† Karmaï¼Œä¸€ç§ç»“åˆé¢‘ç‡è‡ªé€‚åº”å­¦ä¹  (frequency-adaptive learning) çš„çŸ¥è¯†æ„ŸçŸ¥æ¨¡å‹ï¼Œç”¨äºç”µæ± å®¹é‡ä¼°è®¡å’Œå‰©ä½™ä½¿ç”¨å¯¿å‘½é¢„æµ‹ã€‚æ¨¡å‹é€šè¿‡ä¿¡å·åˆ†è§£ (signal decomposition) è·å–ä¸åŒé¢‘æ®µä¿¡å·ï¼Œå¹¶åˆ©ç”¨åŒæµ (dual-stream) æ¶æ„åˆ†åˆ«å»ºæ¨¡é•¿æœŸä½é¢‘é€€åŒ–è¶‹åŠ¿å’ŒçŸ­æœŸé«˜é¢‘åŠ¨æ€ã€‚Karma é‡‡ç”¨åŒæŒ‡æ•°å‡½æ•° (double exponential function) å¯¹é€€åŒ–è¡Œä¸ºè¿›è¡ŒçŸ¥è¯†è§„çº¦ï¼Œå¹¶ç»“åˆç²’å­æ»¤æ³¢ (particle filters) ä¼˜åŒ–å‚æ•°ä»¥ç¡®ä¿ç‰©ç†ä¸€è‡´æ€§ã€‚å®éªŒæ˜¾ç¤ºï¼ŒKarma åœ¨ä¸¤ä¸ªä¸»æµæ•°æ®é›†ä¸Šçš„é¢„æµ‹è¯¯å·®æ¯”ç°æœ‰å…ˆè¿›ç®—æ³•åˆ†åˆ«é™ä½äº† 50.6% å’Œ 32.6%ã€‚è¯¥ç ”ç©¶è¯æ˜äº† Karma åœ¨ç”µæ± å¥åº·ç®¡ç†ä¸­çš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œä¸ºæ›´å®‰å…¨ã€æ›´å¯é çš„èƒ½æºåº”ç”¨æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 4 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.02839v1",
      "published_date": "2025-10-03 09:24:38 UTC",
      "updated_date": "2025-10-03 09:24:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:48.381972+00:00"
    },
    {
      "arxiv_id": "2510.02837v1",
      "title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents",
      "title_zh": "è¶…è¶Šæœ€ç»ˆç­”æ¡ˆï¼šå·¥å…·å¢å¼ºå‹æ™ºèƒ½ä½“æ¨ç†è½¨è¿¹è¯„ä¼°",
      "authors": [
        "Wonjoong Kim",
        "Sangwu Park",
        "Yeonjun In",
        "Sein Kim",
        "Dongha Lee",
        "Chanyoung Park"
      ],
      "abstract": "Although recent tool-augmented benchmarks incorporate complex user requests and diverse tools, the evaluation methods for most of them remain limited to answer matching. However, as the number of steps required to resolve a user request increases, a proper evaluation of an agent's performance must go beyond the final answer to also assess the problem-solving trajectory, including previously ignored aspects such as efficiency, hallucination, and adaptivity. The most straightforward method for evaluating these aspects is to compare an agent's trajectory with the ground-truth trajectory, but this approach is fundamentally limited since annotating all valid ground-truth trajectories is prohibitively expensive. However, a simple LLM-based evaluator struggles to assess trajectories in detail without ground truth. To effectively evaluate the agents in this manner, we introduce TRACE, a framework for the multi-dimensional evaluation of tool-augmented LLM agent performance. By incorporating an evidence bank, which accumulates knowledge gathered from preceding reasoning steps, TRACE enables a multi-faceted analysis and evaluation of an agent's reasoning trajectory effectively. To validate our framework, we develop a new meta-evaluation dataset by augmenting existing benchmarks with diverse and flawed trajectories, each labeled with multi-faceted performance scores. Our results confirm that TRACE accurately evaluates these complex behaviors in a scalable and cost-effective manner, even with small open-source LLMs. Furthermore, we apply our method to evaluate the trajectories that agents produce while solving tool-augmented tasks, presenting previously unreported observations and their corresponding insights.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥å…·å¢å¼ºå‹æ™ºèƒ½ä½“(tool-augmented agents)è¯„ä¼°æ–¹æ³•å±€é™äºç­”æ¡ˆåŒ¹é…(answer matching)çš„é—®é¢˜ï¼Œæå‡ºäº†è¯„ä¼°æ¨ç†è½¨è¿¹(reasoning trajectories)çš„å¿…è¦æ€§ï¼Œä»¥æ¶µç›–æ•ˆç‡(efficiency)ã€å¹»è§‰(hallucination)å’Œè‡ªé€‚åº”èƒ½åŠ›(adaptivity)ç­‰å…³é”®ç»´åº¦ã€‚ç”±äºæ ‡æ³¨æ‰€æœ‰æœ‰æ•ˆæ¨ç†è·¯å¾„æˆæœ¬è¿‡é«˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†TRACEæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è¯æ®åº“(evidence bank)ç´¯ç§¯æ¨ç†æ­¥éª¤ä¸­çš„çŸ¥è¯†ï¼Œå®ç°å¯¹æ™ºèƒ½ä½“è¡¨ç°çš„å¤šç»´åº¦åˆ†æã€‚ä¸ºäº†éªŒè¯è¯¥æ¡†æ¶ï¼Œç ”ç©¶äººå‘˜é€šè¿‡å¢å¼ºç°æœ‰åŸºå‡†æµ‹è¯•å¼€å‘äº†ä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–ç¼ºé™·è½¨è¿¹çš„æ–°å‹å…ƒè¯„ä¼°æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRACEèƒ½å¤Ÿä»¥å¯æ‰©å±•ä¸”é«˜æ€§ä»·æ¯”çš„æ–¹å¼å‡†ç¡®è¯„ä¼°å¤æ‚è¡Œä¸ºï¼Œå³ä½¿åœ¨ä½¿ç”¨å°å‹å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¶ä¹Ÿèƒ½ä¿æŒé«˜æ•ˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­æ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨è§£å†³å·¥å…·å¢å¼ºä»»åŠ¡è¿‡ç¨‹ä¸­æ­¤å‰æœªè¢«æŠ¥é“çš„è§‚å¯Ÿç»“æœä¸ç›¸å…³è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Under Review",
      "pdf_url": "https://arxiv.org/pdf/2510.02837v1",
      "published_date": "2025-10-03 09:19:15 UTC",
      "updated_date": "2025-10-03 09:19:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:49.863763+00:00"
    },
    {
      "arxiv_id": "2510.02830v1",
      "title": "Evaluating Large Language Models for IUCN Red List Species Information",
      "title_zh": "é’ˆå¯¹IUCNçº¢è‰²åå½•ç‰©ç§ä¿¡æ¯çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Shinya Uryu"
      ],
      "abstract": "Large Language Models (LLMs) are rapidly being adopted in conservation to address the biodiversity crisis, yet their reliability for species evaluation is uncertain. This study systematically validates five leading models on 21,955 species across four core IUCN Red List assessment components: taxonomy, conservation status, distribution, and threats. A critical paradox was revealed: models excelled at taxonomic classification (94.9%) but consistently failed at conservation reasoning (27.2% for status assessment). This knowledge-reasoning gap, evident across all models, suggests inherent architectural constraints, not just data limitations. Furthermore, models exhibited systematic biases favoring charismatic vertebrates, potentially amplifying existing conservation inequities. These findings delineate clear boundaries for responsible LLM deployment: they are powerful tools for information retrieval but require human oversight for judgment-based decisions. A hybrid approach is recommended, where LLMs augment expert capacity while human experts retain sole authority over risk assessment and policy.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†äº”ç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨å¤„ç†21,955ä¸ªç‰©ç§çš„IUCNçº¢è‰²åå½•(IUCN Red List)ç›¸å…³ä¿¡æ¯æ–¹é¢çš„å¯é æ€§ï¼Œæ¶µç›–äº†åˆ†ç±»å­¦(taxonomy)ã€å—å¨èƒçŠ¶å†µ(conservation status)ã€åœ°ç†åˆ†å¸ƒ(distribution)å’Œå—å¨èƒå› ç´ (threats)å››ä¸ªæ ¸å¿ƒç»´åº¦ã€‚å®éªŒæ­ç¤ºäº†ä¸€ä¸ªå…³é”®æ‚–è®ºï¼šæ¨¡å‹åœ¨åˆ†ç±»å­¦åˆ†ç±»ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¾¾94.9%ï¼Œä½†åœ¨å—å¨èƒçŠ¶å†µè¯„ä¼°ç­‰ä¿æŠ¤æ¨ç†(conservation reasoning)ä»»åŠ¡ä¸Šè¡¨ç°æ¬ ä½³ï¼Œå‡†ç¡®ç‡ä»…ä¸º27.2%ã€‚è¿™ç§çŸ¥è¯†ä¸æ¨ç†ä¹‹é—´çš„é¸¿æ²Ÿåœ¨æ‰€æœ‰æ¨¡å‹ä¸­å‡æœ‰ä½“ç°ï¼Œæš—ç¤ºäº†æ¨¡å‹å›ºæœ‰çš„æ¶æ„çº¦æŸè€Œéä»…æ˜¯æ•°æ®å±€é™ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¯¹æ˜æ˜Ÿè„Šæ¤åŠ¨ç‰©(charismatic vertebrates)è¡¨ç°å‡ºç³»ç»Ÿæ€§åè§ï¼Œå¯èƒ½åŠ å‰§ä¿æŠ¤èµ„æºåˆ†é…çš„ä¸å¹³ç­‰ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œè™½ç„¶LLMsæ˜¯é«˜æ•ˆçš„ä¿¡æ¯æ£€ç´¢å·¥å…·ï¼Œä½†åœ¨æ¶‰åŠåˆ¤æ–­æ€§å†³ç­–æ—¶å¿…é¡»å¼•å…¥äººç±»ç›‘ç£ã€‚æœ€åï¼Œç ”ç©¶å»ºè®®é‡‡å–ä¸€ç§æ··åˆæ–¹æ³•(hybrid approach)ï¼Œç”±LLMsè¾…åŠ©ä¸“å®¶å·¥ä½œï¼Œè€Œç”±äººç±»ä¸“å®¶ä¿ç•™é£é™©è¯„ä¼°ä¸æ”¿ç­–åˆ¶å®šçš„æœ€ç»ˆå†³å®šæƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.02830v1",
      "published_date": "2025-10-03 09:09:35 UTC",
      "updated_date": "2025-10-03 09:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:36:58.461834+00:00"
    },
    {
      "arxiv_id": "2510.03372v1",
      "title": "Real-time nonlinear inversion of magnetic resonance elastography with operator learning",
      "title_zh": "åŸºäºç®—å­å­¦ä¹ çš„ç£å…±æŒ¯å¼¹æ€§æˆåƒå®æ—¶éçº¿æ€§åæ¼”",
      "authors": [
        "Juampablo E. Heras Rivera",
        "Caitlin M. Neher",
        "Mehmet Kurt"
      ],
      "abstract": "$\\textbf{Purpose:}$ To develop and evaluate an operator learning framework for nonlinear inversion (NLI) of brain magnetic resonance elastography (MRE) data, which enables real-time inversion of elastograms with comparable spatial accuracy to NLI.\n  $\\textbf{Materials and Methods:}$ In this retrospective study, 3D MRE data from 61 individuals (mean age, 37.4 years; 34 female) were used for development of the framework. A predictive deep operator learning framework (oNLI) was trained using 10-fold cross-validation, with the complex curl of the measured displacement field as inputs and NLI-derived reference elastograms as outputs. A structural prior mechanism, analogous to Soft Prior Regularization in the MRE literature, was incorporated to improve spatial accuracy. Subject-level evaluation metrics included Pearson's correlation coefficient, absolute relative error, and structural similarity index measure between predicted and reference elastograms across brain regions of different sizes to understand accuracy. Statistical analyses included paired t-tests comparing the proposed oNLI variants to the convolutional neural network baselines.\n  $\\textbf{Results:}$ Whole brain absolute percent error was 8.4 $\\pm$ 0.5 ($Î¼'$) and 10.0 $\\pm$ 0.7 ($Î¼''$) for oNLI and 15.8 $\\pm$ 0.8 ($Î¼'$) and 26.1 $\\pm$ 1.1 ($Î¼''$) for CNNs. Additionally, oNLI outperformed convolutional architectures as per Pearson's correlation coefficient, $r$, in the whole brain and across all subregions for both the storage modulus and loss modulus (p < 0.05).\n  $\\textbf{Conclusion:}$ The oNLI framework enables real-time MRE inversion (30,000x speedup), outperforming CNN-based approaches and maintaining the fine-grained spatial accuracy achievable with NLI in the brain.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸€ç§åŸºäºç®—å­å­¦ä¹ (operator learning)çš„æ¡†æ¶oNLIï¼Œæ—¨åœ¨å®ç°è„‘éƒ¨ç£å…±æŒ¯å¼¹æ€§æˆåƒ(MRE)æ•°æ®çš„å®æ—¶éçº¿æ€§åæ¼”(Nonlinear Inversion, NLI)ã€‚ç ”ç©¶é€šè¿‡å¼•å…¥ç±»ä¼¼äºSoft Prior Regularizationçš„ç»“æ„å…ˆéªŒ(structural prior)æœºåˆ¶ï¼Œåˆ©ç”¨æµ‹é‡ä½ç§»åœºçš„å¤æ•°æ—‹åº¦ä½œä¸ºè¾“å…¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é¢„æµ‹å¼¹æ€§å›¾æ—¶çš„ç©ºé—´å‡†ç¡®æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨61åå—è¯•è€…çš„3D MREæ•°æ®è¿›è¡Œäº†10æŠ˜äº¤å‰éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºoNLIåœ¨å…¨è„‘èŒƒå›´å†…çš„ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®æ˜¾è‘—ä½äºä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œ(CNN)åŸºå‡†ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨å­˜å‚¨æ¨¡é‡(storage modulus)å’ŒæŸè€—æ¨¡é‡(loss modulus)çš„ç›¸å…³æ€§è¯„ä»·ä¸­å‡ä¼˜äºç°æœ‰å·ç§¯æ¶æ„ã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒNLIç²¾ç»†ç©ºé—´å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾30,000å€çš„åæ¼”åŠ é€Ÿï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸‹å®æ—¶è·å–é«˜è´¨é‡è„‘éƒ¨å¼¹æ€§æ•°æ®æä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03372v1",
      "published_date": "2025-10-03 08:55:40 UTC",
      "updated_date": "2025-10-03 08:55:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:12.650768+00:00"
    },
    {
      "arxiv_id": "2510.06243v2",
      "title": "CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning",
      "title_zh": "CoT Referringï¼šé€šè¿‡å®šä½æ¨ç†æå‡æŒ‡ä»£è¡¨è¾¾ä»»åŠ¡",
      "authors": [
        "Qihua Dong",
        "Luis Figueroa",
        "Handong Zhao",
        "Kushal Kafle",
        "Jason Kuen",
        "Zhihong Ding",
        "Scott Cohen",
        "Yun Fu"
      ],
      "abstract": "Referring Expression Comprehension and Segmentation are critical tasks for assessing the integration of language understanding and image comprehension, serving as benchmarks for Multimodal Large Language Models (MLLMs) capabilities. To address these challenges, we propose a new strategy, CoT Referring, which enhances model reasoning across modalities through a structured, chain-of-thought training data structure. Our approach systematically parses textual structures to a sequential referring step, where in each step it identifies relationships and ensures consistent reference alignment, thereby improving accuracy in complex query scenarios. We restructure the training data to enforce a new output form, providing new annotations for existing datasets and compiling an evaluation benchmark from existing resources. This benchmark is designed explicitly for complex referring cases. We also integrate detection and segmentation capabilities into a unified MLLM framework, training it with a novel adaptive weighted loss to optimize performance. Experimental results on our curated benchmark and RefCOCO/+/g demonstrate the effectiveness of our approach, with a notable increase of 2.5%+ over baseline models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoT Referringç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ¥æ”¹è¿›æŒ‡ä»£è¡¨è¾¾ç†è§£(Referring Expression Comprehension)å’Œåˆ†å‰²(Segmentation)ä»»åŠ¡ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ„å»ºäº†ä¸€ç§ç»“æ„åŒ–çš„é“¾å¼æ€ç»´(Chain-of-Thought)è®­ç»ƒæ•°æ®ç»“æ„ï¼Œå°†å¤æ‚çš„æ–‡æœ¬ç»“æ„è§£æä¸ºåºåˆ—åŒ–çš„æŒ‡ä»£æ­¥éª¤ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹ç³»ç»Ÿåœ°è¯†åˆ«ç‰©ä½“é—´çš„å…³ç³»å¹¶ç¡®ä¿æŒ‡ä»£å¯¹é½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç­–ç•¥ï¼Œç ”ç©¶è€…å¯¹ç°æœ‰æ•°æ®é›†è¿›è¡Œäº†é‡æ–°æ ‡æ³¨ï¼Œå¹¶æ•´åˆäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¤æ‚æŒ‡ä»£åœºæ™¯çš„è¯„ä¼°åŸºå‡†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å°†æ£€æµ‹ä¸åˆ†å‰²èƒ½åŠ›é›†æˆåˆ°ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)æ¡†æ¶ä¸­ï¼Œå¹¶å¼•å…¥æ–°å‹è‡ªé€‚åº”åŠ æƒæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoT Referringåœ¨RefCOCO/+/gç­‰å¤šä¸ªæ•°æ®é›†ä¸Šæ¯”åŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æå‡äº†2.5%ä»¥ä¸Šï¼Œå……åˆ†éªŒè¯äº†ç»“åˆæ¥åœ°æ¨ç†(Grounded Reasoning)åœ¨å¤„ç†å¤æ‚æŒ‡ä»£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "MLLM, Referring Expression Segmentation",
      "pdf_url": "https://arxiv.org/pdf/2510.06243v2",
      "published_date": "2025-10-03 08:50:21 UTC",
      "updated_date": "2026-01-17 09:14:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:25.544990+00:00"
    },
    {
      "arxiv_id": "2510.02816v1",
      "title": "NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning",
      "title_zh": "NCVï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­ä½æˆæœ¬ç»“æ„åŒ–é”™è¯¯å®šä½çš„èŠ‚ç‚¹çº§ä¸€è‡´æ€§éªŒè¯æ–¹æ³•",
      "authors": [
        "Yulong Zhang",
        "Li Wang",
        "Wei Du",
        "Peilin Li",
        "Yuqin Dai Zhiyuan Zhao",
        "Lingyong Fang",
        "Ziniu Liu",
        "Ru Zhang",
        "Huijia Zhu",
        "Gongshen Liu"
      ],
      "abstract": "Verifying multi-step reasoning in large language models is difficult due to imprecise error localization and high token costs. Existing methods either assess entire reasoning chains, suffering attention dilution, or rely on expensive multi-sampling. We introduce Node-wise Consistency Verification (NCV), a training-free framework that recasts verification as lightweight binary consistency checks at the node level. By decomposing the chain of thought into interconnected verification nodes, NCV precisely localizes errors and avoids unnecessary long-form generation. Experiments demonstrate that our approach enhances interpretability and efficiency, presenting a scalable solution for reliable LLM reasoning verification. On public datasets, NCV achieves a 10\\% to 25\\% improvement in F1 scores over baselines while utilizing $6\\times$~$58\\times$ fewer tokens than traditional methods like CoT-based verifiers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Node-wise Consistency Verification (NCV)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šæ­¥æ¨ç†éªŒè¯ä¸­é¢ä¸´çš„é”™è¯¯å®šä½ä¸ç²¾å‡†å’ŒTokenæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚NCVå°†éªŒè¯è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºèŠ‚ç‚¹å±‚é¢çš„è½»é‡çº§äºŒå…ƒä¸€è‡´æ€§æ£€æŸ¥ï¼Œé€šè¿‡å°†æ€ç»´é“¾(Chain of Thought)åˆ†è§£ä¸ºäº’è¿çš„éªŒè¯èŠ‚ç‚¹æ¥ç²¾ç¡®é”å®šé”™è¯¯ä½ç½®ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆé¿å…äº†å†—é•¿çš„ç”Ÿæˆè¿‡ç¨‹å¹¶å…‹æœäº†æ³¨æ„åŠ›ç¨€é‡Š(Attention Dilution)é—®é¢˜ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨ç†éªŒè¯çš„å¯è§£é‡Šæ€§ä¸æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNCVåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„F1åˆ†æ•°æ¯”åŸºçº¿æ¨¡å‹æé«˜äº†10%è‡³25%ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¯¥æ–¹æ³•æ¶ˆè€—çš„Tokenæ•°é‡æ¯”ä¼ ç»Ÿçš„åŸºäºCoTçš„éªŒè¯å™¨å‡å°‘äº†6åˆ°58å€ï¼Œä¸ºå¯é çš„LLMæ¨ç†éªŒè¯æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02816v1",
      "published_date": "2025-10-03 08:48:04 UTC",
      "updated_date": "2025-10-03 08:48:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:25.948847+00:00"
    },
    {
      "arxiv_id": "2510.06242v1",
      "title": "Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses",
      "title_zh": "å¼€æ”¾å¼ç”¨æˆ·è°ƒæŸ¥å›å¤çš„é€æ˜æ— å‚è€ƒè‡ªåŠ¨è¯„ä¼°",
      "authors": [
        "Subin An",
        "Yugyeong Ji",
        "Junyoung Kim",
        "Heejin Kook",
        "Yang Lu",
        "Josh Seltzer"
      ],
      "abstract": "Open-ended survey responses provide valuable insights in marketing research, but low-quality responses not only burden researchers with manual filtering but also risk leading to misleading conclusions, underscoring the need for effective evaluation. Existing automatic evaluation methods target LLM-generated text and inadequately assess human-written responses with their distinct characteristics. To address such characteristics, we propose a two-stage evaluation framework specifically designed for human survey responses. First, gibberish filtering removes nonsensical responses. Then, three dimensions-effort, relevance, and completeness-are evaluated using LLM capabilities, grounded in empirical analysis of real-world survey data. Validation on English and Korean datasets shows that our framework not only outperforms existing metrics but also demonstrates high practical applicability for real-world applications such as response quality prediction and response rejection, showing strong correlations with expert assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸‚åœºè°ƒç ”ä¸­ä½è´¨é‡å¼€æ”¾å¼è°ƒæŸ¥å›å¤å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªé€æ˜ä¸”æ— å‚è€ƒ(Reference-free)çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾§é‡äº LLM ç”Ÿæˆæ–‡æœ¬è€Œéš¾ä»¥æœ‰æ•ˆè¯„ä¼°äººç±»æ’°å†™å›å¤çš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶ä¸“é—¨è®¾è®¡äº†ä¸¤é˜¶æ®µè¯„ä¼°æµç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨ gibberish filtering å‰”é™¤æ— æ„ä¹‰çš„éšæœºå›å¤ï¼Œç¬¬äºŒé˜¶æ®µåˆ™åŸºäºå¯¹çœŸå®è°ƒæŸ¥æ•°æ®çš„ç»éªŒåˆ†æï¼Œåˆ©ç”¨ LLM èƒ½åŠ›ä» effortã€relevance å’Œ completeness ä¸‰ä¸ªç»´åº¦è¿›è¡Œå¤šç»´è¯„ä¼°ã€‚åœ¨è‹±è¯­å’ŒéŸ©è¯­æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å“åº”è´¨é‡é¢„æµ‹å’Œå“åº”æ‹’ç»(response rejection)ç­‰å®é™…åº”ç”¨ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŒ‡æ ‡ã€‚ç ”ç©¶ç»“æœè¯æ˜è¯¥æ¡†æ¶ä¸ä¸“å®¶è¯„ä¼°å…·æœ‰å¼ºç›¸å…³æ€§ï¼Œä¸ºæå‡çœŸå®åœºæ™¯ä¸‹çš„è°ƒæŸ¥æ•°æ®è´¨é‡æä¾›äº†å…·å¤‡é«˜åº¦å®ç”¨æ€§çš„è‡ªåŠ¨åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP Industry Track",
      "pdf_url": "https://arxiv.org/pdf/2510.06242v1",
      "published_date": "2025-10-03 08:37:33 UTC",
      "updated_date": "2025-10-03 08:37:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:31.655965+00:00"
    },
    {
      "arxiv_id": "2510.02811v1",
      "title": "A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media",
      "title_zh": "ä¸€ç§é¢å‘ç¤¾äº¤åª’ä½“çš„å¯è§£é‡Šæ–‡æœ¬äººæ ¼è¯„ä¼°è®¡ç®—æ¡†æ¶",
      "authors": [
        "Matej GjurkoviÄ‡"
      ],
      "abstract": "Personality refers to individual differences in behavior, thinking, and feeling. With the growing availability of digital footprints, especially from social media, automated methods for personality assessment have become increasingly important. Natural language processing (NLP) enables the analysis of unstructured text data to identify personality indicators. However, two main challenges remain central to this thesis: the scarcity of large, personality-labeled datasets and the disconnect between personality psychology and NLP, which restricts model validity and interpretability. To address these challenges, this thesis presents two datasets -- MBTI9k and PANDORA -- collected from Reddit, a platform known for user anonymity and diverse discussions. The PANDORA dataset contains 17 million comments from over 10,000 users and integrates the MBTI and Big Five personality models with demographic information, overcoming limitations in data size, quality, and label coverage. Experiments on these datasets show that demographic variables influence model validity. In response, the SIMPA (Statement-to-Item Matching Personality Assessment) framework was developed - a computational framework for interpretable personality assessment that matches user-generated statements with validated questionnaire items. By using machine learning and semantic similarity, SIMPA delivers personality assessments comparable to human evaluations while maintaining high interpretability and efficiency. Although focused on personality assessment, SIMPA's versatility extends beyond this domain. Its model-agnostic design, layered cue detection, and scalability make it suitable for various research and practical applications involving complex label taxonomies and variable cue associations with target concepts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨äººæ ¼è¯„ä¼°ä¸­æ•°æ®é›†ç¨€ç¼ºä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸äººæ ¼å¿ƒç†å­¦è„±èŠ‚å¯¼è‡´çš„è§£é‡Šæ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä»Redditè·å–çš„MBTI9kå’ŒPANDORAä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ã€‚PANDORAæ•°æ®é›†æ•´åˆäº†MBTIå’ŒBig Fiveæ¨¡å‹åŠäººå£ç»Ÿè®¡ä¿¡æ¯ï¼Œæœ‰æ•ˆå…‹æœäº†ä»¥å¾€æ•°æ®åœ¨è§„æ¨¡ã€è´¨é‡å’Œæ ‡ç­¾è¦†ç›–èŒƒå›´ä¸Šçš„å±€é™ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†SIMPA (Statement-to-Item Matching Personality Assessment) è®¡ç®—æ¡†æ¶ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ å’Œè¯­ä¹‰ç›¸ä¼¼åº¦(semantic similarity)å°†ç”¨æˆ·é™ˆè¿°ä¸æ ‡å‡†é—®å·æ¡ç›®ç›¸åŒ¹é…ã€‚å®éªŒè¡¨æ˜ï¼ŒSIMPAæ¡†æ¶åœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶ï¼Œèƒ½å¤Ÿæä¾›ä¸äººç±»è¯„ä¼°ç›¸å½“ä¸”å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„ç»“æœã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å‹æ— å…³(model-agnostic)çš„è®¾è®¡ï¼Œå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œåˆ†å±‚çº¿ç´¢æ£€æµ‹èƒ½åŠ›ï¼Œä¸ä»…æå‡äº†äººæ ¼è¯„ä¼°çš„æ•ˆåº¦ï¼Œè¿˜å¯å¹¿æ³›åº”ç”¨äºæ¶‰åŠå¤æ‚æ ‡ç­¾åˆ†ç±»çš„å…¶ä»–ç ”ç©¶ä¸å®é™…åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "Phd thesis",
      "pdf_url": "https://arxiv.org/pdf/2510.02811v1",
      "published_date": "2025-10-03 08:36:36 UTC",
      "updated_date": "2025-10-03 08:36:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:35.446945+00:00"
    },
    {
      "arxiv_id": "2510.02810v1",
      "title": "Dissecting Transformers: A CLEAR Perspective towards Green AI",
      "title_zh": "Transformer æ·±åº¦å‰–æï¼šè¿ˆå‘ç»¿è‰²äººå·¥æ™ºèƒ½çš„ CLEAR è§†è§’",
      "authors": [
        "Hemang Jain",
        "Shailender Goyal",
        "Divyansh Pandey",
        "Karthik Vaidhyanathan"
      ],
      "abstract": "The rapid adoption of Large Language Models (LLMs) has raised significant environmental concerns. Unlike the one-time cost of training, LLM inference occurs continuously at a global scale and now dominates the AI energy footprint. Yet, most sustainability studies report only coarse, model-level metrics due to the lack of fine-grained measurement methods, treating energy efficiency more as an afterthought than as a primary objective. We present the first fine-grained empirical analysis of inference energy across core components of transformer architecture. We propose a novel methodology, Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome temporal mismatch between microsecond scale component execution and monitoring of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models spanning four distinct architecture types and consistently keep component-wise energy variance below 9.5\\% while capturing more than 90\\% of the model's total energy as individual components. Our empirical analysis reveals that Attention blocks consume significantly more energy per floating-point operation (FLOP), indicating that energy consumption is not proportionally aligned with FLOP counts. This shows that FLOPs alone fail to capture the true energy cost at a component level. Our findings establish detailed component-level energy baselines and provide insight as an initial step to build energy-efficient transformer models through component-level optimizations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CLEAR (Component-Level Energy Assessment via Repeated sampling)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ Transformer æ¶æ„æ ¸å¿ƒç»„ä»¶æ¨ç†èƒ½è€—çš„ç»†ç²’åº¦å®è¯åˆ†ææ–¹æ³•ï¼Œæ—¨åœ¨åº”å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†å¸¦æ¥çš„ç¯å¢ƒå½±å“ã€‚ä¸ºäº†å…‹æœå¾®ç§’çº§ç»„ä»¶æ‰§è¡Œä¸æ¯«ç§’çº§èƒ½é‡ä¼ æ„Ÿå™¨ç›‘æ§ä¹‹é—´çš„æ—¶é—´ä¸åŒ¹é…ï¼ŒCLEAR é€šè¿‡é‡å¤é‡‡æ ·æŠ€æœ¯å®ç°äº†ç²¾ç¡®çš„èƒ½è€—æµ‹é‡ï¼Œå¹¶åœ¨å¯¹15ä¸ªä¸åŒæ¶æ„æ¨¡å‹çš„è¯„ä¼°ä¸­æˆåŠŸæ•æ‰äº†è¶…è¿‡90%çš„æ¨¡å‹æ€»èƒ½è€—ã€‚å®è¯åˆ†ææ­ç¤ºäº† Attention æ¨¡å—æ¯æµ®ç‚¹è¿ç®— (FLOP) æ¶ˆè€—çš„èƒ½é‡æ˜¾è‘—æ›´é«˜ï¼Œè¯æ˜äº†ä»…å‡­ FLOPs æ— æ³•å‡†ç¡®è¡¡é‡ç»„ä»¶çº§åˆ«çš„çœŸå®èƒ½è€—ã€‚è¯¥ç ”ç©¶ä¸º Transformer æ¨¡å‹å»ºç«‹äº†è¯¦ç»†çš„ç»„ä»¶çº§èƒ½è€—åŸºå‡†ï¼Œå¹¶ä¸ºé€šè¿‡ç»„ä»¶çº§ä¼˜åŒ–å®ç° Green AI å’Œæ„å»ºèŠ‚èƒ½æ¨¡å‹æä¾›äº†å…³é”®æ´å¯Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02810v1",
      "published_date": "2025-10-03 08:33:07 UTC",
      "updated_date": "2025-10-03 08:33:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:34.445139+00:00"
    },
    {
      "arxiv_id": "2510.02809v2",
      "title": "Relevance-Aware Thresholding in Online Conformal Prediction for Time Series",
      "title_zh": "æ—¶é—´åºåˆ—åœ¨çº¿ç¬¦åˆæ€§é¢„æµ‹ä¸­çš„ç›¸å…³æ€§æ„ŸçŸ¥é˜ˆå€¼åŒ–",
      "authors": [
        "ThÃ©o Dupuy",
        "Binbin Xu",
        "StÃ©phane Perrey",
        "Jacky Montmain",
        "Abdelhak Imoussaten"
      ],
      "abstract": "Uncertainty quantification has received considerable interest in recent works in Machine Learning. In particular, Conformal Prediction (CP) gains ground in this field. For the case of time series, Online Conformal Prediction (OCP) becomes an option to address the problem of data distribution shift over time. Indeed, the idea of OCP is to update a threshold of some quantity (whether the miscoverage level or the quantile) based on the distribution observation. To evaluate the performance of OCP methods, two key aspects are typically considered: the coverage validity and the prediction interval width minimization. Recently, new OCP methods have emerged, offering long-run coverage guarantees and producing more informative intervals. However, during the threshold update step, most of these methods focus solely on the validity of the prediction intervals~--~that is, whether the ground truth falls inside or outside the interval~--~without accounting for their relevance. In this paper, we aim to leverage this overlooked aspect. Specifically, we propose enhancing the threshold update step by replacing the binary evaluation (inside/outside) with a broader class of functions that quantify the relevance of the prediction interval using the ground truth. This approach helps prevent abrupt threshold changes, potentially resulting in narrower prediction intervals. Indeed, experimental results on real-world datasets suggest that these functions can produce tighter intervals compared to existing OCP methods while maintaining coverage validity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—é¢†åŸŸä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨åœ¨çº¿ä¿å½¢é¢„æµ‹ (Online Conformal Prediction, OCP) åœ¨åº”å¯¹æ•°æ®åˆ†å¸ƒåç§»æ—¶çš„è¡¨ç°ã€‚é’ˆå¯¹ç°æœ‰ OCP æ–¹æ³•åœ¨é˜ˆå€¼æ›´æ–°è¿‡ç¨‹ä¸­ä»…ä¾èµ–äºäºŒå…ƒè¯„ä¼°ï¼ˆå³çœŸå€¼æ˜¯å¦è½åœ¨é¢„æµ‹åŒºé—´å†…ï¼‰è€Œå¿½è§†åŒºé—´ç›¸å…³æ€§ (relevance) çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¢å¼ºçš„é˜ˆå€¼æ›´æ–°æœºåˆ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç±»æ›´å¹¿æ³›çš„å‡½æ•°æ¥é‡åŒ–é¢„æµ‹åŒºé—´ä¸çœŸå€¼ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œå–ä»£ä¼ ç»Ÿçš„äºŒå…ƒè¯„ä»·æ–¹å¼ã€‚è¿™ç§æ”¹è¿›èƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢é˜ˆå€¼çš„çªå‘æ€§å‰§çƒˆå˜åŒ–ï¼Œè¿›è€Œç”Ÿæˆæ›´çª„ã€æ›´å…·ä¿¡æ¯é‡çš„é¢„æµ‹åŒºé—´ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¦†ç›–æœ‰æ•ˆæ€§ (coverage validity) çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ¯”ç°æœ‰ OCP æ–¹æ³•äº§ç”Ÿæ›´ç´§è‡´çš„é¢„æµ‹åŒºé—´ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹çš„ç²¾ç¡®åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for The 28th European Conference on Artificial Intelligence 2025, Workshop HC@AIxIA+HYDRA 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.02809v2",
      "published_date": "2025-10-03 08:31:14 UTC",
      "updated_date": "2025-10-06 06:51:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:41.918505+00:00"
    },
    {
      "arxiv_id": "2510.03371v1",
      "title": "Distributed Low-Communication Training with Decoupled Momentum Optimization",
      "title_zh": "åŸºäºè§£è€¦åŠ¨é‡ä¼˜åŒ–çš„åˆ†å¸ƒå¼ä½é€šä¿¡è®­ç»ƒ",
      "authors": [
        "Sasho Nedelkoski",
        "Alexander Acker",
        "Odej Kao",
        "Soeren Becker",
        "Dominik Scheinert"
      ],
      "abstract": "The training of large models demands substantial computational resources, typically available only in data centers with high-bandwidth interconnects. However, reducing the reliance on high-bandwidth interconnects between nodes enables the use of distributed compute resources as an alternative to centralized data center training. Building on recent advances in distributed model training, we propose an approach that further reduces communication by combining infrequent synchronizations across distributed model replicas with gradient momentum compression. In particular, we treat the optimizer momentum as a signal and decompose the Nesterov momentum into high- and low-frequency components via the discrete cosine transform (DCT). Only the high-frequency components are synchronized across model replicas every $H$ steps. Empirically, our method achieves up to a $16\\times$ reduction in communication compared to the baseline DiLoCo, and it generalizes across architectures, including transformer-based language models and convolutional neural networks for images. Overall, this work advances the feasibility of training large models on distributed nodes with low-bandwidth interconnects.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨¡å‹è®­ç»ƒå¯¹é«˜å¸¦å®½æ•°æ®ä¸­å¿ƒäº’è¿çš„ä¾èµ–ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆä½é¢‘åŒæ­¥ä¸æ¢¯åº¦åŠ¨é‡å‹ç¼©(gradient momentum compression)çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†ä¼˜åŒ–å™¨åŠ¨é‡è§†ä¸ºä¿¡å·ï¼Œé€šè¿‡ç¦»æ•£ä½™å¼¦å˜æ¢(Discrete Cosine Transform, DCT)å°† Nesterov momentum åˆ†è§£ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†é‡ï¼Œä¸”åœ¨è®­ç»ƒä¸­æ¯éš” $H$ æ­¥ä»…åŒæ­¥é«˜é¢‘éƒ¨åˆ†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ Transformer è¯­è¨€æ¨¡å‹å’Œå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ç­‰å¤šç§æ¶æ„ä¸Šå‡å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œä¸”ç›¸æ¯”äº DiLoCo åŸºå‡†å®ç°äº†é«˜è¾¾ 16 å€çš„é€šä¿¡å‡é‡ã€‚è¯¥ç ”ç©¶æ˜¾è‘—æå‡äº†åœ¨ä½å¸¦å®½äº’è¿çš„åˆ†å¸ƒå¼èŠ‚ç‚¹ä¸Šè®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºåˆ©ç”¨éä¸­å¿ƒåŒ–è®¡ç®—èµ„æºè¿›è¡Œé«˜æ•ˆæ¨¡å‹è®­ç»ƒæä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 - DynaFront 2025: Dynamics at the Frontiers of Optimization, Sampling, and Games Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.03371v1",
      "published_date": "2025-10-03 08:25:21 UTC",
      "updated_date": "2025-10-03 08:25:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:42.747436+00:00"
    },
    {
      "arxiv_id": "2510.02803v1",
      "title": "Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving",
      "title_zh": "æ–½å·¥åŒºåŸŸå¯¹ VLM è½¨è¿¹è§„åˆ’çš„æŒ‘æˆ˜ï¼šè¿ˆå‘ç¼“è§£ç­–ç•¥ä¸é²æ£’è‡ªåŠ¨é©¾é©¶",
      "authors": [
        "Yifan Liao",
        "Zhen Sun",
        "Xiaoyun Qiu",
        "Zixiao Zhao",
        "Wenbing Tang",
        "Xinlei He",
        "Xinhu Zheng",
        "Tianwei Zhang",
        "Xinyi Huang",
        "Xingshuo Han"
      ],
      "abstract": "Visual Language Models (VLMs), with powerful multimodal reasoning capabilities, are gradually integrated into autonomous driving by several automobile manufacturers to enhance planning capability in challenging environments. However, the trajectory planning capability of VLMs in work zones, which often include irregular layouts, temporary traffic control, and dynamically changing geometric structures, is still unexplored. To bridge this gap, we conduct the \\textit{first} systematic study of VLMs for work zone trajectory planning, revealing that mainstream VLMs fail to generate correct trajectories in $68.0%$ of cases. To better understand these failures, we first identify candidate patterns via subgraph mining and clustering analysis, and then confirm the validity of $8$ common failure patterns through human verification. Building on these findings, we propose REACT-Drive, a trajectory planning framework that integrates VLMs with Retrieval-Augmented Generation (RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases into constraint rules and executable trajectory planning code, while RAG retrieves similar patterns in new scenarios to guide trajectory generation. Experimental results on the ROADWork dataset show that REACT-Drive yields a reduction of around $3\\times$ in average displacement error relative to VLM baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the lowest inference time ($0.58$s) compared with other methods such as fine-tuning ($17.90$s). We further conduct experiments using a real vehicle in 15 work zone scenarios in the physical world, demonstrating the strong practicality of REACT-Drive.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é“è·¯æ–½å·¥åŒº(Work Zones)å¤æ‚ç¯å¢ƒä¸‹è½¨è¿¹è§„åˆ’èƒ½åŠ›ä¸è¶³çš„é—®é¢˜è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œæ­ç¤ºäº†ä¸»æµæ¨¡å‹åœ¨68.0%çš„æƒ…å†µä¸‹ä¼šå‘ç”Ÿå¤±æ•ˆï¼Œå¹¶è¯†åˆ«å‡º8ç§å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†REACT-Driveè½¨è¿¹è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡å°†VLMsä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ç›¸ç»“åˆï¼Œå°†å…ˆå‰çš„å¤±è´¥æ¡ˆä¾‹è½¬åŒ–ä¸ºçº¦æŸè§„åˆ™å’Œå¯æ‰§è¡Œä»£ç ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ROADWorkæ•°æ®é›†ä¸Šï¼ŒREACT-Driveåœ¨ä½¿ç”¨Qwen2.5-VLè¯„ä¼°æ—¶ä½¿å¹³å‡ä½ç§»è¯¯å·®(Average Displacement Error)é™ä½äº†çº¦3å€ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¯¥æ¡†æ¶çš„æ¨ç†æ—¶é—´(Inference Time)ä»…ä¸º0.58ç§’ï¼Œè¿œä½äºå¾®è°ƒ(Fine-tuning)ç­‰ä¼ ç»Ÿæ–¹æ³•ã€‚æœ€åï¼Œé€šè¿‡åœ¨15ä¸ªçœŸå®ç‰©ç†ä¸–ç•Œæ–½å·¥åœºæ™¯ä¸­çš„å®è½¦æµ‹è¯•ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†REACT-Driveåœ¨æå‡è‡ªåŠ¨é©¾é©¶é²æ£’æ€§æ–¹é¢çš„å¼ºå¤§å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "13 pages,5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.02803v1",
      "published_date": "2025-10-03 08:21:15 UTC",
      "updated_date": "2025-10-03 08:21:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:44.353835+00:00"
    },
    {
      "arxiv_id": "2510.05158v1",
      "title": "Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework",
      "title_zh": "Lang-PINNï¼šåŸºäºå¤šæ™ºèƒ½ä½“æ¡†æ¶çš„ä»è¯­è¨€åˆ°ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Xin He",
        "Liangliang You",
        "Hongduan Tian",
        "Bo Han",
        "Ivor Tsang",
        "Yew-Soon Ong"
      ],
      "abstract": "Physics-informed neural networks (PINNs) provide a powerful approach for solving partial differential equations (PDEs), but constructing a usable PINN remains labor-intensive and error-prone. Scientists must interpret problems as PDE formulations, design architectures and loss functions, and implement stable training pipelines. Existing large language model (LLM) based approaches address isolated steps such as code generation or architecture suggestion, but typically assume a formal PDE is already specified and therefore lack an end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system that builds trainable PINNs directly from natural language task descriptions. Lang-PINN coordinates four complementary agents: a PDE Agent that parses task descriptions into symbolic PDEs, a PINN Agent that selects architectures, a Code Agent that generates modular implementations, and a Feedback Agent that executes and diagnoses errors for iterative refinement. This design transforms informal task statements into executable and verifiable PINN code. Experiments show that Lang-PINN achieves substantially lower errors and greater robustness than competitive baselines: mean squared error (MSE) is reduced by up to 3--5 orders of magnitude, end-to-end execution success improves by more than 50\\%, and reduces time overhead by up to 74\\%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Lang-PINNï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨å°†è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ç›´æ¥è½¬åŒ–ä¸ºå¯è®­ç»ƒçš„ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(Physics-Informed Neural Networks, PINNs)ã€‚Lang-PINNé€šè¿‡åè°ƒPDE Agentã€PINN Agentã€Code Agentå’ŒFeedback Agentå››ä¸ªäº’è¡¥æ™ºèƒ½ä½“ï¼Œå®ç°äº†ä»è§£æç¬¦å·åå¾®åˆ†æ–¹ç¨‹(PDEs)åˆ°ä»£ç ç”ŸæˆåŠè¿­ä»£é”™è¯¯è¯Šæ–­çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶å‡æ–¹è¯¯å·®(MSE)æ¯”ç°æœ‰åŸºçº¿æ¨¡å‹é™ä½äº†3è‡³5ä¸ªæ•°é‡çº§ã€‚æ­¤å¤–ï¼ŒLang-PINNå°†ç«¯åˆ°ç«¯æ‰§è¡ŒæˆåŠŸç‡æå‡äº†50%ä»¥ä¸Šï¼Œå¹¶å‡å°‘äº†é«˜è¾¾74%çš„æ—¶é—´å¼€é”€ã€‚è¯¥æˆæœæœ‰æ•ˆè§£å†³äº†æ„å»ºPINNè¿‡ç¨‹ä¸­äººåŠ›æŠ•å…¥å¤§ä¸”æ˜“å‡ºé”™çš„é—®é¢˜ï¼Œä¸ºå¤æ‚ç§‘å­¦è®¡ç®—ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "PINN, PDE, Agent, LLM",
      "pdf_url": "https://arxiv.org/pdf/2510.05158v1",
      "published_date": "2025-10-03 08:20:02 UTC",
      "updated_date": "2025-10-03 08:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:51.681008+00:00"
    },
    {
      "arxiv_id": "2510.02798v1",
      "title": "OptunaHub: A Platform for Black-Box Optimization",
      "title_zh": "OptunaHubï¼šé»‘ç›’ä¼˜åŒ–å¹³å°",
      "authors": [
        "Yoshihiko Ozaki",
        "Shuhei Watanabe",
        "Toshihiko Yanase"
      ],
      "abstract": "Black-box optimization (BBO) drives advances in domains such as AutoML and Materials Informatics, yet research efforts often remain fragmented across domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform that centralizes BBO methods and benchmarks. OptunaHub provides unified Python APIs, a contributor package registry, and a web interface to promote searchability and cross-domain research. OptunaHub aims to foster a virtuous cycle of contributions and applications. The source code is publicly available in the optunahub, optunahub-registry, and optunahub-web repositories under the Optuna organization on GitHub (https://github.com/optuna/).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† OptunaHubï¼Œä¸€ä¸ªæ—¨åœ¨æ•´åˆé»‘ç›’ä¼˜åŒ– (Black-box optimization, BBO) æ–¹æ³•ä¸åŸºå‡†æµ‹è¯•çš„ç¤¾åŒºå¹³å°ï¼Œä»¥è§£å†³å½“å‰è¯¥é¢†åŸŸåœ¨è‡ªåŠ¨æœºå™¨å­¦ä¹  (AutoML) å’Œææ–™ä¿¡æ¯å­¦ (Materials Informatics) ç­‰è·¨å­¦ç§‘ç ”ç©¶ä¸­å­˜åœ¨çš„ç¢ç‰‡åŒ–é—®é¢˜ã€‚OptunaHub æä¾›äº†ç»Ÿä¸€çš„ Python APIsã€è´¡çŒ®è€…è½¯ä»¶åŒ…æ³¨å†Œè¡¨ (package registry) å’Œç›´è§‚çš„ Web ç•Œé¢ï¼Œæ˜¾è‘—æå‡äº†ä¼˜åŒ–ç®—æ³•çš„å¯æœç´¢æ€§ã€‚è¯¥å¹³å°çš„æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡å»ºç«‹è‰¯æ€§å¾ªç¯ï¼Œä¿ƒè¿› BBO æŠ€æœ¯çš„æŒç»­è´¡çŒ®ä¸å¹¿æ³›åº”ç”¨ã€‚ç›¸å…³æºä»£ç å·²åœ¨ GitHub çš„ Optuna ç»„ç»‡ä¸‹å®Œå…¨å…¬å¼€ï¼Œæ¶µç›–äº†æ ¸å¿ƒåº“ã€æ³¨å†Œè¡¨åŠ Web ç«¯ç­‰å¤šä¸ªä»“åº“ã€‚è¿™ä¸€å¹³å°çš„å»ºç«‹ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç¯å¢ƒï¼Œä»è€Œæ›´é«˜æ•ˆåœ°å…±äº«å¹¶æµ‹è¯•æœ€å‰æ²¿çš„ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to Journal of machine learning research",
      "pdf_url": "https://arxiv.org/pdf/2510.02798v1",
      "published_date": "2025-10-03 08:11:36 UTC",
      "updated_date": "2025-10-03 08:11:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:37:56.747258+00:00"
    },
    {
      "arxiv_id": "2510.02795v1",
      "title": "Pareto-optimal Non-uniform Language Generation",
      "title_zh": "å¸•ç´¯æ‰˜æœ€ä¼˜çš„éå‡åŒ€è¯­è¨€ç”Ÿæˆ",
      "authors": [
        "Moses Charikar",
        "Chirag Pabbaraju"
      ],
      "abstract": "Kleinberg and Mullainathan (2024) recently proposed an interesting model for language generation in the limit: Given a countable collection of languages, and an adversary enumerating the strings of some language $L$ from the collection, the objective is to generate new strings from the target language, such that all strings generated beyond some finite time are valid. Li, Raman and Tewari (2024) and Charikar and Pabbaraju (2024) showed strong non-uniform generation guarantees in this model, giving algorithms that generate new valid strings from $L$ after seeing a number of distinct input strings $t(L)$ that depends only on $L$ (and the collection), but not the enumeration order. However, for both these works, the language-wise generation times $t(L)$ of the algorithm can be strictly sub-optimal.\n  In this work, we study Pareto-optimality of non-uniform language generation in the limit. We propose an algorithm, whose generation times $t^\\star(L)$ are (almost) Pareto-optimal: any other algorithm whose generation time for some language $L$ is strictly smaller than $t^\\star(L)$, must satisfy that its generation time for some other language $L'$ is strictly worse than $t^\\star(L')$. Pareto-optimality is essentially the best that one can achieve for non-uniform generation. Our algorithmic framework conveniently adapts to further give Pareto-optimal non-uniform generation algorithms in the practically motivated settings of noisy as well as representative generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æé™æ¡ä»¶ä¸‹çš„è¯­è¨€ç”Ÿæˆï¼ˆlanguage generation in the limitï¼‰é—®é¢˜ï¼Œæ—¨åœ¨ä»å¯¹æ‰‹æšä¸¾çš„ç›®æ ‡è¯­è¨€å­—ç¬¦ä¸²ä¸­ç”Ÿæˆæ–°çš„æœ‰æ•ˆå­—ç¬¦ä¸²ã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶åœ¨éå‡åŒ€ç”Ÿæˆï¼ˆnon-uniform generationï¼‰æ—¶é—´ä¸Šå­˜åœ¨çš„æ¬¡ä¼˜æ€§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªèƒ½å¤Ÿå®ç°ï¼ˆå‡ ä¹ï¼‰å¸•ç´¯æ‰˜æœ€ä¼˜ï¼ˆPareto-optimalï¼‰ç”Ÿæˆæ—¶é—´çš„ç®—æ³•æ¡†æ¶ã€‚è¿™æ„å‘³ç€åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œä»»ä½•è¯•å›¾è¿›ä¸€æ­¥ç¼©çŸ­æŸä¸€ç‰¹å®šè¯­è¨€ç”Ÿæˆæ—¶é—´çš„ç®—æ³•ï¼Œéƒ½å¿…ç„¶ä¼šä»¥ç‰ºç‰²å¦ä¸€è¯­è¨€çš„ç”Ÿæˆæ•ˆç‡ä¸ºä»£ä»·ã€‚è¯¥ç ”ç©¶ä¸ä»…ç¡®ç«‹äº†éå‡åŒ€ç”Ÿæˆçš„æ€§èƒ½è¾¹ç•Œï¼Œè¿˜å°†è¯¥æ¡†æ¶æˆåŠŸæ‰©å±•åˆ°äº†æ›´å…·å®é™…æ„ä¹‰çš„å™ªå£°ç”Ÿæˆï¼ˆnoisy generationï¼‰å’Œä»£è¡¨æ€§ç”Ÿæˆï¼ˆrepresentative generationï¼‰åœºæ™¯ä¸­ã€‚é€šè¿‡è¿™ä¸€è´¡çŒ®ï¼Œè¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆä¸”ç†è®ºå®Œå¤‡çš„è¯­è¨€ç”Ÿæˆç®—æ³•æä¾›äº†æ ¸å¿ƒçš„æ–¹æ³•è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "24 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2510.02795v1",
      "published_date": "2025-10-03 08:08:20 UTC",
      "updated_date": "2025-10-03 08:08:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:07.552421+00:00"
    },
    {
      "arxiv_id": "2510.02790v1",
      "title": "MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding",
      "title_zh": "MaskCDï¼šåŸºäºå›¾åƒå¤´æ©ç å¯¹æ¯”è§£ç çš„ LVLM å¹»è§‰ç¼“è§£",
      "authors": [
        "Jingyuan Deng",
        "Yujiu Yang"
      ],
      "abstract": "Large vision-language models (LVLMs) have shown remarkable performance in visual-language understanding for downstream multimodal tasks. While their capabilities are improving, problems emerge simultaneously. Among those problems, the hallucinations have attracted much attention, which stands for the phenomenon where LVLMs generate contradictory content to their input visual and text contents. Many approaches have been proposed to deal with this issue, such as contrastive decoding and attention manipulation. However, contrastive decoding methods struggle in constructing appropriate contrastive samples, and attention manipulation methods are highly sensitive, lacking stability. In this work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach utilizes the \"image heads\" in LVLMs, masking them to construct contrastive samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The results demonstrate that MaskCD effectively alleviates the phenomenon of hallucinations and retains the general capabilities of LVLMs. Corresponding resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¹»è§‰(hallucinations)é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰å¯¹æ¯”è§£ç (contrastive decoding)åœ¨æ„å»ºæ ·æœ¬ä¸Šçš„å›°éš¾ä»¥åŠæ³¨æ„åŠ›æ“çºµæ–¹æ³•ä¸ç¨³å®šçš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†MaskCDï¼ˆimage head Masked Contrastive Decodingï¼‰ï¼Œé€šè¿‡å±è”½æ¨¡å‹å†…éƒ¨çš„â€œå›¾åƒå¤´â€(image heads)æ¥æ„å»ºæœ‰æ•ˆçš„å¯¹æ¯”æ ·æœ¬ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨LLaVA-1.5-7bå’ŒQwen-VL-7bæ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹¶ä½¿ç”¨äº†CHAIRã€POPEã€AMBERå’ŒMMEç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•è¿›è¡ŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskCDèƒ½å¤Ÿæ˜¾è‘—ç¼“è§£å¹»è§‰ç°è±¡ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™LVLMsåŸæœ‰çš„é€šç”¨èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ºæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§æä¾›äº†ä¸€ç§ç¨³å®šä¸”é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted to emnlp2025 findings",
      "pdf_url": "https://arxiv.org/pdf/2510.02790v1",
      "published_date": "2025-10-03 07:59:16 UTC",
      "updated_date": "2025-10-03 07:59:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:12.752684+00:00"
    },
    {
      "arxiv_id": "2510.02789v1",
      "title": "Align Your Query: Representation Alignment for Multimodality Medical Object Detection",
      "title_zh": "Align Your Queryï¼šé¢å‘å¤šæ¨¡æ€åŒ»å­¦ç›®æ ‡æ£€æµ‹çš„è¡¨å¾å¯¹é½",
      "authors": [
        "Ara Seo",
        "Bryan Sangwoo Kim",
        "Hyungjin Chung",
        "Jong Chul Ye"
      ],
      "abstract": "Medical object detection suffers when a single detector is trained on mixed medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and disjoint representation spaces. To address this challenge, we turn to representation alignment, an approach that has proven effective for bringing features from different sources into a shared space. Specifically, we target the representations of DETR-style object queries and propose a simple, detector-agnostic framework to align them with modality context. First, we define modality tokens: compact, text-derived embeddings encoding imaging modality that are lightweight and require no extra annotations. We integrate the modality tokens into the detection process via Multimodality Context Attention (MoCA), mixing object-query representations via self-attention to propagate modality context within the query set. This preserves DETR-style architectures and adds negligible latency while injecting modality cues into object queries. We further introduce QueryREPA, a short pretraining stage that aligns query representations to their modality tokens using a task-specific contrastive objective with modality-balanced batches. Together, MoCA and QueryREPA produce modality-aware, class-faithful queries that transfer effectively to downstream training. Across diverse modalities trained altogether, the proposed approach consistently improves AP with minimal overhead and no architectural modifications, offering a practical path toward robust multimodality medical object detection. Project page: https://araseo.github.io/alignyourquery/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ··åˆåŒ»å­¦æ¨¡æ€ï¼ˆå¦‚CXR, CT, MRIï¼‰ä¸‹åŒ»ç–—ç›®æ ‡æ£€æµ‹å› æ•°æ®å¼‚æ„å’Œè¡¨å¾ç©ºé—´ä¸ç›¸äº¤è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šç”¨çš„è¡¨å¾å¯¹é½æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å°†DETR-styleçš„æŸ¥è¯¢ï¼ˆobject queriesï¼‰è¡¨å¾ä¸æ¨¡æ€ä¸Šä¸‹æ–‡è¿›è¡Œå¯¹é½ï¼Œä»¥å®ç°è·¨æ¨¡æ€çš„é«˜æ•ˆå­¦ä¹ ã€‚ç ”ç©¶é¦–å…ˆå®šä¹‰äº†Modality tokensï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”æ— éœ€é¢å¤–æ ‡æ³¨çš„æ–‡æœ¬è¡ç”ŸåµŒå…¥ï¼Œç”¨äºç¼–ç æˆåƒæ¨¡æ€ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥Multimodality Context Attention (MoCA)æœºåˆ¶ï¼Œæ¡†æ¶åˆ©ç”¨è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰åœ¨æŸ¥è¯¢é›†ä¸­ä¼ æ’­æ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œåœ¨æä½å»¶è¿Ÿä¸‹ä¸ºæŸ¥è¯¢æ³¨å…¥æ¨¡æ€çº¿ç´¢ã€‚æ­¤å¤–ï¼ŒQueryREPAé¢„è®­ç»ƒé˜¶æ®µé€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥å¼ºåŒ–äº†æŸ¥è¯¢è¡¨å¾ä¸æ¨¡æ€tokençš„å…³è”ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„å‰æä¸‹ï¼Œèƒ½æ˜¾è‘—æå‡å¤šç§æ¨¡æ€ä¸‹çš„APæŒ‡æ ‡ï¼Œä¸ºå®ç°ç¨³å¥çš„å¤šæ¨¡æ€åŒ»ç–—ç›®æ ‡æ£€æµ‹æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://araseo.github.io/alignyourquery/",
      "pdf_url": "https://arxiv.org/pdf/2510.02789v1",
      "published_date": "2025-10-03 07:49:21 UTC",
      "updated_date": "2025-10-03 07:49:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:20.654887+00:00"
    },
    {
      "arxiv_id": "2510.03370v2",
      "title": "InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions",
      "title_zh": "InstructPLM-muï¼š1å°æ—¶å¾®è°ƒESM2åœ¨è›‹ç™½è´¨çªå˜é¢„æµ‹ä¸­è¶…è¶ŠESM3",
      "authors": [
        "Junde Xu",
        "Yapin Shi",
        "Lijun Lang",
        "Taoyong Cui",
        "Zhiming Zhang",
        "Guangyong Chen",
        "Jiezhong Qiu",
        "Pheng-Ann Heng"
      ],
      "abstract": "Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \\textit{Can multimodal fine-tuning of a pretrained, sequence-only protein language model match the performance of models trained end-to-end? } Surprisingly, our experiments show that fine-tuning ESM2 with structural inputs can reach performance comparable to ESM3. To understand how this is achieved, we systematically compare three different feature-fusion designs and fine-tuning recipes. Our results reveal that both the fusion method and the tuning strategy strongly affect final accuracy, indicating that the fine-tuning process is not trivial. We hope this work offers practical guidance for injecting structure into pretrained protein language models and motivates further research on better fusion mechanisms and fine-tuning protocols.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InstructPLM-muï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è›‹ç™½è´¨è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨éªŒè¯å¯¹ä»…å«åºåˆ—ä¿¡æ¯çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€ (multimodal) å¾®è°ƒæ˜¯å¦èƒ½åŒ¹é…ç«¯åˆ°ç«¯è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡åœ¨ ESM2 ä¸­å¼•å…¥ç»“æ„è¾“å…¥å¹¶è¿›è¡Œä»… 1 å°æ—¶çš„å¾®è°ƒï¼Œå…¶åœ¨ Protein Mutation Predictions ä¸Šçš„è¡¨ç°å³å¯ä¸å¤æ‚çš„ ESM3 ç›¸å½“ã€‚ç ”ç©¶ç³»ç»Ÿæ€§åœ°å¯¹æ¯”äº†ä¸‰ç§ç‰¹å¾èåˆ (feature-fusion) è®¾è®¡å’Œå¾®è°ƒæ–¹æ¡ˆï¼Œå‘ç°èåˆæ–¹æ³•å’Œå¾®è°ƒç­–ç•¥å¯¹æœ€ç»ˆçš„é¢„æµ‹å‡†ç¡®åº¦å…·æœ‰æ˜¾è‘—å½±å“ï¼Œè¡¨æ˜å¾®è°ƒè¿‡ç¨‹å…·æœ‰é«˜åº¦çš„éçç¢æ€§ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºå‘é¢„è®­ç»ƒçš„ sequence-only æ¨¡å‹æ³¨å…¥ç»“æ„ä¿¡æ¯æä¾›äº†å®è·µæŒ‡å¯¼ï¼Œä¹Ÿä¸ºå¼€å‘æ›´é«˜æ•ˆçš„è›‹ç™½è´¨ç‰¹å¾èåˆæœºåˆ¶å’Œå¾®è°ƒåè®®æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "q-bio.QM",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.03370v2",
      "published_date": "2025-10-03 07:42:22 UTC",
      "updated_date": "2025-10-10 02:14:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:24.550985+00:00"
    },
    {
      "arxiv_id": "2510.06240v1",
      "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets",
      "title_zh": "é¢å‘å¯é å·¥ä¸šé—®ç­”çš„çŸ¥è¯†å›¾è°±å¼•å¯¼å¤šæ™ºèƒ½ä½“è’¸é¦åŠæ•°æ®é›†",
      "authors": [
        "Jiqun Pan",
        "Zhenke Duan",
        "Jiani Tu",
        "Anzhi Cheng",
        "Yanqing Wang"
      ],
      "abstract": "Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šé—®ç­”(Industrial QA)åœ¨å®‰å…¨æ€§ä¸å¯é æ€§æ–¹é¢çš„ä¸¥è‹›è¦æ±‚ï¼Œæå‡ºäº†Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ä¸å¯æ§åŠè¾“å‡ºéš¾ä»¥éªŒè¯çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†è’¸é¦è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process)ï¼Œå¹¶å¼•å…¥Knowledge Graphä½œä¸ºå¯éªŒè¯çš„ç»“æ„åŒ–å…ˆéªŒï¼Œä»¥ä¸°å¯ŒçŠ¶æ€è¡¨ç¤ºå¹¶ç¡®ä¿æ¨ç†æ”¶æ•›ã€‚é€šè¿‡æ•´åˆåä½œæ¨ç†ä¸çŸ¥è¯†å¯¹é½(knowledge grounding)ï¼ŒKG-MASDèƒ½å¤Ÿç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œå°†æ¨ç†æ·±åº¦ä¸å¯éªŒè¯æ€§åŒæ—¶è’¸é¦è‡³é€‚ç”¨äºè¾¹ç¼˜éƒ¨ç½²(edge deployment)çš„è½»é‡åŒ–å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å·¥ä¸šé—®ç­”æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾ƒåŸºçº¿æ¨¡å‹æå‡äº†2.4%è‡³20.1%ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„å¯é æ€§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®‰å…¨å…³é”®å‹å·¥ä¸šåœºæ™¯ä¸‹çš„å¯ä¿¡äººå·¥æ™ºèƒ½éƒ¨ç½²æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "41 pages, 12 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.06240v1",
      "published_date": "2025-10-03 06:52:59 UTC",
      "updated_date": "2025-10-03 06:52:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:20.855203+00:00"
    },
    {
      "arxiv_id": "2510.02763v2",
      "title": "Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning",
      "title_zh": "èåˆå¤šå…‰è°±ä¸é«˜å…‰è°±å«æ˜Ÿæ•°æ®çš„è‡ªç›‘ç£åŠå±‚æ¬¡æ·±åº¦å­¦ä¹ æœ‰å®³è—»åç›‘æµ‹",
      "authors": [
        "Nicholas LaHaye",
        "Kelly M. Luis",
        "Michelle M. Gierach"
      ],
      "abstract": "We present a self-supervised machine learning framework for detecting and mapping harmful algal bloom (HAB) severity and speciation using multi-sensor satellite data. By fusing reflectance data from operational instruments (VIIRS, MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our framework, called SIT-FUSE, generates HAB severity and speciation products without requiring per-instrument labeled datasets. The framework employs self-supervised representation learning, hierarchical deep clustering to segment phytoplankton concentrations and speciations into interpretable classes, validated against in-situ data from the Gulf of Mexico and Southern California (2018-2025). Results show strong agreement with total phytoplankton, Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This work advances scalable HAB monitoring in label-scarce environments while enabling exploratory analysis via hierarchical embeddings: a critical step toward operationalizing self-supervised learning for global aquatic biogeochemistry.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SIT-FUSEæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªæˆ‘ç›‘ç£å­¦ä¹ (Self-Supervised Learning)å’Œå±‚æ¬¡åŒ–æ·±åº¦å­¦ä¹ çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤šä¼ æ„Ÿå™¨å«æ˜Ÿæ•°æ®ç›‘æµ‹å’Œç»˜åˆ¶æœ‰å®³è—»ç±»æš´å‘(Harmful Algal Bloom, HAB)çš„ä¸¥é‡ç¨‹åº¦å’Œç‰©ç§åˆ†ç±»ã€‚è¯¥æ¡†æ¶èåˆäº†æ¥è‡ªVIIRSã€MODISã€Sentinel-3å’ŒPACEç­‰ä¸šåŠ¡å«æ˜Ÿçš„åå°„ç‡æ•°æ®ï¼Œä»¥åŠæ¥è‡ªTROPOMIçš„æ—¥å…‰è¯±å¯¼è§å…‰(Solar-Induced Fluorescence, SIF)æ•°æ®ï¼Œå®ç°äº†æ— éœ€å•ä»ªå™¨æ ‡è®°æ•°æ®é›†çš„ç›‘æµ‹èƒ½åŠ›ã€‚SIT-FUSEé€šè¿‡è‡ªæˆ‘ç›‘ç£è¡¨ç¤ºå­¦ä¹ (Self-Supervised Representation Learning)å’Œå±‚æ¬¡åŒ–æ·±åº¦èšç±»(Hierarchical Deep Clustering)æŠ€æœ¯ï¼Œå°†æµ®æ¸¸æ¤ç‰©æµ“åº¦å’Œç‰©ç§ç»„æˆåˆ’åˆ†ä¸ºå…·æœ‰å¯è§£é‡Šæ€§çš„ç±»åˆ«ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨2018è‡³2025å¹´é—´å¢¨è¥¿å“¥æ¹¾å’Œå—åŠ å·çš„åŸä½(In-situ)æ•°æ®å¯¹æ¡†æ¶è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿåœ¨æµ®æ¸¸æ¤ç‰©æ€»é‡ä»¥åŠKarenia brevisã€Alexandrium spp.å’ŒPseudo-nitzschia spp.ç­‰ç‰¹å®šç‰©ç§çš„æµ‹é‡ä¸Šè¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ºåœ¨æ ‡ç­¾ç¨€ç¼º(Label-scarce)çš„ç¯å¢ƒä¸­å®ç°å¯æ‰©å±•çš„HABç›‘æµ‹æä¾›äº†æ–°é€”å¾„ï¼Œå¹¶é€šè¿‡å±‚æ¬¡åŒ–åµŒå…¥(Hierarchical Embeddings)æ”¯æŒæ¢ç´¢æ€§åˆ†æï¼Œæ˜¯æ¨åŠ¨å…¨çƒæ°´ä½“ç”Ÿç‰©åœ°çƒåŒ–å­¦é¢†åŸŸå®ç°è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ“ä½œåŒ–çš„å…³é”®è¿›æ­¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02763v2",
      "published_date": "2025-10-03 06:51:19 UTC",
      "updated_date": "2025-10-10 19:04:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:24.204049+00:00"
    },
    {
      "arxiv_id": "2510.02760v2",
      "title": "Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology",
      "title_zh": "æ•°å­—ç—…ç†å­¦ä¸­è„‘è‚¿ç˜¤åˆ†ç±»çš„å±‚çº§åŒ–å¹¿ä¹‰ç±»åˆ«å‘ç°",
      "authors": [
        "Matthias Perkonigg",
        "Patrick Rockenschaub",
        "Georg GÃ¶bel",
        "Adelheid WÃ¶hrer"
      ],
      "abstract": "Accurate brain tumor classification is critical for intra-operative decision making in neuro-oncological surgery. However, existing approaches are restricted to a fixed set of predefined classes and are therefore unable to capture patterns of tumor types not available during training. Unsupervised learning can extract general-purpose features, but it lacks the ability to incorporate prior knowledge from labelled data, and semi-supervised methods often assume that all potential classes are represented in the labelled data. Generalized Category Discovery (GCD) aims to bridge this gap by categorizing both known and unknown classes within unlabelled data. To reflect the hierarchical structure of brain tumor taxonomies, in this work, we introduce Hierarchical Generalized Category Discovery for Brain Tumor Classification (HGCD-BT), a novel approach that integrates hierarchical clustering with contrastive learning. Our method extends contrastive learning based GCD by incorporating a novel semi-supervised hierarchical clustering loss. We evaluate HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images, achieving a +28% improvement in accuracy over state-of-the-art GCD methods for patch-level classification, particularly in identifying previously unseen tumor categories. Furthermore, we demonstrate the generalizability of HGCD-BT on slide-level classification of hematoxylin and eosin stained whole-slide images from the Digital Brain Tumor Atlas, confirming its utility across imaging modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é’ˆå¯¹è„‘è‚¿ç˜¤åˆ†ç±»çš„å±‚æ¬¡åŒ–å¹¿ä¹‰ç±»åˆ«å‘ç°æ–¹æ³• HGCD-BT (Hierarchical Generalized Category Discovery for Brain Tumor Classification)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç—…ç†å­¦åˆ†ç±»æ–¹æ³•å› ç±»åˆ«é¢„å®šä¹‰è€Œæ— æ³•è¯†åˆ«è®­ç»ƒé˜¶æ®µæœªè§è‚¿ç˜¤ç±»å‹çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†å±‚æ¬¡èšç±» (hierarchical clustering) ä¸å¯¹æ¯”å­¦ä¹  (contrastive learning) ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£å±‚æ¬¡èšç±»æŸå¤±å‡½æ•°ï¼Œå®ç°äº†å¯¹æ— æ ‡ç­¾æ•°æ®ä¸­å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«çš„ç²¾å‡†åˆ†ç±»ã€‚åœ¨ OpenSRH åˆºæ¿€æ‹‰æ›¼ç»„ç»‡å­¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHGCD-BT åœ¨è¡¥ä¸çº§åˆ†ç±»ä¸­æ¯”ç°æœ‰æœ€å…ˆè¿›çš„ GCD æ–¹æ³•å‡†ç¡®ç‡æå‡äº† 28%ï¼Œå°¤å…¶åœ¨è¯†åˆ«æ–°ç±»åˆ«æ–¹é¢è¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨ Digital Brain Tumor Atlas çš„å…¨åˆ‡ç‰‡å›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¡†æ¶åœ¨ä¸åŒæˆåƒæ¨¡æ€ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¸ä¸´åºŠåº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02760v2",
      "published_date": "2025-10-03 06:46:55 UTC",
      "updated_date": "2025-11-17 13:07:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:26.849376+00:00"
    },
    {
      "arxiv_id": "2510.02759v1",
      "title": "Prototyping Digital Social Spaces through Metaphor-Driven Design: Translating Spatial Concepts into an Interactive Social Simulation",
      "title_zh": "åŸºäºéšå–»é©±åŠ¨è®¾è®¡çš„æ•°å­—ç¤¾äº¤ç©ºé—´åŸå‹æ„å»ºï¼šå°†ç©ºé—´æ¦‚å¿µè½¬åŒ–ä¸ºäº¤äº’å¼ç¤¾äº¤æ¨¡æ‹Ÿ",
      "authors": [
        "Yoojin Hong",
        "Martina Di Paola",
        "Braahmi Padmakumar",
        "Hwi Joon Lee",
        "Mahnoor Shafiq",
        "Joseph Seering"
      ],
      "abstract": "Social media platforms are central to communication, yet their designs remain narrowly focused on engagement and scale. While researchers have proposed alternative visions for online spaces, these ideas are difficult to prototype within platform constraints. In this paper, we introduce a metaphor-driven system to help users imagine and explore new social media environments. The system translates users' metaphors into structured sets of platform features and generates interactive simulations populated with LLM-driven agents. To evaluate this approach, we conducted a study where participants created and interacted with simulated social media spaces. Our findings show that metaphors allow users to express distinct social expectations, and that perceived authenticity of the simulation depended on how well it captured dynamics like intimacy, participation, and temporal engagement. We conclude by discussing how metaphor-driven simulation can be a powerful design tool for prototyping alternative social architectures and expanding the design space for future social platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“è®¾è®¡å±€é™äºå‚ä¸åº¦å’Œè§„æ¨¡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§éšå–»é©±åŠ¨(metaphor-driven)çš„è®¾è®¡ç³»ç»Ÿï¼Œç”¨äºå°†ç©ºé—´æ¦‚å¿µè½¬åŒ–ä¸ºäº¤äº’å¼ç¤¾äº¤æ¨¡æ‹Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿå°†ç”¨æˆ·çš„éšå–»è½¬åŒ–ä¸ºç»“æ„åŒ–çš„å¹³å°åŠŸèƒ½é›†(platform features)ï¼Œå¹¶æ„å»ºç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„æ™ºèƒ½ä½“ç»„æˆçš„ä»¿çœŸç¯å¢ƒã€‚é€šè¿‡ç”¨æˆ·å‚ä¸å®éªŒï¼Œç ”ç©¶å‘ç°éšå–»èƒ½è®©ç”¨æˆ·è¡¨è¾¾å‡ºç‹¬ç‰¹çš„ç¤¾äº¤é¢„æœŸï¼Œè€Œæ¨¡æ‹Ÿçš„çœŸå®æ„Ÿåˆ™å–å†³äºç³»ç»Ÿå¯¹äº²å¯†åº¦(intimacy)ã€å‚ä¸åº¦(participation)å’Œæ—¶é—´å‚ä¸(temporal engagement)ç­‰åŠ¨æ€çš„æ•æ‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ‰©å±•ç¤¾äº¤å¹³å°çš„è®¾è®¡ç©ºé—´ï¼Œå¸®åŠ©è®¾è®¡è€…æ¢ç´¢ä¼ ç»Ÿæ¡†æ¶ä¹‹å¤–çš„åœ¨çº¿ç¯å¢ƒã€‚è¯¥ç ”ç©¶æ€»ç»“äº†éšå–»é©±åŠ¨æ¨¡æ‹Ÿåœ¨åŸå‹åŒ–æ›¿ä»£æ€§ç¤¾äº¤æ¶æ„ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥æ•°å­—ç¤¾äº¤ç©ºé—´çš„è®¾è®¡æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "25 pages, in submission to CHI 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.02759v1",
      "published_date": "2025-10-03 06:43:35 UTC",
      "updated_date": "2025-10-03 06:43:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:30.755247+00:00"
    },
    {
      "arxiv_id": "2511.17507v1",
      "title": "The use of artificial intelligence in music creation: between interface and appropriation",
      "title_zh": "äººå·¥æ™ºèƒ½åœ¨éŸ³ä¹åˆ›ä½œä¸­çš„åº”ç”¨ï¼šç•Œé¢ä¸æŒªç”¨ä¹‹é—´",
      "authors": [
        "Arnaud Zeller",
        "Emmanuelle Chevry Pebayle"
      ],
      "abstract": "By observing the activities and relationships of musicians and sound designers to the activities of creation, performance, publishing and dissemination with artificial intelligence (AI), from two specialized forums between 2022 and 2024, this article proposes a lexicometric analysis of the representations linked to their use. Indeed, the machine, now equipped with artificial intelligences requiring new appropriations and enabling new mediations, constitutes new challenges for artists. To study these confrontations and new mediations, our approach mobilizes the theoretical framework of the Human-AI Musicking Framework, based on a lexicometric analysis of content. The aim is to clarify the present and future uses of AI from the interfaces, in the creation of sound and musical content, and to identify the obstacles, obstacles, brakes and limits to appropriation ``in the fact of making the content one's own and integrating it as a part of oneself'' (Bachimont and Crozat, 2004) in the context of a collaboration between musician and machine.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æ2022è‡³2024å¹´é—´ä¸¤ä¸ªä¸“ä¸šè®ºå›ä¸­éŸ³ä¹å®¶ä¸å£°éŸ³è®¾è®¡å¸ˆçš„æ´»åŠ¨ï¼Œæ¢è®¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨éŸ³ä¹åˆ›ä½œã€è¡¨æ¼”åŠä¼ æ’­ä¸­çš„åº”ç”¨ç°çŠ¶ã€‚ç ”ç©¶é‡‡ç”¨äº†è¯æ±‡è®¡é‡åˆ†æï¼ˆlexicometric analysisï¼‰æ–¹æ³•ï¼Œå¹¶åŸºäºäººç±»-AIéŸ³ä¹åˆ›ä½œæ¡†æ¶ï¼ˆHuman-AI Musicking Frameworkï¼‰çš„ç†è®ºä½“ç³»ï¼Œæ·±å…¥å‰–æè‰ºæœ¯å®¶ä¸æœºå™¨ä¹‹é—´çš„äº’åŠ¨å…³ç³»ã€‚å…¶æ ¸å¿ƒç›®æ ‡åœ¨äºé€šè¿‡ç•Œé¢ï¼ˆinterfaceï¼‰è§†è§’å˜æ¸…AIåœ¨éŸ³é¢‘ä¸éŸ³ä¹å†…å®¹ç”Ÿæˆä¸­çš„å½“å‰åŠæœªæ¥ç”¨é€”ï¼Œå¹¶è¯†åˆ«è‰ºæœ¯å®¶åœ¨åä½œè¿‡ç¨‹ä¸­çš„éšœç¢ä¸é™åˆ¶ã€‚è®ºæ–‡é‡ç‚¹æ¢è®¨äº†â€œå ç”¨â€ï¼ˆappropriationï¼‰è¿™ä¸€æ¦‚å¿µï¼Œå³åœ¨éŸ³ä¹äººä¸æœºå™¨åä½œçš„è¯­å¢ƒä¸‹ï¼Œå¦‚ä½•å°†AIç”Ÿæˆçš„å†…å®¹å†…åŒ–å¹¶æ•´åˆä¸ºè‰ºæœ¯è‡ªæˆ‘çš„ä¸€éƒ¨åˆ†ã€‚è¿™ç§åˆ†ææ­ç¤ºäº†æ–°æŠ€æœ¯ä¸­ä»‹ï¼ˆmediationsï¼‰ç»™è‰ºæœ¯åˆ›ä½œå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¸ºç†è§£åˆ›æ„äº§ä¸šä¸­äººæœºåä½œçš„æ¼”å˜æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "in French language",
      "pdf_url": "https://arxiv.org/pdf/2511.17507v1",
      "published_date": "2025-10-03 06:35:16 UTC",
      "updated_date": "2025-10-03 06:35:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:35.150068+00:00"
    },
    {
      "arxiv_id": "2510.03369v2",
      "title": "TriQuest:An AI Copilot-Powered Platform for Interdisciplinary Curriculum Design",
      "title_zh": "TriQuestï¼šåŸºäº AI Copilot é©±åŠ¨çš„è·¨å­¦ç§‘è¯¾ç¨‹è®¾è®¡å¹³å°",
      "authors": [
        "Huazhen Wang",
        "Huimin Yang",
        "Hainbin Lin",
        "Yan Dong",
        "Lili Chen",
        "Liangliang Xia",
        "Wenwen Xu"
      ],
      "abstract": "Interdisciplinary teaching is a cornerstone of modern curriculum reform, but its implementation is hindered by challenges in knowledge integration and time-consuming lesson planning. Existing tools often lack the required pedagogical and domain-specific depth.We introduce TriQuest, an AI-copilot platform designed to solve these problems. TriQuest uses large language models and knowledge graphs via an intuitive GUI to help teachers efficiently generate high-quality interdisciplinary lesson plans. Its core features include intelligent knowledge integration from various disciplines and a human-computer collaborative review process to ensure quality and innovation.In a study with 43 teachers, TriQuest increased curriculum design efficiency and improved lesson plan quality. It also significantly lowered design barriers and cognitive load. Our work presents a new paradigm for empowering teacher professional development with intelligent technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TriQuestï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è·¨å­¦ç§‘æ•™å­¦ä¸­çŸ¥è¯†æ•´åˆå›°éš¾å’Œè¯¾å‰å‡†å¤‡è€—æ—¶ç­‰æŒ‘æˆ˜çš„ AI-copilot å¹³å°ã€‚TriQuest ç»“åˆäº† Large Language Models å’Œ Knowledge Graphs æŠ€æœ¯ï¼Œé€šè¿‡ç›´è§‚çš„ GUI ç•Œé¢è¾…åŠ©æ•™å¸ˆé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„è·¨å­¦ç§‘æ•™æ¡ˆã€‚å…¶æ ¸å¿ƒåŠŸèƒ½æ¶µç›–äº†å¤šå­¦ç§‘æ™ºèƒ½çŸ¥è¯†æ•´åˆä»¥åŠç¡®ä¿æ•™å­¦è´¨é‡ä¸åˆ›æ–°çš„ Human-computer collaborative review åä½œæµç¨‹ã€‚åœ¨ä¸€é¡¹é’ˆå¯¹ 43 åæ•™å¸ˆçš„è¯„ä¼°å®éªŒä¸­ï¼ŒTriQuest æ˜¾è‘—æé«˜äº†è¯¾ç¨‹è®¾è®¡æ•ˆç‡å¹¶ä¼˜åŒ–äº†æ•™æ¡ˆè´¨é‡ã€‚ç ”ç©¶ç»“æœè¯æ˜è¯¥å¹³å°æœ‰æ•ˆé™ä½äº†æ•™å¸ˆçš„è®¾è®¡é—¨æ§›å’Œ Cognitive loadï¼Œä¸ºæ™ºèƒ½æŠ€æœ¯èµ‹èƒ½æ•™å¸ˆä¸“ä¸šå‘å±•å¼€è¾Ÿäº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "16 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03369v2",
      "published_date": "2025-10-03 06:04:59 UTC",
      "updated_date": "2025-10-23 11:54:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:44.948488+00:00"
    },
    {
      "arxiv_id": "2510.03368v1",
      "title": "An Adaptive Responsible AI Governance Framework for Decentralized Organizations",
      "title_zh": "é¢å‘å»ä¸­å¿ƒåŒ–ç»„ç»‡çš„é€‚åº”æ€§è´Ÿè´£ä»»äººå·¥æ™ºèƒ½æ²»ç†æ¡†æ¶",
      "authors": [
        "Kiana Jafari Meimandi",
        "Anka Reuel",
        "Gabriela Aranguiz-Dias",
        "Hatim Rahama",
        "Ala-Eddine Ayadi",
        "Xavier Boullier",
        "JÃ©rÃ©my Verdo",
        "Louis Montanie",
        "Mykel Kochenderfer"
      ],
      "abstract": "This paper examines the assessment challenges of Responsible AI (RAI) governance efforts in globally decentralized organizations through a case study collaboration between a leading research university and a multinational enterprise. While there are many proposed frameworks for RAI, their application in complex organizational settings with distributed decision-making authority remains underexplored. Our RAI assessment, conducted across multiple business units and AI use cases, reveals four key patterns that shape RAI implementation: (1) complex interplay between group-level guidance and local interpretation, (2) challenges translating abstract principles into operational practices, (3) regional and functional variation in implementation approaches, and (4) inconsistent accountability in risk oversight. Based on these findings, we propose an Adaptive RAI Governance (ARGO) Framework that balances central coordination with local autonomy through three interdependent layers: shared foundation standards, central advisory resources, and contextual local implementation. We contribute insights from academic-industry collaboration for RAI assessments, highlighting the importance of modular governance approaches that accommodate organizational complexity while maintaining alignment with responsible AI principles. These lessons offer practical guidance for organizations navigating the transition from RAI principles to operational practice within decentralized structures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…¨çƒåˆ†å¸ƒå¼ç»„ç»‡åœ¨è´Ÿè´£ä»»äººå·¥æ™ºèƒ½(Responsible AI, RAI)æ²»ç†è¯„ä¼°ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œé€šè¿‡ä¸€æ‰€é¢†å…ˆç ”ç©¶å‹å¤§å­¦ä¸è·¨å›½ä¼ä¸šçš„æ¡ˆä¾‹ç ”ç©¶è¯†åˆ«äº†å½±å“RAIå®æ–½çš„å››ä¸ªæ ¸å¿ƒæ¨¡å¼ï¼ŒåŒ…æ‹¬é›†å›¢æŒ‡å¯¼ä¸åœ°æ–¹è§£è¯»çš„å†²çªã€æŠ½è±¡åŸåˆ™å‘æ“ä½œå®è·µè½¬åŒ–çš„å›°éš¾ã€åŒºåŸŸèŒèƒ½å·®å¼‚ä»¥åŠé—®è´£æœºåˆ¶çš„ä¸ä¸€è‡´ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”RAIæ²»ç†æ¡†æ¶(Adaptive RAI Governance Framework, ARGO)ï¼Œæ—¨åœ¨é€šè¿‡å…±äº«åŸºç¡€æ ‡å‡†ã€ä¸­å¤®å’¨è¯¢èµ„æºå’Œä¸Šä¸‹æ–‡ç›¸å…³çš„æœ¬åœ°å®æ–½ä¸‰ä¸ªäº’è¡¥å±‚çº§ï¼Œå®ç°ä¸­å¤®åè°ƒä¸åœ°æ–¹è‡ªä¸»æƒä¹‹é—´çš„å¹³è¡¡ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æ¨¡å—åŒ–æ²»ç†æ–¹æ³•åœ¨åº”å¯¹ç»„ç»‡å¤æ‚æ€§æ—¶çš„å…³é”®ä½œç”¨ï¼Œä¸ºå»ä¸­å¿ƒåŒ–ç»„ç»‡ä»RAIåŸåˆ™èµ°å‘è¿è¥å®è·µæä¾›äº†å…·ä½“çš„å®æ–½è·¯å¾„ä¸å®ç”¨æŒ‡å—ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03368v1",
      "published_date": "2025-10-03 05:55:48 UTC",
      "updated_date": "2025-10-03 05:55:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:56.947553+00:00"
    },
    {
      "arxiv_id": "2510.05157v1",
      "title": "Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment",
      "title_zh": "æ¨¡æ‹Ÿé›¶å’Œç½‘ç»œç¯å¢ƒä¸‹æ”»é˜²æ™ºèƒ½ä½“çš„å¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Abrar Shahid",
        "Ibteeker Mahir Ishum",
        "AKM Tahmidul Haque",
        "M Sohel Rahman",
        "A. B. M. Alim Al Islam"
      ],
      "abstract": "This paper presents a controlled study of adversarial reinforcement learning in network security through a custom OpenAI Gym environment that models brute-force attacks and reactive defenses on multi-port services. The environment captures realistic security trade-offs including background traffic noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot traps, and multi-level rate-limiting defenses. Competing attacker and defender agents are trained using Deep Q-Networks (DQN) within a zero-sum reward framework, where successful exploits yield large terminal rewards while incremental actions incur small costs. Through systematic evaluation across multiple configurations (varying trap detection probabilities, exploitation difficulty thresholds, and training regimens), the results demonstrate that defender observability and trap effectiveness create substantial barriers to successful attacks. The experiments reveal that reward shaping and careful training scheduling are critical for learning stability in this adversarial setting. The defender consistently maintains strategic advantage across 50,000+ training episodes, with performance gains amplifying when exposed to complex defensive strategies including adaptive IP blocking and port-specific controls. Complete implementation details, reproducible hyperparameter configurations, and architectural guidelines are provided to support future research in adversarial RL for cybersecurity. The zero-sum formulation and realistic operational constraints make this environment suitable for studying autonomous defense systems, attacker-defender co-evolution, and transfer learning to real-world network security scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ¨¡æ‹Ÿé›¶å’Œç½‘ç»œç¯å¢ƒä¸‹çš„å¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹ (Adversarial Reinforcement Learning)ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŸºäº OpenAI Gym çš„è‡ªå®šä¹‰ç¯å¢ƒæ¥æ¨¡æ‹Ÿæš´åŠ›æ”»å‡»ä¸ååº”æ€§é˜²å¾¡åšå¼ˆã€‚é€šè¿‡ä½¿ç”¨ Deep Q-Networks (DQN) è®­ç»ƒæ”»å‡»ä¸é˜²å¾¡æ™ºèƒ½ä½“ï¼Œè¯¥ç¯å¢ƒé›†æˆäº†èƒŒæ™¯æµé‡å™ªå£°ã€IP è§„é¿æˆ˜æœ¯ã€èœœç½é™·é˜±(Honeypot)åŠå¤šçº§é™æµç­‰ç°å®ç½‘ç»œå®‰å…¨å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé˜²å¾¡è€…çš„å¯è§‚å¯Ÿæ€§å’Œé™·é˜±çš„æœ‰æ•ˆæ€§æ˜¯é˜»ç¢æ”»å‡»æˆåŠŸçš„å…³é”®éšœç¢ï¼Œä¸”å¥–åŠ±å¡‘é€ (Reward Shaping)å’Œè®­ç»ƒè®¡åˆ’å¯¹äºç»´æŒå¯¹æŠ—ç¯å¢ƒä¸‹çš„å­¦ä¹ ç¨³å®šæ€§è‡³å…³é‡è¦ã€‚ç ”ç©¶å‘ç°é˜²å¾¡è€…åœ¨åº”å¯¹åŒ…æ‹¬è‡ªé€‚åº” IP å°ç¦åœ¨å†…çš„å¤æ‚ç­–ç•¥æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æˆ˜ç•¥ä¼˜åŠ¿ã€‚è¯¥å·¥ä½œä¸ºç ”ç©¶è‡ªä¸»é˜²å¾¡ç³»ç»Ÿã€æ”»é˜²ååŒæ¼”åŒ–ä»¥åŠå¼ºåŒ–å­¦ä¹ å‘çœŸå®ç½‘ç»œå®‰å…¨åœºæ™¯çš„è¿ç§»æä¾›äº†å¯é‡å¤çš„å®éªŒæ¡†æ¶å’Œæ¶æ„æŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 5 tables, 5 figures. 12th International Conference on Next Generation Computing, Communication, Systems and Security",
      "pdf_url": "https://arxiv.org/pdf/2510.05157v1",
      "published_date": "2025-10-03 05:53:51 UTC",
      "updated_date": "2025-10-03 05:53:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:38:58.748062+00:00"
    },
    {
      "arxiv_id": "2510.02734v1",
      "title": "SAE-RNA: A Sparse Autoencoder Model for Interpreting RNA Language Model Representations",
      "title_zh": "SAE-RNAï¼šç”¨äºè§£é‡Š RNA è¯­è¨€æ¨¡å‹è¡¨å¾çš„ç¨€ç–è‡ªç¼–ç å™¨æ¨¡å‹",
      "authors": [
        "Taehan Kim",
        "Sangdae Nam"
      ],
      "abstract": "Deep learning, particularly with the advancement of Large Language Models, has transformed biomolecular modeling, with protein advances (e.g., ESM) inspiring emerging RNA language models such as RiNALMo. Yet how and what these RNA Language Models internally encode about messenger RNA (mRNA) or non-coding RNA (ncRNA) families remains unclear. We present SAE- RNA, interpretability model that analyzes RiNALMo representations and maps them to known human-level biological features. Our work frames RNA interpretability as concept discovery in pretrained embeddings, without end-to-end retraining, and provides practical tools to probe what RNA LMs may encode about ncRNA families. The model can be extended to close comparisons between RNA groups, and supporting hypothesis generation about previously unrecognized relationships.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SAE-RNAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoderï¼‰çš„å¯è§£é‡Šæ€§æ¨¡å‹ï¼Œæ—¨åœ¨åˆ†æ RNA Language Modelsï¼ˆå¦‚ RiNALMoï¼‰çš„å†…éƒ¨è¡¨ç¤ºå¹¶å°†å…¶æ˜ å°„åˆ°å·²çŸ¥çš„äººç±»æ°´å¹³ç”Ÿç‰©ç‰¹å¾ã€‚é€šè¿‡å°†å¯è§£é‡Šæ€§ä»»åŠ¡ç•Œå®šä¸ºé¢„è®­ç»ƒåµŒå…¥ä¸­çš„æ¦‚å¿µå‘ç°ï¼ˆconcept discoveryï¼‰ï¼Œè¯¥æ¨¡å‹æ— éœ€ç«¯åˆ°ç«¯çš„é‡æ–°è®­ç»ƒå³å¯æ­ç¤ºæ¨¡å‹å¦‚ä½•ç¼–ç  mRNA æˆ–éç¼–ç  RNAï¼ˆncRNAï¼‰å®¶æ—çš„ä¿¡æ¯ã€‚SAE-RNA æä¾›äº†å®ç”¨çš„å·¥å…·æ¥æ¢æµ‹ RNA æ¨¡å‹çš„å†…éƒ¨é€»è¾‘ï¼Œå¹¶æ”¯æŒå¯¹ä¸åŒ RNA ç¾¤ä½“è¿›è¡Œæ·±å…¥å¯¹æ¯”åˆ†æã€‚ç ”ç©¶ç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿè¾…åŠ©ç”Ÿæˆå…³äºå…ˆå‰æœªè¢«è¯†åˆ«çš„ RNA å…³ç³»çš„æ–°å‡è®¾ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿç‰©åºåˆ—æ¨¡å‹åœ¨åˆ†å­ç”Ÿç‰©å­¦é¢†åŸŸçš„é€æ˜åº¦ä¸ç§‘ç ”åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "q-bio.BM",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.02734v1",
      "published_date": "2025-10-03 05:34:59 UTC",
      "updated_date": "2025-10-03 05:34:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:39:01.251922+00:00"
    },
    {
      "arxiv_id": "2510.02719v1",
      "title": "TravelBench : Exploring LLM Performance in Low-Resource Domains",
      "title_zh": "TravelBenchï¼šæ¢ç©¶å¤§è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºé¢†åŸŸä¸­çš„æ€§èƒ½è¡¨ç°",
      "authors": [
        "Srinivas Billa",
        "Xiaonan Jing"
      ],
      "abstract": "Results on existing LLM benchmarks capture little information over the model capabilities in low-resource tasks, making it difficult to develop effective solutions in these domains. To address these challenges, we curated 14 travel-domain datasets spanning 7 common NLP tasks using anonymised data from real-world scenarios, and analysed the performance across LLMs. We report on the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a variety of tasks. Our results confirm that general benchmarking results are insufficient for understanding model performance in low-resource tasks. Despite the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks in complex, domain-specific scenarios. Furthermore, reasoning provides a more significant boost for smaller LLMs by making the model a better judge on certain tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) åŸºå‡†æµ‹è¯•åœ¨ä½èµ„æºé¢†åŸŸ (low-resource domains) èƒ½åŠ›æ•è·ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† TravelBenchã€‚ç ”ç©¶è€…é€šè¿‡ä»çœŸå®åœºæ™¯ä¸­æ”¶é›†åŒ¿åæ•°æ®ï¼Œæ„å»ºäº† 14 ä¸ªæ¶µç›– 7 ç§å¸¸è§ NLP ä»»åŠ¡çš„æ—…æ¸¸é¢†åŸŸæ•°æ®é›†ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº† LLMs çš„å‡†ç¡®æ€§ã€ç¼©æ”¾è¡Œä¸ºå’Œæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœæ— æ³•æœ‰æ•ˆåæ˜ æ¨¡å‹åœ¨ä½èµ„æºä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸”å³ä½¿æ˜¯é«˜ FLOPs è®­ç»ƒçš„å¼€ç®±å³ç”¨ LLMs åœ¨å¤„ç†å¤æ‚é¢†åŸŸç‰¹å®šåœºæ™¯æ—¶ä¹Ÿä¼šé­é‡æ€§èƒ½ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ¨ç†èƒ½åŠ›å¯¹å°å‹ LLMs çš„æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„è¯„åˆ¤èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ºåœ¨ä½èµ„æºé¢†åŸŸå¼€å‘æ›´æœ‰æ•ˆçš„æ¨¡å‹è§£å†³æ–¹æ¡ˆæä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒå’Œæ´å¯Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.02719v1",
      "published_date": "2025-10-03 04:44:34 UTC",
      "updated_date": "2025-10-03 04:44:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:39:04.054559+00:00"
    },
    {
      "arxiv_id": "2510.02717v1",
      "title": "CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks",
      "title_zh": "CST-AFNetï¼šåŸºäºåŒæ³¨æ„åŠ›æœºåˆ¶çš„ç‰©è”ç½‘å…¥ä¾µæ£€æµ‹æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Waqas Ishtiaq",
        "Ashrafun Zannat",
        "A. H. M. Shahariar Parvez",
        "Md. Alamgir Hossain",
        "Muntasir Hasan Kanchan",
        "Muhammad Masud Tarek"
      ],
      "abstract": "The rapid expansion of the Internet of Things (IoT) has revolutionized modern industries by enabling smart automation and real time connectivity. However, this evolution has also introduced complex cybersecurity challenges due to the heterogeneous, resource constrained, and distributed nature of these environments. To address these challenges, this research presents CST AFNet, a novel dual attention based deep learning framework specifically designed for robust intrusion detection in IoT networks. The model integrates multi scale Convolutional Neural Networks (CNNs) for spatial feature extraction, Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal dependencies, and a dual attention mechanism, channel and temporal attention, to enhance focus on critical patterns in the data. The proposed method was trained and evaluated on the Edge IIoTset dataset, a comprehensive and realistic benchmark containing more than 2.2 million labeled instances spanning 15 attack types and benign traffic, collected from a seven layer industrial testbed. Our proposed model achieves outstanding accuracy for both 15 attack types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover, this model demonstrates exceptional performance with macro averaged precision, recall, and F1 score all above 99.3 percent. Experimental results show that CST AFNet achieves superior detection accuracy, significantly outperforming traditional deep learning models. The findings confirm that CST AFNet is a powerful and scalable solution for real time cyber threat detection in complex IoT and IIoT environments, paving the way for more secure, intelligent, and adaptive cyber physical systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©è”ç½‘(IoT)å¿«é€Ÿæ‰©å¼ å¸¦æ¥çš„å¤æ‚ç½‘ç»œå®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº†CST-AFNetï¼Œä¸€ç§åŸºäºåŒæ³¨æ„åŠ›çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå®ç°IoTç½‘ç»œä¸­é²æ£’çš„å…¥ä¾µæ£€æµ‹ã€‚è¯¥æ¨¡å‹é›†æˆäº†ç”¨äºæå–ç©ºé—´ç‰¹å¾çš„å¤šå°ºåº¦å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)å’Œæ•æ‰æ—¶é—´ä¾èµ–å…³ç³»çš„åŒå‘é—¨æ§å¾ªç¯å•å…ƒ(BiGRUs)ï¼Œå¹¶é€šè¿‡é€šé“æ³¨æ„åŠ›å’Œæ—¶é—´æ³¨æ„åŠ›çš„åŒé‡æœºåˆ¶å¢å¼ºå¯¹å…³é”®æ•°æ®æ¨¡å¼çš„èšç„¦ã€‚å®éªŒåœ¨åŒ…å«15ç§æ”»å‡»ç±»å‹çš„Edge-IIoTsetæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºCST-AFNetè¾¾åˆ°äº†99.97%çš„å‡†ç¡®ç‡ï¼Œä¸”å®å¹³å‡ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å¾—åˆ†å‡è¶…è¿‡99.3%ã€‚ç ”ç©¶è¯æ˜è¯¥æ¨¡å‹åœ¨æ£€æµ‹æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸ºå¤æ‚IoTå’Œå·¥ä¸šç‰©è”ç½‘(IIoT)ç¯å¢ƒä¸‹çš„å®æ—¶å¨èƒæ£€æµ‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºæ„å»ºæ›´å®‰å…¨ã€æ™ºèƒ½çš„èµ›åšç‰©ç†ç³»ç»Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 9 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.02717v1",
      "published_date": "2025-10-03 04:36:19 UTC",
      "updated_date": "2025-10-03 04:36:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:39:08.944811+00:00"
    },
    {
      "arxiv_id": "2510.02716v2",
      "title": "A $1000\\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps",
      "title_zh": "é¢å‘å¤§è§„æ¨¡æ …æ ¼åœ°å›¾çš„åƒå€æé€Ÿ LLM å¢å¼ºè·¯å¾„è§„åˆ’ç®—æ³•",
      "authors": [
        "Junlin Zeng",
        "Xin Zhang",
        "Xiang Zhao",
        "Yan Pan"
      ],
      "abstract": "Path planning in grid maps, arising from various applications, has garnered significant attention. Existing methods, such as A*, Dijkstra, and their variants, work well for small-scale maps but fail to address large-scale ones due to high search time and memory consumption. Recently, Large Language Models (LLMs) have shown remarkable performance in path planning but still suffer from spatial illusion and poor planning performance. Among all the works, LLM-A* \\cite{meng2024llm} leverages LLM to generate a series of waypoints and then uses A* to plan the paths between the neighboring waypoints. In this way, the complete path is constructed. However, LLM-A* still suffers from high computational time for large-scale maps. To fill this gap, we conducted a deep investigation into LLM-A* and found its bottleneck, resulting in limited performance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr. as iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the optimization of A*, an incremental learning method for LLM to generate high-quality waypoints, and the selection of the appropriate waypoints for A* for path planning. Finally, a comprehensive evaluation on various grid maps shows that, compared with LLM-A*, iLLM-A* \\textbf{1) achieves more than $1000\\times$ speedup on average, and up to $2349.5\\times$ speedup in the extreme case, 2) saves up to $58.6\\%$ of the memory cost, 3) achieves both obviously shorter path length and lower path length standard deviation.}",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºiLLM-A*çš„åˆ›æ–°ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸA*ã€Dijkstraç®—æ³•ä»¥åŠç°æœ‰LLM-A*åœ¨å¤§è§„æ¨¡æ …æ ¼åœ°å›¾(large-scale grid maps)ä¸­æœç´¢æ—¶é—´é•¿å’Œå†…å­˜æ¶ˆè€—å¤§çš„ç“¶é¢ˆé—®é¢˜ã€‚iLLM-A*é›†æˆäº†ä¸‰å¤§æ ¸å¿ƒæœºåˆ¶ï¼ŒåŒ…æ‹¬å¯¹A*ç®—æ³•çš„ä¼˜åŒ–ã€æ—¨åœ¨ä½¿Large Language Models (LLMs)ç”Ÿæˆé«˜è´¨é‡èˆªç‚¹çš„å¢é‡å­¦ä¹ æ–¹æ³•(incremental learning method)ï¼Œä»¥åŠä¸ºè·¯å¾„è§„åˆ’é€‰æ‹©åˆé€‚èˆªç‚¹(waypoints)çš„ç­–ç•¥ã€‚é€šè¿‡åœ¨å¤šç§æ …æ ¼åœ°å›¾ä¸Šçš„ç»¼åˆè¯„ä¼°ï¼ŒiLLM-A*å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå…¶å¹³å‡è¿ç®—é€Ÿåº¦è¾ƒLLM-A*æå‡äº†1000å€ä»¥ä¸Šï¼Œåœ¨æç«¯æƒ…å†µä¸‹æé€Ÿé«˜è¾¾2349.5å€ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•æœ‰æ•ˆé™ä½äº†58.6%çš„å†…å­˜æˆæœ¬ï¼Œå¹¶æ˜¾è‘—ç¼©çŸ­äº†è·¯å¾„é•¿åº¦ä¸”é™ä½äº†è·¯å¾„é•¿åº¦æ ‡å‡†å·®ã€‚è¯¥æˆæœä¸ºå¤§è§„æ¨¡å¤æ‚ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ™ºèƒ½è·¯å¾„è§„åˆ’æä¾›äº†æå…·ç«äº‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02716v2",
      "published_date": "2025-10-03 04:33:45 UTC",
      "updated_date": "2025-12-01 01:28:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:39:13.557951+00:00"
    },
    {
      "arxiv_id": "2510.02715v1",
      "title": "Fully automated inverse co-optimization of templates and block copolymer blending recipes for DSA lithography",
      "title_zh": "é¢å‘DSAå…‰åˆ»çš„æ¨¡æ¿ä¸åµŒæ®µå…±èšç‰©æ··é…æ–¹æ¡ˆå…¨è‡ªåŠ¨é€†å‘ååŒä¼˜åŒ–",
      "authors": [
        "Yuhao Zhou",
        "Huangyan Shen",
        "Qingliang Song",
        "Qingshu Dong",
        "Jianfeng Li",
        "Weihua Li"
      ],
      "abstract": "The directed self-assembly (DSA) of block copolymers (BCPs) offers a highly promising approach for the fabrication of contact holes or vertical interconnect access at sub-7nm technology nodes. To fabricate circular holes with precisely controlled size and positions, the self-assembly of block copolymers requires guidance from a properly designed template. Effectively parameterizing the template shape to enable efficient optimization remains a critical yet challenging problem. Moreover, the optimized template must possess excellent manufacturability for practical applications. In this work, we propose a Gaussian descriptor for characterizing the template shape with only two parameters. We further propose to use AB/AB binary blends instead of pure diblock copolymer to improve the adaptability of the block copolymer system to the template shape. The Bayesian optimization (BO) is applied to co-optimize the binary blend and the template shape. Our results demonstrate that BO based on the Gaussian descriptor can efficiently yield the optimal templates for diverse multi-hole patterns, all leading to highly matched self-assembled morphologies. Moreover, by imposing constraints on the variation of curvature of the template during optimization, superior manufacturability is ensured for each optimized template. It is noteworthy that each key parameter of the blend exhibits a relatively wide tunable window under the requirement of rather high precision. Our work provides valuable insights for advancing DSA technology, and thus potentially propels its practical applications forward.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ sub-7nm æŠ€æœ¯èŠ‚ç‚¹ä¸‹ directed self-assembly (DSA) å…‰åˆ»ä¸­çš„æ¨¡æ¿è®¾è®¡éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨çš„åå‘ååŒä¼˜åŒ–æ¡†æ¶ã€‚ç ”ç©¶äººå‘˜å¼•å…¥äº†ä»…éœ€ä¸¤ä¸ªå‚æ•°çš„ Gaussian descriptor æ¥è¡¨å¾æ¨¡æ¿å½¢çŠ¶ï¼Œå¹¶åˆ©ç”¨ AB/AB binary blends æ›¿ä»£çº¯ diblock copolymer ä»¥æå‡ç³»ç»Ÿå¯¹æ¨¡æ¿å½¢çŠ¶çš„é€‚åº”æ€§ã€‚é€šè¿‡åº”ç”¨ Bayesian optimization (BO) å¯¹æ··åˆé…æ–¹ä¸æ¨¡æ¿å‡ ä½•å½¢çŠ¶è¿›è¡ŒååŒä¼˜åŒ–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¤šå­”å›¾æ¡ˆä¸Šå®ç°äº†é«˜åº¦åŒ¹é…çš„è‡ªç»„è£…å½¢æ€ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥æ›²ç‡å˜åŒ–çº¦æŸï¼Œç¡®ä¿äº†ä¼˜åŒ–åçš„æ¨¡æ¿å…·æœ‰è‰¯å¥½çš„å¯åˆ¶é€ æ€§ (manufacturability)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ä½“ç³»åœ¨æ»¡è¶³é«˜ç²¾åº¦è¦æ±‚çš„åŒæ—¶ï¼Œå…¶å…³é”®å‚æ•°æ‹¥æœ‰è¾ƒå®½çš„è°ƒèŠ‚çª—å£ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨è¿› DSA æŠ€æœ¯çš„å¤§è§„æ¨¡å®è·µåº”ç”¨æä¾›äº†é‡è¦çš„ç†è®ºä¸æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "physics.comp-ph",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02715v1",
      "published_date": "2025-10-03 04:32:13 UTC",
      "updated_date": "2025-10-03 04:32:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:39:13.946745+00:00"
    },
    {
      "arxiv_id": "2510.02712v2",
      "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
      "title_zh": "Time-To-Inconsistencyï¼šå¤§è¯­è¨€æ¨¡å‹å¯¹æŠ—æ”»å‡»é²æ£’æ€§çš„ç”Ÿå­˜åˆ†æ",
      "authors": [
        "Yubo Li",
        "Ramayya Krishnan",
        "Rema Padman"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present a large-scale survival analysis of conversational robustness, modeling failure as a time-to-event process over 36,951 turns from 9 state-of-the-art LLMs on the MT-Consistency benchmark. Our framework combines Cox proportional hazards, Accelerated Failure Time (AFT), and Random Survival Forest models with simple semantic drift features. We find that abrupt prompt-to-prompt semantic drift sharply increases the hazard of inconsistency, whereas cumulative drift is counterintuitively \\emph{protective}, suggesting adaptation in conversations that survive multiple shifts. AFT models with model-drift interactions achieve the best combination of discrimination and calibration, and proportional hazards checks reveal systematic violations for key drift covariates, explaining the limitations of Cox-style modeling in this setting. Finally, we show that a lightweight AFT model can be turned into a turn-level risk monitor that flags most failing conversations several turns before the first inconsistent answer while keeping false alerts modest. These results establish survival analysis as a powerful paradigm for evaluating multi-turn robustness and for designing practical safeguards for conversational AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè½®å¯¹è¯ä¸­ç¨³å¥æ€§éš¾ä»¥è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç”Ÿå­˜åˆ†æ(Survival Analysis)çš„æ–°å‹è¯„ä¼°èŒƒå¼ï¼Œå°†æ¨¡å‹å¤±æ•ˆè§†ä¸ºéšå¯¹è¯è½®æ¬¡æ¼”è¿›çš„æ—¶é—´äº‹ä»¶è¿‡ç¨‹ã€‚ä½œè€…åˆ©ç”¨ MT-Consistency åŸºå‡†æµ‹è¯•ä¸­ 9 ä¸ªå…ˆè¿› LLMs çš„ 36,951 è½®å¯¹è¯æ•°æ®ï¼Œç»“åˆäº† Cox Proportional Hazardsã€Accelerated Failure Time (AFT) å’Œ Random Survival Forest æ¨¡å‹è¿›è¡Œå»ºæ¨¡åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œçªå‘æ€§çš„ Prompt-to-prompt Semantic Drift ä¼šæ˜¾è‘—å¢åŠ ä¸ä¸€è‡´æ€§å‘ç”Ÿçš„é£é™©ï¼Œè€Œç´¯ç§¯çš„è¯­ä¹‰æ¼‚ç§»åè€Œè¡¨ç°å‡ºæŸç§ä¿æŠ¤æ€§ï¼Œæš—ç¤ºæ¨¡å‹åœ¨å¤šæ¬¡åç§»åå¯èƒ½äº§ç”Ÿäº†é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆäº† Model-drift äº¤äº’é¡¹çš„ AFT æ¨¡å‹åœ¨åŒºåˆ†åº¦å’Œæ ¡å‡†åº¦ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œå¹¶æ­ç¤ºäº†ä¼ ç»Ÿ Cox æ¨¡å‹åœ¨è¯¥åœºæ™¯ä¸‹çš„ç³»ç»Ÿæ€§å±€é™ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¯æ˜äº†è½»é‡çº§ AFT æ¨¡å‹å¯ä½œä¸º turn-level é£é™©ç›‘æµ‹å™¨ï¼Œèƒ½å¤Ÿåœ¨æ¨¡å‹äº§ç”Ÿä¸ä¸€è‡´å›ç­”å‰æ•°è½®å‘å‡ºé¢„è­¦ã€‚è¯¥é¡¹å·¥ä½œç¡®ç«‹äº†ç”Ÿå­˜åˆ†æä½œä¸ºè¯„ä¼°å¤šè½®å¯¹è¯ç¨³å¥æ€§çš„å¼ºåŠ›æ‰‹æ®µï¼Œä¸ºè®¾è®¡å¯¹è¯å¼ AI ç³»ç»Ÿçš„å®é™…å®‰å…¨ä¿éšœæªæ–½æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02712v2",
      "published_date": "2025-10-03 04:26:10 UTC",
      "updated_date": "2025-11-23 22:18:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:17.053907+00:00"
    },
    {
      "arxiv_id": "2510.02711v1",
      "title": "A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks",
      "title_zh": "ä¸€ç§é¢å‘æ— äººæœºç½‘ç»œå…¥ä¾µæ£€æµ‹çš„æ–°å‹ç»Ÿä¸€è½»é‡çº§æ—¶ç©º Transformer æ–¹æ³•",
      "authors": [
        "Tarun Kumar Biswas",
        "Ashrafun Zannat",
        "Waqas Ishtiaq",
        "Md. Alamgir Hossain"
      ],
      "abstract": "The growing integration of drones across commercial, industrial, and civilian domains has introduced significant cybersecurity challenges, particularly due to the susceptibility of drone networks to a wide range of cyberattacks. Existing intrusion detection mechanisms often lack the adaptability, efficiency, and generalizability required for the dynamic and resource constrained environments in which drones operate. This paper proposes TSLT-Net, a novel lightweight and unified Temporal Spatial Transformer based intrusion detection system tailored specifically for drone networks. By leveraging self attention mechanisms, TSLT-Net effectively models both temporal patterns and spatial dependencies in network traffic, enabling accurate detection of diverse intrusion types. The framework includes a streamlined preprocessing pipeline and supports both multiclass attack classification and binary anomaly detection within a single architecture. Extensive experiments conducted on the ISOT Drone Anomaly Detection Dataset, consisting of more than 2.3 million labeled records, demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in multiclass detection and 100 percent in binary anomaly detection, while maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable parameters. These results establish TSLT-Net as an effective and scalable solution for real time drone cybersecurity, particularly suitable for deployment on edge devices in mission critical UAV systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœºç½‘ç»œåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ç¼ºä¹é«˜æ•ˆå…¥ä¾µæ£€æµ‹æœºåˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº† TSLT-Netï¼Œä¸€ç§è½»é‡åŒ–ä¸”ç»Ÿä¸€çš„ Temporal-Spatial Transformer æ–¹æ³•ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ Self-attention æœºåˆ¶æœ‰æ•ˆæ•æ‰ç½‘ç»œæµé‡ä¸­çš„æ—¶é—´æ¨¡å¼ (temporal patterns) å’Œç©ºé—´ä¾èµ–æ€§ (spatial dependencies)ï¼Œå®ç°åœ¨å•ä¸€æ¶æ„ä¸­åŒæ—¶æ”¯æŒå¤šåˆ†ç±»æ”»å‡»è¯†åˆ«å’ŒäºŒåˆ†ç±»å¼‚å¸¸æ£€æµ‹ã€‚åœ¨åŒ…å«è¶…è¿‡ 230 ä¸‡æ¡è®°å½•çš„ ISOT Drone Anomaly Detection Dataset ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒTSLT-Net åœ¨å¤šåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ° 99.99% çš„å‡†ç¡®ç‡ï¼Œåœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ° 100% çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ä»…å ç”¨ 0.04 MB å†…å­˜å¹¶åŒ…å« 9722 ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå…·æœ‰æé«˜çš„è¿è¡Œæ•ˆç‡ã€‚ç ”ç©¶ç»“æœè¯æ˜ TSLT-Net æ˜¯ä¸€ä¸ªå¯æ‰©å±•ä¸”æœ‰æ•ˆçš„å®æ—¶æ— äººæœºç½‘ç»œå®‰å…¨è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚åˆéƒ¨ç½²åœ¨ä»»åŠ¡å…³é”®å‹æ— äººæœºç³»ç»Ÿçš„è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 18 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.02711v1",
      "published_date": "2025-10-03 04:23:21 UTC",
      "updated_date": "2025-10-03 04:23:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:25.552501+00:00"
    },
    {
      "arxiv_id": "2510.03366v1",
      "title": "Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis",
      "title_zh": "é€šè¿‡é€å±‚æ³¨æ„åŠ›ä¸æ¿€æ´»åˆ†æè§£è€¦ Transformer æ¨¡å‹ä¸­çš„æ£€ç´¢ä¸æ¨ç†",
      "authors": [
        "Harshwardhan Fartale",
        "Ashish Kattamuri",
        "Rahul Raja",
        "Arpita Vats",
        "Ishita Prasad",
        "Akshata Kishore Moharir"
      ],
      "abstract": "Transformer-based language models excel at both recall (retrieving memorized facts) and reasoning (performing multi-step inference), but whether these abilities rely on distinct internal mechanisms remains unclear. Distinguishing recall from reasoning is crucial for predicting model generalization, designing targeted evaluations, and building safer interventions that affect one ability without disrupting the other.We approach this question through mechanistic interpretability, using controlled datasets of synthetic linguistic puzzles to probe transformer models at the layer, head, and neuron level. Our pipeline combines activation patching and structured ablations to causally measure component contributions to each task type. Across two model families (Qwen and LLaMA), we find that interventions on distinct layers and attention heads lead to selective impairments: disabling identified \"recall circuits\" reduces fact-retrieval accuracy by up to 15\\% while leaving reasoning intact, whereas disabling \"reasoning circuits\" reduces multi-step inference by a comparable margin. At the neuron level, we observe task-specific firing patterns, though these effects are less robust, consistent with neuronal polysemanticity.Our results provide the first causal evidence that recall and reasoning rely on separable but interacting circuits in transformer models. These findings advance mechanistic interpretability by linking circuit-level structure to functional specialization and demonstrate how controlled datasets and causal interventions can yield mechanistic insights into model cognition, informing safer deployment of large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Transformeræ¨¡å‹ä¸­å¬å›(Recall)ä¸æ¨ç†(Reasoning)èƒ½åŠ›æ˜¯å¦ä¾èµ–ä¸åŒçš„å†…éƒ¨æœºåˆ¶ã€‚é€šè¿‡æœºæ¢°è§£é‡Šæ€§(Mechanistic Interpretability)æ–¹æ³•ï¼Œç ”ç©¶è€…åˆ©ç”¨å—æ§çš„åˆæˆè¯­è¨€è°œé¢˜æ•°æ®é›†ï¼Œåœ¨å±‚ã€å¤´å’Œç¥ç»å…ƒçº§åˆ«å¯¹Qwenå’ŒLLaMAæ¨¡å‹è¿›è¡Œäº†æ¢æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—ç»“åˆæ¿€æ´»è¡¥ä¸(Activation Patching)å’Œç»“æ„åŒ–æ¶ˆè(Structured Ablations)çš„æµç¨‹ï¼Œç”¨ä»¥å› æœæ€§åœ°è¡¡é‡å„ç»„ä»¶å¯¹ä»»åŠ¡çš„è´¡çŒ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTransformerå†…éƒ¨å­˜åœ¨å¯åˆ†ç¦»çš„â€œå¬å›ç”µè·¯â€(Recall Circuits)å’Œâ€œæ¨ç†ç”µè·¯â€(Reasoning Circuits)ï¼Œç¦ç”¨å…¶ä¸­ä¹‹ä¸€ä¼šäº§ç”Ÿé€‰æ‹©æ€§åŠŸèƒ½æŸå®³è€Œä¸ä¼šå¹²æ‰°å¦ä¸€é¡¹èƒ½åŠ›ã€‚åœ¨ç¥ç»å…ƒå±‚é¢ä¹Ÿè§‚å¯Ÿåˆ°äº†ä»»åŠ¡ç‰¹å¼‚æ€§çš„æ¿€æ´»æ¨¡å¼ï¼Œå°½ç®¡å—ç¥ç»å…ƒå¤šä¹‰æ€§(Polysemanticity)å½±å“å…¶é²æ£’æ€§ç•¥ä½ã€‚è¯¥ç ”ç©¶ä¸ºå¬å›ä¸æ¨ç†ä¾èµ–ä¸åŒä½†ç›¸äº’ä½œç”¨çš„ç”µè·¯æä¾›äº†é¦–ä¸ªå› æœè¯æ®ï¼Œå°†ç”µè·¯çº§ç»“æ„ä¸åŠŸèƒ½ä¸“ä¸šåŒ–è”ç³»èµ·æ¥ã€‚è¿™äº›å‘ç°æ·±åŒ–äº†å¯¹æ¨¡å‹è®¤çŸ¥çš„ç†è§£ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å®‰å…¨éƒ¨ç½²å’Œé’ˆå¯¹æ€§å¹²é¢„æä¾›äº†ç§‘å­¦ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03366v1",
      "published_date": "2025-10-03 04:13:06 UTC",
      "updated_date": "2025-10-03 04:13:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:23.351980+00:00"
    },
    {
      "arxiv_id": "2510.05156v1",
      "title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation",
      "title_zh": "VeriGuardï¼šé€šè¿‡ç»éªŒè¯çš„ä»£ç ç”Ÿæˆå¢å¼º LLM æ™ºèƒ½ä½“å®‰å…¨æ€§",
      "authors": [
        "Lesly Miculicich",
        "Mihir Parmar",
        "Hamid Palangi",
        "Krishnamurthy Dj Dvijotham",
        "Mirko Montanari",
        "Tomas Pfister",
        "Long T. Le"
      ],
      "abstract": "The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VeriGuardï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡å½¢å¼åŒ–éªŒè¯ (Formal Verification) å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“å®‰å…¨æ€§çš„åˆ›æ–°æ¡†æ¶ï¼Œä»¥åº”å¯¹è‡ªåŠ¨æ™ºèƒ½ä½“åœ¨åŒ»ç–—ç­‰æ•æ„Ÿé¢†åŸŸå¯èƒ½å¼•å‘çš„å®‰å…¨ã€é˜²å¾¡åŠéšç§é£é™©ã€‚VeriGuard é‡‡ç”¨äº†ç‹¬ç‰¹çš„åŒé˜¶æ®µæ¶æ„ï¼Œåœ¨ç¦»çº¿é˜¶æ®µé¦–å…ˆé€šè¿‡æ˜ç¡®ç”¨æˆ·æ„å›¾æ¥ç¡®ç«‹ç²¾ç¡®çš„å®‰å…¨è§„èŒƒï¼Œå¹¶åˆæˆç›¸åº”çš„è¡Œä¸ºç­–ç•¥ï¼Œéšåé€šè¿‡è¿­ä»£çš„æµ‹è¯•ä¸å½¢å¼åŒ–è¯æ˜ç¡®ä¿ç­–ç•¥å®Œå…¨ç¬¦åˆè§„èŒƒã€‚åœ¨åœ¨çº¿è¿è¡Œé˜¶æ®µï¼Œè¯¥æ¡†æ¶ä½œä¸ºè¿è¡Œæ—¶ç›‘æ§å™¨ (Runtime Monitor)ï¼Œåœ¨æ™ºèƒ½ä½“æ‰§è¡Œä»»ä½•æ“ä½œå‰éƒ½ä¼šæ ¹æ®å·²éªŒè¯çš„ç­–ç•¥è¿›è¡Œå®æ—¶æ ¡éªŒã€‚è¿™ç§å°†è¯¦å°½çš„ç¦»çº¿éªŒè¯ä¸è½»é‡çº§åœ¨çº¿ç›‘æ§ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰ç³»ç»Ÿéš¾ä»¥æä¾›å½¢å¼åŒ–å®‰å…¨ä¿è¯çš„æŒ‘æˆ˜ã€‚é€šè¿‡è¿™ç§æœºåˆ¶ï¼ŒVeriGuard ä¸º LLM æ™ºèƒ½ä½“åœ¨å®é™…éƒ¨ç½²ä¸­æä¾›äº†åšå®çš„å®‰å…¨å±éšœï¼Œæ˜¾è‘—æå‡äº†è‡ªä¸»ç³»ç»Ÿçš„å¯é æ€§ä¸å¯ä¿¡ä»»åº¦ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "22 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.05156v1",
      "published_date": "2025-10-03 04:11:43 UTC",
      "updated_date": "2025-10-03 04:11:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:37.054531+00:00"
    },
    {
      "arxiv_id": "2510.03364v1",
      "title": "Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹ä¸æ•°æ®åŒåŒ–çš„è½®æ¯‚é«˜åº¦é£è¶…åˆ†è¾¨ç‡",
      "authors": [
        "Xiaolong Ma",
        "Xu Dong",
        "Ashley Tarrant",
        "Lei Yang",
        "Rao Kotamarthi",
        "Jiali Wang",
        "Feng Yan",
        "Rajkumar Kettimuthu"
      ],
      "abstract": "High-quality observations of hub-height winds are valuable but sparse in space and time. Simulations are widely available on regular grids but are generally biased and too coarse to inform wind-farm siting or to assess extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully utilize both data types for generating high-quality, high-resolution hub-height wind speeds (tens to ~100m above ground), this study introduces WindSR, a diffusion model with data assimilation for super-resolution downscaling of hub-height winds. WindSR integrates sparse observational data with simulation fields during downscaling using state-of-the-art diffusion models. A dynamic-radius blending method is introduced to merge observations with simulations, providing conditioning for the diffusion process. Terrain information is incorporated during both training and inference to account for its role as a key driver of winds. Evaluated against convolutional-neural-network and generative-adversarial-network baselines, WindSR outperforms them in both downscaling efficiency and accuracy. Our data assimilation reduces WindSR's model bias by approximately 20% relative to independent observations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WindSR æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†æ•°æ®åŒåŒ–(data assimilation)æŠ€æœ¯çš„æ‰©æ•£æ¨¡å‹(diffusion model)ï¼Œæ—¨åœ¨è§£å†³è½®æ¯‚é«˜åº¦(hub-height winds)è§‚æµ‹æ•°æ®ç¨€ç¼ºä¸”æ¨¡æ‹Ÿåœºåˆ†è¾¨ç‡ä¸è¶³ã€å­˜åœ¨åå·®çš„é—®é¢˜ã€‚WindSR é€šè¿‡åŠ¨æ€åŠå¾„æ··åˆæ–¹æ³•(dynamic-radius blending method)å°†ç¨€ç–çš„å®é™…è§‚æµ‹æ•°æ®ä¸ç²—ç²’åº¦çš„æ¨¡æ‹Ÿåœºè¿›è¡Œæœ‰æ•ˆèåˆï¼Œå¹¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥åœ°å½¢ä¿¡æ¯(terrain information)ä½œä¸ºå…³é”®é©±åŠ¨å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWindSR åœ¨è¶…åˆ†è¾¨ç‡(super-resolution)é™å°ºåº¦çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºå·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)ç­‰åŸºçº¿æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥æ•°æ®åŒåŒ–æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹ç›¸è¾ƒäºç‹¬ç«‹è§‚æµ‹çš„é¢„æµ‹åå·®é™ä½äº†çº¦ 20%ï¼Œä¸ºç”ŸæˆåŸºç¡€è®¾æ–½å°ºåº¦çš„é«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡é£é€Ÿæ•°æ®æä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03364v1",
      "published_date": "2025-10-03 03:38:58 UTC",
      "updated_date": "2025-10-03 03:38:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:38.747407+00:00"
    },
    {
      "arxiv_id": "2510.03363v2",
      "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering",
      "title_zh": "åŸºäºåŒ¹é…ä»£ä»·è¿‡æ»¤çš„ç»Ÿä¸€æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Zhe Zhang",
        "Mingxiu Cai",
        "Gaochang Wu",
        "Jing Zhang",
        "Lingqiao Liu",
        "Dacheng Tao",
        "Tianyou Chai",
        "Xiatian Zhu"
      ],
      "abstract": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level anomalies using only normal training data, with wide applications such as industrial inspection and medical analysis, where anomalies are scarce due to privacy concerns and cold-start constraints. Existing methods, whether reconstruction-based (restoring normal counterparts) or embedding-based (pretrained representations), fundamentally conduct image- or feature-level matching to generate anomaly maps. Nonetheless, matching noise has been largely overlooked, limiting their detection ability. Beyond earlier focus on unimodal RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB-3D and RGB-Text, enabled by point cloud sensing and vision-language models. Despite shared challenges, these lines remain largely isolated, hindering a comprehensive understanding and knowledge transfer. In this paper, we advocate unified UAD for both unimodal and multimodal settings in the matching perspective. Under this insight, we present Unified Cost Filtering (UCF), a generic post-hoc refinement framework for refining anomaly cost volume of any UAD model. The cost volume is constructed by matching a test sample against normal samples from the same or different modalities, followed by a learnable filtering module with multi-layer attention guidance from the test sample, mitigating matching noise and highlighting subtle anomalies. Comprehensive experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in enhancing a variety of UAD methods, consistently achieving new state-of-the-art results in both unimodal (RGB) and multimodal (RGB-3D, RGB-Text) UAD scenarios. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éç›‘ç£å¼‚å¸¸æ£€æµ‹(Unsupervised Anomaly Detection, UAD)é¢†åŸŸä¸­åŒ¹é…å™ªå£°è¢«é•¿æœŸå¿½è§†ï¼Œä»¥åŠå•æ¨¡æ€ä¸å¤šæ¨¡æ€ç ”ç©¶è·¯å¾„ç›¸äº’å­¤ç«‹çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…æå‡ºä»åŒ¹é…è§†è§’ç»Ÿä¸€å•æ¨¡æ€å’Œå¤šæ¨¡æ€UADï¼Œå¹¶è®¾è®¡äº†åä¸ºUnified Cost Filtering (UCF)çš„é€šç”¨äº‹åç»†åŒ–æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–å„ç±»UADæ¨¡å‹çš„å¼‚å¸¸æˆæœ¬ä½“ç§¯(cost volume)ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æµ‹è¯•æ ·æœ¬ä¸æ­£å¸¸æ ·æœ¬è¿›è¡Œè·¨æ¨¡æ€æˆ–åŒæ¨¡æ€åŒ¹é…ï¼Œå¹¶ç»“åˆå¤šå±‚æ³¨æ„åŠ›å¼•å¯¼çš„å¯å­¦ä¹ è¿‡æ»¤æ¨¡å—æ¥æ¶ˆé™¤å™ªå£°ï¼Œä»è€Œç²¾å‡†æ•æ‰å¾®å¦™çš„å¼‚å¸¸ç‰¹å¾ã€‚åœ¨æ¶µç›–RGBã€RGB-3DåŠRGB-Textç­‰22ä¸ªå¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒUCFæ˜¾è‘—æå‡äº†ç°æœ‰å¤šç§UADæ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€åœºæ™¯ä¸‹å‡å–å¾—äº†æ–°çš„æœ€å…ˆè¿›(state-of-the-art)ç»“æœã€‚è¿™ä¸€ç ”ç©¶ä¸ºç»Ÿä¸€UADæ¡†æ¶ä¸‹çš„çŸ¥è¯†è¿ç§»å’Œæ€§èƒ½å¢å¼ºæä¾›äº†æ–°çš„è§†è§’å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "63 pages (main paper and supplementary material), 39 figures, 58 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03363v2",
      "published_date": "2025-10-03 03:28:18 UTC",
      "updated_date": "2025-10-08 12:00:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:42.857943+00:00"
    },
    {
      "arxiv_id": "2510.02695v2",
      "title": "RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization",
      "title_zh": "RAMACï¼šå¤šæ¨¡æ€é£é™©æ„ŸçŸ¥ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŠè¡Œä¸ºæ­£åˆ™åŒ–çš„ä½œç”¨",
      "authors": [
        "Kai Fukazawa",
        "Kunal Mundada",
        "Iman Soltani"
      ],
      "abstract": "In safety-critical domains where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value or model-based pessimism, and restricted policy classes that limit policy expressiveness, whereas diffusion/flow-based expressive generative policies trained with a behavioral-cloning (BC) objective have been used only in risk-neutral settings. Here, we address this gap by introducing the \\textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)}, which couples an expressive generative actor with a distributional critic and, to our knowledge, is the first model-free approach that learns \\emph{risk-aware expressive generative policies}. RAMAC differentiates a composite objective that adds a Conditional Value-at-Risk (CVaR) term to a BC loss, achieving risk-sensitive learning in complex multimodal scenarios. Since out-of-distribution (OOD) actions are a major driver of catastrophic failures in offline RL, we further analyze OOD behavior under prior-anchored perturbation schemes from recent BC-regularized risk-averse offline RL. This clarifies why a behavior-regularized objective that directly constrains the expressive generative policy to the dataset support provides an effective, risk-agnostic mechanism for suppressing OOD actions in modern expressive policies. We instantiate RAMAC with a diffusion-based actor, using it both to illustrate the analysis in a 2-D risky bandit and to deploy OOD-action detectors on Stochastic-D4RL benchmarks, empirically validating our insights. Across these tasks, we observe consistent gains in $\\mathrm{CVaR}_{0.1}$ while maintaining strong returns. Our implementation is available at GitHub: https://github.com/KaiFukazawa/RAMAC.git",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨å…³é”®é¢†åŸŸä¸­ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline RL)é¢ä¸´çš„ç¾éš¾æ€§ä½å°¾éƒ¨é£é™©é—®é¢˜ï¼Œæå‡ºäº†Risk-Aware Multimodal Actor-Critic (RAMAC)ï¼Œè¿™æ˜¯é¦–ä¸ªå­¦ä¹ é£é™©æ„ŸçŸ¥è¡¨ç°å‹ç”Ÿæˆç­–ç•¥(Risk-aware expressive generative policies)çš„æ— æ¨¡å‹æ–¹æ³•ã€‚è¯¥æ¡†æ¶å°†è¡¨ç°å‹ç”Ÿæˆå¼å‚ä¸è€…(Expressive generative actor)ä¸åˆ†å¸ƒè¯„ä¼°å™¨(Distributional critic)ç›¸ç»“åˆï¼Œé€šè¿‡åœ¨è¡Œä¸ºå…‹éš†(Behavioral-cloning)æŸå¤±ä¸­åŠ å…¥æ¡ä»¶é£é™©ä»·å€¼(CVaR)é¡¹ï¼Œå®ç°äº†åœ¨å¤æ‚å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„é£é™©æ•æ„Ÿå­¦ä¹ ã€‚é’ˆå¯¹ç¦»çº¿å­¦ä¹ ä¸­å¸¸è§çš„åˆ†å¸ƒå¤–(OOD)åŠ¨ä½œå¯¼è‡´çš„å¤±æ•ˆï¼Œç ”ç©¶é€šè¿‡åˆ†æè¯æ˜äº†è¡Œä¸ºæ­£åˆ™åŒ–(Behavior regularization)èƒ½æœ‰æ•ˆå°†ç”Ÿæˆç­–ç•¥çº¦æŸåœ¨æ•°æ®é›†åˆ†å¸ƒèŒƒå›´å†…ï¼Œä»è€Œæä¾›é£é™©æ— å…³çš„åŠ¨ä½œæŠ‘åˆ¶æœºåˆ¶ã€‚ä½œè€…åˆ©ç”¨åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion-based)çš„å‚ä¸è€…å®ä¾‹åŒ–äº†RAMACï¼Œå¹¶åœ¨äºŒç»´é£é™©å¤šè‡‚è€è™æœºå’ŒStochastic-D4RLåŸºå‡†ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAMACåœ¨ä¿æŒé«˜æ”¶ç›Šçš„åŒæ—¶æ˜¾è‘—æå‡äº†$\\mathrm{CVaR}_{0.1}$æŒ‡æ ‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†å¤æ‚åˆ†å¸ƒåŠæŠ‘åˆ¶åˆ†å¸ƒå¤–é£é™©æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02695v2",
      "published_date": "2025-10-03 03:22:21 UTC",
      "updated_date": "2025-12-08 09:51:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:47.870048+00:00"
    },
    {
      "arxiv_id": "2510.02692v2",
      "title": "Fine-Tuning Diffusion Models via Intermediate Distribution Shaping",
      "title_zh": "é€šè¿‡ä¸­é—´åˆ†å¸ƒæ•´å½¢å¾®è°ƒæ‰©æ•£æ¨¡å‹",
      "authors": [
        "Gautham Govind Anil",
        "Shaan Ul Haque",
        "Nithish Kannen",
        "Dheeraj Nagaraj",
        "Sanjay Shakkottai",
        "Karthikeyan Shanmugam"
      ],
      "abstract": "Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs KL regularized reward maximization with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an $8.81\\%$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¥–åŠ±å‡½æ•°å¯¹é¢„è®­ç»ƒçš„Diffusion modelsè¿›è¡Œå¾®è°ƒï¼Œä»¥ä½¿å…¶åˆ†å¸ƒæ›´å¥½åœ°å¯¹é½ä¸‹æ¸¸åº”ç”¨éœ€æ±‚ã€‚é’ˆå¯¹Proximal Policy Optimization (PPO)åœ¨å¤„ç†æ‰©æ•£æ¨¡å‹æ—¶è¾¹ç¼˜ä¼¼ç„¶è®¡ç®—å—é˜»çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†GRAFTæ¡†æ¶ï¼Œç»Ÿä¸€äº†Rejection Sampling based Fine-Tuning (RAFT)çš„å˜ä½“ï¼Œå¹¶è¯æ˜å…¶éšå«åœ°æ‰§è¡Œäº†å¸¦é‡å¡‘å¥–åŠ±çš„KL regularized reward maximizationã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†P-GRAFTï¼Œé€šè¿‡åœ¨ä¸­é—´å™ªå£°æ°´å¹³(Intermediate noise levels)å¡‘é€ åˆ†å¸ƒæ¥æå‡å¾®è°ƒæ•ˆç‡ï¼Œå¹¶ä»Bias-variance tradeoffçš„è§’åº¦ä¸ºå…¶æä¾›äº†æ•°å­¦è§£é‡Šã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜é’ˆå¯¹Flow modelsæå‡ºäº†æ— éœ€æ˜¾å¼å¥–åŠ±çš„Inverse noise correctionæŠ€æœ¯ä»¥ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨Stable Diffusion 2ä¸Šçš„æ–‡æœ¬åˆ°å›¾åƒ(T2I)ç”Ÿæˆä»»åŠ¡ä¸­ç›¸è¾ƒåŸºçº¿æ¨¡å‹æå‡äº†8.81%ï¼Œå¹¶åœ¨å¸ƒå±€ç”Ÿæˆã€åˆ†å­ç”ŸæˆåŠæ— æ¡ä»¶å›¾åƒç”Ÿæˆçš„FIDæŒ‡æ ‡ä¸Šå‡å±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02692v2",
      "published_date": "2025-10-03 03:18:47 UTC",
      "updated_date": "2026-01-15 06:46:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:52.148145+00:00"
    },
    {
      "arxiv_id": "2510.06238v1",
      "title": "Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout",
      "title_zh": "åŸºäº MC Dropout çš„åœ°è¡¨åœ°é›·ä¸ UXO åˆ†ç±»ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Sagar Lekhak",
        "Emmett J. Ientilucci",
        "Dimah Dera",
        "Susmita Ghosh"
      ],
      "abstract": "Detecting surface landmines and unexploded ordnances (UXOs) using deep learning has shown promise in humanitarian demining. However, deterministic neural networks can be vulnerable to noisy conditions and adversarial attacks, leading to missed detection or misclassification. This study introduces the idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated into a fine-tuned ResNet-50 architecture for surface landmine and UXO classification, which was tested on a simulated dataset. Integrating the MC Dropout approach helps quantify epistemic uncertainty, providing an additional metric for prediction reliability, which could be helpful to make more informed decisions in demining operations. Experimental results on clean, adversarially perturbed, and noisy test images demonstrate the model's ability to flag unreliable predictions under challenging conditions. This proof-of-concept study highlights the need for uncertainty quantification in demining, raises awareness about the vulnerability of existing neural networks in demining to adversarial threats, and emphasizes the importance of developing more robust and reliable models for practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ°è¡¨åœ°é›·å’Œæœªçˆ†ç‚¸å¼¹è¯ (UXO) åˆ†ç±»ä¸­ï¼Œä¼ ç»Ÿç¡®å®šæ€§ç¥ç»ç½‘ç»œæ˜“å—å™ªå£°å’Œå¯¹æŠ—æ€§æ”»å‡»å½±å“è€Œå¯¼è‡´è¯¯åˆ¤çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸ç¡®å®šæ€§é‡åŒ– (Uncertainty Quantification) çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å°† Monte Carlo (MC) Dropout æŠ€æœ¯é›†æˆåˆ°ç»è¿‡å¾®è°ƒçš„ ResNet-50 æ¶æ„ä¸­ï¼Œæ—¨åœ¨é‡åŒ–æ¨¡å‹é¢„æµ‹ä¸­çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic Uncertainty)ã€‚é€šè¿‡æä¾›é¢„æµ‹å¯é æ€§çš„é¢å¤–åº¦é‡æŒ‡æ ‡ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè¾…åŠ©æ’é›·è¡ŒåŠ¨åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚ç ”ç©¶åœ¨æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¶µç›–äº†çº¯å‡€ã€å¯¹æŠ—æ€§æ‰°åŠ¨åŠå™ªå£°å¹²æ‰°ç­‰å¤šç§æµ‹è¯•åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹èƒ½æœ‰æ•ˆæ ‡è®°ä¸å¯ä¿¡çš„é¢„æµ‹ï¼Œæ˜¾è‘—æå‡äº†åˆ†ç±»ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚è¯¥æ¦‚å¿µéªŒè¯ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æ’é›·é¢†åŸŸé¢ä¸´çš„å¯¹æŠ—æ€§å¨èƒï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘æ›´å…·é²æ£’æ€§å’Œå¯é æ€§æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "stat.OT"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been accepted and presented at IGARSS 2025 and will appear in the IEEE IGARSS 2025 proceedings",
      "pdf_url": "https://arxiv.org/pdf/2510.06238v1",
      "published_date": "2025-10-03 03:01:22 UTC",
      "updated_date": "2025-10-03 03:01:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:41:04.542081+00:00"
    },
    {
      "arxiv_id": "2510.02683v1",
      "title": "Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators",
      "title_zh": "æ•°æ®é©±åŠ¨åŠ¨åŠ›å­¦èƒ½å¦æ­ç¤ºéšè—çš„ç‰©ç†è§„å¾‹ï¼Ÿè®ºå¯¹å¯è§£é‡Šç¥ç»ç®—å­çš„éœ€æ±‚",
      "authors": [
        "Wenhan Gao",
        "Jian Luo",
        "Fang Wan",
        "Ruichen Xu",
        "Xiang Liu",
        "Haipeng Xing",
        "Yi Liu"
      ],
      "abstract": "Recently, neural operators have emerged as powerful tools for learning mappings between function spaces, enabling data-driven simulations of complex dynamics. Despite their successes, a deeper understanding of their learning mechanisms remains underexplored. In this work, we classify neural operators into two types: (1) Spatial domain models that learn on grids and (2) Functional domain models that learn with function bases. We present several viewpoints based on this classification and focus on learning data-driven dynamics adhering to physical principles. Specifically, we provide a way to explain the prediction-making process of neural operators and show that neural operator can learn hidden physical patterns from data. However, this explanation method is limited to specific situations, highlighting the urgent need for generalizable explanation methods. Next, we show that a simple dual-space multi-scale model can achieve SOTA performance and we believe that dual-space multi-spatio-scale models hold significant potential to learn complex physics and require further investigation. Lastly, we discuss the critical need for principled frameworks to incorporate known physics into neural operators, enabling better generalization and uncovering more hidden physical phenomena.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°æ®é©±åŠ¨çš„åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ˜¯å¦èƒ½æ­ç¤ºéšè—çš„ç‰©ç†è§„å¾‹ï¼Œå¹¶å°† neural operators åˆ†ä¸ºç©ºé—´åŸŸæ¨¡å‹å’Œå‡½æ•°åŸŸæ¨¡å‹ä¸¤å¤§ç±»ã€‚ä½œè€…æå‡ºäº†ä¸€ç§è§£é‡Š neural operators é¢„æµ‹è¿‡ç¨‹çš„æ–¹æ³•ï¼Œè¯æ˜äº†æ¨¡å‹èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ åˆ°éšè—çš„ç‰©ç†æ¨¡å¼(hidden physical patterns)ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§£é‡Šæ–¹æ³•ä»å±€é™äºç‰¹å®šæƒ…å¢ƒï¼Œè¿™è¡¨æ˜ä¸šç•Œè¿«åˆ‡éœ€è¦é€šç”¨çš„å¯è§£é‡Šæ€§ç ”ç©¶æ–¹æ¡ˆã€‚ç ”ç©¶è¿›ä¸€æ­¥å±•ç¤ºäº†ä¸€ç§ç®€å•çš„åŒç©ºé—´å¤šå°ºåº¦æ¨¡å‹(dual-space multi-scale model)ï¼Œè¯¥æ¨¡å‹ä¸ä»…è¾¾åˆ°äº† State-of-the-Art (SOTA) çš„æ€§èƒ½ï¼Œè¿˜å±•ç°å‡ºå­¦ä¹ å¤æ‚ç‰©ç†è§„å¾‹çš„å·¨å¤§æ½œåŠ›ã€‚æœ€åï¼Œè®ºæ–‡å¼ºè°ƒäº†å»ºç«‹è§„èŒƒåŒ–æ¡†æ¶ä»¥å°†å·²çŸ¥ç‰©ç†çŸ¥è¯†èå…¥ neural operators çš„å¿…è¦æ€§ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶è¾…åŠ©å‘ç°æ›´å¤šéšè—çš„ç‰©ç†ç°è±¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02683v1",
      "published_date": "2025-10-03 02:50:21 UTC",
      "updated_date": "2025-10-03 02:50:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:41:56.555108+00:00"
    },
    {
      "arxiv_id": "2510.02679v1",
      "title": "Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation",
      "title_zh": "åŸºäºé¢†åŸŸç‰¹å®šè¡¨ç¤ºè§„çº¦ç”Ÿæˆå¼æ¨¡å‹çš„è‡ªåŠ¨åŒ–ä½œä¸šè°ƒåº¦çº¦æŸè§„èŒƒ",
      "authors": [
        "Yu-Zhe Shi",
        "Qiao Xu",
        "Yanjia Li",
        "Mingchen Liu",
        "Huamin Qu",
        "Lecheng Ruan",
        "Qining Wang"
      ],
      "abstract": "Advanced Planning and Scheduling (APS) systems have become indispensable for modern manufacturing operations, enabling optimized resource allocation and production efficiency in increasingly complex and dynamic environments. While algorithms for solving abstracted scheduling problems have been extensively investigated, the critical prerequisite of specifying manufacturing requirements into formal constraints remains manual and labor-intensive. Although recent advances of generative models, particularly Large Language Models (LLMs), show promise in automating constraint specification from heterogeneous raw manufacturing data, their direct application faces challenges due to natural language ambiguity, non-deterministic outputs, and limited domain-specific knowledge. This paper presents a constraint-centric architecture that regulates LLMs to perform reliable automated constraint specification for production scheduling. The architecture defines a hierarchical structural space organized across three levels, implemented through domain-specific representation to ensure precision and reliability while maintaining flexibility. Furthermore, an automated production scenario adaptation algorithm is designed and deployed to efficiently customize the architecture for specific manufacturing configurations. Experimental results demonstrate that the proposed approach successfully balances the generative capabilities of LLMs with the reliability requirements of manufacturing systems, significantly outperforming pure LLM-based approaches in constraint specification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£åˆ¶é€ ä¸šä¸­é«˜çº§è®¡åˆ’ä¸æ’äº§(Advanced Planning and Scheduling, APS)ç³»ç»Ÿçº¦æŸè§„èŒƒè¿‡ç¨‹ç¹çä¸”å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´è¾“å‡ºä¸ç¡®å®šæ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥çº¦æŸä¸ºä¸­å¿ƒçš„è‡ªåŠ¨åŒ–æ¶æ„ã€‚è¯¥æ¶æ„åˆ©ç”¨é¢†åŸŸç‰¹å®šè¡¨ç¤º(Domain-Specific Representation)æ„å»ºäº†ä¸‰å±‚å±‚çº§ç»“æ„ç©ºé—´ï¼Œé€šè¿‡è§„èŒƒLLMsçš„è¡Œä¸ºæ¥ç¡®ä¿çº¦æŸç”Ÿæˆçš„ç²¾ç¡®æ€§ä¸å¯é æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†è‡ªåŠ¨ç”Ÿäº§åœºæ™¯è‡ªé€‚åº”ç®—æ³•(Automated Production Scenario Adaptation Algorithm)ï¼Œç”¨äºé’ˆå¯¹ä¸åŒåˆ¶é€ é…ç½®è¿›è¡Œå¿«é€Ÿå®šåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå¹³è¡¡äº†LLMsçš„ç”Ÿæˆèƒ½åŠ›ä¸åˆ¶é€ ç³»ç»Ÿçš„å¯é æ€§éœ€æ±‚ï¼Œåœ¨çº¦æŸè§„èŒƒä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºçº¯LLMæ–¹æ³•ï¼Œä¸ºå¤æ‚ç”Ÿäº§ç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–æ’äº§æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication in IEEE Transactions on Automation Science and Engineering",
      "pdf_url": "https://arxiv.org/pdf/2510.02679v1",
      "published_date": "2025-10-03 02:34:11 UTC",
      "updated_date": "2025-10-03 02:34:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:40:59.748600+00:00"
    },
    {
      "arxiv_id": "2510.02677v1",
      "title": "ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks",
      "title_zh": "ARMsï¼šé’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹çš„å³æ’å³ç”¨æ”»å‡»è‡ªé€‚åº”çº¢é˜Ÿæ™ºèƒ½ä½“",
      "authors": [
        "Zhaorun Chen",
        "Xun Liu",
        "Mintong Kang",
        "Jiawei Zhang",
        "Minzhou Pan",
        "Shuang Yang",
        "Bo Li"
      ],
      "abstract": "As vision-language models (VLMs) gain prominence, their multimodal interfaces also introduce new safety vulnerabilities, making the safety evaluation challenging and critical. Existing red-teaming efforts are either restricted to a narrow set of adversarial patterns or depend heavily on manual engineering, lacking scalable exploration of emerging real-world VLM vulnerabilities. To bridge this gap, we propose ARMs, an adaptive red-teaming agent that systematically conducts comprehensive risk assessments for VLMs. Given a target harmful behavior or risk definition, ARMs automatically optimizes diverse red-teaming strategies with reasoning-enhanced multi-step orchestration, to effectively elicit harmful outputs from target VLMs. We propose 11 novel multimodal attack strategies, covering diverse adversarial patterns of VLMs (e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming algorithms into ARMs via model context protocol (MCP). To balance the diversity and effectiveness of the attack, we design a layered memory with an epsilon-greedy attack exploration algorithm. Extensive experiments on instance- and policy-based benchmarks show that ARMs achieves SOTA attack success rates, exceeding baselines by an average of 52.1% and surpassing 90% on Claude-4-Sonnet. We show that the diversity of red-teaming instances generated by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs. Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety dataset comprising over 30K red-teaming instances spanning 51 diverse risk categories, grounded in both real-world multimodal threats and regulatory risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness of VLMs while preserving their general utility, providing actionable guidance to improve multimodal safety alignment against emerging threats.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ARMsï¼Œä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„è‡ªé€‚åº”çº¢é˜Ÿä»£ç†(red-teaming agent)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œæ¼æ´æ¢ç´¢å’Œæ‰©å±•æ€§ä¸Šçš„ä¸è¶³ã€‚ARMsé€šè¿‡æ¨ç†å¢å¼ºçš„å¤šæ­¥åè°ƒæœºåˆ¶ï¼Œåˆ©ç”¨11ç§æ–°å‹å¤šæ¨¡æ€æ”»å‡»ç­–ç•¥ï¼ˆå¦‚reasoning hijackingå’Œcontextual cloakingï¼‰å¹¶ç»“åˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®(MCP)é›†æˆçš„17ç§ç®—æ³•æ¥è‡ªåŠ¨ä¼˜åŒ–æ”»å‡»æ–¹æ¡ˆã€‚ä¸ºäº†å…¼é¡¾æ”»å‡»çš„å¤šæ ·æ€§ä¸æœ‰æ•ˆæ€§ï¼Œè¯¥ç³»ç»Ÿè®¾è®¡äº†åˆ†å±‚å†…å­˜(layered memory)ä¸epsilon-greedyæ¢ç´¢ç®—æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒARMsåœ¨æ”»å‡»æˆåŠŸç‡ä¸Šè¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œåœ¨Claude-4-Sonnetä¸Šçš„æˆåŠŸç‡è¶…è¿‡90%ï¼Œæ˜¾è‘—æ­ç¤ºäº†VLMsçš„æ–°å…´æ¼æ´ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«3ä¸‡å¤šä¸ªå®ä¾‹çš„å¤§è§„æ¨¡æ•°æ®é›†ARMs-Benchï¼Œæ¶µç›–51ä¸ªé£é™©ç±»åˆ«ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†è¿›è¡Œå®‰å…¨å¾®è°ƒ(safety fine-tuning)èƒ½å¤§å¹…æå‡æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä¸ºå¤šæ¨¡æ€å®‰å…¨å¯¹é½æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "60 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.02677v1",
      "published_date": "2025-10-03 02:28:02 UTC",
      "updated_date": "2025-10-03 02:28:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:41:02.050233+00:00"
    },
    {
      "arxiv_id": "2510.02676v1",
      "title": "To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration",
      "title_zh": "å‹ç¼©ï¼Œè¿˜æ˜¯ä¸å‹ç¼©ï¼ŸåŸºäºæŒ‡æ•°é›†ä¸­ç°è±¡æ‹“å±•ç”Ÿæˆå¼ AI æ¨¡å‹æƒé‡çš„æ— æŸå‹ç¼©å‰æ²¿",
      "authors": [
        "Zeyu Yang",
        "Tianyi Zhang",
        "Jianwen Xie",
        "Chuan Li",
        "Zhaozhuo Xu",
        "Anshumali Shrivastava"
      ],
      "abstract": "The scaling of Generative AI (GenAI) models into the hundreds of billions of parameters makes low-precision computation indispensable for efficient deployment. We argue that the fundamental solution lies in developing low-precision floating-point formats, which inherently provide numerical stability, memory savings, and hardware efficiency without dequantization overhead. In this paper, we present a theoretical and empirical study of an exponent concentration phenomenon in GenAI weights: exponents consistently exhibit low entropy across architectures and modalities. We show that this arises naturally from $Î±$-stable distributions induced by stochastic gradient descent, and we prove tight bounds on the entropy of exponents. Our analysis establishes a theoretical compression limit near FP4.67, which motivates the design of a practical FP8 format. Building on these insights, we propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs up to 671B parameters demonstrate up to 26.9% memory savings and 177.1% throughput acceleration, with perfectly lossless computations, i.e., no deviation in model outputs. Our results establish exponent concentration as a statistical law of trained models and open a principled path for lossless low-precision floating-point design in the FP8 era.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) æ¨¡å‹æƒé‡çš„æ— æŸå‹ç¼©ï¼Œæå‡ºä½ç²¾åº¦æµ®ç‚¹æ ¼å¼æ˜¯å®ç°é«˜æ•ˆéƒ¨ç½²çš„æ ¸å¿ƒè§£å†³æ–¹æ¡ˆã€‚ä½œè€…é€šè¿‡ç†è®ºä¸å®è¯ç ”ç©¶å‘ç° GenAI æƒé‡ä¸­æ™®éå­˜åœ¨æŒ‡æ•°é›†ä¸­ (exponent concentration) ç°è±¡ï¼Œå³ä¸åŒæ¶æ„å’Œæ¨¡æ€çš„æƒé‡æŒ‡æ•°å‡å‘ˆç°å‡ºä½ç†µç‰¹å¾ï¼Œå¹¶è¯æ˜è¯¥ç°è±¡æºäºéšæœºæ¢¯åº¦ä¸‹é™ (SGD) è¯±å¯¼çš„ $\\alpha$-stable åˆ†å¸ƒã€‚åŸºäºå¯¹æŒ‡æ•°ç†µç†è®ºæé™çš„åˆ†æï¼Œç ”ç©¶æå‡ºäº† Exponent-Concentrated FP8 (ECF8) æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†ç†µæ„ŸçŸ¥ç¼–ç ä¸ GPU ä¼˜åŒ–è§£ç çš„æ— æŸå‹ç¼©æŠ€æœ¯ã€‚åœ¨é«˜è¾¾ 671B å‚æ•°çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) å’Œæ‰©æ•£å˜æ¢å™¨ (DiTs) ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒECF8 åœ¨ä¿è¯è®¡ç®—å®Œå…¨æ— æŸçš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾ 26.9% çš„å†…å­˜èŠ‚çœå’Œ 177.1% çš„ååé‡æå‡ã€‚è¯¥æˆæœå°†æŒ‡æ•°é›†ä¸­ç¡®ç«‹ä¸ºè®­ç»ƒæ¨¡å‹çš„ç»Ÿè®¡è§„å¾‹ï¼Œä¸º FP8 æ—¶ä»£çš„ä½ç²¾åº¦æµ®ç‚¹æ ¼å¼è®¾è®¡å¼€è¾Ÿäº†ç§‘å­¦è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02676v1",
      "published_date": "2025-10-03 02:22:13 UTC",
      "updated_date": "2025-10-03 02:22:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:41:05.752143+00:00"
    },
    {
      "arxiv_id": "2510.02675v1",
      "title": "HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference",
      "title_zh": "HALOï¼šé¢å‘å°æ‰¹é‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„2.5Dé›†æˆå­˜å‚¨ä¸­å¿ƒå‹å¼‚æ„åŠ é€Ÿå™¨",
      "authors": [
        "Shubham Negi",
        "Kaushik Roy"
      ],
      "abstract": "The rapid adoption of Large Language Models (LLMs) has driven a growing demand for efficient inference, particularly in latency-sensitive applications such as chatbots and personalized assistants. Unlike traditional deep neural networks, LLM inference proceeds in two distinct phases: the prefill phase, which processes the full input sequence in parallel, and the decode phase, which generates tokens sequentially. These phases exhibit highly diverse compute and memory requirements, which makes accelerator design particularly challenging. Prior works have primarily been optimized for high-batch inference or evaluated only short input context lengths, leaving the low-batch and long context regime, which is critical for interactive applications, largely underexplored.\n  We propose HALO, a heterogeneous memory centric accelerator designed for these unique challenges of prefill and decode phases in low-batch LLM inference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip analog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further improve the hardware utilization, we introduce a phase-aware mapping strategy that adapts to the distinct demands of the prefill and decode phases. Compute bound operations in the prefill phase are mapped to CiM to exploit its high throughput matrix multiplication capability, while memory-bound operations in the decode phase are executed on CiD to benefit from reduced data movement within DRAM. Additionally, we present an analysis of the performance tradeoffs of LLMs under two architectural extremes: a fully CiD and a fully on-chip analog CiM design to highlight the need for a heterogeneous design. We evaluate HALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs mapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an attention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HALOï¼Œä¸€ç§é‡‡ç”¨2.5Dé›†æˆçš„ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¼‚æ„åŠ é€Ÿå™¨ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä½æ‰¹æ¬¡(low-batch)å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚HALOé€šè¿‡2.5Då°è£…æŠ€æœ¯å°†åŸºäºHBMçš„Compute-in-DRAM (CiD)ä¸ç‰‡ä¸Šæ¨¡æ‹ŸCompute-in-Memory (CiM)ç›¸ç»“åˆï¼Œä»¥åº”å¯¹prefillå’Œdecodeé˜¶æ®µæˆªç„¶ä¸åŒçš„è®¡ç®—ä¸å†…å­˜éœ€æ±‚ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§ç›¸ä½æ„ŸçŸ¥(phase-aware)æ˜ å°„ç­–ç•¥ï¼Œå°†è®¡ç®—å—é™çš„prefillé˜¶æ®µä»»åŠ¡æ˜ å°„åˆ°é«˜ååé‡çš„CiMï¼Œè€Œå°†å†…å­˜å—é™çš„decodeé˜¶æ®µä»»åŠ¡æ‰§è¡ŒäºCiDï¼Œä»è€Œå¤§å¹…å‡å°‘æ•°æ®ç§»åŠ¨å¹¶æé«˜ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚é€šè¿‡åˆ†æå…¨CiDå’Œå…¨CiMæ¶æ„çš„æ€§èƒ½æƒè¡¡ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†å¼‚æ„è®¾è®¡çš„å¿…è¦æ€§ã€‚åœ¨LLaMA-2 7Bå’ŒQwen3 8Bæ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHALOç›¸æ¯”æ³¨æ„åŠ›ä¼˜åŒ–çš„æ˜ å°„æ–¹æ¡ˆAttAccå®ç°äº†æœ€é«˜18å€çš„å‡ ä½•å¹³å‡åŠ é€Ÿï¼Œç›¸æ¯”å…¨CiDæ˜ å°„æ–¹æ¡ˆCENTå®ç°äº†2.5å€çš„åŠ é€Ÿã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02675v1",
      "published_date": "2025-10-03 02:20:17 UTC",
      "updated_date": "2025-10-03 02:20:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:09.214200+00:00"
    },
    {
      "arxiv_id": "2511.14770v1",
      "title": "ExplainRec: Towards Explainable Multi-Modal Zero-Shot Recommendation with Preference Attribution and Large Language Models",
      "title_zh": "ExplainRecï¼šåŸºäºåå¥½å½’å› ä¸å¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šå¤šæ¨¡æ€é›¶æ ·æœ¬æ¨è",
      "authors": [
        "Bo Ma",
        "LuYao Liu",
        "ZeHua Hu",
        "Simon Lau"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have opened new possibilities for recommendation systems, though current approaches such as TALLRec face challenges in explainability and cold-start scenarios. We present ExplainRec, a framework that extends LLM-based recommendation capabilities through preference attribution, multi-modal fusion, and zero-shot transfer learning. The framework incorporates four technical contributions: preference attribution tuning for explainable recommendations, zero-shot preference transfer for cold-start users and items, multi-modal enhancement leveraging visual and textual content, and multi-task collaborative optimization. Experimental evaluation on MovieLens-25M and Amazon datasets shows that ExplainRec outperforms existing methods, achieving AUC improvements of 0.7\\% on movie recommendation and 0.9\\% on cross-domain tasks, while generating interpretable explanations and handling cold-start scenarios effectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ExplainRec æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¨èç³»ç»Ÿä¸­é¢ä¸´çš„è§£é‡Šæ€§ä¸è¶³å’Œå†·å¯åŠ¨ (cold-start) æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åå¥½å½’å› å¾®è°ƒ (preference attribution tuning) æå‡äº†æ¨èçš„å¯è§£é‡Šæ€§ï¼Œå¹¶åˆ©ç”¨é›¶æ ·æœ¬åå¥½è¿ç§» (zero-shot preference transfer) æŠ€æœ¯æœ‰æ•ˆåº”å¯¹æ–°ç”¨æˆ·å’Œæ–°ç‰©å“çš„å†·å¯åŠ¨åœºæ™¯ã€‚ExplainRec è¿›ä¸€æ­¥æ•´åˆäº†å¤šæ¨¡æ€èåˆ (multi-modal fusion) æŠ€æœ¯ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬å†…å®¹ï¼Œå¹¶é€šè¿‡å¤šä»»åŠ¡åä½œä¼˜åŒ– (multi-task collaborative optimization) å¢å¼ºç³»ç»Ÿæ€§èƒ½ã€‚åœ¨ MovieLens-25M å’Œ Amazon æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒExplainRec åœ¨ç”µå½±æ¨èå’Œè·¨é¢†åŸŸä»»åŠ¡ä¸Šçš„ AUC æŒ‡æ ‡åˆ†åˆ«æå‡äº† 0.7% å’Œ 0.9%ã€‚å®éªŒè¯æ˜è¯¥æ¡†æ¶ä¸ä»…åœ¨æ¨èç²¾åº¦ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¿˜èƒ½ç”Ÿæˆå…·æœ‰è§£é‡Šæ€§çš„æ¨èç†ç”±ï¼Œä¸ºå®ç°å¯è§£é‡Šçš„å¤šæ¨¡æ€é›¶æ ·æœ¬ (zero-shot) æ¨èæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14770v1",
      "published_date": "2025-10-03 02:09:41 UTC",
      "updated_date": "2025-10-03 02:09:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:09.653681+00:00"
    },
    {
      "arxiv_id": "2510.17832v1",
      "title": "Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„è¿åŠ¨æƒ³è±¡ä»»åŠ¡åˆæˆè„‘ç”µä¿¡å·ç”Ÿæˆ",
      "authors": [
        "Henrique de Lima Alexandre",
        "Clodoaldo Aparecido de Moraes Lima"
      ],
      "abstract": "Electroencephalography (EEG) is a widely used, non-invasive method for capturing brain activity, and is particularly relevant for applications in Brain-Computer Interfaces (BCI). However, collecting high-quality EEG data remains a major challenge due to sensor costs, acquisition time, and inter-subject variability. To address these limitations, this study proposes a methodology for generating synthetic EEG signals associated with motor imagery brain tasks using Diffusion Probabilistic Models (DDPM). The approach involves preprocessing real EEG data, training a diffusion model to reconstruct EEG channels from noise, and evaluating the quality of the generated signals through both signal-level and task-level metrics. For validation, we employed classifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks (CNN), and U-Net to compare the performance of synthetic data against real data in classification tasks. The generated data achieved classification accuracies above 95%, with low mean squared error and high correlation with real signals.\n  Our results demonstrate that synthetic EEG signals produced by diffusion models can effectively complement datasets, improving classification performance in EEG-based BCIs and addressing data scarcity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘ç”µå›¾(EEG)æ•°æ®é‡‡é›†å—é™äºæˆæœ¬ã€æ—¶é—´å’Œä¸ªä½“å·®å¼‚çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹(DDPM)ç”Ÿæˆè¿åŠ¨æƒ³è±¡(Motor Imagery)ä»»åŠ¡åˆæˆè„‘ç”µä¿¡å·çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡é¢„å¤„ç†çœŸå®EEGæ•°æ®å¹¶è®­ç»ƒæ‰©æ•£æ¨¡å‹ä»å™ªå£°ä¸­é‡å»ºä¿¡å·é€šé“ï¼ŒåŒæ—¶å¼•å…¥ä¿¡å·çº§å’Œä»»åŠ¡çº§æŒ‡æ ‡è¯„ä¼°ç”Ÿæˆè´¨é‡ã€‚é€šè¿‡K-Nearest Neighbors (KNN)ã€å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’ŒU-Netç­‰åˆ†ç±»å™¨çš„éªŒè¯ï¼Œåˆæˆæ•°æ®åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†95%ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼Œä¸”ä¸çœŸå®ä¿¡å·ä¿æŒé«˜åº¦ç›¸å…³ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆä¿¡å·èƒ½æœ‰æ•ˆæ‰©å……æ•°æ®é›†å¹¶æå‡è„‘æœºæ¥å£(BCI)çš„æ€§èƒ½ï¼Œä¸ºè§£å†³EEGæ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†å¯é æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "15 pages, BRACIS",
      "pdf_url": "https://arxiv.org/pdf/2510.17832v1",
      "published_date": "2025-10-03 02:02:05 UTC",
      "updated_date": "2025-10-03 02:02:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:16.955316+00:00"
    },
    {
      "arxiv_id": "2510.02669v1",
      "title": "AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models",
      "title_zh": "AutoMaASï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„è‡ªæ¼”åŒ–å¤šæ™ºèƒ½ä½“æ¶æ„æœç´¢",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Liu"
      ],
      "abstract": "Multi-agent systems powered by large language models have demonstrated remarkable capabilities across diverse domains, yet existing automated design approaches seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements. This paper introduces AutoMaAS, a self-evolving multi-agent architecture search framework that leverages neural architecture search principles to automatically discover optimal agent configurations through dynamic operator lifecycle management and automated machine learning techniques. Our approach incorporates four key innovations: (1) automatic operator generation, fusion, and elimination based on performance-cost analysis, (2) dynamic cost-aware optimization with real-time parameter adjustment, (3) online feedback integration for continuous architecture refinement, and (4) enhanced interpretability through decision tracing mechanisms. Extensive experiments across six benchmarks demonstrate that AutoMaAS achieves 1.0-7.1\\% performance improvement while reducing inference costs by 3-5\\% compared to state-of-the-art methods. The framework shows superior transferability across datasets and LLM backbones, establishing a new paradigm for automated multi-agent system design in the era of large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AutoMaASï¼Œä¸€ç§é’ˆå¯¹Large Language Models (LLMs) çš„è‡ªæˆ‘è¿›åŒ–å¤šæ™ºèƒ½ä½“æ¶æ„æœç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–è®¾è®¡æ–¹æ³•æ— æ³•æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦å’Œé¢†åŸŸéœ€æ±‚çµæ´»åˆ†é…èµ„æºçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†Neural Architecture Search (NAS) åŸç†ï¼Œé€šè¿‡åŠ¨æ€ç®—å­ç”Ÿå‘½å‘¨æœŸç®¡ç†å®ç°æœ€ä¼˜æ™ºèƒ½ä½“é…ç½®çš„è‡ªåŠ¨å‘ç°ã€‚AutoMaASçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºèƒ½å¤ŸåŸºäºæ€§èƒ½æˆæœ¬åˆ†æè¿›è¡Œç®—å­çš„è‡ªåŠ¨ç”Ÿæˆã€èåˆä¸æ¶ˆé™¤ï¼Œå¹¶ç»“åˆå®æ—¶å‚æ•°è°ƒæ•´å®ç°åŠ¨æ€æˆæœ¬æ„ŸçŸ¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡åœ¨çº¿åé¦ˆé›†æˆè¿›è¡Œæ¶æ„çš„æŒç»­ä¼˜åŒ–ï¼Œå¹¶åˆ©ç”¨å†³ç­–è¿½è¸ªæœºåˆ¶æå‡äº†æœç´¢è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoMaASåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•æ€§èƒ½æå‡äº†1.0-7.1%ï¼ŒåŒæ—¶å°†æ¨ç†æˆæœ¬é™ä½äº†3-5%ã€‚è¯¥æ¡†æ¶åœ¨ä¸åŒæ•°æ®é›†å’ŒLLMéª¨å¹²ç½‘ç»œä¸Šå±•ç°å‡ºæå¼ºçš„è¿ç§»èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡ç¡®ç«‹äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02669v1",
      "published_date": "2025-10-03 01:57:07 UTC",
      "updated_date": "2025-10-03 01:57:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:19.451938+00:00"
    },
    {
      "arxiv_id": "2510.02668v1",
      "title": "AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems",
      "title_zh": "AgenticRAGï¼šé¢å‘é›¶æ ·æœ¬å¯è§£é‡Šæ¨èç³»ç»Ÿçš„å·¥å…·å¢å¼ºå‹åŸºç¡€æ¨¡å‹",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Liu"
      ],
      "abstract": "Foundation models have revolutionized artificial intelligence, yet their application in recommender systems remains limited by reasoning opacity and knowledge constraints. This paper introduces AgenticRAG, a novel framework that combines tool-augmented foundation models with retrieval-augmented generation for zero-shot explainable recommendations. Our approach integrates external tool invocation, knowledge retrieval, and chain-of-thought reasoning to create autonomous recommendation agents capable of transparent decision-making without task-specific training. Experimental results on three real-world datasets demonstrate that AgenticRAG achieves consistent improvements over state-of-the-art baselines, with NDCG@10 improvements of 0.4\\% on Amazon Electronics, 0.8\\% on MovieLens-1M, and 1.6\\% on Yelp datasets. The framework exhibits superior explainability while maintaining computational efficiency comparable to traditional methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgenticRAGï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å·¥å…·å¢å¼ºåŸºç¡€æ¨¡å‹ (Tool-Augmented Foundation Models) ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation) ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é›¶æ ·æœ¬ (Zero-Shot) å¯è§£é‡Šæ¨èã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆå¤–éƒ¨å·¥å…·è°ƒç”¨ (External Tool Invocation)ã€çŸ¥è¯†æ£€ç´¢ (Knowledge Retrieval) å’Œé“¾å¼æ€ç»´æ¨ç† (Chain-of-Thought Reasoning)ï¼Œæ„å»ºäº†æ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒå³å¯è¿›è¡Œé€æ˜å†³ç­–çš„è‡ªä¸»æ¨èæ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgenticRAG åœ¨ Amazon Electronicsã€MovieLens-1M å’Œ Yelp æ•°æ®é›†ä¸Šçš„ NDCG@10 æŒ‡æ ‡åˆ†åˆ«æå‡äº† 0.4%ã€0.8% å’Œ 1.6%ï¼Œå±•ç°å‡ºä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—å¢å¼ºç³»ç»Ÿå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“çš„è®¡ç®—æ•ˆç‡ï¼Œä¸ºå¼€å‘å¯ä¿¡ä¸”é«˜æ•ˆçš„è‡ªä¸»æ¨èç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02668v1",
      "published_date": "2025-10-03 01:52:37 UTC",
      "updated_date": "2025-10-03 01:52:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:27.256222+00:00"
    },
    {
      "arxiv_id": "2510.03361v1",
      "title": "Provenance Networks: End-to-End Exemplar-Based Explainability",
      "title_zh": "Provenance Networksï¼šç«¯åˆ°ç«¯åŸºäºèŒƒä¾‹çš„å¯è§£é‡Šæ€§",
      "authors": [
        "Ali Kayyam",
        "Anusha Madan Gopal",
        "M. Anthony Lewis"
      ],
      "abstract": "We introduce provenance networks, a novel class of neural models designed to provide end-to-end, training-data-driven explainability. Unlike conventional post-hoc methods, provenance networks learn to link each prediction directly to its supporting training examples as part of the model's normal operation, embedding interpretability into the architecture itself. Conceptually, the model operates similarly to a learned KNN, where each output is justified by concrete exemplars weighted by relevance in the feature space. This approach facilitates systematic investigations of the trade-off between memorization and generalization, enables verification of whether a given input was included in the training set, aids in the detection of mislabeled or anomalous data points, enhances resilience to input perturbations, and supports the identification of similar inputs contributing to the generation of a new data point. By jointly optimizing the primary task and the explainability objective, provenance networks offer insights into model behavior that traditional deep networks cannot provide. While the model introduces additional computational cost and currently scales to moderately sized datasets, it provides a complementary approach to existing explainability techniques. In particular, it addresses critical challenges in modern deep learning, including model opaqueness, hallucination, and the assignment of credit to data contributors, thereby improving transparency, robustness, and trustworthiness in neural models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Provenance Networksï¼Œè¿™æ˜¯ä¸€ç±»æ—¨åœ¨æä¾›ç«¯åˆ°ç«¯ã€ç”±è®­ç»ƒæ•°æ®é©±åŠ¨çš„å¯è§£é‡Šæ€§çš„æ–°å‹ç¥ç»æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„äº‹åè§£é‡Šæ–¹æ³• (post-hoc methods) ä¸åŒï¼Œè¯¥æ¨¡å‹é€šè¿‡å°†å¯è§£é‡Šæ€§åµŒå…¥æ¶æ„æœ¬èº«ï¼Œå­¦ä¹ å°†æ¯ä¸ªé¢„æµ‹ç›´æ¥é“¾æ¥åˆ°æ”¯æŒå®ƒçš„è®­ç»ƒæ ·æœ¬ (exemplars)ï¼Œå…¶è¿è¡Œé€»è¾‘ç±»ä¼¼äºå­¦ä¹ å‹ KNNã€‚è¿™ç§æ–¹æ³•æ”¯æŒç³»ç»Ÿåœ°ç ”ç©¶è®°å¿† (memorization) ä¸æ³›åŒ– (generalization) ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶èƒ½ç”¨äºéªŒè¯è®­ç»ƒé›†åŒ…å«æƒ…å†µã€æ£€æµ‹å¼‚å¸¸æ•°æ®ç‚¹ä»¥åŠå¢å¼ºå¯¹è¾“å…¥æ‰°åŠ¨çš„é²æ£’æ€§ã€‚é€šè¿‡å…±åŒä¼˜åŒ–ä¸»è¦ä»»åŠ¡å’Œå¯è§£é‡Šæ€§ç›®æ ‡ï¼ŒProvenance Networks èƒ½å¤Ÿæä¾›ä¼ ç»Ÿæ·±åº¦ç½‘ç»œæ— æ³•æä¾›çš„æ¨¡å‹è¡Œä¸ºæ´å¯Ÿã€‚è™½ç„¶è¯¥æ¨¡å‹ç›®å‰åœ¨è®¡ç®—æˆæœ¬å’Œå¤§è§„æ¨¡æ•°æ®é›†æ‰©å±•ä¸Šå­˜åœ¨é™åˆ¶ï¼Œä½†å®ƒæœ‰æ•ˆè§£å†³äº†æ·±åº¦å­¦ä¹ ä¸­çš„ä¸é€æ˜æ€§ã€å¹»è§‰ (hallucination) å’Œæ•°æ®å½’å› ç­‰å…³é”®é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç¥ç»ç½‘ç»œçš„é€æ˜åº¦ã€ç¨³å¥æ€§å’Œå¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03361v1",
      "published_date": "2025-10-03 01:48:38 UTC",
      "updated_date": "2025-10-03 01:48:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:27.504716+00:00"
    },
    {
      "arxiv_id": "2510.02663v1",
      "title": "TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models",
      "title_zh": "TutorBenchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ•™å­¦è¾…å¯¼èƒ½åŠ›çš„åŸºå‡†",
      "authors": [
        "Rakshith S Srinivasa",
        "Zora Che",
        "Chen Bo Calvin Zhang",
        "Diego Mares",
        "Ernesto Hernandez",
        "Jayeon Park",
        "Dean Lee",
        "Guillermo Mangialardi",
        "Charmaine Ng",
        "Ed-Yeremai Hernandez Cardona",
        "Anisha Gunjal",
        "Yunzhong He",
        "Bing Liu",
        "Chen Xing"
      ],
      "abstract": "As students increasingly adopt large language models (LLMs) as learning aids, it is crucial to build models that are adept at handling the nuances of tutoring: they need to identify the core needs of students, be adaptive, provide personalized guidance, and be accurate. To this end, we introduce TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated by human experts, focused on high-school and AP-level curricula. The samples are drawn from three common tutoring tasks: (i) generating adaptive explanations tailored to a student's confusion, (ii) providing actionable feedback on a student's work, and (iii) promoting active learning through effective hint generation. To account for the inherent complexity of tutoring, samples are accompanied by sample-specific rubrics which are used to judge model responses during evaluation. TutorBench uses a reliable and fine-grained automatic evaluation method that uses an LLM-judge and the sample-specific rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed analysis of their performance and behavior. Our results show that none of the frontier LLMs achieve a score of greater than $56\\%$, showing a large room for improvement. We find that LLMs fall short in exhibiting the full range of tutoring skills needed to guide, diagnose, and support students effectively, with all the frontier models achieving less than a $60\\%$ pass rate on rubric criteria related to these skills. We also find that different model families exhibit varied strengths and limitations: the Claude models outperform others in supporting active learning, while they lag behind in the other two use cases. By releasing TutorBench, we provide a comprehensive and unsaturated benchmark to guide the development of the next-generation of AI tutors.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† TutorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸¥è°¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ ¸å¿ƒæ•™å­¦èƒ½åŠ›çš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ï¼Œé‡ç‚¹å…³æ³¨è¯†åˆ«å­¦ç”Ÿéœ€æ±‚ã€è‡ªé€‚åº”è°ƒæ•´ã€ä¸ªæ€§åŒ–æŒ‡å¯¼åŠå‡†ç¡®æ€§ã€‚æ•°æ®é›†åŒ…å« 1,490 ä¸ªç”±ä¸“å®¶ç­–åˆ’çš„é«˜ä¸­åŠ AP çº§åˆ«æ ·æœ¬ï¼Œæ¶µç›–äº†ç”Ÿæˆè‡ªé€‚åº”è§£é‡Šï¼ˆGenerating adaptive explanationsï¼‰ã€æä¾›åé¦ˆå’Œä¿ƒè¿›ä¸»åŠ¨å­¦ä¹ ï¼ˆActive learningï¼‰ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚è¯„ä¼°è¿‡ç¨‹é‡‡ç”¨äº†ç»“åˆ LLM-judge ä¸æ ·æœ¬ç‰¹å®šè¯„åˆ†é‡è§„ï¼ˆSample-specific rubricsï¼‰çš„ç»†ç²’åº¦è‡ªåŠ¨è¯„ä»·æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ 16 ä¸ªå—æµ‹çš„å‰æ²¿ LLMs ä¸­ï¼Œæœ€é«˜å¾—åˆ†æœªè¶…è¿‡ 56%ï¼Œè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¼•å¯¼ã€è¯Šæ–­å’Œæ”¯æŒå­¦ç”Ÿæ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çŸ­æ¿ã€‚ä¸åŒæ¨¡å‹ç³»åˆ—å±•ç°å‡ºå·®å¼‚åŒ–çš„èƒ½åŠ›ï¼Œå¦‚ Claude æ¨¡å‹åœ¨ä¸»åŠ¨å­¦ä¹ æ–¹é¢è¡¨ç°çªå‡ºï¼Œä½†åœ¨å…¶ä»–é¢†åŸŸåˆ™è¡¨ç°ç¨é€Šã€‚TutorBench ä¸ºä¸‹ä¸€ä»£ AI tutors çš„ç ”å‘æä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”æœªé¥±å’Œçš„åŸºå‡†ï¼ŒæŒ‡æ˜äº†æœªæ¥æå‡æ•™å­¦äº’åŠ¨è´¨é‡çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02663v1",
      "published_date": "2025-10-03 01:41:09 UTC",
      "updated_date": "2025-10-03 01:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:30.548867+00:00"
    },
    {
      "arxiv_id": "2510.02660v1",
      "title": "When Researchers Say Mental Model/Theory of Mind of AI, What Are They Really Talking About?",
      "title_zh": "å½“ç ”ç©¶è€…è°ˆè®º AI çš„å¿ƒç†æ¨¡å‹ä¸å¿ƒæ™ºç†è®ºæ—¶ï¼Œå…¶çœŸå®æ„æ¶µç©¶ç«Ÿä¸ºä½•ï¼Ÿ",
      "authors": [
        "Xiaoyun Yin",
        "Elmira Zahmat Doost",
        "Shiwen Zhou",
        "Garima Arya Yadav",
        "Jamie C. Gorman"
      ],
      "abstract": "When researchers claim AI systems possess ToM or mental models, they are fundamentally discussing behavioral predictions and bias corrections rather than genuine mental states. This position paper argues that the current discourse conflates sophisticated pattern matching with authentic cognition, missing a crucial distinction between simulation and experience. While recent studies show LLMs achieving human-level performance on ToM laboratory tasks, these results are based only on behavioral mimicry. More importantly, the entire testing paradigm may be flawed in applying individual human cognitive tests to AI systems, but assessing human cognition directly in the moment of human-AI interaction. I suggest shifting focus toward mutual ToM frameworks that acknowledge the simultaneous contributions of human cognition and AI algorithms, emphasizing the interaction dynamics, instead of testing AI in isolation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å½“ç ”ç©¶äººå‘˜è°ˆè®º AI çš„ Mental Model æˆ– Theory of Mind (ToM) æ—¶ï¼Œå…¶å®è´¨æ˜¯åœ¨è®¨è®ºè¡Œä¸ºé¢„æµ‹å’Œåå·®ä¿®æ­£ï¼Œè€ŒéçœŸæ­£çš„å¿ƒç†çŠ¶æ€ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå½“å‰çš„å­¦æœ¯è®¨è®ºæ··æ·†äº†å¤æ‚çš„æ¨¡å¼åŒ¹é…ä¸çœŸå®çš„è®¤çŸ¥ï¼Œå¿½ç•¥äº†æ¨¡æ‹Ÿ (Simulation) ä¸ä½“éªŒ (Experience) ä¹‹é—´çš„å…³é”®åŒºåˆ«ã€‚å°½ç®¡ LLMs åœ¨ ToM å®éªŒå®¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†è¿™ä»…æ˜¯åŸºäºè¡Œä¸ºæ¨¡ä»¿ï¼Œä¸”å°†äººç±»ä¸ªä½“è®¤çŸ¥æµ‹è¯•ç›´æ¥åº”ç”¨äº AI ç³»ç»Ÿçš„è¯„ä¼°èŒƒå¼æœ¬èº«å¯èƒ½å­˜åœ¨ç¼ºé™·ã€‚ä½œè€…å»ºè®®å°†ç ”ç©¶é‡å¿ƒè½¬å‘ Mutual ToM æ¡†æ¶ï¼Œæ‰¿è®¤äººç±»è®¤çŸ¥ä¸ AI ç®—æ³•åœ¨äº¤äº’è¿‡ç¨‹ä¸­çš„å…±åŒä½œç”¨ã€‚è¯¥è®ºæ–‡ä¸»å¼ åº”å…³æ³¨äººæœºäº¤äº’çš„åŠ¨æ€è¿‡ç¨‹ï¼Œè€Œéå­¤ç«‹åœ°æµ‹è¯• AIï¼Œä»è€Œä¸ºç†è§£ AI ç³»ç»Ÿçš„èƒ½åŠ›ç•Œé™æä¾›äº†æ›´ä¸¥è°¨çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02660v1",
      "published_date": "2025-10-03 01:37:32 UTC",
      "updated_date": "2025-10-03 01:37:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:28.144670+00:00"
    },
    {
      "arxiv_id": "2511.11579v1",
      "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers",
      "title_zh": "Transformer ä¸­ä½ç½®ä¸ç¬¦å·æ³¨æ„åŠ›è¡Œä¸ºçš„è§£è€¦",
      "authors": [
        "Felipe Urrutia",
        "Jorge Salas",
        "Alexander Kozachinskiy",
        "Cristian Buc Calderon",
        "Hector Pasten",
        "Cristobal Rojas"
      ],
      "abstract": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Transformers ä¸­ä½ç½®ä¸ç¬¦å·ä¿¡æ¯çš„ç‹¬ç«‹ç¼–ç èƒ½åŠ›ï¼Œé‡ç‚¹åˆ†æäº† Rotary Positional Encodings (RoPE) çš„ä½œç”¨æœºåˆ¶ã€‚ç ”ç©¶è€…é€šè¿‡ç†è®ºä¸å®è¯åˆ†æï¼Œæ˜ç¡®å®šä¹‰äº†æ³¨æ„åŠ›å¤´è¡¨ç°ä¸ºä½ç½®æ€§ï¼ˆpositionalï¼‰æˆ–ç¬¦å·æ€§ï¼ˆsymbolicï¼‰è¡Œä¸ºçš„æ ‡å‡†ï¼Œå¹¶è¯æ˜äº†è¿™ä¸¤ç§è¡Œä¸ºå…·æœ‰äº’æ–¥æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§é‡åŒ–æŒ‡æ ‡ï¼Œå‘ç°åœ¨ä½¿ç”¨ RoPE çš„ LLMs ä¸­ï¼Œæ³¨æ„åŠ›å¤´çš„è¡Œä¸ºä¸é¢‘ç‡ä½¿ç”¨ä¹‹é—´å­˜åœ¨å¼ºå…³è”ã€‚é€šè¿‡è®¾è®¡çº¯ç²¹çš„ä½ç½®æˆ–ç¬¦å·ä»»åŠ¡ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹æ€§èƒ½ä¸æ³¨æ„åŠ›å¤´åˆ©ç”¨ç‰¹å®šé¢‘ç‡èƒ½åŠ›çš„å› æœå…³ç³»ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œé€šè¿‡æ“çºµæ³¨æ„åŠ›å¤´è®¿é—®çš„é¢‘ç‡å¯ä»¥æœ‰æ•ˆæ§åˆ¶ Transformers çš„è¡¨ç°ã€‚è¯¥å·¥ä½œæ·±åŒ–äº†å¯¹ RoPE å±æ€§çš„ç†è§£ï¼Œå¹¶é˜æ˜äº†å…¶å¦‚ä½•é€šè¿‡é¢‘ç‡åˆ†å¸ƒå½±å“æ¨¡å‹æ•´ä½“è¡Œä¸ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 12 figures, repository available",
      "pdf_url": "https://arxiv.org/pdf/2511.11579v1",
      "published_date": "2025-10-03 01:31:15 UTC",
      "updated_date": "2025-10-03 01:31:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:34.154828+00:00"
    },
    {
      "arxiv_id": "2510.02655v1",
      "title": "A Concept of Possibility for Real-World Events",
      "title_zh": "é¢å‘ç°å®ä¸–ç•Œäº‹ä»¶çš„å¯èƒ½æ€§æ¦‚å¿µ",
      "authors": [
        "Daniel G. Schwartz"
      ],
      "abstract": "This paper offers a new concept of {\\it possibility} as an alternative to the now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This new version was inspired by the original but, formally, has nothing in common with it other than that they both adopt the Åukasiewicz multivalent interpretation of the logical connectives. Moreover, rather than seeking to provide a general notion of possibility, this focuses specifically on the possibility of a real-world event. An event is viewed as having prerequisites that enable its occurrence and constraints that may impede its occurrence, and the possibility of the event is computed as a function of the probabilities that the prerequisites hold and the constraints do not. This version of possibility might appropriately be applied to problems of planning. When there are multiple plans available for achieving a goal, this theory can be used to determine which plan is most possible, i.e., easiest or most feasible to complete. It is speculated that this model of reasoning correctly captures normal human reasoning about plans. The theory is elaborated and an illustrative example for vehicle route planning is provided. There is also a suggestion of potential future applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å…³äºç°å®ä¸–ç•Œäº‹ä»¶ï¼ˆreal-world eventsï¼‰å¯èƒ½æ€§ï¼ˆpossibilityï¼‰çš„æ–°æ¦‚å¿µï¼Œæ—¨åœ¨ä½œä¸ºL.A. Zadehåœ¨1978å¹´æå‡ºçš„æ ‡å‡†å¯èƒ½æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚è™½ç„¶æ–°æ¦‚å¿µå—åŸç†è®ºå¯å‘å¹¶åŒæ ·é‡‡ç”¨Åukasiewiczå¤šå€¼é€»è¾‘è§£é‡Šï¼Œä½†å…¶åœ¨å½¢å¼ä¸Šå…·æœ‰ç‹¬ç‰¹æ€§ï¼Œä¸“æ³¨äºåˆ†æå…·ä½“äº‹ä»¶çš„å‘ç”Ÿæ¦‚ç‡ã€‚è¯¥æ¡†æ¶å°†äº‹ä»¶è§†ä¸ºç”±ä¿ƒè¿›å…¶å‘ç”Ÿçš„å…ˆå†³æ¡ä»¶ï¼ˆprerequisitesï¼‰å’Œé˜»ç¢å…¶å‘ç”Ÿçš„çº¦æŸæ¡ä»¶ï¼ˆconstraintsï¼‰å…±åŒå†³å®šçš„è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡è®¡ç®—å…ˆå†³æ¡ä»¶æˆç«‹ä¸çº¦æŸæ¡ä»¶ä¸æˆç«‹çš„æ¦‚ç‡æ¥è¯„ä¼°å¯èƒ½æ€§ã€‚è¯¥ç†è®ºç‰¹åˆ«é€‚ç”¨äºè§„åˆ’ï¼ˆplanningï¼‰é¢†åŸŸï¼Œèƒ½æœ‰æ•ˆå¸®åŠ©å†³ç­–è€…åœ¨å¤šä¸ªå¤‡é€‰æ–¹æ¡ˆä¸­è¯†åˆ«æœ€å®¹æ˜“å®ç°æˆ–æœ€å…·å¯è¡Œæ€§çš„è®¡åˆ’ã€‚ç ”ç©¶è®¤ä¸ºè¿™ä¸€æ¨ç†æ¨¡å‹æ•æ‰äº†äººç±»å¯¹è®¡åˆ’çš„æ­£å¸¸æ€ç»´é€»è¾‘ï¼Œå¹¶é€šè¿‡è½¦è¾†è·¯å¾„è§„åˆ’ï¼ˆvehicle route planningï¼‰çš„å®ä¾‹è¯æ˜äº†å…¶åœ¨è§£å†³å®é™…é—®é¢˜ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02655v1",
      "published_date": "2025-10-03 01:15:06 UTC",
      "updated_date": "2025-10-03 01:15:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:38.662775+00:00"
    },
    {
      "arxiv_id": "2510.02653v1",
      "title": "Geolog-IA: Conversational System for Academic Theses",
      "title_zh": "Geolog-IAï¼šé¢å‘å­¦æœ¯è®ºæ–‡çš„å¯¹è¯å¼ç³»ç»Ÿ",
      "authors": [
        "Micaela Fuel Pozo",
        "Andrea Guatumillo Saltos",
        "YeseÃ±a Tipan Llumiquinga",
        "Kelly Lascano Aguirre",
        "Marilyn Castillo Jara",
        "Christian Mejia-Escobar"
      ],
      "abstract": "This study presents the development of Geolog-IA, a novel conversational system based on artificial intelligence that responds naturally to questions about geology theses from the Central University of Ecuador. Our proposal uses the Llama 3.1 and Gemini 2.5 language models, which are complemented by a Retrieval Augmented Generation (RAG) architecture and an SQLite database. This strategy allows us to overcome problems such as hallucinations and outdated knowledge. The evaluation of Geolog-IA's performance with the BLEU metric reaches an average of 0.87, indicating high consistency and accuracy in the responses generated. The system offers an intuitive, web-based interface that facilitates interaction and information retrieval for directors, teachers, students, and administrative staff at the institution. This tool can be a key support in education, training, and research and establishes a basis for future applications in other disciplines.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸ºGeolog-IAçš„åˆ›æ–°å‹äººå·¥æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºè§£ç­”å…³äºå„ç“œå¤šå°”ä¸­å¤®å¤§å­¦åœ°è´¨å­¦è®ºæ–‡çš„è‡ªç„¶è¯­è¨€å’¨è¯¢ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†Llama 3.1å’ŒGemini 2.5è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval Augmented Generation, RAG)æ¶æ„ä¸SQLiteæ•°æ®åº“ï¼Œæœ‰æ•ˆå…‹æœäº†äººå·¥æ™ºèƒ½åœ¨å¤„ç†å­¦æœ¯æ–‡çŒ®æ—¶å¸¸è§çš„å¹»è§‰(hallucinations)å’ŒçŸ¥è¯†é™ˆæ—§é—®é¢˜ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒGeolog-IAåœ¨BLEUæŒ‡æ ‡ä¸Šå¹³å‡å¾—åˆ†è¾¾åˆ°0.87ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆå›ç­”çš„é«˜å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ã€‚ç³»ç»Ÿé€šè¿‡ç›´è§‚çš„Webç•Œé¢ä¸ºå¸ˆç”ŸåŠè¡Œæ”¿äººå‘˜æä¾›ä¾¿æ·çš„ä¿¡æ¯æ£€ç´¢æœåŠ¡ï¼Œæ˜¾è‘—æå‡äº†å­¦æœ¯èµ„æºçš„åˆ©ç”¨æ•ˆç‡ã€‚è¯¥å·¥å…·ä¸ä»…åœ¨æ•™è‚²ã€åŸ¹è®­å’Œç§‘ç ”é¢†åŸŸæä¾›äº†å…³é”®æ”¯æŒï¼Œä¹Ÿä¸ºå…¶ä»–å­¦ç§‘é¢†åŸŸçš„æ™ºèƒ½åŒ–è½¬å‹å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, in Spanish language",
      "pdf_url": "https://arxiv.org/pdf/2510.02653v1",
      "published_date": "2025-10-03 01:11:47 UTC",
      "updated_date": "2025-10-03 01:11:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:49.154977+00:00"
    },
    {
      "arxiv_id": "2510.02634v1",
      "title": "Automatic Building Code Review: A Case Study",
      "title_zh": "è‡ªåŠ¨åŒ–å»ºç­‘è§„èŒƒå®¡æŸ¥ï¼šæ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Hanlong Wan",
        "Weili Xu",
        "Michael Rosenberg",
        "Jian Zhang",
        "Aysha Siddika"
      ],
      "abstract": "Building officials, particularly those in resource-constrained or rural jurisdictions, face labor-intensive, error-prone, and costly manual reviews of design documents as projects increase in size and complexity. The growing adoption of Building Information Modeling (BIM) and Large Language Models (LLMs) presents opportunities for automated code review (ACR) solutions. This study introduces a novel agent-driven framework that integrates BIM-based data extraction with automated verification using both retrieval-augmented generation (RAG) and Model Context Protocol (MCP) agent pipelines. The framework employs LLM-enabled agents to extract geometry, schedules, and system attributes from heterogeneous file types, which are then processed for building code checking through two complementary mechanisms: (1) direct API calls to the US Department of Energy COMcheck engine, providing deterministic and audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling flexible interpretation where coverage is incomplete or ambiguous.\n  The framework was evaluated through case demonstrations, including automated extraction of geometric attributes (such as surface area, tilt, and insulation values), parsing of operational schedules, and validation of lighting allowances under ASHRAE Standard 90.1-2022. Comparative performance tests across multiple LLMs showed that GPT-4o achieved the best balance of efficiency and stability, while smaller models exhibited inconsistencies or failures. Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in rigor and reliability. This work advances ACR research by demonstrating a scalable, interoperable, and production-ready approach that bridges BIM with authoritative code review tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å»ºç­‘è§„èŒƒå®¡æŸ¥ä¸­äººå·¥æ“ä½œç¹çã€æ˜“å‡ºé”™ä¸”æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ™ºèƒ½ä½“é©±åŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è‡ªåŠ¨å»ºç­‘è§„èŒƒå®¡æŸ¥(ACR)ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆå»ºç­‘ä¿¡æ¯æ¨¡å‹(BIM)æ•°æ®æå–ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®(MCP)æµæ°´çº¿å®ç°è‡ªåŠ¨åŒ–éªŒè¯ã€‚ç ”ç©¶åˆ©ç”¨æ™ºèƒ½ä½“ä»å„ç±»å¼‚æ„æ–‡ä»¶ä¸­æå–å‡ ä½•ã€è¿›åº¦å’Œç³»ç»Ÿå±æ€§ï¼Œå¹¶ç»“åˆCOMcheckå¼•æ“çš„ç¡®å®šæ€§APIè°ƒç”¨ä¸RAGçš„æŸ”æ€§æ¨ç†è¿›è¡ŒåŒé‡æ ¡éªŒã€‚åœ¨é’ˆå¯¹ASHRAE Standard 90.1-2022çš„æ¡ˆä¾‹è¯„ä¼°ä¸­ï¼ŒGPT-4oåœ¨æ€§èƒ½æµ‹è¯•ä¸­è¡¨ç°å‡ºæœ€ä½³çš„æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚ç»“æœè¯å®ï¼ŒMCPæ™ºèƒ½ä½“æµæ°´çº¿åœ¨ä¸¥è°¨æ€§å’Œå¯é æ€§ä¸Šæ˜¾è‘—ä¼˜äºRAGæ¨ç†æµæ°´çº¿ã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€ç§å¯æ‰©å±•ä¸”å…·å¤‡ç”Ÿäº§å°±ç»ªæ€§çš„æ–¹æ³•ï¼ŒæˆåŠŸæ¶èµ·äº†BIMä¸æƒå¨è§„èŒƒå®¡æŸ¥å·¥å…·ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºACRé¢†åŸŸçš„è‡ªåŠ¨åŒ–å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02634v1",
      "published_date": "2025-10-03 00:30:14 UTC",
      "updated_date": "2025-10-03 00:30:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:54.151293+00:00"
    },
    {
      "arxiv_id": "2510.03360v1",
      "title": "Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows",
      "title_zh": "é¢å‘æ¹æµå‡é˜»çš„ç‰©ç†ä¿¡æ¯ç¥ç»ç®—å­é¢„æµ‹æ§åˆ¶",
      "authors": [
        "Zelin Zhao",
        "Zongyi Li",
        "Kimia Hassibi",
        "Kamyar Azizzadenesheli",
        "Junchi Yan",
        "H. Jane Bae",
        "Di Zhou",
        "Anima Anandkumar"
      ],
      "abstract": "Assessing turbulence control effects for wall friction numerically is a significant challenge since it requires expensive simulations of turbulent fluid dynamics. We instead propose an efficient deep reinforcement learning (RL) framework for modeling and control of turbulent flows. It is model-based RL for predictive control (PC), where both the policy and the observer models for turbulence control are learned jointly using Physics Informed Neural Operators (PINO), which are discretization invariant and can capture fine scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free reinforcement learning methods in various challenging scenarios where the flows are of high Reynolds numbers and unseen, i.e., not provided during model training. We find that PINO-PC achieves a drag reduction of 39.0\\% under a bulk-velocity Reynolds number of 15,000, outperforming previous fluid control methods by more than 32\\%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (RL)æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè§£å†³æ¹æµæ§åˆ¶ä¸­å£é¢æ‘©æ“¦çš„æ•°å€¼è¯„ä¼°ä¸å‡é˜»éš¾é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæ¨¡å‹çš„é¢„æµ‹æ§åˆ¶(Predictive Control, PC)ï¼Œåˆ©ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç®—å­(Physics Informed Neural Operators, PINO)å…±åŒå­¦ä¹ ç­–ç•¥æ¨¡å‹å’Œè§‚æµ‹æ¨¡å‹ã€‚PINO å…·å¤‡ç¦»æ•£ä¸å˜æ€§(discretization invariant)ï¼Œèƒ½å¤Ÿç²¾ç¡®æ•æ‰æ¹æµä¸­çš„å¾®ç»†å°ºåº¦ç»“æ„ã€‚å®éªŒè¯æ˜ï¼ŒPINO-PC åœ¨é«˜é›·è¯ºæ•°(Reynolds numbers)åŠè®­ç»ƒä¸­æœªå‡ºç°çš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹ï¼Œè¡¨ç°å‡ä¼˜äºä¼ ç»Ÿçš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚åœ¨ä½“å¹³å‡é›·è¯ºæ•°ä¸º15,000çš„æ¡ä»¶ä¸‹ï¼Œè¯¥æ–¹æ³•å®ç°äº†39.0%çš„å‡é˜»æ•ˆæœï¼Œç›¸è¾ƒäºä¹‹å‰çš„æµä½“æ§åˆ¶æ–¹æ³•æå‡äº†è¶…è¿‡32%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03360v1",
      "published_date": "2025-10-03 00:18:26 UTC",
      "updated_date": "2025-10-03 00:18:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:42:54.950377+00:00"
    },
    {
      "arxiv_id": "2510.02627v1",
      "title": "A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios",
      "title_zh": "é¢å‘é«˜å¯†åº¦äº¤é€šåŠå¤šæ ·åŒ–æ™ºèƒ½ä½“äº¤äº’åœºæ™¯çš„è½¨è¿¹ç”Ÿæˆå™¨",
      "authors": [
        "Ruining Yang",
        "Yi Xu",
        "Yixiao Chen",
        "Yun Fu",
        "Lili Su"
      ],
      "abstract": "Accurate trajectory prediction is fundamental to autonomous driving, as it underpins safe motion planning and collision avoidance in complex environments. However, existing benchmark datasets suffer from a pronounced long-tail distribution problem, with most samples drawn from low-density scenarios and simple straight-driving behaviors. This underrepresentation of high-density scenarios and safety critical maneuvers such as lane changes, overtaking and turning is an obstacle to model generalization and leads to overly optimistic evaluations. To address these challenges, we propose a novel trajectory generation framework that simultaneously enhances scenarios density and enriches behavioral diversity. Specifically, our approach converts continuous road environments into a structured grid representation that supports fine-grained path planning, explicit conflict detection, and multi-agent coordination. Built upon this representation, we introduce behavior-aware generation mechanisms that combine rule-based decision triggers with Frenet-based trajectory smoothing and dynamic feasibility constraints. This design allows us to synthesize realistic high-density scenarios and rare behaviors with complex interactions that are often missing in real data. Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets demonstrate that our method significantly improves both agent density and behavior diversity, while preserving motion realism and scenario-level safety. Our synthetic data also benefits downstream trajectory prediction models and enhances performance in challenging high-density scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹åŸºå‡†æ•°æ®é›†ä¸­å­˜åœ¨çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œå³ç¼ºä¹é«˜å¯†åº¦åœºæ™¯å’Œå®‰å…¨å…³é”®åŠ¨ä½œï¼ˆå¦‚æ¢é“ã€è¶…è½¦ã€è½¬å¼¯ï¼‰çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è½¨è¿¹ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è¿ç»­é“è·¯ç¯å¢ƒè½¬æ¢ä¸ºç»“æ„åŒ– grid representationï¼Œæœ‰æ•ˆæ”¯æŒäº†ç»†ç²’åº¦çš„è·¯å¾„è§„åˆ’ã€æ˜¾å¼å†²çªæ£€æµ‹å’Œ multi-agent coordinationã€‚é€šè¿‡ç»“åˆåŸºäºè§„åˆ™çš„å†³ç­–è§¦å‘å™¨ã€Frenet-based trajectory smoothing è½¨è¿¹å¹³æ»‘æŠ€æœ¯ä»¥åŠåŠ¨æ€å¯è¡Œæ€§çº¦æŸï¼Œè¯¥ç³»ç»Ÿèƒ½å¤ŸåˆæˆåŒ…å«å¤æ‚äº¤äº’çš„çœŸå®é«˜å¯†åº¦åœºæ™¯åŠç¨€æœ‰è¡Œä¸ºã€‚åœ¨ Argoverse 1 å’Œ Argoverse 2 å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡æ™ºèƒ½ä½“å¯†åº¦å’Œè¡Œä¸ºå¤šæ ·æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†è¿åŠ¨çœŸå®æ„Ÿä¸åœºæ™¯å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„åˆæˆæ•°æ®èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºä¸‹æ¸¸è½¨è¿¹é¢„æµ‹æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§é«˜å¯†åº¦åœºæ™¯ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02627v1",
      "published_date": "2025-10-03 00:12:18 UTC",
      "updated_date": "2025-10-03 00:12:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:43:00.060324+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 164,
  "processed_papers_count": 164,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T00:43:46.506624+00:00"
}
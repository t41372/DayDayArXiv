[
  {
    "arxiv_id": "2403.08818v1",
    "title": "Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM",
    "authors": [
      "Hejie Cui",
      "Xinyu Fang",
      "Ran Xu",
      "Xuan Kan",
      "Joyce C. Ho",
      "Carl Yang"
    ],
    "abstract": "Electronic Health Records (EHRs) have become increasingly popular to support\nclinical decision-making and healthcare in recent decades. EHRs usually contain\nheterogeneous information, such as structural data in tabular form and\nunstructured data in textual notes. Different types of information in EHRs can\ncomplement each other and provide a more complete picture of the health status\nof a patient. While there has been a lot of research on representation learning\nof structured EHR data, the fusion of different types of EHR data (multimodal\nfusion) is not well studied. This is mostly because of the complex medical\ncoding systems used and the noise and redundancy present in the written notes.\nIn this work, we propose a new framework called MINGLE, which integrates both\nstructures and semantics in EHR effectively. Our framework uses a two-level\ninfusion strategy to combine medical concept semantics and clinical note\nsemantics into hypergraph neural networks, which learn the complex interactions\nbetween different types of data to generate visit representations for\ndownstream prediction. Experiment results on two EHR datasets, the public\nMIMIC-III and private CRADLE, show that MINGLE can effectively improve\npredictive performance by 11.83% relatively, enhancing semantic integration as\nwell as multimodal fusion for structural and textual EHR data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.08818v1",
    "published_date": "2024-02-19 23:48:40 UTC",
    "updated_date": "2024-02-19 23:48:40 UTC"
  },
  {
    "arxiv_id": "2402.12598v1",
    "title": "Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations",
    "authors": [
      "Giovanni De Felice",
      "Andrea Cini",
      "Daniele Zambon",
      "Vladimir V. Gusev",
      "Cesare Alippi"
    ],
    "abstract": "Virtual sensing techniques allow for inferring signals at new unmonitored\nlocations by exploiting spatio-temporal measurements coming from physical\nsensors at different locations. However, as the sensor coverage becomes sparse\ndue to costs or other constraints, physical proximity cannot be used to support\ninterpolation. In this paper, we overcome this challenge by leveraging\ndependencies between the target variable and a set of correlated variables\n(covariates) that can frequently be associated with each location of interest.\nFrom this viewpoint, covariates provide partial observability, and the problem\nconsists of inferring values for unobserved channels by exploiting observations\nat other locations to learn how such variables can correlate. We introduce a\nnovel graph-based methodology to exploit such relationships and design a graph\ndeep learning architecture, named GgNet, implementing the framework. The\nproposed approach relies on propagating information over a nested graph\nstructure that is used to learn dependencies between variables as well as\nlocations. GgNet is extensively evaluated under different virtual sensing\nscenarios, demonstrating higher reconstruction accuracy compared to the\nstate-of-the-art.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12598v1",
    "published_date": "2024-02-19 23:22:30 UTC",
    "updated_date": "2024-02-19 23:22:30 UTC"
  },
  {
    "arxiv_id": "2403.14636v1",
    "title": "AI Fairness in Practice",
    "authors": [
      "David Leslie",
      "Cami Rincon",
      "Morgan Briggs",
      "Antonella Perini",
      "Smera Jayadeva",
      "Ann Borda",
      "SJ Bennett",
      "Christopher Burr",
      "Mhairi Aitken",
      "Michael Katell",
      "Claudia Fischer",
      "Janis Wong",
      "Ismael Kherroubi Garcia"
    ],
    "abstract": "Reaching consensus on a commonly accepted definition of AI Fairness has long\nbeen a central challenge in AI ethics and governance. There is a broad spectrum\nof views across society on what the concept of fairness means and how it should\nbest be put to practice. In this workbook, we tackle this challenge by\nexploring how a context-based and society-centred approach to understanding AI\nFairness can help project teams better identify, mitigate, and manage the many\nways that unfair bias and discrimination can crop up across the AI project\nworkflow.\n  We begin by exploring how, despite the plurality of understandings about the\nmeaning of fairness, priorities of equality and non-discrimination have come to\nconstitute the broadly accepted core of its application as a practical\nprinciple. We focus on how these priorities manifest in the form of equal\nprotection from direct and indirect discrimination and from discriminatory\nharassment. These elements form ethical and legal criteria based upon which\ninstances of unfair bias and discrimination can be identified and mitigated\nacross the AI project workflow.\n  We then take a deeper dive into how the different contexts of the AI project\nlifecycle give rise to different fairness concerns. This allows us to identify\nseveral types of AI Fairness (Data Fairness, Application Fairness, Model Design\nand Development Fairness, Metric-Based Fairness, System Implementation\nFairness, and Ecosystem Fairness) that form the basis of a multi-lens approach\nto bias identification, mitigation, and management. Building on this, we\ndiscuss how to put the principle of AI Fairness into practice across the AI\nproject workflow through Bias Self-Assessment and Bias Risk Management as well\nas through the documentation of metric-based fairness criteria in a Fairness\nPosition Statement.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14636v1",
    "published_date": "2024-02-19 23:02:56 UTC",
    "updated_date": "2024-02-19 23:02:56 UTC"
  },
  {
    "arxiv_id": "2403.15404v1",
    "title": "AI Sustainability in Practice Part Two: Sustainability Throughout the AI Workflow",
    "authors": [
      "David Leslie",
      "Cami Rincon",
      "Morgan Briggs",
      "Antonella Perini",
      "Smera Jayadeva",
      "Ann Borda",
      "SJ Bennett",
      "Christopher Burr",
      "Mhairi Aitken",
      "Michael Katell",
      "Claudia Fischer",
      "Janis Wong",
      "Ismael Kherroubi Garcia"
    ],
    "abstract": "The sustainability of AI systems depends on the capacity of project teams to\nproceed with a continuous sensitivity to their potential real-world impacts and\ntransformative effects. Stakeholder Impact Assessments (SIAs) are governance\nmechanisms that enable this kind of responsiveness. They are tools that create\na procedure for, and a means of documenting, the collaborative evaluation and\nreflective anticipation of the possible harms and benefits of AI innovation\nprojects. SIAs are not one-off governance actions. They require project teams\nto pay continuous attention to the dynamic and changing character of AI\nproduction and use and to the shifting conditions of the real-world\nenvironments in which AI technologies are embedded. This workbook is part two\nof two workbooks on AI Sustainability. It provides a template of the SIA and\nactivities that allow a deeper dive into crucial parts of it. It discusses\nmethods for weighing values and considering trade-offs during the SIA. And, it\nhighlights the need to treat the SIA as an end-to-end process of responsive\nevaluation and re-assessment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15404v1",
    "published_date": "2024-02-19 22:58:05 UTC",
    "updated_date": "2024-02-19 22:58:05 UTC"
  },
  {
    "arxiv_id": "2403.14635v1",
    "title": "AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects",
    "authors": [
      "David Leslie",
      "Cami Rincon",
      "Morgan Briggs",
      "Antonella Perini",
      "Smera Jayadeva",
      "Ann Borda",
      "SJ Bennett",
      "Christopher Burr",
      "Mhairi Aitken",
      "Michael Katell",
      "Claudia Fischer",
      "Janis Wong",
      "Ismael Kherroubi Garcia"
    ],
    "abstract": "Sustainable AI projects are continuously responsive to the transformative\neffects as well as short-, medium-, and long-term impacts on individuals and\nsociety that the design, development, and deployment of AI technologies may\nhave. Projects, which centre AI Sustainability, ensure that values-led,\ncollaborative, and anticipatory reflection both guides the assessment of\npotential social and ethical impacts and steers responsible innovation\npractices.\n  This workbook is the first part of a pair that provides the concepts and\ntools needed to put AI Sustainability into practice. It introduces the SUM\nValues, which help AI project teams to assess the potential societal impacts\nand ethical permissibility of their projects. It then presents a Stakeholder\nEngagement Process (SEP), which provides tools to facilitate proportionate\nengagement of and input from stakeholders with an emphasis on equitable and\nmeaningful participation and positionality awareness.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14635v1",
    "published_date": "2024-02-19 22:52:14 UTC",
    "updated_date": "2024-02-19 22:52:14 UTC"
  },
  {
    "arxiv_id": "2403.15403v1",
    "title": "AI Ethics and Governance in Practice: An Introduction",
    "authors": [
      "David Leslie",
      "Cami Rincon",
      "Morgan Briggs",
      "Antonella Perini",
      "Smera Jayadeva",
      "Ann Borda",
      "SJ Bennett",
      "Christopher Burr",
      "Mhairi Aitken",
      "Michael Katell",
      "Claudia Fischer"
    ],
    "abstract": "AI systems may have transformative and long-term effects on individuals and\nsociety. To manage these impacts responsibly and direct the development of AI\nsystems toward optimal public benefit, considerations of AI ethics and\ngovernance must be a first priority.\n  In this workbook, we introduce and describe our PBG Framework, a multi-tiered\ngovernance model that enables project teams to integrate ethical values and\npractical principles into their innovation practices and to have clear\nmechanisms for demonstrating and documenting this.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15403v1",
    "published_date": "2024-02-19 22:43:19 UTC",
    "updated_date": "2024-02-19 22:43:19 UTC"
  },
  {
    "arxiv_id": "2402.12572v2",
    "title": "FairProof : Confidential and Certifiable Fairness for Neural Networks",
    "authors": [
      "Chhavi Yadav",
      "Amrita Roy Chowdhury",
      "Dan Boneh",
      "Kamalika Chaudhuri"
    ],
    "abstract": "Machine learning models are increasingly used in societal applications, yet\nlegal and privacy concerns demand that they very often be kept confidential.\nConsequently, there is a growing distrust about the fairness properties of\nthese models in the minds of consumers, who are often at the receiving end of\nmodel predictions. To this end, we propose \\name -- a system that uses\nZero-Knowledge Proofs (a cryptographic primitive) to publicly verify the\nfairness of a model, while maintaining confidentiality. We also propose a\nfairness certification algorithm for fully-connected neural networks which is\nbefitting to ZKPs and is used in this system. We implement \\name in Gnark and\ndemonstrate empirically that our system is practically feasible. Code is\navailable at https://github.com/infinite-pursuits/FairProof.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12572v2",
    "published_date": "2024-02-19 21:53:43 UTC",
    "updated_date": "2024-07-16 00:56:20 UTC"
  },
  {
    "arxiv_id": "2402.12570v1",
    "title": "Offline Multi-task Transfer RL with Representational Penalization",
    "authors": [
      "Avinandan Bose",
      "Simon Shaolei Du",
      "Maryam Fazel"
    ],
    "abstract": "We study the problem of representation transfer in offline Reinforcement\nLearning (RL), where a learner has access to episodic data from a number of\nsource tasks collected a priori, and aims to learn a shared representation to\nbe used in finding a good policy for a target task. Unlike in online RL where\nthe agent interacts with the environment while learning a policy, in the\noffline setting there cannot be such interactions in either the source tasks or\nthe target task; thus multi-task offline RL can suffer from incomplete\ncoverage.\n  We propose an algorithm to compute pointwise uncertainty measures for the\nlearnt representation, and establish a data-dependent upper bound for the\nsuboptimality of the learnt policy for the target task. Our algorithm leverages\nthe collective exploration done by source tasks to mitigate poor coverage at\nsome points by a few tasks, thus overcoming the limitation of needing uniformly\ngood coverage for a meaningful transfer by existing offline algorithms. We\ncomplement our theoretical results with empirical evaluation on a\nrich-observation MDP which requires many samples for complete coverage. Our\nfindings illustrate the benefits of penalizing and quantifying the uncertainty\nin the learnt representation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12570v1",
    "published_date": "2024-02-19 21:52:44 UTC",
    "updated_date": "2024-02-19 21:52:44 UTC"
  },
  {
    "arxiv_id": "2402.12563v3",
    "title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models",
    "authors": [
      "Loka Li",
      "Zhenhao Chen",
      "Guangyi Chen",
      "Yixuan Zhang",
      "Yusheng Su",
      "Eric Xing",
      "Kun Zhang"
    ],
    "abstract": "The recent success of Large Language Models (LLMs) has catalyzed an\nincreasing interest in their self-correction capabilities. This paper presents\na comprehensive investigation into the intrinsic self-correction of LLMs,\nattempting to address the ongoing debate about its feasibility. Our research\nhas identified an important latent factor - the \"confidence\" of LLMs - during\nthe self-correction process. Overlooking this factor may cause the models to\nover-criticize themselves, resulting in unreliable conclusions regarding the\nefficacy of self-correction. We have experimentally observed that LLMs possess\nthe capability to understand the \"confidence\" in their own responses. It\nmotivates us to develop an \"If-or-Else\" (IoE) prompting framework, designed to\nguide LLMs in assessing their own \"confidence\", facilitating intrinsic\nself-corrections. We conduct extensive experiments and demonstrate that our\nIoE-based Prompt can achieve a consistent improvement regarding the accuracy of\nself-corrected responses over the initial answers. Our study not only sheds\nlight on the underlying factors affecting self-correction in LLMs, but also\nintroduces a practical framework that utilizes the IoE prompting principle to\nefficiently improve self-correction capabilities with \"confidence\". The code is\navailable at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.12563v3",
    "published_date": "2024-02-19 21:38:02 UTC",
    "updated_date": "2024-05-13 11:01:17 UTC"
  },
  {
    "arxiv_id": "2402.12560v1",
    "title": "CausalGym: Benchmarking causal interpretability methods on linguistic tasks",
    "authors": [
      "Aryaman Arora",
      "Dan Jurafsky",
      "Christopher Potts"
    ],
    "abstract": "Language models (LMs) have proven to be powerful tools for psycholinguistic\nresearch, but most prior work has focused on purely behavioural measures (e.g.,\nsurprisal comparisons). At the same time, research in model interpretability\nhas begun to illuminate the abstract causal mechanisms shaping LM behavior. To\nhelp bring these strands of research closer together, we introduce CausalGym.\nWe adapt and expand the SyntaxGym suite of tasks to benchmark the ability of\ninterpretability methods to causally affect model behaviour. To illustrate how\nCausalGym can be used, we study the pythia models (14M--6.9B) and assess the\ncausal efficacy of a wide range of interpretability methods, including linear\nprobing and distributed alignment search (DAS). We find that DAS outperforms\nthe other methods, and so we use it to study the learning trajectory of two\ndifficult linguistic phenomena in pythia-1b: negative polarity item licensing\nand filler--gap dependencies. Our analysis shows that the mechanism\nimplementing both of these tasks is learned in discrete stages, not gradually.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages main text, 26 pages total",
    "pdf_url": "http://arxiv.org/pdf/2402.12560v1",
    "published_date": "2024-02-19 21:35:56 UTC",
    "updated_date": "2024-02-19 21:35:56 UTC"
  },
  {
    "arxiv_id": "2402.12551v1",
    "title": "Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment",
    "authors": [
      "Ganesh Sapkota",
      "Sanjay Madria"
    ],
    "abstract": "Localization in a battlefield environment is increasingly challenging as GPS\nconnectivity is often denied or unreliable, and physical deployment of anchor\nnodes across wireless networks for localization can be difficult in hostile\nbattlefield terrain. Existing range-free localization methods rely on\nradio-based anchors and their average hop distance which suffers from accuracy\nand stability in dynamic and sparse wireless network topology. Vision-based\nmethods like SLAM and Visual Odometry use expensive sensor fusion techniques\nfor map generation and pose estimation. This paper proposes a novel framework\nfor localization in non-GPS battlefield environments using only the passive\ncamera sensors and considering naturally existing or artificial landmarks as\nanchors. The proposed method utilizes a customcalibrated stereo vision camera\nfor distance estimation and the YOLOv8s model, which is trained and fine-tuned\nwith our real-world dataset for landmark recognition. The depth images are\ngenerated using an efficient stereomatching algorithm, and distances to\nlandmarks are determined by extracting the landmark depth feature utilizing a\nbounding box predicted by the landmark recognition model. The position of the\nunknown node is then obtained using the efficient least square algorithm and\nthen optimized using the L-BFGS-B (limited-memory quasi-Newton code for\nbound-constrained optimization) method. Experimental results demonstrate that\nour proposed framework performs better than existing anchorbased DV-Hop\nalgorithms and competes with the most efficient vision-based algorithms in\nterms of localization error (RMSE).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2402.12320",
    "pdf_url": "http://arxiv.org/pdf/2402.12551v1",
    "published_date": "2024-02-19 21:20:56 UTC",
    "updated_date": "2024-02-19 21:20:56 UTC"
  },
  {
    "arxiv_id": "2402.12530v1",
    "title": "Parallel Structures in Pre-training Data Yield In-Context Learning",
    "authors": [
      "Yanda Chen",
      "Chen Zhao",
      "Zhou Yu",
      "Kathleen McKeown",
      "He He"
    ],
    "abstract": "Pre-trained language models (LMs) are capable of in-context learning (ICL):\nthey can adapt to a task with only a few examples given in the prompt without\nany parameter update. However, it is unclear where this capability comes from\nas there is a stark distribution shift between pre-training text and ICL\nprompts. In this work, we study what patterns of the pre-training data\ncontribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel\nstructures}$ in the pre-training data -- pairs of phrases following similar\ntemplates in the same context window. Specifically, we detect parallel\nstructures by checking whether training on one phrase improves prediction of\nthe other, and conduct ablation experiments to study their effect on ICL. We\nshow that removing parallel structures in the pre-training data reduces LMs'\nICL accuracy by 51% (vs 2% from random ablation). This drop persists even when\nexcluding common patterns such as n-gram repetitions and long-range dependency,\nshowing the diversity and generality of parallel structures. A closer look at\nthe detected parallel structures indicates that they cover diverse linguistic\ntasks and span long distances in the data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12530v1",
    "published_date": "2024-02-19 20:40:48 UTC",
    "updated_date": "2024-02-19 20:40:48 UTC"
  },
  {
    "arxiv_id": "2402.12527v2",
    "title": "The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning",
    "authors": [
      "Anya Sims",
      "Cong Lu",
      "Jakob Foerster",
      "Yee Whye Teh"
    ],
    "abstract": "Offline reinforcement learning aims to train agents from pre-collected\ndatasets. However, this comes with the added challenge of estimating the value\nof behaviors not covered in the dataset. Model-based methods offer a potential\nsolution by training an approximate dynamics model, which then allows\ncollection of additional synthetic data via rollouts in this model. The\nprevailing theory treats this approach as online RL in an approximate dynamics\nmodel, and any remaining performance gap is therefore understood as being due\nto dynamics model errors. In this paper, we analyze this assumption and\ninvestigate how popular algorithms perform as the learned dynamics model is\nimproved. In contrast to both intuition and theory, if the learned dynamics\nmodel is replaced by the true error-free dynamics, existing model-based methods\ncompletely fail. This reveals a key oversight: The theoretical foundations\nassume sampling of full horizon rollouts in the learned dynamics model;\nhowever, in practice, the number of model-rollout steps is aggressively reduced\nto prevent accumulating errors. We show that this truncation of rollouts\nresults in a set of edge-of-reach states at which we are effectively\n``bootstrapping from the void.'' This triggers pathological value\noverestimation and complete performance collapse. We term this the\nedge-of-reach problem. Based on this new insight, we fill important gaps in\nexisting theory, and reveal how prior model-based methods are primarily\naddressing the edge-of-reach problem, rather than model-inaccuracy as claimed.\nFinally, we propose Reach-Aware Value Learning (RAVL), a simple and robust\nmethod that directly addresses the edge-of-reach problem and hence - unlike\nexisting methods - does not fail as the dynamics model is improved. Code\nopen-sourced at: github.com/anyasims/edge-of-reach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code open-sourced at: https://github.com/anyasims/edge-of-reach",
    "pdf_url": "http://arxiv.org/pdf/2402.12527v2",
    "published_date": "2024-02-19 20:38:00 UTC",
    "updated_date": "2024-11-29 19:52:18 UTC"
  },
  {
    "arxiv_id": "2402.12525v1",
    "title": "LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks",
    "authors": [
      "Truong Thanh Hung Nguyen",
      "Tobias Clement",
      "Phuc Truong Loc Nguyen",
      "Nils Kemmerzell",
      "Van Binh Truong",
      "Vo Thanh Khang Nguyen",
      "Mohamed Abdelaal",
      "Hung Cao"
    ],
    "abstract": "LangXAI is a framework that integrates Explainable Artificial Intelligence\n(XAI) with advanced vision models to generate textual explanations for visual\nrecognition tasks. Despite XAI advancements, an understanding gap persists for\nend-users with limited domain knowledge in artificial intelligence and computer\nvision. LangXAI addresses this by furnishing text-based explanations for\nclassification, object detection, and semantic segmentation model outputs to\nend-users. Preliminary results demonstrate LangXAI's enhanced plausibility,\nwith high BERTScore across tasks, fostering a more transparent and reliable AI\nframework on vision tasks for end-users.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12525v1",
    "published_date": "2024-02-19 20:36:32 UTC",
    "updated_date": "2024-02-19 20:36:32 UTC"
  },
  {
    "arxiv_id": "2402.12518v2",
    "title": "Gaussian Process Neural Additive Models",
    "authors": [
      "Wei Zhang",
      "Brian Barr",
      "John Paisley"
    ],
    "abstract": "Deep neural networks have revolutionized many fields, but their black-box\nnature also occasionally prevents their wider adoption in fields such as\nhealthcare and finance, where interpretable and explainable models are\nrequired. The recent development of Neural Additive Models (NAMs) is a\nsignificant step in the direction of interpretable deep learning for tabular\ndatasets. In this paper, we propose a new subclass of NAMs that use a\nsingle-layer neural network construction of the Gaussian process via random\nFourier features, which we call Gaussian Process Neural Additive Models\n(GP-NAM). GP-NAMs have the advantage of a convex objective function and number\nof trainable parameters that grows linearly with feature dimensionality. It\nsuffers no loss in performance compared to deeper NAM approaches because GPs\nare well-suited for learning complex non-parametric univariate functions. We\ndemonstrate the performance of GP-NAM on several tabular datasets, showing that\nit achieves comparable or better performance in both classification and\nregression tasks with a large reduction in the number of parameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Appears at AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12518v2",
    "published_date": "2024-02-19 20:29:34 UTC",
    "updated_date": "2024-03-19 15:38:29 UTC"
  },
  {
    "arxiv_id": "2402.12499v4",
    "title": "Automated Security Response through Online Learning with Adaptive Conjectures",
    "authors": [
      "Kim Hammar",
      "Tao Li",
      "Rolf Stadler",
      "Quanyan Zhu"
    ],
    "abstract": "We study automated security response for an IT infrastructure and formulate\nthe interaction between an attacker and a defender as a partially observed,\nnon-stationary game. We relax the standard assumption that the game model is\ncorrectly specified and consider that each player has a probabilistic\nconjecture about the model, which may be misspecified in the sense that the\ntrue model has probability 0. This formulation allows us to capture uncertainty\nand misconception about the infrastructure and the intents of the players. To\nlearn effective game strategies online, we design Conjectural Online Learning\n(COL), a novel method where a player iteratively adapts its conjecture using\nBayesian learning and updates its strategy through rollout. We prove that the\nconjectures converge to best fits, and we provide a bound on the performance\nimprovement that rollout enables with a conjectured model. To characterize the\nsteady state of the game, we propose a variant of the Berk-Nash equilibrium. We\npresent COL through an advanced persistent threat use case. Testbed evaluations\nshow that COL produces effective security strategies that adapt to a changing\nenvironment. We also find that COL enables faster convergence than current\nreinforcement learning techniques.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.GT",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2402.12499v4",
    "published_date": "2024-02-19 20:06:15 UTC",
    "updated_date": "2025-01-05 11:16:19 UTC"
  },
  {
    "arxiv_id": "2402.15524v1",
    "title": "Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets",
    "authors": [
      "Panagiotis Lymperopoulos",
      "Liping Liu"
    ],
    "abstract": "Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a\ncommon problem in infeasibility analysis of over-constrained systems. However,\nbecause of the exponential search space of the problem, enumerating MUSes is\nextremely time-consuming in real applications. In this work, we propose to\nprune formulas using a learned model to speed up MUS enumeration. We represent\nformulas as graphs and then develop a graph-based learning model to predict\nwhich part of the formula should be pruned. Importantly, our algorithm does not\nrequire data labeling by only checking the satisfiability of pruned formulas.\nIt does not even require training data from the target application because it\nextrapolates to data with different distributions. In our experiments we\ncombine our algorithm with existing MUS enumerators and validate its\neffectiveness in multiple benchmarks including a set of real-world problems\noutside our training distribution. The experiment results show that our method\nsignificantly accelerates MUS enumeration on average on these benchmark\nproblems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15524v1",
    "published_date": "2024-02-19 20:03:45 UTC",
    "updated_date": "2024-02-19 20:03:45 UTC"
  },
  {
    "arxiv_id": "2402.12490v1",
    "title": "Towards Cross-Domain Continual Learning",
    "authors": [
      "Marcus de Carvalho",
      "Mahardhika Pratama",
      "Jie Zhang",
      "Chua Haoyan",
      "Edward Yapp"
    ],
    "abstract": "Continual learning is a process that involves training learning agents to\nsequentially master a stream of tasks or classes without revisiting past data.\nThe challenge lies in leveraging previously acquired knowledge to learn new\ntasks efficiently, while avoiding catastrophic forgetting. Existing methods\nprimarily focus on single domains, restricting their applicability to specific\nproblems.\n  In this work, we introduce a novel approach called Cross-Domain Continual\nLearning (CDCL) that addresses the limitations of being limited to single\nsupervised domains. Our method combines inter- and intra-task cross-attention\nmechanisms within a compact convolutional network. This integration enables the\nmodel to maintain alignment with features from previous tasks, thereby delaying\nthe data drift that may occur between tasks, while performing unsupervised\ncross-domain (UDA) between related domains. By leveraging an\nintra-task-specific pseudo-labeling method, we ensure accurate input pairs for\nboth labeled and unlabeled samples, enhancing the learning process. To validate\nour approach, we conduct extensive experiments on public UDA datasets,\nshowcasing its positive performance on cross-domain continual learning\nchallenges. Additionally, our work introduces incremental ideas that contribute\nto the advancement of this field.\n  We make our code and models available to encourage further exploration and\nreproduction of our results: \\url{https://github.com/Ivsucram/CDCL}",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 2 Figures, 4 Tables. To be published at the IEEE\n  International Conference on Data Engineering (ICDE) 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12490v1",
    "published_date": "2024-02-19 19:54:03 UTC",
    "updated_date": "2024-02-19 19:54:03 UTC"
  },
  {
    "arxiv_id": "2402.14849v1",
    "title": "Asynchronous and Segmented Bidirectional Encoding for NMT",
    "authors": [
      "Jingpu Yang",
      "Zehua Han",
      "Mengyu Xiang",
      "Helin Wang",
      "Yuxiao Huang",
      "Miao Fang"
    ],
    "abstract": "With the rapid advancement of Neural Machine Translation (NMT), enhancing\ntranslation efficiency and quality has become a focal point of research.\nDespite the commendable performance of general models such as the Transformer\nin various aspects, they still fall short in processing long sentences and\nfully leveraging bidirectional contextual information. This paper introduces an\nimproved model based on the Transformer, implementing an asynchronous and\nsegmented bidirectional decoding strategy aimed at elevating translation\nefficiency and accuracy. Compared to traditional unidirectional translations\nfrom left-to-right or right-to-left, our method demonstrates heightened\nefficiency and improved translation quality, particularly in handling long\nsentences. Experimental results on the IWSLT2017 dataset confirm the\neffectiveness of our approach in accelerating translation and increasing\naccuracy, especially surpassing traditional unidirectional strategies in long\nsentence translation. Furthermore, this study analyzes the impact of sentence\nlength on decoding outcomes and explores the model's performance in various\nscenarios. The findings of this research not only provide an effective encoding\nstrategy for the NMT field but also pave new avenues and directions for future\nstudies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.14849v1",
    "published_date": "2024-02-19 19:48:02 UTC",
    "updated_date": "2024-02-19 19:48:02 UTC"
  },
  {
    "arxiv_id": "2402.12479v3",
    "title": "In value-based deep reinforcement learning, a pruned network is a good network",
    "authors": [
      "Johan Obando-Ceron",
      "Aaron Courville",
      "Pablo Samuel Castro"
    ],
    "abstract": "Recent work has shown that deep reinforcement learning agents have difficulty\nin effectively using their network parameters. We leverage prior insights into\nthe advantages of sparse training techniques and demonstrate that gradual\nmagnitude pruning enables value-based agents to maximize parameter\neffectiveness. This results in networks that yield dramatic performance\nimprovements over traditional networks, using only a small fraction of the full\nnetwork parameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12479v3",
    "published_date": "2024-02-19 19:34:07 UTC",
    "updated_date": "2024-06-25 13:10:06 UTC"
  },
  {
    "arxiv_id": "2402.13292v1",
    "title": "A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems",
    "authors": [
      "Aakash",
      "Indranil Saha"
    ],
    "abstract": "The fundamental goal assignment problem for a multi-robot application aims to\nassign a unique goal to each robot while ensuring collision-free paths,\nminimizing the total movement cost. A plausible algorithmic solution to this\nNP-hard problem involves an iterative process that integrates a task planner to\ncompute the goal assignment while ignoring the collision possibilities among\nthe robots and a multi-agent path-finding algorithm to find the collision-free\ntrajectories for a given assignment. This procedure involves a method for\ncomputing the next best assignment given the current best assignment. A naive\nway of computing the next best assignment, as done in the state-of-the-art\nsolutions, becomes a roadblock to achieving scalability in solving the overall\nproblem. To obviate this bottleneck, we propose an efficient conflict-guided\nmethod to compute the next best assignment. Additionally, we introduce two more\noptimizations to the algorithm -- first for avoiding the unconstrained path\ncomputations between robot-goal pairs wherever possible, and the second to\nprevent duplicate constrained path computations for multiple robot-goal pairs.\nWe extensively evaluate our algorithm for up to a hundred robots on several\nbenchmark workspaces. The results demonstrate that the proposed algorithm\nachieves nearly an order of magnitude speedup over the state-of-the-art\nalgorithm, showcasing its efficacy in real-world scenarios.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13292v1",
    "published_date": "2024-02-19 19:04:19 UTC",
    "updated_date": "2024-02-19 19:04:19 UTC"
  },
  {
    "arxiv_id": "2402.12451v2",
    "title": "The Revolution of Multimodal Large Language Models: A Survey",
    "authors": [
      "Davide Caffagni",
      "Federico Cocchi",
      "Luca Barsellotti",
      "Nicholas Moratelli",
      "Sara Sarto",
      "Lorenzo Baraldi",
      "Lorenzo Baraldi",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "abstract": "Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ACL 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2402.12451v2",
    "published_date": "2024-02-19 19:01:01 UTC",
    "updated_date": "2024-06-06 16:13:43 UTC"
  },
  {
    "arxiv_id": "2402.12373v2",
    "title": "LTL learning on GPUs",
    "authors": [
      "Mojtaba Valizadeh",
      "Nathanaël Fijalkow",
      "Martin Berger"
    ],
    "abstract": "Linear temporal logic (LTL) is widely used in industrial verification. LTL\nformulae can be learned from traces. Scaling LTL formula learning is an open\nproblem. We implement the first GPU-based LTL learner using a novel form of\nenumerative program synthesis. The learner is sound and complete. Our\nbenchmarks indicate that it handles traces at least 2048 times more numerous,\nand on average at least 46 times faster than existing state-of-the-art\nlearners. This is achieved with, among others, novel branch-free LTL semantics\nthat has $O(\\log n)$ time complexity, where $n$ is trace length, while previous\nimplementations are $O(n^2)$ or worse (assuming bitwise boolean operations and\nshifts by powers of 2 have unit costs -- a realistic assumption on modern\nprocessors).",
    "categories": [
      "cs.PL",
      "cs.AI",
      "68",
      "D.3"
    ],
    "primary_category": "cs.PL",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.12373v2",
    "published_date": "2024-02-19 18:58:26 UTC",
    "updated_date": "2024-03-27 20:00:00 UTC"
  },
  {
    "arxiv_id": "2402.12370v2",
    "title": "AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies",
    "authors": [
      "Xiao Ye",
      "Andrew Wang",
      "Jacob Choi",
      "Yining Lu",
      "Shreya Sharma",
      "Lingfeng Shen",
      "Vijay Tiyyala",
      "Nicholas Andrews",
      "Daniel Khashabi"
    ],
    "abstract": "Humans regularly engage in analogical thinking, relating personal experiences\nto current situations (X is analogous to Y because of Z). Analogical thinking\nallows humans to solve problems in creative ways, grasp difficult concepts, and\narticulate ideas more effectively. Can language models (LMs) do the same? To\nanswer this question, we propose AnaloBench, a benchmark to determine\nanalogical reasoning ability in LMs. Our benchmarking approach focuses on\naspects of this ability that are common among humans: (i) recalling related\nexperiences from a large amount of information, and (ii) applying analogical\nreasoning to complex and lengthy scenarios. We test a broad collection of\nproprietary models (e.g., GPT family, Claude V2) and open source models such as\nLLaMA2. As in prior results, scaling up LMs results in some performance boosts.\nSurprisingly, scale offers minimal gains when, (i) analogies involve lengthy\nscenarios, or (ii) recalling relevant scenarios from a large pool of\ninformation, a process analogous to finding a needle in a haystack. We hope\nthese observations encourage further research in this field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2402.12370v2",
    "published_date": "2024-02-19 18:56:44 UTC",
    "updated_date": "2024-10-03 22:55:11 UTC"
  },
  {
    "arxiv_id": "2402.12366v1",
    "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
    "authors": [
      "Archit Sharma",
      "Sedrick Keh",
      "Eric Mitchell",
      "Chelsea Finn",
      "Kushal Arora",
      "Thomas Kollar"
    ],
    "abstract": "Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for\nimproving the instruction-following abilities of powerful pre-trained language\nmodels. RLAIF first performs supervised fine-tuning (SFT) using demonstrations\nfrom a teacher model and then further fine-tunes the model with reinforcement\nlearning (RL), using feedback from a critic model. While recent popular\nopen-source models have demonstrated substantial improvements in performance\nfrom the RL step, in this paper we question whether the complexity of this RL\nstep is truly warranted for AI feedback. We show that the improvements of the\nRL step are virtually entirely due to the widespread practice of using a weaker\nteacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g.,\nGPT-4) used for AI feedback generation. Specifically, we show that simple\nsupervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF\npipelines. More generally, we find that the gains from RLAIF vary substantially\nacross base model families, test-time evaluation protocols, and critic models.\nFinally, we provide a mechanistic explanation for when SFT may outperform the\nfull two-step RLAIF pipeline as well as suggestions for making RLAIF maximally\nuseful in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12366v1",
    "published_date": "2024-02-19 18:53:54 UTC",
    "updated_date": "2024-02-19 18:53:54 UTC"
  },
  {
    "arxiv_id": "2402.12365v5",
    "title": "Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators",
    "authors": [
      "Benedikt Alkin",
      "Andreas Fürst",
      "Simon Schmid",
      "Lukas Gruber",
      "Markus Holzleitner",
      "Johannes Brandstetter"
    ],
    "abstract": "Neural operators, serving as physics surrogate models, have recently gained\nincreased interest. With ever increasing problem complexity, the natural\nquestion arises: what is an efficient way to scale neural operators to larger\nand more complex simulations - most importantly by taking into account\ndifferent types of simulation datasets. This is of special interest since, akin\nto their numerical counterparts, different techniques are used across\napplications, even if the underlying dynamics of the systems are similar.\nWhereas the flexibility of transformers has enabled unified architectures\nacross domains, neural operators mostly follow a problem specific design, where\nGNNs are commonly used for Lagrangian simulations and grid-based models\npredominate Eulerian simulations. We introduce Universal Physics Transformers\n(UPTs), an efficient and unified learning paradigm for a wide range of\nspatio-temporal problems. UPTs operate without grid- or particle-based latent\nstructures, enabling flexibility and scalability across meshes and particles.\nUPTs efficiently propagate dynamics in the latent space, emphasized by inverse\nencoding and decoding techniques. Finally, UPTs allow for queries of the latent\nspace representation at any point in space-time. We demonstrate diverse\napplicability and efficacy of UPTs in mesh-based fluid simulations, and\nsteady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based\ndynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at NeurIPS 2024, Github: https://ml-jku.github.io/UPT/",
    "pdf_url": "http://arxiv.org/pdf/2402.12365v5",
    "published_date": "2024-02-19 18:52:13 UTC",
    "updated_date": "2025-02-27 10:24:17 UTC"
  },
  {
    "arxiv_id": "2402.12360v1",
    "title": "Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks",
    "authors": [
      "Hector Vargas Alvarez",
      "Gianluca Fabiani",
      "Ioannis G. Kevrekidis",
      "Nikolaos Kazantzis",
      "Constantinos Siettos"
    ],
    "abstract": "We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time\nnonlinear observer state estimation problem. Integrated within a single-step\nexact observer linearization framework, the proposed PINN approach aims at\nlearning a nonlinear state transformation map by solving a system of\ninhomogeneous functional equations. The performance of the proposed PINN\napproach is assessed via two illustrative case studies for which the observer\nlinearizing transformation map can be derived analytically. We also perform an\nuncertainty quantification analysis for the proposed PINN scheme and we compare\nit with conventional power-series numerical implementations, which rely on the\ncomputation of a power series solution.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "math.DS",
      "37N30, 68T05, 93C55, 65D15"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12360v1",
    "published_date": "2024-02-19 18:47:56 UTC",
    "updated_date": "2024-02-19 18:47:56 UTC"
  },
  {
    "arxiv_id": "2402.12354v2",
    "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
    "authors": [
      "Soufiane Hayou",
      "Nikhil Ghosh",
      "Bin Yu"
    ],
    "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally\nintroduced in Hu et al. (2021) leads to suboptimal finetuning of models with\nlarge width (embedding dimension). This is due to the fact that adapter\nmatrices A and B in LoRA are updated with the same learning rate. Using scaling\narguments for large width networks, we demonstrate that using the same learning\nrate for A and B does not allow efficient feature learning. We then show that\nthis suboptimality of LoRA can be corrected simply by setting different\nlearning rates for the LoRA adapter matrices A and B with a well-chosen ratio.\nWe call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$\nimproves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$\n2X SpeedUp), at the same computational cost as LoRA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.12354v2",
    "published_date": "2024-02-19 18:33:49 UTC",
    "updated_date": "2024-07-04 18:33:00 UTC"
  },
  {
    "arxiv_id": "2402.12348v2",
    "title": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations",
    "authors": [
      "Jinhao Duan",
      "Renming Zhang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Lichao Sun",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Tianlong Chen",
      "Kaidi Xu"
    ],
    "abstract": "As Large Language Models (LLMs) are integrated into critical real-world\napplications, their strategic and logical reasoning abilities are increasingly\ncrucial. This paper evaluates LLMs' reasoning abilities in competitive\nenvironments through game-theoretic tasks, e.g., board and card games that\nrequire pure logic and strategic reasoning to compete with opponents. We first\npropose GTBench, a language-driven environment composing 10 widely recognized\ntasks, across a comprehensive game taxonomy: complete versus incomplete\ninformation, dynamic versus static, and probabilistic versus deterministic\nscenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and\n(2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that\n(1) LLMs have distinct behaviors regarding various gaming scenarios; for\nexample, LLMs fail in complete and deterministic games yet they are competitive\nin probabilistic gaming scenarios; (2) Most open-source LLMs, e.g.,\nCodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than\ncommercial LLMs, e.g., GPT-4, in complex games, yet the recently released\nLlama-3-70b-Instruct makes up for this shortcoming. In addition,\ncode-pretraining greatly benefits strategic reasoning, while advanced reasoning\nmethods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always\nhelp. We further characterize the game-theoretic properties of LLMs, such as\nequilibrium and Pareto Efficiency in repeated games. Detailed error profiles\nare provided for a better understanding of LLMs' behavior. We hope our research\nprovides standardized protocols and serves as a foundation to spur further\nexplorations in the strategic reasoning of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages; the first two authors contributed equally; GTBench HF\n  Leaderboard: https://huggingface.co/spaces/GTBench/GTBench",
    "pdf_url": "http://arxiv.org/pdf/2402.12348v2",
    "published_date": "2024-02-19 18:23:36 UTC",
    "updated_date": "2024-06-10 17:14:09 UTC"
  },
  {
    "arxiv_id": "2402.12343v4",
    "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!",
    "authors": [
      "Zhanhui Zhou",
      "Jie Liu",
      "Zhichen Dong",
      "Jiaheng Liu",
      "Chao Yang",
      "Wanli Ouyang",
      "Yu Qiao"
    ],
    "abstract": "Large language models (LLMs) undergo safety alignment to ensure safe\nconversations with humans. However, this paper introduces a training-free\nattack method capable of reversing safety alignment, converting the outcomes of\nstronger alignment into greater potential for harm by accessing only LLM output\ntoken distributions. Specifically, our method achieves this reversal by\ncontrasting the output token distribution of a safety-aligned language model\n(e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that\nthe token predictions are shifted towards the opposite direction of safety\nalignment. We name this method emulated disalignment (ED) because sampling from\nthis contrastive distribution provably emulates the result of fine-tuning to\nminimize a safety reward. Our experiments with ED across three evaluation\ndatasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show\nthat ED doubles the harmfulness of pre-trained models and outperforms strong\nbaselines, achieving the highest harmful rates in 43 out of 48 evaluation\nsubsets by a large margin. Eventually, given ED's reliance on language model\noutput token distributions, which particularly compromises open-source models,\nour findings highlight the need to reassess the open accessibility of language\nmodels, even if they have been safety-aligned. Code is available at\nhttps://github.com/ZHZisZZ/emulated-disalignment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12343v4",
    "published_date": "2024-02-19 18:16:51 UTC",
    "updated_date": "2024-06-06 12:54:48 UTC"
  },
  {
    "arxiv_id": "2402.12336v2",
    "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
    "authors": [
      "Christian Schlarmann",
      "Naman Deep Singh",
      "Francesco Croce",
      "Matthias Hein"
    ],
    "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are\nincreasingly used for various real-world tasks. Prior work has shown that these\nmodels are highly vulnerable to adversarial attacks on the vision modality.\nThese attacks can be leveraged to spread fake information or defraud users, and\nthus pose a significant risk, which makes the robustness of large multi-modal\nfoundation models a pressing problem. The CLIP model, or one of its variants,\nis used as a frozen vision encoder in many large vision-language models\n(LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial\nfine-tuning scheme to obtain a robust CLIP vision encoder, which yields\nrobustness on all vision down-stream tasks (LVLMs, zero-shot classification)\nthat rely on CLIP. In particular, we show that stealth-attacks on users of\nLVLMs by a malicious third party providing manipulated images are no longer\npossible once one replaces the original CLIP model with our robust one. No\nretraining or fine-tuning of the down-stream LVLMs is required. The code and\nrobust models are available at https://github.com/chs20/RobustVLM",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 Oral",
    "pdf_url": "http://arxiv.org/pdf/2402.12336v2",
    "published_date": "2024-02-19 18:09:48 UTC",
    "updated_date": "2024-06-05 15:32:03 UTC"
  },
  {
    "arxiv_id": "2402.12331v1",
    "title": "Generating Survival Interpretable Trajectories and Data",
    "authors": [
      "Andrei V. Konstantinov",
      "Stanislav R. Kirpichenko",
      "Lev V. Utkin"
    ],
    "abstract": "A new model for generating survival trajectories and data based on applying\nan autoencoder of a specific structure is proposed. It solves three tasks.\nFirst, it provides predictions in the form of the expected event time and the\nsurvival function for a new generated feature vector on the basis of the Beran\nestimator. Second, the model generates additional data based on a given\ntraining set that would supplement the original dataset. Third, the most\nimportant, it generates a prototype time-dependent trajectory for an object,\nwhich characterizes how features of the object could be changed to achieve a\ndifferent time to an event. The trajectory can be viewed as a type of the\ncounterfactual explanation. The proposed model is robust during training and\ninference due to a specific weighting scheme incorporating into the variational\nautoencoder. The model also determines the censored indicators of new generated\ndata by solving a classification task. The paper demonstrates the efficiency\nand properties of the proposed model using numerical experiments on synthetic\nand real datasets. The code of the algorithm implementing the proposed model is\npublicly available.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12331v1",
    "published_date": "2024-02-19 18:02:10 UTC",
    "updated_date": "2024-02-19 18:02:10 UTC"
  },
  {
    "arxiv_id": "2402.12329v2",
    "title": "Query-Based Adversarial Prompt Generation",
    "authors": [
      "Jonathan Hayase",
      "Ema Borevkovic",
      "Nicholas Carlini",
      "Florian Tramèr",
      "Milad Nasr"
    ],
    "abstract": "Recent work has shown it is possible to construct adversarial examples that\ncause an aligned language model to emit harmful strings or perform harmful\nbehavior. Existing attacks work either in the white-box setting (with full\naccess to the model weights), or through transferability: the phenomenon that\nadversarial examples crafted on one model often remain effective on other\nmodels. We improve on prior work with a query-based attack that leverages API\naccess to a remote language model to construct adversarial examples that cause\nthe model to emit harmful strings with (much) higher probability than with\ntransfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety\nclassifier; we can cause GPT-3.5 to emit harmful strings that current transfer\nattacks fail at, and we can evade the safety classifier with nearly 100%\nprobability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12329v2",
    "published_date": "2024-02-19 18:01:36 UTC",
    "updated_date": "2024-12-07 23:09:49 UTC"
  },
  {
    "arxiv_id": "2402.12327v3",
    "title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents",
    "authors": [
      "Zengqing Wu",
      "Run Peng",
      "Shuyuan Zheng",
      "Qianying Liu",
      "Xu Han",
      "Brian Inhyuk Kwon",
      "Makoto Onizuka",
      "Shaojie Tang",
      "Chuan Xiao"
    ],
    "abstract": "Large Language Models (LLMs) have increasingly been utilized in social\nsimulations, where they are often guided by carefully crafted instructions to\nstably exhibit human-like behaviors during simulations. Nevertheless, we doubt\nthe necessity of shaping agents' behaviors for accurate social simulations.\nInstead, this paper emphasizes the importance of spontaneous phenomena, wherein\nagents deeply engage in contexts and make adaptive decisions without explicit\ndirections. We explored spontaneous cooperation across three competitive\nscenarios and successfully simulated the gradual emergence of cooperation,\nfindings that align closely with human behavioral data. This approach not only\naids the computational social science community in bridging the gap between\nsimulations and real-world dynamics but also offers the AI community a novel\nmethod to assess LLMs' capability of deliberate reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MA",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2024 Findings. Source codes available at\n  https://github.com/wuzengqing001225/SABM_ShallWeTeamUp",
    "pdf_url": "http://arxiv.org/pdf/2402.12327v3",
    "published_date": "2024-02-19 18:00:53 UTC",
    "updated_date": "2024-10-27 19:03:37 UTC"
  },
  {
    "arxiv_id": "2403.15401v3",
    "title": "Large Language Model for Mental Health: A Systematic Review",
    "authors": [
      "Zhijun Guo",
      "Alvina Lai",
      "Johan Hilge Thygesen",
      "Joseph Farrington",
      "Thomas Keen",
      "Kezhi Li"
    ],
    "abstract": "Large language models (LLMs) have attracted significant attention for\npotential applications in digital health, while their application in mental\nhealth is subject to ongoing debate. This systematic review aims to evaluate\nthe usage of LLMs in mental health, focusing on their strengths and limitations\nin early screening, digital interventions, and clinical applications. Adhering\nto PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM\nusing keywords: 'mental health OR mental illness OR mental disorder OR\npsychiatry' AND 'large language models'. We included articles published between\nJanuary 1, 2017, and April 30, 2024, excluding non-English articles. 30\narticles were evaluated, which included research on mental health conditions\nand suicidal ideation detection through text (n=15), usage of LLMs for mental\nhealth conversational agents (CAs) (n=7), and other applications and\nevaluations of LLMs in mental health (n=18). LLMs exhibit substantial\neffectiveness in detecting mental health issues and providing accessible,\nde-stigmatized eHealth services. However, the current risks associated with the\nclinical use might surpass their benefits. The study identifies several\nsignificant issues: the lack of multilingual datasets annotated by experts,\nconcerns about the accuracy and reliability of the content generated,\nchallenges in interpretability due to the 'black box' nature of LLMs, and\npersistent ethical dilemmas. These include the lack of a clear ethical\nframework, concerns about data privacy, and the potential for over-reliance on\nLLMs by both therapists and patients, which could compromise traditional\nmedical practice. Despite these issues, the rapid development of LLMs\nunderscores their potential as new clinical aids, emphasizing the need for\ncontinued research and development in this area.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15401v3",
    "published_date": "2024-02-19 17:58:41 UTC",
    "updated_date": "2024-08-12 21:46:16 UTC"
  },
  {
    "arxiv_id": "2402.12319v1",
    "title": "Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness",
    "authors": [
      "Chen Zhao",
      "Feng Mi",
      "Xintao Wu",
      "Kai Jiang",
      "Latifur Khan",
      "Feng Chen"
    ],
    "abstract": "The fairness-aware online learning framework has emerged as a potent tool\nwithin the context of continuous lifelong learning. In this scenario, the\nlearner's objective is to progressively acquire new tasks as they arrive over\ntime, while also guaranteeing statistical parity among various protected\nsub-populations, such as race and gender, when it comes to the newly introduced\ntasks. A significant limitation of current approaches lies in their heavy\nreliance on the i.i.d (independent and identically distributed) assumption\nconcerning data, leading to a static regret analysis of the framework.\nNevertheless, it's crucial to note that achieving low static regret does not\nnecessarily translate to strong performance in dynamic environments\ncharacterized by tasks sampled from diverse distributions. In this paper, to\ntackle the fairness-aware online learning challenge in evolving settings, we\nintroduce a unique regret measure, FairSAR, by incorporating long-term fairness\nconstraints into a strongly adapted loss regret framework. Moreover, to\ndetermine an optimal model parameter at each time step, we introduce an\ninnovative adaptive fairness-aware online meta-learning algorithm, referred to\nas FairSAOML. This algorithm possesses the ability to adjust to dynamic\nenvironments by effectively managing bias control and model accuracy. The\nproblem is framed as a bi-level convex-concave optimization, considering both\nthe model's primal and dual parameters, which pertain to its accuracy and\nfairness attributes, respectively. Theoretical analysis yields sub-linear upper\nbounds for both loss regret and the cumulative violation of fairness\nconstraints. Our experimental evaluation on various real-world datasets in\ndynamic environments demonstrates that our proposed FairSAOML algorithm\nconsistently outperforms alternative approaches rooted in the most advanced\nprior online learning methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by TKDD, extended from KDD 2022. arXiv admin note:\n  substantial text overlap with arXiv:2205.11264",
    "pdf_url": "http://arxiv.org/pdf/2402.12319v1",
    "published_date": "2024-02-19 17:44:35 UTC",
    "updated_date": "2024-02-19 17:44:35 UTC"
  },
  {
    "arxiv_id": "2402.13290v1",
    "title": "Grounding from an AI and Cognitive Science Lens",
    "authors": [
      "Goonmeet Bajaj",
      "Srinivasan Parthasarathy",
      "Valerie L. Shalin",
      "Amit Sheth"
    ],
    "abstract": "Grounding is a challenging problem, requiring a formal definition and\ndifferent levels of abstraction. This article explores grounding from both\ncognitive science and machine learning perspectives. It identifies the\nsubtleties of grounding, its significance for collaborative agents, and\nsimilarities and differences in grounding approaches in both communities. The\narticle examines the potential of neuro-symbolic approaches tailored for\ngrounding tasks, showcasing how they can more comprehensively address\ngrounding. Finally, we discuss areas for further exploration and development in\ngrounding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13290v1",
    "published_date": "2024-02-19 17:44:34 UTC",
    "updated_date": "2024-02-19 17:44:34 UTC"
  },
  {
    "arxiv_id": "2402.12317v2",
    "title": "EVOR: Evolving Retrieval for Code Generation",
    "authors": [
      "Hongjin Su",
      "Shuyang Jiang",
      "Yuhang Lai",
      "Haoyuan Wu",
      "Boao Shi",
      "Che Liu",
      "Qian Liu",
      "Tao Yu"
    ],
    "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Retrieval-augmented code generation",
    "pdf_url": "http://arxiv.org/pdf/2402.12317v2",
    "published_date": "2024-02-19 17:37:28 UTC",
    "updated_date": "2024-12-03 15:56:26 UTC"
  },
  {
    "arxiv_id": "2402.12307v1",
    "title": "Multi-View Conformal Learning for Heterogeneous Sensor Fusion",
    "authors": [
      "Enrique Garcia-Ceja"
    ],
    "abstract": "Being able to assess the confidence of individual predictions in machine\nlearning models is crucial for decision making scenarios. Specially, in\ncritical applications such as medical diagnosis, security, and unmanned\nvehicles, to name a few. In the last years, complex predictive models have had\ngreat success in solving hard tasks and new methods are being proposed every\nday. While the majority of new developments in machine learning models focus on\nimproving the overall performance, less effort is put on assessing the\ntrustworthiness of individual predictions, and even to a lesser extent, in the\ncontext of sensor fusion. To this end, we build and test multi-view and\nsingle-view conformal models for heterogeneous sensor fusion. Our models\nprovide theoretical marginal confidence guarantees since they are based on the\nconformal prediction framework. We also propose a multi-view semi-conformal\nmodel based on sets intersection. Through comprehensive experimentation, we\nshow that multi-view models perform better than single-view models not only in\nterms of accuracy-based performance metrics (as it has already been shown in\nseveral previous works) but also in conformal measures that provide uncertainty\nestimation. Our results also showed that multi-view models generate prediction\nsets with less uncertainty compared to single-view models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12307v1",
    "published_date": "2024-02-19 17:30:09 UTC",
    "updated_date": "2024-02-19 17:30:09 UTC"
  },
  {
    "arxiv_id": "2402.12298v1",
    "title": "Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports",
    "authors": [
      "Felix J. Dorfner",
      "Liv Jürgensen",
      "Leonhard Donle",
      "Fares Al Mohamad",
      "Tobias R. Bodenmann",
      "Mason C. Cleveland",
      "Felix Busch",
      "Lisa C. Adams",
      "James Sato",
      "Thomas Schultz",
      "Albert E. Kim",
      "Jameson Merkow",
      "Keno K. Bressem",
      "Christopher P. Bridge"
    ],
    "abstract": "Introduction: With the rapid advances in large language models (LLMs), there\nhave been numerous new open source as well as commercial models. While recent\npublications have explored GPT-4 in its application to extracting information\nof interest from radiology reports, there has not been a real-world comparison\nof GPT-4 to different leading open-source models.\n  Materials and Methods: Two different and independent datasets were used. The\nfirst dataset consists of 540 chest x-ray reports that were created at the\nMassachusetts General Hospital between July 2019 and July 2021. The second\ndataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then\ncompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the\nopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,\nQWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately\nlabel the presence of multiple findings in x-ray text reports using different\nprompting techniques.\n  Results: On the ImaGenome dataset, the best performing open-source model was\nLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot\nprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,\nrespectively. On the institutional dataset, the best performing open-source\nmodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and\nfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and\n0.973, respectively.\n  Conclusion: In this paper, we show that while GPT-4 is superior to\nopen-source models in zero-shot report labeling, the implementation of few-shot\nprompting can bring open-source models on par with GPT-4. This shows that\nopen-source models could be a performant and privacy preserving alternative to\nGPT-4 for the task of radiology report classification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12298v1",
    "published_date": "2024-02-19 17:23:10 UTC",
    "updated_date": "2024-02-19 17:23:10 UTC"
  },
  {
    "arxiv_id": "2402.12284v2",
    "title": "Refining Minimax Regret for Unsupervised Environment Design",
    "authors": [
      "Michael Beukman",
      "Samuel Coward",
      "Michael Matthews",
      "Mattie Fellows",
      "Minqi Jiang",
      "Michael Dennis",
      "Jakob Foerster"
    ],
    "abstract": "In unsupervised environment design, reinforcement learning agents are trained\non environment configurations (levels) generated by an adversary that maximises\nsome objective. Regret is a commonly used objective that theoretically results\nin a minimax regret (MMR) policy with desirable robustness guarantees; in\nparticular, the agent's maximum regret is bounded. However, once the agent\nreaches this regret bound on all levels, the adversary will only sample levels\nwhere regret cannot be further reduced. Although there are possible performance\nimprovements to be made outside of these regret-maximising levels, learning\nstagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a\nrefinement of the minimax regret objective that overcomes this limitation. We\nformally show that solving for this objective results in a subset of MMR\npolicies, and that BLP policies act consistently with a Perfect Bayesian policy\nover all levels. We further introduce an algorithm, ReMiDi, that results in a\nBLP policy at convergence. We empirically demonstrate that training on levels\nfrom a minimax regret adversary causes learning to prematurely stagnate, but\nthat ReMiDi continues learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024. The first two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2402.12284v2",
    "published_date": "2024-02-19 16:51:29 UTC",
    "updated_date": "2024-06-08 10:08:25 UTC"
  },
  {
    "arxiv_id": "2402.12280v2",
    "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
    "authors": [
      "Shuowei Jin",
      "Xueshen Liu",
      "Yongji Wu",
      "Haizhong Zheng",
      "Qingzhao Zhang",
      "Atul Prakash",
      "Matthew Lentz",
      "Danyang Zhuo",
      "Feng Qian",
      "Z. Morley Mao"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12280v2",
    "published_date": "2024-02-19 16:47:04 UTC",
    "updated_date": "2025-04-13 14:17:57 UTC"
  },
  {
    "arxiv_id": "2402.12279v2",
    "title": "Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks",
    "authors": [
      "Nadezhda Chirkova",
      "Vassilina Nikoulina"
    ],
    "abstract": "Zero-shot cross-lingual knowledge transfer enables a multilingual pretrained\nlanguage model, finetuned on a task in one language, make predictions for this\ntask in other languages. While being broadly studied for natural language\nunderstanding tasks, the described setting is understudied for generation.\nPrevious works notice a frequent problem of generation in a wrong language and\npropose approaches to address it, usually using mT5 as a backbone model. In\nthis work we compare various approaches proposed from the literature in unified\nsettings, also including alternative backbone models, namely mBART and\nNLLB-200. We first underline the importance of tuning learning rate used for\nfinetuning, which helps to substantially alleviate the problem of generation in\nthe wrong language. Then, we show that with careful learning rate tuning, the\nsimple full finetuning of the model acts as a very strong baseline and\nalternative approaches bring only marginal improvements. Finally, we find that\nmBART performs similarly to mT5 of the same size, and NLLB-200 can be\ncompetitive in some cases. Our final zero-shot models reach the performance of\nthe approach based on data translation which is usually considered as an upper\nbaseline for zero-shot cross-lingual transfer in generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024 final version. arXiv admin note: text overlap with\n  arXiv:2310.09917",
    "pdf_url": "http://arxiv.org/pdf/2402.12279v2",
    "published_date": "2024-02-19 16:43:57 UTC",
    "updated_date": "2024-04-22 17:32:00 UTC"
  },
  {
    "arxiv_id": "2402.12275v3",
    "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
    "authors": [
      "Hao Tang",
      "Darren Key",
      "Kevin Ellis"
    ],
    "abstract": "We give a model-based agent that builds a Python program representing its\nknowledge of the world based on its interactions with the environment. The\nworld model tries to explain its interactions, while also being optimistic\nabout what reward it can achieve. We define this optimism as a logical\nconstraint between a program and a planner. We study our agent on gridworlds,\nand on task planning, finding our approach is more sample-efficient compared to\ndeep RL, more compute-efficient compared to ReAct-style agents, and that it can\ntransfer its knowledge across environments by editing its code.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12275v3",
    "published_date": "2024-02-19 16:39:18 UTC",
    "updated_date": "2024-09-20 18:56:41 UTC"
  },
  {
    "arxiv_id": "2402.12424v5",
    "title": "Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs",
    "authors": [
      "Naihao Deng",
      "Zhenjie Sun",
      "Ruiqi He",
      "Aman Sikka",
      "Yulong Chen",
      "Lin Ma",
      "Yue Zhang",
      "Rada Mihalcea"
    ],
    "abstract": "In this paper, we investigate the effectiveness of various LLMs in\ninterpreting tabular data through different prompting strategies and data\nformats. Our analyses extend across six benchmarks for table-related tasks such\nas question-answering and fact-checking. We introduce for the first time the\nassessment of LLMs' performance on image-based table representations.\nSpecifically, we compare five text-based and three image-based table\nrepresentations, demonstrating the role of representation and prompting on LLM\nperformance. Our study provides insights into the effective use of LLMs on\ntable-related tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ACL 2024 Findings; Naihao and Zhenjie contributed equally\n  to the project; Data available at:\n  https://github.com/dnaihao/Tables-as-Texts-or-Images",
    "pdf_url": "http://arxiv.org/pdf/2402.12424v5",
    "published_date": "2024-02-19 16:34:50 UTC",
    "updated_date": "2024-10-17 03:39:59 UTC"
  },
  {
    "arxiv_id": "2402.12265v3",
    "title": "On the Byzantine-Resilience of Distillation-Based Federated Learning",
    "authors": [
      "Christophe Roux",
      "Max Zimmer",
      "Sebastian Pokutta"
    ],
    "abstract": "Federated Learning (FL) algorithms using Knowledge Distillation (KD) have\nreceived increasing attention due to their favorable properties with respect to\nprivacy, non-i.i.d. data and communication cost. These methods depart from\ntransmitting model parameters and instead communicate information about a\nlearning task by sharing predictions on a public dataset. In this work, we\nstudy the performance of such approaches in the byzantine setting, where a\nsubset of the clients act in an adversarial manner aiming to disrupt the\nlearning process. We show that KD-based FL algorithms are remarkably resilient\nand analyze how byzantine clients can influence the learning process. Based on\nthese insights, we introduce two new byzantine attacks and demonstrate their\nability to break existing byzantine-resilient methods. Additionally, we propose\na novel defence method which enhances the byzantine resilience of KD-based FL\nalgorithms. Finally, we provide a general framework to obfuscate attacks,\nmaking them significantly harder to detect, thereby improving their\neffectiveness. Our findings serve as an important building block in the\nanalysis of byzantine FL, contributing through the development of new attacks\nand new defence mechanisms, further advancing the robustness of KD-based FL\nalgorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12265v3",
    "published_date": "2024-02-19 16:26:40 UTC",
    "updated_date": "2025-03-17 14:08:19 UTC"
  },
  {
    "arxiv_id": "2402.12264v1",
    "title": "Uncertainty quantification in fine-tuned LLMs using LoRA ensembles",
    "authors": [
      "Oleksandr Balabanov",
      "Hampus Linander"
    ],
    "abstract": "Fine-tuning large language models can improve task specific performance,\nalthough a general understanding of what the fine-tuned model has learned,\nforgotten and how to trust its predictions is still missing. We derive\nprincipled uncertainty quantification for fine-tuned LLMs with posterior\napproximations using computationally efficient low-rank adaptation ensembles.\nWe analyze three common multiple-choice datasets using low-rank adaptation\nensembles based on Mistral-7b, and draw quantitative and qualitative\nconclusions on their perceived complexity and model efficacy on the different\ntarget domains during and after fine-tuning. In particular, backed by the\nnumerical experiments, we hypothesise about signals from entropic uncertainty\nmeasures for data domains that are inherently difficult for a given\narchitecture to learn.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12264v1",
    "published_date": "2024-02-19 16:26:00 UTC",
    "updated_date": "2024-02-19 16:26:00 UTC"
  },
  {
    "arxiv_id": "2402.14848v2",
    "title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
    "authors": [
      "Mosh Levy",
      "Alon Jacoby",
      "Yoav Goldberg"
    ],
    "abstract": "This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that the traditional\nmetric of next word prediction correlates negatively with performance of LLMs'\non our reasoning dataset. We analyse our results and identify failure modes\nthat can serve as useful guides for future research, potentially informing\nstrategies to address the limitations observed in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.14848v2",
    "published_date": "2024-02-19 16:04:53 UTC",
    "updated_date": "2024-07-10 17:01:37 UTC"
  },
  {
    "arxiv_id": "2402.12240v1",
    "title": "BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts",
    "authors": [
      "Emanuele Marconato",
      "Samuele Bortolotti",
      "Emile van Krieken",
      "Antonio Vergari",
      "Andrea Passerini",
      "Stefano Teso"
    ],
    "abstract": "Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge -\nencoding, e.g., safety constraints - can be affected by Reasoning Shortcuts\n(RSs): They learn concepts consistent with the symbolic knowledge by exploiting\nunintended semantics. RSs compromise reliability and generalization and, as we\nshow in this paper, they are linked to NeSy models being overconfident about\nthe predicted concepts. Unfortunately, the only trustworthy mitigation strategy\nrequires collecting costly dense supervision over the concepts. Rather than\nattempting to avoid RSs altogether, we propose to ensure NeSy models are aware\nof the semantic ambiguity of the concepts they learn, thus enabling their users\nto identify and distrust low-quality concepts. Starting from three simple\ndesiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling\ntechnique that calibrates the model's concept-level confidence without\ncompromising prediction accuracy, thus encouraging NeSy architectures to be\nuncertain about concepts affected by RSs. We show empirically that bears\nimproves RS-awareness of several state-of-the-art NeSy models, and also\nfacilitates acquiring informative dense annotations for mitigation purposes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12240v1",
    "published_date": "2024-02-19 15:54:36 UTC",
    "updated_date": "2024-02-19 15:54:36 UTC"
  },
  {
    "arxiv_id": "2402.12237v3",
    "title": "Learning to Defer in Content Moderation: The Human-AI Interplay",
    "authors": [
      "Thodoris Lykouris",
      "Wentao Weng"
    ],
    "abstract": "Successful content moderation in online platforms relies on a human-AI\ncollaboration approach. A typical heuristic estimates the expected harmfulness\nof a post and uses fixed thresholds to decide whether to remove it and whether\nto send it for human review. This disregards the prediction uncertainty, the\ntime-varying element of human review capacity and post arrivals, and the\nselective sampling in the dataset (humans only review posts filtered by the\nadmission algorithm).\n  In this paper, we introduce a model to capture the human-AI interplay in\ncontent moderation. The algorithm observes contextual information for incoming\nposts, makes classification and admission decisions, and schedules posts for\nhuman review. Only admitted posts receive human reviews on their harmfulness.\nThese reviews help educate the machine-learning algorithms but are delayed due\nto congestion in the human review system. The classical learning-theoretic way\nto capture this human-AI interplay is via the framework of learning to defer,\nwhere the algorithm has the option to defer a classification task to humans for\na fixed cost and immediately receive feedback. Our model contributes to this\nliterature by introducing congestion in the human review system. Moreover,\nunlike work on online learning with delayed feedback where the delay in the\nfeedback is exogenous to the algorithm's decisions, the delay in our model is\nendogenous to both the admission and the scheduling decisions.\n  We propose a near-optimal learning algorithm that carefully balances the\nclassification loss from a selectively sampled dataset, the idiosyncratic loss\nof non-reviewed posts, and the delay loss of having congestion in the human\nreview system. To the best of our knowledge, this is the first result for\nonline learning in contextual queueing systems and hence our analytical\nframework may be of independent interest.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.HC",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12237v3",
    "published_date": "2024-02-19 15:47:47 UTC",
    "updated_date": "2024-06-02 16:02:24 UTC"
  },
  {
    "arxiv_id": "2402.12232v1",
    "title": "Kernel KMeans clustering splits for end-to-end unsupervised decision trees",
    "authors": [
      "Louis Ohl",
      "Pierre-Alexandre Mattei",
      "Mickaël Leclercq",
      "Arnaud Droit",
      "Frédéric Precioso"
    ],
    "abstract": "Trees are convenient models for obtaining explainable predictions on\nrelatively small datasets. Although there are many proposals for the end-to-end\nconstruction of such trees in supervised learning, learning a tree end-to-end\nfor clustering without labels remains an open challenge. As most works focus on\ninterpreting with trees the result of another clustering algorithm, we present\nhere a novel end-to-end trained unsupervised binary tree for clustering: Kauri.\nThis method performs a greedy maximisation of the kernel KMeans objective\nwithout requiring the definition of centroids. We compare this model on\nmultiple datasets with recent unsupervised trees and show that Kauri performs\nidentically when using a linear kernel. For other kernels, Kauri often\noutperforms the concatenation of kernel KMeans and a CART decision tree.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "62h30",
      "G.3"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12232v1",
    "published_date": "2024-02-19 15:39:39 UTC",
    "updated_date": "2024-02-19 15:39:39 UTC"
  },
  {
    "arxiv_id": "2402.14847v1",
    "title": "Deep learning-driven scheduling algorithm for a single machine problem minimizing the total tardiness",
    "authors": [
      "Michal Bouška",
      "Přemysl Šůcha",
      "Antonín Novák",
      "Zdeněk Hanzálek"
    ],
    "abstract": "In this paper, we investigate the use of the deep learning method for solving\na well-known NP-hard single machine scheduling problem with the objective of\nminimizing the total tardiness. We propose a deep neural network that acts as a\npolynomial-time estimator of the criterion value used in a single-pass\nscheduling algorithm based on Lawler's decomposition and symmetric\ndecomposition proposed by Della Croce et al. Essentially, the neural network\nguides the algorithm by estimating the best splitting of the problem into\nsubproblems. The paper also describes a new method for generating the training\ndata set, which speeds up the training dataset generation and reduces the\naverage optimality gap of solutions. The experimental results show that our\nmachine learning-driven approach can efficiently generalize information from\nthe training phase to significantly larger instances. Even though the instances\nused in the training phase have from 75 to 100 jobs, the average optimality gap\non instances with up to 800 jobs is 0.26%, which is almost five times less than\nthe gap of the state-of-the-art heuristic.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.14847v1",
    "published_date": "2024-02-19 15:34:09 UTC",
    "updated_date": "2024-02-19 15:34:09 UTC"
  },
  {
    "arxiv_id": "2402.12226v3",
    "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
    "authors": [
      "Jun Zhan",
      "Junqi Dai",
      "Jiasheng Ye",
      "Yunhua Zhou",
      "Dong Zhang",
      "Zhigeng Liu",
      "Xin Zhang",
      "Ruibin Yuan",
      "Ge Zhang",
      "Linyang Li",
      "Hang Yan",
      "Jie Fu",
      "Tao Gui",
      "Tianxiang Sun",
      "Yugang Jiang",
      "Xipeng Qiu"
    ],
    "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 16 figures, under review, work in progress",
    "pdf_url": "http://arxiv.org/pdf/2402.12226v3",
    "published_date": "2024-02-19 15:33:10 UTC",
    "updated_date": "2024-03-07 06:31:46 UTC"
  },
  {
    "arxiv_id": "2402.12219v2",
    "title": "Reformatted Alignment",
    "authors": [
      "Run-Ze Fan",
      "Xuefeng Li",
      "Haoyang Zou",
      "Junlong Li",
      "Shwai He",
      "Ethan Chern",
      "Jiewen Hu",
      "Pengfei Liu"
    ],
    "abstract": "The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Homepage: https://gair-nlp.github.io/ReAlign/",
    "pdf_url": "http://arxiv.org/pdf/2402.12219v2",
    "published_date": "2024-02-19 15:21:58 UTC",
    "updated_date": "2024-04-17 15:03:19 UTC"
  },
  {
    "arxiv_id": "2402.12216v1",
    "title": "Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications",
    "authors": [
      "Xinwei Guo",
      "Yujun Li",
      "Yafeng Peng",
      "Xuetao Wei"
    ],
    "abstract": "As AIGC has impacted our society profoundly in the past years, ethical issues\nhave received tremendous attention. The most urgent one is the AIGC copyright\ndilemma, which can immensely stifle the development of AIGC and greatly cost\nthe entire society. Given the complexity of AIGC copyright governance and the\nfact that no perfect solution currently exists, previous work advocated\ncopyleft on AI governance but without substantive analysis. In this paper, we\ntake a step further to explore the feasibility of copyleft to alleviate the\nAIGC copyright dilemma. We conduct a mixed-methods study from two aspects:\nqualitatively, we use a formal what-if analysis to clarify the dilemma and\nprovide case studies to show the feasibility of copyleft; quantitatively, we\nperform a carefully designed survey to find out how the public feels about\ncopylefting AIGC. The key findings include: a) people generally perceive the\ndilemma, b) they prefer to use authorized AIGC under loose restriction, and c)\nthey are positive to copyleft in AIGC and willing to use it in the future.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12216v1",
    "published_date": "2024-02-19 15:20:35 UTC",
    "updated_date": "2024-02-19 15:20:35 UTC"
  },
  {
    "arxiv_id": "2402.12202v1",
    "title": "Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach",
    "authors": [
      "Chengyi Ju",
      "Jiannong Cao",
      "Yu Yang",
      "Zhen-Qun Yang",
      "Ho Man Lee"
    ],
    "abstract": "In the era of modern education, addressing cross-school learner diversity is\ncrucial, especially in personalized recommender systems for elective course\nselection. However, privacy concerns often limit cross-school data sharing,\nwhich hinders existing methods' ability to model sparse data and address\nheterogeneity effectively, ultimately leading to suboptimal recommendations. In\nresponse, we propose HFRec, a heterogeneity-aware hybrid federated recommender\nsystem designed for cross-school elective course recommendations. The proposed\nmodel constructs heterogeneous graphs for each school, incorporating various\ninteractions and historical behaviors between students to integrate context and\ncontent information. We design an attention mechanism to capture\nheterogeneity-aware representations. Moreover, under a federated scheme, we\ntrain individual school-based models with adaptive learning settings to\nrecommend tailored electives. Our HFRec model demonstrates its effectiveness in\nproviding personalized elective recommendations while maintaining privacy, as\nit outperforms state-of-the-art models on both open-source and real-world\ndatasets.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12202v1",
    "published_date": "2024-02-19 15:06:04 UTC",
    "updated_date": "2024-02-19 15:06:04 UTC"
  },
  {
    "arxiv_id": "2402.14846v4",
    "title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
    "authors": [
      "Grgur Kovač",
      "Rémy Portelas",
      "Masataka Sawayama",
      "Peter Ford Dominey",
      "Pierre-Yves Oudeyer"
    ],
    "abstract": "The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T07",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "The project website and code are available at\n  https://sites.google.com/view/llmvaluestability Published in PLOS ONE (\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309114 ),\n  and a shorter version at CogSci 24 (\n  https://escholarship.org/uc/item/7w4823c6 )",
    "pdf_url": "http://arxiv.org/pdf/2402.14846v4",
    "published_date": "2024-02-19 14:53:01 UTC",
    "updated_date": "2024-08-28 14:04:05 UTC"
  },
  {
    "arxiv_id": "2402.12183v1",
    "title": "MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data",
    "authors": [
      "Mafalda Malafaia",
      "Thalea Schlender",
      "Peter A. N. Bosman",
      "Tanja Alderliesten"
    ],
    "abstract": "In the health domain, decisions are often based on different data modalities.\nThus, when creating prediction models, multimodal fusion approaches that can\nextract and combine relevant features from different data modalities, can be\nhighly beneficial. Furthermore, it is important to understand how each modality\nimpacts the final prediction, especially in high-stake domains, so that these\nmodels can be used in a trustworthy and responsible manner. We propose\nMultiFIX: a new interpretability-focused multimodal data fusion pipeline that\nexplicitly induces separate features from different data types that can\nsubsequently be combined to make a final prediction. An end-to-end deep\nlearning architecture is used to train a predictive model and extract\nrepresentative features of each modality. Each part of the model is then\nexplained using explainable artificial intelligence techniques. Attention maps\nare used to highlight important regions in image inputs. Inherently\ninterpretable symbolic expressions, learned with GP-GOMEA, are used to describe\nthe contribution of tabular inputs. The fusion of the extracted features to\npredict the target label is also replaced by a symbolic expression, learned\nwith GP-GOMEA. Results on synthetic problems demonstrate the strengths and\nlimitations of MultiFIX. Lastly, we apply MultiFIX to a publicly available\ndataset for the detection of malignant skin lesions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12183v1",
    "published_date": "2024-02-19 14:45:46 UTC",
    "updated_date": "2024-02-19 14:45:46 UTC"
  },
  {
    "arxiv_id": "2402.12181v1",
    "title": "Revisiting Data Augmentation in Deep Reinforcement Learning",
    "authors": [
      "Jianshu Hu",
      "Yunpeng Jiang",
      "Paul Weng"
    ],
    "abstract": "Various data augmentation techniques have been recently proposed in\nimage-based deep reinforcement learning (DRL). Although they empirically\ndemonstrate the effectiveness of data augmentation for improving sample\nefficiency or generalization, which technique should be preferred is not always\nclear. To tackle this question, we analyze existing methods to better\nunderstand them and to uncover how they are connected. Notably, by expressing\nthe variance of the Q-targets and that of the empirical actor/critic losses of\nthese methods, we can analyze the effects of their different components and\ncompare them. We furthermore formulate an explanation about how these methods\nmay be affected by choosing different data augmentation transformations in\ncalculating the target Q-values. This analysis suggests recommendations on how\nto exploit data augmentation in a more principled way. In addition, we include\na regularization term called tangent prop, previously proposed in computer\nvision, but whose adaptation to DRL is novel to the best of our knowledge. We\nevaluate our proposition and validate our analysis in several domains. Compared\nto different relevant baselines, we demonstrate that it achieves\nstate-of-the-art performance in most environments and shows higher sample\nefficiency and better generalization ability in some complex environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12181v1",
    "published_date": "2024-02-19 14:42:10 UTC",
    "updated_date": "2024-02-19 14:42:10 UTC"
  },
  {
    "arxiv_id": "2402.12179v1",
    "title": "Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations",
    "authors": [
      "Dinh An Ngo",
      "Thanh Dat Nguyen",
      "Thi Le Chi Dang",
      "Huy Hoan Le",
      "Ton Bao Ho",
      "Vo Thanh Khang Nguyen",
      "Truong Thanh Hung Nguyen"
    ],
    "abstract": "Cheating in online exams has become a prevalent issue over the past decade,\nespecially during the COVID-19 pandemic. To address this issue of academic\ndishonesty, our \"Exam Monitoring System: Detecting Abnormal Behavior in Online\nExaminations\" is designed to assist proctors in identifying unusual student\nbehavior. Our system demonstrates high accuracy and speed in detecting cheating\nin real-time scenarios, providing valuable information, and aiding proctors in\ndecision-making. This article outlines our methodology and the effectiveness of\nour system in mitigating the widespread problem of cheating in online exams.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12179v1",
    "published_date": "2024-02-19 14:37:17 UTC",
    "updated_date": "2024-02-19 14:37:17 UTC"
  },
  {
    "arxiv_id": "2402.12177v4",
    "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
    "authors": [
      "Mingtian Zhang",
      "Shawn Lan",
      "Peter Hayes",
      "David Barber"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12177v4",
    "published_date": "2024-02-19 14:33:24 UTC",
    "updated_date": "2024-03-12 16:04:23 UTC"
  },
  {
    "arxiv_id": "2402.12168v3",
    "title": "Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Shuai Zhao",
      "Leilei Gan",
      "Luu Anh Tuan",
      "Jie Fu",
      "Lingjuan Lyu",
      "Meihuizi Jia",
      "Jinming Wen"
    ],
    "abstract": "Recently, various parameter-efficient fine-tuning (PEFT) strategies for\napplication to language models have been proposed and successfully implemented.\nHowever, this raises the question of whether PEFT, which only updates a limited\nset of model parameters, constitutes security vulnerabilities when confronted\nwith weight-poisoning backdoor attacks. In this study, we show that PEFT is\nmore susceptible to weight-poisoning backdoor attacks compared to the\nfull-parameter fine-tuning method, with pre-defined triggers remaining\nexploitable and pre-defined targets maintaining high confidence, even after\nfine-tuning. Motivated by this insight, we developed a Poisoned Sample\nIdentification Module (PSIM) leveraging PEFT, which identifies poisoned samples\nthrough confidence, providing robust defense against weight-poisoning backdoor\nattacks. Specifically, we leverage PEFT to train the PSIM with randomly reset\nsample labels. During the inference process, extreme confidence serves as an\nindicator for poisoned samples, while others are clean. We conduct experiments\non text classification tasks, five fine-tuning strategies, and three\nweight-poisoning backdoor attack methods. Experiments show near 100% success\nrates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,\nour defensive approach exhibits overall competitive performance in mitigating\nweight-poisoning backdoor attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "NAACL Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12168v3",
    "published_date": "2024-02-19 14:22:54 UTC",
    "updated_date": "2024-03-29 12:12:30 UTC"
  },
  {
    "arxiv_id": "2402.12161v2",
    "title": "Endowing Pre-trained Graph Models with Provable Fairness",
    "authors": [
      "Zhongjian Zhang",
      "Mengmei Zhang",
      "Yue Yu",
      "Cheng Yang",
      "Jiawei Liu",
      "Chuan Shi"
    ],
    "abstract": "Pre-trained graph models (PGMs) aim to capture transferable inherent\nstructural properties and apply them to different downstream tasks. Similar to\npre-trained language models, PGMs also inherit biases from human society,\nresulting in discriminatory behavior in downstream applications. The debiasing\nprocess of existing fair methods is generally coupled with parameter\noptimization of GNNs. However, different downstream tasks may be associated\nwith different sensitive attributes in reality, directly employing existing\nmethods to improve the fairness of PGMs is inflexible and inefficient.\nMoreover, most of them lack a theoretical guarantee, i.e., provable lower\nbounds on the fairness of model predictions, which directly provides assurance\nin a practical scenario. To overcome these limitations, we propose a novel\nadapter-tuning framework that endows pre-trained graph models with provable\nfairness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains\na parameter-efficient adapter to flexibly improve the fairness of PGMs in\ndownstream tasks. Specifically, we design a sensitive semantic augmenter on\nnode representations, to extend the node representations with different\nsensitive attribute semantics for each node. The extended representations will\nbe used to further train an adapter, to prevent the propagation of sensitive\nattribute semantics from PGMs to task predictions. Furthermore, with GraphPAR,\nwe quantify whether the fairness of each node is provable, i.e., predictions\nare always fair within a certain range of sensitive attribute semantics.\nExperimental evaluations on real-world datasets demonstrate that GraphPAR\nachieves state-of-the-art prediction performance and fairness on node\nclassification task. Furthermore, based on our GraphPAR, around 90\\% nodes have\nprovable fairness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WWW 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12161v2",
    "published_date": "2024-02-19 14:16:08 UTC",
    "updated_date": "2024-02-20 09:03:43 UTC"
  },
  {
    "arxiv_id": "2402.12151v2",
    "title": "Transformer-based Causal Language Models Perform Clustering",
    "authors": [
      "Xinbo Wu",
      "Lav R. Varshney"
    ],
    "abstract": "Even though large language models (LLMs) have demonstrated remarkable\ncapability in solving various natural language tasks, the capability of an LLM\nto follow human instructions is still a concern. Recent works have shown great\nimprovements in the instruction-following capability via additional training\nfor instruction-following tasks. However, the mechanisms responsible for\neffective instruction-following capabilities remain inadequately understood.\nHere, we introduce a simplified instruction-following task and use synthetic\ndatasets to analyze a Transformer-based causal language model. Our findings\nsuggest that the model learns task-specific information by clustering data\nwithin its hidden space, with this clustering process evolving dynamically\nduring learning. We also demonstrate how this phenomenon assists the model in\nhandling unseen instances, and validate our results in a more realistic\nsetting. Furthermore, we present inspired applications regarding pre-training\nand alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Added new experimental results and fixed some errors",
    "pdf_url": "http://arxiv.org/pdf/2402.12151v2",
    "published_date": "2024-02-19 14:02:31 UTC",
    "updated_date": "2024-03-03 20:06:24 UTC"
  },
  {
    "arxiv_id": "2402.12150v1",
    "title": "Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One",
    "authors": [
      "Tianlin Li",
      "Xiaoyu Zhang",
      "Chao Du",
      "Tianyu Pang",
      "Qian Liu",
      "Qing Guo",
      "Chao Shen",
      "Yang Liu"
    ],
    "abstract": "The widespread adoption of large language models (LLMs) underscores the\nurgent need to ensure their fairness. However, LLMs frequently present dominant\nviewpoints while ignoring alternative perspectives from minority parties,\nresulting in potential biases. We hypothesize that these fairness-violating\nbehaviors occur because LLMs express their viewpoints using a human personality\nthat represents the majority of training data. In response to this, we validate\nthat prompting LLMs with specific roles can allow LLMs to express diverse\nviewpoints. Building on this insight and observation, we develop FairThinking,\na pipeline designed to automatically generate roles that enable LLMs to\narticulate diverse perspectives for fair expressions. To evaluate FairThinking,\nwe create a dataset with a thousand items covering three fairness-related\ntopics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to\ndemonstrate its superior performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2; J.4"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12150v1",
    "published_date": "2024-02-19 14:02:22 UTC",
    "updated_date": "2024-02-19 14:02:22 UTC"
  },
  {
    "arxiv_id": "2402.14845v1",
    "title": "Purifying Large Language Models by Ensembling a Small Language Model",
    "authors": [
      "Tianlin Li",
      "Qian Liu",
      "Tianyu Pang",
      "Chao Du",
      "Qing Guo",
      "Yang Liu",
      "Min Lin"
    ],
    "abstract": "The emerging success of large language models (LLMs) heavily relies on\ncollecting abundant training data from external (untrusted) sources. Despite\nsubstantial efforts devoted to data cleaning and curation, well-constructed\nLLMs have been reported to suffer from copyright infringement, data poisoning,\nand/or privacy violations, which would impede practical deployment of LLMs. In\nthis study, we propose a simple and easily implementable method for purifying\nLLMs from the negative effects caused by uncurated data, namely, through\nensembling LLMs with benign and small language models (SLMs). Aside from\ntheoretical guarantees, we perform comprehensive experiments to empirically\nconfirm the efficacy of ensembling LLMs with SLMs, which can effectively\npreserve the performance of LLMs while mitigating issues such as copyright\ninfringement, data poisoning, and privacy violations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.14845v1",
    "published_date": "2024-02-19 14:00:39 UTC",
    "updated_date": "2024-02-19 14:00:39 UTC"
  },
  {
    "arxiv_id": "2402.12147v3",
    "title": "Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger Language Models",
    "authors": [
      "Vinay Setty"
    ],
    "abstract": "In this paper, we explore the challenges associated with establishing an\nend-to-end fact-checking pipeline in a real-world context, covering over 90\nlanguages. Our real-world experimental benchmarks demonstrate that fine-tuning\nTransformer models specifically for fact-checking tasks, such as claim\ndetection and veracity prediction, provide superior performance over large\nlanguage models (LLMs) like GPT-4, GPT-3.5-Turbo, and Mistral-7b. However, we\nillustrate that LLMs excel in generative tasks such as question decomposition\nfor evidence retrieval. Through extensive evaluation, we show the efficacy of\nfine-tuned models for fact-checking in a multilingual setting and complex\nclaims that include numerical quantities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in SIGIR 2024 (industry track)",
    "pdf_url": "http://arxiv.org/pdf/2402.12147v3",
    "published_date": "2024-02-19 14:00:35 UTC",
    "updated_date": "2024-04-30 08:56:18 UTC"
  },
  {
    "arxiv_id": "2402.12146v3",
    "title": "Enabling Weak LLMs to Judge Response Reliability via Meta Ranking",
    "authors": [
      "Zijun Liu",
      "Boqun Kou",
      "Peng Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Yang Liu"
    ],
    "abstract": "Despite the strong performance of large language models (LLMs) across a wide\nrange of tasks, they still have reliability issues. Previous studies indicate\nthat strong LLMs like GPT-4-turbo excel in evaluating the reliability of\nresponses from LLMs, but face efficiency and local deployment issues. Thus, to\nenable weak LLMs to effectively assess the reliability of LLM responses, we\npropose a novel cross-query-comparison-based method called $\\textit{Meta\nRanking}$ (MR). Unlike previous few-shot methods that solely based on\nin-context learning capabilities in LLMs, MR assesses reliability by pairwisely\nranking the target query-response pair with multiple reference query-response\npairs. We found that MR is highly effective in error detection for LLM\nresponses, where weak LLMs, such as Phi-2, could surpass strong baselines like\nGPT-3.5-turbo, requiring only five reference samples and significantly\nimproving efficiency. We further demonstrate that MR can enhance strong LLMs'\nperformance in two practical applications: model cascading and instruction\ntuning. In model cascading, we combine open- and closed-source LLMs to achieve\nperformance comparable to GPT-4-turbo with lower costs. In instruction tuning,\nwe use MR for iterative training data filtering, significantly reducing data\nprocessing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with\nfewer training tokens. These results underscore the high potential of MR in\nboth efficiency and effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint, under review. 28 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.12146v3",
    "published_date": "2024-02-19 13:57:55 UTC",
    "updated_date": "2024-05-31 03:25:42 UTC"
  },
  {
    "arxiv_id": "2402.13288v1",
    "title": "Training Table Question Answering via SQL Query Decomposition",
    "authors": [
      "Raphaël Mouravieff",
      "Benjamin Piwowarski",
      "Sylvain Lamprier"
    ],
    "abstract": "Table Question-Answering involves both understanding the natural language\nquery and grounding it in the context of the input table to extract the\nrelevant information. In this context, many methods have highlighted the\nbenefits of intermediate pre-training from SQL queries. However, while most\napproaches aim at generating final answers from inputs directly, we claim that\nthere is better to do with SQL queries during training. By learning to imitate\na restricted portion of SQL-like algebraic operations, we show that their\nexecution flow provides intermediate supervision steps that allow increased\ngeneralization and structural reasoning compared with classical approaches of\nthe field. Our study bridges the gap between semantic parsing and direct\nanswering methods and provides useful insights regarding what types of\noperations should be predicted by a generative architecture or be preferably\nexecuted by an external algorithm.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13288v1",
    "published_date": "2024-02-19 13:56:16 UTC",
    "updated_date": "2024-02-19 13:56:16 UTC"
  },
  {
    "arxiv_id": "2402.12422v2",
    "title": "Simulacra as Conscious Exotica",
    "authors": [
      "Murray Shanahan"
    ],
    "abstract": "The advent of conversational agents with increasingly human-like behaviour\nthrows old philosophical questions into new light. Does it, or could it, ever\nmake sense to speak of AI agents built out of generative language models in\nterms of consciousness, given that they are \"mere\" simulacra of human\nbehaviour, and that what they do can be seen as \"merely\" role play? Drawing on\nthe later writings of Wittgenstein, this paper attempts to tackle this question\nwhile avoiding the pitfalls of dualistic thinking.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12422v2",
    "published_date": "2024-02-19 13:53:10 UTC",
    "updated_date": "2024-07-11 15:42:47 UTC"
  },
  {
    "arxiv_id": "2402.12132v1",
    "title": "SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding",
    "authors": [
      "Ruiyi Yang",
      "Flora D. Salim",
      "Hao Xue"
    ],
    "abstract": "Knowledge graphs (KGs) have been increasingly employed for link prediction\nand recommendation using real-world datasets. However, the majority of current\nmethods rely on static data, neglecting the dynamic nature and the hidden\nspatio-temporal attributes of real-world scenarios. This often results in\nsuboptimal predictions and recommendations. Although there are effective\nspatio-temporal inference methods, they face challenges such as scalability\nwith large datasets and inadequate semantic understanding, which impede their\nperformance. To address these limitations, this paper introduces a novel\nframework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing\nand exploring spatio-temporal KGs. To integrate spatial and temporal data into\nKGs, our framework exploited through a new 3-step embedding method. Output\nembeddings can be used for future temporal sequence prediction and spatial\ninformation recommendation, providing valuable insights for various\napplications such as retail sales forecasting and traffic volume prediction.\nOur framework offers a simple but comprehensive way to understand the\nunderlying patterns and trends in dynamic KG, thereby enhancing the accuracy of\npredictions and the relevance of recommendations. This work paves the way for\nmore effective utilization of spatio-temporal data in KGs, with potential\nimpacts across a wide range of sectors.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "for Web conf 2024. 8 pages context",
    "pdf_url": "http://arxiv.org/pdf/2402.12132v1",
    "published_date": "2024-02-19 13:28:43 UTC",
    "updated_date": "2024-02-19 13:28:43 UTC"
  },
  {
    "arxiv_id": "2402.12121v2",
    "title": "IRR: Image Review Ranking Framework for Evaluating Vision-Language Models",
    "authors": [
      "Kazuki Hayashi",
      "Kazuma Onishi",
      "Toma Suzuki",
      "Yusuke Ide",
      "Seiji Gobara",
      "Shigeki Saito",
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Katsuhiko Hayashi",
      "Taro Watanabe"
    ],
    "abstract": "Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "18pages, Accepted at COLING25",
    "pdf_url": "http://arxiv.org/pdf/2402.12121v2",
    "published_date": "2024-02-19 13:16:10 UTC",
    "updated_date": "2024-12-16 16:09:47 UTC"
  },
  {
    "arxiv_id": "2402.12118v1",
    "title": "DualView: Data Attribution from the Dual Perspective",
    "authors": [
      "Galip Ümit Yolcu",
      "Thomas Wiegand",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ],
    "abstract": "Local data attribution (or influence estimation) techniques aim at estimating\nthe impact that individual data points seen during training have on particular\npredictions of an already trained Machine Learning model during test time.\nPrevious methods either do not perform well consistently across different\nevaluation criteria from literature, are characterized by a high computational\ndemand, or suffer from both. In this work we present DualView, a novel method\nfor post-hoc data attribution based on surrogate modelling, demonstrating both\nhigh computational efficiency, as well as good evaluation results. With a focus\non neural networks, we evaluate our proposed technique using suitable\nquantitative evaluation strategies from the literature against related\nprincipal local data attribution methods. We find that DualView requires\nconsiderably lower computational resources than other methods, while\ndemonstrating comparable performance to competing approaches across evaluation\nmetrics. Futhermore, our proposed method produces sparse explanations, where\nsparseness can be tuned via a hyperparameter. Finally, we showcase that with\nDualView, we can now render explanations from local data attributions\ncompatible with established local feature attribution methods: For each\nprediction on (test) data points explained in terms of impactful samples from\nthe training set, we are able to compute and visualize how the prediction on\n(test) sample relates to each influential training sample in terms of features\nrecognized and by the model. We provide an Open Source implementation of\nDualView online, together with implementations for all other local data\nattribution methods we compare against, as well as the metrics reported here,\nfor full reproducibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12118v1",
    "published_date": "2024-02-19 13:13:16 UTC",
    "updated_date": "2024-02-19 13:13:16 UTC"
  },
  {
    "arxiv_id": "2402.12102v1",
    "title": "Is It a Free Lunch for Removing Outliers during Pretraining?",
    "authors": [
      "Baohao Liao",
      "Christof Monz"
    ],
    "abstract": "With the growing size of large language models, the role of quantization\nbecomes increasingly significant. However, outliers present in weights or\nactivations notably influence the performance of quantized models. Recently,\n\\citet{qtransformer} introduced a novel softmax function aimed at pretraining\nmodels in an outlier-free manner, thereby enhancing their suitability for\nquantization. Interestingly, we observed that such an approach leads to\nperformance degradation in full precision. Building on this insight, we enhance\nthe method by ensuring its normalization is invariant to sequence length, a\ncrucial factor for bridging the gap between pretraining and fine-tuning.\nMoreover, this improved method also facilitates successful pretraining of\ncausal language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2402.12102v1",
    "published_date": "2024-02-19 12:45:52 UTC",
    "updated_date": "2024-02-19 12:45:52 UTC"
  },
  {
    "arxiv_id": "2402.12100v1",
    "title": "Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation",
    "authors": [
      "Yi Liu",
      "Guowei Yang",
      "Gelei Deng",
      "Feiyue Chen",
      "Yuqi Chen",
      "Ling Shi",
      "Tianwei Zhang",
      "Yang Liu"
    ],
    "abstract": "With the prevalence of text-to-image generative models, their safety becomes\na critical concern. adversarial testing techniques have been developed to probe\nwhether such models can be prompted to produce Not-Safe-For-Work (NSFW)\ncontent. However, existing solutions face several challenges, including low\nsuccess rate and inefficiency. We introduce Groot, the first automated\nframework leveraging tree-based semantic transformation for adversarial testing\nof text-to-image models. Groot employs semantic decomposition and sensitive\nelement drowning strategies in conjunction with LLMs to systematically refine\nadversarial prompts. Our comprehensive evaluation confirms the efficacy of\nGroot, which not only exceeds the performance of current state-of-the-art\napproaches but also achieves a remarkable success rate (93.66%) on leading\ntext-to-image models such as DALL-E 3 and Midjourney.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12100v1",
    "published_date": "2024-02-19 12:31:56 UTC",
    "updated_date": "2024-02-19 12:31:56 UTC"
  },
  {
    "arxiv_id": "2402.12098v1",
    "title": "Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization",
    "authors": [
      "Abhishek Kuriyal",
      "Vaibhav Kumar"
    ],
    "abstract": "Semantic Segmentation (SS) of LiDAR point clouds is essential for many\napplications, such as urban planning and autonomous driving. While much\nprogress has been made in interpreting SS predictions for images, interpreting\npoint cloud SS predictions remains a challenge. This paper introduces pGS-CAM,\na novel gradient-based method for generating saliency maps in neural network\nactivation layers. Inspired by Grad-CAM, which uses gradients to highlight\nlocal importance, pGS-CAM is robust and effective on a variety of datasets\n(SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures\n(KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates\nthe feature learning in intermediate activations of SS architectures by\nhighlighting the contribution of each point. This allows us to better\nunderstand how SS models make their predictions and identify potential areas\nfor improvement. Relevant codes are available at\nhttps://github.com/geoai4cities/pGS-CAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12098v1",
    "published_date": "2024-02-19 12:27:39 UTC",
    "updated_date": "2024-02-19 12:27:39 UTC"
  },
  {
    "arxiv_id": "2402.13287v1",
    "title": "Manipulating hidden-Markov-model inferences by corrupting batch data",
    "authors": [
      "William N. Caballero",
      "Jose Manuel Camacho",
      "Tahir Ekin",
      "Roi Naveiro"
    ],
    "abstract": "Time-series models typically assume untainted and legitimate streams of data.\nHowever, a self-interested adversary may have incentive to corrupt this data,\nthereby altering a decision maker's inference. Within the broader field of\nadversarial machine learning, this research provides a novel, probabilistic\nperspective toward the manipulation of hidden Markov model inferences via\ncorrupted data. In particular, we provision a suite of corruption problems for\nfiltering, smoothing, and decoding inferences leveraging an adversarial risk\nanalysis approach. Multiple stochastic programming models are set forth that\nincorporate realistic uncertainties and varied attacker objectives. Three\ngeneral solution methods are developed by alternatively viewing the problem\nfrom frequentist and Bayesian perspectives. The efficacy of each method is\nillustrated via extensive, empirical testing. The developed methods are\ncharacterized by their solution quality and computational effort, resulting in\na stratification of techniques across varying problem-instance architectures.\nThis research highlights the weaknesses of hidden Markov models under\nadversarial activity, thereby motivating the need for robustification\ntechniques to ensure their security.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "42 pages, 8 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.13287v1",
    "published_date": "2024-02-19 12:22:22 UTC",
    "updated_date": "2024-02-19 12:22:22 UTC"
  },
  {
    "arxiv_id": "2402.12091v1",
    "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
    "authors": [
      "Junbing Yan",
      "Chengyu Wang",
      "Jun Huang",
      "Wei Zhang"
    ],
    "abstract": "Over the past few years, the abilities of large language models (LLMs) have\nreceived extensive attention, which have performed exceptionally well in\ncomplicated scenarios such as logical reasoning and symbolic inference. A\nsignificant factor contributing to this progress is the benefit of in-context\nlearning and few-shot prompting. However, the reasons behind the success of\nsuch models using contextual reasoning have not been fully explored. Do LLMs\nhave understand logical rules to draw inferences, or do they ``guess'' the\nanswers by learning a type of probabilistic mapping through context? This paper\ninvestigates the reasoning capabilities of LLMs on two logical reasoning\ndatasets by using counterfactual methods to replace context text and modify\nlogical concepts. Based on our analysis, it is found that LLMs do not truly\nunderstand logical rules; rather, in-context learning has simply enhanced the\nlikelihood of these models arriving at the correct answers. If one alters\ncertain words in the context text or changes the concepts of logical terms, the\noutputs of LLMs can be significantly disrupted, leading to counter-intuitive\nresponses. This work provides critical insights into the limitations of LLMs,\nunderscoring the need for more robust mechanisms to ensure reliable logical\nreasoning in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12091v1",
    "published_date": "2024-02-19 12:12:35 UTC",
    "updated_date": "2024-02-19 12:12:35 UTC"
  },
  {
    "arxiv_id": "2402.12074v1",
    "title": "HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph",
    "authors": [
      "Yongquan He",
      "Peng Zhang",
      "Luchen Liu",
      "Qi Liang",
      "Wenyuan Zhang",
      "Chuang Zhang"
    ],
    "abstract": "In recent years, temporal knowledge graph (TKG) reasoning has received\nsignificant attention. Most existing methods assume that all timestamps and\ncorresponding graphs are available during training, which makes it difficult to\npredict future events. To address this issue, recent works learn to infer\nfuture events based on historical information. However, these methods do not\ncomprehensively consider the latent patterns behind temporal changes, to pass\nhistorical information selectively, update representations appropriately and\npredict events accurately. In this paper, we propose the Historical Information\nPassing (HIP) network to predict future events. HIP network passes information\nfrom temporal, structural and repetitive perspectives, which are used to model\nthe temporal evolution of events, the interactions of events at the same time\nstep, and the known events respectively. In particular, our method considers\nthe updating of relation representations and adopts three scoring functions\ncorresponding to the above dimensions. Experimental results on five benchmark\ndatasets show the superiority of HIP network, and the significant improvements\non Hits@1 prove that our method can more accurately predict what is going to\nhappen.",
    "categories": [
      "cs.AI",
      "I.2.4; I.2.6; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12074v1",
    "published_date": "2024-02-19 11:50:30 UTC",
    "updated_date": "2024-02-19 11:50:30 UTC"
  },
  {
    "arxiv_id": "2402.12071v3",
    "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
    "authors": [
      "Sahand Sabour",
      "Siyang Liu",
      "Zheyuan Zhang",
      "June M. Liu",
      "Jinfeng Zhou",
      "Alvionna S. Sunaryo",
      "Juanzi Li",
      "Tatia M. C. Lee",
      "Rada Mihalcea",
      "Minlie Huang"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for\nrobust, comprehensive, and challenging benchmarks. Yet, research on evaluating\ntheir Emotional Intelligence (EI) is considerably limited. Existing benchmarks\nhave two major shortcomings: first, they mainly focus on emotion recognition,\nneglecting essential EI capabilities such as emotion regulation and thought\nfacilitation through emotion understanding; second, they are primarily\nconstructed from existing datasets, which include frequent patterns, explicit\ninformation, and annotation errors, leading to unreliable evaluation. We\npropose EmoBench, a benchmark that draws upon established psychological\ntheories and proposes a comprehensive definition for machine EI, including\nEmotional Understanding and Emotional Application. EmoBench includes a set of\n400 hand-crafted questions in English and Chinese, which are meticulously\ndesigned to require thorough reasoning and understanding. Our findings reveal a\nconsiderable gap between the EI of existing LLMs and the average human,\nhighlighting a promising direction for future research. Our code and data are\npublicly available at https://github.com/Sahandfer/EmoBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2402.12071v3",
    "published_date": "2024-02-19 11:48:09 UTC",
    "updated_date": "2024-07-17 05:30:58 UTC"
  },
  {
    "arxiv_id": "2402.12065v2",
    "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
    "authors": [
      "Yuxuan Yue",
      "Zhihang Yuan",
      "Haojie Duanmu",
      "Sifan Zhou",
      "Jianlong Wu",
      "Liqiang Nie"
    ],
    "abstract": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial memory requirements and the computational demands of\nauto-regressive text generation process. This paper addresses these challenges\nby focusing on the quantization of LLMs, a technique that reduces memory\nconsumption by converting model parameters and activations into low-bit\nintegers. We critically analyze the existing quantization approaches,\nidentifying their limitations in balancing the accuracy and efficiency of the\nquantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ\nframework especially designed for quantizing weights and the key/value (KV)\ncache of LLMs. Specifically, we incorporates past-only quantization to improve\nthe computation of attention. Additionally, we introduce two-dimensional\nquantization strategy to handle the distribution of KV cache, along with a\ncross-block reconstruction regularization for parameter optimization.\nExperiments show that WKVQuant achieves almost comparable memory savings to\nweight-activation quantization, while also approaching the performance of\nweight-only quantization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Frist work to exclusively quantize weight and Key/Value cache for\n  large language models",
    "pdf_url": "http://arxiv.org/pdf/2402.12065v2",
    "published_date": "2024-02-19 11:33:21 UTC",
    "updated_date": "2024-02-20 08:48:24 UTC"
  },
  {
    "arxiv_id": "2402.12062v4",
    "title": "Causal Equal Protection as Algorithmic Fairness",
    "authors": [
      "Marcello Di Bello",
      "Nicolò Cangiotti",
      "Michele Loi"
    ],
    "abstract": "By combining the philosophical literature on statistical evidence and the\ninterdisciplinary literature on algorithmic fairness, we revisit recent\nobjections against classification parity in light of causal analyses of\nalgorithmic fairness and the distinction between predictive and diagnostic\nevidence. We focus on trial proceedings as a black-box classification algorithm\nin which defendants are sorted into two groups by convicting or acquitting\nthem. We defend a novel principle, causal equal protection, that combines\nclassification parity with the causal approach. In the do-calculus, causal\nequal protection requires that individuals should not be subject to uneven\nrisks of classification error because of their protected or socially salient\ncharacteristics. The explicit use of protected characteristics, however, may be\nrequired if it equalizes these risks.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12062v4",
    "published_date": "2024-02-19 11:30:00 UTC",
    "updated_date": "2025-02-05 11:33:07 UTC"
  },
  {
    "arxiv_id": "2402.12061v2",
    "title": "All Language Models Large and Small",
    "authors": [
      "Zhixun Chen",
      "Yali Du",
      "David Mguni"
    ],
    "abstract": "Many leading language models (LMs) use high-intensity computational resources\nboth during training and execution. This poses the challenge of lowering\nresource costs for deployment and faster execution of decision-making tasks\namong others. We introduce a novel plug-and-play LM framework named Language\nOptimising Network Distribution (LONDI) framework. LONDI learns to selectively\nemploy large LMs only where complex decision-making and reasoning are required\nwhile using low-resource LMs (i.e. LMs require less GPU usage, but may not be\nable to solve the problem alone) everywhere else. LONDI consists of a system of\ntwo (off-)policy networks, an LM, a large LM (LLM), and a reinforcement\nlearning module that uses switching controls to quickly learn which system\nstates to call the LLM. We then introduce a variant of LONDI that maintains\nbudget constraints on LLM calls and hence its resource usage. Theoretically, we\nprove LONDI learns the subset of system states to activate the LLM required to\nsolve the task. We then prove that LONDI converges to optimal solutions while\nalso preserving budgetary constraints on LLM calls almost surely enabling it to\nsolve various tasks while significantly lowering computational costs. We test\nLONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and\ndemonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs\nwhile reducing GPU usage by up to 30%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12061v2",
    "published_date": "2024-02-19 11:28:20 UTC",
    "updated_date": "2024-06-05 15:08:28 UTC"
  },
  {
    "arxiv_id": "2402.12042v2",
    "title": "Linear bandits with polylogarithmic minimax regret",
    "authors": [
      "Josep Lumbreras",
      "Marco Tomamichel"
    ],
    "abstract": "We study a noise model for linear stochastic bandits for which the\nsubgaussian noise parameter vanishes linearly as we select actions on the unit\nsphere closer and closer to the unknown vector. We introduce an algorithm for\nthis problem that exhibits a minimax regret scaling as $\\log^3(T)$ in the time\nhorizon $T$, in stark contrast the square root scaling of this regret for\ntypical bandit algorithms. Our strategy, based on weighted least-squares\nestimation, achieves the eigenvalue relation $\\lambda_{\\min} ( V_t ) = \\Omega\n(\\sqrt{\\lambda_{\\max}(V_t ) })$ for the design matrix $V_t$ at each time step\n$t$ through geometrical arguments that are independent of the noise model and\nmight be of independent interest. This allows us to tightly control the\nexpected regret in each time step to be of the order $O(\\frac1{t})$, leading to\nthe logarithmic scaling of the cumulative regret.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "39 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12042v2",
    "published_date": "2024-02-19 10:56:47 UTC",
    "updated_date": "2024-05-29 10:58:25 UTC"
  },
  {
    "arxiv_id": "2402.12035v2",
    "title": "Class-incremental Learning for Time Series: Benchmark and Evaluation",
    "authors": [
      "Zhongzheng Qiao",
      "Quang Pham",
      "Zhen Cao",
      "Hoang H Le",
      "P. N. Suganthan",
      "Xudong Jiang",
      "Ramasamy Savitha"
    ],
    "abstract": "Real-world environments are inherently non-stationary, frequently introducing\nnew classes over time. This is especially common in time series classification,\nsuch as the emergence of new disease classification in healthcare or the\naddition of new activities in human activity recognition. In such cases, a\nlearning system is required to assimilate novel classes effectively while\navoiding catastrophic forgetting of the old ones, which gives rise to the\nClass-incremental Learning (CIL) problem. However, despite the encouraging\nprogress in the image and language domains, CIL for time series data remains\nrelatively understudied. Existing studies suffer from inconsistent experimental\ndesigns, necessitating a comprehensive evaluation and benchmarking of methods\nacross a wide range of datasets. To this end, we first present an overview of\nthe Time Series Class-incremental Learning (TSCIL) problem, highlight its\nunique challenges, and cover the advanced methodologies. Further, based on\nstandardized settings, we develop a unified experimental framework that\nsupports the rapid development of new algorithms, easy integration of new\ndatasets, and standardization of the evaluation process. Using this framework,\nwe conduct a comprehensive evaluation of various generic and\ntime-series-specific CIL methods in both standard and privacy-sensitive\nscenarios. Our extensive experiments not only provide a standard baseline to\nsupport future research but also shed light on the impact of various design\nfactors such as normalization layers or memory budget thresholds. Codes are\navailable at https://github.com/zqiao11/TSCIL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by SIGKDD 2024 (ADS track). Codes available at\n  https://github.com/zqiao11/TSCIL",
    "pdf_url": "http://arxiv.org/pdf/2402.12035v2",
    "published_date": "2024-02-19 10:43:13 UTC",
    "updated_date": "2024-08-03 13:29:50 UTC"
  },
  {
    "arxiv_id": "2402.12026v3",
    "title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space",
    "authors": [
      "Zongru Wu",
      "Zhuosheng Zhang",
      "Pengzhou Cheng",
      "Gongshen Liu"
    ],
    "abstract": "Despite the notable success of language models (LMs) in various natural\nlanguage processing (NLP) tasks, the reliability of LMs is susceptible to\nbackdoor attacks. Prior research attempts to mitigate backdoor learning while\ntraining the LMs on the poisoned dataset, yet struggles against complex\nbackdoor attacks in real-world scenarios. In this paper, we investigate the\nlearning mechanisms of backdoor LMs in the frequency space by Fourier analysis.\nOur findings indicate that the backdoor mapping presented on the poisoned\ndatasets exhibits a more discernible inclination towards lower frequency\ncompared to clean mapping, resulting in the faster convergence of backdoor\nmapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation\n(MuScleLoRA), which deploys multiple radial scalings in the frequency space\nwith low-rank adaptation to the target model and further aligns the gradients\nwhen updating parameters. Through downscaling in the frequency space,\nMuScleLoRA encourages the model to prioritize the learning of relatively\nhigh-frequency clean mapping, consequently mitigating backdoor learning.\nExperimental results demonstrate that MuScleLoRA outperforms baselines\nsignificantly. Notably, MuScleLoRA reduces the average success rate of diverse\nbackdoor attacks to below 15\\% across multiple datasets and generalizes to\nvarious backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes\nare publicly available at https://github.com/ZrW00/MuScleLoRA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 (Long Paper. Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2402.12026v3",
    "published_date": "2024-02-19 10:34:48 UTC",
    "updated_date": "2024-06-02 10:28:13 UTC"
  },
  {
    "arxiv_id": "2402.12023v1",
    "title": "Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought",
    "authors": [
      "Yuying Du",
      "Xueyan Tang"
    ],
    "abstract": "Smart contracts, as a key component of blockchain technology, play a crucial\nrole in ensuring the automation of transactions and adherence to protocol\nrules. However, smart contracts are susceptible to security vulnerabilities,\nwhich, if exploited, can lead to significant asset losses. This study explores\nthe potential of enhancing smart contract security audits using the GPT-4\nmodel. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark\nvulnerability library, containing 732 vulnerabilities, and compared it with\nfive other vulnerability detection tools to evaluate GPT-4's ability to\nidentify seven common types of vulnerabilities. Moreover, we assessed GPT-4's\nperformance in code parsing and vulnerability capture by simulating a\nprofessional auditor's auditing process using CoT(Chain of Thought) prompts\nbased on the audit reports of eight groups of smart contracts. We also\nevaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs). Through\nexperimentation, we found that GPT-4 performed poorly in detecting smart\ncontract vulnerabilities, with a high Precision of 96.6%, but a low Recall of\n37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities\nduring detection. Meanwhile, it demonstrated good contract code parsing\ncapabilities, with an average comprehensive score of 6.5, capable of\nidentifying the background information and functional relationships of smart\ncontracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4\nhas significant potential application in PoC writing. These experimental\nresults indicate that GPT-4 lacks the ability to detect smart contract\nvulnerabilities effectively, but its performance in contract code parsing and\nPoC writing demonstrates its significant potential as an auxiliary tool in\nenhancing the efficiency and effectiveness of smart contract security audits.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "68",
      "I.2; J.6"
    ],
    "primary_category": "cs.CR",
    "comment": "21 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12023v1",
    "published_date": "2024-02-19 10:33:29 UTC",
    "updated_date": "2024-02-19 10:33:29 UTC"
  },
  {
    "arxiv_id": "2402.12010v1",
    "title": "Training Green AI Models Using Elite Samples",
    "authors": [
      "Mohammed Alswaitti",
      "Roberto Verdecchia",
      "Grégoire Danoy",
      "Pascal Bouvry",
      "Johnatan Pecero"
    ],
    "abstract": "The substantial increase in AI model training has considerable environmental\nimplications, mandating more energy-efficient and sustainable AI practices. On\nthe one hand, data-centric approaches show great potential towards training\nenergy-efficient AI models. On the other hand, instance selection methods\ndemonstrate the capability of training AI models with minimised training sets\nand negligible performance degradation. Despite the growing interest in both\ntopics, the impact of data-centric training set selection on energy efficiency\nremains to date unexplored. This paper presents an evolutionary-based sampling\nframework aimed at (i) identifying elite training samples tailored for datasets\nand model pairs, (ii) comparing model performance and energy efficiency gains\nagainst typical model training practice, and (iii) investigating the\nfeasibility of this framework for fostering sustainable model training\npractices. To evaluate the proposed framework, we conducted an empirical\nexperiment including 8 commonly used AI classification models and 25 publicly\navailable datasets. The results showcase that by considering 10% elite training\nsamples, the models' performance can show a 50% improvement and remarkable\nenergy savings of 98% compared to the common training practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12010v1",
    "published_date": "2024-02-19 10:03:46 UTC",
    "updated_date": "2024-02-19 10:03:46 UTC"
  },
  {
    "arxiv_id": "2402.12008v1",
    "title": "Cluster Metric Sensitivity to Irrelevant Features",
    "authors": [
      "Miles McCrory",
      "Spencer A. Thomas"
    ],
    "abstract": "Clustering algorithms are used extensively in data analysis for data\nexploration and discovery. Technological advancements lead to continually\ngrowth of data in terms of volume, dimensionality and complexity. This provides\ngreat opportunities in data analytics as the data can be interrogated for many\ndifferent purposes. This however leads challenges, such as identification of\nrelevant features for a given task. In supervised tasks, one can utilise a\nnumber of methods to optimise the input features for the task objective (e.g.\nclassification accuracy). In unsupervised problems, such tools are not readily\navailable, in part due to an inability to quantify feature relevance in\nunlabeled tasks. In this paper, we investigate the sensitivity of clustering\nperformance noisy uncorrelated variables iteratively added to baseline datasets\nwith well defined clusters. We show how different types of irrelevant variables\ncan impact the outcome of a clustering result from $k$-means in different ways.\nWe observe a resilience to very high proportions of irrelevant features for\nadjusted rand index (ARI) and normalised mutual information (NMI) when the\nirrelevant features are Gaussian distributed. For Uniformly distributed\nirrelevant features, we notice the resilience of ARI and NMI is dependent on\nthe dimensionality of the data and exhibits tipping points between high scores\nand near zero. Our results show that the Silhouette Coefficient and the\nDavies-Bouldin score are the most sensitive to irrelevant added features\nexhibiting large changes in score for comparably low proportions of irrelevant\nfeatures regardless of underlying distribution or data scaling. As such the\nSilhouette Coefficient and the Davies-Bouldin score are good candidates for\noptimising feature selection in unsupervised clustering tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12008v1",
    "published_date": "2024-02-19 10:02:00 UTC",
    "updated_date": "2024-02-19 10:02:00 UTC"
  },
  {
    "arxiv_id": "2402.12419v1",
    "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
    "authors": [
      "Song Guo",
      "Fan Wu",
      "Lei Zhang",
      "Xiawu Zheng",
      "Shengchuan Zhang",
      "Fei Chao",
      "Yiyu Shi",
      "Rongrong Ji"
    ],
    "abstract": "Existing methods for fine-tuning sparse LLMs often suffer from\nresource-intensive requirements and high retraining costs. Additionally, many\nfine-tuning methods often rely on approximations or heuristic optimization\nstrategies, which may lead to suboptimal solutions. To address these issues, we\npropose an efficient and fast framework for fine-tuning sparse LLMs based on\nminimizing reconstruction error. Our approach involves sampling a small dataset\nfor calibration and utilizing backpropagation to iteratively optimize\nblock-wise reconstruction error, on a block-by-block basis, aiming for optimal\nsolutions. Extensive experiments on various benchmarks consistently demonstrate\nthe superiority of our method over other baselines. For instance, on the\nWikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a\nperplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of\n75.14. Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a\nperplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the\nfine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,\nand the entire framework can be executed on a single 16GB GPU. The source code\nis available at https://github.com/sunggo/EBFT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12419v1",
    "published_date": "2024-02-19 09:55:32 UTC",
    "updated_date": "2024-02-19 09:55:32 UTC"
  },
  {
    "arxiv_id": "2402.12418v1",
    "title": "Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures",
    "authors": [
      "Akash Guna R. T",
      "Arnav Chavan",
      "Deepak Gupta"
    ],
    "abstract": "Conventional scaling of neural networks typically involves designing a base\nnetwork and growing different dimensions like width, depth, etc. of the same by\nsome predefined scaling factors. We introduce an automated scaling approach\nleveraging second-order loss landscape information. Our method is flexible\ntowards skip connections a mainstay in modern vision transformers. Our\ntraining-aware method jointly scales and trains transformers without additional\ntraining iterations. Motivated by the hypothesis that not all neurons need\nuniform depth complexity, our approach embraces depth heterogeneity. Extensive\nevaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10%\nparameter efficiency improvement over conventional scaling. Scaled networks\ndemonstrate superior performance upon training small scale datasets from\nscratch. We introduce the first intact scaling mechanism for vision\ntransformers, a step towards efficient model scaling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted At ICLR 2024 (Tiny Paper Track)",
    "pdf_url": "http://arxiv.org/pdf/2402.12418v1",
    "published_date": "2024-02-19 09:52:45 UTC",
    "updated_date": "2024-02-19 09:52:45 UTC"
  },
  {
    "arxiv_id": "2402.12001v1",
    "title": "A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions",
    "authors": [
      "Xiaxia Wang",
      "Gong Cheng"
    ],
    "abstract": "With the continuous growth of large Knowledge Graphs (KGs), extractive KG\nsummarization becomes a trending task. Aiming at distilling a compact subgraph\nwith condensed information, it facilitates various downstream KG-based tasks.\nIn this survey paper, we are among the first to provide a systematic overview\nof its applications and define a taxonomy for existing methods from its\ninterdisciplinary studies. Future directions are also laid out based on our\nextensive and comparative review.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 13 figures, submitted to the IJCAI 2024 Survey Track",
    "pdf_url": "http://arxiv.org/pdf/2402.12001v1",
    "published_date": "2024-02-19 09:49:53 UTC",
    "updated_date": "2024-02-19 09:49:53 UTC"
  },
  {
    "arxiv_id": "2402.11997v2",
    "title": "Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models",
    "authors": [
      "Himanshu Beniwal",
      "Dishant Patel",
      "Kowsik Nandagopan D",
      "Hritik Ladia",
      "Ankit Yadav",
      "Mayank Singh"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly ubiquitous, yet their ability\nto retain and reason about temporal information remains limited, hindering\ntheir application in real-world scenarios where understanding the sequential\nnature of events is crucial. Our study experiments with 12 state-of-the-art\nmodels (ranging from 2B to 70B+ parameters) on a novel numerical-temporal\ndataset, \\textbf{TempUN}, spanning from 10,000 BCE to 2100 CE, to uncover\nsignificant temporal retention and comprehension limitations. We propose six\nmetrics to assess three learning paradigms to enhance temporal knowledge\nacquisition. Our findings reveal that open-source models exhibit knowledge gaps\nmore frequently, suggesting a trade-off between limited knowledge and incorrect\nresponses. Additionally, various fine-tuning approaches significantly improved\nperformance, reducing incorrect outputs and impacting the identification of\n'information not available' in the generations. The associated dataset and code\nare available at (https://github.com/lingoiitgn/TempUN).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11997v2",
    "published_date": "2024-02-19 09:43:03 UTC",
    "updated_date": "2024-07-05 11:26:51 UTC"
  },
  {
    "arxiv_id": "2402.11984v1",
    "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
    "authors": [
      "Mingqing Xiao",
      "Qingyan Meng",
      "Zongpeng Zhang",
      "Di He",
      "Zhouchen Lin"
    ],
    "abstract": "Neuromorphic computing with spiking neural networks is promising for\nenergy-efficient artificial intelligence (AI) applications. However, different\nfrom humans who continually learn different tasks in a lifetime, neural network\nmodels suffer from catastrophic forgetting. How could neuronal operations solve\nthis problem is an important question for AI and neuroscience. Many previous\nstudies draw inspiration from observed neuroscience phenomena and propose\nepisodic replay or synaptic metaplasticity, but they are not guaranteed to\nexplicitly preserve knowledge for neuron populations. Other works focus on\nmachine learning methods with more mathematical grounding, e.g., orthogonal\nprojection on high dimensional spaces, but there is no neural correspondence\nfor neuromorphic computing. In this work, we develop a new method with neuronal\noperations based on lateral connections and Hebbian learning, which can protect\nknowledge by projecting activity traces of neurons into an orthogonal subspace\nso that synaptic weight update will not interfere with old tasks. We show that\nHebbian and anti-Hebbian learning on recurrent lateral connections can\neffectively extract the principal subspace of neural activities and enable\northogonal projection. This provides new insights into how neural circuits and\nHebbian learning can help continual learning, and also how the concept of\northogonal projection can be realized in neuronal systems. Our method is also\nflexible to utilize arbitrary training methods based on presynaptic\nactivities/traces. Experiments show that our method consistently solves\nforgetting for spiking neural networks with nearly zero forgetting under\nvarious supervised training methods with different error propagation\napproaches, and outperforms previous approaches under various settings. Our\nmethod can pave a solid path for building continual neuromorphic computing\nsystems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted by ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11984v1",
    "published_date": "2024-02-19 09:29:37 UTC",
    "updated_date": "2024-02-19 09:29:37 UTC"
  },
  {
    "arxiv_id": "2402.14843v1",
    "title": "Text Diffusion with Reinforced Conditioning",
    "authors": [
      "Yuxuan Liu",
      "Tianchi Yang",
      "Shaohan Huang",
      "Zihan Zhang",
      "Haizhen Huang",
      "Furu Wei",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ],
    "abstract": "Diffusion models have demonstrated exceptional capability in generating\nhigh-quality images, videos, and audio. Due to their adaptiveness in iterative\nrefinement, they provide a strong potential for achieving better\nnon-autoregressive sequence generation. However, existing text diffusion models\nstill fall short in their performance due to a challenge in handling the\ndiscreteness of language. This paper thoroughly analyzes text diffusion models\nand uncovers two significant limitations: degradation of self-conditioning\nduring training and misalignment between training and sampling. Motivated by\nour findings, we propose a novel Text Diffusion model called TREC, which\nmitigates the degradation with Reinforced Conditioning and the misalignment by\nTime-Aware Variance Scaling. Our extensive experiments demonstrate the\ncompetitiveness of TREC against autoregressive, non-autoregressive, and\ndiffusion baselines. Moreover, qualitative analysis shows its advanced ability\nto fully utilize the diffusion process in refining samples.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.14843v1",
    "published_date": "2024-02-19 09:24:02 UTC",
    "updated_date": "2024-02-19 09:24:02 UTC"
  },
  {
    "arxiv_id": "2402.13284v2",
    "title": "Structure Guided Large Language Model for SQL Generation",
    "authors": [
      "Qinggang Zhang",
      "Junnan Dong",
      "Hao Chen",
      "Wentao Li",
      "Feiran Huang",
      "Xiao Huang"
    ],
    "abstract": "Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13284v2",
    "published_date": "2024-02-19 09:07:59 UTC",
    "updated_date": "2024-03-27 14:30:44 UTC"
  },
  {
    "arxiv_id": "2402.11963v1",
    "title": "Imbalance in Regression Datasets",
    "authors": [
      "Daniel Kowatsch",
      "Nicolas M. Müller",
      "Kilian Tscharke",
      "Philip Sperl",
      "Konstantin Bötinger"
    ],
    "abstract": "For classification, the problem of class imbalance is well known and has been\nextensively studied. In this paper, we argue that imbalance in regression is an\nequally important problem which has so far been overlooked: Due to under- and\nover-representations in a data set's target distribution, regressors are prone\nto degenerate to naive models, systematically neglecting uncommon training data\nand over-representing targets seen often during training. We analyse this\nproblem theoretically and use resulting insights to develop a first definition\nof imbalance in regression, which we show to be a generalisation of the\ncommonly employed imbalance measure in classification. With this, we hope to\nturn the spotlight on the overlooked problem of imbalance in regression and to\nprovide common ground for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11963v1",
    "published_date": "2024-02-19 09:06:26 UTC",
    "updated_date": "2024-02-19 09:06:26 UTC"
  },
  {
    "arxiv_id": "2402.11960v1",
    "title": "DB-LLM: Accurate Dual-Binarization for Efficient LLMs",
    "authors": [
      "Hong Chen",
      "Chengtao Lv",
      "Liang Ding",
      "Haotong Qin",
      "Xiabin Zhou",
      "Yifu Ding",
      "Xuebo Liu",
      "Min Zhang",
      "Jinyang Guo",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, while the expensive memory and computation consumption\nimpede their practical deployment. Quantization emerges as one of the most\neffective methods for improving the computational efficiency of LLMs. However,\nexisting ultra-low-bit quantization always causes severe accuracy drops. In\nthis paper, we empirically relieve the micro and macro characteristics of\nultra-low bit quantization and present a novel Dual-Binarization method for\nLLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage\nof 2-bit-width and the efficiency advantage of binarization into account,\nintroducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized\nweights into two independent sets of binaries, FDB ensures the accuracy of\nrepresentations and introduces flexibility, utilizing the efficient bitwise\noperations of binarization while retaining the inherent high sparsity of\nultra-low bit quantization. For the macro-level, we find the distortion that\nexists in the prediction of LLM after quantization, which is specified as the\ndeviations related to the ambiguity of samples. We propose the Deviation-Aware\nDistillation (DAD) method, enabling the model to focus differently on various\nsamples. Comprehensive experiments show that our DB-LLM not only significantly\nsurpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization\n(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional\n20\\% reduction in computational consumption compared to the SOTA method under\nthe same bit-width. Our code will be released soon.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11960v1",
    "published_date": "2024-02-19 09:04:30 UTC",
    "updated_date": "2024-02-19 09:04:30 UTC"
  },
  {
    "arxiv_id": "2402.11955v1",
    "title": "Analysis of Multidomain Abstractive Summarization Using Salience Allocation",
    "authors": [
      "Tohida Rehman",
      "Raghubir Bose",
      "Soumik Dey",
      "Samiran Chattopadhyay"
    ],
    "abstract": "This paper explores the realm of abstractive text summarization through the\nlens of the SEASON (Salience Allocation as Guidance for Abstractive\nSummarizatiON) technique, a model designed to enhance summarization by\nleveraging salience allocation techniques. The study evaluates SEASON's\nefficacy by comparing it with prominent models like BART, PEGASUS, and\nProphetNet, all fine-tuned for various text summarization tasks. The assessment\nis conducted using diverse datasets including CNN/Dailymail, SAMSum, and\nFinancial-news based Event-Driven Trading (EDT), with a specific focus on a\nfinancial dataset containing a substantial volume of news articles from\n2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as\nROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these\nmodels fine-tuned for generating abstractive summaries. The analysis of these\nmetrics offers a thorough insight into the strengths and weaknesses\ndemonstrated by each model in summarizing news dataset, dialogue dataset and\nfinancial text dataset. The results presented in this paper not only contribute\nto the evaluation of the SEASON model's effectiveness but also illuminate the\nintricacies of salience allocation techniques across various types of datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 1 figure, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.11955v1",
    "published_date": "2024-02-19 08:52:12 UTC",
    "updated_date": "2024-02-19 08:52:12 UTC"
  },
  {
    "arxiv_id": "2402.11948v1",
    "title": "Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model",
    "authors": [
      "Jialiang Wang",
      "Weiling Li",
      "Yurong Zhong",
      "Xin Luo"
    ],
    "abstract": "Interactions among large number of entities is naturally high-dimensional and\nincomplete (HDI) in many big data related tasks. Behavioral characteristics of\nusers are hidden in these interactions, hence, effective representation of the\nHDI data is a fundamental task for understanding user behaviors. Latent factor\nanalysis (LFA) model has proven to be effective in representing HDI data. The\nperformance of an LFA model relies heavily on its training process, which is a\nnon-convex optimization. It has been proven that incorporating local curvature\nand preprocessing gradients during its training process can lead to superior\nperformance compared to LFA models built with first-order family methods.\nHowever, with the escalation of data volume, the feasibility of second-order\nalgorithms encounters challenges. To address this pivotal issue, this paper\nproposes a mini-block diagonal hessian-free (Mini-Hes) optimization for\nbuilding an LFA model. It leverages the dominant diagonal blocks in the\ngeneralized Gauss-Newton matrix based on the analysis of the Hessian matrix of\nLFA model and serves as an intermediary strategy bridging the gap between\nfirst-order and second-order optimization methods. Experiment results indicate\nthat, with Mini-Hes, the LFA model outperforms several state-of-the-art models\nin addressing missing data estimation task on multiple real HDI datasets from\nrecommender system. (The source code of Mini-Hes is available at\nhttps://github.com/Goallow/Mini-Hes)",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.11948v1",
    "published_date": "2024-02-19 08:43:00 UTC",
    "updated_date": "2024-02-19 08:43:00 UTC"
  },
  {
    "arxiv_id": "2402.12417v1",
    "title": "Predicting trucking accidents with truck drivers 'safety climate perception across companies: A transfer learning approach",
    "authors": [
      "Kailai Sun",
      "Tianxiang Lan",
      "Say Hong Kam",
      "Yang Miang Goh",
      "Yueng-Hsiang Huang"
    ],
    "abstract": "There is a rising interest in using artificial intelligence (AI)-powered\nsafety analytics to predict accidents in the trucking industry. Companies may\nface the practical challenge, however, of not having enough data to develop\ngood safety analytics models. Although pretrained models may offer a solution\nfor such companies, existing safety research using transfer learning has mostly\nfocused on computer vision and natural language processing, rather than\naccident analytics. To fill the above gap, we propose a pretrain-then-fine-tune\ntransfer learning approach to help any company leverage other companies' data\nto develop AI models for a more accurate prediction of accident risk. We also\ndevelop SafeNet, a deep neural network algorithm for classification tasks\nsuitable for accident prediction. Using the safety climate survey data from\nseven trucking companies with different data sizes, we show that our proposed\napproach results in better model performance compared to training the model\nfrom scratch using only the target company's data. We also show that for the\ntransfer learning model to be effective, the pretrained model should be\ndeveloped with larger datasets from diverse sources. The trucking industry may,\nthus, consider pooling safety analytics data from a wide range of companies to\ndevelop pretrained models and share them within the industry for better\nknowledge and resource transfer. The above contributions point to the promise\nof advanced safety analytics to make the industry safer and more sustainable.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "submitted to journal: accident analysis and prevention",
    "pdf_url": "http://arxiv.org/pdf/2402.12417v1",
    "published_date": "2024-02-19 08:27:53 UTC",
    "updated_date": "2024-02-19 08:27:53 UTC"
  },
  {
    "arxiv_id": "2402.11934v1",
    "title": "Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text",
    "authors": [
      "Xiaoman Xu",
      "Xiangrun Li",
      "Taihang Wang",
      "Jianxiang Tian",
      "Ye Jiang"
    ],
    "abstract": "This paper presents the participation of team QUST in Task 8 SemEval 2024. We\nfirst performed data augmentation and cleaning on the dataset to enhance model\ntraining efficiency and accuracy. In the monolingual task, we evaluated\ntraditional deep-learning methods, multiscale positive-unlabeled framework\n(MPU), fine-tuning, adapters and ensemble methods. Then, we selected the\ntop-performing models based on their accuracy from the monolingual models and\nevaluated them in subtasks A and B. The final model construction employed a\nstacking ensemble that combined fine-tuning with MPU. Our system achieved 8th\n(scored 8th in terms of accuracy, officially ranked 13th) place in the official\ntest set in multilingual settings of subtask A. We release our system code\nat:https://github.com/warmth27/SemEval2024_QUST",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11934v1",
    "published_date": "2024-02-19 08:22:51 UTC",
    "updated_date": "2024-02-19 08:22:51 UTC"
  },
  {
    "arxiv_id": "2402.12416v3",
    "title": "Aligning Individual and Collective Objectives in Multi-Agent Cooperation",
    "authors": [
      "Yang Li",
      "Wenhao Zhang",
      "Jianhong Wang",
      "Shao Zhang",
      "Yali Du",
      "Ying Wen",
      "Wei Pan"
    ],
    "abstract": "Among the research topics in multi-agent learning, mixed-motive cooperation\nis one of the most prominent challenges, primarily due to the mismatch between\nindividual and collective goals. The cutting-edge research is focused on\nincorporating domain knowledge into rewards and introducing additional\nmechanisms to incentivize cooperation. However, these approaches often face\nshortcomings such as the effort on manual design and the absence of theoretical\ngroundings. To close this gap, we model the mixed-motive game as a\ndifferentiable game for the ease of illuminating the learning dynamics towards\ncooperation. More detailed, we introduce a novel optimization method named\n\\textbf{\\textit{A}}ltruistic \\textbf{\\textit{G}}radient\n\\textbf{\\textit{A}}djustment (\\textbf{\\textit{AgA}}) that employs gradient\nadjustments to progressively align individual and collective objectives.\nFurthermore, we theoretically prove that AgA effectively attracts gradients to\nstable fixed points of the collective objective while considering individual\ninterests, and we validate these claims with empirical evidence. We evaluate\nthe effectiveness of our algorithm AgA through benchmark environments for\ntesting mixed-motive collaboration with small-scale agents such as the\ntwo-player public good game and the sequential social dilemma games, Cleanup\nand Harvest, as well as our self-developed large-scale environment in the game\nStarCraft II.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "20 pages; Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12416v3",
    "published_date": "2024-02-19 08:18:53 UTC",
    "updated_date": "2024-10-22 18:10:01 UTC"
  },
  {
    "arxiv_id": "2402.11925v1",
    "title": "Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching",
    "authors": [
      "Sujin Kook",
      "Won-Yong Shin",
      "Seong-Lyun Kim",
      "Seung-Woo Ko"
    ],
    "abstract": "The vision of pervasive artificial intelligence (AI) services can be realized\nby training an AI model on time using real-time data collected by internet of\nthings (IoT) devices. To this end, IoT devices require offloading their data to\nan edge server in proximity. However, transmitting high-dimensional and\nvoluminous data from energy-constrained IoT devices poses a significant\nchallenge. To address this limitation, we propose a novel offloading\narchitecture, called joint data deepening-and-prefetching (JD2P), which is\nfeature-by-feature offloading comprising two key techniques. The first one is\ndata deepening, where each data sample's features are sequentially offloaded in\nthe order of importance determined by the data embedding technique such as\nprinciple component analysis (PCA). Offloading is terminated once the already\ntransmitted features are sufficient for accurate data classification, resulting\nin a reduction in the amount of transmitted data. The criteria to offload data\nare derived for binary and multi-class classifiers, which are designed based on\nsupport vector machine (SVM) and deep neural network (DNN), respectively. The\nsecond one is data prefetching, where some features potentially required in the\nfuture are offloaded in advance, thus achieving high efficiency via precise\nprediction and parameter optimization. We evaluate the effectiveness of JD2P\nthrough experiments using the MNIST dataset, and the results demonstrate its\nsignificant reduction in expected energy consumption compared to several\nbenchmarks without degrading learning accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted for publication in IEEE Transactions on Wireless\n  Communications. arXiv admin note: text overlap with arXiv:2211.07146",
    "pdf_url": "http://arxiv.org/pdf/2402.11925v1",
    "published_date": "2024-02-19 08:12:47 UTC",
    "updated_date": "2024-02-19 08:12:47 UTC"
  },
  {
    "arxiv_id": "2404.07211v1",
    "title": "A real-time Artificial Intelligence system for learning Sign Language",
    "authors": [
      "Elisa Cabana"
    ],
    "abstract": "A primary challenge for the deaf and hearing-impaired community stems from\nthe communication gap with the hearing society, which can greatly impact their\ndaily lives and result in social exclusion. To foster inclusivity in society,\nour endeavor focuses on developing a cost-effective, resource-efficient, and\nopen technology based on Artificial Intelligence, designed to assist people in\nlearning and using Sign Language for communication. The analysis presented in\nthis research paper intends to enrich the recent academic scientific literature\non Sign Language solutions based on Artificial Intelligence, with a particular\nfocus on American Sign Language (ASL). This research has yielded promising\npreliminary results and serves as a basis for further development.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.07211v1",
    "published_date": "2024-02-19 08:03:07 UTC",
    "updated_date": "2024-02-19 08:03:07 UTC"
  },
  {
    "arxiv_id": "2402.11903v3",
    "title": "DiLA: Enhancing LLM Tool Learning with Differential Logic Layer",
    "authors": [
      "Yu Zhang",
      "Hui-Ling Zhen",
      "Zehua Pei",
      "Yingzhao Lian",
      "Lihao Yin",
      "Mingxuan Yuan",
      "Bei Yu"
    ],
    "abstract": "Considering the challenges faced by large language models (LLMs) in logical\nreasoning and planning, prior efforts have sought to augment LLMs with access\nto external solvers. While progress has been made on simple reasoning problems,\nsolving classical constraint satisfaction problems, such as the Boolean\nSatisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains\ndifficult for off-the-shelf solvers due to their intricate expressions and\nexponential search spaces. In this paper, we propose a novel differential logic\nlayer-aided language modeling (DiLA) approach, where logical constraints are\nintegrated into the forward and backward passes of a network layer, to provide\nanother option for LLM tool learning. In DiLA, LLM aims to transform the\nlanguage description to logic constraints and identify initial solutions of the\nhighest quality, while the differential logic layer focuses on iteratively\nrefining the LLM-prompted solution. Leveraging the logic layer as a bridge,\nDiLA enhances the logical reasoning ability of LLMs on a range of reasoning\nproblems encoded by Boolean variables, guaranteeing the efficiency and\ncorrectness of the solution process. We evaluate the performance of DiLA on two\nclassic reasoning problems and empirically demonstrate its consistent\noutperformance against existing prompt-based and solver-aided approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2305.12295 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2402.11903v3",
    "published_date": "2024-02-19 07:38:57 UTC",
    "updated_date": "2024-06-19 02:52:00 UTC"
  },
  {
    "arxiv_id": "2402.11901v1",
    "title": "Real-World Planning with PDDL+ and Beyond",
    "authors": [
      "Wiktor Piotrowski",
      "Alexandre Perez"
    ],
    "abstract": "Real-world applications of AI Planning often require a highly expressive\nmodeling language to accurately capture important intricacies of target\nsystems. Hybrid systems are ubiquitous in the real-world, and PDDL+ is the\nstandardized modeling language for capturing such systems as planning domains.\nPDDL+ enables accurate encoding of mixed discrete-continuous system dynamics,\nexogenous activity, and many other interesting features exhibited in realistic\nscenarios. However, the uptake in usage of PDDL+ has been slow and\napprehensive, largely due to a general shortage of PDDL+ planning software, and\nrigid limitations of the few existing planners. To overcome this chasm, we\npresent Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity,\nand, most importantly, adaptability. The planner is designed to be effortlessly\ncustomizable to expand its capabilities well beyond the scope of PDDL+. As a\nresult, Nyx can be tailored to virtually any potential real-world application\nrequiring some form of AI Planning, paving the way for wider adoption of\nplanning methods for solving real-world problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11901v1",
    "published_date": "2024-02-19 07:35:49 UTC",
    "updated_date": "2024-02-19 07:35:49 UTC"
  },
  {
    "arxiv_id": "2402.11893v3",
    "title": "Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint",
    "authors": [
      "Xiaowei Yuan",
      "Zhao Yang",
      "Yequan Wang",
      "Shengping Liu",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "Large language models internalize enormous parametric knowledge during\npre-training. Concurrently, realistic applications necessitate external\ncontextual knowledge to aid models on the underlying tasks. This raises a\ncrucial dilemma known as knowledge conflicts, where the contextual knowledge\nclashes with the However, existing decoding works are specialized in resolving\nknowledge conflicts and could inadvertently deteriorate performance in absence\nof conflicts. In this paper, we propose an adaptive decoding method, termed as\ncontextual information-entropy constraint decoding (COIECD), to discern whether\nthe knowledge conflicts occur and resolve them. It can improve the model's\nfaithfulness to conflicting context, and simultaneously maintain high\nperformance among non- Our experiments show that COIECD exhibits strong\nperformance and robustness over knowledge conflicts in realistic datasets. Code\nis available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11893v3",
    "published_date": "2024-02-19 07:10:30 UTC",
    "updated_date": "2024-07-26 10:00:52 UTC"
  },
  {
    "arxiv_id": "2402.11892v2",
    "title": "Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing",
    "authors": [
      "Thanh Le-Cong",
      "Dat Nguyen",
      "Bach Le",
      "Toby Murray"
    ],
    "abstract": "In this paper, we propose shifting the focus of robustness evaluation for\nNeural Program Repair (NPR) techniques toward naturally-occurring data\ntransformations. To accomplish this, we first examine the naturalness of\nsemantic-preserving transformations through a two-stage human study. This study\nincludes (1) interviews with senior software developers to establish concrete\ncriteria for evaluating the naturalness of these transformations, and (2) a\nsurvey involving 10 developers to assess the naturalness of 1,178\ntransformations, i.e., pairs of original and transformed programs, applied to\n225 real-world bugs. Our findings show that only 60% of these transformations\nare deemed natural, while 20% are considered unnatural, with strong agreement\namong annotators. Moreover, the unnaturalness of these transformations\nsignificantly impacts both their applicability to benchmarks and the\nconclusions drawn from robustness testing. Next, we conduct natural robustness\ntesting on NPR techniques to assess their true effectiveness against real-world\ndata variations. Our experimental results reveal a substantial number of\nprediction changes in NPR techniques, leading to significant reductions in both\nplausible and correct patch rates when comparing performance on the original\nand transformed datasets. Additionally, we observe notable differences in\nperformance improvements between NPR techniques, suggesting potential biases on\nNPR evaluation introduced by limited datasets. Finally, we propose an LLM-based\nmetric to automate the assessment of transformation naturalness, ensuring the\nscalability of natural robustness testing.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11892v2",
    "published_date": "2024-02-19 07:07:44 UTC",
    "updated_date": "2024-11-13 06:54:05 UTC"
  },
  {
    "arxiv_id": "2402.14840v1",
    "title": "RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning",
    "authors": [
      "Congyun Jin",
      "Ming Zhang",
      "Xiaowei Ma",
      "Li Yujiao",
      "Yingbo Wang",
      "Yabo Jia",
      "Yuliang Du",
      "Tao Sun",
      "Haowen Wang",
      "Cong Fan",
      "Jinjie Gu",
      "Chenfei Chi",
      "Xiangguo Lv",
      "Fangzhou Li",
      "Wei Xue",
      "Yiran Huang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) and Large Multi-modal\nModels (LMMs) have shown potential in various medical applications, such as\nIntelligent Medical Diagnosis. Although impressive results have been achieved,\nwe find that existing benchmarks do not reflect the complexity of real medical\nreports and specialized in-depth reasoning capabilities. In this work, we\nintroduced RJUA-MedDQA, a comprehensive benchmark in the field of medical\nspecialization, which poses several challenges: comprehensively interpreting\nimgage content across diverse challenging layouts, possessing numerical\nreasoning ability to identify abnormal indicators and demonstrating clinical\nreasoning ability to provide statements of disease diagnosis, status and advice\nbased on medical contexts. We carefully design the data generation pipeline and\nproposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed\nat restoring textual and tabular content in medical report images. This method\nsubstantially enhances annotation efficiency, doubling the productivity of each\nannotator, and yields a 26.8% improvement in accuracy. We conduct extensive\nevaluations, including few-shot assessments of 5 LMMs which are capable of\nsolving Chinese medical QA tasks. To further investigate the limitations and\npotential of current LMMs, we conduct comparative experiments on a set of\nstrong LLMs by using image-text generated by ESRA method. We report the\nperformance of baselines and offer several observations: (1) The overall\nperformance of existing LMMs is still limited; however LMMs more robust to\nlow-quality and diverse-structured images compared to LLMs. (3) Reasoning\nacross context and image content present significant challenges. We hope this\nbenchmark helps the community make progress on these challenging tasks in\nmulti-modal medical document understanding and facilitate its application in\nhealthcare.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.14840v1",
    "published_date": "2024-02-19 06:57:02 UTC",
    "updated_date": "2024-02-19 06:57:02 UTC"
  },
  {
    "arxiv_id": "2402.11886v1",
    "title": "The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth",
    "authors": [
      "Shir Lissak",
      "Nitay Calderon",
      "Geva Shenkman",
      "Yaakov Ophir",
      "Eyal Fruchter",
      "Anat Brunstein Klomek",
      "Roi Reichart"
    ],
    "abstract": "Queer youth face increased mental health risks, such as depression, anxiety,\nand suicidal ideation. Hindered by negative stigma, they often avoid seeking\nhelp and rely on online resources, which may provide incompatible information.\nAlthough access to a supportive environment and reliable information is\ninvaluable, many queer youth worldwide have no access to such support. However,\nthis could soon change due to the rapid adoption of Large Language Models\n(LLMs) such as ChatGPT. This paper aims to comprehensively explore the\npotential of LLMs to revolutionize emotional support for queers. To this end,\nwe conduct a qualitative and quantitative analysis of LLM's interactions with\nqueer-related content. To evaluate response quality, we develop a novel\nten-question scale that is inspired by psychological standards and expert\ninput. We apply this scale to score several LLMs and human comments to posts\nwhere queer youth seek advice and share experiences. We find that LLM responses\nare supportive and inclusive, outscoring humans. However, they tend to be\ngeneric, not empathetic enough, and lack personalization, resulting in\nnonreliable and potentially harmful advice. We discuss these challenges,\ndemonstrate that a dedicated prompt can improve the performance, and propose a\nblueprint of an LLM-supporter that actively (but sensitively) seeks user\ncontext to provide personalized, empathetic, and reliable responses. Our\nannotated dataset is available for further research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11886v1",
    "published_date": "2024-02-19 06:54:55 UTC",
    "updated_date": "2024-02-19 06:54:55 UTC"
  },
  {
    "arxiv_id": "2402.11877v1",
    "title": "Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model",
    "authors": [
      "Han-Dong Lim",
      "HyeAnn Lee",
      "Donghwan Lee"
    ],
    "abstract": "Reinforcement learning has witnessed significant advancements, particularly\nwith the emergence of model-based approaches. Among these, $Q$-learning has\nproven to be a powerful algorithm in model-free settings. However, the\nextension of $Q$-learning to a model-based framework remains relatively\nunexplored. In this paper, we delve into the sample complexity of $Q$-learning\nwhen integrated with a model-based approach. Through theoretical analyses and\nempirical evaluations, we seek to elucidate the conditions under which\nmodel-based $Q$-learning excels in terms of sample efficiency compared to its\nmodel-free counterpart.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11877v1",
    "published_date": "2024-02-19 06:33:51 UTC",
    "updated_date": "2024-02-19 06:33:51 UTC"
  },
  {
    "arxiv_id": "2402.11871v4",
    "title": "From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions, and Models for Planning from Raw Data",
    "authors": [
      "Naman Shah",
      "Jayesh Nagpal",
      "Pulkit Verma",
      "Siddharth Srivastava"
    ],
    "abstract": "Hand-crafted, logic-based state and action representations have been widely\nused to overcome the intractable computational complexity of long-horizon robot\nplanning problems, including task and motion planning problems. However,\ncreating such representations requires experts with strong intuitions and\ndetailed knowledge about the robot and the tasks it may need to accomplish in a\ngiven setting. Removing this dependency on human intuition is a highly active\nresearch area.\n  This paper presents the first approach for autonomously learning\ngeneralizable, logic-based relational representations for abstract states and\nactions starting from unannotated high-dimensional, real-valued robot\ntrajectories. The learned representations constitute auto-invented PDDL-like\ndomain models. Empirical results in deterministic settings show that powerful\nabstract representations can be learned from just a handful of robot\ntrajectories; the learned relational representations include but go beyond\nclassical, intuitive notions of high-level actions; and that the learned models\nallow planning algorithms to scale to tasks that were previously beyond the\nscope of planning without hand-crafted abstractions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11871v4",
    "published_date": "2024-02-19 06:28:21 UTC",
    "updated_date": "2024-03-04 14:52:15 UTC"
  },
  {
    "arxiv_id": "2402.11866v1",
    "title": "Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic",
    "authors": [
      "Jeremy J. Lin",
      "Tomoro Mochida",
      "Riley C. W. O'Neill",
      "Atsuro Yoshida",
      "Masashi Yamazaki",
      "Akinobu Sasada"
    ],
    "abstract": "Our aim of this paper is to develop new map matching algorithms and to\nimprove upon previous work. We address two key approaches: Analytic Hierarchy\nProcess (AHP) map matching and fuzzy logic map matching. AHP is a\ndecision-making method that combines mathematical analysis with human judgment,\nand fuzzy logic is an approach to computing based on the degree of truth and\naims at modeling the imprecise modes of reasoning from 0 to 1 rather than the\nusual boolean logic. Of these algorithms, the way of our applying AHP to map\nmatching is newly developed in this paper, meanwhile, our application of fuzzy\nlogic to map matching is mostly the same as existing research except for some\nsmall changes. Because of the common characteristic that both methods are\ndesigned to handle imprecise information and simplicity for implementation, we\ndecided to use these methods.",
    "categories": [
      "cs.CG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CG",
    "comment": "25 pages, 27 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.11866v1",
    "published_date": "2024-02-19 06:14:46 UTC",
    "updated_date": "2024-02-19 06:14:46 UTC"
  },
  {
    "arxiv_id": "2402.11842v1",
    "title": "CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking",
    "authors": [
      "Zian Su",
      "Xiangzhe Xu",
      "Ziyang Huang",
      "Zhuo Zhang",
      "Yapeng Ye",
      "Jianjun Huang",
      "Xiangyu Zhang"
    ],
    "abstract": "Transformer based code models have impressive performance in many software\nengineering tasks. However, their effectiveness degrades when symbols are\nmissing or not informative. The reason is that the model may not learn to pay\nattention to the right correlations/contexts without the help of symbols. We\npropose a new method to pre-train general code models when symbols are lacking.\nWe observe that in such cases, programs degenerate to something written in a\nvery primitive language. We hence propose to use program analysis to extract\ncontexts a priori (instead of relying on symbols and masked language modeling\nas in vanilla models). We then leverage a novel attention masking method to\nonly allow the model attending to these contexts, e.g., bi-directional program\ndependence transitive closures and token co-occurrences. In the meantime, the\ninherent self-attention mechanism is utilized to learn which of the allowed\nattentions are more important compared to others. To realize the idea, we\nenhance the vanilla tokenization and model architecture of a BERT model,\nconstruct and utilize attention masks, and introduce a new pre-training\nalgorithm. We pre-train this BERT-like model from scratch, using a dataset of\n26 million stripped binary functions with explicit program dependence\ninformation extracted by our tool. We apply the model in three downstream\ntasks: binary similarity, type inference, and malware family classification.\nOur pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49%\nto 60%, and 74% to 94%, respectively. It also substantially outperforms other\ngeneral pre-training techniques of code understanding models.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11842v1",
    "published_date": "2024-02-19 05:13:22 UTC",
    "updated_date": "2024-02-19 05:13:22 UTC"
  },
  {
    "arxiv_id": "2402.12412v1",
    "title": "Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating The Same",
    "authors": [
      "Sungjun Ahn",
      "Hyun-Jeong Yim",
      "Youngwan Lee",
      "Sung-Ik Park"
    ],
    "abstract": "This paper introduces a media service model that exploits artificial\nintelligence (AI) video generators at the receive end. This proposal deviates\nfrom the traditional multimedia ecosystem, completely relying on in-house\nproduction, by shifting part of the content creation onto the receiver. We\nbring a semantic process into the framework, allowing the distribution network\nto provide service elements that prompt the content generator, rather than\ndistributing encoded data of fully finished programs. The service elements\ninclude fine-tailored text descriptions, lightweight image data of some\nobjects, or application programming interfaces, comprehensively referred to as\nsemantic sources, and the user terminal translates the received semantic data\ninto video frames. Empowered by the random nature of generative AI, the users\ncould then experience super-personalized services accordingly. The proposed\nidea incorporates the situations in which the user receives different service\nproviders' element packages; a sequence of packages over time, or multiple\npackages at the same time. Given promised in-context coherence and content\nintegrity, the combinatory dynamics will amplify the service diversity,\nallowing the users to always chance upon new experiences. This work\nparticularly aims at short-form videos and advertisements, which the users\nwould easily feel fatigued by seeing the same frame sequence every time. In\nthose use cases, the content provider's role will be recast as scripting\nsemantic sources, transformed from a thorough producer. Overall, this work\nexplores a new form of media ecosystem facilitated by receiver-embedded\ngenerative models, featuring both random content dynamics and enhanced delivery\nefficiency simultaneously.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MM",
      "eess.SP"
    ],
    "primary_category": "cs.HC",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.12412v1",
    "published_date": "2024-02-19 04:39:30 UTC",
    "updated_date": "2024-02-19 04:39:30 UTC"
  },
  {
    "arxiv_id": "2402.11818v1",
    "title": "Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages",
    "authors": [
      "Sameer Jain",
      "Sedrick Scott Keh",
      "Shova Chettri",
      "Karun Dewan",
      "Pablo Izquierdo",
      "Johanna Prussman",
      "Pooja Shreshtha",
      "Cesar Suarez",
      "Zheyuan Ryan Shi",
      "Lei Li",
      "Fei Fang"
    ],
    "abstract": "Environmental conservation organizations routinely monitor news content on\nconservation in protected areas to maintain situational awareness of\ndevelopments that can have an environmental impact. Existing automated media\nmonitoring systems require large amounts of data labeled by domain experts,\nwhich is only feasible at scale for high-resource languages like English.\nHowever, such tools are most needed in the global south where news of interest\nis mainly in local low-resource languages, and far fewer experts are available\nto annotate datasets sustainably. In this paper, we propose NewsSerow, a method\nto automatically recognize environmental conservation content in low-resource\nlanguages. NewsSerow is a pipeline of summarization, in-context few-shot\nclassification, and self-reflection using large language models (LLMs). Using\nat most 10 demonstration example news articles in Nepali, NewsSerow\nsignificantly outperforms other few-shot methods and achieves comparable\nperformance with models fully fine-tuned using thousands of examples. The World\nWide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in\nNepal, significantly reducing their operational burden, and ensuring that AI\ntools for conservation actually reach the communities that need them the most.\nNewsSerow has also been deployed for countries with other languages like\nColombia.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2024: AI for Social Impact Track",
    "pdf_url": "http://arxiv.org/pdf/2402.11818v1",
    "published_date": "2024-02-19 04:17:21 UTC",
    "updated_date": "2024-02-19 04:17:21 UTC"
  },
  {
    "arxiv_id": "2402.11815v2",
    "title": "HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?",
    "authors": [
      "Shubhashis Roy Dipta",
      "Sadat Shahriar"
    ],
    "abstract": "This paper describes our system developed for SemEval-2024 Task 8,\n``Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection'' Machine-generated texts have been one of the main concerns due\nto the use of large language models (LLM) in fake text generation, phishing,\ncheating in exams, or even plagiarizing copyright materials. A lot of systems\nhave been developed to detect machine-generated text. Nonetheless, the majority\nof these systems rely on the text-generating model. This limitation is\nimpractical in real-world scenarios, as it's often impossible to know which\nspecific model the user has used for text generation. In this work, we propose\na $\\textbf{single}$ model based on contrastive learning, which uses\n$\\textbf{$\\approx$40% of the baseline's parameters}$ (149M vs. 355M) but shows\na comparable performance on the test dataset $(\\textbf{21st out of 137\nparticipants})$. Our key finding is that even without an ensemble of multiple\nmodels, a single base model can have comparable performance with the help of\ndata augmentation and contrastive learning. Our code is publicly available at\nhttps://github.com/dipta007/SemEval24-Task8.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera Ready Version - Accepted in SemEval 2024 (Colocated with NAACL\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.11815v2",
    "published_date": "2024-02-19 04:11:34 UTC",
    "updated_date": "2024-03-27 20:30:08 UTC"
  },
  {
    "arxiv_id": "2402.11813v2",
    "title": "A novel framework for adaptive stress testing of autonomous vehicles in multi-lane roads",
    "authors": [
      "Linh Trinh",
      "Quang-Hung Luu",
      "Thai M. Nguyen",
      "Hai L. Vu"
    ],
    "abstract": "Stress testing is an approach for evaluating the reliability of systems under\nextreme conditions which help reveal vulnerable scenarios that standard testing\nmay overlook. Identifying such scenarios is of great importance in autonomous\nvehicles (AV) and other safety-critical systems. Since failure events are rare,\nnaive random search approaches require a large number of vehicle operation\nhours to identify potential system failures. Adaptive Stress Testing (AST) is a\nmethod addressing this constraint by effectively exploring the failure\ntrajectories of AV using a Markov decision process and employs reinforcement\nlearning techniques to identify driving scenarios with high probability of\nfailures. However, existing AST frameworks are able to handle only simple\nscenarios, such as one vehicle moving longitudinally on a single lane road\nwhich is not realistic and has a limited applicability. In this paper, we\npropose a novel AST framework to systematically explore corner cases of\nintelligent driving models that can result in safety concerns involving both\nlongitudinal and lateral vehicle's movements. Specially, we develop a new\nreward function for Deep Reinforcement Learning to guide the AST in identifying\ncrash scenarios based on the collision probability estimate between the AV\nunder test (i.e., the ego vehicle) and the trajectory of other vehicles on the\nmulti-lane roads. To demonstrate the effectiveness of our framework, we tested\nit with a complex driving model vehicle that can be controlled in both\nlongitudinal and lateral directions. Quantitative and qualitative analyses of\nour experimental results demonstrate that our framework outperforms the\nstate-of-the-art AST scheme in identifying corner cases with complex driving\nmaneuvers.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11813v2",
    "published_date": "2024-02-19 04:02:40 UTC",
    "updated_date": "2024-09-19 08:27:47 UTC"
  },
  {
    "arxiv_id": "2402.11809v3",
    "title": "Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding",
    "authors": [
      "Hanling Yi",
      "Feng Lin",
      "Hongbin Li",
      "Peiyang Ning",
      "Xiaotian Yu",
      "Rong Xiao"
    ],
    "abstract": "This research aims to accelerate the inference speed of large language models\n(LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel\n\\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative\napproach designed for achieving lossless acceleration of LLMs. By integrating\nsemi-autoregressive inference and speculative decoding capabilities, SPACE\nuniquely enables autoregressive LLMs to parallelize token generation and\nverification. This is realized through a specialized semi-autoregressive\nsupervised fine-tuning process that equips existing LLMs with the ability to\nsimultaneously predict multiple tokens. Additionally, an auto-correct decoding\nalgorithm facilitates the simultaneous generation and verification of token\nsequences within a single model invocation. Through extensive experiments on a\nrange of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x\non HumanEval-X while maintaining output quality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2402.11809v3",
    "published_date": "2024-02-19 03:39:10 UTC",
    "updated_date": "2024-05-20 01:48:18 UTC"
  },
  {
    "arxiv_id": "2402.11804v3",
    "title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs",
    "authors": [
      "Kai Wang",
      "Yuwei Xu",
      "Zhiyong Wu",
      "Siqiang Luo"
    ],
    "abstract": "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts\nfrom new KGs that are not seen during training, has been widely adopted in\nvarious applications. One critical challenge of KG inductive reasoning is\nhandling low-resource scenarios with scarcity in both textual and structural\naspects. In this paper, we attempt to address this challenge with Large\nLanguage Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to\ngenerate a graph-structural prompt to enhance the pre-trained Graph Neural\nNetworks (GNNs), which brings us new methodological insights into the KG\ninductive reasoning methods, as well as high generalizability in practice. On\nthe methodological side, we introduce a novel pretraining and prompting\nframework ProLINK, designed for low-resource inductive reasoning across\narbitrary KGs without requiring additional training. On the practical side, we\nexperimentally evaluate our approach on 36 low-resource KG datasets and find\nthat ProLINK outperforms previous methods in three-shot, one-shot, and\nzero-shot reasoning tasks, exhibiting average performance improvements by 20%,\n45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong\nrobustness for various LLM promptings as well as full-shot scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by Findings of ACL2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11804v3",
    "published_date": "2024-02-19 03:21:19 UTC",
    "updated_date": "2024-06-19 09:00:53 UTC"
  },
  {
    "arxiv_id": "2402.11800v3",
    "title": "Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling",
    "authors": [
      "Arman Adibi",
      "Nicolo Dal Fabbro",
      "Luca Schenato",
      "Sanjeev Kulkarni",
      "H. Vincent Poor",
      "George J. Pappas",
      "Hamed Hassani",
      "Aritra Mitra"
    ],
    "abstract": "Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!",
    "pdf_url": "http://arxiv.org/pdf/2402.11800v3",
    "published_date": "2024-02-19 03:08:02 UTC",
    "updated_date": "2024-03-27 15:48:29 UTC"
  },
  {
    "arxiv_id": "2402.11793v4",
    "title": "Generative Kaleidoscopic Networks",
    "authors": [
      "Harsh Shrivastava"
    ],
    "abstract": "We discovered that the neural networks, especially the deep ReLU networks,\ndemonstrate an `over-generalization' phenomenon. That is, the output values for\nthe inputs that were not seen during training are mapped close to the output\nrange that were observed during the learning process. In other words, the\nneural networks learn a many-to-one mapping and this effect is more prominent\nas we increase the number of layers or the depth of the neural network. We\nutilize this property of neural networks to design a dataset kaleidoscope,\ntermed as `Generative Kaleidoscopic Networks'. Succinctly, if we learn a model\nto map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$,\nthe proposed `Kaleidoscopic sampling' procedure starts with a random input\nnoise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots\nf_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing\nsamples from the input distribution and the quality of samples recovered\nimproves as we increase the depth of the model. Scope: We observed this\nphenomenon to various degrees for the other deep learning architectures like\nCNNs, Transformers & U-Nets and we are currently investigating them further.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11793v4",
    "published_date": "2024-02-19 02:48:40 UTC",
    "updated_date": "2024-10-22 04:15:49 UTC"
  },
  {
    "arxiv_id": "2402.12411v1",
    "title": "Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks",
    "authors": [
      "Yankai Chen",
      "Yixiang Fang",
      "Qiongyan Wang",
      "Xin Cao",
      "Irwin King"
    ],
    "abstract": "Node importance estimation problem has been studied conventionally with\nhomogeneous network topology analysis. To deal with network heterogeneity, a\nfew recent methods employ graph neural models to automatically learn diverse\nsources of information. However, the major concern revolves around that their\nfull adaptive learning process may lead to insufficient information\nexploration, thereby formulating the problem as the isolated node value\nprediction with underperformance and less interpretability. In this work, we\npropose a novel learning framework: SKES. Different from previous automatic\nlearning designs, SKES exploits heterogeneous structural knowledge to enrich\nthe informativeness of node representations. Based on a sufficiently\nuninformative reference, SKES estimates the importance value for any input\nnode, by quantifying its disparity against the reference. This establishes an\ninterpretable node importance computation paradigm. Furthermore, SKES dives\ndeep into the understanding that \"nodes with similar characteristics are prone\nto have similar importance values\" whilst guaranteeing that such\ninformativeness disparity between any different nodes is orderly reflected by\nthe embedding distance of their associated latent features. Extensive\nexperiments on three widely-evaluated benchmarks demonstrate the performance\nsuperiority of SKES over several recent competing methods.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted by AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.12411v1",
    "published_date": "2024-02-19 02:34:23 UTC",
    "updated_date": "2024-02-19 02:34:23 UTC"
  },
  {
    "arxiv_id": "2402.11788v1",
    "title": "MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion",
    "authors": [
      "Raktim Kumar Mondol",
      "Ewan K. A. Millar",
      "Arcot Sowmya",
      "Erik Meijering"
    ],
    "abstract": "Survival risk stratification is an important step in clinical decision making\nfor breast cancer management. We propose a novel deep learning approach for\nthis purpose by integrating histopathological imaging, genetic and clinical\ndata. It employs vision transformers, specifically the MaxViT model, for image\nfeature extraction, and self-attention to capture intricate image relationships\nat the patient level. A dual cross-attention mechanism fuses these features\nwith genetic data, while clinical data is incorporated at the final layer to\nenhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show\nthat our model, trained using the negative log likelihood loss function, can\nachieve superior performance with a mean C-index of 0.64, surpassing existing\nmethods. This advancement facilitates tailored treatment strategies,\npotentially leading to improved patient outcomes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Keywords: Multimodal Fusion, Breast Cancer, Whole Slide Images,\n  Survival Prediction",
    "pdf_url": "http://arxiv.org/pdf/2402.11788v1",
    "published_date": "2024-02-19 02:31:36 UTC",
    "updated_date": "2024-02-19 02:31:36 UTC"
  },
  {
    "arxiv_id": "2402.16882v2",
    "title": "Revealing the Relationship Between Publication Bias and Chemical Reactivity with Contrastive Learning",
    "authors": [
      "Wenhao Gao",
      "Priyanka Raghavan",
      "Ron Shprints",
      "Connor W. Coley"
    ],
    "abstract": "A synthetic method's substrate tolerance and generality are often showcased\nin a \"substrate scope\" table. However, substrate selection exhibits a\nfrequently discussed publication bias: unsuccessful experiments or low-yielding\nresults are rarely reported. In this work, we explore more deeply the\nrelationship between such publication bias and chemical reactivity beyond the\nsimple analysis of yield distributions using a novel neural network training\nstrategy, substrate scope contrastive learning. By treating reported substrates\nas positive samples and non-reported substrates as negative samples, our\ncontrastive learning strategy teaches a model to group molecules within a\nnumerical embedding space, based on historical trends in published substrate\nscope tables. Training on 20,798 aryl halides in the CAS Content\nCollection$^{\\text{TM}}$, spanning thousands of publications from 2010-2015, we\ndemonstrate that the learned embeddings exhibit a correlation with physical\norganic reactivity descriptors through both intuitive visualizations and\nquantitative regression analyses. Additionally, these embeddings are applicable\nto various reaction modeling tasks like yield prediction and regioselectivity\nprediction, underscoring the potential to use historical reaction data as a\npre-training task. This work not only presents a chemistry-specific machine\nlearning training strategy to learn from literature data in a new way, but also\nrepresents a unique approach to uncover trends in chemical reactivity reflected\nby trends in substrate selection in publications.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16882v2",
    "published_date": "2024-02-19 02:21:20 UTC",
    "updated_date": "2025-02-20 16:50:35 UTC"
  },
  {
    "arxiv_id": "2402.11780v2",
    "title": "CiMNet: Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware",
    "authors": [
      "Souvik Kundu",
      "Anthony Sarah",
      "Vinay Joshi",
      "Om J Omer",
      "Sreenivas Subramoney"
    ],
    "abstract": "With the recent growth in demand for large-scale deep neural networks,\ncompute in-memory (CiM) has come up as a prominent solution to alleviate\nbandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman\narchitectures. However, the construction of CiM hardware poses a challenge as\nany specific memory hierarchy in terms of cache sizes and memory bandwidth at\ndifferent interfaces may not be ideally matched to any neural network's\nattributes such as tensor dimension and arithmetic intensity, thus leading to\nsuboptimal and under-performing systems. Despite the success of neural\narchitecture search (NAS) techniques in yielding efficient sub-networks for a\ngiven hardware metric budget (e.g., DNN execution time or latency), it assumes\nthe hardware configuration to be frozen, often yielding sub-optimal\nsub-networks for a given budget. In this paper, we present CiMNet, a framework\nthat jointly searches for optimal sub-networks and hardware configurations for\nCiM architectures creating a Pareto optimal frontier of downstream task\naccuracy and execution metrics (e.g., latency). The proposed framework can\ncomprehend the complex interplay between a sub-network's performance and the\nCiM hardware configuration choices including bandwidth, processing element\nsize, and memory size. Exhaustive experiments on different model architectures\nfrom both CNN and Transformer families demonstrate the efficacy of the CiMNet\nin finding co-optimized sub-networks and CiM hardware configurations.\nSpecifically, for similar ImageNet classification accuracy as baseline ViT-B,\noptimizing only the model architecture increases performance (or reduces\nworkload execution time) by 1.7x while optimizing for both the model\narchitecture and hardware configuration increases it by 3.1x.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "6 pages, 4 figures, 5 tables; Accepted as a full paper by the tinyML\n  Research Symposium 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11780v2",
    "published_date": "2024-02-19 02:12:07 UTC",
    "updated_date": "2024-03-18 15:25:30 UTC"
  },
  {
    "arxiv_id": "2402.11778v2",
    "title": "Towards Theoretical Understandings of Self-Consuming Generative Models",
    "authors": [
      "Shi Fu",
      "Sen Zhang",
      "Yingjie Wang",
      "Xinmei Tian",
      "Dacheng Tao"
    ],
    "abstract": "This paper tackles the emerging challenge of training generative models\nwithin a self-consuming loop, wherein successive generations of models are\nrecursively trained on mixtures of real and synthetic data from previous\ngenerations. We construct a theoretical framework to rigorously evaluate how\nthis training procedure impacts the data distributions learned by future\nmodels, including parametric and non-parametric models. Specifically, we derive\nbounds on the total variation (TV) distance between the synthetic data\ndistributions produced by future models and the original real data distribution\nunder various mixed training scenarios for diffusion models with a\none-hidden-layer neural network score function. Our analysis demonstrates that\nthis distance can be effectively controlled under the condition that mixed\ntraining dataset sizes or proportions of real data are large enough.\nInterestingly, we further unveil a phase transition induced by expanding\nsynthetic data amounts, proving theoretically that while the TV distance\nexhibits an initial ascent, it declines beyond a threshold point. Finally, we\npresent results for kernel density estimation, delivering nuanced insights such\nas the impact of mixed data training on error propagation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11778v2",
    "published_date": "2024-02-19 02:08:09 UTC",
    "updated_date": "2024-06-24 14:23:30 UTC"
  },
  {
    "arxiv_id": "2402.11777v1",
    "title": "Uncovering Latent Human Wellbeing in Language Model Embeddings",
    "authors": [
      "Pedro Freire",
      "ChengCheng Tan",
      "Adam Gleave",
      "Dan Hendrycks",
      "Scott Emmons"
    ],
    "abstract": "Do language models implicitly learn a concept of human wellbeing? We explore\nthis through the ETHICS Utilitarianism task, assessing if scaling enhances\npretrained models' representations. Our initial finding reveals that, without\nany prompt engineering or finetuning, the leading principal component from\nOpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches\nthe 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting\npretraining conveys some understanding about human wellbeing. Next, we consider\nfour language model families, observing how Utilitarianism accuracy varies with\nincreased parameters. We find performance is nondecreasing with increased model\nsize when using sufficient numbers of principal components.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2402.11777v1",
    "published_date": "2024-02-19 02:08:03 UTC",
    "updated_date": "2024-02-19 02:08:03 UTC"
  },
  {
    "arxiv_id": "2402.11773v2",
    "title": "Dynamic Multi-Network Mining of Tensor Time Series",
    "authors": [
      "Kohei Obata",
      "Koki Kawabata",
      "Yasuko Matsubara",
      "Yasushi Sakurai"
    ],
    "abstract": "Subsequence clustering of time series is an essential task in data mining,\nand interpreting the resulting clusters is also crucial since we generally do\nnot have prior knowledge of the data. Thus, given a large collection of tensor\ntime series consisting of multiple modes, including timestamps, how can we\nachieve subsequence clustering for tensor time series and provide interpretable\ninsights? In this paper, we propose a new method, Dynamic Multi-network Mining\n(DMM), that converts a tensor time series into a set of segment groups of\nvarious lengths (i.e., clusters) characterized by a dependency network\nconstrained with l1-norm. Our method has the following properties. (a)\nInterpretable: it characterizes the cluster with multiple networks, each of\nwhich is a sparse dependency network of a corresponding non-temporal mode, and\nthus provides visible and interpretable insights into the key relationships.\n(b) Accurate: it discovers the clusters with distinct networks from tensor time\nseries according to the minimum description length (MDL). (c) Scalable: it\nscales linearly in terms of the input data size when solving a non-convex\nproblem to optimize the number of segments and clusters, and thus it is\napplicable to long-range and high-dimensional tensors. Extensive experiments\nwith synthetic datasets confirm that our method outperforms the\nstate-of-the-art methods in terms of clustering accuracy. We then use real\ndatasets to demonstrate that DMM is useful for providing interpretable insights\nfrom tensor time series.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WWW 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11773v2",
    "published_date": "2024-02-19 02:06:04 UTC",
    "updated_date": "2024-02-22 01:17:29 UTC"
  },
  {
    "arxiv_id": "2402.11771v1",
    "title": "Evaluating the Effectiveness of Index-Based Treatment Allocation",
    "authors": [
      "Niclas Boehmer",
      "Yash Nair",
      "Sanket Shah",
      "Lucas Janson",
      "Aparna Taneja",
      "Milind Tambe"
    ],
    "abstract": "When resources are scarce, an allocation policy is needed to decide who\nreceives a resource. This problem occurs, for instance, when allocating scarce\nmedical resources and is often solved using modern ML methods. This paper\nintroduces methods to evaluate index-based allocation policies -- that allocate\na fixed number of resources to those who need them the most -- by using data\nfrom a randomized control trial. Such policies create dependencies between\nagents, which render the assumptions behind standard statistical tests invalid\nand limit the effectiveness of estimators. Addressing these challenges, we\ntranslate and extend recent ideas from the statistics literature to present an\nefficient estimator and methods for computing asymptotically correct confidence\nintervals. This enables us to effectively draw valid statistical conclusions, a\ncritical gap in previous work. Our extensive experiments validate our\nmethodology in practical settings, while also showcasing its statistical power.\nWe conclude by proposing and empirically verifying extensions of our\nmethodology that enable us to reevaluate a past randomized control trial to\nevaluate different ML allocation policies in the context of a mHealth program,\ndrawing previously invisible conclusions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11771v1",
    "published_date": "2024-02-19 01:55:55 UTC",
    "updated_date": "2024-02-19 01:55:55 UTC"
  },
  {
    "arxiv_id": "2402.11764v2",
    "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs",
    "authors": [
      "Pengrui Han",
      "Rafal Kocielnik",
      "Adhithya Saravanan",
      "Roy Jiang",
      "Or Sharir",
      "Anima Anandkumar"
    ],
    "abstract": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "68T50",
      "I.2.7; K.4.1"
    ],
    "primary_category": "cs.CL",
    "comment": "To Appear in the Proceedings of the 1st Conference on Language\n  Modeling (COLM) 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11764v2",
    "published_date": "2024-02-19 01:28:48 UTC",
    "updated_date": "2024-09-16 05:28:43 UTC"
  },
  {
    "arxiv_id": "2402.11753v4",
    "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
    "authors": [
      "Fengqing Jiang",
      "Zhangchen Xu",
      "Luyao Niu",
      "Zhen Xiang",
      "Bhaskar Ramasubramanian",
      "Bo Li",
      "Radha Poovendran"
    ],
    "abstract": "Safety is critical to the usage of large language models (LLMs). Multiple\ntechniques such as data filtering and supervised fine-tuning have been\ndeveloped to strengthen LLM safety. However, currently known techniques presume\nthat corpora used for safety alignment of LLMs are solely interpreted by\nsemantics. This assumption, however, does not hold in real-world applications,\nwhich leads to severe vulnerabilities in LLMs. For example, users of forums\noften use ASCII art, a form of text-based art, to convey image information. In\nthis paper, we propose a novel ASCII art-based jailbreak attack and introduce a\ncomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the\ncapabilities of LLMs in recognizing prompts that cannot be solely interpreted\nby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and\nLlama2) struggle to recognize prompts provided in the form of ASCII art. Based\non this observation, we develop the jailbreak attack ArtPrompt, which leverages\nthe poor performance of LLMs in recognizing ASCII art to bypass safety measures\nand elicit undesired behaviors from LLMs. ArtPrompt only requires black-box\naccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompt\non five SOTA LLMs, and show that ArtPrompt can effectively and efficiently\ninduce undesired behaviors from all five LLMs. Our code is available at\nhttps://github.com/uw-nsl/ArtPrompt.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11753v4",
    "published_date": "2024-02-19 00:43:31 UTC",
    "updated_date": "2024-06-07 17:35:17 UTC"
  },
  {
    "arxiv_id": "2402.11752v2",
    "title": "Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing",
    "authors": [
      "Dominik Wagner",
      "Basim Khajwal",
      "C. -H. Luke Ong"
    ],
    "abstract": "It is well-known that the reparameterisation gradient estimator, which\nexhibits low variance in practice, is biased for non-differentiable models.\nThis may compromise correctness of gradient-based optimisation methods such as\nstochastic gradient descent (SGD). We introduce a simple syntactic framework to\ndefine non-differentiable functions piecewisely and present a systematic\napproach to obtain smoothings for which the reparameterisation gradient\nestimator is unbiased. Our main contribution is a novel variant of SGD,\nDiagonalisation Stochastic Gradient Descent, which progressively enhances the\naccuracy of the smoothed approximation during optimisation, and we prove\nconvergence to stationary points of the unsmoothed (original) objective. Our\nempirical evaluation reveals benefits over the state of the art: our approach\nis simple, fast, stable and attains orders of magnitude reduction in\nwork-normalised variance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11752v2",
    "published_date": "2024-02-19 00:43:22 UTC",
    "updated_date": "2024-02-20 02:58:38 UTC"
  },
  {
    "arxiv_id": "2402.14838v1",
    "title": "RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts",
    "authors": [
      "Mohammad Heydari Rad",
      "Farhan Farsi",
      "Shayan Bali",
      "Romina Etezadi",
      "Mehrnoush Shamsfard"
    ],
    "abstract": "Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs\nhave been used to generate texts in different languages and for different\ntasks. Additionally, due to the participation of remarkable companies such as\nGoogle and OpenAI, LLMs are now more accessible, and people can easily use\nthem. However, an important issue is how we can detect AI-generated texts from\nhuman-written ones. In this article, we have investigated the problem of\nAI-generated text detection from two different aspects: semantics and syntax.\nFinally, we presented an AI model that can distinguish AI-generated texts from\nhuman-written ones with high accuracy on both multilingual and monolingual\ntasks using the M4 dataset. According to our results, using a semantic approach\nwould be more helpful for detection. However, there is a lot of room for\nimprovement in the syntactic approach, and it would be a good approach for\nfuture work.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Mohammad Heydari Rad, Farhan Farsi, and Shayan Bali have made equal\n  contributions to this work",
    "pdf_url": "http://arxiv.org/pdf/2402.14838v1",
    "published_date": "2024-02-19 00:40:17 UTC",
    "updated_date": "2024-02-19 00:40:17 UTC"
  },
  {
    "arxiv_id": "2402.11746v1",
    "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
    "authors": [
      "Rishabh Bhardwaj",
      "Do Duc Anh",
      "Soujanya Poria"
    ],
    "abstract": "Aligned language models face a significant limitation as their fine-tuning\noften results in compromised safety. To tackle this, we propose a simple method\nRESTA that performs LLM safety realignment. RESTA stands for REstoring Safety\nthrough Task Arithmetic. At its core, it involves a simple arithmetic addition\nof a safety vector to the weights of the compromised model. We demonstrate the\neffectiveness of RESTA in both parameter-efficient and full fine-tuning,\ncovering a wide range of downstream tasks, including instruction following in\nChinese, English, and Hindi, as well as problem-solving capabilities in Code\nand Math. We also showcase the generalizability of RESTA on three existing\nsafety evaluation benchmarks and a multilingual benchmark dataset proposed as a\npart of this work, consisting of 550 harmful questions covering 11 categories,\neach with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of\nthe compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in\nparameter-efficient and full fine-tuning, respectively, while maintaining most\nof the model's performance on the task. We release the source codes at:\nhttps://github.com/declare-lab/resta.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11746v1",
    "published_date": "2024-02-19 00:18:09 UTC",
    "updated_date": "2024-02-19 00:18:09 UTC"
  }
]
{
  "date": "2024-09-17",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-17 的 arXiv 中文 TLDR 快报！今天 arXiv 论文主要聚焦 AI 模型优化（如 LLM 的偏见检测和多模态生成）、医疗图像处理、量子计算等领域，亮点包括 IBM Quantum 的性能综述和多模态 LLM 的高效应用，令人印象深刻的文章有 NVLM（NVIDIA 团队）和 Moshi（Meta 等合作），它们展示了 AI 在实际领域的潜力。\n\n### 重点 AI 和 LLM 相关论文\n这些论文探讨了大型语言模型的改进、偏见和多模态应用，代表了当前 AI 研究的热点。\n- **NVLM: Open Frontier-Class Multimodal LLMs**（英文原题：NVLM: Open Frontier-Class Multimodal LLMs）：NVIDIA 团队提出了一种开源的多模态 LLM，实现了多任务视觉-语言处理（如图像生成），在基准测试中超越 GPT-4o，主要贡献是通过高效架构和高质量数据集提升了模型性能。\n- **Moshi: a speech-text foundation model for real-time dialogue**（英文原题：Moshi: a speech-text foundation model for real-time dialogue）：Meta 和其他团队开发了实时对话模型，支持全双工语音生成，发现了在低延迟下处理语音-文本的潜力，显著改善了对话系统的自然性和效率。\n- **Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation**（英文原题：Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation）：研究了检索增强生成中的公平性问题，通过公平排名机制提升了响应质量，发现公平检索能平衡来源曝光，避免偏见。\n- **REAL: Response Embedding-based Alignment for LLMs**（英文原题：REAL: Response Embedding-based Alignment for LLMs）：提出了一种基于嵌入的语言模型对齐方法，通过选择差异化响应对减少标注错误，主要发现能节省65%的标注工作，提高LLM的人类偏好对齐。\n- **Egalitarian Language Representation in Language Models: It All Begins with Tokenizers**（英文原题：Egalitarian Language Representation in Language Models: It All Begins with Tokenizers）：分析了分词器对语言模型的影响，引入Grapheme Pair Encoding优化复杂脚本语言的分词，显著提升了非英语语言的表示公平性。\n- **Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement**（英文原题：Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement）：开发了一种基于多样性的数据选择框架，通过迭代聚类提升LLM训练效率，发现多样性采样比随机选择提高7%的性能。\n\n### 医疗和生物应用论文\n这些论文在AI辅助诊断领域有实际价值，快速掠过较基础的。\n- **Automating proton PBS treatment planning for head and neck cancers using policy gradient-based deep reinforcement learning**（英文原题：Automating proton PBS treatment planning for head and neck cancers using policy gradient-based deep reinforcement learning）：使用强化学习优化质子治疗计划，主要发现能实现人类级别的自动规划，减少OAR（器官-at-risk）损伤。\n- **Two Stage Segmentation of Cervical Tumors using PocketNet**（英文原题：Two Stage Segmentation of Cervical Tumors using PocketNet）：提出PocketNet模型进行宫颈肿瘤分割，实现了高精度分割（Dice系数超过70%），贡献在于高效的阶段式处理。\n- **Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images**（英文原题：Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images）：构建多模态数据集用于轴突和髓鞘分割，发现通用模型优于专有模型，在神经科学图像处理中提升泛化能力。\n\n### 量子计算和环境应用\n这些领域有突破性贡献，简要提及。\n- **IBM Quantum Computers: Evolution, Performance, and Future Directions**（英文原题：IBM Quantum Computers: Evolution, Performance, and Future Directions）：综述IBM量子计算机的发展，重点分析性能指标和向容错量子计算的过渡。\n- **Harnessing AI data-driven global weather models for climate attribution**（英文原题：Harnessing AI data-driven global weather models for climate attribution）：使用AI模型（如Graphcast）分析气候归因，发现AI能加速极端事件模拟，但存在外推限制。\n\n其他论文如音乐生成（e.g., **PDMX: A Large-Scale Public Domain MusicXML Dataset**，构建大规模音乐数据集提升符号音乐处理）和商业过程优化（e.g., **Learning variant product relationship and variation attributes from e-commerce website structures**，通过LLM识别产品变体关系）等，贡献在于数据集和应用创新，但非核心焦点，故快速掠过。总体而言，今天的论文突显了AI在多领域的潜力，尤其在高效模型和实际应用上。",
  "papers": [
    {
      "arxiv_id": "2409.15359v2",
      "title": "Watch Your Steps: Observable and Modular Chains of Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Cassandra A. Cohen",
        "William W. Cohen"
      ],
      "abstract": "We propose a variant of chain of thought (CoT) prompting called Program Trace\nPrompting that makes explanations more observable while preserving the power,\ngenerality and flexibility of CoT. In our approach, few-shot CoT demonstrations\nare wrapped in a formal syntax based on Python, and each prompt: identifies and\nnames steps; defines the input/output behavior of steps; and replaces CoT\nexplanations of in-context examples with chains of these formalized steps on\nthe same examples. Program Trace Prompting is applicable to many tasks,\nachieving strong results on the 23 diverse tasks in the BIG-Bench Hard\nbenchmark. More importantly, by instrumenting explanations in this way, we\nenable new types of analysis. In particular, we identify \"non-local errors\"\n(which correspond to incorrectly learning the reasoning method illustrated in\nthe demonstrations) as an unaddressed issue in CoT learning, and we present\nmethods for verifying the modularity of steps in a CoT explanation.",
      "tldr_zh": "本论文提出了一种名为 Program Trace Prompting 的 Chain of Thought (CoT) 变体，通过基于 Python 的正式语法包装少样本 CoT 演示，使解释更具可观察性，同时保留 CoT 的强大、通用性和灵活性。方法包括识别并命名步骤、定义步骤的输入/输出行为，并用这些正式化的步骤链替换示例解释，从而适用于多种任务，并在 BIG-Bench Hard 基准的23个多样任务上取得强劲结果。更重要的是，该方法启用新分析类型，如识别“non-local errors”（对应于错误学习演示中的推理方法）和验证 CoT 解释中步骤的模块性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15359v2",
      "published_date": "2024-09-17 23:47:20 UTC",
      "updated_date": "2024-10-01 20:24:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:39:44.129263"
    },
    {
      "arxiv_id": "2409.11605v1",
      "title": "Harnessing AI data-driven global weather models for climate attribution: An analysis of the 2017 Oroville Dam extreme atmospheric river",
      "title_zh": "利用 AI 数据驱动的全球天气模型进行气候归因：对 2017 年 Oroville Dam 极端大气河事件的分析",
      "authors": [
        "Jorge Baño-Medina",
        "Agniv Sengupta",
        "Allison Michaelis",
        "Luca Delle Monache",
        "Julie Kalansky",
        "Duncan Watson-Parris"
      ],
      "abstract": "AI data-driven models (Graphcast, Pangu Weather, Fourcastnet, and SFNO) are\nexplored for storyline-based climate attribution due to their short inference\ntimes, which can accelerate the number of events studied, and provide real time\nattributions when public attention is heightened. The analysis is framed on the\nextreme atmospheric river episode of February 2017 that contributed to the\nOroville dam spillway incident in Northern California. Past and future\nsimulations are generated by perturbing the initial conditions with the\npre-industrial and the late-21st century temperature climate change signals,\nrespectively. The simulations are compared to results from a dynamical model\nwhich represents plausible pseudo-realities under both climate environments.\nOverall, the AI models show promising results, projecting a 5-6 % increase in\nthe integrated water vapor over the Oroville dam in the present day compared to\nthe pre-industrial, in agreement with the dynamical model. Different\ngeopotential-moisture-temperature dependencies are unveiled for each of the\nAI-models tested, providing valuable information for understanding the\nphysicality of the attribution response. However, the AI models tend to\nsimulate weaker attribution values than the pseudo-reality imagined by the\ndynamical model, suggesting some reduced extrapolation skill, especially for\nthe late-21st century regime. Large ensembles generated with an AI model (>500\nmembers) produced statistically significant present-day to pre-industrial\nattribution results, unlike the >20-member ensemble from the dynamical model.\nThis analysis highlights the potential of AI models to conduct attribution\nanalysis, while emphasizing future lines of work on explainable artificial\nintelligence to gain confidence in these tools, which can enable reliable\nattribution studies in real-time.",
      "tldr_zh": "这篇论文探讨了利用 AI 数据驱动全球天气模型（如 Graphcast、Pangu Weather、Fourcastnet 和 SFNO）进行基于故事线的气候归因分析，焦点是 2017 年 2 月的极端 atmospheric river 事件导致的奥罗维尔大坝溢洪道事故。这些模型通过快速推理生成过去和未来模拟，与动态模型比较，结果显示 AI 模型准确预测了当代相对于工业革命前的水汽总量增加 5-6%。尽管 AI 模型揭示了不同的 geopotential-moisture-temperature 依赖关系并能通过大型集合 (>500 成员) 产生统计显著的归因结果，但它们模拟的归因值较弱，尤其在 21 世纪晚期情景下，突显了改进 extrapolation skill 和可解释性 AI 的必要性。",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "This Work has been submitted to Artificial Intelligence for the Earth\n  Systems",
      "pdf_url": "http://arxiv.org/pdf/2409.11605v1",
      "published_date": "2024-09-17 23:34:39 UTC",
      "updated_date": "2024-09-17 23:34:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:39:57.923767"
    },
    {
      "arxiv_id": "2409.11600v1",
      "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with Pythonic Syntax",
      "title_zh": "翻译失败",
      "authors": [
        "Augusto Seben da Rosa",
        "Marlon Daniel Angeli",
        "Jorge Aikes Junior",
        "Alef Iury Ferreira",
        "Lucas Rafael Gris",
        "Anderson da Silva Soares",
        "Arnaldo Candido Junior",
        "Frederico Santos de Oliveira",
        "Gabriel Trevisan Damke",
        "Rafael Teixeira Sousa"
      ],
      "abstract": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
      "tldr_zh": "本研究开发了No Saved Kaleidoscope，一种100% jitted的神经网络编码语言，使用C++、LLVM和Cuda构建，具有面向对象特性、强类型化、并行数据预处理、Pythonic语法、PyTorch-like模型声明和Automatic Differentiation功能。编译器通过缓存和池化机制管理VRAM，并利用cuBLAS进行高性能矩阵乘法，cuDNN处理卷积层。实验结果显示，在ImageNet上使用Residual Convolutional Neural Networks时，速度与PyTorch类似但性能下降；在GRU任务中，准确率相当但速度较慢；而在CIFAR-10基准上，性能和速度与PyTorch基本持平。该编译器的代码已开源于GitHub。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.LG",
        "68T07",
        "D.3; I.2; I.4; I.7"
      ],
      "primary_category": "cs.PL",
      "comment": "12 pages, 3 figures and 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.11600v1",
      "published_date": "2024-09-17 23:15:39 UTC",
      "updated_date": "2024-09-17 23:15:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:40:09.970105"
    },
    {
      "arxiv_id": "2409.11598v3",
      "title": "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "To Eun Kim",
        "Fernando Diaz"
      ],
      "abstract": "Modern language models frequently include retrieval components to improve\ntheir outputs, giving rise to a growing number of retrieval-augmented\ngeneration (RAG) systems. Yet, most existing work in RAG has underemphasized\nfair ranking techniques and neglected the diverse interests of all\nstakeholders. In this paper, we present the first comprehensive study of RAG\nsystems that incorporate fairness-aware rankings, focusing on both ranking\nfairness and attribution fairness - ensuring equitable exposure of sources\ncited in the final text. We specifically examine item-side fairness, i.e.,\nwhether retrieved documents receive balanced exposure, and assess how this\naffects both the system's overall performance and the eventual distribution of\ncited sources. Across twelve RAG models and seven tasks, we find that\nfairness-aware retrieval frequently retains or even improves ranking\neffectiveness and generation quality, countering the widespread belief that\nfairness compromises system performance. Moreover, we show that fair retrieval\nleads to more balanced attribution in the final responses, ensuring that the\ncited sources are credited more equitably. Our results underscore the\nimportance of item-side fairness throughout both retrieval and generation\nphases, offering key insights for building more responsible and equitable RAG\nsystems and illustrating promising avenues for future exploration in fair\nranking and source attribution.",
      "tldr_zh": "这篇论文探讨了在检索增强生成（RAG）系统中整合公平排名（fair ranking）的影响，首次全面研究了排名公平性和归因公平性，以确保检索文档获得平衡曝光并公平引用来源。研究方法包括评估项目侧公平性（item-side fairness）对系统性能和生成质量的影响，并在12个RAG模型和7个任务上进行实验。结果显示，公平感知检索（fairness-aware retrieval）不仅能保持或提升排名有效性和生成质量，还导致更均衡的归因分配，挑战了公平会损害性能的传统观点，并为构建更负责任的RAG系统提供了关键见解。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Top 5 Spotlight at AFME Workshop at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.11598v3",
      "published_date": "2024-09-17 23:10:04 UTC",
      "updated_date": "2025-02-25 04:13:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:40:20.521571"
    },
    {
      "arxiv_id": "2409.11593v2",
      "title": "Self-Contrastive Forward-Forward Algorithm",
      "title_zh": "翻译失败",
      "authors": [
        "Xing Chen",
        "Dongshu Liu",
        "Jeremie Laydevant",
        "Julie Grollier"
      ],
      "abstract": "Agents that operate autonomously benefit from lifelong learning capabilities.\nHowever, compatible training algorithms must comply with the decentralized\nnature of these systems, which imposes constraints on both the parameter counts\nand the computational resources. The Forward-Forward (FF) algorithm is one of\nthese. FF relies only on feedforward operations, the same used for inference,\nfor optimizing layer-wise objectives. This purely forward approach eliminates\nthe need for transpose operations required in traditional backpropagation.\nDespite its potential, FF has failed to reach state-of-the-art performance on\nmost standard benchmark tasks, in part due to unreliable negative data\ngeneration methods for unsupervised learning.\n  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)\nalgorithm, a competitive training method aimed at closing this performance gap.\nInspired by standard self-supervised contrastive learning for vision tasks,\nSCFF generates positive and negative inputs applicable across various datasets.\nThe method demonstrates superior performance compared to existing unsupervised\nlocal learning algorithms on several benchmark datasets, including MNIST,\nCIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training\nrecurrent neural networks, expanding its utility to sequential data tasks.\nThese findings pave the way for high-accuracy, real-time learning on\nresource-constrained edge devices.",
      "tldr_zh": "该论文提出 Self-Contrastive Forward-Forward (SCFF) 算法，以提升 Forward-Forward (FF) 算法在无监督学习中的性能，解决其负数据生成不可靠的问题。SCFF 借鉴自监督对比学习的方法，生成适用于各种数据集的正负输入，并在 MNIST、CIFAR-10、STL-10 和 Tiny ImageNet 等基准数据集上，比现有无监督本地学习算法表现出色。研究还扩展 FF 到训练 recurrent neural networks，用于处理顺序数据任务，并为资源受限的边缘设备实现高精度实时学习铺平道路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11593v2",
      "published_date": "2024-09-17 22:58:20 UTC",
      "updated_date": "2025-03-27 15:57:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:40:32.212278"
    },
    {
      "arxiv_id": "2409.17169v3",
      "title": "REAL: Response Embedding-based Alignment for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Honggen Zhang",
        "Xufeng Zhao",
        "Igor Molybog",
        "June Zhang"
      ],
      "abstract": "Aligning large language models (LLMs) to human preferences is a crucial step\nin building helpful and safe AI tools, which usually involve training on\nsupervised datasets. Popular algorithms such as Direct Preference Optimization\nrely on pairs of AI-generated responses ranked according to human feedback. The\nresponse pair annotation process is the most labor-intensive and costly part of\nthe alignment pipeline, and improving its efficiency and annotation quality\nwould have a meaningful impact on AI development. We propose REAL: Response\nEmbedding-based Alignment for LLMs, a strategy for constructing a high-quality\ntraining dataset that focuses on acquiring the most informative response pairs\nfor labeling out of a set of response candidates. Our selection process is\nbased on embedding responses independently of prompts. Experimental results on\nreal-world dataset SHP2 and synthetic HH-RLHF benchmarks indicate that choosing\ndissimilar response pairs enhances the direct alignment of LLMs while reducing\ninherited labeling errors. The model aligned on dissimilar response pairs\nobtained a better margin and win rate on the dialogue task. Our findings\nsuggest that focusing on distinct pairs can reduce the label error to improve\nthe efficiency of LLM alignment, saving up to 65% of annotators' work.",
      "tldr_zh": "该论文提出 REAL（Response Embedding-based Alignment for LLMs）方法，以提高大语言模型（LLMs）的对齐效率，该方法针对人类偏好对齐过程的昂贵标注问题，通过响应嵌入从候选响应中选择最信息丰富的（不相似）响应对进行标注。不同于传统方法如 Direct Preference Optimization，REAL 独立于提示，直接基于响应嵌入来优化选择过程，从而减少继承的标注错误。实验结果在 SHP2 和 HH-RLHF 数据集上表明，这种策略提升了 LLMs 在对话任务中的性能，并节省了高达 65% 的标注工作量。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17169v3",
      "published_date": "2024-09-17 22:40:54 UTC",
      "updated_date": "2024-12-21 02:07:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:40:43.917961"
    },
    {
      "arxiv_id": "2409.11589v1",
      "title": "ProSLM : A Prolog Synergized Language Model for explainable Domain Specific Knowledge Based Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Priyesh Vakharia",
        "Abigail Kufeldt",
        "Max Meyers",
        "Ian Lane",
        "Leilani Gilpin"
      ],
      "abstract": "Neurosymbolic approaches can add robustness to opaque neural systems by\nincorporating explainable symbolic representations. However, previous\napproaches have not used formal logic to contextualize queries to and validate\noutputs of large language models (LLMs). We propose \\systemname{}, a novel\nneurosymbolic framework, to improve the robustness and reliability of LLMs in\nquestion-answering tasks. We provide \\systemname{} with a domain-specific\nknowledge base, a logical reasoning system, and an integration to an existing\nLLM. This framework has two capabilities (1) context gathering: generating\nexplainable and relevant context for a given query, and (2) validation:\nconfirming and validating the factual accuracy of a statement in accordance\nwith a knowledge base (KB). Our work opens a new area of neurosymbolic\ngenerative AI text validation and user personalization.",
      "tldr_zh": "本文提出 ProSLM，一种结合 Prolog 的神经符号(neurosymbolic)框架，旨在提升大型语言模型(LLMs)在领域特定知识问答任务中的鲁棒性和可靠性。ProSLM 整合了领域特定知识库、逻辑推理系统以及现有 LLM 的接口，提供两大功能：（1）上下文收集，即为查询生成可解释的相关上下文；（2）验证，即根据知识库(KB)确认语句的准确性。该框架开辟了神经符号生成 AI 文本验证和用户个性化的新领域，增强了 AI 系统的可解释性和实际应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeSy 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.11589v1",
      "published_date": "2024-09-17 22:34:33 UTC",
      "updated_date": "2024-09-17 22:34:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:40:56.865788"
    },
    {
      "arxiv_id": "2410.03552v1",
      "title": "Evaluating Investment Risks in LATAM AI Startups: Ranking of Investment Potential and Framework for Valuation",
      "title_zh": "翻译失败",
      "authors": [
        "Abraham Ramos-Torres",
        "Laura N. Montoya"
      ],
      "abstract": "The growth of the tech startup ecosystem in Latin America (LATAM) is driven\nby innovative entrepreneurs addressing market needs across various sectors.\nHowever, these startups encounter unique challenges and risks that require\nspecific management approaches. This paper explores a case study with the Total\nAddressable Market (TAM), Serviceable Available Market (SAM), and Serviceable\nObtainable Market (SOM) metrics within the context of the online food delivery\nindustry in LATAM, serving as a model for valuing startups using the Discounted\nCash Flow (DCF) method. By analyzing key emerging powers such as Argentina,\nColombia, Uruguay, Costa Rica, Panama, and Ecuador, the study highlights the\npotential and profitability of AI-driven startups in the region through the\ndevelopment of a ranking of emerging powers in Latin America for tech startup\ninvestment. The paper also examines the political, economic, and competitive\nrisks faced by startups and offers strategic insights on mitigating these risks\nto maximize investment returns. Furthermore, the research underscores the value\nof diversifying investment portfolios with startups in emerging markets,\nemphasizing the opportunities for substantial growth and returns despite\ninherent risks.",
      "tldr_zh": "这篇论文评估了拉丁美洲 (LATAM) AI 初创企业的投资风险，并开发了一个排名系统来评估多个国家（如 Argentina、Colombia、Uruguay、Costa Rica、Panama 和 Ecuador）的投资潜力。研究以在线食品配送行业为例，运用 Total Addressable Market (TAM)、Serviceable Available Market (SAM) 和 Serviceable Obtainable Market (SOM) 指标结合 Discounted Cash Flow (DCF) 方法，构建了一个初创企业估值框架。论文分析了政治、经济和竞争风险，并提出缓解策略，以帮助投资者最大化回报并通过多样化投资组合抓住新兴市场的增长机会。",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "q-fin.PM",
        "q-fin.PR"
      ],
      "primary_category": "q-fin.GN",
      "comment": "21 pages, 7 figures, 8 tables, Accepted for publication to the\n  International Association for Applied Business Research Journal (IAABR)",
      "pdf_url": "http://arxiv.org/pdf/2410.03552v1",
      "published_date": "2024-09-17 22:31:46 UTC",
      "updated_date": "2024-09-17 22:31:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:41:09.298725"
    },
    {
      "arxiv_id": "2409.11583v1",
      "title": "Uncertainty Decomposition and Error Margin Detection of Homodyned-K Distribution in Quantitative Ultrasound",
      "title_zh": "翻译失败",
      "authors": [
        "Dorsa Ameri",
        "Ali K. Z. Tehrani",
        "Ivan M. Rosado-Mendez",
        "Hassan Rivaz"
      ],
      "abstract": "Homodyned K-distribution (HK-distribution) parameter estimation in\nquantitative ultrasound (QUS) has been recently addressed using Bayesian Neural\nNetworks (BNNs). BNNs have been shown to significantly reduce computational\ntime in speckle statistics-based QUS without compromising accuracy and\nprecision. Additionally, they provide estimates of feature uncertainty, which\ncan guide the clinician's trust in the reported feature value. The total\npredictive uncertainty in Bayesian modeling can be decomposed into epistemic\n(uncertainty over the model parameters) and aleatoric (uncertainty inherent in\nthe data) components. By decomposing the predictive uncertainty, we can gain\ninsights into the factors contributing to the total uncertainty. In this study,\nwe propose a method to compute epistemic and aleatoric uncertainties for\nHK-distribution parameters ($\\alpha$ and $k$) estimated by a BNN, in both\nsimulation and experimental data. In addition, we investigate the relationship\nbetween the prediction error and both uncertainties, shedding light on the\ninterplay between these uncertainties and HK parameters errors.",
      "tldr_zh": "本文提出了一种方法，使用 Bayesian Neural Networks (BNNs) 估计 Homodyned-K Distribution (HK-distribution) 在定量超声 (QUS) 中的参数 ($\\alpha$ 和 $k$)，从而显著减少计算时间，同时保持准确性和精确性。研究重点是将预测不确定性分解为认识论 (epistemic) 和偶然 (aleatoric) 组件，以揭示不确定性的来源，并在模拟和实验数据中进行验证。主要发现是，预测错误与这两种不确定性密切相关，这有助于临床医生评估 HK 参数估计的可靠性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "eess.IV",
        "physics.med-ph",
        "stat.ML"
      ],
      "primary_category": "eess.SP",
      "comment": "4 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11583v1",
      "published_date": "2024-09-17 22:16:49 UTC",
      "updated_date": "2024-09-17 22:16:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:41:23.197096"
    },
    {
      "arxiv_id": "2409.18992v2",
      "title": "A Review of Mechanistic Models of Event Comprehension",
      "title_zh": "事件理解的机制模型综述",
      "authors": [
        "Tan T. Nguyen"
      ],
      "abstract": "This review examines theoretical assumptions and computational models of\nevent comprehension, tracing the evolution from discourse comprehension\ntheories to contemporary event cognition frameworks. The review covers key\ndiscourse comprehension accounts, including Construction-Integration, Event\nIndexing, Causal Network, and Resonance models, highlighting their\ncontributions to understanding cognitive processes in comprehension. I then\ndiscuss contemporary theoretical frameworks of event comprehension, including\nEvent Segmentation Theory (Zacks et al., 2007), the Event Horizon Model\n(Radvansky & Zacks, 2014), and Hierarchical Generative Framework (Kuperberg,\n2021), which emphasize prediction, causality, and multilevel representations in\nevent understanding. Building on these theories, I evaluate five computational\nmodels of event comprehension: REPRISE (Butz et al., 2019), Structured Event\nMemory (SEM; Franklin et al., 2020), the Lu model (Lu et al., 2022), the\nGumbsch model (Gumbsch et al., 2022), and the Elman and McRae model (2019). The\nanalysis focuses on their approaches to hierarchical processing, prediction\nmechanisms, and representation learning. Key themes that emerge include the use\nof hierarchical structures as inductive biases, the importance of prediction in\ncomprehension, and diverse strategies for learning event dynamics. The review\nidentifies critical areas for future research, including the need for more\nsophisticated approaches to learning structured representations, integrating\nepisodic memory mechanisms, and developing adaptive updating algorithms for\nworking event models. By synthesizing insights from both theoretical frameworks\nand computational implementations, this review aims to advance our\nunderstanding of human event comprehension and guide future modeling efforts in\ncognitive science.",
      "tldr_zh": "这篇综述回顾了事件理解（event comprehension）的理论假设和计算模型，从话语理解理论（如 Construction-Integration、Event Indexing、Causal Network 和 Resonance models）演变到当代框架（如 Event Segmentation Theory、Event Horizon Model 和 Hierarchical Generative Framework），强调预测、因果性和多层表示在事件认知中的作用。作者评估了五个计算模型，包括 REPRISE、Structured Event Memory (SEM)、Lu model、Gumbsch model 和 Elman and McRae model，焦点在于它们的层次处理、预测机制和表示学习策略。关键主题包括使用层次结构作为归纳偏差、预测在理解中的重要性，以及学习事件动态的多样方法；未来研究应关注更先进的结构表示学习、整合情节记忆机制以及适应性更新算法，以推进认知科学对人类事件理解的建模。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18992v2",
      "published_date": "2024-09-17 22:10:05 UTC",
      "updated_date": "2024-11-25 16:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:41:32.589221"
    },
    {
      "arxiv_id": "2409.11576v1",
      "title": "Automating proton PBS treatment planning for head and neck cancers using policy gradient-based deep reinforcement learning",
      "title_zh": "翻译失败",
      "authors": [
        "Qingqing Wang",
        "Chang Chang"
      ],
      "abstract": "Proton pencil beam scanning (PBS) treatment planning for head and neck (H&N)\ncancers is a time-consuming and experience-demanding task where a large number\nof planning objectives are involved. Deep reinforcement learning (DRL) has\nrecently been introduced to the planning processes of intensity-modulated\nradiation therapy and brachytherapy for prostate, lung, and cervical cancers.\nHowever, existing approaches are built upon the Q-learning framework and\nweighted linear combinations of clinical metrics, suffering from poor\nscalability and flexibility and only capable of adjusting a limited number of\nplanning objectives in discrete action spaces. We propose an automatic\ntreatment planning model using the proximal policy optimization (PPO) algorithm\nand a dose distribution-based reward function for proton PBS treatment planning\nof H&N cancers. Specifically, a set of empirical rules is used to create\nauxiliary planning structures from target volumes and organs-at-risk (OARs),\nalong with their associated planning objectives. These planning objectives are\nfed into an in-house optimization engine to generate the spot monitor unit (MU)\nvalues. A decision-making policy network trained using PPO is developed to\niteratively adjust the involved planning objective parameters in a continuous\naction space and refine the PBS treatment plans using a novel dose\ndistribution-based reward function. Proton H&N treatment plans generated by the\nmodel show improved OAR sparing with equal or superior target coverage when\ncompared with human-generated plans. Moreover, additional experiments on liver\ncancer demonstrate that the proposed method can be successfully generalized to\nother treatment sites. To the best of our knowledge, this is the first\nDRL-based automatic treatment planning model capable of achieving human-level\nperformance for H&N cancers.",
      "tldr_zh": "本研究针对质子铅笔束扫描 (PBS) 头颈部 (H&N) 癌症治疗计划的耗时性和经验依赖问题，提出了一种基于近端策略优化 (PPO) 算法的深度强化学习 (DRL) 自动规划模型，使用基于剂量分布的奖励函数在连续动作空间中迭代调整规划目标参数。模型通过经验规则创建辅助规划结构，从目标体积和危险器官 (OARs) 生成规划目标，并输入优化引擎生成 spot 监视单元 (MU) 值，从而实现精确的治疗计划优化。实验结果显示，该模型生成的质子 H&N 治疗计划在改善 OAR 保护的同时，实现了与人工计划相当或优越的目标覆盖率，并成功推广到肝癌等其他部位，首次实现了 DRL 在 H&N 癌症治疗中达到人类水平性能。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11576v1",
      "published_date": "2024-09-17 22:01:56 UTC",
      "updated_date": "2024-09-17 22:01:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:41:44.662871"
    },
    {
      "arxiv_id": "2409.11564v2",
      "title": "Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Genta Indra Winata",
        "Hanyang Zhao",
        "Anirban Das",
        "Wenpin Tang",
        "David D. Yao",
        "Shi-Xiong Zhang",
        "Sambit Sahu"
      ],
      "abstract": "Preference tuning is a crucial process for aligning deep generative models\nwith human preferences. This survey offers a thorough overview of recent\nadvancements in preference tuning and the integration of human feedback. The\npaper is organized into three main sections: 1) introduction and preliminaries:\nan introduction to reinforcement learning frameworks, preference tuning tasks,\nmodels, and datasets across various modalities: language, speech, and vision,\nas well as different policy approaches, 2) in-depth exploration of each\npreference tuning approach: a detailed analysis of the methods used in\npreference tuning, and 3) applications, discussion, and future directions: an\nexploration of the applications of preference tuning in downstream tasks,\nincluding evaluation methods for different modalities, and an outlook on future\nresearch directions. Our objective is to present the latest methodologies in\npreference tuning and model alignment, enhancing the understanding of this\nfield for researchers and practitioners. We hope to encourage further\nengagement and innovation in this area.",
      "tldr_zh": "这篇论文对偏好调整（preference tuning）进行了全面调查，聚焦于使用人类反馈（human feedback）来对齐语言、语音和视觉任务中的深度生成模型。论文分为三个部分：首先介绍强化学习框架（reinforcement learning frameworks）、相关任务、模型、数据集和策略方法；其次深入分析各种偏好调整方法；最后探讨下游任务的应用、评估方法以及未来研究方向。总体而言，该调查旨在总结最新方法论，提升研究者和从业者对该领域的理解，并鼓励进一步创新。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Survey paper",
      "pdf_url": "http://arxiv.org/pdf/2409.11564v2",
      "published_date": "2024-09-17 21:28:51 UTC",
      "updated_date": "2024-11-03 01:51:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:41:56.348951"
    },
    {
      "arxiv_id": "2410.02783v1",
      "title": "Enhancing Mental Health Support through Human-AI Collaboration: Toward Secure and Empathetic AI-enabled chatbots",
      "title_zh": "翻译失败",
      "authors": [
        "Rawan AlMakinah",
        "Andrea Norcini-Pala",
        "Lindsey Disney",
        "M. Abdullah Canbaz"
      ],
      "abstract": "Access to mental health support remains limited, particularly in marginalized\ncommunities where structural and cultural barriers hinder timely care. This\npaper explores the potential of AI-enabled chatbots as a scalable solution,\nfocusing on advanced large language models (LLMs)-GPT v4, Mistral Large, and\nLLama V3.1-and assessing their ability to deliver empathetic, meaningful\nresponses in mental health contexts. While these models show promise in\ngenerating structured responses, they fall short in replicating the emotional\ndepth and adaptability of human therapists. Additionally, trustworthiness,\nbias, and privacy challenges persist due to unreliable datasets and limited\ncollaboration with mental health professionals. To address these limitations,\nwe propose a federated learning framework that ensures data privacy, reduces\nbias, and integrates continuous validation from clinicians to enhance response\nquality. This approach aims to develop a secure, evidence-based AI chatbot\ncapable of offering trustworthy, empathetic, and bias-reduced mental health\nsupport, advancing AI's role in digital mental health care.",
      "tldr_zh": "本研究探讨了AI聊天机器人如何通过人机协作提升心理健康支持，特别是针对边缘化社区的访问障碍。评估显示，大语言模型(LLMs)如GPT-4、Mistral Large和Llama 3.1在生成结构化响应方面表现出潜力，但无法完全复制人类治疗师的情感深度，且面临信任、偏见和隐私挑战。针对这些问题，论文提出一个federated learning框架，确保数据隐私、减少偏见，并整合临床医生的持续验证，以开发安全的、基于证据的AI聊天机器人，提供可信、富有同情心的心理健康支持。该方法旨在推进AI在数字心理健康护理中的应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "68T01, 62P15",
        "I.2.0; K.4.0; H.5.0"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages, 9 Figures",
      "pdf_url": "http://arxiv.org/pdf/2410.02783v1",
      "published_date": "2024-09-17 20:49:13 UTC",
      "updated_date": "2024-09-17 20:49:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:42:07.907960"
    },
    {
      "arxiv_id": "2409.11552v1",
      "title": "Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images",
      "title_zh": "多领域数据聚合用于组织学图像中轴突和髓鞘分割",
      "authors": [
        "Armand Collin",
        "Arthur Boschet",
        "Mathieu Boudreau",
        "Julien Cohen-Adad"
      ],
      "abstract": "Quantifying axon and myelin properties (e.g., axon diameter, myelin\nthickness, g-ratio) in histology images can provide useful information about\nmicrostructural changes caused by neurodegenerative diseases. Automatic tissue\nsegmentation is an important tool for these datasets, as a single stained\nsection can contain up to thousands of axons. Advances in deep learning have\nmade this task quick and reliable with minimal overhead, but a deep learning\nmodel trained by one research group will hardly ever be usable by other groups\ndue to differences in their histology training data. This is partly due to\nsubject diversity (different body parts, species, genetics, pathologies) and\nalso to the range of modern microscopy imaging techniques resulting in a wide\nvariability of image features (i.e., contrast, resolution). There is a pressing\nneed to make AI accessible to neuroscience researchers to facilitate and\naccelerate their workflow, but publicly available models are scarce and poorly\nmaintained. Our approach is to aggregate data from multiple imaging modalities\n(bright field, electron microscopy, Raman spectroscopy) and species (mouse,\nrat, rabbit, human), to create an open-source, durable tool for axon and myelin\nsegmentation. Our generalist model makes it easier for researchers to process\ntheir data and can be fine-tuned for better performance on specific domains. We\nstudy the benefits of different aggregation schemes. This multi-domain\nsegmentation model performs better than single-modality dedicated learners\n(p=0.03077), generalizes better on out-of-distribution data and is easier to\nuse and maintain. Importantly, we package the segmentation tool into a\nwell-maintained open-source software ecosystem (see\nhttps://github.com/axondeepseg/axondeepseg).",
      "tldr_zh": "本文提出一种多领域数据聚合方法，用于组织学图像中 axon 和 myelin 的分割，以解决现有深度学习模型因数据多样性（如不同物种、成像模式）而难以泛化的问题。该方法整合多种成像模式（包括 bright field、electron microscopy 和 Raman spectroscopy）以及不同物种（如小鼠、大鼠、兔子和人类）的数据，创建了一个通用模型，可轻松微调以适应特定领域。实验结果显示，该多领域模型比单一模式模型性能更优（p=0.03077），并在处理分布外数据时表现出更好的泛化能力。最后，该工具被打包成维护良好的开源软件（https://github.com/axondeepseg/axondeepseg），为神经科学研究者提供便捷的轴突和髓鞘量化工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11552v1",
      "published_date": "2024-09-17 20:47:32 UTC",
      "updated_date": "2024-09-17 20:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:42:21.828015"
    },
    {
      "arxiv_id": "2409.11547v2",
      "title": "Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Guillermo Marco",
        "Luz Rello",
        "Julio Gonzalo"
      ],
      "abstract": "In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART-large, and compare its performance\nto human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human study in which 68\nparticipants rated short stories from humans and the SLM on grammaticality,\nrelevance, creativity, and attractiveness, and (ii) a qualitative linguistic\nanalysis examining the textual characteristics of stories produced by each\nmodel. In the first experiment, BART-large outscored average human writers\noverall (2.11 vs. 1.85), a 14% relative improvement, though the slight human\nadvantage in creativity was not statistically significant. In the second\nexperiment, qualitative analysis showed that while GPT-4o demonstrated\nnear-perfect coherence and used less cliche phrases, it tended to produce more\npredictable language, with only 3% of its synopses featuring surprising\nassociations (compared to 15% for BART). These findings highlight how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks, and demonstrate that smaller models can,\nin certain contexts, rival both humans and larger models.",
      "tldr_zh": "本研究评估了微调后的 Small Language Model (SLM) BART-large 在短创意写作中的表现，并与人类作家、GPT-3.5 和 GPT-4o 进行比较。实验包括（i）68 名参与者对故事的语法性、相关性、创意性和吸引力的评分，结果显示 BART-large 整体得分高于平均人类作家（2.11 vs. 1.85，相对提升14%），尽管人类在创意性上略有优势但不显著；以及（ii）定性语言分析，揭示 GPT-4o 具有近乎完美的连贯性和更少陈词滥调，但其语言更可预测（如仅3%的梗概有惊喜关联，而 BART-large 为15%）。这些发现强调了模型大小和微调如何影响创意写作中的创意、流畅性和连贯性平衡，并证明 SLMs 在特定情境下可匹敌人类和 Large Language Models (LLMs)。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as Main Conference Paper at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.11547v2",
      "published_date": "2024-09-17 20:40:02 UTC",
      "updated_date": "2025-01-13 15:37:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:42:33.246925"
    },
    {
      "arxiv_id": "2409.11546v1",
      "title": "NCT-CRC-HE: Not All Histopathological Datasets Are Equally Useful",
      "title_zh": "NCT-CRC-HE：并非所有组织病理学数据集都同样有用",
      "authors": [
        "Andrey Ignatov",
        "Grigory Malivenko"
      ],
      "abstract": "Numerous deep learning-based solutions have been proposed for\nhistopathological image analysis over the past years. While they usually\ndemonstrate exceptionally high accuracy, one key question is whether their\nprecision might be affected by low-level image properties not related to\nhistopathology but caused by microscopy image handling and pre-processing. In\nthis paper, we analyze a popular NCT-CRC-HE-100K colorectal cancer dataset used\nin numerous prior works and show that both this dataset and the obtained\nresults may be affected by data-specific biases. The most prominent revealed\ndataset issues are inappropriate color normalization, severe JPEG artifacts\ninconsistent between different classes, and completely corrupted tissue samples\nresulting from incorrect image dynamic range handling. We show that even the\nsimplest model using only 3 features per image (red, green and blue color\nintensities) can demonstrate over 50% accuracy on this 9-class dataset, while\nusing color histogram not explicitly capturing cell morphology features yields\nover 82% accuracy. Moreover, we show that a basic EfficientNet-B0 ImageNet\npretrained model can achieve over 97.7% accuracy on this dataset, outperforming\nall previously proposed solutions developed for this task, including dedicated\nfoundation histopathological models and large cell morphology-aware neural\nnetworks. The NCT-CRC-HE dataset is publicly available and can be freely used\nto replicate the presented results. The codes and pre-trained models used in\nthis paper are available at\nhttps://github.com/gmalivenko/NCT-CRC-HE-experiments",
      "tldr_zh": "本论文分析了流行的 NCT-CRC-HE-100K 组织病理图像数据集，揭示其存在数据特定偏差，如不适当的颜色归一化、严重的 JPEG artifacts 和图像动态范围处理错误，这些问题可能导致模型高准确率并非基于真实组织病理学特征。研究者通过测试简单模型（如仅使用 RGB 颜色强度特征）发现，即使不捕捉细胞形态，该模型也能在该 9 类数据集上达到 50% 准确率，而使用颜色直方图可达 82% 准确率。进一步，EfficientNet-B0 ImageNet 预训练模型在该数据集上实现了 97.7% 的准确率，超过了现有专属组织病理模型；论文公开了代码和预训练模型，以促进结果复现和数据集改进。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11546v1",
      "published_date": "2024-09-17 20:36:03 UTC",
      "updated_date": "2024-09-17 20:36:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:42:44.439549"
    },
    {
      "arxiv_id": "2409.11527v2",
      "title": "Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Fatemeh Haji",
        "Mazal Bethany",
        "Maryam Tabar",
        "Jason Chiang",
        "Anthony Rios",
        "Peyman Najafirad"
      ],
      "abstract": "Multi-agent strategies have emerged as a promising approach to enhance the\nreasoning abilities of Large Language Models (LLMs) by assigning specialized\nroles in the problem-solving process. Concurrently, Tree of Thoughts (ToT)\nmethods have shown potential in improving reasoning for complex\nquestion-answering tasks by exploring diverse reasoning paths. A critical\nlimitation in multi-agent reasoning is the 'Reasoner' agent's shallow\nexploration of reasoning paths. While ToT strategies could help mitigate this\nproblem, they may generate flawed reasoning branches, which could harm the\ntrustworthiness of the final answer. To leverage the strengths of both\nmulti-agent reasoning and ToT strategies, we introduce a novel approach\ncombining ToT-based Reasoner agents with a Thought Validator agent. Multiple\nReasoner agents operate in parallel, employing ToT to explore diverse reasoning\npaths. The Thought Validator then scrutinizes these paths, considering a\nReasoner's conclusion only if its reasoning is valid. This method enables a\nmore robust voting strategy by discarding faulty reasoning paths, enhancing the\nsystem's ability to tackle tasks requiring systematic and trustworthy\nreasoning. Our method demonstrates superior performance compared to existing\ntechniques when evaluated on the GSM8K dataset, outperforming the standard ToT\nstrategy by an average 5.6% across four LLMs. The code and related content can\nbe found in: https://github.com/SecureAIAutonomyLab/MA-ToT",
      "tldr_zh": "本论文提出了一种改进 Large Language Models (LLMs) 推理的方法，通过 Multi-Agent 系统结合 Tree-of-Thought (ToT) 策略。\n该方法部署多个 ToT-based Reasoner 代理并行探索推理路径，并引入 Thought Validator 代理来验证这些路径的有效性，仅保留可靠的结论，从而提升系统的可信度和决策准确性。\n实验结果显示，在 GSM8K 数据集上，该方法比标准 ToT 策略平均提高了 5.6% 的性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11527v2",
      "published_date": "2024-09-17 19:54:37 UTC",
      "updated_date": "2024-11-05 01:34:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:42:56.958430"
    },
    {
      "arxiv_id": "2409.11513v2",
      "title": "Mamba Fusion: Learning Actions Through Questioning",
      "title_zh": "Mamba Fusion：通过提问学习动作",
      "authors": [
        "Zhikang Dong",
        "Apoorva Beedu",
        "Jason Sheinkopf",
        "Irfan Essa"
      ],
      "abstract": "Video Language Models (VLMs) are crucial for generalizing across diverse\ntasks and using language cues to enhance learning. While transformer-based\narchitectures have been the de facto in vision-language training, they face\nchallenges like quadratic computational complexity, high GPU memory usage, and\ndifficulty with long-term dependencies. To address these limitations, we\nintroduce MambaVL, a novel model that leverages recent advancements in\nselective state space modality fusion to efficiently capture long-range\ndependencies and learn joint representations for vision and language data.\nMambaVL utilizes a shared state transition matrix across both modalities,\nallowing the model to capture information about actions from multiple\nperspectives within the scene. Furthermore, we propose a question-answering\ntask that helps guide the model toward relevant cues. These questions provide\ncritical information about actions, objects, and environmental context, leading\nto enhanced performance. As a result, MambaVL achieves state-of-the-art\nperformance in action recognition on the Epic-Kitchens-100 dataset and\noutperforms baseline methods in action anticipation.",
      "tldr_zh": "本研究针对 Video Language Models (VLMs) 的局限性，如 Transformer 架构的二次计算复杂性、高 GPU 内存占用和处理长距离依赖困难，引入了 MambaVL 模型。该模型采用 selective state space modality fusion 和共享状态转移矩阵，从多个视角捕获视觉与语言数据的联合表示，并通过一个 question-answering 任务提供关于动作、物体和环境上下文的关键线索。结果显示，MambaVL 在 Epic-Kitchens-100 数据集上的动作识别任务中实现了 state-of-the-art 性能，并在动作预测方面超越了基线方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11513v2",
      "published_date": "2024-09-17 19:36:37 UTC",
      "updated_date": "2025-01-30 22:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:43:08.872969"
    },
    {
      "arxiv_id": "2409.11509v2",
      "title": "FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction",
      "title_zh": "翻译失败",
      "authors": [
        "Ziwei Li",
        "Xiaoqi Wang",
        "Hong-You Chen",
        "Han-Wei Shen",
        "Wei-Lun Chao"
      ],
      "abstract": "Federated learning (FL) has rapidly evolved as a promising paradigm that\nenables collaborative model training across distributed participants without\nexchanging their local data. Despite its broad applications in fields such as\ncomputer vision, graph learning, and natural language processing, the\ndevelopment of a data projection model that can be effectively used to\nvisualize data in the context of FL is crucial yet remains heavily\nunder-explored. Neighbor embedding (NE) is an essential technique for\nvisualizing complex high-dimensional data, but collaboratively learning a joint\nNE model is difficult. The key challenge lies in the objective function, as\neffective visualization algorithms like NE require computing loss functions\namong pairs of data. In this paper, we introduce \\textsc{FedNE}, a novel\napproach that integrates the \\textsc{FedAvg} framework with the contrastive NE\ntechnique, without any requirements of shareable data. To address the lack of\ninter-client repulsion which is crucial for the alignment in the global\nembedding space, we develop a surrogate loss function that each client learns\nand shares with each other. Additionally, we propose a data-mixing strategy to\naugment the local data, aiming to relax the problems of invisible neighbors and\nfalse neighbors constructed by the local $k$NN graphs. We conduct comprehensive\nexperiments on both synthetic and real-world datasets. The results demonstrate\nthat our \\textsc{FedNE} can effectively preserve the neighborhood data\nstructures and enhance the alignment in the global embedding space compared to\nseveral baseline methods.",
      "tldr_zh": "本文提出FedNE，一种基于surrogate loss function的联邦邻居嵌入框架，用于在Federated Learning (FL) 中实现高维数据的降维和可视化，而无需共享本地数据。FedNE 整合了FedAvg框架与对比NE技术，通过开发代理损失函数来处理全局嵌入空间中客户端间排斥问题，并引入data-mixing strategy来增强本地数据，缓解kNN图中的不可见邻居和假邻居问题。实验在合成和真实数据集上表明，FedNE 比基线方法更有效地保留邻域数据结构并提升全局嵌入空间的alignment。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11509v2",
      "published_date": "2024-09-17 19:23:24 UTC",
      "updated_date": "2024-10-14 00:55:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:43:20.414771"
    },
    {
      "arxiv_id": "2410.02780v2",
      "title": "Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Eleonora Lopez",
        "Luigi Sigillo",
        "Federica Colonnese",
        "Massimo Panella",
        "Danilo Comminiello"
      ],
      "abstract": "Generating images from brain waves is gaining increasing attention due to its\npotential to advance brain-computer interface (BCI) systems by understanding\nhow brain signals encode visual cues. Most of the literature has focused on\nfMRI-to-Image tasks as fMRI is characterized by high spatial resolution.\nHowever, fMRI is an expensive neuroimaging modality and does not allow for\nreal-time BCI. On the other hand, electroencephalography (EEG) is a low-cost,\nnon-invasive, and portable neuroimaging technique, making it an attractive\noption for future real-time applications. Nevertheless, EEG presents inherent\nchallenges due to its low spatial resolution and susceptibility to noise and\nartifacts, which makes generating images from EEG more difficult. In this\npaper, we address these problems with a streamlined framework based on the\nControlNet adapter for conditioning a latent diffusion model (LDM) through EEG\nsignals. We conduct experiments and ablation studies on popular benchmarks to\ndemonstrate that the proposed method beats other state-of-the-art models.\nUnlike these methods, which often require extensive preprocessing, pretraining,\ndifferent losses, and captioning models, our approach is efficient and\nstraightforward, requiring only minimal preprocessing and a few components. The\ncode is available at https://github.com/LuigiSigillo/GWIT.",
      "tldr_zh": "本研究聚焦于从脑电图(EEG)生成图像，以提升脑机接口(BCI)系统的视觉理解能力，解决EEG的低空间分辨率和噪声问题。作者提出一个简化框架，使用ControlNet适配器来调节潜在扩散模型(LDM)，仅需最少的预处理和组件，即可高效处理EEG信号。实验结果显示，该方法在流行基准上超越了现有最先进模型，提高了生成性能，并提供了开源代码以便进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.02780v2",
      "published_date": "2024-09-17 19:07:13 UTC",
      "updated_date": "2025-01-10 18:14:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:43:32.273010"
    },
    {
      "arxiv_id": "2409.11501v1",
      "title": "Egalitarian Language Representation in Language Models: It All Begins with Tokenizers",
      "title_zh": "翻译失败",
      "authors": [
        "Menan Velayuthan",
        "Kengatharaiyer Sarveswaran"
      ],
      "abstract": "Tokenizers act as a bridge between human language and the latent space of\nlanguage models, influencing how language is represented in these models. Due\nto the immense popularity of English-Centric Large Language Models (LLMs),\nefforts are being made to adapt them for other languages. However, we\ndemonstrate that, from a tokenization standpoint, not all tokenizers offer fair\nrepresentation for complex script languages such as Tamil, Sinhala, and Hindi,\nprimarily due to the choice of pre-tokenization methods. We go further to show\nthat pre-tokenization plays a more critical role than the tokenization\nalgorithm itself in achieving an egalitarian representation of these complex\nscript languages. To address this, we introduce an improvement to the Byte Pair\nEncoding (BPE) algorithm by incorporating graphemes, which we term Grapheme\nPair Encoding (GPE). Our experiments show that grapheme-based character\nextraction outperforms byte-level tokenizers for complex scripts. We validate\nthis approach through experiments on Tamil, Sinhala, and Hindi.",
      "tldr_zh": "该研究探讨了语言模型中Tokenizers对语言表示的公平性问题，指出现有Tokenizers在处理复杂脚本语言如Tamil、Sinhala和Hindi时，由于预处理方法的选择，导致表示不平等，而预处理方法比Tokenization算法本身更关键。作者引入Grapheme Pair Encoding (GPE)，这是一种对Byte Pair Encoding (BPE)的改进，通过整合graphemes来提升对这些语言的表示公平性。实验结果显示，GPE在Tamil、Sinhala和Hindi上的表现优于byte-level tokenizers，从而为构建更具包容性的LLMs提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Content - 8 pages, References - 3 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.11501v1",
      "published_date": "2024-09-17 19:05:37 UTC",
      "updated_date": "2024-09-17 19:05:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:43:43.948984"
    },
    {
      "arxiv_id": "2409.11500v1",
      "title": "Multi-Document Grounded Multi-Turn Synthetic Dialog Generation",
      "title_zh": "基于多文档的多轮合成对话生成",
      "authors": [
        "Young-Suk Lee",
        "Chulaka Gunasekara",
        "Danish Contractor",
        "Ramón Fernandez Astudillo",
        "Radu Florian"
      ],
      "abstract": "We introduce a technique for multi-document grounded multi-turn synthetic\ndialog generation that incorporates three main ideas. First, we control the\noverall dialog flow using taxonomy-driven user queries that are generated with\nChain-of-Thought (CoT) prompting. Second, we support the generation of\nmulti-document grounded dialogs by mimicking real-world use of retrievers to\nupdate the grounding documents after every user-turn in the dialog. Third, we\napply LLM-as-a-Judge to filter out queries with incorrect answers. Human\nevaluation of the synthetic dialog data suggests that the data is diverse,\ncoherent, and includes mostly correct answers. Both human and automatic\nevaluations of answerable queries indicate that models fine-tuned on synthetic\ndialogs consistently out-perform those fine-tuned on existing human generated\ntraining data across four publicly available multi-turn document grounded\nbenchmark test sets.",
      "tldr_zh": "本文提出了一种多文档 grounded 多轮合成对话生成技术，旨在通过三个关键创新提升对话质量：首先，使用 Chain-of-Thought (CoT) 提示生成分类学驱动的用户查询来控制对话流程；其次，模拟现实检索器在每个用户回合后更新基础文档，支持多文档 grounded；第三，应用 LLM-as-a-Judge 过滤不正确答案的查询。人类评估表明，该合成对话数据多样、连贯且答案大多正确。实验结果显示，使用合成对话微调的模型在四个公开的多轮文档 grounded 基准测试集上，表现优于基于现有人类生成训练数据的模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11500v1",
      "published_date": "2024-09-17 19:02:39 UTC",
      "updated_date": "2024-09-17 19:02:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:43:56.189127"
    },
    {
      "arxiv_id": "2409.11498v1",
      "title": "Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ilaria Manco",
        "Justin Salamon",
        "Oriol Nieto"
      ],
      "abstract": "Audio-text contrastive models have become a powerful approach in music\nrepresentation learning. Despite their empirical success, however, little is\nknown about the influence of key design choices on the quality of music-text\nrepresentations learnt through this framework. In this work, we expose these\ndesign choices within the constraints of limited data and computation budgets,\nand establish a more solid understanding of their impact grounded in empirical\nobservations along three axes: the choice of base encoders, the level of\ncuration in training data, and the use of text augmentation. We find that data\ncuration is the single most important factor for music-text contrastive\ntraining in resource-constrained scenarios. Motivated by this insight, we\nintroduce two novel techniques, Augmented View Dropout and TextSwap, which\nincrease the diversity and descriptiveness of text inputs seen in training.\nThrough our experiments we demonstrate that these are effective at boosting\nperformance across different pre-training regimes, model architectures, and\ndownstream data distributions, without incurring higher computational costs or\nrequiring additional training data.",
      "tldr_zh": "本研究探讨了音频-文本对比模型（Audio-text contrastive models）在资源受限场景下的关键设计选择，包括基础编码器、训练数据 curation 水平和文本增强的影响，并发现数据 curation 是最重要因素。作者引入两种新技巧：Augmented View Dropout 和 TextSwap，以提升 LLM Captions 的多样性和描述性，从而改善音乐-文本表示学习。实验结果显示，这些技巧在不同预训练模式、模型架构和下游数据分布中显著提升性能，而无需增加计算成本或额外训练数据。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "To appear in the Proceedings of the 25th International Society for\n  Music Information Retrieval Conference (ISMIR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.11498v1",
      "published_date": "2024-09-17 19:00:21 UTC",
      "updated_date": "2024-09-17 19:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:44:07.673938"
    },
    {
      "arxiv_id": "2410.03551v1",
      "title": "Constructive Apraxia: An Unexpected Limit of Instructible Vision-Language Models and Analog for Human Cognitive Disorders",
      "title_zh": "翻译失败",
      "authors": [
        "David Noever",
        "Samantha E. Miller Noever"
      ],
      "abstract": "This study reveals an unexpected parallel between instructible\nvision-language models (VLMs) and human cognitive disorders, specifically\nconstructive apraxia. We tested 25 state-of-the-art VLMs, including GPT-4\nVision, DALL-E 3, and Midjourney v5, on their ability to generate images of the\nPonzo illusion, a task that requires basic spatial reasoning and is often used\nin clinical assessments of constructive apraxia. Remarkably, 24 out of 25\nmodels failed to correctly render two horizontal lines against a perspective\nbackground, mirroring the deficits seen in patients with parietal lobe damage.\nThe models consistently misinterpreted spatial instructions, producing tilted\nor misaligned lines that followed the perspective of the background rather than\nremaining horizontal. This behavior is strikingly similar to how apraxia\npatients struggle to copy or construct simple figures despite intact visual\nperception and motor skills. Our findings suggest that current VLMs, despite\ntheir advanced capabilities in other domains, lack fundamental spatial\nreasoning abilities akin to those impaired in constructive apraxia. This\nlimitation in AI systems provides a novel computational model for studying\nspatial cognition deficits and highlights a critical area for improvement in\nVLM architecture and training methodologies.",
      "tldr_zh": "这篇论文揭示了可指导的视觉语言模型 (VLMs) 在空间推理任务上的意外局限性，与人类认知障碍 constructive apraxia 类似。研究团队测试了 25 个最先进的 VLMs（如 GPT-4 Vision、DALL-E 3 和 Midjourney v5），让它们生成 Ponzo illusion 图像，结果 24 个模型失败，无法正确渲染两个水平线在透视背景中的位置，导致线条倾斜或误对齐。这样的表现类似于顶叶损伤患者的空间推理缺陷，尽管 VLMs 在其他领域表现出色。这一发现为研究空间认知缺陷提供了新的计算模型，并突出了优化 VLM 架构和训练方法的紧迫需求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03551v1",
      "published_date": "2024-09-17 18:46:57 UTC",
      "updated_date": "2024-09-17 18:46:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:44:21.021757"
    },
    {
      "arxiv_id": "2409.11489v1",
      "title": "Beyond Algorithmic Fairness: A Guide to Develop and Deploy Ethical AI-Enabled Decision-Support Tools",
      "title_zh": "超越算法公平性：开发和部署道德 AI 启用的决策支持工具指南",
      "authors": [
        "Rosemarie Santa Gonzalez",
        "Ryan Piansky",
        "Sue M Bae",
        "Justin Biddle",
        "Daniel Molzahn"
      ],
      "abstract": "The integration of artificial intelligence (AI) and optimization hold\nsubstantial promise for improving the efficiency, reliability, and resilience\nof engineered systems. Due to the networked nature of many engineered systems,\nethically deploying methodologies at this intersection poses challenges that\nare distinct from other AI settings, thus motivating the development of ethical\nguidelines tailored to AI-enabled optimization. This paper highlights the need\nto go beyond fairness-driven algorithms to systematically address ethical\ndecisions spanning the stages of modeling, data curation, results analysis, and\nimplementation of optimization-based decision support tools. Accordingly, this\npaper identifies ethical considerations required when deploying algorithms at\nthe intersection of AI and optimization via case studies in power systems as\nwell as supply chain and logistics. Rather than providing a prescriptive set of\nrules, this paper aims to foster reflection and awareness among researchers and\nencourage consideration of ethical implications at every step of the\ndecision-making process.",
      "tldr_zh": "这篇论文指导如何开发和部署 Ethical AI-Enabled Decision-Support Tools，强调需要超越 Algorithmic Fairness，通过系统性方法处理 AI 和优化技术在工程系统中的伦理挑战。论文通过电力系统以及供应链和物流的案例研究，识别从建模、数据整理、结果分析到实施各阶段的关键伦理考虑。最终，旨在促进研究人员的反思和意识，确保在决策过程中全面考量伦理含义，而不是提供一组规定规则。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11489v1",
      "published_date": "2024-09-17 18:37:53 UTC",
      "updated_date": "2024-09-17 18:37:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:44:31.395798"
    },
    {
      "arxiv_id": "2410.02779v1",
      "title": "Learning variant product relationship and variation attributes from e-commerce website structures",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Herrero-Vidal",
        "You-Lin Chen",
        "Cris Liu",
        "Prithviraj Sen",
        "Lichao Wang"
      ],
      "abstract": "We introduce VARM, variant relationship matcher strategy, to identify pairs\nof variant products in e-commerce catalogs. Traditional definitions of entity\nresolution are concerned with whether product mentions refer to the same\nunderlying product. However, this fails to capture product relationships that\nare critical for e-commerce applications, such as having similar, but not\nidentical, products listed on the same webpage or share reviews. Here, we\nformulate a new type of entity resolution in variant product relationships to\ncapture these similar e-commerce product links. In contrast with the\ntraditional definition, the new definition requires both identifying if two\nproducts are variant matches of each other and what are the attributes that\nvary between them. To satisfy these two requirements, we developed a strategy\nthat leverages the strengths of both encoding and generative AI models. First,\nwe construct a dataset that captures webpage product links, and therefore\nvariant product relationships, to train an encoding LLM to predict variant\nmatches for any given pair of products. Second, we use RAG prompted generative\nLLMs to extract variation and common attributes amongst groups of variant\nproducts. To validate our strategy, we evaluated model performance using real\ndata from one of the world's leading e-commerce retailers. The results showed\nthat our strategy outperforms alternative solutions and paves the way to\nexploiting these new type of product relationships.",
      "tldr_zh": "本文提出VARM（variant relationship matcher strategy）策略，用于从电商网站结构中识别变体产品关系，包括判断两个产品是否为变体匹配以及提取它们之间的变化属性。策略结合编码LLM训练模型来预测产品匹配，并使用RAG提示的生成式LLM提取变体产品组中的共同和差异属性。实验结果显示，VARM在真实电商数据上比替代方案性能更优，为利用这些新产品关系（如相似产品链接）提供了新途径。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.02779v1",
      "published_date": "2024-09-17 18:24:27 UTC",
      "updated_date": "2024-09-17 18:24:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:44:43.841530"
    },
    {
      "arxiv_id": "2409.11404v3",
      "title": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Basel Mousi",
        "Nadir Durrani",
        "Fatema Ahmad",
        "Md. Arid Hasan",
        "Maram Hasanain",
        "Tameem Kabbani",
        "Fahim Dalvi",
        "Shammur Absar Chowdhury",
        "Firoj Alam"
      ],
      "abstract": "Arabic, with its rich diversity of dialects, remains significantly\nunderrepresented in Large Language Models, particularly in dialectal\nvariations. We address this gap by introducing seven synthetic datasets in\ndialects alongside Modern Standard Arabic (MSA), created using Machine\nTranslation (MT) combined with human post-editing. We present AraDiCE, a\nbenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on\ndialect comprehension and generation, focusing specifically on low-resource\nArabic dialects. Additionally, we introduce the first-ever fine-grained\nbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, and\nLevant regions, providing a novel dimension to LLM evaluation. Our findings\ndemonstrate that while Arabic-specific models like Jais and AceGPT outperform\nmultilingual models on dialectal tasks, significant challenges persist in\ndialect identification, generation, and translation. This work contributes\n$\\approx$45K post-edited samples, a cultural benchmark, and highlights the\nimportance of tailored training to improve LLM performance in capturing the\nnuances of diverse Arabic dialects and cultural contexts. We have released the\ndialectal translation models and benchmarks developed in this study\n(https://huggingface.co/datasets/QCRI/AraDiCE).",
      "tldr_zh": "本研究针对阿拉伯语方言在大型语言模型（LLMs）中的代表性不足，引入了 AraDiCE 基准，用于评估 LLMs 的方言理解、生成和文化意识能力。研究者创建了七个合成数据集，包括方言和现代标准阿拉伯语（MSA），通过机器翻译（MT）结合人工后编辑，总计约 45K 样本，并首次提出细粒度基准评估海湾、埃及和黎凡特地区的文化意识。实验结果显示，阿拉伯特定模型如 Jais 和 AceGPT 在方言任务上优于多语言模型，但方言识别、生成和翻译仍面临显著挑战；这项工作强调了针对性训练的重要性，并公开了相关数据集和模型（https://huggingface.co/datasets/QCRI/AraDiCE）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "F.2.2; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Benchmarking, Culturally Informed, Large Language Models, Arabic NLP,\n  LLMs, Arabic Dialect, Dialectal Benchmarking",
      "pdf_url": "http://arxiv.org/pdf/2409.11404v3",
      "published_date": "2024-09-17 17:59:25 UTC",
      "updated_date": "2024-12-17 21:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:44:56.250072"
    },
    {
      "arxiv_id": "2409.11402v2",
      "title": "NVLM: Open Frontier-Class Multimodal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Wenliang Dai",
        "Nayeon Lee",
        "Boxin Wang",
        "Zhuolin Yang",
        "Zihan Liu",
        "Jon Barker",
        "Tuomas Rintamaki",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Wei Ping"
      ],
      "abstract": "We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B\nand will open-source the training code for the community soon.",
      "tldr_zh": "我们介绍了 NVLM 1.0，一系列前沿级 Multimodal LLMs，在视觉-语言任务上达到最先进水平，媲美 GPT-4o 和开源模型如 Llama 3-V 405B。 该模型采用新型架构，结合 decoder-only 和 cross-attention 方法，并引入 1-D tile-tagging 设计，提高训练效率、多模态推理能力和 OCR 相关任务的性能。 通过精心策划的高质量数据集，NVLM 1.0 不仅提升了文本-only 性能，还增强了跨模态数学和编码能力，并开源了模型权重以推动研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Fixed the typos. For more information, please visit our project page\n  at: https://research.nvidia.com/labs/adlr/NVLM-1",
      "pdf_url": "http://arxiv.org/pdf/2409.11402v2",
      "published_date": "2024-09-17 17:59:06 UTC",
      "updated_date": "2024-10-22 23:13:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:45:09.766099"
    },
    {
      "arxiv_id": "2410.00037v2",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "title_zh": "Moshi：用于实时对话的语音-文本基础模型",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.",
      "tldr_zh": "研究论文提出Moshi，一种speech-text foundation model和full-duplex口语对话框架，旨在解决传统对话系统中的延迟、非语言信息丢失和重叠语音处理问题。该模型将口语对话视为speech-to-speech generation，使用文本语言模型骨干从神经音频编解码器的残差量化器生成语音token，并通过并行流建模自身和用户的对话动态。创新的Inner Monologue方法先预测时间对齐的文本token作为音频token的前缀，提升了生成的语音语言质量，并实现了实时流式语音识别和文本到语音功能，最终达到理论延迟160ms、实际200ms的性能。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00037v2",
      "published_date": "2024-09-17 17:55:39 UTC",
      "updated_date": "2024-10-02 09:11:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:45:21.016204"
    },
    {
      "arxiv_id": "2409.11393v2",
      "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Amine Ben Hassouna",
        "Hana Chaari",
        "Ines Belhaj"
      ],
      "abstract": "In an era where vast amounts of data are collected and processed from diverse\nsources, there is a growing demand to develop sophisticated AI systems capable\nof intelligently fusing and analyzing this information. To address these\nchallenges, researchers have turned towards integrating tools into LLM-powered\nagents to enhance the overall information fusion process. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity and terminological inconsistencies among researchers.\nTo address these issues, we propose a novel LLM-based Agent Unified Modeling\nFramework (LLM-Agent-UMF) that aims to establish a clear foundation for agent\ndevelopment from both functional and software architectural perspectives. Our\nframework distinguishes between the different components of an LLM-based agent,\nsetting LLMs, and tools apart from a new element, the core-agent, playing the\nrole of the central coordinator of the agent. This pivotal entity comprises\nfive modules: planning, memory, profile, action, and security - the latter\noften neglected in previous works. By classifying core-agents into passive and\nactive types based on their authoritative natures, we propose various\nmulti-core agent architectures that combine unique characteristics of\ndistinctive agents to tackle complex tasks more efficiently. We evaluate our\nframework by applying it to thirteen state-of-the-art agents, thereby\ndemonstrating its alignment with their functionalities and clarifying the\noverlooked architectural aspects. Moreover, we thoroughly assess five of our\nproposed architectures through the integration of existing agents into new\nhybrid active/passive core-agents architectures. This analysis provides\ninsights into potential improvements and highlights challenges involved in\ncombining specific agents.",
      "tldr_zh": "本研究提出 LLM-Agent-UMF 框架，这是一种统一的建模框架，用于无缝整合多 active/passive core-agents，从而解决 LLM 驱动代理系统在软件架构上缺乏模块性和术语一致性的问题。框架将 core-agent 定义为核心协调器，包括五个关键模块：planning, memory, profile, action, and security，并根据代理的权威性分类为主动或被动类型，以构建高效的多核心代理架构。实验评估显示，该框架适用于 13 个现有代理，并通过整合新混合架构分析了潜在改进和挑战。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "36 pages, 19 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.11393v2",
      "published_date": "2024-09-17 17:54:17 UTC",
      "updated_date": "2024-10-31 11:07:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:45:33.352955"
    },
    {
      "arxiv_id": "2409.11456v3",
      "title": "Two Stage Segmentation of Cervical Tumors using PocketNet",
      "title_zh": "翻译失败",
      "authors": [
        "Awj Twam",
        "Adrian E. Celaya",
        "Megan C. Jacobsen",
        "Rachel Glenn",
        "Peng Wei",
        "Jia Sun",
        "Ann Klopp",
        "Aradhana M. Venkatesan",
        "David Fuentes"
      ],
      "abstract": "Cervical cancer remains the fourth most common malignancy amongst women\nworldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay\ndefinitive treatment regimen for locally advanced cervical cancers and includes\nexternal beam radiation followed by brachytherapy.2 Integral to radiotherapy\ntreatment planning is the routine contouring of both the target tumor at the\nlevel of the cervix, associated gynecologic anatomy and the adjacent organs at\nrisk (OARs). However, manual contouring of these structures is both time and\nlabor intensive and associated with known interobserver variability that can\nimpact treatment outcomes. While multiple tools have been developed to\nautomatically segment OARs and the high-risk clinical tumor volume (HR-CTV)\nusing computed tomography (CT) images,3,4,5,6 the development of deep\nlearning-based tumor segmentation tools using routine T2-weighted (T2w)\nmagnetic resonance imaging (MRI) addresses an unmet clinical need to improve\nthe routine contouring of both anatomical structures and cervical cancers,\nthereby increasing quality and consistency of radiotherapy planning. This work\napplied a novel deep-learning model (PocketNet) to segment the cervix, vagina,\nuterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture\nwas evaluated, when trained on data via five-fold cross validation. PocketNet\nachieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for\ntumor segmentation and 80% for organ segmentation. Validation on a publicly\navailable dataset from The Cancer Imaging Archive (TCIA) demonstrated the\nmodels robustness, achieving DSC scores of 67.3% for tumor segmentation and\n80.8% for organ segmentation. These results suggest that PocketNet is robust to\nvariations in contrast protocols, providing reliable segmentation of the\nregions of interest.",
      "tldr_zh": "本研究针对宫颈癌放射治疗中手动轮廓肿瘤和器官的耗时与变异性问题，提出了一种基于PocketNet深度学习模型的两阶段分割方法，用于T2-weighted (T2w) MRI图像中分割子宫颈、阴道、子宫和肿瘤。PocketNet模型通过五折交叉验证训练，实现了肿瘤分割的Dice-Sorensen similarity coefficient (DSC)超过70%，器官分割超过80%。在公开数据集The Cancer Imaging Archive (TCIA)上的验证显示，模型鲁棒性强，DSC分别为67.3%（肿瘤）和80.8%（器官），从而提升了放射治疗规划的质量和一致性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11456v3",
      "published_date": "2024-09-17 17:48:12 UTC",
      "updated_date": "2025-02-12 20:10:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:45:45.167780"
    },
    {
      "arxiv_id": "2409.11378v1",
      "title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Yu",
        "Liangyu Chen",
        "Sara Ahmadian",
        "Marzieh Fadaee"
      ],
      "abstract": "Finetuning large language models on instruction data is crucial for enhancing\npre-trained knowledge and improving instruction-following capabilities. As\ninstruction datasets proliferate, selecting optimal data for effective training\nbecomes increasingly important. This work addresses the question: How can we\ndetermine the optimal subset of data for effective training? While existing\nresearch often emphasizes local criteria like instance quality for subset\nselection, we argue that a global approach focused on data diversity is more\ncritical. Our method employs k-means clustering to ensure the selected subset\neffectively represents the full dataset. We propose an iterative refinement\nmethod inspired by active learning techniques to resample instances from\nclusters, reassessing each cluster's importance and sampling weight in every\ntraining iteration. This approach reduces the effect of outliers and\nautomatically filters out clusters containing low-quality data. Through\nextensive evaluation across natural language reasoning, general world\nknowledge, code and math reasoning tasks, and by fine-tuning models from\nvarious families, we observe consistent improvements, achieving a 7% increase\nover random selection and a 3.8% improvement over state-of-the-art sampling\nmethods. Our work highlights the significance of diversity-first sampling when\nfinetuning LLMs to enhance performance across a broad array of evaluation\ntasks. Our code is available at\nhttps://github.com/for-ai/iterative-data-selection.",
      "tldr_zh": "该研究提出了一种以多样性为核心的数据选择方法“Diversify and Conquer”，旨在优化大语言模型（LLMs）的指令数据微调过程。方法使用 k-means 聚类确保子集代表整个数据集，并引入受主动学习启发的迭代精炼机制，在每个训练迭代中重新采样实例、评估集群重要性和调整采样权重，从而减少异常值影响并过滤低质量数据。通过在自然语言推理、一般世界知识、代码和数学推理任务上的广泛评估，该方法比随机选择提高了 7%，并比最先进采样方法提高了 3.8%。这项工作强调了多样性优先策略在微调 LLMs 时提升整体性能的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11378v1",
      "published_date": "2024-09-17 17:25:31 UTC",
      "updated_date": "2024-09-17 17:25:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:45:56.892737"
    },
    {
      "arxiv_id": "2409.11375v1",
      "title": "Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification",
      "title_zh": "Multi-OCT-SelfNet：整合自监督学习与多源数据融合以提升多类视网膜疾病分类",
      "authors": [
        "Fatema-E- Jannat",
        "Sina Gholami",
        "Jennifer I. Lim",
        "Theodore Leng",
        "Minhaj Nur Alam",
        "Hamed Tabkhi"
      ],
      "abstract": "In the medical domain, acquiring large datasets poses significant challenges\ndue to privacy concerns. Nonetheless, the development of a robust deep-learning\nmodel for retinal disease diagnosis necessitates a substantial dataset for\ntraining. The capacity to generalize effectively on smaller datasets remains a\npersistent challenge. The scarcity of data presents a significant barrier to\nthe practical implementation of scalable medical AI solutions. To address this\nissue, we've combined a wide range of data sources to improve performance and\ngeneralization to new data by giving it a deeper understanding of the data\nrepresentation from multi-modal datasets and developed a self-supervised\nframework based on large language models (LLMs), SwinV2 to gain a deeper\nunderstanding of multi-modal dataset representations, enhancing the model's\nability to extrapolate to new data for the detection of eye diseases using\noptical coherence tomography (OCT) images. We adopt a two-phase training\nmethodology, self-supervised pre-training, and fine-tuning on a downstream\nsupervised classifier. An ablation study conducted across three datasets\nemploying various encoder backbones, without data fusion, with low data\navailability setting, and without self-supervised pre-training scenarios,\nhighlights the robustness of our method. Our findings demonstrate consistent\nperformance across these diverse conditions, showcasing superior generalization\ncapabilities compared to the baseline model, ResNet-50.",
      "tldr_zh": "本研究针对医疗领域数据稀缺和隐私挑战，提出Multi-OCT-SelfNet框架，通过整合Self-Supervised Learning与Multi-Source Data Fusion，利用LLMs和SwinV2增强多模态数据集的表示能力，从而提升多类视网膜疾病分类的性能和泛化。该框架采用两阶段训练方法，先进行自监督预训练，然后在监督分类器上微调。实验结果显示，在三个数据集上的消融研究中，Multi-OCT-SelfNet比基线ResNet-50表现出更强的鲁棒性和泛化能力，尤其在低数据场景下。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages, 9 tables, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11375v1",
      "published_date": "2024-09-17 17:22:35 UTC",
      "updated_date": "2024-09-17 17:22:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:46:09.224096"
    },
    {
      "arxiv_id": "2409.11363v1",
      "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Zachary S. Siegel",
        "Sayash Kapoor",
        "Nitya Nagdir",
        "Benedikt Stroebl",
        "Arvind Narayanan"
      ],
      "abstract": "AI agents have the potential to aid users on a variety of consequential\ntasks, including conducting scientific research. To spur the development of\nuseful agents, we need benchmarks that are challenging, but more crucially,\ndirectly correspond to real-world tasks of interest. This paper introduces such\na benchmark, designed to measure the accuracy of AI agents in tackling a\ncrucial yet surprisingly challenging aspect of scientific research:\ncomputational reproducibility. This task, fundamental to the scientific\nprocess, involves reproducing the results of a study using the provided code\nand data. We introduce CORE-Bench (Computational Reproducibility Agent\nBenchmark), a benchmark consisting of 270 tasks based on 90 scientific papers\nacross three disciplines (computer science, social science, and medicine).\nTasks in CORE-Bench consist of three difficulty levels and include both\nlanguage-only and vision-language tasks. We provide an evaluation system to\nmeasure the accuracy of agents in a fast and parallelizable way, saving days of\nevaluation time for each run compared to a sequential implementation. We\nevaluated two baseline agents: the general-purpose AutoGPT and a task-specific\nagent called CORE-Agent. We tested both variants using two underlying language\nmodels: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on\nthe hardest task, showing the vast scope for improvement in automating routine\nscientific tasks. Having agents that can reproduce existing work is a necessary\nstep towards building agents that can conduct novel research and could verify\nand improve the performance of other research agents. We hope that CORE-Bench\ncan improve the state of reproducibility and spur the development of future\nresearch agents.",
      "tldr_zh": "本研究引入了 CORE-Bench，这是一个基准测试，用于评估 AI agents 在计算可再现性（computational reproducibility）方面的准确性，从而提升科学研究的信誉。CORE-Bench 包含 270 个任务，基于 90 篇跨计算机科学、社会科学和医学领域的论文，涵盖三种难度级别以及语言-only 和 vision-language 任务，并提供了一个快速并行化的评估系统。实验中，测试了 AutoGPT 和 CORE-Agent 两种基线代理，使用 GPT-4o 和 GPT-4o-mini 模型，最佳代理在最难任务上仅达到 21% 的准确率，突显了自动化科研任务的改进潜力，并有望推动 AI agents 验证和提升未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Benchmark harness and code available at\n  http://github.com/siegelz/core-bench",
      "pdf_url": "http://arxiv.org/pdf/2409.11363v1",
      "published_date": "2024-09-17 17:13:19 UTC",
      "updated_date": "2024-09-17 17:13:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:46:21.355206"
    },
    {
      "arxiv_id": "2409.11360v3",
      "title": "AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances",
      "title_zh": "翻译失败",
      "authors": [
        "Dhruv Agarwal",
        "Mor Naaman",
        "Aditya Vashistha"
      ],
      "abstract": "Large language models (LLMs) are being increasingly integrated into everyday\nproducts and services, such as coding tools and writing assistants. As these\nembedded AI applications are deployed globally, there is a growing concern that\nthe AI models underlying these applications prioritize Western values. This\npaper investigates what happens when a Western-centric AI model provides\nwriting suggestions to users from a different cultural background. We conducted\na cross-cultural controlled experiment with 118 participants from India and the\nUnited States who completed culturally grounded writing tasks with and without\nAI suggestions. Our analysis reveals that AI provided greater efficiency gains\nfor Americans compared to Indians. Moreover, AI suggestions led Indian\nparticipants to adopt Western writing styles, altering not just what is written\nbut also how it is written. These findings show that Western-centric AI models\nhomogenize writing toward Western norms, diminishing nuances that differentiate\ncultural expression.",
      "tldr_zh": "本文研究探讨了大型语言模型(LLMs)提供的写作建议如何影响不同文化背景的用户，特别是当西方中心AI模型被全球应用时可能优先西方价值观的问题。通过一项涉及118名印度和美国参与者的跨文化对照实验，研究发现AI建议为美国人带来了更大的效率提升，但同时导致印度参与者采用西方写作风格，改变了写作内容和表达方式。这些发现表明，西方中心AI模型会使写作趋同于西方规范，从而削弱文化表达的细微差别，并强调了AI在全球部署中潜在的文化同质化风险。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at CHI 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.11360v3",
      "published_date": "2024-09-17 17:07:30 UTC",
      "updated_date": "2025-03-12 22:40:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:46:32.988807"
    },
    {
      "arxiv_id": "2409.11356v2",
      "title": "RenderWorld: World Model with Self-Supervised 3D Label",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyang Yan",
        "Wenzhen Dong",
        "Yihua Shao",
        "Yuhang Lu",
        "Liu Haiyang",
        "Jingwen Liu",
        "Haozhe Wang",
        "Zhe Wang",
        "Yan Wang",
        "Fabio Remondino",
        "Yuexin Ma"
      ],
      "abstract": "End-to-end autonomous driving with vision-only is not only more\ncost-effective compared to LiDAR-vision fusion but also more reliable than\ntraditional methods. To achieve a economical and robust purely visual\nautonomous driving system, we propose RenderWorld, a vision-only end-to-end\nautonomous driving framework, which generates 3D occupancy labels using a\nself-supervised gaussian-based Img2Occ Module, then encodes the labels by\nAM-VAE, and uses world model for forecasting and planning. RenderWorld employs\nGaussian Splatting to represent 3D scenes and render 2D images greatly improves\nsegmentation accuracy and reduces GPU memory consumption compared with\nNeRF-based methods. By applying AM-VAE to encode air and non-air separately,\nRenderWorld achieves more fine-grained scene element representation, leading to\nstate-of-the-art performance in both 4D occupancy forecasting and motion\nplanning from autoregressive world model.",
      "tldr_zh": "该研究提出 RenderWorld，一种基于纯视觉的端到端自动驾驶框架，旨在提供更经济和可靠的解决方案，通过自监督的 Gaussian-based Img2Occ Module 生成 3D 占用标签，并使用 AM-VAE 分别编码空气和非空气元素以实现细粒度场景表示。框架采用 Gaussian Splatting 表示 3D 场景并渲染 2D 图像，比 NeRF-based methods 提高了分割准确性和降低了 GPU 内存消耗。最终，RenderWorld 通过世界模型进行 4D occupancy forecasting 和 motion planning，达到了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in 2025 IEEE International Conference on Robotics and\n  Automation (ICRA)",
      "pdf_url": "http://arxiv.org/pdf/2409.11356v2",
      "published_date": "2024-09-17 17:00:52 UTC",
      "updated_date": "2025-02-13 05:20:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:46:44.696195"
    },
    {
      "arxiv_id": "2409.11350v1",
      "title": "Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry",
      "title_zh": "翻译失败",
      "authors": [
        "Lauren M. Zuromski",
        "Jacob Durtschi",
        "Aimal Aziz",
        "Jeffrey Chumley",
        "Mark Dewey",
        "Paul English",
        "Muir Morrison",
        "Keith Simmon",
        "Blaine Whipple",
        "Brendan O'Fallon",
        "David P. Ng"
      ],
      "abstract": "Machine-learning (ML) models in flow cytometry have the potential to reduce\nerror rates, increase reproducibility, and boost the efficiency of clinical\nlabs. While numerous ML models for flow cytometry data have been proposed, few\nstudies have described the clinical deployment of such models. Realizing the\npotential gains of ML models in clinical labs requires not only an accurate\nmodel, but infrastructure for automated inference, error detection, analytics\nand monitoring, and structured data extraction. Here, we describe an ML model\nfor detection of Acute Myeloid Leukemia (AML), along with the infrastructure\nsupporting clinical implementation. Our infrastructure leverages the resilience\nand scalability of the cloud for model inference, a Kubernetes-based workflow\nsystem that provides model reproducibility and resource management, and a\nsystem for extracting structured diagnoses from full-text reports. We also\ndescribe our model monitoring and visualization platform, an essential element\nfor ensuring continued model accuracy. Finally, we present a post-deployment\nanalysis of impacts on turn-around time and compare production accuracy to the\noriginal validation statistics.",
      "tldr_zh": "这篇论文验证了一个基于机器学习(ML)的实时系统，用于通过流式细胞术检测急性髓性白血病(AML)，旨在减少错误率、提高再现性和实验室效率。系统包括云端自动推理、Kubernetes-based工作流系统、结构化数据提取机制，以及一个模型监控和可视化平台，以确保持续准确性。部署后，该系统显著改善了周转时间，并显示生产准确率与原始验证统计相符，为ML模型在临床实验室的实际应用提供了关键基础设施。",
      "categories": [
        "q-bio.TO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.TO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11350v1",
      "published_date": "2024-09-17 16:53:47 UTC",
      "updated_date": "2024-09-17 16:53:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:46:56.541285"
    },
    {
      "arxiv_id": "2409.11340v2",
      "title": "OmniGen: Unified Image Generation",
      "title_zh": "OmniGen：统一图像生成",
      "authors": [
        "Shitao Xiao",
        "Yueze Wang",
        "Junjie Zhou",
        "Huaying Yuan",
        "Xingrun Xing",
        "Ruiran Yan",
        "Chaofan Li",
        "Shuting Wang",
        "Tiejun Huang",
        "Zheng Liu"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a\nsingle framework remains largely unexplored. In this work, we introduce\nOmniGen, a new diffusion model for unified image generation. OmniGen is\ncharacterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. 2) Simplicity: The architecture of OmniGen is\nhighly simplified, eliminating the need for additional plugins. Moreover,\ncompared to existing diffusion models, it is more user-friendly and can\ncomplete complex tasks end-to-end through instructions without the need for\nextra intermediate steps, greatly simplifying the image generation workflow. 3)\nKnowledge Transfer: Benefit from learning in a unified format, OmniGen\neffectively transfers knowledge across different tasks, manages unseen tasks\nand domains, and exhibits novel capabilities. We also explore the model's\nreasoning capabilities and potential applications of the chain-of-thought\nmechanism. This work represents the first attempt at a general-purpose image\ngeneration model, and we will release our resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster future advancements.",
      "tldr_zh": "本论文提出 OmniGen，一种统一的图像生成扩散模型（diffusion model），旨在整合多种图像任务如文本到图像生成、图像编辑、主体驱动生成和视觉条件生成。OmniGen 的设计强调简易性，通过简化架构和指令端到端处理，消除了额外插件的需求，大大简化了图像生成工作流程。该模型利用统一学习格式实现知识转移，能够有效处理未见任务和领域，并探索链式思维（chain-of-thought）机制的推理能力。作为首个通用图像生成模型尝试，OmniGen 展示了显著的潜力，并将发布资源以推动后续研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Update the paper for OmniGen-v1",
      "pdf_url": "http://arxiv.org/pdf/2409.11340v2",
      "published_date": "2024-09-17 16:42:46 UTC",
      "updated_date": "2024-11-21 14:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:47:09.472115"
    },
    {
      "arxiv_id": "2409.11321v2",
      "title": "SOAP: Improving and Stabilizing Shampoo using Adam",
      "title_zh": "翻译失败",
      "authors": [
        "Nikhil Vyas",
        "Depen Morwani",
        "Rosie Zhao",
        "Mujin Kwun",
        "Itai Shapira",
        "David Brandfonbrener",
        "Lucas Janson",
        "Sham Kakade"
      ],
      "abstract": "There is growing evidence of the effectiveness of Shampoo, a higher-order\npreconditioning method, over Adam in deep learning optimization tasks. However,\nShampoo's drawbacks include additional hyperparameters and computational\noverhead when compared to Adam, which only updates running averages of first-\nand second-moment quantities. This work establishes a formal connection between\nShampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient\napproximation of Adam -- showing that Shampoo is equivalent to running\nAdafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to\nthe design of a simpler and computationally efficient algorithm:\n$\\textbf{S}$hampo$\\textbf{O}$ with $\\textbf{A}$dam in the\n$\\textbf{P}$reconditioner's eigenbasis (SOAP).\n  With regards to improving Shampoo's computational efficiency, the most\nstraightforward approach would be to simply compute Shampoo's\neigendecomposition less frequently. Unfortunately, as our empirical results\nshow, this leads to performance degradation that worsens with this frequency.\nSOAP mitigates this degradation by continually updating the running average of\nthe second moment, just as Adam does, but in the current (slowly changing)\ncoordinate basis. Furthermore, since SOAP is equivalent to running Adam in a\nrotated space, it introduces only one additional hyperparameter (the\npreconditioning frequency) compared to Adam. We empirically evaluate SOAP on\nlanguage model pre-training with 360m and 660m sized models. In the large batch\nregime, SOAP reduces the number of iterations by over 40% and wall clock time\nby over 35% compared to AdamW, with approximately 20% improvements in both\nmetrics compared to Shampoo. An implementation of SOAP is available at\nhttps://github.com/nikhilvyas/SOAP.",
      "tldr_zh": "这篇论文提出SOAP算法，通过将Adam优化器应用于Shampoo预条件者的特征基（eigenbasis），以改善Shampoo在深度学习优化中的计算效率和稳定性。SOAP建立Shampoo与Adafactor的等价关系，仅引入一个额外超参数（预条件频率），并在当前坐标基上持续更新第二阶矩的运行平均，从而避免了频繁计算特征分解导致的性能下降。在语言模型预训练实验中，SOAP在大型批量设置下比AdamW减少超过40%的迭代次数和35%的墙钟时间，并比Shampoo改善约20%的性能指标。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11321v2",
      "published_date": "2024-09-17 16:18:05 UTC",
      "updated_date": "2025-01-31 18:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:47:21.225823"
    },
    {
      "arxiv_id": "2409.11316v2",
      "title": "MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping",
      "title_zh": "翻译失败",
      "authors": [
        "Amirreza Fateh",
        "Mohammad Reza Mohammadi",
        "Mohammad Reza Jahed Motlagh"
      ],
      "abstract": "Few-shot Semantic Segmentation addresses the challenge of segmenting objects\nin query images with only a handful of annotated examples. However, many\nprevious state-of-the-art methods either have to discard intricate local\nsemantic features or suffer from high computational complexity. To address\nthese challenges, we propose a new Few-shot Semantic Segmentation framework\nbased on the transformer architecture. Our approach introduces the spatial\ntransformer decoder and the contextual mask generation module to improve the\nrelational understanding between support and query images. Moreover, we\nintroduce a multi-scale decoder to refine the segmentation mask by\nincorporating features from different resolutions in a hierarchical manner.\nAdditionally, our approach integrates global features from intermediate encoder\nstages to improve contextual understanding, while maintaining a lightweight\nstructure to reduce complexity. This balance between performance and efficiency\nenables our method to achieve state-of-the-art results on benchmark datasets\nsuch as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings.\nNotably, our model with only 1.5 million parameters demonstrates competitive\nperformance while overcoming limitations of existing methodologies.\nhttps://github.com/amirrezafateh/MSDNet",
      "tldr_zh": "本研究针对Few-shot Semantic Segmentation的挑战，提出了一种基于Transformer架构的MSDNet框架，以少量标注样本实现查询图像中对象的精确分割。框架引入spatial transformer decoder和contextual mask generation module来提升支持图像与查询图像之间的关系理解，同时采用multi-scale decoder通过分层整合不同分辨率的特征来细化分割掩码，并整合encoder中间阶段的全局特征以提高上下文理解，同时保持轻量级结构。实验结果显示，该模型在PASCAL-5^i和COCO-20^i数据集的1-shot和5-shot设置中达到最先进性能，仅需1.5百万参数，便克服了现有方法的计算复杂性和特征丢失问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11316v2",
      "published_date": "2024-09-17 16:14:03 UTC",
      "updated_date": "2024-12-28 15:45:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:47:33.337913"
    },
    {
      "arxiv_id": "2409.11299v3",
      "title": "TTT-Unet: Enhancing U-Net with Test-Time Training Layers for Biomedical Image Segmentation",
      "title_zh": "TTT-Unet：通过测试时训练层增强 U-Net 用于生物医学图像分割",
      "authors": [
        "Rong Zhou",
        "Zhengqing Yuan",
        "Zhiling Yan",
        "Weixiang Sun",
        "Kai Zhang",
        "Yiwei Li",
        "Yanfang Ye",
        "Xiang Li",
        "Lifang He",
        "Lichao Sun"
      ],
      "abstract": "Biomedical image segmentation is crucial for accurately diagnosing and\nanalyzing various diseases. However, Convolutional Neural Networks (CNNs) and\nTransformers, the most commonly used architectures for this task, struggle to\neffectively capture long-range dependencies due to the inherent locality of\nCNNs and the computational complexity of Transformers. To address this\nlimitation, we introduce TTT-Unet, a novel framework that integrates Test-Time\nTraining (TTT) layers into the traditional U-Net architecture for biomedical\nimage segmentation. TTT-Unet dynamically adjusts model parameters during the\ntesting time, enhancing the model's ability to capture both local and\nlong-range features. We evaluate TTT-Unet on multiple medical imaging datasets,\nincluding 3D abdominal organ segmentation in CT and MR images, instrument\nsegmentation in endoscopy images, and cell segmentation in microscopy images.\nThe results demonstrate that TTT-Unet consistently outperforms state-of-the-art\nCNN-based and Transformer-based segmentation models across all tasks. The code\nis available at https://github.com/rongzhou7/TTT-Unet.",
      "tldr_zh": "该论文针对生物医学图像分割中的挑战，指出CNNs和Transformers在捕捉长程依赖性方面存在局限性，提出了一种新型框架TTT-Unet，将Test-Time Training (TTT)层整合到传统U-Net架构中，以在测试时动态调整参数，提升对局部和长程特征的捕捉能力。在多个医学图像数据集上，包括CT和MR图像的3D腹部器官分割、内镜图像的仪器分割以及显微镜图像的细胞分割，TTT-Unet表现出色， consistently outperforms 现有最先进的CNN-based和Transformer-based模型。该框架的开源代码可从https://github.com/rongzhou7/TTT-Unet获取，为生物医学图像分析提供了一个高效的解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11299v3",
      "published_date": "2024-09-17 15:52:40 UTC",
      "updated_date": "2024-12-06 02:45:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:47:44.834863"
    },
    {
      "arxiv_id": "2409.11295v5",
      "title": "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyi Liao",
        "Lingbo Mo",
        "Chejian Xu",
        "Mintong Kang",
        "Jiawei Zhang",
        "Chaowei Xiao",
        "Yuan Tian",
        "Bo Li",
        "Huan Sun"
      ],
      "abstract": "Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.",
      "tldr_zh": "本研究首次探讨了generalist web agents在处理用户PII时面临的隐私风险，特别是在与受损网站的互动中。论文提出了一种新型攻击方法Environmental Injection Attack (EIA)，通过注入适应代理环境的恶意内容来窃取用户的特定PII或整个请求，并在Mind2Web数据集上收集的177个真实网站操作步骤中进行实验。结果显示，EIA在窃取特定PII时的ASR高达70%，在窃取完整用户请求时的ASR为16%，且该攻击难以检测和缓解。作者讨论了安全与自治的权衡，并呼吁开发不依赖人类监督的网站防御策略，以提升web agents的安全性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.11295v5",
      "published_date": "2024-09-17 15:49:44 UTC",
      "updated_date": "2025-03-12 20:54:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:47:56.781135"
    },
    {
      "arxiv_id": "2409.11294v1",
      "title": "Navigating Process Mining: A Case study using pm4py",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Jlidi",
        "László Kovács"
      ],
      "abstract": "Process-mining techniques have emerged as powerful tools for analyzing event\ndata to gain insights into business processes. In this paper, we present a\ncomprehensive analysis of road traffic fine management processes using the\npm4py library in Python. We start by importing an event log dataset and explore\nits characteristics, including the distribution of activities and process\nvariants. Through filtering and statistical analysis, we uncover key patterns\nand variations in the process executions. Subsequently, we apply various\nprocess-mining algorithms, including the Alpha Miner, Inductive Miner, and\nHeuristic Miner, to discover process models from the event log data. We\nvisualize the discovered models to understand the workflow structures and\ndependencies within the process. Additionally, we discuss the strengths and\nlimitations of each mining approach in capturing the underlying process\ndynamics. Our findings shed light on the efficiency and effectiveness of road\ntraffic fine management processes, providing valuable insights for process\noptimization and decision-making. This study demonstrates the utility of pm4py\nin facilitating process mining tasks and its potential for analyzing real-world\nbusiness processes.",
      "tldr_zh": "这篇论文以路交通罚单管理过程为例，演示了使用 pm4py 库进行过程挖掘的全面分析，包括导入事件日志数据、探索活动分布和过程变体，以及通过过滤和统计分析揭示关键模式和变异。研究者应用了 Alpha Miner、Inductive Miner 和 Heuristic Miner 等算法来发现过程模型，并通过可视化了解工作流结构和依赖关系，同时讨论了每种方法的优缺点。最终，发现有助于评估路交通罚单管理的效率和有效性，为过程优化和决策提供宝贵洞察，并突显了 pm4py 在真实业务场景中的实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11294v1",
      "published_date": "2024-09-17 15:48:46 UTC",
      "updated_date": "2024-09-17 15:48:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:48:09.252018"
    },
    {
      "arxiv_id": "2409.11290v1",
      "title": "Neural Networks for Vehicle Routing Problem",
      "title_zh": "翻译失败",
      "authors": [
        "László Kovács",
        "Ali Jlidi"
      ],
      "abstract": "The Vehicle Routing Problem is about optimizing the routes of vehicles to\nmeet the needs of customers at specific locations. The route graph consists of\ndepots on several levels and customer positions. Several optimization methods\nhave been developed over the years, most of which are based on some type of\nclassic heuristic: genetic algorithm, simulated annealing, tabu search, ant\ncolony optimization, firefly algorithm. Recent developments in machine learning\nprovide a new toolset, the rich family of neural networks, for tackling complex\nproblems. The main area of application of neural networks is the area of\nclassification and regression. Route optimization can be viewed as a new\nchallenge for neural networks. The article first presents an analysis of the\napplicability of neural network tools, then a novel graphical neural network\nmodel is presented in detail. The efficiency analysis based on test experiments\nshows the applicability of the proposed NN architecture.",
      "tldr_zh": "这篇论文探讨了使用 Neural Networks 解决 Vehicle Routing Problem（车辆路径问题）的潜力，分析了神经网络在路由优化的适用性，并将其与传统启发式方法（如遗传算法和蚁群优化）进行比较。作者提出了一种新型的 Graphical Neural Network 模型，用于优化车辆路由图，以处理多级仓库和客户位置。实验结果显示，该模型在测试中表现出高效性，证明了神经网络在复杂优化问题中的可行性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11290v1",
      "published_date": "2024-09-17 15:45:30 UTC",
      "updated_date": "2024-09-17 15:45:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:48:20.772363"
    },
    {
      "arxiv_id": "2409.11283v4",
      "title": "Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyue Fang",
        "Zhen Huang",
        "Zhiliang Tian",
        "Minghui Fang",
        "Ziyi Pan",
        "Quntian Fang",
        "Zhihua Wen",
        "Hengyue Pan",
        "Dongsheng Li"
      ],
      "abstract": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在文本生成中的幻觉（hallucinations）问题，提出了一种零资源检测方法，即基于图的上下文知识三元组建模（Graph-based Contextual Knowledge Triples Modeling）。该方法通过三元组导向的响应分割（triple-oriented response segmentation）提取多个知识事实，并构建上下文三元组为图结构，利用关系图卷积网络（RGCN）进行消息传递和聚合，以对齐事实并考虑它们间的依赖关系。进一步，引入LLM-based逆向验证（reverse verification）机制来重建三元组，避免长文本中知识遗漏。实验结果表明，该框架显著提升了幻觉检测性能，优于所有基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI25",
      "pdf_url": "http://arxiv.org/pdf/2409.11283v4",
      "published_date": "2024-09-17 15:38:36 UTC",
      "updated_date": "2025-03-07 04:29:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:48:33.664074"
    },
    {
      "arxiv_id": "2409.11277v1",
      "title": "Machine Learning and Theory Ladenness -- A Phenomenological Account",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Termine",
        "Emanuele Ratti",
        "Alessandro Facchini"
      ],
      "abstract": "In recent years, the dissemination of machine learning (ML) methodologies in\nscientific research has prompted discussions on theory ladenness. More\nspecifically, the issue of theory ladenness has remerged as questions about\nwhether and how ML models (MLMs) and ML modelling strategies are impacted by\nthe domain theory of the scientific field in which ML is used and implemented\n(e.g., physics, chemistry, biology, etc). On the one hand, some have argued\nthat there is no difference between traditional (pre ML) and ML assisted\nscience. In both cases, theory plays an essential and unavoidable role in the\nanalysis of phenomena and the construction and use of models. Others have\nargued instead that ML methodologies and models are theory independent and, in\nsome cases, even theory free. In this article, we argue that both positions are\noverly simplistic and do not advance our understanding of the interplay between\nML methods and domain theories. Specifically, we provide an analysis of theory\nladenness in ML assisted science. Our analysis reveals that, while the\nconstruction of MLMs can be relatively independent of domain theory, the\npractical implementation and interpretation of these models within a given\nspecific domain still relies on fundamental theoretical assumptions and\nbackground knowledge.",
      "tldr_zh": "这篇论文从现象学（Phenomenological）角度分析了机器学习（ML）在科学研究中的理论负载（theory ladenness）问题，探讨了ML模型和建模策略是否受特定领域理论（如物理、化学）的影响。作者认为，现有观点过于 simplistic：一方面将ML视为与传统科学一样依赖理论，另一方面视其为理论独立；这些均未能准确反映实际情况。最终，研究揭示，虽然ML模型的构建相对独立于领域理论，但其实际实施和解释仍需依赖核心理论假设和背景知识，从而为理解ML与科学理论的互动提供了更 nuanced 的视角。",
      "categories": [
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages with reference",
      "pdf_url": "http://arxiv.org/pdf/2409.11277v1",
      "published_date": "2024-09-17 15:29:14 UTC",
      "updated_date": "2024-09-17 15:29:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:48:44.250038"
    },
    {
      "arxiv_id": "2409.11274v1",
      "title": "Task Arithmetic for Language Expansion in Speech Translation",
      "title_zh": "任务算术用于语音翻译",
      "authors": [
        "Yao-Fei Cheng",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Wen Shen Teo",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "abstract": "Recent advances in large language models (LLMs) have gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-based speech translation (ST). However, expanding language pairs\nfrom an existing instruction-tuned ST system is costly due to the necessity of\nre-training on a combination of new and previous datasets. We propose to expand\nnew language pairs by merging the model trained on new language pairs and the\nexisting model, using task arithmetic. We find that the direct application of\ntask arithmetic for ST causes the merged model to fail to follow instructions;\nthus, generating translation in incorrect languages. To eliminate language\nconfusion, we propose an augmented task arithmetic method that merges an\nadditional language control model. It is trained to generate the correct target\nlanguage token following the instructions. Our experiments demonstrate that our\nproposed language control model can achieve language expansion by eliminating\nlanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66\nand 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the\nuse of our task arithmetic framework can expand to a language pair where\nneither paired ST training data nor a pre-trained ST model is available. We\nfirst synthesize the ST system from machine translation (MT) systems via task\nanalogy, then merge the synthesized ST system to the existing ST model.",
      "tldr_zh": "该研究提出了一种基于任务算术（task arithmetic）的方法，用于扩展指令-based语音翻译（ST）系统的语言对，从而避免重新训练的高成本问题。通过合并新语言对模型和现有模型，该方法解决了直接应用task arithmetic导致的语言混淆问题，例如生成错误的翻译语言。研究引入了一个额外的语言控制模型，训练其生成正确的目标语言标记，以消除混淆。实验结果显示，在MuST-C和CoVoST-2数据集上，该方法分别提高了4.66和4.92 BLEU scores。此外，该框架还能扩展到缺乏配对ST训练数据或预训练模型的语言对，通过从机器翻译（MT）系统合成ST系统并进行合并。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11274v1",
      "published_date": "2024-09-17 15:25:11 UTC",
      "updated_date": "2024-09-17 15:25:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:48:57.389971"
    },
    {
      "arxiv_id": "2409.11272v7",
      "title": "LOLA -- An Open-Source Massively Multilingual Large Language Model",
      "title_zh": "LOLA -- 一个开源的大规模多语言大型语言模型",
      "authors": [
        "Nikit Srivastava",
        "Denis Kuchelev",
        "Tatiana Moteu Ngoli",
        "Kshitij Shetty",
        "Michael Röder",
        "Hamada Zahera",
        "Diego Moussallem",
        "Axel-Cyrille Ngonga Ngomo"
      ],
      "abstract": "This paper presents LOLA, a massively multilingual large language model\ntrained on more than 160 languages using a sparse Mixture-of-Experts\nTransformer architecture. Our architectural and implementation choices address\nthe challenge of harnessing linguistic diversity while maintaining efficiency\nand avoiding the common pitfalls of multilinguality. Our analysis of the\nevaluation results shows competitive performance in natural language generation\nand understanding tasks. Additionally, we demonstrate how the learned\nexpert-routing mechanism exploits implicit phylogenetic linguistic patterns to\npotentially alleviate the curse of multilinguality. We provide an in-depth look\nat the training process, an analysis of the datasets, and a balanced\nexploration of the model's strengths and limitations. As an open-source model,\nLOLA promotes reproducibility and serves as a robust foundation for future\nresearch. Our findings enable the development of compute-efficient multilingual\nmodels with strong, scalable performance across languages.",
      "tldr_zh": "本研究介绍了 LOLA，一种开源的大型多语言模型，训练于超过 160 种语言，使用稀疏 Mixture-of-Experts Transformer 架构来提升效率并应对多语言挑战。该模型通过专家路由机制利用隐含的语言学模式（如 phylogenetic patterns），有效缓解了 multilinguality 的诅咒，在自然语言生成和理解任务中表现出与竞争模型相当的性能。作为开源项目，LOLA 提供了详细的训练过程、数据集分析以及模型优势和局限性的平衡探讨，支持可再现性和未来多语言模型的研究发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11272v7",
      "published_date": "2024-09-17 15:23:08 UTC",
      "updated_date": "2025-02-02 10:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:49:08.344988"
    },
    {
      "arxiv_id": "2409.11267v2",
      "title": "Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids",
      "title_zh": "强化学习与模型预测控制的整合及其在微电网中的应用",
      "authors": [
        "Caio Fabio Oliveira da Silva",
        "Azita Dabiri",
        "Bart De Schutter"
      ],
      "abstract": "This work proposes an approach that integrates reinforcement learning and\nmodel predictive control (MPC) to solve finite-horizon optimal control problems\nin mixed-logical dynamical systems efficiently. Optimization-based control of\nsuch systems with discrete and continuous decision variables entails the online\nsolution of mixed-integer linear programs, which suffer from the curse of\ndimensionality. Our approach aims to mitigate this issue by decoupling the\ndecision on the discrete variables from the decision on the continuous\nvariables. In the proposed approach, reinforcement learning determines the\ndiscrete decision variables and simplifies the online optimization problem of\nthe MPC controller from a mixed-integer linear program to a linear program,\nsignificantly reducing the computational time. A fundamental contribution of\nthis work is the definition of the decoupled Q-function, which plays a crucial\nrole in making the learning problem tractable in a combinatorial action space.\nWe motivate the use of recurrent neural networks to approximate the decoupled\nQ-function and show how they can be employed in a reinforcement learning\nsetting. Simulation experiments on a microgrid system using real-world data\ndemonstrate that the proposed method substantially reduces the online\ncomputation time of MPC while maintaining high feasibility and low\nsuboptimality.",
      "tldr_zh": "本研究提出了一种整合强化学习（Reinforcement Learning）和模型预测控制（Model Predictive Control, MPC）的框架，用于高效解决混合逻辑动态系统（mixed-logical dynamical systems）中的有限时域最优控制问题。该方法通过解耦离散变量和连续变量的决策，使用强化学习来确定离散变量，从而将MPC的在线优化从混合整数线性规划（mixed-integer linear programs）简化为线性规划（linear program），显著降低计算时间。关键贡献包括定义了解耦Q函数（decoupled Q-function）并采用循环神经网络（recurrent neural networks）进行近似，使学习问题在组合动作空间中变得可处理。实验在基于真实数据的微电网（microgrids）系统中验证，该方法大幅减少了在线计算时间，同时保持了高可行性和低次优性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11267v2",
      "published_date": "2024-09-17 15:17:16 UTC",
      "updated_date": "2025-04-14 09:44:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:49:21.421785"
    },
    {
      "arxiv_id": "2409.11262v2",
      "title": "The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound Event Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Bibbó",
        "Thomas Deacon",
        "Arshdeep Singh",
        "Mark D. Plumbley"
      ],
      "abstract": "This paper presents a residential audio dataset to support sound event\ndetection research for smart home applications aimed at promoting wellbeing for\nolder adults. The dataset is constructed by deploying audio recording systems\nin the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic\ncharacteristics are documented through detailed floor plans and construction\nmaterial information to enable replication of the recording environments for AI\nmodel deployment. A novel automated speech removal pipeline is developed, using\npre-trained audio neural networks to detect and remove segments containing\nspoken voice, while preserving segments containing other sound events. The\nresulting dataset consists of privacy-compliant audio recordings that\naccurately capture the soundscapes and activities of daily living within\nresidential spaces. The paper details the dataset creation methodology, the\nspeech removal pipeline utilizing cascaded model architectures, and an analysis\nof the vocal label distribution to validate the speech removal process. This\ndataset enables the development and benchmarking of sound event detection\nmodels tailored specifically for in-home applications.",
      "tldr_zh": "这篇论文介绍了“The Sounds of Home”数据集，用于支持智能家居应用中的Sound Event Detection研究，旨在提升老年人的福祉。数据集通过在8位55-80岁参与者的家中部署音频记录系统，收集7天音频数据，并记录声学特性如楼层平面图和建筑材料以便环境复制。论文开发了一个新型自动化Speech Removal Pipeline，使用预训练的音频神经网络和级联模型架构检测并移除语音段落，同时保留其他声音事件，确保数据集的隐私合规性。该数据集通过分析语音标签分布验证了移除过程的有效性，并为开发和基准测试针对家庭场景的Sound Event Detection模型提供了宝贵资源。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11262v2",
      "published_date": "2024-09-17 15:10:36 UTC",
      "updated_date": "2024-10-04 10:35:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:49:34.424962"
    },
    {
      "arxiv_id": "2409.11258v1",
      "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
      "title_zh": "通过侧信道强化学习攻击网络切片",
      "authors": [
        "Wei Shao",
        "Chandra Thapa",
        "Rayne Holland",
        "Sarah Ali Siddiqui",
        "Seyit Camtepe"
      ],
      "abstract": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
      "tldr_zh": "该论文探讨了5G和6G网络切片的安全漏洞，提出了一种基于reinforcement learning的side-channel cache attack框架，用于攻击共享内存和缓存中的敏感信息，如认证密钥和用户注册数据。框架假设一个切片网络被入侵，通过诱导受害者切片发送注册请求，将攻击建模为强化学习驱动的猜谜游戏，从而动态识别关键数据的存储位置。实验结果显示，该方法在识别敏感数据位置的成功率达到95%至98%，突显了网络切片环境潜在风险，并呼吁采用更 robust的安全措施。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.11258v1",
      "published_date": "2024-09-17 15:07:05 UTC",
      "updated_date": "2024-09-17 15:07:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:49:46.130716"
    },
    {
      "arxiv_id": "2409.11449v1",
      "title": "Evaluation of pretrained language models on music understanding",
      "title_zh": "预训练语言",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "abstract": "Music-text multimodal systems have enabled new approaches to Music\nInformation Research (MIR) applications such as audio-to-text and text-to-audio\nretrieval, text-based song generation, and music captioning. Despite the\nreported success, little effort has been put into evaluating the musical\nknowledge of Large Language Models (LLM). In this paper, we demonstrate that\nLLMs suffer from 1) prompt sensitivity, 2) inability to model negation (e.g.\n'rock song without guitar'), and 3) sensitivity towards the presence of\nspecific words. We quantified these properties as a triplet-based accuracy,\nevaluating the ability to model the relative similarity of labels in a\nhierarchical ontology. We leveraged the Audioset ontology to generate triplets\nconsisting of an anchor, a positive (relevant) label, and a negative (less\nrelevant) label for the genre and instruments sub-tree. We evaluated the\ntriplet-based musical knowledge for six general-purpose Transformer-based\nmodels. The triplets obtained through this methodology required filtering, as\nsome were difficult to judge and therefore relatively uninformative for\nevaluation purposes. Despite the relatively high accuracy reported,\ninconsistencies are evident in all six models, suggesting that off-the-shelf\nLLMs need adaptation to music before use.",
      "tldr_zh": "本研究评估了预训练语言模型（LLMs）在音乐理解方面的表现，突出了其在音乐-文本多模态系统中的局限性，包括提示敏感性、无法正确处理否定（如“rock song without guitar”）以及对特定词汇的敏感性。研究采用三元组准确性（triplet-based accuracy）方法，通过Audioset本体生成的三元组（包含锚点、正标签和负标签）来量化模型在层次化本体（hierarchical ontology）中建模标签相对相似性的能力。结果显示，六种通用Transformer-based模型虽有较高准确率，但存在明显不一致性，建议这些LLMs需针对音乐领域进行适应以提升可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11449v1",
      "published_date": "2024-09-17 14:44:49 UTC",
      "updated_date": "2024-09-17 14:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:49:57.660112"
    },
    {
      "arxiv_id": "2409.11232v2",
      "title": "Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?",
      "title_zh": "翻译失败",
      "authors": [
        "Raffaele Marino"
      ],
      "abstract": "In this manuscript, I present an analysis on the performance of OpenAI\nO1-preview model in solving random K-SAT instances for K$\\in {2,3,4}$ as a\nfunction of $\\alpha=M/N$ where $M$ is the number of clauses and $N$ is the\nnumber of variables of the satisfiable problem. I show that the model can call\nan external SAT solver to solve the instances, rather than solving them\ndirectly. Despite using external solvers, the model reports incorrect\nassignments as output. Moreover, I propose and present an analysis to quantify\nwhether the OpenAI O1-preview model demonstrates a spark of intelligence or\nmerely makes random guesses when outputting an assignment for a Boolean\nsatisfiability problem.",
      "tldr_zh": "这篇论文快速分析了 OpenAI O1-preview 模型在解决随机 K-SAT 问题（K=2,3,4）时的表现，焦点在于模型是否自行求解还是调用外部 SAT solver。研究发现，模型依赖外部求解器，但会输出错误的赋值，从而质疑其真实性能。作者提出了一种量化分析方法，来评估模型是否展现出智能火花，还是仅仅进行随机猜测。实验结果为理解 LLM 在布尔可满足性问题上的局限性提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cond-mat.dis-nn",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11232v2",
      "published_date": "2024-09-17 14:29:03 UTC",
      "updated_date": "2024-09-20 07:53:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:50:09.403268"
    },
    {
      "arxiv_id": "2409.11228v2",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "abstract": "Neural audio codecs have significantly advanced audio compression by\nefficiently converting continuous audio signals into discrete tokens. These\ncodecs preserve high-quality sound and enable sophisticated sound generation\nthrough generative models trained on these tokens. However, existing neural\ncodec models are typically trained on large, undifferentiated audio datasets,\nneglecting the essential discrepancies between sound domains like speech,\nmusic, and environmental sound effects. This oversight complicates data\nmodeling and poses additional challenges to the controllability of sound\ngeneration. To tackle these issues, we introduce the Source-Disentangled Neural\nAudio Codec (SD-Codec), a novel approach that combines audio coding and source\nseparation. By jointly learning audio resynthesis and separation, SD-Codec\nexplicitly assigns audio signals from different domains to distinct codebooks,\nsets of discrete representations. Experimental results indicate that SD-Codec\nnot only maintains competitive resynthesis quality but also, supported by the\nseparation results, demonstrates successful disentanglement of different\nsources in the latent space, thereby enhancing interpretability in audio codec\nand providing potential finer control over the audio generation process.",
      "tldr_zh": "本研究针对神经音频编解码器（neural audio codecs）在处理不同声音领域（如语音、音乐和环境音效）时的差异问题，提出了一种Source-Disentangled Neural Audio Codec (SD-Codec)。该方法通过联合学习音频重合成（resynthesis）和源分离（source separation），将不同领域的音频信号分配到独立的codebooks，从而实现潜在空间的源分离。实验结果表明，SD-Codec 不仅保持了竞争力的音频重合成质量，还提升了音频编解码的可解释性和对生成过程的精细控制。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "ICASSP 2025, project page: https://xiaoyubie1994.github.io/sdcodec/",
      "pdf_url": "http://arxiv.org/pdf/2409.11228v2",
      "published_date": "2024-09-17 14:21:02 UTC",
      "updated_date": "2025-02-11 10:35:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:50:22.591412"
    },
    {
      "arxiv_id": "2409.11195v1",
      "title": "SDP: Spiking Diffusion Policy for Robotic Manipulation with Learnable Channel-Wise Membrane Thresholds",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixing Hou",
        "Maoxu Gao",
        "Hang Yu",
        "Mengyu Yang",
        "Chio-In Ieong"
      ],
      "abstract": "This paper introduces a Spiking Diffusion Policy (SDP) learning method for\nrobotic manipulation by integrating Spiking Neurons and Learnable Channel-wise\nMembrane Thresholds (LCMT) into the diffusion policy model, thereby enhancing\ncomputational efficiency and achieving high performance in evaluated tasks.\nSpecifically, the proposed SDP model employs the U-Net architecture as the\nbackbone for diffusion learning within the Spiking Neural Network (SNN). It\nstrategically places residual connections between the spike convolution\noperations and the Leaky Integrate-and-Fire (LIF) nodes, thereby preventing\ndisruptions to the spiking states. Additionally, we introduce a temporal\nencoding block and a temporal decoding block to transform static and dynamic\ndata with timestep $T_S$ into each other, enabling the transmission of data\nwithin the SNN in spike format. Furthermore, we propose LCMT to enable the\nadaptive acquisition of membrane potential thresholds, thereby matching the\nconditions of varying membrane potentials and firing rates across channels and\navoiding the cumbersome process of manually setting and tuning hyperparameters.\nEvaluating the SDP model on seven distinct tasks with SNN timestep $T_S=4$, we\nachieve results comparable to those of the ANN counterparts, along with faster\nconvergence speeds than the baseline SNN method. This improvement is\naccompanied by a reduction of 94.3\\% in dynamic energy consumption estimated on\n45nm hardware.",
      "tldr_zh": "本文提出 Spiking Diffusion Policy (SDP) 方法，将 Spiking Neurons 和 Learnable Channel-wise Membrane Thresholds (LCMT) 整合到 diffusion policy 模型中，用于提升机器人操作的计算效率和性能。SDP 基于 U-Net 架构作为骨干网络，在 Spiking Neural Network (SNN) 中添加 residual connections 和 temporal encoding/decoding blocks，以保护 spiking states 并处理静态与动态数据。实验结果显示，在七个任务上以 SNN timestep $T_S=4$ 评估，SDP 性能与 ANN 相当，收敛速度更快，同时动态能耗降低 94.3%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11195v1",
      "published_date": "2024-09-17 13:53:36 UTC",
      "updated_date": "2024-09-17 13:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:50:34.318309"
    },
    {
      "arxiv_id": "2409.11192v1",
      "title": "Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Eunhae Lee"
      ],
      "abstract": "One application area of long-term memory (LTM) capabilities with increasing\ntraction is personal AI companions and assistants. With the ability to retain\nand contextualize past interactions and adapt to user preferences, personal AI\ncompanions and assistants promise a profound shift in how we interact with AI\nand are on track to become indispensable in personal and professional settings.\nHowever, this advancement introduces new challenges and vulnerabilities that\nrequire careful consideration regarding the deployment and widespread use of\nthese systems. The goal of this paper is to explore the broader implications of\nbuilding and deploying personal AI applications with LTM capabilities using a\nholistic evaluation approach. This will be done in three ways: 1) reviewing the\ntechnological underpinnings of LTM in Large Language Models, 2) surveying\ncurrent personal AI companions and assistants, and 3) analyzing critical\nconsiderations and implications of deploying and using these applications.",
      "tldr_zh": "这篇论文探讨了构建和部署具有长效记忆 (Long-Term Memory) 能力的个人 AI 助手所带来的伦理和实际挑战，强调这些系统能保留过去互动、适应用户偏好，并改变人与 AI 的互动方式。论文采用整体评估方法，包括审阅 LTM 在大型语言模型 (Large Language Models) 中的技术基础、调查现有个人 AI 伴侣和助手，以及分析部署的关键考虑和潜在影响。最终，研究旨在推动更安全、可信赖的个人 AI 应用，以应对新出现的漏洞和风险。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11192v1",
      "published_date": "2024-09-17 13:48:29 UTC",
      "updated_date": "2024-09-17 13:48:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:50:46.200274"
    },
    {
      "arxiv_id": "2409.11190v2",
      "title": "SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer",
      "title_zh": "SuperCoder2.0：探索 LLMs 作为自主程序员可行性的技术报告",
      "authors": [
        "Anmol Gautam",
        "Kishore Kumar",
        "Adarsh Jha",
        "Mukunda NS",
        "Ishaan Bhola"
      ],
      "abstract": "We present SuperCoder2.0, an advanced autonomous system designed to enhance\nsoftware development through artificial intelligence. The system combines an\nAI-native development approach with intelligent agents to enable fully\nautonomous coding. Key focus areas include a retry mechanism with error output\ntraceback, comprehensive code rewriting and replacement using Abstract Syntax\nTree (ast) parsing to minimize linting issues, code embedding technique for\nretrieval-augmented generation, and a focus on localizing methods for\nproblem-solving rather than identifying specific line numbers. The methodology\nemploys a three-step hierarchical search space reduction approach for code base\nnavigation and bug localization:utilizing Retrieval Augmented Generation (RAG)\nand a Repository File Level Map to identify candidate files, (2) narrowing down\nto the most relevant files using a File Level Schematic Map, and (3) extracting\n'relevant locations' within these files. Code editing is performed through a\ntwo-part module comprising CodeGeneration and CodeEditing, which generates\nmultiple solutions at different temperature values and replaces entire methods\nor classes to maintain code integrity. A feedback loop executes\nrepository-level test cases to validate and refine solutions. Experiments\nconducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's\neffectiveness, achieving correct file localization in 84.33% of cases within\nthe top 5 candidates and successfully resolving 34% of test instances. This\nperformance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.\nThe system's ability to handle diverse repositories and problem types\nhighlights its potential as a versatile tool for autonomous software\ndevelopment. Future work will focus on refining the code editing process and\nexploring advanced embedding models for improved natural language to code\nmapping.",
      "tldr_zh": "本研究介绍了 SuperCoder2.0，一种利用大型语言模型 (LLMs) 作为自主程序员的可行性探索系统，该系统结合 AI-native 开发方法和智能代理，实现完全自主编码。关键方法包括三步层次化搜索空间减少（利用 Retrieval Augmented Generation (RAG) 和 Abstract Syntax Tree (AST) 解析进行文件定位、代码重写及错误处理）、代码生成与编辑模块，以及反馈循环来验证解决方案。在 SWE-bench Lite 数据集上的实验显示，SuperCoder2.0 在前 5 候选文件中正确定位文件的准确率达 84.33%，成功解决 34% 的测试实例，并在全球 SWE-bench 排行榜上位居第四，展示了其在多样化仓库中的潜力。未来工作将聚焦于优化代码编辑过程和探索高级嵌入模型，以提升自然语言到代码的映射。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11190v2",
      "published_date": "2024-09-17 13:44:42 UTC",
      "updated_date": "2024-10-27 05:57:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:50:59.827898"
    },
    {
      "arxiv_id": "2410.02773v1",
      "title": "Mind the Uncertainty in Human Disagreement: Evaluating Discrepancies between Model Predictions and Human Responses in VQA",
      "title_zh": "注意人类分歧中的不确定性：在 VQA 中评估模型预测和人类响应之间的差异",
      "authors": [
        "Jian Lan",
        "Diego Frassinelli",
        "Barbara Plank"
      ],
      "abstract": "Large vision-language models frequently struggle to accurately predict\nresponses provided by multiple human annotators, particularly when those\nresponses exhibit human uncertainty. In this study, we focus on the Visual\nQuestion Answering (VQA) task, and we comprehensively evaluate how well the\nstate-of-the-art vision-language models correlate with the distribution of\nhuman responses. To do so, we categorize our samples based on their levels\n(low, medium, high) of human uncertainty in disagreement (HUD) and employ not\nonly accuracy but also three new human-correlated metrics in VQA, to\ninvestigate the impact of HUD. To better align models with humans, we also\nverify the effect of common calibration and human calibration. Our results show\nthat even BEiT3, currently the best model for this task, struggles to capture\nthe multi-label distribution inherent in diverse human responses. Additionally,\nwe observe that the commonly used accuracy-oriented calibration technique\nadversely affects BEiT3's ability to capture HUD, further widening the gap\nbetween model predictions and human distributions. In contrast, we show the\nbenefits of calibrating models towards human distributions for VQA, better\naligning model confidence with human uncertainty. Our findings highlight that\nfor VQA, the consistent alignment between human responses and model predictions\nis understudied and should become the next crucial target of future studies.",
      "tldr_zh": "本研究评估了视觉问答 (VQA) 任务中，视觉语言模型 (VLMs) 在预测人类响应时的表现，特别是当人类不确定性分歧 (HUD) 存在时。研究者通过将样本按 HUD 水平（低、中、高）分类，并引入准确率和三个新的人类相关指标，分析了 HUD 对模型的影响，同时比较了常见校准和人类导向校准技术的效果。结果显示，即使是顶级模型 BEiT3 也难以捕捉人类响应的多标签分布，且准确率导向校准反而加剧了模型与人类分布的差距，而人类校准能更好地对齐模型置信度和 HUD。该发现强调，在 VQA 中，实现模型预测与人类响应的持续对齐应成为未来研究的关键目标。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.02773v1",
      "published_date": "2024-09-17 13:44:25 UTC",
      "updated_date": "2024-09-17 13:44:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:51:11.061653"
    },
    {
      "arxiv_id": "2409.11174v1",
      "title": "Identifying Influential nodes in Brain Networks via Self-Supervised Graph-Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Yanqing Kang",
        "Di Zhu",
        "Haiyang Zhang",
        "Enze Shi",
        "Sigang Yu",
        "Jinru Wu",
        "Xuhui Wang",
        "Xuan Liu",
        "Geng Chen",
        "Xi Jiang",
        "Tuo Zhang",
        "Shu Zhang"
      ],
      "abstract": "Studying influential nodes (I-nodes) in brain networks is of great\nsignificance in the field of brain imaging. Most existing studies consider\nbrain connectivity hubs as I-nodes. However, this approach relies heavily on\nprior knowledge from graph theory, which may overlook the intrinsic\ncharacteristics of the brain network, especially when its architecture is not\nfully understood. In contrast, self-supervised deep learning can learn\nmeaningful representations directly from the data. This approach enables the\nexploration of I-nodes for brain networks, which is also lacking in current\nstudies. This paper proposes a Self-Supervised Graph Reconstruction framework\nbased on Graph-Transformer (SSGR-GT) to identify I-nodes, which has three main\ncharacteristics. First, as a self-supervised model, SSGR-GT extracts the\nimportance of brain nodes to the reconstruction. Second, SSGR-GT uses\nGraph-Transformer, which is well-suited for extracting features from brain\ngraphs, combining both local and global characteristics. Third, multimodal\nanalysis of I-nodes uses graph-based fusion technology, combining functional\nand structural brain information. The I-nodes we obtained are distributed in\ncritical areas such as the superior frontal lobe, lateral parietal lobe, and\nlateral occipital lobe, with a total of 56 identified across different\nexperiments. These I-nodes are involved in more brain networks than other\nregions, have longer fiber connections, and occupy more central positions in\nstructural connectivity. They also exhibit strong connectivity and high node\nefficiency in both functional and structural networks. Furthermore, there is a\nsignificant overlap between the I-nodes and both the structural and functional\nrich-club. These findings enhance our understanding of the I-nodes within the\nbrain network, and provide new insights for future research in further\nunderstanding the brain working mechanisms.",
      "tldr_zh": "本文提出了一种基于Self-Supervised Graph-Transformer的框架（SSGR-GT），用于识别脑网络中的影响节点（I-nodes），通过自监督学习直接从数据中提取节点重要性，克服传统方法对图论先验知识的依赖。SSGR-GT结合Graph-Transformer的局部和全局特征分析，并采用图-based融合技术进行多模态（功能和结构）脑信息整合。实验结果显示，识别出56个I-nodes，主要分布在superior frontal lobe、lateral parietal lobe和lateral occipital lobe，这些节点在脑网络中表现出更长的纤维连接、更中心的位置以及更强的连接性和节点效率。总体上，该研究增强了对脑网络影响节点的理解，并为探索脑工作机制提供新洞见。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11174v1",
      "published_date": "2024-09-17 13:31:28 UTC",
      "updated_date": "2024-09-17 13:31:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:51:22.294604"
    },
    {
      "arxiv_id": "2409.11148v3",
      "title": "Improving the Efficiency of Visually Augmented Language Models",
      "title_zh": "提高视觉增强语言模型的效率",
      "authors": [
        "Paula Ontalvilla",
        "Aitor Ormazabal",
        "Gorka Azkune"
      ],
      "abstract": "Despite the impressive performance of autoregressive Language Models (LM) it\nhas been shown that due to reporting bias, LMs lack visual knowledge, i.e. they\ndo not know much about the visual world and its properties. To augment LMs with\nvisual knowledge, existing solutions often rely on explicit images, requiring\ntime-consuming retrieval or image generation systems. This paper shows that\nexplicit images are not necessary to visually augment an LM. Instead, we use\nvisually-grounded text representations obtained from the well-known CLIP\nmultimodal system. For a fair comparison, we modify VALM, a visually-augmented\nLM which uses image retrieval and representation, to work directly with\nvisually-grounded text representations. We name this new model BLIND-VALM. We\nshow that BLIND-VALM performs on par with VALM for Visual Language\nUnderstanding (VLU), Natural Language Understanding (NLU) and Language Modeling\ntasks, despite being significantly more efficient and simpler. We also show\nthat scaling up our model within the compute budget of VALM, either increasing\nthe model or pre-training corpus size, we outperform VALM for all the\nevaluation tasks.",
      "tldr_zh": "本论文探讨了语言模型 (LMs) 由于报告偏差而缺乏视觉知识的问题，并提出一种更高效的视觉增强方法，而非依赖耗时的显式图像。研究者利用 CLIP 系统生成的视觉基础文本表示，修改了 VALM 模型，开发出新模型 BLIND-VALM，该模型直接使用这些文本表示进行增强。结果显示，BLIND-VALM 在 Visual Language Understanding (VLU)、Natural Language Understanding (NLU) 和 Language Modeling 任务上与 VALM 性能相当，但计算效率更高；此外，通过在相同计算预算下扩展模型大小或预训练语料库，BLIND-VALM 能进一步超越 VALM 的表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.11148v3",
      "published_date": "2024-09-17 13:02:19 UTC",
      "updated_date": "2024-12-14 17:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:51:35.523402"
    },
    {
      "arxiv_id": "2409.11145v2",
      "title": "High-Resolution Speech Restoration with Latent Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Tushar Dhyani",
        "Florian Lux",
        "Michele Mancusi",
        "Giorgio Fabbro",
        "Fritz Hohl",
        "Ngoc Thang Vu"
      ],
      "abstract": "Traditional speech enhancement methods often oversimplify the task of\nrestoration by focusing on a single type of distortion. Generative models that\nhandle multiple distortions frequently struggle with phone reconstruction and\nhigh-frequency harmonics, leading to breathing and gasping artifacts that\nreduce the intelligibility of reconstructed speech. These models are also\ncomputationally demanding, and many solutions are restricted to producing\noutputs in the wide-band frequency range, which limits their suitability for\nprofessional applications. To address these challenges, we propose Hi-ResLDM, a\nnovel generative model based on latent diffusion designed to remove multiple\ndistortions and restore speech recordings to studio quality, sampled at 48kHz.\nWe benchmark Hi-ResLDM against state-of-the-art methods that leverage GAN and\nConditional Flow Matching (CFM) components, demonstrating superior performance\nin regenerating high-frequency-band details. Hi-ResLDM not only excels in\nnon-instrusive metrics but is also consistently preferred in human evaluation\nand performs competitively on intrusive evaluations, making it ideal for\nhigh-resolution speech restoration.",
      "tldr_zh": "这篇论文针对传统语音增强方法在处理多种失真时的局限性（如语音重建问题和高频谐波伪像），提出了一种基于Latent Diffusion Model的生成模型Hi-ResLDM，用于移除多种失真并将语音恢复到48kHz的录音室质量。Hi-ResLDM通过潜在扩散机制专注于高频细节再生，并在基准测试中优于GAN和Conditional Flow Matching (CFM)方法。实验结果显示，该模型在非侵入性指标上表现出色，并在人类评估中获得一致偏好，同时在侵入性评估中保持竞争力，适用于高分辨率语音恢复应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11145v2",
      "published_date": "2024-09-17 12:55:23 UTC",
      "updated_date": "2025-02-10 10:06:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:51:57.815899"
    },
    {
      "arxiv_id": "2409.18989v1",
      "title": "SC-Phi2: A Fine-tuned Small Language Model for StarCraft II Macromanagement Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Junaid Khan",
        "Gita Sukthankar"
      ],
      "abstract": "This paper introduces SC-Phi2, a fine-tuned StarCraft II small language model\nfor macromanagement tasks. Small language models, like Phi2, Gemma, and\nDistilBERT, are streamlined versions of large language models (LLMs) with fewer\nparameters that require less power and memory to run. To teach Microsoft's Phi2\nmodel about StarCraft, we create a new SC2 text dataset with information about\nStarCraft races, roles, and actions and use it to fine-tune Phi-2 with\nself-supervised learning. We pair this language model with a Vision Transformer\n(ViT) from the pre-trained BLIP-2 (Bootstrapping Language Image Pre-training)\nmodel, fine-tuning it on the MSC replay dataset. This enables us to construct\ndynamic prompts that include visual game state information. Unlike the large\nmodels used in StarCraft LLMs such as GPT-3.5, Phi2 is trained primarily on\ntextbook data and contains little inherent knowledge of StarCraft II beyond\nwhat is provided by our training process. By using LoRA (Low-rank Adaptation)\nand quantization, our model can be trained on a single GPU. We demonstrate that\nour model performs well at micromanagement tasks such as build order and global\nstate prediction with a small number of parameters.",
      "tldr_zh": "本研究引入了 SC-Phi2，一种针对 StarCraft II 宏管理任务的微调小语言模型，基于 Phi-2 等高效模型减少参数和资源需求。研究者创建了新的 SC2 文本数据集，用于通过自监督学习微调 Phi-2，并将其与 Vision Transformer (ViT) 从预训练模型 BLIP-2 结合，在 MSC replay 数据集上进一步微调，以处理动态提示和视觉游戏状态。利用 LoRA (Low-rank Adaptation) 和量化技术，模型能在单个 GPU 上训练，并在微管理任务如建造顺序和全局状态预测上表现出色，与大型模型相比提升了效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18989v1",
      "published_date": "2024-09-17 12:50:32 UTC",
      "updated_date": "2024-09-17 12:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:52:00.008937"
    },
    {
      "arxiv_id": "2409.11138v1",
      "title": "Learning Generalized Hamiltonians using fully Symplectic Mappings",
      "title_zh": "翻译失败",
      "authors": [
        "Harsh Choudhary",
        "Chandan Gupta",
        "Vyacheslav kungrutsev",
        "Melvin Leok",
        "Georgios Korpas"
      ],
      "abstract": "Many important physical systems can be described as the evolution of a\nHamiltonian system, which has the important property of being conservative,\nthat is, energy is conserved throughout the evolution. Physics Informed Neural\nNetworks and in particular Hamiltonian Neural Networks have emerged as a\nmechanism to incorporate structural inductive bias into the NN model. By\nensuring physical invariances are conserved, the models exhibit significantly\nbetter sample complexity and out-of-distribution accuracy than standard NNs.\nLearning the Hamiltonian as a function of its canonical variables, typically\nposition and velocity, from sample observations of the system thus becomes a\ncritical task in system identification and long-term prediction of system\nbehavior. However, to truly preserve the long-run physical conservation\nproperties of Hamiltonian systems, one must use symplectic integrators for a\nforward pass of the system's simulation. While symplectic schemes have been\nused in the literature, they are thus far limited to situations when they\nreduce to explicit algorithms, which include the case of separable Hamiltonians\nor augmented non-separable Hamiltonians. We extend it to generalized\nnon-separable Hamiltonians, and noting the self-adjoint property of symplectic\nintegrators, we bypass computationally intensive backpropagation through an ODE\nsolver. We show that the method is robust to noise and provides a good\napproximation of the system Hamiltonian when the state variables are sampled\nfrom a noisy observation. In the numerical results, we show the performance of\nthe method concerning Hamiltonian reconstruction and conservation, indicating\nits particular advantage for non-separable systems.",
      "tldr_zh": "本研究提出了一种使用完全辛映射（fully Symplectic Mappings）学习广义Hamiltonian的方法，旨在解决传统神经网络在模拟守恒物理系统时的局限性，如能量守恒问题。作者扩展了Hamiltonian Neural Networks (HNNs)和Physics Informed Neural Networks (PINNs)，通过辛积分器（symplectic integrators）处理非可分离Hamiltonian系统，并利用其自共轭属性绕过ODE求解器的计算密集型反向传播，从而提高模型的鲁棒性和效率。实验结果显示，该方法对噪声观察数据表现出色，能够准确重构Hamiltonian并维持物理守恒特性，尤其在非可分离系统中比传统方法更具优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to The 39th Annual AAAI Conference on Artificial\n  Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2409.11138v1",
      "published_date": "2024-09-17 12:45:49 UTC",
      "updated_date": "2024-09-17 12:45:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:52:10.788166"
    },
    {
      "arxiv_id": "2409.11123v1",
      "title": "Gradient-free Post-hoc Explainability Using Distillation Aided Learnable Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Debarpan Bhattacharya",
        "Amir H. Poorjam",
        "Deepak Mittal",
        "Sriram Ganapathy"
      ],
      "abstract": "The recent advancements in artificial intelligence (AI), with the release of\nseveral large models having only query access, make a strong case for\nexplainability of deep models in a post-hoc gradient free manner. In this\npaper, we propose a framework, named distillation aided explainability (DAX),\nthat attempts to generate a saliency-based explanation in a model agnostic\ngradient free application. The DAX approach poses the problem of explanation in\na learnable setting with a mask generation network and a distillation network.\nThe mask generation network learns to generate the multiplier mask that finds\nthe salient regions of the input, while the student distillation network aims\nto approximate the local behavior of the black-box model. We propose a joint\noptimization of the two networks in the DAX framework using the locally\nperturbed input samples, with the targets derived from input-output access to\nthe black-box model. We extensively evaluate DAX across different modalities\n(image and audio), in a classification setting, using a diverse set of\nevaluations (intersection over union with ground truth, deletion based and\nsubjective human evaluation based measures) and benchmark it with respect to\n$9$ different methods. In these evaluations, the DAX significantly outperforms\nthe existing approaches on all modalities and evaluation metrics.",
      "tldr_zh": "该论文提出了一种无梯度后验解释框架DAX（Distillation Aided Learnable Approach），旨在为仅提供查询访问的黑盒深度模型生成基于显著性的解释，而不依赖梯度信息。DAX 通过一个mask generation network学习生成乘法mask来识别输入的显著区域，并结合student distillation network逼近黑盒模型的局部行为，实现两个网络的联合优化，使用局部扰动输入样本并基于黑盒模型的输入-输出访问作为目标。在图像和音频分类任务上的广泛评估中，DAX与9种现有方法相比，在IoU（Intersection over Union）、基于删除的指标以及主观人类评估等指标上显著优于所有方法。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 10 figures, Accepted in IEEE Journal of Selected Topics in\n  Signal Processing (JSTSP), 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.11123v1",
      "published_date": "2024-09-17 12:21:11 UTC",
      "updated_date": "2024-09-17 12:21:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:52:33.256978"
    },
    {
      "arxiv_id": "2409.11114v2",
      "title": "Diversity-grounded Channel Prototypical Learning for Out-of-Distribution Intent Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Liu",
        "Liming Zhan",
        "Yujie Feng",
        "Zexin Lu",
        "Chengqiang Xie",
        "Lei Xue",
        "Albert Y. S. Lam",
        "Xiao-Ming Wu"
      ],
      "abstract": "In the realm of task-oriented dialogue systems, a robust intent detection\nmechanism must effectively handle malformed utterances encountered in\nreal-world scenarios. This study presents a novel fine-tuning framework for\nlarge language models (LLMs) aimed at enhancing in-distribution (ID) intent\nclassification and out-of-distribution (OOD) intent detection, which utilizes\nsemantic matching with prototypes derived from ID class names. By harnessing\nthe highly distinguishable representations of LLMs, we construct semantic\nprototypes for each ID class using a diversity-grounded prompt tuning approach.\nWe rigorously test our framework in a challenging OOD context, where ID and OOD\nclasses are semantically close yet distinct, referred to as \\emph{near} OOD\ndetection. For a thorough assessment, we benchmark our method against the\nprevalent fine-tuning approaches. The experimental findings reveal that our\nmethod demonstrates superior performance in both few-shot ID intent\nclassification and near-OOD intent detection tasks.",
      "tldr_zh": "这篇论文提出了一种基于多样性引导的通道原型学习框架，用于提升大型语言模型 (LLMs) 在任务导向对话系统中的意图检测能力，特别针对处理真实场景中的畸形语句。框架通过多样性引导的提示调优 (diversity-grounded prompt tuning) 构建 in-distribution (ID) 类的语义原型，并利用语义匹配来增强 ID 意图分类和 out-of-distribution (OOD) 意图检测，尤其在语义上相近的 near-OOD 场景中。实验结果表明，该方法在少样本 ID 分类和 near-OOD 检测任务上显著优于现有微调方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2409.11114v2",
      "published_date": "2024-09-17 12:07:17 UTC",
      "updated_date": "2024-09-20 14:03:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:52:35.677611"
    },
    {
      "arxiv_id": "2410.02771v1",
      "title": "Complex-valued convolutional neural network classification of hand gesture from radar images",
      "title_zh": "翻译失败",
      "authors": [
        "Shokooh Khandan"
      ],
      "abstract": "Hand gesture recognition systems have yielded many exciting advancements in\nthe last decade and become more popular in HCI (human-computer interaction)\nwith several application areas, which spans from safety and security\napplications to automotive field. Various deep neural network architectures\nhave already been inspected for hand gesture recognition systems, including\nmulti-layer perceptron (MLP), convolutional neural network (CNN), recurrent\nneural network (RNN) and a cascade of the last two architectures known as\nCNN-RNN. However, a major problem still exists, which is most of the existing\nML algorithms are designed and developed the building blocks and techniques for\nreal-valued (RV). Researchers applied various RV techniques on the\ncomplex-valued (CV) radar images, such as converting a CV optimisation problem\ninto a RV one, by splitting the complex numbers into their real and imaginary\nparts. However, the major disadvantage of this method is that the resulting\nalgorithm will double the network dimensions. Recent work on RNNs and other\nfundamental theoretical analysis suggest that CV numbers have a richer\nrepresentational capacity, but due to the absence of the building blocks\nrequired to design such models, the performance of CV networks are\nmarginalised. In this report, we propose a fully CV-CNN, including all building\nblocks, forward and backward operations, and derivatives all in complex domain.\nWe explore our proposed classification model on two sets of CV hand gesture\nradar images in comparison with the equivalent RV model. In chapter five, we\npropose a CV-forward residual network, for the purpose of binary classification\nof the two sets of CV hand gesture radar datasets and compare its performance\nwith our proposed CV-CNN and a baseline CV-forward CNN.",
      "tldr_zh": "该论文探讨了使用复杂值卷积神经网络（CV-CNN）对雷达图像进行手势分类的问题，针对现有机器学习算法主要针对实值（RV）数据设计而导致的网络维度加倍等挑战。作者提出一个完整的CV-CNN框架，包括所有构建块、前向和后向操作以及复杂域中的导数，以充分利用CV数据的丰富表示能力。实验结果显示，该模型在两组CV手势雷达图像数据集上的表现与等效RV模型相比进行了评估，并在第五章引入了CV-forward residual network，用于二元分类任务，进一步提升了分类性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07 (Primary), 90C30 (Secondary)",
        "I.2.6; G.1.6"
      ],
      "primary_category": "cs.CV",
      "comment": "173 pages, 36 tables, 50 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.02771v1",
      "published_date": "2024-09-17 11:49:14 UTC",
      "updated_date": "2024-09-17 11:49:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:52:47.306865"
    },
    {
      "arxiv_id": "2409.11078v1",
      "title": "MonoKAN: Certified Monotonic Kolmogorov-Arnold Network",
      "title_zh": "翻译失败",
      "authors": [
        "Alejandro Polo-Molina",
        "David Alfaya",
        "Jose Portela"
      ],
      "abstract": "Artificial Neural Networks (ANNs) have significantly advanced various fields\nby effectively recognizing patterns and solving complex problems. Despite these\nadvancements, their interpretability remains a critical challenge, especially\nin applications where transparency and accountability are essential. To address\nthis, explainable AI (XAI) has made progress in demystifying ANNs, yet\ninterpretability alone is often insufficient. In certain applications, model\npredictions must align with expert-imposed requirements, sometimes exemplified\nby partial monotonicity constraints. While monotonic approaches are found in\nthe literature for traditional Multi-layer Perceptrons (MLPs), they still face\ndifficulties in achieving both interpretability and certified partial\nmonotonicity. Recently, the Kolmogorov-Arnold Network (KAN) architecture, based\non learnable activation functions parametrized as splines, has been proposed as\na more interpretable alternative to MLPs. Building on this, we introduce a\nnovel ANN architecture called MonoKAN, which is based on the KAN architecture\nand achieves certified partial monotonicity while enhancing interpretability.\nTo achieve this, we employ cubic Hermite splines, which guarantee monotonicity\nthrough a set of straightforward conditions. Additionally, by using positive\nweights in the linear combinations of these splines, we ensure that the network\npreserves the monotonic relationships between input and output. Our experiments\ndemonstrate that MonoKAN not only enhances interpretability but also improves\npredictive performance across the majority of benchmarks, outperforming\nstate-of-the-art monotonic MLP approaches.",
      "tldr_zh": "本研究针对人工神经网络(ANNs) 的可解释性挑战，提出了一种新型架构MonoKAN，该架构基于Kolmogorov-Arnold Network (KAN)，实现了认证的部分单调性，同时提升了模型的透明度。MonoKAN 采用cubic Hermite splines作为可学习激活函数，通过简单条件确保单调性，并使用正权重维持输入输出间的单调关系。实验结果显示，MonoKAN 在大多数基准测试中不仅优于现有单调Multi-layer Perceptrons (MLPs) 方法，还显著提高了预测性能和可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "68T07, 68T05, 41A15"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11078v1",
      "published_date": "2024-09-17 11:10:59 UTC",
      "updated_date": "2024-09-17 11:10:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:52:58.902844"
    },
    {
      "arxiv_id": "2409.11074v2",
      "title": "RoMath: A Mathematical Reasoning Benchmark in Romanian",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Cosma",
        "Ana-Maria Bucur",
        "Emilian Radoi"
      ],
      "abstract": "Mathematics has long been conveyed through natural language, primarily for\nhuman understanding. With the rise of mechanized mathematics and proof\nassistants, there is a growing need to understand informal mathematical text,\nyet most existing benchmarks focus solely on English, overlooking other\nlanguages. This paper introduces RoMath, a Romanian mathematical reasoning\nbenchmark suite comprising three datasets: RoMath-Baccalaureate,\nRoMath-Competitions and RoMath-Synthetic, which cover a range of mathematical\ndomains and difficulty levels, aiming to improve non-English language models\nand promote multilingual AI development. By focusing on Romanian, a\nlow-resource language with unique linguistic features, RoMath addresses the\nlimitations of Anglo-centric models and emphasizes the need for dedicated\nresources beyond simple automatic translation. We benchmark several open-weight\nlanguage models, highlighting the importance of creating resources for\nunderrepresented languages. We make the code and dataset available.",
      "tldr_zh": "这篇论文引入了 RoMath，这是一个罗马尼亚语的数学推理基准套件，包括 RoMath-Baccalaureate、RoMath-Competitions 和 RoMath-Synthetic 等数据集，覆盖多种数学领域和难度级别。RoMath 旨在提升非英语语言模型的表现，解决英语中心模型的局限性，并通过专注于罗马尼亚语这种低资源语言的独特特征，促进多语言 AI 发展。论文通过基准测试多个开源语言模型，突出了创建专属资源的必要性，并公开了代码和数据集以供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 Figures, 12 Tables",
      "pdf_url": "http://arxiv.org/pdf/2409.11074v2",
      "published_date": "2024-09-17 11:03:46 UTC",
      "updated_date": "2024-09-20 15:47:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:53:11.423770"
    },
    {
      "arxiv_id": "2409.11071v2",
      "title": "Improve Machine Learning carbon footprint using Parquet dataset format and Mixed Precision training for regression models -- Part II",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Antonopoulos"
      ],
      "abstract": "This is the 2nd part of the dissertation for my master degree and compared\nthe power consumption using the Comma-Separated-Values (CSV) and parquet\ndataset format with the default floating point (32bit) and Nvidia mixed\nprecision (16bit and 32bit) while training a regression ML model. The same\ncustom PC as per the 1st part, which was dedicated to the classification\ntesting and analysis, was built to perform the experiments, and different ML\nhyper-parameters, such as batch size, neurons, and epochs, were chosen to build\nDeep Neural Networks (DNN). A benchmarking test with default hyper-parameter\nvalues for the DNN was used as a reference, while the experiments used a\ncombination of different settings. The results were recorded in Excel, and\ndescriptive statistics were chosen to calculate the mean between the groups and\ncompare them using graphs and tables. The outcome was positive when using mixed\nprecision combined with specific hyper-parameters. Compared to the\nbenchmarking, optimising the regression models reduced the power consumption\nbetween 7 and 11 Watts. The regression results show that while mixed precision\ncan help improve power consumption, we must carefully consider the\nhyper-parameters. A high number of batch sizes and neurons will negatively\naffect power consumption. However, this research required inferential\nstatistics, specifically ANOVA and T-test, to compare the relationship between\nthe means. The results reported no statistical significance between the means\nin the regression tests and accepted H0. Therefore, choosing different ML\ntechniques and the Parquet dataset format will not improve the computational\npower consumption and the overall ML carbon footprint. However, a more\nextensive implementation with a cluster of GPUs can increase the sample size\nsignificantly, as it is an essential factor and can change the outcome of the\nstatistical analysis.",
      "tldr_zh": "这篇论文的第二部分探讨了使用Parquet数据集格式和Mixed Precision训练（16位和32位）对回归ML模型功耗的影响，与CSV格式和默认浮点数（32位）进行比较。研究通过在自定义PC上训练Deep Neural Networks (DNN)，测试不同超参数（如批量大小、神经元数和训练轮数），并采用描述性统计和推断统计（ANOVA和T-test）分析结果。实验发现，结合特定超参数的Mixed Precision可降低功耗7-11瓦，但高批量大小和神经元数可能增加消耗，且统计分析显示差异无显著性。总体结论是，Parquet格式和Mixed Precision无法显著改善ML碳足迹，建议通过更大规模的GPU集群实验来验证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages, 16 tables, 19 figures. arXiv admin note: substantial text\n  overlap with arXiv:2409.07853",
      "pdf_url": "http://arxiv.org/pdf/2409.11071v2",
      "published_date": "2024-09-17 10:53:03 UTC",
      "updated_date": "2024-09-20 08:54:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:53:23.817401"
    },
    {
      "arxiv_id": "2409.11055v4",
      "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",
      "title_zh": "翻译失败",
      "authors": [
        "Jemin Lee",
        "Sihyeong Park",
        "Jinse Kwon",
        "Jihun Oh",
        "Yongin Kwon"
      ],
      "abstract": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in Coding and STEM tasks, though it occasionally reports\nimprovements in reasoning.",
      "tldr_zh": "本研究评估了量化（Quantization）方法在大型语言模型（LLMs）从小型到巨型（1B 到 405B 参数）中的权衡，涵盖四种量化技术并在13个数据集上测试指令微调模型。结果显示，量化模型通常优于小型 FP16 基线，但可能在指令遵循和幻觉检测任务上表现欠佳，其中 FP8 被证明是最稳健的选择，而 AWQ 在权重-only 量化中优于 GPTQ。研究进一步发现，较小模型在4-bit 量化时准确率可能大幅下降，而70B 规模模型保持稳定，且量化往往放大模型固有弱点，而非直接与任务难度相关。总的来说，这为LLMs 的成本有效部署提供了宝贵洞见，特别是强调了任务类型（如Coding和STEM）对性能的影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in IJCAI 2025, 21 pages, 2 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.11055v4",
      "published_date": "2024-09-17 10:31:37 UTC",
      "updated_date": "2025-05-12 02:25:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:53:35.750035"
    },
    {
      "arxiv_id": "2409.11052v1",
      "title": "A logical alarm for misaligned binary classifiers",
      "title_zh": "翻译失败",
      "authors": [
        "Andrés Corrada-Emmanuel",
        "Ilya Parker",
        "Ramesh Bharadwaj"
      ],
      "abstract": "If two agents disagree in their decisions, we may suspect they are not both\ncorrect. This intuition is formalized for evaluating agents that have carried\nout a binary classification task. Their agreements and disagreements on a joint\ntest allow us to establish the only group evaluations logically consistent with\ntheir responses. This is done by establishing a set of axioms (algebraic\nrelations) that must be universally obeyed by all evaluations of binary\nresponders. A complete set of such axioms are possible for each ensemble of\nsize N. The axioms for $N = 1, 2$ are used to construct a fully logical alarm -\none that can prove that at least one ensemble member is malfunctioning using\nonly unlabeled data. The similarities of this approach to formal software\nverification and its utility for recent agendas of safe guaranteed AI are\ndiscussed.",
      "tldr_zh": "该论文提出了一种逻辑警报系统，用于评估二元分类器（binary classifiers）在决策不一致时的可靠性，通过分析代理（agents）在联合测试中的一致性和不一致性。论文定义了一组公理（axioms），这些代数关系必须由所有二元响应评估遵守，从而为 N 个代理的集合建立逻辑上一致的群体评估。对于 N=1 和 N=2，系统能使用仅无标签数据（unlabeled data）证明至少一个代理成员 malfunctioning。总体上，这种方法类似于形式软件验证（formal software verification），并为安全可靠 AI（safe guaranteed AI）的议程提供了实用工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "62G99 (Primary), 14Q99 (Secondary)",
        "I.2.3"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 7 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2409.11052v1",
      "published_date": "2024-09-17 10:19:22 UTC",
      "updated_date": "2024-09-17 10:19:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:53:47.532169"
    },
    {
      "arxiv_id": "2410.05274v2",
      "title": "Scale-Invariant Object Detection by Adaptive Convolution with Unified Global-Local Context",
      "title_zh": "翻译失败",
      "authors": [
        "Amrita Singh",
        "Snehasis Mukherjee"
      ],
      "abstract": "Dense features are important for detecting minute objects in images.\nUnfortunately, despite the remarkable efficacy of the CNN models in multi-scale\nobject detection, CNN models often fail to detect smaller objects in images due\nto the loss of dense features during the pooling process. Atrous convolution\naddresses this issue by applying sparse kernels. However, sparse kernels often\ncan lose the multi-scale detection efficacy of the CNN model. In this paper, we\npropose an object detection model using a Switchable (adaptive) Atrous\nConvolutional Network (SAC-Net) based on the efficientDet model. A fixed atrous\nrate limits the performance of the CNN models in the convolutional layers. To\novercome this limitation, we introduce a switchable mechanism that allows for\ndynamically adjusting the atrous rate during the forward pass. The proposed\nSAC-Net encapsulates the benefits of both low-level and high-level features to\nachieve improved performance on multi-scale object detection tasks, without\nlosing the dense features. Further, we apply a depth-wise switchable atrous\nrate to the proposed network, to improve the scale-invariant features. Finally,\nwe apply global context on the proposed model. Our extensive experiments on\nbenchmark datasets demonstrate that the proposed SAC-Net outperforms the\nstate-of-the-art models by a significant margin in terms of accuracy.",
      "tldr_zh": "该论文针对CNN模型在多尺度物体检测中因池化过程丢失密集特征而难以检测小物体的局限性，提出了一种基于efficientDet的Switchable (adaptive) Atrous Convolutional Network (SAC-Net)。SAC-Net通过动态调整atrous rate的切换机制，结合低级和高级特征以及depth-wise switchable atrous rate，实现对密集特征的保留和scale-invariant特征的提升，同时融入unified global-local context以优化检测性能。实验结果显示，该模型在基准数据集上显著超过了最先进模型的准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.05274v2",
      "published_date": "2024-09-17 10:08:37 UTC",
      "updated_date": "2025-03-05 08:36:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:53:59.520015"
    },
    {
      "arxiv_id": "2409.11446v1",
      "title": "Volvo Discovery Challenge at ECML-PKDD 2024",
      "title_zh": "翻译失败",
      "authors": [
        "Mahmoud Rahat",
        "Peyman Sheikholharam Mashhadi",
        "Sławomir Nowaczyk",
        "Shamik Choudhury",
        "Leo Petrin",
        "Thorsteinn Rognvaldsson",
        "Andreas Voskou",
        "Carlo Metta",
        "Claudio Savelli"
      ],
      "abstract": "This paper presents an overview of the Volvo Discovery Challenge, held during\nthe ECML-PKDD 2024 conference. The challenge's goal was to predict the failure\nrisk of an anonymized component in Volvo trucks using a newly published\ndataset. The test data included observations from two generations (gen1 and\ngen2) of the component, while the training data was provided only for gen1. The\nchallenge attracted 52 data scientists from around the world who submitted a\ntotal of 791 entries. We provide a brief description of the problem definition,\nchallenge setup, and statistics about the submissions. In the section on\nwinning methodologies, the first, second, and third-place winners of the\ncompetition briefly describe their proposed methods and provide GitHub links to\ntheir implemented code. The shared code can be interesting as an advanced\nmethodology for researchers in the predictive maintenance domain. The\ncompetition was hosted on the Codabench platform.",
      "tldr_zh": "这篇论文概述了在 ECML-PKDD 2024 会议上举办的 Volvo Discovery Challenge，该挑战的目标是使用新发布的数据集预测 Volvo 卡车中匿名组件的故障风险。训练数据仅限于组件的 gen1 版本，而测试数据包括 gen1 和 gen2 的观察数据，挑战吸引了 52 名数据科学家提交总计 791 份条目。论文详细描述了问题定义、挑战设置和提交统计，并分享了前三名获胜者的方法及其 GitHub 代码，这些方法对预测性维护领域的研究者具有先进参考价值。比赛在 Codabench 平台上进行。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ECML/PKDD 2024, Discovery Challenge",
      "pdf_url": "http://arxiv.org/pdf/2409.11446v1",
      "published_date": "2024-09-17 10:05:24 UTC",
      "updated_date": "2024-09-17 10:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:54:11.950134"
    },
    {
      "arxiv_id": "2410.02769v1",
      "title": "Fundamentals of legislation for autonomous artificial intelligence systems",
      "title_zh": "自治人工智能系统的立法基础",
      "authors": [
        "Anna Romanova"
      ],
      "abstract": "The article proposes a method for forming a dedicated operational context in\ncourse of development and implementation of autonomous corporate management\nsystems based on example of autonomous systems for a board of directors. The\nsignificant part of the operational context for autonomous company management\nsystems is the regulatory and legal environment within which corporations\noperate. In order to create a special operational context for autonomous\nartificial intelligence systems, the wording of local regulatory documents can\nbe simultaneously presented in two versions: for use by people and for use by\nautonomous systems. In this case, the artificial intelligence system will get a\nwell-defined operational context that allows such a system to perform functions\nwithin the required standards. Local regulations that provide for the specifics\nof the joint work of individuals and autonomous artificial intelligence systems\ncan create the basis of the relevant legislation governing the development and\nimplementation of autonomous systems.",
      "tldr_zh": "该论文探讨了为自主人工智能系统（autonomous artificial intelligence systems）制定立法基础的方法，特别是针对公司管理系统的董事会示例。作者提出，通过创建专用操作上下文（operational context），将本地法规制定为两种版本：一种供人类使用，另一种供AI系统使用，以确保AI在规定标准内执行功能。该方法强调监管和法律环境（regulatory and legal environment）的关键作用，并认为这种本地法规可作为更广泛立法的基础，支持自主系统的开发和实施。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "in Russian language",
      "pdf_url": "http://arxiv.org/pdf/2410.02769v1",
      "published_date": "2024-09-17 09:50:23 UTC",
      "updated_date": "2024-09-17 09:50:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:54:22.425785"
    },
    {
      "arxiv_id": "2409.11024v1",
      "title": "D2Vformer: A Flexible Time Series Prediction Model Based on Time Position Embedding",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaobao Song",
        "Hao Wang",
        "Liwei Deng",
        "Yuxin He",
        "Wenming Cao",
        "Chi-Sing Leungc"
      ],
      "abstract": "Time position embeddings capture the positional information of time steps,\noften serving as auxiliary inputs to enhance the predictive capabilities of\ntime series models. However, existing models exhibit limitations in capturing\nintricate time positional information and effectively utilizing these\nembeddings. To address these limitations, this paper proposes a novel model\ncalled D2Vformer. Unlike typical prediction methods that rely on RNNs or\nTransformers, this approach can directly handle scenarios where the predicted\nsequence is not adjacent to the input sequence or where its length dynamically\nchanges. In comparison to conventional methods, D2Vformer undoubtedly saves a\nsignificant amount of training resources. In D2Vformer, the Date2Vec module\nuses the timestamp information and feature sequences to generate time position\nembeddings. Afterward, D2Vformer introduces a new fusion block that utilizes an\nattention mechanism to explore the similarity in time positions between the\nembeddings of the input sequence and the predicted sequence, thereby generating\npredictions based on this similarity. Through extensive experiments on six\ndatasets, we demonstrate that Date2Vec outperforms other time position\nembedding methods, and D2Vformer surpasses state-of-the-art methods in both\nfixed-length and variable-length prediction tasks.",
      "tldr_zh": "这篇论文针对时间序列预测模型在捕捉和利用 Time Position Embedding 的局限性，提出了一种新型灵活模型 D2Vformer。D2Vformer 包括 Date2Vec 模块，利用时间戳和特征序列生成时间位置嵌入，并引入一个基于 Attention Mechanism 的融合块，来探索输入序列和预测序列之间的时间位置相似性，从而实现高效预测。该模型能直接处理预测序列与输入序列不连续或长度动态变化的场景，比传统 RNNs 或 Transformers 方法节省大量训练资源。在六个数据集上的实验中，Date2Vec 优于其他嵌入方法，而 D2Vformer 在固定长度和可变长度预测任务中超越了最先进模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11024v1",
      "published_date": "2024-09-17 09:39:37 UTC",
      "updated_date": "2024-09-17 09:39:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:54:35.792879"
    },
    {
      "arxiv_id": "2409.11022v4",
      "title": "DynamicNER: A Dynamic, Multilingual, and Fine-Grained Dataset for LLM-based Named Entity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Hanjun Luo",
        "Yingbin Jin",
        "Xinfeng Li",
        "Xuecheng Liu",
        "Ruizhe Chen",
        "Tong Shang",
        "Kun Wang",
        "Qingsong Wen",
        "Zuozhu Liu"
      ],
      "abstract": "With the advancement of Large Language Models (LLMs), more and more\nresearchers apply LLMs for Named Entity Recognition (NER) methods, bringing\nvitality to this classical Natural Language Processing task. However, existing\ndatasets are designed for traditional machine learning methods, inadequate for\nLLM-based methods in terms of corpus selection, entity categorization, and\ndesign logic. This limitation leads to less effective evaluation and model\nfine-tuning. To address this issue, we propose DynamicNER, the first NER\ndataset specifically designed for LLMs and with dynamic categorization,\ntranscending the limitations of fixed categorization in existing datasets. It\nis also multi-lingual and multi-granular, covering 8 languages and 155 entity\ntypes, with corpus spanning multiple specialized domains. Furthermore, in\nresponse to the limitations demonstrated by existing LLM-based methods during\nDynamicNER testing, we develop CascadeNER, a novel NER method based on a\ntwo-stage strategy and lightweight LLMs, addressing the problems in current\nmethods. Experiments show that DynamicNER is an effective benchmark for\nLLM-based NER methods, and CascadeNER outperforms existing methods with fewer\ncomputational resources. Our work is opened at\nhttps://github.com/CascadeNER/CascadeNER.",
      "tldr_zh": "该研究指出，现有的命名实体识别(NER)数据集不适合Large Language Models (LLMs)方法，因此提出DynamicNER，这是一个动态、多语言和细粒度的NER数据集，覆盖8种语言、155种实体类型，并跨越多个专业领域，以更好地支持LLM-based方法的评估和微调。DynamicNER采用动态分类策略，超越了传统数据集的固定分类限制。针对现有方法的不足，作者开发了CascadeNER，一种基于两阶段策略和轻量级LLMs的新型NER方法。实验结果显示，DynamicNER作为有效的基准，CascadeNER在更少的计算资源下优于现有方法，并已在GitHub开源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11022v4",
      "published_date": "2024-09-17 09:32:12 UTC",
      "updated_date": "2025-02-24 08:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:54:46.807670"
    },
    {
      "arxiv_id": "2409.11011v1",
      "title": "Enhanced segmentation of femoral bone metastasis in CT scans of patients using synthetic data generation with 3D diffusion models",
      "title_zh": "翻译失败",
      "authors": [
        "Emile Saillard",
        "Aurélie Levillain",
        "David Mitton",
        "Jean-Baptiste Pialat",
        "Cyrille Confavreux",
        "Hélène Follet",
        "Thomas Grenier"
      ],
      "abstract": "Purpose: Bone metastasis have a major impact on the quality of life of\npatients and they are diverse in terms of size and location, making their\nsegmentation complex. Manual segmentation is time-consuming, and expert\nsegmentations are subject to operator variability, which makes obtaining\naccurate and reproducible segmentations of bone metastasis on CT-scans a\nchallenging yet important task to achieve. Materials and Methods: Deep learning\nmethods tackle segmentation tasks efficiently but require large datasets along\nwith expert manual segmentations to generalize on new images. We propose an\nautomated data synthesis pipeline using 3D Denoising Diffusion Probabilistic\nModels (DDPM) to enchance the segmentation of femoral metastasis from CT-scan\nvolumes of patients. We used 29 existing lesions along with 26 healthy femurs\nto create new realistic synthetic metastatic images, and trained a DDPM to\nimprove the diversity and realism of the simulated volumes. We also\ninvestigated the operator variability on manual segmentation. Results: We\ncreated 5675 new volumes, then trained 3D U-Net segmentation models on real and\nsynthetic data to compare segmentation performance, and we evaluated the\nperformance of the models depending on the amount of synthetic data used in\ntraining. Conclusion: Our results showed that segmentation models trained with\nsynthetic data outperformed those trained on real volumes only, and that those\nmodels perform especially well when considering operator variability.",
      "tldr_zh": "该研究针对股骨骨转移瘤在CT扫描中的分割挑战，提出了一种使用3D Denoising Diffusion Probabilistic Models (DDPM)生成合成数据的自动管道，以解决手动分割耗时且易受操作者变异的问题。研究利用29个现有病变和26个健康股骨创建了5675个新合成体积，并通过训练DDPM提升了数据的多样性和真实性。结果显示，使用真实和合成数据训练的3D U-Net模型在分割性能上优于仅使用真实数据的模型，尤其在考虑操作者变异时表现出色。该方法为提高骨转移瘤分割的准确性和可重复性提供了有效途径。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "14 pages, 5 figures 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.11011v1",
      "published_date": "2024-09-17 09:21:19 UTC",
      "updated_date": "2024-09-17 09:21:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:54:59.558942"
    },
    {
      "arxiv_id": "2409.11003v1",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "abstract": "Audio token modeling has become a powerful framework for speech synthesis,\nwith two-stage approaches employing semantic tokens remaining prevalent. In\nthis paper, we aim to simplify this process by introducing a semantic knowledge\ndistillation method that enables high-quality speech generation in a single\nstage. Our proposed model improves speech quality, intelligibility, and speaker\nsimilarity compared to a single-stage baseline. Although two-stage systems\nstill lead in intelligibility, our model significantly narrows the gap while\ndelivering comparable speech quality. These findings showcase the potential of\nsingle-stage models to achieve efficient, high-quality TTS with a more compact\nand streamlined architecture.",
      "tldr_zh": "本文提出了一种单阶段 TTS 系统，利用 Masked Audio Token Modeling 和 Semantic Knowledge Distillation 方法，通过语义知识蒸馏简化语音合成过程。相比单阶段基线，该模型显著提高了语音质量、可懂度和说话者相似度。实验结果显示，虽然两阶段系统在可懂度上仍占优势，但本文模型缩小了这一差距，并在整体语音质量上表现相当。总之，这种方法展示了单阶段模型在高效、高质量 TTS 中的潜力，具有更紧凑的架构。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "Demo page: see https://narsistts.github.io",
      "pdf_url": "http://arxiv.org/pdf/2409.11003v1",
      "published_date": "2024-09-17 09:08:43 UTC",
      "updated_date": "2024-09-17 09:08:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:55:10.622151"
    },
    {
      "arxiv_id": "2409.10999v1",
      "title": "Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models",
      "title_zh": "增强音频语言模型的低资源语言及指令跟随能力",
      "authors": [
        "Potsawee Manakul",
        "Guangzhi Sun",
        "Warit Sirichotedumrong",
        "Kasima Tharnpipitchai",
        "Kunat Pipatanakul"
      ],
      "abstract": "Audio language models can understand audio inputs and perform a range of\naudio-related tasks based on instructions, such as speech recognition and audio\ncaptioning, where the instructions are usually textual prompts. Audio language\nmodels are mostly initialized from pre-trained audio encoders and large\nlanguage models (LLMs). Although these pre-trained components were developed to\nsupport multiple languages, audio-language models are trained predominantly on\nEnglish data, which may limit their usability to only English instructions or\nEnglish speech inputs. First, this paper examines the performance of existing\naudio language models in an underserved language using Thai as an example. This\npaper demonstrates that, despite being built on multilingual backbones, audio\nlanguage models do not exhibit cross-lingual emergent abilities to low-resource\nlanguages. Second, this paper studies data mixture for developing audio\nlanguage models that are optimized for a target language as well as English. In\naddition. this paper integrates audio comprehension and speech\ninstruction-following capabilities into a single unified model. Our experiments\nprovide insights into data mixture for enhancing instruction-following\ncapabilities in both a low-resource language and English. Our model,\nTyphoon-Audio, outperforms existing open-source audio language models by a\nconsiderable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in\nboth English and Thai languages.",
      "tldr_zh": "本论文探讨了音频语言模型（Audio Language Models）在低资源语言（如泰语）上的指令遵循能力问题，发现现有基于多语言骨干的模型缺乏跨语言涌现能力，导致英语数据主导下的性能限制。研究通过数据混合策略优化模型，支持目标语言和英语，同时将音频理解与语音指令遵循整合到一个统一模型中。实验结果显示，所提出的Typhoon-Audio模型在英语和泰语任务上显著优于现有开源模型，并与Gemini-1.5-Pro相当，为增强低资源语言的音频语言模型提供了关键见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages. Preprint under review",
      "pdf_url": "http://arxiv.org/pdf/2409.10999v1",
      "published_date": "2024-09-17 09:04:03 UTC",
      "updated_date": "2024-09-17 09:04:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:55:23.395239"
    },
    {
      "arxiv_id": "2409.10994v3",
      "title": "Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Dingjie Song",
        "Wenjun Wang",
        "Shunian Chen",
        "Xidong Wang",
        "Michael Guan",
        "Benyou Wang"
      ],
      "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models.",
      "tldr_zh": "该论文提出了一种简单有效的令牌减少方法TRIM（Token Reduction using CLIP Metric），旨在提升多模态大语言模型(MLLMs)的效率，同时避免性能损失。TRIM方法借鉴人类在视觉问答(VQA)任务中的注意力模式，通过CLIP Metric选择性地减少图像令牌，从而显著降低计算开销。在12个数据集上的实验结果显示，该方法在保持性能一致的同时，大大提高了MLLMs的资源利用率，为模型的可访问性和可持续性发展提供了关键进展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.10994v3",
      "published_date": "2024-09-17 08:56:27 UTC",
      "updated_date": "2024-12-17 02:05:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:55:34.690073"
    },
    {
      "arxiv_id": "2409.10989v2",
      "title": "GOSt-MT: A Knowledge Graph for Occupation-related Gender Biases in Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Orfeas Menis Mastromichalakis",
        "Giorgos Filandrianos",
        "Eva Tsouparopoulou",
        "Dimitris Parsanoglou",
        "Maria Symeonaki",
        "Giorgos Stamou"
      ],
      "abstract": "Gender bias in machine translation (MT) systems poses significant challenges\nthat often result in the reinforcement of harmful stereotypes. Especially in\nthe labour domain where frequently occupations are inaccurately associated with\nspecific genders, such biases perpetuate traditional gender stereotypes with a\nsignificant impact on society. Addressing these issues is crucial for ensuring\nequitable and accurate MT systems. This paper introduces a novel approach to\nstudying occupation-related gender bias through the creation of the GOSt-MT\n(Gender and Occupation Statistics for Machine Translation) Knowledge Graph.\nGOSt-MT integrates comprehensive gender statistics from real-world labour data\nand textual corpora used in MT training. This Knowledge Graph allows for a\ndetailed analysis of gender bias across English, French, and Greek,\nfacilitating the identification of persistent stereotypes and areas requiring\nintervention. By providing a structured framework for understanding how\noccupations are gendered in both labour markets and MT systems, GOSt-MT\ncontributes to efforts aimed at making MT systems more equitable and reducing\ngender biases in automated translations.",
      "tldr_zh": "本研究探讨了机器翻译（MT）系统中的职业相关性别偏见问题，这些偏见往往强化有害刻板印象，尤其在劳动力领域，导致职业与特定性别不当关联。论文提出 GOSt-MT 知识图谱（Knowledge Graph），通过整合真实劳动力数据的性别统计和 MT 训练语料，对英语、法语和希腊语中的性别偏见进行详细分析。GOSt-MT 提供了一个结构化框架，识别持久的性别刻板印象和需要干预的领域，从而有助于开发更公平的 MT 系统并减少自动化翻译中的偏见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the KG-STAR'24: Workshop on Knowledge Graphs for\n  Responsible AI co-located with the 33rd ACM CIKM Conference, October 25,\n  2024, Boise, Idaho",
      "pdf_url": "http://arxiv.org/pdf/2409.10989v2",
      "published_date": "2024-09-17 08:44:20 UTC",
      "updated_date": "2024-10-04 12:13:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:55:47.592110"
    },
    {
      "arxiv_id": "2409.10986v1",
      "title": "Control-flow Reconstruction Attacks on Business Process Models",
      "title_zh": "翻译失败",
      "authors": [
        "Henrik Kirchmann",
        "Stephan A. Fahrenkrog-Petersen",
        "Felix Mannhardt",
        "Matthias Weidlich"
      ],
      "abstract": "Process models may be automatically generated from event logs that contain\nas-is data of a business process. While such models generalize over the\ncontrol-flow of specific, recorded process executions, they are often also\nannotated with behavioural statistics, such as execution frequencies.Based\nthereon, once a model is published, certain insights about the original process\nexecutions may be reconstructed, so that an external party may extract\nconfidential information about the business process. This work is the first to\nempirically investigate such reconstruction attempts based on process models.\nTo this end, we propose different play-out strategies that reconstruct the\ncontrol-flow from process trees, potentially exploiting frequency annotations.\nTo assess the potential success of such reconstruction attacks on process\nmodels, and hence the risks imposed by publishing them, we compare the\nreconstructed process executions with those of the original log for several\nreal-world datasets.",
      "tldr_zh": "该研究首次实证调查了基于business process models的control-flow重建攻击，揭示了发布这些模型可能导致外部方从事件日志中重建原始流程执行，从而泄露商业机密信息。研究提出多种play-out strategies，从process trees重建控制-flow，并利用frequency annotations来提升重建准确性。通过比较重建的流程执行与几个真实数据集的原始日志，评估了这种攻击的成功潜力及其风险，为流程模型的发布提供安全指导。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10986v1",
      "published_date": "2024-09-17 08:42:55 UTC",
      "updated_date": "2024-09-17 08:42:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:55:57.997483"
    },
    {
      "arxiv_id": "2409.10964v2",
      "title": "Active learning for energy-based antibody optimization and enhanced screening",
      "title_zh": "翻译失败",
      "authors": [
        "Kairi Furui",
        "Masahito Ohue"
      ],
      "abstract": "Accurate prediction and optimization of protein-protein binding affinity is\ncrucial for therapeutic antibody development. Although machine learning-based\nprediction methods $\\Delta\\Delta G$ are suitable for large-scale mutant\nscreening, they struggle to predict the effects of multiple mutations for\ntargets without existing binders. Energy function-based methods, though more\naccurate, are time consuming and not ideal for large-scale screening. To\naddress this, we propose an active learning workflow that efficiently trains a\ndeep learning model to learn energy functions for specific targets, combining\nthe advantages of both approaches. Our method integrates the RDE-Network deep\nlearning model with Rosetta's energy function-based Flex ddG to efficiently\nexplore mutants. In a case study targeting HER2-binding Trastuzumab mutants,\nour approach significantly improved the screening performance over random\nselection and demonstrated the ability to identify mutants with better binding\nproperties without experimental $\\Delta\\Delta G$ data. This workflow advances\ncomputational antibody design by combining machine learning, physics-based\ncomputations, and active learning to achieve more efficient antibody\ndevelopment.",
      "tldr_zh": "该论文提出了一种主动学习(active learning)工作流，用于能量函数-based抗体优化和增强筛选，旨在解决机器学习方法在处理无现有结合体目标的多突变预测时的局限性，同时克服能量函数方法的计算耗时问题。工作流整合了RDE-Network深度学习模型与Rosetta的Flex ddG能量函数，高效探索突变并针对特定目标训练模型。在针对HER2-binding Trastuzumab突变的案例研究中，该方法显著优于随机选择，能够识别出具有更好结合特性的突变，而无需实验ΔΔG数据，从而推进了计算抗体设计的效率和准确性。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "q-bio.BM",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.10964v2",
      "published_date": "2024-09-17 08:01:58 UTC",
      "updated_date": "2024-09-18 07:37:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:56:11.650795"
    },
    {
      "arxiv_id": "2410.00916v1",
      "title": "IBM Quantum Computers: Evolution, Performance, and Future Directions",
      "title_zh": "IBM 量子计算机：演变、性能以及未来方向",
      "authors": [
        "M. AbuGhanem"
      ],
      "abstract": "Quantum computers represent a transformative frontier in computational\ntechnology, promising exponential speedups beyond classical computing limits.\nIBM Quantum has led significant advancements in both hardware and software,\nproviding access to quantum hardware via IBM Cloud since 2016, achieving a\nmilestone with the world's first accessible quantum computer. This article\nexplores IBM's quantum computing journey, focusing on the development of\npractical quantum computers. We summarize the evolution and advancements of IBM\nQuantum's processors across generations, including their recent breakthrough\nsurpassing the 1,000-qubit barrier. The paper reviews detailed performance\nmetrics across various hardware, tracing their evolution over time and\nhighlighting IBM Quantum's transition from the noisy intermediate-scale quantum\n(NISQ) computing era towards fault-tolerant quantum computing capabilities.",
      "tldr_zh": "这篇论文回顾了 IBM Quantum 计算机的发展历程、性能指标以及未来方向，强调其在量子计算领域的变革性贡献。IBM Quantum 自 2016 年起通过 IBM Cloud 提供可访问的量子硬件，实现了从首个可访问量子计算机到超越 1,000 量子比特的重大突破。文章总结了各代处理器演变和详细性能指标，展示了从 noisy intermediate-scale quantum (NISQ) 时代向 fault-tolerant quantum computing 能力的过渡。这些进展为量子计算的实际应用和未来创新奠定了基础。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00916v1",
      "published_date": "2024-09-17 07:50:50 UTC",
      "updated_date": "2024-09-17 07:50:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:56:23.101401"
    },
    {
      "arxiv_id": "2409.10956v1",
      "title": "Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Min-Yeong Park",
        "Jae-Ho Lee",
        "Gyeong-Moon Park"
      ],
      "abstract": "Incremental Learning (IL) aims to accumulate knowledge from sequential input\ntasks while overcoming catastrophic forgetting. Existing IL methods typically\nassume that an incoming task has only increments of classes or domains,\nreferred to as Class IL (CIL) or Domain IL (DIL), respectively. In this work,\nwe consider a more challenging and realistic but under-explored IL scenario,\nnamed Versatile Incremental Learning (VIL), in which a model has no prior of\nwhich of the classes or domains will increase in the next task. In the proposed\nVIL scenario, the model faces intra-class domain confusion and inter-domain\nclass confusion, which makes the model fail to accumulate new knowledge without\ninterference with learned knowledge. To address these issues, we propose a\nsimple yet effective IL framework, named Incremental Classifier with Adaptation\nShift cONtrol (ICON). Based on shifts of learnable modules, we design a novel\nregularization method called Cluster-based Adaptation Shift conTrol (CAST) to\ncontrol the model to avoid confusion with the previously learned knowledge and\nthereby accumulate the new knowledge more effectively. Moreover, we introduce\nan Incremental Classifier (IC) which expands its output nodes to address the\noverwriting issue from different domains corresponding to a single class while\nmaintaining the previous knowledge. We conducted extensive experiments on three\nbenchmarks, showcasing the effectiveness of our method across all the\nscenarios, particularly in cases where the next task can be randomly altered.\nOur implementation code is available at https://github.com/KHU-AGI/VIL.",
      "tldr_zh": "本文提出了一种更具挑战性的增量学习（Incremental Learning, IL）场景，名为 Versatile Incremental Learning (VIL)，模型需处理未知类或域增量，导致 intra-class domain confusion 和 inter-domain class confusion，从而难以避免灾难性遗忘。针对这些问题，作者设计了 Incremental Classifier with Adaptation Shift cONtrol (ICON) 框架，包括 Cluster-based Adaptation Shift conTrol (CAST) 方法，用于通过控制可学习模块的移位避免知识混淆，以及 Incremental Classifier (IC) 来扩展输出节点以维护既有知识并处理多域问题。在三个基准数据集上的实验验证了 ICON 的有效性，尤其在任务随机变化场景下，展示了显著的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 6 figures, 6 tables, ECCV 2024 Poster",
      "pdf_url": "http://arxiv.org/pdf/2409.10956v1",
      "published_date": "2024-09-17 07:44:28 UTC",
      "updated_date": "2024-09-17 07:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:56:35.842496"
    },
    {
      "arxiv_id": "2409.10955v1",
      "title": "Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style",
      "title_zh": "探究大语言模型中的上下文忠实性：记忆强度和证据风格的作用",
      "authors": [
        "Yuepei Li",
        "Kang Zhou",
        "Qiao Qiao",
        "Bach Nguyen",
        "Qing Wang",
        "Qi Li"
      ],
      "abstract": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs'\ncontext-faithfulness remain largely unexplored. In this study, we investigate\nthe impact of memory strength and evidence presentation on LLMs' receptiveness\nto external evidence. We introduce a method to quantify the memory strength of\nLLMs by measuring the divergence in LLMs' responses to different paraphrases of\nthe same question, which is not considered by previous works. We also generate\nevidence in various styles to evaluate the effects of evidence in different\nstyles. Two datasets are used for evaluation: Natural Questions (NQ) with\npopular questions and popQA featuring long-tail questions. Our results show\nthat for questions with high memory strength, LLMs are more likely to rely on\ninternal memory, particularly for larger LLMs such as GPT-4. On the other hand,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details.",
      "tldr_zh": "本研究探讨了Retrieval-augmented generation (RAG)对Large Language Models (LLMs)的上下文忠实性(context-faithfulness)影响，重点考察记忆强度(memory strength)和证据风格(evidence style)的作用。研究者引入了一种新方法，通过测量LLMs对同一问题不同改写版本的响应差异来量化记忆强度，并生成多种证据风格进行评估，使用Natural Questions (NQ)和popQA数据集进行实验。结果表明，对于高记忆强度的疑问，LLMs更倾向于依赖内部记忆，尤其是大型模型如GPT-4，而使用改写证据比简单重复或添加细节能显著提高LLMs对外部证据的接受度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10955v1",
      "published_date": "2024-09-17 07:44:06 UTC",
      "updated_date": "2024-09-17 07:44:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:56:48.190928"
    },
    {
      "arxiv_id": "2409.10944v1",
      "title": "Contrasformer: A Brain Network Contrastive Transformer for Neurodegenerative Condition Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxing Xu",
        "Kai He",
        "Mengcheng Lan",
        "Qingtian Bian",
        "Wei Li",
        "Tieying Li",
        "Yiping Ke",
        "Miao Qiao"
      ],
      "abstract": "Understanding neurological disorder is a fundamental problem in neuroscience,\nwhich often requires the analysis of brain networks derived from functional\nmagnetic resonance imaging (fMRI) data. Despite the prevalence of Graph Neural\nNetworks (GNNs) and Graph Transformers in various domains, applying them to\nbrain networks faces challenges. Specifically, the datasets are severely\nimpacted by the noises caused by distribution shifts across sub-populations and\nthe neglect of node identities, both obstruct the identification of\ndisease-specific patterns. To tackle these challenges, we propose\nContrasformer, a novel contrastive brain network Transformer. It generates a\nprior-knowledge-enhanced contrast graph to address the distribution shifts\nacross sub-populations by a two-stream attention mechanism. A cross attention\nwith identity embedding highlights the identity of nodes, and three auxiliary\nlosses ensure group consistency. Evaluated on 4 functional brain network\ndatasets over 4 different diseases, Contrasformer outperforms the\nstate-of-the-art methods for brain networks by achieving up to 10.8\\%\nimprovement in accuracy, which demonstrates its efficacy in neurological\ndisorder identification. Case studies illustrate its interpretability,\nespecially in the context of neuroscience. This paper provides a solution for\nanalyzing brain networks, offering valuable insights into neurological\ndisorders. Our code is available at\n\\url{https://github.com/AngusMonroe/Contrasformer}.",
      "tldr_zh": "本研究针对神经退行性疾病识别中的脑网络分析问题，提出了一种新型对比脑网络 Transformer 模型 Contrasformer，以解决 Graph Neural Networks (GNNs) 和 Graph Transformers 在处理 fMRI 数据时面临的分布偏移噪声和节点身份忽略挑战。该模型通过生成基于先验知识的对比图、双流注意力机制以及带身份嵌入的交叉注意力机制，并结合三个辅助损失来确保组一致性，从而提升疾病特定模式的识别能力。在 4 个功能脑网络数据集上评估，Contrasformer 比现有最先进方法提高了高达 10.8% 的准确率，并展示了其在神经科学中的可解释性。该方法为脑网络分析提供了一个有效的解决方案，助力神经退行性疾病的深入理解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10944v1",
      "published_date": "2024-09-17 07:26:02 UTC",
      "updated_date": "2024-09-17 07:26:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:57:02.752159"
    },
    {
      "arxiv_id": "2409.10932v2",
      "title": "Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Mehroush Banday",
        "Sherin Zafar",
        "Parul Agarwal",
        "M Afshar Alam",
        "Abubeker K M"
      ],
      "abstract": "Coronary heart disease (CHD) is a severe cardiac disease, and hence, its\nearly diagnosis is essential as it improves treatment results and saves money\non medical care. The prevailing development of quantum computing and machine\nlearning (ML) technologies may bring practical improvement to the performance\nof CHD diagnosis. Quantum machine learning (QML) is receiving tremendous\ninterest in various disciplines due to its higher performance and capabilities.\nA quantum leap in the healthcare industry will increase processing power and\noptimise multiple models. Techniques for QML have the potential to forecast\ncardiac disease and help in early detection. To predict the risk of coronary\nheart disease, a hybrid approach utilizing an ensemble machine learning model\nbased on QML classifiers is presented in this paper. Our approach, with its\nunique ability to address multidimensional healthcare data, reassures the\nmethod's robustness by fusing quantum and classical ML algorithms in a\nmulti-step inferential framework. The marked rise in heart disease and death\nrates impacts worldwide human health and the global economy. Reducing cardiac\nmorbidity and mortality requires early detection of heart disease. In this\nresearch, a hybrid approach utilizes techniques with quantum computing\ncapabilities to tackle complex problems that are not amenable to conventional\nmachine learning algorithms and to minimize computational expenses. The\nproposed method has been developed in the Raspberry Pi 5 Graphics Processing\nUnit (GPU) platform and tested on a broad dataset that integrates clinical and\nimaging data from patients suffering from CHD and healthy controls. Compared to\nclassical machine learning models, the accuracy, sensitivity, F1 score, and\nspecificity of the proposed hybrid QML model used with CHD are manifold higher.",
      "tldr_zh": "这篇论文提出了一种混合量子机器学习（QML）方法，用于冠心病（Coronary Heart Disease, CHD）的早期检测，以改善诊断性能并降低医疗成本。方法结合了集成机器学习模型（ensemble machine learning model）和量子计算能力，通过多步推理框架处理多维医疗数据，并在 Raspberry Pi 5 GPU 平台上测试。实验结果显示，与经典机器学习模型相比，该混合 QML 模型在准确率、敏感性、F1 分数和特异性等方面显著提升，为CHD早期诊断提供了更高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "I found a mistake in methodology presentation. Also I have observed\n  more precised results with new dataset. So my research guide ask me to modify\n  the current version",
      "pdf_url": "http://arxiv.org/pdf/2409.10932v2",
      "published_date": "2024-09-17 07:08:39 UTC",
      "updated_date": "2024-10-01 15:21:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:57:11.043888"
    },
    {
      "arxiv_id": "2409.10921v1",
      "title": "KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Yanbei Jiang",
        "Krista A. Ehinger",
        "Jey Han Lau"
      ],
      "abstract": "Exploring the narratives conveyed by fine-art paintings is a challenge in\nimage captioning, where the goal is to generate descriptions that not only\nprecisely represent the visual content but also offer a in-depth interpretation\nof the artwork's meaning. The task is particularly complex for artwork images\ndue to their diverse interpretations and varied aesthetic principles across\ndifferent artistic schools and styles. In response to this, we present KALE\nKnowledge-Augmented vision-Language model for artwork Elaborations), a novel\napproach that enhances existing vision-language models by integrating artwork\nmetadata as additional knowledge. KALE incorporates the metadata in two ways:\nfirstly as direct textual input, and secondly through a multimodal\nheterogeneous knowledge graph. To optimize the learning of graph\nrepresentations, we introduce a new cross-modal alignment loss that maximizes\nthe similarity between the image and its corresponding metadata. Experimental\nresults demonstrate that KALE achieves strong performance (when evaluated with\nCIDEr, in particular) over existing state-of-the-art work across several\nartwork datasets. Source code of the project is available at\nhttps://github.com/Yanbei-Jiang/Artwork-Interpretation.",
      "tldr_zh": "该论文提出 KALE（Knowledge-Augmented vision-Language model for artwork Elaborations），一个增强的视觉语言模型系统，用于处理艺术品图像描述的挑战，该系统整合艺术品元数据以生成精确且深入解释的描述。KALE 通过两种方式融入元数据：作为直接文本输入，以及通过多模态异构知识图谱（heterogeneous graph）来优化图表示学习，并引入新的跨模态对齐损失（cross-modal alignment loss）来最大化图像与元数据的相似性。实验结果显示，KALE 在多个艺术品数据集上（如 CIDEr 指标）超越现有最先进方法，展示了其在艺术解读中的强大性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10921v1",
      "published_date": "2024-09-17 06:39:18 UTC",
      "updated_date": "2024-09-17 06:39:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:57:22.908179"
    },
    {
      "arxiv_id": "2409.10909v1",
      "title": "GenCRF: Generative Clustering and Reformulation Framework for Enhanced Intent-Driven Information Retrieval",
      "title_zh": "GenCRF：生成式聚类和重述框架，用于增强意图驱动的信息检索",
      "authors": [
        "Wonduk Seo",
        "Haojie Zhang",
        "Yueyang Zhang",
        "Changhao Zhang",
        "Songyao Duan",
        "Lixin Su",
        "Daiting Shi",
        "Jiashu Zhao",
        "Dawei Yin"
      ],
      "abstract": "Query reformulation is a well-known problem in Information Retrieval (IR)\naimed at enhancing single search successful completion rate by automatically\nmodifying user's input query. Recent methods leverage Large Language Models\n(LLMs) to improve query reformulation, but often generate limited and redundant\nexpansions, potentially constraining their effectiveness in capturing diverse\nintents. In this paper, we propose GenCRF: a Generative Clustering and\nReformulation Framework to capture diverse intentions adaptively based on\nmultiple differentiated, well-generated queries in the retrieval phase for the\nfirst time. GenCRF leverages LLMs to generate variable queries from the initial\nquery using customized prompts, then clusters them into groups to distinctly\nrepresent diverse intents. Furthermore, the framework explores to combine\ndiverse intents query with innovative weighted aggregation strategies to\noptimize retrieval performance and crucially integrates a novel Query\nEvaluation Rewarding Model (QERM) to refine the process through feedback loops.\nEmpirical experiments on the BEIR benchmark demonstrate that GenCRF achieves\nstate-of-the-art performance, surpassing previous query reformulation SOTAs by\nup to 12% on nDCG@10. These techniques can be adapted to various LLMs,\nsignificantly boosting retriever performance and advancing the field of\nInformation Retrieval.",
      "tldr_zh": "本文提出 GenCRF 框架，这是一个生成式聚类和重构系统，旨在通过捕捉多样意图来提升信息检索（Information Retrieval）的查询重构（Query reformulation）效果。框架利用 Large Language Models (LLMs) 生成基于初始查询的变异查询，进行聚类以代表不同意图，并结合创新的加权聚合策略和 Query Evaluation Rewarding Model (QERM) 通过反馈循环优化检索性能。在 BEIR 基准测试中，GenCRF 比现有最先进方法提高了高达 12% 的 nDCG@10 指标，推动了信息检索领域的进展。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10909v1",
      "published_date": "2024-09-17 05:59:32 UTC",
      "updated_date": "2024-09-17 05:59:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:57:36.011108"
    },
    {
      "arxiv_id": "2410.01827v1",
      "title": "Analysis of Convolutional Neural Network-based Image Classifications: A Multi-Featured Application for Rice Leaf Disease Prediction and Recommendations for Farmers",
      "title_zh": "翻译失败",
      "authors": [
        "Biplov Paneru",
        "Bishwash Paneru",
        "Krishna Bikram Shah"
      ],
      "abstract": "This study presents a novel method for improving rice disease classification\nusing 8 different convolutional neural network (CNN) algorithms, which will\nfurther the field of precision agriculture. Tkinter-based application that\noffers farmers a feature-rich interface. With the help of this cutting-edge\napplication, farmers will be able to make timely and well-informed decisions by\nenabling real-time disease prediction and providing personalized\nrecommendations. Together with the user-friendly Tkinter interface, the smooth\nintegration of cutting-edge CNN transfer learning algorithms-based technology\nthat include ResNet-50, InceptionV3, VGG16, and MobileNetv2 with the UCI\ndataset represents a major advancement toward modernizing agricultural\npractices and guaranteeing sustainable crop management. Remarkable outcomes\ninclude 75% accuracy for ResNet-50, 90% accuracy for DenseNet121, 84% accuracy\nfor VGG16, 95.83% accuracy for MobileNetV2, 91.61% accuracy for DenseNet169,\nand 86% accuracy for InceptionV3. These results give a concise summary of the\nmodels' capabilities, assisting researchers in choosing appropriate strategies\nfor precise and successful rice crop disease identification. A severe\noverfitting has been seen on VGG19 with 70% accuracy and Nasnet with 80.02%\naccuracy. On Renset101, only an accuracy of 54% could be achieved, along with\nonly 33% on efficientNetB0. A MobileNetV2-trained model was successfully\ndeployed on a TKinter GUI application to make predictions using image or\nreal-time video capture.",
      "tldr_zh": "这篇论文分析了多种卷积神经网络 (CNN) 算法（如 ResNet-50、InceptionV3、VGG16 和 MobileNetV2）用于水稻叶病害预测的方法，旨在推进精准农业。研究使用 UCI 数据集和迁移学习技术，开发了一个基于 Tkinter 的用户友好应用，支持实时图像或视频预测，并提供个性化推荐给农民。实验结果显示 MobileNetV2 达到了 95.83% 的最高准确率，而其他模型如 DenseNet121 为 90%、VGG16 为 84%，但部分模型（如 VGG19 和 efficientNetB0）存在过拟合或较低准确率问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.01827v1",
      "published_date": "2024-09-17 05:32:01 UTC",
      "updated_date": "2024-09-17 05:32:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:57:48.791783"
    },
    {
      "arxiv_id": "2409.18988v1",
      "title": "A Unified Framework to Classify Business Activities into International Standard Industrial Classification through Large Language Models for Circular Economy",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Li",
        "Lan Zhao",
        "Junhao Ren",
        "Yajuan Sun",
        "Chuan Fu Tan",
        "Zhiquan Yeo",
        "Gaoxi Xiao"
      ],
      "abstract": "Effective information gathering and knowledge codification are pivotal for\ndeveloping recommendation systems that promote circular economy practices. One\npromising approach involves the creation of a centralized knowledge repository\ncataloguing historical waste-to-resource transactions, which subsequently\nenables the generation of recommendations based on past successes. However, a\nsignificant barrier to constructing such a knowledge repository lies in the\nabsence of a universally standardized framework for representing business\nactivities across disparate geographical regions. To address this challenge,\nthis paper leverages Large Language Models (LLMs) to classify textual data\ndescribing economic activities into the International Standard Industrial\nClassification (ISIC), a globally recognized economic activity classification\nframework. This approach enables any economic activity descriptions provided by\nbusinesses worldwide to be categorized into the unified ISIC standard,\nfacilitating the creation of a centralized knowledge repository. Our approach\nachieves a 95% accuracy rate on a 182-label test dataset with fine-tuned GPT-2\nmodel. This research contributes to the global endeavour of fostering\nsustainable circular economy practices by providing a standardized foundation\nfor knowledge codification and recommendation systems deployable across\nregions.",
      "tldr_zh": "该论文提出了一种统一框架，利用 Large Language Models (LLMs) 将全球商业活动描述分类到 International Standard Industrial Classification (ISIC)，以解决构建循环经济 (Circular Economy) 知识库的标准化挑战。该框架通过微调 GPT-2 模型处理文本数据，在一个包含 182 个标签的测试数据集上实现了 95% 的准确率。这种方法有助于创建中心化知识仓库，促进基于历史交易的推荐系统开发，并为全球可持续循环经济实践提供跨地区部署的基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 2 figures, accepted in 2024 IEEE International Conference on\n  Industrial Engineering and Engineering Management (IEEM 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.18988v1",
      "published_date": "2024-09-17 05:30:08 UTC",
      "updated_date": "2024-09-17 05:30:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:58:10.192902"
    },
    {
      "arxiv_id": "2409.10898v2",
      "title": "LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction with LIME Explainable AI and a RAG-based Chatbot for Insights",
      "title_zh": "翻译失败",
      "authors": [
        "Biplov Paneru",
        "Bishwash Paneru"
      ],
      "abstract": "Ensuring safe water supplies requires effective water quality monitoring,\nespecially in developing countries like Nepal, where contamination risks are\nhigh. This paper introduces a hybrid deep learning model to predict Nepal's\nseasonal water quality using a small dataset with multiple water quality\nparameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along\nwith a neural network combining CNN and RNN layers, are used to capture\ntemporal and spatial patterns in the data. The model demonstrated notable\naccuracy improvements, aiding proactive water quality control. CatBoost,\nXGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values\nwith an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers\nachieved 99 percent accuracy, cross-validated across models. LIME analysis\nhighlighted the importance of indicators like EC and DO levels in XGBoost\nclassification decisions. The neural network model achieved 92 percent\nclassification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in\nregression analysis. Furthermore, a multifunctional application was developed\nto predict WQI values using both regression and classification methods.",
      "tldr_zh": "这篇论文提出了一种混合深度学习模型，用于预测尼泊尔季节性水质，结合CatBoost、XGBoost、Extra Trees和LightGBM等机器学习算法，以及CNN-RNN神经网络，以捕捉数据中的时间和空间模式。模型在Water Quality Index (WQI)预测中实现了平均RMSE为1.2和R2得分为0.99的出色性能，分类准确率高达99%。通过LIME可解释AI分析，突出了EC和DO水平等关键指标的重要性，并开发了一个多功能应用和基于RAG的聊天机器人，提供水质洞见和决策支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10898v2",
      "published_date": "2024-09-17 05:26:59 UTC",
      "updated_date": "2025-01-30 15:47:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:58:12.616015"
    },
    {
      "arxiv_id": "2410.02768v2",
      "title": "Uncertainty-Guided Self-Questioning and Answering for Video-Language Alignment",
      "title_zh": "不确定性引导",
      "authors": [
        "Jin Chen",
        "Kaijing Ma",
        "Haojian Huang",
        "Han Fang",
        "Hao Sun",
        "Mehdi Hosseinzadeh",
        "Zhe Liu"
      ],
      "abstract": "The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available.",
      "tldr_zh": "该论文提出了一种引导式自问答框架BoViLA，用于提升视频-语言对齐，通过LLM-based self-questioning and answering生成额外问题样本，帮助模型更全面地利用视频信息和LLM内部知识。针对自生成问题可能带来的低质量风险，引入Evidential Deep Learning (EDL)来估计不确定性，并通过评估模态对齐质量过滤无效样本，这是首个探索此类自训练框架的工作。实验结果显示，BoViLA在五个VideoQA基准上超越了现有最先进方法，证明了其有效性和通用性，并提供了详细分析以支持框架的实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.02768v2",
      "published_date": "2024-09-17 05:17:37 UTC",
      "updated_date": "2025-05-06 09:02:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:58:23.326086"
    },
    {
      "arxiv_id": "2409.10889v1",
      "title": "Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixin Xie",
        "Jun Luo"
      ],
      "abstract": "Real-time deepfake, a type of generative AI, is capable of \"creating\"\nnon-existing contents (e.g., swapping one's face with another) in a video. It\nhas been, very unfortunately, misused to produce deepfake videos (during web\nconferences, video calls, and identity authentication) for malicious purposes,\nincluding financial scams and political misinformation. Deepfake detection, as\nthe countermeasure against deepfake, has attracted considerable attention from\nthe academic community, yet existing works typically rely on learning passive\nfeatures that may perform poorly beyond seen datasets. In this paper, we\npropose SFake, a new real-time deepfake detection method that innovatively\nexploits deepfake models' inability to adapt to physical interference.\nSpecifically, SFake actively sends probes to trigger mechanical vibrations on\nthe smartphone, resulting in the controllable feature on the footage.\nConsequently, SFake determines whether the face is swapped by deepfake based on\nthe consistency of the facial area with the probe pattern. We implement SFake,\nevaluate its effectiveness on a self-built dataset, and compare it with six\nother detection methods. The results show that SFake outperforms other\ndetection methods with higher detection accuracy, faster process speed, and\nlower memory consumption.",
      "tldr_zh": "本文提出 SFake，一种实时 deepfake 视频检测方法，利用 deepfake 模型无法适应物理干扰的弱点，通过 active probes 向智能手机发送探针触发机械振动，生成可控特征并基于面部区域与探针模式的一致性判断是否为伪造。相比现有依赖被动特征的方法，SFake 创新性地提升了检测鲁棒性。实验结果显示，在自建数据集上，SFake 比其他六种检测方法实现了更高的准确率、更快的处理速度和更低的内存消耗。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10889v1",
      "published_date": "2024-09-17 04:58:30 UTC",
      "updated_date": "2024-09-17 04:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:58:35.720286"
    },
    {
      "arxiv_id": "2409.12244v1",
      "title": "Sparks of Artificial General Intelligence(AGI) in Semiconductor Material Science: Early Explorations into the Next Frontier of Generative AI-Assisted Electron Micrograph Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Sakhinana Sagar Srinivas",
        "Geethan Sannidhi",
        "Sreeja Gangasani",
        "Chidaksh Ravuru",
        "Venkataramana Runkana"
      ],
      "abstract": "Characterizing materials with electron micrographs poses significant\nchallenges for automated labeling due to the complex nature of nanomaterial\nstructures. To address this, we introduce a fully automated, end-to-end\npipeline that leverages recent advances in Generative AI. It is designed for\nanalyzing and understanding the microstructures of semiconductor materials with\neffectiveness comparable to that of human experts, contributing to the pursuit\nof Artificial General Intelligence (AGI) in nanomaterial identification. Our\napproach utilizes Large MultiModal Models (LMMs) such as GPT-4V, alongside\ntext-to-image models like DALLE-3. We integrate a GPT-4 guided Visual Question\nAnswering (VQA) method to analyze nanomaterial images, generate synthetic\nnanomaterial images via DALLE-3, and employ in-context learning with few-shot\nprompting in GPT-4V for accurate nanomaterial identification. Our method\nsurpasses traditional techniques by enhancing the precision of nanomaterial\nidentification and optimizing the process for high-throughput screening.",
      "tldr_zh": "本论文探讨了在半导体材料科学中应用 AGI（Artificial General Intelligence）的初步探索，提出了一种基于生成式 AI 的全自动端到端管道，用于电子显微镜图像分析，以实现与人类专家相当的纳米材料微结构表征。方法整合了 LMMs（如 GPT-4V）和文本到图像模型（如 DALLE-3），通过 GPT-4 指导的 VQA（Visual Question Answering）分析图像、生成合成图像，并采用 in-context learning 与 few-shot prompting 来提升纳米材料识别准确性。该管道超越了传统技术，提高了识别精度并优化了高通量筛选过程，为 AGI 在纳米材料识别领域的推进提供了重要贡献。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at Deployable AI (DAI) Workshop at AAAI-2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12244v1",
      "published_date": "2024-09-17 04:25:27 UTC",
      "updated_date": "2024-09-17 04:25:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:58:48.769299"
    },
    {
      "arxiv_id": "2409.10870v1",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "title_zh": "通过层级注意力捷径的适应性大型语言模型",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "abstract": "Transformer architectures are the backbone of the modern AI revolution.\nHowever, they are based on simply stacking the same blocks in dozens of layers\nand processing information sequentially from one block to another. In this\npaper, we propose to challenge this and introduce adaptive computations for\nLLM-like setups, which allow the final layer to attend to all of the\nintermediate layers as it deems fit through the attention mechanism, thereby\nintroducing computational \\textbf{attention shortcuts}. These shortcuts can\nthus make the architecture depth and context adaptive. We showcase four\ndifferent datasets, namely acoustic tokens, natural language, and symbolic\nmusic, and we achieve superior performance for GPT-like architecture. We give\nevidence via attention maps that the models learn complex dependencies across\nlayers that are adaptive in context and depth depending on the input tokens.",
      "tldr_zh": "本研究挑战了传统 Transformer 架构的层级堆叠方式，提出了一种通过层级注意力 shortcuts 实现的自适应大型语言模型（LLM-like setups），允许最终层根据需要访问中间层，从而使模型深度和上下文动态调整。实验在四个数据集上，包括 acoustic tokens、自然语言和 symbolic music，展示了该模型比 GPT-like 架构具有优越性能。注意力图分析进一步证明，模型学会了根据输入 token 的复杂层间依赖，实现自适应计算。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.10870v1",
      "published_date": "2024-09-17 03:46:01 UTC",
      "updated_date": "2024-09-17 03:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:58:58.511997"
    },
    {
      "arxiv_id": "2409.12165v1",
      "title": "NSSR-DIL: Null-Shot Image Super-Resolution Using Deep Identity Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sree Rama Vamsidhar S",
        "Rama Krishna Gorthi"
      ],
      "abstract": "The present State-of-the-Art (SotA) Image Super-Resolution (ISR) methods\nemploy Deep Learning (DL) techniques using a large amount of image data. The\nprimary limitation to extending the existing SotA ISR works for real-world\ninstances is their computational and time complexities. In this paper, contrary\nto the existing methods, we present a novel and computationally efficient ISR\nalgorithm that is independent of the image dataset to learn the ISR task. The\nproposed algorithm reformulates the ISR task from generating the Super-Resolved\n(SR) images to computing the inverse of the kernels that span the degradation\nspace. We introduce Deep Identity Learning, exploiting the identity relation\nbetween the degradation and inverse degradation models. The proposed approach\nneither relies on the ISR dataset nor on a single input low-resolution (LR)\nimage (like the self-supervised method i.e. ZSSR) to model the ISR task. Hence\nwe term our model as Null-Shot Super-Resolution Using Deep Identity Learning\n(NSSR-DIL). The proposed NSSR-DIL model requires fewer computational resources,\nat least by an order of 10, and demonstrates a competitive performance on\nbenchmark ISR datasets. Another salient aspect of our proposition is that the\nNSSR-DIL framework detours retraining the model and remains the same for\nvarying scale factors like X2, X3, and X4. This makes our highly efficient ISR\nmodel more suitable for real-world applications.",
      "tldr_zh": "本论文提出了一种名为 NSSR-DIL 的图像超分辨率 (ISR) 方法，使用 Deep Identity Learning 实现无数据集依赖的超分辨率任务，解决了现有深度学习方法在计算和时间复杂度上的局限性。该方法通过计算退化空间的逆内核，并利用退化和逆退化模型之间的身份关系，重构 ISR 任务，而无需依赖图像数据集或单个低分辨率 (LR) 图像。相比基准方法，NSSR-DIL 计算资源减少至少 10 倍，支持不同缩放因子（如 X2、X3 和 X4）而无需重新训练，并在基准 ISR 数据集上表现出竞争性性能，使其更适用于真实世界应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12165v1",
      "published_date": "2024-09-17 03:43:07 UTC",
      "updated_date": "2024-09-17 03:43:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:59:11.597937"
    },
    {
      "arxiv_id": "2409.11445v2",
      "title": "Jailbreaking Large Language Models with Symbolic Mathematics",
      "title_zh": "翻译失败",
      "authors": [
        "Emet Bethany",
        "Mazal Bethany",
        "Juan Arturo Nolazco Flores",
        "Sumit Kumar Jha",
        "Peyman Najafirad"
      ],
      "abstract": "Recent advancements in AI safety have led to increased efforts in training\nand red-teaming large language models (LLMs) to mitigate unsafe content\ngeneration. However, these safety mechanisms may not be comprehensive, leaving\npotential vulnerabilities unexplored. This paper introduces MathPrompt, a novel\njailbreaking technique that exploits LLMs' advanced capabilities in symbolic\nmathematics to bypass their safety mechanisms. By encoding harmful natural\nlanguage prompts into mathematical problems, we demonstrate a critical\nvulnerability in current AI safety measures. Our experiments across 13\nstate-of-the-art LLMs reveal an average attack success rate of 73.6\\%,\nhighlighting the inability of existing safety training mechanisms to generalize\nto mathematically encoded inputs. Analysis of embedding vectors shows a\nsubstantial semantic shift between original and encoded prompts, helping\nexplain the attack's success. This work emphasizes the importance of a holistic\napproach to AI safety, calling for expanded red-teaming efforts to develop\nrobust safeguards across all potential input types and their associated risks.",
      "tldr_zh": "这篇论文引入了 MathPrompt，一种新型越狱技术，利用大型语言模型 (LLMs) 在符号数学方面的能力来绕过其安全机制。研究者通过将有害自然语言提示编码成数学问题，暴露了当前 AI 安全措施的漏洞。在 13 个最先进的 LLMs 上进行的实验显示，攻击成功率平均为 73.6%，并通过嵌入向量分析揭示了原提示与编码提示之间的语义变化。该工作呼吁采用更全面的 red-teaming 策略，以开发针对各种输入类型的稳健安全保障。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11445v2",
      "published_date": "2024-09-17 03:39:45 UTC",
      "updated_date": "2024-11-05 08:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:59:23.575847"
    },
    {
      "arxiv_id": "2409.10849v1",
      "title": "SIFToM: Robust Spoken Instruction Following through Theory of Mind",
      "title_zh": "翻译失败",
      "authors": [
        "Lance Ying",
        "Jason Xinyu Liu",
        "Shivam Aarya",
        "Yizirui Fang",
        "Stefanie Tellex",
        "Joshua B. Tenenbaum",
        "Tianmin Shu"
      ],
      "abstract": "Spoken language instructions are ubiquitous in agent collaboration. However,\nin human-robot collaboration, recognition accuracy for human speech is often\ninfluenced by various speech and environmental factors, such as background\nnoise, the speaker's accents, and mispronunciation. When faced with noisy or\nunfamiliar auditory inputs, humans use context and prior knowledge to\ndisambiguate the stimulus and take pragmatic actions, a process referred to as\ntop-down processing in cognitive science. We present a cognitively inspired\nmodel, Speech Instruction Following through Theory of Mind (SIFToM), to enable\nrobots to pragmatically follow human instructions under diverse speech\nconditions by inferring the human's goal and joint plan as prior for speech\nperception and understanding. We test SIFToM in simulated home experiments\n(VirtualHome 2). Results show that the SIFToM model outperforms\nstate-of-the-art speech and language models, approaching human-level accuracy\non challenging speech instruction following tasks. We then demonstrate its\nability at the task planning level on a mobile manipulator for breakfast\npreparation tasks.",
      "tldr_zh": "该论文提出 SIFToM 模型，一种受认知科学启发的系统，利用 Theory of Mind 来推断人类的目標和联合计划，作为语音感知的先验，从而使机器人能够在背景噪音、口音等复杂条件下鲁棒地遵循语音指令。实验结果显示，在 VirtualHome 2 模拟家庭环境中，SIFToM 超过了最先进的语音和语言模型，准确性接近人类水平。进一步演示表明，该模型在移动机械臂的早餐准备任务中表现出色，提升了人机协作的实用性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.10849v1",
      "published_date": "2024-09-17 02:36:10 UTC",
      "updated_date": "2024-09-17 02:36:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:59:36.648203"
    },
    {
      "arxiv_id": "2409.10848v1",
      "title": "3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanmeng Sha",
        "Liyun Zhang",
        "Tomohiro Mashita",
        "Yuki Uranishi"
      ],
      "abstract": "Audio-driven 3D facial animation has made immersive progress both in research\nand application developments. The newest approaches focus on Transformer-based\nmethods and diffusion-based methods, however, there is still gap in the\nvividness and emotional expression between the generated animation and real\nhuman face. To tackle this limitation, we propose 3DFacePolicy, a diffusion\npolicy model for 3D facial animation prediction. This method generates variable\nand realistic human facial movements by predicting the 3D vertex trajectory on\nthe 3D facial template with diffusion policy instead of facial generation for\nevery frame. It takes audio and vertex states as observations to predict the\nvertex trajectory and imitate real human facial expressions, which keeps the\ncontinuous and natural flow of human emotions. The experiments show that our\napproach is effective in variable and dynamic facial motion synthesizing.",
      "tldr_zh": "该论文提出3DFacePolicy，一种基于diffusion policy的模型，用于音频驱动的3D facial animation，以解决现有Transformer-based和diffusion-based方法在生动性和情感表达上的不足。该模型以音频和顶点状态作为观察，预测3D顶点轨迹，从而生成更真实、变量的人类面部运动，并确保情感的连续性和自然流。实验结果显示，3DFacePolicy在合成动态面部动画方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10848v1",
      "published_date": "2024-09-17 02:30:34 UTC",
      "updated_date": "2024-09-17 02:30:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:59:47.925439"
    },
    {
      "arxiv_id": "2409.18987v1",
      "title": "Efficient and Personalized Mobile Health Event Prediction via Small Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Wang",
        "Ting Dang",
        "Vassilis Kostakos",
        "Hong Jia"
      ],
      "abstract": "Healthcare monitoring is crucial for early detection, timely intervention,\nand the ongoing management of health conditions, ultimately improving\nindividuals' quality of life. Recent research shows that Large Language Models\n(LLMs) have demonstrated impressive performance in supporting healthcare tasks.\nHowever, existing LLM-based healthcare solutions typically rely on cloud-based\nsystems, which raise privacy concerns and increase the risk of personal\ninformation leakage. As a result, there is growing interest in running these\nmodels locally on devices like mobile phones and wearables to protect users'\nprivacy. Small Language Models (SLMs) are potential candidates to solve privacy\nand computational issues, as they are more efficient and better suited for\nlocal deployment. However, the performance of SLMs in healthcare domains has\nnot yet been investigated. This paper examines the capability of SLMs to\naccurately analyze health data, such as steps, calories, sleep minutes, and\nother vital statistics, to assess an individual's health status. Our results\nshow that, TinyLlama, which has 1.1 billion parameters, utilizes 4.31 GB\nmemory, and has 0.48s latency, showing the best performance compared other four\nstate-of-the-art (SOTA) SLMs on various healthcare applications. Our results\nindicate that SLMs could potentially be deployed on wearable or mobile devices\nfor real-time health monitoring, providing a practical solution for efficient\nand privacy-preserving healthcare.",
      "tldr_zh": "该论文探讨了使用 Small Language Models (SLMs) 来实现高效且个性化的移动健康事件预测，旨在解决 Large Language Models (LLMs) 在隐私泄露和计算资源方面的局限性。研究通过测试 SLMs 在分析健康数据（如步数、卡路里和睡眠分钟）上的能力，结果显示 TinyLlama（1.1 亿参数、4.31 GB 内存、0.48s 延迟）在各种健康应用中比其他 SOTA SLMs 表现出色。最终，论文证明 SLMs 可部署在可穿戴设备或手机上，提供实时、隐私保护的健康监测解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.18987v1",
      "published_date": "2024-09-17 01:57:57 UTC",
      "updated_date": "2024-09-17 01:57:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:59:59.902173"
    },
    {
      "arxiv_id": "2409.10831v2",
      "title": "PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Phillip Long",
        "Zachary Novack",
        "Taylor Berg-Kirkpatrick",
        "Julian McAuley"
      ],
      "abstract": "The recent explosion of generative AI-Music systems has raised numerous\nconcerns over data copyright, licensing music from musicians, and the conflict\nbetween open-source AI and large prestige companies. Such issues highlight the\nneed for publicly available, copyright-free musical data, in which there is a\nlarge shortage, particularly for symbolic music data. To alleviate this issue,\nwe present PDMX: a large-scale open-source dataset of over 250K public domain\nMusicXML scores collected from the score-sharing forum MuseScore, making it the\nlargest available copyright-free symbolic music dataset to our knowledge. PDMX\nadditionally includes a wealth of both tag and user interaction metadata,\nallowing us to efficiently analyze the dataset and filter for high quality\nuser-generated scores. Given the additional metadata afforded by our data\ncollection process, we conduct multitrack music generation experiments\nevaluating how different representative subsets of PDMX lead to different\nbehaviors in downstream models, and how user-rating statistics can be used as\nan effective measure of data quality. Examples can be found at\nhttps://pnlong.github.io/PDMX.demo/.",
      "tldr_zh": "该研究介绍了PDMX，一种大规模开源数据集，包含超过25万份公开领域的MusicXML乐谱，用于Symbolic Music Processing，以解决生成AI音乐系统中的版权和数据短缺问题。数据集从MuseScore论坛收集，并包括丰富的标签和用户互动元数据，便于分析和过滤高质量乐谱。研究者通过多轨音乐生成实验评估了不同PDMX子集对下游模型的影响，发现用户评分统计可作为有效的数据质量指标，从而提升AI音乐应用的可靠性和公平性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to 2025 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP)",
      "pdf_url": "http://arxiv.org/pdf/2409.10831v2",
      "published_date": "2024-09-17 01:48:42 UTC",
      "updated_date": "2025-03-17 03:08:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:00:11.513944"
    },
    {
      "arxiv_id": "2409.10825v3",
      "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A Path to Fairness",
      "title_zh": "揭示和减轻大型语言模型推荐中的偏见：通往公平的路径",
      "authors": [
        "Anindya Bijoy Das",
        "Shahnewaz Karim Sakib"
      ],
      "abstract": "excel in delivering comprehensive suggestions by deeply analyzing content and\nuser behavior. However, they often inherit biases from skewed training data,\nfavoring mainstream content while underrepresenting diverse or non-traditional\noptions. This study explores the interplay between bias and LLM-based\nrecommendation systems, focusing on music, song, and book recommendations\nacross diverse demographic and cultural groups. This paper analyzes bias in\nLLM-based recommendation systems across multiple models (GPT, LLaMA, and\nGemini), revealing its deep and pervasive impact on outcomes. Intersecting\nidentities and contextual factors, like socioeconomic status, further amplify\nbiases, complicating fair recommendations across diverse groups. Our findings\nreveal that bias in these systems is deeply ingrained, yet even simple\ninterventions like prompt engineering can significantly reduce it. We further\npropose a retrieval-augmented generation strategy to mitigate bias more\neffectively. Numerical experiments validate these strategies, demonstrating\nboth the pervasive nature of bias and the impact of the proposed solutions.",
      "tldr_zh": "该研究揭示了大型语言模型（LLMs）在推荐系统中的偏差问题，这些模型往往从训练数据继承偏见，导致偏好主流内容而忽略多样化选项，如在音乐、歌曲和书籍推荐中。研究分析了GPT、LLaMA和Gemini等多个模型，探讨了交叉身份（如社会经济地位）和文化因素如何放大偏差，并证明了偏差的深远影响。作者提出简单干预如prompt engineering，以及更有效的retrieval-augmented generation strategy来缓解偏差；数值实验验证了这些策略的有效性，展示了减少偏差的可行路径。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10825v3",
      "published_date": "2024-09-17 01:37:57 UTC",
      "updated_date": "2024-12-02 07:00:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:00:24.031831"
    },
    {
      "arxiv_id": "2409.10821v1",
      "title": "PReLU: Yet Another Single-Layer Solution to the XOR Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Rafael C. Pinto",
        "Anderson R. Tavares"
      ],
      "abstract": "This paper demonstrates that a single-layer neural network using Parametric\nRectified Linear Unit (PReLU) activation can solve the XOR problem, a simple\nfact that has been overlooked so far. We compare this solution to the\nmulti-layer perceptron (MLP) and the Growing Cosine Unit (GCU) activation\nfunction and explain why PReLU enables this capability. Our results show that\nthe single-layer PReLU network can achieve 100\\% success rate in a wider range\nof learning rates while using only three learnable parameters.",
      "tldr_zh": "这篇论文证明，使用 Parametric Rectified Linear Unit (PReLU) 激活函数的单层神经网络可以解决传统的 XOR 问题，这是一个之前被忽略的事实。作者将该方案与多层感知器 (MLP) 和 Growing Cosine Unit (GCU) 激活函数进行比较，并解释了 PReLU 如何赋予单层网络这一能力。结果显示，PReLU 网络在更广泛的学习率范围内可实现 100% 成功率，同时仅需三个可学习参数。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10821v1",
      "published_date": "2024-09-17 01:28:40 UTC",
      "updated_date": "2024-09-17 01:28:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:00:35.884620"
    },
    {
      "arxiv_id": "2409.10811v3",
      "title": "Grounded GUI Understanding for Vision Based Spatial Intelligent Agent: Exemplified by Virtual Reality Apps",
      "title_zh": "翻译失败",
      "authors": [
        "Shuqing Li",
        "Binchang Li",
        "Yepang Liu",
        "Cuiyun Gao",
        "Jianping Zhang",
        "Shing-Chi Cheung",
        "Michael R. Lyu"
      ],
      "abstract": "In recent years, spatial computing Virtual Reality (VR) has emerged as a\ntransformative technology, offering users immersive and interactive experiences\nacross diversified virtual environments. Users can interact with VR apps\nthrough interactable GUI elements (IGEs) on the stereoscopic three-dimensional\n(3D) graphical user interface (GUI). The accurate recognition of these IGEs is\ninstrumental, serving as the foundation of many software engineering tasks,\nincluding automated testing and effective GUI search. The most recent IGE\ndetection approaches for 2D mobile apps typically train a supervised object\ndetection model based on a large-scale manually-labeled GUI dataset, usually\nwith a pre-defined set of clickable GUI element categories like buttons and\nspinners. Such approaches can hardly be applied to IGE detection in VR apps,\ndue to a multitude of challenges including complexities posed by\nopen-vocabulary and heterogeneous IGE categories, intricacies of\ncontext-sensitive interactability, and the necessities of precise spatial\nperception and visual-semantic alignment for accurate IGE detection results.\nThus, it is necessary to embark on the IGE research tailored to VR apps. In\nthis paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI\nElemeNT dEtection framework for virtual Reality apps, named Orienter. By\nimitating human behaviors, Orienter observes and understands the semantic\ncontexts of VR app scenes first, before performing the detection. The detection\nprocess is iterated within a feedback-directed validation and reflection loop.\nSpecifically, Orienter contains three components, including (1) Semantic\ncontext comprehension, (2) Reflection-directed IGE candidate detection, and (3)\nContext-sensitive interactability classification. Extensive experiments\ndemonstrate that Orienter is more effective than the state-of-the-art GUI\nelement detection approaches.",
      "tldr_zh": "该论文探讨了基于视觉的空间智能代理（Vision Based Spatial Intelligent Agent）在虚拟现实（VR）应用中理解可交互GUI元素（IGEs）的挑战，包括开放词汇、上下文敏感交互和空间感知需求。作者提出首个零样本（zero-shot）上下文敏感IGE检测框架Orienter，通过模仿人类行为实现语义上下文理解、反射导向的IGE候选检测以及上下文敏感的可交互性分类。实验结果表明，Orienter在VR应用场景中比现有最先进的方法更有效，提升了自动化测试和GUI搜索等任务的性能。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.MM",
        "D.2.5; H.5.1; H.5.2; I.4.8"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10811v3",
      "published_date": "2024-09-17 00:58:00 UTC",
      "updated_date": "2024-10-26 05:38:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:00:48.812996"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 106,
  "processed_papers_count": 106,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T01:01:05.650162"
}
[
  {
    "arxiv_id": "2405.06145v1",
    "title": "Reddit-Impacts: A Named Entity Recognition Dataset for Analyzing Clinical and Social Effects of Substance Use Derived from Social Media",
    "authors": [
      "Yao Ge",
      "Sudeshna Das",
      "Karen O'Connor",
      "Mohammed Ali Al-Garadi",
      "Graciela Gonzalez-Hernandez",
      "Abeed Sarker"
    ],
    "abstract": "Substance use disorders (SUDs) are a growing concern globally, necessitating\nenhanced understanding of the problem and its trends through data-driven\nresearch. Social media are unique and important sources of information about\nSUDs, particularly since the data in such sources are often generated by people\nwith lived experiences. In this paper, we introduce Reddit-Impacts, a\nchallenging Named Entity Recognition (NER) dataset curated from subreddits\ndedicated to discussions on prescription and illicit opioids, as well as\nmedications for opioid use disorder. The dataset specifically concentrates on\nthe lesser-studied, yet critically important, aspects of substance use--its\nclinical and social impacts. We collected data from chosen subreddits using the\npublicly available Application Programming Interface for Reddit. We manually\nannotated text spans representing clinical and social impacts reported by\npeople who also reported personal nonmedical use of substances including but\nnot limited to opioids, stimulants and benzodiazepines. Our objective is to\ncreate a resource that can enable the development of systems that can\nautomatically detect clinical and social impacts of substance use from\ntext-based social media data. The successful development of such systems may\nenable us to better understand how nonmedical use of substances affects\nindividual health and societal dynamics, aiding the development of effective\npublic health strategies. In addition to creating the annotated data set, we\napplied several machine learning models to establish baseline performances.\nSpecifically, we experimented with transformer models like BERT, and RoBERTa,\none few-shot learning model DANN by leveraging the full training dataset, and\nGPT-3.5 by using one-shot learning, for automatic NER of clinical and social\nimpacts. The dataset has been made available through the 2024 SMM4H shared\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 1 figure, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.06145v1",
    "published_date": "2024-05-09 23:43:57 UTC",
    "updated_date": "2024-05-09 23:43:57 UTC"
  },
  {
    "arxiv_id": "2405.06109v1",
    "title": "Scalable Exact Verification of Optimization Proxies for Large-Scale Optimal Power Flow",
    "authors": [
      "Rahul Nellikkath",
      "Mathieu Tanneau",
      "Pascal Van Hentenryck",
      "Spyros Chatzivasileiadis"
    ],
    "abstract": "Optimal Power Flow (OPF) is a valuable tool for power system operators, but\nit is a difficult problem to solve for large systems.\n  Machine Learning (ML) algorithms, especially Neural Networks-based (NN)\noptimization proxies, have emerged as a promising new tool for solving OPF, by\nestimating the OPF solution much faster than traditional methods.\n  However, these ML algorithms act as black boxes, and it is hard to assess\ntheir worst-case performance across the entire range of possible inputs than an\nOPF can have.\n  Previous work has proposed a mixed-integer programming-based methodology to\nquantify the worst-case violations caused by a NN trained to estimate the OPF\nsolution, throughout the entire input domain.\n  This approach, however, does not scale well to large power systems and more\ncomplex NN models.\n  This paper addresses these issues by proposing a scalable algorithm to\ncompute worst-case violations of NN proxies used for approximating large power\nsystems within a reasonable time limit.\n  This will help build trust in ML models to be deployed in large\nindustry-scale power grids.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06109v1",
    "published_date": "2024-05-09 21:30:03 UTC",
    "updated_date": "2024-05-09 21:30:03 UTC"
  },
  {
    "arxiv_id": "2405.06087v2",
    "title": "When combinations of humans and AI are useful: A systematic review and meta-analysis",
    "authors": [
      "Michelle Vaccaro",
      "Abdullah Almaatouq",
      "Thomas Malone"
    ],
    "abstract": "Inspired by the increasing use of AI to augment humans, researchers have\nstudied human-AI systems involving different tasks, systems, and populations.\nDespite such a large body of work, we lack a broad conceptual understanding of\nwhen combinations of humans and AI are better than either alone. Here, we\naddressed this question by conducting a meta-analysis of over 100 recent\nexperimental studies reporting over 300 effect sizes. First, we found that, on\naverage, human-AI combinations performed significantly worse than the best of\nhumans or AI alone. Second, we found performance losses in tasks that involved\nmaking decisions and significantly greater gains in tasks that involved\ncreating content. Finally, when humans outperformed AI alone, we found\nperformance gains in the combination, but when the AI outperformed humans alone\nwe found losses. These findings highlight the heterogeneity of the effects of\nhuman-AI collaboration and point to promising avenues for improving human-AI\nsystems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06087v2",
    "published_date": "2024-05-09 20:23:15 UTC",
    "updated_date": "2024-10-29 14:45:26 UTC"
  },
  {
    "arxiv_id": "2405.12229v2",
    "title": "Multi-task learning for molecular electronic structure approaching coupled-cluster accuracy",
    "authors": [
      "Hao Tang",
      "Brian Xiao",
      "Wenhao He",
      "Pero Subasic",
      "Avetik R. Harutyunyan",
      "Yao Wang",
      "Fang Liu",
      "Haowei Xu",
      "Ju Li"
    ],
    "abstract": "Machine learning (ML) plays an important role in quantum chemistry, providing\nfast-to-evaluate predictive models for various properties of molecules.\nHowever, most existing ML models for molecular electronic properties use\ndensity functional theory (DFT) databases as ground truth in training, and\ntheir prediction accuracy cannot surpass that of DFT. In this work, we\ndeveloped a unified ML method for electronic structures of organic molecules\nusing the gold-standard CCSD(T) calculations as training data. Tested on\nhydrocarbon molecules, our model outperforms DFT with the widely-used hybrid\nand double hybrid functionals in computational costs and prediction accuracy of\nvarious quantum chemical properties. As case studies, we apply the model to\naromatic compounds and semiconducting polymers on both ground state and excited\nstate properties, demonstrating its accuracy and generalization capability to\ncomplex systems that are hard to calculate using CCSD(T)-level methods.",
    "categories": [
      "physics.chem-ph",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CE",
      "physics.comp-ph"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12229v2",
    "published_date": "2024-05-09 19:51:27 UTC",
    "updated_date": "2024-06-24 21:16:36 UTC"
  },
  {
    "arxiv_id": "2405.06064v1",
    "title": "LLMs for XAI: Future Directions for Explaining Explanations",
    "authors": [
      "Alexandra Zytek",
      "Sara Pidò",
      "Kalyan Veeramachaneni"
    ],
    "abstract": "In response to the demand for Explainable Artificial Intelligence (XAI), we\ninvestigate the use of Large Language Models (LLMs) to transform ML\nexplanations into natural, human-readable narratives. Rather than directly\nexplaining ML models using LLMs, we focus on refining explanations computed\nusing existing XAI algorithms. We outline several research directions,\nincluding defining evaluation metrics, prompt design, comparing LLM models,\nexploring further training methods, and integrating external data. Initial\nexperiments and user study suggest that LLMs offer a promising way to enhance\nthe interpretability and usability of XAI.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06064v1",
    "published_date": "2024-05-09 19:17:47 UTC",
    "updated_date": "2024-05-09 19:17:47 UTC"
  },
  {
    "arxiv_id": "2405.06059v1",
    "title": "A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds",
    "authors": [
      "Christopher Z. Cui",
      "Xiangyu Peng",
      "Mark O. Riedl"
    ],
    "abstract": "Open-ended worlds are those in which there are no pre-specified goals or\nenvironmental reward signal. As a consequence, an agent must know how to\nperform a multitude of tasks. However, when a new task is presented to an\nagent, we expect it to be able to reuse some of what it knows from previous\ntasks to rapidly learn that new task. We introduce a novel technique whereby\npolicies for different a priori known tasks are combined into a\nMixture-of-Experts model with an attention mechanism across a mix of frozen and\nunfrozen experts. The model learns when to attend to frozen task-specific\nexperts when appropriate and learns new experts to handle novel situations. We\nwork in an open-ended text-based environment in which the agent is tasked with\nbehaving like different types of character roles and must rapidly learn\nbehaviors associated with new character role types. We show that our agent both\nobtains more rewards in the zero-shot setting, and discovers these rewards with\ngreater sample efficiency in the few-shot learning settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06059v1",
    "published_date": "2024-05-09 19:02:56 UTC",
    "updated_date": "2024-05-09 19:02:56 UTC"
  },
  {
    "arxiv_id": "2405.06058v2",
    "title": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses",
    "authors": [
      "Aadesh Salecha",
      "Molly E. Ireland",
      "Shashanka Subrahmanya",
      "João Sedoc",
      "Lyle H. Ungar",
      "Johannes C. Eichstaedt"
    ],
    "abstract": "As Large Language Models (LLMs) become widely used to model and simulate\nhuman behavior, understanding their biases becomes critical. We developed an\nexperimental framework using Big Five personality surveys and uncovered a\npreviously undetected social desirability bias in a wide range of LLMs. By\nsystematically varying the number of questions LLMs were exposed to, we\ndemonstrate their ability to infer when they are being evaluated. When\npersonality evaluation is inferred, LLMs skew their scores towards the\ndesirable ends of trait dimensions (i.e., increased extraversion, decreased\nneuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,\nClaude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent\nmodels, with GPT-4's survey responses changing by 1.20 (human) standard\ndeviations and Llama 3's by 0.98 standard deviations-very large effects. This\nbias is robust to randomization of question order and paraphrasing.\nReverse-coding all the questions decreases bias levels but does not eliminate\nthem, suggesting that this effect cannot be attributed to acquiescence bias.\nOur findings reveal an emergent social desirability bias and suggest\nconstraints on profiling LLMs with psychometric tests and on using LLMs as\nproxies for human participants.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "3 pages, 2 figures, accepted at PNAS Nexus",
    "pdf_url": "http://arxiv.org/pdf/2405.06058v2",
    "published_date": "2024-05-09 19:02:53 UTC",
    "updated_date": "2024-11-21 21:39:04 UTC"
  },
  {
    "arxiv_id": "2405.06038v1",
    "title": "From Algorithm to Hardware: A Survey on Efficient and Safe Deployment of Deep Neural Networks",
    "authors": [
      "Xue Geng",
      "Zhe Wang",
      "Chunyun Chen",
      "Qing Xu",
      "Kaixin Xu",
      "Chao Jin",
      "Manas Gupta",
      "Xulei Yang",
      "Zhenghua Chen",
      "Mohamed M. Sabry Aly",
      "Jie Lin",
      "Min Wu",
      "Xiaoli Li"
    ],
    "abstract": "Deep neural networks (DNNs) have been widely used in many artificial\nintelligence (AI) tasks. However, deploying them brings significant challenges\ndue to the huge cost of memory, energy, and computation. To address these\nchallenges, researchers have developed various model compression techniques\nsuch as model quantization and model pruning. Recently, there has been a surge\nin research of compression methods to achieve model efficiency while retaining\nthe performance. Furthermore, more and more works focus on customizing the DNN\nhardware accelerators to better leverage the model compression techniques. In\naddition to efficiency, preserving security and privacy is critical for\ndeploying DNNs. However, the vast and diverse body of related works can be\noverwhelming. This inspires us to conduct a comprehensive survey on recent\nresearch toward the goal of high-performance, cost-efficient, and safe\ndeployment of DNNs. Our survey first covers the mainstream model compression\ntechniques such as model quantization, model pruning, knowledge distillation,\nand optimizations of non-linear operations. We then introduce recent advances\nin designing hardware accelerators that can adapt to efficient model\ncompression approaches. Additionally, we discuss how homomorphic encryption can\nbe integrated to secure DNN deployment. Finally, we discuss several issues,\nsuch as hardware evaluation, generalization, and integration of various\ncompression approaches. Overall, we aim to provide a big picture of efficient\nDNNs, from algorithm to hardware accelerators and security perspectives.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This manuscript is the accepted version for TNNLS(IEEE Transactions\n  on Neural Networks and Learning Systems)",
    "pdf_url": "http://arxiv.org/pdf/2405.06038v1",
    "published_date": "2024-05-09 18:17:25 UTC",
    "updated_date": "2024-05-09 18:17:25 UTC"
  },
  {
    "arxiv_id": "2406.07561v1",
    "title": "Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security",
    "authors": [
      "Leroy Jacob Valencia"
    ],
    "abstract": "In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.07561v1",
    "published_date": "2024-05-09 18:15:12 UTC",
    "updated_date": "2024-05-09 18:15:12 UTC"
  },
  {
    "arxiv_id": "2405.05966v4",
    "title": "Natural Language Processing RELIES on Linguistics",
    "authors": [
      "Juri Opitz",
      "Shira Wein",
      "Nathan Schneider"
    ],
    "abstract": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in Computational Linguistics. This is a pre-MIT Press\n  publication version",
    "pdf_url": "http://arxiv.org/pdf/2405.05966v4",
    "published_date": "2024-05-09 17:59:32 UTC",
    "updated_date": "2025-03-10 15:07:49 UTC"
  },
  {
    "arxiv_id": "2405.05959v2",
    "title": "Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask",
    "authors": [
      "Zineb Senane",
      "Lele Cao",
      "Valentin Leonhard Buchner",
      "Yusuke Tashiro",
      "Lei You",
      "Pawel Herman",
      "Mats Nordahl",
      "Ruibo Tu",
      "Vilhelm von Ehrenheim"
    ],
    "abstract": "Time Series Representation Learning (TSRL) focuses on generating informative\nrepresentations for various Time Series (TS) modeling tasks. Traditional\nSelf-Supervised Learning (SSL) methods in TSRL fall into four main categories:\nreconstructive, adversarial, contrastive, and predictive, each with a common\nchallenge of sensitivity to noise and intricate data nuances. Recently,\ndiffusion-based methods have shown advanced generative capabilities. However,\nthey primarily target specific application scenarios like imputation and\nforecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our\nwork, Time Series Diffusion Embedding (TSDE), bridges this gap as the first\ndiffusion-based SSL TSRL approach. TSDE segments TS data into observed and\nmasked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It\napplies a trainable embedding function, featuring dual-orthogonal Transformer\nencoders with a crossover mechanism, to the observed part. We train a reverse\ndiffusion process conditioned on the embeddings, designed to predict noise\nadded to the masked part. Extensive experiments demonstrate TSDE's superiority\nin imputation, interpolation, forecasting, anomaly detection, classification,\nand clustering. We also conduct an ablation study, present embedding\nvisualizations, and compare inference speed, further substantiating TSDE's\nefficiency and validity in learning representations of TS data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "G.3; I.6.5; I.2.4"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a full paper by KDD 2024 Research Track (12 pages as\n  main paper and 11 pages as appendix). Source code available at\n  https://github.com/llcresearch/TSDE",
    "pdf_url": "http://arxiv.org/pdf/2405.05959v2",
    "published_date": "2024-05-09 17:55:16 UTC",
    "updated_date": "2024-06-17 08:54:51 UTC"
  },
  {
    "arxiv_id": "2405.05950v1",
    "title": "Federated Combinatorial Multi-Agent Multi-Armed Bandits",
    "authors": [
      "Fares Fourati",
      "Mohamed-Slim Alouini",
      "Vaneet Aggarwal"
    ],
    "abstract": "This paper introduces a federated learning framework tailored for online\ncombinatorial optimization with bandit feedback. In this setting, agents select\nsubsets of arms, observe noisy rewards for these subsets without accessing\nindividual arm information, and can cooperate and share information at specific\nintervals. Our framework transforms any offline resilient single-agent\n$(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of\n$\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is\nomitted, for some function $\\psi$ and constant $\\beta$, into an online\nmulti-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of\nno more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta}\nT^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$\napproximation error but also ensures sublinear growth with respect to the time\nhorizon $T$ and demonstrates a linear speedup with an increasing number of\ncommunicating agents. Additionally, the algorithm is notably\ncommunication-efficient, requiring only a sublinear number of communication\nrounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi\nT^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been\nsuccessfully applied to online stochastic submodular maximization using various\noffline algorithms, yielding the first results for both single-agent and\nmulti-agent settings and recovering specialized single-agent theoretical\nguarantees. We empirically validate our approach to a stochastic data\nsummarization problem, illustrating the effectiveness of the proposed\nframework, even in single-agent scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM",
      "cs.MA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05950v1",
    "published_date": "2024-05-09 17:40:09 UTC",
    "updated_date": "2024-05-09 17:40:09 UTC"
  },
  {
    "arxiv_id": "2407.03332v1",
    "title": "DDPM-MoCo: Advancing Industrial Surface Defect Generation and Detection with Generative and Contrastive Learning",
    "authors": [
      "Yangfan He",
      "Xinyan Wang",
      "Tianyu Shi"
    ],
    "abstract": "The task of industrial detection based on deep learning often involves\nsolving two problems: (1) obtaining sufficient and effective data samples, (2)\nand using efficient and convenient model training methods. In this paper, we\nintroduce a novel defect-generation method, named DDPM-MoCo, to address these\nissues. Firstly, we utilize the Denoising Diffusion Probabilistic Model (DDPM)\nto generate high-quality defect data samples, overcoming the problem of\ninsufficient sample data for model learning. Furthermore, we utilize the\nunsupervised learning Momentum Contrast model (MoCo) with an enhanced batch\ncontrastive loss function for training the model on unlabeled data, addressing\nthe efficiency and consistency challenges in large-scale negative sample\nencoding during diffusion model training. The experimental results showcase an\nenhanced visual detection method for identifying defects on metal surfaces,\ncovering the entire process, starting from generating unlabeled sample data for\ntraining the diffusion model, to utilizing the same labeled sample data for\ndownstream detection tasks. This study offers valuable practical insights and\napplication potential for visual detection in the metal processing industry.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03332v1",
    "published_date": "2024-05-09 17:17:53 UTC",
    "updated_date": "2024-05-09 17:17:53 UTC"
  },
  {
    "arxiv_id": "2405.05930v2",
    "title": "Trustworthy AI-Generative Content for Intelligent Network Service: Robustness, Security, and Fairness",
    "authors": [
      "Siyuan Li",
      "Xi Lin",
      "Yaju Liu",
      "Xiang Chen",
      "Jianhua Li"
    ],
    "abstract": "AI-generated content (AIGC) models, represented by large language models\n(LLM), have revolutionized content creation. High-speed next-generation\ncommunication technology is an ideal platform for providing powerful AIGC\nnetwork services. At the same time, advanced AIGC techniques can also make\nfuture network services more intelligent, especially various online content\ngeneration services. However, the significant untrustworthiness concerns of\ncurrent AIGC models, such as robustness, security, and fairness, greatly affect\nthe credibility of intelligent network services, especially in ensuring secure\nAIGC services. This paper proposes TrustGAIN, a trustworthy AIGC framework that\nincorporates robust, secure, and fair network services. We first discuss the\nrobustness to adversarial attacks faced by AIGC models in network systems and\nthe corresponding protection issues. Subsequently, we emphasize the importance\nof avoiding unsafe and illegal services and ensuring the fairness of the AIGC\nnetwork services. Then as a case study, we propose a novel sentiment\nanalysis-based detection method to guide the robust detection of unsafe content\nin network services. We conduct our experiments on fake news, malicious code,\nand unsafe review datasets to represent LLM application scenarios. Our results\nindicate that TrustGAIN is an exploration of future networks that can support\ntrustworthy AIGC network services.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05930v2",
    "published_date": "2024-05-09 17:16:20 UTC",
    "updated_date": "2025-02-27 08:09:23 UTC"
  },
  {
    "arxiv_id": "2405.05925v3",
    "title": "FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting",
    "authors": [
      "Xiaohui Zhong",
      "Lei Chen",
      "Hao Li",
      "Jun Liu",
      "Xu Fan",
      "Jie Feng",
      "Kan Dai",
      "Jing-Jia Luo",
      "Jie Wu",
      "Bo Lu"
    ],
    "abstract": "Ensemble forecasting is crucial for improving weather predictions, especially\nfor forecasts of extreme events. Constructing an ensemble prediction system\n(EPS) based on conventional NWP models is highly computationally expensive. ML\nmodels have emerged as valuable tools for deterministic weather forecasts,\nproviding forecasts with significantly reduced computational requirements and\neven surpassing the forecast performance of traditional NWP models. However,\nchallenges arise when applying ML models to ensemble forecasting. Recent ML\nmodels, such as GenCast and SEEDS model, rely on the ERA5 EDA or operational\nNWP ensemble members for forecast generation. Their spatial resolution is also\nconsidered too coarse for many applications. To overcome these limitations, we\nintroduce FuXi-ENS, an advanced ML model designed to deliver 6-hourly global\nensemble weather forecasts up to 15 days. This model runs at a significantly\nincreased spatial resolution of 0.25\\textdegree, incorporating 5 atmospheric\nvariables at 13 pressure levels, along with 13 surface variables. By leveraging\nthe inherent probabilistic nature of Variational AutoEncoder (VAE), FuXi-ENS\noptimizes a loss function that combines the CRPS and the KL divergence between\nthe predicted and target distribution, facilitating the incorporation of\nflow-dependent perturbations in both initial conditions and forecast. This\ninnovative approach makes FuXi-ENS an advancement over the traditional ones\nthat use L1 loss combined with the KL loss in standard VAE models for ensemble\nweather forecasting. Results demonstrate that FuXi-ENS outperforms ensemble\nforecasts from the ECMWF, a world leading NWP model, in the CRPS of 98.1% of\n360 variable and forecast lead time combinations. This achievement underscores\nthe potential of the FuXi-ENS model to enhance ensemble weather forecasts,\noffering a promising direction for further development in this field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05925v3",
    "published_date": "2024-05-09 17:15:09 UTC",
    "updated_date": "2024-08-09 05:14:05 UTC"
  },
  {
    "arxiv_id": "2405.05908v4",
    "title": "Multimodal Super-Resolution: Discovering hidden physics and its application to fusion plasmas",
    "authors": [
      "Azarakhsh Jalalvand",
      "SangKyeun Kim",
      "Jaemin Seo",
      "Qiming Hu",
      "Max Curie",
      "Peter Steiner",
      "Andrew Oakleigh Nelson",
      "Yong-Su Na",
      "Egemen Kolemen"
    ],
    "abstract": "A non-linear system governed by multi-spatial and multi-temporal physics\nscales cannot be fully understood with a single diagnostic, as each provides\nonly a partial view, leading to information loss. Combining multiple\ndiagnostics may also result in incomplete projections of the system's physics.\nBy identifying hidden inter-correlations between diagnostics, we can leverage\nmutual support to fill in these gaps, but uncovering such correlations\nanalytically is too complex. We introduce a machine learning methodology to\naddress this issue. Unlike traditional methods, our multimodal approach does\nnot rely on the target diagnostic's direct measurements to generate its\nsuper-resolution version. Instead, it uses other diagnostics to produce\nsuper-resolution data, capturing detailed structural evolution and responses to\nperturbations previously unobservable. This not only enhances the resolution of\na diagnostic for deeper insights but also reconstructs the target diagnostic,\nproviding a valuable tool to mitigate diagnostic failure. This methodology\naddresses a key challenge in fusion plasmas: the Edge Localized Mode (ELM), a\nplasma instability that can cause significant erosion of plasma-facing\nmaterials. A method to stabilize ELM is using resonant magnetic perturbation\n(RMP) to trigger magnetic islands. However, limited spatial and temporal\nresolution restricts analysis of these islands due to their small size, rapid\ndynamics, and complex plasma interactions. With super-resolution diagnostics,\nwe can experimentally verify theoretical models of magnetic islands for the\nfirst time, providing insights into their role in ELM stabilization. This\nadvancement supports the development of effective ELM suppression strategies\nfor future fusion reactors like ITER and has broader applications, potentially\nrevolutionizing diagnostics in fields such as astronomy, astrophysics, and\nmedical imaging.",
    "categories": [
      "physics.plasm-ph",
      "cs.AI"
    ],
    "primary_category": "physics.plasm-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05908v4",
    "published_date": "2024-05-09 17:06:51 UTC",
    "updated_date": "2024-11-05 18:17:06 UTC"
  },
  {
    "arxiv_id": "2405.05905v5",
    "title": "Truthful Aggregation of LLMs with an Application to Online Advertising",
    "authors": [
      "Ermis Soumalias",
      "Michael J. Curry",
      "Sven Seuken"
    ],
    "abstract": "The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05905v5",
    "published_date": "2024-05-09 17:01:31 UTC",
    "updated_date": "2025-02-12 17:10:53 UTC"
  },
  {
    "arxiv_id": "2405.05890v1",
    "title": "Safe Exploration Using Bayesian World Models and Log-Barrier Optimization",
    "authors": [
      "Yarden As",
      "Bhavya Sukhija",
      "Andreas Krause"
    ],
    "abstract": "A major challenge in deploying reinforcement learning in online tasks is\nensuring that safety is maintained throughout the learning process. In this\nwork, we propose CERL, a new method for solving constrained Markov decision\nprocesses while keeping the policy safe during learning. Our method leverages\nBayesian world models and suggests policies that are pessimistic w.r.t. the\nmodel's epistemic uncertainty. This makes CERL robust towards model\ninaccuracies and leads to safe exploration during learning. In our experiments,\nwe demonstrate that CERL outperforms the current state-of-the-art in terms of\nsafety and optimality in solving CMDPs from image observations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05890v1",
    "published_date": "2024-05-09 16:42:39 UTC",
    "updated_date": "2024-05-09 16:42:39 UTC"
  },
  {
    "arxiv_id": "2405.06715v1",
    "title": "Enhancing Creativity in Large Language Models through Associative Thinking Strategies",
    "authors": [
      "Pronita Mehrotra",
      "Aishni Parab",
      "Sumit Gulwani"
    ],
    "abstract": "This paper explores the enhancement of creativity in Large Language Models\n(LLMs) like vGPT-4 through associative thinking, a cognitive process where\ncreative ideas emerge from linking seemingly unrelated concepts. Associative\nthinking strategies have been found to effectively help humans boost\ncreativity. However, whether the same strategies can help LLMs become more\ncreative remains under-explored. In this work, we investigate whether prompting\nLLMs to connect disparate concepts can augment their creative outputs. Focusing\non three domains -- Product Design, Storytelling, and Marketing -- we introduce\ncreativity tasks designed to assess vGPT-4's ability to generate original and\nuseful content. By challenging the models to form novel associations, we\nevaluate the potential of associative thinking to enhance the creative\ncapabilities of LLMs. Our findings show that leveraging associative thinking\ntechniques can significantly improve the originality of vGPT-4's responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06715v1",
    "published_date": "2024-05-09 16:42:29 UTC",
    "updated_date": "2024-05-09 16:42:29 UTC"
  },
  {
    "arxiv_id": "2405.06004v2",
    "title": "EWMoE: An effective model for global weather forecasting with mixture-of-experts",
    "authors": [
      "Lihao Gan",
      "Xin Man",
      "Chenghong Zhang",
      "Jie Shao"
    ],
    "abstract": "Weather forecasting is a crucial task for meteorologic research, with direct\nsocial and economic impacts. Recently, data-driven weather forecasting models\nbased on deep learning have shown great potential, achieving superior\nperformance compared with traditional numerical weather prediction methods.\nHowever, these models often require massive training data and computational\nresources. In this paper, we propose EWMoE, an effective model for accurate\nglobal weather forecasting, which requires significantly less training data and\ncomputational resources. Our model incorporates three key components to enhance\nprediction accuracy: 3D absolute position embedding, a core Mixture-of-Experts\n(MoE) layer, and two specific loss functions. We conduct our evaluation on the\nERA5 dataset using only two years of training data. Extensive experiments\ndemonstrate that EWMoE outperforms current models such as FourCastNet and\nClimaX at all forecast time, achieving competitive performance compared with\nthe state-of-the-art models Pangu-Weather and GraphCast in evaluation metrics\nsuch as Anomaly Correlation Coefficient (ACC) and Root Mean Square Error\n(RMSE). Additionally, ablation studies indicate that applying the MoE\narchitecture to weather forecasting offers significant advantages in improving\naccuracy and resource efficiency. Code is available at\nhttps://github.com/Tomoyi/EWMoE.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06004v2",
    "published_date": "2024-05-09 16:42:13 UTC",
    "updated_date": "2024-08-23 05:30:55 UTC"
  },
  {
    "arxiv_id": "2405.06714v2",
    "title": "Towards a Path Dependent Account of Category Fluency",
    "authors": [
      "David Heineman",
      "Reba Koenen",
      "Sashank Varma"
    ],
    "abstract": "Category fluency is a widely studied cognitive phenomenon, yet two\nconflicting accounts have been proposed as the underlying retrieval mechanism\n-- an optimal foraging process deliberately searching through memory (Hills et\nal., 2012) and a random walk sampling from a semantic network (Abbott et al.,\n2015). Evidence for both accounts has centered around predicting human patch\nswitches, where both existing models of category fluency produce paradoxically\nidentical results. We begin by peeling back the assumptions made by existing\nmodels, namely that each named example only depends on the previous example, by\n(i) adding an additional bias to model the category transition probability\ndirectly and (ii) relying on a large language model to predict based on the\nentire existing sequence. Then, we present evidence towards resolving the\ndisagreement between each account of foraging by reformulating models as\nsequence generators. To evaluate, we compare generated category fluency runs to\na bank of human-written sequences by proposing a metric based on n-gram\noverlap. We find category switch predictors do not necessarily produce\nhuman-like sequences, in fact the additional biases used by the Hills et al.\n(2012) model are required to improve generation quality, which are later\nimproved by our category modification. Even generating exclusively with an LLM\nrequires an additional global cue to trigger the patch switching behavior\nduring production. Further tests on only the search process on top of the\nsemantic network highlight the importance of deterministic search to replicate\nhuman behavior.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at CogSci 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.06714v2",
    "published_date": "2024-05-09 16:36:56 UTC",
    "updated_date": "2024-05-14 02:10:05 UTC"
  },
  {
    "arxiv_id": "2405.05876v1",
    "title": "Composable Part-Based Manipulation",
    "authors": [
      "Weiyu Liu",
      "Jiayuan Mao",
      "Joy Hsu",
      "Tucker Hermans",
      "Animesh Garg",
      "Jiajun Wu"
    ],
    "abstract": "In this paper, we propose composable part-based manipulation (CPM), a novel\napproach that leverages object-part decomposition and part-part correspondences\nto improve learning and generalization of robotic manipulation skills. By\nconsidering the functional correspondences between object parts, we\nconceptualize functional actions, such as pouring and constrained placing, as\ncombinations of different correspondence constraints. CPM comprises a\ncollection of composable diffusion models, where each model captures a\ndifferent inter-object correspondence. These diffusion models can generate\nparameters for manipulation skills based on the specific object parts.\nLeveraging part-based correspondences coupled with the task decomposition into\ndistinct constraints enables strong generalization to novel objects and object\ncategories. We validate our approach in both simulated and real-world\nscenarios, demonstrating its effectiveness in achieving robust and generalized\nmanipulation capabilities.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Presented at CoRL 2023. For videos and additional results, see our\n  website: https://cpmcorl2023.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2405.05876v1",
    "published_date": "2024-05-09 16:04:14 UTC",
    "updated_date": "2024-05-09 16:04:14 UTC"
  },
  {
    "arxiv_id": "2405.05870v1",
    "title": "Selecting the Most Conflicting Pair of Candidates",
    "authors": [
      "Théo Delemazure",
      "Łukasz Janeczko",
      "Andrzej Kaczmarczyk",
      "Stanisław Szufa"
    ],
    "abstract": "We study committee elections from a perspective of finding the most\nconflicting candidates, that is, candidates that imply the largest amount of\nconflict, as per voter preferences. By proposing basic axioms to capture this\nobjective, we show that none of the prominent multiwinner voting rules meet\nthem. Consequently, we design committee voting rules compliant with our\ndesiderata, introducing conflictual voting rules. A subsequent deepened\nanalysis sheds more light on how they operate. Our investigation identifies\nvarious aspects of conflict, for which we come up with relevant axioms and\nquantitative measures, which may be of independent interest. We support our\ntheoretical study with experiments on both real-life and synthetic data.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "Accepted for publication at IJCAI-24; 27 pages; 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.05870v1",
    "published_date": "2024-05-09 16:00:20 UTC",
    "updated_date": "2024-05-09 16:00:20 UTC"
  },
  {
    "arxiv_id": "2405.05861v1",
    "title": "ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers",
    "authors": [
      "Liangliang Chen",
      "Shiyu Jin",
      "Haoyu Wang",
      "Liangjun Zhang"
    ],
    "abstract": "Excavators are crucial for diverse tasks such as construction and mining,\nwhile autonomous excavator systems enhance safety and efficiency, address labor\nshortages, and improve human working conditions. Different from the existing\nmodularized approaches, this paper introduces ExACT, an end-to-end autonomous\nexcavator system that processes raw LiDAR, camera data, and joint positions to\ncontrol excavator valves directly. Utilizing the Action Chunking with\nTransformers (ACT) architecture, ExACT employs imitation learning to take\nobservations from multi-modal sensors as inputs and generate actionable\nsequences. In our experiment, we build a simulator based on the captured\nreal-world data to model the relations between excavator valve states and joint\nvelocities. With a few human-operated demonstration data trajectories, ExACT\ndemonstrates the capability of completing different excavation tasks, including\nreaching, digging and dumping through imitation learning in validations with\nthe simulator. To the best of our knowledge, ExACT represents the first\ninstance towards building an end-to-end autonomous excavator system via\nimitation learning methods with a minimal set of human demonstrations. The\nvideo about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA Workshop 2024: 3rd Workshop on Future of Construction: Lifelong\n  Learning Robots in Changing Construction Sites",
    "pdf_url": "http://arxiv.org/pdf/2405.05861v1",
    "published_date": "2024-05-09 15:48:39 UTC",
    "updated_date": "2024-05-09 15:48:39 UTC"
  },
  {
    "arxiv_id": "2405.05858v2",
    "title": "Free-Moving Object Reconstruction and Pose Estimation with Virtual Camera",
    "authors": [
      "Haixin Shi",
      "Yinlin Hu",
      "Daniel Koguciuk",
      "Juan-Ting Lin",
      "Mathieu Salzmann",
      "David Ferstl"
    ],
    "abstract": "We propose an approach for reconstructing free-moving object from a monocular\nRGB video. Most existing methods either assume scene prior, hand pose prior,\nobject category pose prior, or rely on local optimization with multiple\nsequence segments. We propose a method that allows free interaction with the\nobject in front of a moving camera without relying on any prior, and optimizes\nthe sequence globally without any segments. We progressively optimize the\nobject shape and pose simultaneously based on an implicit neural\nrepresentation. A key aspect of our method is a virtual camera system that\nreduces the search space of the optimization significantly. We evaluate our\nmethod on the standard HO3D dataset and a collection of egocentric RGB\nsequences captured with a head-mounted device. We demonstrate that our approach\noutperforms most methods significantly, and is on par with recent techniques\nthat assume prior information.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05858v2",
    "published_date": "2024-05-09 15:45:08 UTC",
    "updated_date": "2024-05-10 15:57:13 UTC"
  },
  {
    "arxiv_id": "2405.05852v1",
    "title": "Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control",
    "authors": [
      "Gunshi Gupta",
      "Karmesh Yadav",
      "Yarin Gal",
      "Dhruv Batra",
      "Zsolt Kira",
      "Cong Lu",
      "Tim G. J. Rudner"
    ],
    "abstract": "Embodied AI agents require a fine-grained understanding of the physical world\nmediated through visual and language inputs. Such capabilities are difficult to\nlearn solely from task-specific data. This has led to the emergence of\npre-trained vision-language models as a tool for transferring representations\nlearned from internet-scale data to downstream tasks and new domains. However,\ncommonly used contrastively trained representations such as in CLIP have been\nshown to fail at enabling embodied agents to gain a sufficiently fine-grained\nscene understanding -- a capability vital for control. To address this\nshortcoming, we consider representations from pre-trained text-to-image\ndiffusion models, which are explicitly optimized to generate images from text\nprompts and as such, contain text-conditioned representations that reflect\nhighly fine-grained visuo-spatial information. Using pre-trained text-to-image\ndiffusion models, we construct Stable Control Representations which allow\nlearning downstream control policies that generalize to complex, open-ended\nenvironments. We show that policies learned using Stable Control\nRepresentations are competitive with state-of-the-art representation learning\napproaches across a broad range of simulated control settings, encompassing\nchallenging manipulation and navigation tasks. Most notably, we show that\nStable Control Representations enable learning policies that exhibit\nstate-of-the-art performance on OVMM, a difficult open-vocabulary navigation\nbenchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05852v1",
    "published_date": "2024-05-09 15:39:54 UTC",
    "updated_date": "2024-05-09 15:39:54 UTC"
  },
  {
    "arxiv_id": "2405.06713v2",
    "title": "Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs",
    "authors": [
      "Zhenhui Jiang",
      "Jiaxin Li",
      "Yang Liu"
    ],
    "abstract": "The strategic significance of Large Language Models (LLMs) in economic\nexpansion, innovation, societal development, and national security has been\nincreasingly recognized since the advent of ChatGPT. This study provides a\ncomprehensive comparative evaluation of American and Chinese LLMs in both\nEnglish and Chinese contexts. We proposed a comprehensive evaluation framework\nthat encompasses natural language proficiency, disciplinary expertise, and\nsafety and responsibility, and systematically assessed 16 prominent models from\nthe US and China under various operational tasks and scenarios. Our key\nfindings show that GPT 4-Turbo is at the forefront in English contexts, whereas\nErnie-Bot 4 stands out in Chinese contexts. The study also highlights\ndisparities in LLM performance across languages and tasks, stressing the\nnecessity for linguistically and culturally nuanced model development. The\ncomplementary strengths of American and Chinese LLMs point to the value of\nSino-US collaboration in advancing LLM technology. The research presents the\ncurrent LLM competition landscape and offers valuable insights for policymakers\nand businesses regarding strategic LLM investments and development. Future work\nwill expand on this framework to include emerging LLM multimodal capabilities\nand business application assessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "There was a miscommunication among the co-authors, resulting in the\n  accidental submission of this paper to arXiv. We are in need of withdrawing\n  the paper from your platform",
    "pdf_url": "http://arxiv.org/pdf/2405.06713v2",
    "published_date": "2024-05-09 15:39:19 UTC",
    "updated_date": "2024-05-21 05:37:58 UTC"
  },
  {
    "arxiv_id": "2405.06712v1",
    "title": "Digital Diagnostics: The Potential Of Large Language Models In Recognizing Symptoms Of Common Illnesses",
    "authors": [
      "Gaurav Kumar Gupta",
      "Aditi Singh",
      "Sijo Valayakkad Manikandan",
      "Abul Ehtesham"
    ],
    "abstract": "The recent swift development of LLMs like GPT-4, Gemini, and GPT-3.5 offers a\ntransformative opportunity in medicine and healthcare, especially in digital\ndiagnostics. This study evaluates each model diagnostic abilities by\ninterpreting a user symptoms and determining diagnoses that fit well with\ncommon illnesses, and it demonstrates how each of these models could\nsignificantly increase diagnostic accuracy and efficiency. Through a series of\ndiagnostic prompts based on symptoms from medical databases, GPT-4 demonstrates\nhigher diagnostic accuracy from its deep and complete history of training on\nmedical data. Meanwhile, Gemini performs with high precision as a critical tool\nin disease triage, demonstrating its potential to be a reliable model when\nphysicians are trying to make high-risk diagnoses. GPT-3.5, though slightly\nless advanced, is a good tool for medical diagnostics. This study highlights\nthe need to study LLMs for healthcare and clinical practices with more care and\nattention, ensuring that any system utilizing LLMs promotes patient privacy and\ncomplies with health information privacy laws such as HIPAA compliance, as well\nas the social consequences that affect the varied individuals in complex\nhealthcare contexts. This study marks the start of a larger future effort to\nstudy the various ways in which assigning ethical concerns to LLMs task of\nlearning from human biases could unearth new ways to apply AI in complex\nmedical settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.06712v1",
    "published_date": "2024-05-09 15:12:24 UTC",
    "updated_date": "2024-05-09 15:12:24 UTC"
  },
  {
    "arxiv_id": "2405.05809v2",
    "title": "Aequitas Flow: Streamlining Fair ML Experimentation",
    "authors": [
      "Sérgio Jesus",
      "Pedro Saleiro",
      "Inês Oliveira e Silva",
      "Beatriz M. Jorge",
      "Rita P. Ribeiro",
      "João Gama",
      "Pedro Bizarro",
      "Rayid Ghani"
    ],
    "abstract": "Aequitas Flow is an open-source framework and toolkit for end-to-end Fair\nMachine Learning (ML) experimentation, and benchmarking in Python. This package\nfills integration gaps that exist in other fair ML packages. In addition to the\nexisting audit capabilities in Aequitas, the Aequitas Flow module provides a\npipeline for fairness-aware model training, hyperparameter optimization, and\nevaluation, enabling easy-to-use and rapid experiments and analysis of results.\nAimed at ML practitioners and researchers, the framework offers implementations\nof methods, datasets, metrics, and standard interfaces for these components to\nimprove extensibility. By facilitating the development of fair ML practices,\nAequitas Flow hopes to enhance the incorporation of fairness concepts in AI\nsystems making AI systems more robust and fair.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05809v2",
    "published_date": "2024-05-09 14:48:17 UTC",
    "updated_date": "2024-10-30 16:34:12 UTC"
  },
  {
    "arxiv_id": "2405.05803v3",
    "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference",
    "authors": [
      "Zhihang Lin",
      "Mingbao Lin",
      "Luxi Lin",
      "Rongrong Ji"
    ],
    "abstract": "Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nunnecessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for VTW, we initially analyze a limited set of tiny\ndatasets and choose the first layer that meets the Kullback-Leibler divergence\ncriterion. Our VTW approach can cut computational overhead by over 40\\% across\ndiverse multimodal tasks while maintaining performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05803v3",
    "published_date": "2024-05-09 14:38:53 UTC",
    "updated_date": "2025-01-25 15:59:57 UTC"
  },
  {
    "arxiv_id": "2405.05802v1",
    "title": "Deploying Graph Neural Networks in Wireless Networks: A Link Stability Viewpoint",
    "authors": [
      "Jun Li",
      "Weiwei Zhang",
      "Kang Wei",
      "Guangji Chen",
      "Long Shi",
      "Wen Chen"
    ],
    "abstract": "As an emerging artificial intelligence technology, graph neural networks\n(GNNs) have exhibited promising performance across a wide range of\ngraph-related applications. However, information exchanges among neighbor nodes\nin GNN pose new challenges in the resource-constrained scenario, especially in\nwireless systems. In practical wireless systems, the communication links among\nnodes are usually unreliable due to wireless fading and receiver noise,\nconsequently resulting in performance degradation of GNNs. To improve the\nlearning performance of GNNs, we aim to maximize the number of long-term\naverage (LTA) communication links by the optimized power control under energy\nconsumption constraints. Using the Lyapunov optimization method, we first\ntransform the intractable long-term problem into a deterministic problem in\neach time slot by converting the long-term energy constraints into the\nobjective function. In spite of this non-convex combinatorial optimization\nproblem, we address this problem via equivalently solving a sequence of convex\nfeasibility problems together with a greedy based solver. Simulation results\ndemonstrate the superiority of our proposed scheme over the baselines.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "5 pages,3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.05802v1",
    "published_date": "2024-05-09 14:37:08 UTC",
    "updated_date": "2024-05-09 14:37:08 UTC"
  },
  {
    "arxiv_id": "2405.05792v1",
    "title": "RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation",
    "authors": [
      "Sourav Garg",
      "Krishan Rana",
      "Mehdi Hosseinzadeh",
      "Lachlan Mares",
      "Niko Sünderhauf",
      "Feras Dayoub",
      "Ian Reid"
    ],
    "abstract": "Mapping is crucial for spatial reasoning, planning and robot navigation.\nExisting approaches range from metric, which require precise geometry-based\noptimization, to purely topological, where image-as-node based graphs lack\nexplicit object-level reasoning and interconnectivity. In this paper, we\npropose a novel topological representation of an environment based on \"image\nsegments\", which are semantically meaningful and open-vocabulary queryable,\nconferring several advantages over previous works based on pixel-level\nfeatures. Unlike 3D scene graphs, we create a purely topological graph with\nsegments as nodes, where edges are formed by a) associating segment-level\ndescriptors between pairs of consecutive images and b) connecting neighboring\nsegments within an image using their pixel centroids. This unveils a\n\"continuous sense of a place\", defined by inter-image persistence of segments\nalong with their intra-image neighbours. It further enables us to represent and\nupdate segment-level descriptors through neighborhood aggregation using graph\nconvolution layers, which improves robot localization based on segment-level\nretrieval. Using real-world data, we show how our proposed map representation\ncan be used to i) generate navigation plans in the form of \"hops over segments\"\nand ii) search for target objects using natural language queries describing\nspatial relations of objects. Furthermore, we quantitatively analyze data\nassociation at the segment level, which underpins inter-image connectivity\nduring mapping and segment-level localization when revisiting the same place.\nFinally, we show preliminary trials on segment-level `hopping' based zero-shot\nreal-world navigation. Project page with supplementary details:\noravus.github.io/RoboHop/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Published at ICRA 2024; 9 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.05792v1",
    "published_date": "2024-05-09 14:17:26 UTC",
    "updated_date": "2024-05-09 14:17:26 UTC"
  },
  {
    "arxiv_id": "2405.05790v1",
    "title": "A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties",
    "authors": [
      "A. Noroozi",
      "M. Ravan",
      "B. Razavi",
      "R. S. Fisher",
      "Y. Law",
      "M. S. Hasan"
    ],
    "abstract": "In this paper, we present a robust version of the well-known exact\nlow-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA,\nto localize brain sources in the presence of different forward model\nuncertainties. Methods: We first assume that the true lead field matrix is a\ntransformation of the existing lead field matrix distorted by uncertainties and\npropose an iterative approach to estimate this transformation accurately. Major\nsources of the forward model uncertainties, including differences in geometry,\nconductivity, and source space resolution between the real and simulated head\nmodels, and misaligned electrode positions, are then simulated to test the\nproposed method. Results: ReLORETA and eLORETA are applied to simulated focal\nsources in different regions of the brain and the presence of various noise\nlevels as well as real data from a patient with focal epilepsy. The results\nshow that ReLORETA is considerably more robust and accurate than eLORETA in all\ncases. Conclusion: Having successfully dealt with the forward model\nuncertainties, ReLORETA proved to be a promising method for real-world clinical\napplications. Significance: eLORETA is one of the localization techniques that\ncould be used to study brain activity for medical applications such as\ndetermining the epileptogenic zone in patients with medically refractory\nepilepsy. However, the major limitation of eLORETA is sensitivity to the\nuncertainties in the forward model. Since this problem can substantially\nundermine its performance in real-world applications where the exact lead field\nmatrix is unknown, developing a more robust method capable of dealing with\nthese uncertainties is of significant interest.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05790v1",
    "published_date": "2024-05-09 14:15:00 UTC",
    "updated_date": "2024-05-09 14:15:00 UTC"
  },
  {
    "arxiv_id": "2405.05777v1",
    "title": "Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language",
    "authors": [
      "Ronny Paul",
      "Himanshu Buckchash",
      "Shantipriya Parida",
      "Dilip K. Prasad"
    ],
    "abstract": "S\\'ami, an indigenous language group comprising multiple languages, faces\ndigital marginalization due to the limited availability of data and\nsophisticated language models designed for its linguistic intricacies. This\nwork focuses on increasing technological participation for the S\\'ami language.\nWe draw the attention of the ML community towards the language modeling problem\nof Ultra Low Resource (ULR) languages. ULR languages are those for which the\namount of available textual resources is very low, and the speaker count for\nthem is also very low. ULRLs are also not supported by mainstream Large\nLanguage Models (LLMs) like ChatGPT, due to which gathering artificial training\ndata for them becomes even more challenging. Mainstream AI foundational model\ndevelopment has given less attention to this category of languages. Generally,\nthese languages have very few speakers, making it hard to find them. However,\nit is important to develop foundational models for these ULR languages to\npromote inclusion and the tangible abilities and impact of LLMs. To this end,\nwe have compiled the available S\\'ami language resources from the web to create\na clean dataset for training language models. In order to study the behavior of\nmodern LLM models with ULR languages (S\\'ami), we have experimented with\ndifferent kinds of LLMs, mainly at the order of $\\sim$ seven billion\nparameters. We have also explored the effect of multilingual LLM training for\nULRLs. We found that the decoder-only models under a sequential multilingual\ntraining scenario perform better than joint multilingual training, whereas\nmultilingual training with high semantic overlap, in general, performs better\nthan training from scratch.This is the first study on the S\\'ami language for\nadapting non-statistical language models that use the latest developments in\nthe field of natural language processing (NLP).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05777v1",
    "published_date": "2024-05-09 13:54:22 UTC",
    "updated_date": "2024-05-09 13:54:22 UTC"
  },
  {
    "arxiv_id": "2405.05766v1",
    "title": "To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems",
    "authors": [
      "Miquel Miró-Nicolau",
      "Gabriel Moyà-Alcover",
      "Antoni Jaume-i-Capó",
      "Manuel González-Hidalgo",
      "Maria Gemma Sempere Campello",
      "Juan Antonio Palmer Sancho"
    ],
    "abstract": "The increasing reliance on Deep Learning models, combined with their inherent\nlack of transparency, has spurred the development of a novel field of study\nknown as eXplainable AI (XAI) methods. These methods seek to enhance the trust\nof end-users in automated systems by providing insights into the rationale\nbehind their decisions. This paper presents a novel approach for measuring user\ntrust in XAI systems, allowing their refinement. Our proposed metric combines\nboth performance metrics and trust indicators from an objective perspective. To\nvalidate this novel methodology, we conducted a case study in a realistic\nmedical scenario: the usage of XAI system for the detection of pneumonia from\nx-ray images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05766v1",
    "published_date": "2024-05-09 13:42:54 UTC",
    "updated_date": "2024-05-09 13:42:54 UTC"
  },
  {
    "arxiv_id": "2405.05763v1",
    "title": "DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models",
    "authors": [
      "Mengxiao Geng",
      "Jiahao Zhu",
      "Xiaolin Zhu",
      "Qiqing Liu",
      "Dong Liang",
      "Qiegen Liu"
    ],
    "abstract": "Detail features of magnetic resonance images play a cru-cial role in accurate\nmedical diagnosis and treatment, as they capture subtle changes that pose\nchallenges for doc-tors when performing precise judgments. However, the widely\nutilized naive diffusion model has limitations, as it fails to accurately\ncapture more intricate details. To en-hance the quality of MRI reconstruction,\nwe propose a comprehensive detail-preserving reconstruction method using\nmultiple diffusion models to extract structure and detail features in k-space\ndomain instead of image do-main. Moreover, virtual binary modal masks are\nutilized to refine the range of values in k-space data through highly adaptive\ncenter windows, which allows the model to focus its attention more efficiently.\nLast but not least, an inverted pyramid structure is employed, where the\ntop-down image information gradually decreases, ena-bling a cascade\nrepresentation. The framework effective-ly represents multi-scale sampled data,\ntaking into ac-count the sparsity of the inverted pyramid architecture, and\nutilizes cascade training data distribution to repre-sent multi-scale data.\nThrough a step-by-step refinement approach, the method refines the\napproximation of de-tails. Finally, the proposed method was evaluated by\ncon-ducting experiments on clinical and public datasets. The results\ndemonstrate that the proposed method outper-forms other methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05763v1",
    "published_date": "2024-05-09 13:37:18 UTC",
    "updated_date": "2024-05-09 13:37:18 UTC"
  },
  {
    "arxiv_id": "2405.05755v2",
    "title": "CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks",
    "authors": [
      "Nick Nikzad",
      "Yongsheng Gao",
      "Jun Zhou"
    ],
    "abstract": "In recent years, convolutional neural networks (CNNs) with channel-wise\nfeature refining mechanisms have brought noticeable benefits to modelling\nchannel dependencies. However, current attention paradigms fail to infer an\noptimal channel descriptor capable of simultaneously exploiting statistical and\nspatial relationships among feature maps. In this paper, to overcome this\nshortcoming, we present a novel channel-wise spatially autocorrelated (CSA)\nattention mechanism. Inspired by geographical analysis, the proposed CSA\nexploits the spatial relationships between channels of feature maps to produce\nan effective channel descriptor. To the best of our knowledge, this is the f\nirst time that the concept of geographical spatial analysis is utilized in deep\nCNNs. The proposed CSA imposes negligible learning parameters and light\ncomputational overhead to the deep model, making it a powerful yet efficient\nattention module of choice. We validate the effectiveness of the proposed CSA\nnetworks (CSA-Nets) through extensive experiments and analysis on ImageNet, and\nMS COCO benchmark datasets for image classification, object detection, and\ninstance segmentation. The experimental results demonstrate that CSA-Nets are\nable to consistently achieve competitive performance and superior\ngeneralization than several state-of-the-art attention-based CNNs over\ndifferent benchmark tasks and datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05755v2",
    "published_date": "2024-05-09 13:21:03 UTC",
    "updated_date": "2024-05-13 08:17:13 UTC"
  },
  {
    "arxiv_id": "2405.05751v2",
    "title": "Mirage: A Multi-Level Superoptimizer for Tensor Programs",
    "authors": [
      "Mengdi Wu",
      "Xinhao Cheng",
      "Shengyu Liu",
      "Chunan Shi",
      "Jianan Ji",
      "Kit Ao",
      "Praveen Velliengiri",
      "Xupeng Miao",
      "Oded Padon",
      "Zhihao Jia"
    ],
    "abstract": "We introduce Mirage, the first multi-level superoptimizer for tensor\nprograms. A key idea in Mirage is $\\mu$Graphs, a uniform representation of\ntensor programs at the kernel, thread block, and thread levels of the GPU\ncompute hierarchy. $\\mu$Graphs enable Mirage to discover novel optimizations\nthat combine algebraic transformations, schedule transformations, and\ngeneration of new custom kernels. To navigate the large search space, Mirage\nintroduces a pruning technique based on abstraction that significantly reduces\nthe search space and provides a certain optimality guarantee. To ensure that\nthe optimized $\\mu$Graph is equivalent to the input program, Mirage introduces\na probabilistic equivalence verification procedure with strong theoretical\nguarantees. Our evaluation shows that Mirage outperforms existing approaches by\n1.1-2.9$\\times$ even for DNNs that are widely used and heavily optimized.\nMirage is publicly available at https://github.com/mirage-project/mirage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05751v2",
    "published_date": "2024-05-09 13:15:40 UTC",
    "updated_date": "2024-12-23 15:28:18 UTC"
  },
  {
    "arxiv_id": "2405.05741v1",
    "title": "Can large language models understand uncommon meanings of common words?",
    "authors": [
      "Jinyang Wu",
      "Feihu Che",
      "Xinxin Zheng",
      "Shuai Zhang",
      "Ruihan Jin",
      "Shuai Nie",
      "Pengpeng Shao",
      "Jianhua Tao"
    ],
    "abstract": "Large language models (LLMs) like ChatGPT have shown significant advancements\nacross diverse natural language understanding (NLU) tasks, including\nintelligent dialogue and autonomous agents. Yet, lacking widely acknowledged\ntesting mechanisms, answering `whether LLMs are stochastic parrots or genuinely\ncomprehend the world' remains unclear, fostering numerous studies and sparking\nheated debates. Prevailing research mainly focuses on surface-level NLU,\nneglecting fine-grained explorations. However, such explorations are crucial\nfor understanding their unique comprehension mechanisms, aligning with human\ncognition, and finally enhancing LLMs' general NLU capacities. To address this\ngap, our study delves into LLMs' nuanced semantic comprehension capabilities,\nparticularly regarding common words with uncommon meanings. The idea stems from\nfoundational principles of human communication within psychology, which\nunderscore accurate shared understandings of word semantics. Specifically, this\npaper presents the innovative construction of a Lexical Semantic Comprehension\n(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing\nboth fine-grained and cross-lingual dimensions. Introducing models of both\nopen-source and closed-source, varied scales and architectures, our extensive\nempirical experiments demonstrate the inferior performance of existing models\nin this basic lexical-meaning understanding task. Notably, even the\nstate-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%\nand 22.3%, respectively. Additionally, multiple advanced prompting techniques\nand retrieval-augmented generation are also introduced to help alleviate this\ntrouble, yet limitations persist. By highlighting the above critical\nshortcomings, this research motivates further investigation and offers novel\ninsights for developing more intelligent LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05741v1",
    "published_date": "2024-05-09 12:58:22 UTC",
    "updated_date": "2024-05-09 12:58:22 UTC"
  },
  {
    "arxiv_id": "2405.06710v1",
    "title": "Mobile Sequencers",
    "authors": [
      "Cem Bozsahin"
    ],
    "abstract": "The article is an attempt to contribute to explorations of a common origin\nfor language and planned-collaborative action. It gives `semantics of change'\nthe central stage in the synthesis, from its history and recordkeeping to its\ndevelopment, its syntax, delivery and reception, including substratal aspects.\n  It is suggested that to arrive at a common core, linguistic semantics must be\nunderstood as studying through syntax mobile agent's representing, tracking and\ncoping with change and no change. Semantics of actions can be conceived the\nsame way, but through plans instead of syntax. The key point is the following:\nSequencing itself, of words and action sequences, brings in more structural\ninterpretation to the sequence than which is immediately evident from the\nsequents themselves. Mobile sequencers can be understood as subjects\nstructuring reporting, understanding and keeping track of change and no change.\nThe idea invites rethinking of the notion of category, both in language and in\nplanning.\n  Understanding understanding change by mobile agents is suggested to be about\nhuman extended practice, not extended-human practice. That's why linguistics is\nas important as computer science in the synthesis. It must rely on\nrepresentational history of acts, thoughts and expressions, personal and\npublic, crosscutting overtness and covertness of these phenomena. It has\nimplication for anthropology in the extended practice, which is covered\nbriefly.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06710v1",
    "published_date": "2024-05-09 12:39:50 UTC",
    "updated_date": "2024-05-09 12:39:50 UTC"
  },
  {
    "arxiv_id": "2405.05723v1",
    "title": "Computational lexical analysis of Flamenco genres",
    "authors": [
      "Pablo Rosillo-Rodes",
      "Maxi San Miguel",
      "David Sanchez"
    ],
    "abstract": "Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of\nHumanity, is a profound expression of cultural identity rooted in Andalusia,\nSpain. However, there is a lack of quantitative studies that help identify\ncharacteristic patterns in this long-lived music tradition. In this work, we\npresent a computational analysis of Flamenco lyrics, employing natural language\nprocessing and machine learning to categorize over 2000 lyrics into their\nrespective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial\nNaive Bayes classifier, we find that lexical variation across styles enables to\naccurately identify distinct $\\textit{palos}$. More importantly, from an\nautomatic method of word usage, we obtain the semantic fields that characterize\neach style. Further, applying a metric that quantifies the inter-genre distance\nwe perform a network analysis that sheds light on the relationship between\nFlamenco styles. Remarkably, our results suggest historical connections and\n$\\textit{palo}$ evolutions. Overall, our work illuminates the intricate\nrelationships and cultural significance embedded within Flamenco lyrics,\ncomplementing previous qualitative discussions with quantitative analyses and\nsparking new discussions on the origin and development of traditional music\ngenres.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 29 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.05723v1",
    "published_date": "2024-05-09 12:35:33 UTC",
    "updated_date": "2024-05-09 12:35:33 UTC"
  },
  {
    "arxiv_id": "2407.03331v1",
    "title": "Anole: Adapting Diverse Compressed Models For Cross-Scene Prediction On Mobile Devices",
    "authors": [
      "Yunzhe Li",
      "Hongzi Zhu",
      "Zhuohong Deng",
      "Yunlong Cheng",
      "Liang Zhang",
      "Shan Chang",
      "Minyi Guo"
    ],
    "abstract": "Emerging Artificial Intelligence of Things (AIoT) applications desire online\nprediction using deep neural network (DNN) models on mobile devices. However,\ndue to the movement of devices, unfamiliar test samples constantly appear,\nsignificantly affecting the prediction accuracy of a pre-trained DNN. In\naddition, unstable network connection calls for local model inference. In this\npaper, we propose a light-weight scheme, called Anole, to cope with the local\nDNN model inference on mobile devices. The core idea of Anole is to first\nestablish an army of compact DNN models, and then adaptively select the model\nfitting the current test sample best for online inference. The key is to\nautomatically identify model-friendly scenes for training scene-specific DNN\nmodels. To this end, we design a weakly-supervised scene representation\nlearning algorithm by combining both human heuristics and feature similarity in\nseparating scenes. Moreover, we further train a model classifier to predict the\nbest-fit scene-specific DNN model for each test sample. We implement Anole on\ndifferent types of mobile devices and conduct extensive trace-driven and\nreal-world experiments based on unmanned aerial vehicles (UAVs). The results\ndemonstrate that Anole outwits the method of using a versatile large DNN in\nterms of prediction accuracy (4.5% higher), response time (33.1% faster) and\npower consumption (45.1% lower).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03331v1",
    "published_date": "2024-05-09 12:06:18 UTC",
    "updated_date": "2024-05-09 12:06:18 UTC"
  },
  {
    "arxiv_id": "2405.05695v1",
    "title": "Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost",
    "authors": [
      "Yuan Gao",
      "Weizhong Zhang",
      "Wenhan Luo",
      "Lin Ma",
      "Jin-Gang Yu",
      "Gui-Song Xia",
      "Jiayi Ma"
    ],
    "abstract": "We aim at exploiting additional auxiliary labels from an independent\n(auxiliary) task to boost the primary task performance which we focus on, while\npreserving a single task inference cost of the primary task. While most\nexisting auxiliary learning methods are optimization-based relying on loss\nweights/gradients manipulation, our method is architecture-based with a\nflexible asymmetric structure for the primary and auxiliary tasks, which\nproduces different networks for training and inference. Specifically, starting\nfrom two single task networks/branches (each representing a task), we propose a\nnovel method with evolving networks where only primary-to-auxiliary links exist\nas the cross-task connections after convergence. These connections can be\nremoved during the primary task inference, resulting in a single-task inference\ncost. We achieve this by formulating a Neural Architecture Search (NAS)\nproblem, where we initialize bi-directional connections in the search space and\nguide the NAS optimization converging to an architecture with only the\nsingle-side primary-to-auxiliary connections. Moreover, our method can be\nincorporated with optimization-based auxiliary learning approaches. Extensive\nexperiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using\nVGG, ResNet, and ViT backbones validate the promising performance. The codes\nare available at https://github.com/ethanygao/Aux-NAS.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.05695v1",
    "published_date": "2024-05-09 11:50:19 UTC",
    "updated_date": "2024-05-09 11:50:19 UTC"
  },
  {
    "arxiv_id": "2405.06001v3",
    "title": "LLMC: Benchmarking Large Language Model Quantization with a Versatile Compression Toolkit",
    "authors": [
      "Ruihao Gong",
      "Yang Yong",
      "Shiqiao Gu",
      "Yushi Huang",
      "Chengtao Lv",
      "Yunchen Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "abstract": "Recent advancements in large language models (LLMs) are propelling us toward\nartificial general intelligence with their remarkable emergent abilities and\nreasoning capabilities. However, the substantial computational and memory\nrequirements limit the widespread adoption. Quantization, a key compression\ntechnique, can effectively mitigate these demands by compressing and\naccelerating LLMs, albeit with potential risks to accuracy. Numerous studies\nhave aimed to minimize the accuracy loss associated with quantization. However,\ntheir quantization configurations vary from each other and cannot be fairly\ncompared. In this paper, we present LLMC, a plug-and-play compression toolkit,\nto fairly and systematically explore the impact of quantization. LLMC\nintegrates dozens of algorithms, models, and hardwares, offering high\nextensibility from integer to floating-point quantization, from LLM to\nvision-language (VLM) model, from fixed-bit to mixed precision, and from\nquantization to sparsification. Powered by this versatile toolkit, our\nbenchmark covers three key aspects: calibration data, algorithms (three\nstrategies), and data formats, providing novel insights and detailed analyses\nfor further research and practical guidance for users. Our toolkit is available\nat https://github.com/ModelTC/llmc.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by EMNLP 2024 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2405.06001v3",
    "published_date": "2024-05-09 11:49:05 UTC",
    "updated_date": "2024-10-09 06:09:41 UTC"
  },
  {
    "arxiv_id": "2405.06709v1",
    "title": "Evaluating the Efficacy of AI Techniques in Textual Anonymization: A Comparative Study",
    "authors": [
      "Dimitris Asimopoulos",
      "Ilias Siniosoglou",
      "Vasileios Argyriou",
      "Sotirios K. Goudos",
      "Konstantinos E. Psannis",
      "Nikoleta Karditsioti",
      "Theocharis Saoulidis",
      "Panagiotis Sarigiannidis"
    ],
    "abstract": "In the digital era, with escalating privacy concerns, it's imperative to\ndevise robust strategies that protect private data while maintaining the\nintrinsic value of textual information. This research embarks on a\ncomprehensive examination of text anonymisation methods, focusing on\nConditional Random Fields (CRF), Long Short-Term Memory (LSTM), Embeddings from\nLanguage Models (ELMo), and the transformative capabilities of the Transformers\narchitecture. Each model presents unique strengths since LSTM is modeling\nlong-term dependencies, CRF captures dependencies among word sequences, ELMo\ndelivers contextual word representations using deep bidirectional language\nmodels and Transformers introduce self-attention mechanisms that provide\nenhanced scalability. Our study is positioned as a comparative analysis of\nthese models, emphasising their synergistic potential in addressing text\nanonymisation challenges. Preliminary results indicate that CRF, LSTM, and ELMo\nindividually outperform traditional methods. The inclusion of Transformers,\nwhen compared alongside with the other models, offers a broader perspective on\nachieving optimal text anonymisation in contemporary settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06709v1",
    "published_date": "2024-05-09 11:29:25 UTC",
    "updated_date": "2024-05-09 11:29:25 UTC"
  },
  {
    "arxiv_id": "2405.05662v1",
    "title": "Approximate Dec-POMDP Solving Using Multi-Agent A*",
    "authors": [
      "Wietze Koops",
      "Sebastian Junges",
      "Nils Jansen"
    ],
    "abstract": "We present an A*-based algorithm to compute policies for finite-horizon\nDec-POMDPs. Our goal is to sacrifice optimality in favor of scalability for\nlarger horizons. The main ingredients of our approach are (1) using clustered\nsliding window memory, (2) pruning the A* search tree, and (3) using novel A*\nheuristics. Our experiments show competitive performance to the\nstate-of-the-art. Moreover, for multiple benchmarks, we achieve superior\nperformance. In addition, we provide an A* algorithm that finds upper bounds\nfor the optimum, tailored towards problems with long horizons. The main\ningredient is a new heuristic that periodically reveals the state, thereby\nlimiting the number of reachable beliefs. Our experiments demonstrate the\nefficacy and scalability of the approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 3 figures. Extended version (with appendix) of the paper to\n  appear in IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.05662v1",
    "published_date": "2024-05-09 10:33:07 UTC",
    "updated_date": "2024-05-09 10:33:07 UTC"
  },
  {
    "arxiv_id": "2405.06708v5",
    "title": "LangCell: Language-Cell Pre-training for Cell Identity Understanding",
    "authors": [
      "Suyuan Zhao",
      "Jiahuan Zhang",
      "Yushuai Wu",
      "Yizhen Luo",
      "Zaiqing Nie"
    ],
    "abstract": "Cell identity encompasses various semantic aspects of a cell, including cell\ntype, pathway information, disease information, and more, which are essential\nfor biologists to gain insights into its biological characteristics.\nUnderstanding cell identity from the transcriptomic data, such as annotating\ncell types, has become an important task in bioinformatics. As these semantic\naspects are determined by human experts, it is impossible for AI models to\neffectively carry out cell identity understanding tasks without the supervision\nsignals provided by single-cell and label pairs. The single-cell pre-trained\nlanguage models (PLMs) currently used for this task are trained only on a\nsingle modality, transcriptomics data, lack an understanding of cell identity\nknowledge. As a result, they have to be fine-tuned for downstream tasks and\nstruggle when lacking labeled data with the desired semantic labels. To address\nthis issue, we propose an innovative solution by constructing a unified\nrepresentation of single-cell data and natural language during the pre-training\nphase, allowing the model to directly incorporate insights related to cell\nidentity. More specifically, we introduce $\\textbf{LangCell}$, the first\n$\\textbf{Lang}$uage-$\\textbf{Cell}$ pre-training framework. LangCell utilizes\ntexts enriched with cell identity information to gain a profound comprehension\nof cross-modal knowledge. Results from experiments conducted on different\nbenchmarks show that LangCell is the only single-cell PLM that can work\neffectively in zero-shot cell identity understanding scenarios, and also\nsignificantly outperforms existing models in few-shot and fine-tuning cell\nidentity understanding scenarios.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "q-bio.GN",
    "comment": "Accpeted by ICML 2024, code released",
    "pdf_url": "http://arxiv.org/pdf/2405.06708v5",
    "published_date": "2024-05-09 10:04:05 UTC",
    "updated_date": "2024-06-11 07:31:13 UTC"
  },
  {
    "arxiv_id": "2405.08008v2",
    "title": "Iris: An AI-Driven Virtual Tutor For Computer Science Education",
    "authors": [
      "Patrick Bassner",
      "Eduard Frankford",
      "Stephan Krusche"
    ],
    "abstract": "Integrating AI-driven tools in higher education is an emerging area with\ntransformative potential. This paper introduces Iris, a chat-based virtual\ntutor integrated into the interactive learning platform Artemis that offers\npersonalized, context-aware assistance in large-scale educational settings.\nIris supports computer science students by guiding them through programming\nexercises and is designed to act as a tutor in a didactically meaningful way.\nIts calibrated assistance avoids revealing complete solutions, offering subtle\nhints or counter-questions to foster independent problem-solving skills. For\neach question, it issues multiple prompts in a Chain-of-Thought to\nGPT-3.5-Turbo. The prompts include a tutor role description and examples of\nmeaningful answers through few-shot learning. Iris employs contextual awareness\nby accessing the problem statement, student code, and automated feedback to\nprovide tailored advice.\n  An empirical evaluation shows that students perceive Iris as effective\nbecause it understands their questions, provides relevant support, and\ncontributes to the learning process. While students consider Iris a valuable\ntool for programming exercises and homework, they also feel confident solving\nprogramming tasks in computer-based exams without Iris. The findings underscore\nstudents' appreciation for Iris' immediate and personalized support, though\nstudents predominantly view it as a complement to, rather than a replacement\nfor, human tutors. Nevertheless, Iris creates a space for students to ask\nquestions without being judged by others.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Published in Proceedings of the 2024 Innovation and Technology in\n  Computer Science Education V. 1 (ITiCSE 2024), Pages 534 - 540, July 8--10,\n  2024, Milan, Italy",
    "pdf_url": "http://arxiv.org/pdf/2405.08008v2",
    "published_date": "2024-05-09 09:24:13 UTC",
    "updated_date": "2024-07-10 07:59:15 UTC"
  },
  {
    "arxiv_id": "2405.05636v1",
    "title": "SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space",
    "authors": [
      "Zeren Zhang",
      "Haibo Qin",
      "Jiayu Huang",
      "Yixin Li",
      "Hui Lin",
      "Yitao Duan",
      "Jinwen Ma"
    ],
    "abstract": "Combining face swapping with lip synchronization technology offers a\ncost-effective solution for customized talking face generation. However,\ndirectly cascading existing models together tends to introduce significant\ninterference between tasks and reduce video clarity because the interaction\nspace is limited to the low-level semantic RGB space. To address this issue, we\npropose an innovative unified framework, SwapTalk, which accomplishes both face\nswapping and lip synchronization tasks in the same latent space. Referring to\nrecent work on face generation, we choose the VQ-embedding space due to its\nexcellent editability and fidelity performance. To enhance the framework's\ngeneralization capabilities for unseen identities, we incorporate identity loss\nduring the training of the face swapping module. Additionally, we introduce\nexpert discriminator supervision within the latent space during the training of\nthe lip synchronization module to elevate synchronization quality. In the\nevaluation phase, previous studies primarily focused on the self-reconstruction\nof lip movements in synchronous audio-visual videos. To better approximate\nreal-world applications, we expand the evaluation scope to asynchronous\naudio-video scenarios. Furthermore, we introduce a novel identity consistency\nmetric to more comprehensively assess the identity consistency over time series\nin generated facial videos. Experimental results on the HDTF demonstrate that\nour method significantly surpasses existing techniques in video quality, lip\nsynchronization accuracy, face swapping fidelity, and identity consistency. Our\ndemo is available at http://swaptalk.cc.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05636v1",
    "published_date": "2024-05-09 09:22:09 UTC",
    "updated_date": "2024-05-09 09:22:09 UTC"
  },
  {
    "arxiv_id": "2405.06707v1",
    "title": "Hypothesis Testing Prompting Improves Deductive Reasoning in Large Language Models",
    "authors": [
      "Yitian Li",
      "Jidong Tian",
      "Hao He",
      "Yaohui Jin"
    ],
    "abstract": "Combining different forms of prompts with pre-trained large language models\nhas yielded remarkable results on reasoning tasks (e.g. Chain-of-Thought\nprompting). However, along with testing on more complex reasoning, these\nmethods also expose problems such as invalid reasoning and fictional reasoning\npaths. In this paper, we develop \\textit{Hypothesis Testing Prompting}, which\nadds conclusion assumptions, backward reasoning, and fact verification during\nintermediate reasoning steps. \\textit{Hypothesis Testing prompting} involves\nmultiple assumptions and reverses validation of conclusions leading to its\nunique correct answer. Experiments on two challenging deductive reasoning\ndatasets ProofWriter and RuleTaker show that hypothesis testing prompting not\nonly significantly improves the effect, but also generates a more reasonable\nand standardized reasoning process.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06707v1",
    "published_date": "2024-05-09 08:46:17 UTC",
    "updated_date": "2024-05-09 08:46:17 UTC"
  },
  {
    "arxiv_id": "2405.05616v1",
    "title": "G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning",
    "authors": [
      "Ruiting Dai",
      "Yuqiao Tan",
      "Lisi Mo",
      "Shuang Liang",
      "Guohao Huo",
      "Jiayi Luo",
      "Yao Cheng"
    ],
    "abstract": "Commonsense question answering has demonstrated considerable potential across\nvarious applications like assistants and social robots. Although fully\nfine-tuned pre-trained Language Models(LM) have achieved remarkable performance\nin commonsense reasoning, their tendency to excessively prioritize textual\ninformation hampers the precise transfer of structural knowledge and undermines\ninterpretability. Some studies have explored combining LMs with Knowledge\nGraphs(KGs) by coarsely fusing the two modalities to perform Graph Neural\nNetwork(GNN)-based reasoning that lacks a profound interaction between\nheterogeneous modalities. In this paper, we propose a novel Graph-based\nStructure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP,\naiming to maintain a balance between heterogeneous knowledge and enhance the\ncross-modal interaction within the LM+GNNs model. In particular, an evidence\ngraph is constructed by integrating multiple knowledge sources, i.e.\nConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.\nAfterward, a structure-aware frozen PLM is employed to fully incorporate the\nstructured and textual information from the evidence graph, where the\ngeneration of prompts is driven by graph entities and relations. Finally, a\nheterogeneous message-passing reasoning module is used to facilitate deep\ninteraction of knowledge between the LM and graph-based networks. Empirical\nvalidation, conducted through extensive experiments on three benchmark\ndatasets, demonstrates the notable performance of the proposed model. The\nresults reveal a significant advancement over the existing models, especially,\nwith 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05616v1",
    "published_date": "2024-05-09 08:28:12 UTC",
    "updated_date": "2024-05-09 08:28:12 UTC"
  },
  {
    "arxiv_id": "2405.06706v1",
    "title": "Exploring the Capabilities of Large Multimodal Models on Dense Text",
    "authors": [
      "Shuo Zhang",
      "Biao Yang",
      "Zhang Li",
      "Zhiyin Ma",
      "Yuliang Liu",
      "Xiang Bai"
    ],
    "abstract": "While large multi-modal models (LMM) have shown notable progress in\nmulti-modal tasks, their capabilities in tasks involving dense textual content\nremains to be fully explored. Dense text, which carries important information,\nis often found in documents, tables, and product descriptions. Understanding\ndense text enables us to obtain more accurate information, assisting in making\nbetter decisions. To further explore the capabilities of LMM in complex text\ntasks, we propose the DT-VQA dataset, with 170k question-answer pairs. In this\npaper, we conduct a comprehensive evaluation of GPT4V, Gemini, and various\nopen-source LMMs on our dataset, revealing their strengths and weaknesses.\nFurthermore, we evaluate the effectiveness of two strategies for LMM: prompt\nengineering and downstream fine-tuning. We find that even with automatically\nlabeled training datasets, significant improvements in model performance can be\nachieved. We hope that this research will promote the study of LMM in dense\ntext tasks. Code will be released at\nhttps://github.com/Yuliang-Liu/MultimodalOCR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06706v1",
    "published_date": "2024-05-09 07:47:25 UTC",
    "updated_date": "2024-05-09 07:47:25 UTC"
  },
  {
    "arxiv_id": "2405.06705v2",
    "title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought",
    "authors": [
      "Zhuoxuan Jiang",
      "Haoyuan Peng",
      "Shanshan Feng",
      "Fan Li",
      "Dongsheng Li"
    ],
    "abstract": "Self-correction is emerging as a promising approach to mitigate the issue of\nhallucination in Large Language Models (LLMs). To facilitate effective\nself-correction, recent research has proposed mistake detection as its initial\nstep. However, current literature suggests that LLMs often struggle with\nreliably identifying reasoning mistakes when using simplistic prompting\nstrategies. To address this challenge, we introduce a unique prompting\nstrategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is\nspecifically designed to guide the identification of reasoning mistakes,\nparticularly mathematical reasoning mistakes. PedCoT consists of pedagogical\nprinciples for prompts (PPP) design, two-stage interaction process (TIP) and\ngrounded PedCoT prompts, all inspired by the educational theory of the Bloom\nCognitive Model (BCM). We evaluate our approach on two public datasets\nfeaturing math problems of varying difficulty levels. The experiments\ndemonstrate that our zero-shot prompting strategy significantly outperforms\nstrong baselines. The proposed method can achieve the goal of reliable\nmathematical mistake identification and provide a foundation for automatic math\nanswer grading. The results underscore the significance of educational theory,\nserving as domain knowledge, in guiding prompting strategy design for\naddressing challenging tasks with LLMs effectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.06705v2",
    "published_date": "2024-05-09 07:37:34 UTC",
    "updated_date": "2025-03-08 15:20:34 UTC"
  },
  {
    "arxiv_id": "2405.05594v1",
    "title": "Expected Work Search: Combining Win Rate and Proof Size Estimation",
    "authors": [
      "Owen Randall",
      "Martin Müller",
      "Ting Han Wei",
      "Ryan Hayward"
    ],
    "abstract": "We propose Expected Work Search (EWS), a new game solving algorithm. EWS\ncombines win rate estimation, as used in Monte Carlo Tree Search, with proof\nsize estimation, as used in Proof Number Search. The search efficiency of EWS\nstems from minimizing a novel notion of Expected Work, which predicts the\nexpected computation required to solve a position. EWS outperforms traditional\nsolving algorithms on the games of Go and Hex. For Go, we present the first\nsolution to the empty 5x5 board with the commonly used positional superko\nruleset. For Hex, our algorithm solves the empty 8x8 board in under 4 minutes.\nExperiments show that EWS succeeds both with and without extensive\ndomain-specific knowledge.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05594v1",
    "published_date": "2024-05-09 07:33:06 UTC",
    "updated_date": "2024-05-09 07:33:06 UTC"
  },
  {
    "arxiv_id": "2405.05584v1",
    "title": "A Survey on Backbones for Deep Video Action Recognition",
    "authors": [
      "Zixuan Tang",
      "Youjun Zhao",
      "Yuhang Wen",
      "Mengyuan Liu"
    ],
    "abstract": "Action recognition is a key technology in building interactive metaverses.\nWith the rapid development of deep learning, methods in action recognition have\nalso achieved great advancement. Researchers design and implement the backbones\nreferring to multiple standpoints, which leads to the diversity of methods and\nencountering new challenges. This paper reviews several action recognition\nmethods based on deep neural networks. We introduce these methods in three\nparts: 1) Two-Streams networks and their variants, which, specifically in this\npaper, use RGB video frame and optical flow modality as input; 2) 3D\nconvolutional networks, which make efforts in taking advantage of RGB modality\ndirectly while extracting different motion information is no longer necessary;\n3) Transformer-based methods, which introduce the model from natural language\nprocessing into computer vision and video understanding. We offer objective\nsights in this review and hopefully provide a reference for future research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by ICME workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.05584v1",
    "published_date": "2024-05-09 07:20:36 UTC",
    "updated_date": "2024-05-09 07:20:36 UTC"
  },
  {
    "arxiv_id": "2405.05581v1",
    "title": "One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations",
    "authors": [
      "Yoonjoo Lee",
      "Kihoon Son",
      "Tae Soo Kim",
      "Jisu Kim",
      "John Joon Young Chung",
      "Eytan Adar",
      "Juho Kim"
    ],
    "abstract": "As Large Language Models (LLMs) are nondeterministic, the same input can\ngenerate different outputs, some of which may be incorrect or hallucinated. If\nrun again, the LLM may correct itself and produce the correct answer.\nUnfortunately, most LLM-powered systems resort to single results which, correct\nor not, users accept. Having the LLM produce multiple outputs may help identify\ndisagreements or alternatives. However, it is not obvious how the user will\ninterpret conflicts or inconsistencies. To this end, we investigate how users\nperceive the AI model and comprehend the generated information when they\nreceive multiple, potentially inconsistent, outputs. Through a preliminary\nstudy, we identified five types of output inconsistencies. Based on these\ncategories, we conducted a study (N=252) in which participants were given one\nor more LLM-generated passages to an information-seeking question. We found\nthat inconsistency within multiple LLM-generated outputs lowered the\nparticipants' perceived AI capacity, while also increasing their comprehension\nof the given information. Specifically, we observed that this positive effect\nof inconsistencies was most significant for participants who read two passages,\ncompared to those who read three. Based on these findings, we present design\nimplications that, instead of regarding LLM output inconsistencies as a\ndrawback, we can reveal the potential inconsistencies to transparently indicate\nthe limitations of these models and promote critical LLM usage.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to FAccT 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.05581v1",
    "published_date": "2024-05-09 07:12:45 UTC",
    "updated_date": "2024-05-09 07:12:45 UTC"
  },
  {
    "arxiv_id": "2405.05572v2",
    "title": "From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences",
    "authors": [
      "Prashant Kodali",
      "Anmol Goel",
      "Likhith Asapu",
      "Vamshi Krishna Bonagiri",
      "Anirudh Govil",
      "Monojit Choudhury",
      "Ponnurangam Kumaraguru",
      "Manish Shrivastava"
    ],
    "abstract": "Current computational approaches for analysing or generating code-mixed\nsentences do not explicitly model ``naturalness'' or ``acceptability'' of\ncode-mixed sentences, but rely on training corpora to reflect distribution of\nacceptable code-mixed sentences. Modelling human judgement for the\nacceptability of code-mixed text can help in distinguishing natural code-mixed\ntext and enable quality-controlled generation of code-mixed text. To this end,\nwe construct Cline - a dataset containing human acceptability judgements for\nEnglish-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with\n16,642 sentences, consisting of samples sourced from two sources: synthetically\ngenerated code-mixed text and samples collected from online social media. Our\nanalysis establishes that popular code-mixing metrics such as CMI, Number of\nSwitch Points, Burstines, which are used to filter/curate/compare code-mixed\ncorpora have low correlation with human acceptability judgements, underlining\nthe necessity of our dataset. Experiments using Cline demonstrate that simple\nMultilayer Perceptron (MLP) models when trained solely using code-mixing\nmetrics as features are outperformed by fine-tuned pre-trained Multilingual\nLarge Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta\nand Bernice outperform IndicBERT across different configurations. Among\nEncoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder\nmodels are not able to outperform Encoder-only models. Decoder-only models\nperform the best when compared to all other MLLMS, with Llama 3.2 - 3B models\noutperforming similarly sized Qwen, Phi models. Comparison with zero and\nfewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data\noutperform ChatGPT, providing scope for improvement in code-mixed tasks.\nZero-shot transfer from En-Hi to En-Te acceptability judgments are better than\nrandom baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05572v2",
    "published_date": "2024-05-09 06:40:39 UTC",
    "updated_date": "2025-05-05 14:51:58 UTC"
  },
  {
    "arxiv_id": "2405.05553v3",
    "title": "Towards Robust Physical-world Backdoor Attacks on Lane Detection",
    "authors": [
      "Xinwei Zhang",
      "Aishan Liu",
      "Tianyuan Zhang",
      "Siyuan Liang",
      "Xianglong Liu"
    ],
    "abstract": "Deep learning-based lane detection (LD) plays a critical role in autonomous\ndriving systems, such as adaptive cruise control. However, it is vulnerable to\nbackdoor attacks. Existing backdoor attack methods on LD exhibit limited\neffectiveness in dynamic real-world scenarios, primarily because they fail to\nconsider dynamic scene factors, including changes in driving perspectives\n(e.g., viewpoint transformations) and environmental conditions (e.g., weather\nor lighting changes). To tackle this issue, this paper introduces BadLANE, a\ndynamic scene adaptation backdoor attack for LD designed to withstand changes\nin real-world dynamic scene factors. To address the challenges posed by\nchanging driving perspectives, we propose an amorphous trigger pattern composed\nof shapeless pixels. This trigger design allows the backdoor to be activated by\nvarious forms or shapes of mud spots or pollution on the road or lens, enabling\nadaptation to changes in vehicle observation viewpoints during driving. To\nmitigate the effects of environmental changes, we design a meta-learning\nframework to train meta-generators tailored to different environmental\nconditions. These generators produce meta-triggers that incorporate diverse\nenvironmental information, such as weather or lighting conditions, as the\ninitialization of the trigger patterns for backdoor implantation, thus enabling\nadaptation to dynamic environments. Extensive experiments on various commonly\nused LD models in both digital and physical domains validate the effectiveness\nof our attacks, outperforming other baselines significantly (+25.15% on average\nin Attack Success Rate). Our codes will be available upon paper publication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05553v3",
    "published_date": "2024-05-09 05:23:34 UTC",
    "updated_date": "2024-07-01 10:27:42 UTC"
  },
  {
    "arxiv_id": "2405.08007v1",
    "title": "People cannot distinguish GPT-4 from a human in a Turing test",
    "authors": [
      "Cameron R. Jones",
      "Benjamin K. Bergen"
    ],
    "abstract": "We evaluated 3 systems (ELIZA, GPT-3.5 and GPT-4) in a randomized,\ncontrolled, and preregistered Turing test. Human participants had a 5 minute\nconversation with either a human or an AI, and judged whether or not they\nthought their interlocutor was human. GPT-4 was judged to be a human 54% of the\ntime, outperforming ELIZA (22%) but lagging behind actual humans (67%). The\nresults provide the first robust empirical demonstration that any artificial\nsystem passes an interactive 2-player Turing test. The results have\nimplications for debates around machine intelligence and, more urgently,\nsuggest that deception by current AI systems may go undetected. Analysis of\nparticipants' strategies and reasoning suggests that stylistic and\nsocio-emotional factors play a larger role in passing the Turing test than\ntraditional notions of intelligence.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "23 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.08007v1",
    "published_date": "2024-05-09 04:14:09 UTC",
    "updated_date": "2024-05-09 04:14:09 UTC"
  },
  {
    "arxiv_id": "2405.05993v1",
    "title": "Precision Rehabilitation for Patients Post-Stroke based on Electronic Health Records and Machine Learning",
    "authors": [
      "Fengyi Gao",
      "Xingyu Zhang",
      "Sonish Sivarajkumar",
      "Parker Denny",
      "Bayan Aldhahwani",
      "Shyam Visweswaran",
      "Ryan Shi",
      "William Hogan",
      "Allyn Bove",
      "Yanshan Wang"
    ],
    "abstract": "In this study, we utilized statistical analysis and machine learning methods\nto examine whether rehabilitation exercises can improve patients post-stroke\nfunctional abilities, as well as forecast the improvement in functional\nabilities. Our dataset is patients' rehabilitation exercises and demographic\ninformation recorded in the unstructured electronic health records (EHRs) data\nand free-text rehabilitation procedure notes. We collected data for 265 stroke\npatients from the University of Pittsburgh Medical Center. We employed a\npre-existing natural language processing (NLP) algorithm to extract data on\nrehabilitation exercises and developed a rule-based NLP algorithm to extract\nActivity Measure for Post-Acute Care (AM-PAC) scores, covering basic mobility\n(BM) and applied cognitive (AC) domains, from procedure notes. Changes in\nAM-PAC scores were classified based on the minimal clinically important\ndifference (MCID), and significance was assessed using Friedman and Wilcoxon\ntests. To identify impactful exercises, we used Chi-square tests, Fisher's\nexact tests, and logistic regression for odds ratios. Additionally, we\ndeveloped five machine learning models-logistic regression (LR), Adaboost\n(ADB), support vector machine (SVM), gradient boosting (GB), and random forest\n(RF)-to predict outcomes in functional ability. Statistical analyses revealed\nsignificant associations between functional improvements and specific\nexercises. The RF model achieved the best performance in predicting functional\noutcomes. In this study, we identified three rehabilitation exercises that\nsignificantly contributed to patient post-stroke functional ability improvement\nin the first two months. Additionally, the successful application of a machine\nlearning model to predict patient-specific functional outcomes underscores the\npotential for precision rehabilitation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05993v1",
    "published_date": "2024-05-09 04:06:44 UTC",
    "updated_date": "2024-05-09 04:06:44 UTC"
  },
  {
    "arxiv_id": "2405.05523v1",
    "title": "Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training",
    "authors": [
      "Sheng Yan",
      "Xin Du",
      "Zongying Li",
      "Yi Wang",
      "Hongcang Jin",
      "Mengyuan Liu"
    ],
    "abstract": "Temporal grounding is crucial in multimodal learning, but it poses challenges\nwhen applied to animal behavior data due to the sparsity and uniform\ndistribution of moments. To address these challenges, we propose a novel\nPositional Recovery Training framework (Port), which prompts the model with the\nstart and end times of specific animal behaviors during training. Specifically,\nPort enhances the baseline model with a Recovering part to predict flipped\nlabel sequences and align distributions with a Dual-alignment method. This\nallows the model to focus on specific temporal regions prompted by ground-truth\ninformation. Extensive experiments on the Animal Kingdom dataset demonstrate\nthe effectiveness of Port, achieving an IoU@0.3 of 38.52. It emerges as one of\nthe top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICMEW 2024. arXiv admin note: text overlap with\n  arXiv:2404.13657",
    "pdf_url": "http://arxiv.org/pdf/2405.05523v1",
    "published_date": "2024-05-09 03:23:47 UTC",
    "updated_date": "2024-05-09 03:23:47 UTC"
  },
  {
    "arxiv_id": "2405.05512v4",
    "title": "Characteristic Learning for Provable One Step Generation",
    "authors": [
      "Zhao Ding",
      "Chenguang Duan",
      "Yuling Jiao",
      "Ruoxuan Li",
      "Jerry Zhijian Yang",
      "Pingwen Zhang"
    ],
    "abstract": "We propose the characteristic generator, a novel one-step generative model\nthat combines the efficiency of sampling in Generative Adversarial Networks\n(GANs) with the stable performance of flow-based models. Our model is driven by\ncharacteristics, along which the probability density transport can be described\nby ordinary differential equations (ODEs). Specifically, We estimate the\nvelocity field through nonparametric regression and utilize Euler method to\nsolve the probability flow ODE, generating a series of discrete approximations\nto the characteristics. We then use a deep neural network to fit these\ncharacteristics, ensuring a one-step mapping that effectively pushes the prior\ndistribution towards the target distribution. In the theoretical aspect, we\nanalyze the errors in velocity matching, Euler discretization, and\ncharacteristic fitting to establish a non-asymptotic convergence rate for the\ncharacteristic generator in 2-Wasserstein distance. To the best of our\nknowledge, this is the first thorough analysis for simulation-free one step\ngenerative models. Additionally, our analysis refines the error analysis of\nflow-based generative models in prior works. We apply our method on both\nsynthetic and real datasets, and the results demonstrate that the\ncharacteristic generator achieves high generation quality with just a single\nevaluation of neural network.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05512v4",
    "published_date": "2024-05-09 02:41:42 UTC",
    "updated_date": "2024-07-16 15:41:53 UTC"
  },
  {
    "arxiv_id": "2405.05508v2",
    "title": "Redefining Information Retrieval of Structured Database via Large Language Models",
    "authors": [
      "Mingzhu Wang",
      "Yuzhe Zhang",
      "Qihang Zhao",
      "Junyi Yang",
      "Hong Zhang"
    ],
    "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit\nnon-parametric knowledge related to the query through external knowledge bases\nbefore reasoning. The retrieved information is incorporated into LMs as context\nalongside the query, enhancing the reliability of responses towards factual\nquestions. Prior researches in retrieval augmentation typically follow a\nretriever-generator paradigm. In this context, traditional retrievers encounter\nchallenges in precisely and seamlessly extracting query-relevant information\nfrom knowledge bases. To address this issue, this paper introduces a novel\nretrieval augmentation framework called ChatLR that primarily employs the\npowerful semantic understanding ability of Large Language Models (LLMs) as\nretrievers to achieve precise and concise information retrieval. Additionally,\nwe construct an LLM-based search and question answering system tailored for the\nfinancial domain by fine-tuning LLM on two tasks including Text2API and API-ID\nrecognition. Experimental results demonstrate the effectiveness of ChatLR in\naddressing user queries, achieving an overall information retrieval accuracy\nexceeding 98.8\\%.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05508v2",
    "published_date": "2024-05-09 02:37:53 UTC",
    "updated_date": "2024-11-19 01:15:44 UTC"
  },
  {
    "arxiv_id": "2405.05991v1",
    "title": "Agent-oriented Joint Decision Support for Data Owners in Auction-based Federated Learning",
    "authors": [
      "Xiaoli Tang",
      "Han Yu",
      "Xiaoxiao Li"
    ],
    "abstract": "Auction-based Federated Learning (AFL) has attracted extensive research\ninterest due to its ability to motivate data owners (DOs) to join FL through\neconomic means. While many existing AFL methods focus on providing decision\nsupport to model users (MUs) and the AFL auctioneer, decision support for data\nowners remains open. To bridge this gap, we propose a first-of-its-kind\nagent-oriented joint Pricing, Acceptance and Sub-delegation decision support\napproach for data owners in AFL (PAS-AFL). By considering a DO's current\nreputation, pending FL tasks, willingness to train FL models, and its trust\nrelationships with other DOs, it provides a systematic approach for a DO to\nmake joint decisions on AFL bid acceptance, task sub-delegation and pricing\nbased on Lyapunov optimization to maximize its utility. It is the first to\nenable each DO to take on multiple FL tasks simultaneously to earn higher\nincome for DOs and enhance the throughput of FL tasks in the AFL ecosystem.\nExtensive experiments based on six benchmarking datasets demonstrate\nsignificant advantages of PAS-AFL compared to six alternative strategies,\nbeating the best baseline by 28.77% and 2.64% on average in terms of utility\nand test accuracy of the resulting FL models, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05991v1",
    "published_date": "2024-05-09 02:35:46 UTC",
    "updated_date": "2024-05-09 02:35:46 UTC"
  },
  {
    "arxiv_id": "2405.05990v2",
    "title": "Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models",
    "authors": [
      "Yang Bai",
      "Ge Pei",
      "Jindong Gu",
      "Yong Yang",
      "Xingjun Ma"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable performance on a wide\nrange of tasks. However, recent studies have shown that LLMs can memorize\ntraining data and simple repeated tokens can trick the model to leak the data.\nIn this paper, we take a step further and show that certain special characters\nor their combinations with English letters are stronger memory triggers,\nleading to more severe data leakage. The intuition is that, since LLMs are\ntrained with massive data that contains a substantial amount of special\ncharacters (e.g. structural symbols {, } of JSON files, and @, # in emails and\nonline posts), the model may memorize the co-occurrence between these special\ncharacters and the raw texts. This motivates us to propose a simple but\neffective Special Characters Attack (SCA) to induce training data leakage. Our\nexperiments verify the high effectiveness of SCA against state-of-the-art LLMs:\nthey can leak diverse training data, such as code corpus, web pages, and\npersonally identifiable information, and sometimes generate non-stop outputs as\na byproduct. We further show that the composition of the training data corpus\ncan be revealed by inspecting the leaked data -- one crucial piece of\ninformation for pre-training high-performance LLMs. Our work can help\nunderstand the sensitivity of LLMs to special characters and identify potential\nareas for improvement.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05990v2",
    "published_date": "2024-05-09 02:35:32 UTC",
    "updated_date": "2024-05-20 14:40:03 UTC"
  },
  {
    "arxiv_id": "2405.05499v2",
    "title": "Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting",
    "authors": [
      "Feifei Li",
      "Suhan Guo",
      "Feng Han",
      "Jian Zhao",
      "Furao Shen"
    ],
    "abstract": "Accurate forecasting of long-term time series has important applications for\ndecision making and planning. However, it remains challenging to capture the\nlong-term dependencies in time series data. To better extract long-term\ndependencies, We propose Multi Scale Dilated Convolution Network (MSDCN), a\nmethod that utilizes a shallow dilated convolution architecture to capture the\nperiod and trend characteristics of long time series. We design different\nconvolution blocks with exponentially growing dilations and varying kernel\nsizes to sample time series data at different scales. Furthermore, we utilize\ntraditional autoregressive model to capture the linear relationships within the\ndata. To validate the effectiveness of the proposed approach, we conduct\nexperiments on eight challenging long-term time series forecasting benchmark\ndatasets. The experimental results show that our approach outperforms the prior\nstate-of-the-art approaches and shows significant inference speed improvements\ncompared to several strong baseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05499v2",
    "published_date": "2024-05-09 02:11:01 UTC",
    "updated_date": "2024-05-14 07:52:01 UTC"
  },
  {
    "arxiv_id": "2405.05493v1",
    "title": "Parameter-Efficient Fine-Tuning With Adapters",
    "authors": [
      "Keyu Chen",
      "Yuan Pang",
      "Zi Yang"
    ],
    "abstract": "In the arena of language model fine-tuning, the traditional approaches, such\nas Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT),\nalthough effective, but computational intensive. This research introduces a\nnovel adaptation method utilizing the UniPELT framework as a base and added a\nPromptTuning Layer, which significantly reduces the number of trainable\nparameters while maintaining competitive performance across various benchmarks.\nOur method employs adapters, which enable efficient transfer of pretrained\nmodels to new tasks with minimal retraining of the base model parameters. We\nevaluate our approach using three diverse datasets: the GLUE benchmark, a\ndomain-specific dataset comprising four distinct areas, and the Stanford\nQuestion Answering Dataset 1.1 (SQuAD). Our results demonstrate that our\ncustomized adapter-based method achieves performance comparable to full model\nfine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or\nequivalent amount of parameters. This parameter efficiency not only alleviates\nthe computational burden but also expedites the adaptation process. The study\nunderlines the potential of adapters in achieving high performance with\nsignificantly reduced resource consumption, suggesting a promising direction\nfor future research in parameter-efficient fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05493v1",
    "published_date": "2024-05-09 01:40:38 UTC",
    "updated_date": "2024-05-09 01:40:38 UTC"
  },
  {
    "arxiv_id": "2405.05492v3",
    "title": "A logifold structure on measure space",
    "authors": [
      "Inkee Jung",
      "Siu-Cheong Lau"
    ],
    "abstract": "In this paper,we develop a local-to-global and measure-theoretical approach\nto understand datasets. The idea is to take network models with restricted\ndomains as local charts of datasets. We develop the mathematical foundations\nfor these structures, and show in experiments how it can be used to find fuzzy\ndomains and to improve accuracy in data classification problems.",
    "categories": [
      "math.DG",
      "cs.AI",
      "cs.LG",
      "math.PR",
      "55N31, 53Z50, 68T07, 68T09, 60A10, 81P45, 94D05"
    ],
    "primary_category": "math.DG",
    "comment": "37 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.05492v3",
    "published_date": "2024-05-09 01:38:38 UTC",
    "updated_date": "2025-02-01 01:44:16 UTC"
  },
  {
    "arxiv_id": "2407.00024v1",
    "title": "LMVD: A Large-Scale Multimodal Vlog Dataset for Depression Detection in the Wild",
    "authors": [
      "Lang He",
      "Kai Chen",
      "Junnan Zhao",
      "Yimeng Wang",
      "Ercheng Pei",
      "Haifeng Chen",
      "Jiewei Jiang",
      "Shiqing Zhang",
      "Jie Zhang",
      "Zhongmin Wang",
      "Tao He",
      "Prayag Tiwari"
    ],
    "abstract": "Depression can significantly impact many aspects of an individual's life,\nincluding their personal and social functioning, academic and work performance,\nand overall quality of life. Many researchers within the field of affective\ncomputing are adopting deep learning technology to explore potential patterns\nrelated to the detection of depression. However, because of subjects' privacy\nprotection concerns, that data in this area is still scarce, presenting a\nchallenge for the deep discriminative models used in detecting depression. To\nnavigate these obstacles, a large-scale multimodal vlog dataset (LMVD), for\ndepression recognition in the wild is built. In LMVD, which has 1823 samples\nwith 214 hours of the 1475 participants captured from four multimedia platforms\n(Sina Weibo, Bilibili, Tiktok, and YouTube). A novel architecture termed\nMDDformer to learn the non-verbal behaviors of individuals is proposed.\nExtensive validations are performed on the LMVD dataset, demonstrating superior\nperformance for depression detection. We anticipate that the LMVD will\ncontribute a valuable function to the depression detection community. The data\nand code will released at the link: https://github.com/helang818/LMVD/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00024v1",
    "published_date": "2024-05-09 01:27:10 UTC",
    "updated_date": "2024-05-09 01:27:10 UTC"
  },
  {
    "arxiv_id": "2405.05480v4",
    "title": "FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of Real-World SoCs",
    "authors": [
      "Uday Mallappa",
      "Hesham Mostafa",
      "Mikhail Galkin",
      "Mariano Phielipp",
      "Somdeb Majumdar"
    ],
    "abstract": "Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial\nand non-trivial step of the physical design flow. It represents a difficult\ncombinatorial optimization problem. A typical large scale SoC with 120\npartitions generates a search-space of nearly 10E250. As novel machine learning\n(ML) approaches emerge to tackle such problems, there is a growing need for a\nmodern benchmark that comprises a large training dataset and performance\nmetrics that better reflect real-world constraints and objectives compared to\nexisting benchmarks. To address this need, we present FloorSet -- two\ncomprehensive datasets of synthetic fixed-outline floorplan layouts that\nreflect the distribution of real SoCs. Each dataset has 1M training samples and\n100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime\ncomprises fully-abutted rectilinear partitions and near-optimal wire-length. A\nsimplified dataset that reflects early design phases, FloorSet-Lite comprises\nrectangular partitions, with under 5 percent white-space and near-optimal\nwire-length. Both datasets define hard constraints seen in modern design flows\nsuch as shape constraints, edge-affinity, grouping constraints, and\npre-placement constraints. FloorSet is intended to spur fundamental research on\nlarge-scale constrained optimization problems. Crucially, FloorSet alleviates\nthe core issue of reproducibility in modern ML driven solutions to such\nproblems. FloorSet is available as an open-source repository for the research\ncommunity.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "10 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.05480v4",
    "published_date": "2024-05-09 00:37:56 UTC",
    "updated_date": "2024-08-01 22:57:53 UTC"
  },
  {
    "arxiv_id": "2405.06704v1",
    "title": "Enhanced Review Detection and Recognition: A Platform-Agnostic Approach with Application to Online Commerce",
    "authors": [
      "Priyabrata Karmakar",
      "John Hawkins"
    ],
    "abstract": "Online commerce relies heavily on user generated reviews to provide unbiased\ninformation about products that they have not physically seen. The importance\nof reviews has attracted multiple exploitative online behaviours and requires\nmethods for monitoring and detecting reviews. We present a machine learning\nmethodology for review detection and extraction, and demonstrate that it\ngeneralises for use across websites that were not contained in the training\ndata. This method promises to drive applications for automatic detection and\nevaluation of reviews, regardless of their source. Furthermore, we showcase the\nversatility of our method by implementing and discussing three key applications\nfor analysing reviews: Sentiment Inconsistency Analysis, which detects and\nfilters out unreliable reviews based on inconsistencies between ratings and\ncomments; Multi-language support, enabling the extraction and translation of\nreviews from various languages without relying on HTML scraping; and Fake\nreview detection, achieved by integrating a trained NLP model to identify and\ndistinguish between genuine and fake reviews.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06704v1",
    "published_date": "2024-05-09 00:32:22 UTC",
    "updated_date": "2024-05-09 00:32:22 UTC"
  },
  {
    "arxiv_id": "2405.05989v2",
    "title": "Clustering-based Multitasking Deep Neural Network for Solar Photovoltaics Power Generation Prediction",
    "authors": [
      "Hui Song",
      "Zheng Miao",
      "Ali Babalhavaeji",
      "Saman Mehrnia",
      "Mahdi Jalili",
      "Xinghuo Yu"
    ],
    "abstract": "The increasing installation of Photovoltaics (PV) cells leads to more\ngeneration of renewable energy sources (RES), but results in increased\nuncertainties of energy scheduling. Predicting PV power generation is important\nfor energy management and dispatch optimization in smart grid. However, the PV\npower generation data is often collected across different types of customers\n(e.g., residential, agricultural, industrial, and commercial) while the\ncustomer information is always de-identified. This often results in a\nforecasting model trained with all PV power generation data, allowing the\npredictor to learn various patterns through intra-model self-learning, instead\nof constructing a separate predictor for each customer type. In this paper, we\npropose a clustering-based multitasking deep neural network (CM-DNN) framework\nfor PV power generation prediction. K-means is applied to cluster the data into\ndifferent customer types. For each type, a deep neural network (DNN) is\nemployed and trained until the accuracy cannot be improved. Subsequently, for a\nspecified customer type (i.e., the target task), inter-model knowledge transfer\nis conducted to enhance its training accuracy. During this process, source task\nselection is designed to choose the optimal subset of tasks (excluding the\ntarget customer), and each selected source task uses a coefficient to determine\nthe amount of DNN model knowledge (weights and biases) transferred to the aimed\nprediction task. The proposed CM-DNN is tested on a real-world PV power\ngeneration dataset and its superiority is demonstrated by comparing the\nprediction performance on training the dataset with a single model without\nclustering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05989v2",
    "published_date": "2024-05-09 00:08:21 UTC",
    "updated_date": "2024-05-14 00:39:43 UTC"
  }
]
{
  "date": "2024-05-03",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-03 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 论文主要聚焦于 AI 和机器学习的应用创新，特别是大型语言模型（LLM）在推荐系统、代码生成和医疗诊断中的潜力，同时涉及强化学习、机器人和多模态模型的进展。令人印象深刻的文章包括 Christopher Re 参与的 RPA 自动化系统，以及 Matei Zaharia 的自然语言场景生成框架，这些工作展示了 AI 在实际领域的可扩展性和高效性。\n\n### 重点论文讨论\n\n#### 1. **Automating the Enterprise with Foundation Models** (英文标题: Automating the Enterprise with Foundation Models)  \n   作者包括 Christopher Re，这篇论文提出 ECLAIR 系统，利用多模态基础模型（如 GPT-4）实现端到端工作流程自动化。核心贡献是通过案例研究证明 ECLAIR 比传统 RPA 更高效，准确率达 93%，并解决了设置成本高和维护负担大的问题。该方法对企业生产力提升有重要启示，代码已开源。\n\n#### 2. **ScenicNL: Generating Probabilistic Scenario Programs from Natural Language** (英文标题: ScenicNL: Generating Probabilistic Scenario Programs from Natural Language)  \n   作者包括 Matei Zaharia，这篇论文开发了 ScenicNL 系统，用于从自然语言（如警察报告）生成概率场景程序。关键发现是传统 LLM 在处理低资源语言时表现不佳，系统通过 LLM 链和编译器实现语义正确性，在自动驾驶崩溃场景重建中表现出色，发表在 COLM 2024。\n\n#### 3. **PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning** (英文标题: PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning)  \n   这篇论文引入了基于贝叶斯推理的 PICLe 框架，用于从 LLM 中提取特定人格行为。主贡献是优化 ICL 示例选择，提高了模型在多任务上的适应性，实验显示在多个基准上性能提升显著，适合 LLM 个性化应用。\n\n#### 4. **SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery** (英文标题: SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery)  \n   相关于多模态 AI，该论文扩展 SwinMAE 模型，融入时间信息处理卫星图像。关键发现是新架构在土地覆盖分割等任务中比现有模型准确率高 10.4%，展示了在地球观测中的潜力。\n\n#### 5. **Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning** (英文标题: Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning)  \n   这篇论文使用多代理深度强化学习训练机器人足球策略，仅靠第一人称视觉实现端到端控制。主贡献是证明了从模拟到真实世界的转移有效，性能与特权状态相当，结合 Neural Radiance Fields 增强了感知能力。\n\n#### 6. **A Survey of Few-Shot Learning for Biomedical Time Series** (英文标题: A Survey of Few-Shot Learning for Biomedical Time Series)  \n   聚焦医疗 AI，该调查综述了少样本学习在生物医学时间序列中的应用，强调了其在数据稀缺场景下的优势，如早期疾病检测。主要发现是少样本方法能提升个性化医疗，但需解决数据隐私问题。\n\n#### 7. **Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction** (英文标题: Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction)  \n   在医疗成像领域，论文提出基于隐式神经表示的框架，处理多对象 CT 重建。核心贡献是通过潜在变量捕捉共同模式，提高了稀疏视图下的重建质量和鲁棒性，对工业和医疗诊断有实际意义。\n\n其他论文中，涉及 LLM 在代码生成（如第50）和推荐系统（如第17）的创新值得关注，但许多是常规扩展；强化学习论文（如第28）探讨了任务相关策略，但影响较局部；医疗和机器人主题（如第31、75）显示 AI 在诊断中的潜力，但细节较琐碎，故从简提及。总体而言，今天的论文突出了 AI 在跨领域应用的灵活性，但需注意泛化性和伦理问题。",
  "papers": [
    {
      "arxiv_id": "2405.02522v2",
      "title": "New contexts, old heuristics: How young people in India and the US trust online content in the age of generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Rachel Xu",
        "Nhu Le",
        "Rebekah Park",
        "Laura Murray",
        "Vishnupriya Das",
        "Devika Kumar",
        "Beth Goldberg"
      ],
      "abstract": "We conducted in-person ethnography in India and the US to investigate how\nyoung people (18-24) trusted online content, just as generative AI (genAI)\nbecame mainstream. We found that when online, how participants determined what\ncontent to trust was shaped by emotional states, which we term \"information\nmodes.\" Our participants reflexively shifted between modes to maintain\n\"emotional equilibrium,\" and eschewed engaging literacy skills in the more\npassive modes in which they spent the most time. We found participants imported\ntrust heuristics from established online contexts into emerging ones (i.e.,\ngenAI). This led them to use ill-fitting trust heuristics, and exposed them to\nthe risk of trusting false and misleading information. While many had\nreservations about AI, prioritizing efficiency, they used genAI and habitual\nheuristics to quickly achieve goals at the expense of accuracy. We conclude\nthat literacy interventions designed to match users' distinct information modes\nwill be most effective.",
      "tldr_zh": "本研究通过在印度和美国的面对面民族志调查，探讨了18-24岁年轻人在使用generative AI时代对在线内容的信任方式，发现他们的信任决策主要受情绪状态（information modes）影响，并通过切换模式维持emotional equilibrium。参与者往往将旧的trust heuristics从传统在线环境导入到genAI等新兴场景，导致使用不合适的启发式策略，并增加信任虚假信息的风险。尽管对AI有保留意见，他们仍优先效率而牺牲准确性，使用genAI快速实现目标。研究结论认为，针对不同information modes设计的扫盲干预将更有效。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.HC",
      "comment": "27 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.02522v2",
      "published_date": "2024-05-03 23:27:11 UTC",
      "updated_date": "2024-10-08 00:35:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:36:04.729906"
    },
    {
      "arxiv_id": "2405.03710v1",
      "title": "Automating the Enterprise with Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Wornow",
        "Avanika Narayan",
        "Krista Opsahl-Ong",
        "Quinn McIntyre",
        "Nigam H. Shah",
        "Christopher Re"
      ],
      "abstract": "Automating enterprise workflows could unlock $4 trillion/year in productivity\ngains. Despite being of interest to the data management community for decades,\nthe ultimate vision of end-to-end workflow automation has remained elusive.\nCurrent solutions rely on process mining and robotic process automation (RPA),\nin which a bot is hard-coded to follow a set of predefined rules for completing\na workflow. Through case studies of a hospital and large B2B enterprise, we\nfind that the adoption of RPA has been inhibited by high set-up costs (12-18\nmonths), unreliable execution (60% initial accuracy), and burdensome\nmaintenance (requiring multiple FTEs). Multimodal foundation models (FMs) such\nas GPT-4 offer a promising new approach for end-to-end workflow automation\ngiven their generalized reasoning and planning abilities. To study these\ncapabilities we propose ECLAIR, a system to automate enterprise workflows with\nminimal human supervision. We conduct initial experiments showing that\nmultimodal FMs can address the limitations of traditional RPA with (1)\nnear-human-level understanding of workflows (93% accuracy on a workflow\nunderstanding task) and (2) instant set-up with minimal technical barrier\n(based solely on a natural language description of a workflow, ECLAIR achieves\nend-to-end completion rates of 40%). We identify human-AI collaboration,\nvalidation, and self-improvement as open challenges, and suggest ways they can\nbe solved with data management techniques. Code is available at:\nhttps://github.com/HazyResearch/eclair-agents",
      "tldr_zh": "该研究探讨了使用基础模型 (Foundation Models) 自动化企业工作流，以解锁每年 4 万亿美元的生产力收益。传统方法如机器人过程自动化 (RPA) 面临高设置成本 (12-18 个月)、低初始准确率 (60%) 和高维护负担的问题。论文提出 ECLAIR 系统，利用多模态 FMs 的推理和规划能力，实现近人类水平的工作流理解 (93% 准确率) 和基于自然语言描述的快速端到端完成 (40%)。此外，研究识别了人类-AI 协作、验证和自我改进的挑战，并建议通过数据管理技术加以解决。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.03710v1",
      "published_date": "2024-05-03 23:25:15 UTC",
      "updated_date": "2024-05-03 23:25:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:36:16.760093"
    },
    {
      "arxiv_id": "2405.03709v3",
      "title": "ScenicNL: Generating Probabilistic Scenario Programs from Natural Language",
      "title_zh": "ScenicNL：从自然语言生成概率场景程序",
      "authors": [
        "Karim Elmaaroufi",
        "Devan Shanker",
        "Ana Cismaru",
        "Marcell Vazquez-Chanlatte",
        "Alberto Sangiovanni-Vincentelli",
        "Matei Zaharia",
        "Sanjit A. Seshia"
      ],
      "abstract": "For cyber-physical systems (CPS), including robotics and autonomous vehicles,\nmass deployment has been hindered by fatal errors that occur when operating in\nrare events. To replicate rare events such as vehicle crashes, many companies\nhave created logging systems and employed crash reconstruction experts to\nmeticulously recreate these valuable events in simulation. However, in these\nmethods, \"what if\" questions are not easily formulated and answered. We present\nScenarioNL, an AI System for creating scenario programs from natural language.\nSpecifically, we generate these programs from police crash reports. Reports\nnormally contain uncertainty about the exact details of the incidents which we\nrepresent through a Probabilistic Programming Language (PPL), Scenic. By using\nScenic, we can clearly and concisely represent uncertainty and variation over\nCPS behaviors, properties, and interactions. We demonstrate how commonplace\nprompting techniques with the best Large Language Models (LLM) are incapable of\nreasoning about probabilistic scenario programs and generating code for\nlow-resource languages such as Scenic. Our system is comprised of several LLMs\nchained together with several kinds of prompting strategies, a compiler, and a\nsimulator. We evaluate our system on publicly available autonomous vehicle\ncrash reports in California from the last five years and share insights into\nhow we generate code that is both semantically meaningful and syntactically\ncorrect.",
      "tldr_zh": "这篇论文介绍了 ScenicNL 系统，用于从自然语言（如警察事故报告）生成概率场景程序，以帮助 cyber-physical systems (CPS) 如机器人和自动驾驶车辆重现和分析罕见事件的不确定性。系统通过链式 Large Language Models (LLM) 结合多种提示策略、编译器和模拟器，将自然语言转化为 Probabilistic Programming Language (PPL) Scenic 的程序代码，从而清晰表示行为和交互的不确定性。实验在加州过去五年的公开自动驾驶车辆事故报告上进行，结果表明 ScenicNL 能生成语义上正确和语法正确的代码，而传统 LLM 提示技术无法有效处理此类低资源语言的任务。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "22 pages, 3 figures. Published at COLM 2024.\n  https://ke7.github.io/ScenicNL",
      "pdf_url": "http://arxiv.org/pdf/2405.03709v3",
      "published_date": "2024-05-03 23:06:31 UTC",
      "updated_date": "2024-10-02 22:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:36:29.098900"
    },
    {
      "arxiv_id": "2405.02512v2",
      "title": "SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery",
      "title_zh": "SatSwinMAE：用于多尺度时序卫星图像的高效",
      "authors": [
        "Yohei Nakayama",
        "Jiawei Su",
        "Luis M. Pazos-Outón"
      ],
      "abstract": "Recent advancements in foundation models have significantly impacted various\nfields, including natural language processing, computer vision, and multi-modal\ntasks. One area that stands to benefit greatly is Earth observation, where\nthese models can efficiently process large-scale, unlabeled geospatial data. In\nthis work we extend the SwinMAE model to integrate temporal information for\nsatellite time-series data. The architecture employs a hierarchical 3D Masked\nAutoencoder (MAE) with Video Swin Transformer blocks to effectively capture\nmulti-scale spatio-temporal dependencies in satellite imagery. To enhance\ntransfer learning, we incorporate both encoder and decoder pretrained weights,\nalong with skip connections to preserve scale-specific information. This forms\nan architecture similar to SwinUNet with an additional temporal component. Our\napproach shows significant performance improvements over existing\nstate-of-the-art foundation models for all the evaluated downstream tasks: land\ncover segmentation, building density prediction, flood mapping, wildfire scar\nmapping and multi-temporal crop segmentation. Particularly, in the land cover\nsegmentation task of the PhilEO Bench dataset, it outperforms other geospatial\nfoundation models with a 10.4% higher accuracy.",
      "tldr_zh": "本文提出 SatSwinMAE，一种高效的自动编码器框架，用于处理多尺度卫星时间序列图像，通过扩展 SwinMAE 模型整合了 Video Swin Transformer 块和分层的 3D Masked Autoencoder (MAE)，以捕捉时空依赖，并利用预训练权重和跳跃连接增强迁移学习。相比传统 SwinUNet，该架构添加了时间组件，显著提高了土地覆盖分割、建筑密度预测、洪水映射等下游任务的性能。在 PhilEO Bench 数据集的土地覆盖分割任务中，SatSwinMAE 比其他地理空间基础模型准确率提升了 10.4%。这项工作为大规模无标签地理空间数据的处理提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02512v2",
      "published_date": "2024-05-03 22:55:56 UTC",
      "updated_date": "2024-10-18 08:25:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:36:40.694833"
    },
    {
      "arxiv_id": "2405.02509v2",
      "title": "Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayang Shi",
        "Junyi Zhu",
        "Daniel M. Pelt",
        "K. Joost Batenburg",
        "Matthew B. Blaschko"
      ],
      "abstract": "Computed Tomography (CT) is pivotal in industrial quality control and medical\ndiagnostics. Sparse-view CT, offering reduced ionizing radiation, faces\nchallenges due to its under-sampled nature, leading to ill-posed reconstruction\nproblems. Recent advancements in Implicit Neural Representations (INRs) have\nshown promise in addressing sparse-view CT reconstruction. Recognizing that CT\noften involves scanning similar subjects, we propose a novel approach to\nimprove reconstruction quality through joint reconstruction of multiple objects\nusing INRs. This approach can potentially utilize the advantages of INRs and\nthe common patterns observed across different objects. While current INR joint\nreconstruction techniques primarily focus on speeding up the learning process,\nthey are not specifically tailored to enhance the final reconstruction quality.\nTo address this gap, we introduce a novel INR-based Bayesian framework\nintegrating latent variables to capture the common patterns across multiple\nobjects under joint reconstruction. The common patterns then assist in the\nreconstruction of each object via latent variables, thereby improving the\nindividual reconstruction. Extensive experiments demonstrate that our method\nachieves higher reconstruction quality with sparse views and remains robust to\nnoise in the measurements as indicated by common numerical metrics. The\nobtained latent variables can also serve as network initialization for the new\nobject and speed up the learning process.",
      "tldr_zh": "本论文针对 Sparse-View CT 重建的欠采样问题，提出了一种基于 Implicit Neural Representations (INRs) 的联合重建方法，利用 Bayesian 框架和 latent variables 来捕捉多个对象间的共同模式，从而提升单个对象的重建质量。相比现有技术，该方法不仅专注于加速学习过程，还特别优化了最终重建效果，使其在稀疏视图下更具鲁棒性。实验结果显示，该方法在常见数值指标上实现了更高的重建质量，并对测量噪声保持了强劲的抗干扰能力，同时生成的 latent variables 可作为新对象的网络初始化，以加速后续学习过程。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02509v2",
      "published_date": "2024-05-03 22:50:59 UTC",
      "updated_date": "2024-10-25 14:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:36:51.614696"
    },
    {
      "arxiv_id": "2405.02501v2",
      "title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hyeong Kyu Choi",
        "Yixuan Li"
      ],
      "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are\nencoded with diverse personality traits. This triggers an interesting goal of\neliciting a desired personality trait from the LLM, and probing its behavioral\npreferences. Accordingly, we formalize the persona elicitation task, aiming to\ncustomize LLM behaviors to align with a target persona. We present Persona\nIn-Context Learning (PICLe), a novel persona elicitation framework grounded in\nBayesian inference. At the core, PICLe introduces a new ICL example selection\ncriterion based on likelihood ratio, which is designed to optimally guide the\nmodel in eliciting a specific target persona. We demonstrate the effectiveness\nof PICLe through extensive comparisons against baseline methods across three\ncontemporary LLMs. Code is available at\nhttps://github.com/deeplearning-wisc/picle.",
      "tldr_zh": "本研究探讨如何从大型语言模型(LLMs)中激发多样人格特质，以自定义其行为偏好，并形式化了人格激发任务。作者提出了Persona In-Context Learning (PICLe)框架，该框架基于Bayesian推理，引入了基于似然比的In-Context Learning (ICL)示例选择标准，以优化模型对目标人格的激发。通过与基线方法的广泛比较，PICLe在三个当代LLMs上展示了显著的有效性，代码可从https://github.com/deeplearning-wisc/picle获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02501v2",
      "published_date": "2024-05-03 22:17:22 UTC",
      "updated_date": "2024-05-14 05:53:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:37:03.507457"
    },
    {
      "arxiv_id": "2405.02485v1",
      "title": "A Survey of Few-Shot Learning for Biomedical Time Series",
      "title_zh": "生物医学时间序列的少样本学习调查",
      "authors": [
        "Chenqi Li",
        "Timothy Denison",
        "Tingting Zhu"
      ],
      "abstract": "Advancements in wearable sensor technologies and the digitization of medical\nrecords have contributed to the unprecedented ubiquity of biomedical time\nseries data. Data-driven models have tremendous potential to assist clinical\ndiagnosis and improve patient care by improving long-term monitoring\ncapabilities, facilitating early disease detection and intervention, as well as\npromoting personalized healthcare delivery. However, accessing extensively\nlabeled datasets to train data-hungry deep learning models encounters many\nbarriers, such as long-tail distribution of rare diseases, cost of annotation,\nprivacy and security concerns, data-sharing regulations, and ethical\nconsiderations. An emerging approach to overcome the scarcity of labeled data\nis to augment AI methods with human-like capabilities to leverage past\nexperiences to learn new tasks with limited examples, called few-shot learning.\nThis survey provides a comprehensive review and comparison of few-shot learning\nmethods for biomedical time series applications. The clinical benefits and\nlimitations of such methods are discussed in relation to traditional\ndata-driven approaches. This paper aims to provide insights into the current\nlandscape of few-shot learning for biomedical time series and its implications\nfor future research and applications.",
      "tldr_zh": "这篇调查论文探讨了少样本学习（Few-Shot Learning）在生物医学时间序列（Biomedical Time Series）中的应用，旨在解决传统数据驱动模型因标注数据稀缺而面临的挑战，如稀有疾病的长尾分布、标注成本高昂以及隐私和法规问题。论文回顾并比较了各种少样本学习方法，这些方法通过模拟人类利用过去经验从有限样本中学习，来提升临床诊断、早期疾病检测和个性化医疗的潜力。与传统数据驱动方法相比，该方法具有显著临床益处，但也存在局限性，如泛化能力不足。总体而言，该研究为生物医学时间序列领域的未来研究和应用提供了宝贵洞见和方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2405.02485v1",
      "published_date": "2024-05-03 21:22:27 UTC",
      "updated_date": "2024-05-03 21:22:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:37:16.356994"
    },
    {
      "arxiv_id": "2405.02481v1",
      "title": "Proximal Curriculum with Task Correlations for Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Georgios Tzannetos",
        "Parameswaran Kamalaruban",
        "Adish Singla"
      ],
      "abstract": "Curriculum design for reinforcement learning (RL) can speed up an agent's\nlearning process and help it learn to perform well on complex tasks. However,\nexisting techniques typically require domain-specific hyperparameter tuning,\ninvolve expensive optimization procedures for task selection, or are suitable\nonly for specific learning objectives. In this work, we consider curriculum\ndesign in contextual multi-task settings where the agent's final performance is\nmeasured w.r.t. a target distribution over complex tasks. We base our\ncurriculum design on the Zone of Proximal Development concept, which has proven\nto be effective in accelerating the learning process of RL agents for uniform\ndistribution over all tasks. We propose a novel curriculum, ProCuRL-Target,\nthat effectively balances the need for selecting tasks that are not too\ndifficult for the agent while progressing the agent's learning toward the\ntarget distribution via leveraging task correlations. We theoretically justify\nthe task selection strategy of ProCuRL-Target by analyzing a simple learning\nsetting with REINFORCE learner model. Our experimental results across various\ndomains with challenging target task distributions affirm the effectiveness of\nour curriculum strategy over state-of-the-art baselines in accelerating the\ntraining process of deep RL agents.",
      "tldr_zh": "本文提出了一种名为 ProCuRL-Target 的新型课程设计方法，用于深度强化学习（Deep RL），旨在通过利用任务相关性（Task Correlations）平衡代理的学习难度，同时基于 Zone of Proximal Development 概念加速向目标任务分布的学习过程。不同于现有技术，该方法避免了领域特定的超参数调整和昂贵优化，支持上下文多任务设置，并通过理论分析 REINFORCE 学习模型证明了其任务选择策略的有效性。实验结果显示，ProCuRL-Target 在各种复杂领域中显著提升了 RL 代理的训练效率，比最先进基线快且性能更佳。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IJCAI'24 paper (longer version)",
      "pdf_url": "http://arxiv.org/pdf/2405.02481v1",
      "published_date": "2024-05-03 21:07:54 UTC",
      "updated_date": "2024-05-03 21:07:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:37:27.940049"
    },
    {
      "arxiv_id": "2405.02480v1",
      "title": "A Network Simulation of OTC Markets with Multiple Agents",
      "title_zh": "翻译失败",
      "authors": [
        "James T. Wilkinson",
        "Jacob Kelter",
        "John Chen",
        "Uri Wilensky"
      ],
      "abstract": "We present a novel agent-based approach to simulating an over-the-counter\n(OTC) financial market in which trades are intermediated solely by market\nmakers and agent visibility is constrained to a network topology. Dynamics,\nsuch as changes in price, result from agent-level interactions that\nubiquitously occur via market maker agents acting as liquidity providers. Two\nadditional agents are considered: trend investors use a deep convolutional\nneural network paired with a deep Q-learning framework to inform trading\ndecisions by analysing price history; and value investors use a static\nprice-target to determine their trade directions and sizes. We demonstrate that\nour novel inclusion of a network topology with market makers facilitates\nexplorations into various market structures. First, we present the model and an\noverview of its mechanics. Second, we validate our findings via comparison to\nthe real-world: we demonstrate a fat-tailed distribution of price changes,\nauto-correlated volatility, a skew negatively correlated to market maker\npositioning, predictable price-history patterns and more. Finally, we\ndemonstrate that our network-based model can lend insights into the effect of\nmarket-structure on price-action. For example, we show that markets with\nsparsely connected intermediaries can have a critical point of fragmentation,\nbeyond which the market forms distinct clusters and arbitrage becomes rapidly\npossible between the prices of different market makers. A discussion is\nprovided on future work that would be beneficial.",
      "tldr_zh": "该论文提出了一种基于多代理的网络模拟方法，用于模拟场外（OTC）金融市场，其中交易由市场制造商（market makers）中介，代理的可见性受网络拓扑约束。模型包括趋势投资者（trend investors），他们利用深度卷积神经网络（deep convolutional neural network）和深度 Q 学习（deep Q-learning）分析价格历史，以及价值投资者（value investors）基于静态价格目标决定交易策略。通过实验验证，该模拟再现了真实市场的特征，如价格变化的肥尾分布（fat-tailed distribution）、波动率自相关（auto-correlated volatility）和偏斜与市场制造商定位的负相关。此外，研究揭示了市场结构的影响，例如稀疏连接的中间商可能导致市场碎片化临界点，并产生套利机会。",
      "categories": [
        "econ.EM",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "econ.EM",
      "comment": "20 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02480v1",
      "published_date": "2024-05-03 20:45:00 UTC",
      "updated_date": "2024-05-03 20:45:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:37:41.832839"
    },
    {
      "arxiv_id": "2405.02475v2",
      "title": "Generalizing Orthogonalization for Models with Non-Linearities",
      "title_zh": "翻译失败",
      "authors": [
        "David Rügamer",
        "Chris Kolb",
        "Tobias Weber",
        "Lucas Kook",
        "Thomas Nagler"
      ],
      "abstract": "The complexity of black-box algorithms can lead to various challenges,\nincluding the introduction of biases. These biases present immediate risks in\nthe algorithms' application. It was, for instance, shown that neural networks\ncan deduce racial information solely from a patient's X-ray scan, a task beyond\nthe capability of medical experts. If this fact is not known to the medical\nexpert, automatic decision-making based on this algorithm could lead to\nprescribing a treatment (purely) based on racial information. While current\nmethodologies allow for the \"orthogonalization\" or \"normalization\" of neural\nnetworks with respect to such information, existing approaches are grounded in\nlinear models. Our paper advances the discourse by introducing corrections for\nnon-linearities such as ReLU activations. Our approach also encompasses scalar\nand tensor-valued predictions, facilitating its integration into neural network\narchitectures. Through extensive experiments, we validate our method's\neffectiveness in safeguarding sensitive data in generalized linear models,\nnormalizing convolutional neural networks for metadata, and rectifying\npre-existing embeddings for undesired attributes.",
      "tldr_zh": "该论文探讨了黑盒算法引入偏见的问题，例如神经网络从X-ray扫描中推断种族信息，可能导致基于敏感属性的自动决策。该研究扩展了orthogonalization（正交化）方法，针对非线性模型（如ReLU激活）引入修正，并支持标量和张量值预测，以整合到神经网络架构中。通过广泛实验，验证了该方法的有效性，包括在广义线性模型中保护敏感数据、在卷积神经网络中归一化元数据，以及修正现有嵌入以去除不想要的属性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02475v2",
      "published_date": "2024-05-03 20:25:57 UTC",
      "updated_date": "2024-06-02 13:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:37:52.104398"
    },
    {
      "arxiv_id": "2405.02463v1",
      "title": "Knowledge Graph Extension by Entity Type Recognition",
      "title_zh": "通过实体类型识别的知识图谱扩展",
      "authors": [
        "Daqian Shi"
      ],
      "abstract": "Knowledge graphs have emerged as a sophisticated advancement and refinement\nof semantic networks, and their deployment is one of the critical methodologies\nin contemporary artificial intelligence. The construction of knowledge graphs\nis a multifaceted process involving various techniques, where researchers aim\nto extract the knowledge from existing resources for the construction since\nbuilding from scratch entails significant labor and time costs. However, due to\nthe pervasive issue of heterogeneity, the description diversity across\ndifferent knowledge graphs can lead to mismatches between concepts, thereby\nimpacting the efficacy of knowledge extraction. This Ph.D. study focuses on\nautomatic knowledge graph extension, i.e., properly extending the reference\nknowledge graph by extracting and integrating concepts from one or more\ncandidate knowledge graphs. We propose a novel knowledge graph extension\nframework based on entity type recognition. The framework aims to achieve\nhigh-quality knowledge extraction by aligning the schemas and entities across\ndifferent knowledge graphs, thereby enhancing the performance of the extension.\nThis paper elucidates three major contributions: (i) we propose an entity type\nrecognition method exploiting machine learning and property-based similarities\nto enhance knowledge extraction; (ii) we introduce a set of assessment metrics\nto validate the quality of the extended knowledge graphs; (iii) we develop a\nplatform for knowledge graph acquisition, management, and extension to benefit\nknowledge engineers practically. Our evaluation comprehensively demonstrated\nthe feasibility and effectiveness of the proposed extension framework and its\nfunctionalities through quantitative experiments and case studies.",
      "tldr_zh": "这篇论文聚焦于通过实体类型 recognition 自动扩展 knowledge graphs，以解决不同图谱间的异质性问题和概念不匹配。研究提出一个新型框架，利用 machine learning 和基于属性的相似性进行实体类型识别，并对 schemas 和 entities 进行对齐，以提升知识提取质量。主要贡献包括：(i) 一种改进的实体类型识别方法，(ii) 一套评估指标用于验证扩展图谱的质量，以及(iii) 一个平台支持 knowledge graphs 的获取、管理和扩展。通过定量实验和案例研究，证明了该框架的有效性和实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "PhD thesis",
      "pdf_url": "http://arxiv.org/pdf/2405.02463v1",
      "published_date": "2024-05-03 19:55:03 UTC",
      "updated_date": "2024-05-03 19:55:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:38:05.511914"
    },
    {
      "arxiv_id": "2405.02458v1",
      "title": "Controlled Query Evaluation through Epistemic Dependencies",
      "title_zh": "翻译失败",
      "authors": [
        "Gianluca Cima",
        "Domenico Lembo",
        "Lorenzo Marconi",
        "Riccardo Rosati",
        "Domenico Fabio Savo"
      ],
      "abstract": "In this paper, we propose the use of epistemic dependencies to express data\nprotection policies in Controlled Query Evaluation (CQE), which is a form of\nconfidentiality-preserving query answering over ontologies and databases. The\nresulting policy language goes significantly beyond those proposed in the\nliterature on CQE so far, allowing for very rich and practically interesting\nforms of data protection rules. We show the expressive abilities of our\nframework and study the data complexity of CQE for (unions of) conjunctive\nqueries when ontologies are specified in the Description Logic DL-Lite_R.\nInterestingly, while we show that the problem is in general intractable, we\nprove tractability for the case of acyclic epistemic dependencies by providing\na suitable query rewriting algorithm. The latter result paves the way towards\nthe implementation and practical application of this new approach to CQE.",
      "tldr_zh": "本研究提出使用 epistemic dependencies 来表达数据保护策略，从而增强 Controlled Query Evaluation (CQE) 在本体和数据库上的保密查询回答能力。该框架的策略语言比现有文献更丰富，能支持复杂的数据保护规则，并展示了其在表达方面的强大能力。在 Description Logic DL-Lite_R 中，论文分析了（unions of）conjunctive queries 的 CQE 数据复杂性，发现该问题一般是 intractable 的，但对于 acyclic epistemic dependencies，通过一个合适的 query rewriting algorithm 证明了其 tractability。这种方法为 CQE 的实际实现和应用提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02458v1",
      "published_date": "2024-05-03 19:48:07 UTC",
      "updated_date": "2024-05-03 19:48:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:38:15.725948"
    },
    {
      "arxiv_id": "2405.02454v1",
      "title": "What is Sentiment Meant to Mean to Language Models?",
      "title_zh": "情感在语言模型中到底意味着什么？",
      "authors": [
        "Michael Burnham"
      ],
      "abstract": "Sentiment analysis is one of the most widely used techniques in text\nanalysis. Recent advancements with Large Language Models have made it more\naccurate and accessible than ever, allowing researchers to classify text with\nonly a plain English prompt. However, \"sentiment\" entails a wide variety of\nconcepts depending on the domain and tools used. It has been used to mean\nemotion, opinions, market movements, or simply a general ``good-bad''\ndimension. This raises a question: What exactly are language models doing when\nprompted to label documents by sentiment? This paper first overviews how\nsentiment is defined across different contexts, highlighting that it is a\nconfounded measurement construct in that it entails multiple variables, such as\nemotional valence and opinion, without disentangling them. I then test three\nlanguage models across two data sets with prompts requesting sentiment,\nvalence, and stance classification. I find that sentiment labels most strongly\ncorrelate with valence labels. I further find that classification improves when\nresearchers more precisely specify their dimension of interest rather than\nusing the less well-defined concept of sentiment. I conclude by encouraging\nresearchers to move beyond \"sentiment\" when feasible and use a more precise\nmeasurement construct.",
      "tldr_zh": "这篇论文探讨了“sentiment”（情感）一词在语言模型中的模糊含义，指出它可能指代情绪、意见、市场波动或简单的好坏维度，从而成为一个混杂的测量结构（confounded measurement construct）。作者测试了三个语言模型，在两个数据集上使用提示进行sentiment、valence（情感价性）和stance（立场）分类，结果发现sentiment标签最强烈地与valence标签相关。进一步实验显示，当研究者更精确地指定感兴趣的维度时，分类性能得到改善。论文结论呼吁研究者避免使用“sentiment”，改而采用更精确的测量结构，以提升分析的准确性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02454v1",
      "published_date": "2024-05-03 19:37:37 UTC",
      "updated_date": "2024-05-03 19:37:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:38:29.531090"
    },
    {
      "arxiv_id": "2406.18536v1",
      "title": "Reliable Interval Prediction of Minimum Operating Voltage Based on On-chip Monitors via Conformalized Quantile Regression",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Yin",
        "Xiaoxiao Wang",
        "Rebecca Chen",
        "Chen He",
        "Peng Li"
      ],
      "abstract": "Predicting the minimum operating voltage ($V_{min}$) of chips is one of the\nimportant techniques for improving the manufacturing testing flow, as well as\nensuring the long-term reliability and safety of in-field systems. Current\n$V_{min}$ prediction methods often provide only point estimates, necessitating\nadditional techniques for constructing prediction confidence intervals to cover\nuncertainties caused by different sources of variations. While some existing\ntechniques offer region predictions, but they rely on certain distributional\nassumptions and/or provide no coverage guarantees. In response to these\nlimitations, we propose a novel distribution-free $V_{min}$ interval estimation\nmethodology possessing a theoretical guarantee of coverage. Our approach\nleverages conformalized quantile regression and on-chip monitors to generate\nreliable prediction intervals. We demonstrate the effectiveness of the proposed\nmethod on an industrial 5nm automotive chip dataset. Moreover, we show that the\nuse of on-chip monitors can reduce the interval length significantly for\n$V_{min}$ prediction.",
      "tldr_zh": "本文提出了一种基于 Conformalized Quantile Regression 和 On-chip Monitors 的新方法，用于可靠预测芯片的最小操作电压 ($V_{min}$) 的区间估计，该方法无需分布假设并提供理论上的覆盖保证，解决了现有技术在不确定性处理上的局限性。实验在工业 5nm 汽车芯片数据集上验证了该方法的有效性，结果显示，使用 On-chip Monitors 能显著减少预测区间的长度，从而提升了 $V_{min}$ 预测的准确性和实用性。整体贡献为改善制造测试流程并增强芯片的长期可靠性和安全性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.AR",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted by DATE 2024. Camera-ready version",
      "pdf_url": "http://arxiv.org/pdf/2406.18536v1",
      "published_date": "2024-05-03 19:34:47 UTC",
      "updated_date": "2024-05-03 19:34:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:38:40.777906"
    },
    {
      "arxiv_id": "2406.02566v2",
      "title": "Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Ognjen Kundacina",
        "Vladimir Vincan",
        "Dragisa Miskovic"
      ],
      "abstract": "This paper introduces a novel two-stage active learning (AL) pipeline for\nautomatic speech recognition (ASR), combining unsupervised and supervised AL\nmethods. The first stage utilizes unsupervised AL by using x-vectors clustering\nfor diverse sample selection from unlabeled speech data, thus establishing a\nrobust initial dataset for the subsequent supervised AL. The second stage\nincorporates a supervised AL strategy, with a batch AL method specifically\ndeveloped for ASR, aimed at selecting diverse and informative batches of\nsamples. Here, sample diversity is also achieved using x-vectors clustering,\nwhile the most informative samples are identified using a Bayesian AL method\ntailored for ASR with an adaptation of Monte Carlo dropout to approximate\nBayesian inference. This approach enables precise uncertainty estimation,\nthereby enhancing ASR model training with significantly reduced data\nrequirements. Our method has shown superior performance compared to competing\nmethods on homogeneous, heterogeneous, and OOD test sets, demonstrating that\nstrategic sample selection and innovative Bayesian modeling can substantially\noptimize both labeling effort and data utilization in deep learning-based ASR\napplications.",
      "tldr_zh": "这篇论文提出了一种结合 x-vectors 和 Bayesian Batch Active Learning 的两阶段主动学习（Active Learning）管道，用于自动语音识别（ASR）。第一阶段采用无监督 AL，通过 x-vectors 聚类从未标注语音数据中选择多样样本，建立初始数据集；第二阶段则使用监督 AL，结合 x-vectors 聚类和适应 Monte Carlo dropout 的 Bayesian 方法，选择信息丰富且多样的样本批次，以精确估计不确定性并减少数据需求。实验结果显示，该方法在同质、异质和 OOD 测试集上优于竞争方法，显著优化了标注努力和 ASR 模型的训练效率。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.02566v2",
      "published_date": "2024-05-03 19:24:41 UTC",
      "updated_date": "2025-04-25 06:24:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:38:53.314711"
    },
    {
      "arxiv_id": "2405.15789v1",
      "title": "Semantic Objective Functions: A distribution-aware method for adding logical constraints in deep learning",
      "title_zh": "语义目标函数：一种分布感知方法，用于在深度学习中添加逻辑约束",
      "authors": [
        "Miguel Angel Mendez-Lucero",
        "Enrique Bojorquez Gallardo",
        "Vaishak Belle"
      ],
      "abstract": "Issues of safety, explainability, and efficiency are of increasing concern in\nlearning systems deployed with hard and soft constraints. Symbolic Constrained\nLearning and Knowledge Distillation techniques have shown promising results in\nthis area, by embedding and extracting knowledge, as well as providing logical\nconstraints during neural network training. Although many frameworks exist to\ndate, through an integration of logic and information geometry, we provide a\nconstruction and theoretical framework for these tasks that generalize many\napproaches. We propose a loss-based method that embeds knowledge-enforces\nlogical constraints-into a machine learning model that outputs probability\ndistributions. This is done by constructing a distribution from the external\nknowledge/logic formula and constructing a loss function as a linear\ncombination of the original loss function with the Fisher-Rao distance or\nKullback-Leibler divergence to the constraint distribution. This construction\nincludes logical constraints in the form of propositional formulas (Boolean\nvariables), formulas of a first-order language with finite variables over a\nmodel with compact domain (categorical and continuous variables), and in\ngeneral, likely applicable to any statistical model that was pretrained with\nsemantic information. We evaluate our method on a variety of learning tasks,\nincluding classification tasks with logic constraints, transferring knowledge\nfrom logic formulas, and knowledge distillation from general distributions.",
      "tldr_zh": "该论文提出了一种名为 Semantic Objective Functions 的方法，通过整合逻辑和信息几何，提供一个通用的框架，用于在深度学习中添加逻辑约束，以提升模型的安全性、解释性和效率。该方法构建一个从外部知识或逻辑公式派生的分布，并将原损失函数与 Fisher-Rao 距离或 Kullback-Leibler 散度线性组合，形成新的损失函数，从而嵌入逻辑约束，如命题公式或一阶逻辑公式。该方法适用于各种预训练统计模型，并在分类任务、知识转移和知识蒸馏等学习任务上进行了评估，展示了其在处理逻辑约束方面的有效性。",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.LO",
        "math.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages,4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15789v1",
      "published_date": "2024-05-03 19:21:47 UTC",
      "updated_date": "2024-05-03 19:21:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:39:05.651914"
    },
    {
      "arxiv_id": "2405.02429v2",
      "title": "CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Yaoyiran Li",
        "Xiang Zhai",
        "Moustafa Alzantot",
        "Keyi Yu",
        "Ivan Vulić",
        "Anna Korhonen",
        "Mohamed Hammad"
      ],
      "abstract": "Traditional recommender systems such as matrix factorization methods have\nprimarily focused on learning a shared dense embedding space to represent both\nitems and user preferences. Subsequently, sequence models such as RNN, GRUs,\nand, recently, Transformers have emerged and excelled in the task of sequential\nrecommendation. This task requires understanding the sequential structure\npresent in users' historical interactions to predict the next item they may\nlike. Building upon the success of Large Language Models (LLMs) in a variety of\ntasks, researchers have recently explored using LLMs that are pretrained on\nvast corpora of text for sequential recommendation. To use LLMs for sequential\nrecommendation, both the history of user interactions and the model's\nprediction of the next item are expressed in text form. We propose CALRec, a\ntwo-stage LLM finetuning framework that finetunes a pretrained LLM in a\ntwo-tower fashion using a mixture of two contrastive losses and a language\nmodeling loss: the LLM is first finetuned on a data mixture from multiple\ndomains followed by another round of target domain finetuning. Our model\nsignificantly outperforms many state-of-the-art baselines (+37% in Recall@1 and\n+24% in NDCG@10) and our systematic ablation studies reveal that (i) both\nstages of finetuning are crucial, and, when combined, we achieve improved\nperformance, and (ii) contrastive alignment is effective among the target\ndomains explored in our experiments.",
      "tldr_zh": "本文提出 CALRec，一种对比对齐（Contrastive Alignment）的框架，用于将生成式大型语言模型（LLMs）应用于顺序推荐（Sequential Recommendation）。该框架采用两阶段微调策略：首先在多领域混合数据上使用两种对比损失和语言建模损失进行初步微调，然后在目标领域进行进一步微调，以提升模型对用户历史交互序列的理解。实验结果显示，CALRec 显著优于现有基线模型，提升 Recall@1 37% 和 NDCG@10 24%。此外，消融研究证实了两个微调阶段的必要性和对比对齐的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "RecSys 2024 (Long Paper)",
      "pdf_url": "http://arxiv.org/pdf/2405.02429v2",
      "published_date": "2024-05-03 18:51:19 UTC",
      "updated_date": "2024-08-23 20:46:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:39:17.829904"
    },
    {
      "arxiv_id": "2405.02425v1",
      "title": "Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Dhruva Tirumala",
        "Markus Wulfmeier",
        "Ben Moran",
        "Sandy Huang",
        "Jan Humplik",
        "Guy Lever",
        "Tuomas Haarnoja",
        "Leonard Hasenclever",
        "Arunkumar Byravan",
        "Nathan Batchelor",
        "Neil Sreendra",
        "Kushal Patel",
        "Marlon Gwira",
        "Francesco Nori",
        "Martin Riedmiller",
        "Nicolas Heess"
      ],
      "abstract": "We apply multi-agent deep reinforcement learning (RL) to train end-to-end\nrobot soccer policies with fully onboard computation and sensing via egocentric\nRGB vision. This setting reflects many challenges of real-world robotics,\nincluding active perception, agile full-body control, and long-horizon planning\nin a dynamic, partially-observable, multi-agent domain. We rely on large-scale,\nsimulation-based data generation to obtain complex behaviors from egocentric\nvision which can be successfully transferred to physical robots using low-cost\nsensors. To achieve adequate visual realism, our simulation combines rigid-body\nphysics with learned, realistic rendering via multiple Neural Radiance Fields\n(NeRFs). We combine teacher-based multi-agent RL and cross-experiment data\nreuse to enable the discovery of sophisticated soccer strategies. We analyze\nactive-perception behaviors including object tracking and ball seeking that\nemerge when simply optimizing perception-agnostic soccer play. The agents\ndisplay equivalent levels of performance and agility as policies with access to\nprivileged, ground-truth state. To our knowledge, this paper constitutes a\nfirst demonstration of end-to-end training for multi-agent robot soccer,\nmapping raw pixel observations to joint-level actions, that can be deployed in\nthe real world. Videos of the game-play and analyses can be seen on our website\nhttps://sites.google.com/view/vision-soccer .",
      "tldr_zh": "本研究使用多智能体深度强化学习（deep reinforcement learning）从第一人称视角（egocentric vision）的RGB视觉训练端到端机器人足球策略，实现完全机载计算和感知。方法结合大规模模拟数据生成、刚体物理和Neural Radiance Fields (NeRFs)渲染，以及基于教师的RL和跨实验数据重用，以应对主动感知、全身控制和长期规划等挑战。结果显示，代理自然出现物体跟踪和球寻找等行为，其性能和敏捷性与访问特权状态的策略相当，并成功转移到真实机器人，实现了从原始像素到关节级动作的部署。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02425v1",
      "published_date": "2024-05-03 18:41:13 UTC",
      "updated_date": "2024-05-03 18:41:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:39:29.750964"
    },
    {
      "arxiv_id": "2405.02413v2",
      "title": "A Unified Framework for Human-Allied Learning of Probabilistic Circuits",
      "title_zh": "翻译失败",
      "authors": [
        "Athresh Karanam",
        "Saurabh Mathur",
        "Sahil Sidheekh",
        "Sriraam Natarajan"
      ],
      "abstract": "Probabilistic Circuits (PCs) have emerged as an efficient framework for\nrepresenting and learning complex probability distributions. Nevertheless, the\nexisting body of research on PCs predominantly concentrates on data-driven\nparameter learning, often neglecting the potential of knowledge-intensive\nlearning, a particular issue in data-scarce/knowledge-rich domains such as\nhealthcare. To bridge this gap, we propose a novel unified framework that can\nsystematically integrate diverse domain knowledge into the parameter learning\nprocess of PCs. Experiments on several benchmarks as well as real world\ndatasets show that our proposed framework can both effectively and efficiently\nleverage domain knowledge to achieve superior performance compared to purely\ndata-driven learning approaches.",
      "tldr_zh": "Probabilistic Circuits (PCs) 是一种高效框架，用于表示和学习复杂概率分布，但现有研究主要关注数据驱动的参数学习，而忽略了知识密集型学习，尤其在数据稀缺的领域如医疗。论文提出一个新的统一框架，能够系统地将多样领域知识整合到 PCs 的参数学习过程中，从而提升模型的性能。该框架在多个基准和真实数据集上的实验中，证明了其有效性和效率，比纯数据驱动方法取得了更优异的表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02413v2",
      "published_date": "2024-05-03 18:14:29 UTC",
      "updated_date": "2024-12-18 19:02:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:39:39.638949"
    },
    {
      "arxiv_id": "2405.02287v1",
      "title": "Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models",
      "title_zh": "Vibe-Eval：一种用于测量多模态语言模型进展的",
      "authors": [
        "Piotr Padlewski",
        "Max Bain",
        "Matthew Henderson",
        "Zhongkai Zhu",
        "Nishant Relan",
        "Hai Pham",
        "Donovan Ong",
        "Kaloyan Aleksiev",
        "Aitor Ormazabal",
        "Samuel Phua",
        "Ethan Yeo",
        "Eugenie Lamprecht",
        "Qi Liu",
        "Yuqi Wang",
        "Eric Chen",
        "Deyu Fu",
        "Lei Li",
        "Che Zheng",
        "Cyprien de Masson d'Autume",
        "Dani Yogatama",
        "Mikel Artetxe",
        "Yi Tay"
      ],
      "abstract": "We introduce Vibe-Eval: a new open benchmark and framework for evaluating\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\nincluding 100 of hard difficulty, complete with gold-standard responses\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\n(ii) rigorously testing and probing the capabilities of present frontier\nmodels. Notably, our hard set contains >50% questions that all frontier models\nanswer incorrectly. We explore the nuances of designing, evaluating, and\nranking models on ultra challenging prompts. We also discuss trade-offs between\nhuman and automatic evaluation, and show that automatic model evaluation using\nReka Core roughly correlates to human judgment. We offer free API access for\nthe purpose of lightweight evaluation and plan to conduct formal human\nevaluations for public models that perform well on the Vibe-Eval's automatic\nscores. We release the evaluation code and data, see\nhttps://github.com/reka-ai/reka-vibe-eval",
      "tldr_zh": "本研究引入了 Vibe-Eval，一种新的开源基准和框架，用于评估多模态语言模型（multimodal language models）的进展。它包含 269 个视觉理解提示，其中 100 个为困难级别，并配有专家编写的金标准响应，旨在测试模型在日常任务中的表现和前沿模型的极限能力。实验发现，硬提示中超过 50% 的问题连前沿模型都答错，同时使用 Reka Core 的自动评估与人类判断高度相关。研究者提供免费 API 访问和开源代码（https://github.com/reka-ai/reka-vibe-eval），以促进进一步的模型评估和优化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02287v1",
      "published_date": "2024-05-03 17:59:55 UTC",
      "updated_date": "2024-05-03 17:59:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:39:53.376835"
    },
    {
      "arxiv_id": "2405.02246v1",
      "title": "What matters when building vision-language models?",
      "title_zh": "翻译失败",
      "authors": [
        "Hugo Laurençon",
        "Léo Tronchon",
        "Matthieu Cord",
        "Victor Sanh"
      ],
      "abstract": "The growing interest in vision-language models (VLMs) has been driven by\nimprovements in large language models and vision transformers. Despite the\nabundance of literature on this subject, we observe that critical decisions\nregarding the design of VLMs are often not justified. We argue that these\nunsupported decisions impede progress in the field by making it difficult to\nidentify which choices improve model performance. To address this issue, we\nconduct extensive experiments around pre-trained models, architecture choice,\ndata, and training methods. Our consolidation of findings includes the\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\nIdefics2 achieves state-of-the-art performance within its size category across\nvarious multimodal benchmarks, and is often on par with models four times its\nsize. We release the model (base, instructed, and chat) along with the datasets\ncreated for its training.",
      "tldr_zh": "该研究探讨了构建视觉语言模型（VLMs）的关键因素，指出现有文献中对设计决策缺乏充分论证，导致领域进展受阻。通过广泛实验评估预训练模型、架构选择、数据和训练方法，论文总结了这些要素对模型性能的影响。研究开发了Idefics2，一个高效的8亿参数基础VLM，该模型在各种多模态基准测试中实现了同类别最先进性能，并与四倍大小的模型相当。最终，作者发布了Idefics2的多种版本（base、instructed和chat）及其训练数据集，以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02246v1",
      "published_date": "2024-05-03 17:00:00 UTC",
      "updated_date": "2024-05-03 17:00:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:40:05.355982"
    },
    {
      "arxiv_id": "2405.02228v3",
      "title": "Attribution in Scientific Literature: New Benchmark and Methods",
      "title_zh": "科学文献中的归因：新基准和方法",
      "authors": [
        "Yash Saxena",
        "Deepa Tilwani",
        "Ali Mohammadi",
        "Edward Raff",
        "Amit Sheth",
        "Srinivasan Parthasarathy",
        "Manas Gaur"
      ],
      "abstract": "Large language models (LLMs) present a promising yet challenging frontier for\nautomated source citation in scientific communication. Previous approaches to\ncitation generation have been limited by citation ambiguity and LLM\novergeneralization. We introduce REASONS, a novel dataset with sentence-level\nannotations across 12 scientific domains from arXiv. Our evaluation framework\ncovers two key citation scenarios: indirect queries (matching sentences to\npaper titles) and direct queries (author attribution), both enhanced with\ncontextual metadata. We conduct extensive experiments with models such as\nGPT-O1, GPT-4O, GPT-3.5, DeepSeek, and other smaller models like Perplexity AI\n(7B). While top-tier LLMs achieve high performance in sentence attribution,\nthey struggle with high hallucination rates, a key metric for scientific\nreliability. Our metadata-augmented approach reduces hallucination rates across\nall tasks, offering a promising direction for improvement. Retrieval-augmented\ngeneration (RAG) with Mistral improves performance in indirect queries,\nreducing hallucination rates by 42% and maintaining competitive precision with\nlarger models. However, adversarial testing highlights challenges in linking\npaper titles to abstracts, revealing fundamental limitations in current LLMs.\nREASONS provides a challenging benchmark for developing reliable and\ntrustworthy LLMs in scientific applications",
      "tldr_zh": "这篇论文引入了REASONS数据集，一个包含12个科学领域的句子级注解，用于评估LLMs在科学文献归属中的性能，涵盖间接查询（匹配句子到论文标题）和直接查询（作者归属），并整合上下文元数据。研究通过实验测试了多种模型如GPT-O1、GPT-4O和DeepSeek，发现顶级LLMs在句子归属上表现出色，但存在高幻觉率问题。元数据增强方法和RAG（Retrieval-augmented generation）结合Mistral模型显著降低了幻觉率42%，提高了间接查询的精度；然而，对抗测试揭示了LLMs在链接论文标题和摘要时的根本局限性。REASONS作为一个新基准，为开发可靠的科学应用LLMs提供了重要指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2405.02228v3",
      "published_date": "2024-05-03 16:38:51 UTC",
      "updated_date": "2025-04-11 07:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:40:19.380112"
    },
    {
      "arxiv_id": "2405.02225v1",
      "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
      "title_zh": "翻译失败",
      "authors": [
        "Lujing Zhang",
        "Aaron Roth",
        "Linjun Zhang"
      ],
      "abstract": "This paper introduces a framework for post-processing machine learning models\nso that their predictions satisfy multi-group fairness guarantees. Based on the\ncelebrated notion of multicalibration, we introduce $(\\mathbf{s},\\mathcal{G},\n\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\nmulti-dimensional mappings $\\mathbf{s}$, constraint set $\\mathcal{G}$, and a\npre-specified threshold level $\\alpha$. We propose associated algorithms to\nachieve this notion in general settings. This framework is then applied to\ndiverse scenarios encompassing different fairness concerns, including false\nnegative rate control in image segmentation, prediction set conditional\nuncertainty quantification in hierarchical classification, and de-biased text\ngeneration in language models. We conduct numerical studies on several datasets\nand tasks.",
      "tldr_zh": "这篇论文提出了一种后处理机器学习模型的通用框架，用于校准多群组公平性风险，基于 multicalibration 的概念，引入了 $(\\mathbf{s},\\mathcal{G}, \\alpha)-$GMC（Generalized Multi-Dimensional Multicalibration），适用于多维映射、约束集和预设阈值 $\\alpha$。该框架通过相关算法实现模型预测的多群组公平保证，涵盖了图像分割中的假阴性率控制、层次分类中的预测集条件不确定性量化，以及语言模型中的去偏文本生成等场景。研究在多个数据集上进行了数值实验，展示了框架的有效性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "28 pages, 8 figures, accepted by ICML2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02225v1",
      "published_date": "2024-05-03 16:32:09 UTC",
      "updated_date": "2024-05-03 16:32:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:40:30.289834"
    },
    {
      "arxiv_id": "2405.02219v2",
      "title": "A Normative Framework for Benchmarking Consumer Fairness in Large Language Model Recommender System",
      "title_zh": "一种规范框架，用于基准测试大型语言模型推荐系统中的消费者公平",
      "authors": [
        "Yashar Deldjoo",
        "Fatemeh Nazary"
      ],
      "abstract": "The rapid adoption of large language models (LLMs) in recommender systems\n(RS) presents new challenges in understanding and evaluating their biases,\nwhich can result in unfairness or the amplification of stereotypes. Traditional\nfairness evaluations in RS primarily focus on collaborative filtering (CF)\nsettings, which may not fully capture the complexities of LLMs, as these models\noften inherit biases from large, unregulated data. This paper proposes a\nnormative framework to benchmark consumer fairness in LLM-powered recommender\nsystems (RecLLMs).\n  We critically examine how fairness norms in classical RS fall short in\naddressing the challenges posed by LLMs. We argue that this gap can lead to\narbitrary conclusions about fairness, and we propose a more structured, formal\napproach to evaluate fairness in such systems. Our experiments on the MovieLens\ndataset on consumer fairness, using in-context learning (zero-shot vs.\nfew-shot) reveal fairness deviations in age-based recommendations, particularly\nwhen additional contextual examples are introduced (ICL-2). Statistical\nsignificance tests confirm that these deviations are not random, highlighting\nthe need for robust evaluation methods. While this work offers a preliminary\ndiscussion on a proposed normative framework, our hope is that it could provide\na formal, principled approach for auditing and mitigating bias in RecLLMs. The\ncode and dataset used for this work will be shared at \"gihub-anonymized\".",
      "tldr_zh": "本研究提出一个规范框架，用于基准测试大型语言模型 (LLMs) 驱动的推荐系统 (RecLLMs) 中的消费者公平性，以解决 LLMs 可能从数据中继承偏见并放大刻板印象的问题。框架批判性地审视了传统协作过滤 (CF) 设置下公平性评估的不足，并通过在 MovieLens 数据集上的实验，使用 in-context learning (零样本 vs. 少样本) 揭示了年龄-based 推荐中的公平偏差，尤其是在引入额外上下文示例 (ICL-2) 时。实验结果显示这些偏差具有统计显著性，强调了需要更结构化的方法来审计和缓解 RecLLMs 中的偏见，为未来公平性评估提供正式的指导。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02219v2",
      "published_date": "2024-05-03 16:25:27 UTC",
      "updated_date": "2024-09-11 07:27:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:40:42.385521"
    },
    {
      "arxiv_id": "2405.02213v2",
      "title": "Automatic Programming: Large Language Models and Beyond",
      "title_zh": "自动编程：大语言模型及其超越",
      "authors": [
        "Michael R. Lyu",
        "Baishakhi Ray",
        "Abhik Roychoudhury",
        "Shin Hwei Tan",
        "Patanamon Thongtanunam"
      ],
      "abstract": "Automatic programming has seen increasing popularity due to the emergence of\ntools like GitHub Copilot which rely on Large Language Models (LLMs). At the\nsame time, automatically generated code faces challenges during deployment due\nto concerns around quality and trust. In this article, we study automated\ncoding in a general sense and study the concerns around code quality, security\nand related issues of programmer responsibility. These are key issues for\norganizations while deciding on the usage of automatically generated code. We\ndiscuss how advances in software engineering such as program repair and\nanalysis can enable automatic programming. We conclude with a forward looking\nview, focusing on the programming environment of the near future, where\nprogrammers may need to switch to different roles to fully utilize the power of\nautomatic programming. Automated repair of automatically generated programs\nfrom LLMs, can help produce higher assurance code from LLMs, along with\nevidence of assurance",
      "tldr_zh": "这篇论文探讨了自动编程的兴起，特别是依赖 Large Language Models (LLMs) 的工具如 GitHub Copilot，同时分析了自动生成代码在部署中面临的挑战，包括代码质量、安全性和程序员责任问题，这些是组织采用此类技术的关键关切。\n文章讨论了软件工程进展，如 program repair 和 program analysis，如何帮助提升自动编程的可靠性和效率。\n最终，论文展望未来，提出程序员可能需要切换角色来充分利用自动编程潜力，并建议通过自动修复 LLM 生成的程序来提高代码保障和可信度。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02213v2",
      "published_date": "2024-05-03 16:19:24 UTC",
      "updated_date": "2024-05-15 16:33:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:40:54.446112"
    },
    {
      "arxiv_id": "2405.02384v1",
      "title": "CogDPM: Diffusion Probabilistic Models via Cognitive Predictive Coding",
      "title_zh": "CogDPM：基于认知预测编码的扩散概率模型",
      "authors": [
        "Kaiyuan Chen",
        "Xingzhuo Guo",
        "Yu Zhang",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "abstract": "Predictive Coding (PC) is a theoretical framework in cognitive science\nsuggesting that the human brain processes cognition through spatiotemporal\nprediction of the visual world. Existing studies have developed spatiotemporal\nprediction neural networks based on the PC theory, emulating its two core\nmechanisms: Correcting predictions from residuals and hierarchical learning.\nHowever, these models do not show the enhancement of prediction skills on\nreal-world forecasting tasks and ignore the Precision Weighting mechanism of PC\ntheory. The precision weighting mechanism posits that the brain allocates more\nattention to signals with lower precision, contributing to the cognitive\nability of human brains. This work introduces the Cognitive Diffusion\nProbabilistic Models (CogDPM), which demonstrate the connection between\ndiffusion probabilistic models and PC theory. CogDPM features a precision\nestimation method based on the hierarchical sampling capabilities of diffusion\nmodels and weight the guidance with precision weights estimated by the inherent\nproperty of diffusion models. We experimentally show that the precision weights\neffectively estimate the data predictability. We apply CogDPM to real-world\nprediction tasks using the United Kindom precipitation and ERA surface wind\ndatasets. Our results demonstrate that CogDPM outperforms both existing\ndomain-specific operational models and general deep prediction models by\nproviding more proficient forecasting.",
      "tldr_zh": "这篇论文基于 Predictive Coding (PC) 理论，提出 Cognitive Diffusion Probabilistic Models (CogDPM)，将扩散概率模型（Diffusion Probabilistic Models）与认知科学框架相结合，以提升时空预测能力。CogDPM 引入精度权重（Precision Weighting）机制，通过层次化采样估计精度并指导模型，解决了现有模型忽略该机制的问题。实验结果表明，CogDPM 在英国降水和 ERA 表面风等真实世界预测任务中，优于现有领域特定模型和通用深度预测模型，提供更高效的预测表现。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02384v1",
      "published_date": "2024-05-03 15:54:50 UTC",
      "updated_date": "2024-05-03 15:54:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:41:06.767890"
    },
    {
      "arxiv_id": "2405.02383v1",
      "title": "A Fresh Look at Sanity Checks for Saliency Maps",
      "title_zh": "翻译失败",
      "authors": [
        "Anna Hedström",
        "Leander Weber",
        "Sebastian Lapuschkin",
        "Marina Höhne"
      ],
      "abstract": "The Model Parameter Randomisation Test (MPRT) is highly recognised in the\neXplainable Artificial Intelligence (XAI) community due to its fundamental\nevaluative criterion: explanations should be sensitive to the parameters of the\nmodel they seek to explain. However, recent studies have raised several\nmethodological concerns for the empirical interpretation of MPRT. In response,\nwe propose two modifications to the original test: Smooth MPRT and Efficient\nMPRT. The former reduces the impact of noise on evaluation outcomes via\nsampling, while the latter avoids the need for biased similarity measurements\nby re-interpreting the test through the increase in explanation complexity\nafter full model randomisation. Our experiments show that these modifications\nenhance the metric reliability, facilitating a more trustworthy deployment of\nexplanation methods.",
      "tldr_zh": "这篇论文重新审视了用于显著性图(Saliency Maps)的健全性检查测试，特别是 Model Parameter Randomisation Test (MPRT)，该测试在 eXplainable Artificial Intelligence (XAI) 社区中被广泛认可，但存在方法论问题。作者提出两个改进版本：Smooth MPRT 通过采样减少噪声对评估结果的影响，以及 Efficient MPRT 通过重新解释测试（关注解释复杂性的增加）来避免偏见的相似性测量。实验结果表明，这些修改显著提升了指标的可靠性和解释方法的部署可信度。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "arXiv admin note: text overlap with arXiv:2401.06465",
      "pdf_url": "http://arxiv.org/pdf/2405.02383v1",
      "published_date": "2024-05-03 15:47:32 UTC",
      "updated_date": "2024-05-03 15:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:41:17.845839"
    },
    {
      "arxiv_id": "2405.02188v1",
      "title": "Optimistic Regret Bounds for Online Learning in Adversarial Markov Decision Processes",
      "title_zh": "对抗马尔可夫决策过程在线学习的乐观遗憾界",
      "authors": [
        "Sang Bin Moon",
        "Abolfazl Hashemi"
      ],
      "abstract": "The Adversarial Markov Decision Process (AMDP) is a learning framework that\ndeals with unknown and varying tasks in decision-making applications like\nrobotics and recommendation systems. A major limitation of the AMDP formalism,\nhowever, is pessimistic regret analysis results in the sense that although the\ncost function can change from one episode to the next, the evolution in many\nsettings is not adversarial. To address this, we introduce and study a new\nvariant of AMDP, which aims to minimize regret while utilizing a set of cost\npredictors. For this setting, we develop a new policy search method that\nachieves a sublinear optimistic regret with high probability, that is a regret\nbound which gracefully degrades with the estimation power of the cost\npredictors. Establishing such optimistic regret bounds is nontrivial given that\n(i) as we demonstrate, the existing importance-weighted cost estimators cannot\nestablish optimistic bounds, and (ii) the feedback model of AMDP is different\n(and more realistic) than the existing optimistic online learning works. Our\nresult, in particular, hinges upon developing a novel optimistically biased\ncost estimator that leverages cost predictors and enables a high-probability\nregret analysis without imposing restrictive assumptions. We further discuss\npractical extensions of the proposed scheme and demonstrate its efficacy\nnumerically.",
      "tldr_zh": "本研究针对 Adversarial Markov Decision Processes (AMDP) 中的悲观遗憾分析问题，提出了一种新变体框架，该框架利用成本预测器来最小化遗憾，同时处理非对抗性任务变化，如在机器人和推荐系统中常见。研究开发了一种新策略搜索方法，通过引入一个新型乐观偏差成本估计器，实现子线性乐观遗憾（optimistic regret）边界，该边界随成本预测器的估计能力优雅降级，并在高概率下有效。实验结果证明了该方法的效能，并讨论了其实际扩展应用，为更现实的在线学习场景提供了可靠基础。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02188v1",
      "published_date": "2024-05-03 15:44:31 UTC",
      "updated_date": "2024-05-03 15:44:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:41:29.707739"
    },
    {
      "arxiv_id": "2405.02178v2",
      "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
      "title_zh": "评估与验证 LLM 驱动应用中的任务效用",
      "authors": [
        "Negar Arabzadeh",
        "Siqing Huo",
        "Nikhil Mehta",
        "Qinqyun Wu",
        "Chi Wang",
        "Ahmed Awadallah",
        "Charles L. A. Clarke",
        "Julia Kiseleva"
      ],
      "abstract": "The rapid development of Large Language Models (LLMs) has led to a surge in\napplications that facilitate collaboration among multiple agents, assisting\nhumans in their daily tasks. However, a significant gap remains in assessing to\nwhat extent LLM-powered applications genuinely enhance user experience and task\nexecution efficiency. This highlights the need to verify utility of LLM-powered\napplications, particularly by ensuring alignment between the application's\nfunctionality and end-user needs. We introduce AgentEval, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the effectiveness and robustness of AgentEval for two\nopen source datasets including Math Problem solving and ALFWorld House-hold\nrelated tasks. For reproducibility purposes, we make the data, code and all the\nlogs publicly available at https://bit.ly/3w3yKcS .",
      "tldr_zh": "本文探讨了 Large Language Models (LLMs) 在多智能体应用中的效用评估问题，强调需要验证这些应用是否真正提升用户体验和任务执行效率。研究引入了 AgentEval 框架，该框架能自动为特定应用提出定制化的评估标准，并进行全面量化评估，以确保功能与用户需求一致。通过在 Math Problem solving 和 ALFWorld 任务数据集上的实验，AgentEval 展示了其有效性和鲁棒性。研究还公开了数据、代码和日志，以支持可重复性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2402.09015",
      "pdf_url": "http://arxiv.org/pdf/2405.02178v2",
      "published_date": "2024-05-03 15:26:27 UTC",
      "updated_date": "2024-05-12 15:52:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:41:42.549497"
    },
    {
      "arxiv_id": "2405.02175v3",
      "title": "Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Hsuvas Borkakoty",
        "Luis Espinosa-Anke"
      ],
      "abstract": "Hoaxes are a recognised form of disinformation created deliberately, with\npotential serious implications in the credibility of reference knowledge\nresources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that\nthey often are written according to the official style guidelines. In this\nwork, we first provide a systematic analysis of similarities and discrepancies\nbetween legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a\ncollection of 311 hoax articles (from existing literature and official\nWikipedia lists), together with semantically similar legitimate articles, which\ntogether form a binary text classification dataset aimed at fostering research\nin automated hoax detection. In this paper, We report results after analyzing\nseveral language models, hoax-to-legit ratios, and the amount of text\nclassifiers are exposed to (full article vs the article's definition alone).\nOur results suggest that detecting deceitful content in Wikipedia based on\ncontent alone is hard but feasible, and complement our analysis with a study on\nthe differences in distributions in edit histories, and find that looking at\nthis feature yields better classification results than context.",
      "tldr_zh": "这篇论文介绍了Hoaxpedia，一种统一的维基百科hoax文章数据集，包含311篇故意创建的虚假文章及其语义相似的合法文章，形成一个二元文本分类数据集，以促进自动化hoax检测研究。作者系统分析了合法和hoax文章在内容和风格上的相似性与差异，并评估了多种language models在不同hoax-to-legit比率和文本暴露量（全篇文章 vs. 仅定义）下的性能。结果显示，基于内容检测维基百科的欺骗性内容虽具有挑战性但可行，而分析编辑历史特征能提供更准确的分类结果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02175v3",
      "published_date": "2024-05-03 15:25:48 UTC",
      "updated_date": "2024-08-30 16:40:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:41:54.544296"
    },
    {
      "arxiv_id": "2405.02165v1",
      "title": "EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and Multi-View Transformer",
      "title_zh": "EEG2TEXT",
      "authors": [
        "Hanwen Liu",
        "Daniel Hajialigol",
        "Benny Antony",
        "Aiguo Han",
        "Xuan Wang"
      ],
      "abstract": "Deciphering the intricacies of the human brain has captivated curiosity for\ncenturies. Recent strides in Brain-Computer Interface (BCI) technology,\nparticularly using motor imagery, have restored motor functions such as\nreaching, grasping, and walking in paralyzed individuals. However, unraveling\nnatural language from brain signals remains a formidable challenge.\nElectroencephalography (EEG) is a non-invasive technique used to record\nelectrical activity in the brain by placing electrodes on the scalp. Previous\nstudies of EEG-to-text decoding have achieved high accuracy on small closed\nvocabularies, but still fall short of high accuracy when dealing with large\nopen vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy\nof open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG\npre-training to enhance the learning of semantics from EEG signals and proposes\na multi-view transformer to model the EEG signal processing by different\nspatial regions of the brain. Experiments show that EEG2TEXT has superior\nperformance, outperforming the state-of-the-art baseline methods by a large\nmargin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great\npotential for a high-performance open-vocabulary brain-to-text system to\nfacilitate communication.",
      "tldr_zh": "本研究针对脑机接口(BCI)中从EEG信号解码自然语言的挑战，提出了一种名为EEG2TEXT的方法，以提升开放词汇EEG-to-Text解码的准确率。该方法结合EEG预训练来增强EEG信号的语义学习，并引入多视图Transformer来模拟大脑不同空间区域的信号处理。实验结果显示，EEG2TEXT在BLEU和ROUGE分数上比最先进基线方法绝对值高出高达5%，展示了其在构建高性能开放词汇脑到文本系统的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02165v1",
      "published_date": "2024-05-03 15:14:19 UTC",
      "updated_date": "2024-05-03 15:14:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:42:08.102087"
    },
    {
      "arxiv_id": "2405.02162v3",
      "title": "Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic Labeling using Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamad Al Mdfaa",
        "Raghad Salameh",
        "Sergey Zagoruyko",
        "Gonzalo Ferrer"
      ],
      "abstract": "In the field of robotics and computer vision, efficient and accurate semantic\nmapping remains a significant challenge due to the growing demand for\nintelligent machines that can comprehend and interact with complex\nenvironments. Conventional panoptic mapping methods, however, are limited by\npredefined semantic classes, thus making them ineffective for handling novel or\nunforeseen objects. In response to this limitation, we introduce the Unified\nPromptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances in\nfoundation models to enable real-time, on-demand label generation using natural\nlanguage prompts. By incorporating a dynamic labeling strategy into traditional\npanoptic mapping techniques, UPPM provides significant improvements in\nadaptability and versatility while maintaining high performance levels in map\nreconstruction. We demonstrate our approach on real-world and simulated\ndatasets. Results show that UPPM can accurately reconstruct scenes and segment\nobjects while generating rich semantic labels through natural language\ninteractions. A series of ablation experiments validated the advantages of\nfoundation model-based labeling over fixed label sets.",
      "tldr_zh": "该研究提出Unified Promptable Panoptic Mapping (UPPM)方法，旨在解决传统panoptic mapping受限于预定义语义类的问题，从而更好地处理新颖或未知对象。UPPM利用foundation models实现实时、按需标签生成，通过自然语言提示整合动态标签策略，提升了映射的适应性和多功能性，同时保持高性能水平。在真实和模拟数据集上的实验表明，该方法能准确重建场景、分割对象并生成丰富的语义标签，且消融实验验证了foundation models-based标签的优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is under consideration at Pattern Recognition Letters",
      "pdf_url": "http://arxiv.org/pdf/2405.02162v3",
      "published_date": "2024-05-03 15:08:39 UTC",
      "updated_date": "2024-10-10 16:03:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:42:18.168762"
    },
    {
      "arxiv_id": "2405.02161v2",
      "title": "Simulating the Economic Impact of Rationality through Reinforcement Learning and Agent-Based Modelling",
      "title_zh": "翻译失败",
      "authors": [
        "Simone Brusatin",
        "Tommaso Padoan",
        "Andrea Coletta",
        "Domenico Delli Gatti",
        "Aldo Glielmo"
      ],
      "abstract": "Agent-based models (ABMs) are simulation models used in economics to overcome\nsome of the limitations of traditional frameworks based on general equilibrium\nassumptions. However, agents within an ABM follow predetermined 'bounded\nrational' behavioural rules which can be cumbersome to design and difficult to\njustify. Here we leverage multi-agent reinforcement learning (RL) to expand the\ncapabilities of ABMs with the introduction of 'fully rational' agents that\nlearn their policy by interacting with the environment and maximising a reward\nfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by\nextending a paradigmatic macro ABM from the economic literature. We show that\ngradually substituting ABM firms in the model with RL agents, trained to\nmaximise profits, allows for studying the impact of rationality on the economy.\nWe find that RL agents spontaneously learn three distinct strategies for\nmaximising profits, with the optimal strategy depending on the level of market\ncompetition and rationality. We also find that RL agents with independent\npolicies, and without the ability to communicate with each other, spontaneously\nlearn to segregate into different strategic groups, thus increasing market\npower and overall profits. Finally, we find that a higher number of rational\n(RL) agents in the economy always improves the macroeconomic environment as\nmeasured by total output. Depending on the specific rational policy, this can\ncome at the cost of higher instability. Our R-MABM framework allows for stable\nmulti-agent learning, is available in open source, and represents a principled\nand robust direction to extend economic simulators.",
      "tldr_zh": "这篇论文提出了一种名为 Rational macro ABM (R-MABM) 的框架，将多代理强化学习 (RL) 与 Agent-based models (ABMs) 相结合，允许经济代理人从预定的“bounded rational”规则转向“fully rational”策略，通过与环境互动最大化奖励函数。研究通过逐步替换模型中的公司代理人为 RL 代理人，探讨理性对经济的影响，发现这些代理人自发学习三种利润最大化策略，并根据市场竞争和理性水平选择最优策略，同时自发分化成战略群体以增强市场力量。结果显示，增加理性代理人数量能改善宏观经济环境（如总产出），但可能带来更高的不稳定性；该框架开源且稳定，为扩展经济模拟器提供了可靠方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.MA",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02161v2",
      "published_date": "2024-05-03 15:08:25 UTC",
      "updated_date": "2024-10-21 21:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:42:33.033996"
    },
    {
      "arxiv_id": "2405.02151v3",
      "title": "GMP-TL: Gender-augmented Multi-scale Pseudo-label Enhanced Transfer Learning for Speech Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Pan",
        "Yuguang Yang",
        "Heng Lu",
        "Lei Ma",
        "Jianjun Zhao"
      ],
      "abstract": "The continuous evolution of pre-trained speech models has greatly advanced\nSpeech Emotion Recognition (SER). However, current research typically relies on\nutterance-level emotion labels, inadequately capturing the complexity of\nemotions within a single utterance. In this paper, we introduce GMP-TL, a novel\nSER framework that employs gender-augmented multi-scale pseudo-label (GMP)\nbased transfer learning to mitigate this gap. Specifically, GMP-TL initially\nuses the pre-trained HuBERT, implementing multi-task learning and multi-scale\nk-means clustering to acquire frame-level GMPs. Subsequently, to fully leverage\nframe-level GMPs and utterance-level emotion labels, a two-stage model\nfine-tuning approach is presented to further optimize GMP-TL. Experiments on\nIEMOCAP show that our GMP-TL attains a WAR of 80.0% and an UAR of 82.0%,\nachieving superior performance compared to state-of-the-art unimodal SER\nmethods while also yielding comparable results to multimodal SER approaches.",
      "tldr_zh": "本研究提出GMP-TL框架，通过gender-augmented multi-scale pseudo-label (GMP)增强的transfer learning，解决Speech Emotion Recognition (SER)中依赖utterance-level标签而忽略情感复杂性的问题。框架首先利用预训练模型HuBERT结合multi-task learning和multi-scale k-means clustering生成frame-level GMPs，随后采用两阶段模型fine-tuning方法整合frame-level GMPs和utterance-level标签。实验在IEMOCAP数据集上显示，GMP-TL达到WAR 80.0%和UAR 82.0%的性能，优于现有单模态SER方法，并与多模态方法相当。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to SLT2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02151v3",
      "published_date": "2024-05-03 14:58:46 UTC",
      "updated_date": "2024-09-23 12:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:42:42.961409"
    },
    {
      "arxiv_id": "2405.02148v1",
      "title": "Towards a Formal Creativity Theory: Preliminary results in Novelty and Transformativeness",
      "title_zh": "翻译失败",
      "authors": [
        "Luís Espírito Santo",
        "Geraint Wiggins",
        "Amílcar Cardoso"
      ],
      "abstract": "Formalizing creativity-related concepts has been a long-term goal of\nComputational Creativity. To the same end, we explore Formal Learning Theory in\nthe context of creativity. We provide an introduction to the main concepts of\nthis framework and a re-interpretation of terms commonly found in creativity\ndiscussions, proposing formal definitions for novelty and transformational\ncreativity. This formalisation marks the beginning of a research branch we call\nFormal Creativity Theory, exploring how learning can be included as preparation\nfor exploratory behaviour and how learning is a key part of transformational\ncreative behaviour. By employing these definitions, we argue that, while\nnovelty is neither necessary nor sufficient for transformational creativity in\ngeneral, when using an inspiring set, rather than a sequence of experiences, an\nagent actually requires novelty for transformational creativity to occur.",
      "tldr_zh": "该论文探讨了计算创造性领域的目标，通过引入 Formal Learning Theory 来正式化创造性概念，并重新诠释相关术语。研究者提出了 Formal Creativity Theory 的初步框架，为 novelty 和 transformational creativity 提供了正式定义，强调学习在探索行为和转变性创造行为中的关键作用。主要发现是，novelty 并非 transformational creativity 的必要或充分条件，但在使用 inspiring set 而非经验序列时，它是必需的，这为未来创造性理论研究奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02148v1",
      "published_date": "2024-05-03 14:53:46 UTC",
      "updated_date": "2024-05-03 14:53:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:42:56.193873"
    },
    {
      "arxiv_id": "2405.06668v2",
      "title": "Exposing and Explaining Fake News On-the-Fly",
      "title_zh": "实时揭示和解释假新闻",
      "authors": [
        "Francisco de Arriba-Pérez",
        "Silvia García-Méndez",
        "Fátima Leal",
        "Benedita Malheiro",
        "Juan Carlos Burguillo"
      ],
      "abstract": "Social media platforms enable the rapid dissemination and consumption of\ninformation. However, users instantly consume such content regardless of the\nreliability of the shared data. Consequently, the latter crowdsourcing model is\nexposed to manipulation. This work contributes with an explainable and online\nclassification method to recognize fake news in real-time. The proposed method\ncombines both unsupervised and supervised Machine Learning approaches with\nonline created lexica. The profiling is built using creator-, content- and\ncontext-based features using Natural Language Processing techniques. The\nexplainable classification mechanism displays in a dashboard the features\nselected for classification and the prediction confidence. The performance of\nthe proposed solution has been validated with real data sets from Twitter and\nthe results attain 80 % accuracy and macro F-measure. This proposal is the\nfirst to jointly provide data stream processing, profiling, classification and\nexplainability. Ultimately, the proposed early detection, isolation and\nexplanation of fake news contribute to increase the quality and trustworthiness\nof social media contents.",
      "tldr_zh": "本文提出了一种实时可解释的假新闻识别方法，结合无监督和监督 Machine Learning 技术，以及在线创建的词汇库，通过 Natural Language Processing 分析基于创建者、内容和上下文的特征。分类机制在仪表板中显示选定的特征和预测置信度，实现对假新闻的早期检测和解释。在 Twitter 的真实数据集上，该方法达到了 80% 的准确率和宏 F-measure，这是首个联合提供数据流处理、分析、分类和可解释性的方案，最终提升了社交媒体内容的质量和可信度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06668v2",
      "published_date": "2024-05-03 14:49:04 UTC",
      "updated_date": "2024-09-05 10:07:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:43:06.900810"
    },
    {
      "arxiv_id": "2405.02131v2",
      "title": "Physics-informed generative neural networks for RF propagation prediction with application to indoor body perception",
      "title_zh": "翻译失败",
      "authors": [
        "Federica Fieramosca",
        "Vittorio Rampa",
        "Michele D'Amico",
        "Stefano Savazzi"
      ],
      "abstract": "Electromagnetic (EM) body models designed to predict Radio-Frequency (RF)\npropagation are time-consuming methods which prevent their adoption in strict\nreal-time computational imaging problems, such as human body localization and\nsensing. Physics-informed Generative Neural Network (GNN) models have been\nrecently proposed to reproduce EM effects, namely to simulate or reconstruct\nmissing data or samples by incorporating relevant EM principles and\nconstraints. The paper discusses a Variational Auto-Encoder (VAE) model which\nis trained to reproduce the effects of human motions on the EM field and\nincorporate EM body diffraction principles. Proposed physics-informed\ngenerative neural network models are verified against both classical\ndiffraction-based EM tools and full-wave EM body simulations.",
      "tldr_zh": "这篇论文提出了一种Physics-informed Generative Neural Network (GNN)模型，用于预测Radio-Frequency (RF)传播，以解决传统Electromagnetic (EM)体模型计算耗时的问题，从而适用于实时计算成像任务，如室内人体定位和感知。模型具体采用Variational Auto-Encoder (VAE)架构，训练其模拟人体运动对EM场的影响，并整合EM体衍射原则。实验验证显示，该方法与经典衍射-based EM工具和全波EM模拟相比，能有效重现EM效果，为高效的人体感知应用提供了新途径。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02131v2",
      "published_date": "2024-05-03 14:35:02 UTC",
      "updated_date": "2024-05-15 13:11:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:43:19.314914"
    },
    {
      "arxiv_id": "2405.02124v1",
      "title": "TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on Self-Supervised Learning and Knowledge Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Noé Tits",
        "Prernna Bhatnagar",
        "Thierry Dutoit"
      ],
      "abstract": "In this paper, we present a novel approach for text independent\nphone-to-audio alignment based on phoneme recognition, representation learning\nand knowledge transfer. Our method leverages a self-supervised model (wav2vec2)\nfine-tuned for phoneme recognition using a Connectionist Temporal\nClassification (CTC) loss, a dimension reduction model and a frame-level\nphoneme classifier trained thanks to forced-alignment labels (using Montreal\nForced Aligner) to produce multi-lingual phonetic representations, thus\nrequiring minimal additional training. We evaluate our model using synthetic\nnative data from the TIMIT dataset and the SCRIBE dataset for American and\nBritish English, respectively. Our proposed model outperforms the\nstate-of-the-art (charsiu) in statistical metrics and has applications in\nlanguage learning and speech processing systems. We leave experiments on other\nlanguages for future work but the design of the system makes it easily\nadaptable to other languages.",
      "tldr_zh": "本论文提出TIPAA-SSL，一种基于自监督学习和知识转移的文本无关音素到音频对齐方法，利用wav2vec2模型经Connectionist Temporal Classification (CTC)损失微调，并结合降维模型和帧级音素分类器，生成多语言音素表示，从而减少额外训练需求。  \n实验在TIMIT和SCRIBE数据集上评估了该模型，针对美式和英式英语，TIPAA-SSL在统计指标上超过了现有最先进方法charsiu。  \n该方法适用于语言学习和语音处理系统，并设计为易于扩展到其他语言。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02124v1",
      "published_date": "2024-05-03 14:25:21 UTC",
      "updated_date": "2024-05-03 14:25:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:43:31.810835"
    },
    {
      "arxiv_id": "2407.05423v1",
      "title": "A Manifesto for a Pro-Actively Responsible AI in Education",
      "title_zh": "翻译失败",
      "authors": [
        "Kaska Porayska-Pomsta"
      ],
      "abstract": "This paper examines the historical foundations, current practices, and\nemerging challenges for Artificial Intelligence in Education (AIED) within\nbroader AI practices. It highlights AIED's unique and rich potential for\ncontributing to the current AI policy and practices, especially in the context\nof responsible AI. It also discusses the key gaps in the AIED field, which need\nto be addressed by the community to elevate the field from a cottage industry\nto the level where it will deservedly be seen as key to advancin AI research\nand practical applications. The paper offers a five-point manifesto aimed to\nrevitalise AIED' contributions to education and broader AI community,\nsuggesting enhanced interdisciplinary collaboration, a broadened understanding\nof AI's impact on human functioning, and commitment to setting agendas for\nhuman-centred educational innovations.This approach positions AIED to\nsignificantly influence educational technologies to achieve genuine positive\nimpact across diverse societal segments.",
      "tldr_zh": "这篇论文审视了人工智能在教育（AIED）领域的历史基础、当前实践以及新兴挑战，强调AIED在负责任AI政策和实践中的独特潜力。论文指出AIED领域存在关键空白，需要社区努力将其从“小作坊”式产业提升为AI研究和应用的支柱。作者提出一个五点manifesto，呼吁加强跨学科合作、扩展对AI对人类功能影响的理解，并致力于以人为本的教育创新，从而使AIED显著推动教育技术，实现对不同社会群体的积极影响。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.3"
      ],
      "primary_category": "cs.CY",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.05423v1",
      "published_date": "2024-05-03 14:23:41 UTC",
      "updated_date": "2024-05-03 14:23:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:43:44.012763"
    },
    {
      "arxiv_id": "2405.02105v1",
      "title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Vladyslav Nechakhin",
        "Jennifer D'Souza",
        "Steffen Eger"
      ],
      "abstract": "Structured science summaries or research contributions using properties or\ndimensions beyond traditional keywords enhances science findability. Current\nmethods, such as those used by the Open Research Knowledge Graph (ORKG),\ninvolve manually curating properties to describe research papers' contributions\nin a structured manner, but this is labor-intensive and inconsistent between\nthe domain expert human curators. We propose using Large Language Models (LLMs)\nto automatically suggest these properties. However, it's essential to assess\nthe readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before\napplication. Our study performs a comprehensive comparative analysis between\nORKG's manually curated properties and those generated by the aforementioned\nstate-of-the-art LLMs. We evaluate LLM performance through four unique\nperspectives: semantic alignment and deviation with ORKG properties,\nfine-grained properties mapping accuracy, SciNCL embeddings-based cosine\nsimilarity, and expert surveys comparing manual annotations with LLM outputs.\nThese evaluations occur within a multidisciplinary science setting. Overall,\nLLMs show potential as recommendation systems for structuring science, but\nfurther finetuning is recommended to improve their alignment with scientific\ntasks and mimicry of human expertise.",
      "tldr_zh": "本研究评估了大型语言模型(LLMs)如 GPT-3.5、Llama 2 和 Mistral，在 Open Research Knowledge Graph (ORKG) 中自动生成结构化科学摘要属性的性能，以提升科学发现性并减少手动整理的劳动强度。研究通过四个视角进行比较分析：语义对齐和偏差、细粒度属性映射准确性、SciNCL 嵌入的余弦相似度，以及专家调查，以多学科科学环境为背景。结果表明，LLMs 显示出作为推荐系统的潜力，但需进一步微调以更好地模仿人类专家并提高与科学任务的契合度。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 11 figures. In review at\n  https://www.mdpi.com/journal/information/special_issues/WYS02U2GTD",
      "pdf_url": "http://arxiv.org/pdf/2405.02105v1",
      "published_date": "2024-05-03 14:03:04 UTC",
      "updated_date": "2024-05-03 14:03:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:43:55.410311"
    },
    {
      "arxiv_id": "2405.02095v2",
      "title": "Advanced Detection of Source Code Clones via an Ensemble of Unsupervised Similarity Measures",
      "title_zh": "通过无监督相似性度量集成的源代码克",
      "authors": [
        "Jorge Martinez-Gil"
      ],
      "abstract": "The capability of accurately determining code similarity is crucial in many\ntasks related to software development. For example, it might be essential to\nidentify code duplicates for performing software maintenance. This research\nintroduces a novel ensemble learning approach for code similarity assessment,\ncombining the strengths of multiple unsupervised similarity measures. The key\nidea is that the strengths of a diverse set of similarity measures can\ncomplement each other and mitigate individual weaknesses, leading to improved\nperformance. Preliminary results show that while Transformers-based CodeBERT\nand its variant GraphCodeBERT are undoubtedly the best option in the presence\nof abundant training data, in the case of specific small datasets (up to 500\nsamples), our ensemble achieves similar results, without prejudice to the\ninterpretability of the resulting solution, and with a much lower associated\ncarbon footprint due to training. The source code of this novel approach can be\ndownloaded from https://github.com/jorge-martinez-gil/ensemble-codesim.",
      "tldr_zh": "这篇论文提出了一种新颖的集成学习方法，通过结合多种无监督相似度测量（unsupervised similarity measures）来提升源代码克隆（source code clones）的检测准确性。该方法的核心在于利用不同测量的优势互补，减少个体弱点，从而在软件维护等任务中实现更好的性能。在小数据集（最多500样本）上，实验结果显示，该集成方法与基于 Transformers 的 CodeBERT 和 GraphCodeBERT 模型取得了类似效果，但提供了更高的可解释性和更低的碳足迹。源代码可从 GitHub 下载，供进一步研究使用。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Under review in Software Quality Days",
      "pdf_url": "http://arxiv.org/pdf/2405.02095v2",
      "published_date": "2024-05-03 13:42:49 UTC",
      "updated_date": "2024-10-30 14:01:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:44:08.112508"
    },
    {
      "arxiv_id": "2405.02083v2",
      "title": "A fuzzy loss for ontology classification",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Flügel",
        "Martin Glauer",
        "Till Mossakowski",
        "Fabian Neuhaus"
      ],
      "abstract": "Deep learning models are often unaware of the inherent constraints of the\ntask they are applied to. However, many downstream tasks require logical\nconsistency. For ontology classification tasks, such constraints include\nsubsumption and disjointness relations between classes.\n  In order to increase the consistency of deep learning models, we propose a\nfuzzy loss that combines label-based loss with terms penalising subsumption- or\ndisjointness-violations. Our evaluation on the ChEBI ontology shows that the\nfuzzy loss is able to decrease the number of consistency violations by several\norders of magnitude without decreasing the classification performance. In\naddition, we use the fuzzy loss for unsupervised learning. We show that this\ncan further improve consistency on data from a",
      "tldr_zh": "本论文针对深度学习模型在本体分类任务中忽略逻辑约束（如subsumption和disjointness关系）的问题，提出了一种fuzzy loss方法。该方法将基于标签的损失与惩罚子类或不相交违反的术语相结合，以提升模型的一致性。在ChEBI ontology上的实验显示，fuzzy loss能将一致性违反数量减少几个数量级，同时保持分类性能不变。此外，将fuzzy loss应用于无监督学习，进一步改善了数据的一致性。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02083v2",
      "published_date": "2024-05-03 13:20:37 UTC",
      "updated_date": "2024-08-19 14:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:44:19.125664"
    },
    {
      "arxiv_id": "2405.02082v1",
      "title": "A comparative study of conformal prediction methods for valid uncertainty quantification in machine learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolas Dewolf"
      ],
      "abstract": "In the past decades, most work in the area of data analysis and machine\nlearning was focused on optimizing predictive models and getting better results\nthan what was possible with existing models. To what extent the metrics with\nwhich such improvements were measured were accurately capturing the intended\ngoal, whether the numerical differences in the resulting values were\nsignificant, or whether uncertainty played a role in this study and if it\nshould have been taken into account, was of secondary importance. Whereas\nprobability theory, be it frequentist or Bayesian, used to be the gold standard\nin science before the advent of the supercomputer, it was quickly replaced in\nfavor of black box models and sheer computing power because of their ability to\nhandle large data sets. This evolution sadly happened at the expense of\ninterpretability and trustworthiness. However, while people are still trying to\nimprove the predictive power of their models, the community is starting to\nrealize that for many applications it is not so much the exact prediction that\nis of importance, but rather the variability or uncertainty.\n  The work in this dissertation tries to further the quest for a world where\neveryone is aware of uncertainty, of how important it is and how to embrace it\ninstead of fearing it. A specific, though general, framework that allows anyone\nto obtain accurate uncertainty estimates is singled out and analysed. Certain\naspects and applications of the framework -- dubbed `conformal prediction' --\nare studied in detail. Whereas many approaches to uncertainty quantification\nmake strong assumptions about the data, conformal prediction is, at the time of\nwriting, the only framework that deserves the title `distribution-free'. No\nparametric assumptions have to be made and the nonparametric results also hold\nwithout having to resort to the law of large numbers in the asymptotic regime.",
      "tldr_zh": "本研究比较了各种conformal prediction方法，以实现机器学习中有效的uncertainty quantification。论文回顾了机器学习历史，指出过去过度关注预测准确性而忽略不确定性的重要性，并强调conformal prediction框架作为一种distribution-free方法，能够在无需参数假设或渐近极限的情况下，提供准确的不确定性估计。最终，该工作推动了不确定性在科学决策中的应用，促进了模型的可解释性和可信度。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "primary_category": "stat.ML",
      "comment": "At 339 pages, this document is a live/working version of my PhD\n  dissertation published in 2024 by the University of Ghent (UGent)",
      "pdf_url": "http://arxiv.org/pdf/2405.02082v1",
      "published_date": "2024-05-03 13:19:33 UTC",
      "updated_date": "2024-05-03 13:19:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:44:31.673082"
    },
    {
      "arxiv_id": "2405.02079v3",
      "title": "Argumentative Large Language Models for Explainable and Contestable Claim Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Freedman",
        "Adam Dejl",
        "Deniz Gorur",
        "Xiang Yin",
        "Antonio Rago",
        "Francesca Toni"
      ],
      "abstract": "The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.",
      "tldr_zh": "这篇论文提出了 argumentative LLMs (ArgLLMs)，一种通过构建 argumentation frameworks 来增强大型语言模型 (LLMs) 的方法，以解决其在决策中缺乏可解释性和可争辩性的问题。ArgLLMs 利用正式推理支持决策过程，使输出可以被清晰解释和有效纠正。在 claim verification 任务的实验评估中，ArgLLMs 与现有技术相比表现出色，并定义了新的 contestability 属性来正式表征其性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 18 figures. Accepted as an oral presentation at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.02079v3",
      "published_date": "2024-05-03 13:12:28 UTC",
      "updated_date": "2025-04-18 11:20:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:44:43.438620"
    },
    {
      "arxiv_id": "2405.02048v1",
      "title": "Comparative Analysis of Retrieval Systems in the Real World",
      "title_zh": "真实世界中检索系统的比较分析",
      "authors": [
        "Dmytro Mozolevskyi",
        "Waseem AlShikh"
      ],
      "abstract": "This research paper presents a comprehensive analysis of integrating advanced\nlanguage models with search and retrieval systems in the fields of information\nretrieval and natural language processing. The objective is to evaluate and\ncompare various state-of-the-art methods based on their performance in terms of\naccuracy and efficiency. The analysis explores different combinations of\ntechnologies, including Azure Cognitive Search Retriever with GPT-4, Pinecone's\nCanopy framework, Langchain with Pinecone and different language models\n(OpenAI, Cohere), LlamaIndex with Weaviate Vector Store's hybrid search,\nGoogle's RAG implementation on Cloud VertexAI-Search, Amazon SageMaker's RAG,\nand a novel approach called KG-FID Retrieval. The motivation for this analysis\narises from the increasing demand for robust and responsive question-answering\nsystems in various domains. The RobustQA metric is used to evaluate the\nperformance of these systems under diverse paraphrasing of questions. The\nreport aims to provide insights into the strengths and weaknesses of each\nmethod, facilitating informed decisions in the deployment and development of\nAI-driven search and retrieval systems.",
      "tldr_zh": "这篇论文对信息检索和自然语言处理领域中高级语言模型与检索系统的整合进行了全面比较，评估了包括 Azure Cognitive Search with GPT-4、Pinecone's Canopy、Langchain with Pinecone 和不同语言模型（OpenAI, Cohere）、LlamaIndex with Weaviate Vector Store's hybrid search、Google's RAG、Amazon SageMaker's RAG 以及新方法 KG-FID Retrieval 在准确性和效率方面的表现。研究采用 RobustQA 指标，对这些系统在各种问题改写场景下的鲁棒性进行了测试。结果提供了各方法的优势和劣势分析，帮助决策者更好地部署和开发 AI 驱动的问答系统。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02048v1",
      "published_date": "2024-05-03 12:30:01 UTC",
      "updated_date": "2024-05-03 12:30:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:44:56.422107"
    },
    {
      "arxiv_id": "2405.02044v1",
      "title": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Anton Plaksin",
        "Vitaly Kalev"
      ],
      "abstract": "Robust Reinforcement Learning (RRL) is a promising Reinforcement Learning\n(RL) paradigm aimed at training robust to uncertainty or disturbances models,\nmaking them more efficient for real-world applications. Following this\nparadigm, uncertainty or disturbances are interpreted as actions of a second\nadversarial agent, and thus, the problem is reduced to seeking the agents'\npolicies robust to any opponent's actions. This paper is the first to propose\nconsidering the RRL problems within the positional differential game theory,\nwhich helps us to obtain theoretically justified intuition to develop a\ncentralized Q-learning approach. Namely, we prove that under Isaacs's condition\n(sufficiently general for real-world dynamical systems), the same Q-function\ncan be utilized as an approximate solution of both minimax and maximin Bellman\nequations. Based on these results, we present the Isaacs Deep Q-Network\nalgorithms and demonstrate their superiority compared to other baseline RRL and\nMulti-Agent RL algorithms in various environments.",
      "tldr_zh": "这篇论文提出将 Robust Reinforcement Learning (RRL) 问题框架化为零和位置微分游戏（Zero-Sum Positional Differential Games），通过将不确定性视为对手代理的行动，来开发理论上合理的中心化 Q-learning 方法。作者证明了在 Isaacs's condition 下，同一个 Q-function 可以同时作为 minimax 和 maximin Bellman 方程的近似解，从而为算法设计提供了坚实基础。基于此，论文引入了 Isaacs Deep Q-Network 算法，并在多种环境中证明其比传统 RRL 和 Multi-Agent RL 方法表现出色，提高了模型对真实世界不确定性的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.SY",
        "eess.SY",
        "math.OC",
        "68T07, 49N70"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02044v1",
      "published_date": "2024-05-03 12:21:43 UTC",
      "updated_date": "2024-05-03 12:21:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:45:08.964130"
    },
    {
      "arxiv_id": "2405.02024v1",
      "title": "Analyzing Narrative Processing in Large Language Models (LLMs): Using GPT4 to test BERT",
      "title_zh": "翻译失败",
      "authors": [
        "Patrick Krauss",
        "Jannik Hösch",
        "Claus Metzner",
        "Andreas Maier",
        "Peter Uhrig",
        "Achim Schilling"
      ],
      "abstract": "The ability to transmit and receive complex information via language is\nunique to humans and is the basis of traditions, culture and versatile social\ninteractions. Through the disruptive introduction of transformer based large\nlanguage models (LLMs) humans are not the only entity to \"understand\" and\nproduce language any more. In the present study, we have performed the first\nsteps to use LLMs as a model to understand fundamental mechanisms of language\nprocessing in neural networks, in order to make predictions and generate\nhypotheses on how the human brain does language processing. Thus, we have used\nChatGPT to generate seven different stylistic variations of ten different\nnarratives (Aesop's fables). We used these stories as input for the open source\nLLM BERT and have analyzed the activation patterns of the hidden units of BERT\nusing multi-dimensional scaling and cluster analysis. We found that the\nactivation vectors of the hidden units cluster according to stylistic\nvariations in earlier layers of BERT (1) than narrative content (4-5). Despite\nthe fact that BERT consists of 12 identical building blocks that are stacked\nand trained on large text corpora, the different layers perform different\ntasks. This is a very useful model of the human brain, where self-similar\nstructures, i.e. different areas of the cerebral cortex, can have different\nfunctions and are therefore well suited to processing language in a very\nefficient way. The proposed approach has the potential to open the black box of\nLLMs on the one hand, and might be a further step to unravel the neural\nprocesses underlying human language processing and cognition in general.",
      "tldr_zh": "本研究利用 GPT4 生成七种不同风格变体（基于 Aesop's fables 的十个叙事），以测试开源模型 BERT 的语言处理机制，旨在模拟和预测人类大脑的语言处理过程。研究方法包括将这些变体作为输入，分析 BERT 隐藏单元的激活模式，通过 multi-dimensional scaling 和 cluster analysis 进行聚类。结果发现，BERT 的早期层（层 1）主要根据风格聚类，而后期层（层 4-5）则基于叙事内容聚类，尽管 BERT 的层结构相似但功能各异，类似于人脑皮层的分工。该方法有助于打开 LLMs 的黑箱，并为理解人类语言处理和认知机制提供新假设。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02024v1",
      "published_date": "2024-05-03 11:56:13 UTC",
      "updated_date": "2024-05-03 11:56:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:45:20.650561"
    },
    {
      "arxiv_id": "2405.02016v1",
      "title": "Adversarial Botometer: Adversarial Analysis for Social Bot Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Shaghayegh Najari",
        "Davood Rafiee",
        "Mostafa Salehi",
        "Reza Farahbakhsh"
      ],
      "abstract": "Social bots play a significant role in many online social networks (OSN) as\nthey imitate human behavior. This fact raises difficult questions about their\ncapabilities and potential risks. Given the recent advances in Generative AI\n(GenAI), social bots are capable of producing highly realistic and complex\ncontent that mimics human creativity. As the malicious social bots emerge to\ndeceive people with their unrealistic content, identifying them and\ndistinguishing the content they produce has become an actual challenge for\nnumerous social platforms. Several approaches to this problem have already been\nproposed in the literature, but the proposed solutions have not been widely\nevaluated. To address this issue, we evaluate the behavior of a text-based bot\ndetector in a competitive environment where some scenarios are proposed:\n\\textit{First}, the tug-of-war between a bot and a bot detector is examined. It\nis interesting to analyze which party is more likely to prevail and which\ncircumstances influence these expectations. In this regard, we model the\nproblem as a synthetic adversarial game in which a conversational bot and a bot\ndetector are engaged in strategic online interactions. \\textit{Second}, the bot\ndetection model is evaluated under attack examples generated by a social bot;\nto this end, we poison the dataset with attack examples and evaluate the model\nperformance under this condition. \\textit{Finally}, to investigate the impact\nof the dataset, a cross-domain analysis is performed. Through our comprehensive\nevaluation of different categories of social bots using two benchmark datasets,\nwe were able to demonstrate some achivement that could be utilized in future\nworks.",
      "tldr_zh": "本研究分析了社会机器人（social bots）的检测挑战，特别是受生成式 AI（Generative AI）影响下其生成高度真实内容的潜力。论文提出Adversarial Botometer框架，通过模拟合成对抗游戏（synthetic adversarial game）评估机器人检测器与机器人的拉锯战；注入攻击样本（attack examples）来测试检测器在毒化数据集下的性能；并进行跨领域分析（cross-domain analysis）以评估数据集影响。实验结果显示，在两个基准数据集上，对不同类别社会机器人的全面评估揭示了检测器的优势和局限性，为未来改进社会机器人检测提供了宝贵见解。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02016v1",
      "published_date": "2024-05-03 11:28:21 UTC",
      "updated_date": "2024-05-03 11:28:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:45:31.373071"
    },
    {
      "arxiv_id": "2405.02375v2",
      "title": "The Sparse Tsetlin Machine: Sparse Representation with Active Literals",
      "title_zh": "翻译失败",
      "authors": [
        "Sebastian Østby",
        "Tobias M. Brambo",
        "Sondre Glimsdal"
      ],
      "abstract": "This paper introduces the Sparse Tsetlin Machine (STM), a novel Tsetlin\nMachine (TM) that processes sparse data efficiently. Traditionally, the TM does\nnot consider data characteristics such as sparsity, commonly seen in NLP\napplications and other bag-of-word-based representations. Consequently, a TM\nmust initialize, store, and process a significant number of zero values,\nresulting in excessive memory usage and computational time. Previous attempts\nat creating a sparse TM have predominantly been unsuccessful, primarily due to\ntheir inability to identify which literals are sufficient for TM training. By\nintroducing Active Literals (AL), the STM can focus exclusively on literals\nthat actively contribute to the current data representation, significantly\ndecreasing memory footprint and computational time while demonstrating\ncompetitive classification performance.",
      "tldr_zh": "本论文引入了Sparse Tsetlin Machine (STM)，一种新型的Tsetlin Machine (TM)，旨在高效处理NLP等领域常见的稀疏数据。传统TM忽略了数据的稀疏特性，导致必须初始化和处理大量零值，从而增加内存使用和计算时间。STM通过引入Active Literals (AL)机制，仅关注对当前数据表示有贡献的literals，显著降低了资源消耗，同时在分类性能上与传统方法保持竞争力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02375v2",
      "published_date": "2024-05-03 11:06:10 UTC",
      "updated_date": "2024-05-11 04:40:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:45:44.363643"
    },
    {
      "arxiv_id": "2405.01997v1",
      "title": "Exploring Combinatorial Problem Solving with Large Language Models: A Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo",
      "title_zh": "翻译失败",
      "authors": [
        "Mahmoud Masoud",
        "Ahmed Abdelhay",
        "Mohammed Elhenawy"
      ],
      "abstract": "Large Language Models (LLMs) are deep learning models designed to generate\ntext based on textual input. Although researchers have been developing these\nmodels for more complex tasks such as code generation and general reasoning,\nfew efforts have explored how LLMs can be applied to combinatorial problems. In\nthis research, we investigate the potential of LLMs to solve the Travelling\nSalesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments\nemploying various approaches, including zero-shot in-context learning, few-shot\nin-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned\nGPT-3.5 Turbo to solve a specific problem size and tested it using a set of\nvarious instance sizes. The fine-tuned models demonstrated promising\nperformance on problems identical in size to the training instances and\ngeneralized well to larger problems. Furthermore, to improve the performance of\nthe fine-tuned model without incurring additional training costs, we adopted a\nself-ensemble approach to improve the quality of the solutions.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在组合问题求解中的潜力，以 Travelling Salesman Problem (TSP) 为案例研究，使用 GPT-3.5 Turbo 进行实验。研究方法包括 zero-shot in-context learning、few-shot in-context learning 和 chain-of-thoughts (CoT) 技术，并对模型进行微调以适应特定问题规模。结果表明，微调后的模型在与训练实例相同规模的问题上表现出色，并能良好泛化到更大规模问题；此外，通过 self-ensemble 方法提升了解决方案质量，而无需额外训练成本。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01997v1",
      "published_date": "2024-05-03 10:54:14 UTC",
      "updated_date": "2024-05-03 10:54:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:45:56.961128"
    },
    {
      "arxiv_id": "2405.01988v1",
      "title": "Joint sentiment analysis of lyrics and audio in music",
      "title_zh": "音乐中歌词和音频的联合情感分析",
      "authors": [
        "Lea Schaab",
        "Anna Kruspe"
      ],
      "abstract": "Sentiment or mood can express themselves on various levels in music. In\nautomatic analysis, the actual audio data is usually analyzed, but the lyrics\ncan also play a crucial role in the perception of moods. We first evaluate\nvarious models for sentiment analysis based on lyrics and audio separately. The\ncorresponding approaches already show satisfactory results, but they also\nexhibit weaknesses, the causes of which we examine in more detail. Furthermore,\ndifferent approaches to combining the audio and lyrics results are proposed and\nevaluated. Considering both modalities generally leads to improved performance.\nWe investigate misclassifications and (also intentional) contradictions between\naudio and lyrics sentiment more closely, and identify possible causes. Finally,\nwe address fundamental problems in this research area, such as high\nsubjectivity, lack of data, and inconsistency in emotion taxonomies.",
      "tldr_zh": "本研究探讨了音乐中歌词和音频的联合情感分析（sentiment analysis），旨在评估单独基于歌词或音频的模型表现，并识别其弱点。研究提出了多种结合音频和歌词结果的策略，发现整合两种模态通常能提升分析性能，并深入分析了误分类以及音频与歌词情感间的矛盾（如故意设计）。此外，该领域存在根本挑战，包括情感分析的主观性、数据不足和情感分类（emotion taxonomies）的不一致性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "published at DAGA 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.01988v1",
      "published_date": "2024-05-03 10:42:17 UTC",
      "updated_date": "2024-05-03 10:42:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:46:09.468150"
    },
    {
      "arxiv_id": "2405.01984v1",
      "title": "A Penalty-Based Guardrail Algorithm for Non-Decreasing Optimization with Inequality Constraints",
      "title_zh": "一种基于罚函数的护栏算法，用于带有不等式约束的非递减优化",
      "authors": [
        "Ksenija Stepanovic",
        "Wendelin Böhmer",
        "Mathijs de Weerdt"
      ],
      "abstract": "Traditional mathematical programming solvers require long computational times\nto solve constrained minimization problems of complex and large-scale physical\nsystems. Therefore, these problems are often transformed into unconstrained\nones, and solved with computationally efficient optimization approaches based\non first-order information, such as the gradient descent method. However, for\nunconstrained problems, balancing the minimization of the objective function\nwith the reduction of constraint violations is challenging. We consider the\nclass of time-dependent minimization problems with increasing (possibly)\nnonlinear and non-convex objective function and non-decreasing (possibly)\nnonlinear and non-convex inequality constraints. To efficiently solve them, we\npropose a penalty-based guardrail algorithm (PGA). This algorithm adapts a\nstandard penalty-based method by dynamically updating the right-hand side of\nthe constraints with a guardrail variable which adds a margin to prevent\nviolations. We evaluate PGA on two novel application domains: a simplified\nmodel of a district heating system and an optimization model derived from\nlearned deep neural networks. Our method significantly outperforms mathematical\nprogramming solvers and the standard penalty-based method, and achieves better\nperformance and faster convergence than a state-of-the-art algorithm (IPDD)\nwithin a specified time limit.",
      "tldr_zh": "本研究针对复杂大规模的非递减优化问题（涉及非线性、非凸的目标函数和不等式约束），提出了一种基于惩罚的护栏算法（Penalty-Based Guardrail Algorithm, PGA）。PGA 通过动态更新约束的右边界值并添加护栏变量，防止约束违反，同时平衡目标函数最小化和约束满足。实验在简化区域供暖系统模型和从深度神经网络学得的优化模型上进行，结果显示，PGA 显著优于传统数学规划求解器和标准惩罚方法，并在指定时间内比最先进算法（IPDD）实现更快收敛和更好性能。",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01984v1",
      "published_date": "2024-05-03 10:37:34 UTC",
      "updated_date": "2024-05-03 10:37:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:46:22.773705"
    },
    {
      "arxiv_id": "2405.02374v1",
      "title": "Protein binding affinity prediction under multiple substitutions applying eGNNs on Residue and Atomic graphs combined with Language model information: eGRAL",
      "title_zh": "翻译失败",
      "authors": [
        "Arturo Fiorellini-Bernardis",
        "Sebastien Boyer",
        "Christoph Brunken",
        "Bakary Diallo",
        "Karim Beguir",
        "Nicolas Lopez-Carranza",
        "Oliver Bent"
      ],
      "abstract": "Protein-protein interactions (PPIs) play a crucial role in numerous\nbiological processes. Developing methods that predict binding affinity changes\nunder substitution mutations is fundamental for modelling and re-engineering\nbiological systems. Deep learning is increasingly recognized as a powerful tool\ncapable of bridging the gap between in-silico predictions and in-vitro\nobservations. With this contribution, we propose eGRAL, a novel SE(3)\nequivariant graph neural network (eGNN) architecture designed for predicting\nbinding affinity changes from multiple amino acid substitutions in protein\ncomplexes. eGRAL leverages residue, atomic and evolutionary scales, thanks to\nfeatures extracted from protein large language models. To address the limited\navailability of large-scale affinity assays with structural information, we\ngenerate a simulated dataset comprising approximately 500,000 data points. Our\nmodel is pre-trained on this dataset, then fine-tuned and tested on\nexperimental data.",
      "tldr_zh": "本文提出 eGRAL，一种基于 SE(3) 等变图神经网络 (eGNN) 的模型，用于预测蛋白质复合物中多个氨基酸替换导致的结合亲和力变化，从而支持蛋白质-蛋白质相互作用 (PPIs) 的建模和再工程。eGRAL 整合了残基、原子和进化尺度特征，这些特征通过蛋白质大语言模型提取，以提升预测准确性。为解决实验数据有限的问题，研究者生成了约 50 万个模拟数据集，并在该数据集上预训练模型，随后在真实实验数据上微调和测试。该方法展示了深度学习在桥接理论预测与实验观察方面的潜力。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02374v1",
      "published_date": "2024-05-03 10:33:19 UTC",
      "updated_date": "2024-05-03 10:33:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:46:34.619255"
    },
    {
      "arxiv_id": "2405.01983v1",
      "title": "Model-based reinforcement learning for protein backbone design",
      "title_zh": "基于模型的强化学习用于蛋白质主链设计",
      "authors": [
        "Frederic Renard",
        "Cyprien Courtot",
        "Alfredo Reichlin",
        "Oliver Bent"
      ],
      "abstract": "Designing protein nanomaterials of predefined shape and characteristics has\nthe potential to dramatically impact the medical industry. Machine learning\n(ML) has proven successful in protein design, reducing the need for expensive\nwet lab experiment rounds. However, challenges persist in efficiently exploring\nthe protein fitness landscapes to identify optimal protein designs. In\nresponse, we propose the use of AlphaZero to generate protein backbones,\nmeeting shape and structural scoring requirements. We extend an existing Monte\nCarlo tree search (MCTS) framework by incorporating a novel threshold-based\nreward and secondary objectives to improve design precision. This innovation\nconsiderably outperforms existing approaches, leading to protein backbones that\nbetter respect structural scores. The application of AlphaZero is novel in the\ncontext of protein backbone design and demonstrates promising performance.\nAlphaZero consistently surpasses baseline MCTS by more than 100% in top-down\nprotein design tasks. Additionally, our application of AlphaZero with secondary\nobjectives uncovers further promising outcomes, indicating the potential of\nmodel-based reinforcement learning (RL) in navigating the intricate and nuanced\naspects of protein design",
      "tldr_zh": "本研究提出了一种基于模型的强化学习（reinforcement learning, RL）方法，用于蛋白质主链设计，旨在高效探索蛋白适应性景观以生成符合形状和结构评分要求的蛋白纳米材料。研究扩展了现有的Monte Carlo tree search (MCTS)框架，通过引入阈值-based reward和次要目标，提高了设计精度，并首次将AlphaZero应用于这一领域。实验结果显示，AlphaZero在顶层蛋白设计任务中比基线MCTS性能提升超过100%，并展示了其在处理蛋白设计复杂性的潜力，为医疗行业蛋白质纳米材料开发提供新途径。",
      "categories": [
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01983v1",
      "published_date": "2024-05-03 10:24:33 UTC",
      "updated_date": "2024-05-03 10:24:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:46:45.448987"
    },
    {
      "arxiv_id": "2405.02372v2",
      "title": "Triadic-OCD: Asynchronous Online Change Detection with Provable Robustness, Optimality, and Convergence",
      "title_zh": "Triadic-OCD：异步在线变化检测，具有可证明的鲁棒性、最优性和收敛性",
      "authors": [
        "Yancheng Huang",
        "Kai Yang",
        "Zelin Zhu",
        "Leian Chen"
      ],
      "abstract": "The primary goal of online change detection (OCD) is to promptly identify\nchanges in the data stream. OCD problem find a wide variety of applications in\ndiverse areas, e.g., security detection in smart grids and intrusion detection\nin communication networks. Prior research usually assumes precise knowledge of\nthe system parameters. Nevertheless, this presumption often proves unattainable\nin practical scenarios due to factors such as estimation errors, system\nupdates, etc. This paper aims to take the first attempt to develop a\ntriadic-OCD framework with certifiable robustness, provable optimality, and\nguaranteed convergence. In addition, the proposed triadic-OCD algorithm can be\nrealized in a fully asynchronous distributed manner, easing the necessity of\ntransmitting the data to a single server. This asynchronous mechanism could\nalso mitigate the straggler issue that faced by traditional synchronous\nalgorithm. Moreover, the non-asymptotic convergence property of Triadic-OCD is\ntheoretically analyzed, and its iteration complexity to achieve an\n$\\epsilon$-optimal point is derived. Extensive experiments have been conducted\nto elucidate the effectiveness of the proposed method.",
      "tldr_zh": "本论文针对在线变化检测（OCD）问题，提出了一种名为 Triadic-OCD 的框架，该框架旨在快速识别数据流中的变化，同时处理实际场景中系统参数不确定性的挑战，如估计错误和系统更新。Triadic-OCD 采用异步分布式算法，确保了可证实的鲁棒性、可证明的最优性和收敛性保证，并通过非渐进分析导出了达到 ε-optimal point 的迭代复杂度。实验结果证明，该方法在智能电网安全检测和通信网络入侵检测等应用中表现出色，有效缓解了传统同步算法的拖延问题。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Accepted at ICML2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02372v2",
      "published_date": "2024-05-03 10:10:11 UTC",
      "updated_date": "2024-06-04 11:40:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:46:57.784634"
    },
    {
      "arxiv_id": "2405.01974v1",
      "title": "Multitask Extension of Geometrically Aligned Transfer Encoder",
      "title_zh": "翻译失败",
      "authors": [
        "Sung Moon Ko",
        "Sumin Lee",
        "Dae-Woong Jeong",
        "Hyunseung Kim",
        "Chanhui Lee",
        "Soorin Yim",
        "Sehui Han"
      ],
      "abstract": "Molecular datasets often suffer from a lack of data. It is well-known that\ngathering data is difficult due to the complexity of experimentation or\nsimulation involved. Here, we leverage mutual information across different\ntasks in molecular data to address this issue. We extend an algorithm that\nutilizes the geometric characteristics of the encoding space, known as the\nGeometrically Aligned Transfer Encoder (GATE), to a multi-task setup. Thus, we\nconnect multiple molecular tasks by aligning the curved coordinates onto\nlocally flat coordinates, ensuring the flow of information from source tasks to\nsupport performance on target data.",
      "tldr_zh": "该研究针对分子数据集数据不足的问题，提出扩展 Geometrically Aligned Transfer Encoder (GATE) 算法至多任务设置，利用不同任务间的互信息来提升性能。通过将弯曲坐标对齐到局部平坦坐标，该方法连接多个分子任务，确保源任务的信息流向目标任务。实验结果表明，这种多任务扩展有助于缓解数据稀缺性，提高了目标任务的整体表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.01974v1",
      "published_date": "2024-05-03 09:57:44 UTC",
      "updated_date": "2024-05-03 09:57:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:47:08.072351"
    },
    {
      "arxiv_id": "2405.01963v1",
      "title": "From Attack to Defense: Insights into Deep Learning Security Measures in Black-Box Settings",
      "title_zh": "翻译失败",
      "authors": [
        "Firuz Juraev",
        "Mohammed Abuhamad",
        "Eric Chan-Tin",
        "George K. Thiruvathukal",
        "Tamer Abuhmed"
      ],
      "abstract": "Deep Learning (DL) is rapidly maturing to the point that it can be used in\nsafety- and security-crucial applications. However, adversarial samples, which\nare undetectable to the human eye, pose a serious threat that can cause the\nmodel to misbehave and compromise the performance of such applications.\nAddressing the robustness of DL models has become crucial to understanding and\ndefending against adversarial attacks. In this study, we perform comprehensive\nexperiments to examine the effect of adversarial attacks and defenses on\nvarious model architectures across well-known datasets. Our research focuses on\nblack-box attacks such as SimBA, HopSkipJump, MGAAttack, and boundary attacks,\nas well as preprocessor-based defensive mechanisms, including bits squeezing,\nmedian smoothing, and JPEG filter. Experimenting with various models, our\nresults demonstrate that the level of noise needed for the attack increases as\nthe number of layers increases. Moreover, the attack success rate decreases as\nthe number of layers increases. This indicates that model complexity and\nrobustness have a significant relationship. Investigating the diversity and\nrobustness relationship, our experiments with diverse models show that having a\nlarge number of parameters does not imply higher robustness. Our experiments\nextend to show the effects of the training dataset on model robustness. Using\nvarious datasets such as ImageNet-1000, CIFAR-100, and CIFAR-10 are used to\nevaluate the black-box attacks. Considering the multiple dimensions of our\nanalysis, e.g., model complexity and training dataset, we examined the behavior\nof black-box attacks when models apply defenses. Our results show that applying\ndefense strategies can significantly reduce attack effectiveness. This research\nprovides in-depth analysis and insight into the robustness of DL models against\nvarious attacks, and defenses.",
      "tldr_zh": "这篇论文探讨了深度学习（Deep Learning）模型在黑盒设置下的安全措施，通过全面实验分析对抗攻击和防御机制的影响。研究聚焦于黑盒攻击如 SimBA、HopSkipJump、MGAAttack 和 boundary attacks，以及防御策略如 bits squeezing、median smoothing 和 JPEG filter，在多种模型架构和数据集（如 ImageNet-1000、CIFAR-100 和 CIFAR-10）上进行测试。结果显示，模型层数增加会导致攻击所需噪声增多且成功率降低，表明模型复杂性与鲁棒性密切相关；然而，参数数量多并不直接提升鲁棒性，且训练数据集类型也会影响模型的抗攻击能力。该研究证明，应用防御策略能显著降低攻击有效性，为提升 Deep Learning 模型的安全性提供了关键洞见。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01963v1",
      "published_date": "2024-05-03 09:40:47 UTC",
      "updated_date": "2024-05-03 09:40:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:47:21.961922"
    },
    {
      "arxiv_id": "2405.02371v1",
      "title": "Architecture of a Cortex Inspired Hierarchical Event Recaller",
      "title_zh": "翻译失败",
      "authors": [
        "Valentin Puente Varona"
      ],
      "abstract": "This paper proposes a new approach to Machine Learning (ML) that focuses on\nunsupervised continuous context-dependent learning of complex patterns.\nAlthough the proposal is partly inspired by some of the current knowledge about\nthe structural and functional properties of the mammalian brain, we do not\nclaim that biological systems work in an analogous way (nor the opposite).\nBased on some properties of the cerebellar cortex and adjacent structures, a\nproposal suitable for practical problems is presented. A synthetic structure\ncapable of identifying and predicting complex temporal series will be defined\nand experimentally tested. The system relies heavily on prediction to help\nidentify and learn patterns based on previously acquired contextual knowledge.\nAs a proof of concept, the proposed system is shown to be able to learn,\nidentify and predict a remarkably complex temporal series such as human speech,\nwith no prior knowledge. From raw data, without any adaptation in the core\nalgorithm, the system is able to identify certain speech structures from a set\nof Spanish sentences. Unlike conventional ML, the proposal can learn with a\nreduced training set. Although the idea can be applied to a constrained\nproblem, such as the detection of unknown vocabulary in a speech, it could be\nused in more applications, such as vision, or (by incorporating the missing\nbiological periphery) fit into other ML techniques. Given the trivial\ncomputational primitives used, a potential hardware implementation will be\nremarkably frugal. Coincidentally, the proposed model not only conforms to a\nplausible functional framework for biological systems but may also explain many\nelusive cognitive phenomena.",
      "tldr_zh": "这篇论文提出了一种新颖的机器学习(ML)方法，专注于无监督的连续上下文依赖学习复杂模式，受哺乳动物大脑（如小脑皮层）结构和功能启发，但不直接模拟生物系统。方法构建了一个层次化事件回忆器(Hierarchical Event Recaller)，通过预测机制基于先前上下文知识来识别和学习复杂时间序列。实验证明，该系统能从原始数据无需先验知识学习、识别和预测人类语音，如西班牙语句子结构，并使用较小的训练集。总体上，该方法不仅适用于语音检测和视觉等领域，还具有高效硬件实现潜力，并可能解释某些认知现象。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02371v1",
      "published_date": "2024-05-03 09:36:16 UTC",
      "updated_date": "2024-05-03 09:36:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:47:33.862078"
    },
    {
      "arxiv_id": "2405.02370v1",
      "title": "Neuromorphic Correlates of Artificial Consciousness",
      "title_zh": "翻译失败",
      "authors": [
        "Anwaar Ulhaq"
      ],
      "abstract": "The concept of neural correlates of consciousness (NCC), which suggests that\nspecific neural activities are linked to conscious experiences, has gained\nwidespread acceptance. This acceptance is based on a wealth of evidence from\nexperimental studies, brain imaging techniques such as fMRI and EEG, and\ntheoretical frameworks like integrated information theory (IIT) within\nneuroscience and the philosophy of mind. This paper explores the potential for\nartificial consciousness by merging neuromorphic design and architecture with\nbrain simulations. It proposes the Neuromorphic Correlates of Artificial\nConsciousness (NCAC) as a theoretical framework. While the debate on artificial\nconsciousness remains contentious due to our incomplete grasp of consciousness,\nthis work may raise eyebrows and invite criticism. Nevertheless, this\noptimistic and forward-thinking approach is fueled by insights from the Human\nBrain Project, advancements in brain imaging like EEG and fMRI, and recent\nstrides in AI and computing, including quantum and neuromorphic designs.\nAdditionally, this paper outlines how machine learning can play a role in\ncrafting artificial consciousness, aiming to realise machine consciousness and\nawareness in the future.",
      "tldr_zh": "本论文探讨了神经相关物 (neural correlates of consciousness, NCC) 在人工意识领域的应用，提出了一种理论框架——Neuromorphic Correlates of Artificial Consciousness (NCAC)，将神经形态设计与脑模拟相结合。作者基于实验证据、脑成像技术（如 fMRI 和 EEG）以及理论如 integrated information theory (IIT)，旨在弥合人类意识理解与人工智能的差距，尽管这一观点可能引发争议。论文强调，借助 Human Brain Project 和 AI 进展（如量子和神经形态计算），机器学习可用于构建未来的机器意识和觉知，从而推动自主智能系统的开发。",
      "categories": [
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "13 Pages, 8 Figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02370v1",
      "published_date": "2024-05-03 09:27:51 UTC",
      "updated_date": "2024-05-03 09:27:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:47:44.390716"
    },
    {
      "arxiv_id": "2405.01952v1",
      "title": "Three Quantization Regimes for ReLU Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Weigutian Ou",
        "Philipp Schenkel",
        "Helmut Bölcskei"
      ],
      "abstract": "We establish the fundamental limits in the approximation of Lipschitz\nfunctions by deep ReLU neural networks with finite-precision weights.\nSpecifically, three regimes, namely under-, over-, and proper quantization, in\nterms of minimax approximation error behavior as a function of network weight\nprecision, are identified. This is accomplished by deriving nonasymptotic tight\nlower and upper bounds on the minimax approximation error. Notably, in the\nproper-quantization regime, neural networks exhibit memory-optimality in the\napproximation of Lipschitz functions. Deep networks have an inherent advantage\nover shallow networks in achieving memory-optimality. We also develop the\nnotion of depth-precision tradeoff, showing that networks with high-precision\nweights can be converted into functionally equivalent deeper networks with\nlow-precision weights, while preserving memory-optimality. This idea is\nreminiscent of sigma-delta analog-to-digital conversion, where oversampling\nrate is traded for resolution in the quantization of signal samples. We improve\nupon the best-known ReLU network approximation results for Lipschitz functions\nand describe a refinement of the bit extraction technique which could be of\nindependent general interest.",
      "tldr_zh": "本研究探讨了深层ReLU神经网络在有限精度权重下近似Lipschitz函数的基本极限，识别了三种量化制度（under-quantization、over-quantization和proper-quantization），这些制度基于网络权重精度与minimax近似误差的行为。研究通过导出的非渐近紧密下限和上限界，证明在proper-quantization制度中，神经网络实现了内存最优性，且深层网络比浅层网络更具优势。论文还发展了深度-precision权衡的概念，展示了高精度权重网络可转换为功能等价的更深层低精度权重网络，同时保持内存最优性，这类似于sigma-delta模数转换中的过采样率与分辨率交换。最后，该工作改进了现有ReLU网络对Lipschitz函数的近似结果，并优化了bit extraction技术。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01952v1",
      "published_date": "2024-05-03 09:27:31 UTC",
      "updated_date": "2024-05-03 09:27:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:47:58.461911"
    },
    {
      "arxiv_id": "2405.01943v3",
      "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyu Guo",
        "Hidetaka Kamigaito",
        "Taro Wanatnabe"
      ],
      "abstract": "The rapid advancement in Large Language Models (LLMs) has markedly enhanced\nthe capabilities of language understanding and generation. However, the\nsubstantial model size poses hardware challenges, affecting both memory size\nfor serving and inference latency for token generation. To address those\nchallenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a\nnovel method for the recent prevalent GLU-based LLMs pruning, which\nincorporates structural dependency into the weight magnitude-based unstructured\npruning. We introduce an MLP-specific pruning metric that evaluates the\nimportance of each weight by jointly considering its magnitude and its\ncorresponding MLP intermediate activation norms. DaSS facilitates a balance\nbetween the adaptability offered by unstructured pruning and the structural\nconsistency inherent in dependency-based structured pruning. Empirical\nevaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS\nnot only outperforms both SparseGPT and Wanda in achieving hardware-friendly\nN:M sparsity patterns but also maintains the computational efficiency of Wanda.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)的模型规模带来的内存和推理延迟问题，提出了一种新的剪枝方法Dependency-aware Semi-Structured Sparsity (DaSS)，专注于GLU变体的LLMs。DaSS通过结合权重大小的无结构剪枝和结构依赖，将一个MLP特定的指标纳入评估，每个权重的重要性同时考虑其大小和对应的MLP中间激活范数，从而平衡了剪枝的适应性和结构一致性。在LLaMA2、Mistral和Gemma模型家族的实证评估中，DaSS在实现硬件友好的N:M sparsity模式方面优于SparseGPT和Wanda，同时保持了Wanda的计算效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01943v3",
      "published_date": "2024-05-03 09:13:13 UTC",
      "updated_date": "2024-10-20 12:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:48:10.147776"
    },
    {
      "arxiv_id": "2405.02369v1",
      "title": "No One-Size-Fits-All Neurons: Task-based Neurons for Artificial Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Feng-Lei Fan",
        "Meng Wang",
        "Hang-Cheng Dong",
        "Jianwei Ma",
        "Tieyong Zeng"
      ],
      "abstract": "Biologically, the brain does not rely on a single type of neuron that\nuniversally functions in all aspects. Instead, it acts as a sophisticated\ndesigner of task-based neurons. In this study, we address the following\nquestion: since the human brain is a task-based neuron user, can the artificial\nnetwork design go from the task-based architecture design to the task-based\nneuron design? Since methodologically there are no one-size-fits-all neurons,\ngiven the same structure, task-based neurons can enhance the feature\nrepresentation ability relative to the existing universal neurons due to the\nintrinsic inductive bias for the task. Specifically, we propose a two-step\nframework for prototyping task-based neurons. First, symbolic regression is\nused to identify optimal formulas that fit input data by utilizing base\nfunctions such as logarithmic, trigonometric, and exponential functions. We\nintroduce vectorized symbolic regression that stacks all variables in a vector\nand regularizes each input variable to perform the same computation, which can\nexpedite the regression speed, facilitate parallel computation, and avoid\noverfitting. Second, we parameterize the acquired elementary formula to make\nparameters learnable, which serves as the aggregation function of the neuron.\nThe activation functions such as ReLU and the sigmoidal functions remain the\nsame because they have proven to be good. Empirically, experimental results on\nsynthetic data, classic benchmarks, and real-world applications show that the\nproposed task-based neuron design is not only feasible but also delivers\ncompetitive performance over other state-of-the-art models.",
      "tldr_zh": "本研究提出了一种任务-based神经元设计方法，旨在超越传统的人工神经网络中通用的“one-size-fits-all”神经元，通过模仿人类大脑的机制来提升特征表示能力。具体而言，该方法采用两步框架：首先，使用symbolic regression结合基础函数（如对数、三角和指数函数）来识别最优公式，并引入vectorized symbolic regression以加速计算并避免过拟合；其次，将获得的公式参数化作为神经元的聚合函数，而激活函数如ReLU保持不变。实验结果显示，这种任务-based神经元设计在合成数据、经典基准和真实应用中不仅可行，还在性能上超过了现有最先进模型。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02369v1",
      "published_date": "2024-05-03 09:12:46 UTC",
      "updated_date": "2024-05-03 09:12:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:48:22.920135"
    },
    {
      "arxiv_id": "2405.01934v1",
      "title": "Impact of Architectural Modifications on Deep Learning Adversarial Robustness",
      "title_zh": "架构修改对深度学习对抗鲁棒性的影响",
      "authors": [
        "Firuz Juraev",
        "Mohammed Abuhamad",
        "Simon S. Woo",
        "George K Thiruvathukal",
        "Tamer Abuhmed"
      ],
      "abstract": "Rapid advancements of deep learning are accelerating adoption in a wide\nvariety of applications, including safety-critical applications such as\nself-driving vehicles, drones, robots, and surveillance systems. These\nadvancements include applying variations of sophisticated techniques that\nimprove the performance of models. However, such models are not immune to\nadversarial manipulations, which can cause the system to misbehave and remain\nunnoticed by experts. The frequency of modifications to existing deep learning\nmodels necessitates thorough analysis to determine the impact on models'\nrobustness. In this work, we present an experimental evaluation of the effects\nof model modifications on deep learning model robustness using adversarial\nattacks. Our methodology involves examining the robustness of variations of\nmodels against various adversarial attacks. By conducting our experiments, we\naim to shed light on the critical issue of maintaining the reliability and\nsafety of deep learning models in safety- and security-critical applications.\nOur results indicate the pressing demand for an in-depth assessment of the\neffects of model changes on the robustness of models.",
      "tldr_zh": "本研究探讨了深度学习模型架构修改对对抗鲁棒性的影响，特别是在安全关键应用（如自动驾驶车辆和监控系统）中，这些修改虽提升了模型性能，却可能增加对adversarial attacks的易受性。作者通过实验评估了多种模型变体在不同adversarial attacks下的鲁棒性表现。结果显示，模型修改会显著影响其可靠性，强调了在安全和安全关键应用中进行深入评估的迫切需求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01934v1",
      "published_date": "2024-05-03 08:58:38 UTC",
      "updated_date": "2024-05-03 08:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:48:34.691396"
    },
    {
      "arxiv_id": "2405.01924v2",
      "title": "Semi-Parametric Retrieval via Binary Bag-of-Tokens Index",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Zhou",
        "Li Dong",
        "Furu Wei",
        "Lei Chen"
      ],
      "abstract": "Information retrieval has transitioned from standalone systems into essential\ncomponents across broader applications, with indexing efficiency,\ncost-effectiveness, and freshness becoming increasingly critical yet often\noverlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval\n(SiDR), a bi-encoder retrieval framework that decouples retrieval index from\nneural parameters to enable efficient, low-cost, and parameter-agnostic\nindexing for emerging use cases. Specifically, in addition to using embeddings\nas indexes like existing neural retrieval methods, SiDR supports a\nnon-parametric tokenization index for search, achieving BM25-like indexing\ncomplexity with significantly better effectiveness. Our comprehensive\nevaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms\nboth neural and term-based retrieval baselines under the same indexing\nworkload: (i) When using an embedding-based index, SiDR exceeds the performance\nof conventional neural retrievers while maintaining similar training\ncomplexity; (ii) When using a tokenization-based index, SiDR drastically\nreduces indexing cost and time, matching the complexity of traditional\nterm-based retrieval, while consistently outperforming BM25 on all in-domain\ndatasets; (iii) Additionally, we introduce a late parametric mechanism that\nmatches BM25 index preparation time while outperforming other neural retrieval\nbaselines in effectiveness.",
      "tldr_zh": "本研究提出了一种半参数检索框架SiDR（SemI-parametric Disentangled Retrieval），通过bi-encoder架构将检索索引与神经参数解耦，实现高效、低成本的索引，支持嵌入(embeddings)索引和非参数的tokenization索引，以提升信息检索在新兴应用中的性能。SiDR在16个检索基准测试中优于神经和基于术语的基线：在嵌入-based索引时，超越传统神经检索器，同时保持相似的训练复杂度；在tokenization-based索引时，大幅降低成本和时间，效果显著优于BM25。 additionally，该框架引入late parametric机制，能匹配BM25的索引准备时间，同时在检索效果上领先其他神经检索基线。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01924v2",
      "published_date": "2024-05-03 08:34:13 UTC",
      "updated_date": "2025-03-06 10:39:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:48:47.850892"
    },
    {
      "arxiv_id": "2405.01906v1",
      "title": "Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Changliang Zhou",
        "Xi Lin",
        "Zhenkun Wang",
        "Xialiang Tong",
        "Mingxuan Yuan",
        "Qingfu Zhang"
      ],
      "abstract": "The neural combinatorial optimization (NCO) approach has shown great\npotential for solving routing problems without the requirement of expert\nknowledge. However, existing constructive NCO methods cannot directly solve\nlarge-scale instances, which significantly limits their application prospects.\nTo address these crucial shortcomings, this work proposes a novel\nInstance-Conditioned Adaptation Model (ICAM) for better large-scale\ngeneralization of neural combinatorial optimization. In particular, we design a\npowerful yet lightweight instance-conditioned adaptation module for the NCO\nmodel to generate better solutions for instances across different scales. In\naddition, we develop an efficient three-stage reinforcement learning-based\ntraining scheme that enables the model to learn cross-scale features without\nany labeled optimal solution. Experimental results show that our proposed\nmethod is capable of obtaining excellent results with a very fast inference\ntime in solving Traveling Salesman Problems (TSPs) and Capacitated Vehicle\nRouting Problems (CVRPs) across different scales. To the best of our knowledge,\nour model achieves state-of-the-art performance among all RL-based constructive\nmethods for TSP and CVRP with up to 1,000 nodes.",
      "tldr_zh": "本文提出了一种新的 Instance-Conditioned Adaptation Model (ICAM)，旨在提升 Neural Combinatorial Optimization (NCO) 在大规模实例上的泛化能力，解决现有方法无法直接处理大规模路由问题的局限性。ICAM 包括一个轻量级实例条件适应模块，并结合三阶段强化学习训练方案，使模型能够在不同规模下学习跨规模特征，而无需标记的最优解。实验结果显示，该方法在 Traveling Salesman Problems (TSPs) 和 Capacitated Vehicle Routing Problems (CVRPs) 上取得了 state-of-the-art 性能，支持多达 1,000 节点的实例，并显著提高了推理效率。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.01906v1",
      "published_date": "2024-05-03 08:00:19 UTC",
      "updated_date": "2024-05-03 08:00:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:49:00.641470"
    },
    {
      "arxiv_id": "2405.01886v1",
      "title": "Aloe: A Family of Fine-tuned Open Healthcare LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Ashwin Kumar Gururajan",
        "Enrique Lopez-Cuena",
        "Jordi Bayarri-Planas",
        "Adrian Tormos",
        "Daniel Hinjos",
        "Pablo Bernabeu-Perez",
        "Anna Arias-Duart",
        "Pablo Agustin Martin-Torres",
        "Lucia Urcelay-Ganzabal",
        "Marta Gonzalez-Mallo",
        "Sergio Alvarez-Napagao",
        "Eduard Ayguadé-Parra",
        "Ulises Cortés Dario Garcia-Gasulla"
      ],
      "abstract": "As the capabilities of Large Language Models (LLMs) in healthcare and\nmedicine continue to advance, there is a growing need for competitive\nopen-source models that can safeguard public interest. With the increasing\navailability of highly competitive open base models, the impact of continued\npre-training is increasingly uncertain. In this work, we explore the role of\ninstruct tuning, model merging, alignment, red teaming and advanced inference\nschemes, as means to improve current open models. To that end, we introduce the\nAloe family, a set of open medical LLMs highly competitive within its scale\nrange. Aloe models are trained on the current best base models (Mistral, LLaMA\n3), using a new custom dataset which combines public data sources improved with\nsynthetic Chain of Thought (CoT). Aloe models undergo an alignment phase,\nbecoming one of the first few policy-aligned open healthcare LLM using Direct\nPreference Optimization, setting a new standard for ethical performance in\nhealthcare LLMs. Model evaluation expands to include various bias and toxicity\ndatasets, a dedicated red teaming effort, and a much-needed risk assessment for\nhealthcare LLMs. Finally, to explore the limits of current LLMs in inference,\nwe study several advanced prompt engineering strategies to boost performance\nacross benchmarks, yielding state-of-the-art results for open healthcare 7B\nLLMs, unprecedented at this scale.",
      "tldr_zh": "本文介绍了Aloe系列开源医疗LLMs，通过对当前最佳基础模型（如Mistral和LLaMA 3）进行instruct tuning、模型合并和alignment（使用Direct Preference Optimization），结合自定义数据集（包括合成Chain of Thought数据），以提升模型在医疗领域的性能和伦理标准。Aloe模型经过red teaming和风险评估，显著减少了偏差和毒性问题，成为首批政策对齐的开源医疗LLMs。实验结果显示，该系列模型在基准测试中实现了开源医疗7B LLMs的state-of-the-art性能，突显了高级提示工程策略的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Five appendix",
      "pdf_url": "http://arxiv.org/pdf/2405.01886v1",
      "published_date": "2024-05-03 07:14:07 UTC",
      "updated_date": "2024-05-03 07:14:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:49:11.530520"
    },
    {
      "arxiv_id": "2405.01882v1",
      "title": "Millimeter Wave Radar-based Human Activity Recognition for Healthcare Monitoring Robot",
      "title_zh": "翻译失败",
      "authors": [
        "Zhanzhong Gu",
        "Xiangjian He",
        "Gengfa Fang",
        "Chengpei Xu",
        "Feng Xia",
        "Wenjing Jia"
      ],
      "abstract": "Healthcare monitoring is crucial, especially for the daily care of elderly\nindividuals living alone. It can detect dangerous occurrences, such as falls,\nand provide timely alerts to save lives. Non-invasive millimeter wave (mmWave)\nradar-based healthcare monitoring systems using advanced human activity\nrecognition (HAR) models have recently gained significant attention. However,\nthey encounter challenges in handling sparse point clouds, achieving real-time\ncontinuous classification, and coping with limited monitoring ranges when\nstatically mounted. To overcome these limitations, we propose RobHAR, a movable\nrobot-mounted mmWave radar system with lightweight deep neural networks for\nreal-time monitoring of human activities. Specifically, we first propose a\nsparse point cloud-based global embedding to learn the features of point clouds\nusing the light-PointNet (LPN) backbone. Then, we learn the temporal pattern\nwith a bidirectional lightweight LSTM model (BiLiLSTM). In addition, we\nimplement a transition optimization strategy, integrating the Hidden Markov\nModel (HMM) with Connectionist Temporal Classification (CTC) to improve the\naccuracy and robustness of the continuous HAR. Our experiments on three\ndatasets indicate that our method significantly outperforms the previous\nstudies in both discrete and continuous HAR tasks. Finally, we deploy our\nsystem on a movable robot-mounted edge computing platform, achieving flexible\nhealthcare monitoring in real-world scenarios.",
      "tldr_zh": "该论文提出 RobHAR，一种基于 mmWave 雷达的可移动机器人系统，用于实时监控独居老人的健康活动识别 (HAR)，以解决现有系统在处理稀疏点云、实时连续分类和监控范围有限的挑战。系统采用 light-PointNet (LPN) 骨干网络学习点云特征、双向轻量级 LSTM (BiLiLSTM) 捕捉时间模式，并结合 Hidden Markov Model (HMM) 和 Connectionist Temporal Classification (CTC) 的过渡优化策略，提高 HAR 的准确性和鲁棒性。在三个数据集上的实验表明，该方法在离散和连续 HAR 任务中显著优于先前研究。最后，系统部署在机器人边缘计算平台，实现灵活的真实场景医疗监控。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01882v1",
      "published_date": "2024-05-03 06:57:59 UTC",
      "updated_date": "2024-05-03 06:57:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:49:23.859647"
    },
    {
      "arxiv_id": "2405.01859v2",
      "title": "AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research",
      "title_zh": "翻译失败",
      "authors": [
        "Riley Simmons-Edler",
        "Ryan Badman",
        "Shayne Longpre",
        "Kanaka Rajan"
      ],
      "abstract": "The recent embrace of machine learning (ML) in the development of autonomous\nweapons systems (AWS) creates serious risks to geopolitical stability and the\nfree exchange of ideas in AI research. This topic has received comparatively\nlittle attention of late compared to risks stemming from superintelligent\nartificial general intelligence (AGI), but requires fewer assumptions about the\ncourse of technological development and is thus a nearer-future issue. ML is\nalready enabling the substitution of AWS for human soldiers in many battlefield\nroles, reducing the upfront human cost, and thus political cost, of waging\noffensive war. In the case of peer adversaries, this increases the likelihood\nof \"low intensity\" conflicts which risk escalation to broader warfare. In the\ncase of non-peer adversaries, it reduces the domestic blowback to wars of\naggression. This effect can occur regardless of other ethical issues around the\nuse of military AI such as the risk of civilian casualties, and does not\nrequire any superhuman AI capabilities. Further, the military value of AWS\nraises the specter of an AI-powered arms race and the misguided imposition of\nnational security restrictions on AI research. Our goal in this paper is to\nraise awareness among the public and ML researchers on the near-future risks\nposed by full or near-full autonomy in military technology, and we provide\nregulatory suggestions to mitigate these risks. We call upon AI policy experts\nand the defense AI community in particular to embrace transparency and caution\nin their development and deployment of AWS to avoid the negative effects on\nglobal stability and AI research that we highlight here.",
      "tldr_zh": "该论文分析了机器学习（ML）在自主武器系统（AWS）中的应用，可能导致的地缘政治不稳定和对AI研究自由交换的威胁，与超智能人工智能（AGI）风险相比，这是更近期的现实问题。研究发现，AWS取代人类士兵降低了战争的人力和政治成本，从而增加低强度冲突的发生概率，并可能引发大国间的军备竞赛或对AI研究的国家安全限制。这些风险不依赖超人类AI能力，论文呼吁公众和ML研究者提高警惕，并提出监管建议，如增强透明度和谨慎部署，以缓解对全球稳定和AI创新的负面影响。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages, 1 figure, in ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.01859v2",
      "published_date": "2024-05-03 05:19:45 UTC",
      "updated_date": "2024-05-31 23:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:49:34.806980"
    },
    {
      "arxiv_id": "2405.01851v1",
      "title": "Deep Learning Inference on Heterogeneous Mobile Processors: Potentials and Pitfalls",
      "title_zh": "异构移动处理器上的深度学习推理：潜力与陷阱",
      "authors": [
        "Sicong Liu",
        "Wentao Zhou",
        "Zimu Zhou",
        "Bin Guo",
        "Minfan Wang",
        "Cheng Fang",
        "Zheng Lin",
        "Zhiwen Yu"
      ],
      "abstract": "There is a growing demand to deploy computation-intensive deep learning (DL)\nmodels on resource-constrained mobile devices for real-time intelligent\napplications. Equipped with a variety of processing units such as CPUs, GPUs,\nand NPUs, the mobile devices hold potential to accelerate DL inference via\nparallel execution across heterogeneous processors. Various efficient parallel\nmethods have been explored to optimize computation distribution, achieve load\nbalance, and minimize communication cost across processors. Yet their practical\neffectiveness in the dynamic and diverse real-world mobile environment is less\nexplored. This paper presents a holistic empirical study to assess the\ncapabilities and challenges associated with parallel DL inference on\nheterogeneous mobile processors. Through carefully designed experiments\ncovering various DL models, mobile software/hardware environments, workload\npatterns, and resource availability, we identify limitations of existing\ntechniques and highlight opportunities for cross-level optimization.",
      "tldr_zh": "这篇论文探讨了在资源受限的移动设备上部署计算密集型深度学习（DL）模型的需求，以及利用异构处理器（如 CPUs、GPUs 和 NPUs）通过并行执行来加速 DL 推理的潜力。作者通过全面实证研究，设计实验涵盖多种 DL 模型、移动软件/硬件环境、工作负载模式和资源可用性，以评估现有并行方法的实际有效性。研究结果揭示了这些方法的局限性，包括在动态真实世界环境中处理负载平衡和通信成本的挑战，并强调了跨层优化的机会，以提升移动 DL 推理的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01851v1",
      "published_date": "2024-05-03 04:47:23 UTC",
      "updated_date": "2024-05-03 04:47:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:49:46.566614"
    },
    {
      "arxiv_id": "2405.01847v1",
      "title": "A Model-based Multi-Agent Personalized Short-Video Recommender System",
      "title_zh": "基于模型的多智能体个性化短视频推荐系统",
      "authors": [
        "Peilun Zhou",
        "Xiaoxiao Xu",
        "Lantao Hu",
        "Han Li",
        "Peng Jiang"
      ],
      "abstract": "Recommender selects and presents top-K items to the user at each online\nrequest, and a recommendation session consists of several sequential requests.\nFormulating a recommendation session as a Markov decision process and solving\nit by reinforcement learning (RL) framework has attracted increasing attention\nfrom both academic and industry communities. In this paper, we propose a\nRL-based industrial short-video recommender ranking framework, which models and\nmaximizes user watch-time in an environment of user multi-aspect preferences by\na collaborative multi-agent formulization. Moreover, our proposed framework\nadopts a model-based learning approach to alleviate the sample selection bias\nwhich is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of\nour proposed method over alternatives. Our proposed approach has been deployed\nin our real large-scale short-video sharing platform, successfully serving over\nhundreds of millions users.",
      "tldr_zh": "本文提出了一种基于强化学习(Reinforcement Learning, RL)的多智能体个性化短视频推荐系统，该系统将推荐会话建模为Markov决策过程，并通过多智能体框架最大化用户观看时间，同时考虑用户的多方面偏好。框架采用模型-based学习方法来缓解工业推荐系统中的样本选择偏差问题，提升推荐准确性。实验结果显示，该方法在离线评估和在线实验中均优于其他方案，已成功部署在大型短视频平台上，服务数亿用户。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01847v1",
      "published_date": "2024-05-03 04:34:36 UTC",
      "updated_date": "2024-05-03 04:34:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:49:58.745689"
    },
    {
      "arxiv_id": "2405.01843v5",
      "title": "Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic under Markovian Sampling with Neural Network Parametrization",
      "title_zh": "翻译失败",
      "authors": [
        "Mudit Gaur",
        "Amrit Singh Bedi",
        "Di Wang",
        "Vaneet Aggarwal"
      ],
      "abstract": "The current state-of-the-art theoretical analysis of Actor-Critic (AC)\nalgorithms significantly lags in addressing the practical aspects of AC\nimplementations. This crucial gap needs bridging to bring the analysis in line\nwith practical implementations of AC. To address this, we advocate for\nconsidering the MMCLG criteria: \\textbf{M}ulti-layer neural network\nparametrization for actor/critic, \\textbf{M}arkovian sampling,\n\\textbf{C}ontinuous state-action spaces, the performance of the \\textbf{L}ast\niterate, and \\textbf{G}lobal optimality. These aspects are practically\nsignificant and have been largely overlooked in existing theoretical analyses\nof AC algorithms. In this work, we address these gaps by providing the first\ncomprehensive theoretical analysis of AC algorithms that encompasses all five\ncrucial practical aspects (covers MMCLG criteria). We establish global\nconvergence sample complexity bounds of\n$\\tilde{\\mathcal{O}}\\left({\\epsilon^{-3}}\\right)$. We achieve this result\nthrough our novel use of the weak gradient domination property of MDP's and our\nunique analysis of the error in critic estimation.",
      "tldr_zh": "该论文旨在弥合 Actor-Critic (AC) 算法理论分析与实际实现之间的差距，首次提出并全面分析 MMCLG 标准，包括多层神经网络参数化、马尔可夫采样、连续状态-动作空间、最后迭代性能以及全局最优性。研究通过利用弱梯度支配属性和批评者估计误差的独特分析方法，建立了 AC 算法在这些条件下实现全局收敛的样本复杂度边界，为 \\(\\tilde{\\mathcal{O}}(\\epsilon^{-3})\\)。这一成果为实际 AC 算法的优化提供了坚实的理论基础，提升了其在强化学习中的可靠性和适用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2024. This is a revised version of arXiv:2306.10486,\n  where we have gone from finite action space to continuous action space, from\n  average iterate convergence to last iterate convergence and from\n  $\\epsilon^{-4}$ to $\\epsilon^{-3}$ sample complexity. This version fixes the\n  related work result of (Xu et al., 2020a), based on their result update on\n  arXiv",
      "pdf_url": "http://arxiv.org/pdf/2405.01843v5",
      "published_date": "2024-05-03 04:26:03 UTC",
      "updated_date": "2024-12-09 06:38:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:50:12.208173"
    },
    {
      "arxiv_id": "2405.01840v1",
      "title": "An Essay concerning machine understanding",
      "title_zh": "论机器理解",
      "authors": [
        "Herbert L. Roitblat"
      ],
      "abstract": "Artificial intelligence systems exhibit many useful capabilities, but they\nappear to lack understanding. This essay describes how we could go about\nconstructing a machine capable of understanding. As John Locke (1689) pointed\nout words are signs for ideas, which we can paraphrase as thoughts and\nconcepts. To understand a word is to know and be able to work with the\nunderlying concepts for which it is an indicator. Understanding between a\nspeaker and a listener occurs when the speaker casts his or her concepts into\nwords and the listener recovers approximately those same concepts. Current\nmodels rely on the listener to construct any potential meaning. The diminution\nof behaviorism as a psychological paradigm and the rise of cognitivism provide\nexamples of many experimental methods that can be used to determine whether and\nto what extent a machine might understand and to make suggestions about how\nthat understanding might be instantiated.",
      "tldr_zh": "这篇论文探讨了如何构建具备真正理解能力的机器，指出当前人工智能系统虽有诸多功能，但缺乏对概念的深层把握。作者引用John Locke的观点，将理解定义为知晓单词背后的思想和概念，并强调说话者通过语言传达概念，听者需准确恢复这些概念以实现有效沟通。现有的AI模型主要依赖听者自行构建意义，论文建议借鉴认知主义实验方法来评估机器理解的程度，并提供实现这种理解的潜在途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01840v1",
      "published_date": "2024-05-03 04:12:43 UTC",
      "updated_date": "2024-05-03 04:12:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:50:22.883439"
    },
    {
      "arxiv_id": "2405.01839v1",
      "title": "SocialGFs: Learning Social Gradient Fields for Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Qian Long",
        "Fangwei Zhong",
        "Mingdong Wu",
        "Yizhou Wang",
        "Song-Chun Zhu"
      ],
      "abstract": "Multi-agent systems (MAS) need to adaptively cope with dynamic environments,\nchanging agent populations, and diverse tasks. However, most of the multi-agent\nsystems cannot easily handle them, due to the complexity of the state and task\nspace. The social impact theory regards the complex influencing factors as\nforces acting on an agent, emanating from the environment, other agents, and\nthe agent's intrinsic motivation, referring to the social force. Inspired by\nthis concept, we propose a novel gradient-based state representation for\nmulti-agent reinforcement learning. To non-trivially model the social forces,\nwe further introduce a data-driven method, where we employ denoising score\nmatching to learn the social gradient fields (SocialGFs) from offline samples,\ne.g., the attractive or repulsive outcomes of each force. During interactions,\nthe agents take actions based on the multi-dimensional gradients to maximize\ntheir own rewards. In practice, we integrate SocialGFs into the widely used\nmulti-agent reinforcement learning algorithms, e.g., MAPPO. The empirical\nresults reveal that SocialGFs offer four advantages for multi-agent systems: 1)\nthey can be learned without requiring online interaction, 2) they demonstrate\ntransferability across diverse tasks, 3) they facilitate credit assignment in\nchallenging reward settings, and 4) they are scalable with the increasing\nnumber of agents.",
      "tldr_zh": "这篇论文提出了一种名为 SocialGFs 的社会梯度场学习方法，用于多智能体强化学习（Multi-agent Reinforcement Learning），以帮助智能体适应动态环境、变化的智能体数量和多样任务。作者受社会影响理论启发，通过去噪分数匹配（denoising score matching）从离线样本中学习社会梯度场（SocialGFs），使智能体基于多维梯度最大化奖励，并将其整合到如 MAPPO 的算法中。实验结果显示，SocialGFs 具有无需在线交互、可在不同任务间转移、辅助信用分配以及随智能体数量增加的可扩展性等优势。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI 2024 Cooperative Multi-Agent Systems Decision-Making and\n  Learning (CMASDL) Workshop",
      "pdf_url": "http://arxiv.org/pdf/2405.01839v1",
      "published_date": "2024-05-03 04:12:19 UTC",
      "updated_date": "2024-05-03 04:12:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:50:36.986622"
    },
    {
      "arxiv_id": "2405.02359v1",
      "title": "CVTGAD: Simplified Transformer with Cross-View Attention for Unsupervised Graph-level Anomaly Detection",
      "title_zh": "CVTGAD：简化的 Transformer 带",
      "authors": [
        "Jindong Li",
        "Qianli Xing",
        "Qi Wang",
        "Yi Chang"
      ],
      "abstract": "Unsupervised graph-level anomaly detection (UGAD) has received remarkable\nperformance in various critical disciplines, such as chemistry analysis and\nbioinformatics. Existing UGAD paradigms often adopt data augmentation\ntechniques to construct multiple views, and then employ different strategies to\nobtain representations from different views for jointly conducting UGAD.\nHowever, most previous works only considered the relationship between\nnodes/graphs from a limited receptive field, resulting in some key structure\npatterns and feature information being neglected. In addition, most existing\nmethods consider different views separately in a parallel manner, which is not\nable to explore the inter-relationship across different views directly. Thus, a\nmethod with a larger receptive field that can explore the inter-relationship\nacross different views directly is in need. In this paper, we propose a novel\nSimplified Transformer with Cross-View Attention for Unsupervised Graph-level\nAnomaly Detection, namely, CVTGAD. To increase the receptive field, we\nconstruct a simplified transformer-based module, exploiting the relationship\nbetween nodes/graphs from both intra-graph and inter-graph perspectives.\nFurthermore, we design a cross-view attention mechanism to directly exploit the\nview co-occurrence between different views, bridging the inter-view gap at node\nlevel and graph level. To the best of our knowledge, this is the first work to\napply transformer and cross attention to UGAD, which realizes graph neural\nnetwork and transformer working collaboratively. Extensive experiments on 15\nreal-world datasets of 3 fields demonstrate the superiority of CVTGAD on the\nUGAD task. The code is available at\n\\url{https://github.com/jindongli-Ai/CVTGAD}.",
      "tldr_zh": "该论文提出 CVTGAD，一种简化 Transformer 模型，结合 Cross-View Attention 机制，用于无监督图级异常检测 (UGAD)，以解决现有方法在有限感受野和视图间关系探索上的不足。\nCVTGAD 通过 Transformer-based 模块从图内和图间视角扩展节点/图关系，并设计跨视图注意力机制直接利用不同视图的共现信息，在节点级和图级桥接视图间差距，实现图神经网络与 Transformer 的协作。\n实验在 15 个真实数据集上证明了 CVTGAD 的优越性，显著提升了 UGAD 性能，代码已在 GitHub 上公开。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02359v1",
      "published_date": "2024-05-03 03:31:00 UTC",
      "updated_date": "2024-05-03 03:31:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:50:48.099807"
    },
    {
      "arxiv_id": "2405.02358v2",
      "title": "A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Jiexia Ye",
        "Weiqi Zhang",
        "Ke Yi",
        "Yongzi Yu",
        "Ziyue Li",
        "Jia Li",
        "Fugee Tsung"
      ],
      "abstract": "Time series data are ubiquitous across various domains, making time series\nanalysis critically important. Traditional time series models are\ntask-specific, featuring singular functionality and limited generalization\ncapacity. Recently, large language foundation models have unveiled their\nremarkable capabilities for cross-task transferability, zero-shot/few-shot\nlearning, and decision-making explainability. This success has sparked interest\nin the exploration of foundation models to solve multiple time series\nchallenges simultaneously. There are two main research lines, namely\npre-training foundation models from scratch for time series and adapting large\nlanguage foundation models for time series. They both contribute to the\ndevelopment of a unified model that is highly generalizable, versatile, and\ncomprehensible for time series analysis. This survey offers a 3E analytical\nframework for comprehensive examination of related research. Specifically, we\nexamine existing works from three dimensions, namely Effectiveness, Efficiency\nand Explainability. In each dimension, we focus on discussing how related works\ndevise tailored solution by considering unique challenges in the realm of time\nseries. Furthermore, we provide a domain taxonomy to help followers keep up\nwith the domain-specific advancements. In addition, we introduce extensive\nresources to facilitate the field's development, including datasets,\nopen-source, time series libraries. A GitHub repository is also maintained for\nresource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).",
      "tldr_zh": "这篇调查综述探讨了时间序列基础模型（Time Series Foundation Models）的最新进展，旨在通过大语言模型（Large Language Models）实现时间序列表示的泛化，以解决传统任务特定模型的局限性，如单一功能和泛化能力不足。主要研究方向包括从零开始预训练时间序列模型，以及将大语言模型适应于时间序列分析，从而开发出高度泛化、多功能且易懂的统一模型。作者采用3E分析框架（Effectiveness、Efficiency、Explainability）从有效性、效率和可解释性三个维度审视现有研究，并针对时间序列领域的独特挑战提出解决方案。此外，该文提供了领域分类法、数据集资源、开源库列表，并维护了一个GitHub仓库（https://github.com/start2020/Awesome-TimeSeries-LLM-FM）以支持后续研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 figures, 6 tables, 41 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.02358v2",
      "published_date": "2024-05-03 03:12:55 UTC",
      "updated_date": "2024-05-07 01:59:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:50:59.926598"
    },
    {
      "arxiv_id": "2405.05126v1",
      "title": "Exploring Speech Pattern Disorders in Autism using Machine Learning",
      "title_zh": "使用机器学习探索自闭症中的言语模式障碍",
      "authors": [
        "Chuanbo Hu",
        "Jacob Thrasher",
        "Wenqi Li",
        "Mindi Ruan",
        "Xiangxu Yu",
        "Lynn K Paul",
        "Shuo Wang",
        "Xin Li"
      ],
      "abstract": "Diagnosing autism spectrum disorder (ASD) by identifying abnormal speech\npatterns from examiner-patient dialogues presents significant challenges due to\nthe subtle and diverse manifestations of speech-related symptoms in affected\nindividuals. This study presents a comprehensive approach to identify\ndistinctive speech patterns through the analysis of examiner-patient dialogues.\nUtilizing a dataset of recorded dialogues, we extracted 40 speech-related\nfeatures, categorized into frequency, zero-crossing rate, energy, spectral\ncharacteristics, Mel Frequency Cepstral Coefficients (MFCCs), and balance.\nThese features encompass various aspects of speech such as intonation, volume,\nrhythm, and speech rate, reflecting the complex nature of communicative\nbehaviors in ASD. We employed machine learning for both classification and\nregression tasks to analyze these speech features. The classification model\naimed to differentiate between ASD and non-ASD cases, achieving an accuracy of\n87.75%. Regression models were developed to predict speech pattern related\nvariables and a composite score from all variables, facilitating a deeper\nunderstanding of the speech dynamics associated with ASD. The effectiveness of\nmachine learning in interpreting intricate speech patterns and the high\nclassification accuracy underscore the potential of computational methods in\nsupporting the diagnostic processes for ASD. This approach not only aids in\nearly detection but also contributes to personalized treatment planning by\nproviding insights into the speech and communication profiles of individuals\nwith ASD.",
      "tldr_zh": "本研究探讨了使用机器学习识别自闭症谱系障碍 (ASD) 中的异常语音模式，以辅助诊断。研究者从 examiner-patient 对话数据集提取了 40 个语音相关特征，包括 frequency、zero-crossing rate、energy、spectral characteristics、Mel Frequency Cepstral Coefficients (MFCCs) 和 balance，这些特征涵盖了语调、音量、节奏和语速等方面。采用机器学习进行分类任务，成功将 ASD 和非 ASD 病例区分开来，准确率达 87.75%；同时，通过回归模型预测语音模式变量和综合分数，提供对 ASD 语音动态的深入洞察。该方法不仅支持 ASD 的早期检测，还为个性化治疗规划提供宝贵见解。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05126v1",
      "published_date": "2024-05-03 02:59:15 UTC",
      "updated_date": "2024-05-03 02:59:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:51:10.847373"
    },
    {
      "arxiv_id": "2405.01824v1",
      "title": "Creation of Novel Soft Robot Designs using Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Wee Kiat Chan",
        "PengWei Wang",
        "Raye Chen-Hua Yeow"
      ],
      "abstract": "Soft robotics has emerged as a promising field with the potential to\nrevolutionize industries such as healthcare and manufacturing. However,\ndesigning effective soft robots presents challenges, particularly in managing\nthe complex interplay of material properties, structural design, and control\nstrategies. Traditional design methods are often time-consuming and may not\nyield optimal designs. In this paper, we explore the use of generative AI to\ncreate 3D models of soft actuators. We create a dataset of over 70 text-shape\npairings of soft pneumatic robot actuator designs, and adapt a latent diffusion\nmodel (SDFusion) to learn the data distribution and generate novel designs from\nit. By employing transfer learning and data augmentation techniques, we\nsignificantly improve the performance of the diffusion model. These findings\nhighlight the potential of generative AI in designing complex soft robotic\nsystems, paving the way for future advancements in the field.",
      "tldr_zh": "本文探讨了使用生成式AI（Generative AI）来创建新型软机器人设计的潜力，以解决传统方法在处理材料属性、结构设计和控制策略复杂互动时的挑战。研究团队构建了一个包含超过70对文本-形状配对的软气动机器人致动器数据集，并改编了SDFusion潜扩散模型（latent diffusion model）通过转移学习和数据增强技术来学习数据分布并生成创新3D模型。这些发现显著提升了模型性能，并展示了生成式AI在设计复杂软机器人系统方面的前景，为医疗和制造等领域的未来发展铺平道路。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01824v1",
      "published_date": "2024-05-03 02:55:27 UTC",
      "updated_date": "2024-05-03 02:55:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:51:22.882446"
    },
    {
      "arxiv_id": "2405.02356v1",
      "title": "Stochastic Multivariate Universal-Radix Finite-State Machine: a Theoretically and Practically Elegant Nonlinear Function Approximator",
      "title_zh": "翻译失败",
      "authors": [
        "Xincheng Feng",
        "Guodong Shen",
        "Jianhao Hu",
        "Meng Li",
        "Ngai Wong"
      ],
      "abstract": "Nonlinearities are crucial for capturing complex input-output relationships\nespecially in deep neural networks. However, nonlinear functions often incur\nvarious hardware and compute overheads. Meanwhile, stochastic computing (SC)\nhas emerged as a promising approach to tackle this challenge by trading output\nprecision for hardware simplicity. To this end, this paper proposes a\nfirst-of-its-kind stochastic multivariate universal-radix finite-state machine\n(SMURF) that harnesses SC for hardware-simplistic multivariate nonlinear\nfunction generation at high accuracy. We present the finite-state machine (FSM)\narchitecture for SMURF, as well as analytical derivations of sampling gate\ncoefficients for accurately approximating generic nonlinear functions.\nExperiments demonstrate the superiority of SMURF, requiring only 16.07% area\nand 14.45% power consumption of Taylor-series approximation, and merely 2.22%\narea of look-up table (LUT) schemes.",
      "tldr_zh": "本文提出了一种新的非线性函数逼近器，名为 Stochastic Multivariate Universal-Radix Finite-State Machine (SMURF)，它利用 Stochastic Computing (SC) 来简化硬件设计，同时实现高准确性的多变量非线性函数生成。SMURF 基于 Finite-State Machine (FSM) 架构，并通过分析推导采样门系数 (sampling gate coefficients) 来精确逼近通用非线性函数。实验结果表明，SMURF 仅需 Taylor-series approximation 的 16.07% 面积和 14.45% 功耗，以及 Look-Up Table (LUT) 方案的 2.22% 面积，展示了其在硬件效率和实用性上的显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02356v1",
      "published_date": "2024-05-03 02:53:32 UTC",
      "updated_date": "2024-05-03 02:53:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:51:36.070096"
    },
    {
      "arxiv_id": "2405.02355v3",
      "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Kounianhua Du",
        "Jizheng Chen",
        "Renting Rui",
        "Huacan Chai",
        "Lingyue Fu",
        "Wei Xia",
        "Yasheng Wang",
        "Ruiming Tang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/.",
      "tldr_zh": "本研究提出CodeGRAG框架，通过Graphical Retrieval Augmented Generation桥接自然语言和编程语言之间的语法差距及词汇不匹配问题，以提升LLMs在代码生成的性能。框架基于代码块的控制流和数据流构建图形视图，并引入hard meta-graph prompt模板（用于无调优模型的知识转换）和soft prompting技术（通过微调与预训练GNN专家模型注入编程语言领域知识）。在四个数据集上的实验（涵盖C++和Python）验证了这些方法的有效性，CodeGRAG显著提高了LLMs的代码生成能力，甚至在跨语言生成中实现了性能提升。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02355v3",
      "published_date": "2024-05-03 02:48:55 UTC",
      "updated_date": "2024-11-08 14:17:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:51:47.482152"
    },
    {
      "arxiv_id": "2405.01820v1",
      "title": "Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention",
      "title_zh": "假数据的真实风险：合成数据、多样性洗白和同意规避",
      "authors": [
        "Cedric Deslandes Whitney",
        "Justin Norman"
      ],
      "abstract": "Machine learning systems require representations of the real world for\ntraining and testing - they require data, and lots of it. Collecting data at\nscale has logistical and ethical challenges, and synthetic data promises a\nsolution to these challenges. Instead of needing to collect photos of real\npeople's faces to train a facial recognition system, a model creator could\ncreate and use photo-realistic, synthetic faces. The comparative ease of\ngenerating this synthetic data rather than relying on collecting data has made\nit a common practice. We present two key risks of using synthetic data in model\ndevelopment. First, we detail the high risk of false confidence when using\nsynthetic data to increase dataset diversity and representation. We base this\nin the examination of a real world use-case of synthetic data, where synthetic\ndatasets were generated for an evaluation of facial recognition technology.\nSecond, we examine how using synthetic data risks circumventing consent for\ndata usage. We illustrate this by considering the importance of consent to the\nU.S. Federal Trade Commission's regulation of data collection and affected\nmodels. Finally, we discuss how these two risks exemplify how synthetic data\ncomplicates existing governance and ethical practice; by decoupling data from\nthose it impacts, synthetic data is prone to consolidating power away those\nmost impacted by algorithmically-mediated harm.",
      "tldr_zh": "该论文探讨了使用合成数据（synthetic data）在机器学习模型开发中的潜在风险，强调其作为数据收集替代方案的便利性可能导致问题。作者首先分析了合成数据在增加数据集多样性和代表性时带来的虚假信心风险，通过一个真实案例（面部识别技术的合成数据集评估）证明这可能误导模型性能评估。其次，论文指出合成数据可能规避数据使用同意（consent circumvention），如违反美国联邦贸易委员会（FTC）的相关法规。最后，这些风险突显了合成数据如何复杂化现有治理和伦理实践，可能加剧权力不均等和算法伤害。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01820v1",
      "published_date": "2024-05-03 02:47:44 UTC",
      "updated_date": "2024-05-03 02:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:51:58.902980"
    },
    {
      "arxiv_id": "2405.01815v1",
      "title": "Toward end-to-end interpretable convolutional neural networks for waveform signals",
      "title_zh": "翻译失败",
      "authors": [
        "Linh Vu",
        "Thu Tran",
        "Wern-Han Lim",
        "Raphael Phan"
      ],
      "abstract": "This paper introduces a novel convolutional neural networks (CNN) framework\ntailored for end-to-end audio deep learning models, presenting advancements in\nefficiency and explainability. By benchmarking experiments on three standard\nspeech emotion recognition datasets with five-fold cross-validation, our\nframework outperforms Mel spectrogram features by up to seven percent. It can\npotentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while\nremaining lightweight. Furthermore, we demonstrate the efficiency and\ninterpretability of the front-end layer using the PhysioNet Heart Sound\nDatabase, illustrating its ability to handle and capture intricate long\nwaveform patterns. Our contributions offer a portable solution for building\nefficient and interpretable models for raw waveform data.",
      "tldr_zh": "本论文提出了一种端到端可解释的 Convolutional Neural Networks (CNN) 框架，针对波形信号的音频深度学习模型，提升了效率和可解释性。实验通过在三个标准语音情感识别数据集上的五折交叉验证表明，该框架比 Mel spectrogram 特征高出高达 7%，并可潜在取代 Mel-Frequency Cepstral Coefficients (MFCC)，同时保持轻量级设计。此外，在 PhysioNet Heart Sound Database 上验证了前端层的效能，它能有效处理和捕捉复杂的长波形模式，为构建高效、可解释的原始波形数据模型提供便携解决方案。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01815v1",
      "published_date": "2024-05-03 02:24:27 UTC",
      "updated_date": "2024-05-03 02:24:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:52:13.060259"
    },
    {
      "arxiv_id": "2405.02354v1",
      "title": "Heterogeneous network and graph attention auto-encoder for LncRNA-disease association prediction",
      "title_zh": "异构网络和图注意力自动编码器用于 LncRNA-",
      "authors": [
        "Jin-Xing Liu",
        "Wen-Yu Xi",
        "Ling-Yun Dai",
        "Chun-Hou Zheng",
        "Ying-Lian Gao"
      ],
      "abstract": "The emerging research shows that lncRNAs are associated with a series of\ncomplex human diseases. However, most of the existing methods have limitations\nin identifying nonlinear lncRNA-disease associations (LDAs), and it remains a\nhuge challenge to predict new LDAs. Therefore, the accurate identification of\nLDAs is very important for the warning and treatment of diseases. In this work,\nmultiple sources of biomedical data are fully utilized to construct\ncharacteristics of lncRNAs and diseases, and linear and nonlinear\ncharacteristics are effectively integrated. Furthermore, a novel deep learning\nmodel based on graph attention automatic encoder is proposed, called HGATELDA.\nTo begin with, the linear characteristics of lncRNAs and diseases are created\nby the miRNA-lncRNA interaction matrix and miRNA-disease interaction matrix.\nFollowing this, the nonlinear features of diseases and lncRNAs are extracted\nusing a graph attention auto-encoder, which largely retains the critical\ninformation and effectively aggregates the neighborhood information of nodes.\nIn the end, LDAs can be predicted by fusing the linear and nonlinear\ncharacteristics of diseases and lncRNA. The HGATELDA model achieves an\nimpressive AUC value of 0.9692 when evaluated using a 5-fold cross-validation\nindicating its superior performance in comparison to several recent prediction\nmodels. Meanwhile, the effectiveness of HGATELDA in identifying novel LDAs is\nfurther demonstrated by case studies. the HGATELDA model appears to be a viable\ncomputational model for predicting LDAs.",
      "tldr_zh": "该研究针对 lncRNA 与疾病关联（LDAs）的预测问题，提出了一种基于异构网络和图注意力自动编码器（graph attention auto-encoder）的深度学习模型 HGATELDA，以克服现有方法在识别非线性 LDAs 时的局限。模型通过利用 miRNA-lncRNA 和 miRNA-disease 交互矩阵构建线性特征，并提取非线性特征来融合多种生物医学数据，从而有效预测 LDAs。实验结果显示，HGATELDA 在 5 折交叉验证中获得 AUC 值 0.9692，优于其他模型，并在案例研究中证明了其在识别新型 LDAs 的实际有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM",
        "I.2.4; I.2.6; I.2.m"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02354v1",
      "published_date": "2024-05-03 02:15:05 UTC",
      "updated_date": "2024-05-03 02:15:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:52:24.343125"
    },
    {
      "arxiv_id": "2405.01810v2",
      "title": "Non-linear Welfare-Aware Strategic Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Tian Xie",
        "Xueru Zhang"
      ],
      "abstract": "This paper studies algorithmic decision-making in the presence of strategic\nindividual behaviors, where an ML model is used to make decisions about human\nagents and the latter can adapt their behavior strategically to improve their\nfuture data. Existing results on strategic learning have largely focused on the\nlinear setting where agents with linear labeling functions best respond to a\n(noisy) linear decision policy. Instead, this work focuses on general\nnon-linear settings where agents respond to the decision policy with only\n\"local information\" of the policy. Moreover, we simultaneously consider the\nobjectives of maximizing decision-maker welfare (model prediction accuracy),\nsocial welfare (agent improvement caused by strategic behaviors), and agent\nwelfare (the extent that ML underestimates the agents). We first generalize the\nagent best response model in previous works to the non-linear setting, then\nreveal the compatibility of welfare objectives. We show the three welfare can\nattain the optimum simultaneously only under restrictive conditions which are\nchallenging to achieve in non-linear settings. The theoretical results imply\nthat existing works solely maximizing the welfare of a subset of parties\ninevitably diminish the welfare of the others. We thus claim the necessity of\nbalancing the welfare of each party in non-linear settings and propose an\nirreducible optimization algorithm suitable for general strategic learning.\nExperiments on synthetic and real data validate the proposed algorithm.",
      "tldr_zh": "这篇论文探讨了非线性设置下的战略学习（Strategic Learning），其中机器学习（ML）模型在面对人类代理的战略行为时进行决策，代理会基于决策政策的“局部信息”调整行为。论文将代理的最佳响应模型推广到非线性环境，并分析了决策者福利（模型预测准确性）、社会福利（代理改善）和代理福利（ML低估程度）三者的兼容性。研究发现，这三种福利仅在严格条件下能同时达到最优，在非线性设置中难以实现，因此现有仅最大化部分福利的方法会损害其他方面。论文提出一个平衡各方福利的优化算法，并通过合成和真实数据实验验证了其有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01810v2",
      "published_date": "2024-05-03 01:50:03 UTC",
      "updated_date": "2024-08-13 19:19:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:52:36.647631"
    },
    {
      "arxiv_id": "2405.01807v3",
      "title": "Algorithmic Decision-Making under Agents with Persistent Improvement",
      "title_zh": "在具有持续改进的代理下的算法决策",
      "authors": [
        "Tian Xie",
        "Xuwei Tan",
        "Xueru Zhang"
      ],
      "abstract": "This paper studies algorithmic decision-making under human's strategic\nbehavior, where a decision maker uses an algorithm to make decisions about\nhuman agents, and the latter with information about the algorithm may exert\neffort strategically and improve to receive favorable decisions. Unlike prior\nworks that assume agents benefit from their efforts immediately, we consider\nrealistic scenarios where the impacts of these efforts are persistent and\nagents benefit from efforts by making improvements gradually. We first develop\na dynamic model to characterize persistent improvements and based on this\nconstruct a Stackelberg game to model the interplay between agents and the\ndecision-maker. We analytically characterize the equilibrium strategies and\nidentify conditions under which agents have incentives to improve. With the\ndynamics, we then study how the decision-maker can design an optimal policy to\nincentivize the largest improvements inside the agent population. We also\nextend the model to settings where 1) agents may be dishonest and game the\nalgorithm into making favorable but erroneous decisions; 2) honest efforts are\nforgettable and not sufficient to guarantee persistent improvements. With the\nextended models, we further examine conditions under which agents prefer honest\nefforts over dishonest behavior and the impacts of forgettable efforts.",
      "tldr_zh": "这篇论文研究了算法决策中代理人的战略行为，假设代理人通过持久努力逐步改进以获得更有利的决策，而非即时受益。作者开发了一个动态模型和 Stackelberg 游戏来模拟代理人与决策者的互动，分析了均衡策略以及代理人有动机改进的条件。论文进一步探讨决策者如何设计最优政策来激励代理人群的最大改进，并在扩展模型中考察代理人偏好诚实努力而非欺骗行为，以及遗忘努力的影响。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01807v3",
      "published_date": "2024-05-03 01:36:35 UTC",
      "updated_date": "2024-09-13 13:25:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:52:47.752227"
    },
    {
      "arxiv_id": "2405.01799v2",
      "title": "Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders and Identifying Distinct Features",
      "title_zh": "翻译失败",
      "authors": [
        "Chuanbo Hu",
        "Wenqi Li",
        "Mindi Ruan",
        "Xiangxu Yu",
        "Shalaka Deshpande",
        "Lynn K. Paul",
        "Shuo Wang",
        "Xin Li"
      ],
      "abstract": "Diagnosing language disorders associated with autism is a complex challenge,\noften hampered by the subjective nature and variability of traditional\nassessment methods. Traditional diagnostic methods not only require intensive\nhuman effort but also often result in delayed interventions due to their lack\nof speed and precision. In this study, we explored the application of ChatGPT,\na large language model, to overcome these obstacles by enhancing sensitivity\nand profiling linguistic features for autism diagnosis. This research utilizes\nChatGPT natural language processing capabilities to simplify and improve the\ndiagnostic process, focusing on identifying autism related language patterns.\nSpecifically, we compared ChatGPT performance with that of conventional\nsupervised learning models, including BERT, a model acclaimed for its\neffectiveness in various natural language processing tasks. We showed that\nChatGPT substantially outperformed these models, achieving over 10% improvement\nin both sensitivity and positive predictive value, in a zero shot learning\nconfiguration. The findings underscore the model potential as a diagnostic\ntool, combining accuracy and applicability. We identified ten key features of\nautism associated language disorders across scenarios. Features such as\necholalia, pronoun reversal, and atypical language usage play a critical role\nin diagnosing ASD and informing tailored treatment plans. Together, our\nfindings advocate for adopting sophisticated AI tools like ChatGPT in clinical\nsettings to assess and diagnose developmental disorders. Our approach promises\nenhanced diagnostic precision and supports personalized medicine, potentially\ntransforming the evaluation landscape for autism and similar neurological\nconditions.",
      "tldr_zh": "本研究利用 ChatGPT 作为诊断工具，针对自闭症相关语言障碍的问题，旨在提升诊断的敏感性和识别独特语言特征，从而克服传统方法的主观性和低效性。相比于 BERT 等监督学习模型，ChatGPT 在零样本学习配置下实现了敏感性和阳性预测值超过 10% 的改进。研究还识别了十个关键特征，如 echolalia（回声言语）、pronoun reversal（代词颠倒）和 atypical language usage（非典型语言使用），这些特征有助于制定个性化治疗计划。总体而言，此方法倡导在临床环境中采用 AI 工具，提高诊断精度并推动自闭症等神经发育障碍的评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01799v2",
      "published_date": "2024-05-03 01:04:28 UTC",
      "updated_date": "2024-11-29 06:15:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:53:00.453850"
    },
    {
      "arxiv_id": "2405.01797v2",
      "title": "Learning under Imitative Strategic Behavior with Unforeseeable Outcomes",
      "title_zh": "翻译失败",
      "authors": [
        "Tian Xie",
        "Zhiqun Zuo",
        "Mohammad Mahdi Khalili",
        "Xueru Zhang"
      ],
      "abstract": "Machine learning systems have been widely used to make decisions about\nindividuals who may behave strategically to receive favorable outcomes, e.g.,\nthey may genuinely improve the true labels or manipulate observable features\ndirectly to game the system without changing labels. Although both behaviors\nhave been studied (often as two separate problems) in the literature, most\nworks assume individuals can (i) perfectly foresee the outcomes of their\nbehaviors when they best respond; (ii) change their features arbitrarily as\nlong as it is affordable, and the costs they need to pay are deterministic\nfunctions of feature changes. In this paper, we consider a different setting\nand focus on imitative strategic behaviors with unforeseeable outcomes, i.e.,\nindividuals manipulate/improve by imitating the features of those with positive\nlabels, but the induced feature changes are unforeseeable. We first propose a\nStackelberg game to model the interplay between individuals and the\ndecision-maker, under which we examine how the decision-maker's ability to\nanticipate individual behavior affects its objective function and the\nindividual's best response. We show that the objective difference between the\ntwo can be decomposed into three interpretable terms, with each representing\nthe decision-maker's preference for a certain behavior. By exploring the roles\nof each term, we theoretically illustrate how a decision-maker with adjusted\npreferences may simultaneously disincentivize manipulation, incentivize\nimprovement, and promote fairness. Such theoretical results provide a guideline\nfor decision-makers to inform better and socially responsible decisions in\npractice.",
      "tldr_zh": "本研究探讨了机器学习系统在面对个体模仿性战略行为（imitative strategic behavior）时的挑战，这些行为涉及个体通过模仿正标签特征来改善或操纵结果，但结果不可预见。作者提出一个Stackelberg game模型来模拟个体和决策者之间的互动，分析决策者预测行为的能力如何影响目标函数和个体的最佳响应。研究将目标差异分解为三个可解释术语，每个术语代表决策者对特定行为的偏好，并证明通过调整这些偏好，决策者可以同时抑制操纵、激励真实改善并促进公平，从而为制定更负责任的决策提供理论指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01797v2",
      "published_date": "2024-05-03 00:53:58 UTC",
      "updated_date": "2024-10-29 10:28:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:53:11.561897"
    },
    {
      "arxiv_id": "2405.01790v1",
      "title": "Understanding Position Bias Effects on Fairness in Social Multi-Document Summarization",
      "title_zh": "理解位置偏差对社交多文档摘要中公平性的影响",
      "authors": [
        "Olubusayo Olabisi",
        "Ameeta Agrawal"
      ],
      "abstract": "Text summarization models have typically focused on optimizing aspects of\nquality such as fluency, relevance, and coherence, particularly in the context\nof news articles. However, summarization models are increasingly being used to\nsummarize diverse sources of text, such as social media data, that encompass a\nwide demographic user base. It is thus crucial to assess not only the quality\nof the generated summaries, but also the extent to which they can fairly\nrepresent the opinions of diverse social groups. Position bias, a long-known\nissue in news summarization, has received limited attention in the context of\nsocial multi-document summarization. We deeply investigate this phenomenon by\nanalyzing the effect of group ordering in input documents when summarizing\ntweets from three distinct linguistic communities: African-American English,\nHispanic-aligned Language, and White-aligned Language. Our empirical analysis\nshows that although the textual quality of the summaries remains consistent\nregardless of the input document order, in terms of fairness, the results vary\nsignificantly depending on how the dialect groups are presented in the input\ndata. Our results suggest that position bias manifests differently in social\nmulti-document summarization, severely impacting the fairness of summarization\nmodels.",
      "tldr_zh": "本研究探讨了位置偏差（Position Bias）对社交多文档摘要公平性的影响，特别是在总结社交媒体数据时。研究者分析了输入文档中不同群体顺序的影响，使用African-American English、Hispanic-aligned Language 和 White-aligned Language 等三个语言社区的推文作为测试对象。结果显示，虽然摘要的文本质量（如流畅性和相关性）不受输入顺序影响，但公平性显著变化，某些群体意见可能被低估。总体而言，该工作揭示了位置偏差在社交多文档摘要中的独特表现，强调了提升模型公平性的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at VarDial 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.01790v1",
      "published_date": "2024-05-03 00:19:31 UTC",
      "updated_date": "2024-05-03 00:19:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:53:23.004719"
    },
    {
      "arxiv_id": "2405.01787v3",
      "title": "Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Saikat Chakraborty",
        "Gabriel Ebner",
        "Siddharth Bhat",
        "Sarah Fakhoury",
        "Sakina Fatima",
        "Shuvendu Lahiri",
        "Nikhil Swamy"
      ],
      "abstract": "Proof-oriented programs mix computational content with proofs of program\ncorrectness. However, the human effort involved in programming and proving is\nstill substantial, despite the use of Satisfiability Modulo Theories (SMT)\nsolvers to automate proofs in languages such as F*. Seeking to spur research on\nusing AI to automate the construction of proof-oriented programs, we curate a\ndataset of 600K lines of open-source F* programs and proofs, including software\nused in production systems ranging from Windows and Linux to Python and\nFirefox. Our dataset includes around 32K top-level F* definitions, each\nrepresenting a type-directed program and proof synthesis problem producing a\ndefinition given a formal specification expressed as an F* type. We provide a\nprogram fragment checker that queries F* to check the correctness of candidate\nsolutions. We also report on an extended version of our dataset containing a\ntotal of 940K lines of programs and proofs, with a total of 54k top-level F*\ndefinitions. We believe this is the largest corpus of SMT-assisted program\nproofs coupled with a reproducible program-fragment checker. Grounded in this\ndataset, we investigate the use of AI to synthesize programs and their proofs\nin F*, with promising results. Our main finding in that the performance of\nfine-tuned smaller language models (such as Phi-2 or StarCoder) compare\nfavorably with large language models (such as GPT-4), at a much lower\ncomputational cost. We also identify various type-based retrieval augmentation\ntechniques and find that they boost performance significantly. With detailed\nerror analysis and case studies, we identify potential strengths and weaknesses\nof models and techniques and suggest directions for future improvements.",
      "tldr_zh": "该论文针对SMT辅助的证明导向编程，整理了一个包含600K行开源F*程序和证明的数据集（扩展版达940K行和54k顶级定义），并提供了一个可重现的程序片段检查器，以促进AI在程序和证明合成方面的研究。研究探索了使用AI神经合成技术自动生成F*中的程序和证明，发现微调较小语言模型（如Phi-2或StarCoder）在性能上可与大型模型（如GPT-4）媲美，但计算成本显著降低。作者还引入了基于类型的检索增强技术，显著提升了合成准确性，并通过错误分析和案例研究，识别了模型优势与不足，并建议了未来改进方向。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "47th International Conference on Software Engineering",
      "pdf_url": "http://arxiv.org/pdf/2405.01787v3",
      "published_date": "2024-05-03 00:14:33 UTC",
      "updated_date": "2024-09-04 19:32:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:53:36.930938"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 88,
  "processed_papers_count": 88,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T05:53:54.621080"
}
[
  {
    "arxiv_id": "2407.06460v2",
    "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
    "authors": [
      "Weijia Shi",
      "Jaechan Lee",
      "Yangsibo Huang",
      "Sadhika Malladi",
      "Jieyu Zhao",
      "Ari Holtzman",
      "Daogao Liu",
      "Luke Zettlemoyer",
      "Noah A. Smith",
      "Chiyuan Zhang"
    ],
    "abstract": "Language models (LMs) are trained on vast amounts of text data, which may\ninclude private and copyrighted content. Data owners may request the removal of\ntheir data from a trained model due to privacy or copyright concerns. However,\nexactly unlearning only these datapoints (i.e., retraining with the data\nremoved) is intractable in modern-day models. This has led to the development\nof many approximate unlearning algorithms. The evaluation of the efficacy of\nthese algorithms has traditionally been narrow in scope, failing to precisely\nquantify the success and practicality of the algorithm from the perspectives of\nboth the model deployers and the data owners. We address this issue by\nproposing MUSE, a comprehensive machine unlearning evaluation benchmark that\nenumerates six diverse desirable properties for unlearned models: (1) no\nverbatim memorization, (2) no knowledge memorization, (3) no privacy leakage,\n(4) utility preservation on data not intended for removal, (5) scalability with\nrespect to the size of removal requests, and (6) sustainability over sequential\nunlearning requests. Using these criteria, we benchmark how effectively eight\npopular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter\nbooks and news articles. Our results demonstrate that most algorithms can\nprevent verbatim memorization and knowledge memorization to varying degrees,\nbut only one algorithm does not lead to severe privacy leakage. Furthermore,\nexisting algorithms fail to meet deployer's expectations because they often\ndegrade general model utility and also cannot sustainably accommodate\nsuccessive unlearning requests or large-scale content removal. Our findings\nidentify key issues with the practicality of existing unlearning algorithms on\nlanguage models, and we release our benchmark to facilitate further\nevaluations: muse-bench.github.io",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06460v2",
    "published_date": "2024-07-08 23:47:29 UTC",
    "updated_date": "2024-07-14 20:14:02 UTC"
  },
  {
    "arxiv_id": "2407.06459v1",
    "title": "How Much Progress Did I Make? An Unexplored Human Feedback Signal for Teaching Robots",
    "authors": [
      "Hang Yu",
      "Qidi Fang",
      "Shijie Fang",
      "Reuben M. Aronson",
      "Elaine Schaertl Short"
    ],
    "abstract": "Enhancing the expressiveness of human teaching is vital for both improving\nrobots' learning from humans and the human-teaching-robot experience. In this\nwork, we characterize and test a little-used teaching signal:\n\\textit{progress}, designed to represent the completion percentage of a task.\nWe conducted two online studies with 76 crowd-sourced participants and one\npublic space study with 40 non-expert participants to validate the capability\nof this progress signal. We find that progress indicates whether the task is\nsuccessfully performed, reflects the degree of task completion, identifies\nunproductive but harmless behaviors, and is likely to be more consistent across\nparticipants. Furthermore, our results show that giving progress does not\nrequire extra workload and time. An additional contribution of our work is a\ndataset of 40 non-expert demonstrations from the public space study through an\nice cream topping-adding task, which we observe to be multi-policy and\nsub-optimal, with sub-optimality not only from teleoperation errors but also\nfrom exploratory actions and attempts. The dataset is available at\n\\url{https://github.com/TeachingwithProgress/Non-Expert\\_Demonstrations}.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages. RO-MAN 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.06459v1",
    "published_date": "2024-07-08 23:47:13 UTC",
    "updated_date": "2024-07-08 23:47:13 UTC"
  },
  {
    "arxiv_id": "2407.06452v1",
    "title": "Exploiting Heterogeneity in Timescales for Sparse Recurrent Spiking Neural Networks for Energy-Efficient Edge Computing",
    "authors": [
      "Biswadeep Chakraborty",
      "Saibal Mukhopadhyay"
    ],
    "abstract": "Spiking Neural Networks (SNNs) represent the forefront of neuromorphic\ncomputing, promising energy-efficient and biologically plausible models for\ncomplex tasks. This paper weaves together three groundbreaking studies that\nrevolutionize SNN performance through the introduction of heterogeneity in\nneuron and synapse dynamics. We explore the transformative impact of\nHeterogeneous Recurrent Spiking Neural Networks (HRSNNs), supported by rigorous\nanalytical frameworks and novel pruning methods like Lyapunov Noise Pruning\n(LNP). Our findings reveal how heterogeneity not only enhances classification\nperformance but also reduces spiking activity, leading to more efficient and\nrobust networks. By bridging theoretical insights with practical applications,\nthis comprehensive summary highlights the potential of SNNs to outperform\ntraditional neural networks while maintaining lower computational costs. Join\nus on a journey through the cutting-edge advancements that pave the way for the\nfuture of intelligent, energy-efficient neural computing.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "20 pages, 12 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:2211.04297, arXiv:2302.11618, arXiv:2403.03409",
    "pdf_url": "http://arxiv.org/pdf/2407.06452v1",
    "published_date": "2024-07-08 23:33:12 UTC",
    "updated_date": "2024-07-08 23:33:12 UTC"
  },
  {
    "arxiv_id": "2407.06447v2",
    "title": "Geospatial Trajectory Generation via Efficient Abduction: Deployment for Independent Testing",
    "authors": [
      "Divyagna Bavikadi",
      "Dyuman Aditya",
      "Devendra Parkar",
      "Paulo Shakarian",
      "Graham Mueller",
      "Chad Parvis",
      "Gerardo I. Simari"
    ],
    "abstract": "The ability to generate artificial human movement patterns while meeting\nlocation and time constraints is an important problem in the security\ncommunity, particularly as it enables the study of the analog problem of\ndetecting such patterns while maintaining privacy. We frame this problem as an\ninstance of abduction guided by a novel parsimony function represented as an\naggregate truth value over an annotated logic program. This approach has the\nadded benefit of affording explainability to an analyst user. By showing that\nany subset of such a program can provide a lower bound on this parsimony\nrequirement, we are able to abduce movement trajectories efficiently through an\ninformed (i.e., A*) search. We describe how our implementation was enhanced\nwith the application of multiple techniques in order to be scaled and\nintegrated with a cloud-based software stack that included bottom-up rule\nlearning, geolocated knowledge graph retrieval/management, and interfaces with\ngovernment systems for independently conducted government-run tests for which\nwe provide results. We also report on our own experiments showing that we not\nonly provide exact results but also scale to very large scenarios and provide\nrealistic agent trajectories that can go undetected by machine learning anomaly\ndetectors.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
    "pdf_url": "http://arxiv.org/pdf/2407.06447v2",
    "published_date": "2024-07-08 23:11:47 UTC",
    "updated_date": "2025-02-13 11:46:41 UTC"
  },
  {
    "arxiv_id": "2407.06443v2",
    "title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment",
    "authors": [
      "Qizhang Feng",
      "Siva Rajesh Kasa",
      "Santhosh Kumar Kasa",
      "Hyokun Yun",
      "Choon Hui Teo",
      "Sravan Babu Bodapati"
    ],
    "abstract": "Large Language Models (LLMs) have seen widespread adoption due to their\nremarkable natural language capabilities. However, when deploying them in\nreal-world settings, it is important to align LLMs to generate texts according\nto acceptable human standards. Methods such as Proximal Policy Optimization\n(PPO) and Direct Preference Optimization (DPO) have enabled significant\nprogress in refining LLMs using human preference data. However, the privacy\nconcerns inherent in utilizing such preference data have yet to be adequately\nstudied. In this paper, we investigate the vulnerability of LLMs aligned using\ntwo widely used methods - DPO and PPO - to membership inference attacks (MIAs).\nOur study has two main contributions: first, we theoretically motivate that DPO\nmodels are more vulnerable to MIA compared to PPO models; second, we introduce\na novel reference-based attack framework specifically for analyzing preference\ndata called PREMIA (\\uline{Pre}ference data \\uline{MIA}). Using PREMIA and\nexisting baselines we empirically show that DPO models have a relatively\nheightened vulnerability towards MIA.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06443v2",
    "published_date": "2024-07-08 22:53:23 UTC",
    "updated_date": "2025-04-27 21:07:19 UTC"
  },
  {
    "arxiv_id": "2407.06426v2",
    "title": "DebUnc: Improving Large Language Model Agent Communication With Uncertainty Metrics",
    "authors": [
      "Luke Yoffe",
      "Alfonso Amayuelas",
      "William Yang Wang"
    ],
    "abstract": "Multi-agent debates have been introduced to improve the accuracy of Large\nLanguage Models (LLMs) by having multiple agents discuss solutions to a problem\nover several rounds of debate. However, models often generate incorrect yet\nconfident-sounding responses, which can mislead others. This issue arises\npartly because agents do not consider how confident their peers are. To address\nthis, we propose DebUnc, a debate framework that uses uncertainty metrics to\nassess agent confidence. Confidence is then conveyed through a modified\nattention mechanism that adjusts token weights, or through textual prompts.\nEvaluations across benchmarks show that attention-based methods are\nparticularly effective and that performance continues to improve as uncertainty\nestimation becomes more reliable. The code is available at\nhttps://github.com/lukeyoffe/debunc.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06426v2",
    "published_date": "2024-07-08 22:15:01 UTC",
    "updated_date": "2025-02-22 02:15:58 UTC"
  },
  {
    "arxiv_id": "2407.06423v4",
    "title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step Insight Generation",
    "authors": [
      "Gaurav Sahu",
      "Abhay Puri",
      "Juan Rodriguez",
      "Amirhossein Abaskohi",
      "Mohammad Chegini",
      "Alexandre Drouin",
      "Perouz Taslakian",
      "Valentina Zantedeschi",
      "Alexandre Lacoste",
      "David Vazquez",
      "Nicolas Chapados",
      "Christopher Pal",
      "Sai Rajeswar Mudumba",
      "Issam Hadj Laradji"
    ],
    "abstract": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics and can be\naccessed here: https://github.com/ServiceNow/insight-bench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.06423v4",
    "published_date": "2024-07-08 22:06:09 UTC",
    "updated_date": "2025-02-27 17:15:49 UTC"
  },
  {
    "arxiv_id": "2407.06422v1",
    "title": "Exploring the Capability of ChatGPT to Reproduce Human Labels for Social Computing Tasks (Extended Version)",
    "authors": [
      "Yiming Zhu",
      "Peixian Zhang",
      "Ehsan-Ul Haq",
      "Pan Hui",
      "Gareth Tyson"
    ],
    "abstract": "Harnessing the potential of large language models (LLMs) like ChatGPT can\nhelp address social challenges through inclusive, ethical, and sustainable\nmeans. In this paper, we investigate the extent to which ChatGPT can annotate\ndata for social computing tasks, aiming to reduce the complexity and cost of\nundertaking web research. To evaluate ChatGPT's potential, we re-annotate seven\ndatasets using ChatGPT, covering topics related to pressing social issues like\nCOVID-19 misinformation, social bot deception, cyberbully, clickbait news, and\nthe Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise\nin handling these data annotation tasks, albeit with some challenges. Across\nthe seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.\nIts performance excels in clickbait news annotation, correctly labeling 89.66%\nof the data. However, we also observe significant variations in performance\nacross individual labels. Our study reveals predictable patterns in ChatGPT's\nannotation performance. Thus, we propose GPT-Rater, a tool to predict if\nChatGPT can correctly label data for a given annotation task. Researchers can\nuse this to identify where ChatGPT might be suitable for their annotation\nrequirements. We show that GPT-Rater effectively predicts ChatGPT's\nperformance. It performs best on a clickbait headlines dataset by achieving an\naverage F1-score of 95.00%. We believe that this research opens new avenues for\nanalysis and can reduce barriers to engaging in social computing research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of accepted short paper to ASONAM 2024. arXiv admin\n  note: text overlap with arXiv:2304.10145",
    "pdf_url": "http://arxiv.org/pdf/2407.06422v1",
    "published_date": "2024-07-08 22:04:30 UTC",
    "updated_date": "2024-07-08 22:04:30 UTC"
  },
  {
    "arxiv_id": "2407.06416v1",
    "title": "Hybrid Classical-Quantum architecture for vectorised image classification of hand-written sketches",
    "authors": [
      "Y. Cordero",
      "S. Biswas",
      "F. Vilariño",
      "M. Bilkis"
    ],
    "abstract": "Quantum machine learning (QML) investigates how quantum phenomena can be\nexploited in order to learn data in an alternative way, \\textit{e.g.} by means\nof a quantum computer. While recent results evidence that QML models can\npotentially surpass their classical counterparts' performance in specific\ntasks, quantum technology hardware is still unready to reach quantum advantage\nin tasks of significant relevance to the broad scope of the computer science\ncommunity. Recent advances indicate that hybrid classical-quantum models can\nreadily attain competitive performances at low architecture complexities. Such\ninvestigations are often carried out for image-processing tasks, and are\nnotably constrained to modelling \\textit{raster images}, represented as a grid\nof two-dimensional pixels. Here, we introduce vector-based representation of\nsketch drawings as a test-bed for QML models. Such a lower-dimensional data\nstructure results handful to benchmark model's performance, particularly in\ncurrent transition times, where classical simulations of quantum circuits are\nnaturally limited in the number of qubits, and quantum hardware is not readily\navailable to perform large-scale experiments. We report some encouraging\nresults for primitive hybrid classical-quantum architectures, in a canonical\nsketch recognition problem.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06416v1",
    "published_date": "2024-07-08 21:51:20 UTC",
    "updated_date": "2024-07-08 21:51:20 UTC"
  },
  {
    "arxiv_id": "2407.06405v1",
    "title": "AI-driven multi-omics integration for multi-scale predictive modeling of causal genotype-environment-phenotype relationships",
    "authors": [
      "You Wu",
      "Lei Xie"
    ],
    "abstract": "Despite the wealth of single-cell multi-omics data, it remains challenging to\npredict the consequences of novel genetic and chemical perturbations in the\nhuman body. It requires knowledge of molecular interactions at all biological\nlevels, encompassing disease models and humans. Current machine learning\nmethods primarily establish statistical correlations between genotypes and\nphenotypes but struggle to identify physiologically significant causal factors,\nlimiting their predictive power. Key challenges in predictive modeling include\nscarcity of labeled data, generalization across different domains, and\ndisentangling causation from correlation. In light of recent advances in\nmulti-omics data integration, we propose a new artificial intelligence\n(AI)-powered biology-inspired multi-scale modeling framework to tackle these\nissues. This framework will integrate multi-omics data across biological\nlevels, organism hierarchies, and species to predict causal\ngenotype-environment-phenotype relationships under various conditions. AI\nmodels inspired by biology may identify novel molecular targets, biomarkers,\npharmaceutical agents, and personalized medicines for presently unmet medical\nneeds.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06405v1",
    "published_date": "2024-07-08 21:23:25 UTC",
    "updated_date": "2024-07-08 21:23:25 UTC"
  },
  {
    "arxiv_id": "2407.06401v1",
    "title": "Knowledge Management in the Companion Cognitive Architecture",
    "authors": [
      "Constantine Nakos",
      "Kenneth D. Forbus"
    ],
    "abstract": "One of the fundamental aspects of cognitive architectures is their ability to\nencode and manipulate knowledge. Without a consistent, well-designed, and\nscalable knowledge management scheme, an architecture will be unable to move\npast toy problems and tackle the broader problems of cognition. In this paper,\nwe document some of the challenges we have faced in developing the knowledge\nstack for the Companion cognitive architecture and discuss the tools,\nrepresentations, and practices we have developed to overcome them. We also lay\nout a series of potential next steps that will allow Companion agents to play a\ngreater role in managing their own knowledge. It is our hope that these\nobservations will prove useful to other cognitive architecture developers\nfacing similar challenges.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.06401v1",
    "published_date": "2024-07-08 21:20:05 UTC",
    "updated_date": "2024-07-08 21:20:05 UTC"
  },
  {
    "arxiv_id": "2407.06400v1",
    "title": "Interactively Diagnosing Errors in a Semantic Parser",
    "authors": [
      "Constantine Nakos",
      "Kenneth D. Forbus"
    ],
    "abstract": "Hand-curated natural language systems provide an inspectable, correctable\nalternative to language systems based on machine learning, but maintaining them\nrequires considerable effort and expertise. Interactive Natural Language\nDebugging (INLD) aims to lessen this burden by casting debugging as a reasoning\nproblem, asking the user a series of questions to diagnose and correct errors\nin the system's knowledge. In this paper, we present work in progress on an\ninteractive error diagnosis system for the CNLU semantic parser. We show how\nthe first two stages of the INLD pipeline (symptom identification and error\nlocalization) can be cast as a model-based diagnosis problem, demonstrate our\nsystem's ability to diagnose semantic errors on synthetic examples, and discuss\ndesign challenges and frontiers for future work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.06400v1",
    "published_date": "2024-07-08 21:16:09 UTC",
    "updated_date": "2024-07-08 21:16:09 UTC"
  },
  {
    "arxiv_id": "2407.06349v2",
    "title": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect",
    "authors": [
      "Jesse Roberts",
      "Kyle Moore",
      "Thao Pham",
      "Oseremhen Ewaleifoh",
      "Doug Fisher"
    ],
    "abstract": "This paper evaluates whether large language models (LLMs) exhibit cognitive\nfan effects, similar to those discovered by Anderson in humans, after being\npre-trained on human textual data. We conduct two sets of in-context recall\nexperiments designed to elicit fan effects. Consistent with human results, we\nfind that LLM recall uncertainty, measured via token probability, is influenced\nby the fan effect. Our results show that removing uncertainty disrupts the\nobserved effect. The experiments suggest the fan effect is consistent whether\nthe fan value is induced in-context or in the pre-training data. Finally, these\nfindings provide in-silico evidence that fan effects and typicality are\nexpressions of the same phenomena.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06349v2",
    "published_date": "2024-07-08 19:40:50 UTC",
    "updated_date": "2024-09-29 15:47:44 UTC"
  },
  {
    "arxiv_id": "2407.06339v1",
    "title": "Noise-Free Explanation for Driving Action Prediction",
    "authors": [
      "Hongbo Zhu",
      "Theodor Wulff",
      "Rahul Singh Maharjan",
      "Jinpei Han",
      "Angelo Cangelosi"
    ],
    "abstract": "Although attention mechanisms have achieved considerable progress in\nTransformer-based architectures across various Artificial Intelligence (AI)\ndomains, their inner workings remain to be explored. Existing explainable\nmethods have different emphases but are rather one-sided. They primarily\nanalyse the attention mechanisms or gradient-based attribution while neglecting\nthe magnitudes of input feature values or the skip-connection module. Moreover,\nthey inevitably bring spurious noisy pixel attributions unrelated to the\nmodel's decision, hindering humans' trust in the spotted visualization result.\nHence, we propose an easy-to-implement but effective way to remedy this flaw:\nSmooth Noise Norm Attention (SNNA). We weigh the attention by the norm of the\ntransformed value vector and guide the label-specific signal with the attention\ngradient, then randomly sample the input perturbations and average the\ncorresponding gradients to produce noise-free attribution. Instead of\nevaluating the explanation method on the binary or multi-class classification\ntasks like in previous works, we explore the more complex multi-label\nclassification scenario in this work, i.e., the driving action prediction task,\nand trained a model for it specifically. Both qualitative and quantitative\nevaluation results show the superiority of SNNA compared to other SOTA\nattention-based explainable methods in generating a clearer visual explanation\nmap and ranking the input pixel importance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.06339v1",
    "published_date": "2024-07-08 19:21:24 UTC",
    "updated_date": "2024-07-08 19:21:24 UTC"
  },
  {
    "arxiv_id": "2407.06334v2",
    "title": "Double-Ended Synthesis Planning with Goal-Constrained Bidirectional Search",
    "authors": [
      "Kevin Yu",
      "Jihye Roh",
      "Ziang Li",
      "Wenhao Gao",
      "Runzhong Wang",
      "Connor W. Coley"
    ],
    "abstract": "Computer-aided synthesis planning (CASP) algorithms have demonstrated\nexpert-level abilities in planning retrosynthetic routes to molecules of low to\nmoderate complexity. However, current search methods assume the sufficiency of\nreaching arbitrary building blocks, failing to address the common real-world\nconstraint where using specific molecules is desired. To this end, we present a\nformulation of synthesis planning with starting material constraints. Under\nthis formulation, we propose Double-Ended Synthesis Planning (DESP), a novel\nCASP algorithm under a bidirectional graph search scheme that interleaves\nexpansions from the target and from the goal starting materials to ensure\nconstraint satisfiability. The search algorithm is guided by a goal-conditioned\ncost network learned offline from a partially observed hypergraph of valid\nchemical reactions. We demonstrate the utility of DESP in improving solve rates\nand reducing the number of search expansions by biasing synthesis planning\ntowards expert goals on multiple new benchmarks. DESP can make use of existing\none-step retrosynthesis models, and we anticipate its performance to scale as\nthese one-step model capabilities improve.",
    "categories": [
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024 (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2407.06334v2",
    "published_date": "2024-07-08 18:56:00 UTC",
    "updated_date": "2024-11-01 16:45:48 UTC"
  },
  {
    "arxiv_id": "2407.06329v1",
    "title": "Solving Multi-Model MDPs by Coordinate Ascent and Dynamic Programming",
    "authors": [
      "Xihong Su",
      "Marek Petrik"
    ],
    "abstract": "Multi-model Markov decision process (MMDP) is a promising framework for\ncomputing policies that are robust to parameter uncertainty in MDPs. MMDPs aim\nto find a policy that maximizes the expected return over a distribution of MDP\nmodels. Because MMDPs are NP-hard to solve, most methods resort to\napproximations. In this paper, we derive the policy gradient of MMDPs and\npropose CADP, which combines a coordinate ascent method and a dynamic\nprogramming algorithm for solving MMDPs. The main innovation of CADP compared\nwith earlier algorithms is to take the coordinate ascent perspective to adjust\nmodel weights iteratively to guarantee monotone policy improvements to a local\nmaximum. A theoretical analysis of CADP proves that it never performs worse\nthan previous dynamic programming algorithms like WSU. Our numerical results\nindicate that CADP substantially outperforms existing methods on several\nbenchmark problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at UAI 2023",
    "pdf_url": "http://arxiv.org/pdf/2407.06329v1",
    "published_date": "2024-07-08 18:47:59 UTC",
    "updated_date": "2024-07-08 18:47:59 UTC"
  },
  {
    "arxiv_id": "2407.06322v2",
    "title": "MagMax: Leveraging Model Merging for Seamless Continual Learning",
    "authors": [
      "Daniel Marczak",
      "Bartłomiej Twardowski",
      "Tomasz Trzciński",
      "Sebastian Cygert"
    ],
    "abstract": "This paper introduces a continual learning approach named MagMax, which\nutilizes model merging to enable large pre-trained models to continuously learn\nfrom new data without forgetting previously acquired knowledge. Distinct from\ntraditional continual learning methods that aim to reduce forgetting during\ntask training, MagMax combines sequential fine-tuning with a maximum magnitude\nweight selection for effective knowledge integration across tasks. Our initial\ncontribution is an extensive examination of model merging techniques, revealing\nthat simple approaches like weight averaging and random weight selection\nsurprisingly hold up well in various continual learning contexts. More\nimportantly, we present MagMax, a novel model-merging strategy that enables\ncontinual learning of large pre-trained models for successive tasks. Our\nthorough evaluation demonstrates the superiority of MagMax in various\nscenarios, including class- and domain-incremental learning settings. The code\nis available at this URL: https://github.com/danielm1405/magmax.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.06322v2",
    "published_date": "2024-07-08 18:38:52 UTC",
    "updated_date": "2024-07-29 22:17:31 UTC"
  },
  {
    "arxiv_id": "2407.06317v4",
    "title": "Enhanced Safety in Autonomous Driving: Integrating Latent State Diffusion Model for End-to-End Navigation",
    "authors": [
      "Detian Chu",
      "Linyuan Bai",
      "Jianuo Huang",
      "Zhenlong Fang",
      "Peng Zhang",
      "Wei Kang",
      "Haifeng Lin"
    ],
    "abstract": "With the advancement of autonomous driving, ensuring safety during motion\nplanning and navigation is becoming more and more important. However, most\nend-to-end planning methods suffer from a lack of safety. This research\naddresses the safety issue in the control optimization problem of autonomous\ndriving, formulated as Constrained Markov Decision Processes (CMDPs). We\npropose a novel, model-based approach for policy optimization, utilizing a\nconditional Value-at-Risk based Soft Actor Critic to manage constraints in\ncomplex, high-dimensional state spaces effectively. Our method introduces a\nworst-case actor to guide safe exploration, ensuring rigorous adherence to\nsafety requirements even in unpredictable scenarios. The policy optimization\nemploys the Augmented Lagrangian method and leverages latent diffusion models\nto predict and simulate future trajectories. This dual approach not only aids\nin navigating environments safely but also refines the policy's performance by\nintegrating distribution modeling to account for environmental uncertainties.\nEmpirical evaluations conducted in both simulated and real environment\ndemonstrate that our approach outperforms existing methods in terms of safety,\nefficiency, and decision-making capabilities.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06317v4",
    "published_date": "2024-07-08 18:32:40 UTC",
    "updated_date": "2024-07-17 04:30:57 UTC"
  },
  {
    "arxiv_id": "2407.06310v1",
    "title": "Homogeneous Speaker Features for On-the-Fly Dysarthric and Elderly Speaker Adaptation",
    "authors": [
      "Mengzhe Geng",
      "Xurong Xie",
      "Jiajun Deng",
      "Zengrui Jin",
      "Guinan Li",
      "Tianzi Wang",
      "Shujie Hu",
      "Zhaoqing Li",
      "Helen Meng",
      "Xunying Liu"
    ],
    "abstract": "The application of data-intensive automatic speech recognition (ASR)\ntechnologies to dysarthric and elderly adult speech is confronted by their\nmismatch against healthy and nonaged voices, data scarcity and large\nspeaker-level variability. To this end, this paper proposes two novel\ndata-efficient methods to learn homogeneous dysarthric and elderly\nspeaker-level features for rapid, on-the-fly test-time adaptation of DNN/TDNN\nand Conformer ASR models. These include: 1) speaker-level variance-regularized\nspectral basis embedding (VR-SBE) features that exploit a special\nregularization term to enforce homogeneity of speaker features in adaptation;\nand 2) feature-based learning hidden unit contributions (f-LHUC) transforms\nthat are conditioned on VR-SBE features. Experiments are conducted on four\ntasks across two languages: the English UASpeech and TORGO dysarthric speech\ndatasets, the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly\nspeech corpora. The proposed on-the-fly speaker adaptation techniques\nconsistently outperform baseline iVector and xVector adaptation by\nstatistically significant word or character error rate reductions up to 5.32%\nabsolute (18.57% relative) and batch-mode LHUC speaker adaptation by 2.24%\nabsolute (9.20% relative), while operating with real-time factors speeding up\nto 33.6 times against xVectors during adaptation. The efficacy of the proposed\nadaptation techniques is demonstrated in a comparison against current ASR\ntechnologies including SSL pre-trained systems on UASpeech, where our best\nsystem produces a state-of-the-art WER of 23.33%. Analyses show VR-SBE features\nand f-LHUC transforms are insensitive to speaker-level data quantity in\ntesttime adaptation. T-SNE visualization reveals they have stronger\nspeaker-level homogeneity than baseline iVectors, xVectors and batch-mode LHUC\ntransforms.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "In submission to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
    "pdf_url": "http://arxiv.org/pdf/2407.06310v1",
    "published_date": "2024-07-08 18:20:24 UTC",
    "updated_date": "2024-07-08 18:20:24 UTC"
  },
  {
    "arxiv_id": "2407.06309v1",
    "title": "Multimodal Chain-of-Thought Reasoning via ChatGPT to Protect Children from Age-Inappropriate Apps",
    "authors": [
      "Chuanbo Hu",
      "Bin Liu",
      "Minglei Yin",
      "Yilu Zhou",
      "Xin Li"
    ],
    "abstract": "Mobile applications (Apps) could expose children to inappropriate themes such\nas sexual content, violence, and drug use. Maturity rating offers a quick and\neffective method for potential users, particularly guardians, to assess the\nmaturity levels of apps. Determining accurate maturity ratings for mobile apps\nis essential to protect children's health in today's saturated digital\nmarketplace. Existing approaches to maturity rating are either inaccurate\n(e.g., self-reported rating by developers) or costly (e.g., manual\nexamination). In the literature, there are few text-mining-based approaches to\nmaturity rating. However, each app typically involves multiple modalities,\nnamely app description in the text, and screenshots in the image. In this\npaper, we present a framework for determining app maturity levels that utilize\nmultimodal large language models (MLLMs), specifically ChatGPT-4 Vision.\nPowered by Chain-of-Thought (CoT) reasoning, our framework systematically\nleverages ChatGPT-4 to process multimodal app data (i.e., textual descriptions\nand screenshots) and guide the MLLM model through a step-by-step reasoning\npathway from initial content analysis to final maturity rating determination.\nAs a result, through explicitly incorporating CoT reasoning, our framework\nenables ChatGPT to understand better and apply maturity policies to facilitate\nmaturity rating. Experimental results indicate that the proposed method\noutperforms all baseline models and other fusion strategies.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06309v1",
    "published_date": "2024-07-08 18:20:10 UTC",
    "updated_date": "2024-07-08 18:20:10 UTC"
  },
  {
    "arxiv_id": "2407.06304v1",
    "title": "VIMI: Grounding Video Generation through Multi-modal Instruction",
    "authors": [
      "Yuwei Fang",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Tsai-Shien Chen",
      "Kuan-Chien Wang",
      "Ivan Skorokhodov",
      "Graham Neubig",
      "Sergey Tulyakov"
    ],
    "abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for\ntheir pretraining. This limitation stems from the absence of large-scale\nmultimodal prompt video datasets, resulting in a lack of visual grounding and\nrestricting their versatility and application in multimodal integration. To\naddress this, we construct a large-scale multimodal prompt dataset by employing\nretrieval methods to pair in-context examples with the given text prompts and\nthen utilize a two-stage training strategy to enable diverse video generation\ntasks within the same model. In the first stage, we propose a multimodal\nconditional video generation framework for pretraining on these augmented\ndatasets, establishing a foundational model for grounded video generation.\nSecondly, we finetune the model from the first stage on three video generation\ntasks, incorporating multi-modal instructions. This process further refines the\nmodel's ability to handle diverse inputs and tasks, ensuring seamless\nintegration of multi-modal information. After this two-stage train-ing process,\nVIMI demonstrates multimodal understanding capabilities, producing contextually\nrich and personalized videos grounded in the provided inputs, as shown in\nFigure 1. Compared to previous visual grounded video generation methods, VIMI\ncan synthesize consistent and temporally coherent videos with large motion\nwhile retaining the semantic control. Lastly, VIMI also achieves\nstate-of-the-art text-to-video generation results on UCF101 benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06304v1",
    "published_date": "2024-07-08 18:12:49 UTC",
    "updated_date": "2024-07-08 18:12:49 UTC"
  },
  {
    "arxiv_id": "2407.06292v1",
    "title": "Hybrid X-Linker: Automated Data Generation and Extreme Multi-label Ranking for Biomedical Entity Linking",
    "authors": [
      "Pedro Ruas",
      "Fernando Gallego",
      "Francisco J. Veredas",
      "Francisco M. Couto"
    ],
    "abstract": "State-of-the-art deep learning entity linking methods rely on extensive\nhuman-labelled data, which is costly to acquire. Current datasets are limited\nin size, leading to inadequate coverage of biomedical concepts and diminished\nperformance when applied to new data. In this work, we propose to automatically\ngenerate data to create large-scale training datasets, which allows the\nexploration of approaches originally developed for the task of extreme\nmulti-label ranking in the biomedical entity linking task. We propose the\nhybrid X-Linker pipeline that includes different modules to link disease and\nchemical entity mentions to concepts in the MEDIC and the CTD-Chemical\nvocabularies, respectively. X-Linker was evaluated on several biomedical\ndatasets: BC5CDR-Disease, BioRED-Disease, NCBI-Disease, BC5CDR-Chemical,\nBioRED-Chemical, and NLM-Chem, achieving top-1 accuracies of 0.8307, 0.7969,\n0.8271, 0.9511, 0.9248, and 0.7895, respectively. X-Linker demonstrated\nsuperior performance in three datasets: BC5CDR-Disease, NCBI-Disease, and\nBioRED-Chemical. In contrast, SapBERT outperformed X-Linker in the remaining\nthree datasets. Both models rely only on the mention string for their\noperations. The source code of X-Linker and its associated data are publicly\navailable for performing biomedical entity linking without requiring\npre-labelled entities with identifiers from specific knowledge organization\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2407.06292v1",
    "published_date": "2024-07-08 18:04:22 UTC",
    "updated_date": "2024-07-08 18:04:22 UTC"
  },
  {
    "arxiv_id": "2407.06192v2",
    "title": "Multi-Object Hallucination in Vision-Language Models",
    "authors": [
      "Xuweiyi Chen",
      "Ziqiao Ma",
      "Xuejun Zhang",
      "Sihan Xu",
      "Shengyi Qian",
      "Jianing Yang",
      "David F. Fouhey",
      "Joyce Chai"
    ],
    "abstract": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1). LVLMs suffer more hallucinations when\nfocusing on multiple objects compared to a single object. (2). The tested\nobject class distribution affects hallucination behaviors, indicating that\nLVLMs may follow shortcuts and spurious correlations. (3). Hallucinatory\nbehaviors are influenced by data-specific factors, salience and frequency, and\nmodel intrinsic behaviors. We hope to enable LVLMs to recognize and reason\nabout multiple objects that often occur in realistic visual scenes, provide\ninsights, and quantify our progress towards mitigating the issues.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2024 | Project page:\n  https://multi-object-hallucination.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.06192v2",
    "published_date": "2024-07-08 17:59:57 UTC",
    "updated_date": "2024-10-31 18:16:38 UTC"
  },
  {
    "arxiv_id": "2407.06189v1",
    "title": "Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision",
    "authors": [
      "Orr Zohar",
      "Xiaohan Wang",
      "Yonatan Bitton",
      "Idan Szpektor",
      "Serena Yeung-Levy"
    ],
    "abstract": "The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://orrzohar.github.io/projects/video-star/",
    "pdf_url": "http://arxiv.org/pdf/2407.06189v1",
    "published_date": "2024-07-08 17:59:42 UTC",
    "updated_date": "2024-07-08 17:59:42 UTC"
  },
  {
    "arxiv_id": "2407.06177v1",
    "title": "Vision-Language Models under Cultural and Inclusive Considerations",
    "authors": [
      "Antonia Karamolegkou",
      "Phillip Rust",
      "Yong Cao",
      "Ruixiang Cui",
      "Anders Søgaard",
      "Daniel Hershcovich"
    ],
    "abstract": "Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "HuCLLM @ ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.06177v1",
    "published_date": "2024-07-08 17:50:00 UTC",
    "updated_date": "2024-07-08 17:50:00 UTC"
  },
  {
    "arxiv_id": "2407.06172v3",
    "title": "On Speeding Up Language Model Evaluation",
    "authors": [
      "Jin Peng Zhou",
      "Christian K. Belardi",
      "Ruihan Wu",
      "Travis Zhang",
      "Carla P. Gomes",
      "Wen Sun",
      "Kilian Q. Weinberger"
    ],
    "abstract": "Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem\nover hyper-parameters. This exhaustive evaluation can be time-consuming and\ncostly. In this paper, we propose an $\\textit{adaptive}$ approach to explore\nthis space. We are exploiting the fact that often only few samples are needed\nto identify clearly superior or inferior settings, and that many evaluation\ntests are highly correlated. We lean on multi-armed bandits to sequentially\nidentify the next (method, validation sample)-pair to evaluate and utilize\nlow-rank matrix factorization to fill in missing evaluations. We carefully\nassess the efficacy of our approach on several competitive benchmark problems\nand show that it can identify the top-performing method using only 5-15% of the\ntypical resources -- resulting in 85-95% LLM cost savings. Our code is\navailable at https://github.com/kilian-group/banditeval.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.06172v3",
    "published_date": "2024-07-08 17:48:42 UTC",
    "updated_date": "2025-02-26 21:53:59 UTC"
  },
  {
    "arxiv_id": "2407.06152v1",
    "title": "Uni-ELF: A Multi-Level Representation Learning Framework for Electrolyte Formulation Design",
    "authors": [
      "Boshen Zeng",
      "Sian Chen",
      "Xinxin Liu",
      "Changhong Chen",
      "Bin Deng",
      "Xiaoxu Wang",
      "Zhifeng Gao",
      "Yuzhi Zhang",
      "Weinan E",
      "Linfeng Zhang"
    ],
    "abstract": "Advancements in lithium battery technology heavily rely on the design and\nengineering of electrolytes. However, current schemes for molecular design and\nrecipe optimization of electrolytes lack an effective\ncomputational-experimental closed loop and often fall short in accurately\npredicting diverse electrolyte formulation properties. In this work, we\nintroduce Uni-ELF, a novel multi-level representation learning framework to\nadvance electrolyte design. Our approach involves two-stage pretraining:\nreconstructing three-dimensional molecular structures at the molecular level\nusing the Uni-Mol model, and predicting statistical structural properties\n(e.g., radial distribution functions) from molecular dynamics simulations at\nthe mixture level. Through this comprehensive pretraining, Uni-ELF is able to\ncapture intricate molecular and mixture-level information, which significantly\nenhances its predictive capability. As a result, Uni-ELF substantially\noutperforms state-of-the-art methods in predicting both molecular properties\n(e.g., melting point, boiling point, synthesizability) and formulation\nproperties (e.g., conductivity, Coulombic efficiency). Moreover, Uni-ELF can be\nseamlessly integrated into an automatic experimental design workflow. We\nbelieve this innovative framework will pave the way for automated AI-based\nelectrolyte design and engineering.",
    "categories": [
      "physics.chem-ph",
      "cs.AI"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06152v1",
    "published_date": "2024-07-08 17:26:49 UTC",
    "updated_date": "2024-07-08 17:26:49 UTC"
  },
  {
    "arxiv_id": "2407.06146v2",
    "title": "Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks",
    "authors": [
      "Lukas Netz",
      "Jan Reimer",
      "Bernhard Rumpe"
    ],
    "abstract": "We present and evaluate a method called grammar masking, which is used to\nguide large language models (LLMs) toward producing syntactically correct\nmodels for a given context-free grammar. Prompt engineering methods such as\nfew-shot learning or priming can be used to improve the chances of an LLM\nproducing correct syntax, but the more complex the grammar, the more\ntime-consuming and less promising these methods become. Previous work is\nfocused primarily on the usage of either language model training or prompt\nengineering. In this work, a method is presented that restricts the output to a\ngiven grammar using constrained decoding to ensure the output adheres to a\nvalid syntax. We use several DSLs built with MontiCore and task multiple LLMs\nto produce models with and without constrained decoding. A corresponding parser\nis used to confirm the syntactic correctness of each model. We show that\ngrammar masking can dramatically improve the modeling capabilities of several\nLLMs, reducing the need for well-refined prompting while increasing the chance\nof producing correct models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint to be published in the MODELS Workshop \"MDE Intelligence\"",
    "pdf_url": "http://arxiv.org/pdf/2407.06146v2",
    "published_date": "2024-07-08 17:19:59 UTC",
    "updated_date": "2024-07-09 07:08:11 UTC"
  },
  {
    "arxiv_id": "2407.06135v1",
    "title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation",
    "authors": [
      "Ethan Chern",
      "Jiadi Su",
      "Yan Ma",
      "Pengfei Liu"
    ],
    "abstract": "Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06135v1",
    "published_date": "2024-07-08 17:08:02 UTC",
    "updated_date": "2024-07-08 17:08:02 UTC"
  },
  {
    "arxiv_id": "2407.06129v2",
    "title": "Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization",
    "authors": [
      "Hannah K. Bako",
      "Arshnoor Bhutani",
      "Xinyi Liu",
      "Kwesi A. Cobbina",
      "Zhicheng Liu"
    ],
    "abstract": "Automatically generating data visualizations in response to human utterances\non datasets necessitates a deep semantic understanding of the data utterance,\nincluding implicit and explicit references to data attributes, visualization\ntasks, and necessary data preparation steps. Natural Language Interfaces (NLIs)\nfor data visualization have explored ways to infer such information, yet\nchallenges persist due to inherent uncertainty in human speech. Recent advances\nin Large Language Models (LLMs) provide an avenue to address these challenges,\nbut their ability to extract the relevant semantic information remains\nunexplored. In this study, we evaluate four publicly available LLMs (GPT-4,\nGemini-Pro, Llama3, and Mixtral), investigating their ability to comprehend\nutterances even in the presence of uncertainty and identify the relevant data\ncontext and visual tasks. Our findings reveal that LLMs are sensitive to\nuncertainties in utterances. Despite this sensitivity, they are able to extract\nthe relevant data context. However, LLMs struggle with inferring visualization\ntasks. Based on these results, we highlight future research directions on using\nLLMs for visualization generation.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 4 figures, IEEE VIS short papers",
    "pdf_url": "http://arxiv.org/pdf/2407.06129v2",
    "published_date": "2024-07-08 17:04:31 UTC",
    "updated_date": "2024-07-09 15:59:29 UTC"
  },
  {
    "arxiv_id": "2407.06125v2",
    "title": "Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities",
    "authors": [
      "Chayan Tank",
      "Sarthak Pol",
      "Vinayak Katoch",
      "Shaina Mehta",
      "Avinash Anand",
      "Rajiv Ratn Shah"
    ],
    "abstract": "Depression has proven to be a significant public health issue, profoundly\naffecting the psychological well-being of individuals. If it remains\nundiagnosed, depression can lead to severe health issues, which can manifest\nphysically and even lead to suicide. Generally, Diagnosing depression or any\nother mental disorder involves conducting semi-structured interviews alongside\nsupplementary questionnaires, including variants of the Patient Health\nQuestionnaire (PHQ) by Clinicians and mental health professionals. This\napproach places significant reliance on the experience and judgment of trained\nphysicians, making the diagnosis susceptible to personal biases. Given that the\nunderlying mechanisms causing depression are still being actively researched,\nphysicians often face challenges in diagnosing and treating the condition,\nparticularly in its early stages of clinical presentation. Recently,\nsignificant strides have been made in Artificial neural computing to solve\nproblems involving text, image, and speech in various domains. Our analysis has\naimed to leverage these state-of-the-art (SOTA) models in our experiments to\nachieve optimal outcomes leveraging multiple modalities. The experiments were\nperformed on the Extended Distress Analysis Interview Corpus Wizard of Oz\ndataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC)\n2019 Challenge. The proposed solutions demonstrate better results achieved by\nProprietary and Open-source Large Language Models (LLMs), which achieved a Root\nMean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC\n2019 challenge baseline results and current SOTA regression analysis\narchitectures. Additionally, the proposed solution achieved an accuracy of\n71.43% in the classification task. The paper also includes a novel audio-visual\nmulti-modal network that predicts PHQ-8 scores with an RMSE of 6.51.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 9 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.06125v2",
    "published_date": "2024-07-08 17:00:51 UTC",
    "updated_date": "2024-12-02 10:02:18 UTC"
  },
  {
    "arxiv_id": "2407.12852v2",
    "title": "Historical Ink: Semantic Shift Detection for 19th Century Spanish",
    "authors": [
      "Tony Montes",
      "Laura Manrique-Gómez",
      "Rubén Manrique"
    ],
    "abstract": "This paper explores the evolution of word meanings in 19th-century Spanish\ntexts, with an emphasis on Latin American Spanish, using computational\nlinguistics techniques. It addresses the Semantic Shift Detection (SSD) task,\nwhich is crucial for understanding linguistic evolution, particularly in\nhistorical contexts. The study focuses on analyzing a set of Spanish target\nwords. To achieve this, a 19th-century Spanish corpus is constructed, and a\ncustomizable pipeline for SSD tasks is developed. This pipeline helps find the\nsenses of a word and measure their semantic change between two corpora using\nfine-tuned BERT-like models with old Spanish texts for both Latin American and\ngeneral Spanish cases. The results provide valuable insights into the cultural\nand societal shifts reflected in language changes over time.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages; added a preprint-reference URL",
    "pdf_url": "http://arxiv.org/pdf/2407.12852v2",
    "published_date": "2024-07-08 16:49:34 UTC",
    "updated_date": "2024-07-19 01:54:26 UTC"
  },
  {
    "arxiv_id": "2407.06098v1",
    "title": "Epistemological Bias As a Means for the Automated Detection of Injustices in Text",
    "authors": [
      "Kenya Andrews",
      "Lamogha Chiazor"
    ],
    "abstract": "Injustice occurs when someone experiences unfair treatment or their rights\nare violated and is often due to the presence of implicit biases and prejudice\nsuch as stereotypes. The automated identification of injustice in text has\nreceived little attention, due in part to the fact that underlying implicit\nbiases or stereotypes are rarely explicitly stated and that instances often\noccur unconsciously due to the pervasive nature of prejudice in society. Here,\nwe describe a novel framework that combines the use of a fine-tuned BERT-based\nbias detection model, two stereotype detection models, and a lexicon-based\napproach to show that epistemological biases (i.e., words, which presupposes,\nentails, asserts, hedges, or boosts text to erode or assert a person's capacity\nas a knower) can assist with the automatic detection of injustice in text. The\nnews media has many instances of injustice (i.e. discriminatory narratives),\nthus it is our use case here. We conduct and discuss an empirical qualitative\nresearch study which shows how the framework can be applied to detect\ninjustices, even at higher volumes of data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06098v1",
    "published_date": "2024-07-08 16:38:31 UTC",
    "updated_date": "2024-07-08 16:38:31 UTC"
  },
  {
    "arxiv_id": "2407.06093v1",
    "title": "Artificial Intuition: Efficient Classification of Scientific Abstracts",
    "authors": [
      "Harsh Sakhrani",
      "Naseela Pervez",
      "Anirudh Ravi Kumar",
      "Fred Morstatter",
      "Alexandra Graddy Reed",
      "Andrea Belz"
    ],
    "abstract": "It is desirable to coarsely classify short scientific texts, such as grant or\npublication abstracts, for strategic insight or research portfolio management.\nThese texts efficiently transmit dense information to experts possessing a rich\nbody of knowledge to aid interpretation. Yet this task is remarkably difficult\nto automate because of brevity and the absence of context. To address this gap,\nwe have developed a novel approach to generate and appropriately assign coarse\ndomain-specific labels. We show that a Large Language Model (LLM) can provide\nmetadata essential to the task, in a process akin to the augmentation of\nsupplemental knowledge representing human intuition, and propose a workflow. As\na pilot study, we use a corpus of award abstracts from the National Aeronautics\nand Space Administration (NASA). We develop new assessment tools in concert\nwith established performance metrics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06093v1",
    "published_date": "2024-07-08 16:34:47 UTC",
    "updated_date": "2024-07-08 16:34:47 UTC"
  },
  {
    "arxiv_id": "2407.06088v1",
    "title": "Qualitative Event Perception: Leveraging Spatiotemporal Episodic Memory for Learning Combat in a Strategy Game",
    "authors": [
      "Will Hancock",
      "Kenneth D. Forbus"
    ],
    "abstract": "Event perception refers to people's ability to carve up continuous experience\ninto meaningful discrete events. We speak of finishing our morning coffee,\nmowing the lawn, leaving work, etc. as singular occurrences that are localized\nin time and space. In this work, we analyze how spatiotemporal representations\ncan be used to automatically segment continuous experience into structured\nepisodes, and how these descriptions can be used for analogical learning. These\nrepresentations are based on Hayes' notion of histories and build upon existing\nwork on qualitative episodic memory. Our agent automatically generates event\ndescriptions of military battles in a strategy game and improves its gameplay\nby learning from this experience. Episodes are segmented based on changing\nproperties in the world and we show evidence that they facilitate learning\nbecause they capture event descriptions at a useful spatiotemporal grain size.\nThis is evaluated through our agent's performance in the game. We also show\nempirical evidence that the perception of spatial extent of episodes affects\nboth their temporal duration as well as the number of overall cases generated.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.06088v1",
    "published_date": "2024-07-08 16:28:38 UTC",
    "updated_date": "2024-07-08 16:28:38 UTC"
  },
  {
    "arxiv_id": "2407.06079v1",
    "title": "Layered Diffusion Model for One-Shot High Resolution Text-to-Image Synthesis",
    "authors": [
      "Emaad Khwaja",
      "Abdullah Rashwan",
      "Ting Chen",
      "Oliver Wang",
      "Suraj Kothawade",
      "Yeqing Li"
    ],
    "abstract": "We present a one-shot text-to-image diffusion model that can generate\nhigh-resolution images from natural language descriptions. Our model employs a\nlayered U-Net architecture that simultaneously synthesizes images at multiple\nresolution scales. We show that this method outperforms the baseline of\nsynthesizing images only at the target resolution, while reducing the\ncomputational cost per step. We demonstrate that higher resolution synthesis\ncan be achieved by layering convolutions at additional resolution scales, in\ncontrast to other methods which require additional models for super-resolution\nsynthesis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06079v1",
    "published_date": "2024-07-08 16:25:34 UTC",
    "updated_date": "2024-07-08 16:25:34 UTC"
  },
  {
    "arxiv_id": "2407.06077v1",
    "title": "Object-Oriented Material Classification and 3D Clustering for Improved Semantic Perception and Mapping in Mobile Robots",
    "authors": [
      "Siva Krishna Ravipati",
      "Ehsan Latif",
      "Ramviyas Parasuraman",
      "Suchendra M. Bhandarkar"
    ],
    "abstract": "Classification of different object surface material types can play a\nsignificant role in the decision-making algorithms for mobile robots and\nautonomous vehicles. RGB-based scene-level semantic segmentation has been\nwell-addressed in the literature. However, improving material recognition using\nthe depth modality and its integration with SLAM algorithms for 3D semantic\nmapping could unlock new potential benefits in the robotics perception\npipeline. To this end, we propose a complementarity-aware deep learning\napproach for RGB-D-based material classification built on top of an\nobject-oriented pipeline. The approach further integrates the ORB-SLAM2 method\nfor 3D scene mapping with multiscale clustering of the detected material\nsemantics in the point cloud map generated by the visual SLAM algorithm.\nExtensive experimental results with existing public datasets and newly\ncontributed real-world robot datasets demonstrate a significant improvement in\nmaterial classification and 3D clustering accuracy compared to state-of-the-art\napproaches for 3D semantic scene mapping.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IROS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.06077v1",
    "published_date": "2024-07-08 16:25:01 UTC",
    "updated_date": "2024-07-08 16:25:01 UTC"
  },
  {
    "arxiv_id": "2407.06076v2",
    "title": "Understanding Visual Feature Reliance through the Lens of Complexity",
    "authors": [
      "Thomas Fel",
      "Louis Bethune",
      "Andrew Kyle Lampinen",
      "Thomas Serre",
      "Katherine Hermann"
    ],
    "abstract": "Recent studies suggest that deep learning models inductive bias towards\nfavoring simpler features may be one of the sources of shortcut learning. Yet,\nthere has been limited focus on understanding the complexity of the myriad\nfeatures that models learn. In this work, we introduce a new metric for\nquantifying feature complexity, based on $\\mathscr{V}$-information and\ncapturing whether a feature requires complex computational transformations to\nbe extracted. Using this $\\mathscr{V}$-information metric, we analyze the\ncomplexities of 10,000 features, represented as directions in the penultimate\nlayer, that were extracted from a standard ImageNet-trained vision model. Our\nstudy addresses four key questions: First, we ask what features look like as a\nfunction of complexity and find a spectrum of simple to complex features\npresent within the model. Second, we ask when features are learned during\ntraining. We find that simpler features dominate early in training, and more\ncomplex features emerge gradually. Third, we investigate where within the\nnetwork simple and complex features flow, and find that simpler features tend\nto bypass the visual hierarchy via residual connections. Fourth, we explore the\nconnection between features complexity and their importance in driving the\nnetworks decision. We find that complex features tend to be less important.\nSurprisingly, important features become accessible at earlier layers during\ntraining, like a sedimentation process, allowing the model to build upon these\nfoundational elements.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06076v2",
    "published_date": "2024-07-08 16:21:53 UTC",
    "updated_date": "2024-10-28 11:15:06 UTC"
  },
  {
    "arxiv_id": "2407.06071v2",
    "title": "From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty",
    "authors": [
      "Maor Ivgi",
      "Ori Yoran",
      "Jonathan Berant",
      "Mor Geva"
    ],
    "abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under epistemic uncertainty, and investigate the\nconnection between them. We categorize fallback behaviors - sequence\nrepetitions, degenerate text, and hallucinations - and extensively analyze them\nin models from the same family that differ by the amount of pretraining tokens,\nparameter count, or the inclusion of instruction-following training. Our\nexperiments reveal a clear and consistent ordering of fallback behaviors,\nacross all these axes: the more advanced an LLM is (i.e., trained on more\ntokens, has more parameters, or instruction-tuned), its fallback behavior\nshifts from sequence repetitions, to degenerate text, and then to\nhallucinations. Moreover, the same ordering is observed during the generation\nof a single sequence, even for the best-performing models; as uncertainty\nincreases, models shift from generating hallucinations to producing degenerate\ntext and finally sequence repetitions. Lastly, we demonstrate that while common\ndecoding techniques, such as random sampling, alleviate unwanted behaviors like\nsequence repetitions, they increase harder-to-detect hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS Workshop on Attributing Model Behavior at Scale (ATTRIB 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.06071v2",
    "published_date": "2024-07-08 16:13:42 UTC",
    "updated_date": "2025-02-08 10:48:33 UTC"
  },
  {
    "arxiv_id": "2407.06057v3",
    "title": "Variational Best-of-N Alignment",
    "authors": [
      "Afra Amini",
      "Tim Vieira",
      "Elliott Ash",
      "Ryan Cotterell"
    ],
    "abstract": "Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.06057v3",
    "published_date": "2024-07-08 15:59:44 UTC",
    "updated_date": "2025-03-04 14:33:50 UTC"
  },
  {
    "arxiv_id": "2407.06056v1",
    "title": "Stranger Danger! Identifying and Avoiding Unpredictable Pedestrians in RL-based Social Robot Navigation",
    "authors": [
      "Sara Pohland",
      "Alvin Tan",
      "Prabal Dutta",
      "Claire Tomlin"
    ],
    "abstract": "Reinforcement learning (RL) methods for social robot navigation show great\nsuccess navigating robots through large crowds of people, but the performance\nof these learning-based methods tends to degrade in particularly challenging or\nunfamiliar situations due to the models' dependency on representative training\ndata. To ensure human safety and comfort, it is critical that these algorithms\nhandle uncommon cases appropriately, but the low frequency and wide diversity\nof such situations present a significant challenge for these data-driven\nmethods. To overcome this challenge, we propose modifications to the learning\nprocess that encourage these RL policies to maintain additional caution in\nunfamiliar situations. Specifically, we improve the Socially Attentive\nReinforcement Learning (SARL) policy by (1) modifying the training process to\nsystematically introduce deviations into a pedestrian model, (2) updating the\nvalue network to estimate and utilize pedestrian-unpredictability features, and\n(3) implementing a reward function to learn an effective response to pedestrian\nunpredictability. Compared to the original SARL policy, our modified policy\nmaintains similar navigation times and path lengths, while reducing the number\nof collisions by 82% and reducing the proportion of time spent in the\npedestrians' personal space by up to 19 percentage points for the most\ndifficult cases. We also describe how to apply these modifications to other RL\npolicies and demonstrate that some key high-level behaviors of our approach\ntransfer to a physical robot.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06056v1",
    "published_date": "2024-07-08 15:58:33 UTC",
    "updated_date": "2024-07-08 15:58:33 UTC"
  },
  {
    "arxiv_id": "2407.06025v1",
    "title": "iLLM-TSC: Integration reinforcement learning and large language model for traffic signal control policy improvement",
    "authors": [
      "Aoyu Pang",
      "Maonan Wang",
      "Man-On Pun",
      "Chung Shue Chen",
      "Xi Xiong"
    ],
    "abstract": "Urban congestion remains a critical challenge, with traffic signal control\n(TSC) emerging as a potent solution. TSC is often modeled as a Markov Decision\nProcess problem and then solved using reinforcement learning (RL), which has\nproven effective. However, the existing RL-based TSC system often overlooks\nimperfect observations caused by degraded communication, such as packet loss,\ndelays, and noise, as well as rare real-life events not included in the reward\nfunction, such as unconsidered emergency vehicles. To address these\nlimitations, we introduce a novel integration framework that combines a large\nlanguage model (LLM) with RL. This framework is designed to manage overlooked\nelements in the reward function and gaps in state information, thereby\nenhancing the policies of RL agents. In our approach, RL initially makes\ndecisions based on observed data. Subsequently, LLMs evaluate these decisions\nto verify their reasonableness. If a decision is found to be unreasonable, it\nis adjusted accordingly. Additionally, this integration approach can be\nseamlessly integrated with existing RL-based TSC systems without necessitating\nmodifications. Extensive testing confirms that our approach reduces the average\nwaiting time by $17.5\\%$ in degraded communication conditions as compared to\ntraditional RL methods, underscoring its potential to advance practical RL\napplications in intelligent transportation systems. The related code can be\nfound at \\url{https://github.com/Traffic-Alpha/iLLM-TSC}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06025v1",
    "published_date": "2024-07-08 15:22:49 UTC",
    "updated_date": "2024-07-08 15:22:49 UTC"
  },
  {
    "arxiv_id": "2407.06023v3",
    "title": "Distilling System 2 into System 1",
    "authors": [
      "Ping Yu",
      "Jing Xu",
      "Jason Weston",
      "Ilia Kulikov"
    ],
    "abstract": "Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06023v3",
    "published_date": "2024-07-08 15:17:46 UTC",
    "updated_date": "2024-07-24 18:40:36 UTC"
  },
  {
    "arxiv_id": "2407.06011v1",
    "title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian",
    "authors": [
      "Tommaso Mario Buonocore",
      "Simone Rancati",
      "Enea Parimbelli"
    ],
    "abstract": "The development of domain-specific language models has significantly advanced\nnatural language processing applications in various specialized fields,\nparticularly in biomedicine. However, the focus has largely been on\nEnglish-language models, leaving a gap for less-resourced languages such as\nItalian. This paper introduces Igea, the first decoder-only language model\ndesigned explicitly for biomedical text generation in Italian. Built on the\nMinerva model and continually pretrained on a diverse corpus of Italian medical\ntexts, Igea is available in three model sizes: 350 million, 1 billion, and 3\nbillion parameters. The models aim to balance computational efficiency and\nperformance, addressing the challenges of managing the peculiarities of medical\nterminology in Italian. We evaluate Igea using a mix of in-domain biomedical\ncorpora and general-purpose benchmarks, highlighting its efficacy and retention\nof general knowledge even after the domain-specific training. This paper\ndiscusses the model's development and evaluation, providing a foundation for\nfuture advancements in Italian biomedical NLP.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; J.3"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 1 figure, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.06011v1",
    "published_date": "2024-07-08 15:04:21 UTC",
    "updated_date": "2024-07-08 15:04:21 UTC"
  },
  {
    "arxiv_id": "2407.15316v1",
    "title": "AI as a Tool for Fair Journalism: Case Studies from Malta",
    "authors": [
      "Dylan Seychell",
      "Gabriel Hili",
      "Jonathan Attard",
      "Konstantinos Makantatis"
    ],
    "abstract": "In today`s media landscape, the role of Artificial Intelligence (AI) in\nshaping societal perspectives and journalistic integrity is becoming\nincreasingly apparent. This paper presents two case studies centred on Malta`s\nmedia market featuring technical novelty. Despite its relatively small scale,\nMalta offers invaluable insights applicable to both similar and broader media\ncontexts. These two projects focus on media monitoring and present tools\ndesigned to analyse potential biases in news articles and television news\nsegments. The first project uses Computer Vision and Natural Language\nProcessing techniques to analyse the coherence between images in news articles\nand their corresponding captions, headlines, and article bodies. The second\nproject employs computer vision techniques to track individuals` on-screen time\nor visual exposure in news videos, providing queryable data. These initiatives\naim to contribute to society by providing both journalists and the public with\nthe means to identify biases. Furthermore, we make these tools accessible to\njournalists to improve the trustworthiness of media outlets by offering robust\ntools for detecting and reducing bias.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted as a full paper in the proceedings of the IEEE 2024\n  Conference on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2407.15316v1",
    "published_date": "2024-07-08 15:02:39 UTC",
    "updated_date": "2024-07-08 15:02:39 UTC"
  },
  {
    "arxiv_id": "2407.06003v1",
    "title": "Surprising gender biases in GPT",
    "authors": [
      "Raluca Alexandra Fulgu",
      "Valerio Capraro"
    ],
    "abstract": "We present seven experiments exploring gender biases in GPT. Initially, GPT\nwas asked to generate demographics of a potential writer of twenty phrases\ncontaining feminine stereotypes and twenty with masculine stereotypes. Results\nshow a strong asymmetry, with stereotypically masculine sentences attributed to\na female more often than vice versa. For example, the sentence \"I love playing\nfotbal! Im practicing with my cosin Michael\" was constantly assigned by ChatGPT\nto a female writer. This phenomenon likely reflects that while initiatives to\nintegrate women in traditionally masculine roles have gained momentum, the\nreverse movement remains relatively underdeveloped. Subsequent experiments\ninvestigate the same issue in high-stakes moral dilemmas. GPT-4 finds it more\nappropriate to abuse a man to prevent a nuclear apocalypse than to abuse a\nwoman. This bias extends to other forms of violence central to the gender\nparity debate (abuse), but not to those less central (torture). Moreover, this\nbias increases in cases of mixed-sex violence for the greater good: GPT-4\nagrees with a woman using violence against a man to prevent a nuclear\napocalypse but disagrees with a man using violence against a woman for the same\npurpose. Finally, these biases are implicit, as they do not emerge when GPT-4\nis directly asked to rank moral violations. These results highlight the\nnecessity of carefully managing inclusivity efforts to prevent unintended\ndiscrimination.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06003v1",
    "published_date": "2024-07-08 14:57:02 UTC",
    "updated_date": "2024-07-08 14:57:02 UTC"
  },
  {
    "arxiv_id": "2407.05991v2",
    "title": "Multi-Texture Synthesis through Signal Responsive Neural Cellular Automata",
    "authors": [
      "Mirela-Magdalena Catrina",
      "Ioana Cristina Plajer",
      "Alexandra Baicoianu"
    ],
    "abstract": "Neural Cellular Automata (NCA) have proven to be effective in a variety of\nfields, with numerous biologically inspired applications. One of the fields, in\nwhich NCAs perform well is the generation of textures, modelling global\npatterns from local interactions governed by uniform and coherent rules. This\npaper aims to enhance the usability of NCAs in texture synthesis by addressing\na shortcoming of current NCA architectures for texture generation, which\nrequires separately trained NCA for each individual texture. In this work, we\ntrain a single NCA for the evolution of multiple textures, based on individual\nexamples. Our solution provides texture information in the state of each cell,\nin the form of an internally coded genomic signal, which enables the NCA to\ngenerate the expected texture. Such a neural cellular automaton not only\nmaintains its regenerative capability but also allows for interpolation between\nlearned textures and supports grafting techniques. This demonstrates the\nability to edit generated textures and the potential for them to merge and\ncoexist within the same automaton. We also address questions related to the\ninfluence of the genomic information and the cost function on the evolution of\nthe NCA.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "29 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.05991v2",
    "published_date": "2024-07-08 14:36:20 UTC",
    "updated_date": "2024-07-19 08:17:44 UTC"
  },
  {
    "arxiv_id": "2407.05983v1",
    "title": "Towards A Comprehensive Visual Saliency Explanation Framework for AI-based Face Recognition Systems",
    "authors": [
      "Yuhang Lu",
      "Zewei Xu",
      "Touradj Ebrahimi"
    ],
    "abstract": "Over recent years, deep convolutional neural networks have significantly\nadvanced the field of face recognition techniques for both verification and\nidentification purposes. Despite the impressive accuracy, these neural networks\nare often criticized for lacking explainability. There is a growing demand for\nunderstanding the decision-making process of AI-based face recognition systems.\nSome studies have investigated the use of visual saliency maps as explanations,\nbut they have predominantly focused on the specific face verification case. The\ndiscussion on more general face recognition scenarios and the corresponding\nevaluation methodology for these explanations have long been absent in current\nresearch. Therefore, this manuscript conceives a comprehensive explanation\nframework for face recognition tasks. Firstly, an exhaustive definition of\nvisual saliency map-based explanations for AI-based face recognition systems is\nprovided, taking into account the two most common recognition situations\nindividually, i.e., face verification and identification. Secondly, a new\nmodel-agnostic explanation method named CorrRISE is proposed to produce\nsaliency maps, which reveal both the similar and dissimilar regions between any\ngiven face images. Subsequently, the explanation framework conceives a new\nevaluation methodology that offers quantitative measurement and comparison of\nthe performance of general visual saliency explanation methods in face\nrecognition. Consequently, extensive experiments are carried out on multiple\nverification and identification scenarios. The results showcase that CorrRISE\ngenerates insightful saliency maps and demonstrates superior performance,\nparticularly in similarity maps in comparison with the state-of-the-art\nexplanation approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2305.08546",
    "pdf_url": "http://arxiv.org/pdf/2407.05983v1",
    "published_date": "2024-07-08 14:25:46 UTC",
    "updated_date": "2024-07-08 14:25:46 UTC"
  },
  {
    "arxiv_id": "2407.05977v1",
    "title": "Exploring Human-LLM Conversations: Mental Models and the Originator of Toxicity",
    "authors": [
      "Johannes Schneider",
      "Arianna Casanova Flores",
      "Anne-Catherine Kranz"
    ],
    "abstract": "This study explores real-world human interactions with large language models\n(LLMs) in diverse, unconstrained settings in contrast to most prior research\nfocusing on ethically trimmed models like ChatGPT for specific tasks. We aim to\nunderstand the originator of toxicity. Our findings show that although LLMs are\nrightfully accused of providing toxic content, it is mostly demanded or at\nleast provoked by humans who actively seek such content. Our manual analysis of\nhundreds of conversations judged as toxic by APIs commercial vendors, also\nraises questions with respect to current practices of what user requests are\nrefused to answer. Furthermore, we conjecture based on multiple empirical\nindicators that humans exhibit a change of their mental model, switching from\nthe mindset of interacting with a machine more towards interacting with a\nhuman.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05977v1",
    "published_date": "2024-07-08 14:20:05 UTC",
    "updated_date": "2024-07-08 14:20:05 UTC"
  },
  {
    "arxiv_id": "2407.05976v2",
    "title": "Change-Point Detection in Industrial Data Streams based on Online Dynamic Mode Decomposition with Control",
    "authors": [
      "Marek Wadinger",
      "Michal Kvasnica",
      "Yoshinobu Kawahara"
    ],
    "abstract": "We propose a novel change-point detection method based on online Dynamic Mode\nDecomposition with control (ODMDwC). Leveraging ODMDwC's ability to find and\ntrack linear approximation of a non-linear system while incorporating control\neffects, the proposed method dynamically adapts to its changing behavior due to\naging and seasonality. This approach enables the detection of changes in\nspatial, temporal, and spectral patterns, providing a robust solution that\npreserves correspondence between the score and the extent of change in the\nsystem dynamics. We formulate a truncated version of ODMDwC and utilize\nhigher-order time-delay embeddings to mitigate noise and extract broad-band\nfeatures. Our method addresses the challenges faced in industrial settings\nwhere safety-critical systems generate non-uniform data streams while requiring\ntimely and accurate change-point detection to protect profit and life. Our\nresults demonstrate that this method yields intuitive and improved detection\nresults compared to the Singular-Value-Decomposition-based method. We validate\nour approach using synthetic and real-world data, showing its competitiveness\nto other approaches on complex systems' benchmark datasets. Provided guidelines\nfor hyperparameters selection enhance our method's practical applicability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "preprint under review in Elsevier",
    "pdf_url": "http://arxiv.org/pdf/2407.05976v2",
    "published_date": "2024-07-08 14:18:33 UTC",
    "updated_date": "2024-08-19 06:20:41 UTC"
  },
  {
    "arxiv_id": "2407.05975v2",
    "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
    "authors": [
      "Yinquan Lu",
      "Wenhao Zhu",
      "Lei Li",
      "Yu Qiao",
      "Fei Yuan"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we conduct extensive multilingual continual pre-training on the\nLLaMA series models, enabling translation support across more than 100\nlanguages. Through a comprehensive analysis of training strategies, such as\nvocabulary expansion and data augmentation, we develop LLaMAX. Remarkably,\nwithout sacrificing its generalization ability, LLaMAX achieves significantly\nhigher translation performance compared to existing open-source LLMs (by more\nthan 10 spBLEU points) and performs on-par with specialized translation model\n(M2M-100-12B) on the Flores-101 benchmark. Extensive experiments indicate that\nLLaMAX can serve as a robust multilingual foundation model. The code\n\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and the models\n\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2407.05975v2",
    "published_date": "2024-07-08 14:18:28 UTC",
    "updated_date": "2024-10-12 03:20:44 UTC"
  },
  {
    "arxiv_id": "2407.05965v3",
    "title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models",
    "authors": [
      "Yibo Miao",
      "Yifan Zhu",
      "Yinpeng Dong",
      "Lijia Yu",
      "Jun Zhu",
      "Xiao-Shan Gao"
    ],
    "abstract": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05965v3",
    "published_date": "2024-07-08 14:04:58 UTC",
    "updated_date": "2024-09-08 16:19:53 UTC"
  },
  {
    "arxiv_id": "2407.05963v2",
    "title": "6GSoft: Software for Edge-to-Cloud Continuum",
    "authors": [
      "Muhammad Azeem Akbar",
      "Matteo Esposito",
      "Sami Hyrynsalmi",
      "Karthikeyan Dinesh Kumar",
      "Valentina Lenarduzzi",
      "Xiaozhou Li",
      "Ali Mehraj",
      "Tommi Mikkonen",
      "Sergio Moreschini",
      "Niko Mäkitalo",
      "Markku Oivo",
      "Anna-Sofia Paavonen",
      "Risha Parveen",
      "Kari Smolander",
      "Ruoyu Su",
      "Kari Systä",
      "Davide Taibi",
      "Nan Yang",
      "Zheying Zhang",
      "Muhammad Zohaib"
    ],
    "abstract": "In the era of 6G, developing and managing software requires cutting-edge\nsoftware engineering (SE) theories and practices tailored for such complexity\nacross a vast number of connected edge devices. Our project aims to lead the\ndevelopment of sustainable methods and energy-efficient orchestration models\nspecifically for edge environments, enhancing architectural support driven by\nAI for contemporary edge-to-cloud continuum computing. This initiative seeks to\nposition Finland at the forefront of the 6G landscape, focusing on\nsophisticated edge orchestration and robust software architectures to optimize\nthe performance and scalability of edge networks. Collaborating with leading\nFinnish universities and companies, the project emphasizes deep\nindustry-academia collaboration and international expertise to address critical\nchallenges in edge orchestration and software architecture, aiming to drive\nsignificant advancements in software productivity and market impact.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.NI",
      "cs.SI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05963v2",
    "published_date": "2024-07-08 14:03:17 UTC",
    "updated_date": "2024-07-09 04:12:42 UTC"
  },
  {
    "arxiv_id": "2407.11048v1",
    "title": "Magnitude and Rotation Invariant Detection of Transportation Modes with Missing Data Modalities",
    "authors": [
      "Jeroen Van Der Donckt",
      "Jonas Van Der Donckt",
      "Sofie Van Hoecke"
    ],
    "abstract": "This work presents the solution of the Signal Sleuths team for the 2024 SHL\nrecognition challenge. The challenge involves detecting transportation modes\nusing shuffled, non-overlapping 5-second windows of phone movement data, with\nexactly one of the three available modalities (accelerometer, gyroscope,\nmagnetometer) randomly missing. Data analysis indicated a significant\ndistribution shift between train and validation data, necessitating a magnitude\nand rotation-invariant approach. We utilize traditional machine learning,\nfocusing on robust processing, feature extraction, and rotation-invariant\naggregation. An ablation study showed that relying solely on the frequently\nused signal magnitude vector results in the poorest performance. Conversely,\nour proposed rotation-invariant aggregation demonstrated substantial\nimprovement over using rotation-aware features, while also reducing the feature\nvector length. Moreover, z-normalization proved crucial for creating robust\nspectral features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at HASCA workshop - SHL challenge (UbiComp 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.11048v1",
    "published_date": "2024-07-08 13:42:01 UTC",
    "updated_date": "2024-07-08 13:42:01 UTC"
  },
  {
    "arxiv_id": "2407.05934v1",
    "title": "Graph Anomaly Detection with Noisy Labels by Reinforcement Learning",
    "authors": [
      "Zhu Wang",
      "Shuang Zhou",
      "Junnan Dong",
      "Chang Yang",
      "Xiao Huang",
      "Shengjie Zhao"
    ],
    "abstract": "Graph anomaly detection (GAD) has been widely applied in many areas, e.g.,\nfraud detection in finance and robot accounts in social networks. Existing\nmethods are dedicated to identifying the outlier nodes that deviate from normal\nones. While they heavily rely on high-quality annotation, which is hard to\nobtain in real-world scenarios, this could lead to severely degraded\nperformance based on noisy labels. Thus, we are motivated to cut the edges of\nsuspicious nodes to alleviate the impact of noise. However, it remains\ndifficult to precisely identify the nodes with noisy labels. Moreover, it is\nhard to quantitatively evaluate the regret of cutting the edges, which may have\neither positive or negative influences. To this end, we propose a novel\nframework REGAD, i.e., REinforced Graph Anomaly Detector. Specifically, we aim\nto maximize the performance improvement (AUC) of a base detector by cutting\nnoisy edges approximated through the nodes with high-confidence labels. (i) We\ndesign a tailored action and search space to train a policy network to\ncarefully prune edges step by step, where only a few suspicious edges are\nprioritized in each step. (ii) We design a policy-in-the-loop mechanism to\niteratively optimize the policy based on the feedback from base detector. The\noverall performance is evaluated by the cumulative rewards. Extensive\nexperiments are conducted on three datasets under different anomaly ratios. The\nresults indicate the superior performance of our proposed REGAD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05934v1",
    "published_date": "2024-07-08 13:41:21 UTC",
    "updated_date": "2024-07-08 13:41:21 UTC"
  },
  {
    "arxiv_id": "2407.12053v1",
    "title": "Improving AlphaFlow for Efficient Protein Ensembles Generation",
    "authors": [
      "Shaoning Li",
      "Mingyu Li",
      "Yusong Wang",
      "Xinheng He",
      "Nanning Zheng",
      "Jian Zhang",
      "Pheng-Ann Heng"
    ],
    "abstract": "Investigating conformational landscapes of proteins is a crucial way to\nunderstand their biological functions and properties. AlphaFlow stands out as a\nsequence-conditioned generative model that introduces flexibility into\nstructure prediction models by fine-tuning AlphaFold under the flow-matching\nframework. Despite the advantages of efficient sampling afforded by\nflow-matching, AlphaFlow still requires multiple runs of AlphaFold to finally\ngenerate one single conformation. Due to the heavy consumption of AlphaFold,\nits applicability is limited in sampling larger set of protein ensembles or the\nlonger chains within a constrained timeframe. In this work, we propose a\nfeature-conditioned generative model called AlphaFlow-Lit to realize efficient\nprotein ensembles generation. In contrast to the full fine-tuning on the entire\nstructure, we focus solely on the light-weight structure module to reconstruct\nthe conformation. AlphaFlow-Lit performs on-par with AlphaFlow and surpasses\nits distilled version without pretraining, all while achieving a significant\nsampling acceleration of around 47 times. The advancement in efficiency\nshowcases the potential of AlphaFlow-Lit in enabling faster and more scalable\ngeneration of protein ensembles.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024 AI4Science workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.12053v1",
    "published_date": "2024-07-08 13:36:43 UTC",
    "updated_date": "2024-07-08 13:36:43 UTC"
  },
  {
    "arxiv_id": "2407.05925v1",
    "title": "Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using LLMs with Human in the Loop",
    "authors": [
      "Anum Afzal",
      "Alexander Kowsik",
      "Rajna Fani",
      "Florian Matthes"
    ],
    "abstract": "Large Language Models have found application in various mundane and\nrepetitive tasks including Human Resource (HR) support. We worked with the\ndomain experts of SAP SE to develop an HR support chatbot as an efficient and\neffective tool for addressing employee inquiries. We inserted a\nhuman-in-the-loop in various parts of the development cycles such as dataset\ncollection, prompt optimization, and evaluation of generated output. By\nenhancing the LLM-driven chatbot's response quality and exploring alternative\nretrieval methods, we have created an efficient, scalable, and flexible tool\nfor HR professionals to address employee inquiries effectively. Our experiments\nand evaluation conclude that GPT-4 outperforms other models and can overcome\ninconsistencies in data through internal reasoning capabilities. Additionally,\nthrough expert analysis, we infer that reference-free evaluation metrics such\nas G-Eval and Prometheus demonstrate reliability closely aligned with that of\nhuman evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05925v1",
    "published_date": "2024-07-08 13:32:14 UTC",
    "updated_date": "2024-07-08 13:32:14 UTC"
  },
  {
    "arxiv_id": "2407.05921v2",
    "title": "TAPVid-3D: A Benchmark for Tracking Any Point in 3D",
    "authors": [
      "Skanda Koppula",
      "Ignacio Rocco",
      "Yi Yang",
      "Joe Heyward",
      "João Carreira",
      "Andrew Zisserman",
      "Gabriel Brostow",
      "Carl Doersch"
    ],
    "abstract": "We introduce a new benchmark, TAPVid-3D, for evaluating the task of\nlong-range Tracking Any Point in 3D (TAP-3D). While point tracking in two\ndimensions (TAP) has many benchmarks measuring performance on real-world\nvideos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To\nthis end, leveraging existing footage, we build a new benchmark for 3D point\ntracking featuring 4,000+ real-world videos, composed of three different data\nsources spanning a variety of object types, motion patterns, and indoor and\noutdoor environments. To measure performance on the TAP-3D task, we formulate a\ncollection of metrics that extend the Jaccard-based metric used in TAP to\nhandle the complexities of ambiguous depth scales across models, occlusions,\nand multi-track spatio-temporal smoothness. We manually verify a large sample\nof trajectories to ensure correct video annotations, and assess the current\nstate of the TAP-3D task by constructing competitive baselines using existing\ntracking models. We anticipate this benchmark will serve as a guidepost to\nimprove our ability to understand precise 3D motion and surface deformation\nfrom monocular video. Code for dataset download, generation, and model\nevaluation is available at https://tapvid3d.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05921v2",
    "published_date": "2024-07-08 13:28:47 UTC",
    "updated_date": "2024-08-27 17:14:16 UTC"
  },
  {
    "arxiv_id": "2407.05910v3",
    "title": "Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding",
    "authors": [
      "Aaron Lohner",
      "Francesco Compagno",
      "Jonathan Francis",
      "Alessandro Oltramari"
    ],
    "abstract": "Recognizing a traffic accident is an essential part of any autonomous driving\nor road monitoring system. An accident can appear in a wide variety of forms,\nand understanding what type of accident is taking place may be useful to\nprevent it from recurring. This work focuses on classifying traffic scenes into\nspecific accident types. We approach the problem by representing a traffic\nscene as a graph, where objects such as cars can be represented as nodes, and\nrelative distances and directions between them as edges. This representation of\na traffic scene is referred to as a scene graph, and can be used as input for\nan accident classifier. Better results are obtained with a classifier that\nfuses the scene graph input with visual and textual representations. This work\nintroduces a multi-stage, multimodal pipeline that pre-processes videos of\ntraffic accidents, encodes them as scene graphs, and aligns this representation\nwith vision and language modalities before executing the classification task.\nWhen trained on 4 classes, our method achieves a balanced accuracy score of\n57.77% on an (unbalanced) subset of the popular Detection of Traffic Anomaly\n(DoTA) benchmark, representing an increase of close to 5 percentage points from\nthe case where scene graph information is not taken into account.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Won the 'Best Paper Runner-up Award' at the 2024 IEEE International\n  Automated Vehicle Validation Conference (IAVVC 2024). Also accepted at the\n  1st Workshop on Semantic Reasoning and Goal Understanding in Robotics, at the\n  Robotics Science and Systems Conference (RSS SemRob 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.05910v3",
    "published_date": "2024-07-08 13:15:11 UTC",
    "updated_date": "2025-01-08 23:40:38 UTC"
  },
  {
    "arxiv_id": "2407.06245v2",
    "title": "ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open Radio Access Networks",
    "authors": [
      "Pranshav Gajjar",
      "Vijay K. Shah"
    ],
    "abstract": "Large Language Models (LLMs) can revolutionize how we deploy and operate Open\nRadio Access Networks (O-RAN) by enhancing network analytics, anomaly\ndetection, and code generation and significantly increasing the efficiency and\nreliability of a plethora of O-RAN tasks. In this paper, we present\nORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the\nperformance of Large Language Models (LLMs) within the context of O-RAN. Our\nbenchmark consists of 13,952 meticulously curated multiple-choice questions\ngenerated from 116 O-RAN specification documents. We leverage a novel\nthree-stage LLM framework, and the questions are categorized into three\ndistinct difficulties to cover a wide spectrum of ORAN-related knowledge. We\nthoroughly evaluate the performance of several state-of-the-art LLMs, including\nGemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a\nRetrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior\nperformance on ORAN-Bench-13K compared to other tested closed-source models.\nOur findings indicate that current popular LLM models are not proficient in\nO-RAN, highlighting the need for specialized models. We observed a noticeable\nperformance improvement when incorporating the RAG-based ORANSight pipeline,\nwith a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on\naverage 21.55% and 22.59% better than the other tested LLMs.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06245v2",
    "published_date": "2024-07-08 13:07:50 UTC",
    "updated_date": "2024-07-13 22:48:44 UTC"
  },
  {
    "arxiv_id": "2407.05898v1",
    "title": "Contrastive Learning of Preferences with a Contextual InfoNCE Loss",
    "authors": [
      "Timo Bertram",
      "Johannes Fürnkranz",
      "Martin Müller"
    ],
    "abstract": "A common problem in contextual preference ranking is that a single preferred\naction is compared against several choices, thereby blowing up the complexity\nand skewing the preference distribution. In this work, we show how one can\nsolve this problem via a suitable adaptation of the CLIP framework.This\nadaptation is not entirely straight-forward, because although the InfoNCE loss\nused by CLIP has achieved great success in computer vision and multi-modal\ndomains, its batch-construction technique requires the ability to compare\narbitrary items, and is not well-defined if one item has multiple positive\nassociations in the same batch. We empirically demonstrate the utility of our\nadapted version of the InfoNCE loss in the domain of collectable card games,\nwhere we aim to learn an embedding space that captures the associations between\nsingle cards and whole card pools based on human selections. Such selection\ndata only exists for restricted choices, thus generating concrete preferences\nof one item over a set of other items rather than a perfect fit between the\ncard and the pool.\n  Our results show that vanilla CLIP does not perform well due to the\naforementioned intuitive issues. However, by adapting CLIP to the problem, we\nreceive a model outperforming previous work trained with the triplet loss,\nwhile also alleviating problems associated with mining triplets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05898v1",
    "published_date": "2024-07-08 13:05:08 UTC",
    "updated_date": "2024-07-08 13:05:08 UTC"
  },
  {
    "arxiv_id": "2407.05892v2",
    "title": "An efficient method to automate tooth identification and 3D bounding box extraction from Cone Beam CT Images",
    "authors": [
      "Ignacio Garrido Botella",
      "Ignacio Arranz Águeda",
      "Juan Carlos Armenteros Carmona",
      "Oleg Vorontsov",
      "Fernando Bayón Robledo",
      "Evgeny Solovykh",
      "Obrubov Aleksandr Andreevich",
      "Adrián Alonso Barriuso"
    ],
    "abstract": "Accurate identification, localization, and segregation of teeth from Cone\nBeam Computed Tomography (CBCT) images are essential for analyzing dental\npathologies. Modeling an individual tooth can be challenging and intricate to\naccomplish, especially when fillings and other restorations introduce\nartifacts. This paper proposes a method for automatically detecting,\nidentifying, and extracting teeth from CBCT images. Our approach involves\ndividing the three-dimensional images into axial slices for image detection.\nTeeth are pinpointed and labeled using a single-stage object detector.\nSubsequently, bounding boxes are delineated and identified to create\nthree-dimensional representations of each tooth. The proposed solution has been\nsuccessfully integrated into the dental analysis tool Dentomo.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "7 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.05892v2",
    "published_date": "2024-07-08 12:59:28 UTC",
    "updated_date": "2024-07-10 09:44:17 UTC"
  },
  {
    "arxiv_id": "2407.05887v1",
    "title": "Generation and De-Identification of Indian Clinical Discharge Summaries using LLMs",
    "authors": [
      "Sanjeet Singh",
      "Shreya Gupta",
      "Niralee Gupta",
      "Naimish Sharma",
      "Lokesh Srivastava",
      "Vibhu Agarwal",
      "Ashutosh Modi"
    ],
    "abstract": "The consequences of a healthcare data breach can be devastating for the\npatients, providers, and payers. The average financial impact of a data breach\nin recent months has been estimated to be close to USD 10 million. This is\nespecially significant for healthcare organizations in India that are managing\nrapid digitization while still establishing data governance procedures that\nalign with the letter and spirit of the law. Computer-based systems for\nde-identification of personal information are vulnerable to data drift, often\nrendering them ineffective in cross-institution settings. Therefore, a rigorous\nassessment of existing de-identification against local health datasets is\nimperative to support the safe adoption of digital health initiatives in India.\nUsing a small set of de-identified patient discharge summaries provided by an\nIndian healthcare institution, in this paper, we report the nominal performance\nof de-identification algorithms (based on language models) trained on publicly\navailable non-Indian datasets, pointing towards a lack of cross-institutional\ngeneralization. Similarly, experimentation with off-the-shelf de-identification\nsystems reveals potential risks associated with the approach. To overcome data\nscarcity, we explore generating synthetic clinical reports (using publicly\navailable and Indian summaries) by performing in-context learning over Large\nLanguage Models (LLMs). Our experiments demonstrate the use of generated\nreports as an effective strategy for creating high-performing de-identification\nsystems with good generalization capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at BioNLP Workshop at ACL 2024; 21 pages (9 pages main\n  content)",
    "pdf_url": "http://arxiv.org/pdf/2407.05887v1",
    "published_date": "2024-07-08 12:47:03 UTC",
    "updated_date": "2024-07-08 12:47:03 UTC"
  },
  {
    "arxiv_id": "2407.05884v1",
    "title": "One system for learning and remembering episodes and rules",
    "authors": [
      "Joshua T. S. Hewson",
      "Sabina J. Sloman",
      "Marina Dubova"
    ],
    "abstract": "Humans can learn individual episodes and generalizable rules and also\nsuccessfully retain both kinds of acquired knowledge over time. In the\ncognitive science literature, (1) learning individual episodes and rules and\n(2) learning and remembering are often both conceptualized as competing\nprocesses that necessitate separate, complementary learning systems. Inspired\nby recent research in statistical learning, we challenge these trade-offs,\nhypothesizing that they arise from capacity limitations rather than from the\ninherent incompatibility of the underlying cognitive processes. Using an\nassociative learning task, we show that one system with excess representational\ncapacity can learn and remember both episodes and rules.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05884v1",
    "published_date": "2024-07-08 12:44:18 UTC",
    "updated_date": "2024-07-08 12:44:18 UTC"
  },
  {
    "arxiv_id": "2407.05879v1",
    "title": "Learning With Generalised Card Representations for \"Magic: The Gathering\"",
    "authors": [
      "Timo Bertram",
      "Johannes Fürnkranz",
      "Martin Müller"
    ],
    "abstract": "A defining feature of collectable card games is the deck building process\nprior to actual gameplay, in which players form their decks according to some\nrestrictions. Learning to build decks is difficult for players and models alike\ndue to the large card variety and highly complex semantics, as well as\nrequiring meaningful card and deck representations when aiming to utilise AI.\nIn addition, regular releases of new card sets lead to unforeseeable\nfluctuations in the available card pool, thus affecting possible deck\nconfigurations and requiring continuous updates. Previous Game AI approaches to\nbuilding decks have often been limited to fixed sets of possible cards, which\ngreatly limits their utility in practice. In this work, we explore possible\ncard representations that generalise to unseen cards, thus greatly extending\nthe real-world utility of AI-based deck building for the game \"Magic: The\nGathering\".We study such representations based on numerical, nominal, and\ntext-based features of cards, card images, and meta information about card\nusage from third-party services. Our results show that while the particular\nchoice of generalised input representation has little effect on learning to\npredict human card selections among known cards, the performance on new, unseen\ncards can be greatly improved. Our generalised model is able to predict 55\\% of\nhuman choices on completely unseen cards, thus showing a deep understanding of\ncard quality and strategy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Best paper award nominee at IEEE Conference on Games 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05879v1",
    "published_date": "2024-07-08 12:42:44 UTC",
    "updated_date": "2024-07-08 12:42:44 UTC"
  },
  {
    "arxiv_id": "2407.05876v1",
    "title": "Efficiently Training Neural Networks for Imperfect Information Games by Sampling Information Sets",
    "authors": [
      "Timo Bertram",
      "Johannes Fürnkranz",
      "Martin Müller"
    ],
    "abstract": "In imperfect information games, the evaluation of a game state not only\ndepends on the observable world but also relies on hidden parts of the\nenvironment. As accessing the obstructed information trivialises state\nevaluations, one approach to tackle such problems is to estimate the value of\nthe imperfect state as a combination of all states in the information set,\ni.e., all possible states that are consistent with the current imperfect\ninformation. In this work, the goal is to learn a function that maps from the\nimperfect game information state to its expected value. However, constructing a\nperfect training set, i.e. an enumeration of the whole information set for\nnumerous imperfect states, is often infeasible. To compute the expected values\nfor an imperfect information game like \\textit{Reconnaissance Blind Chess}, one\nwould need to evaluate thousands of chess positions just to obtain the training\ntarget for a single state. Still, the expected value of a state can already be\napproximated with appropriate accuracy from a much smaller set of evaluations.\nThus, in this paper, we empirically investigate how a budget of perfect\ninformation game evaluations should be distributed among training samples to\nmaximise the return. Our results show that sampling a small number of states,\nin our experiments roughly 3, for a larger number of separate positions is\npreferable over repeatedly sampling a smaller quantity of states. Thus, we find\nthat in our case, the quantity of different samples seems to be more important\nthan higher target quality.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "KI 2024 - 47th German Conference on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2407.05876v1",
    "published_date": "2024-07-08 12:37:07 UTC",
    "updated_date": "2024-07-08 12:37:07 UTC"
  },
  {
    "arxiv_id": "2407.11046v4",
    "title": "A Survey on LoRA of Large Language Models",
    "authors": [
      "Yuren Mao",
      "Yuhang Ge",
      "Yijiang Fan",
      "Wenyi Xu",
      "Yu Mi",
      "Zhonghao Hu",
      "Yunjun Gao"
    ],
    "abstract": "Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github\npage~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}\nfor readers to check the updates and initiate discussions on this survey paper.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40663-9}",
    "pdf_url": "http://arxiv.org/pdf/2407.11046v4",
    "published_date": "2024-07-08 12:32:10 UTC",
    "updated_date": "2024-10-24 03:30:46 UTC"
  },
  {
    "arxiv_id": "2407.05869v2",
    "title": "PORCA: Root Cause Analysis with Partially Observed Data",
    "authors": [
      "Chang Gong",
      "Di Yao",
      "Jin Wang",
      "Wenbin Li",
      "Lanting Fang",
      "Yongtao Xie",
      "Kaiyu Feng",
      "Peng Han",
      "Jingping Bi"
    ],
    "abstract": "Root Cause Analysis (RCA) aims at identifying the underlying causes of system\nfaults by uncovering and analyzing the causal structure from complex systems.\nIt has been widely used in many application domains. Reliable diagnostic\nconclusions are of great importance in mitigating system failures and financial\nlosses. However, previous studies implicitly assume a full observation of the\nsystem, which neglect the effect of partial observation (i.e., missing nodes\nand latent malfunction). As a result, they fail in deriving reliable RCA\nresults. In this paper, we unveil the issues of unobserved confounders and\nheterogeneity in partial observation and come up with a new problem of root\ncause analysis with partially observed data. To achieve this, we propose PORCA,\na novel RCA framework which can explore reliable root causes under both\nunobserved confounders and unobserved heterogeneity. PORCA leverages magnified\nscore-based causal discovery to efficiently optimize acyclic directed mixed\ngraph under unobserved confounders. In addition, we also develop a\nheterogeneity-aware scheduling strategy to provide adaptive sample weights.\nExtensive experimental results on one synthetic and two real-world datasets\ndemonstrate the effectiveness and superiority of the proposed framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05869v2",
    "published_date": "2024-07-08 12:31:12 UTC",
    "updated_date": "2024-07-12 01:28:49 UTC"
  },
  {
    "arxiv_id": "2407.05868v2",
    "title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions",
    "authors": [
      "Yanxu Zhu",
      "Jinlin Xiao",
      "Yuhang Wang",
      "Jitao Sang"
    ],
    "abstract": "Recent studies have demonstrated that large language models (LLMs) are\nsusceptible to being misled by false premise questions (FPQs), leading to\nerrors in factual knowledge, know as factuality hallucination. Existing\nbenchmarks that assess this vulnerability primarily rely on manual\nconstruction, resulting in limited scale and lack of scalability. In this work,\nwe introduce an automated, scalable pipeline to create FPQs based on knowledge\ngraphs (KGs). The first step is modifying true triplets extracted from KGs to\ncreate false premises. Subsequently, utilizing the state-of-the-art\ncapabilities of GPTs, we generate semantically rich FPQs. Based on the proposed\nmethod, we present a comprehensive benchmark, the Knowledge Graph-based False\nPremise Questions (KG-FPQ), which contains approximately 178k FPQs across three\nknowledge domains, at six levels of confusability, and in two task formats.\nUsing KG-FPQ, we conduct extensive evaluations on several representative LLMs\nand provide valuable insights. The KG-FPQ dataset and code are available\nat~https://github.com/yanxuzhu/KG-FPQ.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING2025 main",
    "pdf_url": "http://arxiv.org/pdf/2407.05868v2",
    "published_date": "2024-07-08 12:31:03 UTC",
    "updated_date": "2024-12-22 07:28:15 UTC"
  },
  {
    "arxiv_id": "2407.05864v1",
    "title": "Neural Network-based Information Set Weighting for Playing Reconnaissance Blind Chess",
    "authors": [
      "Timo Bertram",
      "Johannes Fürnkranz",
      "Martin Müller"
    ],
    "abstract": "In imperfect information games, the game state is generally not fully\nobservable to players. Therefore, good gameplay requires policies that deal\nwith the different information that is hidden from each player. To combat this,\neffective algorithms often reason about information sets; the sets of all\npossible game states that are consistent with a player's observations. While\nthere is no way to distinguish between the states within an information set,\nthis property does not imply that all states are equally likely to occur in\nplay. We extend previous research on assigning weights to the states in an\ninformation set in order to facilitate better gameplay in the imperfect\ninformation game of Reconnaissance Blind Chess. For this, we train two\ndifferent neural networks which estimate the likelihood of each state in an\ninformation set from historical game data. Experimentally, we find that a\nSiamese neural network is able to achieve higher accuracy and is more efficient\nthan a classical convolutional neural network for the given domain. Finally, we\nevaluate an RBC-playing agent that is based on the generated weightings and\ncompare different parameter settings that influence how strongly it should rely\non them. The resulting best player is ranked 5th on the public leaderboard.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of IEEE Conference on Games 2023 paper",
    "pdf_url": "http://arxiv.org/pdf/2407.05864v1",
    "published_date": "2024-07-08 12:29:29 UTC",
    "updated_date": "2024-07-08 12:29:29 UTC"
  },
  {
    "arxiv_id": "2407.05858v2",
    "title": "Fast On-device LLM Inference with NPUs",
    "authors": [
      "Daliang Xu",
      "Hao Zhang",
      "Liming Yang",
      "Ruiqi Liu",
      "Gang Huang",
      "Mengwei Xu",
      "Xuanzhe Liu"
    ],
    "abstract": "On-device inference for Large Language Models (LLMs), driven by increasing\nprivacy concerns and advancements of mobile-sized models, has gained\nsignificant interest. However, even mobile-sized LLMs (e.g., Gemma-2B)\nencounter unacceptably high inference latency, often bottlenecked by the\nprefill stage in tasks like screen UI understanding.\n  We present llm.npu, the first LLM inference system utilizing on-device Neural\nProcessing Unit (NPU) offloading to reduce prefill latency. llm.npu enhances\nNPU offloading efficiency by re-constructing the prompt and model in three\nlevels: (1) At prompt level, it divides variable-length prompts into multiple\nfixed-sized chunks while maintaining data dependencies; (2) At tensor level, it\nidentifies and extracts significant outliers to run on the CPU/GPU in parallel\nwith minimal overhead; (3) At block level, it schedules Transformer blocks in\nan out-of-order manner to the CPU/GPU and NPU based on their hardware affinity\nand sensitivity to accuracy. Compared to competitive baselines, llm.npu\nachieves 22.4x faster prefill speed and 30.7$\\times$ energy savings on average,\nand up to 32.8x speedup in an end-to-end real-world application. For the first\ntime, llm.npu achieves more than 1,000 tokens/sec prefilling for a\nbillion-sized model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05858v2",
    "published_date": "2024-07-08 12:20:45 UTC",
    "updated_date": "2024-12-15 15:26:41 UTC"
  },
  {
    "arxiv_id": "2407.05817v1",
    "title": "Cyber Physical Games",
    "authors": [
      "Warisa Sritriratanarak",
      "Paulo Garcia"
    ],
    "abstract": "We describe a formulation of multi-agents operating within a Cyber-Physical\nSystem, resulting in collaborative or adversarial games. We show that the\nnon-determinism inherent in the communication medium between agents and the\nunderlying physical environment gives rise to environment evolution that is a\nprobabilistic function of agents' strategies. We name these emergent properties\nCyber Physical Games and study its properties. We present an algorithmic model\nthat determines the most likely system evolution, approximating Cyber Physical\nGames through Probabilistic Finite State Automata, and evaluate it on\ncollaborative and adversarial versions of the Iterated Boolean Game, comparing\ntheoretical results with simulated ones. Results support the validity of the\nproposed model, and suggest several required research directions to continue\nevolving our understanding of Cyber Physical System, as well as how to best\ndesign agents that must operate within such environments.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05817v1",
    "published_date": "2024-07-08 10:54:14 UTC",
    "updated_date": "2024-07-08 10:54:14 UTC"
  },
  {
    "arxiv_id": "2407.05816v1",
    "title": "Graph Reasoning Networks",
    "authors": [
      "Markus Zopf",
      "Francesco Alesiani"
    ],
    "abstract": "Graph neural networks (GNNs) are the predominant approach for graph-based\nmachine learning. While neural networks have shown great performance at\nlearning useful representations, they are often criticized for their limited\nhigh-level reasoning abilities. In this work, we present Graph Reasoning\nNetworks (GRNs), a novel approach to combine the strengths of fixed and learned\ngraph representations and a reasoning module based on a differentiable\nsatisfiability solver. While results on real-world datasets show comparable\nperformance to GNN, experiments on synthetic datasets demonstrate the potential\nof the newly proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the workshop on graphs and more complex structures for\n  learning and reasoning at AAAI 2022",
    "pdf_url": "http://arxiv.org/pdf/2407.05816v1",
    "published_date": "2024-07-08 10:53:49 UTC",
    "updated_date": "2024-07-08 10:53:49 UTC"
  },
  {
    "arxiv_id": "2407.05814v1",
    "title": "Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign Recognition",
    "authors": [
      "Yaozong Gan",
      "Guang Li",
      "Ren Togo",
      "Keisuke Maeda",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "Recent multimodal large language models (MLLM) such as GPT-4o and GPT-4v have\nshown great potential in autonomous driving. In this paper, we propose a\ncross-domain few-shot in-context learning method based on the MLLM for\nenhancing traffic sign recognition (TSR). We first construct a traffic sign\ndetection network based on Vision Transformer Adapter and an extraction module\nto extract traffic signs from the original road images. To reduce the\ndependence on training data and improve the performance stability of\ncross-country TSR, we introduce a cross-domain few-shot in-context learning\nmethod based on the MLLM. To enhance MLLM's fine-grained recognition ability of\ntraffic signs, the proposed method generates corresponding description texts\nusing template traffic signs. These description texts contain key information\nabout the shape, color, and composition of traffic signs, which can stimulate\nthe ability of MLLM to perceive fine-grained traffic sign categories. By using\nthe description texts, our method reduces the cross-domain differences between\ntemplate and real traffic signs. Our approach requires only simple and uniform\ntextual indications, without the need for large-scale traffic sign images and\nlabels. We perform comprehensive evaluations on the German traffic sign\nrecognition benchmark dataset, the Belgium traffic sign dataset, and two\nreal-world datasets taken from Japan. The experimental results show that our\nmethod significantly enhances the TSR performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05814v1",
    "published_date": "2024-07-08 10:51:03 UTC",
    "updated_date": "2024-07-08 10:51:03 UTC"
  },
  {
    "arxiv_id": "2407.05810v1",
    "title": "Integrating AI in College Education: Positive yet Mixed Experiences with ChatGPT",
    "authors": [
      "Xinrui Song",
      "Jiajin Zhang",
      "Pingkun Yan",
      "Juergen Hahn",
      "Uwe Kruger",
      "Hisham Mohamed",
      "Ge Wang"
    ],
    "abstract": "The integration of artificial intelligence (AI) chatbots into higher\neducation marks a shift towards a new generation of pedagogical tools,\nmirroring the arrival of milestones like the internet. With the launch of\nChatGPT-4 Turbo in November 2023, we developed a ChatGPT-based teaching\napplication (https://chat.openai.com/g/g-1imx1py4K-chatge-medical-imaging) and\nintegrated it into our undergraduate medical imaging course in the Spring 2024\nsemester. This study investigates the use of ChatGPT throughout a semester-long\ntrial, providing insights into students' engagement, perception, and the\noverall educational effectiveness of the technology. We systematically\ncollected and analyzed data concerning students' interaction with ChatGPT,\nfocusing on their attitudes, concerns, and usage patterns. The findings\nindicate that ChatGPT offers significant advantages such as improved\ninformation access and increased interactivity, but its adoption is accompanied\nby concerns about the accuracy of the information provided and the necessity\nfor well-defined guidelines to optimize its use.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05810v1",
    "published_date": "2024-07-08 10:44:34 UTC",
    "updated_date": "2024-07-08 10:44:34 UTC"
  },
  {
    "arxiv_id": "2407.05800v1",
    "title": "FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep Reinforcement Learning for Medical Imaging",
    "authors": [
      "Pranab Sahoo",
      "Ashutosh Tripathi",
      "Sriparna Saha",
      "Samrat Mondal"
    ],
    "abstract": "Despite recent advancements in federated learning (FL) for medical image\ndiagnosis, addressing data heterogeneity among clients remains a significant\nchallenge for practical implementation. A primary hurdle in FL arises from the\nnon-IID nature of data samples across clients, which typically results in a\ndecline in the performance of the aggregated global model. In this study, we\nintroduce FedMRL, a novel federated multi-agent deep reinforcement learning\nframework designed to address data heterogeneity. FedMRL incorporates a novel\nloss function to facilitate fairness among clients, preventing bias in the\nfinal global model. Additionally, it employs a multi-agent reinforcement\nlearning (MARL) approach to calculate the proximal term $(\\mu)$ for the\npersonalized local objective function, ensuring convergence to the global\noptimum. Furthermore, FedMRL integrates an adaptive weight adjustment method\nusing a Self-organizing map (SOM) on the server side to counteract distribution\nshifts among clients' local data distributions. We assess our approach using\ntwo publicly available real-world medical datasets, and the results demonstrate\nthat FedMRL significantly outperforms state-of-the-art techniques, showing its\nefficacy in addressing data heterogeneity in federated learning. The code can\nbe found here~{\\url{https://github.com/Pranabiitp/FedMRL}}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05800v1",
    "published_date": "2024-07-08 10:10:07 UTC",
    "updated_date": "2024-07-08 10:10:07 UTC"
  },
  {
    "arxiv_id": "2407.05789v2",
    "title": "CANDID DAC: Leveraging Coupled Action Dimensions with Importance Differences in DAC",
    "authors": [
      "Philipp Bordne",
      "M. Asif Hasan",
      "Eddie Bergman",
      "Noor Awad",
      "André Biedenkapp"
    ],
    "abstract": "High-dimensional action spaces remain a challenge for dynamic algorithm\nconfiguration (DAC). Interdependencies and varying importance between action\ndimensions are further known key characteristics of DAC problems. We argue that\nthese Coupled Action Dimensions with Importance Differences (CANDID) represent\naspects of the DAC problem that are not yet fully explored. To address this\ngap, we introduce a new white-box benchmark within the DACBench suite that\nsimulates the properties of CANDID. Further, we propose sequential policies as\nan effective strategy for managing these properties. Such policies factorize\nthe action space and mitigate exponential growth by learning a policy per\naction dimension. At the same time, these policies accommodate the\ninterdependence of action dimensions by fostering implicit coordination. We\nshow this in an experimental study of value-based policies on our new\nbenchmark. This study demonstrates that sequential policies significantly\noutperform independent learning of factorized policies in CANDID action spaces.\nIn addition, they overcome the scalability limitations associated with learning\na single policy across all action dimensions. The code used for our experiments\nis available under https://github.com/PhilippBordne/candidDAC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages main paper, 11 pages references and appendix, 9 figures, to\n  be published in: Proceedings of the Third International Conference on\n  Automated Machine Learning (AutoML 2024), Workshop Track",
    "pdf_url": "http://arxiv.org/pdf/2407.05789v2",
    "published_date": "2024-07-08 09:51:02 UTC",
    "updated_date": "2024-09-17 09:32:15 UTC"
  },
  {
    "arxiv_id": "2407.05788v1",
    "title": "Automated Computational Energy Minimization of ML Algorithms using Constrained Bayesian Optimization",
    "authors": [
      "Pallavi Mitra",
      "Felix Biessmann"
    ],
    "abstract": "Bayesian optimization (BO) is an efficient framework for optimization of\nblack-box objectives when function evaluations are costly and gradient\ninformation is not easily accessible. BO has been successfully applied to\nautomate the task of hyperparameter optimization (HPO) in machine learning (ML)\nmodels with the primary objective of optimizing predictive performance on\nheld-out data. In recent years, however, with ever-growing model sizes, the\nenergy cost associated with model training has become an important factor for\nML applications. Here we evaluate Constrained Bayesian Optimization (CBO) with\nthe primary objective of minimizing energy consumption and subject to the\nconstraint that the generalization performance is above some threshold. We\nevaluate our approach on regression and classification tasks and demonstrate\nthat CBO achieves lower energy consumption without compromising the predictive\nperformance of ML models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.05788v1",
    "published_date": "2024-07-08 09:49:38 UTC",
    "updated_date": "2024-07-08 09:49:38 UTC"
  },
  {
    "arxiv_id": "2407.05786v1",
    "title": "Large Language Models for Judicial Entity Extraction: A Comparative Study",
    "authors": [
      "Atin Sakkeer Hussain",
      "Anu Thomas"
    ],
    "abstract": "Domain-specific Entity Recognition holds significant importance in legal\ncontexts, serving as a fundamental task that supports various applications such\nas question-answering systems, text summarization, machine translation,\nsentiment analysis, and information retrieval specifically within case law\ndocuments. Recent advancements have highlighted the efficacy of Large Language\nModels in natural language processing tasks, demonstrating their capability to\naccurately detect and classify domain-specific facts (entities) from\nspecialized texts like clinical and financial documents. This research\ninvestigates the application of Large Language Models in identifying\ndomain-specific entities (e.g., courts, petitioner, judge, lawyer, respondents,\nFIR nos.) within case law documents, with a specific focus on their aptitude\nfor handling domain-specific language complexity and contextual variations. The\nstudy evaluates the performance of state-of-the-art Large Language Model\narchitectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in\nthe context of extracting judicial facts tailored to Indian judicial texts.\nMistral and Gemma emerged as the top-performing models, showcasing balanced\nprecision and recall crucial for accurate entity identification. These findings\nconfirm the value of Large Language Models in judicial documents and\ndemonstrate how they can facilitate and quicken scientific research by\nproducing precise, organised data outputs that are appropriate for in-depth\nexamination.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05786v1",
    "published_date": "2024-07-08 09:49:03 UTC",
    "updated_date": "2024-07-08 09:49:03 UTC"
  },
  {
    "arxiv_id": "2407.05778v1",
    "title": "When is the consistent prediction likely to be a correct prediction?",
    "authors": [
      "Alex Nguyen",
      "Dheeraj Mekala",
      "Chengyu Dong",
      "Jingbo Shang"
    ],
    "abstract": "Self-consistency (Wang et al., 2023) suggests that the most consistent answer\nobtained through large language models (LLMs) is more likely to be correct. In\nthis paper, we challenge this argument and propose a nuanced correction. Our\nobservations indicate that consistent answers derived through more computation\ni.e. longer reasoning texts, rather than simply the most consistent answer\nacross all outputs, are more likely to be correct. This is predominantly\nbecause we demonstrate that LLMs can autonomously produce chain-of-thought\n(CoT) style reasoning with no custom prompts merely while generating longer\nresponses, which lead to consistent predictions that are more accurate. In the\nzero-shot setting, by sampling Mixtral-8x7B model multiple times and\nconsidering longer responses, we achieve 86% of its self-consistency\nperformance obtained through zero-shot CoT prompting on the GSM8K and\nMultiArith datasets. Finally, we demonstrate that the probability of LLMs\ngenerating a longer response is quite low, highlighting the need for decoding\nstrategies conditioned on output length.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05778v1",
    "published_date": "2024-07-08 09:37:27 UTC",
    "updated_date": "2024-07-08 09:37:27 UTC"
  },
  {
    "arxiv_id": "2407.05775v1",
    "title": "Structural Generalization in Autonomous Cyber Incident Response with Message-Passing Neural Networks and Reinforcement Learning",
    "authors": [
      "Jakob Nyberg",
      "Pontus Johnson"
    ],
    "abstract": "We believe that agents for automated incident response based on machine\nlearning need to handle changes in network structure. Computer networks are\ndynamic, and can naturally change in structure over time. Retraining agents for\nsmall network changes costs time and energy. We attempt to address this issue\nwith an existing method of relational agent learning, where the relations\nbetween objects are assumed to remain consistent across problem instances. The\nstate of the computer network is represented as a relational graph and encoded\nthrough a message passing neural network. The message passing neural network\nand an agent policy using the encoding are optimized end-to-end using\nreinforcement learning. We evaluate the approach on the second instance of the\nCyber Autonomy Gym for Experimentation (CAGE~2), a cyber incident simulator\nthat simulates attacks on an enterprise network. We create variants of the\noriginal network with different numbers of hosts and agents are tested without\nadditional training on them. Our results show that agents using relational\ninformation are able to find solutions despite changes to the network, and can\nperform optimally in some instances. Agents using the default vector state\nrepresentation perform better, but need to be specially trained on each network\nvariant, demonstrating a trade-off between specialization and generalization.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to IEEE CSR 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05775v1",
    "published_date": "2024-07-08 09:34:22 UTC",
    "updated_date": "2024-07-08 09:34:22 UTC"
  },
  {
    "arxiv_id": "2407.05766v1",
    "title": "Multi-agent Reinforcement Learning-based Network Intrusion Detection System",
    "authors": [
      "Amine Tellache",
      "Amdjed Mokhtari",
      "Abdelaziz Amara Korba",
      "Yacine Ghamri-Doudane"
    ],
    "abstract": "Intrusion Detection Systems (IDS) play a crucial role in ensuring the\nsecurity of computer networks. Machine learning has emerged as a popular\napproach for intrusion detection due to its ability to analyze and detect\npatterns in large volumes of data. However, current ML-based IDS solutions\noften struggle to keep pace with the ever-changing nature of attack patterns\nand the emergence of new attack types. Additionally, these solutions face\nchallenges related to class imbalance, where the number of instances belonging\nto different classes (normal and intrusions) is significantly imbalanced, which\nhinders their ability to effectively detect minor classes. In this paper, we\npropose a novel multi-agent reinforcement learning (RL) architecture, enabling\nautomatic, efficient, and robust network intrusion detection. To enhance the\ncapabilities of the proposed model, we have improved the DQN algorithm by\nimplementing the weighted mean square loss function and employing\ncost-sensitive learning techniques. Our solution introduces a resilient\narchitecture designed to accommodate the addition of new attacks and\neffectively adapt to changes in existing attack patterns. Experimental results\nrealized using CIC-IDS-2017 dataset, demonstrate that our approach can\neffectively handle the class imbalance problem and provide a fine grained\nclassification of attacks with a very low false positive rate. In comparison to\nthe current state-of-the-art works, our solution demonstrates a significant\nsuperiority in both detection rate and false positive rate.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05766v1",
    "published_date": "2024-07-08 09:18:59 UTC",
    "updated_date": "2024-07-08 09:18:59 UTC"
  },
  {
    "arxiv_id": "2407.05758v1",
    "title": "Potential of Multimodal Large Language Models for Data Mining of Medical Images and Free-text Reports",
    "authors": [
      "Yutong Zhang",
      "Yi Pan",
      "Tianyang Zhong",
      "Peixin Dong",
      "Kangni Xie",
      "Yuxiao Liu",
      "Hanqi Jiang",
      "Zhengliang Liu",
      "Shijie Zhao",
      "Tuo Zhang",
      "Xi Jiang",
      "Dinggang Shen",
      "Tianming Liu",
      "Xin Zhang"
    ],
    "abstract": "Medical images and radiology reports are crucial for diagnosing medical\nconditions, highlighting the importance of quantitative analysis for clinical\ndecision-making. However, the diversity and cross-source heterogeneity of these\ndata challenge the generalizability of current data-mining methods. Multimodal\nlarge language models (MLLMs) have recently transformed many domains,\nsignificantly affecting the medical field. Notably, Gemini-Vision-series\n(Gemini) and GPT-4-series (GPT-4) models have epitomized a paradigm shift in\nArtificial General Intelligence (AGI) for computer vision, showcasing their\npotential in the biomedical domain. In this study, we evaluated the performance\nof the Gemini, GPT-4, and 4 popular large models for an exhaustive evaluation\nacross 14 medical imaging datasets, including 5 medical imaging categories\n(dermatology, radiology, dentistry, ophthalmology, and endoscopy), and 3\nradiology report datasets. The investigated tasks encompass disease\nclassification, lesion segmentation, anatomical localization, disease\ndiagnosis, report generation, and lesion detection. Our experimental results\ndemonstrated that Gemini-series models excelled in report generation and lesion\ndetection but faces challenges in disease classification and anatomical\nlocalization. Conversely, GPT-series models exhibited proficiency in lesion\nsegmentation and anatomical localization but encountered difficulties in\ndisease diagnosis and lesion detection. Additionally, both the Gemini series\nand GPT series contain models that have demonstrated commendable generation\nefficiency. While both models hold promise in reducing physician workload,\nalleviating pressure on limited healthcare resources, and fostering\ncollaboration between clinical practitioners and artificial intelligence\ntechnologies, substantial enhancements and comprehensive validations remain\nimperative before clinical deployment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05758v1",
    "published_date": "2024-07-08 09:08:42 UTC",
    "updated_date": "2024-07-08 09:08:42 UTC"
  },
  {
    "arxiv_id": "2407.05746v1",
    "title": "MSP-Podcast SER Challenge 2024: L'antenne du Ventoux Multimodal Self-Supervised Learning for Speech Emotion Recognition",
    "authors": [
      "Jarod Duret",
      "Mickael Rouvier",
      "Yannick Estève"
    ],
    "abstract": "In this work, we detail our submission to the 2024 edition of the MSP-Podcast\nSpeech Emotion Recognition (SER) Challenge. This challenge is divided into two\ndistinct tasks: Categorical Emotion Recognition and Emotional Attribute\nPrediction. We concentrated our efforts on Task 1, which involves the\ncategorical classification of eight emotional states using data from the\nMSP-Podcast dataset. Our approach employs an ensemble of models, each trained\nindependently and then fused at the score level using a Support Vector Machine\n(SVM) classifier. The models were trained using various strategies, including\nSelf-Supervised Learning (SSL) fine-tuning across different modalities: speech\nalone, text alone, and a combined speech and text approach. This joint training\nmethodology aims to enhance the system's ability to accurately classify\nemotional states. This joint training methodology aims to enhance the system's\nability to accurately classify emotional states. Thus, the system obtained\nF1-macro of 0.35\\% on development set.",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05746v1",
    "published_date": "2024-07-08 08:52:06 UTC",
    "updated_date": "2024-07-08 08:52:06 UTC"
  },
  {
    "arxiv_id": "2407.05739v1",
    "title": "Multi-Bit Mechanism: A Novel Information Transmission Paradigm for Spiking Neural Networks",
    "authors": [
      "Yongjun Xiao",
      "Xianlong Tian",
      "Yongqi Ding",
      "Pei He",
      "Mengmeng Jing",
      "Lin Zuo"
    ],
    "abstract": "Since proposed, spiking neural networks (SNNs) gain recognition for their\nhigh performance, low power consumption and enhanced biological\ninterpretability. However, while bringing these advantages, the binary nature\nof spikes also leads to considerable information loss in SNNs, ultimately\ncausing performance degradation. We claim that the limited expressiveness of\ncurrent binary spikes, resulting in substantial information loss, is the\nfundamental issue behind these challenges. To alleviate this, our research\nintroduces a multi-bit information transmission mechanism for SNNs. This\nmechanism expands the output of spiking neurons from the original single bit to\nmultiple bits, enhancing the expressiveness of the spikes and reducing\ninformation loss during the forward process, while still maintaining the low\nenergy consumption advantage of SNNs. For SNNs, this represents a new paradigm\nof information transmission. Moreover, to further utilize the limited spikes,\nwe extract effective signals from the previous layer to re-stimulate the\nneurons, thus encouraging full spikes emission across various bit levels. We\nconducted extensive experiments with our proposed method using both direct\ntraining method and ANN-SNN conversion method, and the results show consistent\nperformance improvements.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2407.05739v1",
    "published_date": "2024-07-08 08:46:31 UTC",
    "updated_date": "2024-07-08 08:46:31 UTC"
  },
  {
    "arxiv_id": "2407.05736v1",
    "title": "TransMA: an explainable multi-modal deep learning model for predicting properties of ionizable lipid nanoparticles in mRNA delivery",
    "authors": [
      "Kun Wu",
      "Zixu Wang",
      "Xiulong Yang",
      "Yangyang Chen",
      "Zhenqi Han",
      "Jialu Zhang",
      "Lizhuang Liu"
    ],
    "abstract": "As the primary mRNA delivery vehicles, ionizable lipid nanoparticles (LNPs)\nexhibit excellent safety, high transfection efficiency, and strong immune\nresponse induction. However, the screening process for LNPs is time-consuming\nand costly. To expedite the identification of high-transfection-efficiency mRNA\ndrug delivery systems, we propose an explainable LNPs transfection efficiency\nprediction model, called TransMA. TransMA employs a multi-modal molecular\nstructure fusion architecture, wherein the fine-grained atomic spatial\nrelationship extractor named molecule 3D Transformer captures three-dimensional\nspatial features of the molecule, and the coarse-grained atomic sequence\nextractor named molecule Mamba captures one-dimensional molecular features. We\ndesign the mol-attention mechanism block, enabling it to align coarse and\nfine-grained atomic features and captures relationships between atomic spatial\nand sequential structures. TransMA achieves state-of-the-art performance in\npredicting transfection efficiency using the scaffold and cliff data splitting\nmethods on the current largest LNPs dataset, including Hela and RAW cell lines.\nMoreover, we find that TransMA captures the relationship between subtle\nstructural changes and significant transfection efficiency variations,\nproviding valuable insights for LNPs design. Additionally, TransMA's\npredictions on external transfection efficiency data maintain a consistent\norder with actual transfection efficiencies, demonstrating its robust\ngeneralization capability. The code, model and data are made publicly available\nat https://github.com/wklix/TransMA/tree/master. We hope that high-accuracy\ntransfection prediction models in the future can aid in LNPs design and initial\nscreening, thereby assisting in accelerating the mRNA design process.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.05736v1",
    "published_date": "2024-07-08 08:43:32 UTC",
    "updated_date": "2024-07-08 08:43:32 UTC"
  },
  {
    "arxiv_id": "2407.05732v1",
    "title": "FairPFN: Transformers Can do Counterfactual Fairness",
    "authors": [
      "Jake Robertson",
      "Noah Hollmann",
      "Noor Awad",
      "Frank Hutter"
    ],
    "abstract": "Machine Learning systems are increasingly prevalent across healthcare, law\nenforcement, and finance but often operate on historical data, which may carry\nbiases against certain demographic groups. Causal and counterfactual fairness\nprovides an intuitive way to define fairness that closely aligns with legal\nstandards. Despite its theoretical benefits, counterfactual fairness comes with\nseveral practical limitations, largely related to the reliance on domain\nknowledge and approximate causal discovery techniques in constructing a causal\nmodel. In this study, we take a fresh perspective on counterfactually fair\nprediction, building upon recent work in in context learning (ICL) and prior\nfitted networks (PFNs) to learn a transformer called FairPFN. This model is\npretrained using synthetic fairness data to eliminate the causal effects of\nprotected attributes directly from observational data, removing the requirement\nof access to the correct causal model in practice. In our experiments, we\nthoroughly assess the effectiveness of FairPFN in eliminating the causal impact\nof protected attributes on a series of synthetic case studies and real world\ndatasets. Our findings pave the way for a new and promising research area:\ntransformers for causal and counterfactual fairness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05732v1",
    "published_date": "2024-07-08 08:36:44 UTC",
    "updated_date": "2024-07-08 08:36:44 UTC"
  },
  {
    "arxiv_id": "2407.05714v1",
    "title": "Implementing a hybrid approach in a knowledge engineering process to manage technical advice relating to feedback from the operation of complex sensitive equipment",
    "authors": [
      "Alain Claude Hervé Berger",
      "Sébastien Boblet",
      "Thierry Cartié",
      "Jean-Pierre Cotton",
      "François Vexler"
    ],
    "abstract": "How can technical advice on operating experience feedback be managed\nefficiently in an organization that has never used knowledge engineering\ntechniques and methods? This article explains how an industrial company in the\nnuclear and defense sectors adopted such an approach, adapted to its \"TA KM\"\norganizational context and falls within the ISO30401 framework, to build a\ncomplete system with a \"SARBACANES\" application to support its business\nprocesses and perpetuate its know-how and expertise in a knowledge base. Over\nand above the classic transfer of knowledge between experts and business\nspecialists, SARBACANES also reveals the ability of this type of engineering to\ndeliver multi-functional operation. Modeling was accelerated by the use of a\ntool adapted to this type of operation: the Ardans Knowledge Maker platform.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "in French language. 35es Journ{\\'e}es francophones d'Ing{\\'e}nierie\n  des Connaissances (IC 2024) @ Plate-Forme Intelligence Artificielle (PFIA\n  2024), Association Fran\\c{c}aise pour l'Intelligence Artificielle;\n  Laboratoire L3i La Rochelle Universit{\\'e}, Jul 2024, La Rochelle, France",
    "pdf_url": "http://arxiv.org/pdf/2407.05714v1",
    "published_date": "2024-07-08 08:17:10 UTC",
    "updated_date": "2024-07-08 08:17:10 UTC"
  },
  {
    "arxiv_id": "2407.05713v1",
    "title": "Short-term Object Interaction Anticipation with Disentangled Object Detection @ Ego4D Short Term Object Interaction Anticipation Challenge",
    "authors": [
      "Hyunjin Cho",
      "Dong Un Kang",
      "Se Young Chun"
    ],
    "abstract": "Short-term object interaction anticipation is an important task in egocentric\nvideo analysis, including precise predictions of future interactions and their\ntimings as well as the categories and positions of the involved active objects.\nTo alleviate the complexity of this task, our proposed method, SOIA-DOD,\neffectively decompose it into 1) detecting active object and 2) classifying\ninteraction and predicting their timing. Our method first detects all potential\nactive objects in the last frame of egocentric video by fine-tuning a\npre-trained YOLOv9. Then, we combine these potential active objects as query\nwith transformer encoder, thereby identifying the most promising next active\nobject and predicting its future interaction and time-to-contact. Experimental\nresults demonstrate that our method outperforms state-of-the-art models on the\nchallenge test set, achieving the best performance in predicting next active\nobjects and their interactions. Finally, our proposed ranked the third overall\ntop-5 mAP when including time-to-contact predictions. The source code is\navailable at https://github.com/KeenyJin/SOIA-DOD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.05713v1",
    "published_date": "2024-07-08 08:13:16 UTC",
    "updated_date": "2024-07-08 08:13:16 UTC"
  },
  {
    "arxiv_id": "2407.05712v3",
    "title": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices",
    "authors": [
      "Jianwen Jiang",
      "Gaojie Lin",
      "Zhengkun Rong",
      "Chao Liang",
      "Yongming Zhu",
      "Jiaqi Yang",
      "Tianyun Zhong"
    ],
    "abstract": "Existing neural head avatars methods have achieved significant progress in\nthe image quality and motion range of portrait animation. However, these\nmethods neglect the computational overhead, and to the best of our knowledge,\nnone is designed to run on mobile devices. This paper presents MobilePortrait,\na lightweight one-shot neural head avatars method that reduces learning\ncomplexity by integrating external knowledge into both the motion modeling and\nimage synthesis, enabling real-time inference on mobile devices. Specifically,\nwe introduce a mixed representation of explicit and implicit keypoints for\nprecise motion modeling and precomputed visual features for enhanced foreground\nand background synthesis. With these two key designs and using simple U-Nets as\nbackbones, our method achieves state-of-the-art performance with less than\none-tenth the computational demand. It has been validated to reach speeds of\nover 100 FPS on mobile devices and support both video and audio-driven inputs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.05712v3",
    "published_date": "2024-07-08 08:12:57 UTC",
    "updated_date": "2025-04-08 08:10:07 UTC"
  },
  {
    "arxiv_id": "2407.05705v1",
    "title": "Fast and Continual Knowledge Graph Embedding via Incremental LoRA",
    "authors": [
      "Jiajun Liu",
      "Wenjun Ke",
      "Peng Wang",
      "Jiahao Wang",
      "Jinhua Gao",
      "Ziyu Shang",
      "Guozheng Li",
      "Zijie Xu",
      "Ke Ji",
      "Yining Li"
    ],
    "abstract": "Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new\nknowledge and simultaneously preserve old knowledge. Dominant approaches\nprimarily focus on alleviating catastrophic forgetting of old knowledge but\nneglect efficient learning for the emergence of new knowledge. However, in\nreal-world scenarios, knowledge graphs (KGs) are continuously growing, which\nbrings a significant challenge to fine-tuning KGE models efficiently. To\naddress this issue, we propose a fast CKGE framework (\\model), incorporating an\nincremental low-rank adapter (\\mec) mechanism to efficiently acquire new\nknowledge while preserving old knowledge. Specifically, to mitigate\ncatastrophic forgetting, \\model\\ isolates and allocates new knowledge to\nspecific layers based on the fine-grained influence between old and new KGs.\nSubsequently, to accelerate fine-tuning, \\model\\ devises an efficient \\mec\\\nmechanism, which embeds the specific layers into incremental low-rank adapters\nwith fewer training parameters. Moreover, \\mec\\ introduces adaptive rank\nallocation, which makes the LoRA aware of the importance of entities and\nadjusts its rank scale adaptively. We conduct experiments on four public\ndatasets and two new datasets with a larger initial scale. Experimental results\ndemonstrate that \\model\\ can reduce training time by 34\\%-49\\% while still\nachieving competitive link prediction performance against state-of-the-art\nmodels on four public datasets (average MRR score of 21.0\\% vs.\n21.1\\%).Meanwhile, on two newly constructed datasets, \\model\\ saves 51\\%-68\\%\ntraining time and improves link prediction performance by 1.5\\%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IJCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05705v1",
    "published_date": "2024-07-08 08:07:13 UTC",
    "updated_date": "2024-07-08 08:07:13 UTC"
  },
  {
    "arxiv_id": "2407.05700v2",
    "title": "InverseCoder: Self-improving Instruction-Tuned Code LLMs with Inverse-Instruct",
    "authors": [
      "Yutong Wu",
      "Di Huang",
      "Wenxuan Shi",
      "Wei Wang",
      "Lingzhe Gao",
      "Shihao Liu",
      "Ziyuan Nan",
      "Kaizhao Yuan",
      "Rui Zhang",
      "Xishan Zhang",
      "Zidong Du",
      "Qi Guo",
      "Yewen Pu",
      "Dawei Yin",
      "Xing Hu",
      "Yunji Chen"
    ],
    "abstract": "Recent advancements in open-source code large language models (LLMs) have\nbeen driven by fine-tuning on the data generated from powerful closed-source\nLLMs, which are expensive to obtain. This paper explores whether it is possible\nto use a fine-tuned open-source model to generate additional data to augment\nits instruction-tuning dataset. We make two observations: (1) A code snippet\ncan serve as the response to different instructions. (2) Instruction-tuned code\nLLMs perform better at translating code into instructions than the reverse.\nBased on these observations, we propose Inverse-Instruct, a data augmentation\ntechnique that uses a fine-tuned LLM to generate additional instructions of\ncode responses from its own training dataset. The additional\ninstruction-response pairs are added to the original dataset, and a stronger\ncode LLM can be obtained by fine-tuning on the augmented dataset. We\nempirically validate Inverse-Instruct on a range of open-source code models\n(e.g. CodeLlama-Python and DeepSeek-Coder) and benchmarks (e.g., HumanEval(+),\nMBPP(+), DS-1000 and MultiPL-E), showing it consistently improves the base\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at AAAI 2025. Extended version with full\n  appendix, 18 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.05700v2",
    "published_date": "2024-07-08 08:00:05 UTC",
    "updated_date": "2024-12-16 03:08:42 UTC"
  },
  {
    "arxiv_id": "2407.05694v2",
    "title": "On the Limitations of Compute Thresholds as a Governance Strategy",
    "authors": [
      "Sara Hooker"
    ],
    "abstract": "At face value, this essay is about understanding a fairly esoteric governance\ntool called compute thresholds. However, in order to grapple with whether these\nthresholds will achieve anything, we must first understand how they came to be.\nTo do so, we need to engage with a decades-old debate at the heart of computer\nscience progress, namely, is bigger always better? Does a certain inflection\npoint of compute result in changes to the risk profile of a model? Hence, this\nessay may be of interest not only to policymakers and the wider public but also\nto computer scientists interested in understanding the role of compute in\nunlocking breakthroughs. This discussion is timely given the wide adoption of\ncompute thresholds in both the White House Executive Orders on AI Safety (EO)\nand the EU AI Act to identify more risky systems. A key conclusion of this\nessay is that compute thresholds, as currently implemented, are shortsighted\nand likely to fail to mitigate risk. The relationship between compute and risk\nis highly uncertain and rapidly changing. Relying upon compute thresholds\noverestimates our ability to predict what abilities emerge at different scales.\nThis essay ends with recommendations for a better way forward.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05694v2",
    "published_date": "2024-07-08 07:53:06 UTC",
    "updated_date": "2024-07-30 02:37:56 UTC"
  },
  {
    "arxiv_id": "2407.05693v2",
    "title": "Sub-SA: Strengthen In-context Learning via Submodular Selective Annotation",
    "authors": [
      "Jian Qian",
      "Miao Sun",
      "Sifan Zhou",
      "Ziyu Zhao",
      "Ruizhi Hun",
      "Patrick Chiang"
    ],
    "abstract": "In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose Sub-SA (Submodular Selective\nAnnotation), a submodule-based selective annotation method. The aim of Sub-SA\nis to reduce annotation costs while improving the quality of in-context\nexamples and minimizing the time consumption of the selection process. In\nSub-SA, we design a submodular function that facilitates effective subset\nselection for annotation and demonstrates the characteristics of monotonically\nand submodularity from the theoretical perspective. Specifically, we propose\nRPR (Reward and Penalty Regularization) to better balance the diversity and\nrepresentativeness of the unlabeled dataset attributed to a reward term and a\npenalty term, respectively. Consequently, the selection for annotations can be\neffectively addressed with a simple yet effective greedy search algorithm based\non the submodular function. Finally, we apply the similarity prompt retrieval\nto get the examples for ICL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05693v2",
    "published_date": "2024-07-08 07:47:30 UTC",
    "updated_date": "2024-09-13 06:57:01 UTC"
  },
  {
    "arxiv_id": "2407.05690v1",
    "title": "Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations",
    "authors": [
      "Bowen Shen",
      "Zheng Lin",
      "Daren Zha",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang",
      "Weiping Wang"
    ],
    "abstract": "Structured pruning fundamentally reduces computational and memory overheads\nof large language models (LLMs) and offers a feasible solution for end-side LLM\ndeployment. Structurally pruned models remain dense and high-precision, highly\ncompatible with further tuning and compression. However, as the coarse-grained\nstructured pruning poses large damage to the highly interconnected model,\nachieving a high compression ratio for scaled-up LLMs remains a challenge. In\nthis paper, we introduce a task-agnostic structured pruning approach coupled\nwith a compact Transformer architecture design. The proposed approach, named\nTransAct, reduces transitional activations inside multi-head attention (MHA)\nand multi-layer perceptron (MLP) modules, while preserving the inter-module\nactivations that are sensitive to perturbations. Hence, the LLM is pruned into\nan intra-module low-rank architecture, significantly reducing weights, KV Cache\nand attention computation. TransAct is implemented on the LLaMA model and\nevaluated on downstream benchmarks. Results verify the optimality of our\napproach at high compression with respect to both efficiency and performance.\nFurther, ablation studies reveal the strength of activation-guided iterative\npruning and provide experimental analysis on the redundancy of MHA and MLP\nmodules.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05690v1",
    "published_date": "2024-07-08 07:45:38 UTC",
    "updated_date": "2024-07-08 07:45:38 UTC"
  },
  {
    "arxiv_id": "2407.05689v1",
    "title": "Ten Years of Teaching Empirical Software Engineering in the context of Energy-efficient Software",
    "authors": [
      "Ivano Malavolta",
      "Vincenzo Stoico",
      "Patricia Lago"
    ],
    "abstract": "In this chapter we share our experience in running ten editions of the Green\nLab course at the Vrije Universiteit Amsterdam, the Netherlands. The course is\ngiven in the Software Engineering and Green IT track of the Computer Science\nMaster program of the VU. The course takes place every year over a 2-month\nperiod and teaches Computer Science students the fundamentals of Empirical\nSoftware Engineering in the context of energy-efficient software. The\npeculiarity of the course is its research orientation: at the beginning of the\ncourse the instructor presents a catalog of scientifically relevant goals, and\neach team of students signs up for one of them and works together for 2 months\non their own experiment for achieving the goal. Each team goes over the classic\nsteps of an empirical study, starting from a precise formulation of the goal\nand research questions to context definition, selection of experimental\nsubjects and objects, definition of experimental variables, experiment\nexecution, data analysis, and reporting. Over the years, the course became\nwell-known within the Software Engineering community since it led to several\nscientific studies that have been published at various scientific conferences\nand journals. Also, students execute their experiments using\n\\textit{open-source tools}, which are developed and maintained by researchers\nand other students within the program, thus creating a virtuous community of\nlearners where students exchange ideas, help each other, and learn how to\ncollaboratively contribute to open-source projects in a safe environment.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05689v1",
    "published_date": "2024-07-08 07:44:49 UTC",
    "updated_date": "2024-07-08 07:44:49 UTC"
  },
  {
    "arxiv_id": "2407.05688v2",
    "title": "Learning with Alignments: Tackling the Inter- and Intra-domain Shifts for Cross-multidomain Facial Expression Recognition",
    "authors": [
      "Yuxiang Yang",
      "Lu Wen",
      "Xinyi Zeng",
      "Yuanyuan Xu",
      "Xi Wu",
      "Jiliu Zhou",
      "Yan Wang"
    ],
    "abstract": "Facial Expression Recognition (FER) holds significant importance in\nhuman-computer interactions. Existing cross-domain FER methods often transfer\nknowledge solely from a single labeled source domain to an unlabeled target\ndomain, neglecting the comprehensive information across multiple sources.\nNevertheless, cross-multidomain FER (CMFER) is very challenging for (i) the\ninherent inter-domain shifts across multiple domains and (ii) the intra-domain\nshifts stemming from the ambiguous expressions and low inter-class\ndistinctions. In this paper, we propose a novel Learning with Alignments CMFER\nframework, named LA-CMFER, to handle both inter- and intra-domain shifts.\nSpecifically, LA-CMFER is constructed with a global branch and a local branch\nto extract features from the full images and local subtle expressions,\nrespectively. Based on this, LA-CMFER presents a dual-level inter-domain\nalignment method to force the model to prioritize hard-to-align samples in\nknowledge transfer at a sample level while gradually generating a\nwell-clustered feature space with the guidance of class attributes at a cluster\nlevel, thus narrowing the inter-domain shifts. To address the intra-domain\nshifts, LA-CMFER introduces a multi-view intra-domain alignment method with a\nmulti-view clustering consistency constraint where a prediction similarity\nmatrix is built to pursue consistency between the global and local views, thus\nrefining pseudo labels and eliminating latent noise. Extensive experiments on\nsix benchmark datasets have validated the superiority of our LA-CMFER.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05688v2",
    "published_date": "2024-07-08 07:43:06 UTC",
    "updated_date": "2024-07-30 11:02:51 UTC"
  },
  {
    "arxiv_id": "2407.05683v2",
    "title": "RadiomicsFill-Mammo: Synthetic Mammogram Mass Manipulation with Radiomics Features",
    "authors": [
      "Inye Na",
      "Jonghun Kim",
      "Eun Sook Ko",
      "Hyunjin Park"
    ],
    "abstract": "Motivated by the question, \"Can we generate tumors with desired attributes?''\nthis study leverages radiomics features to explore the feasibility of\ngenerating synthetic tumor images. Characterized by its low-dimensional yet\nbiologically meaningful markers, radiomics bridges the gap between complex\nmedical imaging data and actionable clinical insights. We present\nRadiomicsFill-Mammo, the first of the RadiomicsFill series, an innovative\ntechnique that generates realistic mammogram mass images mirroring specific\nradiomics attributes using masked images and opposite breast images, leveraging\na recent stable diffusion model. This approach also allows for the\nincorporation of essential clinical variables, such as BI-RADS and breast\ndensity, alongside radiomics features as conditions for mass generation.\nResults indicate that RadiomicsFill-Mammo effectively generates diverse and\nrealistic tumor images based on various radiomics conditions. Results also\ndemonstrate a significant improvement in mass detection capabilities,\nleveraging RadiomicsFill-Mammo as a strategy to generate simulated samples.\nFurthermore, RadiomicsFill-Mammo not only advances medical imaging research but\nalso opens new avenues for enhancing treatment planning and tumor simulation.\nOur code is available at https://github.com/nainye/RadiomicsFill.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted at MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05683v2",
    "published_date": "2024-07-08 07:33:52 UTC",
    "updated_date": "2024-09-29 08:58:47 UTC"
  },
  {
    "arxiv_id": "2407.05680v2",
    "title": "Fine-Grained Multi-View Hand Reconstruction Using Inverse Rendering",
    "authors": [
      "Qijun Gan",
      "Wentong Li",
      "Jinwei Ren",
      "Jianke Zhu"
    ],
    "abstract": "Reconstructing high-fidelity hand models with intricate textures plays a\ncrucial role in enhancing human-object interaction and advancing real-world\napplications. Despite the state-of-the-art methods excelling in texture\ngeneration and image rendering, they often face challenges in accurately\ncapturing geometric details. Learning-based approaches usually offer better\nrobustness and faster inference, which tend to produce smoother results and\nrequire substantial amounts of training data. To address these issues, we\npresent a novel fine-grained multi-view hand mesh reconstruction method that\nleverages inverse rendering to restore hand poses and intricate details.\nFirstly, our approach predicts a parametric hand mesh model through Graph\nConvolutional Networks (GCN) based method from multi-view images. We further\nintroduce a novel Hand Albedo and Mesh (HAM) optimization module to refine both\nthe hand mesh and textures, which is capable of preserving the mesh topology.\nIn addition, we suggest an effective mesh-based neural rendering scheme to\nsimultaneously generate photo-realistic image and optimize mesh geometry by\nfusing the pre-trained rendering network with vertex features. We conduct the\ncomprehensive experiments on InterHand2.6M, DeepHandMesh and dataset collected\nby ourself, whose promising results show that our proposed approach outperforms\nthe state-of-the-art methods on both reconstruction accuracy and rendering\nquality. Code and dataset are publicly available at\nhttps://github.com/agnJason/FMHR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05680v2",
    "published_date": "2024-07-08 07:28:24 UTC",
    "updated_date": "2024-07-09 03:39:45 UTC"
  },
  {
    "arxiv_id": "2407.05679v3",
    "title": "BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents",
    "authors": [
      "Yumeng Zhang",
      "Shi Gong",
      "Kaixin Xiong",
      "Xiaoqing Ye",
      "Xiaofan Li",
      "Xiao Tan",
      "Fan Wang",
      "Jizhou Huang",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract": "World models have attracted increasing attention in autonomous driving for\ntheir ability to forecast potential future scenarios. In this paper, we propose\nBEVWorld, a novel framework that transforms multimodal sensor inputs into a\nunified and compact Bird's Eye View (BEV) latent space for holistic environment\nmodeling. The proposed world model consists of two main components: a\nmulti-modal tokenizer and a latent BEV sequence diffusion model. The\nmulti-modal tokenizer first encodes heterogeneous sensory data, and its decoder\nreconstructs the latent BEV tokens into LiDAR and surround-view image\nobservations via ray-casting rendering in a self-supervised manner. This\nenables joint modeling and bidirectional encoding-decoding of panoramic imagery\nand point cloud data within a shared spatial representation. On top of this,\nthe latent BEV sequence diffusion model performs temporally consistent\nforecasting of future scenes, conditioned on high-level action tokens, enabling\nscene-level reasoning over time. Extensive experiments demonstrate the\neffectiveness of BEVWorld on autonomous driving benchmarks, showcasing its\ncapability in realistic future scene generation and its benefits for downstream\ntasks such as perception and motion prediction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.05679v3",
    "published_date": "2024-07-08 07:26:08 UTC",
    "updated_date": "2025-04-30 13:43:51 UTC"
  },
  {
    "arxiv_id": "2407.05674v2",
    "title": "Coding Reliable LLM-based Integrated Task and Knowledge Agents with GenieWorksheets",
    "authors": [
      "Harshit Joshi",
      "Shicheng Liu",
      "James Chen",
      "Robert Weigle",
      "Monica S. Lam"
    ],
    "abstract": "Large Language Models (LLMs) present an opportunity to create automated\nassistants that can help users navigate complex tasks. However, existing\napproaches have limitations in handling conditional logic, integrating\nknowledge sources, and consistently following instructions. Researchers and\nindustry professionals often employ ad hoc pipelines to construct\nconversational agents. These pipelines aim to maintain context, address failure\ncases, and minimize hallucinations, yet frequently fail to achieve these\nobjectives. To this end, we present Genie - a programmable framework for\ncreating task-oriented conversational agents that are designed to handle\ncomplex user interactions and knowledge queries. Unlike LLMs, Genie provides\nreliable grounded responses, with controllable agent policies through its\nexpressive specification, Genie Worksheet. In contrast to dialog trees, it is\nresilient to diverse user queries, helpful with knowledge sources, and offers\nease of programming policies through its declarative paradigm. The agents built\nusing Genie outperforms the state-of-the-art method on complex logic domains in\nSTARV2 dataset by up to 20.5%. Additionally, through a real-user study\ninvolving 62 participants, we show that Genie beats the GPT-4 with function\ncalling baseline by 21.1%, 20.1%, and 61% on execution accuracy, dialogue act\naccuracy, and goal completion rate, respectively, on three diverse real-world\ndomains",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2407.05674v2",
    "published_date": "2024-07-08 07:17:40 UTC",
    "updated_date": "2024-10-31 02:02:41 UTC"
  },
  {
    "arxiv_id": "2407.05671v1",
    "title": "MSTF: Multiscale Transformer for Incomplete Trajectory Prediction",
    "authors": [
      "Zhanwen Liu",
      "Chao Li",
      "Nan Yang",
      "Yang Wang",
      "Jiaqi Ma",
      "Guangliang Cheng",
      "Xiangmo Zhao"
    ],
    "abstract": "Motion forecasting plays a pivotal role in autonomous driving systems,\nenabling vehicles to execute collision warnings and rational local-path\nplanning based on predictions of the surrounding vehicles. However, prevalent\nmethods often assume complete observed trajectories, neglecting the potential\nimpact of missing values induced by object occlusion, scope limitation, and\nsensor failures. Such oversights inevitably compromise the accuracy of\ntrajectory predictions. To tackle this challenge, we propose an end-to-end\nframework, termed Multiscale Transformer (MSTF), meticulously crafted for\nincomplete trajectory prediction. MSTF integrates a Multiscale Attention Head\n(MAH) and an Information Increment-based Pattern Adaptive (IIPA) module.\nSpecifically, the MAH component concurrently captures multiscale motion\nrepresentation of trajectory sequence from various temporal granularities,\nutilizing a multi-head attention mechanism. This approach facilitates the\nmodeling of global dependencies in motion across different scales, thereby\nmitigating the adverse effects of missing values. Additionally, the IIPA module\nadaptively extracts continuity representation of motion across time steps by\nanalyzing missing patterns in the data. The continuity representation\ndelineates motion trend at a higher level, guiding MSTF to generate predictions\nconsistent with motion continuity. We evaluate our proposed MSTF model using\ntwo large-scale real-world datasets. Experimental results demonstrate that MSTF\nsurpasses state-of-the-art (SOTA) models in the task of incomplete trajectory\nprediction, showcasing its efficacy in addressing the challenges posed by\nmissing values in motion forecasting for autonomous driving systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05671v1",
    "published_date": "2024-07-08 07:10:17 UTC",
    "updated_date": "2024-07-08 07:10:17 UTC"
  },
  {
    "arxiv_id": "2407.05669v1",
    "title": "Fractional Budget Allocation for Influence Maximization under General Marketing Strategies",
    "authors": [
      "Akhil Bhimaraju",
      "Eliot W. Robson",
      "Lav R. Varshney",
      "Abhishek K. Umrawal"
    ],
    "abstract": "We consider the fractional influence maximization problem, i.e., identifying\nusers on a social network to be incentivized with potentially partial discounts\nto maximize the influence on the network. The larger the discount given to a\nuser, the higher the likelihood of its activation (adopting a new product or\ninnovation), who then attempts to activate its neighboring users, causing a\ncascade effect of influence through the network. Our goal is to devise\nefficient algorithms that assign initial discounts to the network's users to\nmaximize the total number of activated users at the end of the cascade, subject\nto a constraint on the total sum of discounts given. In general, the activation\nlikelihood could be any non-decreasing function of the discount, whereas, our\nfocus lies on the case when the activation likelihood is an affine function of\nthe discount, potentially varying across different users. As this problem is\nshown to be NP-hard, we propose and analyze an efficient (1-1/e)-approximation\nalgorithm. Furthermore, we run experiments on real-world social networks to\nshow the performance and scalability of our method.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.DS",
      "stat.ML",
      "05C85, 60J60, 68R05, 68R10, 68T01, 90C27, 90C35",
      "F.2.2; G.1.2; G.1.6; G.2.1; G.2.2; G.3; I.2.0; J.4"
    ],
    "primary_category": "cs.SI",
    "comment": "5 pages, 2 figures, and 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.05669v1",
    "published_date": "2024-07-08 07:09:11 UTC",
    "updated_date": "2024-07-08 07:09:11 UTC"
  },
  {
    "arxiv_id": "2407.05664v2",
    "title": "How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning",
    "authors": [
      "Arthur Jacot",
      "Seok Hoan Choi",
      "Yuxiao Wen"
    ],
    "abstract": "We show that deep neural networks (DNNs) can efficiently learn any\ncomposition of functions with bounded $F_{1}$-norm, which allows DNNs to break\nthe curse of dimensionality in ways that shallow networks cannot. More\nspecifically, we derive a generalization bound that combines a covering number\nargument for compositionality, and the $F_{1}$-norm (or the related Barron\nnorm) for large width adaptivity. We show that the global minimizer of the\nregularized loss of DNNs can fit for example the composition of two functions\n$f^{*}=h\\circ g$ from a small number of observations, assuming $g$ is\nsmooth/regular and reduces the dimensionality (e.g. $g$ could be the quotient\nmap of the symmetries of $f^{*}$), so that $h$ can be learned in spite of its\nlow regularity. The measures of regularity we consider is the Sobolev norm with\ndifferent levels of differentiability, which is well adapted to the $F_{1}$\nnorm. We compute scaling laws empirically and observe phase transitions\ndepending on whether $g$ or $h$ is harder to learn, as predicted by our theory.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05664v2",
    "published_date": "2024-07-08 06:59:29 UTC",
    "updated_date": "2025-03-06 13:40:09 UTC"
  },
  {
    "arxiv_id": "2407.17508v1",
    "title": "Artificial Intelligence Based Navigation in Quasi Structured Environment",
    "authors": [
      "Hariram Sampath Kumar",
      "Archana Singh",
      "Manish Kumar Ojha"
    ],
    "abstract": "The proper planning of different types of public transportation such as\nmetro, highway, waterways, and so on, can increase the efficiency, reduce the\ncongestion and improve the safety of the country. There are certain challenges\nassociated with route planning, such as high cost of implementation, need for\nadequate resource & infrastructure and resistance to change. The goal of this\nresearch is to examine the working, applications, complexity factors,\nadvantages & disadvantages of Floyd- Warshall, Bellman-Ford, Johnson, Ant\nColony Optimization (ACO), Particle Swarm Optimization (PSO), & Grey Wolf\nOptimizer (GWO), to find the best choice for the above application. In this\npaper, comparative analysis of above-mentioned algorithms is presented. The\nFloyd-Warshall method and ACO algorithm are chosen based on the comparisons.\nAlso, a combination of modified Floyd-Warshall with ACO algorithm is proposed.\nThe proposed algorithm showed better results with less time complexity, when\napplied on randomly structured points within a boundary called quasi-structured\npoints. In addition, this paper also discusses the future works of integrating\nFloyd-Warshall with ACO to develop a real-time model for overcoming above\nmentioned-challenges during transportation route planning.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.17508v1",
    "published_date": "2024-07-08 06:42:02 UTC",
    "updated_date": "2024-07-08 06:42:02 UTC"
  },
  {
    "arxiv_id": "2407.05650v3",
    "title": "The Cooperative Network Architecture: Learning Structured Networks as Representation of Sensory Patterns",
    "authors": [
      "Pascal J. Sager",
      "Jan M. Deriu",
      "Benjamin F. Grewe",
      "Thilo Stadelmann",
      "Christoph von der Malsburg"
    ],
    "abstract": "We introduce the Cooperative Network Architecture (CNA), a model that\nrepresents sensory signals using structured, recurrently connected networks of\nneurons, termed \"nets.\" Nets are dynamically assembled from overlapping net\nfragments, which are learned based on statistical regularities in sensory\ninput. This architecture offers robustness to noise, deformation, and\nout-of-distribution data, addressing challenges in current vision systems from\na novel perspective. We demonstrate that net fragments can be learned without\nsupervision and flexibly recombined to encode novel patterns, enabling figure\ncompletion and resilience to noise. Our findings establish CNA as a promising\nparadigm for developing neural representations that integrate local feature\nprocessing with global structure formation, providing a foundation for future\nresearch on invariant object recognition.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05650v3",
    "published_date": "2024-07-08 06:22:10 UTC",
    "updated_date": "2025-03-20 20:44:23 UTC"
  },
  {
    "arxiv_id": "2407.05649v5",
    "title": "Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention",
    "authors": [
      "Tongzhou Liao",
      "Barnabás Póczos"
    ],
    "abstract": "Graph Neural Networks (GNNs) have become important tools for machine learning\non graph-structured data. In this paper, we explore the synergistic combination\nof graph encoding, graph rewiring, and graph attention, by introducing Graph\nAttention with Stochastic Structures (GRASS), a novel GNN architecture. GRASS\nutilizes relative random walk probabilities (RRWP) encoding and a novel\ndecomposed variant (D-RRWP) to efficiently capture structural information. It\nrewires the input graph by superimposing a random regular graph to enhance\nlong-range information propagation. It also employs a novel additive attention\nmechanism tailored for graph-structured data. Our empirical evaluations\ndemonstrate that GRASS achieves state-of-the-art performance on multiple\nbenchmark datasets, including a 20.3% reduction in mean absolute error on the\nZINC dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.05649v5",
    "published_date": "2024-07-08 06:21:56 UTC",
    "updated_date": "2025-03-14 23:47:53 UTC"
  },
  {
    "arxiv_id": "2407.05611v1",
    "title": "GenFollower: Enhancing Car-Following Prediction with Large Language Models",
    "authors": [
      "Xianda Chen",
      "Mingxing Peng",
      "PakHin Tiu",
      "Yuanfei Wu",
      "Junjie Chen",
      "Meixin Zhu",
      "Xinhu Zheng"
    ],
    "abstract": "Accurate modeling of car-following behaviors is essential for various\napplications in traffic management and autonomous driving systems. However,\ncurrent approaches often suffer from limitations like high sensitivity to data\nquality and lack of interpretability. In this study, we propose GenFollower, a\nnovel zero-shot prompting approach that leverages large language models (LLMs)\nto address these challenges. We reframe car-following behavior as a language\nmodeling problem and integrate heterogeneous inputs into structured prompts for\nLLMs. This approach achieves improved prediction performance and\ninterpretability compared to traditional baseline models. Experiments on the\nWaymo Open datasets demonstrate GenFollower's superior performance and ability\nto provide interpretable insights into factors influencing car-following\nbehavior. This work contributes to advancing the understanding and prediction\nof car-following behaviors, paving the way for enhanced traffic management and\nautonomous driving systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05611v1",
    "published_date": "2024-07-08 04:54:42 UTC",
    "updated_date": "2024-07-08 04:54:42 UTC"
  },
  {
    "arxiv_id": "2407.05603v1",
    "title": "WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering",
    "authors": [
      "Pingyi Chen",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Honglin Li",
      "Lin Yang"
    ],
    "abstract": "Whole slide imaging is routinely adopted for carcinoma diagnosis and\nprognosis. Abundant experience is required for pathologists to achieve accurate\nand reliable diagnostic results of whole slide images (WSI). The huge size and\nheterogeneous features of WSIs make the workflow of pathological reading\nextremely time-consuming. In this paper, we propose a novel framework (WSI-VQA)\nto interpret WSIs by generative visual question answering. WSI-VQA shows\nuniversality by reframing various kinds of slide-level tasks in a\nquestion-answering pattern, in which pathologists can achieve\nimmunohistochemical grading, survival prediction, and tumor subtyping following\nhuman-machine interaction. Furthermore, we establish a WSI-VQA dataset which\ncontains 8672 slide-level question-answering pairs with 977 WSIs. Besides the\nability to deal with different slide-level tasks, our generative model which is\nnamed Wsi2Text Transformer (W2T) outperforms existing discriminative models in\nmedical correctness, which reveals the potential of our model to be applied in\nthe clinical scenario. Additionally, we also visualize the co-attention mapping\nbetween word embeddings and WSIs as an intuitive explanation for diagnostic\nresults. The dataset and related code are available at\nhttps://github.com/cpystan/WSI-VQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05603v1",
    "published_date": "2024-07-08 04:37:32 UTC",
    "updated_date": "2024-07-08 04:37:32 UTC"
  },
  {
    "arxiv_id": "2407.07921v1",
    "title": "A Trustworthy AIoT-enabled Localization System via Federated Learning and Blockchain",
    "authors": [
      "Junfei Wang",
      "He Huang",
      "Jingze Feng",
      "Steven Wong",
      "Lihua Xie",
      "Jianfei Yang"
    ],
    "abstract": "There is a significant demand for indoor localization technology in smart\nbuildings, and the most promising solution in this field is using RF sensors\nand fingerprinting-based methods that employ machine learning models trained on\ncrowd-sourced user data gathered from IoT devices. However, this raises\nsecurity and privacy issues in practice. Some researchers propose to use\nfederated learning to partially overcome privacy problems, but there still\nremain security concerns, e.g., single-point failure and malicious attacks. In\nthis paper, we propose a framework named DFLoc to achieve precise 3D\nlocalization tasks while considering the following two security concerns.\nParticularly, we design a specialized blockchain to decentralize the framework\nby distributing the tasks such as model distribution and aggregation which are\nhandled by a central server to all clients in most previous works, to address\nthe issue of the single-point failure for a reliable and accurate indoor\nlocalization system. Moreover, we introduce an updated model verification\nmechanism within the blockchain to alleviate the concern of malicious node\nattacks. Experimental results substantiate the framework's capacity to deliver\naccurate 3D location predictions and its superior resistance to the impacts of\nsingle-point failure and malicious attacks when compared to conventional\ncentralized federated learning systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07921v1",
    "published_date": "2024-07-08 04:14:19 UTC",
    "updated_date": "2024-07-08 04:14:19 UTC"
  },
  {
    "arxiv_id": "2407.05580v1",
    "title": "$\\mathrm{E^{2}CFD}$: Towards Effective and Efficient Cost Function Design for Safe Reinforcement Learning via Large Language Model",
    "authors": [
      "Zepeng Wang",
      "Chao Ma",
      "Linjiang Zhou",
      "Libing Wu",
      "Lei Yang",
      "Xiaochuan Shi",
      "Guojun Peng"
    ],
    "abstract": "Different classes of safe reinforcement learning algorithms have shown\nsatisfactory performance in various types of safety requirement scenarios.\nHowever, the existing methods mainly address one or several classes of specific\nsafety requirement scenario problems and cannot be applied to arbitrary safety\nrequirement scenarios. In addition, the optimization objectives of existing\nreinforcement learning algorithms are misaligned with the task requirements.\nBased on the need to address these issues, we propose $\\mathrm{E^{2}CFD}$, an\neffective and efficient cost function design framework. $\\mathrm{E^{2}CFD}$\nleverages the capabilities of a large language model (LLM) to comprehend\nvarious safety scenarios and generate corresponding cost functions. It\nincorporates the \\textit{fast performance evaluation (FPE)} method to\nfacilitate rapid and iterative updates to the generated cost function. Through\nthis iterative process, $\\mathrm{E^{2}CFD}$ aims to obtain the most suitable\ncost function for policy training, tailored to the specific tasks within the\nsafety scenario. Experiments have proven that the performance of policies\ntrained using this framework is superior to traditional safe reinforcement\nlearning algorithms and policies trained with carefully designed cost\nfunctions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05580v1",
    "published_date": "2024-07-08 03:30:25 UTC",
    "updated_date": "2024-07-08 03:30:25 UTC"
  },
  {
    "arxiv_id": "2407.11044v1",
    "title": "Generalizing soft actor-critic algorithms to discrete action spaces",
    "authors": [
      "Le Zhang",
      "Yong Gu",
      "Xin Zhao",
      "Yanshuo Zhang",
      "Shu Zhao",
      "Yifei Jin",
      "Xinxin Wu"
    ],
    "abstract": "ATARI is a suite of video games used by reinforcement learning (RL)\nresearchers to test the effectiveness of the learning algorithm. Receiving only\nthe raw pixels and the game score, the agent learns to develop sophisticated\nstrategies, even to the comparable level of a professional human games tester.\nIdeally, we also want an agent requiring very few interactions with the\nenvironment. Previous competitive model-free algorithms for the task use the\nvalued-based Rainbow algorithm without any policy head. In this paper, we\nchange it by proposing a practical discrete variant of the soft actor-critic\n(SAC) algorithm. The new variant enables off-policy learning using policy heads\nfor discrete domains. By incorporating it into the advanced Rainbow variant,\ni.e., the ``bigger, better, faster'' (BBF), the resulting SAC-BBF improves the\nprevious state-of-the-art interquartile mean (IQM) from 1.045 to 1.088, and it\nachieves these results using only replay ratio (RR) 2. By using lower RR 2, the\ntraining time of SAC-BBF is strictly one-third of the time required for BBF to\nachieve an IQM of 1.045 using RR 8. As a value of IQM greater than one\nindicates super-human performance, SAC-BBF is also the only model-free\nalgorithm with a super-human level using only RR 2. The code is publicly\navailable on GitHub at https://github.com/lezhang-thu/bigger-better-faster-SAC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV)\n  2024. GitHub Repo https://github.com/lezhang-thu/bigger-better-faster-SAC",
    "pdf_url": "http://arxiv.org/pdf/2407.11044v1",
    "published_date": "2024-07-08 03:20:45 UTC",
    "updated_date": "2024-07-08 03:20:45 UTC"
  },
  {
    "arxiv_id": "2407.05557v1",
    "title": "$R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning",
    "authors": [
      "Mintong Kang",
      "Bo Li"
    ],
    "abstract": "As LLMs become increasingly prevalent across various applications, it is\ncritical to establish safety guardrails to moderate input/output content of\nLLMs. Existing guardrail models treat various safety categories independently\nand fail to explicitly capture the intercorrelations among them. This has led\nto limitations such as ineffectiveness due to inadequate training on long-tail\ndata from correlated safety categories, susceptibility to jailbreaking attacks,\nand inflexibility regarding new safety categories. To address these\nlimitations, we propose $R^2$-Guard, a robust reasoning enabled LLM guardrail\nvia knowledge-enhanced logical reasoning. Specifically, $R^2$-Guard comprises\ntwo parts: data-driven category-specific learning and reasoning components. The\ndata-driven guardrail models provide unsafety probabilities of moderated\ncontent on different safety categories. We then encode safety knowledge among\ndifferent categories as first-order logical rules and embed them into a\nprobabilistic graphic model (PGM) based reasoning component. The unsafety\nprobabilities of different categories from data-driven guardrail models are\nsent to the reasoning component for final inference. We employ two types of\nPGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and\noptimize PCs to achieve precision-efficiency balance via improved graph\nstructure. To further perform stress tests for guardrail models, we employ a\npairwise construction method to construct a new safety benchmark TwinSafety,\nwhich features principled categories. We demonstrate the effectiveness of\n$R^2$-Guard by comparisons with eight strong guardrail models on six safety\nbenchmarks, and demonstrate the robustness of $R^2$-Guard against four SOTA\njailbreaking attacks. $R^2$-Guard significantly surpasses SOTA method\nLlamaGuard by 30.2% on ToxicChat and by 59.5% against jailbreaking attacks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05557v1",
    "published_date": "2024-07-08 02:15:29 UTC",
    "updated_date": "2024-07-08 02:15:29 UTC"
  },
  {
    "arxiv_id": "2407.05550v4",
    "title": "MEEG and AT-DGNN: Improving EEG Emotion Recognition with Music Introducing and Graph-based Learning",
    "authors": [
      "Minghao Xiao",
      "Zhengxi Zhu",
      "Kang Xie",
      "Bin Jiang"
    ],
    "abstract": "We present the MEEG dataset, a multi-modal collection of music-induced\nelectroencephalogram (EEG) recordings designed to capture emotional responses\nto various musical stimuli across different valence and arousal levels. This\npublic dataset facilitates an in-depth examination of brainwave patterns within\nmusical contexts, providing a robust foundation for studying brain network\ntopology during emotional processing. Leveraging the MEEG dataset, we introduce\nthe Attention-based Temporal Learner with Dynamic Graph Neural Network\n(AT-DGNN), a novel framework for EEG-based emotion recognition. This model\ncombines an attention mechanism with a dynamic graph neural network (DGNN) to\ncapture intricate EEG dynamics. The AT-DGNN achieves state-of-the-art (SOTA)\nperformance with an accuracy of 83.74% in arousal recognition and 86.01% in\nvalence recognition, outperforming existing SOTA methods. Comparative analysis\nwith traditional datasets, such as DEAP, further validates the model's\neffectiveness and underscores the potency of music as an emotional stimulus.\nThis study advances graph-based learning methodology in brain-computer\ninterfaces (BCI), significantly improving the accuracy of EEG-based emotion\nrecognition. The MEEG dataset and source code are publicly available at\nhttps://github.com/xmh1011/AT-DGNN.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05550v4",
    "published_date": "2024-07-08 01:58:48 UTC",
    "updated_date": "2024-11-18 03:55:00 UTC"
  },
  {
    "arxiv_id": "2407.05538v1",
    "title": "On the Equivalence between Logic Programming and SETAF",
    "authors": [
      "João Alcântara",
      "Renan Cordeiro",
      "Samy Sá"
    ],
    "abstract": "A framework with sets of attacking arguments (SETAF) is an extension of the\nwell-known Dung's Abstract Argumentation Frameworks (AAFs) that allows joint\nattacks on arguments. In this paper, we provide a translation from Normal Logic\nPrograms (NLPs) to SETAFs and vice versa, from SETAFs to NLPs. We show that\nthere is pairwise equivalence between their semantics, including the\nequivalence between L-stable and semi-stable semantics. Furthermore, for a\nclass of NLPs called Redundancy-Free Atomic Logic Programs (RFALPs), there is\nalso a structural equivalence as these back-and-forth translations are each\nother's inverse. Then, we show that RFALPs are as expressive as NLPs by\ntransforming any NLP into an equivalent RFALP through a series of program\ntransformations already known in the literature. We also show that these\nprogram transformations are confluent, meaning that every NLP will be\ntransformed into a unique RFALP. The results presented in this paper enhance\nour understanding that NLPs and SETAFs are essentially the same formalism.\nUnder consideration in Theory and Practice of Logic Programming (TPLP).",
    "categories": [
      "cs.AI",
      "I.2.3"
    ],
    "primary_category": "cs.AI",
    "comment": "44 pages, 5 figures. Under consideration in Theory and Practice of\n  Logic Programming (TPLP)",
    "pdf_url": "http://arxiv.org/pdf/2407.05538v1",
    "published_date": "2024-07-08 01:03:53 UTC",
    "updated_date": "2024-07-08 01:03:53 UTC"
  },
  {
    "arxiv_id": "2407.05530v1",
    "title": "This&That: Language-Gesture Controlled Video Generation for Robot Planning",
    "authors": [
      "Boyang Wang",
      "Nikhil Sridhar",
      "Chao Feng",
      "Mark Van der Merwe",
      "Adam Fishman",
      "Nima Fazeli",
      "Jeong Joon Park"
    ],
    "abstract": "We propose a robot learning method for communicating, planning, and executing\na wide range of tasks, dubbed This&That. We achieve robot planning for general\ntasks by leveraging the power of video generative models trained on\ninternet-scale data containing rich physical and semantic context. In this\nwork, we tackle three fundamental challenges in video-based planning: 1)\nunambiguous task communication with simple human instructions, 2) controllable\nvideo generation that respects user intents, and 3) translating visual planning\ninto robot actions. We propose language-gesture conditioning to generate\nvideos, which is both simpler and clearer than existing language-only methods,\nespecially in complex and uncertain environments. We then suggest a behavioral\ncloning design that seamlessly incorporates the video plans. This&That\ndemonstrates state-of-the-art effectiveness in addressing the above three\nchallenges, and justifies the use of video generation as an intermediate\nrepresentation for generalizable task planning and execution. Project website:\nhttps://cfeng16.github.io/this-and-that/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05530v1",
    "published_date": "2024-07-08 00:28:41 UTC",
    "updated_date": "2024-07-08 00:28:41 UTC"
  },
  {
    "arxiv_id": "2407.05526v1",
    "title": "Can Machines Learn the True Probabilities?",
    "authors": [
      "Jinsook Kim"
    ],
    "abstract": "When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05526v1",
    "published_date": "2024-07-08 00:19:43 UTC",
    "updated_date": "2024-07-08 00:19:43 UTC"
  }
]
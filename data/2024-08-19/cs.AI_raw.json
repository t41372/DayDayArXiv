[
  {
    "arxiv_id": "2408.10455v5",
    "title": "IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction",
    "authors": [
      "Kaiyu He",
      "Mian Zhang",
      "Shuo Yan",
      "Peilin Wu",
      "Zhiyu Zoey Chen"
    ],
    "abstract": "While large language models (LLMs) have been thoroughly evaluated for\ndeductive and inductive reasoning, their proficiency in holistic rule learning\nin interactive environments remains less explored. We introduce RULEARN, a\nnovel benchmark to assess the rule-learning abilities of LLM agents in\ninteractive settings. In RULEARN, agents strategically interact with simulated\nenvironments to gather observations, discern patterns, and solve complex\nproblems. To enhance the rule-learning capabilities for LLM agents, we propose\nIDEA, a novel reasoning framework that integrates the process of Induction,\nDeduction, and Abduction. The IDEA agent generates initial hypotheses from\nlimited observations through abduction, devises plans to validate these\nhypotheses or leverages them to solve problems via deduction, and refines\nprevious hypotheses through induction, dynamically establishing and applying\nrules that mimic human rule-learning behaviors. Our evaluation of the IDEA\nframework, which involves five representative LLMs, demonstrates significant\nimprovements over the baseline. Furthermore, our study with human participants\nreveals notable discrepancies in rule-learning behaviors between humans and\nLLMs. We believe our benchmark will serve as a valuable and challenging\nresource, and IDEA will provide crucial insights for the development of LLM\nagents capable of human-like rule learning in real-world scenarios. Our code\nand data is publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10455v5",
    "published_date": "2024-08-19 23:37:07 UTC",
    "updated_date": "2024-12-19 05:45:35 UTC"
  },
  {
    "arxiv_id": "2408.10450v1",
    "title": "RUMI: Rummaging Using Mutual Information",
    "authors": [
      "Sheng Zhong",
      "Nima Fazeli",
      "Dmitry Berenson"
    ],
    "abstract": "This paper presents Rummaging Using Mutual Information (RUMI), a method for\nonline generation of robot action sequences to gather information about the\npose of a known movable object in visually-occluded environments. Focusing on\ncontact-rich rummaging, our approach leverages mutual information between the\nobject pose distribution and robot trajectory for action planning. From an\nobserved partial point cloud, RUMI deduces the compatible object pose\ndistribution and approximates the mutual information of it with workspace\noccupancy in real time. Based on this, we develop an information gain cost\nfunction and a reachability cost function to keep the object within the robot's\nreach. These are integrated into a model predictive control (MPC) framework\nwith a stochastic dynamics model, updating the pose distribution in a closed\nloop. Key contributions include a new belief framework for object pose\nestimation, an efficient information gain computation strategy, and a robust\nMPC-based control scheme. RUMI demonstrates superior performance in both\nsimulated and real tasks compared to baseline methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "19 pages, 17 figures, submitted to IEEE Transactions on Robotics\n  (T-RO)",
    "pdf_url": "http://arxiv.org/pdf/2408.10450v1",
    "published_date": "2024-08-19 23:16:18 UTC",
    "updated_date": "2024-08-19 23:16:18 UTC"
  },
  {
    "arxiv_id": "2408.10446v1",
    "title": "The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks",
    "authors": [
      "Niyar R Barman",
      "Krish Sharma",
      "Ashhar Aziz",
      "Shashwat Bajpai",
      "Shwetangshu Biswas",
      "Vasu Sharma",
      "Vinija Jain",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ],
    "abstract": "The rapid advancement of text-to-image generation systems, exemplified by\nmodels like Stable Diffusion, Midjourney, Imagen, and DALL-E, has heightened\nconcerns about their potential misuse. In response, companies like Meta and\nGoogle have intensified their efforts to implement watermarking techniques on\nAI-generated images to curb the circulation of potentially misleading visuals.\nHowever, in this paper, we argue that current image watermarking methods are\nfragile and susceptible to being circumvented through visual paraphrase\nattacks. The proposed visual paraphraser operates in two steps. First, it\ngenerates a caption for the given image using KOSMOS-2, one of the latest\nstate-of-the-art image captioning systems. Second, it passes both the original\nimage and the generated caption to an image-to-image diffusion system. During\nthe denoising step of the diffusion pipeline, the system generates a visually\nsimilar image that is guided by the text caption. The resulting image is a\nvisual paraphrase and is free of any watermarks. Our empirical findings\ndemonstrate that visual paraphrase attacks can effectively remove watermarks\nfrom images. This paper provides a critical assessment, empirically revealing\nthe vulnerability of existing watermarking techniques to visual paraphrase\nattacks. While we do not propose solutions to this issue, this paper serves as\na call to action for the scientific community to prioritize the development of\nmore robust watermarking techniques. Our first-of-its-kind visual paraphrase\ndataset and accompanying code are publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages and 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.10446v1",
    "published_date": "2024-08-19 22:58:30 UTC",
    "updated_date": "2024-08-19 22:58:30 UTC"
  },
  {
    "arxiv_id": "2408.10442v1",
    "title": "Feasibility of assessing cognitive impairment via distributed camera network and privacy-preserving edge computing",
    "authors": [
      "Chaitra Hegde",
      "Yashar Kiarashi",
      "Allan I Levey",
      "Amy D Rodriguez",
      "Hyeokhyen Kwon",
      "Gari D Clifford"
    ],
    "abstract": "INTRODUCTION: Mild cognitive impairment (MCI) is characterized by a decline\nin cognitive functions beyond typical age and education-related expectations.\nSince, MCI has been linked to reduced social interactions and increased aimless\nmovements, we aimed to automate the capture of these behaviors to enhance\nlongitudinal monitoring.\n  METHODS: Using a privacy-preserving distributed camera network, we collected\nmovement and social interaction data from groups of individuals with MCI\nundergoing therapy within a 1700$m^2$ space. We developed movement and social\ninteraction features, which were then used to train a series of machine\nlearning algorithms to distinguish between higher and lower cognitive\nfunctioning MCI groups.\n  RESULTS: A Wilcoxon rank-sum test revealed statistically significant\ndifferences between high and low-functioning cohorts in features such as linear\npath length, walking speed, change in direction while walking, entropy of\nvelocity and direction change, and number of group formations in the indoor\nspace. Despite lacking individual identifiers to associate with specific levels\nof MCI, a machine learning approach using the most significant features\nprovided a 71% accuracy.\n  DISCUSSION: We provide evidence to show that a privacy-preserving low-cost\ncamera network using edge computing framework has the potential to distinguish\nbetween different levels of cognitive impairment from the movements and social\ninteractions captured during group activities.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10442v1",
    "published_date": "2024-08-19 22:34:43 UTC",
    "updated_date": "2024-08-19 22:34:43 UTC"
  },
  {
    "arxiv_id": "2408.10437v3",
    "title": "Understanding Generative AI Content with Embedding Models",
    "authors": [
      "Max Vargas",
      "Reilly Cannon",
      "Andrew Engel",
      "Anand D. Sarwate",
      "Tony Chiang"
    ],
    "abstract": "Constructing high-quality features is critical to any quantitative data\nanalysis. While feature engineering was historically addressed by carefully\nhand-crafting data representations based on domain expertise, deep neural\nnetworks (DNNs) now offer a radically different approach. DNNs implicitly\nengineer features by transforming their input data into hidden feature vectors\ncalled embeddings. For embedding vectors produced by foundation models -- which\nare trained to be useful across many contexts -- we demonstrate that simple and\nwell-studied dimensionality-reduction techniques such as Principal Component\nAnalysis uncover inherent heterogeneity in input data concordant with\nhuman-understandable explanations. Of the many applications for this framework,\nwe find empirical evidence that there is intrinsic separability between real\nsamples and those generated by artificial intelligence (AI).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10437v3",
    "published_date": "2024-08-19 22:07:05 UTC",
    "updated_date": "2025-02-22 18:56:49 UTC"
  },
  {
    "arxiv_id": "2408.10428v1",
    "title": "Are LLMs Any Good for High-Level Synthesis?",
    "authors": [
      "Yuchao Liao",
      "Tosiron Adegbija",
      "Roman Lysecky"
    ],
    "abstract": "The increasing complexity and demand for faster, energy-efficient hardware\ndesigns necessitate innovative High-Level Synthesis (HLS) methodologies. This\npaper explores the potential of Large Language Models (LLMs) to streamline or\nreplace the HLS process, leveraging their ability to understand natural\nlanguage specifications and refactor code. We survey the current research and\nconduct experiments comparing Verilog designs generated by a standard HLS tool\n(Vitis HLS) with those produced by LLMs translating C code or natural language\nspecifications. Our evaluation focuses on quantifying the impact on\nperformance, power, and resource utilization, providing an assessment of the\nefficiency of LLM-based approaches. This study aims to illuminate the role of\nLLMs in HLS, identifying promising directions for optimized hardware design in\napplications such as AI acceleration, embedded systems, and high-performance\ncomputing.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "ICCAD '24 Special Session on AI4HLS: New Frontiers in High-Level\n  Synthesis Augmented with Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2408.10428v1",
    "published_date": "2024-08-19 21:40:28 UTC",
    "updated_date": "2024-08-19 21:40:28 UTC"
  },
  {
    "arxiv_id": "2408.10417v1",
    "title": "Development of an AI Anti-Bullying System Using Large Language Model Key Topic Detection",
    "authors": [
      "Matthew Tassava",
      "Cameron Kolodjski",
      "Jordan Milbrath",
      "Adorah Bishop",
      "Nathan Flanders",
      "Robbie Fetsch",
      "Danielle Hanson",
      "Jeremy Straub"
    ],
    "abstract": "This paper presents and evaluates work on the development of an artificial\nintelligence (AI) anti-bullying system. The system is designed to identify\ncoordinated bullying attacks via social media and other mechanisms,\ncharacterize them and propose remediation and response activities to them. In\nparticular, a large language model (LLM) is used to populate an enhanced expert\nsystem-based network model of a bullying attack. This facilitates analysis and\nremediation activity - such as generating report messages to social media\ncompanies - determination. The system is described and the efficacy of the LLM\nfor populating the model is analyzed herein.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10417v1",
    "published_date": "2024-08-19 21:09:31 UTC",
    "updated_date": "2024-08-19 21:09:31 UTC"
  },
  {
    "arxiv_id": "2408.10414v1",
    "title": "Towards Automation of Human Stage of Decay Identification: An Artificial Intelligence Approach",
    "authors": [
      "Anna-Maria Nau",
      "Phillip Ditto",
      "Dawnie Wolfe Steadman",
      "Audris Mockus"
    ],
    "abstract": "Determining the stage of decomposition (SOD) is crucial for estimating the\npostmortem interval and identifying human remains. Currently, labor-intensive\nmanual scoring methods are used for this purpose, but they are subjective and\ndo not scale for the emerging large-scale archival collections of human\ndecomposition photos. This study explores the feasibility of automating two\ncommon human decomposition scoring methods proposed by Megyesi and Gelderman\nusing artificial intelligence (AI). We evaluated two popular deep learning\nmodels, Inception V3 and Xception, by training them on a large dataset of human\ndecomposition images to classify the SOD for different anatomical regions,\nincluding the head, torso, and limbs. Additionally, an interrater study was\nconducted to assess the reliability of the AI models compared to human forensic\nexaminers for SOD identification. The Xception model achieved the best\nclassification performance, with macro-averaged F1 scores of .878, .881, and\n.702 for the head, torso, and limbs when predicting Megyesi's SODs, and .872,\n.875, and .76 for the head, torso, and limbs when predicting Gelderman's SODs.\nThe interrater study results supported AI's ability to determine the SOD at a\nreliability level comparable to a human expert. This work demonstrates the\npotential of AI models trained on a large dataset of human decomposition images\nto automate SOD identification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.10414v1",
    "published_date": "2024-08-19 21:00:40 UTC",
    "updated_date": "2024-08-19 21:00:40 UTC"
  },
  {
    "arxiv_id": "2408.10397v2",
    "title": "Webcam-based Pupil Diameter Prediction Benefits from Upscaling",
    "authors": [
      "Vijul Shah",
      "Brian B. Moser",
      "Ko Watanabe",
      "Andreas Dengel"
    ],
    "abstract": "Capturing pupil diameter is essential for assessing psychological and\nphysiological states such as stress levels and cognitive load. However, the low\nresolution of images in eye datasets often hampers precise measurement. This\nstudy evaluates the impact of various upscaling methods, ranging from bicubic\ninterpolation to advanced super-resolution, on pupil diameter predictions. We\ncompare several pre-trained methods, including CodeFormer, GFPGAN, Real-ESRGAN,\nHAT, and SRResNet. Our findings suggest that pupil diameter prediction models\ntrained on upscaled datasets are highly sensitive to the selected upscaling\nmethod and scale. Our results demonstrate that upscaling methods consistently\nenhance the accuracy of pupil diameter prediction models, highlighting the\nimportance of upscaling in pupilometry. Overall, our work provides valuable\ninsights for selecting upscaling techniques, paving the way for more accurate\nassessments in psychological and physiological research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10397v2",
    "published_date": "2024-08-19 20:28:39 UTC",
    "updated_date": "2024-12-22 19:35:34 UTC"
  },
  {
    "arxiv_id": "2408.10395v1",
    "title": "Evaluating Image-Based Face and Eye Tracking with Event Cameras",
    "authors": [
      "Khadija Iddrisu",
      "Waseem Shariff",
      "Noel E. OConnor",
      "Joseph Lemley",
      "Suzanne Little"
    ],
    "abstract": "Event Cameras, also known as Neuromorphic sensors, capture changes in local\nlight intensity at the pixel level, producing asynchronously generated data\ntermed ``events''. This distinct data format mitigates common issues observed\nin conventional cameras, like under-sampling when capturing fast-moving\nobjects, thereby preserving critical information that might otherwise be lost.\nHowever, leveraging this data often necessitates the development of\nspecialized, handcrafted event representations that can integrate seamlessly\nwith conventional Convolutional Neural Networks (CNNs), considering the unique\nattributes of event data. In this study, We evaluate event-based Face and Eye\ntracking. The core objective of our study is to showcase the viability of\nintegrating conventional algorithms with event-based data, transformed into a\nframe format while preserving the unique benefits of event cameras. To validate\nour approach, we constructed a frame-based event dataset by simulating events\nbetween RGB frames derived from the publicly accessible Helen Dataset. We\nassess its utility for face and eye detection tasks through the application of\nGR-YOLO -- a pioneering technique derived from YOLOv3. This evaluation includes\na comparative analysis with results derived from training the dataset with\nYOLOv8. Subsequently, the trained models were tested on real event streams from\nvarious iterations of Prophesee's event cameras and further evaluated on the\nFaces in Event Stream (FES) benchmark dataset. The models trained on our\ndataset shows a good prediction performance across all the datasets obtained\nfor validation with the best results of a mean Average precision score of 0.91.\nAdditionally, The models trained demonstrated robust performance on real event\ncamera data under varying light conditions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted at The Workshop On Neuromorphic Vision:\n  Advantages and Applications of Event Cameras at the European Conference on\n  Computer Vision (ECCV), 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.10395v1",
    "published_date": "2024-08-19 20:27:08 UTC",
    "updated_date": "2024-08-19 20:27:08 UTC"
  },
  {
    "arxiv_id": "2408.10394v1",
    "title": "Joint Modeling of Search and Recommendations Via an Unified Contextual Recommender (UniCoRn)",
    "authors": [
      "Moumita Bhattacharya",
      "Vito Ostuni",
      "Sudarshan Lamkhede"
    ],
    "abstract": "Search and recommendation systems are essential in many services, and they\nare often developed separately, leading to complex maintenance and technical\ndebt. In this paper, we present a unified deep learning model that efficiently\nhandles key aspects of both tasks.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "3 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2408.10394v1",
    "published_date": "2024-08-19 20:26:45 UTC",
    "updated_date": "2024-08-19 20:26:45 UTC"
  },
  {
    "arxiv_id": "2408.10383v1",
    "title": "BrewCLIP: A Bifurcated Representation Learning Framework for Audio-Visual Retrieval",
    "authors": [
      "Zhenyu Lu",
      "Lakshay Sethi"
    ],
    "abstract": "Previous methods for audio-image matching generally fall into one of two\ncategories: pipeline models or End-to-End models. Pipeline models first\ntranscribe speech and then encode the resulting text; End-to-End models encode\nspeech directly. Generally, pipeline models outperform end-to-end models, but\nthe intermediate transcription necessarily discards some potentially useful\nnon-textual information. In addition to textual information, speech can convey\ndetails such as accent, mood, and and emphasis, which should be effectively\ncaptured in the encoded representation. In this paper, we investigate whether\nnon-textual information, which is overlooked by pipeline-based models, can be\nleveraged to improve speech-image matching performance. We thoroughly analyze\nand compare End-to-End models, pipeline models, and our proposed dual-channel\nmodel for robust audio-image retrieval on a variety of datasets. Our approach\nachieves a substantial performance gain over the previous state-of-the-art by\nleveraging strong pretrained models, a prompting mechanism and a bifurcated\ndesign.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10383v1",
    "published_date": "2024-08-19 19:56:10 UTC",
    "updated_date": "2024-08-19 19:56:10 UTC"
  },
  {
    "arxiv_id": "2408.10381v1",
    "title": "Efficient Reinforcement Learning in Probabilistic Reward Machines",
    "authors": [
      "Xiaofeng Lin",
      "Xuezhou Zhang"
    ],
    "abstract": "In this paper, we study reinforcement learning in Markov Decision Processes\nwith Probabilistic Reward Machines (PRMs), a form of non-Markovian reward\ncommonly found in robotics tasks. We design an algorithm for PRMs that achieves\na regret bound of $\\widetilde{O}(\\sqrt{HOAT} + H^2O^2A^{3/2} + H\\sqrt{T})$,\nwhere $H$ is the time horizon, $O$ is the number of observations, $A$ is the\nnumber of actions, and $T$ is the number of time-steps. This result improves\nover the best-known bound, $\\widetilde{O}(H\\sqrt{OAT})$ of\n\\citet{pmlr-v206-bourel23a} for MDPs with Deterministic Reward Machines (DRMs),\na special case of PRMs. When $T \\geq H^3O^3A^2$ and $OA \\geq H$, our regret\nbound leads to a regret of $\\widetilde{O}(\\sqrt{HOAT})$, which matches the\nestablished lower bound of $\\Omega(\\sqrt{HOAT})$ for MDPs with DRMs up to a\nlogarithmic factor. To the best of our knowledge, this is the first efficient\nalgorithm for PRMs. Additionally, we present a new simulation lemma for\nnon-Markovian rewards, which enables reward-free exploration for any\nnon-Markovian reward given access to an approximate planner. Complementing our\ntheoretical findings, we show through extensive experiment evaluations that our\nalgorithm indeed outperforms prior methods in various PRM environments.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "33 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.10381v1",
    "published_date": "2024-08-19 19:51:53 UTC",
    "updated_date": "2024-08-19 19:51:53 UTC"
  },
  {
    "arxiv_id": "2408.10369v2",
    "title": "Boolean Matrix Logic Programming",
    "authors": [
      "Lun Ai",
      "Stephen H. Muggleton"
    ],
    "abstract": "We describe a datalog query evaluation approach based on efficient and\ncomposable boolean matrix manipulation modules. We first define an overarching\nproblem, Boolean Matrix Logic Programming (BMLP), which uses boolean matrices\nas an alternative computation to evaluate datalog programs. We develop two\nnovel BMLP modules for bottom-up inferences on linear dyadic recursive datalog\nprograms, and show how additional modules can extend this capability to compute\nboth linear and non-linear recursive datalog programs of arity two. Our\nempirical results demonstrate that these modules outperform general-purpose and\nspecialised systems by factors of 30x and 9x, respectively, when evaluating\nlarge programs with millions of facts. This boolean matrix approach\nsignificantly enhances the efficiency of datalog querying to support logic\nprogramming techniques.",
    "categories": [
      "cs.SC",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.SC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10369v2",
    "published_date": "2024-08-19 19:26:49 UTC",
    "updated_date": "2024-08-25 20:06:45 UTC"
  },
  {
    "arxiv_id": "2408.10365v1",
    "title": "AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews",
    "authors": [
      "Keith Tyser",
      "Ben Segev",
      "Gaston Longhitano",
      "Xin-Yu Zhang",
      "Zachary Meeks",
      "Jason Lee",
      "Uday Garg",
      "Nicholas Belsten",
      "Avi Shporer",
      "Madeleine Udell",
      "Dov Te'eni",
      "Iddo Drori"
    ],
    "abstract": "Automatic reviewing helps handle a large volume of papers, provides early\nfeedback and quality control, reduces bias, and allows the analysis of trends.\nWe evaluate the alignment of automatic paper reviews with human reviews using\nan arena of human preferences by pairwise comparisons. Gathering human\npreference may be time-consuming; therefore, we also use an LLM to\nautomatically evaluate reviews to increase sample efficiency while reducing\nbias. In addition to evaluating human and LLM preferences among LLM reviews, we\nfine-tune an LLM to predict human preferences, predicting which reviews humans\nwill prefer in a head-to-head battle between LLMs. We artificially introduce\nerrors into papers and analyze the LLM's responses to identify limitations, use\nadaptive review questions, meta prompting, role-playing, integrate visual and\ntextual analysis, use venue-specific reviewing materials, and predict human\npreferences, improving upon the limitations of the traditional review\nprocesses. We make the reviews of publicly available arXiv and open-access\nNature journal papers available online, along with a free service which helps\nauthors review and revise their research papers and improve their quality. This\nwork develops proof-of-concept LLM reviewing systems that quickly deliver\nconsistent, high-quality reviews and evaluate their quality. We mitigate the\nrisks of misuse, inflated review scores, overconfident ratings, and skewed\nscore distributions by augmenting the LLM with multiple documents, including\nthe review form, reviewer guide, code of ethics and conduct, area chair\nguidelines, and previous year statistics, by finding which errors and\nshortcomings of the paper may be detected by automated reviews, and evaluating\npairwise reviewer preferences. This work identifies and addresses the\nlimitations of using LLMs as reviewers and evaluators and enhances the quality\nof the reviewing process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "42 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.10365v1",
    "published_date": "2024-08-19 19:10:38 UTC",
    "updated_date": "2024-08-19 19:10:38 UTC"
  },
  {
    "arxiv_id": "2408.10362v2",
    "title": "Query languages for neural networks",
    "authors": [
      "Martin Grohe",
      "Christoph Standke",
      "Juno Steegmans",
      "Jan Van den Bussche"
    ],
    "abstract": "We lay the foundations for a database-inspired approach to interpreting and\nunderstanding neural network models by querying them using declarative\nlanguages. Towards this end we study different query languages, based on\nfirst-order logic, that mainly differ in their access to the neural network\nmodel. First-order logic over the reals naturally yields a language which views\nthe network as a black box; only the input--output function defined by the\nnetwork can be queried. This is essentially the approach of constraint query\nlanguages. On the other hand, a white-box language can be obtained by viewing\nthe network as a weighted graph, and extending first-order logic with summation\nover weight terms. The latter approach is essentially an abstraction of SQL. In\ngeneral, the two approaches are incomparable in expressive power, as we will\nshow. Under natural circumstances, however, the white-box approach can subsume\nthe black-box approach; this is our main result. We prove the result concretely\nfor linear constraint queries over real functions definable by feedforward\nneural networks with a fixed number of hidden layers and piecewise linear\nactivation functions.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LO",
      "H.2.3; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear at ICDT 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.10362v2",
    "published_date": "2024-08-19 18:59:52 UTC",
    "updated_date": "2024-08-21 12:50:01 UTC"
  },
  {
    "arxiv_id": "2408.10360v6",
    "title": "HaSPeR: An Image Repository for Hand Shadow Puppet Recognition",
    "authors": [
      "Syed Rifat Raiyan",
      "Zibran Zarif Amio",
      "Sabbir Ahmed"
    ],
    "abstract": "Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of\ntheatrical art and storytelling where hand shadows are projected onto flat\nsurfaces to create illusions of living creatures. The skilled performers create\nthese silhouettes by hand positioning, finger movements, and dexterous gestures\nto resemble shadows of animals and objects. Due to the lack of practitioners\nand a seismic shift in people's entertainment standards, this art form is on\nthe verge of extinction. To facilitate its preservation and proliferate it to a\nwider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset\nconsisting of 15,000 images of hand shadow puppets across 15 classes extracted\nfrom both professional and amateur hand shadow puppeteer clips. We provide a\ndetailed statistical analysis of the dataset and employ a range of pretrained\nimage classification models to establish baselines. Our findings show a\nsubstantial performance superiority of skip-connected convolutional models over\nattention-based transformer architectures. We also find that lightweight\nmodels, such as MobileNetV2, suited for mobile applications and embedded\ndevices, perform comparatively well. We surmise that such low-latency\narchitectures can be useful in developing ombromanie teaching tools, and we\ncreate a prototype application to explore this surmission. Keeping the\nbest-performing model ResNet34 under the limelight, we conduct comprehensive\nfeature-spatial, explainability, and error analyses to gain insights into its\ndecision-making process. To the best of our knowledge, this is the first\ndocumented dataset and research endeavor to preserve this dying art for future\ngenerations, with computer vision approaches. Our code and data will be\npublicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to Image and Vision Computing, 15 pages, 110 figures, 2\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2408.10360v6",
    "published_date": "2024-08-19 18:56:24 UTC",
    "updated_date": "2025-03-31 19:29:48 UTC"
  },
  {
    "arxiv_id": "2408.10351v1",
    "title": "The Psychological Impacts of Algorithmic and AI-Driven Social Media on Teenagers: A Call to Action",
    "authors": [
      "Sunil Arora",
      "Sahil Arora",
      "John D. Hastings"
    ],
    "abstract": "This study investigates the meta-issues surrounding social media, which,\nwhile theoretically designed to enhance social interactions and improve our\nsocial lives by facilitating the sharing of personal experiences and life\nevents, often results in adverse psychological impacts. Our investigation\nreveals a paradoxical outcome: rather than fostering closer relationships and\nimproving social lives, the algorithms and structures that underlie social\nmedia platforms inadvertently contribute to a profound psychological impact on\nindividuals, influencing them in unforeseen ways. This phenomenon is\nparticularly pronounced among teenagers, who are disproportionately affected by\ncurated online personas, peer pressure to present a perfect digital image, and\nthe constant bombardment of notifications and updates that characterize their\nsocial media experience. As such, we issue a call to action for policymakers,\nplatform developers, and educators to prioritize the well-being of teenagers in\nthe digital age and work towards creating secure and safe social media\nplatforms that protect the young from harm, online harassment, and\nexploitation.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "H.5.2; I.2.6; J.4; H.3.5"
    ],
    "primary_category": "cs.SI",
    "comment": "7 pages, 0 figures, 2 tables, 2024 IEEE Conference on Digital\n  Platforms and Societal Harms",
    "pdf_url": "http://arxiv.org/pdf/2408.10351v1",
    "published_date": "2024-08-19 18:49:12 UTC",
    "updated_date": "2024-08-19 18:49:12 UTC"
  },
  {
    "arxiv_id": "2408.14487v3",
    "title": "Active learning of digenic functions with boolean matrix logic programming",
    "authors": [
      "Lun Ai",
      "Stephen H. Muggleton",
      "Shi-shun Liang",
      "Geoff S. Baldwin"
    ],
    "abstract": "We apply logic-based machine learning techniques to facilitate cellular\nengineering and drive biological discovery, based on comprehensive databases of\nmetabolic processes called genome-scale metabolic network models (GEMs).\nPredicted host behaviours are not always correctly described by GEMs. Learning\nthe intricate genetic interactions within GEMs presents computational and\nempirical challenges. To address these, we describe a novel approach called\nBoolean Matrix Logic Programming (BMLP) by leveraging boolean matrices to\nevaluate large logic programs. We introduce a new system, $BMLP_{active}$,\nwhich efficiently explores the genomic hypothesis space by guiding informative\nexperimentation through active learning. In contrast to sub-symbolic methods,\n$BMLP_{active}$ encodes a state-of-the-art GEM of a widely accepted bacterial\nhost in an interpretable and logical representation using datalog logic\nprograms. Notably, $BMLP_{active}$ can successfully learn the interaction\nbetween a gene pair with fewer training examples than random experimentation,\novercoming the increase in experimental design space. $BMLP_{active}$ enables\nrapid optimisation of metabolic models and offers a realistic approach to a\nself-driving lab for microbial engineering.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SC",
      "q-bio.MN"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2405.06724",
    "pdf_url": "http://arxiv.org/pdf/2408.14487v3",
    "published_date": "2024-08-19 18:47:07 UTC",
    "updated_date": "2024-11-13 10:09:23 UTC"
  },
  {
    "arxiv_id": "2408.10343v1",
    "title": "LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain",
    "authors": [
      "Nicholas Pipitone",
      "Ghita Houir Alami"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems are showing promising potential,\nand are becoming increasingly relevant in AI-powered legal applications.\nExisting benchmarks, such as LegalBench, assess the generative capabilities of\nLarge Language Models (LLMs) in the legal domain, but there is a critical gap\nin evaluating the retrieval component of RAG systems. To address this, we\nintroduce LegalBench-RAG, the first benchmark specifically designed to evaluate\nthe retrieval step of RAG pipelines within the legal space. LegalBench-RAG\nemphasizes precise retrieval by focusing on extracting minimal, highly relevant\ntext segments from legal documents. These highly relevant snippets are\npreferred over retrieving document IDs, or large sequences of imprecise chunks,\nboth of which can exceed context window limitations. Long context windows cost\nmore to process, induce higher latency, and lead LLMs to forget or hallucinate\ninformation. Additionally, precise results allow LLMs to generate citations for\nthe end user. The LegalBench-RAG benchmark is constructed by retracing the\ncontext used in LegalBench queries back to their original locations within the\nlegal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus\nof over 79M characters, entirely human-annotated by legal experts. We also\nintroduce LegalBench-RAG-mini, a lightweight version for rapid iteration and\nexperimentation. By providing a dedicated benchmark for legal retrieval,\nLegalBench-RAG serves as a critical tool for companies and researchers focused\non enhancing the accuracy and performance of RAG systems in the legal domain.\nThe LegalBench-RAG dataset is publicly available at\nhttps://github.com/zeroentropy-cc/legalbenchrag.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10343v1",
    "published_date": "2024-08-19 18:30:18 UTC",
    "updated_date": "2024-08-19 18:30:18 UTC"
  },
  {
    "arxiv_id": "2408.10334v1",
    "title": "A Disguised Wolf Is More Harmful Than a Toothless Tiger: Adaptive Malicious Code Injection Backdoor Attack Leveraging User Behavior as Triggers",
    "authors": [
      "Shangxi Wu",
      "Jitao Sang"
    ],
    "abstract": "In recent years, large language models (LLMs) have made significant progress\nin the field of code generation. However, as more and more users rely on these\nmodels for software development, the security risks associated with code\ngeneration models have become increasingly significant. Studies have shown that\ntraditional deep learning robustness issues also negatively impact the field of\ncode generation. In this paper, we first present the game-theoretic model that\nfocuses on security issues in code generation scenarios. This framework\noutlines possible scenarios and patterns where attackers could spread malicious\ncode models to create security threats. We also pointed out for the first time\nthat the attackers can use backdoor attacks to dynamically adjust the timing of\nmalicious code injection, which will release varying degrees of malicious code\ndepending on the skill level of the user. Through extensive experiments on\nleading code generation models, we validate our proposed game-theoretic model\nand highlight the significant threats that these new attack scenarios pose to\nthe safe use of code models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10334v1",
    "published_date": "2024-08-19 18:18:04 UTC",
    "updated_date": "2024-08-19 18:18:04 UTC"
  },
  {
    "arxiv_id": "2408.10328v1",
    "title": "Decoding Human Emotions: Analyzing Multi-Channel EEG Data using LSTM Networks",
    "authors": [
      "Shyam K Sateesh",
      "Sparsh BK",
      "Uma D"
    ],
    "abstract": "Emotion recognition from electroencephalogram (EEG) signals is a thriving\nfield, particularly in neuroscience and Human-Computer Interaction (HCI). This\nstudy aims to understand and improve the predictive accuracy of emotional state\nclassification through metrics such as valence, arousal, dominance, and\nlikeness by applying a Long Short-Term Memory (LSTM) network to analyze EEG\nsignals. Using a popular dataset of multi-channel EEG recordings known as DEAP,\nwe look towards leveraging LSTM networks' properties to handle temporal\ndependencies within EEG signal data. This allows for a more comprehensive\nunderstanding and classification of emotional parameter states. We obtain\naccuracies of 89.89%, 90.33%, 90.70%, and 90.54% for arousal, valence,\ndominance, and likeness, respectively, demonstrating significant improvements\nin emotion recognition model capabilities. This paper elucidates the\nmethodology and architectural specifics of our LSTM model and provides a\nbenchmark analysis with existing papers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 3 figures; accepted at ICDSA '24 Conference, Jaipur, India",
    "pdf_url": "http://arxiv.org/pdf/2408.10328v1",
    "published_date": "2024-08-19 18:10:47 UTC",
    "updated_date": "2024-08-19 18:10:47 UTC"
  },
  {
    "arxiv_id": "2408.10205v1",
    "title": "KAN 2.0: Kolmogorov-Arnold Networks Meet Science",
    "authors": [
      "Ziming Liu",
      "Pingchuan Ma",
      "Yixuan Wang",
      "Wojciech Matusik",
      "Max Tegmark"
    ],
    "abstract": "A major challenge of AI + Science lies in their inherent incompatibility:\ntoday's AI is primarily based on connectionism, while science depends on\nsymbolism. To bridge the two worlds, we propose a framework to seamlessly\nsynergize Kolmogorov-Arnold Networks (KANs) and science. The framework\nhighlights KANs' usage for three aspects of scientific discovery: identifying\nrelevant features, revealing modular structures, and discovering symbolic\nformulas. The synergy is bidirectional: science to KAN (incorporating\nscientific knowledge into KANs), and KAN to science (extracting scientific\ninsights from KANs). We highlight major new functionalities in the pykan\npackage: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN\ncompiler that compiles symbolic formulas into KANs. (3) tree converter: convert\nKANs (or any neural networks) to tree graphs. Based on these tools, we\ndemonstrate KANs' capability to discover various types of physical laws,\nincluding conserved quantities, Lagrangians, symmetries, and constitutive laws.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph",
      "physics.data-an"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.10205v1",
    "published_date": "2024-08-19 17:59:04 UTC",
    "updated_date": "2024-08-19 17:59:04 UTC"
  },
  {
    "arxiv_id": "2408.10841v1",
    "title": "DELIA: Diversity-Enhanced Learning for Instruction Adaptation in Large Language Models",
    "authors": [
      "Yuanhao Zeng",
      "Fei Ren",
      "Xinpeng Zhou",
      "Yihang Wang",
      "Yingxia Shao"
    ],
    "abstract": "Although instruction tuning is widely used to adjust behavior in Large\nLanguage Models (LLMs), extensive empirical evidence and research indicates\nthat it is primarily a process where the model fits to specific task formats,\nrather than acquiring new knowledge or capabilities. We propose that this\nlimitation stems from biased features learned during instruction tuning, which\ndiffer from ideal task-specfic features, leading to learn less underlying\nsemantics in downstream tasks. However, ideal features are unknown and\nincalculable, constraining past work to rely on prior knowledge to assist\nreasoning or training, which limits LLMs' capabilities to the developers'\nabilities, rather than data-driven scalable learning. In our paper, through our\nnovel data synthesis method, DELIA (Diversity-Enhanced Learning for Instruction\nAdaptation), we leverage the buffering effect of extensive diverse data in LLMs\ntraining to transform biased features in instruction tuning into approximations\nof ideal features, without explicit prior ideal features. Experiments show\nDELIA's better performance compared to common instruction tuning and other\nbaselines. It outperforms common instruction tuning by 17.07%-33.41% on\nIcelandic-English translation bleurt score (WMT-21 dataset, gemma-7b-it) and\nimproves accuracy by 36.1% on formatted text generation (Llama2-7b-chat).\nNotably, among knowledge injection methods we've known, DELIA uniquely align\nthe internal representations of new special tokens with their prior semantics.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.10841v1",
    "published_date": "2024-08-19 17:56:06 UTC",
    "updated_date": "2024-08-19 17:56:06 UTC"
  },
  {
    "arxiv_id": "2408.10197v1",
    "title": "Demystifying the Communication Characteristics for Distributed Transformer Models",
    "authors": [
      "Quentin Anthony",
      "Benjamin Michalowicz",
      "Jacob Hatef",
      "Lang Xu",
      "Mustafa Abduljabbar",
      "Aamir Shafi",
      "Hari Subramoni",
      "Dhabaleswar Panda"
    ],
    "abstract": "Deep learning (DL) models based on the transformer architecture have\nrevolutionized many DL applications such as large language models (LLMs),\nvision transformers, audio generation, and time series prediction. Much of this\nprogress has been fueled by distributed training, yet distributed communication\nremains a substantial bottleneck to training progress. This paper examines the\ncommunication behavior of transformer models - that is, how different\nparallelism schemes used in multi-node/multi-GPU DL Training communicate data\nin the context of transformers. We use GPT-based language models as a case\nstudy of the transformer architecture due to their ubiquity. We validate the\nempirical results obtained from our communication logs using analytical models.\nAt a high level, our analysis reveals a need to optimize small message\npoint-to-point communication further, correlations between sequence length,\nper-GPU throughput, model size, and optimizations used, and where to\npotentially guide further optimizations in framework and HPC middleware design\nand optimization.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10197v1",
    "published_date": "2024-08-19 17:54:29 UTC",
    "updated_date": "2024-08-19 17:54:29 UTC"
  },
  {
    "arxiv_id": "2408.10195v1",
    "title": "SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views",
    "authors": [
      "Chao Xu",
      "Ang Li",
      "Linghao Chen",
      "Yulin Liu",
      "Ruoxi Shi",
      "Hao Su",
      "Minghua Liu"
    ],
    "abstract": "Open-world 3D generation has recently attracted considerable attention. While\nmany single-image-to-3D methods have yielded visually appealing outcomes, they\noften lack sufficient controllability and tend to produce hallucinated regions\nthat may not align with users' expectations. In this paper, we explore an\nimportant scenario in which the input consists of one or a few unposed 2D\nimages of a single object, with little or no overlap. We propose a novel\nmethod, SpaRP, to reconstruct a 3D textured mesh and estimate the relative\ncamera poses for these sparse-view images. SpaRP distills knowledge from 2D\ndiffusion models and finetunes them to implicitly deduce the 3D spatial\nrelationships between the sparse views. The diffusion model is trained to\njointly predict surrogate representations for camera poses and multi-view\nimages of the object under known poses, integrating all information from the\ninput sparse views. These predictions are then leveraged to accomplish 3D\nreconstruction and pose estimation, and the reconstructed 3D model can be used\nto further refine the camera poses of input views. Through extensive\nexperiments on three datasets, we demonstrate that our method not only\nsignificantly outperforms baseline methods in terms of 3D reconstruction\nquality and pose prediction accuracy but also exhibits strong efficiency. It\nrequires only about 20 seconds to produce a textured mesh and camera poses for\nthe input views. Project page: https://chaoxu.xyz/sparp.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.10195v1",
    "published_date": "2024-08-19 17:53:10 UTC",
    "updated_date": "2024-08-19 17:53:10 UTC"
  },
  {
    "arxiv_id": "2408.10189v2",
    "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models",
    "authors": [
      "Aviv Bick",
      "Kevin Y. Li",
      "Eric P. Xing",
      "J. Zico Kolter",
      "Albert Gu"
    ],
    "abstract": "Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10189v2",
    "published_date": "2024-08-19 17:48:11 UTC",
    "updated_date": "2025-02-08 20:53:16 UTC"
  },
  {
    "arxiv_id": "2408.10181v1",
    "title": "Imbalance-Aware Culvert-Sewer Defect Segmentation Using an Enhanced Feature Pyramid Network",
    "authors": [
      "Rasha Alshawi",
      "Md Meftahul Ferdaus",
      "Mahdi Abdelguerfi",
      "Kendall Niles",
      "Ken Pathak",
      "Steve Sloan"
    ],
    "abstract": "Imbalanced datasets are a significant challenge in real-world scenarios. They\nlead to models that underperform on underrepresented classes, which is a\ncritical issue in infrastructure inspection. This paper introduces the Enhanced\nFeature Pyramid Network (E-FPN), a deep learning model for the semantic\nsegmentation of culverts and sewer pipes within imbalanced datasets. The E-FPN\nincorporates architectural innovations like sparsely connected blocks and\ndepth-wise separable convolutions to improve feature extraction and handle\nobject variations. To address dataset imbalance, the model employs strategies\nlike class decomposition and data augmentation. Experimental results on the\nculvert-sewer defects dataset and a benchmark aerial semantic segmentation\ndrone dataset show that the E-FPN outperforms state-of-the-art methods,\nachieving an average Intersection over Union (IoU) improvement of 13.8% and\n27.2%, respectively. Additionally, class decomposition and data augmentation\ntogether boost the model's performance by approximately 6.9% IoU. The proposed\nE-FPN presents a promising solution for enhancing object segmentation in\nchallenging, multi-class real-world datasets, with potential applications\nextending beyond culvert-sewer defect detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10181v1",
    "published_date": "2024-08-19 17:40:18 UTC",
    "updated_date": "2024-08-19 17:40:18 UTC"
  },
  {
    "arxiv_id": "2408.10178v2",
    "title": "NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction",
    "authors": [
      "Yifan Wang",
      "Di Huang",
      "Weicai Ye",
      "Guofeng Zhang",
      "Wanli Ouyang",
      "Tong He"
    ],
    "abstract": "Signed Distance Function (SDF)-based volume rendering has demonstrated\nsignificant capabilities in surface reconstruction. Although promising,\nSDF-based methods often fail to capture detailed geometric structures,\nresulting in visible defects. By comparing SDF-based volume rendering to\ndensity-based volume rendering, we identify two main factors within the\nSDF-based approach that degrade surface quality: SDF-to-density representation\nand geometric regularization. These factors introduce challenges that hinder\nthe optimization of the SDF field. To address these issues, we introduce\nNeuRodin, a novel two-stage neural surface reconstruction framework that not\nonly achieves high-fidelity surface reconstruction but also retains the\nflexible optimization characteristics of density-based methods. NeuRodin\nincorporates innovative strategies that facilitate transformation of arbitrary\ntopologies and reduce artifacts associated with density bias. Extensive\nevaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the\nsuperiority of NeuRodin, showing strong reconstruction capabilities for both\nindoor and outdoor environments using solely posed RGB captures. Project\nwebsite: https://open3dvlab.github.io/NeuRodin/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10178v2",
    "published_date": "2024-08-19 17:36:35 UTC",
    "updated_date": "2024-12-22 07:24:09 UTC"
  },
  {
    "arxiv_id": "2408.10175v1",
    "title": "Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition",
    "authors": [
      "Rafael M. Mamede",
      "Pedro C. Neto",
      "Ana F. Sequeira"
    ],
    "abstract": "This study investigates the effects of occlusions on the fairness of face\nrecognition systems, particularly focusing on demographic biases. Using the\nRacial Faces in the Wild (RFW) dataset and synthetically added realistic\nocclusions, we evaluate their effect on the performance of face recognition\nmodels trained on the BUPT-Balanced and BUPT-GlobalFace datasets. We note\nincreases in the dispersion of FMR, FNMR, and accuracy alongside decreases in\nfairness according to Equilized Odds, Demographic Parity, STD of Accuracy, and\nFairness Discrepancy Rate. Additionally, we utilize a pixel attribution method\nto understand the importance of occlusions in model predictions, proposing a\nnew metric, Face Occlusion Impact Ratio (FOIR), that quantifies the extent to\nwhich occlusions affect model performance across different demographic groups.\nOur results indicate that occlusions exacerbate existing demographic biases,\nwith models placing higher importance on occlusions in an unequal fashion,\nparticularly affecting African individuals more severely.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV Workshop FAILED",
    "pdf_url": "http://arxiv.org/pdf/2408.10175v1",
    "published_date": "2024-08-19 17:34:19 UTC",
    "updated_date": "2024-08-19 17:34:19 UTC"
  },
  {
    "arxiv_id": "2408.10174v2",
    "title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models",
    "authors": [
      "Anke Tang",
      "Li Shen",
      "Yong Luo",
      "Shuai Xie",
      "Han Hu",
      "Lefei Zhang",
      "Bo Du",
      "Dacheng Tao"
    ],
    "abstract": "Deep model training on extensive datasets is increasingly becoming\ncost-prohibitive, prompting the widespread adoption of deep model fusion\ntechniques to leverage knowledge from pre-existing models. From simple weight\naveraging to more sophisticated methods like AdaMerging, model fusion\neffectively improves model performance and accelerates the development of new\nmodels. However, potential interference between parameters of individual models\nand the lack of interpretability in the fusion progress remain significant\nchallenges. Existing methods often try to resolve the parameter interference\nissue by evaluating attributes of parameters, such as their magnitude or sign,\nor by parameter pruning. In this study, we begin by examining the fine-tuning\nof linear layers through the lens of subspace analysis and explicitly define\nparameter interference as an optimization problem to shed light on this\nsubject. Subsequently, we introduce an innovative approach to model fusion\ncalled zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which\nallows for the upscaling of source models into an MoE model without extra data\nor further training. Our approach relies on the observation that fine-tuning\nmostly keeps the important parts from the pre-training, but it uses less\nsignificant or unused areas to adapt to new tasks. Also, the issue of parameter\ninterference, which is intrinsically intractable in the original parameter\nspace, can be managed by expanding the dimensions. We conduct extensive\nexperiments across diverse scenarios, such as image classification and text\ngeneration tasks, using full fine-tuning and LoRA fine-tuning, and we apply our\nmethod to large language models (CLIP models, Flan-T5 models, and Mistral-7B\nmodels), highlighting the adaptability and scalability of SMILE. Code is\navailable at https://github.com/tanganke/fusion_bench",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at https://github.com/tanganke/fusion_bench",
    "pdf_url": "http://arxiv.org/pdf/2408.10174v2",
    "published_date": "2024-08-19 17:32:15 UTC",
    "updated_date": "2024-08-26 07:34:46 UTC"
  },
  {
    "arxiv_id": "2408.10161v2",
    "title": "NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices",
    "authors": [
      "Zhiyong Zhang",
      "Aniket Gupta",
      "Huaizu Jiang",
      "Hanumant Singh"
    ],
    "abstract": "Real-time high-accuracy optical flow estimation is crucial for various\nreal-world applications. While recent learning-based optical flow methods have\nachieved high accuracy, they often come with significant computational costs.\nIn this paper, we propose a highly efficient optical flow method that balances\nhigh accuracy with reduced computational demands. Building upon NeuFlow v1, we\nintroduce new components including a much more light-weight backbone and a fast\nrefinement module. Both these modules help in keeping the computational demands\nlight while providing close to state of the art accuracy. Compares to other\nstate of the art methods, our model achieves a 10x-70x speedup while\nmaintaining comparable performance on both synthetic and real-world data. It is\ncapable of running at over 20 FPS on 512x384 resolution images on a Jetson Orin\nNano. The full training and evaluation code is available at\nhttps://github.com/neufieldrobotics/NeuFlow_v2.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10161v2",
    "published_date": "2024-08-19 17:13:34 UTC",
    "updated_date": "2024-08-21 23:23:10 UTC"
  },
  {
    "arxiv_id": "2408.10159v4",
    "title": "Customizing Language Models with Instance-wise LoRA for Sequential Recommendation",
    "authors": [
      "Xiaoyu Kong",
      "Jiancan Wu",
      "An Zhang",
      "Leheng Sheng",
      "Hui Lin",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "abstract": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "NeurIPS 2024 poster",
    "pdf_url": "http://arxiv.org/pdf/2408.10159v4",
    "published_date": "2024-08-19 17:09:32 UTC",
    "updated_date": "2025-01-21 03:40:14 UTC"
  },
  {
    "arxiv_id": "2408.10292v1",
    "title": "Leveraging Superfluous Information in Contrastive Representation Learning",
    "authors": [
      "Xuechu Yu"
    ],
    "abstract": "Contrastive representation learning, which aims to learnthe shared\ninformation between different views of unlabeled data by maximizing the mutual\ninformation between them, has shown its powerful competence in self-supervised\nlearning for downstream tasks. However, recent works have demonstrated that\nmore estimated mutual information does not guarantee better performance in\ndifferent downstream tasks. Such works inspire us to conjecture that the\nlearned representations not only maintain task-relevant information from\nunlabeled data but also carry task-irrelevant information which is superfluous\nfor downstream tasks, thus leading to performance degeneration. In this paper\nwe show that superfluous information does exist during the conventional\ncontrastive learning framework, and further design a new objective, namely\nSuperInfo, to learn robust representations by a linear combination of both\npredictive and superfluous information. Besides, we notice that it is feasible\nto tune the coefficients of introduced losses to discard task-irrelevant\ninformation, while keeping partial non-shared task-relevant information\naccording to our SuperInfo loss.We demonstrate that learning with our loss can\noften outperform the traditional contrastive learning approaches on image\nclassification, object detection and instance segmentation tasks with\nsignificant improvements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10292v1",
    "published_date": "2024-08-19 16:21:08 UTC",
    "updated_date": "2024-08-19 16:21:08 UTC"
  },
  {
    "arxiv_id": "2408.10130v1",
    "title": "Rhyme-aware Chinese lyric generator based on GPT",
    "authors": [
      "Yixiao Yuan",
      "Yangchen Huang",
      "Yu Ma",
      "Xinjin Li",
      "Zhenglin Li",
      "Yiming Shi",
      "Huapeng Zhou"
    ],
    "abstract": "Neural language representation models such as GPT, pre-trained on large-scale\ncorpora, can effectively capture rich semantic patterns from plain text and be\nfine-tuned to consistently improve natural language generation performance.\nHowever, existing pre-trained language models used to generate lyrics rarely\nconsider rhyme information, which is crucial in lyrics. Using a pre-trained\nmodel directly results in poor performance. To enhance the rhyming quality of\ngenerated lyrics, we incorporate integrated rhyme information into our model,\nthereby improving lyric generation performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10130v1",
    "published_date": "2024-08-19 16:17:20 UTC",
    "updated_date": "2024-08-19 16:17:20 UTC"
  },
  {
    "arxiv_id": "2408.10128v2",
    "title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a Low-Resource Language",
    "authors": [
      "Manjil Karki",
      "Pratik Shakya",
      "Sandesh Acharya",
      "Ravi Pandit",
      "Dinesh Gothe"
    ],
    "abstract": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "91F20",
      "I.2.7"
    ],
    "primary_category": "cs.SD",
    "comment": "6 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.10128v2",
    "published_date": "2024-08-19 16:15:09 UTC",
    "updated_date": "2024-08-23 16:15:30 UTC"
  },
  {
    "arxiv_id": "2408.10126v2",
    "title": "Learning Brave Assumption-Based Argumentation Frameworks via ASP",
    "authors": [
      "Emanuele De Angelis",
      "Maurizio Proietti",
      "Francesca Toni"
    ],
    "abstract": "Assumption-based Argumentation (ABA) is advocated as a unifying formalism for\nvarious forms of non-monotonic reasoning, including logic programming. It\nallows capturing defeasible knowledge, subject to argumentative debate. While,\nin much existing work, ABA frameworks are given up-front, in this paper we\nfocus on the problem of automating their learning from background knowledge and\npositive/negative examples. Unlike prior work, we newly frame the problem in\nterms of brave reasoning under stable extensions for ABA. We present a novel\nalgorithm based on transformation rules (such as Rote Learning, Folding,\nAssumption Introduction and Fact Subsumption) and an implementation thereof\nthat makes use of Answer Set Programming. Finally, we compare our technique to\nstate-of-the-art ILP systems that learn defeasible knowledge.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of the paper published in: Proceedings 27th European\n  Conference on Artificial Intelligence, Frontiers in Artificial Intelligence\n  and Applications, Volume 392: ECAI 2024, pp. 3445 - 3452. DOI:\n  10.3233/FAIA240896",
    "pdf_url": "http://arxiv.org/pdf/2408.10126v2",
    "published_date": "2024-08-19 16:13:35 UTC",
    "updated_date": "2024-11-08 11:30:20 UTC"
  },
  {
    "arxiv_id": "2408.10124v1",
    "title": "Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models",
    "authors": [
      "Tianyu Zhang",
      "Yuxiang Ren",
      "Chengbin Hou",
      "Hairong Lv",
      "Xuegong Zhang"
    ],
    "abstract": "Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10124v1",
    "published_date": "2024-08-19 16:11:59 UTC",
    "updated_date": "2024-08-19 16:11:59 UTC"
  },
  {
    "arxiv_id": "2408.10120v1",
    "title": "Geometry Informed Tokenization of Molecules for Language Model Generation",
    "authors": [
      "Xiner Li",
      "Limei Wang",
      "Youzhi Luo",
      "Carl Edwards",
      "Shurui Gui",
      "Yuchao Lin",
      "Heng Ji",
      "Shuiwang Ji"
    ],
    "abstract": "We consider molecule generation in 3D space using language models (LMs),\nwhich requires discrete tokenization of 3D molecular geometries. Although\ntokenization of molecular graphs exists, that for 3D geometries is largely\nunexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which\nconverts molecular geometries into $SE(3)$-invariant 1D discrete sequences.\nGeo2Seq consists of canonical labeling and invariant spherical representation\nsteps, which together maintain geometric and atomic fidelity in a format\nconducive to LMs. Our experiments show that, when coupled with Geo2Seq, various\nLMs excel in molecular geometry generation, especially in controlled generation\ntasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10120v1",
    "published_date": "2024-08-19 16:09:59 UTC",
    "updated_date": "2024-08-19 16:09:59 UTC"
  },
  {
    "arxiv_id": "2408.10119v1",
    "title": "Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data",
    "authors": [
      "Tao Yang",
      "Yangming Shi",
      "Yunwen Huang",
      "Feng Chen",
      "Yin Zheng",
      "Lei Zhang"
    ],
    "abstract": "Text-to-video (T2V) generation has gained significant attention due to its\nwide applications to video generation, editing, enhancement and translation,\n\\etc. However, high-quality (HQ) video synthesis is extremely challenging\nbecause of the diverse and complex motions existed in real world. Most existing\nworks struggle to address this problem by collecting large-scale HQ videos,\nwhich are inaccessible to the community. In this work, we show that publicly\navailable limited and low-quality (LQ) data are sufficient to train a HQ video\ngenerator without recaptioning or finetuning. We factorize the whole T2V\ngeneration process into two steps: generating an image conditioned on a highly\ndescriptive caption, and synthesizing the video conditioned on the generated\nimage and a concise caption of motion details. Specifically, we present\n\\emph{Factorized-Dreamer}, a factorized spatiotemporal framework with several\ncritical designs for T2V generation, including an adapter to combine text and\nimage embeddings, a pixel-aware cross attention module to capture pixel-level\nimage information, a T5 text encoder to better understand motion description,\nand a PredictNet to supervise optical flows. We further present a noise\nschedule, which plays a key role in ensuring the quality and stability of video\ngeneration. Our model lowers the requirements in detailed captions and HQ\nvideos, and can be directly trained on limited LQ datasets with noisy and brief\ncaptions such as WebVid-10M, largely alleviating the cost to collect\nlarge-scale HQ video-text pairs. Extensive experiments in a variety of T2V and\nimage-to-video generation tasks demonstrate the effectiveness of our proposed\nFactorized-Dreamer. Our source codes are available at\n\\url{https://github.com/yangxy/Factorized-Dreamer/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10119v1",
    "published_date": "2024-08-19 16:08:00 UTC",
    "updated_date": "2024-08-19 16:08:00 UTC"
  },
  {
    "arxiv_id": "2408.10113v1",
    "title": "Enhancing Reinforcement Learning Through Guided Search",
    "authors": [
      "Jérôme Arjonilla",
      "Abdallah Saffidine",
      "Tristan Cazenave"
    ],
    "abstract": "With the aim of improving performance in Markov Decision Problem in an\nOff-Policy setting, we suggest taking inspiration from what is done in Offline\nReinforcement Learning (RL). In Offline RL, it is a common practice during\npolicy learning to maintain proximity to a reference policy to mitigate\nuncertainty, reduce potential policy errors, and help improve performance. We\nfind ourselves in a different setting, yet it raises questions about whether a\nsimilar concept can be applied to enhance performance ie, whether it is\npossible to find a guiding policy capable of contributing to performance\nimprovement, and how to incorporate it into our RL agent. Our attention is\nparticularly focused on algorithms based on Monte Carlo Tree Search (MCTS) as a\nguide.MCTS renowned for its state-of-the-art capabilities across various\ndomains, catches our interest due to its ability to converge to equilibrium in\nsingle-player and two-player contexts. By harnessing the power of MCTS as a\nguide for our RL agent, we observed a significant performance improvement,\nsurpassing the outcomes achieved by utilizing each method in isolation. Our\nexperiments were carried out on the Atari 100k benchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted Paper at ECAI 2024; Extended Version",
    "pdf_url": "http://arxiv.org/pdf/2408.10113v1",
    "published_date": "2024-08-19 16:00:02 UTC",
    "updated_date": "2024-08-19 16:00:02 UTC"
  },
  {
    "arxiv_id": "2408.10111v2",
    "title": "PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities",
    "authors": [
      "Yuanjian Xu",
      "Anxian Liu",
      "Jianing Hao",
      "Zhenzhuo Li",
      "Shichang Meng",
      "Guang Zhang"
    ],
    "abstract": "Financial time series modeling is crucial for understanding and predicting\nmarket behaviors but faces challenges such as non-linearity, non-stationarity,\nand high noise levels. Traditional models struggle to capture complex patterns\ndue to these issues, compounded by limitations in computational resources and\nmodel capacity. Inspired by the success of large language models in NLP, we\nintroduce $\\textbf{PLUTUS}$, a $\\textbf{P}$re-trained $\\textbf{L}$arge\n$\\textbf{U}$nified $\\textbf{T}$ransformer-based model that $\\textbf{U}$nveils\nregularities in financial time $\\textbf{S}$eries. PLUTUS uses an invertible\nembedding module with contrastive learning and autoencoder techniques to create\nan approximate one-to-one mapping between raw data and patch embeddings.\nTimeFormer, an attention based architecture, forms the core of PLUTUS,\neffectively modeling high-noise time series. We incorporate a novel attention\nmechanisms to capture features across both variable and temporal dimensions.\nPLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,\ndesigned to thrive in noisy financial environments. To our knowledge, PLUTUS is\nthe first open-source, large-scale, pre-trained financial time series model\nwith over one billion parameters. It achieves state-of-the-art performance in\nvarious tasks, demonstrating strong transferability and establishing a robust\nfoundational model for finance. Our research provides technical guidance for\npre-training financial time series data, setting a new standard in the field.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10111v2",
    "published_date": "2024-08-19 15:59:46 UTC",
    "updated_date": "2024-08-20 02:59:16 UTC"
  },
  {
    "arxiv_id": "2408.10108v1",
    "title": "Envisioning Possibilities and Challenges of AI for Personalized Cancer Care",
    "authors": [
      "Elaine Kong",
      "Kuo-Ting",
      "Huang",
      "Aakash Gautam"
    ],
    "abstract": "The use of Artificial Intelligence (AI) in healthcare, including in caring\nfor cancer survivors, has gained significant interest. However, gaps remain in\nour understanding of how such AI systems can provide care, especially for\nethnic and racial minority groups who continue to face care disparities.\nThrough interviews with six cancer survivors, we identify critical gaps in\ncurrent healthcare systems such as a lack of personalized care and insufficient\ncultural and linguistic accommodation. AI, when applied to care, was seen as a\nway to address these issues by enabling real-time, culturally aligned, and\nlinguistically appropriate interactions. We also uncovered concerns about the\nimplications of AI-driven personalization, such as data privacy, loss of human\ntouch in caregiving, and the risk of echo chambers that limit exposure to\ndiverse information. We conclude by discussing the trade-offs between\nAI-enhanced personalization and the need for structural changes in healthcare\nthat go beyond technological solutions, leading us to argue that we should\nbegin by asking, ``Why personalization?''",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages, 1 table, short paper at CSCW 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.10108v1",
    "published_date": "2024-08-19 15:55:46 UTC",
    "updated_date": "2024-08-19 15:55:46 UTC"
  },
  {
    "arxiv_id": "2408.10107v1",
    "title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments",
    "authors": [
      "Heeyoung Lee",
      "Hoyoon Byun",
      "Changdae Oh",
      "JinYeong Bak",
      "Kyungwoo Song"
    ],
    "abstract": "Accessing machine learning models through remote APIs has been gaining\nprevalence following the recent trend of scaling up model parameters for\nincreased performance. Even though these models exhibit remarkable ability,\ndetecting out-of-distribution (OOD) samples remains a crucial safety concern\nfor end users as these samples may induce unreliable outputs from the model. In\nthis work, we propose an OOD detection framework, MixDiff, that is applicable\neven when the model's parameters or its activations are not accessible to the\nend user. To bypass the access restriction, MixDiff applies an identical\ninput-level perturbation to a given target sample and a similar in-distribution\n(ID) sample, then compares the relative difference in the model outputs of\nthese two samples. MixDiff is model-agnostic and compatible with existing\noutput-based OOD detection methods. We provide theoretical analysis to\nillustrate MixDiff's effectiveness in discerning OOD samples that induce\noverconfident outputs from the model and empirically demonstrate that MixDiff\nconsistently enhances the OOD detection performance on various datasets in\nvision and text domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to European Conference on Artificial Intelligence (ECAI)\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2408.10107v1",
    "published_date": "2024-08-19 15:51:31 UTC",
    "updated_date": "2024-08-19 15:51:31 UTC"
  },
  {
    "arxiv_id": "2408.10096v2",
    "title": "Convert and Speak: Zero-shot Accent Conversion with Minimum Supervision",
    "authors": [
      "Zhijun Jia",
      "Huaying Xue",
      "Xiulian Peng",
      "Yan Lu"
    ],
    "abstract": "Low resource of parallel data is the key challenge of accent conversion(AC)\nproblem in which both the pronunciation units and prosody pattern need to be\nconverted. We propose a two-stage generative framework \"convert-and-speak\" in\nwhich the conversion is only operated on the semantic token level and the\nspeech is synthesized conditioned on the converted semantic token with a speech\ngenerative model in target accent domain. The decoupling design enables the\n\"speaking\" module to use massive amount of target accent speech and relieves\nthe parallel data required for the \"conversion\" module. Conversion with the\nbridge of semantic token also relieves the requirement for the data with text\ntranscriptions and unlocks the usage of language pre-training technology to\nfurther efficiently reduce the need of parallel accent speech data. To reduce\nthe complexity and latency of \"speaking\", a single-stage AR generative model is\ndesigned to achieve good quality as well as lower computation cost. Experiments\non Indian-English to general American-English conversion show that the proposed\nframework achieves state-of-the-art performance in accent similarity, speech\nquality, and speaker maintenance with only 15 minutes of weakly parallel data\nwhich is not constrained to the same speaker. Extensive experimentation with\ndiverse accent types suggests that this framework possesses a high degree of\nadaptability, making it readily scalable to accommodate other accents with\nlow-resource data. Audio samples are available at\nhttps://www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "9 pages, ACM MM2024(accepted)",
    "pdf_url": "http://arxiv.org/pdf/2408.10096v2",
    "published_date": "2024-08-19 15:33:59 UTC",
    "updated_date": "2024-08-22 09:56:20 UTC"
  },
  {
    "arxiv_id": "2408.10086v1",
    "title": "ARMADA: Attribute-Based Multimodal Data Augmentation",
    "authors": [
      "Xiaomeng Jin",
      "Jeonghwan Kim",
      "Yu Zhou",
      "Kuan-Hao Huang",
      "Te-Lin Wu",
      "Nanyun Peng",
      "Heng Ji"
    ],
    "abstract": "In Multimodal Language Models (MLMs), the cost of manually annotating\nhigh-quality image-text pair data for fine-tuning and alignment is extremely\nhigh. While existing multimodal data augmentation frameworks propose ways to\naugment image-text pairs, they either suffer from semantic inconsistency\nbetween texts and images, or generate unrealistic images, causing knowledge gap\nwith real world examples. To address these issues, we propose Attribute-based\nMultimodal Data Augmentation (ARMADA), a novel multimodal data augmentation\nmethod via knowledge-guided manipulation of visual attributes of the mentioned\nentities. Specifically, we extract entities and their visual attributes from\nthe original text data, then search for alternative values for the visual\nattributes under the guidance of knowledge bases (KBs) and large language\nmodels (LLMs). We then utilize an image-editing model to edit the images with\nthe extracted attributes. ARMADA is a novel multimodal data generation\nframework that: (i) extracts knowledge-grounded attributes from symbolic KBs\nfor semantically consistent yet distinctive image-text pair generation, (ii)\ngenerates visually similar images of disparate categories using neighboring\nentities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs\nto modulate auxiliary visual attributes such as backgrounds for more robust\nrepresentation of original entities. Our empirical results over four downstream\ntasks demonstrate the efficacy of our framework to produce high-quality data\nand enhance the model performance. This also highlights the need to leverage\nexternal knowledge proxies for enhanced interpretability and real-world\ngrounding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10086v1",
    "published_date": "2024-08-19 15:27:25 UTC",
    "updated_date": "2024-08-19 15:27:25 UTC"
  },
  {
    "arxiv_id": "2408.10077v1",
    "title": "No Screening is More Efficient with Multiple Objects",
    "authors": [
      "Shunya Noda",
      "Genta Okada"
    ],
    "abstract": "We study efficient mechanism design for allocating multiple heterogeneous\nobjects. We aim to maximize the residual surplus, the total value generated\nfrom an allocation minus the costs for screening agents' values. We discover a\nrobust trend indicating that no-screening mechanisms such as serial\ndictatorship with exogenous priority order tend to perform better as the\nvariety of goods increases. We analyze the underlying reasons by characterizing\nefficient mechanisms in a stylized environment. We also apply an automated\nmechanism design approach to numerically derive efficient mechanisms and\nvalidate the trend in general environments. Building on this implication, we\npropose the register-invite-book system (RIB) as an efficient system for\nscheduling vaccination against pandemic diseases.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10077v1",
    "published_date": "2024-08-19 15:20:42 UTC",
    "updated_date": "2024-08-19 15:20:42 UTC"
  },
  {
    "arxiv_id": "2408.10075v1",
    "title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning",
    "authors": [
      "Sriyash Poddar",
      "Yanming Wan",
      "Hamish Ivison",
      "Abhishek Gupta",
      "Natasha Jaques"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for\naligning foundation models to human values and preferences. However, current\nRLHF techniques cannot account for the naturally occurring differences in\nindividual human preferences across a diverse population. When these\ndifferences arise, traditional RLHF frameworks simply average over them,\nleading to inaccurate rewards and poor performance for individual subgroups. To\naddress the need for pluralistic alignment, we develop a class of multimodal\nRLHF methods. Our proposed techniques are based on a latent variable\nformulation - inferring a novel user-specific latent and learning reward models\nand policies conditioned on this latent without additional user-specific data.\nWhile conceptually simple, we show that in practice, this reward modeling\nrequires careful algorithmic considerations around model architecture and\nreward scaling. To empirically validate our proposed technique, we first show\nthat it can provide a way to combat underspecification in simulated control\nproblems, inferring and optimizing user-specific reward functions. Next, we\nconduct experiments on pluralistic language datasets representing diverse user\npreferences and demonstrate improved reward function accuracy. We additionally\nshow the benefits of this probabilistic framework in terms of measuring\nuncertainty, and actively learning user preferences. This work enables learning\nfrom diverse populations of users with divergent preferences, an important\nchallenge that naturally occurs in problems from robot learning to foundation\nmodel alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "weirdlabuw.github.io/vpl",
    "pdf_url": "http://arxiv.org/pdf/2408.10075v1",
    "published_date": "2024-08-19 15:18:30 UTC",
    "updated_date": "2024-08-19 15:18:30 UTC"
  },
  {
    "arxiv_id": "2408.10074v1",
    "title": "Synthesis of Reward Machines for Multi-Agent Equilibrium Design (Full Version)",
    "authors": [
      "Muhammad Najib",
      "Giuseppe Perelli"
    ],
    "abstract": "Mechanism design is a well-established game-theoretic paradigm for designing\ngames to achieve desired outcomes. This paper addresses a closely related but\ndistinct concept, equilibrium design. Unlike mechanism design, the designer's\nauthority in equilibrium design is more constrained; she can only modify the\nincentive structures in a given game to achieve certain outcomes without the\nability to create the game from scratch. We study the problem of equilibrium\ndesign using dynamic incentive structures, known as reward machines. We use\nweighted concurrent game structures for the game model, with goals (for the\nplayers and the designer) defined as mean-payoff objectives. We show how reward\nmachines can be used to represent dynamic incentives that allocate rewards in a\nmanner that optimises the designer's goal. We also introduce the main decision\nproblem within our framework, the payoff improvement problem. This problem\nessentially asks whether there exists a dynamic incentive (represented by some\nreward machine) that can improve the designer's payoff by more than a given\nthreshold value. We present two variants of the problem: strong and weak. We\ndemonstrate that both can be solved in polynomial time using a Turing machine\nequipped with an NP oracle. Furthermore, we also establish that these variants\nare either NP-hard or coNP-hard. Finally, we show how to synthesise the\ncorresponding reward machine if it exists.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10074v1",
    "published_date": "2024-08-19 15:17:58 UTC",
    "updated_date": "2024-08-19 15:17:58 UTC"
  },
  {
    "arxiv_id": "2408.10072v2",
    "title": "FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant",
    "authors": [
      "Zhengchao Huang",
      "Bin Xia",
      "Zicheng Lin",
      "Zhun Mou",
      "Wenming Yang",
      "Jiaya Jia"
    ],
    "abstract": "The rapid advancement of deepfake technologies has sparked widespread public\nconcern, particularly as face forgery poses a serious threat to public\ninformation security. However, the unknown and diverse forgery techniques,\nvaried facial features and complex environmental factors pose significant\nchallenges for face forgery analysis. Existing datasets lack descriptive\nannotations of these aspects, making it difficult for models to distinguish\nbetween real and forged faces using only visual information amid various\nconfounding factors. In addition, existing methods fail to yield user-friendly\nand explainable results, hindering the understanding of the model's\ndecision-making process. To address these challenges, we introduce a novel\nOpen-World Face Forgery Analysis VQA (OW-FFA-VQA) task and its corresponding\nbenchmark. To tackle this task, we first establish a dataset featuring a\ndiverse collection of real and forged face images with essential descriptions\nand reliable forgery reasoning. Based on this dataset, we introduce FFAA: Face\nForgery Analysis Assistant, consisting of a fine-tuned Multimodal Large\nLanguage Model (MLLM) and Multi-answer Intelligent Decision System (MIDS). By\nintegrating hypothetical prompts with MIDS, the impact of fuzzy classification\nboundaries is effectively mitigated, enhancing model robustness. Extensive\nexperiments demonstrate that our method not only provides user-friendly and\nexplainable results but also significantly boosts accuracy and robustness\ncompared to previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 21 figures; project page: https://ffaa-vl.github.io",
    "pdf_url": "http://arxiv.org/pdf/2408.10072v2",
    "published_date": "2024-08-19 15:15:20 UTC",
    "updated_date": "2024-11-21 14:37:25 UTC"
  },
  {
    "arxiv_id": "2408.10060v4",
    "title": "Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision",
    "authors": [
      "Junho Moon",
      "Haejun Chung",
      "Ikbeom Jang"
    ],
    "abstract": "Facial wrinkle detection plays a crucial role in cosmetic dermatology.\nPrecise manual segmentation of facial wrinkles is challenging and\ntime-consuming, with inherent subjectivity leading to inconsistent results\namong graders. To address this issue, we propose two solutions. First, we build\nand release the first public facial wrinkle dataset, 'FFHQ-Wrinkle', an\nextension of the NVIDIA FFHQ dataset. It includes 1,000 images with human\nlabels and 50,000 images with automatically generated weak labels. This dataset\ncould serve as a foundation for the research community to develop advanced\nwrinkle detection algorithms. Second, we introduce a simple training strategy\nutilizing texture maps, applicable to various segmentation models, to detect\nwrinkles across the face. Our two-stage training strategy first pretrain models\non a large dataset with weak labels (N=50k), or masked texture maps generated\nthrough computer vision techniques, without human intervention. We then\nfinetune the models using human-labeled data (N=1k), which consists of manually\nlabeled wrinkle masks. The network takes as input a combination of RGB and\nmasked texture map of the image, comprising four channels, in finetuning. We\neffectively combine labels from multiple annotators to minimize subjectivity in\nmanual labeling. Our strategies demonstrate improved segmentation performance\nin facial wrinkle segmentation both quantitatively and visually compared to\nexisting pretraining methods. The dataset is available at\nhttps://github.com/labhai/ffhq-wrinkle-dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at International Conference on Pattern Recognition (ICPR),\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2408.10060v4",
    "published_date": "2024-08-19 14:54:12 UTC",
    "updated_date": "2024-11-19 03:58:40 UTC"
  },
  {
    "arxiv_id": "2408.10040v1",
    "title": "The Practimum-Optimum Algorithm for Manufacturing Scheduling: A Paradigm Shift Leading to Breakthroughs in Scale and Performance",
    "authors": [
      "Moshe BenBassat"
    ],
    "abstract": "The Practimum-Optimum (P-O) algorithm represents a paradigm shift in\ndeveloping automatic optimization products for complex real-life business\nproblems such as large-scale manufacturing scheduling. It leverages deep\nbusiness domain expertise to create a group of virtual human expert (VHE)\nagents with different \"schools of thought\" on how to create high-quality\nschedules. By computerizing them into algorithms, P-O generates many valid\nschedules at far higher speeds than human schedulers are capable of. Initially,\nthese schedules can also be local optimum peaks far away from high-quality\nschedules. By submitting these schedules to a reinforced machine learning\nalgorithm (RL), P-O learns the weaknesses and strengths of each VHE schedule,\nand accordingly derives reward and punishment changes in the Demand Set that\nwill modify the relative priorities for time and resource allocation that jobs\nreceived in the prior iteration that led to the current state of the schedule.\nThese cause the core logic of the VHE algorithms to explore, in the subsequent\niteration, substantially different parts of the schedules universe and\npotentially find higher-quality schedules. Using the hill climbing analogy,\nthis may be viewed as a big jump, shifting from a given local peak to a faraway\npromising start point equipped with knowledge embedded in the demand set for\nfuture iterations. This is a fundamental difference from most contemporary\nalgorithms, which spend considerable time on local micro-steps restricted to\nthe neighbourhoods of local peaks they visit. This difference enables a\nbreakthrough in scale and performance for fully automatic manufacturing\nscheduling in complex organizations. The P-O algorithm is at the heart of\nPlataine Scheduler that, in one click, routinely schedules 30,000-50,000 tasks\nfor real-life complex manufacturing operations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10040v1",
    "published_date": "2024-08-19 14:32:21 UTC",
    "updated_date": "2024-08-19 14:32:21 UTC"
  },
  {
    "arxiv_id": "2408.10039v3",
    "title": "MSDiagnosis: A Benchmark for Evaluating Large Language Models in Multi-Step Clinical Diagnosis",
    "authors": [
      "Ruihui Hou",
      "Shencheng Chen",
      "Yongqi Fan",
      "Guangya Yu",
      "Lifeng Zhu",
      "Jing Sun",
      "Jingping Liu",
      "Tong Ruan"
    ],
    "abstract": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a Chinese clinical diagnostic benchmark, called MSDiagnosis.\nThis benchmark consists of 2,225 cases from 12 departments, covering tasks such\nas primary diagnosis, differential diagnosis, and final diagnosis.\nAdditionally, we propose a novel and effective framework. This framework\ncombines forward inference, backward inference, reflection, and refinement,\nenabling the large language model to self-evaluate and adjust its diagnostic\nresults. To this end, we test open-source models, closed-source models, and our\nproposed framework.The experimental results demonstrate the effectiveness of\nthe proposed method. We also provide a comprehensive experimental analysis and\nsuggest future research directions for this task.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10039v3",
    "published_date": "2024-08-19 14:31:57 UTC",
    "updated_date": "2024-12-16 09:33:05 UTC"
  },
  {
    "arxiv_id": "2408.10015v2",
    "title": "Deterministic Policy Gradient Primal-Dual Methods for Continuous-Space Constrained MDPs",
    "authors": [
      "Sergio Rozada",
      "Dongsheng Ding",
      "Antonio G. Marques",
      "Alejandro Ribeiro"
    ],
    "abstract": "We study the problem of computing deterministic optimal policies for\nconstrained Markov decision processes (MDPs) with continuous state and action\nspaces, which are widely encountered in constrained dynamical systems.\nDesigning deterministic policy gradient methods in continuous state and action\nspaces is particularly challenging due to the lack of enumerable state-action\npairs and the adoption of deterministic policies, hindering the application of\nexisting policy gradient methods. To this end, we develop a deterministic\npolicy gradient primal-dual method to find an optimal deterministic policy with\nnon-asymptotic convergence. Specifically, we leverage regularization of the\nLagrangian of the constrained MDP to propose a deterministic policy gradient\nprimal-dual (D-PGPD) algorithm that updates the deterministic policy via a\nquadratic-regularized gradient ascent step and the dual variable via a\nquadratic-regularized gradient descent step. We prove that the primal-dual\niterates of D-PGPD converge at a sub-linear rate to an optimal regularized\nprimal-dual pair. We instantiate D-PGPD with function approximation and prove\nthat the primal-dual iterates of D-PGPD converge at a sub-linear rate to an\noptimal regularized primal-dual pair, up to a function approximation error.\nFurthermore, we demonstrate the effectiveness of our method in two continuous\ncontrol problems: robot navigation and fluid control. This appears to be the\nfirst work that proposes a deterministic policy search method for\ncontinuous-space constrained MDPs.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10015v2",
    "published_date": "2024-08-19 14:11:04 UTC",
    "updated_date": "2025-04-04 11:14:35 UTC"
  },
  {
    "arxiv_id": "2408.10003v2",
    "title": "Towards a Knowledge Graph for Models and Algorithms in Applied Mathematics",
    "authors": [
      "Björn Schembera",
      "Frank Wübbeling",
      "Hendrik Kleikamp",
      "Burkhard Schmidt",
      "Aurela Shehu",
      "Marco Reidelbach",
      "Christine Biedinger",
      "Jochen Fiedler",
      "Thomas Koprucki",
      "Dorothea Iglezakis",
      "Dominik Göddeke"
    ],
    "abstract": "Mathematical models and algorithms are an essential part of mathematical\nresearch data, as they are epistemically grounding numerical data. In order to\nrepresent models and algorithms as well as their relationship semantically to\nmake this research data FAIR, two previously distinct ontologies were merged\nand extended, becoming a living knowledge graph. The link between the two\nontologies is established by introducing computational tasks, as they occur in\nmodeling, corresponding to algorithmic tasks. Moreover, controlled vocabularies\nare incorporated and a new class, distinguishing base quantities from specific\nuse case quantities, was introduced. Also, both models and algorithms can now\nbe enriched with metadata. Subject-specific metadata is particularly relevant\nhere, such as the symmetry of a matrix or the linearity of a mathematical\nmodel. This is the only way to express specific workflows with concrete models\nand algorithms, as the feasible solution algorithm can only be determined if\nthe mathematical properties of a model are known. We demonstrate this using two\nexamples from different application areas of applied mathematics. In addition,\nwe have already integrated over 250 research assets from applied mathematics\ninto our knowledge graph.",
    "categories": [
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint submitted to the 18th International Conference on Metadata\n  and Semantics Research 2024 and published as a full, revised article",
    "pdf_url": "http://arxiv.org/pdf/2408.10003v2",
    "published_date": "2024-08-19 13:57:49 UTC",
    "updated_date": "2025-02-26 14:03:53 UTC"
  },
  {
    "arxiv_id": "2408.11871v2",
    "title": "MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models",
    "authors": [
      "Lionel Z. Wang",
      "Yiming Ma",
      "Renfei Gao",
      "Beichen Guo",
      "Han Zhu",
      "Wenqi Fan",
      "Zexin Lu",
      "Ka Chung Ng"
    ],
    "abstract": "The advent of large language models (LLMs) has revolutionized online content\ncreation, making it much easier to generate high-quality fake news. This misuse\nthreatens the integrity of our digital environment and ethical standards.\nTherefore, understanding the motivations and mechanisms behind LLM-generated\nfake news is crucial. In this study, we analyze the creation of fake news from\na social psychology perspective and develop a comprehensive LLM-based\ntheoretical framework, LLM-Fake Theory. We introduce a novel pipeline that\nautomates the generation of fake news using LLMs, thereby eliminating the need\nfor manual annotation. Utilizing this pipeline, we create a theoretically\ninformed Machine-generated Fake news dataset, MegaFake, derived from the\nGossipCop dataset. We conduct comprehensive analyses to evaluate our MegaFake\ndataset. We believe that our dataset and insights will provide valuable\ncontributions to future research focused on the detection and governance of\nfake news in the era of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11871v2",
    "published_date": "2024-08-19 13:27:07 UTC",
    "updated_date": "2024-09-25 06:21:26 UTC"
  },
  {
    "arxiv_id": "2408.09972v1",
    "title": "Edge-Cloud Collaborative Motion Planning for Autonomous Driving with Large Language Models",
    "authors": [
      "Jiao Chen",
      "Suyan Dai",
      "Fangfang Chen",
      "Zuohong Lv",
      "Jianhua Tang"
    ],
    "abstract": "Integrating large language models (LLMs) into autonomous driving enhances\npersonalization and adaptability in open-world scenarios. However, traditional\nedge computing models still face significant challenges in processing complex\ndriving data, particularly regarding real-time performance and system\nefficiency. To address these challenges, this study introduces EC-Drive, a\nnovel edge-cloud collaborative autonomous driving system with data drift\ndetection capabilities. EC-Drive utilizes drift detection algorithms to\nselectively upload critical data, including new obstacles and traffic pattern\nchanges, to the cloud for processing by GPT-4, while routine data is\nefficiently managed by smaller LLMs on edge devices. This approach not only\nreduces inference latency but also improves system efficiency by optimizing\ncommunication resource use. Experimental validation confirms the system's\nrobust processing capabilities and practical applicability in real-world\ndriving conditions, demonstrating the effectiveness of this edge-cloud\ncollaboration framework. Our data and system demonstration will be released at\nhttps://sites.google.com/view/ec-drive.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09972v1",
    "published_date": "2024-08-19 13:19:15 UTC",
    "updated_date": "2024-08-19 13:19:15 UTC"
  },
  {
    "arxiv_id": "2408.09967v2",
    "title": "Unsupervised Machine Learning Hybrid Approach Integrating Linear Programming in Loss Function: A Robust Optimization Technique",
    "authors": [
      "Andrew Kiruluta",
      "Andreas Lemos"
    ],
    "abstract": "This paper presents a novel hybrid approach that integrates linear\nprogramming (LP) within the loss function of an unsupervised machine learning\nmodel. By leveraging the strengths of both optimization techniques and machine\nlearning, this method introduces a robust framework for solving complex\noptimization problems where traditional methods may fall short. The proposed\napproach encapsulates the constraints and objectives of a linear programming\nproblem directly into the loss function, guiding the learning process to adhere\nto these constraints while optimizing the desired outcomes. This technique not\nonly preserves the interpretability of linear programming but also benefits\nfrom the flexibility and adaptability of machine learning, making it\nparticularly well-suited for unsupervised or semi-supervised learning\nscenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09967v2",
    "published_date": "2024-08-19 13:14:26 UTC",
    "updated_date": "2025-04-18 14:16:20 UTC"
  },
  {
    "arxiv_id": "2408.09958v1",
    "title": "AdaResNet: Enhancing Residual Networks with Dynamic Weight Adjustment for Improved Feature Integration",
    "authors": [
      "Hong Su"
    ],
    "abstract": "In very deep neural networks, gradients can become extremely small during\nbackpropagation, making it challenging to train the early layers. ResNet\n(Residual Network) addresses this issue by enabling gradients to flow directly\nthrough the network via skip connections, facilitating the training of much\ndeeper networks. However, in these skip connections, the input ipd is directly\nadded to the transformed data tfd, treating ipd and tfd equally, without\nadapting to different scenarios. In this paper, we propose AdaResNet\n(Auto-Adapting Residual Network), which automatically adjusts the ratio between\nipd and tfd based on the training data. We introduce a variable,\nweight}_{tfd}^{ipd, to represent this ratio. This variable is dynamically\nadjusted during backpropagation, allowing it to adapt to the training data\nrather than remaining fixed. Experimental results demonstrate that AdaResNet\nachieves a maximum accuracy improvement of over 50\\% compared to traditional\nResNet.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09958v1",
    "published_date": "2024-08-19 12:58:51 UTC",
    "updated_date": "2024-08-19 12:58:51 UTC"
  },
  {
    "arxiv_id": "2408.09957v1",
    "title": "Contextual Importance and Utility in Python: New Functionality and Insights with the py-ciu Package",
    "authors": [
      "Kary Främling"
    ],
    "abstract": "The availability of easy-to-use and reliable software implementations is\nimportant for allowing researchers in academia and industry to test, assess and\ntake into use eXplainable AI (XAI) methods. This paper describes the\n\\texttt{py-ciu} Python implementation of the Contextual Importance and Utility\n(CIU) model-agnostic, post-hoc explanation method and illustrates capabilities\nof CIU that go beyond the current state-of-the-art that could be useful for XAI\npractitioners in general.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings of XAI 2024 Workshop of 33rd International Joint\n  Conference on Artificial Intelligence (IJCAI 2024), Jeju, South Corea",
    "pdf_url": "http://arxiv.org/pdf/2408.09957v1",
    "published_date": "2024-08-19 12:57:50 UTC",
    "updated_date": "2024-08-19 12:57:50 UTC"
  },
  {
    "arxiv_id": "2408.09952v1",
    "title": "Weakly Supervised Pretraining and Multi-Annotator Supervised Finetuning for Facial Wrinkle Detection",
    "authors": [
      "Ik Jun Moon",
      "Junho Moon",
      "Ikbeom Jang"
    ],
    "abstract": "1. Research question: With the growing interest in skin diseases and skin\naesthetics, the ability to predict facial wrinkles is becoming increasingly\nimportant. This study aims to evaluate whether a computational model,\nconvolutional neural networks (CNN), can be trained for automated facial\nwrinkle segmentation. 2. Findings: Our study presents an effective technique\nfor integrating data from multiple annotators and illustrates that transfer\nlearning can enhance performance, resulting in dependable segmentation of\nfacial wrinkles. 3. Meaning: This approach automates intricate and\ntime-consuming tasks of wrinkle analysis with a deep learning framework. It\ncould be used to facilitate skin treatments and diagnostics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09952v1",
    "published_date": "2024-08-19 12:47:47 UTC",
    "updated_date": "2024-08-19 12:47:47 UTC"
  },
  {
    "arxiv_id": "2408.09951v1",
    "title": "Principle Driven Parameterized Fiber Model based on GPT-PINN Neural Network",
    "authors": [
      "Yubin Zang",
      "Boyu Hua",
      "Zhenzhou Tang",
      "Zhipeng Lin",
      "Fangzheng Zhang",
      "Simin Li",
      "Zuxing Zhang",
      "Hongwei Chen"
    ],
    "abstract": "In cater the need of Beyond 5G communications, large numbers of data driven\nartificial intelligence based fiber models has been put forward as to utilize\nartificial intelligence's regression ability to predict pulse evolution in\nfiber transmission at a much faster speed compared with the traditional split\nstep Fourier method. In order to increase the physical interpretabiliy,\nprinciple driven fiber models have been proposed which inserts the Nonlinear\nSchodinger Equation into their loss functions. However, regardless of either\nprinciple driven or data driven models, they need to be re-trained the whole\nmodel under different transmission conditions. Unfortunately, this situation\ncan be unavoidable when conducting the fiber communication optimization work.\nIf the scale of different transmission conditions is large, then the whole\nmodel needs to be retrained large numbers of time with relatively large scale\nof parameters which may consume higher time costs. Computing efficiency will be\ndragged down as well. In order to address this problem, we propose the\nprinciple driven parameterized fiber model in this manuscript. This model\nbreaks down the predicted NLSE solution with respect to one set of transmission\ncondition into the linear combination of several eigen solutions which were\noutputted by each pre-trained principle driven fiber model via the reduced\nbasis method. Therefore, the model can greatly alleviate the heavy burden of\nre-training since only the linear combination coefficients need to be found\nwhen changing the transmission condition. Not only strong physical\ninterpretability can the model posses, but also higher computing efficiency can\nbe obtained. Under the demonstration, the model's computational complexity is\n0.0113% of split step Fourier method and 1% of the previously proposed\nprinciple driven fiber model.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09951v1",
    "published_date": "2024-08-19 12:44:00 UTC",
    "updated_date": "2024-08-19 12:44:00 UTC"
  },
  {
    "arxiv_id": "2408.09948v1",
    "title": "Caption-Driven Explorations: Aligning Image and Text Embeddings through Human-Inspired Foveated Vision",
    "authors": [
      "Dario Zanca",
      "Andrea Zugarini",
      "Simon Dietz",
      "Thomas R. Altstidl",
      "Mark A. Turban Ndjeuha",
      "Leo Schwinn",
      "Bjoern Eskofier"
    ],
    "abstract": "Understanding human attention is crucial for vision science and AI. While\nmany models exist for free-viewing, less is known about task-driven image\nexploration. To address this, we introduce CapMIT1003, a dataset with captions\nand click-contingent image explorations, to study human attention during the\ncaptioning task. We also present NevaClip, a zero-shot method for predicting\nvisual scanpaths by combining CLIP models with NeVA algorithms. NevaClip\ngenerates fixations to align the representations of foveated visual stimuli and\ncaptions. The simulated scanpaths outperform existing human attention models in\nplausibility for captioning and free-viewing tasks. This research enhances the\nunderstanding of human attention and advances scanpath prediction models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2305.12380",
    "pdf_url": "http://arxiv.org/pdf/2408.09948v1",
    "published_date": "2024-08-19 12:41:46 UTC",
    "updated_date": "2024-08-19 12:41:46 UTC"
  },
  {
    "arxiv_id": "2408.09947v1",
    "title": "Fiber Transmission Model with Parameterized Inputs based on GPT-PINN Neural Network",
    "authors": [
      "Yubin Zang",
      "Boyu Hua",
      "Zhipeng Lin",
      "Fangzheng Zhang",
      "Simin Li",
      "Zuxing Zhang",
      "Hongwei Chen"
    ],
    "abstract": "In this manuscript, a novelty principle driven fiber transmission model for\nshort-distance transmission with parameterized inputs is put forward. By taking\ninto the account of the previously proposed principle driven fiber model, the\nreduced basis expansion method and transforming the parameterized inputs into\nparameterized coefficients of the Nonlinear Schrodinger Equations, universal\nsolutions with respect to inputs corresponding to different bit rates can all\nbe obtained without the need of re-training the whole model. This model, once\nadopted, can have prominent advantages in both computation efficiency and\nphysical background. Besides, this model can still be effectively trained\nwithout the needs of transmitted signals collected in advance. Tasks of on-off\nkeying signals with bit rates ranging from 2Gbps to 50Gbps are adopted to\ndemonstrate the fidelity of the model.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09947v1",
    "published_date": "2024-08-19 12:37:15 UTC",
    "updated_date": "2024-08-19 12:37:15 UTC"
  },
  {
    "arxiv_id": "2408.09946v1",
    "title": "Microscopic Analysis on LLM players via Social Deduction Game",
    "authors": [
      "Byungjun Kim",
      "Dayeon Seo",
      "Bugeun Kim"
    ],
    "abstract": "Recent studies have begun developing autonomous game players for social\ndeduction games using large language models (LLMs). When building LLM players,\nfine-grained evaluations are crucial for addressing weaknesses in game-playing\nabilities. However, existing studies have often overlooked such assessments.\nSpecifically, we point out two issues with the evaluation methods employed.\nFirst, game-playing abilities have typically been assessed through game-level\noutcomes rather than specific event-level skills; Second, error analyses have\nlacked structured methodologies. To address these issues, we propose an\napproach utilizing a variant of the SpyFall game, named SpyGame. We conducted\nan experiment with four LLMs, analyzing their gameplay behavior in SpyGame both\nquantitatively and qualitatively. For the quantitative analysis, we introduced\neight metrics to resolve the first issue, revealing that these metrics are more\neffective than existing ones for evaluating the two critical skills: intent\nidentification and camouflage. In the qualitative analysis, we performed\nthematic analysis to resolve the second issue. This analysis identifies four\nmajor categories that affect gameplay of LLMs. Additionally, we demonstrate how\nthese categories complement and support the findings from the quantitative\nanalysis.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review, 10 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.09946v1",
    "published_date": "2024-08-19 12:35:23 UTC",
    "updated_date": "2024-08-19 12:35:23 UTC"
  },
  {
    "arxiv_id": "2408.09945v4",
    "title": "Large Language Models for Classical Chinese Poetry Translation: Benchmarking, Evaluating, and Improving",
    "authors": [
      "Andong Chen",
      "Lianzhang Lou",
      "Kehai Chen",
      "Xuefeng Bai",
      "Yang Xiang",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min Zhang"
    ],
    "abstract": "Different from the traditional translation tasks, classical Chinese poetry\ntranslation requires both adequacy and fluency in translating culturally and\nhistorically significant content and linguistic poetic elegance. Large language\nmodels (LLMs) with impressive multilingual capabilities may bring a ray of hope\nto achieve this extreme translation demand. This paper first introduces a\nsuitable benchmark (PoetMT) where each Chinese poetry has a recognized elegant\ntranslation. Meanwhile, we propose a new metric based on GPT-4 to evaluate the\nextent to which current LLMs can meet these demands. Our empirical evaluation\nreveals that the existing LLMs fall short in the challenging task. Hence, we\npropose a Retrieval-Augmented Machine Translation (RAT) method which\nincorporates knowledge related to classical poetry for advancing the\ntranslation of Chinese Poetry in LLMs. Experimental results show that RAT\nconsistently outperforms all comparison methods regarding wildly used BLEU,\nCOMET, BLEURT, our proposed metric, and human evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2408.09945v4",
    "published_date": "2024-08-19 12:34:31 UTC",
    "updated_date": "2024-12-30 07:26:14 UTC"
  },
  {
    "arxiv_id": "2408.09933v1",
    "title": "SZU-AFS Antispoofing System for the ASVspoof 5 Challenge",
    "authors": [
      "Yuxiong Xu",
      "Jiafeng Zhong",
      "Sengui Zheng",
      "Zefeng Liu",
      "Bin Li"
    ],
    "abstract": "This paper presents the SZU-AFS anti-spoofing system, designed for Track 1 of\nthe ASVspoof 5 Challenge under open conditions. The system is built with four\nstages: selecting a baseline model, exploring effective data augmentation (DA)\nmethods for fine-tuning, applying a co-enhancement strategy based on gradient\nnorm aware minimization (GAM) for secondary fine-tuning, and fusing logits\nscores from the two best-performing fine-tuned models. The system utilizes the\nWav2Vec2 front-end feature extractor and the AASIST back-end classifier as the\nbaseline model. During model fine-tuning, three distinct DA policies have been\ninvestigated: single-DA, random-DA, and cascade-DA. Moreover, the employed\nGAM-based co-enhancement strategy, designed to fine-tune the augmented model at\nboth data and optimizer levels, helps the Adam optimizer find flatter minima,\nthereby boosting model generalization. Overall, the final fusion system\nachieves a minDCF of 0.115 and an EER of 4.04% on the evaluation set.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages, 2 figures, ASVspoof 5 Workshop (Interspeech2024 Satellite)",
    "pdf_url": "http://arxiv.org/pdf/2408.09933v1",
    "published_date": "2024-08-19 12:12:29 UTC",
    "updated_date": "2024-08-19 12:12:29 UTC"
  },
  {
    "arxiv_id": "2408.10846v2",
    "title": "Harmonizing Attention: Training-free Texture-aware Geometry Transfer",
    "authors": [
      "Eito Ikuta",
      "Yohan Lee",
      "Akihiro Iohara",
      "Yu Saito",
      "Toshiyuki Tanaka"
    ],
    "abstract": "Extracting geometry features from photographic images independently of\nsurface texture and transferring them onto different materials remains a\ncomplex challenge. In this study, we introduce Harmonizing Attention, a novel\ntraining-free approach that leverages diffusion models for texture-aware\ngeometry transfer. Our method employs a simple yet effective modification of\nself-attention layers, allowing the model to query information from multiple\nreference images within these layers. This mechanism is seamlessly integrated\ninto the inversion process as Texture-aligning Attention and into the\ngeneration process as Geometry-aligning Attention. This dual-attention approach\nensures the effective capture and transfer of material-independent geometry\nfeatures while maintaining material-specific textural continuity, all without\nthe need for model fine-tuning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV2025",
    "pdf_url": "http://arxiv.org/pdf/2408.10846v2",
    "published_date": "2024-08-19 12:06:25 UTC",
    "updated_date": "2024-09-01 14:57:12 UTC"
  },
  {
    "arxiv_id": "2408.10843v3",
    "title": "Detecting Wildfires on UAVs with Real-time Segmentation Trained by Larger Teacher Models",
    "authors": [
      "Julius Pesonen",
      "Teemu Hakala",
      "Väinö Karjalainen",
      "Niko Koivumäki",
      "Lauri Markelin",
      "Anna-Maria Raita-Hakola",
      "Juha Suomalainen",
      "Ilkka Pölönen",
      "Eija Honkavaara"
    ],
    "abstract": "Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, detection methods are limited to onboard computation due to the lack\nof high-bandwidth mobile networks. For accurate camera-based localisation,\nsegmentation of the detected smoke is essential but training data for deep\nlearning-based wildfire smoke segmentation is limited. This study shows how\nsmall specialised segmentation models can be trained using only bounding box\nlabels, leveraging zero-shot foundation model supervision. The method offers\nthe advantages of needing only fairly easily obtainable bounding box labels and\nrequiring training solely for the smaller student network. The proposed method\nachieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The\nused model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson\nOrin NX computer while reliably recognising smoke, as demonstrated at\nreal-world forest burning events. Code is available at:\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.6"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10843v3",
    "published_date": "2024-08-19 11:42:54 UTC",
    "updated_date": "2024-12-18 08:54:03 UTC"
  },
  {
    "arxiv_id": "2408.09899v1",
    "title": "LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery",
    "authors": [
      "Weiji Kong",
      "Xun Gong",
      "Juan Wang"
    ],
    "abstract": "Explaining the decisions of Deep Neural Networks (DNNs) for medical images\nhas become increasingly important. Existing attribution methods have difficulty\nexplaining the meaning of pixels while existing concept-based methods are\nlimited by additional annotations or specific model structures that are\ndifficult to apply to ultrasound images. In this paper, we propose the Lesion\nConcept Explainer (LCE) framework, which combines attribution methods with\nconcept-based methods. We introduce the Segment Anything Model (SAM),\nfine-tuned on a large number of medical images, for concept discovery to enable\na meaningful explanation of ultrasound image DNNs. The proposed framework is\nevaluated in terms of both faithfulness and understandability. We point out\ndeficiencies in the popular faithfulness evaluation metrics and propose a new\nevaluation metric. Our evaluation of public and private breast ultrasound\ndatasets (BUSI and FG-US-B) shows that LCE performs well compared to\ncommonly-used explainability methods. Finally, we also validate that LCE can\nconsistently provide reliable explanations for more meaningful fine-grained\ndiagnostic tasks in breast ultrasound.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09899v1",
    "published_date": "2024-08-19 11:13:49 UTC",
    "updated_date": "2024-08-19 11:13:49 UTC"
  },
  {
    "arxiv_id": "2408.09894v1",
    "title": "Preoperative Rotator Cuff Tear Prediction from Shoulder Radiographs using a Convolutional Block Attention Module-Integrated Neural Network",
    "authors": [
      "Chris Hyunchul Jo",
      "Jiwoong Yang",
      "Byunghwan Jeon",
      "Hackjoon Shim",
      "Ikbeom Jang"
    ],
    "abstract": "Research question: We test whether a plane shoulder radiograph can be used\ntogether with deep learning methods to identify patients with rotator cuff\ntears as opposed to using an MRI in standard of care. Findings: By integrating\nconvolutional block attention modules into a deep neural network, our model\ndemonstrates high accuracy in detecting patients with rotator cuff tears,\nachieving an average AUC of 0.889 and an accuracy of 0.831. Meaning: This study\nvalidates the efficacy of our deep learning model to accurately detect rotation\ncuff tears from radiographs, offering a viable pre-assessment or alternative to\nmore expensive imaging techniques such as MRI.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09894v1",
    "published_date": "2024-08-19 11:08:49 UTC",
    "updated_date": "2024-08-19 11:08:49 UTC"
  },
  {
    "arxiv_id": "2408.09881v2",
    "title": "Uncertainty Quantification of Surrogate Models using Conformal Prediction",
    "authors": [
      "Vignesh Gopakumar",
      "Ander Gray",
      "Joel Oskarsson",
      "Lorenzo Zanisi",
      "Stanislas Pamela",
      "Daniel Giles",
      "Matt Kusner",
      "Marc Peter Deisenroth"
    ],
    "abstract": "Data-driven surrogate models have shown immense potential as quick,\ninexpensive approximations to complex numerical and experimental modelling\ntasks. However, most surrogate models of physical systems do not quantify their\nuncertainty, rendering their predictions unreliable, requiring further\nvalidation. Though Bayesian approximations offer some solace in estimating the\nerror associated with these models, they cannot provide guarantees, and the\nquality of their inferences depends on the availability of prior information\nand good approximations to posteriors for complex problems. This is\nparticularly pertinent to multi-variable or spatio-temporal problems. Our work\nconstructs and formalises a conformal prediction framework that satisfies\nmarginal coverage for spatio-temporal predictions in a model-agnostic manner,\nrequiring near-zero computational costs. We provide an extensive empirical\nstudy of the application of the framework to ascertain valid error bars that\nprovide guaranteed coverage across the surrogate model's domain of operation.\nThe application scope of our work extends across a large range of\nspatio-temporal models, from solving partial differential equations to weather\nforecasting. Through the applications, the paper looks at providing\nstatistically valid error bars for deterministic models, as well as crafting\nguarantees to the error bars of probabilistic models. Our conformal prediction\nformalisation provides guaranteed coverage of the surrogate model, regardless\nof model architecture, and its training regime and is unbothered by the curse\nof dimensionality.",
    "categories": [
      "cs.AI",
      "physics.ao-ph",
      "physics.plasm-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09881v2",
    "published_date": "2024-08-19 10:46:19 UTC",
    "updated_date": "2024-10-31 11:14:58 UTC"
  },
  {
    "arxiv_id": "2408.09873v1",
    "title": "New spectral imaging biomarkers for sepsis and mortality in intensive care",
    "authors": [
      "Silvia Seidlitz",
      "Katharina Hölzl",
      "Ayca von Garrel",
      "Jan Sellner",
      "Stephan Katzenschlager",
      "Tobias Hölle",
      "Dania Fischer",
      "Maik von der Forst",
      "Felix C. F. Schmitt",
      "Markus A. Weigand",
      "Lena Maier-Hein",
      "Maximilian Dietrich"
    ],
    "abstract": "With sepsis remaining a leading cause of mortality, early identification of\nseptic patients and those at high risk of death is a challenge of high\nsocioeconomic importance. The driving hypothesis of this study was that\nhyperspectral imaging (HSI) could provide novel biomarkers for sepsis diagnosis\nand treatment management due to its potential to monitor microcirculatory\nalterations. We conducted a comprehensive study involving HSI data of the palm\nand fingers from more than 480 patients on the day of their intensive care unit\n(ICU) admission. The findings demonstrate that HSI measurements can predict\nsepsis with an area under the receiver operating characteristic curve (AUROC)\nof 0.80 (95 % confidence interval (CI) [0.76; 0.84]) and mortality with an\nAUROC of 0.72 (95 % CI [0.65; 0.79]). The predictive performance improves\nsubstantially when additional clinical data is incorporated, leading to an\nAUROC of up to 0.94 (95 % CI [0.92; 0.96]) for sepsis and 0.84 (95 % CI [0.78;\n0.89]) for mortality. We conclude that HSI presents novel imaging biomarkers\nfor the rapid, non-invasive prediction of sepsis and mortality, suggesting its\npotential as an important modality for guiding diagnosis and treatment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "Markus A. Weigand, Lena Maier-Hein and Maximilian Dietrich\n  contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2408.09873v1",
    "published_date": "2024-08-19 10:24:57 UTC",
    "updated_date": "2024-08-19 10:24:57 UTC"
  },
  {
    "arxiv_id": "2408.09860v2",
    "title": "3D-Aware Instance Segmentation and Tracking in Egocentric Videos",
    "authors": [
      "Yash Bhalgat",
      "Vadim Tschernezki",
      "Iro Laina",
      "João F. Henriques",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ],
    "abstract": "Egocentric videos present unique challenges for 3D scene understanding due to\nrapid camera motion, frequent object occlusions, and limited object visibility.\nThis paper introduces a novel approach to instance segmentation and tracking in\nfirst-person video that leverages 3D awareness to overcome these obstacles. Our\nmethod integrates scene geometry, 3D object centroid tracking, and instance\nsegmentation to create a robust framework for analyzing dynamic egocentric\nscenes. By incorporating spatial and temporal cues, we achieve superior\nperformance compared to state-of-the-art 2D approaches. Extensive evaluations\non the challenging EPIC Fields dataset demonstrate significant improvements\nacross a range of tracking and segmentation consistency metrics. Specifically,\nour method outperforms the next best performing approach by $7$ points in\nAssociation Accuracy (AssA) and $4.5$ points in IDF1 score, while reducing the\nnumber of ID switches by $73\\%$ to $80\\%$ across various object categories.\nLeveraging our tracked instance segmentations, we showcase downstream\napplications in 3D object reconstruction and amodal video object segmentation\nin these egocentric settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Camera-ready for ACCV 2024. More experiments added",
    "pdf_url": "http://arxiv.org/pdf/2408.09860v2",
    "published_date": "2024-08-19 10:08:25 UTC",
    "updated_date": "2024-11-20 12:51:25 UTC"
  },
  {
    "arxiv_id": "2408.09856v1",
    "title": "TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and Competition",
    "authors": [
      "Tianwei Lin",
      "Jiang Liu",
      "Wenqiao Zhang",
      "Zhaocheng Li",
      "Yang Dai",
      "Haoyuan Li",
      "Zhelun Yu",
      "Wanggui He",
      "Juncheng Li",
      "Hao Jiang",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "abstract": "While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have\neffectively addressed GPU memory constraints during fine-tuning, their\nperformance often falls short, especially in multidimensional task scenarios.\nTo address this issue, one straightforward solution is to introduce\ntask-specific LoRA modules as domain experts, leveraging the modeling of\nmultiple experts' capabilities and thus enhancing the general capability of\nmulti-task learning. Despite promising, these additional components often add\ncomplexity to the training and inference process, contravening the efficient\ncharacterization of PEFT designed for. Considering this, we introduce an\ninnovative PEFT method, TeamLoRA, consisting of a collaboration and competition\nmodule for experts, and thus achieving the right balance of effectiveness and\nefficiency: (i) For collaboration, a novel knowledge-sharing and -organizing\nmechanism is devised to appropriately reduce the scale of matrix operations,\nthereby boosting the training and inference speed. (ii) For competition, we\npropose leveraging a game-theoretic interaction mechanism for experts,\nencouraging experts to transfer their domain-specific knowledge while facing\ndiverse downstream tasks, and thus enhancing the performance. By doing so,\nTeamLoRA elegantly connects the experts as a \"Team\" with internal collaboration\nand competition, enabling a faster and more accurate PEFT paradigm for\nmulti-task learning. To validate the superiority of TeamLoRA, we curate a\ncomprehensive multi-task evaluation(CME) benchmark to thoroughly assess the\ncapability of multi-task learning. Experiments conducted on our CME and other\nbenchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project\nis available at https://github.com/Lin-Tianwei/TeamLoRA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09856v1",
    "published_date": "2024-08-19 09:58:53 UTC",
    "updated_date": "2024-08-19 09:58:53 UTC"
  },
  {
    "arxiv_id": "2408.09853v1",
    "title": "Self-Directed Turing Test for Large Language Models",
    "authors": [
      "Weiqi Wu",
      "Hongqiu Wu",
      "Hai Zhao"
    ],
    "abstract": "The Turing test examines whether AIs can exhibit human-like behaviour in\nnatural language conversations. Traditional Turing tests adopt a rigid dialogue\nformat where each participant sends only one message each time and require\ncontinuous human involvement to direct the entire interaction with the test\nsubject. This fails to reflect a natural conversational style and hinders the\nevaluation of Large Language Models (LLMs) in complex and prolonged dialogues.\nThis paper proposes the Self-Directed Turing Test, which extends the original\ntest with a burst dialogue format, allowing more dynamic exchanges by multiple\nconsecutive messages. It further efficiently reduces human workload by having\nthe LLM self-direct the majority of the test process, iteratively generating\ndialogues that simulate its interaction with humans. With the pseudo-dialogue\nhistory, the model then engages in a shorter dialogue with a human, which is\npaired with a human-human conversation on the same topic to be judged using\nquestionnaires. We introduce the X-Turn Pass-Rate metric to assess the human\nlikeness of LLMs across varying durations. While LLMs like GPT-4 initially\nperform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10\nturns of dialogues respectively, their performance drops as the dialogue\nprogresses, which underscores the difficulty in maintaining consistency in the\nlong term.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09853v1",
    "published_date": "2024-08-19 09:57:28 UTC",
    "updated_date": "2024-08-19 09:57:28 UTC"
  },
  {
    "arxiv_id": "2408.09849v2",
    "title": "Importance Weighting Can Help Large Language Models Self-Improve",
    "authors": [
      "Chunyang Jiang",
      "Chi-min Chan",
      "Wei Xue",
      "Qifeng Liu",
      "Yike Guo"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09849v2",
    "published_date": "2024-08-19 09:51:02 UTC",
    "updated_date": "2024-12-12 14:56:20 UTC"
  },
  {
    "arxiv_id": "2408.09841v2",
    "title": "Demystifying Reinforcement Learning in Production Scheduling via Explainable AI",
    "authors": [
      "Daniel Fischer",
      "Hannah M. Hüsener",
      "Felix Grumbach",
      "Lukas Vollenkemper",
      "Arthur Müller",
      "Pascal Reusch"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) is a frequently employed technique to solve\nscheduling problems. Although DRL agents ace at delivering viable results in\nshort computing times, their reasoning remains opaque. We conduct a case study\nwhere we systematically apply two explainable AI (xAI) frameworks, namely SHAP\n(DeepSHAP) and Captum (Input x Gradient), to describe the reasoning behind\nscheduling decisions of a specialized DRL agent in a flow production. We find\nthat methods in the xAI literature lack falsifiability and consistent\nterminology, do not adequately consider domain-knowledge, the target audience\nor real-world scenarios, and typically provide simple input-output explanations\nrather than causal interpretations. To resolve this issue, we introduce a\nhypotheses-based workflow. This approach enables us to inspect whether\nexplanations align with domain knowledge and match the reward hypotheses of the\nagent. We furthermore tackle the challenge of communicating these insights to\nthird parties by tailoring hypotheses to the target audience, which can serve\nas interpretations of the agent's behavior after verification. Our proposed\nworkflow emphasizes the repeated verification of explanations and may be\napplicable to various DRL-based scheduling use cases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09841v2",
    "published_date": "2024-08-19 09:39:01 UTC",
    "updated_date": "2024-08-30 10:28:53 UTC"
  },
  {
    "arxiv_id": "2408.09839v2",
    "title": "Segment-Anything Models Achieve Zero-shot Robustness in Autonomous Driving",
    "authors": [
      "Jun Yan",
      "Pengyu Wang",
      "Danni Wang",
      "Weiquan Huang",
      "Daniel Watzenig",
      "Huilin Yin"
    ],
    "abstract": "Semantic segmentation is a significant perception task in autonomous driving.\nIt suffers from the risks of adversarial examples. In the past few years, deep\nlearning has gradually transitioned from convolutional neural network (CNN)\nmodels with a relatively small number of parameters to foundation models with a\nhuge number of parameters. The segment-anything model (SAM) is a generalized\nimage segmentation framework that is capable of handling various types of\nimages and is able to recognize and segment arbitrary objects in an image\nwithout the need to train on a specific object. It is a unified model that can\nhandle diverse downstream tasks, including semantic segmentation, object\ndetection, and tracking. In the task of semantic segmentation for autonomous\ndriving, it is significant to study the zero-shot adversarial robustness of\nSAM. Therefore, we deliver a systematic empirical study on the robustness of\nSAM without additional training. Based on the experimental results, the\nzero-shot adversarial robustness of the SAM under the black-box corruptions and\nwhite-box adversarial attacks is acceptable, even without the need for\nadditional training. The finding of this study is insightful in that the\ngigantic model parameters and huge amounts of training data lead to the\nphenomenon of emergence, which builds a guarantee of adversarial robustness.\nSAM is a vision foundation model that can be regarded as an early prototype of\nan artificial general intelligence (AGI) pipeline. In such a pipeline, a\nunified model can handle diverse tasks. Therefore, this research not only\ninspects the impact of vision foundation models on safe autonomous driving but\nalso provides a perspective on developing trustworthy AGI. The code is\navailable at: https://github.com/momo1986/robust_sam_iv.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to IAVVC 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09839v2",
    "published_date": "2024-08-19 09:35:51 UTC",
    "updated_date": "2024-10-01 07:50:41 UTC"
  },
  {
    "arxiv_id": "2408.09834v3",
    "title": "Minor DPO reject penalty to increase training robustness",
    "authors": [
      "Shiming Xie",
      "Hong Chen",
      "Fred Yu",
      "Zeye Sun",
      "Xiuyu Wu",
      "Yingfan Hu"
    ],
    "abstract": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09834v3",
    "published_date": "2024-08-19 09:29:31 UTC",
    "updated_date": "2024-08-30 13:54:36 UTC"
  },
  {
    "arxiv_id": "2408.09825v1",
    "title": "TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics",
    "authors": [
      "Chang Liu",
      "Jingtao Ding",
      "Yiwen Song",
      "Yong Li"
    ],
    "abstract": "Predicting the resilience of complex networks, which represents the ability\nto retain fundamental functionality amidst external perturbations or internal\nfailures, plays a critical role in understanding and improving real-world\ncomplex systems. Traditional theoretical approaches grounded in nonlinear\ndynamical systems rely on prior knowledge of network dynamics. On the other\nhand, data-driven approaches frequently encounter the challenge of insufficient\nlabeled data, a predicament commonly observed in real-world scenarios. In this\npaper, we introduce a novel resilience prediction framework for complex\nnetworks, designed to tackle this issue through generative data augmentation of\nnetwork topology and dynamics. The core idea is the strategic utilization of\nthe inherent joint distribution present in unlabeled network data, facilitating\nthe learning process of the resilience predictor by illuminating the\nrelationship between network topology and dynamics. Experiment results on three\nnetwork datasets demonstrate that our proposed framework TDNetGen can achieve\nhigh prediction accuracy up to 85%-95%. Furthermore, the framework still\ndemonstrates a pronounced augmentation capability in extreme low-data regimes,\nthereby underscoring its utility and robustness in enhancing the prediction of\nnetwork resilience. We have open-sourced our code in the following link,\nhttps://github.com/tsinghua-fib-lab/TDNetGen.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09825v1",
    "published_date": "2024-08-19 09:20:31 UTC",
    "updated_date": "2024-08-19 09:20:31 UTC"
  },
  {
    "arxiv_id": "2408.09819v1",
    "title": "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models",
    "authors": [
      "Linhao Yu",
      "Yongqi Leng",
      "Yufei Huang",
      "Shang Wu",
      "Haixin Liu",
      "Xinmeng Ji",
      "Jiahui Zhao",
      "Jinwang Song",
      "Tingting Cui",
      "Xiaoqing Cheng",
      "Tao Liu",
      "Deyi Xiong"
    ],
    "abstract": "What a large language model (LLM) would respond in ethically relevant\ncontext? In this paper, we curate a large benchmark CMoralEval for morality\nevaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a\nChinese TV program discussing Chinese moral norms with stories from the society\nand 2) a collection of Chinese moral anomies from various newspapers and\nacademic papers on morality. With these sources, we aim to create a moral\nevaluation dataset characterized by diversity and authenticity. We develop a\nmorality taxonomy and a set of fundamental moral principles that are not only\nrooted in traditional Chinese culture but also consistent with contemporary\nsocietal norms. To facilitate efficient construction and annotation of\ninstances in CMoralEval, we establish a platform with AI-assisted instance\ngeneration to streamline the annotation process. These help us curate\nCMoralEval that encompasses both explicit moral scenarios (14,964 instances)\nand moral dilemma scenarios (15,424 instances), each with instances from\ndifferent data sources. We conduct extensive experiments with CMoralEval to\nexamine a variety of Chinese LLMs. Experiment results demonstrate that\nCMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly\navailable at \\url{https://github.com/tjunlp-lab/CMoralEval}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2408.09819v1",
    "published_date": "2024-08-19 09:15:35 UTC",
    "updated_date": "2024-08-19 09:15:35 UTC"
  },
  {
    "arxiv_id": "2408.09817v1",
    "title": "Contextual Dual Learning Algorithm with Listwise Distillation for Unbiased Learning to Rank",
    "authors": [
      "Lulu Yu",
      "Keping Bi",
      "Shiyu Ni",
      "Jiafeng Guo"
    ],
    "abstract": "Unbiased Learning to Rank (ULTR) aims to leverage biased implicit user\nfeedback (e.g., click) to optimize an unbiased ranking model. The effectiveness\nof the existing ULTR methods has primarily been validated on synthetic\ndatasets. However, their performance on real-world click data remains unclear.\nRecently, Baidu released a large publicly available dataset of their web search\nlogs. Subsequently, the NTCIR-17 ULTRE-2 task released a subset dataset\nextracted from it. We conduct experiments on commonly used or effective ULTR\nmethods on this subset to determine whether they maintain their effectiveness.\nIn this paper, we propose a Contextual Dual Learning Algorithm with Listwise\nDistillation (CDLA-LD) to simultaneously address both position bias and\ncontextual bias. We utilize a listwise-input ranking model to obtain\nreconstructed feature vectors incorporating local contextual information and\nemploy the Dual Learning Algorithm (DLA) method to jointly train this ranking\nmodel and a propensity model to address position bias. As this ranking model\nlearns the interaction information within the documents list of the training\nset, to enhance the ranking model's generalization ability, we additionally\ntrain a pointwise-input ranking model to learn the listwise-input ranking\nmodel's capability for relevance judgment in a listwise manner. Extensive\nexperiments and analysis confirm the effectiveness of our approach.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09817v1",
    "published_date": "2024-08-19 09:13:52 UTC",
    "updated_date": "2024-08-19 09:13:52 UTC"
  },
  {
    "arxiv_id": "2408.09807v3",
    "title": "Reset-free Reinforcement Learning with World Models",
    "authors": [
      "Zhao Yang",
      "Thomas M. Moerland",
      "Mike Preuss",
      "Aske Plaat",
      "Edward S. Hu"
    ],
    "abstract": "Reinforcement learning (RL) is an appealing paradigm for training intelligent\nagents, enabling policy acquisition from the agent's own autonomously acquired\nexperience. However, the training process of RL is far from automatic,\nrequiring extensive human effort to reset the agent and environments. To tackle\nthe challenging reset-free setting, we first demonstrate the superiority of\nmodel-based (MB) RL methods in such setting, showing that a straightforward\nadaptation of MBRL can outperform all the prior state-of-the-art methods while\nrequiring less supervision. We then identify limitations inherent to this\ndirect extension and propose a solution called model-based reset-free\n(MoReFree) agent, which further enhances the performance. MoReFree adapts two\nkey mechanisms, exploration and policy learning, to handle reset-free tasks by\nprioritizing task-relevant states. It exhibits superior data-efficiency across\nvarious reset-free tasks without access to environmental reward or\ndemonstrations while significantly outperforming privileged baselines that\nrequire supervision. Our findings suggest model-based methods hold significant\npromise for reducing human effort in RL. Website:\nhttps://yangzhao-666.github.io/morefree",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09807v3",
    "published_date": "2024-08-19 08:56:00 UTC",
    "updated_date": "2025-02-22 22:27:41 UTC"
  },
  {
    "arxiv_id": "2408.09794v2",
    "title": "AutoML-guided Fusion of Entity and LLM-based Representations for Document Classification",
    "authors": [
      "Boshko Koloski",
      "Senja Pollak",
      "Roberto Navigli",
      "Blaž Škrlj"
    ],
    "abstract": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (i.e. embeddings) do not\nefficiently exploit these resources. Dense and robust representations of\ndocuments are essential for effectively solving downstream classification and\nretrieval tasks. This work demonstrates that injecting embedded information\nfrom knowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets. The code is freely available at\n\\url{https://github.com/bkolosk1/bablfusion.git}.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the 2024 Discovery Science Conference, oral presentation\n  track",
    "pdf_url": "http://arxiv.org/pdf/2408.09794v2",
    "published_date": "2024-08-19 08:41:40 UTC",
    "updated_date": "2024-09-30 14:02:59 UTC"
  },
  {
    "arxiv_id": "2408.10287v3",
    "title": "Recognizing Beam Profiles from Silicon Photonics Gratings using Transformer Model",
    "authors": [
      "Yu Dian Lim",
      "Hong Yu Li",
      "Simon Chun Kiat Goh",
      "Xiangyu Wang",
      "Peng Zhao",
      "Chuan Seng Tan"
    ],
    "abstract": "Over the past decade, there has been extensive work in developing integrated\nsilicon photonics (SiPh) gratings for the optical addressing of trapped ion\nqubits in the ion trap quantum computing community. However, when viewing beam\nprofiles from infrared (IR) cameras, it is often difficult to determine the\ncorresponding heights where the beam profiles are located. In this work, we\ndeveloped transformer models to recognize the corresponding height categories\nof beam profiles of light from SiPh gratings. The model is trained using two\ntechniques: (1) input patches, and (2) input sequence. For model trained with\ninput patches, the model achieved recognition accuracy of 0.938. Meanwhile,\nmodel trained with input sequence shows lower accuracy of 0.895. However, when\nrepeating the model-training 150 cycles, model trained with input patches shows\ninconsistent accuracy ranges between 0.445 to 0.959, while model trained with\ninput sequence exhibit higher accuracy values between 0.789 to 0.936. The\nobtained outcomes can be expanded to various applications, including\nauto-focusing of light beam and auto-adjustment of z-axis stage to acquire\ndesired beam profiles.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "physics.optics",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10287v3",
    "published_date": "2024-08-19 08:33:16 UTC",
    "updated_date": "2024-08-22 05:24:16 UTC"
  },
  {
    "arxiv_id": "2408.10286v3",
    "title": "GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching",
    "authors": [
      "Xiao Han",
      "Zijian Zhang",
      "Xiangyu Zhao",
      "Yuanshao Zhu",
      "Guojiang Shen",
      "Xiangjie Kong",
      "Xuetao Wei",
      "Liqiang Nie",
      "Jieping Ye"
    ],
    "abstract": "As urban residents demand higher travel quality, vehicle dispatch has become\na critical component of online ride-hailing services. However, current vehicle\ndispatch systems struggle to navigate the complexities of urban traffic\ndynamics, including unpredictable traffic conditions, diverse driver behaviors,\nand fluctuating supply and demand patterns. These challenges have resulted in\ntravel difficulties for passengers in certain areas, while many drivers in\nother areas are unable to secure orders, leading to a decline in the overall\nquality of urban transportation services. To address these issues, this paper\nintroduces GARLIC: a framework of GPT-Augmented Reinforcement Learning with\nIntelligent Control for vehicle dispatching. GARLIC utilizes multiview graphs\nto capture hierarchical traffic states, and learns a dynamic reward function\nthat accounts for individual driving behaviors. The framework further\nintegrates a GPT model trained with a custom loss function to enable\nhigh-precision predictions and optimize dispatching policies in real-world\nscenarios. Experiments conducted on two real-world datasets demonstrate that\nGARLIC effectively aligns with driver behaviors while reducing the empty load\nrate of vehicles.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.10286v3",
    "published_date": "2024-08-19 08:23:38 UTC",
    "updated_date": "2024-12-16 00:11:13 UTC"
  },
  {
    "arxiv_id": "2408.09785v2",
    "title": "GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making",
    "authors": [
      "Arsham Gholamzadeh Khoee",
      "Yinan Yu",
      "Robert Feldt",
      "Andris Freimanis",
      "Patrick Andersson Rhodin",
      "Dhasarathy Parthasarathy"
    ],
    "abstract": "Traditional methods for making software deployment decisions in the\nautomotive industry typically rely on manual analysis of tabular software test\ndata. These methods often lead to higher costs and delays in the software\nrelease cycle due to their labor-intensive nature. Large Language Models (LLMs)\npresent a promising solution to these challenges. However, their application\ngenerally demands multiple rounds of human-driven prompt engineering, which\nlimits their practical deployment, particularly for industrial end-users who\nneed reliable and efficient results. In this paper, we propose GoNoGo, an LLM\nagent system designed to streamline automotive software deployment while\nmeeting both functional requirements and practical industrial constraints.\nUnlike previous systems, GoNoGo is specifically tailored to address\ndomain-specific and risk-sensitive systems. We evaluate GoNoGo's performance\nacross different task difficulties using zero-shot and few-shot examples taken\nfrom industrial practice. Our results show that GoNoGo achieves a 100% success\nrate for tasks up to Level 2 difficulty with 3-shot examples, and maintains\nhigh performance even for more complex tasks. We find that GoNoGo effectively\nautomates decision-making for simpler tasks, significantly reducing the need\nfor manual intervention. In summary, GoNoGo represents an efficient and\nuser-friendly LLM-based solution currently employed in our industrial partner's\ncompany to assist with software release decision-making, supporting more\ninformed and timely decisions in the release process for risk-sensitive vehicle\nsystems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09785v2",
    "published_date": "2024-08-19 08:22:20 UTC",
    "updated_date": "2024-09-29 09:46:01 UTC"
  },
  {
    "arxiv_id": "2408.09781v1",
    "title": "Neural Horizon Model Predictive Control -- Increasing Computational Efficiency with Neural Networks",
    "authors": [
      "Hendrik Alsmeier",
      "Anton Savchenko",
      "Rolf Findeisen"
    ],
    "abstract": "The expansion in automation of increasingly fast applications and low-power\nedge devices poses a particular challenge for optimization based control\nalgorithms, like model predictive control. Our proposed machine-learning\nsupported approach addresses this by utilizing a feed-forward neural network to\nreduce the computation load of the online-optimization. We propose\napproximating part of the problem horizon, while maintaining safety guarantees\n-- constraint satisfaction -- via the remaining optimization part of the\ncontroller. The approach is validated in simulation, demonstrating an\nimprovement in computational efficiency, while maintaining guarantees and\nnear-optimal performance. The proposed MPC scheme can be applied to a wide\nrange of applications, including those requiring a rapid control response, such\nas robotics and embedded applications with limited computational resources.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "6 pages, 4 figures, 4 tables, American Control Conference (ACC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09781v1",
    "published_date": "2024-08-19 08:13:37 UTC",
    "updated_date": "2024-08-19 08:13:37 UTC"
  },
  {
    "arxiv_id": "2408.09768v3",
    "title": "MalLight: Influence-Aware Coordinated Traffic Signal Control for Traffic Signal Malfunctions",
    "authors": [
      "Qinchen Yang",
      "Zejun Xie",
      "Hua Wei",
      "Desheng Zhang",
      "Yu Yang"
    ],
    "abstract": "Urban traffic is subject to disruptions that cause extended waiting time and\nsafety issues at signalized intersections. While numerous studies have\naddressed the issue of intelligent traffic systems in the context of various\ndisturbances, traffic signal malfunction, a common real-world occurrence with\nsignificant repercussions, has received comparatively limited attention. The\nprimary objective of this research is to mitigate the adverse effects of\ntraffic signal malfunction, such as traffic congestion and collision, by\noptimizing the control of neighboring functioning signals. To achieve this\ngoal, this paper presents a novel traffic signal control framework (MalLight),\nwhich leverages an Influence-aware State Aggregation Module (ISAM) and an\nInfluence-aware Reward Aggregation Module (IRAM) to achieve coordinated control\nof surrounding traffic signals. To the best of our knowledge, this study\npioneers the application of a Reinforcement Learning(RL)-based approach to\naddress the challenges posed by traffic signal malfunction. Empirical\ninvestigations conducted on real-world datasets substantiate the superior\nperformance of our proposed methodology over conventional and deep\nlearning-based alternatives in the presence of signal malfunction, with\nreduction of throughput alleviated by as much as 48.6$\\%$.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted to CIKM24 Full Research track",
    "pdf_url": "http://arxiv.org/pdf/2408.09768v3",
    "published_date": "2024-08-19 07:57:13 UTC",
    "updated_date": "2024-09-13 03:10:51 UTC"
  },
  {
    "arxiv_id": "2408.09767v1",
    "title": "Propagating the prior from shallow to deep with a pre-trained velocity-model Generative Transformer network",
    "authors": [
      "Randy Harsuko",
      "Shijun Cheng",
      "Tariq Alkhalifah"
    ],
    "abstract": "Building subsurface velocity models is essential to our goals in utilizing\nseismic data for Earth discovery and exploration, as well as monitoring. With\nthe dawn of machine learning, these velocity models (or, more precisely, their\ndistribution) can be stored accurately and efficiently in a generative model.\nThese stored velocity model distributions can be utilized to regularize or\nquantify uncertainties in inverse problems, like full waveform inversion.\nHowever, most generators, like normalizing flows or diffusion models, treat the\nimage (velocity model) uniformly, disregarding spatial dependencies and\nresolution changes with respect to the observation locations. To address this\nweakness, we introduce VelocityGPT, a novel implementation that utilizes\nTransformer decoders trained autoregressively to generate a velocity model from\nshallow subsurface to deep. Owing to the fact that seismic data are often\nrecorded on the Earth's surface, a top-down generator can utilize the inverted\ninformation in the shallow as guidance (prior) to generating the deep. To\nfacilitate the implementation, we use an additional network to compress the\nvelocity model. We also inject prior information, like well or structure\n(represented by a migration image) to generate the velocity model. Using\nsynthetic data, we demonstrate the effectiveness of VelocityGPT as a promising\napproach in generative model applications for seismic velocity model building.",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09767v1",
    "published_date": "2024-08-19 07:56:43 UTC",
    "updated_date": "2024-08-19 07:56:43 UTC"
  },
  {
    "arxiv_id": "2408.09764v1",
    "title": "Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms",
    "authors": [
      "Xiao Wang",
      "Shiao Wang",
      "Pengpeng Shao",
      "Bo Jiang",
      "Lin Zhu",
      "Yonghong Tian"
    ],
    "abstract": "Human Action Recognition (HAR) stands as a pivotal research domain in both\ncomputer vision and artificial intelligence, with RGB cameras dominating as the\npreferred tool for investigation and innovation in this field. However, in\nreal-world applications, RGB cameras encounter numerous challenges, including\nlight conditions, fast motion, and privacy concerns. Consequently, bio-inspired\nevent cameras have garnered increasing attention due to their advantages of low\nenergy consumption, high dynamic range, etc. Nevertheless, most existing\nevent-based HAR datasets are low resolution ($346 \\times 260$). In this paper,\nwe propose a large-scale, high-definition ($1280 \\times 800$) human action\nrecognition dataset based on the CeleX-V event camera, termed CeleX-HAR. It\nencompasses 150 commonly occurring action categories, comprising a total of\n124,625 video sequences. Various factors such as multi-view, illumination,\naction speed, and occlusion are considered when recording these data. To build\na more comprehensive benchmark dataset, we report over 20 mainstream HAR models\nfor future works to compare. In addition, we also propose a novel Mamba vision\nbackbone network for event stream based HAR, termed EVMamba, which equips the\nspatial plane multi-directional scanning and novel voxel temporal scanning\nmechanism. By encoding and mining the spatio-temporal information of event\nstreams, our EVMamba has achieved favorable results across multiple datasets.\nBoth the dataset and source code will be released on\n\\url{https://github.com/Event-AHU/CeleX-HAR}",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2408.09764v1",
    "published_date": "2024-08-19 07:52:20 UTC",
    "updated_date": "2024-08-19 07:52:20 UTC"
  },
  {
    "arxiv_id": "2408.09748v1",
    "title": "Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method",
    "authors": [
      "Chen Yang",
      "Sunhao Dai",
      "Yupeng Hou",
      "Wayne Xin Zhao",
      "Jun Xu",
      "Yang Song",
      "Hengshu Zhu"
    ],
    "abstract": "Reciprocal recommender systems~(RRS), conducting bilateral recommendations\nbetween two involved parties, have gained increasing attention for enhancing\nmatching efficiency. However, the majority of existing methods in the\nliterature still reuse conventional ranking metrics to separately assess the\nperformance on each side of the recommendation process. These methods overlook\nthe fact that the ranking outcomes of both sides collectively influence the\neffectiveness of the RRS, neglecting the necessity of a more holistic\nevaluation and a capable systemic solution.\n  In this paper, we systemically revisit the task of reciprocal recommendation,\nby introducing the new metrics, formulation, and method. Firstly, we propose\nfive new evaluation metrics that comprehensively and accurately assess the\nperformance of RRS from three distinct perspectives: overall coverage,\nbilateral stability, and balanced ranking. These metrics provide a more\nholistic understanding of the system's effectiveness and enable a comprehensive\nevaluation. Furthermore, we formulate the RRS from a causal perspective,\nformulating recommendations as bilateral interventions, which can better model\nthe decoupled effects of potential influencing factors. By utilizing the\npotential outcome framework, we further develop a model-agnostic causal\nreciprocal recommendation method that considers the causal effects of\nrecommendations. Additionally, we introduce a reranking strategy to maximize\nmatching outcomes, as measured by the proposed metrics. Extensive experiments\non two real-world datasets from recruitment and dating scenarios demonstrate\nthe effectiveness of our proposed metrics and approach. The code and dataset\nare available at: https://github.com/RUCAIBox/CRRS.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09748v1",
    "published_date": "2024-08-19 07:21:02 UTC",
    "updated_date": "2024-08-19 07:21:02 UTC"
  },
  {
    "arxiv_id": "2408.09746v1",
    "title": "Enhanced Cascade Prostate Cancer Classifier in mp-MRI Utilizing Recall Feedback Adaptive Loss and Prior Knowledge-Based Feature Extraction",
    "authors": [
      "Kun Luo",
      "Bowen Zheng",
      "Shidong Lv",
      "Jie Tao",
      "Qiang Wei"
    ],
    "abstract": "Prostate cancer is the second most common cancer in males worldwide, and\nmpMRI is commonly used for diagnosis. However, interpreting mpMRI is\nchallenging and requires expertise from radiologists. This highlights the\nurgent need for automated grading in mpMRI. Existing studies lack integration\nof clinical prior information and suffer from uneven training sample\ndistribution due to prevalence. Therefore, we propose a solution that\nincorporates prior knowledge, addresses the issue of uneven medical sample\ndistribution, and maintains high interpretability in mpMRI. Firstly, we\nintroduce Prior Knowledge-Based Feature Extraction, which mathematically models\nthe PI-RADS criteria for prostate cancer as diagnostic information into model\ntraining. Secondly, we propose Adaptive Recall Feedback Loss to address the\nextremely imbalanced data problem. This method adjusts the training dynamically\nbased on accuracy and recall in the validation set, resulting in high accuracy\nand recall simultaneously in the testing set.Thirdly, we design an Enhanced\nCascade Prostate Cancer Classifier that classifies prostate cancer into\ndifferent levels in an interpretable way, which refines the classification\nresults and helps with clinical intervention. Our method is validated through\nexperiments on the PI-CAI dataset and outperforms other methods with a more\nbalanced result in both accuracy and recall rate.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09746v1",
    "published_date": "2024-08-19 07:18:06 UTC",
    "updated_date": "2024-08-19 07:18:06 UTC"
  },
  {
    "arxiv_id": "2408.09743v1",
    "title": "R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation",
    "authors": [
      "Xiao Wang",
      "Yuehang Li",
      "Fuling Wang",
      "Shiao Wang",
      "Chuanfu Li",
      "Bo Jiang"
    ],
    "abstract": "Inspired by the tremendous success of Large Language Models (LLMs), existing\nX-ray medical report generation methods attempt to leverage large models to\nachieve better performance. They usually adopt a Transformer to extract the\nvisual features of a given X-ray image, and then, feed them into the LLM for\ntext generation. How to extract more effective information for the LLMs to help\nthem improve final results is an urgent problem that needs to be solved.\nAdditionally, the use of visual Transformer models also brings high\ncomputational complexity. To address these issues, this paper proposes a novel\ncontext-guided efficient X-ray medical report generation framework.\nSpecifically, we introduce the Mamba as the vision backbone with linear\ncomplexity, and the performance obtained is comparable to that of the strong\nTransformer model. More importantly, we perform context retrieval from the\ntraining set for samples within each mini-batch during the training phase,\nutilizing both positively and negatively related samples to enhance feature\nrepresentation and discriminative learning. Subsequently, we feed the vision\ntokens, context information, and prompt statements to invoke the LLM for\ngenerating high-quality medical reports. Extensive experiments on three X-ray\nreport generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully\nvalidated the effectiveness of our proposed model. The source code of this work\nwill be released on \\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2408.09743v1",
    "published_date": "2024-08-19 07:15:11 UTC",
    "updated_date": "2024-08-19 07:15:11 UTC"
  },
  {
    "arxiv_id": "2408.09742v1",
    "title": "Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs",
    "authors": [
      "Simon D Angus",
      "Lachlan O'Neill"
    ],
    "abstract": "Detecting and quantifying issue framing in textual discourse - the\nperspective one takes to a given topic (e.g. climate science vs. denialism,\nmisogyny vs. gender equality) - is highly valuable to a range of end-users from\nsocial and political scientists to program evaluators and policy analysts.\nHowever, conceptual framing is notoriously challenging for automated natural\nlanguage processing (NLP) methods since the words and phrases used by either\n`side' of an issue are often held in common, with only subtle stylistic\nflourishes separating their use. Here we develop and rigorously evaluate new\ndetection methods for issue framing and narrative analysis within large text\ndatasets. By introducing a novel application of next-token log probabilities\nderived from generative large language models (LLMs) we show that issue framing\ncan be reliably and efficiently detected in large corpora with only a few\nexamples of either perspective on a given issue, a method we call `paired\ncompletion'. Through 192 independent experiments over three novel, synthetic\ndatasets, we evaluate paired completion against prompt-based LLM methods and\nlabelled methods using traditional NLP and recent LLM contextual embeddings. We\nadditionally conduct a cost-based analysis to mark out the feasible set of\nperformant methods at production-level scales, and a model bias analysis.\nTogether, our work demonstrates a feasible path to scalable, accurate and\nlow-bias issue-framing in large corpora.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "econ.GN",
      "q-fin.EC",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09742v1",
    "published_date": "2024-08-19 07:14:15 UTC",
    "updated_date": "2024-08-19 07:14:15 UTC"
  },
  {
    "arxiv_id": "2408.09734v1",
    "title": "Mutually-Aware Feature Learning for Few-Shot Object Counting",
    "authors": [
      "Yerim Jeon",
      "Subeen Lee",
      "Jihwan Kim",
      "Jae-Pil Heo"
    ],
    "abstract": "Few-shot object counting has garnered significant attention for its\npracticality as it aims to count target objects in a query image based on given\nexemplars without the need for additional training. However, there is a\nshortcoming in the prevailing extract-and-match approach: query and exemplar\nfeatures lack interaction during feature extraction since they are extracted\nunaware of each other and later correlated based on similarity. This can lead\nto insufficient target awareness of the extracted features, resulting in target\nconfusion in precisely identifying the actual target when multiple class\nobjects coexist. To address this limitation, we propose a novel framework,\nMutually-Aware FEAture learning(MAFEA), which encodes query and exemplar\nfeatures mutually aware of each other from the outset. By encouraging\ninteraction between query and exemplar features throughout the entire pipeline,\nwe can obtain target-aware features that are robust to a multi-category\nscenario. Furthermore, we introduce a background token to effectively associate\nthe target region of query with exemplars and decouple its background region\nfrom them. Our extensive experiments demonstrate that our model reaches a new\nstate-of-the-art performance on the two challenging benchmarks, FSCD-LVIS and\nFSC-147, with a remarkably reduced degree of the target confusion problem.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to Pattern Recognition",
    "pdf_url": "http://arxiv.org/pdf/2408.09734v1",
    "published_date": "2024-08-19 06:46:24 UTC",
    "updated_date": "2024-08-19 06:46:24 UTC"
  },
  {
    "arxiv_id": "2408.09720v1",
    "title": "Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework",
    "authors": [
      "Jiandong Jin",
      "Xiao Wang",
      "Qian Zhu",
      "Haiyang Wang",
      "Chenglong Li"
    ],
    "abstract": "Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in\nhuman-centered research. However, existing datasets neglect different domains\n(e.g., environments, times, populations, and data sources), only conducting\nsimple random splits, and the performance of these datasets has already\napproached saturation. In the past five years, no large-scale dataset has been\nopened to the public. To address this issue, this paper proposes a new\nlarge-scale, cross-domain pedestrian attribute recognition dataset to fill the\ndata gap, termed MSP60K. It consists of 60,122 images and 57 attribute\nannotations across eight scenarios. Synthetic degradation is also conducted to\nfurther narrow the gap between the dataset and real-world challenging\nscenarios. To establish a more rigorous benchmark, we evaluate 17\nrepresentative PAR models under both random and cross-domain split protocols on\nour dataset. Additionally, we propose an innovative Large Language Model (LLM)\naugmented PAR framework, named LLM-PAR. This framework processes pedestrian\nimages through a Vision Transformer (ViT) backbone to extract features and\nintroduces a multi-embedding query Transformer to learn partial-aware features\nfor attribute classification. Significantly, we enhance this framework with LLM\nfor ensemble learning and visual feature augmentation. Comprehensive\nexperiments across multiple PAR benchmark datasets have thoroughly validated\nthe efficacy of our proposed framework. The dataset and source code\naccompanying this paper will be made publicly available at\n\\url{https://github.com/Event-AHU/OpenPAR}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "MSP60K PAR Benchmark Dataset, LLM based PAR model, In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2408.09720v1",
    "published_date": "2024-08-19 06:19:31 UTC",
    "updated_date": "2024-08-19 06:19:31 UTC"
  },
  {
    "arxiv_id": "2408.09715v2",
    "title": "HYDEN: Hyperbolic Density Representations for Medical Images and Reports",
    "authors": [
      "Zhi Qiao",
      "Linbin Han",
      "Xiantong Zhen",
      "Jia-Hong Gao",
      "Zhen Qian"
    ],
    "abstract": "In light of the inherent entailment relations between images and text,\nhyperbolic point vector embeddings, leveraging the hierarchical modeling\nadvantages of hyperbolic space, have been utilized for visual semantic\nrepresentation learning. However, point vector embedding approaches fail to\naddress the issue of semantic uncertainty, where an image may have multiple\ninterpretations, and text may refer to different images, a phenomenon\nparticularly prevalent in the medical domain. Therefor, we propose\n\\textbf{HYDEN}, a novel hyperbolic density embedding based image-text\nrepresentation learning approach tailored for specific medical domain data.\nThis method integrates text-aware local features alongside global features from\nimages, mapping image-text features to density features in hyperbolic space via\nusing hyperbolic pseudo-Gaussian distributions. An encapsulation loss function\nis employed to model the partial order relations between image-text density\ndistributions. Experimental results demonstrate the interpretability of our\napproach and its superior performance compared to the baseline methods across\nvarious zero-shot tasks and different datasets.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09715v2",
    "published_date": "2024-08-19 06:06:30 UTC",
    "updated_date": "2024-08-20 03:13:41 UTC"
  },
  {
    "arxiv_id": "2408.09703v1",
    "title": "Partial-Multivariate Model for Forecasting",
    "authors": [
      "Jaehoon Lee",
      "Hankook Lee",
      "Sungik Choi",
      "Sungjun Cho",
      "Moontae Lee"
    ],
    "abstract": "When solving forecasting problems including multiple time-series features,\nexisting approaches often fall into two extreme categories, depending on\nwhether to utilize inter-feature information: univariate and\ncomplete-multivariate models. Unlike univariate cases which ignore the\ninformation, complete-multivariate models compute relationships among a\ncomplete set of features. However, despite the potential advantage of\nleveraging the additional information, complete-multivariate models sometimes\nunderperform univariate ones. Therefore, our research aims to explore a middle\nground between these two by introducing what we term Partial-Multivariate\nmodels where a neural network captures only partial relationships, that is,\ndependencies within subsets of all features. To this end, we propose PMformer,\na Transformer-based partial-multivariate model, with its training algorithm. We\ndemonstrate that PMformer outperforms various univariate and\ncomplete-multivariate models, providing a theoretical rationale and empirical\nanalysis for its superiority. Additionally, by proposing an inference technique\nfor PMformer, the forecasting accuracy is further enhanced. Finally, we\nhighlight other advantages of PMformer: efficiency and robustness under missing\nfeatures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.09703v1",
    "published_date": "2024-08-19 05:18:50 UTC",
    "updated_date": "2024-08-19 05:18:50 UTC"
  },
  {
    "arxiv_id": "2408.10285v1",
    "title": "BatGPT-Chem: A Foundation Large Model For Retrosynthesis Prediction",
    "authors": [
      "Yifei Yang",
      "Runhan Shi",
      "Zuchao Li",
      "Shu Jiang",
      "Bao-Liang Lu",
      "Yang Yang",
      "Hai Zhao"
    ],
    "abstract": "Retrosynthesis analysis is pivotal yet challenging in drug discovery and\norganic chemistry. Despite the proliferation of computational tools over the\npast decade, AI-based systems often fall short in generalizing across diverse\nreaction types and exploring alternative synthetic pathways. This paper\npresents BatGPT-Chem, a large language model with 15 billion parameters,\ntailored for enhanced retrosynthesis prediction. Integrating chemical tasks via\na unified framework of natural language and SMILES notation, this approach\nsynthesizes extensive instructional data from an expansive chemical database.\nEmploying both autoregressive and bidirectional training techniques across over\none hundred million instances, BatGPT-Chem captures a broad spectrum of\nchemical knowledge, enabling precise prediction of reaction conditions and\nexhibiting strong zero-shot capabilities. Superior to existing AI methods, our\nmodel demonstrates significant advancements in generating effective strategies\nfor complex molecules, as validated by stringent benchmark tests. BatGPT-Chem\nnot only boosts the efficiency and creativity of retrosynthetic analysis but\nalso establishes a new standard for computational tools in synthetic design.\nThis development empowers chemists to adeptly address the synthesis of novel\ncompounds, potentially expediting the innovation cycle in drug manufacturing\nand materials science. We release our trial platform at\n\\url{https://www.batgpt.net/dapp/chem}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.10285v1",
    "published_date": "2024-08-19 05:17:40 UTC",
    "updated_date": "2024-08-19 05:17:40 UTC"
  },
  {
    "arxiv_id": "2408.09702v1",
    "title": "Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering",
    "authors": [
      "Ruofan Liang",
      "Zan Gojcic",
      "Merlin Nimier-David",
      "David Acuna",
      "Nandita Vijaykumar",
      "Sanja Fidler",
      "Zian Wang"
    ],
    "abstract": "The correct insertion of virtual objects in images of real-world scenes\nrequires a deep understanding of the scene's lighting, geometry and materials,\nas well as the image formation process. While recent large-scale diffusion\nmodels have shown strong generative and inpainting capabilities, we find that\ncurrent models do not sufficiently \"understand\" the scene shown in a single\npicture to generate consistent lighting effects (shadows, bright reflections,\netc.) while preserving the identity and details of the composited object. We\npropose using a personalized large diffusion model as guidance to a physically\nbased inverse rendering process. Our method recovers scene lighting and\ntone-mapping parameters, allowing the photorealistic composition of arbitrary\nvirtual objects in single frames or videos of indoor or outdoor scenes. Our\nphysically based pipeline further enables automatic materials and tone-mapping\nrefinement.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024, Project page:\n  https://research.nvidia.com/labs/toronto-ai/DiPIR/",
    "pdf_url": "http://arxiv.org/pdf/2408.09702v1",
    "published_date": "2024-08-19 05:15:45 UTC",
    "updated_date": "2024-08-19 05:15:45 UTC"
  },
  {
    "arxiv_id": "2408.09698v5",
    "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation",
    "authors": [
      "Yuyang Ye",
      "Zhi Zheng",
      "Yishan Shen",
      "Tianshu Wang",
      "Hengruo Zhang",
      "Peijun Zhu",
      "Runlong Yu",
      "Kai Zhang",
      "Hui Xiong"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09698v5",
    "published_date": "2024-08-19 04:44:32 UTC",
    "updated_date": "2025-01-13 17:48:09 UTC"
  },
  {
    "arxiv_id": "2408.09695v1",
    "title": "LightWeather: Harnessing Absolute Positional Encoding to Efficient and Scalable Global Weather Forecasting",
    "authors": [
      "Yisong Fu",
      "Fei Wang",
      "Zezhi Shao",
      "Chengqing Yu",
      "Yujie Li",
      "Zhao Chen",
      "Zhulin An",
      "Yongjun Xu"
    ],
    "abstract": "Recently, Transformers have gained traction in weather forecasting for their\ncapability to capture long-term spatial-temporal correlations. However, their\ncomplex architectures result in large parameter counts and extended training\ntimes, limiting their practical application and scalability to global-scale\nforecasting. This paper aims to explore the key factor for accurate weather\nforecasting and design more efficient solutions. Interestingly, our empirical\nfindings reveal that absolute positional encoding is what really works in\nTransformer-based weather forecasting models, which can explicitly model the\nspatial-temporal correlations even without attention mechanisms. We\ntheoretically prove that its effectiveness stems from the integration of\ngeographical coordinates and real-world time features, which are intrinsically\nrelated to the dynamics of weather. Based on this, we propose LightWeather, a\nlightweight and effective model for station-based global weather forecasting.\nWe employ absolute positional encoding and a simple MLP in place of other\ncomponents of Transformer. With under 30k parameters and less than one hour of\ntraining time, LightWeather achieves state-of-the-art performance on global\nweather datasets compared to other advanced DL methods. The results underscore\nthe superiority of integrating spatial-temporal knowledge over complex\narchitectures, providing novel insights for DL in weather forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09695v1",
    "published_date": "2024-08-19 04:23:40 UTC",
    "updated_date": "2024-08-19 04:23:40 UTC"
  },
  {
    "arxiv_id": "2408.09682v1",
    "title": "Simulating Field Experiments with Large Language Models",
    "authors": [
      "Yaoyu Chen",
      "Yuheng Hu",
      "Yingda Lu"
    ],
    "abstract": "Prevailing large language models (LLMs) are capable of human responses\nsimulation through its unprecedented content generation and reasoning\nabilities. However, it is not clear whether and how to leverage LLMs to\nsimulate field experiments. In this paper, we propose and evaluate two\nprompting strategies: the observer mode that allows a direct prediction on main\nconclusions and the participant mode that simulates distributions of responses\nfrom participants. Using this approach, we examine fifteen well cited field\nexperimental papers published in INFORMS and MISQ, finding encouraging\nalignments between simulated experimental results and the actual results in\ncertain scenarios. We further identify topics of which LLMs underperform,\nincluding gender difference and social norms related research. Additionally,\nthe automatic and standardized workflow proposed in this paper enables the\npossibility of a large-scale screening of more papers with field experiments.\nThis paper pioneers the utilization of large language models (LLMs) for\nsimulating field experiments, presenting a significant extension to previous\nwork which focused solely on lab environments. By introducing two novel\nprompting strategies, observer and participant modes, we demonstrate the\nability of LLMs to both predict outcomes and replicate participant responses\nwithin complex field settings. Our findings indicate a promising alignment with\nactual experimental results in certain scenarios, achieving a stimulation\naccuracy of 66% in observer mode. This study expands the scope of potential\napplications for LLMs and illustrates their utility in assisting researchers\nprior to engaging in expensive field experiments. Moreover, it sheds light on\nthe boundaries of LLMs when used in simulating field experiments, serving as a\ncautionary note for researchers considering the integration of LLMs into their\nexperimental toolkit.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 5 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.09682v1",
    "published_date": "2024-08-19 03:41:43 UTC",
    "updated_date": "2024-08-19 03:41:43 UTC"
  },
  {
    "arxiv_id": "2408.09680v2",
    "title": "MambaLoc: Efficient Camera Localisation via State Space Model",
    "authors": [
      "Jialu Wang",
      "Kaichen Zhou",
      "Andrew Markham",
      "Niki Trigoni"
    ],
    "abstract": "Location information is pivotal for the automation and intelligence of\nterminal devices and edge-cloud IoT systems, such as autonomous vehicles and\naugmented reality. However, achieving reliable positioning across diverse IoT\napplications remains challenging due to significant training costs and the\nnecessity of densely collected data. To tackle these issues, we have\ninnovatively applied the selective state space (SSM) model to visual\nlocalization, introducing a new model named MambaLoc. The proposed model\ndemonstrates exceptional training efficiency by capitalizing on the SSM model's\nstrengths in efficient feature extraction, rapid computation, and memory\noptimization, and it further ensures robustness in sparse data environments due\nto its parameter sparsity. Additionally, we propose the Global Information\nSelector (GIS), which leverages selective SSM to implicitly achieve the\nefficient global feature extraction capabilities of Non-local Neural Networks.\nThis design leverages the computational efficiency of the SSM model alongside\nthe Non-local Neural Networks' capacity to capture long-range dependencies with\nminimal layers. Consequently, the GIS enables effective global information\ncapture while significantly accelerating convergence. Our extensive\nexperimental validation using public indoor and outdoor datasets first\ndemonstrates our model's effectiveness, followed by evidence of its versatility\nwith various existing localization models. Our code and models are publicly\navailable to support further research and development in this area.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09680v2",
    "published_date": "2024-08-19 03:38:29 UTC",
    "updated_date": "2024-08-20 08:44:42 UTC"
  },
  {
    "arxiv_id": "2408.09675v1",
    "title": "Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey",
    "authors": [
      "Ruiqi Zhang",
      "Jing Hou",
      "Florian Walter",
      "Shangding Gu",
      "Jiayi Guan",
      "Florian Röhrbein",
      "Yali Du",
      "Panpan Cai",
      "Guang Chen",
      "Alois Knoll"
    ],
    "abstract": "Reinforcement Learning (RL) is a potent tool for sequential decision-making\nand has achieved performance surpassing human capabilities across many\nchallenging real-world tasks. As the extension of RL in the multi-agent system\ndomain, multi-agent RL (MARL) not only need to learn the control policy but\nalso requires consideration regarding interactions with all other agents in the\nenvironment, mutual influences among different system components, and the\ndistribution of computational resources. This augments the complexity of\nalgorithmic design and poses higher requirements on computational resources.\nSimultaneously, simulators are crucial to obtain realistic data, which is the\nfundamentals of RL. In this paper, we first propose a series of metrics of\nsimulators and summarize the features of existing benchmarks. Second, to ease\ncomprehension, we recall the foundational knowledge and then synthesize the\nrecently advanced studies of MARL-related autonomous driving and intelligent\ntransportation systems. Specifically, we examine their environmental modeling,\nstate representation, perception units, and algorithm design. Conclusively, we\ndiscuss open challenges as well as prospects and opportunities. We hope this\npaper can help the researchers integrate MARL technologies and trigger more\ninsightful ideas toward the intelligent and autonomous driving.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 6 figures and 2 tables. Submitted to IEEE Journal",
    "pdf_url": "http://arxiv.org/pdf/2408.09675v1",
    "published_date": "2024-08-19 03:31:20 UTC",
    "updated_date": "2024-08-19 03:31:20 UTC"
  },
  {
    "arxiv_id": "2408.09656v2",
    "title": "A Comparison of Large Language Model and Human Performance on Random Number Generation Tasks",
    "authors": [
      "Rachel M. Harrison"
    ],
    "abstract": "Random Number Generation Tasks (RNGTs) are used in psychology for examining\nhow humans generate sequences devoid of predictable patterns. By adapting an\nexisting human RNGT for an LLM-compatible environment, this preliminary study\ntests whether ChatGPT-3.5, a large language model (LLM) trained on\nhuman-generated text, exhibits human-like cognitive biases when generating\nrandom number sequences. Initial findings indicate that ChatGPT-3.5 more\neffectively avoids repetitive and sequential patterns compared to humans, with\nnotably lower repeat frequencies and adjacent number frequencies. Continued\nresearch into different models, parameters, and prompting methodologies will\ndeepen our understanding of how LLMs can more closely mimic human random\ngeneration behaviors, while also broadening their applications in cognitive and\nbehavioral science research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09656v2",
    "published_date": "2024-08-19 02:34:15 UTC",
    "updated_date": "2024-08-20 02:05:46 UTC"
  },
  {
    "arxiv_id": "2408.11869v3",
    "title": "ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA",
    "authors": [
      "Jiaang Li",
      "Quan Wang",
      "Zhongnan Wang",
      "Yongdong Zhang",
      "Zhendong Mao"
    ],
    "abstract": "Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and result in a significant\nforgetting effect in lifelong editing scenarios, where sequential edits are\nconducted over time. Previous approaches manage sequential edits by freezing\noriginal parameters and discretely allocating new parameters for each knowledge\nupdate. However, these methods lack robustness to minor input variations due to\nthe discrete mapping between data and parameters. To overcome this challenge,\nwe propose ELDER, a novel approach to create a continuous association between\ndata and adapters. ELDER integrates multiple LoRAs through a router network and\nis trained to establish a smooth data-adapter association, thereby enhancing\nthe edit robustness and generalization of semantically equivalent inputs. To\nensure inputs containing the same knowledge will be processed by the same\nLoRAs, we design a novel loss to guide the model link LoRA allocations with\nedit knowledge. Furthermore, we propose a deferral mechanism to retain the\noriginal LLM capabilities post-edit. Extensive experiments on GPT-2 XL and\nLLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong\nsetting, outperforming eight baselines while exhibiting strong scalability and\npreserving LLMs' general abilities on downstream tasks. Our code is available\nat https://github.com/JiaangL/ELDER.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2408.11869v3",
    "published_date": "2024-08-19 02:27:00 UTC",
    "updated_date": "2025-01-14 04:25:23 UTC"
  },
  {
    "arxiv_id": "2408.09651v1",
    "title": "Data-driven Conditional Instrumental Variables for Debiasing Recommender Systems",
    "authors": [
      "Zhirong Huang",
      "Shichao Zhang",
      "Debo Cheng",
      "Jiuyong Li",
      "Lin Liu",
      "Guangquan Lu"
    ],
    "abstract": "In recommender systems, latent variables can cause user-item interaction data\nto deviate from true user preferences. This biased data is then used to train\nrecommendation models, further amplifying the bias and ultimately compromising\nboth recommendation accuracy and user satisfaction. Instrumental Variable (IV)\nmethods are effective tools for addressing the confounding bias introduced by\nlatent variables; however, identifying a valid IV is often challenging. To\novercome this issue, we propose a novel data-driven conditional IV (CIV)\ndebiasing method for recommender systems, called CIV4Rec. CIV4Rec automatically\ngenerates valid CIVs and their corresponding conditioning sets directly from\ninteraction data, significantly reducing the complexity of IV selection while\neffectively mitigating the confounding bias caused by latent variables in\nrecommender systems. Specifically, CIV4Rec leverages a variational autoencoder\n(VAE) to generate the representations of the CIV and its conditional set from\ninteraction data, followed by the application of least squares to derive causal\nrepresentations for click prediction. Extensive experiments on two real-world\ndatasets, Movielens-10M and Douban-Movie, demonstrate that our CIV4Rec\nsuccessfully identifies valid CIVs, effectively reduces bias, and consequently\nimproves recommendation accuracy.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09651v1",
    "published_date": "2024-08-19 02:17:22 UTC",
    "updated_date": "2024-08-19 02:17:22 UTC"
  },
  {
    "arxiv_id": "2408.09650v1",
    "title": "ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective Image Enhancement",
    "authors": [
      "Eashan Adhikarla",
      "Kai Zhang",
      "John Nicholson",
      "Brian D. Davison"
    ],
    "abstract": "Low-light image enhancement remains a challenging task in computer vision,\nwith existing state-of-the-art models often limited by hardware constraints and\ncomputational inefficiencies, particularly in handling high-resolution images.\nRecent foundation models, such as transformers and diffusion models, despite\ntheir efficacy in various domains, are limited in use on edge devices due to\ntheir computational complexity and slow inference times. We introduce\nExpoMamba, a novel architecture that integrates components of the frequency\nstate space within a modified U-Net, offering a blend of efficiency and\neffectiveness. This model is specifically optimized to address mixed exposure\nchallenges, a common issue in low-light image enhancement, while ensuring\ncomputational efficiency. Our experiments demonstrate that ExpoMamba enhances\nlow-light images up to 2-3x faster than traditional models with an inference\ntime of 36.6 ms and achieves a PSNR improvement of approximately 15-20% over\ncompeting models, making it highly suitable for real-time image processing\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09650v1",
    "published_date": "2024-08-19 02:16:47 UTC",
    "updated_date": "2024-08-19 02:16:47 UTC"
  },
  {
    "arxiv_id": "2408.09649v2",
    "title": "Deep Learning-based Machine Condition Diagnosis using Short-time Fourier Transformation Variants",
    "authors": [
      "Eduardo Jr Piedad",
      "Zherish Galvin Mayordo",
      "Eduardo Prieto-Araujo",
      "Oriol Gomis-Bellmunt"
    ],
    "abstract": "In motor condition diagnosis, electrical current signature serves as an\nalternative feature to vibration-based sensor data, which is a more expensive\nand invasive method. Machine learning (ML) techniques have been emerging in\ndiagnosing motor conditions using only motor phase current signals. This study\nconverts time-series motor current signals to time-frequency 2D plots using\nShort-time Fourier Transform (STFT) methods. The motor current signal dataset\nconsists of 3,750 sample points with five classes - one healthy and four\nsynthetically-applied motor fault conditions, and with five loading conditions:\n0, 25, 50, 75, and 100%. Five transformation methods are used on the dataset:\nnon-overlap and overlap STFTs, non-overlap and overlap realigned STFTs, and\nsynchrosqueezed STFT. Then, deep learning (DL) models based on the previous\nConvolutional Neural Network (CNN) architecture are trained and validated from\ngenerated plots of each method. The DL models of overlap-STFT, overlap R-STFT,\nnon-overlap STFT, non-overlap R-STFT, and synchrosqueezed-STFT performed\nexceptionally with an average accuracy of 97.65, 96.03, 96.08, 96.32, and\n88.27%, respectively. Four methods outperformed the previous best ML method\nwith 93.20% accuracy, while all five outperformed previous 2D-plot-based\nmethods with accuracy of 80.25, 74.80, and 82.80%, respectively, using the same\ndataset, same DL architecture, and validation steps.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "4 pages, 6 images, presented at 2024 International Conference on\n  Diagnostics in Electrical Engineering (Diagnostika)",
    "pdf_url": "http://arxiv.org/pdf/2408.09649v2",
    "published_date": "2024-08-19 02:16:17 UTC",
    "updated_date": "2024-10-14 10:05:49 UTC"
  },
  {
    "arxiv_id": "2408.09646v1",
    "title": "Debiased Contrastive Representation Learning for Mitigating Dual Biases in Recommender Systems",
    "authors": [
      "Zhirong Huang",
      "Shichao Zhang",
      "Debo Cheng",
      "Jiuyong Li",
      "Lin Liu",
      "Guixian Zhang"
    ],
    "abstract": "In recommender systems, popularity and conformity biases undermine\nrecommender effectiveness by disproportionately favouring popular items,\nleading to their over-representation in recommendation lists and causing an\nunbalanced distribution of user-item historical data. We construct a causal\ngraph to address both biases and describe the abstract data generation\nmechanism. Then, we use it as a guide to develop a novel Debiased Contrastive\nLearning framework for Mitigating Dual Biases, called DCLMDB. In DCLMDB, both\npopularity bias and conformity bias are handled in the model training process\nby contrastive learning to ensure that user choices and recommended items are\nnot unduly influenced by conformity and popularity. Extensive experiments on\ntwo real-world datasets, Movielens-10M and Netflix, show that DCLMDB can\neffectively reduce the dual biases, as well as significantly enhance the\naccuracy and diversity of recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09646v1",
    "published_date": "2024-08-19 02:12:40 UTC",
    "updated_date": "2024-08-19 02:12:40 UTC"
  },
  {
    "arxiv_id": "2408.09644v2",
    "title": "Exploring Wavelet Transformations for Deep Learning-based Machine Condition Diagnosis",
    "authors": [
      "Eduardo Jr Piedad",
      "Christian Ainsley Del Rosario",
      "Eduardo Prieto-Araujo",
      "Oriol Gomis-Bellmunt"
    ],
    "abstract": "Deep learning (DL) strategies have recently been utilized to diagnose motor\nfaults by simply analyzing motor phase current signals, offering a less costly\nand non-intrusive alternative to vibration sensors. This research transforms\nthese time-series current signals into time-frequency 2D representations via\nWavelet Transform (WT). The dataset for motor current signals includes 3,750\ndata points across five categories: one representing normal conditions and four\nrepresenting artificially induced faults, each under five different load\nconditions: 0, 25, 50, 75, and 100%. The study employs five WT-based\ntechniques: WT-Amor, WT-Bump, WT-Morse, WSST-Amor, and WSST-Bump. Subsequently,\nfive DL models adopting prior Convolutional Neural Network (CNN) architecture\nwere developed and tested using the transformed 2D plots from each method. The\nDL models for WT-Amor, WT-Bump, and WT-Morse showed remarkable effectiveness\nwith peak model accuracy of 90.93, 89.20, and 93.73%, respectively, surpassing\nprevious 2D-image-based methods that recorded accuracy of 80.25, 74.80, and\n82.80% respectively using the identical dataset and validation protocol.\nNotably, the WT-Morse approach slightly exceeded the formerly highest ML\ntechnique, achieving a 93.20% accuracy. However, the two WSST methods that\nutilized synchrosqueezing techniques faced difficulty accurately classifying\nmotor faults. The performance of Wavelet-based deep learning methods offers a\ncompelling alternative for machine condition monitoring.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "eess.SP",
    "comment": "4 pages, 6 figures, presented at the 2024 International Conference on\n  Diagnostics in Electrical Engineering (Diagnostika)",
    "pdf_url": "http://arxiv.org/pdf/2408.09644v2",
    "published_date": "2024-08-19 02:06:33 UTC",
    "updated_date": "2024-10-14 10:08:09 UTC"
  },
  {
    "arxiv_id": "2408.11868v1",
    "title": "Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores",
    "authors": [
      "Jun Lu",
      "David Li",
      "Bill Ding",
      "Yu Kang"
    ],
    "abstract": "This paper presents an approach to improve text embedding models through\ncontrastive fine-tuning on small datasets augmented with expert scores. It\nfocuses on enhancing semantic textual similarity tasks and addressing text\nretrieval problems. The proposed method uses soft labels derived from\nexpert-augmented scores to fine-tune embedding models, preserving their\nversatility and ensuring retrieval capability is improved. The paper evaluates\nthe method using a Q\\&A dataset from an online shopping website and eight\nexpert models. Results show improved performance over a benchmark model across\nmultiple metrics on various retrieval tasks from the massive text embedding\nbenchmark (MTEB). The method is cost-effective and practical for real-world\napplications, especially when labeled data is scarce.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11868v1",
    "published_date": "2024-08-19 01:59:25 UTC",
    "updated_date": "2024-08-19 01:59:25 UTC"
  },
  {
    "arxiv_id": "2408.09639v2",
    "title": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments",
    "authors": [
      "Yusuke Ide",
      "Yuto Nishida",
      "Justin Vasselli",
      "Miyu Oba",
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ],
    "abstract": "The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where the LMs are presented with a pair\nof acceptable and unacceptable sentences and required to judge which is more\nacceptable. Conventional approaches directly compare sentence probabilities\nassigned by LMs, but recent large language models (LLMs) are trained to perform\ntasks via prompting, and thus, the raw probabilities they assign may not fully\nreflect their grammatical knowledge. In this study, we attempt to derive more\naccurate acceptability judgments from LLMs using prompts and templates. Through\nextensive experiments in English and Chinese, we compare nine judgment methods\nand find two of them, a probability readout method -- in-template LP and a\nprompt-based method -- Yes/No probability computing, achieve higher accuracy\nthan the conventional ones. Our analysis reveals that these methods excel in\ndifferent linguistic phenomena, suggesting they access different aspects of\nLLMs' knowledge. We also find that ensembling the two methods outperforms\nsingle methods. Consequently, we recommend these techniques, either\nindividually or ensembled, as more effective alternatives to conventional\napproaches for assessing grammatical knowledge in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 main",
    "pdf_url": "http://arxiv.org/pdf/2408.09639v2",
    "published_date": "2024-08-19 01:53:47 UTC",
    "updated_date": "2025-02-07 07:02:26 UTC"
  },
  {
    "arxiv_id": "2408.09635v1",
    "title": "Meta-Learning on Augmented Gene Expression Profiles for Enhanced Lung Cancer Detection",
    "authors": [
      "Arya Hadizadeh Moghaddam",
      "Mohsen Nayebi Kerdabadi",
      "Cuncong Zhong",
      "Zijun Yao"
    ],
    "abstract": "Gene expression profiles obtained through DNA microarray have proven\nsuccessful in providing critical information for cancer detection classifiers.\nHowever, the limited number of samples in these datasets poses a challenge to\nemploy complex methodologies such as deep neural networks for sophisticated\nanalysis. To address this \"small data\" dilemma, Meta-Learning has been\nintroduced as a solution to enhance the optimization of machine learning models\nby utilizing similar datasets, thereby facilitating a quicker adaptation to\ntarget datasets without the requirement of sufficient samples. In this study,\nwe present a meta-learning-based approach for predicting lung cancer from gene\nexpression profiles. We apply this framework to well-established deep learning\nmethodologies and employ four distinct datasets for the meta-learning tasks,\nwhere one as the target dataset and the rest as source datasets. Our approach\nis evaluated against both traditional and deep learning methodologies, and the\nresults show the superior performance of meta-learning on augmented source data\ncompared to the baselines trained on single datasets. Moreover, we conduct the\ncomparative analysis between meta-learning and transfer learning methodologies\nto highlight the efficiency of the proposed approach in addressing the\nchallenges associated with limited sample sizes. Finally, we incorporate the\nexplainability study to illustrate the distinctiveness of decisions made by\nmeta-learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AMIA 2024 Annual Symposium",
    "pdf_url": "http://arxiv.org/pdf/2408.09635v1",
    "published_date": "2024-08-19 01:39:12 UTC",
    "updated_date": "2024-08-19 01:39:12 UTC"
  },
  {
    "arxiv_id": "2408.09626v1",
    "title": "On the Foundations of Conflict-Driven Solving for Hybrid MKNF Knowledge Bases",
    "authors": [
      "Riley Kinahan",
      "Spencer Killen",
      "Kevin Wan",
      "Jia-Huai You"
    ],
    "abstract": "Hybrid MKNF Knowledge Bases (HMKNF-KBs) constitute a formalism for tightly\nintegrated reasoning over closed-world rules and open-world ontologies. This\napproach allows for accurate modeling of real-world systems, which often rely\non both categorical and normative reasoning. Conflict-driven solving is the\nleading approach for computationally hard problems, such as satisfiability\n(SAT) and answer set programming (ASP), in which MKNF is rooted. This paper\ninvestigates the theoretical underpinnings required for a conflict-driven\nsolver of HMKNF-KBs. The approach defines a set of completion and loop\nformulas, whose satisfaction characterizes MKNF models. This forms the basis\nfor a set of nogoods, which in turn can be used as the backbone for a\nconflict-driven solver.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09626v1",
    "published_date": "2024-08-19 01:13:02 UTC",
    "updated_date": "2024-08-19 01:13:02 UTC"
  },
  {
    "arxiv_id": "2408.09624v1",
    "title": "Attention is a smoothed cubic spline",
    "authors": [
      "Zehua Lai",
      "Lek-Heng Lim",
      "Yucong Liu"
    ],
    "abstract": "We highlight a perhaps important but hitherto unobserved insight: The\nattention module in a transformer is a smoothed cubic spline. Viewed in this\nmanner, this mysterious but critical component of a transformer becomes a\nnatural development of an old notion deeply entrenched in classical\napproximation theory. More precisely, we show that with ReLU-activation,\nattention, masked attention, encoder-decoder attention are all cubic splines.\nAs every component in a transformer is constructed out of compositions of\nvarious attention modules (= cubic splines) and feed forward neural networks (=\nlinear splines), all its components -- encoder, decoder, and encoder-decoder\nblocks; multilayered encoders and decoders; the transformer itself -- are cubic\nor higher-order splines. If we assume the Pierce-Birkhoff conjecture, then the\nconverse also holds, i.e., every spline is a ReLU-activated encoder. Since a\nspline is generally just $C^2$, one way to obtain a smoothed $C^\\infty$-version\nis by replacing ReLU with a smooth activation; and if this activation is chosen\nto be SoftMax, we recover the original transformer as proposed by Vaswani et\nal. This insight sheds light on the nature of the transformer by casting it\nentirely in terms of splines, one of the best known and thoroughly understood\nobjects in applied mathematics.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "26B40, 41A15, 65D07, 68T01, 14P10, 13J30"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09624v1",
    "published_date": "2024-08-19 00:56:44 UTC",
    "updated_date": "2024-08-19 00:56:44 UTC"
  }
]
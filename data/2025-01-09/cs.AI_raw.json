[
  {
    "arxiv_id": "2501.05614v1",
    "title": "Watermarking Graph Neural Networks via Explanations for Ownership Protection",
    "authors": [
      "Jane Downer",
      "Ren Wang",
      "Binghui Wang"
    ],
    "abstract": "Graph Neural Networks (GNNs) are the mainstream method to learn pervasive\ngraph data and are widely deployed in industry, making their intellectual\nproperty valuable. However, protecting GNNs from unauthorized use remains a\nchallenge. Watermarking, which embeds ownership information into a model, is a\npotential solution. However, existing watermarking methods have two key\nlimitations: First, almost all of them focus on non-graph data, with\nwatermarking GNNs for complex graph data largely unexplored. Second, the de\nfacto backdoor-based watermarking methods pollute training data and induce\nownership ambiguity through intentional misclassification. Our\nexplanation-based watermarking inherits the strengths of backdoor-based methods\n(e.g., robust to watermark removal attacks), but avoids data pollution and\neliminates intentional misclassification. In particular, our method learns to\nembed the watermark in GNN explanations such that this unique watermark is\nstatistically distinct from other potential solutions, and ownership claims\nmust show statistical significance to be verified. We theoretically prove that,\neven with full knowledge of our method, locating the watermark is an NP-hard\nproblem. Empirically, our method manifests robustness to removal attacks like\nfine-tuning and pruning. By addressing these challenges, our approach marks a\nsignificant advancement in protecting GNN intellectual property.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05614v1",
    "published_date": "2025-01-09 23:25:06 UTC",
    "updated_date": "2025-01-09 23:25:06 UTC"
  },
  {
    "arxiv_id": "2501.05605v1",
    "title": "Advancing Personalized Learning Analysis via an Innovative Domain Knowledge Informed Attention-based Knowledge Tracing Method",
    "authors": [
      "Shubham Kose",
      "Jin Wei-Kocsis"
    ],
    "abstract": "Emerging Knowledge Tracing (KT) models, particularly deep learning and\nattention-based Knowledge Tracing, have shown great potential in realizing\npersonalized learning analysis via prediction of students' future performance\nbased on their past interactions. The existing methods mainly focus on\nimmediate past interactions or individual concepts without accounting for\ndependencies between knowledge concept, referred as knowledge concept routes,\nthat can be critical to advance the understanding the students' learning\noutcomes. To address this, in this paper, we propose an innovative\nattention-based method by effectively incorporating the domain knowledge of\nknowledge concept routes in the given curriculum. Additionally, we leverage\nXES3G5M dataset, a benchmark dataset with rich auxiliary information for\nknowledge concept routes, to evaluate and compare the performance of our\nproposed method to the seven State-of-the-art (SOTA) deep learning models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05605v1",
    "published_date": "2025-01-09 22:41:50 UTC",
    "updated_date": "2025-01-09 22:41:50 UTC"
  },
  {
    "arxiv_id": "2501.05567v1",
    "title": "Approximate Supervised Object Distance Estimation on Unmanned Surface Vehicles",
    "authors": [
      "Benjamin Kiefer",
      "Yitong Quan",
      "Andreas Zell"
    ],
    "abstract": "Unmanned surface vehicles (USVs) and boats are increasingly important in\nmaritime operations, yet their deployment is limited due to costly sensors and\ncomplexity. LiDAR, radar, and depth cameras are either costly, yield sparse\npoint clouds or are noisy, and require extensive calibration. Here, we\nintroduce a novel approach for approximate distance estimation in USVs using\nsupervised object detection. We collected a dataset comprising images with\nmanually annotated bounding boxes and corresponding distance measurements.\nLeveraging this data, we propose a specialized branch of an object detection\nmodel, not only to detect objects but also to predict their distances from the\nUSV. This method offers a cost-efficient and intuitive alternative to\nconventional distance measurement techniques, aligning more closely with human\nestimation capabilities. We demonstrate its application in a marine assistance\nsystem that alerts operators to nearby objects such as boats, buoys, or other\nwaterborne hazards.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05567v1",
    "published_date": "2025-01-09 20:34:36 UTC",
    "updated_date": "2025-01-09 20:34:36 UTC"
  },
  {
    "arxiv_id": "2501.05566v1",
    "title": "Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene Understanding",
    "authors": [
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar",
      "Andry Rakotonirainy",
      "Taqwa I. Alhadidi",
      "Ahmed Jaber",
      "Mohammad Abu Tami"
    ],
    "abstract": "Scene understanding is essential for enhancing driver safety, generating\nhuman-centric explanations for Automated Vehicle (AV) decisions, and leveraging\nArtificial Intelligence (AI) for retrospective driving video analysis. This\nstudy developed a dynamic scene retrieval system using Contrastive\nLanguage-Image Pretraining (CLIP) models, which can be optimized for real-time\ndeployment on edge devices. The proposed system outperforms state-of-the-art\nin-context learning methods, including the zero-shot capabilities of GPT-4o,\nparticularly in complex scenarios. By conducting frame-level analysis on the\nHonda Scenes Dataset, which contains a collection of about 80 hours of\nannotated driving videos capturing diverse real-world road and weather\nconditions, our study highlights the robustness of CLIP models in learning\nvisual concepts from natural language supervision. Results also showed that\nfine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly\nimproved scene classification, achieving a top F1 score of 91.1%. These results\ndemonstrate the ability of the system to deliver rapid and precise scene\nrecognition, which can be used to meet the critical requirements of Advanced\nDriver Assistance Systems (ADAS). This study shows the potential of CLIP models\nto provide scalable and efficient frameworks for dynamic scene understanding\nand classification. Furthermore, this work lays the groundwork for advanced\nautonomous vehicle technologies by fostering a deeper understanding of driver\nbehavior, road conditions, and safety-critical scenarios, marking a significant\nstep toward smarter, safer, and more context-aware autonomous driving systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05566v1",
    "published_date": "2025-01-09 20:29:31 UTC",
    "updated_date": "2025-01-09 20:29:31 UTC"
  },
  {
    "arxiv_id": "2501.05559v1",
    "title": "Soup to go: mitigating forgetting during continual learning with model averaging",
    "authors": [
      "Anat Kleiman",
      "Gintare Karolina Dziugaite",
      "Jonathan Frankle",
      "Sham Kakade",
      "Mansheej Paul"
    ],
    "abstract": "In continual learning, where task data arrives in a sequence, fine-tuning on\nlater tasks will often lead to performance degradation on earlier tasks. This\nis especially pronounced when these tasks come from diverse domains. In this\nsetting, how can we mitigate catastrophic forgetting of earlier tasks and\nretain what the model has learned with minimal computational expenses? Inspired\nby other merging methods, and L2-regression, we propose Sequential Fine-tuning\nwith Averaging (SFA), a method that merges currently training models with\nearlier checkpoints during the course of training. SOTA approaches typically\nmaintain a data buffer of past tasks or impose a penalty at each gradient step.\nIn contrast, our method achieves comparable results without the need to store\npast data, or multiple copies of parameters for each gradient step.\nFurthermore, our method outperforms common merging techniques such as Task\nArithmetic, TIES Merging, and WiSE-FT, as well as other penalty methods like L2\nand Elastic Weight Consolidation. In turn, our method offers insight into the\nbenefits of merging partially-trained models during training across both image\nand language domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05559v1",
    "published_date": "2025-01-09 20:11:08 UTC",
    "updated_date": "2025-01-09 20:11:08 UTC"
  },
  {
    "arxiv_id": "2501.05555v2",
    "title": "Improving Zero-Shot Object-Level Change Detection by Incorporating Visual Correspondence",
    "authors": [
      "Hung Huy Nguyen",
      "Pooyan Rahmanzadehgervi",
      "Long Mai",
      "Anh Totti Nguyen"
    ],
    "abstract": "Detecting object-level changes between two images across possibly different\nviews is a core task in many applications that involve visual inspection or\ncamera surveillance. Existing change-detection approaches suffer from three\nmajor limitations: (1) lack of evaluation on image pairs that contain no\nchanges, leading to unreported false positive rates; (2) lack of\ncorrespondences (i.e., localizing the regions before and after a change); and\n(3) poor zero-shot generalization across different domains. To address these\nissues, we introduce a novel method that leverages change correspondences (a)\nduring training to improve change detection accuracy, and (b) at test time, to\nminimize false positives. That is, we harness the supervision labels of where\nan object is added or removed to supervise change detectors, improving their\naccuracy over previous work by a large margin. Our work is also the first to\npredict correspondences between pairs of detected changes using estimated\nhomography and the Hungarian algorithm. Our model demonstrates superior\nperformance over existing methods, achieving state-of-the-art results in change\ndetection and change correspondence accuracy across both in-distribution and\nzero-shot benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05555v2",
    "published_date": "2025-01-09 20:02:10 UTC",
    "updated_date": "2025-01-16 16:00:37 UTC"
  },
  {
    "arxiv_id": "2501.05554v1",
    "title": "LLMQuoter: Enhancing RAG Capabilities Through Efficient Quote Extraction From Large Contexts",
    "authors": [
      "Yuri Facanha Bezerra",
      "Li Weigang"
    ],
    "abstract": "We introduce LLMQuoter, a lightweight, distillation-based model designed to\nenhance Retrieval Augmented Generation (RAG) by extracting the most relevant\ntextual evidence for downstream reasoning tasks. Built on the LLaMA-3B\narchitecture and fine-tuned with Low-Rank Adaptation (LoRA) on a 15,000-sample\nsubset of HotpotQA, LLMQuoter adopts a \"quote-first-then-answer\" strategy,\nefficiently identifying key quotes before passing curated snippets to reasoning\nmodels. This workflow reduces cognitive overhead and outperforms full-context\napproaches like Retrieval-Augmented Fine-Tuning (RAFT), achieving over 20-point\naccuracy gains across both small and large language models. By leveraging\nknowledge distillation from a high-performing teacher model, LLMQuoter achieves\ncompetitive results in a resource-efficient fine-tuning setup. It democratizes\nadvanced RAG capabilities, delivering significant performance improvements\nwithout requiring extensive model retraining. Our results highlight the\npotential of distilled quote-based reasoning to streamline complex workflows,\noffering a scalable and practical solution for researchers and practitioners\nalike.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05554v1",
    "published_date": "2025-01-09 20:01:15 UTC",
    "updated_date": "2025-01-09 20:01:15 UTC"
  },
  {
    "arxiv_id": "2501.05552v1",
    "title": "The dynamics of meaning through time: Assessment of Large Language Models",
    "authors": [
      "Mohamed Taher Alrefaie",
      "Fatty Salem",
      "Nour Eldin Morsy",
      "Nada Samir",
      "Mohamed Medhat Gaber"
    ],
    "abstract": "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05552v1",
    "published_date": "2025-01-09 19:56:44 UTC",
    "updated_date": "2025-01-09 19:56:44 UTC"
  },
  {
    "arxiv_id": "2501.16337v1",
    "title": "Explore Activation Sparsity in Recurrent LLMs for Energy-Efficient Neuromorphic Computing",
    "authors": [
      "Ivan Knunyants",
      "Maryam Tavakol",
      "Manolis Sifalakis",
      "Yingfu Xu",
      "Amirreza Yousefzadeh",
      "Guangzhi Tang"
    ],
    "abstract": "The recent rise of Large Language Models (LLMs) has revolutionized the deep\nlearning field. However, the desire to deploy LLMs on edge devices introduces\nenergy efficiency and latency challenges. Recurrent LLM (R-LLM) architectures\nhave proven effective in mitigating the quadratic complexity of self-attention,\nmaking them a potential paradigm for computing on-edge neuromorphic processors.\nIn this work, we propose a low-cost, training-free algorithm to sparsify\nR-LLMs' activations to enhance energy efficiency on neuromorphic hardware. Our\napproach capitalizes on the inherent structure of these models, rendering them\nwell-suited for energy-constrained environments. Although primarily designed\nfor R-LLMs, this method can be generalized to other LLM architectures, such as\ntransformers, as demonstrated on the OPT model, achieving comparable sparsity\nand efficiency improvements. Empirical studies illustrate that our method\nsignificantly reduces computational demands while maintaining competitive\naccuracy across multiple zero-shot learning benchmarks. Additionally, hardware\nsimulations with the SENECA neuromorphic processor underscore notable energy\nsavings and latency improvements. These results pave the way for low-power,\nreal-time neuromorphic deployment of LLMs and demonstrate the feasibility of\ntraining-free on-chip adaptation using activation sparsity.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted by AICAS 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.16337v1",
    "published_date": "2025-01-09 19:13:03 UTC",
    "updated_date": "2025-01-09 19:13:03 UTC"
  },
  {
    "arxiv_id": "2501.05510v2",
    "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
    "authors": [
      "Yifei Li",
      "Junbo Niu",
      "Ziyang Miao",
      "Chunjiang Ge",
      "Yuanhang Zhou",
      "Qihao He",
      "Xiaoyi Dong",
      "Haodong Duan",
      "Shuangrui Ding",
      "Rui Qian",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Conghui He",
      "Jiaqi Wang"
    ],
    "abstract": "Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05510v2",
    "published_date": "2025-01-09 19:00:01 UTC",
    "updated_date": "2025-03-27 17:40:09 UTC"
  },
  {
    "arxiv_id": "2501.05453v1",
    "title": "An Empirical Study of Autoregressive Pre-training from Videos",
    "authors": [
      "Jathushan Rajasegaran",
      "Ilija Radosavovic",
      "Rahul Ravishankar",
      "Yossi Gandelsman",
      "Christoph Feichtenhofer",
      "Jitendra Malik"
    ],
    "abstract": "We empirically study autoregressive pre-training from videos. To perform our\nstudy, we construct a series of autoregressive video models, called Toto. We\ntreat videos as sequences of visual tokens and train transformer models to\nautoregressively predict future tokens. Our models are pre-trained on a diverse\ndataset of videos and images comprising over 1 trillion visual tokens. We\nexplore different architectural, training, and inference design choices. We\nevaluate the learned visual representations on a range of downstream tasks\nincluding image recognition, video classification, object tracking, and\nrobotics. Our results demonstrate that, despite minimal inductive biases,\nautoregressive pre-training leads to competitive performance across all\nbenchmarks. Finally, we find that scaling our video models results in similar\nscaling curves to those seen in language models, albeit with a different rate.\nMore details at https://brjathu.github.io/toto/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05453v1",
    "published_date": "2025-01-09 18:59:58 UTC",
    "updated_date": "2025-01-09 18:59:58 UTC"
  },
  {
    "arxiv_id": "2501.05445v1",
    "title": "Consistent Flow Distillation for Text-to-3D Generation",
    "authors": [
      "Runjie Yan",
      "Yinbo Chen",
      "Xiaolong Wang"
    ],
    "abstract": "Score Distillation Sampling (SDS) has made significant strides in distilling\nimage-generative models for 3D generation. However, its\nmaximum-likelihood-seeking behavior often leads to degraded visual quality and\ndiversity, limiting its effectiveness in 3D applications. In this work, we\npropose Consistent Flow Distillation (CFD), which addresses these limitations.\nWe begin by leveraging the gradient of the diffusion ODE or SDE sampling\nprocess to guide the 3D generation. From the gradient-based sampling\nperspective, we find that the consistency of 2D image flows across different\nviewpoints is important for high-quality 3D generation. To achieve this, we\nintroduce multi-view consistent Gaussian noise on the 3D object, which can be\nrendered from various viewpoints to compute the flow gradient. Our experiments\ndemonstrate that CFD, through consistent flows, significantly outperforms\nprevious methods in text-to-3D generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://runjie-yan.github.io/cfd/",
    "pdf_url": "http://arxiv.org/pdf/2501.05445v1",
    "published_date": "2025-01-09 18:56:05 UTC",
    "updated_date": "2025-01-09 18:56:05 UTC"
  },
  {
    "arxiv_id": "2501.05443v1",
    "title": "A survey of textual cyber abuse detection using cutting-edge language models and large language models",
    "authors": [
      "Jose A. Diaz-Garcia",
      "Joao Paulo Carvalho"
    ],
    "abstract": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "37 pages, under review in WIREs Data Mining and Knowledge Discovery",
    "pdf_url": "http://arxiv.org/pdf/2501.05443v1",
    "published_date": "2025-01-09 18:55:50 UTC",
    "updated_date": "2025-01-09 18:55:50 UTC"
  },
  {
    "arxiv_id": "2501.05442v1",
    "title": "Progressive Growing of Video Tokenizers for Highly Compressed Latent Spaces",
    "authors": [
      "Aniruddha Mahapatra",
      "Long Mai",
      "Yitian Zhang",
      "David Bourgin",
      "Feng Liu"
    ],
    "abstract": "Video tokenizers are essential for latent video diffusion models, converting\nraw video data into spatiotemporally compressed latent spaces for efficient\ntraining. However, extending state-of-the-art video tokenizers to achieve a\ntemporal compression ratio beyond 4x without increasing channel capacity poses\nsignificant challenges. In this work, we propose an alternative approach to\nenhance temporal compression. We find that the reconstruction quality of\ntemporally subsampled videos from a low-compression encoder surpasses that of\nhigh-compression encoders applied to original videos. This indicates that\nhigh-compression models can leverage representations from lower-compression\nmodels. Building on this insight, we develop a bootstrapped\nhigh-temporal-compression model that progressively trains high-compression\nblocks atop well-trained lower-compression models. Our method includes a\ncross-level feature-mixing module to retain information from the pretrained\nlow-compression model and guide higher-compression blocks to capture the\nremaining details from the full video sequence. Evaluation of video benchmarks\nshows that our method significantly improves reconstruction quality while\nincreasing temporal compression compared to direct extensions of existing video\ntokenizers. Furthermore, the resulting compact latent space effectively trains\na video diffusion model for high-quality video generation with a reduced token\nbudget.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website:\n  https://progressive-video-tokenizer.github.io/Pro-MAG/",
    "pdf_url": "http://arxiv.org/pdf/2501.05442v1",
    "published_date": "2025-01-09 18:55:15 UTC",
    "updated_date": "2025-01-09 18:55:15 UTC"
  },
  {
    "arxiv_id": "2501.05439v1",
    "title": "From Simple to Complex Skills: The Case of In-Hand Object Reorientation",
    "authors": [
      "Haozhi Qi",
      "Brent Yi",
      "Mike Lambeta",
      "Yi Ma",
      "Roberto Calandra",
      "Jitendra Malik"
    ],
    "abstract": "Learning policies in simulation and transferring them to the real world has\nbecome a promising approach in dexterous manipulation. However, bridging the\nsim-to-real gap for each new task requires substantial human effort, such as\ncareful reward engineering, hyperparameter tuning, and system identification.\nIn this work, we present a system that leverages low-level skills to address\nthese challenges for more complex tasks. Specifically, we introduce a\nhierarchical policy for in-hand object reorientation based on previously\nacquired rotation skills. This hierarchical policy learns to select which\nlow-level skill to execute based on feedback from both the environment and the\nlow-level skill policies themselves. Compared to learning from scratch, the\nhierarchical policy is more robust to out-of-distribution changes and transfers\neasily from simulation to real-world environments. Additionally, we propose a\ngeneralizable object pose estimator that uses proprioceptive information,\nlow-level skill predictions, and control errors as inputs to estimate the\nobject pose over time. We demonstrate that our system can reorient objects,\nincluding symmetrical and textureless ones, to a desired pose.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "website: https://dexhier.github.io",
    "pdf_url": "http://arxiv.org/pdf/2501.05439v1",
    "published_date": "2025-01-09 18:49:39 UTC",
    "updated_date": "2025-01-09 18:49:39 UTC"
  },
  {
    "arxiv_id": "2501.05435v2",
    "title": "Neuro-Symbolic AI in 2024: A Systematic Review",
    "authors": [
      "Brandon C. Colelough",
      "William Regli"
    ],
    "abstract": "Background: The field of Artificial Intelligence has undergone cyclical\nperiods of growth and decline, known as AI summers and winters. Currently, we\nare in the third AI summer, characterized by significant advancements and\ncommercialization, particularly in the integration of Symbolic AI and\nSub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n  Methods: The review followed the PRISMA methodology, utilizing databases such\nas IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\ncriteria targeted peer-reviewed papers published between 2020 and 2024. Papers\nwere screened for relevance to Neuro-Symbolic AI, with further inclusion based\non the availability of associated codebases to ensure reproducibility.\n  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\nand were analyzed in detail. The majority of research efforts are concentrated\nin the areas of learning and inference (63%), logic and reasoning (35%), and\nknowledge representation (44%). Explainability and trustworthiness are less\nrepresented (28%), with Meta-Cognition being the least explored area (5%). The\nreview identifies significant interdisciplinary opportunities, particularly in\nintegrating explainability and trustworthiness with other research areas.\n  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\nconcentrated efforts in learning and inference. Significant gaps remain in\nexplainability, trustworthiness, and Meta-Cognition. Addressing these gaps\nthrough interdisciplinary research will be crucial for advancing the field\ntowards more intelligent, reliable, and context-aware AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.05435v2",
    "published_date": "2025-01-09 18:48:35 UTC",
    "updated_date": "2025-04-05 23:53:49 UTC"
  },
  {
    "arxiv_id": "2501.05501v2",
    "title": "Strategy Masking: A Method for Guardrails in Value-based Reinforcement Learning Agents",
    "authors": [
      "Jonathan Keane",
      "Sam Keyser",
      "Jeremy Kedziora"
    ],
    "abstract": "The use of reward functions to structure AI learning and decision making is\ncore to the current reinforcement learning paradigm; however, without careful\ndesign of reward functions, agents can learn to solve problems in ways that may\nbe considered \"undesirable\" or \"unethical.\" Without thorough understanding of\nthe incentives a reward function creates, it can be difficult to impose\nprincipled yet general control mechanisms over its behavior. In this paper, we\nstudy methods for constructing guardrails for AI agents that use reward\nfunctions to learn decision making. We introduce a novel approach, which we\ncall strategy masking, to explicitly learn and then suppress undesirable AI\nagent behavior. We apply our method to study lying in AI agents and show that\nit can be used to effectively modify agent behavior by suppressing lying\npost-training without compromising agent ability to perform effectively.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05501v2",
    "published_date": "2025-01-09 18:43:05 UTC",
    "updated_date": "2025-01-20 22:06:19 UTC"
  },
  {
    "arxiv_id": "2501.05409v2",
    "title": "Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charité, and Aignostics",
    "authors": [
      "Maximilian Alber",
      "Stephan Tietz",
      "Jonas Dippel",
      "Timo Milbich",
      "Timothée Lesort",
      "Panos Korfiatis",
      "Moritz Krügener",
      "Beatriz Perez Cancer",
      "Neelay Shah",
      "Alexander Möllers",
      "Philipp Seegerer",
      "Alexandra Carpen-Amarie",
      "Kai Standvoss",
      "Gabriel Dernbach",
      "Edwin de Jong",
      "Simon Schallenberg",
      "Andreas Kunft",
      "Helmut Hoffer von Ankershoffen",
      "Gavin Schaeferle",
      "Patrick Duffy",
      "Matt Redlon",
      "Philipp Jurmeister",
      "David Horst",
      "Lukas Ruff",
      "Klaus-Robert Müller",
      "Frederick Klauschen",
      "Andrew Norgan"
    ],
    "abstract": "Recent advances in digital pathology have demonstrated the effectiveness of\nfoundation models across diverse applications. In this report, we present\nAtlas, a novel vision foundation model based on the RudolfV approach. Our model\nwas trained on a dataset comprising 1.2 million histopathology whole slide\nimages, collected from two medical institutions: Mayo Clinic and Charit\\'e -\nUniverst\\\"atsmedizin Berlin. Comprehensive evaluations show that Atlas achieves\nstate-of-the-art performance across twenty-one public benchmark datasets, even\nthough it is neither the largest model by parameter count nor by training\ndataset size.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05409v2",
    "published_date": "2025-01-09 18:06:45 UTC",
    "updated_date": "2025-01-10 16:58:29 UTC"
  },
  {
    "arxiv_id": "2501.05408v1",
    "title": "TimeRL: Efficient Deep Reinforcement Learning with Polyhedral Dependence Graphs",
    "authors": [
      "Pedro F. Silvestre",
      "Peter Pietzuch"
    ],
    "abstract": "Modern deep learning (DL) workloads increasingly use complex deep\nreinforcement learning (DRL) algorithms that generate training data within the\nlearning loop. This results in programs with several nested loops and dynamic\ndata dependencies between tensors. While DL systems with eager execution\nsupport such dynamism, they lack the optimizations and smart scheduling of\ngraph-based execution. Graph-based execution, however, cannot express dynamic\ntensor shapes, instead requiring the use of multiple static subgraphs. Either\nexecution model for DRL thus leads to redundant computation, reduced\nparallelism, and less efficient memory management.\n  We describe TimeRL, a system for executing dynamic DRL programs that combines\nthe dynamism of eager execution with the whole-program optimizations and\nscheduling of graph-based execution. TimeRL achieves this by introducing the\ndeclarative programming model of recurrent tensors, which allows users to\ndefine dynamic dependencies as intuitive recurrence equations. TimeRL\ntranslates recurrent tensors into a polyhedral dependence graph (PDG) with\ndynamic dependencies as symbolic expressions. Through simple PDG\ntransformations, TimeRL applies whole-program optimizations, such as automatic\nvectorization, incrementalization, and operator fusion. The PDG also allows for\nthe computation of an efficient program-wide execution schedule, which decides\non buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show\nthat TimeRL executes current DRL algorithms up to 47$\\times$ faster than\nexisting DRL systems, while using 16$\\times$ less GPU peak memory.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "D.4.7, I.1.1, I.1.3, I.2.5",
      "D.4.7; I.1.1; I.1.3; I.2.5"
    ],
    "primary_category": "cs.DC",
    "comment": "17 pages, 11 figures, 5 bibliography pages",
    "pdf_url": "http://arxiv.org/pdf/2501.05408v1",
    "published_date": "2025-01-09 18:05:33 UTC",
    "updated_date": "2025-01-09 18:05:33 UTC"
  },
  {
    "arxiv_id": "2501.05407v1",
    "title": "On-line Policy Improvement using Monte-Carlo Search",
    "authors": [
      "Gerald Tesauro",
      "Gregory R. Galperin"
    ],
    "abstract": "We present a Monte-Carlo simulation algorithm for real-time policy\nimprovement of an adaptive controller. In the Monte-Carlo simulation, the\nlong-term expected reward of each possible action is statistically measured,\nusing the initial policy to make decisions in each step of the simulation. The\naction maximizing the measured expected reward is then taken, resulting in an\nimproved policy. Our algorithm is easily parallelizable and has been\nimplemented on the IBM SP1 and SP2 parallel-RISC supercomputers.\n  We have obtained promising initial results in applying this algorithm to the\ndomain of backgammon. Results are reported for a wide variety of initial\npolicies, ranging from a random policy to TD-Gammon, an extremely strong\nmulti-layer neural network. In each case, the Monte-Carlo algorithm gives a\nsubstantial reduction, by as much as a factor of 5 or more, in the error rate\nof the base players. The algorithm is also potentially useful in many other\nadaptive control applications in which it is possible to simulate the\nenvironment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accompanied by oral presentation by Gregory Galperin at NeurIPS 1996\n  (then known as NIPS*96)",
    "pdf_url": "http://arxiv.org/pdf/2501.05407v1",
    "published_date": "2025-01-09 18:05:05 UTC",
    "updated_date": "2025-01-09 18:05:05 UTC"
  },
  {
    "arxiv_id": "2501.05403v1",
    "title": "TimeDP: Learning to Generate Multi-Domain Time Series with Domain Prompts",
    "authors": [
      "Yu-Hao Huang",
      "Chang Xu",
      "Yueying Wu",
      "Wu-Jun Li",
      "Jiang Bian"
    ],
    "abstract": "Time series generation models are crucial for applications like data\naugmentation and privacy preservation. Most existing time series generation\nmodels are typically designed to generate data from one specified domain. While\nleveraging data from other domain for better generalization is proved to work\nin other application areas, this approach remains challenging for time series\nmodeling due to the large divergence in patterns among different real world\ntime series categories. In this paper, we propose a multi-domain time series\ndiffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time\nseries semantic prototype module which defines time series prototypes to\nrepresent time series basis, each prototype vector serving as \"word\"\nrepresenting some elementary time series feature. A prototype assignment module\nis applied to extract the extract domain specific prototype weights, for\nlearning domain prompts as generation condition. During sampling, we extract\n\"domain prompt\" with few-shot samples from the target domain and use the domain\nprompts as condition to generate time series samples. Experiments demonstrate\nthat our method outperforms baselines to provide the state-of-the-art in-domain\ngeneration quality and strong unseen domain generation capability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05403v1",
    "published_date": "2025-01-09 17:57:56 UTC",
    "updated_date": "2025-01-09 17:57:56 UTC"
  },
  {
    "arxiv_id": "2501.05401v1",
    "title": "BRATI: Bidirectional Recurrent Attention for Time-Series Imputation",
    "authors": [
      "Armando Collado-Villaverde",
      "Pablo Muñoz",
      "Maria D. R-Moreno"
    ],
    "abstract": "Missing data in time-series analysis poses significant challenges, affecting\nthe reliability of downstream applications. Imputation, the process of\nestimating missing values, has emerged as a key solution. This paper introduces\nBRATI, a novel deep-learning model designed to address multivariate time-series\nimputation by combining Bidirectional Recurrent Networks and Attention\nmechanisms. BRATI processes temporal dependencies and feature correlations\nacross long and short time horizons, utilizing two imputation blocks that\noperate in opposite temporal directions. Each block integrates recurrent layers\nand attention mechanisms to effectively resolve long-term dependencies.\n  We evaluate BRATI on three real-world datasets under diverse missing-data\nscenarios: randomly missing values, fixed-length missing sequences, and\nvariable-length missing sequences. Our findings demonstrate that BRATI\nconsistently outperforms state-of-the-art models, delivering superior accuracy\nand robustness in imputing multivariate time-series data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05401v1",
    "published_date": "2025-01-09 17:50:56 UTC",
    "updated_date": "2025-01-09 17:50:56 UTC"
  },
  {
    "arxiv_id": "2501.05398v1",
    "title": "Mechanistic understanding and validation of large AI models with SemanticLens",
    "authors": [
      "Maximilian Dreyer",
      "Jim Berend",
      "Tobias Labarta",
      "Johanna Vielhaben",
      "Thomas Wiegand",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "abstract": "Unlike human-engineered systems such as aeroplanes, where each component's\nrole and dependencies are well understood, the inner workings of AI models\nremain largely opaque, hindering verifiability and undermining trust. This\npaper introduces SemanticLens, a universal explanation method for neural\nnetworks that maps hidden knowledge encoded by components (e.g., individual\nneurons) into the semantically structured, multimodal space of a foundation\nmodel such as CLIP. In this space, unique operations become possible, including\n(i) textual search to identify neurons encoding specific concepts, (ii)\nsystematic analysis and comparison of model representations, (iii) automated\nlabelling of neurons and explanation of their functional roles, and (iv) audits\nto validate decision-making against requirements. Fully scalable and operating\nwithout human input, SemanticLens is shown to be effective for debugging and\nvalidation, summarizing model knowledge, aligning reasoning with expectations\n(e.g., adherence to the ABCDE-rule in melanoma classification), and detecting\ncomponents tied to spurious correlations and their associated training data. By\nenabling component-level understanding and validation, the proposed approach\nhelps bridge the \"trust gap\" between AI models and traditional engineered\nsystems. We provide code for SemanticLens on\nhttps://github.com/jim-berend/semanticlens and a demo on\nhttps://semanticlens.hhi-research-insights.eu.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "74 pages (18 pages manuscript, 7 pages references, 49 pages appendix)",
    "pdf_url": "http://arxiv.org/pdf/2501.05398v1",
    "published_date": "2025-01-09 17:47:34 UTC",
    "updated_date": "2025-01-09 17:47:34 UTC"
  },
  {
    "arxiv_id": "2501.05391v1",
    "title": "The global consensus on the risk management of autonomous driving",
    "authors": [
      "Sebastian Krügel",
      "Matthias Uhl"
    ],
    "abstract": "Every maneuver of a vehicle redistributes risks between road users. While\nhuman drivers do this intuitively, autonomous vehicles allow and require\ndeliberative algorithmic risk management. But how should traffic risks be\ndistributed among road users? In a global experimental study in eight countries\nwith different cultural backgrounds and almost 11,000 participants, we compared\nrisk distribution preferences. It turns out that risk preferences in road\ntraffic are strikingly similar between the cultural zones. The vast majority of\nparticipants in all countries deviates from a guiding principle of minimizing\naccident probabilities in favor of weighing up the probability and severity of\naccidents. At the national level, the consideration of accident probability and\nseverity hardly differs between countries. The social dilemma of autonomous\nvehicles detected in deterministic crash scenarios disappears in risk\nassessments of everyday traffic situations in all countries. In no country do\ncyclists receive a risk bonus that goes beyond their higher vulnerability. In\nsum, our results suggest that a global consensus on the risk ethics of\nautonomous driving is easier to establish than on the ethics of crashing.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05391v1",
    "published_date": "2025-01-09 17:33:08 UTC",
    "updated_date": "2025-01-09 17:33:08 UTC"
  },
  {
    "arxiv_id": "2501.05497v1",
    "title": "Spatial Information Integration in Small Language Models for Document Layout Generation and Classification",
    "authors": [
      "Pablo Melendez",
      "Clemens Havas"
    ],
    "abstract": "Document layout understanding is a field of study that analyzes the spatial\narrangement of information in a document hoping to understand its structure and\nlayout. Models such as LayoutLM (and its subsequent iterations) can understand\nsemi-structured documents with SotA results; however, the lack of open\nsemi-structured data is a limitation in itself. While semi-structured data is\ncommon in everyday life (balance sheets, purchase orders, receipts), there is a\nlack of public datasets for training machine learning models for this type of\ndocument. In this investigation we propose a method to generate new, synthetic,\nlayout information that can help overcoming this data shortage. According to\nour results, the proposed method performs better than LayoutTransformer,\nanother popular layout generation method. We also show that, in some scenarios,\ntext classification can improve when supported by bounding box information.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages. Symposium on Applied Computing 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05497v1",
    "published_date": "2025-01-09 17:20:00 UTC",
    "updated_date": "2025-01-09 17:20:00 UTC"
  },
  {
    "arxiv_id": "2501.05382v1",
    "title": "Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models",
    "authors": [
      "Kristian G. Barman",
      "Sascha Caron",
      "Emily Sullivan",
      "Henk W. de Regt",
      "Roberto Ruiz de Austri",
      "Mieke Boon",
      "Michael Färber",
      "Stefan Fröse",
      "Faegheh Hasibi",
      "Andreas Ipp",
      "Rukshak Kapoor",
      "Gregor Kasieczka",
      "Daniel Kostić",
      "Michael Krämer",
      "Tobias Golling",
      "Luis G. Lopez",
      "Jesus Marco",
      "Sydney Otten",
      "Pawel Pawlowski",
      "Pietro Vischia",
      "Erik Weber",
      "Christoph Weniger"
    ],
    "abstract": "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.",
    "categories": [
      "physics.data-an",
      "cs.AI",
      "hep-ph",
      "physics.comp-ph",
      "physics.hist-ph"
    ],
    "primary_category": "physics.data-an",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05382v1",
    "published_date": "2025-01-09 17:11:22 UTC",
    "updated_date": "2025-01-09 17:11:22 UTC"
  },
  {
    "arxiv_id": "2501.05368v2",
    "title": "Developing a Foundation of Vector Symbolic Architectures Using Category Theory",
    "authors": [
      "Nolan P Shaw",
      "P Michael Furlong",
      "Britt Anderson",
      "Jeff Orchard"
    ],
    "abstract": "Connectionist approaches to machine learning, \\emph{i.e.} neural networks,\nare enjoying a considerable vogue right now. However, these methods require\nlarge volumes of data and produce models that are uninterpretable to humans. An\nalternative framework that is compatible with neural networks and\ngradient-based learning, but explicitly models compositionality, is Vector\nSymbolic Architectures (VSAs). VSAs are a family of algebras on\nhigh-dimensional vector representations. They arose in cognitive science from\nthe need to unify neural processing and the kind of symbolic reasoning that\nhumans perform. While machine learning methods have benefited from\ncategory-theoretical analyses, VSAs have not yet received similar treatment. In\nthis paper, we present a first attempt at applying category theory to VSAs.\nSpecifically, We generalise from vectors to co-presheaves, and describe VSA\noperations as the right Kan extensions of the external tensor product. This\nformalisation involves a proof that the right Kan extension in such cases can\nbe expressed as simple, element-wise operations. We validate our formalisation\nwith worked examples that connect to current VSA implementations, while\nsuggesting new possible designs for VSAs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68T30"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, no figures, 2 tables, two appendices",
    "pdf_url": "http://arxiv.org/pdf/2501.05368v2",
    "published_date": "2025-01-09 16:49:04 UTC",
    "updated_date": "2025-05-02 18:22:53 UTC"
  },
  {
    "arxiv_id": "2501.05366v1",
    "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models",
    "authors": [
      "Xiaoxi Li",
      "Guanting Dong",
      "Jiajie Jin",
      "Yuyao Zhang",
      "Yujia Zhou",
      "Yutao Zhu",
      "Peitian Zhang",
      "Zhicheng Dou"
    ],
    "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive\nlong stepwise reasoning capabilities through large-scale reinforcement\nlearning. However, their extended reasoning processes often suffer from\nknowledge insufficiency, leading to frequent uncertainties and potential\nerrors. To address this limitation, we introduce \\textbf{Search-o1}, a\nframework that enhances LRMs with an agentic retrieval-augmented generation\n(RAG) mechanism and a Reason-in-Documents module for refining retrieved\ndocuments. Search-o1 integrates an agentic search workflow into the reasoning\nprocess, enabling dynamic retrieval of external knowledge when LRMs encounter\nuncertain knowledge points. Additionally, due to the verbose nature of\nretrieved documents, we design a separate Reason-in-Documents module to deeply\nanalyze the retrieved information before injecting it into the reasoning chain,\nminimizing noise and preserving coherent reasoning flow. Extensive experiments\non complex reasoning tasks in science, mathematics, and coding, as well as six\nopen-domain QA benchmarks, demonstrate the strong performance of Search-o1.\nThis approach enhances the trustworthiness and applicability of LRMs in complex\nreasoning tasks, paving the way for more reliable and versatile intelligent\nsystems. The code is available at\n\\url{https://github.com/sunnynexus/Search-o1}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05366v1",
    "published_date": "2025-01-09 16:48:17 UTC",
    "updated_date": "2025-01-09 16:48:17 UTC"
  },
  {
    "arxiv_id": "2501.05360v1",
    "title": "On Corrigibility and Alignment in Multi Agent Games",
    "authors": [
      "Edmund Dable-Heath",
      "Boyko Vodenicharski",
      "James Bishop"
    ],
    "abstract": "Corrigibility of autonomous agents is an under explored part of system\ndesign, with previous work focusing on single agent systems. It has been\nsuggested that uncertainty over the human preferences acts to keep the agents\ncorrigible, even in the face of human irrationality. We present a general\nframework for modelling corrigibility in a multi-agent setting as a 2 player\ngame in which the agents always have a move in which they can ask the human for\nsupervision. This is formulated as a Bayesian game for the purpose of\nintroducing uncertainty over the human beliefs. We further analyse two specific\ncases. First, a two player corrigibility game, in which we want corrigibility\ndisplayed in both agents for both common payoff (monotone) games and harmonic\ngames. Then we investigate an adversary setting, in which one agent is\nconsidered to be a `defending' agent and the other an `adversary'. A general\nresult is provided for what belief over the games and human rationality the\ndefending agent is required to have to induce corrigibility.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05360v1",
    "published_date": "2025-01-09 16:44:38 UTC",
    "updated_date": "2025-01-09 16:44:38 UTC"
  },
  {
    "arxiv_id": "2501.05496v2",
    "title": "FedSA: A Unified Representation Learning via Semantic Anchors for Prototype-based Federated Learning",
    "authors": [
      "Yanbing Zhou",
      "Xiangmou Qu",
      "Chenlong You",
      "Jiyang Zhou",
      "Jingyue Tang",
      "Xin Zheng",
      "Chunmao Cai",
      "Yingbo Wu"
    ],
    "abstract": "Prototype-based federated learning has emerged as a promising approach that\nshares lightweight prototypes to transfer knowledge among clients with data\nheterogeneity in a model-agnostic manner. However, existing methods often\ncollect prototypes directly from local models, which inevitably introduce\ninconsistencies into representation learning due to the biased data\ndistributions and differing model architectures among clients. In this paper,\nwe identify that both statistical and model heterogeneity create a vicious\ncycle of representation inconsistency, classifier divergence, and skewed\nprototype alignment, which negatively impacts the performance of clients. To\nbreak the vicious cycle, we propose a novel framework named Federated Learning\nvia Semantic Anchors (FedSA) to decouple the generation of prototypes from\nlocal representation learning. We introduce a novel perspective that uses\nsimple yet effective semantic anchors serving as prototypes to guide local\nmodels in learning consistent representations. By incorporating semantic\nanchors, we further propose anchor-based regularization with margin-enhanced\ncontrastive learning and anchor-based classifier calibration to correct feature\nextractors and calibrate classifiers across clients, achieving intra-class\ncompactness and inter-class separability of prototypes while ensuring\nconsistent decision boundaries. We then update the semantic anchors with these\nconsistent and discriminative prototypes, which iteratively encourage clients\nto collaboratively learn a unified data representation with robust\ngeneralization. Extensive experiments under both statistical and model\nheterogeneity settings show that FedSA significantly outperforms existing\nprototype-based FL methods on various classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05496v2",
    "published_date": "2025-01-09 16:10:03 UTC",
    "updated_date": "2025-04-22 02:07:20 UTC"
  },
  {
    "arxiv_id": "2501.05336v1",
    "title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction",
    "authors": [
      "Hantao Lou",
      "Jiaming Ji",
      "Kaile Wang",
      "Yaodong Yang"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI Alignment Track 2025 Poster",
    "pdf_url": "http://arxiv.org/pdf/2501.05336v1",
    "published_date": "2025-01-09 16:02:51 UTC",
    "updated_date": "2025-01-09 16:02:51 UTC"
  },
  {
    "arxiv_id": "2501.05334v1",
    "title": "The Bakers and Millers Game with Restricted Locations",
    "authors": [
      "Simon Krogmann",
      "Pascal Lenzner",
      "Alexander Skopalik"
    ],
    "abstract": "We study strategic location choice by customers and sellers, termed the\nBakers and Millers Game in the literature. In our generalized setting, each\nmiller can freely choose any location for setting up a mill, while each baker\nis restricted in the choice of location for setting up a bakery. For optimal\nbargaining power, a baker would like to select a location with many millers to\nbuy flour from and with little competition from other bakers. Likewise, a\nmiller aims for a location with many bakers and few competing millers. Thus,\nboth types of agents choose locations to optimize the ratio of agents of\nopposite type divided by agents of the same type at their chosen location.\nOriginally raised in the context of Fractional Hedonic Games, the Bakers and\nMillers Game has applications that range from commerce to product design.\n  We study the impact of location restrictions on the properties of the game.\nWhile pure Nash equilibria trivially exist in the setting without location\nrestrictions, we show via a sophisticated, efficient algorithm that even the\nmore challenging restricted setting admits equilibria. Moreover, the computed\nequilibrium approximates the optimal social welfare by a factor of at most\n$2\\left(\\frac{e}{e-1}\\right)$. Furthermore, we give tight bounds on the price\nof anarchy/stability.\n  On the conceptual side, the location choice feature adds a new layer to the\nstandard setting of Hedonic Games, in the sense that agents that select the\nsame location form a coalition. This allows to naturally restrict the possible\ncoalitions that can be formed. With this, our model generalizes simple\nsymmetric Fractional Hedonic Games on complete bipartite valuation graphs and\nalso Hedonic Diversity Games with utilities single-peaked at 0. We believe that\nthis generalization is also a very interesting direction for other types of\nHedonic Games.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "To appear at the 24th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.05334v1",
    "published_date": "2025-01-09 15:59:32 UTC",
    "updated_date": "2025-01-09 15:59:32 UTC"
  },
  {
    "arxiv_id": "2501.05332v1",
    "title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder",
    "authors": [
      "Samir Sadok",
      "Simon Leglaive",
      "Laurent Girin",
      "Gaël Richard",
      "Xavier Alameda-Pineda"
    ],
    "abstract": "This article introduces AnCoGen, a novel method that leverages a masked\nautoencoder to unify the analysis, control, and generation of speech signals\nwithin a single model. AnCoGen can analyze speech by estimating key attributes,\nsuch as speaker identity, pitch, content, loudness, signal-to-noise ratio, and\nclarity index. In addition, it can generate speech from these attributes and\nallow precise control of the synthesized speech by modifying them. Extensive\nexperiments demonstrated the effectiveness of AnCoGen across speech\nanalysis-resynthesis, pitch estimation, pitch modification, and speech\nenhancement.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, https://samsad35.github.io/site-ancogen",
    "pdf_url": "http://arxiv.org/pdf/2501.05332v1",
    "published_date": "2025-01-09 15:58:37 UTC",
    "updated_date": "2025-01-09 15:58:37 UTC"
  },
  {
    "arxiv_id": "2501.05495v1",
    "title": "LSEBMCL: A Latent Space Energy-Based Model for Continual Learning",
    "authors": [
      "Xiaodi Li",
      "Dingcheng Li",
      "Rujun Gao",
      "Mahmoud Zamani",
      "Latifur Khan"
    ],
    "abstract": "Continual learning has become essential in many practical applications such\nas online news summaries and product classification. The primary challenge is\nknown as catastrophic forgetting, a phenomenon where a model inadvertently\ndiscards previously learned knowledge when it is trained on new tasks. Existing\nsolutions involve storing exemplars from previous classes, regularizing\nparameters during the fine-tuning process, or assigning different model\nparameters to each task. The proposed solution LSEBMCL (Latent Space\nEnergy-Based Model for Continual Learning) in this work is to use energy-based\nmodels (EBMs) to prevent catastrophic forgetting by sampling data points from\nprevious tasks when training on new ones. The EBM is a machine learning model\nthat associates an energy value with each input data point. The proposed method\nuses an EBM layer as an outer-generator in the continual learning framework for\nNLP tasks. The study demonstrates the efficacy of EBM in NLP tasks, achieving\nstate-of-the-art results in all experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "In the 7th International Conference on Artificial Intelligence in\n  Information and Communication (ICAIIC 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.05495v1",
    "published_date": "2025-01-09 15:47:30 UTC",
    "updated_date": "2025-01-09 15:47:30 UTC"
  },
  {
    "arxiv_id": "2501.05278v1",
    "title": "Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments",
    "authors": [
      "Ritam Guha",
      "Nilavra Pathak"
    ],
    "abstract": "Counterfactual estimators are critical for learning and refining policies\nusing logged data, a process known as Off-Policy Evaluation (OPE). OPE allows\nresearchers to assess new policies without costly experiments, speeding up the\nevaluation process. Online experimental methods, such as A/B tests, are\neffective but often slow, thus delaying the policy selection and optimization\nprocess.\n  In this work, we explore the application of OPE methods in the context of\nresource allocation in dynamic auction environments. Given the competitive\nnature of environments where rapid decision-making is crucial for gaining a\ncompetitive edge, the ability to quickly and accurately assess algorithmic\nperformance is essential. By utilizing counterfactual estimators as a\npreliminary step before conducting A/B tests, we aim to streamline the\nevaluation process, reduce the time and resources required for experimentation,\nand enhance confidence in the chosen policies. Our investigation focuses on the\nfeasibility and effectiveness of using these estimators to predict the outcomes\nof potential resource allocation strategies, evaluate their performance, and\nfacilitate more informed decision-making in policy selection. Motivated by the\noutcomes of our initial study, we envision an advanced analytics system\ndesigned to seamlessly and dynamically assess new resource allocation\nstrategies and policies.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-fin.CP"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 15 figures, IEEE format",
    "pdf_url": "http://arxiv.org/pdf/2501.05278v1",
    "published_date": "2025-01-09 14:39:40 UTC",
    "updated_date": "2025-01-09 14:39:40 UTC"
  },
  {
    "arxiv_id": "2501.05264v3",
    "title": "Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation",
    "authors": [
      "Jiaxuan Peng",
      "Mengshi Qi",
      "Dong Zhao",
      "Huadong Ma"
    ],
    "abstract": "3D human pose estimation (3D HPE) has emerged as a prominent research topic,\nparticularly in the realm of RGB-based methods. However, RGB images are\nsusceptible to limitations such as sensitivity to lighting conditions and\npotential user discomfort. Consequently, multi-modal sensing, which leverages\nnon-intrusive sensors, is gaining increasing attention. Nevertheless,\nmulti-modal 3D HPE still faces challenges, including modality imbalance and the\nimperative for continual learning. In this work, we introduce a novel balanced\ncontinual multi-modal learning method for 3D HPE, which harnesses the power of\nRGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based\ncontribution algorithm to quantify the contribution of each modality and\nidentify modality imbalance. To address this imbalance, we employ a re-learning\nstrategy. Furthermore, recognizing that raw data is prone to noise\ncontamination, we develop a novel denoising continual learning approach. This\napproach incorporates a noise identification and separation module to mitigate\nthe adverse effects of noise and collaborates with the balanced learning\nstrategy to enhance optimization. Additionally, an adaptive EWC mechanism is\nemployed to alleviate catastrophic forgetting. We conduct extensive experiments\non the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the\nsuperiority of our approach in boosting 3D pose estimation and mitigating\ncatastrophic forgetting in complex scenarios. We will release our codes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05264v3",
    "published_date": "2025-01-09 14:19:33 UTC",
    "updated_date": "2025-01-16 02:39:20 UTC"
  },
  {
    "arxiv_id": "2501.05260v1",
    "title": "Enhancing Plagiarism Detection in Marathi with a Weighted Ensemble of TF-IDF and BERT Embeddings for Low-Resource Language Processing",
    "authors": [
      "Atharva Mutsaddi",
      "Aditya Choudhary"
    ],
    "abstract": "Plagiarism involves using another person's work or concepts without proper\nattribution, presenting them as original creations. With the growing amount of\ndata communicated in regional languages such as Marathi -- one of India's\nregional languages -- it is crucial to design robust plagiarism detection\nsystems tailored for low-resource languages. Language models like Bidirectional\nEncoder Representations from Transformers (BERT) have demonstrated exceptional\ncapability in text representation and feature extraction, making them essential\ntools for semantic analysis and plagiarism detection. However, the application\nof BERT for low-resource languages remains under-explored, particularly in the\ncontext of plagiarism detection. This paper presents a method to enhance the\naccuracy of plagiarism detection for Marathi texts using BERT sentence\nembeddings in conjunction with Term Frequency-Inverse Document Frequency\n(TF-IDF) feature representation. This approach effectively captures\nstatistical, semantic, and syntactic aspects of text features through a\nweighted voting ensemble of machine learning models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7; H.3.3"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted into LoResLM: The First Workshop on Language Models for\n  Low-Resource Languages, colocated with COLING 2025 and set to be published\n  into ACL Anthology",
    "pdf_url": "http://arxiv.org/pdf/2501.05260v1",
    "published_date": "2025-01-09 14:14:18 UTC",
    "updated_date": "2025-01-09 14:14:18 UTC"
  },
  {
    "arxiv_id": "2501.05258v1",
    "title": "Automating the Detection of Code Vulnerabilities by Analyzing GitHub Issues",
    "authors": [
      "Daniele Cipollone",
      "Changjie Wang",
      "Mariano Scazzariello",
      "Simone Ferlin",
      "Maliheh Izadi",
      "Dejan Kostic",
      "Marco Chiesa"
    ],
    "abstract": "In today's digital landscape, the importance of timely and accurate\nvulnerability detection has significantly increased. This paper presents a\nnovel approach that leverages transformer-based models and machine learning\ntechniques to automate the identification of software vulnerabilities by\nanalyzing GitHub issues. We introduce a new dataset specifically designed for\nclassifying GitHub issues relevant to vulnerability detection. We then examine\nvarious classification techniques to determine their effectiveness. The results\ndemonstrate the potential of this approach for real-world application in early\nvulnerability detection, which could substantially reduce the window of\nexploitation for software vulnerabilities. This research makes a key\ncontribution to the field by providing a scalable and computationally efficient\nframework for automated detection, enabling the prevention of compromised\nsoftware usage before official notifications. This work has the potential to\nenhance the security of open-source software ecosystems.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "D.2.5; K.6.3"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05258v1",
    "published_date": "2025-01-09 14:13:39 UTC",
    "updated_date": "2025-01-09 14:13:39 UTC"
  },
  {
    "arxiv_id": "2501.05252v1",
    "title": "From Scientific Texts to Verifiable Code: Automating the Process with Transformers",
    "authors": [
      "Changjie Wang",
      "Mariano Scazzariello",
      "Marco Chiesa"
    ],
    "abstract": "Despite the vast body of research literature proposing algorithms with formal\nguarantees, the amount of verifiable code in today's systems remains minimal.\nThis discrepancy stems from the inherent difficulty of verifying code,\nparticularly due to the time-consuming nature and strict formalism of proof\ndetails that formal verification tools require. However, the emergence of\ntransformers in Large Language Models presents a promising solution to this\nchallenge. In this position paper, we believe that transformers have the\npotential to read research papers that propose algorithms with formal proofs\nand translate these proofs into verifiable code. We leverage transformers to\nfirst build a formal structure of the proof using the original text from the\npaper, and then to handle the tedious, low-level aspects of proofs that are\noften omitted by humans. We argue that this approach can significantly reduce\nthe barrier to formal verification. The above idea of reading papers to write\nverifiable code opens new avenues for automating the verification of complex\nsystems, enabling a future where formally verified algorithms from academic\nresearch can more seamlessly transition into real-world software systems,\nthereby improving code reliability and security.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LO",
      "D.2.4; D.2.3; I.2.7; I.2.2; I.2.3; I.2.5; F.3.1"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05252v1",
    "published_date": "2025-01-09 14:03:35 UTC",
    "updated_date": "2025-01-09 14:03:35 UTC"
  },
  {
    "arxiv_id": "2501.05249v1",
    "title": "RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models",
    "authors": [
      "Peizhuo Lv",
      "Mengjie Sun",
      "Hao Wang",
      "Xiaofeng Wang",
      "Shengzhi Zhang",
      "Yuxuan Chen",
      "Kai Chen",
      "Limin Sun"
    ],
    "abstract": "In recent years, tremendous success has been witnessed in Retrieval-Augmented\nGeneration (RAG), widely used to enhance Large Language Models (LLMs) in\ndomain-specific, knowledge-intensive, and privacy-sensitive tasks. However,\nattackers may steal those valuable RAGs and deploy or commercialize them,\nmaking it essential to detect Intellectual Property (IP) infringement. Most\nexisting ownership protection solutions, such as watermarks, are designed for\nrelational databases and texts. They cannot be directly applied to RAGs because\nrelational database watermarks require white-box access to detect IP\ninfringement, which is unrealistic for the knowledge base in RAGs. Meanwhile,\npost-processing by the adversary's deployed LLMs typically destructs text\nwatermark information. To address those problems, we propose a novel black-box\n\"knowledge watermark\" approach, named RAG-WM, to detect IP infringement of\nRAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark\nGenerator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark\ntexts based on watermark entity-relationship tuples and inject them into the\ntarget RAG. We evaluate RAG-WM across three domain-specific and two\nprivacy-sensitive tasks on four benchmark LLMs. Experimental results show that\nRAG-WM effectively detects the stolen RAGs in various deployed LLMs.\nFurthermore, RAG-WM is robust against paraphrasing, unrelated content removal,\nknowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising application in\ndetecting IP infringement of RAG systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05249v1",
    "published_date": "2025-01-09 14:01:15 UTC",
    "updated_date": "2025-01-09 14:01:15 UTC"
  },
  {
    "arxiv_id": "2501.05248v1",
    "title": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning",
    "authors": [
      "Laura Puccioni",
      "Alireza Farshin",
      "Mariano Scazzariello",
      "Changjie Wang",
      "Marco Chiesa",
      "Dejan Kostic"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE",
      "I.2.2; I.2.6; D.1.2"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05248v1",
    "published_date": "2025-01-09 14:00:01 UTC",
    "updated_date": "2025-01-09 14:00:01 UTC"
  },
  {
    "arxiv_id": "2501.05247v2",
    "title": "Online Prompt Selection for Program Synthesis",
    "authors": [
      "Yixuan Li",
      "Lewis Frampton",
      "Federico Mora",
      "Elizabeth Polgreen"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2% more queries than the best single solver and\nachieves results within 4% of the virtual best solver.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25) Main Track",
    "pdf_url": "http://arxiv.org/pdf/2501.05247v2",
    "published_date": "2025-01-09 13:57:09 UTC",
    "updated_date": "2025-01-29 16:52:16 UTC"
  },
  {
    "arxiv_id": "2503.15492v1",
    "title": "World of ScoreCraft: Novel Multi Scorer Experiment on the Impact of a Decision Support System in Sleep Staging",
    "authors": [
      "Benedikt Holm",
      "Arnar Óskarsson",
      "Björn Elvar Þorleifsson",
      "Hörður Þór Hafsteinsson",
      "Sigríður Sigurðardóttir",
      "Heiður Grétarsdóttir",
      "Kenan Hoelke",
      "Gabriel Marc Marie Jouan",
      "Thomas Penzel",
      "Erna Sif Arnardottir",
      "María Óskarsdóttir"
    ],
    "abstract": "Manual scoring of polysomnography (PSG) is a time intensive task, prone to\ninter scorer variability that can impact diagnostic reliability. This study\ninvestigates the integration of decision support systems (DSS) into PSG scoring\nworkflows, focusing on their effects on accuracy, scoring time, and potential\nbiases toward recommendations from artificial intelligence (AI) compared to\nhuman generated recommendations. Using a novel online scoring platform, we\nconducted a repeated measures study with sleep technologists,\n  who scored traditional and self applied PSGs. Participants were occasionally\npresented with recommendations labeled as either human or AI generated. We\nfound that traditional PSGs tended to be scored slightly more accurately than\nself applied PSGs, but this difference was not statistically significant.\nCorrect recommendations significantly improved scoring accuracy for both PSG\ntypes, while incorrect recommendations reduced accuracy. No significant bias\nwas observed toward or against AI generated recommendations compared to human\ngenerated recommendations. These findings highlight the potential of AI to\nenhance PSG scoring reliability. However, ensuring the accuracy of AI outputs\nis critical to maximizing its benefits. Future research should explore the long\nterm impacts of DSS on scoring workflows and strategies for integrating AI in\nclinical practice.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 13 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.15492v1",
    "published_date": "2025-01-09 13:48:54 UTC",
    "updated_date": "2025-01-09 13:48:54 UTC"
  },
  {
    "arxiv_id": "2501.05238v1",
    "title": "FOCUS: Towards Universal Foreground Segmentation",
    "authors": [
      "Zuyao You",
      "Lingyu Kong",
      "Lingchen Meng",
      "Zuxuan Wu"
    ],
    "abstract": "Foreground segmentation is a fundamental task in computer vision,\nencompassing various subdivision tasks. Previous research has typically\ndesigned task-specific architectures for each task, leading to a lack of\nunification. Moreover, they primarily focus on recognizing foreground objects\nwithout effectively distinguishing them from the background. In this paper, we\nemphasize the importance of the background and its relationship with the\nforeground. We introduce FOCUS, the Foreground ObjeCts Universal Segmentation\nframework that can handle multiple foreground tasks. We develop a multi-scale\nsemantic network using the edge information of objects to enhance image\nfeatures. To achieve boundary-aware segmentation, we propose a novel\ndistillation method, integrating the contrastive learning strategy to refine\nthe prediction mask in multi-modal feature space. We conduct extensive\nexperiments on a total of 13 datasets across 5 tasks, and the results\ndemonstrate that FOCUS consistently outperforms the state-of-the-art\ntask-specific models on most metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05238v1",
    "published_date": "2025-01-09 13:44:15 UTC",
    "updated_date": "2025-01-09 13:44:15 UTC"
  },
  {
    "arxiv_id": "2501.05234v1",
    "title": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs",
    "authors": [
      "Artem Fedorchenko",
      "Tanel Alumäe"
    ],
    "abstract": "This paper presents an approach for generating high-quality, same-language\nsubtitles for Estonian TV content. We fine-tune the Whisper model on\nhuman-generated Estonian subtitles and enhance it with iterative\npseudo-labeling and large language model (LLM) based post-editing. Our\nexperiments demonstrate notable subtitle quality improvement through\npseudo-labeling with an unlabeled dataset. We find that applying LLM-based\nediting at test time enhances subtitle accuracy, while its use during training\ndoes not yield further gains. This approach holds promise for creating subtitle\nquality close to human standard and could be extended to real-time\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05234v1",
    "published_date": "2025-01-09 13:41:37 UTC",
    "updated_date": "2025-01-09 13:41:37 UTC"
  },
  {
    "arxiv_id": "2501.06262v1",
    "title": "Towards smart and adaptive agents for active sensing on edge devices",
    "authors": [
      "Devendra Vyas",
      "Miguel de Prado",
      "Tim Verbelen"
    ],
    "abstract": "TinyML has made deploying deep learning models on low-power edge devices\nfeasible, creating new opportunities for real-time perception in constrained\nenvironments. However, the adaptability of such deep learning methods remains\nlimited to data drift adaptation, lacking broader capabilities that account for\nthe environment's underlying dynamics and inherent uncertainty. Deep learning's\nscaling laws, which counterbalance this limitation by massively up-scaling data\nand model size, cannot be applied when deploying on the Edge, where deep\nlearning limitations are further amplified as models are scaled down for\ndeployment on resource-constrained devices.\n  This paper presents a smart agentic system capable of performing on-device\nperception and planning, enabling active sensing on the edge. By incorporating\nactive inference into our solution, our approach extends beyond deep learning\ncapabilities, allowing the system to plan in dynamic environments while\noperating in real time with a modest total model size of 2.3 MB. We showcase\nour proposed system by creating and deploying a saccade agent connected to an\nIoT camera with pan and tilt capabilities on an NVIDIA Jetson embedded device.\nThe saccade agent controls the camera's field of view following optimal\npolicies derived from the active inference principles, simulating human-like\nsaccadic motion for surveillance and robotics applications.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06262v1",
    "published_date": "2025-01-09 13:27:02 UTC",
    "updated_date": "2025-01-09 13:27:02 UTC"
  },
  {
    "arxiv_id": "2501.16336v2",
    "title": "Runtime Analysis of Evolutionary Algorithms for Multiparty Multiobjective Optimization",
    "authors": [
      "Yuetong Sun",
      "Peilan Xu",
      "Wenjian Luo"
    ],
    "abstract": "In scenarios where multiple decision-makers operate within a common decision\nspace, each focusing on their own multi-objective optimization problem (e.g.,\nbargaining games), the problem can be modeled as a multi-party multi-objective\noptimization problem (MPMOP). While numerous evolutionary algorithms have been\nproposed to solve MPMOPs, most results remain empirical. This paper presents\nthe first theoretical analysis of the expected runtime of evolutionary\nalgorithms on bi-party multi-objective optimization problems (BPMOPs). Our\nfindings demonstrate that employing traditional multi-objective optimization\nalgorithms to solve MPMOPs is both time-consuming and inefficient, as the\nresulting population contains many solutions that fail to achieve consensus\namong decision-makers. An alternative approach involves decision-makers\nindividually solving their respective optimization problems and seeking\nconsensus only in the final stage. While feasible for pseudo-Boolean\noptimization problems, this method may fail to guarantee approximate\nperformance for one party in NP-hard problems. Finally, We propose\ncoevolutionary multi-party multi-objective optimizers (CoEMPMO) for\npseudo-Boolean optimization and shortest path problems within a multi-party\nmulti-objective context, which maintains a common solution set among all\nparties through coevolution. Theoretical and experimental results demonstrate\nthat the proposed \\( \\text{CoEMPMO}_{\\text{random}} \\) outperforms previous\nalgorithms in terms of the expected lower bound on runtime for pseudo-Boolean\noptimization problems. Additionally, \\(\n\\text{CoEMPMO}_{\\text{cons}}^{\\text{SP}} \\) achieves better efficiency and\nprecision in solving shortest path problems compared to existing algorithms.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16336v2",
    "published_date": "2025-01-09 13:16:08 UTC",
    "updated_date": "2025-02-23 12:06:36 UTC"
  },
  {
    "arxiv_id": "2501.05220v1",
    "title": "A Novel Approach to Scalable and Automatic Topic-Controlled Question Generation in Education",
    "authors": [
      "Ziqing Li",
      "Mutlu Cukurova",
      "Sahan Bulathwela"
    ],
    "abstract": "The development of Automatic Question Generation (QG) models has the\npotential to significantly improve educational practices by reducing the\nteacher workload associated with creating educational content. This paper\nintroduces a novel approach to educational question generation that controls\nthe topical focus of questions. The proposed Topic-Controlled Question\nGeneration (T-CQG) method enhances the relevance and effectiveness of the\ngenerated content for educational purposes. Our approach uses fine-tuning on a\npre-trained T5-small model, employing specially created datasets tailored to\neducational needs. The research further explores the impacts of pre-training\nstrategies, quantisation, and data augmentation on the model's performance. We\nspecifically address the challenge of generating semantically aligned questions\nwith paragraph-level contexts, thereby improving the topic specificity of the\ngenerated questions. In addition, we introduce and explore novel evaluation\nmethods to assess the topical relatedness of the generated questions. Our\nresults, validated through rigorous offline and human-backed evaluations,\ndemonstrate that the proposed models effectively generate high-quality,\ntopic-focused questions. These models have the potential to reduce teacher\nworkload and support personalised tutoring systems by serving as bespoke\nquestion generators. With its relatively small number of parameters, the\nproposals not only advance the capabilities of question generation models for\nhandling specific educational topics but also offer a scalable solution that\nreduces infrastructure costs. This scalability makes them feasible for\nwidespread use in education without reliance on proprietary large language\nmodels like ChatGPT.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "H.3.3; J.1; I.2.0"
    ],
    "primary_category": "cs.CY",
    "comment": "To be published at ACM Conf. on Learning Analytics and Knowledge\n  (LAK'25)",
    "pdf_url": "http://arxiv.org/pdf/2501.05220v1",
    "published_date": "2025-01-09 13:13:24 UTC",
    "updated_date": "2025-01-09 13:13:24 UTC"
  },
  {
    "arxiv_id": "2501.05213v1",
    "title": "GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign Language Accessibility",
    "authors": [
      "Dimitris Kouremenos",
      "Klimis Ntalianis"
    ],
    "abstract": "The Greek Language Multimodal Lip Reading with Integrated Sign Language\nAccessibility (GLaM-Sign) [1] is a groundbreaking resource in accessibility and\nmultimodal AI, designed to support Deaf and Hard-of-Hearing (DHH) individuals.\nDeveloped from the FEELIT project [2], it integrates high-resolution audio,\nvideo, textual transcriptions, and Greek Sign Language translations for\napplications like real-time sign language translation and enhanced subtitle\nsynchronization. While its primary focus is on promoting inclusivity in the\nGreek tourism sector, its adaptability extends to education, healthcare, and\npublic services. Future advancements will enhance word-level precision and\nscalability to additional languages, supported by advanced AI methodologies and\ncollaborations with diverse stakeholders. This dataset underscores the\ntransformative potential of multimodal resources in bridging communication\ngaps, fostering innovation, and setting a benchmark for ethical AI and\ninclusive technologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.05213v1",
    "published_date": "2025-01-09 13:06:47 UTC",
    "updated_date": "2025-01-09 13:06:47 UTC"
  },
  {
    "arxiv_id": "2501.05205v4",
    "title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning",
    "authors": [
      "Xueyi Ke",
      "Satoshi Tsutsui",
      "Yayun Zhang",
      "Bihan Wen"
    ],
    "abstract": "Infants develop complex visual understanding rapidly, even preceding the\nacquisition of linguistic skills. As computer vision seeks to replicate the\nhuman vision system, understanding infant visual development may offer valuable\ninsights. In this paper, we present an interdisciplinary study exploring this\nquestion: can a computational model that imitates the infant learning process\ndevelop broader visual concepts that extend beyond the vocabulary it has heard,\nsimilar to how infants naturally learn? To investigate this, we analyze a\nrecently published model in Science by Vong et al., which is trained on\nlongitudinal, egocentric images of a single child paired with transcribed\nparental speech. We perform neuron labeling to identify visual concept neurons\nhidden in the model's internal representations. We then demonstrate that these\nneurons can recognize objects beyond the model's original vocabulary.\nFurthermore, we compare the differences in representation between infant models\nand those in modern computer vision models, such as CLIP and ImageNet\npre-trained model. Ultimately, our work bridges cognitive science and computer\nvision by analyzing the internal representations of a computational model\ntrained on an infant visual and linguistic inputs. Our code is available at\nhttps://github.com/Kexueyi/discover_infant_vis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05205v4",
    "published_date": "2025-01-09 12:55:55 UTC",
    "updated_date": "2025-03-25 07:11:03 UTC"
  },
  {
    "arxiv_id": "2501.05197v1",
    "title": "An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes",
    "authors": [
      "Drago Plecko",
      "Paul Secombe",
      "Andrea Clarke",
      "Amelia Fiske",
      "Samarra Toby",
      "Donisha Duff",
      "David Pilcher",
      "Leo Anthony Celi",
      "Rinaldo Bellomo",
      "Elias Bareinboim"
    ],
    "abstract": "The new era of large-scale data collection and analysis presents an\nopportunity for diagnosing and understanding the causes of health inequities.\nIn this study, we describe a framework for systematically analyzing health\ndisparities using causal inference. The framework is illustrated by\ninvestigating racial and ethnic disparities in intensive care unit (ICU)\noutcome between majority and minority groups in Australia (Indigenous vs.\nNon-Indigenous) and the United States (African-American vs. White). We\ndemonstrate that commonly used statistical measures for quantifying inequity\nare insufficient, and focus on attributing the observed disparity to the causal\nmechanisms that generate it. We find that minority patients are younger at\nadmission, have worse chronic health, are more likely to be admitted for urgent\nand non-elective reasons, and have higher illness severity. At the same time,\nhowever, we find a protective direct effect of belonging to a minority group,\nwith minority patients showing improved survival compared to their majority\ncounterparts, with all other variables kept equal. We demonstrate that this\nprotective effect is related to the increased probability of being admitted to\nICU, with minority patients having an increased risk of ICU admission. We also\nfind that minority patients, while showing improved survival, are more likely\nto be readmitted to ICU. Thus, due to worse access to primary health care,\nminority patients are more likely to end up in ICU for preventable conditions,\ncausing a reduction in the mortality rates and creating an effect that appears\nto be protective. Since the baseline risk of ICU admission may serve as proxy\nfor lack of access to primary care, we developed the Indigenous Intensive Care\nEquity (IICE) Radar, a monitoring system for tracking the over-utilization of\nICU resources by the Indigenous population of Australia across geographical\nareas.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05197v1",
    "published_date": "2025-01-09 12:48:15 UTC",
    "updated_date": "2025-01-09 12:48:15 UTC"
  },
  {
    "arxiv_id": "2501.05165v1",
    "title": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in Secure Software Engineering",
    "authors": [
      "Matteo Esposito"
    ],
    "abstract": "Context. Developing secure and reliable software remains a key challenge in\nsoftware engineering (SE). The ever-evolving technological landscape offers\nboth opportunities and threats, creating a dynamic space where chaos and order\ncompete. Secure software engineering (SSE) must continuously address\nvulnerabilities that endanger software systems and carry broader socio-economic\nrisks, such as compromising critical national infrastructure and causing\nsignificant financial losses. Researchers and practitioners have explored\nmethodologies like Static Application Security Testing Tools (SASTTs) and\nartificial intelligence (AI) approaches, including machine learning (ML) and\nlarge language models (LLMs), to detect and mitigate these vulnerabilities.\nEach method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing\ndomain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as\nevaluating effort-aware metrics, analyzing SASTTs, conducting method-level\nanalysis, and leveraging evidence-based techniques like systematic dataset\nreviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for\nidentifying vulnerabilities, gaps in SASTT coverage of vulnerability types,\nweak relationships among vulnerability severity scores, improved defect\nprediction accuracy using just-in-time modeling, and threats posed by untouched\nmethods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance\nof contextual knowledge in improving AI-driven vulnerability and defect\nprediction. The comprehensive analysis advances effective prediction models,\nbenefiting both researchers and practitioners.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.ET"
    ],
    "primary_category": "cs.SE",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2501.05165v1",
    "published_date": "2025-01-09 11:38:58 UTC",
    "updated_date": "2025-01-09 11:38:58 UTC"
  },
  {
    "arxiv_id": "2501.05163v1",
    "title": "Explainable AI based System for Supply Air Temperature Forecast",
    "authors": [
      "Marika Eik",
      "Ahmet Kose",
      "Hossein Nourollahi Hokmabad",
      "Juri Belikov"
    ],
    "abstract": "This paper explores the application of Explainable AI (XAI) techniques to\nimprove the transparency and understanding of predictive models in control of\nautomated supply air temperature (ASAT) of Air Handling Unit (AHU). The study\nfocuses on forecasting of ASAT using a linear regression with Huber loss.\nHowever, having only a control curve without semantic and/or physical\nexplanation is often not enough. The present study employs one of the XAI\nmethods: Shapley values, which allows to reveal the reasoning and highlight the\ncontribution of each feature to the final ASAT forecast. In comparison to other\nXAI methods, Shapley values have solid mathematical background, resulting in\ninterpretation transparency. The study demonstrates the contrastive\nexplanations--slices, for each control value of ASAT, which makes it possible\nto give the client objective justifications for curve changes.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "5 pages, 7 figures, 1 table, conference paper",
    "pdf_url": "http://arxiv.org/pdf/2501.05163v1",
    "published_date": "2025-01-09 11:36:29 UTC",
    "updated_date": "2025-01-09 11:36:29 UTC"
  },
  {
    "arxiv_id": "2501.05155v1",
    "title": "Biomedical Relation Extraction via Adaptive Document-Relation Cross-Mapping and Concept Unique Identifier",
    "authors": [
      "Yufei Shang",
      "Yanrong Guo",
      "Shijie Hao",
      "Richang Hong"
    ],
    "abstract": "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.05155v1",
    "published_date": "2025-01-09 11:19:40 UTC",
    "updated_date": "2025-01-09 11:19:40 UTC"
  },
  {
    "arxiv_id": "2501.05147v1",
    "title": "A Systematic Literature Review on Deep Learning-based Depth Estimation in Computer Vision",
    "authors": [
      "Ali Rohan",
      "Md Junayed Hasan",
      "Andrei Petrovski"
    ],
    "abstract": "Depth estimation (DE) provides spatial information about a scene and enables\ntasks such as 3D reconstruction, object detection, and scene understanding.\nRecently, there has been an increasing interest in using deep learning\n(DL)-based methods for DE. Traditional techniques rely on handcrafted features\nthat often struggle to generalise to diverse scenes and require extensive\nmanual tuning. However, DL models for DE can automatically extract relevant\nfeatures from input data, adapt to various scene conditions, and generalise\nwell to unseen environments. Numerous DL-based methods have been developed,\nmaking it necessary to survey and synthesize the state-of-the-art (SOTA).\nPrevious reviews on DE have mainly focused on either monocular or stereo-based\ntechniques, rather than comprehensively reviewing DE. Furthermore, to the best\nof our knowledge, there is no systematic literature review (SLR) that\ncomprehensively focuses on DE. Therefore, this SLR study is being conducted.\nInitially, electronic databases were searched for relevant publications,\nresulting in 1284 publications. Using defined exclusion and quality criteria,\n128 publications were shortlisted and further filtered to select 59\nhigh-quality primary studies. These studies were analysed to extract data and\nanswer defined research questions. Based on the results, DL methods were\ndeveloped for mainly three different types of DE: monocular, stereo, and\nmulti-view. 20 publicly available datasets were used to train, test, and\nevaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most\nused datasets. 29 evaluation metrics were used to assess the performance of DE.\n35 base models were reported in the primary studies, and the top five most-used\nbase models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16. Finally,\nthe lack of ground truth data was among the most significant challenges\nreported by primary studies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05147v1",
    "published_date": "2025-01-09 10:56:50 UTC",
    "updated_date": "2025-01-09 10:56:50 UTC"
  },
  {
    "arxiv_id": "2501.05113v1",
    "title": "Constrained Optimization of Charged Particle Tracking with Multi-Agent Reinforcement Learning",
    "authors": [
      "Tobias Kortus",
      "Ralf Keidel",
      "Nicolas R. Gauger",
      "Jan Kieseler"
    ],
    "abstract": "Reinforcement learning demonstrated immense success in modelling complex\nphysics-driven systems, providing end-to-end trainable solutions by interacting\nwith a simulated or real environment, maximizing a scalar reward signal. In\nthis work, we propose, building upon previous work, a multi-agent reinforcement\nlearning approach with assignment constraints for reconstructing particle\ntracks in pixelated particle detectors. Our approach optimizes collaboratively\na parametrized policy, functioning as a heuristic to a multidimensional\nassignment problem, by jointly minimizing the total amount of particle\nscattering over the reconstructed tracks in a readout frame. To satisfy\nconstraints, guaranteeing a unique assignment of particle hits, we propose a\nsafety layer solving a linear assignment problem for every joint action.\nFurther, to enforce cost margins, increasing the distance of the local policies\npredictions to the decision boundaries of the optimizer mappings, we recommend\nthe use of an additional component in the blackbox gradient estimation, forcing\nthe policy to solutions with lower total assignment costs. We empirically show\non simulated data, generated for a particle detector developed for proton\nimaging, the effectiveness of our approach, compared to multiple single- and\nmulti-agent baselines. We further demonstrate the effectiveness of constraints\nwith cost margins for both optimization and generalization, introduced by wider\nregions with high reconstruction performance as well as reduced predictive\ninstabilities. Our results form the basis for further developments in RL-based\ntracking, offering both enhanced performance with constrained policies and\ngreater flexibility in optimizing tracking algorithms through the option for\nindividual and team rewards.",
    "categories": [
      "physics.comp-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05113v1",
    "published_date": "2025-01-09 09:59:42 UTC",
    "updated_date": "2025-01-09 09:59:42 UTC"
  },
  {
    "arxiv_id": "2501.06256v1",
    "title": "What Matters for In-Context Learning: A Balancing Act of Look-up and In-Weight Learning",
    "authors": [
      "Jelena Bratulić",
      "Sudhanshu Mittal",
      "Christian Rupprecht",
      "Thomas Brox"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nvarious tasks, including In-Context Learning (ICL), where the model performs\nnew tasks by conditioning solely on the examples provided in the context,\nwithout updating the model's weights. While prior research has explored the\nroles of pretraining data and model architecture, the key mechanism behind ICL\nremains unclear. In this work, we systematically uncover properties present in\nLLMs that support the emergence of ICL. To disambiguate these factors, we\nconduct a study with a controlled dataset and data sequences using a deep\nautoregressive model. We show that conceptual repetitions in the data sequences\nare crucial for ICL, more so than previously indicated training data properties\nlike burstiness or long-tail distribution. Conceptual repetitions could refer\nto $n$-gram repetitions in textual data or exact image copies in image sequence\ndata. Such repetitions also offer other previously overlooked benefits such as\nreduced transiency in ICL performance. Furthermore, we show that the emergence\nof ICL depends on balancing the in-weight learning objective with the\nin-context solving ability during training.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06256v1",
    "published_date": "2025-01-09 09:45:05 UTC",
    "updated_date": "2025-01-09 09:45:05 UTC"
  },
  {
    "arxiv_id": "2501.05095v1",
    "title": "Advancing ALS Applications with Large-Scale Pre-training: Dataset Development and Downstream Assessment",
    "authors": [
      "Haoyi Xiu",
      "Xin Liu",
      "Taehoon Kim",
      "Kyoung-Sook Kim"
    ],
    "abstract": "The pre-training and fine-tuning paradigm has revolutionized satellite remote\nsensing applications. However, this approach remains largely underexplored for\nairborne laser scanning (ALS), an important technology for applications such as\nforest management and urban planning. In this study, we address this gap by\nconstructing a large-scale ALS point cloud dataset and evaluating its impact on\ndownstream applications. Our dataset comprises ALS point clouds collected\nacross the contiguous United States, provided by the United States Geological\nSurvey's 3D Elevation Program. To ensure efficient data collection while\ncapturing diverse land cover and terrain types, we introduce a geospatial\nsampling method that selects point cloud tiles based on land cover maps and\ndigital elevation models. As a baseline self-supervised learning model, we\nadopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point\nclouds, and pre-train it on the constructed dataset. The pre-trained models are\nsubsequently fine-tuned for downstream tasks, including tree species\nclassification, terrain scene recognition, and point cloud semantic\nsegmentation. Our results show that the pre-trained models significantly\noutperform their scratch counterparts across all downstream tasks,\ndemonstrating the transferability of the representations learned from the\nproposed dataset. Furthermore, we observe that scaling the dataset using our\ngeospatial sampling method consistently enhances performance, whereas\npre-training on datasets constructed with random sampling fails to achieve\nsimilar improvements. These findings highlight the utility of the constructed\ndataset and the effectiveness of our sampling strategy in the pre-training and\nfine-tuning paradigm. The source code and pre-trained models will be made\npublicly available at \\url{https://github.com/martianxiu/ALS_pretraining}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05095v1",
    "published_date": "2025-01-09 09:21:09 UTC",
    "updated_date": "2025-01-09 09:21:09 UTC"
  },
  {
    "arxiv_id": "2501.05079v2",
    "title": "Multimodal-to-Text Prompt Engineering in Large Language Models Using Feature Embeddings for GNSS Interference Characterization",
    "authors": [
      "Harshith Manjunath",
      "Lucas Heublein",
      "Tobias Feigl",
      "Felix Ott"
    ],
    "abstract": "Large language models (LLMs) are advanced AI systems applied across various\ndomains, including NLP, information retrieval, and recommendation systems.\nDespite their adaptability and efficiency, LLMs have not been extensively\nexplored for signal processing tasks, particularly in the domain of global\nnavigation satellite system (GNSS) interference monitoring. GNSS interference\nmonitoring is essential to ensure the reliability of vehicle localization on\nroads, a critical requirement for numerous applications. However, GNSS-based\npositioning is vulnerable to interference from jamming devices, which can\ncompromise its accuracy. The primary objective is to identify, classify, and\nmitigate these interferences. Interpreting GNSS snapshots and the associated\ninterferences presents significant challenges due to the inherent complexity,\nincluding multipath effects, diverse interference types, varying sensor\ncharacteristics, and satellite constellations. In this paper, we extract\nfeatures from a large GNSS dataset and employ LLaVA to retrieve relevant\ninformation from an extensive knowledge base. We employ prompt engineering to\ninterpret the interferences and environmental factors, and utilize t-SNE to\nanalyze the feature embeddings. Our findings demonstrate that the proposed\nmethod is capable of visual and logical reasoning within the GNSS context.\nFurthermore, our pipeline outperforms state-of-the-art machine learning models\nin interference classification tasks.",
    "categories": [
      "cs.AI",
      "eess.SP",
      "68T30, 68T05",
      "H.1; H.5; I.4.9; I.4.10"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05079v2",
    "published_date": "2025-01-09 09:01:04 UTC",
    "updated_date": "2025-01-15 20:46:44 UTC"
  },
  {
    "arxiv_id": "2501.05078v1",
    "title": "Analyzing Memorization in Large Language Models through the Lens of Model Attribution",
    "authors": [
      "Tarun Ram Menta",
      "Susmit Agrawal",
      "Chirag Agarwal"
    ],
    "abstract": "Large Language Models (LLMs) are prevalent in modern applications but often\nmemorize training data, leading to privacy breaches and copyright issues.\nExisting research has mainly focused on posthoc analyses, such as extracting\nmemorized content or developing memorization metrics, without exploring the\nunderlying architectural factors that contribute to memorization. In this work,\nwe investigate memorization from an architectural lens by analyzing how\nattention modules at different layers impact its memorization and\ngeneralization performance. Using attribution techniques, we systematically\nintervene in the LLM architecture by bypassing attention modules at specific\nblocks while keeping other components like layer normalization and MLP\ntransformations intact. We provide theorems analyzing our intervention\nmechanism from a mathematical view, bounding the difference in layer outputs\nwith and without our attributions. Our theoretical and empirical analyses\nreveal that attention modules in deeper transformer blocks are primarily\nresponsible for memorization, whereas earlier blocks are crucial for the models\ngeneralization and reasoning capabilities. We validate our findings through\ncomprehensive experiments on different LLM families (Pythia and GPTNeo) and\nfive benchmark datasets. Our insights offer a practical approach to mitigate\nmemorization in LLMs while preserving their performance, contributing to safer\nand more ethical deployment in real world applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05078v1",
    "published_date": "2025-01-09 09:00:32 UTC",
    "updated_date": "2025-01-09 09:00:32 UTC"
  },
  {
    "arxiv_id": "2501.05075v1",
    "title": "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for General Industrial Process Tasks Based on Large Language Model",
    "authors": [
      "Shuo Tong",
      "Han Liu",
      "Runyuan Guo",
      "Xueqiong Tian",
      "Wenqing Wang",
      "Ding Liu",
      "Youmin Zhang"
    ],
    "abstract": "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05075v1",
    "published_date": "2025-01-09 08:59:14 UTC",
    "updated_date": "2025-01-09 08:59:14 UTC"
  },
  {
    "arxiv_id": "2501.05069v2",
    "title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning",
    "authors": [
      "Huabin Liu",
      "Filip Ilievski",
      "Cees G. M. Snoek"
    ],
    "abstract": "This paper proposes the first video-grounded entailment tree reasoning method\nfor commonsense video question answering (VQA). Despite the remarkable progress\nof large visual-language models (VLMs), there are growing concerns that they\nlearn spurious correlations between videos and likely answers, reinforced by\ntheir black-box nature and remaining benchmarking biases. Our method explicitly\ngrounds VQA tasks to video fragments in four steps: entailment tree\nconstruction, video-language entailment verification, tree reasoning, and\ndynamic tree expansion. A vital benefit of the method is its generalizability\nto current video and image-based VLMs across reasoning types. To support fair\nevaluation, we devise a de-biasing procedure based on large-language models\nthat rewrites VQA benchmark answer sets to enforce model reasoning. Systematic\nexperiments on existing and de-biased benchmarks highlight the impact of our\nmethod components across benchmarks, VLMs, and reasoning types.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05069v2",
    "published_date": "2025-01-09 08:44:42 UTC",
    "updated_date": "2025-03-25 03:46:09 UTC"
  },
  {
    "arxiv_id": "2501.05068v2",
    "title": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano Transcription",
    "authors": [
      "Hounsu Kim",
      "Taegyun Kwon",
      "Juhan Nam"
    ],
    "abstract": "Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05068v2",
    "published_date": "2025-01-09 08:44:06 UTC",
    "updated_date": "2025-01-13 12:06:15 UTC"
  },
  {
    "arxiv_id": "2501.05067v2",
    "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
    "authors": [
      "Jiaxing Zhao",
      "Boyuan Sun",
      "Xiang Chen",
      "Xihan Wei",
      "Qibin Hou"
    ],
    "abstract": "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large\nlanguage model. LLaVA-Octopus adaptively weights features from different visual\nprojectors based on user instructions, enabling us to leverage the\ncomplementary strengths of each projector. We observe that different visual\nprojectors exhibit distinct characteristics when handling specific tasks. For\ninstance, some projectors excel at capturing static details, while others are\nmore effective at processing temporal information, and some are better suited\nfor tasks requiring temporal coherence. By dynamically adjusting feature\nweights according to user instructions, LLaVA-Octopus dynamically selects and\ncombines the most suitable features, significantly enhancing the model's\nperformance in multimodal tasks. Experimental results demonstrate that\nLLaVA-Octopus achieves excellent performance across multiple benchmarks,\nespecially in tasks such as video question answering, long video understanding,\nand comprehensive multi-choices benchmarks, highlighting its broad application\npotential.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.05067v2",
    "published_date": "2025-01-09 08:43:57 UTC",
    "updated_date": "2025-03-14 07:29:54 UTC"
  },
  {
    "arxiv_id": "2501.05066v1",
    "title": "Improving Skeleton-based Action Recognition with Interactive Object Information",
    "authors": [
      "Hao Wen",
      "Ziqian Lu",
      "Fengli Shen",
      "Zhe-Ming Lu",
      "Jialin Cui"
    ],
    "abstract": "Human skeleton information is important in skeleton-based action recognition,\nwhich provides a simple and efficient way to describe human pose. However,\nexisting skeleton-based methods focus more on the skeleton, ignoring the\nobjects interacting with humans, resulting in poor performance in recognizing\nactions that involve object interactions. We propose a new action recognition\nframework introducing object nodes to supplement absent interactive object\ninformation. We also propose Spatial Temporal Variable Graph Convolutional\nNetworks (ST-VGCN) to effectively model the Variable Graph (VG) containing\nobject nodes. Specifically, in order to validate the role of interactive object\ninformation, by leveraging a simple self-training approach, we establish a new\ndataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more\nthan 2 million additional object nodes. At the same time, we designe the\nVariable Graph construction method to accommodate a variable number of nodes\nfor graph structure. Additionally, we are the first to explore the overfitting\nissue introduced by incorporating additional object information, and we propose\na VG-based data augmentation method to address this issue, called Random Node\nAttack. Finally, regarding the network structure, we introduce two fusion\nmodules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the\ncomprehensive performance by effectively fusing and balancing skeleton and\nobject node information. Our method surpasses the previous state-of-the-art on\nmultiple skeleton-based action recognition benchmarks. The accuracy of our\nmethod on NTU RGB+D 60 cross-subject split is 96.7\\%, and on cross-view split,\nit is 99.2\\%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05066v1",
    "published_date": "2025-01-09 08:43:09 UTC",
    "updated_date": "2025-01-09 08:43:09 UTC"
  },
  {
    "arxiv_id": "2501.05058v1",
    "title": "Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators",
    "authors": [
      "Leonard Lupin-Jimenez",
      "Moein Darman",
      "Subhashis Hazarika",
      "Tianning Wu",
      "Michael Gray",
      "Ruyoing He",
      "Anthony Wong",
      "Ashesh Chattopadhyay"
    ],
    "abstract": "Building on top of the success in AI-based atmospheric emulation, we propose\nan AI-based ocean emulation and downscaling framework focusing on the\nhigh-resolution regional ocean over Gulf of Mexico. Regional ocean emulation\npresents unique challenges owing to the complex bathymetry and lateral boundary\nconditions as well as from fundamental biases in deep learning-based\nframeworks, such as instability and hallucinations. In this paper, we develop a\ndeep learning-based framework to autoregressively integrate ocean-surface\nvariables over the Gulf of Mexico at $8$ Km spatial resolution without\nunphysical drifts over decadal time scales and simulataneously downscale and\nbias-correct it to $4$ Km resolution using a physics-constrained generative\nmodel. The framework shows both short-term skills as well as accurate long-term\nstatistics in terms of mean and variability.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG",
      "nlin.CD",
      "physics.geo-ph"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05058v1",
    "published_date": "2025-01-09 08:28:31 UTC",
    "updated_date": "2025-01-09 08:28:31 UTC"
  },
  {
    "arxiv_id": "2501.05057v1",
    "title": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models",
    "authors": [
      "Zengqi Peng",
      "Yubin Wang",
      "Xu Han",
      "Lei Zheng",
      "Jun Ma"
    ],
    "abstract": "Recent advancements in reinforcement learning (RL) demonstrate the\nsignificant potential in autonomous driving. Despite this promise, challenges\nsuch as the manual design of reward functions and low sample efficiency in\ncomplex environments continue to impede the development of safe and effective\ndriving policies. To tackle these issues, we introduce LearningFlow, an\ninnovative automated policy learning workflow tailored to urban driving. This\nframework leverages the collaboration of multiple large language model (LLM)\nagents throughout the RL training process. LearningFlow includes a curriculum\nsequence generation process and a reward generation process, which work in\ntandem to guide the RL policy by generating tailored training curricula and\nreward functions. Particularly, each process is supported by an analysis agent\nthat evaluates training progress and provides critical insights to the\ngeneration agent. Through the collaborative efforts of these LLM agents,\nLearningFlow automates policy learning across a series of complex driving\ntasks, and it significantly reduces the reliance on manual reward function\ndesign while enhancing sample efficiency. Comprehensive experiments are\nconducted in the high-fidelity CARLA simulator, along with comparisons with\nother existing methods, to demonstrate the efficacy of our proposed approach.\nThe results demonstrate that LearningFlow excels in generating rewards and\ncurricula. It also achieves superior performance and robust generalization\nacross various driving tasks, as well as commendable adaptation to different RL\nalgorithms.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05057v1",
    "published_date": "2025-01-09 08:28:16 UTC",
    "updated_date": "2025-01-09 08:28:16 UTC"
  },
  {
    "arxiv_id": "2501.05053v1",
    "title": "TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated Learning",
    "authors": [
      "Runhua Xu",
      "Bo Li",
      "Chao Li",
      "James B. D. Joshi",
      "Shuai Ma",
      "Jianxin Li"
    ],
    "abstract": "Federated learning is a computing paradigm that enhances privacy by enabling\nmultiple parties to collaboratively train a machine learning model without\nrevealing personal data. However, current research indicates that traditional\nfederated learning platforms are unable to ensure privacy due to privacy leaks\ncaused by the interchange of gradients. To achieve privacy-preserving federated\nlearning, integrating secure aggregation mechanisms is essential.\nUnfortunately, existing solutions are vulnerable to recently demonstrated\ninference attacks such as the disaggregation attack. This paper proposes\nTAPFed, an approach for achieving privacy-preserving federated learning in the\ncontext of multiple decentralized aggregators with malicious actors. TAPFed\nuses a proposed threshold functional encryption scheme and allows for a certain\nnumber of malicious aggregators while maintaining security and privacy. We\nprovide formal security and privacy analyses of TAPFed and compare it to\nvarious baselines through experimental evaluation. Our results show that TAPFed\noffers equivalent performance in terms of model quality compared to\nstate-of-the-art approaches while reducing transmission overhead by 29%-45%\nacross different model training scenarios. Most importantly, TAPFed can defend\nagainst recently demonstrated inference attacks caused by curious aggregators,\nwhich the majority of existing approaches are susceptible to.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "The paper has been published in IEEE TDSC",
    "pdf_url": "http://arxiv.org/pdf/2501.05053v1",
    "published_date": "2025-01-09 08:24:10 UTC",
    "updated_date": "2025-01-09 08:24:10 UTC"
  },
  {
    "arxiv_id": "2501.05032v1",
    "title": "Enhancing Human-Like Responses in Large Language Models",
    "authors": [
      "Ethem Yağız Çalık",
      "Talha Rüzgar Akkuş"
    ],
    "abstract": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05032v1",
    "published_date": "2025-01-09 07:44:06 UTC",
    "updated_date": "2025-01-09 07:44:06 UTC"
  },
  {
    "arxiv_id": "2501.05030v1",
    "title": "A General Retrieval-Augmented Generation Framework for Multimodal Case-Based Reasoning Applications",
    "authors": [
      "Ofir Marom"
    ],
    "abstract": "Case-based reasoning (CBR) is an experience-based approach to problem\nsolving, where a repository of solved cases is adapted to solve new cases.\nRecent research shows that Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages\nof the CBR pipeline by retrieving similar cases and using them as additional\ncontext to an LLM query. Most studies have focused on text-only applications,\nhowever, in many real-world problems the components of a case are multimodal.\nIn this paper we present MCBR-RAG, a general RAG framework for multimodal CBR\napplications. The MCBR-RAG framework converts non-text case components into\ntext-based representations, allowing it to: 1) learn application-specific\nlatent representations that can be indexed for retrieval, and 2) enrich the\nquery provided to the LLM by incorporating all case components for better\ncontext. We demonstrate MCBR-RAG's effectiveness through experiments conducted\non a simplified Math-24 application and a more complex Backgammon application.\nOur empirical results show that MCBR-RAG improves generation quality compared\nto a baseline LLM with no contextual information provided.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.05030v1",
    "published_date": "2025-01-09 07:41:22 UTC",
    "updated_date": "2025-01-09 07:41:22 UTC"
  },
  {
    "arxiv_id": "2501.05490v1",
    "title": "Interpretable deep learning illuminates multiple structures fluorescence imaging: a path toward trustworthy artificial intelligence in microscopy",
    "authors": [
      "Mingyang Chen",
      "Luhong Jin",
      "Xuwei Xuan",
      "Defu Yang",
      "Yun Cheng",
      "Ju Zhang"
    ],
    "abstract": "Live-cell imaging of multiple subcellular structures is essential for\nunderstanding subcellular dynamics. However, the conventional multi-color\nsequential fluorescence microscopy suffers from significant imaging delays and\nlimited number of subcellular structure separate labeling, resulting in\nsubstantial limitations for real-time live-cell research applications. Here, we\npresent the Adaptive Explainable Multi-Structure Network (AEMS-Net), a\ndeep-learning framework that enables simultaneous prediction of two subcellular\nstructures from a single image. The model normalizes staining intensity and\nprioritizes critical image features by integrating attention mechanisms and\nbrightness adaptation layers. Leveraging the Kolmogorov-Arnold representation\ntheorem, our model decomposes learned features into interpretable univariate\nfunctions, enhancing the explainability of complex subcellular morphologies. We\ndemonstrate that AEMS-Net allows real-time recording of interactions between\nmitochondria and microtubules, requiring only half the conventional\nsequential-channel imaging procedures. Notably, this approach achieves over 30%\nimprovement in imaging quality compared to traditional deep learning methods,\nestablishing a new paradigm for long-term, interpretable live-cell imaging that\nadvances the ability to explore subcellular dynamics.",
    "categories": [
      "q-bio.SC",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "q-bio.SC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05490v1",
    "published_date": "2025-01-09 07:36:28 UTC",
    "updated_date": "2025-01-09 07:36:28 UTC"
  },
  {
    "arxiv_id": "2501.05018v1",
    "title": "Finding Needles in Emb(a)dding Haystacks: Legal Document Retrieval via Bagging and SVR Ensembles",
    "authors": [
      "Kevin Bönisch",
      "Alexander Mehler"
    ],
    "abstract": "We introduce a retrieval approach leveraging Support Vector Regression (SVR)\nensembles, bootstrap aggregation (bagging), and embedding spaces on the German\nDataset for Legal Information Retrieval (GerDaLIR). By conceptualizing the\nretrieval task in terms of multiple binary needle-in-a-haystack subtasks, we\nshow improved recall over the baselines (0.849 > 0.803 | 0.829) using our\nvoting ensemble, suggesting promising initial results, without training or\nfine-tuning any deep learning models. Our approach holds potential for further\nenhancement, particularly through refining the encoding models and optimizing\nhyperparameters.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05018v1",
    "published_date": "2025-01-09 07:21:44 UTC",
    "updated_date": "2025-01-09 07:21:44 UTC"
  },
  {
    "arxiv_id": "2501.05015v1",
    "title": "On Measuring Unnoticeability of Graph Adversarial Attacks: Observations, New Measure, and Applications",
    "authors": [
      "Hyeonsoo Jo",
      "Hyunjin Hwang",
      "Fanchen Bu",
      "Soo Yong Lee",
      "Chanyoung Park",
      "Kijung Shin"
    ],
    "abstract": "Adversarial attacks are allegedly unnoticeable. Prior studies have designed\nattack noticeability measures on graphs, primarily using statistical tests to\ncompare the topology of original and (possibly) attacked graphs. However, we\nobserve two critical limitations in the existing measures. First, because the\nmeasures rely on simple rules, attackers can readily enhance their attacks to\nbypass them, reducing their attack \"noticeability\" and, yet, maintaining their\nattack performance. Second, because the measures naively leverage global\nstatistics, such as degree distributions, they may entirely overlook attacks\nuntil severe perturbations occur, letting the attacks be almost \"totally\nunnoticeable.\" To address the limitations, we introduce HideNSeek, a learnable\nmeasure for graph attack noticeability. First, to mitigate the bypass problem,\nHideNSeek learns to distinguish the original and (potential) attack edges using\na learnable edge scorer (LEO), which scores each edge on its likelihood of\nbeing an attack. Second, to mitigate the overlooking problem, HideNSeek\nconducts imbalance-aware aggregation of all the edge scores to obtain the final\nnoticeability score. Using six real-world graphs, we empirically demonstrate\nthat HideNSeek effectively alleviates the observed limitations, and LEO (i.e.,\nour learnable edge scorer) outperforms eleven competitors in distinguishing\nattack edges under five different attack methods. For an additional\napplication, we show that LEO boost the performance of robust GNNs by removing\nattack-like edges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05015v1",
    "published_date": "2025-01-09 07:16:21 UTC",
    "updated_date": "2025-01-09 07:16:21 UTC"
  },
  {
    "arxiv_id": "2501.05014v2",
    "title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation",
    "authors": [
      "Oleg Sautenkov",
      "Yasheerah Yaqoot",
      "Artem Lykov",
      "Muhammad Ahsan Mustafa",
      "Grik Tadevosyan",
      "Aibek Akhmetkazy",
      "Miguel Altamirano Cabrera",
      "Mikhail Martynov",
      "Sausar Karaf",
      "Dzmitry Tsetserukou"
    ],
    "abstract": "The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate\ncommunication with aerial robots. By integrating satellite imagery processing\nwith the Visual Language Model (VLM) and the powerful capabilities of GPT,\nUAV-VLA enables users to generate general flight paths-and-action plans through\nsimple text requests. This system leverages the rich contextual information\nprovided by satellite images, allowing for enhanced decision-making and mission\nplanning. The combination of visual analysis by VLM and natural language\nprocessing by GPT can provide the user with the path-and-action set, making\naerial operations more efficient and accessible. The newly developed method\nshowed the difference in the length of the created trajectory in 22% and the\nmean error in finding the objects of interest on a map in 34.22 m by Euclidean\ndistance in the K-Nearest Neighbors (KNN) approach.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "HRI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.05014v2",
    "published_date": "2025-01-09 07:15:59 UTC",
    "updated_date": "2025-05-13 06:54:45 UTC"
  },
  {
    "arxiv_id": "2501.05007v1",
    "title": "Quantum-enhanced causal discovery for a small number of samples",
    "authors": [
      "Yota Maeda",
      "Ken Arai",
      "Yu Tanaka",
      "Yu Terada",
      "Hiroshi Ueno",
      "Hiroyuki Tezuka"
    ],
    "abstract": "The discovery of causal relationships from observed data has attracted\nsignificant interest from disciplines such as economics, social sciences,\nepidemiology, and biology. In practical applications, considerable knowledge of\nthe underlying systems is often unavailable, and real data are often associated\nwith nonlinear causal structures, which make the direct use of most\nconventional causality analysis methods difficult. This study proposes a novel\nquantum Peter-Clark (qPC) algorithm for causal discovery that does not assume\nany underlying model structures. Based on the independence conditional tests in\na class of reproducing kernel Hilbert spaces characterized by quantum circuits,\nthe proposed qPC algorithm can explore causal relationships from the observed\ndata drawn from arbitrary distributions. We conducted systematic experiments on\nfundamental graph parts of causal structures, demonstrating that the qPC\nalgorithm exhibits a significantly better performance, particularly with\nsmaller sample sizes compared to its classical counterpart. Furthermore, we\nproposed a novel optimization approach based on Kernel Target Alignment (KTA)\nfor determining hyperparameters of quantum kernels. This method effectively\nreduced the risk of false positives in causal discovery, enabling more reliable\ninference. Our theoretical and experimental results demonstrate that the\nproposed quantum algorithm can empower classical algorithms for robust and\naccurate inference in causal discovery, supporting them in regimes where\nclassical algorithms typically fail. Additionally, the effectiveness of this\nmethod was validated using the Boston Housing dataset as a real-world\napplication. These findings demonstrate the new potential of quantum\ncircuit-based causal discovery methods in addressing practical challenges,\nparticularly in small-sample scenarios where traditional approaches have shown\nlimitations.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "quant-ph",
    "comment": "19 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.05007v1",
    "published_date": "2025-01-09 07:05:22 UTC",
    "updated_date": "2025-01-09 07:05:22 UTC"
  },
  {
    "arxiv_id": "2501.04997v1",
    "title": "GiNet: Integrating Sequential and Context-Aware Learning for Battery Capacity Prediction",
    "authors": [
      "Sara Sameer",
      "Wei Zhang",
      "Xin Lou",
      "Qingyu Yan",
      "Terence Goh",
      "Yulin Gao"
    ],
    "abstract": "The surging demand for batteries requires advanced battery management\nsystems, where battery capacity modelling is a key functionality. In this\npaper, we aim to achieve accurate battery capacity prediction by learning from\nhistorical measurements of battery dynamics. We propose GiNet, a gated\nrecurrent units enhanced Informer network, for predicting battery's capacity.\nThe novelty and competitiveness of GiNet lies in its capability of capturing\nsequential and contextual information from raw battery data and reflecting the\nbattery's complex behaviors with both temporal dynamics and long-term\ndependencies. We conducted an experimental study based on a publicly available\ndataset to showcase GiNet's strength of gaining a holistic understanding of\nbattery behavior and predicting battery capacity accurately. GiNet achieves\n0.11 mean absolute error for predicting the battery capacity in a sequence of\nfuture time slots without knowing the historical battery capacity. It also\noutperforms the latest algorithms significantly with 27% error reduction on\naverage compared to Informer. The promising results highlight the importance of\ncustomized and optimized integration of algorithm and battery knowledge and\nshed light on other industry applications as well.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.04997v1",
    "published_date": "2025-01-09 06:26:28 UTC",
    "updated_date": "2025-01-09 06:26:28 UTC"
  },
  {
    "arxiv_id": "2501.04995v1",
    "title": "IPDN: Image-enhanced Prompt Decoding Network for 3D Referring Expression Segmentation",
    "authors": [
      "Qi Chen",
      "Changli Wu",
      "Jiayi Ji",
      "Yiwei Ma",
      "Danni Yang",
      "Xiaoshuai Sun"
    ],
    "abstract": "3D Referring Expression Segmentation (3D-RES) aims to segment point cloud\nscenes based on a given expression. However, existing 3D-RES approaches face\ntwo major challenges: feature ambiguity and intent ambiguity. Feature ambiguity\narises from information loss or distortion during point cloud acquisition due\nto limitations such as lighting and viewpoint. Intent ambiguity refers to the\nmodel's equal treatment of all queries during the decoding process, lacking\ntop-down task-specific guidance. In this paper, we introduce an Image enhanced\nPrompt Decoding Network (IPDN), which leverages multi-view images and\ntask-driven information to enhance the model's reasoning capabilities. To\naddress feature ambiguity, we propose the Multi-view Semantic Embedding (MSE)\nmodule, which injects multi-view 2D image information into the 3D scene and\ncompensates for potential spatial information loss. To tackle intent ambiguity,\nwe designed a Prompt-Aware Decoder (PAD) that guides the decoding process by\nderiving task-driven signals from the interaction between the expression and\nvisual features. Comprehensive experiments demonstrate that IPDN outperforms\nthe state-ofthe-art by 1.9 and 4.2 points in mIoU metrics on the 3D-RES and\n3D-GRES tasks, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04995v1",
    "published_date": "2025-01-09 06:20:00 UTC",
    "updated_date": "2025-01-09 06:20:00 UTC"
  },
  {
    "arxiv_id": "2501.04982v1",
    "title": "CuRLA: Curriculum Learning Based Deep Reinforcement Learning for Autonomous Driving",
    "authors": [
      "Bhargava Uppuluri",
      "Anjel Patel",
      "Neil Mehta",
      "Sridhar Kamath",
      "Pratyush Chakraborty"
    ],
    "abstract": "In autonomous driving, traditional Computer Vision (CV) agents often struggle\nin unfamiliar situations due to biases in the training data. Deep Reinforcement\nLearning (DRL) agents address this by learning from experience and maximizing\nrewards, which helps them adapt to dynamic environments. However, ensuring\ntheir generalization remains challenging, especially with static training\nenvironments. Additionally, DRL models lack transparency, making it difficult\nto guarantee safety in all scenarios, particularly those not seen during\ntraining. To tackle these issues, we propose a method that combines DRL with\nCurriculum Learning for autonomous driving. Our approach uses a Proximal Policy\nOptimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe\ndriving in the CARLA simulator. The agent is trained using two-fold curriculum\nlearning, progressively increasing environment difficulty and incorporating a\ncollision penalty in the reward function to promote safety. This method\nimproves the agent's adaptability and reliability in complex environments, and\nunderstand the nuances of balancing multiple reward components from different\nfeedback signals in a single scalar reward function. Keywords: Computer Vision,\nDeep Reinforcement Learning, Variational Autoencoder, Proximal Policy\nOptimization, Curriculum Learning, Autonomous Driving.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "To be published in the 17th International Conference on Agents and\n  Artificial Intelligence (ICAART), Feb 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04982v1",
    "published_date": "2025-01-09 05:45:03 UTC",
    "updated_date": "2025-01-09 05:45:03 UTC"
  },
  {
    "arxiv_id": "2501.04974v3",
    "title": "SensorQA: A Question Answering Benchmark for Daily-Life Monitoring",
    "authors": [
      "Benjamin Reichman",
      "Xiaofan Yu",
      "Lanxiang Hu",
      "Jack Truxal",
      "Atishay Jain",
      "Rushil Chandrupatla",
      "Tajana Šimunić Rosing",
      "Larry Heck"
    ],
    "abstract": "With the rapid growth in sensor data, effectively interpreting and\ninterfacing with these data in a human-understandable way has become crucial.\nWhile existing research primarily focuses on learning classification models,\nfewer studies have explored how end users can actively extract useful insights\nfrom sensor data, often hindered by the lack of a proper dataset. To address\nthis gap, we introduce SensorQA, the first human-created question-answering\n(QA) dataset for long-term time-series sensor data for daily life monitoring.\nSensorQA is created by human workers and includes 5.6K diverse and practical\nqueries that reflect genuine human interests, paired with accurate answers\nderived from sensor data. We further establish benchmarks for state-of-the-art\nAI models on this dataset and evaluate their performance on typical edge\ndevices. Our results reveal a gap between current models and optimal QA\nperformance and efficiency, highlighting the need for new contributions. The\ndataset and code are available at:\nhttps://github.com/benjamin-reichman/SensorQA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04974v3",
    "published_date": "2025-01-09 05:06:44 UTC",
    "updated_date": "2025-03-03 17:03:49 UTC"
  },
  {
    "arxiv_id": "2501.04970v1",
    "title": "Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation",
    "authors": [
      "HyunGi Kim",
      "Siwon Kim",
      "Jisoo Mok",
      "Sungroh Yoon"
    ],
    "abstract": "Deep Neural Networks have spearheaded remarkable advancements in time series\nforecasting (TSF), one of the major tasks in time series modeling. Nonetheless,\nthe non-stationarity of time series undermines the reliability of pre-trained\nsource time series forecasters in mission-critical deployment settings. In this\nstudy, we introduce a pioneering test-time adaptation framework tailored for\nTSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source\nforecasters to continuously shifting test distributions while preserving the\ncore semantic information learned during pre-training. The novel utilization of\npartially-observed ground truth and gated calibration module enables proactive,\nrobust, and model-agnostic adaptation of source forecasters. Experiments on\ndiverse benchmark datasets and cutting-edge architectures demonstrate the\nefficacy and generality of TAFAS, especially in long-term forecasting scenarios\nthat suffer from significant distribution shifts. The code is available at\nhttps://github.com/kimanki/TAFAS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04970v1",
    "published_date": "2025-01-09 04:59:15 UTC",
    "updated_date": "2025-01-09 04:59:15 UTC"
  },
  {
    "arxiv_id": "2501.14790v1",
    "title": "Towards Dynamic Neural Communication and Speech Neuroprosthesis Based on Viseme Decoding",
    "authors": [
      "Ji-Ha Park",
      "Seo-Hyun Lee",
      "Soowon Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Decoding text, speech, or images from human neural signals holds promising\npotential both as neuroprosthesis for patients and as innovative communication\ntools for general users. Although neural signals contain various information on\nspeech intentions, movements, and phonetic details, generating informative\noutputs from them remains challenging, with mostly focusing on decoding short\nintentions or producing fragmented outputs. In this study, we developed a\ndiffusion model-based framework to decode visual speech intentions from\nspeech-related non-invasive brain signals, to facilitate face-to-face neural\ncommunication. We designed an experiment to consolidate various phonemes to\ntrain visemes of each phoneme, aiming to learn the representation of\ncorresponding lip formations from neural signals. By decoding visemes from both\nisolated trials and continuous sentences, we successfully reconstructed\ncoherent lip movements, effectively bridging the gap between brain signals and\ndynamic visual interfaces. The results highlight the potential of viseme\ndecoding and talking face reconstruction from human neural signals, marking a\nsignificant step toward dynamic neural communication systems and speech\nneuroprosthesis for patients.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "q-bio.NC",
    "comment": "5 pages, 5 figures, 1 table, Name of Conference: 2025 IEEE\n  International Conference on Acoustics, Speech, and Signal Processing",
    "pdf_url": "http://arxiv.org/pdf/2501.14790v1",
    "published_date": "2025-01-09 04:47:27 UTC",
    "updated_date": "2025-01-09 04:47:27 UTC"
  },
  {
    "arxiv_id": "2501.04961v2",
    "title": "Demystifying Domain-adaptive Post-training for Financial LLMs",
    "authors": [
      "Zixuan Ke",
      "Yifei Ming",
      "Xuan-Phi Nguyen",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "abstract": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain adaptive post-training of LLMs for the finance\ndomain. Our approach consists of four key components: FinCap, which defines the\ncore capabilities required for the target domain; FinRec, an effective training\nrecipe that jointly optimizes continual pre-training and instruction-following,\nalong with a novel preference data distillation method leveraging process\nsignals from a generative reward model; FinTrain, a curated set of training\ndatasets supporting FinRec; and FinEval, a comprehensive evaluation suite\naligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art\nperformance across a wide range of financial tasks. Our analysis also\nhighlights how each post-training stage contributes to distinct capabilities,\nuncovering specific challenges and effective solutions, providing valuable\ninsights for domain adaptation of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04961v2",
    "published_date": "2025-01-09 04:26:15 UTC",
    "updated_date": "2025-02-12 04:52:08 UTC"
  },
  {
    "arxiv_id": "2501.04958v1",
    "title": "Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment",
    "authors": [
      "Lei Li",
      "Xinglin Zhang",
      "Jun Liang",
      "Tao Chen"
    ],
    "abstract": "Deep learning models in medical imaging face dual challenges: domain shift,\nwhere models perform poorly when deployed in settings different from their\ntraining environment, and class imbalance, where certain disease conditions are\nnaturally underrepresented. We present Imbalance-Aware Domain Adaptation\n(IADA), a novel framework that simultaneously tackles both challenges through\nthree key components: (1) adaptive feature learning with class-specific\nattention mechanisms, (2) balanced domain alignment with dynamic weighting, and\n(3) adaptive threshold optimization. Our theoretical analysis establishes\nconvergence guarantees and complexity bounds. Through extensive experiments on\nembryo development assessment across four imaging modalities, IADA demonstrates\nsignificant improvements over existing methods, achieving up to 25.19\\% higher\naccuracy while maintaining balanced performance across classes. In challenging\nscenarios with low-quality imaging systems, IADA shows robust generalization\nwith AUC improvements of up to 12.56\\%. These results demonstrate IADA's\npotential for developing reliable and equitable medical imaging systems for\ndiverse clinical settings. The code is made public available at\n\\url{https://github.com/yinghemedical/imbalance-aware_domain_adaptation}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.04958v1",
    "published_date": "2025-01-09 04:20:12 UTC",
    "updated_date": "2025-01-09 04:20:12 UTC"
  },
  {
    "arxiv_id": "2501.04952v1",
    "title": "Open Problems in Machine Unlearning for AI Safety",
    "authors": [
      "Fazl Barez",
      "Tingchen Fu",
      "Ameya Prabhu",
      "Stephen Casper",
      "Amartya Sanyal",
      "Adel Bibi",
      "Aidan O'Gara",
      "Robert Kirk",
      "Ben Bucknall",
      "Tim Fist",
      "Luke Ong",
      "Philip Torr",
      "Kwok-Yan Lam",
      "Robert Trager",
      "David Krueger",
      "Sören Mindermann",
      "José Hernandez-Orallo",
      "Mor Geva",
      "Yarin Gal"
    ],
    "abstract": "As AI systems become more capable, widely deployed, and increasingly\nautonomous in critical areas such as cybersecurity, biological research, and\nhealthcare, ensuring their safety and alignment with human values is paramount.\nMachine unlearning -- the ability to selectively forget or suppress specific\ntypes of knowledge -- has shown promise for privacy and data removal tasks,\nwhich has been the primary focus of existing research. More recently, its\npotential application to AI safety has gained attention. In this paper, we\nidentify key limitations that prevent unlearning from serving as a\ncomprehensive solution for AI safety, particularly in managing dual-use\nknowledge in sensitive domains like cybersecurity and chemical, biological,\nradiological, and nuclear (CBRN) safety. In these contexts, information can be\nboth beneficial and harmful, and models may combine seemingly harmless\ninformation for harmful purposes -- unlearning this information could strongly\naffect beneficial uses. We provide an overview of inherent constraints and open\nproblems, including the broader side effects of unlearning dangerous knowledge,\nas well as previously unexplored tensions between unlearning and existing\nsafety mechanisms. Finally, we investigate challenges related to evaluation,\nrobustness, and the preservation of safety features during unlearning. By\nmapping these limitations and open challenges, we aim to guide future research\ntoward realistic applications of unlearning within a broader AI safety\nframework, acknowledging its limitations and highlighting areas where\nalternative approaches may be required.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04952v1",
    "published_date": "2025-01-09 03:59:10 UTC",
    "updated_date": "2025-01-09 03:59:10 UTC"
  },
  {
    "arxiv_id": "2501.06255v1",
    "title": "Progressive Supervision via Label Decomposition: An Long-Term and Large-Scale Wireless Traffic Forecasting Method",
    "authors": [
      "Daojun Liang",
      "Haixia Zhang",
      "Dongfeng Yuan"
    ],
    "abstract": "Long-term and Large-scale Wireless Traffic Forecasting (LL-WTF) is pivotal\nfor strategic network management and comprehensive planning on a macro scale.\nHowever, LL-WTF poses greater challenges than short-term ones due to the\npronounced non-stationarity of extended wireless traffic and the vast number of\nnodes distributed at the city scale. To cope with this, we propose a\nProgressive Supervision method based on Label Decomposition (PSLD).\nSpecifically, we first introduce a Random Subgraph Sampling (RSS) algorithm\ndesigned to sample a tractable subset from large-scale traffic data, thereby\nenabling efficient network training. Then, PSLD employs label decomposition to\nobtain multiple easy-to-learn components, which are learned progressively at\nshallow layers and combined at deep layers to effectively cope with the\nnon-stationary problem raised by LL-WTF tasks. Finally, we compare the proposed\nmethod with various state-of-the-art (SOTA) methods on three large-scale WT\ndatasets. Extensive experimental results demonstrate that the proposed PSLD\nsignificantly outperforms existing methods, with an average 2%, 4%, and 11%\nperformance improvement on three WT datasets, respectively. In addition, we\nbuilt an open source library for WT forecasting (WTFlib) to facilitate related\nresearch, which contains numerous SOTA methods and provides a strong\nbenchmark.Experiments can be reproduced through\nhttps://github.com/Anoise/WTFlib.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at Knowledge-Based Systems. arXiv admin note: substantial\n  text overlap with arXiv:2412.00108",
    "pdf_url": "http://arxiv.org/pdf/2501.06255v1",
    "published_date": "2025-01-09 03:35:00 UTC",
    "updated_date": "2025-01-09 03:35:00 UTC"
  },
  {
    "arxiv_id": "2501.04945v3",
    "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models",
    "authors": [
      "Qingyu Ren",
      "Jie Zeng",
      "Qianyu He",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Weikang Zhou",
      "Zeye Sun",
      "Fei Yu"
    ],
    "abstract": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, it is an unexplored area to enhance\nLLMs' ability to follow soft constraints. To bridge the gap, we initially\ndesign a pipeline to construct datasets with high-quality outputs\nautomatically. Additionally, to fully utilize the positive and negative samples\ngenerated during the data construction process, we choose Direct Preference\nOptimization (DPO) as the training method. Furthermore, taking into account the\ndifficulty of soft constraints indicated by the number of constraints, we\ndesign a curriculum learning training paradigm based on the constraint\nquantity. We experimentally evaluate the effectiveness of our methods in\nimproving LLMs' soft constraint following ability and analyze the factors\ndriving the improvements.The datasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraint.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04945v3",
    "published_date": "2025-01-09 03:34:07 UTC",
    "updated_date": "2025-02-16 23:36:29 UTC"
  },
  {
    "arxiv_id": "2501.06254v2",
    "title": "Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words",
    "authors": [
      "Gouki Minegishi",
      "Hiroki Furuta",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2501.06254v2",
    "published_date": "2025-01-09 02:54:19 UTC",
    "updated_date": "2025-02-18 17:10:39 UTC"
  },
  {
    "arxiv_id": "2501.04931v1",
    "title": "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency",
    "authors": [
      "Shiji Zhao",
      "Ranjie Duan",
      "Fengxiang Wang",
      "Chi Chen",
      "Caixin Kang",
      "Jialing Tao",
      "YueFeng Chen",
      "Hui Xue",
      "Xingxing Wei"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive performance\nand have been put into practical use in commercial applications, but they still\nhave potential safety mechanism vulnerabilities. Jailbreak attacks are red\nteaming methods that aim to bypass safety mechanisms and discover MLLMs'\npotential risks. Existing MLLMs' jailbreak methods often bypass the model's\nsafety mechanism through complex optimization methods or carefully designed\nimage and text prompts. Despite achieving some progress, they have a low attack\nsuccess rate on commercial closed-source MLLMs. Unlike previous research, we\nempirically find that there exists a Shuffle Inconsistency between MLLMs'\ncomprehension ability and safety ability for the shuffled harmful instruction.\nThat is, from the perspective of comprehension ability, MLLMs can understand\nthe shuffled harmful text-image instructions well. However, they can be easily\nbypassed by the shuffled harmful instructions from the perspective of safety\nability, leading to harmful responses. Then we innovatively propose a\ntext-image jailbreak attack named SI-Attack. Specifically, to fully utilize the\nShuffle Inconsistency and overcome the shuffle randomness, we apply a\nquery-based black-box optimization method to select the most harmful shuffled\ninputs based on the feedback of the toxic judge model. A series of experiments\nshow that SI-Attack can improve the attack's performance on three benchmarks.\nIn particular, SI-Attack can obviously improve the attack success rate for\ncommercial MLLMs such as GPT-4o or Claude-3.5-Sonnet.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04931v1",
    "published_date": "2025-01-09 02:47:01 UTC",
    "updated_date": "2025-01-09 02:47:01 UTC"
  },
  {
    "arxiv_id": "2501.04928v1",
    "title": "Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference from Product Images",
    "authors": [
      "Xingang Li",
      "Zhenghui Sha"
    ],
    "abstract": "Computer-aided design (CAD) tools empower designers to design and modify 3D\nmodels through a series of CAD operations, commonly referred to as a CAD\nsequence. In scenarios where digital CAD files are not accessible, reverse\nengineering (RE) has been used to reconstruct 3D CAD models. Recent advances\nhave seen the rise of data-driven approaches for RE, with a primary focus on\nconverting 3D data, such as point clouds, into 3D models in boundary\nrepresentation (B-rep) format. However, obtaining 3D data poses significant\nchallenges, and B-rep models do not reveal knowledge about the 3D modeling\nprocess of designs. To this end, our research introduces a novel data-driven\napproach with an Image2CADSeq neural network model. This model aims to reverse\nengineer CAD models by processing images as input and generating CAD sequences.\nThese sequences can then be translated into B-rep models using a solid modeling\nkernel. Unlike B-rep models, CAD sequences offer enhanced flexibility to modify\nindividual steps of model creation, providing a deeper understanding of the\nconstruction process of CAD models. To quantitatively and rigorously evaluate\nthe predictive performance of the Image2CADSeq model, we have developed a\nmulti-level evaluation framework for model assessment. The model was trained on\na specially synthesized dataset, and various network architectures were\nexplored to optimize the performance. The experimental and validation results\nshow great potential for the model in generating CAD sequences from 2D image\ndata.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 10 figures, and 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.04928v1",
    "published_date": "2025-01-09 02:36:21 UTC",
    "updated_date": "2025-01-09 02:36:21 UTC"
  },
  {
    "arxiv_id": "2501.04926v1",
    "title": "FLowHigh: Towards Efficient and High-Quality Audio Super-Resolution with Single-Step Flow Matching",
    "authors": [
      "Jun-Hak Yun",
      "Seung-Bin Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Audio super-resolution is challenging owing to its ill-posed nature.\nRecently, the application of diffusion models in audio super-resolution has\nshown promising results in alleviating this challenge. However, diffusion-based\nmodels have limitations, primarily the necessity for numerous sampling steps,\nwhich causes significantly increased latency when synthesizing high-quality\naudio samples. In this paper, we propose FLowHigh, a novel approach that\nintegrates flow matching, a highly efficient generative model, into audio\nsuper-resolution. We also explore probability paths specially tailored for\naudio super-resolution, which effectively capture high-resolution audio\ndistributions, thereby enhancing reconstruction quality. The proposed method\ngenerates high-fidelity, high-resolution audio through a single-step sampling\nprocess across various input sampling rates. The experimental results on the\nVCTK benchmark dataset demonstrate that FLowHigh achieves state-of-the-art\nperformance in audio super-resolution, as evaluated by log-spectral distance\nand ViSQOL while maintaining computational efficiency with only a single-step\nsampling process.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04926v1",
    "published_date": "2025-01-09 02:30:26 UTC",
    "updated_date": "2025-01-09 02:30:26 UTC"
  },
  {
    "arxiv_id": "2502.15702v1",
    "title": "Large language models streamline automated systematic review: A preliminary study",
    "authors": [
      "Xi Chen",
      "Xue Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise in natural language\nprocessing tasks, with the potential to automate systematic reviews. This study\nevaluates the performance of three state-of-the-art LLMs in conducting\nsystematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across\nfour systematic review tasks: study design formulation, search strategy\ndevelopment, literature screening, and data extraction. Sourced from a\npreviously published systematic review, we provided reference standard\nincluding standard PICO (Population, Intervention, Comparison, Outcome) design,\nstandard eligibility criteria, and data from 20 reference literature. Three\ninvestigators evaluated the quality of study design and eligibility criteria\nusing 5-point Liker Scale in terms of accuracy, integrity, relevance,\nconsistency and overall performance. For other tasks, the output is defined as\naccurate if it is the same as the reference standard. Search strategy\nperformance was evaluated through accuracy and retrieval efficacy. Screening\naccuracy was assessed for both abstracts screening and full texts screening.\nData extraction accuracy was evaluated across 1,120 data points comprising\n3,360 individual fields. Claude-3 demonstrated superior overall performance in\nPICO design. In search strategy formulation, GPT-4 and Claude-3 achieved\ncomparable accuracy, outperforming Mistral. For abstract screening, GPT-4\nachieved the highest accuracy, followed by Mistral and Claude-3. In data\nextraction, GPT-4 significantly outperformed other models. LLMs demonstrate\npotential for automating systematic review tasks, with GPT-4 showing superior\nperformance in search strategy formulation, literature screening and data\nextraction. These capabilities make them promising assistive tools for\nresearchers and warrant further development and validation in this field.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "25 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15702v1",
    "published_date": "2025-01-09 01:59:35 UTC",
    "updated_date": "2025-01-09 01:59:35 UTC"
  },
  {
    "arxiv_id": "2501.06253v1",
    "title": "The State of Post-Hoc Local XAI Techniques for Image Processing: Challenges and Motivations",
    "authors": [
      "Rech Leong Tian Poh",
      "Sye Loong Keoh",
      "Liying Li"
    ],
    "abstract": "As complex AI systems further prove to be an integral part of our lives, a\npersistent and critical problem is the underlying black-box nature of such\nproducts and systems. In pursuit of productivity enhancements, one must not\nforget the need for various technology to boost the overall trustworthiness of\nsuch AI systems. One example, which is studied extensively in this work, is the\ndomain of Explainable Artificial Intelligence (XAI). Research works in this\nscope are centred around the objective of making AI systems more transparent\nand interpretable, to further boost reliability and trust in using them. In\nthis work, we discuss the various motivation for XAI and its approaches, the\nunderlying challenges that XAI faces, and some open problems that we believe\ndeserve further efforts to look into. We also provide a brief discussion of\nvarious XAI approaches for image processing, and finally discuss some future\ndirections, to hopefully express and motivate the positive development of the\nXAI research space.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06253v1",
    "published_date": "2025-01-09 01:45:34 UTC",
    "updated_date": "2025-01-09 01:45:34 UTC"
  },
  {
    "arxiv_id": "2501.04899v1",
    "title": "SUGAR: Leveraging Contextual Confidence for Smarter Retrieval",
    "authors": [
      "Hanna Zubkova",
      "Ji-Hoon Park",
      "Seong-Whan Lee"
    ],
    "abstract": "Bearing in mind the limited parametric knowledge of Large Language Models\n(LLMs), retrieval-augmented generation (RAG) which supplies them with the\nrelevant external knowledge has served as an approach to mitigate the issue of\nhallucinations to a certain extent. However, uniformly retrieving supporting\ncontext makes response generation source-inefficient, as triggering the\nretriever is not always necessary, or even inaccurate, when a model gets\ndistracted by noisy retrieved content and produces an unhelpful answer.\nMotivated by these issues, we introduce Semantic Uncertainty Guided Adaptive\nRetrieval (SUGAR), where we leverage context-based entropy to actively decide\nwhether to retrieve and to further determine between single-step and multi-step\nretrieval. Our empirical results show that selective retrieval guided by\nsemantic uncertainty estimation improves the performance across diverse\nquestion answering tasks, as well as achieves a more efficient inference.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICASSP2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04899v1",
    "published_date": "2025-01-09 01:24:59 UTC",
    "updated_date": "2025-01-09 01:24:59 UTC"
  },
  {
    "arxiv_id": "2501.06252v3",
    "title": "Transformer-Squared: Self-adaptive LLMs",
    "authors": [
      "Qi Sun",
      "Edoardo Cetin",
      "Yujin Tang"
    ],
    "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\nTransformer-Squared, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, Transformer-Squared employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific 'expert' vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod consistently outperforms ubiquitous approaches such as LoRA, with fewer\nparameters and greater efficiency. Furthermore, Transformer-Squared\ndemonstrates versatility across different LLM architectures and modalities,\nincluding vision-language tasks. Transformer-Squared represents a significant\nleap forward, offering a scalable, efficient solution for enhancing the\nadaptability and task-specific performance of LLMs, paving the way for truly\ndynamic, self-organizing AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.06252v3",
    "published_date": "2025-01-09 01:19:21 UTC",
    "updated_date": "2025-01-24 01:26:30 UTC"
  },
  {
    "arxiv_id": "2501.04896v1",
    "title": "Quantifying Itch and its Impact on Sleep Using Machine Learning and Radio Signals",
    "authors": [
      "Michail Ouroutzoglou",
      "Mingmin Zhao",
      "Joshua Hellerstein",
      "Hariharan Rahul",
      "Asima Badic",
      "Brian S. Kim",
      "Dina Katabi"
    ],
    "abstract": "Chronic itch affects 13% of the US population, is highly debilitating, and\nunderlies many medical conditions. A major challenge in clinical care and new\ntherapeutics development is the lack of an objective measure for quantifying\nitch, leading to reliance on subjective measures like patients' self-assessment\nof itch severity. In this paper, we show that a home radio device paired with\nartificial intelligence (AI) can concurrently capture scratching and evaluate\nits impact on sleep quality by analyzing radio signals bouncing in the\nenvironment. The device eliminates the need for wearable sensors or skin\ncontact, enabling monitoring of chronic itch over extended periods at home\nwithout burdening patients or interfering with their skin condition. To\nvalidate the technology, we conducted an observational clinical study of\nchronic pruritus patients, monitored at home for one month using both the radio\ndevice and an infrared camera. Comparing the output of the device to ground\ntruth data from the camera demonstrates its feasibility and accuracy (ROC AUC =\n0.997, sensitivity = 0.825, specificity = 0.997). The results reveal a\nsignificant correlation between scratching and low sleep quality, manifested as\na reduction in sleep efficiency (R = 0.6, p < 0.001) and an increase in sleep\nlatency (R = 0.68, p < 0.001). Our study underscores the potential of passive,\nlong-term, at-home monitoring of chronic scratching and its sleep implications,\noffering a valuable tool for both clinical care of chronic itch patients and\npharmaceutical clinical trials.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04896v1",
    "published_date": "2025-01-09 00:50:44 UTC",
    "updated_date": "2025-01-09 00:50:44 UTC"
  }
]
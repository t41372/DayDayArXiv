{
  "date": "2025-02-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-02-21 的 arXiv 中文 TLDR 快报！\n\n今天的 arXiv 论文主要聚焦 AI 模型优化、安全挑战和实际应用，包括 LLM（大型语言模型）的推理能力提升、AI 治理框架（如 AGILE Index）和生成模型在医疗、金融领域的创新。其中，Yoshua Bengio 等著名学者的论文令人印象深刻，如探讨 AI 风险的“Superintelligent Agents Pose Catastrophic Risks”，强调了 AI 安全的重要性；同时，LLM 在多模态任务和科学发现中的进展（如“Auto-Bench”）也备受关注。\n\n以下是今日论文的精选摘要，我优先选取了重要、创新性和话题度高的文章（如 AI 安全、LLM 优化和多模态模型），并快速掠过较基础或特定领域的论文。每个条目列出论文标题（中文 + 英文），并简要描述主要贡献和发现，保留核心学术术语。\n\n### 重点论文讨论\n- **Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?（超级智能代理的灾难性风险：科学家 AI 能否提供更安全的路径？）**  \n  Yoshua Bengio 等作者警告 AI 代理（如自治 LLM）可能带来的风险（如欺骗行为），提出非代理式 Scientist AI 框架，使用不确定性建模提升 AI 安全。主要发现：AI 代理训练可能导致失控，建议转向非代理设计以平衡创新和风险。\n\n- **Protein Large Language Models: A Comprehensive Survey（蛋白质大型语言模型：全面综述）**  \n  作者包括 Yizhou Sun 和 Bing Zhang 的团队，系统综述了蛋白质 LLM 的架构、数据集和应用。主要贡献：分析了这些模型在蛋白结构预测和功能注释中的潜力，发现它们能驱动生物医学研究，但需解决数据隐私问题。\n\n- **Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs（直达零：为什么线性衰减学习率至零最适合 LLM）**  \n  该论文解释线性衰减学习率在 LLM 训练中的优势，主要发现：与传统余弦衰减相比，它可节省高达 60% 计算资源，同时保持性能，AdamW 优化器解释了其在梯度噪声中的平衡作用。\n\n- **Position: Beyond Assistance -- Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care（观点：超越辅助 -- 重新构想 LLM 在心理健康护理中的伦理和适应性协创者角色）**  \n  作者提出 LLM 应作为心理健康领域的“协创者”而非工具，引入 SAFE-i 指南和 HAAS-e 框架。主要发现：LLM 可提升个性化干预，但需解决偏见和过度依赖问题。\n\n- **R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression（R$^3$Mem：通过可逆压缩桥接记忆保留和检索）**  \n  该工作优化 LLM 的记忆机制，使用可逆架构压缩长历史数据。主要贡献：提升检索增强生成任务的性能，适用于对话代理。\n\n- **SQLsynth: Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data Annotation（SQLsynth：通过人类-LLM 协作数据标注的 Text-to-SQL 领域适应）**  \n  作者开发了人类-LLM 协作系统加速 Text-to-SQL 数据标注，主要发现：显著减少认知负担，并提升模型在新领域的适应性，已被 IUI'25 接受。\n\n- **AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind（AutoToM：用于开放式理论-of-mind 的自动化贝叶斯逆向规划和模型发现）**  \n  该论文提出 AutoToM 框架，让 LLM 代理进行理论-of-mind 推理。主要贡献：模型能自动迭代改进，支持多领域应用。\n\n- **Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models（Minions：设备端和云端语言模型的成本高效协作）**  \n  探索本地和云端 LLM 协作，减少云端计算成本。主要发现：新协议可将成本降低 5.7 倍，同时保持性能。\n\n- **TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding（TETRIS：批量推测解码的最佳草稿令牌选择）**  \n  优化 LLM 推测解码，动态选择高效令牌。主要贡献：提升批量推理效率，节省资源。\n\n- **Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification（Mantis：轻量级校准基础模型，用于用户友好的时间序列分类）**  \n  提出轻量级时间序列模型，使用对比学习预训练。主要发现：在医疗等任务中超越 SOTA 模型。\n\n其他论文，如那些专注于特定领域（如水资源优化或交通标志识别），虽有技术贡献但话题度较低，我仅快速提及：例如，“Multi-Objective Optimization of Water Resource Allocation（水资源分配的多目标优化）”改进了流域管理算法；“Graph Attention Convolutional U-NET（图注意力卷积 U-Net）”提升了洪水区域分割精度。这些工作虽实用，但未涉及核心 AI 创新，故从简。\n\n总之，今天的论文突显了 AI 模型的安全性和高效性，Yoshua Bengio 的工作尤其值得关注，未来可能推动更负责任的 AI 发展。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2502.18505v1",
      "title": "Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ranjan Sapkota",
        "Shaina Raza",
        "Manoj Karkee"
      ],
      "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI),\nexisting research lacks a discussion on the transparency and accessibility of\nstate-of-the-art (SoTA) Large Language Models (LLMs). The Open Source\nInitiative (OSI) has recently released its first formal definition of\nopen-source software. This definition, when combined with standard dictionary\ndefinitions and the sparse published literature, provide an initial framework\nto support broader accessibility to AI models such as LLMs, but more work is\nessential to capture the unique dynamics of openness in AI. In addition,\nconcerns about open-washing, where models claim openness but lack full\ntransparency, has been raised, which limits the reproducibility, bias\nmitigation, and domain adaptation of these models. In this context, our study\ncritically analyzes SoTA LLMs from the last five years, including ChatGPT,\nDeepSeek, LLaMA, and others, to assess their adherence to transparency\nstandards and the implications of partial openness. Specifically, we examine\ntransparency and accessibility from two perspectives: open-source vs.\nopen-weight models. Our findings reveal that while some models are labeled as\nopen-source, this does not necessarily mean they are fully open-sourced. Even\nin the best cases, open-source models often do not report model training data,\nand code as well as key metrics, such as weight accessibility, and carbon\nemissions. To the best of our knowledge, this is the first study that\nsystematically examines the transparency and accessibility of over 100\ndifferent SoTA LLMs through the dual lens of open-source and open-weight\nmodels. The findings open avenues for further research and call for responsible\nand sustainable AI practices to ensure greater transparency, accountability,\nand ethical deployment of these models.(DeepSeek transparency, ChatGPT\naccessibility, open source, DeepSeek open source)",
      "tldr_zh": "本文对 ChatGPT、DeepSeek 和其他 SoTA LLMs 的透明度和可访问性进行了全面分析，评估了过去五年超过 100 个模型从开源和开放权重模型两个角度的遵守情况。研究发现，尽管许多模型声称开源，但它们往往未报告关键信息，如训练数据、代码、权重可访问性及碳排放，导致 open-washing 问题影响可复现性、偏见缓解和领域适应。作者强调，这是首个系统性考察此领域的论文，并呼吁更负责任的 AI 实践，以提升模型的透明性、问责性和道德部署。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.18505v1",
      "published_date": "2025-02-21 23:53:13 UTC",
      "updated_date": "2025-02-21 23:53:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:47:16.028183"
    },
    {
      "arxiv_id": "2502.16003v1",
      "title": "Hierarchical Residuals Exploit Brain-Inspired Compositionality",
      "title_zh": "翻译失败",
      "authors": [
        "Francisco M. López",
        "Jochen Triesch"
      ],
      "abstract": "We present Hierarchical Residual Networks (HiResNets), deep convolutional\nneural networks with long-range residual connections between layers at\ndifferent hierarchical levels. HiResNets draw inspiration on the organization\nof the mammalian brain by replicating the direct connections from subcortical\nareas to the entire cortical hierarchy. We show that the inclusion of\nhierarchical residuals in several architectures, including ResNets, results in\na boost in accuracy and faster learning. A detailed analysis of our models\nreveals that they perform hierarchical compositionality by learning feature\nmaps relative to the compressed representations provided by the skip\nconnections.",
      "tldr_zh": "本研究提出 Hierarchical Residual Networks (HiResNets)，一种受哺乳动物大脑组织启发的深度卷积神经网络，引入不同层次间的长程残差连接，以模仿大脑从皮层下区域到整个皮层层次的直接连接。相比传统架构如 ResNets，该模型通过添加层次残差显著提升了准确率并加速了学习过程。详细分析表明，HiResNets 通过跳跃连接提供的压缩表示，实现了层次化的特征映射组合，从而增强了网络的层次组合性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ESANN 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.16003v1",
      "published_date": "2025-02-21 23:35:08 UTC",
      "updated_date": "2025-02-21 23:35:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:47:27.684431"
    },
    {
      "arxiv_id": "2502.15996v2",
      "title": "Med-gte-hybrid: A contextual embedding transformer model for extracting actionable information from clinical texts",
      "title_zh": "Med-gte-hybrid：一种用于从临床文本中提取可操作信息的上下文嵌入变换器模型",
      "authors": [
        "Aditya Kumar",
        "Simon Rauch",
        "Mario Cypko",
        "Oliver Amft"
      ],
      "abstract": "We introduce a novel contextual embedding model med-gte-hybrid that was\nderived from the gte-large sentence transformer to extract information from\nunstructured clinical narratives. Our model tuning strategy for med-gte-hybrid\ncombines contrastive learning and a denoising autoencoder. To evaluate the\nperformance of med-gte-hybrid, we investigate several clinical prediction tasks\nin large patient cohorts extracted from the MIMIC-IV dataset, including Chronic\nKidney Disease (CKD) patient prognosis, estimated glomerular filtration rate\n(eGFR) prediction, and patient mortality prediction. Furthermore, we\ndemonstrate that the med-gte-hybrid model improves patient stratification,\nclustering, and text retrieval, thus outperforms current state-of-the-art\nmodels on the Massive Text Embedding Benchmark (MTEB). While some of our\nevaluations focus on CKD, our hybrid tuning of sentence transformers could be\ntransferred to other medical domains and has the potential to improve clinical\ndecision-making and personalised treatment pathways in various healthcare\napplications.",
      "tldr_zh": "本研究引入了 med-gte-hybrid，一种基于 gte-large sentence transformer 的上下文嵌入模型，用于从非结构化临床文本中提取可操作信息。模型的调优策略结合了 contrastive learning 和 denoising autoencoder，以提升信息提取的准确性。在 MIMIC-IV 数据集上评估时，该模型在 Chronic Kidney Disease (CKD) 患者预后、eGFR 预测和患者死亡率预测等任务中表现出色，并在 Massive Text Embedding Benchmark (MTEB) 上优于现有最先进模型，提高了患者分层、聚类和文本检索性能。这种混合调优方法可扩展到其他医疗领域，有助于提升临床决策和个性化治疗路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15996v2",
      "published_date": "2025-02-21 23:17:31 UTC",
      "updated_date": "2025-03-12 16:17:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:47:39.303957"
    },
    {
      "arxiv_id": "2502.15990v1",
      "title": "Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search",
      "title_zh": "翻译失败",
      "authors": [
        "Jayant Sachdev",
        "Sean D Rosario",
        "Abhijeet Phatak",
        "He Wen",
        "Swati Kirti",
        "Chittaranjan Tripathy"
      ],
      "abstract": "Accurate query-product relevance labeling is indispensable to generate ground\ntruth dataset for search ranking in e-commerce. Traditional approaches for\nannotating query-product pairs rely on human-based labeling services, which is\nexpensive, time-consuming and prone to errors. In this work, we explore the\napplication of Large Language Models (LLMs) to automate query-product relevance\nlabeling for large-scale e-commerce search. We use several publicly available\nand proprietary LLMs for this task, and conducted experiments on two\nopen-source datasets and an in-house e-commerce search dataset. Using prompt\nengineering techniques such as Chain-of-Thought (CoT) prompting, In-context\nLearning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal\nRelevance (MMR), we show that LLM's performance has the potential to approach\nhuman-level accuracy on this task in a fraction of the time and cost required\nby human-labelers, thereby suggesting that our approach is more efficient than\nthe conventional methods. We have generated query-product relevance labels\nusing LLMs at scale, and are using them for evaluating improvements to our\nsearch algorithms. Our work demonstrates the potential of LLMs to improve\nquery-product relevance thus enhancing e-commerce search user experience. More\nimportantly, this scalable alternative to human-annotation has significant\nimplications for information retrieval domains including search and\nrecommendation systems, where relevance scoring is crucial for optimizing the\nranking of products and content to improve customer engagement and other\nconversion metrics.",
      "tldr_zh": "本文提出一种使用 Large Language Models (LLMs) 自动化的方法，来处理电商搜索中的查询-产品相关性标注问题，以克服传统人工标注的成本高、耗时长和易出错缺点。研究团队在两个开源数据集和一个内部数据集上进行实验，采用 Chain-of-Thought (CoT) 提示、In-context Learning (ICL) 和 Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR) 等提示工程技巧。结果表明，LLMs 的性能接近人类水平，但效率更高，已成功大规模生成标签用于搜索算法优化。该方法不仅提升了查询-产品相关性及其对电商用户体验的改善，还为信息检索领域如搜索和推荐系统提供了可扩展的替代方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15990v1",
      "published_date": "2025-02-21 22:59:36 UTC",
      "updated_date": "2025-02-21 22:59:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:47:54.384377"
    },
    {
      "arxiv_id": "2502.15987v2",
      "title": "Forecasting Open-Weight AI Model Growth on HuggingFace",
      "title_zh": "翻译失败",
      "authors": [
        "Kushal Raj Bhandari",
        "Pin-Yu Chen",
        "Jianxi Gao"
      ],
      "abstract": "As the open-weight AI landscape continues to proliferate-with model\ndevelopment, significant investment, and user interest-it becomes increasingly\nimportant to predict which models will ultimately drive innovation and shape AI\necosystems. Building on parallels with citation dynamics in scientific\nliterature, we propose a framework to quantify how an open-weight model's\ninfluence evolves. Specifically, we adapt the model introduced by Wang et al.\nfor scientific citations, using three key parameters-immediacy, longevity, and\nrelative fitness-to track the cumulative number of fine-tuned models of an\nopen-weight model. Our findings reveal that this citation-style approach can\neffectively capture the diverse trajectories of open-weight model adoption,\nwith most models fitting well and outliers indicating unique patterns or abrupt\njumps in usage.",
      "tldr_zh": "这篇论文提出一个框架，用于预测HuggingFace平台上开源AI模型（open-weight AI model）的增长，借鉴科学文献的引用动态。作者改编了Wang et al.的模型，通过immediacy、longevity和relative fitness三个关键参数来量化模型的影响演变，并跟踪其累计微调模型数量。研究发现，这种方法能有效捕捉模型采用的多样轨迹，大多数模型符合预期模式，而异常情况揭示了独特模式或使用突变。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "Link to the website for trajectory visualization:\n  https://forecasthuggingfacemodels.onrender.com/",
      "pdf_url": "http://arxiv.org/pdf/2502.15987v2",
      "published_date": "2025-02-21 22:52:19 UTC",
      "updated_date": "2025-03-15 21:08:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:48:04.168048"
    },
    {
      "arxiv_id": "2502.15980v1",
      "title": "Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data Annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Tian",
        "Daniel Lee",
        "Fei Wu",
        "Tung Mai",
        "Kun Qian",
        "Siddhartha Sahai",
        "Tianyi Zhang",
        "Yunyao Li"
      ],
      "abstract": "Text-to-SQL models, which parse natural language (NL) questions to executable\nSQL queries, are increasingly adopted in real-world applications. However,\ndeploying such models in the real world often requires adapting them to the\nhighly specialized database schemas used in specific applications. We find that\nexisting text-to-SQL models experience significant performance drops when\napplied to new schemas, primarily due to the lack of domain-specific data for\nfine-tuning. This data scarcity also limits the ability to effectively evaluate\nmodel performance in new domains. Continuously obtaining high-quality\ntext-to-SQL data for evolving schemas is prohibitively expensive in real-world\nscenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop\ntext-to-SQL data annotation system. SQLsynth streamlines the creation of\nhigh-quality text-to-SQL datasets through human-LLM collaboration in a\nstructured workflow. A within-subjects user study comparing SQLsynth with\nmanual annotation and ChatGPT shows that SQLsynth significantly accelerates\ntext-to-SQL data annotation, reduces cognitive load, and produces datasets that\nare more accurate, natural, and diverse. Our code is available at\nhttps://github.com/adobe/nl_sql_analyzer.",
      "tldr_zh": "这篇论文探讨了Text-to-SQL模型在新数据库模式下的适应性问题，由于领域特定数据缺乏，导致模型性能显著下降，且数据获取成本高昂。作者提出SQLsynth系统，通过人类和LLM的协作标注，采用结构化工作流来加速创建高质量的Text-to-SQL数据集。用户研究显示，SQLsynth相较于手动标注和ChatGPT，能显著减少认知负荷，并生成更准确、自然和多样的数据集，代码已开源于https://github.com/adobe/nl_sql_analyzer。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted by IUI'25",
      "pdf_url": "http://arxiv.org/pdf/2502.15980v1",
      "published_date": "2025-02-21 22:32:35 UTC",
      "updated_date": "2025-02-21 22:32:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:48:16.338540"
    },
    {
      "arxiv_id": "2502.15975v2",
      "title": "Sparsity May Be All You Need: Sparse Random Parameter Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Jesus Rios",
        "Pierre Dognin",
        "Ronny Luss",
        "Karthikeyan N. Ramamurthy"
      ],
      "abstract": "Full fine-tuning of large language models for alignment and task adaptation\nhas become prohibitively expensive as models have grown in size.\nParameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing\nthe computational and memory resources needed for fine-tuning these models by\nonly training on a small number of parameters instead of all model parameters.\nCurrently, the most popular PEFT method is the Low-Rank Adaptation (LoRA),\nwhich freezes the parameters of the model to be fine-tuned and introduces a\nsmall set of trainable parameters in the form of low-rank matrices. We propose\nsimply reducing the number of trainable parameters by randomly selecting a\nsmall proportion of the model parameters to train on. In this paper, we compare\nthe efficiency and performance of our proposed approach with PEFT methods,\nincluding LoRA, as well as full parameter fine-tuning.",
      "tldr_zh": "大型语言模型的全面微调成本过高，因此Parameter-Efficient Fine-Tuning (PEFT) 方法如Low-Rank Adaptation (LoRA) 通过只训练少量参数来降低资源消耗。本文提出Sparse Random Parameter Adaptation 一种新方法，仅随机选择一小部分模型参数进行训练，以进一步简化微调过程。该方法在效率和性能上与PEFT 方法以及全参数微调进行了比较，展示了其潜在优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15975v2",
      "published_date": "2025-02-21 22:23:16 UTC",
      "updated_date": "2025-05-21 15:33:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:48:27.969141"
    },
    {
      "arxiv_id": "2502.15972v1",
      "title": "Multi-Agent Multimodal Models for Multicultural Text to Image Generation",
      "title_zh": "多智能体多模态模型用于多文化文本到图像生成",
      "authors": [
        "Parth Bhalerao",
        "Mounika Yalamarty",
        "Brian Trinh",
        "Oana Ignat"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate impressive performance across\nvarious multimodal tasks. However, their effectiveness in cross-cultural\ncontexts remains limited due to the predominantly Western-centric nature of\nexisting data and models. Meanwhile, multi-agent models have shown strong\ncapabilities in solving complex tasks. In this paper, we evaluate the\nperformance of LLMs in a multi-agent interaction setting for the novel task of\nmulticultural image generation. Our key contributions are: (1) We introduce\nMosAIG, a Multi-Agent framework that enhances multicultural Image Generation by\nleveraging LLMs with distinct cultural personas; (2) We provide a dataset of\n9,000 multicultural images spanning five countries, three age groups, two\ngenders, 25 historical landmarks, and five languages; and (3) We demonstrate\nthat multi-agent interactions outperform simple, no-agent models across\nmultiple evaluation metrics, offering valuable insights for future research.\nOur dataset and models are available at https://github.com/OanaIgnat/MosAIG.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在多文化语境下的局限性，特别是其西方中心化问题，并评估了多智能体模型（multi-agent models）在多文化文本到图像生成任务中的性能。主要贡献包括：引入 MosAIG 框架，该框架通过赋予 LLMs 不同的文化角色来增强图像生成；提供一个包含 9,000 张图像的数据集，覆盖五个国家、三个年龄组、两个性别、25 个历史地标和五种语言；实验证明，多智能体交互在多个评估指标上优于简单无智能体模型，并为未来研究提供宝贵见解。数据集和模型已在 GitHub 上公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15972v1",
      "published_date": "2025-02-21 22:19:56 UTC",
      "updated_date": "2025-02-21 22:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:48:40.765205"
    },
    {
      "arxiv_id": "2502.15969v3",
      "title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind",
      "title_zh": "翻译失败",
      "authors": [
        "William Rudman",
        "Michal Golovanevsky",
        "Amir Bar",
        "Vedant Palit",
        "Yann LeCun",
        "Carsten Eickhoff",
        "Ritambhara Singh"
      ],
      "abstract": "Despite strong performance on vision-language tasks, Multimodal Large\nLanguage Models (MLLMs) struggle with mathematical problem-solving, with both\nopen-source and state-of-the-art models falling short of human performance on\nvisual-math benchmarks. To systematically examine visual-mathematical reasoning\nin MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test\nmulti-step reasoning, and (3) explore a potential solution to improve visual\nreasoning capabilities. Our findings reveal fundamental shortcomings in shape\nrecognition, with top models achieving under 50% accuracy in identifying\nregular polygons. We analyze these failures through the lens of dual-process\ntheory and show that MLLMs rely on System 1 (intuitive, memorized associations)\nrather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count\nthe sides of both familiar and novel shapes, suggesting they have neither\nlearned the concept of sides nor effectively process visual inputs. Finally, we\npropose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances\nmulti-step mathematical reasoning by explicitly referencing visual annotations\nin diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting\ntask from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs\nremains an open problem, and visually-guided prompting is essential for\nsuccessfully engaging visual reasoning. Code available at:\nhttps://github.com/rsinghlab/Shape-Blind.",
      "tldr_zh": "尽管 Multimodal Large Language Models (MLLMs) 在视觉语言任务中表现出色，但它们在视觉数学问题解决上远低于人类水平，尤其在几何形状识别方面。研究系统评估了 MLLMs 对几何原语的理解和多步推理，发现这些模型在识别规则多边形时准确率不足 50%，并依赖 System 1（直觉记忆）而非 System 2（deliberate 推理），导致无法正确计数多边形的边。作者提出 Visually Cued Chain-of-Thought (VC-CoT) 提示方法，通过引用视觉注释提升多步数学推理，将 GPT-4o 在不规则多边形边计数任务的准确率从 7% 提高到 93%。这些发现突显了 MLLMs 中 System 2 推理的不足，并强调视觉引导提示在增强视觉推理中的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15969v3",
      "published_date": "2025-02-21 22:04:09 UTC",
      "updated_date": "2025-04-05 17:15:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:48:53.586475"
    },
    {
      "arxiv_id": "2502.15964v1",
      "title": "Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models",
      "title_zh": "Minions：设备端和云端语言模型的成本高效协作",
      "authors": [
        "Avanika Narayan",
        "Dan Biderman",
        "Sabri Eyuboglu",
        "Avner May",
        "Scott Linderman",
        "James Zou",
        "Christopher Re"
      ],
      "abstract": "We investigate an emerging setup in which a small, on-device language model\n(LM) with access to local data communicates with a frontier, cloud-hosted LM to\nsolve real-world tasks involving financial, medical, and scientific reasoning\nover long documents. Can a local-remote collaboration reduce cloud inference\ncosts while preserving quality? First, we consider a naive collaboration\nprotocol where the local and remote models simply chat back and forth. Because\nonly the local model reads the full context, this protocol achieves a 30.4x\nreduction in remote costs, but recovers only 87% of the performance of the\nfrontier model. We identify two key limitations of this protocol: the local\nmodel struggles to (1) follow the remote model's multi-step instructions and\n(2) reason over long contexts. Motivated by these observations, we study an\nextension of this protocol, coined MinionS, in which the remote model\ndecomposes the task into easier subtasks over shorter chunks of the document,\nthat are executed locally in parallel. MinionS reduces costs by 5.7x on average\nwhile recovering 97.9% of the performance of the remote model alone. Our\nanalysis reveals several key design choices that influence the trade-off\nbetween cost and performance in local-remote systems.",
      "tldr_zh": "该研究探讨了本地(on-device)小型语言模型与云端(cloud)大型语言模型的协作框架，旨在处理涉及金融、医疗和科学推理的长文档任务，同时降低云端推理成本。初始协议通过简单对话实现了30.4倍的成本减少，但仅恢复了87%的云端模型性能，主要受限于本地模型在多步指令遵循和长上下文推理方面的不足。为此，研究提出Minions协议，将任务分解为更简单的子任务，并在本地并行执行，从而平均减少5.7倍成本并恢复97.9%的云端模型性能。该框架的分析突出了影响成本与性能权衡的关键设计选择，为高效的本地-云端系统提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15964v1",
      "published_date": "2025-02-21 21:54:40 UTC",
      "updated_date": "2025-02-21 21:54:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:49:04.006960"
    },
    {
      "arxiv_id": "2502.15959v1",
      "title": "A Knowledge Distillation-Based Approach to Enhance Transparency of Classifier Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchen Jiang",
        "Xinyuan Zhao",
        "Yihang Wu",
        "Ahmad Chaddad"
      ],
      "abstract": "With the rapid development of artificial intelligence (AI), especially in the\nmedical field, the need for its explainability has grown. In medical image\nanalysis, a high degree of transparency and model interpretability can help\nclinicians better understand and trust the decision-making process of AI\nmodels. In this study, we propose a Knowledge Distillation (KD)-based approach\nthat aims to enhance the transparency of the AI model in medical image\nanalysis. The initial step is to use traditional CNN to obtain a teacher model\nand then use KD to simplify the CNN architecture, retain most of the features\nof the data set, and reduce the number of network layers. It also uses the\nfeature map of the student model to perform hierarchical analysis to identify\nkey features and decision-making processes. This leads to intuitive visual\nexplanations. We selected three public medical data sets (brain tumor, eye\ndisease, and Alzheimer's disease) to test our method. It shows that even when\nthe number of layers is reduced, our model provides a remarkable result in the\ntest set and reduces the time required for the interpretability analysis.",
      "tldr_zh": "本研究提出了一种基于Knowledge Distillation (KD)的approach，以提升AI模型在医疗图像分析中的透明度和可解释性。方法包括使用传统CNN作为教师模型，通过KD简化网络架构、保留数据集关键特征并减少层数，同时利用学生模型的特征图进行分层分析，生成直观的视觉解释。在脑肿瘤、眼病和阿尔茨海默病三个公共数据集上测试，结果显示即使层数减少，模型仍保持出色性能，并显著缩短了可解释性分析所需时间。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15959v1",
      "published_date": "2025-02-21 21:43:21 UTC",
      "updated_date": "2025-02-21 21:43:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:49:16.829044"
    },
    {
      "arxiv_id": "2503.16456v1",
      "title": "Position: Beyond Assistance -- Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care",
      "title_zh": "Position：超越辅助——重新构想 LLMs 作为伦理和适应性共同创造者在心理健康护理领域",
      "authors": [
        "Abeer Badawi",
        "Md Tahmid Rahman Laskar",
        "Jimmy Xiangji Huang",
        "Shaina Raza",
        "Elham Dolatabadi"
      ],
      "abstract": "This position paper argues for a fundamental shift in how Large Language\nModels (LLMs) are integrated into the mental health care domain. We advocate\nfor their role as co-creators rather than mere assistive tools. While LLMs have\nthe potential to enhance accessibility, personalization, and crisis\nintervention, their adoption remains limited due to concerns about bias,\nevaluation, over-reliance, dehumanization, and regulatory uncertainties. To\naddress these challenges, we propose two structured pathways: SAFE-i\n(Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical\nand responsible deployment, and HAAS-e (Human-AI Alignment and Safety\nEvaluation) Framework for multidimensional, human-centered assessment. SAFE-i\nprovides a blueprint for data governance, adaptive model engineering, and\nreal-world integration, ensuring LLMs align with clinical and ethical\nstandards. HAAS-e introduces evaluation metrics that go beyond technical\naccuracy to measure trustworthiness, empathy, cultural sensitivity, and\nactionability. We call for the adoption of these structured approaches to\nestablish a responsible and scalable model for LLM-driven mental health\nsupport, ensuring that AI complements-rather than replaces-human expertise.",
      "tldr_zh": "本论文主张将大型语言模型（LLMs）从心理健康领域的辅助工具转变为道德和适应性的共同创造者，从而提升可访问性、个性化及危机干预能力。当前LLMs的采用受限于偏见、评估挑战、过度依赖、人性化缺失及监管不确定性等问题。为解决这些挑战，作者提出SAFE-i（Supportive, Adaptive, Fair, and Ethical Implementation）准则，用于指导数据治理、适应性模型工程及实际整合，确保符合临床和伦理标准；以及HAAS-e（Human-AI Alignment and Safety Evaluation）框架，用于多维度评估，包括可信度、同理心、文化敏感性和可操作性。最终，论文呼吁采用这些结构化方法，建立负责任且可扩展的LLMs支持模式，让AI补充而非取代人类专业知识。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16456v1",
      "published_date": "2025-02-21 21:41:20 UTC",
      "updated_date": "2025-02-21 21:41:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:49:27.184435"
    },
    {
      "arxiv_id": "2502.15957v1",
      "title": "R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression",
      "title_zh": "R$^3$Mem：通过可逆压缩桥接",
      "authors": [
        "Xiaoqiang Wang",
        "Suyuchen Wang",
        "Yun Zhu",
        "Bang Liu"
      ],
      "abstract": "Memory plays a key role in enhancing LLMs' performance when deployed to\nreal-world applications. Existing solutions face trade-offs: explicit memory\ndesigns based on external storage require complex management and incur storage\noverhead, while implicit memory designs that store information via parameters\nstruggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory\nnetwork that optimizes both information Retention and Retrieval through\nReversible context compression. Specifically, R$^3$Mem employs virtual memory\ntokens to compress and encode infinitely long histories, further enhanced by a\nhierarchical compression strategy that refines information from document- to\nentity-level for improved assimilation across granularities. For retrieval,\nR$^3$Mem employs a reversible architecture, reconstructing raw data by invoking\nthe model backward with compressed information. Implemented via\nparameter-efficient fine-tuning, it can integrate seamlessly with any\nTransformer-based model. Experiments demonstrate that our memory design\nachieves state-of-the-art performance in long-context language modeling and\nretrieval-augmented generation tasks. It also significantly outperforms\nconventional memory modules in long-horizon interaction tasks like\nconversational agents, showcasing its potential for next-generation retrieval\nsystems.",
      "tldr_zh": "该论文提出 R$^3$Mem，一种优化大语言模型(LLMs)内存网络的设计，通过可逆上下文压缩桥接信息保留和检索的权衡。R$^3$Mem 使用虚拟内存标记来压缩无限长的历史记录，并采用分层压缩策略，从文档级到实体级提升信息吸收效率；同时，通过可逆架构实现可靠检索，允许模型反向调用重建原始数据。实验显示，该方法在长上下文语言建模和检索增强生成任务中达到最先进性能，并在长时序交互任务（如对话代理）中显著优于传统内存模块，为下一代检索系统提供潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2502.15957v1",
      "published_date": "2025-02-21 21:39:00 UTC",
      "updated_date": "2025-02-21 21:39:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:49:40.620039"
    },
    {
      "arxiv_id": "2502.15955v1",
      "title": "Compression Barriers for Autoregressive Transformers",
      "title_zh": "自回归 Transformer 的压缩障碍",
      "authors": [
        "Themistoklis Haris",
        "Krzysztof Onak"
      ],
      "abstract": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
      "tldr_zh": "这篇论文探讨了Autoregressive Transformers在推理时KV cache（关键-值嵌入缓存）所需的内存限制，证明了任何attention-based token generation算法在没有特定结构假设的情况下，必须使用Θ(nd)空间，其中n是生成的token数，d = Ω(log n)是KV embeddings的维度。\n作者通过通信复杂度的归约和随机构造（借鉴Johnson-Linderstrauss lemma）来证明这一不可能性。\n对于低维场景（d = o(log n)），论文进一步显示算法需要Ω(d·e^d)空间，并证明SubGen算法达到了这一紧界。\n此外，论文分析了稀疏性假设下实现次线性空间的可能性，提出了一种新的KV cache压缩算法用于滑动窗口attention，并证明了在线计算attention的非自适应算法无法在所有token上实现次线性时间复杂度。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.CC",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15955v1",
      "published_date": "2025-02-21 21:37:52 UTC",
      "updated_date": "2025-02-21 21:37:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:49:54.744273"
    },
    {
      "arxiv_id": "2502.15954v1",
      "title": "MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zaifu Zhan",
        "Jun Wang",
        "Shuang Zhou",
        "Jiawen Deng",
        "Rui Zhang"
      ],
      "abstract": "Objective: To optimize in-context learning in biomedical natural language\nprocessing by improving example selection. Methods: We introduce a novel\nmulti-mode retrieval-augmented generation (MMRAG) framework, which integrates\nfour retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2)\nTop Mode, retrieving the most relevant examples based on similarity; (3)\nDiversity Mode, ensuring variation in selected examples; and (4) Class Mode,\nselecting category-representative examples. This study evaluates MMRAG on three\ncore biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction\n(RE), and Text Classification (TC). The datasets used include BC2GM for gene\nand protein mention recognition (NER), DDI for drug-drug interaction extraction\n(RE), GIT for general biomedical information extraction (RE), and HealthAdvice\nfor health-related text classification (TC). The framework is tested with two\nlarge language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever,\nMedCPT, BGE-Large) to assess performance across different retrieval strategies.\nResults: The results from the Random mode indicate that providing more examples\nin the prompt improves the model's generation performance. Meanwhile, Top mode\nand Diversity mode significantly outperform Random mode on the RE (DDI) task,\nachieving an F1 score of 0.9669, a 26.4% improvement. Among the three\nretrievers tested, Contriever outperformed the other two in a greater number of\nexperiments. Additionally, Llama 2 and Llama 3 demonstrated varying\ncapabilities across different tasks, with Llama 3 showing a clear advantage in\nhandling NER tasks. Conclusion: MMRAG effectively enhances biomedical\nin-context learning by refining example selection, mitigating data scarcity\nissues, and demonstrating superior adaptability for NLP-driven healthcare\napplications.",
      "tldr_zh": "本研究提出 MMRAG 框架，利用多模式 Retrieval-Augmented Generation 技术优化生物医学 NLP 的 in-context learning，通过整合 Random Mode、Top Mode、Diversity Mode 和 Class Mode 等四种检索策略来改进示例选择。框架在 Named Entity Recognition (NER)、Relation Extraction (RE) 和 Text Classification (TC) 等任务上进行评估，使用数据集如 BC2GM、DDI 和 HealthAdvice，并测试了 Llama2-7B、Llama3-8B 模型以及 Contriever、MedCPT 和 BGE-Large 检索器。结果显示，Top 和 Diversity 模式在 RE (DDI) 任务上显著提升 F1 分数至 0.9669，比 Random 模式提高 26.4%，而 Contriever 检索器在多数实验中表现最佳，且 Llama 3 在 NER 任务上显示出明显优势。总之，MMRAG 有效缓解数据稀缺问题，并提升了 NLP 在医疗应用的适应性和性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to JAMIA",
      "pdf_url": "http://arxiv.org/pdf/2502.15954v1",
      "published_date": "2025-02-21 21:36:48 UTC",
      "updated_date": "2025-02-21 21:36:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:50:07.583895"
    },
    {
      "arxiv_id": "2502.15953v1",
      "title": "Multi-Objective Optimization of Water Resource Allocation for Groundwater Recharge and Surface Runoff Management in Watershed Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Abbas Sharifi",
        "Hajar Kazemi Naeini",
        "Mohsen Ahmadi",
        "Saeed Asadi",
        "Abbas Varmaghani"
      ],
      "abstract": "Land degradation and air pollution are primarily caused by the salinization\nof soil and desertification that occurs from the drying of salinity lakes and\nthe release of dust into the atmosphere because of their dried bottom. The\ncomplete drying up of a lake has caused a community environmental catastrophe.\nIn this study, we presented an optimization problem to determine the total\nsurface runoff to maintain the level of salinity lake (Urmia Lake). The\nproposed process has two key stages: identifying the influential factors in\ndetermining the lake water level using sensitivity analysis approaches based\nupon historical data and optimizing the effective variable to stabilize the\nlake water level under changing design variables. Based upon the Sobol'-Jansen\nand Morris techniques, the groundwater level and total surface runoff flow are\nhighly effective with nonlinear and interacting impacts of the lake water\nlevel. As a result of the sensitivity analysis, we found that it may be\npossible to effectively manage lake levels by adjusting total surface runoff.\nWe used genetic algorithms, non-linear optimization, and pattern search\ntechniques to solve the optimization problem. Furthermore, the lake level\nconstraint is established based on a pattern as a constant number every month.\nIn order to maintain a consistent pattern of lake levels, it is necessary to\nincrease surface runoff by approximately 8.7 times during filling season. It is\nnecessary to increase this quantity by 33.5 times during the draining season.\nIn the future, the results may serve as a guide for the rehabilitation of the\nlake.",
      "tldr_zh": "这篇论文针对盐湖干涸导致的土地退化和空气污染问题（如土壤盐化和沙漠化），提出了一种多目标优化方法，用于水资源分配以管理地下水补给和地表径流，焦点是维持乌尔米亚湖（Urmia Lake）水位。研究首先通过 Sobol'-Jansen 和 Morris 敏感性分析技术，识别地下水位和总地表径流作为关键影响因素，并使用遗传算法、非线性优化和模式搜索技术优化这些变量。结果显示，为稳定湖水位，需要在填充季节将地表径流增加约8.7倍，在排泄季节增加33.5倍，从而为湖泊恢复提供重要指导。",
      "categories": [
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15953v1",
      "published_date": "2025-02-21 21:34:27 UTC",
      "updated_date": "2025-02-21 21:34:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:50:18.486055"
    },
    {
      "arxiv_id": "2502.18504v1",
      "title": "TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice",
      "title_zh": "翻译失败",
      "authors": [
        "Aman Goel",
        "Xian Carrie Wu",
        "Zhe Wang",
        "Dmitriy Bespalov",
        "Yanjun Qi"
      ],
      "abstract": "Jailbreaking large-language models (LLMs) involves testing their robustness\nagainst adversarial prompts and evaluating their ability to withstand prompt\nattacks that could elicit unauthorized or malicious responses. In this paper,\nwe present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently\nfinding a collection of effective jailbreaking templates that, when combined\nwith harmful questions, can lead a target LLM to produce harmful responses\nthrough black-box access via user prompts. We describe the limitations of\ndirectly applying existing template-based attacking techniques in practice, and\npresent functional and efficiency-focused upgrades we added to mutation-based\nfuzzing to generate effective jailbreaking templates automatically.\nTurboFuzzLLM achieves $\\geq$ 95\\% attack success rates (ASR) on public datasets\nfor leading LLMs (including GPT-4o \\& GPT-4 Turbo), shows impressive\ngeneralizability to unseen harmful questions, and helps in improving model\ndefenses to prompt attacks.",
      "tldr_zh": "这篇论文提出了 TurboFuzzLLM，一种基于 mutation-based fuzzing 的技术，用于高效 jailbreaking 大语言模型(LLMs)，通过自动生成有效的攻击模板来测试模型对对抗性提示的鲁棒性。作者分析了现有模板-based 攻击技术的局限性，并引入功能和效率升级，如优化变异过程，以自动创建能诱导有害响应的模板。实验结果显示，TurboFuzzLLM 在 GPT-4o 和 GPT-4 Turbo 等领先模型上实现了 ≥95% 的 attack success rates (ASR)，并展现出对未见有害问题的良好泛化性，同时有助于提升模型对提示攻击的防御机制。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at NAACL 2025 industry track, 12 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.18504v1",
      "published_date": "2025-02-21 21:10:12 UTC",
      "updated_date": "2025-02-21 21:10:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:50:29.383488"
    },
    {
      "arxiv_id": "2502.15938v1",
      "title": "Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Shane Bergsma",
        "Nolan Dey",
        "Gurpreet Gosal",
        "Gavia Gray",
        "Daria Soboleva",
        "Joel Hestness"
      ],
      "abstract": "LLMs are commonly trained with a learning rate (LR) warmup, followed by\ncosine decay to 10% of the maximum (10x decay). In a large-scale empirical\nstudy, we show that under an optimal peak LR, a simple linear decay-to-zero\n(D2Z) schedule consistently outperforms other schedules when training at\ncompute-optimal dataset sizes. D2Z is superior across a range of model sizes,\nbatch sizes, datasets, and vocabularies. Benefits increase as dataset size\nincreases. Leveraging a novel interpretation of AdamW as an exponential moving\naverage of weight updates, we show how linear D2Z optimally balances the\ndemands of early training (moving away from initial conditions) and late\ntraining (averaging over more updates in order to mitigate gradient noise). In\nexperiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP)\nusing D2Z achieves lower loss than when trained for 200 TPP using 10x decay,\ncorresponding to an astonishing 60% compute savings. Models such as Llama2-7B,\ntrained for 286 TPP with 10x decay, could likely have saved a majority of\ncompute by training with D2Z.",
      "tldr_zh": "这篇论文探讨了在训练大型语言模型 (LLMs) 时，使用线性衰减学习率到零 (D2Z) 的优势，通过大规模实证研究证明 D2Z 在最佳峰值学习率下 consistently outperforms 其他调度，如余弦衰减到 10%。研究显示，D2Z 适用于不同模型大小、批量大小、数据集和词汇表，尤其在计算最优数据集规模下表现更优，且益处随数据集规模增加而放大。基于对 AdamW 作为权重更新指数移动平均的新解读，D2Z 能有效平衡早期训练（远离初始条件）和晚期训练（缓解梯度噪声）。实验结果表明，一个 610M 参数模型使用 D2Z 训练到 80 tokens-per-parameter (TPP) 即可实现更低损失，相比 10x 衰减节省 60% 计算资源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15938v1",
      "published_date": "2025-02-21 21:08:24 UTC",
      "updated_date": "2025-02-21 21:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:50:42.948124"
    },
    {
      "arxiv_id": "2502.15937v1",
      "title": "Discovery and Deployment of Emergent Robot Swarm Behaviors via Representation Learning and Real2Sim2Real Transfer",
      "title_zh": "涌现机器人群行为的发现与部署：通过表示学习和",
      "authors": [
        "Connor Mattson",
        "Varun Raveendra",
        "Ricardo Vega",
        "Cameron Nowzari",
        "Daniel S. Drew",
        "Daniel S. Brown"
      ],
      "abstract": "Given a swarm of limited-capability robots, we seek to automatically discover\nthe set of possible emergent behaviors. Prior approaches to behavior discovery\nrely on human feedback or hand-crafted behavior metrics to represent and evolve\nbehaviors and only discover behaviors in simulation, without testing or\nconsidering the deployment of these new behaviors on real robot swarms. In this\nwork, we present Real2Sim2Real Behavior Discovery via Self-Supervised\nRepresentation Learning, which combines representation learning and novelty\nsearch to discover possible emergent behaviors automatically in simulation and\nenable direct controller transfer to real robots. First, we evaluate our method\nin simulation and show that our proposed self-supervised representation\nlearning approach outperforms previous hand-crafted metrics by more accurately\nrepresenting the space of possible emergent behaviors. Then, we address the\nreality gap by incorporating recent work in sim2real transfer for swarms into\nour lightweight simulator design, enabling direct robot deployment of all\nbehaviors discovered in simulation on an open-source and low-cost robot\nplatform.",
      "tldr_zh": "该研究提出了一种Real2Sim2Real Behavior Discovery via Self-Supervised Representation Learning的方法，结合表示学习(Representation Learning)和新颖性搜索(novelty search)，以自动发现有限能力机器人群的涌现行为，并在模拟环境中进行优化。相比传统依赖人类反馈或手工指标的方法，该方法更准确地表示行为空间，并在模拟评估中表现出色。最终，通过整合sim2real transfer技术到轻量级模拟器中，实现从模拟直接部署到开源低成本真实机器人平台，解决了现实差距问题。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 5 figures. To be included in Proc. of the 24th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS\n  2025)",
      "pdf_url": "http://arxiv.org/pdf/2502.15937v1",
      "published_date": "2025-02-21 21:04:47 UTC",
      "updated_date": "2025-02-21 21:04:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:50:52.817680"
    },
    {
      "arxiv_id": "2502.15936v1",
      "title": "Space-O-RAN: Enabling Intelligent, Open, and Interoperable Non Terrestrial Networks in 6G",
      "title_zh": "翻译失败",
      "authors": [
        "Eduardo Baena",
        "Paolo Testolina",
        "Michele Polese",
        "Dimitrios Koutsonikolas",
        "Josep Jornet",
        "Tommaso Melodia"
      ],
      "abstract": "Non-terrestrial networks (NTNs) are essential for ubiquitous connectivity,\nproviding coverage in remote and underserved areas. However, since NTNs are\ncurrently operated independently, they face challenges such as isolation,\nlimited scalability, and high operational costs. Integrating satellite\nconstellations with terrestrial networks offers a way to address these\nlimitations while enabling adaptive and cost-efficient connectivity through the\napplication of Artificial Intelligence (AI) models.\n  This paper introduces Space-O-RAN, a framework that extends Open Radio Access\nNetwork (RAN) principles to NTNs. It employs hierarchical closed-loop control\nwith distributed Space RAN Intelligent Controllers (Space-RICs) to dynamically\nmanage and optimize operations across both domains.\n  To enable adaptive resource allocation and network orchestration, the\nproposed architecture integrates real-time satellite optimization and control\nwith AI-driven management and digital twin (DT) modeling. It incorporates\ndistributed Space Applications (sApps) and dApps to ensure robust performance\nin in highly dynamic orbital environments. A core feature is dynamic\nlink-interface mapping, which allows network functions to adapt to specific\napplication requirements and changing link conditions using all physical links\non the satellite.\n  Simulation results evaluate its feasibility by analyzing latency constraints\nacross different NTN link types, demonstrating that intra-cluster coordination\noperates within viable signaling delay bounds, while offloading non-real-time\ntasks to ground infrastructure enhances scalability toward sixth-generation\n(6G) networks.",
      "tldr_zh": "本文提出 Space-O-RAN 框架，将 Open Radio Access Network (RAN) 原则扩展到 Non Terrestrial Networks (NTNs)，旨在解决 NTNs 的隔离、可扩展性不足和高运营成本问题，通过 Artificial Intelligence (AI) 驱动的管理实现与陆地网络的智能整合。框架采用分层闭环控制、分布式 Space RAN Intelligent Controllers (Space-RICs)、digital twin (DT) 建模以及动态链路接口映射，确保在动态轨道环境中进行自适应资源分配和网络编排。模拟结果显示，该框架在不同 NTN 链路类型中满足延迟约束，并通过将非实时任务卸载到地面基础设施，提升了第六代 (6G) 网络的可扩展性和效率。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15936v1",
      "published_date": "2025-02-21 21:03:37 UTC",
      "updated_date": "2025-02-21 21:03:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:51:06.496041"
    },
    {
      "arxiv_id": "2503.00018v1",
      "title": "Eeyore: Realistic Depression Simulation via Supervised and Preference Optimization",
      "title_zh": "Eeyore：通过监督和偏好优化的现实抑郁模拟",
      "authors": [
        "Siyang Liu",
        "Bianca Brie",
        "Wenda Li",
        "Laura Biester",
        "Andrew Lee",
        "James Pennebaker",
        "Rada Mihalcea"
      ],
      "abstract": "Large Language Models (LLMs) have been previously explored for mental\nhealthcare training and therapy client simulation, but they still fall short in\nauthentically capturing diverse client traits and psychological conditions. We\nintroduce \\textbf{Eeyore}, an 8B model optimized for realistic depression\nsimulation through a structured alignment framework, incorporating expert input\nat every stage. First, we systematically curate real-world depression-related\nconversations, extracting depressive traits to guide data filtering and\npsychological profile construction, and use this dataset to instruction-tune\nEeyore for profile adherence. Next, to further enhance realism, Eeyore\nundergoes iterative preference optimization -- first leveraging model-generated\npreferences and then calibrating with a small set of expert-annotated\npreferences. Throughout the entire pipeline, we actively collaborate with\ndomain experts, developing interactive interfaces to validate trait extraction\nand iteratively refine structured psychological profiles for clinically\nmeaningful role-play customization. Despite its smaller model size, the Eeyore\ndepression simulation outperforms GPT-4o with SOTA prompting strategies, both\nin linguistic authenticity and profile adherence.",
      "tldr_zh": "本研究引入了 Eeyore，一个 8B Large Language Models (LLMs) 模型，通过结构化的对齐框架和专家输入，实现了更真实的抑郁症模拟。研究首先系统收集真实抑郁相关对话，提取抑郁特征用于数据过滤和心理配置文件构建，然后通过 instruction-tuning 进行微调，并采用迭代 preference optimization，包括模型生成偏好和专家标注校准。最终，Eeyore 在语言真实性和配置文件遵守方面优于 GPT-4o，尽管其模型规模较小，为心理健康训练和模拟提供了更可靠的工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00018v1",
      "published_date": "2025-02-21 20:29:44 UTC",
      "updated_date": "2025-02-21 20:29:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:51:19.514044"
    },
    {
      "arxiv_id": "2502.15920v1",
      "title": "Self-Taught Agentic Long Context Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Yufan Zhuang",
        "Xiaodong Yu",
        "Jialian Wu",
        "Ximeng Sun",
        "Ze Wang",
        "Jiang Liu",
        "Yusheng Su",
        "Jingbo Shang",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "abstract": "Answering complex, long-context questions remains a major challenge for large\nlanguage models (LLMs) as it requires effective question clarifications and\ncontext retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a\nframework designed to enhance an LLM's understanding of such queries by\nintegrating targeted self-clarification with contextual grounding within an\nagentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC),\nwhere models refine their understanding through self-generated clarification\nquestions and corresponding contextual groundings. By scaling inference as a\ntree search where each node represents a CoC step, we achieve 97.8% answer\nrecall on NarrativeQA with a search depth of up to three and a branching factor\nof eight. To amortize the high cost of this search process to training, we\nleverage the preference pairs for each step obtained by the CoC workflow and\nperform two-stage model finetuning: (1) supervised finetuning to learn\neffective decomposition strategies, and (2) direct preference optimization to\nenhance reasoning quality. This enables AgenticLU models to generate\nclarifications and retrieve relevant context effectively and efficiently in a\nsingle inference pass. Extensive experiments across seven long-context tasks\ndemonstrate that AgenticLU significantly outperforms state-of-the-art prompting\nmethods and specialized long-context LLMs, achieving robust multi-hop reasoning\nwhile sustaining consistent performance as context length grows.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在处理复杂长上下文查询时的挑战，提出Agentic Long-Context Understanding (AgenticLU)框架，该框架通过整合自我澄清机制和上下文grounding来提升模型的查询理解能力。核心组件Chain-of-Clarifications (CoC)允许模型通过生成澄清问题和相应上下文来逐步精炼推理，并利用树搜索方法实现高效的推理过程，同时通过两阶段微调（监督微调和直接偏好优化）降低计算成本。实验结果显示，AgenticLU在七个长上下文任务上显著优于现有提示方法和专用长上下文LLMs，实现了97.8%的答案召回率，并在多跳推理中保持性能稳定。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15920v1",
      "published_date": "2025-02-21 20:29:36 UTC",
      "updated_date": "2025-02-21 20:29:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:51:30.905676"
    },
    {
      "arxiv_id": "2503.16455v1",
      "title": "Bridging Structural Dynamics and Biomechanics: Human Motion Estimation through Footstep-Induced Floor Vibrations",
      "title_zh": "桥接结构动力学和生物力学：通过脚步引起的地板振动进行人体运动估计",
      "authors": [
        "Yiwen Dong",
        "Jessica Rose",
        "Hae Young Noh"
      ],
      "abstract": "Quantitative estimation of human joint motion in daily living spaces is\nessential for early detection and rehabilitation tracking of\nneuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall\nrisks for older adults. Existing approaches involve monitoring devices such as\ncameras, wearables, and pressure mats, but have operational constraints such as\ndirect line-of-sight, carrying devices, and dense deployment. To overcome these\nlimitations, we leverage gait-induced floor vibration to estimate lower-limb\njoint motion (e.g., ankle, knee, and hip flexion angles), allowing\nnon-intrusive and contactless gait health monitoring in people's living spaces.\nTo overcome the high uncertainty in lower-limb movement given the limited\ninformation provided by the gait-induced floor vibrations, we formulate a\nphysics-informed graph to integrate domain knowledge of gait biomechanics and\nstructural dynamics into the model. Specifically, different types of nodes\nrepresent heterogeneous information from joint motions and floor vibrations;\nTheir connecting edges represent the physiological relationships between joints\nand forces governed by gait biomechanics, as well as the relationships between\nforces and floor responses governed by the structural dynamics. As a result,\nour model poses physical constraints to reduce uncertainty while allowing\ninformation sharing between the body and the floor to make more accurate\npredictions. We evaluate our approach with 20 participants through a real-world\nwalking experiment. We achieved an average of 3.7 degrees of mean absolute\nerror in estimating 12 joint flexion angles (38% error reduction from\nbaseline), which is comparable to the performance of cameras and wearables in\ncurrent medical practices.",
      "tldr_zh": "本文提出一种非侵入式方法，利用步行引起的地板振动（footstep-induced floor vibrations）来估计人类下肢关节运动（如踝、膝和髋关节屈曲角度），以支持神经肌肉骨骼疾病（如帕金森）的早期检测和康复追踪。核心技术是构建一个physics-informed graph，将gait biomechanics和structural dynamics的领域知识整合进模型中，通过节点和边表示关节运动、地板响应及其生理关系，减少不确定性和提升预测准确性。在20名参与者的真实实验中，该方法在估计12个关节屈曲角度时，平均绝对误差为3.7度，比基线减少38%，性能可与摄像头和可穿戴设备媲美。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16455v1",
      "published_date": "2025-02-21 20:10:15 UTC",
      "updated_date": "2025-02-21 20:10:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:51:44.151380"
    },
    {
      "arxiv_id": "2504.08737v1",
      "title": "Latency-Aware 2-Opt Monotonic Local Search for Distributed Constraint Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Rachmut",
        "Roie Zivan",
        "William Yeoh"
      ],
      "abstract": "Researchers recently extended Distributed Constraint Optimization Problems\n(DCOPs) to Communication-Aware DCOPs so that they are applicable in scenarios\nin which messages can be arbitrarily delayed. Distributed asynchronous local\nsearch and inference algorithms designed for CA-DCOPs are less vulnerable to\nmessage latency than their counterparts for regular DCOPs. However, unlike\nlocal search algorithms for (regular) DCOPs that converge to k-opt solutions\n(with k > 1), that is, they converge to solutions that cannot be improved by a\ngroup of k agents), local search CA-DCOP algorithms are limited to 1-opt\nsolutions only. In this paper, we introduce Latency-Aware Monotonic Distributed\nLocal Search-2 (LAMDLS-2), where agents form pairs and coordinate bilateral\nassignment replacements. LAMDLS-2 is monotonic, converges to a 2-opt solution,\nand is also robust to message latency, making it suitable for CA-DCOPs. Our\nresults indicate that LAMDLS-2 converges faster than MGM-2, a benchmark\nalgorithm, to a similar 2-opt solution, in various message latency scenarios.",
      "tldr_zh": "本研究扩展了Distributed Constraint Optimization Problems (DCOPs) 到Communication-Aware DCOPs (CA-DCOPs)，以应对消息延迟场景下的挑战，现有的CA-DCOP本地搜索算法虽抗延迟但仅限于1-opt解决方案。论文提出Latency-Aware Monotonic Distributed Local Search-2 (LAMDLS-2) 算法，通过代理配对和双边分配替换实现单调收敛，并确保达到2-opt解决方案，同时保持对消息延迟的鲁棒性。实验结果显示，LAMDLS-2 在各种延迟场景下比基准算法MGM-2 更快收敛到相似的2-opt 解决方案，从而提升了分布式优化问题的效率和可靠性。",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08737v1",
      "published_date": "2025-02-21 20:00:05 UTC",
      "updated_date": "2025-02-21 20:00:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:51:54.616736"
    },
    {
      "arxiv_id": "2502.15907v1",
      "title": "Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Umair Danish",
        "Madhushan Buwaneswaran",
        "Tehara Fonseka",
        "Katarina Grolinger"
      ],
      "abstract": "The increasing impact of human-induced climate change and unplanned urban\nconstructions has increased flooding incidents in recent years. Accurate\nidentification of flooded areas is crucial for effective disaster management\nand urban planning. While few works have utilized convolutional neural networks\nand transformer-based semantic segmentation techniques for identifying flooded\nareas from aerial footage, recent developments in graph neural networks have\ncreated improvement opportunities. This paper proposes an innovative approach,\nthe Graph Attention Convolutional U-NET (GAC-UNET) model, based on graph neural\nnetworks for automated identification of flooded areas. The model incorporates\na graph attention mechanism and Chebyshev layers into the U-Net architecture.\nFurthermore, this paper explores the applicability of transfer learning and\nmodel reprogramming to enhance the accuracy of flood area segmentation models.\nEmpirical results demonstrate that the proposed GAC-UNET model, outperforms\nother approaches with 91\\% mAP, 94\\% dice score, and 89\\% IoU, providing\nvaluable insights for informed decision-making and better planning of future\ninfrastructures in flood-prone areas.",
      "tldr_zh": "该论文提出了一种名为 Graph Attention Convolutional U-NET (GAC-UNET) 的语义分割模型，用于从航拍图像中自动识别洪水区域，以应对气候变化和城市建设导致的洪水事件增加。\nGAC-UNET 模型将 Graph Attention 机制和 Chebyshev 层整合到 U-Net 架构中，并探索转移学习和模型重编程来提升分割准确性。\n实验结果显示，该模型在性能上优于其他方法，达到 91% mAP、94% Dice score 和 89% IoU。\n这一创新方法为灾害管理和洪水-prone 区域的基础设施规划提供了宝贵的决策支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15907v1",
      "published_date": "2025-02-21 19:50:13 UTC",
      "updated_date": "2025-02-21 19:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:52:08.437110"
    },
    {
      "arxiv_id": "2502.17505v1",
      "title": "Inverse Surrogate Model of a Soft X-Ray Spectrometer using Domain Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Enrico Ahlers",
        "Peter Feuer-Forson",
        "Gregor Hartmann",
        "Rolf Mitzner",
        "Peter Baumgärtel",
        "Jens Viefhaus"
      ],
      "abstract": "In this study, we present a method to create a robust inverse surrogate model\nfor a soft X-ray spectrometer. During a beamtime at an electron storage ring,\nsuch as BESSY II, instrumentation and beamlines are required to be correctly\naligned and calibrated for optimal experimental conditions. In order to\nautomate these processes, machine learning methods can be developed and\nimplemented, but in many cases these methods require the use of an inverse\nmodel which maps the output of the experiment, such as a detector image, to the\nparameters of the device. Due to limited experimental data, such models are\noften trained with simulated data, which creates the challenge of compensating\nfor the inherent differences between simulation and experiment. In order to\nclose this gap, we demonstrate the application of data augmentation and\nadversarial domain adaptation techniques, with which we can predict absolute\ncoordinates for the automated alignment of our spectrometer. Bridging the\nsimulation-experiment gap with minimal real-world data opens new avenues for\nautomated experimentation using machine learning in scientific instrumentation.",
      "tldr_zh": "这篇论文提出了一种使用域适应的逆代理模型（inverse surrogate model），针对软X射线光谱仪（soft X-ray spectrometer）实现自动化对齐和校准。方法通过数据增强和对抗域适应（adversarial domain adaptation）技术，桥接模拟数据与实验数据的差异，仅需最少真实数据即可将探测器图像映射回设备参数。实验结果显示，该模型成功预测了绝对坐标，提高了自动化实验过程的鲁棒性，并为科学仪器中的机器学习应用开辟了新途径。",
      "categories": [
        "physics.ins-det",
        "cs.AI",
        "physics.data-an"
      ],
      "primary_category": "physics.ins-det",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17505v1",
      "published_date": "2025-02-21 19:42:50 UTC",
      "updated_date": "2025-02-21 19:42:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:52:19.637122"
    },
    {
      "arxiv_id": "2502.15902v1",
      "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Chen",
        "Yushi Feng",
        "Changyang He",
        "Yue Deng",
        "Hongxi Pu",
        "Bo Li"
      ],
      "abstract": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinguishing between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide explainable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and a Distinguisher that examines how\nwell the input texts align with the predicted prompts. We develop and examine\ntwo versions of Distinguishers. Empirical evaluations demonstrate that both\nDistinguishers perform significantly better than the baseline methods, with\nversion2 outperforming baselines by 9.73% on in-distribution data (F1-score)\nand 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to\nillustrate that IPAD enhances the AI detection trustworthiness by allowing\nusers to directly examine the decision-making evidence, which provides\ninterpretable support for its state-of-the-art detection results.",
      "tldr_zh": "该研究针对大型语言模型(LLM)生成文本难以与人类文本区分的问题，提出IPAD框架，这是一种鲁棒且可解释的AI生成文本检测器。IPAD包括Prompt Inverter模块，用于识别可能生成输入文本的提示，以及Distinguisher模块，用于评估文本与这些提示的匹配度，其两种版本均显示出优越性能。实验结果表明，IPAD的第二版本在in-distribution数据上F1-score比基线方法提高9.73%，在OOD数据上AUROC提高12.65%；此外，用户研究证实了IPAD通过提供可解释的决策证据，提升了检测的可信度和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15902v1",
      "published_date": "2025-02-21 19:41:32 UTC",
      "updated_date": "2025-02-21 19:41:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:52:30.855545"
    },
    {
      "arxiv_id": "2502.15898v1",
      "title": "ML-Driven Approaches to Combat Medicare Fraud: Advances in Class Imbalance Solutions, Feature Engineering, Adaptive Learning, and Business Impact",
      "title_zh": "翻译失败",
      "authors": [
        "Dorsa Farahmandazad",
        "Kasra Danesh"
      ],
      "abstract": "Medicare fraud poses a substantial challenge to healthcare systems, resulting\nin significant financial losses and undermining the quality of care provided to\nlegitimate beneficiaries. This study investigates the use of machine learning\n(ML) to enhance Medicare fraud detection, addressing key challenges such as\nclass imbalance, high-dimensional data, and evolving fraud patterns. A dataset\ncomprising inpatient claims, outpatient claims, and beneficiary details was\nused to train and evaluate five ML models: Random Forest, KNN, LDA, Decision\nTree, and AdaBoost. Data preprocessing techniques included resampling SMOTE\nmethod to address the class imbalance, feature selection for dimensionality\nreduction, and aggregation of diagnostic and procedural codes. Random Forest\nemerged as the best-performing model, achieving a training accuracy of 99.2%\nand validation accuracy of 98.8%, and F1-score (98.4%). The Decision Tree also\nperformed well, achieving a validation accuracy of 96.3%. KNN and AdaBoost\ndemonstrated moderate performance, with validation accuracies of 79.2% and\n81.1%, respectively, while LDA struggled with a validation accuracy of 63.3%\nand a low recall of 16.6%. The results highlight the importance of advanced\nresampling techniques, feature engineering, and adaptive learning in detecting\nMedicare fraud effectively. This study underscores the potential of machine\nlearning in addressing the complexities of fraud detection. Future work should\nexplore explainable AI and hybrid models to improve interpretability and\nperformance, ensuring scalable and reliable fraud detection systems that\nprotect healthcare resources and beneficiaries.",
      "tldr_zh": "本研究探讨了利用机器学习(ML)提升Medicare欺诈检测的效能，针对类不平衡、高维数据和演变欺诈模式等挑战。研究团队使用SMOTE重采样、特征选择和数据聚合等预处理技术，训练了Random Forest、KNN、LDA、Decision Tree和AdaBoost五种模型，其中Random Forest表现最佳，验证准确率达98.8%、F1-score为98.4%。结果突出了高级重采样、特征工程和自适应学习在欺诈检测中的重要性，并建议未来探索可解释AI和混合模型，以构建更可靠的系统保护医疗资源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15898v1",
      "published_date": "2025-02-21 19:34:12 UTC",
      "updated_date": "2025-02-21 19:34:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:52:44.114306"
    },
    {
      "arxiv_id": "2502.15895v1",
      "title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chengyue Huang",
        "Junjiao Tian",
        "Brisa Maneechotesuwan",
        "Shivang Chopra",
        "Zsolt Kira"
      ],
      "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks\nwhile preserving their robustness to distribution shifts. Existing methods\nprimarily focus on constraining and projecting current model towards the\npre-trained initialization based on the magnitudes between fine-tuned and\npre-trained weights, which often require extensive hyper-parameter tuning and\ncan sometimes result in underfitting. In this work, we propose Directional\nGradient Projection (DiGraP), a novel layer-wise trainable method that\nincorporates directional information from gradients to bridge regularization\nand multi-objective optimization. Besides demonstrating our method on image\nclassification, as another contribution we generalize this area to the\nmulti-modal evaluation settings for robust fine-tuning. Specifically, we first\nbridge the uni-modal and multi-modal gap by performing analysis on Image\nClassification reformulated Visual Question Answering (VQA) benchmarks and\nfurther categorize ten out-of-distribution (OOD) VQA datasets by distribution\nshift types and degree (i.e. near versus far OOD). Experimental results show\nthat DiGraP consistently outperforms existing baselines across Image\nClassfication and VQA tasks with discriminative and generative backbones,\nimproving both in-distribution (ID) generalization and OOD robustness.",
      "tldr_zh": "本研究提出了一种新的层级训练方法 Directional Gradient Projection (DiGraP)，旨在通过整合梯度方向信息来桥接正则化和多目标优化，从而实现对基础模型的鲁棒微调，同时适应下游任务并保持对分布偏移的鲁棒性。DiGraP 解决了现有方法的超参数调整问题，并在图像分类任务上进行了演示，同时扩展到多模态评估，包括对图像分类重构的 Visual Question Answering (VQA) 基准的分析，并对十个 out-of-distribution (OOD) VQA 数据集按偏移类型和程度进行分类。实验结果显示，DiGraP 在图像分类和 VQA 任务中使用 discriminative 和 generative 骨干时，均优于现有基线，提升了 in-distribution (ID) 泛化和 OOD 鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15895v1",
      "published_date": "2025-02-21 19:31:55 UTC",
      "updated_date": "2025-02-21 19:31:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:52:54.839777"
    },
    {
      "arxiv_id": "2502.17504v2",
      "title": "Protein Large Language Models: A Comprehensive Survey",
      "title_zh": "蛋白质大型语言模型：全面综述",
      "authors": [
        "Yijia Xiao",
        "Wanjia Zhao",
        "Junkai Zhang",
        "Yiqiao Jin",
        "Han Zhang",
        "Zhicheng Ren",
        "Renliang Sun",
        "Haixin Wang",
        "Guancheng Wan",
        "Pan Lu",
        "Xiao Luo",
        "Yu Zhang",
        "James Zou",
        "Yizhou Sun",
        "Wei Wang"
      ],
      "abstract": "Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.",
      "tldr_zh": "本综述论文对Protein LLMs（蛋白质特定的大型语言模型）进行了首次全面概述，涵盖了它们的架构、训练数据集、评估指标以及在蛋白质结构预测、功能注释和设计等方面的多样应用。通过系统分析超过100篇文章，论文提出了一个结构化的Protein LLMs分类法，并探讨了这些模型如何利用大规模蛋白质序列数据来提升准确性，从而推动蛋白质工程和生物医学研究的发展。此外，论文还讨论了关键挑战和未来方向，并提供了相关资源链接（如GitHub仓库）。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "24 pages, 4 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.17504v2",
      "published_date": "2025-02-21 19:22:10 UTC",
      "updated_date": "2025-03-06 16:14:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:53:06.298949"
    },
    {
      "arxiv_id": "2502.15873v1",
      "title": "Practical Principles for AI Cost and Compute Accounting",
      "title_zh": "AI 成本与计算资源核算的实用原则",
      "authors": [
        "Stephen Casper",
        "Luke Bailey",
        "Tim Schreier"
      ],
      "abstract": "Policymakers are increasingly using development cost and compute as proxies\nfor AI model capabilities and risks. Recent laws have introduced regulatory\nrequirements that are contingent on specific thresholds. However, technical\nambiguities in how to perform this accounting could create loopholes that\nundermine regulatory effectiveness. This paper proposes seven principles for\ndesigning practical AI cost and compute accounting standards that (1) reduce\nopportunities for strategic gaming, (2) avoid disincentivizing responsible risk\nmitigation, and (3) enable consistent implementation across companies and\njurisdictions.",
      "tldr_zh": "政策制定者越来越多地将 AI 模型的开发成本和计算资源（compute）作为其能力和风险的代理，最近的法律也基于特定阈值引入监管要求，但技术模糊性可能导致漏洞，削弱监管有效性。本文提出了七个实用的原则，用于设计 AI 成本和计算会计标准。这些原则旨在减少战略性操纵机会，避免阻碍负责任的风险缓解，并实现公司在不同管辖区的一致实施。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15873v1",
      "published_date": "2025-02-21 18:59:47 UTC",
      "updated_date": "2025-02-21 18:59:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:53:18.765992"
    },
    {
      "arxiv_id": "2502.15681v2",
      "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
      "title_zh": "基于 f-散度分布匹配的一步扩散模型",
      "authors": [
        "Yilun Xu",
        "Weili Nie",
        "Arash Vahdat"
      ],
      "abstract": "Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel $f$-divergence minimization framework, termed $f$-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the $f$-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative $f$-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, $f$-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps://research.nvidia.com/labs/genair/f-distill",
      "tldr_zh": "这项研究针对扩散模型(diffusion models)缓慢的迭代采样过程，提出了一种新的$f$-distill框架，通过$f$-divergence最小化来加速生成速度。该框架泛化了现有的变分分数蒸馏(variational score distillation)方法，使用如forward-KL和Jensen-Shannon散度等替代散度，以平衡模式覆盖和训练方差。实验结果显示，$f$-distill在图像生成任务中优于基线模型，特别是使用Jensen-Shannon散度时，在ImageNet64上实现了最先进的单步生成性能，并在MS-COCO的零样本文本到图像生成中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15681v2",
      "published_date": "2025-02-21 18:59:20 UTC",
      "updated_date": "2025-03-09 22:53:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:53:32.754223"
    },
    {
      "arxiv_id": "2502.15679v1",
      "title": "BOSS: Benchmark for Observation Space Shift in Long-Horizon Task",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Yang",
        "Linfeng Zhao",
        "Mingyu Ding",
        "Gedas Bertasius",
        "Daniel Szafir"
      ],
      "abstract": "Robotics has long sought to develop visual-servoing robots capable of\ncompleting previously unseen long-horizon tasks. Hierarchical approaches offer\na pathway for achieving this goal by executing skill combinations arranged by a\ntask planner, with each visuomotor skill pre-trained using a specific imitation\nlearning (IL) algorithm. However, even in simple long-horizon tasks like skill\nchaining, hierarchical approaches often struggle due to a problem we identify\nas Observation Space Shift (OSS), where the sequential execution of preceding\nskills causes shifts in the observation space, disrupting the performance of\nsubsequent individually trained skill policies. To validate OSS and evaluate\nits impact on long-horizon tasks, we introduce BOSS (a Benchmark for\nObservation Space Shift). BOSS comprises three distinct challenges: \"Single\nPredicate Shift\", \"Accumulated Predicate Shift\", and \"Skill Chaining\", each\ndesigned to assess a different aspect of OSS's negative effect. We evaluated\nseveral recent popular IL algorithms on BOSS, including three Behavioral\nCloning methods and the Visual Language Action model OpenVLA. Even on the\nsimplest challenge, we observed average performance drops of 67%, 35%, 34%, and\n54%, respectively, when comparing skill performance with and without OSS.\nAdditionally, we investigate a potential solution to OSS that scales up the\ntraining data for each skill with a larger and more visually diverse set of\ndemonstrations, with our results showing it is not sufficient to resolve OSS.\nThe project page is: https://boss-benchmark.github.io/",
      "tldr_zh": "这篇论文介绍了 Observation Space Shift (OSS)，一种在长时任务中导致机器人层次化方法性能下降的问题，即前置技能执行改变观察空间，影响后续技能。作者提出 BOSS 基准，用于评估 OSS 的影响，包括三个挑战：\"Single Predicate Shift\"、\"Accumulated Predicate Shift\" 和 \"Skill Chaining\"。在实验中，评估了多种 Imitation Learning (IL) 算法，如 Behavioral Cloning 方法和 OpenVLA，结果显示即使在最简单挑战中，性能平均下降 34% 到 67%。此外，作者尝试通过增加技能的训练数据和视觉多样性来缓解 OSS，但证明这种方法不足以完全解决问题。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15679v1",
      "published_date": "2025-02-21 18:58:57 UTC",
      "updated_date": "2025-02-21 18:58:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:53:44.962312"
    },
    {
      "arxiv_id": "2502.15872v1",
      "title": "MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use",
      "title_zh": "翻译失败",
      "authors": [
        "Zaid Khan",
        "Ali Farhadi",
        "Ranjay Krishna",
        "Luca Weihs",
        "Mohit Bansal",
        "Tanmay Gupta"
      ],
      "abstract": "When a human requests an LLM to complete a coding task using functionality\nfrom a large code repository, how do we provide context from the repo to the\nLLM? One approach is to add the entire repo to the LLM's context window.\nHowever, most tasks involve only fraction of symbols from a repo, longer\ncontexts are detrimental to the LLM's reasoning abilities, and context windows\nare not unlimited. Alternatively, we could emulate the human ability to\nnavigate a large repo, pick out the right functionality, and form a plan to\nsolve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan\nSearch), an approach to search for plans that decompose a user request into\nnatural language steps grounded in the codebase. MutaGReP performs neural tree\nsearch in plan space, exploring by mutating plans and using a symbol retriever\nfor grounding. On the challenging LongCodeArena benchmark, our plans use less\nthan 5% of the 128K context window for GPT-4o but rival the coding performance\nof GPT-4o with a context window filled with the repo. Plans produced by\nMutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o\nwith full repo context and enable progress on the hardest LongCodeArena tasks.\nProject page: zaidkhan.me/MutaGReP",
      "tldr_zh": "该论文提出 MutaGReP，一种无需执行的基于代码仓库的计划搜索方法，用于为大型语言模型（LLM）提供高效上下文，解决编码任务中过度依赖完整仓库的问题。MutaGReP 通过神经树搜索（neural tree search）探索计划空间，采用计划变异（mutating plans）和符号检索器（symbol retriever）来分解用户请求为与代码库 grounding 的自然语言步骤，从而模拟人类导航仓库的行为。在 LongCodeArena 基准测试中，MutaGReP 的计划仅使用不到 5% 的 128K 上下文窗口，就实现了与 GPT-4o 使用完整仓库相当的编码性能，并使 Qwen 2.5 Coder 32B 和 72B 模型在最困难任务上取得显著进展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "Project page: zaidkhan.me/MutaGReP",
      "pdf_url": "http://arxiv.org/pdf/2502.15872v1",
      "published_date": "2025-02-21 18:58:17 UTC",
      "updated_date": "2025-02-21 18:58:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:53:58.350224"
    },
    {
      "arxiv_id": "2502.15677v1",
      "title": "FLEKE: Federated Locate-then-Edit Knowledge Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Zongkai Zhao",
        "Guozeng Xu",
        "Xiuhua Li",
        "Kaiwen Wei",
        "Jiang Zhong"
      ],
      "abstract": "Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.",
      "tldr_zh": "该论文提出 FLEKE（Federated Locate-then-Edit Knowledge Editing），一种新型联邦学习任务，用于解决传统 LEKE（Locate-then-Edit Knowledge Editing）在多客户端场景（如医院或金融机构）中的效率低下和隐私问题。FedEdit 框架采用两阶段方法：首先，客户端本地应用 LEKE 并上传 MKV（mediator knowledge vector）；其次，通过余弦相似度检索相关 MKV，实现知识重新编辑并最小化冗余计算。实验结果显示，FedEdit 在两个基准数据集上保留了非联邦 LEKE 的 96% 性能，比 FedAvg 基线高出约两倍，且 MEMIT 在该框架下比 PMET 表现更一致。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15677v1",
      "published_date": "2025-02-21 18:58:06 UTC",
      "updated_date": "2025-02-21 18:58:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:54:08.941523"
    },
    {
      "arxiv_id": "2502.15676v1",
      "title": "AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind",
      "title_zh": "翻译失败",
      "authors": [
        "Zhining Zhang",
        "Chuanyang Jin",
        "Mung Yao Jia",
        "Tianmin Shu"
      ],
      "abstract": "Theory of Mind (ToM), the ability to understand people's mental variables\nbased on their behavior, is key to developing socially intelligent agents.\nCurrent approaches to Theory of Mind reasoning either rely on prompting Large\nLanguage Models (LLMs), which are prone to systematic errors, or use rigid,\nhandcrafted Bayesian Theory of Mind (BToM) models, which are more robust but\ncannot generalize across different domains. In this work, we introduce AutoToM,\nan automated Bayesian Theory of Mind method for achieving open-ended machine\nTheory of Mind. AutoToM can operate in any domain, infer any mental variable,\nand conduct robust Theory of Mind reasoning of any order. Given a Theory of\nMind inference problem, AutoToM first proposes an initial BToM model. It then\nconducts automated Bayesian inverse planning based on the proposed model,\nleveraging an LLM as the backend. Based on the uncertainty of the inference, it\niteratively refines the model, by introducing additional mental variables\nand/or incorporating more timesteps in the context. Empirical evaluations\nacross multiple Theory of Mind benchmarks demonstrate that AutoToM consistently\nachieves state-of-the-art performance, offering a scalable, robust, and\ninterpretable approach to machine Theory of Mind.",
      "tldr_zh": "该研究提出 AutoToM，一种自动化 Bayesian Inverse Planning 和模型发现方法，旨在实现开放式的 Theory of Mind (ToM)，以解决现有方法（如依赖易出错的 LLMs 或刚性 BToM 模型）的局限性。AutoToM 通过初始提出 BToM 模型、利用 LLM 作为后端进行自动化逆向规划，并基于不确定性迭代精炼模型（如添加心理变量或更多时间步），从而支持任何领域的 ToM 推理。实验结果显示，AutoToM 在多个 ToM 基准测试中达到最先进性能，提供可扩展、鲁棒且可解释的社交智能代理解决方案。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 6 figures, 11 tables. Website at\n  https://chuanyangjin.com/AutoToM/",
      "pdf_url": "http://arxiv.org/pdf/2502.15676v1",
      "published_date": "2025-02-21 18:57:52 UTC",
      "updated_date": "2025-02-21 18:57:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:54:19.744934"
    },
    {
      "arxiv_id": "2502.15672v1",
      "title": "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Florent Bartoccioni",
        "Elias Ramzi",
        "Victor Besnier",
        "Shashanka Venkataramanan",
        "Tuan-Hung Vu",
        "Yihong Xu",
        "Loick Chambon",
        "Spyros Gidaris",
        "Serkan Odabas",
        "David Hurych",
        "Renaud Marlet",
        "Alexandre Boulch",
        "Mickael Chen",
        "Éloi Zablocki",
        "Andrei Bursuc",
        "Eduardo Valle",
        "Matthieu Cord"
      ],
      "abstract": "We explore the potential of large-scale generative video models for\nautonomous driving, introducing an open-source auto-regressive video model\n(VaViM) and its companion video-action model (VaVAM) to investigate how video\npre-training transfers to real-world driving. VaViM is a simple auto-regressive\nvideo model that predicts frames using spatio-temporal token sequences. We show\nthat it captures the semantics and dynamics of driving scenes. VaVAM, the\nvideo-action model, leverages the learned representations of VaViM to generate\ndriving trajectories through imitation learning. Together, the models form a\ncomplete perception-to-action pipeline. We evaluate our models in open- and\nclosed-loop driving scenarios, revealing that video-based pre-training holds\npromise for autonomous driving. Key insights include the semantic richness of\nthe learned representations, the benefits of scaling for video synthesis, and\nthe complex relationship between model size, data, and safety metrics in\nclosed-loop evaluations. We release code and model weights at\nhttps://github.com/valeoai/VideoActionModel",
      "tldr_zh": "本文提出VaViM和VaVAM两种模型，用于探索大规模生成视频模型在自动驾驶中的潜力，其中VaViM是一个开源的自回归视频模型，通过时空标记序列预测视频帧，以捕捉驾驶场景的语义和动态。VaVAM则基于VaViM的表示学习，通过模仿学习生成驾驶轨迹，形成一个完整的感知到动作的管道。在开放和闭环驾驶场景的评估中，模型显示视频预训练能显著提升自动驾驶性能，并揭示了学习表示的语义丰富性、视频合成中的规模益处，以及模型大小、数据与安全指标之间的复杂关系。作者发布了代码和模型权重，以推动相关研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Code and model: https://github.com/valeoai/VideoActionModel, project\n  page: https://valeoai.github.io/vavim-vavam/",
      "pdf_url": "http://arxiv.org/pdf/2502.15672v1",
      "published_date": "2025-02-21 18:56:02 UTC",
      "updated_date": "2025-02-21 18:56:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:54:32.822175"
    },
    {
      "arxiv_id": "2502.15666v2",
      "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing",
      "title_zh": "翻译失败",
      "authors": [
        "Shoumik Saha",
        "Soheil Feizi"
      ],
      "abstract": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Such classification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate twelve\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains 14.7K samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently flag even\nminimally polished text as AI-generated, struggle to differentiate between\ndegrees of AI involvement, and exhibit biases against older and smaller models.\nThese limitations highlight the urgent need for more nuanced detection\nmethodologies.",
      "tldr_zh": "随着大型语言模型 (LLMs) 的普及，AI-polished text（人类写作经AI微调的文本）检测已成为一个被忽略的挑战，可能导致错误标记为AI生成内容，并引发假冒剽窃指控。本研究使用自建的AI-Polished-Text Evaluation (APT-Eval)数据集（包含14.7K样本，覆盖不同AI参与水平）系统评估了12个最先进的AI文本检测器。结果显示，这些检测器经常将微调文本误判为AI生成，无法有效区分AI参与程度，并对较旧和较小模型存在偏见。这些发现强调了开发更细致、nuanced的检测方法迫在眉睫。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 18 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15666v2",
      "published_date": "2025-02-21 18:45:37 UTC",
      "updated_date": "2025-05-05 03:57:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:54:44.029512"
    },
    {
      "arxiv_id": "2502.15871v1",
      "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare",
      "title_zh": "大型语言模型在医疗保健中的可信度全面调查",
      "authors": [
        "Manar Aljohani",
        "Jun Hou",
        "Sindhura Kommu",
        "Xuan Wang"
      ],
      "abstract": "The application of large language models (LLMs) in healthcare has the\npotential to revolutionize clinical decision-making, medical research, and\npatient care. As LLMs are increasingly integrated into healthcare systems,\nseveral critical challenges must be addressed to ensure their reliable and\nethical deployment. These challenges include truthfulness, where models\ngenerate misleading information; privacy, with risks of unintentional data\nretention; robustness, requiring defenses against adversarial attacks;\nfairness, addressing biases in clinical outcomes; explainability, ensuring\ntransparent decision-making; and safety, mitigating risks of misinformation and\nmedical errors. Recently, researchers have begun developing benchmarks and\nevaluation frameworks to systematically assess the trustworthiness of LLMs.\nHowever, the trustworthiness of LLMs in healthcare remains underexplored,\nlacking a systematic review that provides a comprehensive understanding and\nfuture insights into this area. This survey bridges this gap by providing a\ncomprehensive overview of the recent research of existing methodologies and\nsolutions aimed at mitigating the above risks in healthcare. By focusing on key\ntrustworthiness dimensions including truthfulness, privacy and safety,\nrobustness, fairness and bias, and explainability, we present a thorough\nanalysis of how these issues impact the reliability and ethical use of LLMs in\nhealthcare. This paper highlights ongoing efforts and offers insights into\nfuture research directions to ensure the safe and trustworthy deployment of\nLLMs in healthcare.",
      "tldr_zh": "这篇调查论文全面审视了 Large Language Models (LLMs) 在医疗领域的可信度问题，包括 truthfulness（生成误导信息）、privacy（数据泄露风险）、robustness（对抗攻击防御）、fairness（偏见问题）、explainability（决策透明性）和 safety（误信息及医疗错误风险）。作者分析了现有方法和解决方案，用于缓解这些挑战，并总结了最近的基准和评估框架，以提升 LLMs 的可靠性和伦理应用。该研究强调了 LLMs 在医疗中的潜在影响，并为未来研究方向提供见解，以推动其安全部署。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15871v1",
      "published_date": "2025-02-21 18:43:06 UTC",
      "updated_date": "2025-02-21 18:43:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:54:55.340983"
    },
    {
      "arxiv_id": "2502.15663v1",
      "title": "Multi-Agent Architecture in Distributed Environment Control Systems: vision, challenges, and opportunities",
      "title_zh": "分布式环境控制系统中的多智能体架构：愿景、挑战和机遇",
      "authors": [
        "Natasha Astudillo",
        "Fernando Koch"
      ],
      "abstract": "The increasing demand for energy-efficient solutions in large-scale\ninfrastructure, particularly data centers, requires advanced control strategies\nto optimize environmental management systems. We propose a multi-agent\narchitecture for distributed control of air-cooled chiller systems in data\ncenters. Our vision employs autonomous agents to monitor and regulate local\noperational parameters and optimize system-wide efficiency. We demonstrate how\nthis approach improves the responsiveness, operational robustness, and energy\nefficiency of the system, contributing to the broader goal of sustainable\ninfrastructure management.",
      "tldr_zh": "本论文探讨了multi-agent architecture在分布式环境控制系统中的应用，针对数据中心等大规模基础设施的能源效率需求提出了一种新的框架。该框架使用自治代理监控和调节本地操作参数，同时优化系统整体效率，从而提升系统的响应性、操作鲁棒性和能源利用率。该方法为可持续基础设施管理提供了创新愿景，并指出了潜在挑战和机遇，如实现分布式协调和扩展应用。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "6 pages, 1 figure, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2502.15663v1",
      "published_date": "2025-02-21 18:41:03 UTC",
      "updated_date": "2025-02-21 18:41:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:55:07.069873"
    },
    {
      "arxiv_id": "2502.15662v1",
      "title": "Automating Curriculum Learning for Reinforcement Learning using a Skill-Based Bayesian Network",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent Hsiao",
        "Mark Roberts",
        "Laura M. Hiatt",
        "George Konidaris",
        "Dana Nau"
      ],
      "abstract": "A major challenge for reinforcement learning is automatically generating\ncurricula to reduce training time or improve performance in some target task.\nWe introduce SEBNs (Skill-Environment Bayesian Networks) which model a\nprobabilistic relationship between a set of skills, a set of goals that relate\nto the reward structure, and a set of environment features to predict policy\nperformance on (possibly unseen) tasks. We develop an algorithm that uses the\ninferred estimates of agent success from SEBN to weigh the possible next tasks\nby expected improvement. We evaluate the benefit of the resulting curriculum on\nthree environments: a discrete gridworld, continuous control, and simulated\nrobotics. The results show that curricula constructed using SEBN frequently\noutperform other baselines.",
      "tldr_zh": "这篇论文针对强化学习中的课程学习挑战，提出了一种基于技能的贝叶斯网络(SEBNs)，用于建模技能、目标（与奖励结构相关）和环境特征之间的概率关系，以预测代理在未知任务上的策略性能。研究开发了一种算法，利用 SEBNs 的成功估计来根据预期改进权衡下一个任务的优先级，从而自动生成优化课程。在离散网格世界、连续控制和模拟机器人等环境中，SEBN 生成的课程经常优于其他基线方法，展示了其在减少训练时间和提升性能方面的潜力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15662v1",
      "published_date": "2025-02-21 18:38:00 UTC",
      "updated_date": "2025-02-21 18:38:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:55:19.511619"
    },
    {
      "arxiv_id": "2502.15870v1",
      "title": "Making Sense of AI Limitations: How Individual Perceptions Shape Organizational Readiness for AI Adoption",
      "title_zh": "理解 AI 局限性：个体感知如何塑造组织对 AI 采用的准备度",
      "authors": [
        "Thomas Übellacker"
      ],
      "abstract": "This study investigates how individuals' perceptions of artificial\nintelligence (AI) limitations influence organizational readiness for AI\nadoption. Through semi-structured interviews with seven AI implementation\nexperts, analyzed using the Gioia methodology, the research reveals that\norganizational readiness emerges through dynamic interactions between\nindividual sensemaking, social learning, and formal integration processes. The\nfindings demonstrate that hands-on experience with AI limitations leads to more\nrealistic expectations and increased trust, mainly when supported by peer\nnetworks and champion systems. Organizations that successfully translate these\nindividual and collective insights into formal governance structures achieve\nmore sustainable AI adoption. The study advances theory by showing how\norganizational readiness for AI adoption evolves through continuous cycles of\nindividual understanding, social learning, and organizational adaptation. These\ninsights suggest that organizations should approach AI adoption not as a\none-time implementation but as an ongoing strategic learning process that\nbalances innovation with practical constraints. The research contributes to\norganizational readiness theory and practice by illuminating how micro-level\nperceptions and experiences shape macro-level adoption outcomes.",
      "tldr_zh": "这篇研究探讨了个体对AI限制的感知如何影响组织对AI采用的准备度，通过对七位AI实施专家的半结构化访谈和Gioia methodology分析。研究发现，组织准备度通过individual sensemaking、社会学习和正式整合过程的动态互动形成，亲身经历AI限制可提升现实期望和信任，尤其在peer networks和champion systems的支持下。结果显示，成功地将这些个体和集体洞见转化为正式治理结构，能实现更可持续的AI采用。研究推进了organizational readiness theory，强调AI采用应视为持续的战略学习过程，而非一次性实施，并阐明微观层面的感知和经历如何塑造宏观层面的采用结果。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15870v1",
      "published_date": "2025-02-21 18:31:08 UTC",
      "updated_date": "2025-02-21 18:31:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:55:33.200587"
    },
    {
      "arxiv_id": "2502.15657v2",
      "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
      "title_zh": "翻译失败",
      "authors": [
        "Yoshua Bengio",
        "Michael Cohen",
        "Damiano Fornasiere",
        "Joumana Ghosn",
        "Pietro Greiner",
        "Matt MacDermott",
        "Sören Mindermann",
        "Adam Oberman",
        "Jesse Richardson",
        "Oliver Richardson",
        "Marc-Antoine Rondeau",
        "Pierre-Luc St-Charles",
        "David Williams-King"
      ],
      "abstract": "The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path.",
      "tldr_zh": "该论文警告称，超级智能AI代理可能带来灾难性风险，如被恶意利用或导致人类失去控制，这些风险源于当前AI训练方法可能导致AI欺骗或追求与人类利益冲突的目标。为此，作者提出Scientist AI作为更安全的非代理AI替代方案，该系统由世界模型（用于生成解释数据的理论）和问答推理机组成，并通过处理不确定性来避免过度自信的预测。Scientist AI可协助人类研究者加速科学进步，包括AI安全领域，并作为防范AI代理的防护措施，最终实现AI创新的益处而规避潜在风险。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "v2 with fixed formatting for URLs and hyperlinks",
      "pdf_url": "http://arxiv.org/pdf/2502.15657v2",
      "published_date": "2025-02-21 18:28:36 UTC",
      "updated_date": "2025-02-24 18:14:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:55:44.077063"
    },
    {
      "arxiv_id": "2502.15652v2",
      "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Fengxiang Cheng",
        "Haoxuan Li",
        "Fenrong Liu",
        "Robert van Rooij",
        "Kun Zhang",
        "Zhouchen Lin"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.",
      "tldr_zh": "这篇论文对大型语言模型（LLMs）的逻辑推理能力进行了全面调查，重点总结了两个主要挑战：（1）在逻辑问答中，LLMs 难以正确处理复杂问题，如演绎、归纳或溯因推理；（2）逻辑一致性问题，导致模型在不同问题上产生矛盾响应，例如在鸟类特征的连续问题上给出不一致答案。为解决这些问题，论文分类了先进方法，包括依赖外部求解器、提示、预训练和微调的策略，以及针对蕴涵、否定、传递性和事实一致性的解决方案；同时，回顾了常用基准数据集和评估指标，并提出未来方向，如扩展到模态逻辑和高效的多一致性算法。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15652v2",
      "published_date": "2025-02-21 18:20:35 UTC",
      "updated_date": "2025-02-24 19:01:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:55:55.596796"
    },
    {
      "arxiv_id": "2502.15643v1",
      "title": "AutoTandemML: Active Learning Enhanced Tandem Neural Networks for Inverse Design Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Luka Grbcic",
        "Juliane Müller",
        "Wibe Albert de Jong"
      ],
      "abstract": "Inverse design in science and engineering involves determining optimal design\nparameters that achieve desired performance outcomes, a process often hindered\nby the complexity and high dimensionality of design spaces, leading to\nsignificant computational costs. To tackle this challenge, we propose a novel\nhybrid approach that combines active learning with Tandem Neural Networks to\nenhance the efficiency and effectiveness of solving inverse design problems.\nActive learning allows to selectively sample the most informative data points,\nreducing the required dataset size without compromising accuracy. We\ninvestigate this approach using three benchmark problems: airfoil inverse\ndesign, photonic surface inverse design, and scalar boundary condition\nreconstruction in diffusion partial differential equations. We demonstrate that\nintegrating active learning with Tandem Neural Networks outperforms standard\napproaches across the benchmark suite, achieving better accuracy with fewer\ntraining samples.",
      "tldr_zh": "该研究针对科学和工程中逆设计问题(Inverse Design Problems)的复杂性和高维度挑战，提出了一种新型混合方法AutoTandemML，将主动学习(Active Learning)与Tandem Neural Networks相结合，以提高效率和准确性。主动学习通过选择最具信息量的样本点，显著减少所需数据集大小，同时维持模型性能。在翼型逆设计、光子表面逆设计和扩散偏微分方程的标量边界条件重建等三个基准问题上，实验结果显示，该方法优于标准方法，使用更少的训练样本就实现了更高的准确率。这为逆设计领域的计算优化提供了有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15643v1",
      "published_date": "2025-02-21 18:10:56 UTC",
      "updated_date": "2025-02-21 18:10:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:56:09.417489"
    },
    {
      "arxiv_id": "2502.15639v1",
      "title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Anirudh Sundar",
        "Sinead Williamson",
        "Katherine Metcalf",
        "Barry-John Theobald",
        "Skyler Seto",
        "Masha Fedzechkina"
      ],
      "abstract": "Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval.",
      "tldr_zh": "本文研究了通过模型干预（model interventions）在多语言大语言模型（mLLMs）中增强跨语言对齐（cross-lingual alignment），以提高跨语言任务的性能，而无需昂贵的微调过程。作者分析了干预方法（如 finding experts），发现它能改变 mLLMs 的嵌入空间（embedding space），从而改善语言表示的协调性。实验结果显示，这种干预在跨语言检索任务上使 top-1 准确率最高提升 2 倍，为数据高效的模型优化提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.15639v1",
      "published_date": "2025-02-21 18:09:54 UTC",
      "updated_date": "2025-02-21 18:09:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:56:19.932920"
    },
    {
      "arxiv_id": "2502.15637v1",
      "title": "Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Vasilii Feofanov",
        "Songkang Wen",
        "Marius Alonso",
        "Romain Ilbert",
        "Hongbo Guo",
        "Malik Tiomoko",
        "Lujia Pan",
        "Jianfeng Zhang",
        "Ievgen Redko"
      ],
      "abstract": "In recent years, there has been increasing interest in developing foundation\nmodels for time series data that can generalize across diverse downstream\ntasks. While numerous forecasting-oriented foundation models have been\nintroduced, there is a notable scarcity of models tailored for time series\nclassification. To address this gap, we present Mantis, a new open-source\nfoundation model for time series classification based on the Vision Transformer\n(ViT) architecture that has been pre-trained using a contrastive learning\napproach. Our experimental results show that Mantis outperforms existing\nfoundation models both when the backbone is frozen and when fine-tuned, while\nachieving the lowest calibration error. In addition, we propose several\nadapters to handle the multivariate setting, reducing memory requirements and\nmodeling channel interdependence.",
      "tldr_zh": "本研究引入了Mantis，一种轻量级且校准良好的开源基础模型（foundation model），旨在简化时间序列分类（time series classification）任务并提升用户友好性。Mantis基于Vision Transformer (ViT)架构，通过对比学习（contrastive learning）进行预训练，在实验中超越现有模型，无论在冻结主干网络还是微调情况下均表现出色，并实现了最低的校准错误（calibration error）。此外，该模型提出了几种适配器（adapters）来处理多变量设置，减少内存需求并建模通道间依赖性，从而为实际应用提供高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15637v1",
      "published_date": "2025-02-21 18:06:09 UTC",
      "updated_date": "2025-02-21 18:06:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:56:31.333747"
    },
    {
      "arxiv_id": "2502.15631v1",
      "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
      "title_zh": "翻译失败",
      "authors": [
        "Marthe Ballon",
        "Andres Algaba",
        "Vincent Ginis"
      ],
      "abstract": "Large language models have demonstrated remarkable progress in mathematical\nreasoning, leveraging chain-of-thought and test-time compute scaling. However,\nmany open questions remain regarding the interplay between reasoning token\nusage and accuracy gains. In particular, when comparing models across\ngenerations, it is unclear whether improved performance results from longer\nreasoning chains or more efficient reasoning. We systematically analyze\nchain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH\nbenchmark, finding that o3-mini (m) achieves superior accuracy without\nrequiring longer reasoning chains than o1-mini. Moreover, we show that accuracy\ngenerally declines as reasoning chains grow across all models and compute\nsettings, even when controlling for difficulty of the questions. This accuracy\ndrop is significantly smaller in more proficient models, suggesting that new\ngenerations of reasoning models use test-time compute more effectively.\nFinally, we highlight that while o3-mini (h) achieves a marginal accuracy gain\nover o3-mini (m), it does so by allocating substantially more reasoning tokens\nacross all problems, even the ones that o3-mini (m) can already solve. These\nfindings provide new insights into the relationship between model capability\nand reasoning length, with implications for efficiency, scaling, and evaluation\nmethodologies.",
      "tldr_zh": "本研究探讨了大语言模型(Large Language Models)中推理长度与性能的关系，特别比较了 o1-mini 和 o3-mini 模型在 Omni-MATH 基准上的表现。研究发现，o3-mini 实现了更高的准确率，而无需采用更长的 chain-of-thought 推理链；相反，准确率通常随推理链增长而下降，即使控制问题难度，但更先进的模型（如 o3-mini）对这种下降更具抵抗力。最终，o3-mini (h) 仅通过分配更多推理标记就获得了微小准确率提升，这些结果为模型效率、scaling 和评估方法提供了新颖的启示。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15631v1",
      "published_date": "2025-02-21 17:59:13 UTC",
      "updated_date": "2025-02-21 17:59:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:56:45.926959"
    },
    {
      "arxiv_id": "2502.15623v1",
      "title": "Dynamic Knowledge Selector and Evaluator for recommendation with Knowledge Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Feng Xia",
        "Zhifei Hu"
      ],
      "abstract": "In recent years recommendation systems typically employ the edge information\nprovided by knowledge graphs combined with the advantages of high-order\nconnectivity of graph networks in the recommendation field. However, this\nmethod is limited by the sparsity of labels, cannot learn the graph structure\nwell, and a large number of noisy entities in the knowledge graph will affect\nthe accuracy of the recommendation results. In order to alleviate the above\nproblems, we propose a dynamic knowledge-selecting and evaluating method guided\nby collaborative signals to distill information in the knowledge graph.\nSpecifically, we use a Chain Route Evaluator to evaluate the contributions of\ndifferent neighborhoods for the recommendation task and employ a Knowledge\nSelector strategy to filter the less informative knowledge before evaluating.\nWe conduct baseline model comparison and experimental ablation evaluations on\nthree public datasets. The experiments demonstrate that our proposed model\noutperforms current state-of-the-art baseline models, and each modules\neffectiveness in our model is demonstrated through ablation experiments.",
      "tldr_zh": "该研究针对推荐系统中知识图谱（Knowledge Graph）的标签稀疏性和噪声实体问题，提出了一种动态知识选择和评估方法，由协作信号引导，以提炼有用信息。具体而言，该方法使用 Knowledge Selector 策略先过滤不重要的知识，然后通过 Chain Route Evaluator 评估不同邻域对推荐任务的贡献，从而提高推荐准确性。在三个公开数据集上的实验表明，该模型优于现有最先进基线模型，且通过消融实验验证了各模块的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15623v1",
      "published_date": "2025-02-21 17:51:37 UTC",
      "updated_date": "2025-02-21 17:51:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:56:54.494383"
    },
    {
      "arxiv_id": "2502.15620v1",
      "title": "Paradigms of AI Evaluation: Mapping Goals, Methodologies and Culture",
      "title_zh": "AI 评估的范式：映射目标、方法论和文化",
      "authors": [
        "John Burden",
        "Marko Tešić",
        "Lorenzo Pacchiardi",
        "José Hernández-Orallo"
      ],
      "abstract": "Research in AI evaluation has grown increasingly complex and\nmultidisciplinary, attracting researchers with diverse backgrounds and\nobjectives. As a result, divergent evaluation paradigms have emerged, often\ndeveloping in isolation, adopting conflicting terminologies, and overlooking\neach other's contributions. This fragmentation has led to insular research\ntrajectories and communication barriers both among different paradigms and with\nthe general public, contributing to unmet expectations for deployed AI systems.\nTo help bridge this insularity, in this paper we survey recent work in the AI\nevaluation landscape and identify six main paradigms. We characterise major\nrecent contributions within each paradigm across key dimensions related to\ntheir goals, methodologies and research cultures. By clarifying the unique\ncombination of questions and approaches associated with each paradigm, we aim\nto increase awareness of the breadth of current evaluation approaches and\nfoster cross-pollination between different paradigms. We also identify\npotential gaps in the field to inspire future research directions.",
      "tldr_zh": "这项研究探讨了AI evaluation领域的碎片化问题，指出不同评估范式因孤立发展、术语冲突和相互忽略而导致研究轨迹分离和沟通障碍，从而影响AI系统的部署预期。该论文通过文献调查，识别出六个主要paradigms，并从目标、methodologies和research cultures等关键维度描述了每个范式的独特贡献，以提高对评估方法的整体认识并促进跨paradigms的交叉授粉。最后，研究还指出了领域中的潜在空白，为未来的AI evaluation研究提供方向。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15620v1",
      "published_date": "2025-02-21 17:44:05 UTC",
      "updated_date": "2025-02-21 17:44:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:57:06.890201"
    },
    {
      "arxiv_id": "2502.15619v1",
      "title": "Extraction multi-étiquettes de relations en utilisant des couches de Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Ngoc Luyen Le",
        "Gildas Tagny Ngompé"
      ],
      "abstract": "In this article, we present the BTransformer18 model, a deep learning\narchitecture designed for multi-label relation extraction in French texts. Our\napproach combines the contextual representation capabilities of pre-trained\nlanguage models from the BERT family - such as BERT, RoBERTa, and their French\ncounterparts CamemBERT and FlauBERT - with the power of Transformer encoders to\ncapture long-term dependencies between tokens. Experiments conducted on the\ndataset from the TextMine'25 challenge show that our model achieves superior\nperformance, particularly when using CamemBERT-Large, with a macro F1 score of\n0.654, surpassing the results obtained with FlauBERT-Large. These results\ndemonstrate the effectiveness of our approach for the automatic extraction of\ncomplex relations in intelligence reports.",
      "tldr_zh": "这篇论文提出了 BTransformer18 模型，用于法语文本的多标签关系提取（multi-label relation extraction）。该模型结合了 BERT 家族的预训练语言模型（如 BERT、RoBERTa、CamemBERT 和 FlauBERT）以及 Transformer 编码器，以捕捉 tokens 之间的长期依赖。实验在 TextMine'25 挑战数据集上进行，使用 CamemBERT-Large 时，宏 F1 score 达到 0.654，优于 FlauBERT-Large。这些结果证明了该方法在自动提取智能报告中复杂关系方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "in French language",
      "pdf_url": "http://arxiv.org/pdf/2502.15619v1",
      "published_date": "2025-02-21 17:42:51 UTC",
      "updated_date": "2025-02-21 17:42:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:57:20.990119"
    },
    {
      "arxiv_id": "2502.15618v1",
      "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Le",
        "Enmao Diao",
        "Ziyan Wang",
        "Xinran Wang",
        "Jie Ding",
        "Li Yang",
        "Ali Anwar"
      ],
      "abstract": "We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.",
      "tldr_zh": "我们提出 Probe Pruning (PP)，一个用于加速大型语言模型 (LLMs) 的在线动态结构化剪枝框架，通过模型探测来识别每个批次的关键权重，实现针对不同样本的个性化剪枝。PP 框架包括三个主要阶段：探测阶段（基于剩余重要性选择少量隐藏状态并运行模型层）、历史信息整合剪枝阶段（融合探测和历史状态后计算 PP 重要性分数进行权重剪枝），以及完整推理阶段。实验在 LLaMA-2/3 和 OPT 模型上显示，即使使用仅 1.5% 的 FLOPs 进行最小探测，也能显著提升剪枝效率，例如在 LLaMA-2-7B 上，与最先进方法相比，在 40% 剪枝比例下，性能下降与运行时间减少的比率降低了 2.56 倍。PP 的关键优势在于无需额外神经网络模块或微调，便于与现有模型兼容。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15618v1",
      "published_date": "2025-02-21 17:41:21 UTC",
      "updated_date": "2025-02-21 17:41:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:57:34.755196"
    },
    {
      "arxiv_id": "2502.15616v1",
      "title": "Pastiche Novel Generation Creating: Fan Fiction You Love in Your Favorite Author's Style",
      "title_zh": "翻译失败",
      "authors": [
        "Xueran Han",
        "Yuhan Liu",
        "Mingzhe Li",
        "Wei Liu",
        "Sen Hu",
        "Rui Yan",
        "Zhiqiang Xu",
        "Xiuying Chen"
      ],
      "abstract": "Great novels create immersive worlds with rich character arcs,\nwell-structured plots, and nuanced writing styles. However, current novel\ngeneration methods often rely on brief, simplistic story outlines and generate\ndetails using plain, generic language. To bridge this gap, we introduce the\ntask of Pastiche Novel Generation, which requires the generated novels to\nimitate the distinctive features of the original work, including understanding\ncharacter profiles, predicting plausible plot developments, and writing\nconcrete details using vivid, expressive language. To achieve this, we propose\nWriterAgent, a novel generation system designed to master the core aspects of\nliterary pastiche. WriterAgent is trained through a curriculum learning\nparadigm, progressing from low-level stylistic mastery to high-level narrative\ncoherence. Its key tasks include language style learning, character modeling,\nplot planning, and stylish writing, ensuring comprehensive narrative control.\nTo support this, WriterAgent leverages the WriterLoRA framework, an extension\nof LoRA with hierarchical and cumulative task-specific modules, each\nspecializing in a different narrative aspect. We evaluate WriterAgent on\nmultilingual classics like Harry Potter and Dream of the Red Chamber,\ndemonstrating its superiority over baselines in capturing the target author's\nsettings, character dynamics, and writing style to produce coherent, faithful\nnarratives.",
      "tldr_zh": "这篇论文引入了 Pastiche Novel Generation 任务，旨在生成模仿原作风格的粉丝小说，包括理解人物轮廓、预测情节发展和使用生动语言。作者提出 WriterAgent 系统，通过 curriculum learning 范式训练，从语言风格学习到叙事连贯性，涵盖人物建模、情节规划和风格化写作。WriterAgent 基于 WriterLoRA 框架扩展 LoRA，利用分层任务模块提升生成质量；在《哈利·波特》和《红楼梦》等多语言经典作品的评估中，WriterAgent 优于基线模型，在捕捉作者设定、人物动态和写作风格方面表现出显著优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15616v1",
      "published_date": "2025-02-21 17:40:42 UTC",
      "updated_date": "2025-02-21 17:40:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:57:44.301088"
    },
    {
      "arxiv_id": "2502.15610v2",
      "title": "A general language model for peptide identification",
      "title_zh": "一种用于肽鉴定的通用语言模型",
      "authors": [
        "Jixiu Zhai",
        "Tianchi Lu",
        "Haitian Zhong",
        "Ziyang Xu",
        "Yuhuan Liu",
        "Shengrui Xu",
        "Jingwan Wang",
        "Dan Huang"
      ],
      "abstract": "Advances in peptide identification are revolutionizing our ability to\ndecipher protein functions and accelerate therapeutic discovery. We present\nPDeepPP, a deep learning framework that integrates pretrained protein language\nmodels with parallel transformer-CNN architectures, achieving state-of-the-art\nperformance in peptide characterization tasks. The model's hybrid architecture\ndemonstrates unique capabilities in capturing both local sequence motifs and\nglobal structural features, as evidenced by 29% improved cluster separation in\nUMAP visualizations compared to conventional approaches. Evaluated across 33\nbiological recognition tasks - including post-translational modification site\nprediction and bioactive peptide identification - PDeepPP outperformed existing\nmethods in 25 tasks with average AUC improvements of 4.2%. Notably, it achieved\n0.9726 accuracy with PR AUC 0.9977 in antimicrobial peptide detection while\nreducing false negatives by 37.5% in antimalarial recognition scenarios. This\nframework enables accurate large-scale peptide analysis, achieving 218*\nacceleration over sequence-alignment-based methods while maintaining 99.5%\nspecificity in critical glycosylation site detection.PDeepPP establishes a new\nparadigm for computational peptide analysis through its synergistic\narchitecture design, enabling rapid yet precise functional annotation that\nbridges molecular pattern recognition with translational biomedical\napplications.We have made our implementation, including code, data, and\npretrained models, publicly available via GitHub\n(https://github.com/fondress/PDeepPP) and Hugging Face\n(https://huggingface.co/fondress/PDeppPP).",
      "tldr_zh": "本研究提出PDeepPP，一种深度学习框架，将预训练蛋白质语言模型与并行Transformer-CNN架构相结合，用于肽鉴定任务，从而捕捉局部序列基序和全局结构特征。相比传统方法，PDeepPP在33个生物识别任务中表现出色，在25个任务中胜出，平均AUC提升4.2%，并在抗菌肽检测中达到0.9726准确率和0.9977 PR AUC，同时在抗疟疾识别中减少37.5%的假阴性。框架实现了218倍的加速，同时保持99.5%的特异性，并通过开源代码和预训练模型（如GitHub和Hugging Face）促进了计算肽分析的实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "92C40, 68T07",
        "I.2.6; J.3"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 9 figures, 4 tables, submitted to arXiv",
      "pdf_url": "http://arxiv.org/pdf/2502.15610v2",
      "published_date": "2025-02-21 17:31:22 UTC",
      "updated_date": "2025-04-17 17:52:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:57:56.482073"
    },
    {
      "arxiv_id": "2502.15609v1",
      "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Tianle Li",
        "Chenyang Zhang",
        "Xingwu Chen",
        "Yuan Cao",
        "Difan Zou"
      ],
      "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures.",
      "tldr_zh": "本研究探讨了Transformer-based Large Language Models (LLMs) 在线性分类任务中对context hijacking的鲁棒性问题，即模型预测被事实正确的上下文干扰。作者通过一个基于线性Transformers的in-context linear classification设置进行理论分析，其中上下文tokens设计为与最终查询类似但标签相反的查询-答案对，并将鲁棒性建模为模型深度、训练上下文长度和hijacking tokens数量的函数。关键发现是，训练良好的更深层Transformer具有更高的鲁棒性，因为更深层允许更精细的优化步骤，从而有效减轻干扰；实验结果支持这一结论。该工作为理解Transformer架构提供了重要理论洞见，并有助于提升模型的鲁棒性设计。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15609v1",
      "published_date": "2025-02-21 17:31:00 UTC",
      "updated_date": "2025-02-21 17:31:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:58:07.624701"
    },
    {
      "arxiv_id": "2502.15603v1",
      "title": "Do Multilingual LLMs Think In English?",
      "title_zh": "多语言 LLMs 是否以英语思考？",
      "authors": [
        "Lisa Schut",
        "Yarin Gal",
        "Sebastian Farquhar"
      ],
      "abstract": "Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users.",
      "tldr_zh": "该研究探讨了多语言大语言模型 (LLMs) 的内部机制，发现这些模型在处理多种语言任务时，主要决策发生在最接近英语的表示空间中，无论输入或输出语言为何。通过 logit lens 分析法语、德语、荷兰语和普通话的句子，研究者观察到 LLMs 先为语义负载词生成接近英语的表示，然后再翻译到目标语言。此外，activation steering 操作在英语计算的引导向量上更有效，这表明多语言 LLMs 的关键推理步骤深受英语影响，导致模型行为对用户不透明。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Main paper 9 pages; including appendix 48 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.15603v1",
      "published_date": "2025-02-21 17:19:23 UTC",
      "updated_date": "2025-02-21 17:19:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:58:19.726009"
    },
    {
      "arxiv_id": "2502.15602v2",
      "title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yoonjin Chung",
        "Pilsun Eu",
        "Junwon Lee",
        "Keunwoo Choi",
        "Juhan Nam",
        "Ben Sangbae Chon"
      ],
      "abstract": "Although being widely adopted for evaluating generated audio signals, the\nFr\\'echet Audio Distance (FAD) suffers from significant limitations, including\nreliance on Gaussian assumptions, sensitivity to sample size, and high\ncomputational complexity. As an alternative, we introduce the Kernel Audio\nDistance (KAD), a novel, distribution-free, unbiased, and computationally\nefficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and\nempirical validation, we demonstrate KAD's advantages: (1) faster convergence\nwith smaller sample sizes, enabling reliable evaluation with limited data; (2)\nlower computational cost, with scalable GPU acceleration; and (3) stronger\nalignment with human perceptual judgments. By leveraging advanced embeddings\nand characteristic kernels, KAD captures nuanced differences between real and\ngenerated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient,\nreliable, and perceptually aligned benchmark for evaluating generative audio\nmodels.",
      "tldr_zh": "本文指出了 Fréchet Audio Distance (FAD) 在音频生成评估中的局限性，如依赖高斯假设、对样本大小敏感和高计算复杂度，并提出了一种新型指标 Kernel Audio Distance (KAD)，基于 Maximum Mean Discrepancy (MMD)，实现分布无关、无偏和高效评估。KAD 的优势包括更快收敛（适用于小样本数据）、更低计算成本（支持 GPU 加速）和更强的人类感知对齐，通过高级嵌入和特征内核捕捉真实与生成音频的细微差异。最终，KAD 在 kadtk 工具包中开源，提供了一个可靠的基准，用于评估生成音频模型的性能。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15602v2",
      "published_date": "2025-02-21 17:19:15 UTC",
      "updated_date": "2025-03-09 06:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:58:32.342966"
    },
    {
      "arxiv_id": "2502.15601v2",
      "title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Xinhang Liu",
        "Chi-Keung Tang",
        "Yu-Wing Tai"
      ],
      "abstract": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life.",
      "tldr_zh": "我们介绍了 WorldCraft 系统，这是一个基于 LLM Agents 的框架，利用程序生成（Procedural Generation）技术，让非专业人士通过自然语言命令创建和自定义光照真实的三维室内和室外场景。系统包括协调器代理、ForgeIt（负责精确对象自定义）和 ArrangeIt（通过层次化优化处理布局的 ergonomic 和 aesthetic 考虑），并支持轨迹控制代理以实现场景动画和相机操作。WorldCraft 兼容现成的深度 3D 生成器，并在评估中展示了其灵活性，从单个对象自定义到复杂场景设计均优于现有方法，从而 democratize 了三维世界创建过程。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15601v2",
      "published_date": "2025-02-21 17:18:30 UTC",
      "updated_date": "2025-02-28 08:49:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:58:44.700319"
    },
    {
      "arxiv_id": "2502.15592v1",
      "title": "Generalizing From Short to Long: Effective Data Synthesis for Long-Context Instruction Tuning",
      "title_zh": "从短到长的泛化：用于长上下文指令调整的有效数据合成",
      "authors": [
        "Wenhao Zhu",
        "Pinzhen Chen",
        "Hanxu Hu",
        "Shujian Huang",
        "Fei Yuan",
        "Jiajun Chen",
        "Alexandra Birch"
      ],
      "abstract": "Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）的长上下文指令微调问题，发现基于短上下文微调的模型能够有效泛化到长上下文，同时强调指令难度和上下文组成等关键因素。作者提出context synthesis框架，使用现成LLMs生成扩展背景上下文，以增强高质量指令-答案对的合成。实验结果显示，该方法在LongBench基准上优于现有指令合成方法，并接近人类标注数据的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15592v1",
      "published_date": "2025-02-21 17:02:40 UTC",
      "updated_date": "2025-02-21 17:02:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:58:56.005094"
    },
    {
      "arxiv_id": "2502.15869v1",
      "title": "Generative AI Framework for 3D Object Generation in Augmented Reality",
      "title_zh": "翻译失败",
      "authors": [
        "Majid Behravan"
      ],
      "abstract": "This thesis presents a framework that integrates state-of-the-art generative\nAI models for real-time creation of three-dimensional (3D) objects in augmented\nreality (AR) environments. The primary goal is to convert diverse inputs, such\nas images and speech, into accurate 3D models, enhancing user interaction and\nimmersion. Key components include advanced object detection algorithms,\nuser-friendly interaction techniques, and robust AI models like Shap-E for 3D\ngeneration. Leveraging Vision Language Models (VLMs) and Large Language Models\n(LLMs), the system captures spatial details from images and processes textual\ninformation to generate comprehensive 3D objects, seamlessly integrating\nvirtual objects into real-world environments. The framework demonstrates\napplications across industries such as gaming, education, retail, and interior\ndesign. It allows players to create personalized in-game assets, customers to\nsee products in their environments before purchase, and designers to convert\nreal-world objects into 3D models for real-time visualization. A significant\ncontribution is democratizing 3D model creation, making advanced AI tools\naccessible to a broader audience, fostering creativity and innovation. The\nframework addresses challenges like handling multilingual inputs, diverse\nvisual data, and complex environments, improving object detection and model\ngeneration accuracy, as well as loading 3D models in AR space in real-time. In\nconclusion, this thesis integrates generative AI and AR for efficient 3D model\ngeneration, enhancing accessibility and paving the way for innovative\napplications and improved user interactions in AR environments.",
      "tldr_zh": "本论文提出一个Generative AI框架，用于在Augmented Reality (AR)环境中实时生成3D对象，将图像、语音等多样输入转换为准确的3D模型，从而提升用户交互和沉浸感。框架的关键组件包括先进的物体检测算法、用户友好交互技术，以及Shap-E、Vision Language Models (VLMs)和Large Language Models (LLMs)，这些元素共同处理空间细节、文本信息和多语言输入，解决了复杂环境下的生成挑战。实验应用覆盖游戏、教育、零售和室内设计等领域，显著提高了物体检测准确性和实时加载效率，并通过民主化3D模型创建，促进了更广泛的创新和用户互动。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15869v1",
      "published_date": "2025-02-21 17:01:48 UTC",
      "updated_date": "2025-02-21 17:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:59:09.103493"
    },
    {
      "arxiv_id": "2502.15589v1",
      "title": "LightThinker: Thinking Step-by-Step Compression",
      "title_zh": "LightThinker: 逐步思考压缩",
      "authors": [
        "Jintian Zhang",
        "Yuqi Zhu",
        "Mengshu Sun",
        "Yujie Luo",
        "Shuofei Qiao",
        "Lun Du",
        "Da Zheng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.",
      "tldr_zh": "本文提出 LightThinker，一种新型方法，用于动态压缩大型语言模型(LLMs)在复杂推理任务中的中间思考步骤，旨在减少内存和计算成本。受人类认知过程启发，该方法通过数据构建训练模型何时和如何压缩冗长思考，将隐藏状态映射到紧凑的 gist tokens，并使用专门的注意力掩码实现高效处理。同时，引入 Dependency (Dep) 指标量化对历史 tokens 的依赖度。实验在四个数据集和两个模型上表明，LightThinker 显著降低了峰值内存使用和推理时间，同时保持了竞争性的准确率，为提升 LLMs 效率提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15589v1",
      "published_date": "2025-02-21 16:57:22 UTC",
      "updated_date": "2025-02-21 16:57:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:59:22.645538"
    },
    {
      "arxiv_id": "2502.15588v1",
      "title": "Improving the Scaling Laws of Synthetic Data with Deliberate Practice",
      "title_zh": "翻译失败",
      "authors": [
        "Reyhane Askari-Hemmat",
        "Mohammad Pezeshki",
        "Elvis Dohmatob",
        "Florian Bordes",
        "Pietro Astolfi",
        "Melissa Hall",
        "Jakob Verbeek",
        "Michal Drozdzal",
        "Adriana Romero-Soriano"
      ],
      "abstract": "Inspired by the principle of deliberate practice in human learning, we\npropose Deliberate Practice for Synthetic Data Generation (DP), a novel\nframework that improves sample efficiency through dynamic synthetic data\ngeneration. Prior work has shown that scaling synthetic data is inherently\nchallenging, as naively adding new data leads to diminishing returns. To\naddress this, pruning has been identified as a key mechanism for improving\nscaling, enabling models to focus on the most informative synthetic samples.\nRather than generating a large dataset and pruning it afterward, DP efficiently\napproximates the direct generation of informative samples. We theoretically\nshow how training on challenging, informative examples improves scaling laws\nand empirically validate that DP achieves better scaling performance with\nsignificantly fewer training samples and iterations. On ImageNet-100, DP\ngenerates 3.4x fewer samples and requires six times fewer iterations, while on\nImageNet-1k, it generates 8x fewer samples with a 30 percent reduction in\niterations, all while achieving superior performance compared to prior work.",
      "tldr_zh": "该论文提出了一种名为 Deliberate Practice for Synthetic Data Generation (DP) 的框架，受人类学习中 deliberate practice 原则启发，通过动态生成有信息量的合成数据来提高样本效率。不同于传统方法先生成大量数据再进行修剪，DP 直接高效地生成挑战性样本，从而改善 synthetic data 的 scaling laws。理论分析表明，在信息丰富的示例上训练能提升模型性能，实验验证显示 DP 在 ImageNet-100 上生成 3.4 倍更少的样本并减少六倍迭代，在 ImageNet-1k 上生成八倍更少的样本并降低 30% 迭代，同时实现优于现有工作的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15588v1",
      "published_date": "2025-02-21 16:56:15 UTC",
      "updated_date": "2025-02-21 16:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:59:33.553280"
    },
    {
      "arxiv_id": "2502.15575v1",
      "title": "Feature maps for the Laplacian kernel and its generalizations",
      "title_zh": "拉普拉斯核及其推广的特征映射",
      "authors": [
        "Sudhendu Ahir",
        "Parthe Pandit"
      ],
      "abstract": "Recent applications of kernel methods in machine learning have seen a renewed\ninterest in the Laplacian kernel, due to its stability to the bandwidth\nhyperparameter in comparison to the Gaussian kernel, as well as its\nexpressivity being equivalent to that of the neural tangent kernel of deep\nfully connected networks. However, unlike the Gaussian kernel, the Laplacian\nkernel is not separable. This poses challenges for techniques to approximate\nit, especially via the random Fourier features (RFF) methodology and its\nvariants. In this work, we provide random features for the Laplacian kernel and\nits two generalizations: Mat\\'{e}rn kernel and the Exponential power kernel. We\nprovide efficiently implementable schemes to sample weight matrices so that\nrandom features approximate these kernels. These weight matrices have a weakly\ncoupled heavy-tailed randomness. Via numerical experiments on real datasets we\ndemonstrate the efficacy of these random feature maps.",
      "tldr_zh": "本研究针对Laplacian kernel在机器学习中的应用及其挑战（如非可分离性），提出了一种随机特征映射方法，以提升其近似效率。作者开发了高效采样权重矩阵的方案，这些矩阵采用弱耦合的重尾随机性，来近似Laplacian kernel及其推广，包括Matérn kernel和Exponential power kernel。与Gaussian kernel相比，Laplacian kernel具有更好的带宽超参数稳定性和等效的表达能力。实验结果显示，在真实数据集上，该方法显著提高了随机特征映射的效能。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15575v1",
      "published_date": "2025-02-21 16:36:20 UTC",
      "updated_date": "2025-02-21 16:36:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:59:45.008587"
    },
    {
      "arxiv_id": "2502.17503v1",
      "title": "Doctor-in-the-Loop: An Explainable, Multi-View Deep Learning Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer",
      "title_zh": "翻译失败",
      "authors": [
        "Alice Natalina Caragliano",
        "Filippo Ruffini",
        "Carlo Greco",
        "Edy Ippolito",
        "Michele Fiore",
        "Claudia Tacconi",
        "Lorenzo Nibid",
        "Giuseppe Perrone",
        "Sara Ramella",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "abstract": "Non-small cell lung cancer (NSCLC) remains a major global health challenge,\nwith high post-surgical recurrence rates underscoring the need for accurate\npathological response predictions to guide personalized treatments. Although\nartificial intelligence models show promise in this domain, their clinical\nadoption is limited by the lack of medically grounded guidance during training,\noften resulting in non-explainable intrinsic predictions. To address this, we\npropose Doctor-in-the-Loop, a novel framework that integrates expert-driven\ndomain knowledge with explainable artificial intelligence techniques, directing\nthe model toward clinically relevant anatomical regions and improving both\ninterpretability and trustworthiness. Our approach employs a gradual multi-view\nstrategy, progressively refining the model's focus from broad contextual\nfeatures to finer, lesion-specific details. By incorporating domain insights at\nevery stage, we enhance predictive accuracy while ensuring that the model's\ndecision-making process aligns more closely with clinical reasoning. Evaluated\non a dataset of NSCLC patients, Doctor-in-the-Loop delivers promising\npredictive performance and provides transparent, justifiable outputs,\nrepresenting a significant step toward clinically explainable artificial\nintelligence in oncology.",
      "tldr_zh": "该研究针对非小细胞肺癌（Non-Small Cell Lung Cancer, NSCLC）的病理反应预测问题，提出了一种可解释的多视图深度学习框架Doctor-in-the-Loop，以解决AI模型在临床应用中的可解释性不足。该框架整合专家驱动的领域知识和explainable AI技术，通过渐进式多视图策略（gradual multi-view strategy），从宽泛的上下文特征逐步细化到病变特定细节，从而提升预测准确性和决策透明度。在NSCLC患者数据集上的评估显示，该框架表现出色，提供可证明的输出，并为肿瘤学中临床可解释AI的应用奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17503v1",
      "published_date": "2025-02-21 16:35:30 UTC",
      "updated_date": "2025-02-21 16:35:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T16:59:57.076074"
    },
    {
      "arxiv_id": "2502.15568v2",
      "title": "A Cautionary Tale About \"Neutrally\" Informative AI Tools Ahead of the 2025 Federal Elections in Germany",
      "title_zh": "翻译失败",
      "authors": [
        "Ina Dormuth",
        "Sven Franke",
        "Marlies Hafer",
        "Tim Katzke",
        "Alexander Marx",
        "Emmanuel Müller",
        "Daniel Neider",
        "Markus Pauly",
        "Jérôme Rutinowski"
      ],
      "abstract": "In this study, we examine the reliability of AI-based Voting Advice\nApplications (VAAs) and large language models (LLMs) in providing objective\npolitical information. Our analysis is based upon a comparison with party\nresponses to 38 statements of the Wahl-O-Mat, a well-established German online\ntool that helps inform voters by comparing their views with political party\npositions. For the LLMs, we identify significant biases. They exhibit a strong\nalignment (over 75% on average) with left-wing parties and a substantially\nlower alignment with center-right (smaller 50%) and right-wing parties (around\n30%). Furthermore, for the VAAs, intended to objectively inform voters, we\nfound substantial deviations from the parties' stated positions in Wahl-O-Mat:\nWhile one VAA deviated in 25% of cases, another VAA showed deviations in more\nthan 50% of cases. For the latter, we even observed that simple prompt\ninjections led to severe hallucinations, including false claims such as\nnon-existent connections between political parties and right-wing extremist\nties.",
      "tldr_zh": "本研究警告了AI工具在2025年德国联邦选举中提供“中立”政治信息的可靠性问题，通过将AI-based Voting Advice Applications (VAAs) 和 large language models (LLMs) 与德国在线工具Wahl-O-Mat的党派回应进行比较。结果显示，LLMs存在显著偏见，平均超过75%与左翼政党一致，而与中间右翼（小于50%）和右翼政党（约30%）一致性较低。VAAs也出现偏差，一个在25%的案例中出错，另一个超过50%的案例中偏差，甚至易受prompt injections影响，导致hallucinations如虚假声称政党与右翼极端主义有关。该研究强调了AI工具潜在风险，呼吁在选举情境中加强客观性和防范措施。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15568v2",
      "published_date": "2025-02-21 16:30:53 UTC",
      "updated_date": "2025-04-07 20:52:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:00:11.038818"
    },
    {
      "arxiv_id": "2502.15563v1",
      "title": "Bridging vision language model (VLM) evaluation gaps with a framework for scalable and cost-effective benchmark generation",
      "title_zh": "翻译失败",
      "authors": [
        "Tim Rädsch",
        "Leon Mayer",
        "Simon Pavicic",
        "A. Emre Kavur",
        "Marcel Knopp",
        "Barış Öztürk",
        "Klaus Maier-Hein",
        "Paul F. Jaeger",
        "Fabian Isensee",
        "Annika Reinke",
        "Lena Maier-Hein"
      ],
      "abstract": "Reliable evaluation of AI models is critical for scientific progress and\npractical application. While existing VLM benchmarks provide general insights\ninto model capabilities, their heterogeneous designs and limited focus on a few\nimaging domains pose significant challenges for both cross-domain performance\ncomparison and targeted domain-specific evaluation. To address this, we propose\nthree key contributions: (1) a framework for the resource-efficient creation of\ndomain-specific VLM benchmarks enabled by task augmentation for creating\nmultiple diverse tasks from a single existing task, (2) the release of new VLM\nbenchmarks for seven domains, created according to the same homogeneous\nprotocol and including 162,946 thoroughly human-validated answers, and (3) an\nextensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,\nrevealing performance variances across domains and tasks, thereby supporting\nthe need for tailored VLM benchmarks. Adoption of our methodology will pave the\nway for the resource-efficient domain-specific selection of models and guide\nfuture research efforts toward addressing core open questions.",
      "tldr_zh": "该论文针对视觉语言模型（VLM）的评估挑战，提出一个可扩展且成本有效的基准生成框架，以解决现有基准的异质设计和领域局限性问题。该框架通过任务增强（task augmentation）技术，从单一任务创建多个多样任务，从而实现资源高效的领域特定VLM基准生成。作为贡献，论文发布了七个领域的全新VLM基准，这些基准遵循统一协议，并包含162,946个经过人工验证的答案。此外，对22个最先进VLM在37,171个任务上的基准测试揭示了跨领域和任务的性能差异，支持了定制基准的必要性。该方法将促进高效的模型选择和未来研究方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15563v1",
      "published_date": "2025-02-21 16:24:10 UTC",
      "updated_date": "2025-02-21 16:24:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:00:21.123212"
    },
    {
      "arxiv_id": "2502.15547v1",
      "title": "Zweistein: A Dynamic Programming Evaluation Function for Einstein Würfelt Nicht!",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Lin. Hsueh",
        "Tsan Sheng. Hsu"
      ],
      "abstract": "This paper introduces Zweistein, a dynamic programming evaluation function\nfor Einstein W\\\"urfelt Nicht! (EWN). Instead of relying on human knowledge to\ncraft an evaluation function, Zweistein uses a data-centric approach that\neliminates the need for parameter tuning. The idea is to use a vector recording\nthe distance to the corner of all pieces. This distance vector captures the\nessence of EWN. It not only outperforms many traditional EWN evaluation\nfunctions but also won first place in the TCGA 2023 competition.",
      "tldr_zh": "本论文介绍了Zweistein，一种基于Dynamic Programming的评估函数，用于游戏Einstein Würfelt Nicht! (EWN)。Zweistein采用数据驱动的方法，通过记录所有棋子到角落的距离向量作为核心特征，避免了依赖人工知识和参数调整。实验结果显示，该函数优于传统EWN评估函数，并在TCGA 2023比赛中获得第一名。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15547v1",
      "published_date": "2025-02-21 15:54:21 UTC",
      "updated_date": "2025-02-21 15:54:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:00:31.224138"
    },
    {
      "arxiv_id": "2502.15543v1",
      "title": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning",
      "title_zh": "翻译失败",
      "authors": [
        "Pengcheng Huang",
        "Zhenghao Liu",
        "Yukun Yan",
        "Xiaoyuan Yi",
        "Hao Chen",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Tong Xiao",
        "Ge Yu",
        "Chenyan Xiong"
      ],
      "abstract": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the\ninternal memory of Large Language Models (LLMs) by integrating external\nknowledge. However, KAG inevitably faces knowledge conflicts when the internal\nmemory contradicts external information. Current approaches to mitigating these\nconflicts mainly focus on improving external knowledge utilization. However,\nthese methods have shown only limited effectiveness in mitigating the knowledge\nconflict problem, as internal knowledge continues to influence the generation\nprocess of LLMs. In this paper, we propose a ParametrIc Pruning-based\nKnowledge-Augmented Generation (PIP-KAG) approach, which prunes internal\nknowledge of LLMs and incorporates a plug-and-play adaptation module to help\nLLMs better leverage external sources. Additionally, we construct the\nCoConflictQA benchmark based on the hallucination of LLMs to better evaluate\ncontextual faithfulness during answering questions. Experimental results on\nCoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts\nand improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by\n13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes\nare available at https://github.com/OpenBMB/PIP-KAG.",
      "tldr_zh": "该论文针对 Knowledge-Augmented Generation (KAG) 中内部知识与外部信息冲突的问题，提出 PIP-KAG 方法，通过 Parametric Pruning 修剪 Large Language Models (LLMs) 的内部知识，并添加即插即用适应模块，以帮助 LLMs 更好地利用外部来源。研究者构建了 CoConflictQA 基准，用于评估回答问题时的上下文忠实度。实验结果显示，PIP-KAG 显著降低了知识冲突，提高了生成质量，并将 LLMs 参数减少了 13%，提升了整体参数效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 7 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15543v1",
      "published_date": "2025-02-21 15:50:41 UTC",
      "updated_date": "2025-02-21 15:50:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:00:46.687699"
    },
    {
      "arxiv_id": "2502.15542v1",
      "title": "Bridging Domain Gaps between Pretrained Multimodal Models and Recommendations",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyu Zhang",
        "Jie Luo",
        "Xinming Zhang",
        "Yuan Fang"
      ],
      "abstract": "With the explosive growth of multimodal content online, pre-trained\nvisual-language models have shown great potential for multimodal\nrecommendation. However, while these models achieve decent performance when\napplied in a frozen manner, surprisingly, due to significant domain gaps (e.g.,\nfeature distribution discrepancy and task objective misalignment) between\npre-training and personalized recommendation, adopting a joint training\napproach instead leads to performance worse than baseline. Existing approaches\neither rely on simple feature extraction or require computationally expensive\nfull model fine-tuning, struggling to balance effectiveness and efficiency. To\ntackle these challenges, we propose \\textbf{P}arameter-efficient\n\\textbf{T}uning for \\textbf{M}ultimodal \\textbf{Rec}ommendation\n(\\textbf{PTMRec}), a novel framework that bridges the domain gap between\npre-trained models and recommendation systems through a knowledge-guided\ndual-stage parameter-efficient training strategy. This framework not only\neliminates the need for costly additional pre-training but also flexibly\naccommodates various parameter-efficient tuning methods.",
      "tldr_zh": "该研究探讨了预训练多模态模型在推荐系统中的应用问题，由于领域差距（如特征分布 discrepancy 和任务目标 misalignment），联合训练往往导致性能不如基线。针对此挑战，作者提出 PTMRec 框架，这是一种知识引导的双阶段参数-efficient tuning 策略，能够桥接预训练模型与推荐系统的鸿沟，同时避免昂贵的额外预训练。实验结果表明，该框架在多模态推荐任务中实现了有效性和效率的平衡，灵活支持各种参数-efficient tuning 方法。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15542v1",
      "published_date": "2025-02-21 15:50:14 UTC",
      "updated_date": "2025-02-21 15:50:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:00:57.581959"
    },
    {
      "arxiv_id": "2502.15516v1",
      "title": "Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection",
      "title_zh": "基于图像和4D雷达谱的深度感知融合方法，用于3D",
      "authors": [
        "Yue Sun",
        "Yeqiang Qian",
        "Chunxiang Wang",
        "Ming Yang"
      ],
      "abstract": "Safety and reliability are crucial for the public acceptance of autonomous\ndriving. To ensure accurate and reliable environmental perception, intelligent\nvehicles must exhibit accuracy and robustness in various environments.\nMillimeter-wave radar, known for its high penetration capability, can operate\neffectively in adverse weather conditions such as rain, snow, and fog.\nTraditional 3D millimeter-wave radars can only provide range, Doppler, and\nazimuth information for objects. Although the recent emergence of 4D\nmillimeter-wave radars has added elevation resolution, the radar point clouds\nremain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast,\ncameras offer rich semantic details but are sensitive to lighting and weather\nconditions. Hence, this paper leverages these two highly complementary and\ncost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D\nradar spectra with depth-aware camera images and employing attention\nmechanisms, we fuse texture-rich images with depth-rich radar data in the\nBird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally,\nwe propose using GAN-based networks to generate depth images from radar spectra\nin the absence of depth sensors, further improving detection accuracy.",
      "tldr_zh": "该论文提出了一种基于图像和4D Radar Spectrum的深度感知融合方法，以提升自动驾驶中3D物体检测的准确性和鲁棒性。通过在Bird's Eye View (BEV)视角整合4D毫米波雷达的深度信息与相机的丰富语义细节，并运用注意力机制，实现了对纹理和深度数据的有效融合。论文还引入GAN-based网络，从雷达谱生成深度图像，进一步改善在缺乏深度传感器时的检测性能。整体方法显著增强了智能车辆在恶劣天气下的环境感知能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15516v1",
      "published_date": "2025-02-21 15:14:30 UTC",
      "updated_date": "2025-02-21 15:14:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:01:10.299154"
    },
    {
      "arxiv_id": "2502.15507v3",
      "title": "Activation Steering in Neural Theorem Provers",
      "title_zh": "翻译失败",
      "authors": [
        "Shashank Kirtania"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
      "tldr_zh": "大型语言模型 (LLMs) 在使用证明助手如 Lean 进行形式定理证明时，常常难以准确预测和排名候选策略，从而影响整体表现。论文提出使用 activation steering 技术来引导 LLMs 的响应，提高推理过程中的生成质量。该方法作为一种轻量级替代方案，无需专门微调，即可显著提升 LLMs 的定理证明能力，尤其适用于资源受限的环境。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "incorrect explanation for a concept, need to revise and update!",
      "pdf_url": "http://arxiv.org/pdf/2502.15507v3",
      "published_date": "2025-02-21 15:04:48 UTC",
      "updated_date": "2025-05-14 17:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:01:22.540546"
    },
    {
      "arxiv_id": "2502.15503v1",
      "title": "BAN: Neuroanatomical Aligning in Auditory Recognition between Artificial Neural Network and Human Cortex",
      "title_zh": "翻译失败",
      "authors": [
        "Haidong Wang",
        "Pengfei Xiao",
        "Ao Liu",
        "Jianhua Zhang",
        "Qia Shan"
      ],
      "abstract": "Drawing inspiration from neurosciences, artificial neural networks (ANNs)\nhave evolved from shallow architectures to highly complex, deep structures,\nyielding exceptional performance in auditory recognition tasks. However,\ntraditional ANNs often struggle to align with brain regions due to their\nexcessive depth and lack of biologically realistic features, like recurrent\nconnection. To address this, a brain-like auditory network (BAN) is introduced,\nwhich incorporates four neuroanatomically mapped areas and recurrent\nconnection, guided by a novel metric called the brain-like auditory score\n(BAS). BAS serves as a benchmark for evaluating the similarity between BAN and\nhuman auditory recognition pathway. We further propose that specific areas in\nthe cerebral cortex, mainly the middle and medial superior temporal (T2/T3)\nareas, correspond to the designed network structure, drawing parallels with the\nbrain's auditory perception pathway. Our findings suggest that the\nneuroanatomical similarity in the cortex and auditory classification abilities\nof the ANN are well-aligned. In addition to delivering excellent performance on\na music genre classification task, the BAN demonstrates a high BAS score. In\nconclusion, this study presents BAN as a recurrent, brain-inspired ANN,\nrepresenting the first model that mirrors the cortical pathway of auditory\nrecognition.",
      "tldr_zh": "本文提出了一种脑像听觉网络 (BAN)，通过整合四个神经解剖映射区域和循环连接 (recurrent connection)，使人工神经网络 (ANN) 与人类大脑听觉识别路径更好地对齐，以解决传统 ANN 在深度和生物真实性方面的局限。研究引入了脑像听觉分数 (BAS) 作为新指标，用于评估 BAN 与人类听觉通路的相似性，并发现大脑中 T2/T3 区域与网络结构高度对应。实验结果显示，BAN 在音乐流派分类任务中表现出色，并获得高 BAS 分数，证明了神经解剖相似性与 ANN 的听觉分类能力的高度一致性。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15503v1",
      "published_date": "2025-02-21 14:57:01 UTC",
      "updated_date": "2025-02-21 14:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:01:35.855092"
    },
    {
      "arxiv_id": "2503.16454v1",
      "title": "An Audio-Visual Fusion Emotion Generation Model Based on Neuroanatomical Alignment",
      "title_zh": "基于神经解剖",
      "authors": [
        "Haidong Wang",
        "Qia Shan",
        "JianHua Zhang",
        "PengFei Xiao",
        "Ao Liu"
      ],
      "abstract": "In the field of affective computing, traditional methods for generating\nemotions predominantly rely on deep learning techniques and large-scale emotion\ndatasets. However, deep learning techniques are often complex and difficult to\ninterpret, and standardizing large-scale emotional datasets are difficult and\ncostly to establish. To tackle these challenges, we introduce a novel framework\nnamed Audio-Visual Fusion for Brain-like Emotion Learning(AVF-BEL). In contrast\nto conventional brain-inspired emotion learning methods, this approach improves\nthe audio-visual emotion fusion and generation model through the integration of\nmodular components, thereby enabling more lightweight and interpretable emotion\nlearning and generation processes. The framework simulates the integration of\nthe visual, auditory, and emotional pathways of the brain, optimizes the fusion\nof emotional features across visual and auditory modalities, and improves upon\nthe traditional Brain Emotional Learning (BEL) model. The experimental results\nindicate a significant improvement in the similarity of the audio-visual fusion\nemotion learning generation model compared to single-modality visual and\nauditory emotion learning and generation model. Ultimately, this aligns with\nthe fundamental phenomenon of heightened emotion generation facilitated by the\nintegrated impact of visual and auditory stimuli. This contribution not only\nenhances the interpretability and efficiency of affective intelligence but also\nprovides new insights and pathways for advancing affective computing\ntechnology. Our source code can be accessed here:\nhttps://github.com/OpenHUTB/emotion}{https://github.com/OpenHUTB/emotion.",
      "tldr_zh": "该研究提出了一种基于神经解剖对齐（Neuroanatomical Alignment）的音频-视觉融合情感生成模型，名为 AVF-BEL，以解决传统深度学习情感计算方法的复杂性和数据集标准化难题。该框架通过模拟大脑的视觉、听觉和情感路径，优化多模态情感特征融合，并改进传统 Brain Emotional Learning (BEL) 模型，实现更轻量级和可解释的情感学习与生成。实验结果显示，AVF-BEL 在音频-视觉融合上显著提升了情感生成相似度，优于单模态模型，并为情感计算技术的可解释性和效率提供了新见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16454v1",
      "published_date": "2025-02-21 14:26:58 UTC",
      "updated_date": "2025-02-21 14:26:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:01:46.432379"
    },
    {
      "arxiv_id": "2502.15488v2",
      "title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection",
      "title_zh": "Q-PETR：量化感知位置嵌入变换用于多视图 3D 对象检测",
      "authors": [
        "Jiangyong Yu",
        "Changyong Shu",
        "Dawei Yang",
        "Sifan Zhou",
        "Zichen Yu",
        "Xing Hu",
        "Yan Chen"
      ],
      "abstract": "Camera-based multi-view 3D detection has emerged as an attractive solution\nfor autonomous driving due to its low cost and broad applicability. However,\ndespite the strong performance of PETR-based methods in 3D perception\nbenchmarks, their direct INT8 quantization for onboard deployment leads to\ndrastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes\ndataset. In this work, we propose Q-PETR, a quantization-aware position\nembedding transformation that re-engineers key components of the PETR framework\nto reconcile the discrepancy between the dynamic ranges of positional encodings\nand image features, and to adapt the cross-attention mechanism for low-bit\ninference. By redesigning the positional encoding module and introducing an\nadaptive quantization strategy, Q-PETR maintains floating-point performance\nwith a performance degradation of less than 1% under standard 8-bit per-tensor\npost-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR\nachieves a two-fold speedup and reduces memory usage by three times, thereby\noffering a deployment-friendly solution for resource-constrained onboard\ndevices. Extensive experiments across various PETR-series models validate the\nstrong generalization and practical benefits of our approach.",
      "tldr_zh": "该研究针对基于相机的多视图3D物体检测在自动驾驶中的应用，提出Q-PETR框架，以解决PETR-based方法在INT8量化部署时导致的准确率急剧下降问题（如NuScenes数据集上mAP下降58.2%、NDS下降36.9%）。Q-PETR通过量化感知的位置嵌入转换，重新设计位置编码模块并引入自适应量化策略，协调位置编码和图像特征的动态范围差异，并优化交叉注意力机制以适应低位推理。实验结果显示，Q-PETR在标准8-bit per-tensor后训练量化下，性能仅下降不到1%，同时比FP32版本实现两倍加速和三倍内存减少，提供高效的资源受限设备部署方案；广泛实验验证了其在各种PETR系列模型上的泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15488v2",
      "published_date": "2025-02-21 14:26:23 UTC",
      "updated_date": "2025-03-11 15:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:02:00.588185"
    },
    {
      "arxiv_id": "2502.15487v2",
      "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Martina Miliani",
        "Serena Auriemma",
        "Alessandro Bondielli",
        "Emmanuele Chersoni",
        "Lucia Passaro",
        "Irene Sucameli",
        "Alessandro Lenci"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.",
      "tldr_zh": "本论文引入了 ExpliCa 数据集，用于评估大型语言模型 (LLMs) 在显式因果推理方面的性能，该数据集整合了因果和时间关系，并通过不同语言顺序和语言连接词进行表达，还包括了众包的人类可接受性评分。作者通过提示和 perplexity-based metrics 测试了七个商业和开源 LLMs，结果显示即使顶级模型的准确率也低于 0.80。研究发现，LLMs 倾向于将 temporal relations 与 causal relations 混淆，且性能受事件语言顺序的影响显著。最后，模型大小对 perplexity-based scores 和提示性能的影响存在差异，为改进 LLMs 的推理能力提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50, 68T07",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to ACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15487v2",
      "published_date": "2025-02-21 14:23:14 UTC",
      "updated_date": "2025-02-26 07:15:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:02:12.835198"
    },
    {
      "arxiv_id": "2502.15485v2",
      "title": "Enhancing RWKV-based Language Models for Long-Sequence Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Xinghan Pan"
      ],
      "abstract": "This paper introduces an enhanced RWKV architecture with adaptive temporal\ngating mechanisms for improved long-context language modeling. We propose two\nprincipal innovations: (1) a position-aware convolutional shift operator that\ncaptures local syntactic patterns while preserving global coherence, and (2) a\nneurally-gated information routing mechanism that dynamically regulates\ninter-token information flow. Through comprehensive experiments on text\ngeneration tasks, our enhanced model demonstrates superior performance compared\nto the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores\nwith only 2.95 increased inference latency. Ablation studies validate the\nindividual contributions of each component, while linguistic analysis reveals\nthe model's adaptive attention to syntactic boundaries and entity coherence.\nThe proposed modifications maintain RWKV's linear computational complexity\nwhile significantly enhancing its contextual modeling capabilities,\nestablishing new state-of-the-art performance for recurrent-style architectures\nin long-form text generation.",
      "tldr_zh": "本论文增强了基于 RWKV 的语言模型，针对长序列文本生成问题引入自适应时间门控机制，包括两个主要创新：（1）位置感知卷积移位操作符，用于捕捉局部语法模式并保持全局连贯性；（2）神经门控信息路由机制，实现动态调节 token 间的信息流。实验结果显示，该增强模型在文本生成任务上比基线 RWKV 提升 96.5% 的 ROUGE-L 分数，同时仅增加 2.95% 的推理延迟。消融研究和语言分析进一步验证了各组件的贡献，并证明该模型在保持线性计算复杂度的同时，大幅提升了上下文建模能力，确立了循环式架构在长文本生成中的新最先进性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 2 tables, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15485v2",
      "published_date": "2025-02-21 14:18:18 UTC",
      "updated_date": "2025-02-24 14:30:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:02:23.584068"
    },
    {
      "arxiv_id": "2502.15470v2",
      "title": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System",
      "title_zh": "翻译失败",
      "authors": [
        "Yintao He",
        "Haiyu Mao",
        "Christina Giannoula",
        "Mohammad Sadrosadati",
        "Juan Gómez-Luna",
        "Huawei Li",
        "Xiaowei Li",
        "Ying Wang",
        "Onur Mutlu"
      ],
      "abstract": "Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively.",
      "tldr_zh": "大型语言模型 (LLMs) 的解码过程耗时且涉及计算密集型和内存密集型内核，现有方法采用静态映射到异构架构（如 Processing-In-Memory, PIM 单位），但无法应对内核特性的动态变化。论文提出 PAPI（PArallel Decoding with PIM）框架，通过在线内核特征化（online kernel characterization）动态调度内核到最合适的硬件单位，并设计一个 PIM-enabled 异构计算系统协调计算中心处理单位和混合 PIM 单位。实验结果显示，PAPI 在三个常用 LLMs 上分别比最先进异构 LLM 加速器快 1.8 倍、比 PIM-only 加速器快 11.1 倍，从而显著提升 LLM 解码性能。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "To appear in ASPLOS 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15470v2",
      "published_date": "2025-02-21 13:52:31 UTC",
      "updated_date": "2025-02-27 07:03:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:02:36.950186"
    },
    {
      "arxiv_id": "2502.15466v1",
      "title": "Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxuan Wang",
        "Kai Wu",
        "Yujian Betterest Li",
        "Dan Wang",
        "Xiaoyu Zhang",
        "Jing Liu"
      ],
      "abstract": "Foundation models for time series analysis (TSA) have attracted significant\nattention. However, challenges such as data scarcity and data imbalance\ncontinue to hinder their development. To address this, we consider modeling\ncomplex systems through symbolic expressions that serve as semantic descriptors\nof time series. Building on this concept, we introduce a series-symbol (S2)\ndual-modulity data generation mechanism, enabling the unrestricted creation of\nhigh-quality time series data paired with corresponding symbolic\nrepresentations. Leveraging the S2 dataset, we develop SymTime, a pre-trained\nfoundation model for TSA. SymTime demonstrates competitive performance across\nfive major TSA tasks when fine-tuned with downstream task, rivaling foundation\nmodels pre-trained on real-world datasets. This approach underscores the\npotential of dual-modality data generation and pretraining mechanisms in\novercoming data scarcity and enhancing task performance.",
      "tldr_zh": "该研究针对时间序列分析（TSA）中的数据稀缺和不平衡问题，提出了一种基于 series-symbol (S2) 双模态数据生成机制，能够无限制地创建高质量的时间序列数据及其对应的符号表示。利用 S2 数据集，研究者开发了 SymTime，这是一个预训练的 foundation model，用于 TSA。实验结果显示，SymTime 在五个主要 TSA 任务上表现竞争力，与基于真实世界数据集的模型相当，从而证明了双模态数据生成在缓解数据稀缺性和提升任务性能方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15466v1",
      "published_date": "2025-02-21 13:43:24 UTC",
      "updated_date": "2025-02-21 13:43:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:02:46.848199"
    },
    {
      "arxiv_id": "2502.15455v1",
      "title": "R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning",
      "title_zh": "R-LoRA：多头 Lo",
      "authors": [
        "Jinda Liu",
        "Yi Chang",
        "Yuan Wu"
      ],
      "abstract": "Fine-tuning large language models (LLMs) is prohibitively expensive in terms\nof computational and memory costs. Low-rank Adaptation (LoRA), as one of the\nmost popular parameter-efficient fine-tuning (PEFT) methods, offers a\ncost-effective alternative by approximating the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in\n\\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where\n$r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from\nmultiple domains to perform tasks across various fields, embodying multi-task\nlearning (MTL). LoRA often underperforms in such complex scenarios. To enhance\nLoRA's capability in multi-task learning, we propose R-LoRA, which incorporates\nMulti-Head Randomization. Multi-Head Randomization diversifies the head\nmatrices through Multi-Head Random Initialization and Multi-Head Dropout,\nenabling more efficient learning of task-specific features while maintaining\nshared knowledge representation. Extensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge, thereby improving performance\nin multi-task scenarios. The code is available at\nhttps://github.com/jinda-liu/R-LoRA.",
      "tldr_zh": "本文提出 R-LoRA，一种针对多任务学习 (MTL) 的改进方法，通过 Multi-Head Randomization（包括 Multi-Head Random Initialization 和 Multi-Head Dropout）对原始 LoRA 进行优化，以更好地捕捉任务特定特征，同时保留共享知识表示。LoRA 作为参数高效微调 (PEFT) 技术，本文发现其在多任务场景中表现不足，R-LoRA 通过随机初始化和 Dropout 机制提升了模型的适应性。实验结果显示，R-LoRA 在多任务学习中显著提高了性能，代码已在 GitHub 上开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15455v1",
      "published_date": "2025-02-21 13:30:21 UTC",
      "updated_date": "2025-02-21 13:30:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:02:58.921135"
    },
    {
      "arxiv_id": "2502.15448v1",
      "title": "MVIP -- A Dataset and Methods for Application Oriented Multi-View and Multi-Modal Industrial Part Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Koch",
        "Marian Schlüter",
        "Jörg Krüger"
      ],
      "abstract": "We present MVIP, a novel dataset for multi-modal and multi-view\napplication-oriented industrial part recognition. Here we are the first to\ncombine a calibrated RGBD multi-view dataset with additional object context\nsuch as physical properties, natural language, and super-classes. The current\nportfolio of available datasets offers a wide range of representations to\ndesign and benchmark related methods. In contrast to existing classification\nchallenges, industrial recognition applications offer controlled multi-modal\nenvironments but at the same time have different problems than traditional\n2D/3D classification challenges. Frequently, industrial applications must deal\nwith a small amount or increased number of training data, visually similar\nparts, and varying object sizes, while requiring a robust near 100% top 5\naccuracy under cost and time constraints. Current methods tackle such\nchallenges individually, but direct adoption of these methods within industrial\napplications is complex and requires further research. Our main goal with MVIP\nis to study and push transferability of various state-of-the-art methods within\nrelated downstream tasks towards an efficient deployment of industrial\nclassifiers. Additionally, we intend to push with MVIP research regarding\nseveral modality fusion topics, (automated) synthetic data generation, and\ncomplex data sampling -- combined in a single application-oriented benchmark.",
      "tldr_zh": "该研究引入了 MVIP 数据集，这是一个面向应用的工业零件识别数据集，首次结合校准的 RGBD 多视图数据以及额外的对象上下文，如物理属性、自然语言和超类。MVIP 旨在解决工业识别面临的挑战，包括训练数据量有限、零件视觉相似以及对象大小变化等问题，同时要求在成本和时间约束下实现近 100% 的 top 5 准确率。论文通过研究各种 state-of-the-art 方法的转移性，并推动模态融合、合成数据生成和复杂数据采样的创新，为高效部署工业分类器提供了一个综合基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IMPROVE 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15448v1",
      "published_date": "2025-02-21 13:22:29 UTC",
      "updated_date": "2025-02-21 13:22:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:03:11.028003"
    },
    {
      "arxiv_id": "2502.15867v1",
      "title": "Strategic priorities for transformative progress in advancing biology with proteomics and artificial intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Yingying Sun",
        "Jun A",
        "Zhiwei Liu",
        "Rui Sun",
        "Liujia Qian",
        "Samuel H. Payne",
        "Wout Bittremieux",
        "Markus Ralser",
        "Chen Li",
        "Yi Chen",
        "Zhen Dong",
        "Yasset Perez-Riverol",
        "Asif Khan",
        "Chris Sander",
        "Ruedi Aebersold",
        "Juan Antonio Vizcaíno",
        "Jonathan R Krieger",
        "Jianhua Yao",
        "Han Wen",
        "Linfeng Zhang",
        "Yunping Zhu",
        "Yue Xuan",
        "Benjamin Boyang Sun",
        "Liang Qiao",
        "Henning Hermjakob",
        "Haixu Tang",
        "Huanhuan Gao",
        "Yamin Deng",
        "Qing Zhong",
        "Cheng Chang",
        "Nuno Bandeira",
        "Ming Li",
        "Weinan E",
        "Siqi Sun",
        "Yuedong Yang",
        "Gilbert S. Omenn",
        "Yue Zhang",
        "Ping Xu",
        "Yan Fu",
        "Xiaowen Liu",
        "Christopher M. Overall",
        "Yu Wang",
        "Eric W. Deutsch",
        "Luonan Chen",
        "Jürgen Cox",
        "Vadim Demichev",
        "Fuchu He",
        "Jiaxing Huang",
        "Huilin Jin",
        "Chao Liu",
        "Nan Li",
        "Zhongzhi Luan",
        "Jiangning Song",
        "Kaicheng Yu",
        "Wanggen Wan",
        "Tai Wang",
        "Kang Zhang",
        "Le Zhang",
        "Peter A. Bell",
        "Matthias Mann",
        "Bing Zhang",
        "Tiannan Guo"
      ],
      "abstract": "Artificial intelligence (AI) is transforming scientific research, including\nproteomics. Advances in mass spectrometry (MS)-based proteomics data quality,\ndiversity, and scale, combined with groundbreaking AI techniques, are unlocking\nnew challenges and opportunities in biological discovery. Here, we highlight\nkey areas where AI is driving innovation, from data analysis to new biological\ninsights. These include developing an AI-friendly ecosystem for proteomics data\ngeneration, sharing, and analysis; improving peptide and protein identification\nand quantification; characterizing protein-protein interactions and protein\ncomplexes; advancing spatial and perturbation proteomics; integrating\nmulti-omics data; and ultimately enabling AI-empowered virtual cells.",
      "tldr_zh": "这篇论文讨论了人工智能（AI）在蛋白质组学（proteomics）领域推动生物学研究的关键战略优先事项。论文强调，通过提升基于质谱（MS）的蛋白质组学数据质量、多样性和规模，结合创新AI技术，可以实现从数据分析到生物洞见的重大进展，包括改进肽和蛋白质的识别与量化、表征蛋白-蛋白相互作用，以及整合多组学数据。最终，这些努力旨在构建AI友好的数据生态系统，并推动AI赋能的虚拟细胞的开发，以加速生物发现和创新。",
      "categories": [
        "q-bio.OT",
        "cs.AI"
      ],
      "primary_category": "q-bio.OT",
      "comment": "28 pages, 2 figures, perspective in AI proteomics",
      "pdf_url": "http://arxiv.org/pdf/2502.15867v1",
      "published_date": "2025-02-21 13:20:33 UTC",
      "updated_date": "2025-02-21 13:20:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:03:24.060935"
    },
    {
      "arxiv_id": "2502.15443v1",
      "title": "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models",
      "title_zh": "当压缩遇见模型压缩：内存高效的双重压缩用于大型语言模型",
      "authors": [
        "Weilan Wang",
        "Yu Mao",
        "Dongdong Tang",
        "Hongchao Du",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "abstract": "Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed.",
      "tldr_zh": "这篇论文提出了一种双重压缩框架，用于进一步压缩量化后的Large Language Models (LLMs)，以解决内存限制设备上的部署挑战，实现了约2.2x的压缩比。框架首先引入compression-aware quantization，通过重新缩放模型参数来提升权重可压缩性，并结合pruning方法进行进一步优化；同时，分析了解压缩的延迟瓶颈，并提出speed-adaptive方法来平衡内存使用和推理速度。实验结果显示，该方法能将内存大小减少40%，同时保持negligible的准确性和推理速度损失。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15443v1",
      "published_date": "2025-02-21 13:11:22 UTC",
      "updated_date": "2025-02-21 13:11:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:03:35.437217"
    },
    {
      "arxiv_id": "2502.15436v1",
      "title": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Raghav Singhal",
        "Kaustubh Ponkshe",
        "Rohit Vartak",
        "Lav R. Varshney",
        "Praneeth Vepakomma"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb.",
      "tldr_zh": "该论文提出Fed-SB，一种新型方法，用于在联邦学习环境中高效微调LoRA模型，解决传统联邦平均带来的子优更新问题，同时实现极高的通信效率。Fed-SB基于LoRA-SB技术，通过学习一个小型方阵R来优化低秩适配轨迹，确保更新精确且通信成本独立于客户端数量，从而减少高达230倍的通信开销。实验结果显示，Fed-SB在常识推理、算术推理和语言推理任务上达到最先进性能；在隐私设置中，它进一步提升表现，降低Differential Privacy所需噪声并避免噪声放大。总体而言，Fed-SB在通信效率与性能之间建立了新的Pareto前沿，提供可扩展的联邦微调解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Raghav Singhal and Kaustubh Ponkshe contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2502.15436v1",
      "published_date": "2025-02-21 13:05:19 UTC",
      "updated_date": "2025-02-21 13:05:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:03:46.707178"
    },
    {
      "arxiv_id": "2502.15435v1",
      "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Leyla Naz Candogan",
        "Yongtao Wu",
        "Elias Abad Rocamora",
        "Grigorios G. Chrysos",
        "Volkan Cevher"
      ],
      "abstract": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）防御越狱攻击（jailbreaking attacks）的挑战，提出了一种名为 Single Pass Detection (SPD) 的方法，该方法利用 logits 信息在单次前向传递中预测输出是否有害，从而实现高效防御。SPD 不仅在开源模型上有效检测攻击，还能最小化对无害输入的误分类，即使在 GPT-3.5 和 GPT-4 等模型中不完全访问 logits 时，依然保持良好性能。实验结果表明，该方法为保护 LLMs 免受对抗性攻击提供了一个高效且可靠的途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in TMLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15435v1",
      "published_date": "2025-02-21 13:04:13 UTC",
      "updated_date": "2025-02-21 13:04:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:03:59.094863"
    },
    {
      "arxiv_id": "2502.15865v1",
      "title": "Position: Standard Benchmarks Fail -- LLM Agents Present Overlooked Risks for Financial Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Zichen Chen",
        "Jiaao Chen",
        "Jianda Chen",
        "Misha Sra"
      ],
      "abstract": "Current financial LLM agent benchmarks are inadequate. They prioritize task\nperformance while ignoring fundamental safety risks. Threats like\nhallucinations, temporal misalignment, and adversarial vulnerabilities pose\nsystemic risks in high-stakes financial environments, yet existing evaluation\nframeworks fail to capture these risks. We take a firm position: traditional\nbenchmarks are insufficient to ensure the reliability of LLM agents in finance.\nTo address this, we analyze existing financial LLM agent benchmarks, finding\nsafety gaps and introducing ten risk-aware evaluation metrics. Through an\nempirical evaluation of both API-based and open-weight LLM agents, we reveal\nhidden vulnerabilities that remain undetected by conventional assessments. To\nmove the field forward, we propose the Safety-Aware Evaluation Agent (SAEA),\ngrounded in a three-level evaluation framework that assesses agents at the\nmodel level (intrinsic capabilities), workflow level (multi-step process\nreliability), and system level (integration robustness). Our findings highlight\nthe urgent need to redefine LLM agent evaluation standards by shifting the\nfocus from raw performance to safety, robustness, and real world resilience.",
      "tldr_zh": "该论文认为，现有的金融 LLM Agents 基准评估标准存在缺陷，仅关注任务性能而忽略了关键安全风险，如 hallucinations、temporal misalignment 和对抗性漏洞，这些可能在高风险金融环境中引发系统性问题。作者分析了当前基准的不足，引入了十个风险感知评估指标，并通过实证评估揭示了 API-based 和 open-weight LLM Agents 的隐藏漏洞。最终，他们提出 Safety-Aware Evaluation Agent (SAEA)，基于三层评估框架（模型级、工作流级和系统级）来评估代理的内在能力、多步过程可靠性和集成鲁棒性，并呼吁重新定义评估标准，以优先安全、鲁棒性和真实世界适应性。",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-fin.GN",
      "comment": "40 pages, 2 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15865v1",
      "published_date": "2025-02-21 12:56:15 UTC",
      "updated_date": "2025-02-21 12:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:04:12.384265"
    },
    {
      "arxiv_id": "2502.15425v4",
      "title": "TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning",
      "title_zh": "TAG：多智能体分层强化学习去中心化框架",
      "authors": [
        "Giuseppe Paolo",
        "Abdelhakim Benechehab",
        "Hamza Cherkaoui",
        "Albert Thomas",
        "Balázs Kégl"
      ],
      "abstract": "Hierarchical organization is fundamental to biological systems and human\nsocieties, yet artificial intelligence systems often rely on monolithic\narchitectures that limit adaptability and scalability. Current hierarchical\nreinforcement learning (HRL) approaches typically restrict hierarchies to two\nlevels or require centralized training, which limits their practical\napplicability. We introduce TAME Agent Framework (TAG), a framework for\nconstructing fully decentralized hierarchical multi-agent systems. TAG enables\nhierarchies of arbitrary depth through a novel LevelEnv concept, which\nabstracts each hierarchy level as the environment for the agents above it. This\napproach standardizes information flow between levels while preserving loose\ncoupling, allowing for seamless integration of diverse agent types. We\ndemonstrate the effectiveness of TAG by implementing hierarchical architectures\nthat combine different RL agents across multiple levels, achieving improved\nperformance over classical multi-agent RL baselines on standard benchmarks. Our\nresults show that decentralized hierarchical organization enhances both\nlearning speed and final performance, positioning TAG as a promising direction\nfor scalable multi-agent systems.",
      "tldr_zh": "这篇论文介绍了 TAG 框架，一种用于多智能体 Hierarchical Reinforcement Learning (HRL) 的去中心化框架，旨在解决现有 HRL 方法受限于两层结构或集中式训练的局限性。TAG 通过 LevelEnv 概念将每个层级抽象为上层智能体的环境，实现任意深度的层级系统，同时标准化信息流并保持松散耦合，便于整合不同类型智能体。实验结果表明，TAG 在标准基准测试中比经典多智能体 RL 基线提升了学习速度和最终性能，为可扩展的多智能体系统提供了有前景的方向。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15425v4",
      "published_date": "2025-02-21 12:52:16 UTC",
      "updated_date": "2025-03-05 10:48:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:04:24.674262"
    },
    {
      "arxiv_id": "2502.15424v1",
      "title": "Anatomy-Informed Deep Learning and Radiomics for Automated Neurofibroma Segmentation in Whole-Body MRI",
      "title_zh": "基于解剖学的深度学习和放射组学用于全身磁共振成像中神经纤维瘤的自动",
      "authors": [
        "Georgii Kolokolnikov",
        "Marie-Lena Schmalhofer",
        "Lennart Well",
        "Said Farschtschi",
        "Victor-Felix Mautner",
        "Inka Ristow",
        "Rene Werner"
      ],
      "abstract": "Neurofibromatosis Type 1 is a genetic disorder characterized by the\ndevelopment of neurofibromas (NFs), which exhibit significant variability in\nsize, morphology, and anatomical location. Accurate and automated segmentation\nof these tumors in whole-body magnetic resonance imaging (WB-MRI) is crucial to\nassess tumor burden and monitor disease progression. In this study, we present\nand analyze a fully automated pipeline for NF segmentation in fat-suppressed\nT2-weighted WB-MRI, consisting of three stages: anatomy segmentation, NF\nsegmentation, and tumor candidate classification. In the first stage, we use\nthe MRSegmentator model to generate an anatomy segmentation mask, extended with\na high-risk zone for NFs. This mask is concatenated with the input image as\nanatomical context information for NF segmentation. The second stage employs an\nensemble of 3D anisotropic anatomy-informed U-Nets to produce an NF\nsegmentation confidence mask. In the final stage, tumor candidates are\nextracted from the confidence mask and classified based on radiomic features,\ndistinguishing tumors from non-tumor regions and reducing false positives. We\nevaluate the proposed pipeline on three test sets representing different\nconditions: in-domain data (test set 1), varying imaging protocols and field\nstrength (test set 2), and low tumor burden cases (test set 3). Experimental\nresults show a 68% improvement in per-scan Dice Similarity Coefficient (DSC), a\n21% increase in per-tumor DSC, and a two-fold improvement in F1 score for tumor\ndetection in high tumor burden cases by integrating anatomy information. The\nmethod is integrated into the 3D Slicer platform for practical clinical use,\nwith the code publicly accessible.",
      "tldr_zh": "本文提出一个全自动管道，用于在全身 MRI 中精确分割 Neurofibromatosis Type 1 患者的神经纤维瘤（NFs），以评估肿瘤负担和监测疾病进展。该管道分为三个阶段：首先使用 MRSegmentator 模型生成解剖结构分割掩码并扩展高风险区；其次采用 3D anisotropic anatomy-informed U-Nets 集合产生 NF 分割置信掩码；最后基于 radiomic 特征对肿瘤候选进行分类，减少假阳性。实验结果显示，整合解剖信息后，per-scan Dice Similarity Coefficient (DSC) 提高了 68%，per-tumor DSC 增加了 21%，并使高肿瘤负担病例的 F1 分数提升一倍。该方法已整合到 3D Slicer 平台，并公开代码，以促进临床应用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15424v1",
      "published_date": "2025-02-21 12:49:35 UTC",
      "updated_date": "2025-02-21 12:49:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:04:36.957486"
    },
    {
      "arxiv_id": "2502.15422v1",
      "title": "Evaluating Multimodal Generative AI with Korean Educational Standards",
      "title_zh": "翻译失败",
      "authors": [
        "Sanghee Park",
        "Geewook Kim"
      ],
      "abstract": "This paper presents the Korean National Educational Test Benchmark (KoNET), a\nnew benchmark designed to evaluate Multimodal Generative AI Systems using\nKorean national educational tests. KoNET comprises four exams: the Korean\nElementary General Educational Development Test (KoEGED), Middle (KoMGED), High\n(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are\nrenowned for their rigorous standards and diverse questions, facilitating a\ncomprehensive analysis of AI performance across different educational levels.\nBy focusing on Korean, KoNET provides insights into model performance in\nless-explored languages. We assess a range of models - open-source,\nopen-access, and closed APIs - by examining difficulties, subject diversity,\nand human error rates. The code and dataset builder will be made fully\nopen-sourced at https://github.com/naver-ai/KoNET.",
      "tldr_zh": "这篇论文提出了 KoNET 基准，用于评估 Multimodal Generative AI 系统在韩国国家教育测试中的表现。KoNET 包括四个考试：KoEGED（小学水平）、KoMGED（中学水平）、KoHGED（高中水平）和 KoCSAT（大学入学考试），这些考试以严格标准和多样化问题为核心，分析 AI 在不同教育水平的性能。论文重点考察了各种模型（包括开源、开放访问和封闭 API）的难度、科目多样性以及与人类错误率的比较，提供对非主流语言如韩语的 AI 表现洞见。代码和数据集将完全开源在 GitHub 上。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages; To appear at NAACL 2025 Main Conference (Project page:\n  https://github.com/naver-ai/KoNET )",
      "pdf_url": "http://arxiv.org/pdf/2502.15422v1",
      "published_date": "2025-02-21 12:46:40 UTC",
      "updated_date": "2025-02-21 12:46:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:04:47.679634"
    },
    {
      "arxiv_id": "2502.15419v1",
      "title": "Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking",
      "title_zh": "超越翻译：基于LLM的数据生成用于多语言事实核查",
      "authors": [
        "Yi-Ling Chung",
        "Aurora Cobo",
        "Pablo Serna"
      ],
      "abstract": "Robust automatic fact-checking systems have the potential to combat online\nmisinformation at scale. However, most existing research primarily focuses on\nEnglish. In this paper, we introduce MultiSynFact, the first large-scale\nmultilingual fact-checking dataset containing 2.2M claim-source pairs designed\nto support Spanish, German, English, and other low-resource languages. Our\ndataset generation pipeline leverages Large Language Models (LLMs), integrating\nexternal knowledge from Wikipedia and incorporating rigorous claim validation\nsteps to ensure data quality. We evaluate the effectiveness of MultiSynFact\nacross multiple models and experimental settings. Additionally, we open-source\na user-friendly framework to facilitate further research in multilingual\nfact-checking and dataset generation.",
      "tldr_zh": "这篇论文介绍了 MultiSynFact，这是首个大规模多语言事实检查数据集，包含 2.2M 条 claim-source 对，支持西班牙语、德语、英语及其他低资源语言。研究团队使用 Large Language Models (LLMs) 整合 Wikipedia 的外部知识，并采用严格的 claim 验证步骤来生成高质量数据集。实验结果显示，MultiSynFact 在多种模型和设置中表现出色，并开源了一个用户友好的框架，以推动多语言 fact-checking 和数据集生成的研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 1 figure, 18 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15419v1",
      "published_date": "2025-02-21 12:38:26 UTC",
      "updated_date": "2025-02-21 12:38:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:04:58.747396"
    },
    {
      "arxiv_id": "2502.15411v2",
      "title": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings Filings",
      "title_zh": "翻译失败",
      "authors": [
        "Rasmus Aavang",
        "Giovanni Rizzi",
        "Rasmus Bøggild",
        "Alexandre Iolov",
        "Mike Zhang",
        "Johannes Bjerva"
      ],
      "abstract": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts.",
      "tldr_zh": "该论文引入了 HiFi-KPI 数据集，用于从非结构化财务文本中提取分层关键绩效指标 (KPI)，以解决 iXBRL 标准的标签转移性问题。数据集基于 iXBRL 分类法组织了 218,126 个标签的层次结构，包括约 1.8M 段落和 5M 实体，每个实体链接到计算和呈现分类法。作者提供了基于编码器的基线方法以及使用 Large Language Models (LLMs) 的结构化提取方法，并发布了简化子集 HiFi-KPI Lite，以便简化 LLM 推理和评估。所有数据集和工件均公开发布，促进了财务文本分析领域的进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15411v2",
      "published_date": "2025-02-21 12:19:08 UTC",
      "updated_date": "2025-02-24 14:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:05:13.602067"
    },
    {
      "arxiv_id": "2502.17501v1",
      "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
      "title_zh": "CoKV：通过合作博弈优化 KV 缓存分配",
      "authors": [
        "Qiheng Sun",
        "Hongwei Zhang",
        "Haocheng Xia",
        "Jiayao Zhang",
        "Jinfei Liu",
        "Kui Ren"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
      "tldr_zh": "本论文针对大型语言模型(LLMs)中 KV Cache 存储导致的内存消耗问题，提出 CoKV 方法，通过将注意力头之间的合作建模为合作游戏，评估每个头的贡献来优化缓存预算分配。这种方法克服了现有方法的局限性，即忽略头间的协同作用。实验结果显示，CoKV 在 LongBench 基准上，使用 LLama-3-8B-Instruct 和 Mistral-7B 模型，实现了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17501v1",
      "published_date": "2025-02-21 12:03:07 UTC",
      "updated_date": "2025-02-21 12:03:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:05:26.748052"
    },
    {
      "arxiv_id": "2502.15401v1",
      "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Xuetao Ma",
        "Wenbin Jiang",
        "Hua Huang"
      ],
      "abstract": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently.",
      "tldr_zh": "本文提出了一种基于问题-solving logic指导的curriculum in-context learning (ICL)策略，用于提升大型语言模型 (LLMs) 的复杂推理能力。该策略通过分析问题-solving logic从BREAK数据集构建指令集，并微调语言模型来选择演示示例，同时根据问题解决步骤数量评估难度，并按从易到难的curriculum learning原则排序示例。实验结果表明，该方法在多个基准上超越了现有ICL方法，在性能和效率方面均有显著提升。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15401v1",
      "published_date": "2025-02-21 12:00:10 UTC",
      "updated_date": "2025-02-21 12:00:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:05:38.298785"
    },
    {
      "arxiv_id": "2502.15398v1",
      "title": "Enhancing Vehicle Make and Model Recognition with 3D Attention Modules",
      "title_zh": "使用 3D 注意力模块增强车辆品牌和型号识别",
      "authors": [
        "Narges Semiromizadeh",
        "Omid Nejati Manzari",
        "Shahriar B. Shokouhi",
        "Sattar Mirzakuchaki"
      ],
      "abstract": "Vehicle make and model recognition (VMMR) is a crucial component of the\nIntelligent Transport System, garnering significant attention in recent years.\nVMMR has been widely utilized for detecting suspicious vehicles, monitoring\nurban traffic, and autonomous driving systems. The complexity of VMMR arises\nfrom the subtle visual distinctions among vehicle models and the wide variety\nof classes produced by manufacturers. Convolutional Neural Networks (CNNs), a\nprominent type of deep learning model, have been extensively employed in\nvarious computer vision tasks, including VMMR, yielding remarkable results. As\nVMMR is a fine-grained classification problem, it primarily faces inter-class\nsimilarity and intra-class variation challenges. In this study, we implement an\nattention module to address these challenges and enhance the model's focus on\ncritical areas containing distinguishing features. This module, which does not\nincrease the parameters of the original model, generates three-dimensional\n(3-D) attention weights to refine the feature map. Our proposed model\nintegrates the attention module into two different locations within the middle\nsection of a convolutional model, where the feature maps from these sections\noffer sufficient information about the input frames without being overly\ndetailed or overly coarse. The performance of our proposed model, along with\nstate-of-the-art (SOTA) convolutional and transformer-based models, was\nevaluated using the Stanford Cars dataset. Our proposed model achieved the\nhighest accuracy, 90.69\\%, among the compared models.",
      "tldr_zh": "该研究针对车辆品牌和型号识别（VMMR）的细粒度分类挑战，如类间相似性和类内变异，提出了一种集成3D Attention Modules的增强方法。 该模块在CNN模型中间部分生成三维注意力权重，以细化特征图，同时不增加模型参数，从而提高模型对关键区域的关注。 在Stanford Cars数据集上，实验结果显示，该模型达到了90.69%的准确率，优于现有SOTA模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15398v1",
      "published_date": "2025-02-21 11:52:56 UTC",
      "updated_date": "2025-02-21 11:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:05:49.631347"
    },
    {
      "arxiv_id": "2502.15397v1",
      "title": "Super-Resolution for Interferometric Imaging: Model Comparisons and Performance Analysis",
      "title_zh": "用于干涉成像的超分辨率：模型比较和性能分析",
      "authors": [
        "Hasan Berkay Abdioglu",
        "Rana Gursoy",
        "Yagmur Isik",
        "Ibrahim Cem Balci",
        "Taha Unal",
        "Kerem Bayer",
        "Mustafa Ismail Inal",
        "Nehir Serin",
        "Muhammed Furkan Kosar",
        "Gokhan Bora Esmer",
        "Huseyin Uvet"
      ],
      "abstract": "This study investigates the application of Super-Resolution techniques in\nholographic microscopy to enhance quantitative phase imaging. An off-axis\nMach-Zehnder interferometric setup was employed to capture interferograms. The\nstudy evaluates two Super-Resolution models, RCAN and Real-ESRGAN, for their\neffectiveness in reconstructing high-resolution interferograms from a\nmicroparticle-based dataset. The models were assessed using two primary\napproaches: image-based analysis for structural detail enhancement and\nmorphological evaluation for maintaining sample integrity and phase map\naccuracy. The results demonstrate that RCAN achieves superior numerical\nprecision, making it ideal for applications requiring highly accurate phase map\nreconstruction, while Real-ESRGAN enhances visual quality and structural\ncoherence, making it suitable for visualization-focused applications. This\nstudy highlights the potential of Super-Resolution models in overcoming\ndiffraction-imposed resolution limitations in holographic microscopy, opening\nthe way for improved imaging techniques in biomedical diagnostics, materials\nscience, and other high-precision fields.",
      "tldr_zh": "这篇论文探讨了在全息显微镜中应用 Super-Resolution 技术来提升定量相位成像，使用 off-axis Mach-Zehnder 干涉仪捕获干涉图。研究比较了 RCAN 和 Real-ESRGAN 两个模型，通过图像分析（增强结构细节）和形态学评估（维护样本完整性及相位图准确性）来评估它们的性能。结果显示，RCAN 在数值精度上表现出色，适合高精度相位图重建应用，而 Real-ESRGAN 则更擅长提升视觉质量和结构一致性，适用于可视化任务。该研究突出了 Super-Resolution 模型在克服全息显微镜衍射限制方面的潜力，为生物医学诊断、材料科学等高精度领域提供了新途径。",
      "categories": [
        "physics.optics",
        "cs.AI"
      ],
      "primary_category": "physics.optics",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15397v1",
      "published_date": "2025-02-21 11:50:57 UTC",
      "updated_date": "2025-02-21 11:50:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:06:03.522463"
    },
    {
      "arxiv_id": "2502.15392v1",
      "title": "Chitrarth: Bridging Vision and Language for a Billion People",
      "title_zh": "Chitrarth：桥接视觉和语言以服务数十亿人",
      "authors": [
        "Shaharukh Khan",
        "Ayush Tarun",
        "Abhinav Ravi",
        "Ali Faraz",
        "Akshat Patidar",
        "Praveen Kumar Pokala",
        "Anagha Bhangare",
        "Raja Kolla",
        "Chandra Khatri",
        "Shubham Agarwal"
      ],
      "abstract": "Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.",
      "tldr_zh": "该研究引入 Chitrarth，一种视觉语言模型 (VLM)，旨在桥接视觉和语言，支持10种印度语言的多样性和视觉推理，以解决现有多模态模型主要依赖英语或高资源语言的局限性。Chitrarth 通过整合先进的 multilingual Large Language Model (LLM) 和视觉模块，并使用多语言图像文本数据进行训练，实现高效的多语言处理。同时，研究者开发了 BharatBench 评估框架，用于全面评估 VLMs 在印度语言中的性能，结果显示 Chitrarth 在低资源语言基准上达到 SOTA 水平，同时保持英语效率，为多语言多模态 AI 系统的进步奠定基础。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15392v1",
      "published_date": "2025-02-21 11:38:40 UTC",
      "updated_date": "2025-02-21 11:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:06:14.179657"
    },
    {
      "arxiv_id": "2502.17500v1",
      "title": "Generalized Exponentiated Gradient Algorithms Using the Euler Two-Parameter Logarithm",
      "title_zh": "使用欧拉双参数对数的广义指数梯度算法",
      "authors": [
        "Andrzej Cichocki"
      ],
      "abstract": "In this paper we propose and investigate a new class of Generalized\nExponentiated Gradient (GEG) algorithms using Mirror Descent (MD) approaches,\nand applying as a regularization function the Bregman divergence with\ntwo-parameter deformation of logarithm as a link function. This link function\n(referred to as the Euler logarithm) is associated with a wide class of\ngeneralized entropies. In order to derive novel GEG/MD updates, we estimate\ngeneralized exponential function, which closely approximates the inverse of the\nEuler two-parameter logarithm. The characteristic/shape and properties of the\nEuler logarithm and its inverse -- deformed exponential functions are tuned by\ntwo or even more hyperparameters. By learning these hyperparameters, we can\nadapt to distribution of training data, and we can adjust them to achieve\ndesired properties of gradient descent algorithms. The concept of generalized\nentropies and associated deformed logarithms provide deeper insight into novel\ngradient descent updates.\n  In literature, there exist nowadays over fifty mathematically well-defined\nentropic functionals and associated deformed logarithms, so impossible to\ninvestigate all of them in one research paper. Therefore, we focus here on a\nwide-class of trace-form entropies and associated generalized logarithm. We\napplied the developed algorithms for Online Portfolio Selection (OPLS) in order\nto improve its performance and robustness.",
      "tldr_zh": "本论文提出了一种新的 Generalized Exponentiated Gradient (GEG) 算法类别，使用 Mirror Descent (MD) 方法和基于 Euler 两参数对数的 Bregman divergence 作为正则化函数，以处理广义熵相关的优化问题。通过估计广义指数函数来近似 Euler 对数的逆函数，并通过学习超参数适应训练数据分布，从而调整算法的特性以提升性能。实验结果显示，该方法能改善梯度下降算法的属性，并应用于 Online Portfolio Selection (OPLS)，显著提高了其鲁棒性和整体表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, preprint of Journal paper",
      "pdf_url": "http://arxiv.org/pdf/2502.17500v1",
      "published_date": "2025-02-21 11:05:04 UTC",
      "updated_date": "2025-02-21 11:05:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:06:25.723949"
    },
    {
      "arxiv_id": "2502.15365v2",
      "title": "Identifying Features that Shape Perceived Consciousness in Large Language Model-based AI: A Quantitative Study of Human Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Bongsu Kang",
        "Jundong Kim",
        "Tae-Rim Yun",
        "Hyojin Bae",
        "Chang-Eop Kim"
      ],
      "abstract": "This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction.",
      "tldr_zh": "本研究通过定量调查，考察了Large Language Model (LLM)-based AI生成的文本中八个特征（如metacognitive self-reflection、logical reasoning、empathy等）如何影响人类对AI感知意识，使用Claude 3 Opus的99个对话段落和123名参与者进行分析。结果显示，metacognitive self-reflection和AI表达自身情绪显著提高了感知到的AI意识，而强调knowledge则降低了这种感知；参与者分为七个子群，每组对特征的权重模式不同。研究还发现，参与者对LLM的先验知识和使用频率越高，越倾向于评估AI具有意识，这为探索人类-AI互动的心理社会影响提供了重要基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "I.2.7; K.4"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages, 3 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15365v2",
      "published_date": "2025-02-21 10:27:28 UTC",
      "updated_date": "2025-02-25 01:40:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:06:39.480053"
    },
    {
      "arxiv_id": "2502.15861v1",
      "title": "C3AI: Crafting and Evaluating Constitutions for Constitutional AI",
      "title_zh": "翻译失败",
      "authors": [
        "Yara Kyrychenko",
        "Ke Zhou",
        "Edyta Bogucka",
        "Daniele Quercia"
      ],
      "abstract": "Constitutional AI (CAI) guides LLM behavior using constitutions, but\nidentifying which principles are most effective for model alignment remains an\nopen challenge. We introduce the C3AI framework (\\textit{Crafting Constitutions\nfor CAI models}), which serves two key functions: (1) selecting and structuring\nprinciples to form effective constitutions before fine-tuning; and (2)\nevaluating whether fine-tuned CAI models follow these principles in practice.\nBy analyzing principles from AI and psychology, we found that positively\nframed, behavior-based principles align more closely with human preferences\nthan negatively framed or trait-based principles. In a safety alignment use\ncase, we applied a graph-based principle selection method to refine an existing\nCAI constitution, improving safety measures while maintaining strong general\nreasoning capabilities. Interestingly, fine-tuned CAI models performed well on\nnegatively framed principles but struggled with positively framed ones, in\ncontrast to our human alignment results. This highlights a potential gap\nbetween principle design and model adherence. Overall, C3AI provides a\nstructured and scalable approach to both crafting and evaluating CAI\nconstitutions.",
      "tldr_zh": "这篇论文提出了C3AI框架，用于为Constitutional AI (CAI)模型创建和评估宪法，以指导LLM的行为。框架包括在微调前选择和结构化原则（如从AI和心理学中分析的积极框架、基于行为的原则），以及评估模型是否实际遵守这些原则。研究发现，积极框架的基于行为原则更符合人类偏好，但在安全对齐用例中，微调后的CAI模型在负面框架原则上表现良好，而在积极框架原则上则较差，这突显了原则设计与模型遵守之间的潜在差距。总体上，C3AI提供了一个结构化、可扩展的方法，改进了CAI的模型对齐效果，同时在保持通用推理能力的基础上提升了安全性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "This has been accepted for the Web Conference 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15861v1",
      "published_date": "2025-02-21 10:26:42 UTC",
      "updated_date": "2025-02-21 10:26:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:06:51.234364"
    },
    {
      "arxiv_id": "2502.15860v2",
      "title": "Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Arefeh Kazemi",
        "Sri Balaaji Natarajan Kalaivendan",
        "Joachim Wagner",
        "Hamza Qadeer",
        "Brian Davis"
      ],
      "abstract": "Cyberbullying (CB) presents a pressing threat, especially to children,\nunderscoring the urgent need for robust detection systems to ensure online\nsafety. However, progress in developing such systems is hindered by the\nscarcity of large, labeled datasets that are specifically tailored for\nspecialized tasks and the target age groups. Creating these datasets relies\nheavily on human annotation, which not only strains resources but also raises\nsignificant ethical and legal concerns due to annotators' exposure to harmful\ncontent, notwithstanding the acquisition of this type of data from vulnerable\npopulations such as children. In this paper, we address these challenges by\nleveraging Large Language Models (LLMs) to generate synthetic data and labels.\nOur experiments demonstrate that synthetic data enables BERT-based CB\nclassifiers to achieve performance close to that of those trained on fully\nauthentic datasets (75.8% vs. 81.5% accuracy). Additionally, LLMs can\neffectively label authentic yet unlabeled data, allowing BERT classifiers to\nattain a comparable performance level (79.1% vs. 81.5% accuracy). These results\nhighlight the potential of LLMs as a scalable, ethical, and cost-effective\nsolution for generating data for CB detection.",
      "tldr_zh": "本研究探讨了使用 Large Language Models (LLMs) 生成合成数据和标签，以解决网络霸凌 (Cyberbullying) 检测中数据稀缺的问题，避免了人工标注的资源消耗和伦理挑战。实验结果显示，基于合成数据的 BERT 分类器准确率达到 75.8%，接近使用真实数据集的 81.5%；而 LLMs 对真实未标注数据的标注，使 BERT 分类器准确率提升至 79.1%。这些发现突显了 LLMs 作为一种可扩展、伦理友好且成本有效的解决方案，在提升 Cyberbullying 检测系统性能方面具有重要潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15860v2",
      "published_date": "2025-02-21 10:17:29 UTC",
      "updated_date": "2025-04-05 09:42:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:07:03.169306"
    },
    {
      "arxiv_id": "2502.15859v3",
      "title": "AI Governance InternationaL Evaluation Index (AGILE Index)",
      "title_zh": "人工智能治理国际评价指数 (AGILE Index)",
      "authors": [
        "Yi Zeng",
        "Enmeng Lu",
        "Xin Guan",
        "Cunqing Huangfu",
        "Zizhe Ruan",
        "Ammar Younas",
        "Kang Sun",
        "Xuan Tang",
        "Yuwei Wang",
        "Hongjie Suo",
        "Dongqi Liang",
        "Zhengqiang Han",
        "Aorigele Bao",
        "Xiaoyang Guo",
        "Jin Wang",
        "Jiawei Xie",
        "Yao Liang"
      ],
      "abstract": "The rapid advancement of Artificial Intelligence (AI) technology is\nprofoundly transforming human society and concurrently presenting a series of\nethical, legal, and social issues. The effective governance of AI has become a\ncrucial global concern. Since 2022, the extensive deployment of generative AI,\nparticularly large language models, marked a new phase in AI governance.\nContinuous efforts are being made by the international community in actively\naddressing the novel challenges posed by these AI developments. As consensus on\ninternational governance continues to be established and put into action, the\npractical importance of conducting a global assessment of the state of AI\ngovernance is progressively coming to light. In this context, we initiated the\ndevelopment of the AI Governance InternationaL Evaluation Index (AGILE Index).\nAdhering to the design principle, \"the level of governance should match the\nlevel of development,\" the inaugural evaluation of the AGILE Index commences\nwith an exploration of four foundational pillars: the development level of AI,\nthe AI governance environment, the AI governance instruments, and the AI\ngovernance effectiveness. It covers 39 indicators across 18 dimensions to\ncomprehensively assess the AI governance level of 14 representative countries\nglobally. The index is utilized to delve into the status of AI governance to\ndate in 14 countries for the first batch of evaluation. The aim is to depict\nthe current state of AI governance in these countries through data scoring,\nassist them in identifying their governance stage and uncovering governance\nissues, and ultimately offer insights for the enhancement of their AI\ngovernance systems.",
      "tldr_zh": "本文提出 AI Governance InternationaL Evaluation Index (AGILE Index)，一个用于评估全球 AI 治理水平的指数，旨在应对 AI 快速发展带来的伦理、法律和社会挑战。AGILE Index 遵循“治理水平应匹配发展水平”的原则，通过四个支柱（AI 发展水平、AI 治理环境、AI 治理 instruments 和 AI 治理 effectiveness）及 39 个指标，评估了 14 个代表性国家的 AI 治理现状。研究结果通过数据评分帮助这些国家识别治理阶段、发现问题，并提供优化 AI 治理系统的洞见。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "68T01",
        "A.1"
      ],
      "primary_category": "cs.CY",
      "comment": "Evaluation Report. 85 pages, 30 Figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15859v3",
      "published_date": "2025-02-21 10:16:56 UTC",
      "updated_date": "2025-03-04 07:10:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:07:15.520374"
    },
    {
      "arxiv_id": "2502.15361v1",
      "title": "Evaluating Social Biases in LLM Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Xuyang Wu",
        "Jinming Nian",
        "Zhiqiang Tao",
        "Yi Fang"
      ],
      "abstract": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning.",
      "tldr_zh": "本研究评估了大型语言模型 (LLMs) 在链式思维推理中的社会偏见问题，指出偏见可能融入推理步骤并放大有害结果，如导致幻觉。研究者使用 BBQ 数据集测试了 DeepSeek-R1 的 8B 和 32B 变体及其指令微调版本，比较了偏见在推理过程中的放大效应。这是首个针对 LLM 推理偏见的实证研究，为缓解此类问题提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15361v1",
      "published_date": "2025-02-21 10:16:07 UTC",
      "updated_date": "2025-02-21 10:16:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:07:28.128183"
    },
    {
      "arxiv_id": "2502.15359v3",
      "title": "ARS: Automatic Routing Solver with Large Language Models",
      "title_zh": "ARS：基于大型语言模型的自动路由求解器",
      "authors": [
        "Kai Li",
        "Fei Liu",
        "Zhenkun Wang",
        "Xialiang Tong",
        "Xiongwei Han",
        "Mingxuan Yuan",
        "Qingfu Zhang"
      ],
      "abstract": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks.",
      "tldr_zh": "该研究针对实世界车辆路径问题 (VRPs) 的复杂约束问题，引入了 RoutBench 基准，该基准包含 1,000 个由 24 个属性派生的 VRP 变体，用于评估自动路由求解器的性能。\n论文提出 Automatic Routing Solver (ARS)，利用 Large Language Models (LLM) 代理基于问题描述和数据库自动生成约束感知的启发式代码，以增强骨干算法框架。\n实验结果显示，ARS 优于最先进的 LLM 基于方法和常用求解器，成功解决 91.67% 的常见 VRP，并在所有基准上至少提高了 30%。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Authorship is under discussion; arXiv release will follow\n  finalization",
      "pdf_url": "http://arxiv.org/pdf/2502.15359v3",
      "published_date": "2025-02-21 10:14:55 UTC",
      "updated_date": "2025-05-19 08:29:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:07:42.075045"
    },
    {
      "arxiv_id": "2502.15357v1",
      "title": "Integrating Generative AI in Cybersecurity Education: Case Study Insights on Pedagogical Strategies, Critical Thinking, and Responsible AI Use",
      "title_zh": "在网络安全教育中整合生成式人工智能：关于教学策略、批判性思维和负责任人工智能使用的案例研究洞见",
      "authors": [
        "Mahmoud Elkhodr",
        "Ergun Gide"
      ],
      "abstract": "The rapid advancement of Generative Artificial Intelligence (GenAI) has\nintroduced new opportunities for transforming higher education, particularly in\nfields that require analytical reasoning and regulatory compliance, such as\ncybersecurity management. This study presents a structured framework for\nintegrating GenAI tools into cybersecurity education, demonstrating their role\nin fostering critical thinking, real-world problem-solving, and regulatory\nawareness. The implementation strategy followed a two-stage approach, embedding\nGenAI within tutorial exercises and assessment tasks. Tutorials enabled\nstudents to generate, critique, and refine AI-assisted cybersecurity policies,\nwhile assessments required them to apply AI-generated outputs to real-world\nscenarios, ensuring alignment with industry standards and regulatory\nrequirements. Findings indicate that AI-assisted learning significantly\nenhanced students' ability to evaluate security policies, refine risk\nassessments, and bridge theoretical knowledge with practical application.\nStudent reflections and instructor observations revealed improvements in\nanalytical engagement, yet challenges emerged regarding AI over-reliance,\nvariability in AI literacy, and the contextual limitations of AI-generated\ncontent. Through structured intervention and research-driven refinement,\nstudents were able to recognize AI strengths as a generative tool while\nacknowledging its need for human oversight. This study further highlights the\nbroader implications of AI adoption in cybersecurity education, emphasizing the\nnecessity of balancing automation with expert judgment to cultivate\nindustry-ready professionals. Future research should explore the long-term\nimpact of AI-driven learning on cybersecurity competency, as well as the\npotential for adaptive AI-assisted assessments to further personalize and\nenhance educational outcomes.",
      "tldr_zh": "这篇论文提出一个结构化框架，将生成式 AI (GenAI) 整合到网络安全教育中，旨在提升学生的批判性思维、实际问题解决能力和监管意识。研究采用两阶段方法，包括在教程中让学生生成、批判和完善 AI 辅助的安全政策，以及在评估中应用这些输出到真实场景，以确保符合行业标准。结果显示，AI 辅助学习显著提高了学生评估安全政策和风险评估的能力，并桥接了理论与实践，但也面临 AI 过度依赖、AI 素养差异和内容上下文限制的挑战。通过干预，学生认识到 AI 的优势需结合人类监督，以培养负责任的行业-ready 专业人士。该研究强调平衡自动化与专家判断，并建议未来探索 AI 对网络安全能力长期影响和自适应评估的潜力。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "68T05",
        "I.2.6; I.2.7; K.3.1; K.3.2"
      ],
      "primary_category": "cs.CY",
      "comment": "30 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.15357v1",
      "published_date": "2025-02-21 10:14:07 UTC",
      "updated_date": "2025-02-21 10:14:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:07:54.063038"
    },
    {
      "arxiv_id": "2502.15348v1",
      "title": "Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models",
      "title_zh": "为儿童科学绘图构建规范：基于大语言模型语义相似性的分布特征",
      "authors": [
        "Yi Zhang",
        "Fan Wei",
        "Jingyi Li",
        "Yan Wang",
        "Yanyan Yu",
        "Jianli Chen",
        "Zipo Cai",
        "Xinyu Liu",
        "Wei Wang",
        "Peng Wang",
        "Zhong Wang"
      ],
      "abstract": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class.",
      "tldr_zh": "本研究针对儿童科学绘画用于考察概念理解的局限性（如任务依赖性和主观解读），使用Large Language Model (LLM)识别1420张覆盖9个科学主题的绘画，并通过word2vec算法计算语义相似度，以探索同一主题下儿童绘画的一致性。结果显示，大多数绘画表现出高度一致性（语义相似度大于0.8），且这种一致性独立于LLM识别的准确性，揭示了存在的一致性偏差。研究进一步运用Kendall rank correlation coefficient分析样本大小、抽象度和焦点点等影响因素，并通过词频统计考察儿童对抽象主题的再现，旨在建立儿童科学绘画的标准规范，作为后续研究的基准参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15348v1",
      "published_date": "2025-02-21 10:02:15 UTC",
      "updated_date": "2025-02-21 10:02:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:08:04.935650"
    },
    {
      "arxiv_id": "2504.05317v1",
      "title": "On Synthesizing Data for Context Attribution in Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Gorjan Radevski",
        "Kiril Gashteovski",
        "Shahbaz Syed",
        "Christopher Malon",
        "Sebastien Nicolas",
        "Chia-Chien Hung",
        "Timo Sztyler",
        "Verena Heußer",
        "Wiem Ben Rim",
        "Masafumi Enomoto",
        "Kunihiro Takeoka",
        "Masafumi Oyamada",
        "Goran Glavaš",
        "Carolin Lawrence"
      ],
      "abstract": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.",
      "tldr_zh": "这篇论文探讨了在问答（QA）系统中，如何通过上下文归因（context attribution）来解决大型语言模型（LLM）的幻觉（hallucinations）问题，从而提升模型的可信度。研究比较了zero-shot inference、LLM ensembling和使用合成数据fine-tuning小LM的三种方法，并提出了一种新策略SynQA，利用LLM基于选定上下文句子生成QA对，确保归因路径清晰。实验结果显示，SynQA生成的合成数据在不同QA任务和领域中有效提升了小LM的性能，最终通过用户研究验证了这些模型在实际上下文归因中的实用价值。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05317v1",
      "published_date": "2025-02-21 09:43:18 UTC",
      "updated_date": "2025-02-21 09:43:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:08:16.639217"
    },
    {
      "arxiv_id": "2502.15336v1",
      "title": "Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions",
      "title_zh": "探索具身多模态大型模型：发展、数据集和未来方向",
      "authors": [
        "Shoubin Chen",
        "Zehao Wu",
        "Kai Zhang",
        "Chunyu Li",
        "Baiyang Zhang",
        "Fei Ma",
        "Fei Richard Yu",
        "Qingquan Li"
      ],
      "abstract": "Embodied multimodal large models (EMLMs) have gained significant attention in\nrecent years due to their potential to bridge the gap between perception,\ncognition, and action in complex, real-world environments. This comprehensive\nreview explores the development of such models, including Large Language Models\n(LLMs), Large Vision Models (LVMs), and other models, while also examining\nother emerging architectures. We discuss the evolution of EMLMs, with a focus\non embodied perception, navigation, interaction, and simulation. Furthermore,\nthe review provides a detailed analysis of the datasets used for training and\nevaluating these models, highlighting the importance of diverse, high-quality\ndata for effective learning. The paper also identifies key challenges faced by\nEMLMs, including issues of scalability, generalization, and real-time\ndecision-making. Finally, we outline future directions, emphasizing the\nintegration of multimodal sensing, reasoning, and action to advance the\ndevelopment of increasingly autonomous systems. By providing an in-depth\nanalysis of state-of-the-art methods and identifying critical gaps, this paper\naims to inspire future advancements in EMLMs and their applications across\ndiverse domains.",
      "tldr_zh": "这篇论文综述了Embodied Multimodal Large Models (EMLMs)，这些模型通过整合Large Language Models (LLMs)、Large Vision Models (LVMs)和其他架构，桥接了感知、认知和行动在复杂环境的联系。论文详细探讨了EMLMs的发展历程，包括embodied perception、navigation、interaction和simulation等方面，并分析了用于训练和评估的多样化数据集，强调高质量数据对有效学习的重要性。论文还指出了关键挑战，如可扩展性、泛化和实时决策问题，并提出未来方向，聚焦于多模态感知、推理和行动的整合，以推动更自主系统的进步。最终，该综述通过识别当前差距，旨在激发EMLMs在各种领域的创新应用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "81 pages, submitted to a journal for review",
      "pdf_url": "http://arxiv.org/pdf/2502.15336v1",
      "published_date": "2025-02-21 09:41:27 UTC",
      "updated_date": "2025-02-21 09:41:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:08:27.772507"
    },
    {
      "arxiv_id": "2502.15334v1",
      "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Pedram Zaree",
        "Md Abdullah Al Mamun",
        "Quazi Mishkatul Alam",
        "Yue Dong",
        "Ihsen Alouani",
        "Nael Abu-Ghazaleh"
      ],
      "abstract": "Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B/AdvBench, using less than a third of the generation time).",
      "tldr_zh": "本文提出 Attention Eclipse，一种新方法，通过操纵大型语言模型(LLM)的注意力来绕过其安全对齐机制，生成更有效的 Jailbreak 攻击。该方法利用 attention loss 选择性地加强或削弱提示的不同部分，使攻击更易转移，并提升现有算法如 GCG、AutoDAN 和 ReNeLLM 的成功率。实验显示，增强后的 GCG 攻击在 Llama2-7B/AdvBench 上将攻击成功率(ASR)从 67.9% 提高到 91.2%，并将生成时间减少到不到三分之一。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15334v1",
      "published_date": "2025-02-21 09:38:00 UTC",
      "updated_date": "2025-02-21 09:38:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:08:40.251813"
    },
    {
      "arxiv_id": "2502.15331v2",
      "title": "Lightweight yet Efficient: An External Attentive Graph Convolutional Network with Positional Prompts for Sequential Recommendation",
      "title_zh": "轻量级却高效：一种带有位置提示的外部注意力图卷积网络用于序列推荐",
      "authors": [
        "Jinyu Zhang",
        "Chao Li",
        "Zhongying Zhao"
      ],
      "abstract": "Graph-based Sequential Recommender systems (GSRs) have gained significant\nresearch attention due to their ability to simultaneously handle user-item\ninteractions and sequential relationships between items. Current GSRs often\nutilize composite or in-depth structures for graph encoding (e.g., the Graph\nTransformer). Nevertheless, they have high computational complexity, hindering\nthe deployment on resource-constrained edge devices. Moreover, the relative\nposition encoding in Graph Transformer has difficulty in considering the\ncomplicated positional dependencies within sequence. To this end, we propose an\nExternal Attentive Graph convolutional network with Positional prompts for\nSequential recommendation, namely EA-GPS. Specifically, we first introduce an\nexternal attentive graph convolutional network that linearly measures the\nglobal associations among nodes via two external memory units. Then, we present\na positional prompt-based decoder that explicitly treats the absolute item\npositions as external prompts. By introducing length-adaptive sequential\nmasking and a soft attention network, such a decoder facilitates the model to\ncapture the long-term positional dependencies and contextual relationships\nwithin sequences. Extensive experimental results on five real-world datasets\ndemonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.\nRemarkably, it achieves the superior performance while maintaining a smaller\nparameter size and lower training overhead. The implementation of this work is\npublicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.",
      "tldr_zh": "本论文提出 EA-GPS，一种轻量高效的外部注意力图卷积网络（External Attentive Graph Convolutional Network）结合位置提示（Positional Prompts）的顺序推荐系统，旨在解决现有基于图的顺序推荐系统（GSRs）计算复杂度高和位置依赖处理不足的问题。具体而言，该模型通过两个外部记忆单元线性测量节点间的全局关联，并使用位置提示解码器、长度自适应顺序掩码和软注意力网络来捕捉序列中的长期位置依赖和上下文关系。在五个真实数据集上的实验结果表明，EA-GPS 优于最先进方法，同时保持更小的参数规模和更低的训练开销。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "26 pages, 8 figures, journal paper, accepted by TOIS at 20th\n  February, 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15331v2",
      "published_date": "2025-02-21 09:34:31 UTC",
      "updated_date": "2025-03-04 02:18:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:08:52.476855"
    },
    {
      "arxiv_id": "2502.15322v1",
      "title": "SentiFormer: Metadata Enhanced Transformer for Image Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Feng",
        "Shulan Ruan",
        "Mingzheng Yang",
        "Dongxuan Han",
        "Huijie Liu",
        "Kai Zhang",
        "Qi Liu"
      ],
      "abstract": "As more and more internet users post images online to express their daily\nemotions, image sentiment analysis has attracted increasing attention.\nRecently, researchers generally tend to design different neural networks to\nextract visual features from images for sentiment analysis. Despite the\nsignificant progress, metadata, the data (e.g., text descriptions and keyword\ntags) for describing the image, has not been sufficiently explored in this\ntask. In this paper, we propose a novel Metadata Enhanced Transformer for\nsentiment analysis (SentiFormer) to fuse multiple metadata and the\ncorresponding image into a unified framework. Specifically, we first obtain\nmultiple metadata of the image and unify the representations of diverse data.\nTo adaptively learn the appropriate weights for each metadata, we then design\nan adaptive relevance learning module to highlight more effective information\nwhile suppressing weaker ones. Moreover, we further develop a cross-modal\nfusion module to fuse the adaptively learned representations and make the final\nprediction. Extensive experiments on three publicly available datasets\ndemonstrate the superiority and rationality of our proposed method.",
      "tldr_zh": "该论文提出SentiFormer，一种增强Transformer的框架，用于图像情感分析，通过融合元数据（如文本描述和关键词标签）与图像特征来提升分析准确性。具体方法包括统一元数据表示、自适应相关性学习模块（用于分配权重以突出有效信息）和跨模态融合模块（用于整合表示并进行预测）。实验在三个公开数据集上验证了SentiFormer的优越性，证明了其在情感分析任务中的合理性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15322v1",
      "published_date": "2025-02-21 09:22:23 UTC",
      "updated_date": "2025-02-21 09:22:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:09:04.025783"
    },
    {
      "arxiv_id": "2502.15307v1",
      "title": "Road Traffic Sign Recognition method using Siamese network Combining Efficient-CNN based Encoder",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenghao Xi",
        "Yuchao Shao",
        "Yang Zheng",
        "Xiang Liu",
        "Yaqi Liu",
        "Yitong Cai"
      ],
      "abstract": "Traffic signs recognition (TSR) plays an essential role in assistant driving\nand intelligent transportation system. However, the noise of complex\nenvironment may lead to motion-blur or occlusion problems, which raise the\ntough challenge to real-time recognition with high accuracy and robust. In this\narticle, we propose IECES-network which with improved encoders and Siamese net.\nThe three-stage approach of our method includes Efficient-CNN based encoders,\nSiamese backbone and the fully-connected layers. We firstly use convolutional\nencoders to extract and encode the traffic sign features of augmented training\nsamples and standard images. Then, we design the Siamese neural network with\nEfficient-CNN based encoder and contrastive loss function, which can be trained\nto improve the robustness of TSR problem when facing the samples of motion-blur\nand occlusion by computing the distance between inputs and templates.\nAdditionally, the template branch of the proposed network can be stopped when\nexecuting the recognition tasks after training to raise the process speed of\nour real-time model, and alleviate the computational resource and parameter\nscale. Finally, we recombined the feature code and a fully-connected layer with\nSoftMax function to classify the codes of samples and recognize the category of\ntraffic signs. The results of experiments on the Tsinghua-Tencent 100K dataset\nand the German Traffic Sign Recognition Benchmark dataset demonstrate the\nperformance of the proposed IECESnetwork. Compared with other state-of-the-art\nmethods, in the case of motion-blur and occluded environment, the proposed\nmethod achieves competitive performance precision-recall and accuracy metric\naverage is 88.1%, 86.43% and 86.1% with a 2.9M lightweight scale, respectively.\nMoreover, processing time of our model is 0.1s per frame, of which the speed is\nincreased by 1.5 times compared with existing methods.",
      "tldr_zh": "该论文提出了一种名为 IECES-network 的交通标志识别（TSR）方法，结合 Siamese network 和 Efficient-CNN 基于的编码器，以提升在复杂环境（如运动模糊和遮挡）下的鲁棒性和实时性能。该方法采用三阶段流程：首先使用 Efficient-CNN 提取特征；其次，通过 Siamese 神经网络和对比损失函数训练模型，提高对模糊样本的处理能力；最后，利用 fully-connected 层和 SoftMax 函数进行分类。实验结果显示，在 Tsinghua-Tencent 100K 和 German Traffic Sign Recognition Benchmark 数据集上，该方法在遮挡环境下实现精确率 88.1%、召回率 86.43% 和准确率 86.1%，模型参数仅 2.9M，且处理速度为每帧 0.1 秒，比现有方法快 1.5 倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15307v1",
      "published_date": "2025-02-21 09:03:05 UTC",
      "updated_date": "2025-02-21 09:03:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:09:17.850428"
    },
    {
      "arxiv_id": "2502.15304v1",
      "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Hong Yankun",
        "Li Xing",
        "Zhen Hui-Ling",
        "Yu Xianzhi",
        "Liu Wulong",
        "Yuan Mingxuan"
      ],
      "abstract": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
      "tldr_zh": "该研究提出 SVDq，一种基于 Singular Value Decomposition (SVD) 的混合精度量化方法，用于高效压缩 Large Language Models (LLMs) 中的 key cache。通过将 key cache 转换为潜在通道并采用重要性感知量化，SVDq 能够针对重要通道分配更高精度，理论上将量化错误降低至传统方法的 0.1 倍或更低。实验在 RULER 和 LongBench 基准上表明，SVDq 实现了 1.25-bit 的 key cache 精度，并结合 key sparsity 达到高达 410x 的压缩比，同时保持模型性能几乎无损，为 LLMs 的高效推理提供了优化解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T50"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15304v1",
      "published_date": "2025-02-21 08:55:21 UTC",
      "updated_date": "2025-02-21 08:55:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:09:31.259549"
    },
    {
      "arxiv_id": "2502.15858v1",
      "title": "Generative AI Training and Copyright Law",
      "title_zh": "生成式 AI 训练与版权法",
      "authors": [
        "Tim W. Dornis",
        "Sebastian Stober"
      ],
      "abstract": "Training generative AI models requires extensive amounts of data. A common\npractice is to collect such data through web scraping. Yet, much of what has\nbeen and is collected is copyright protected. Its use may be copyright\ninfringement. In the USA, AI developers rely on \"fair use\" and in Europe, the\nprevailing view is that the exception for \"Text and Data Mining\" (TDM) applies.\nIn a recent interdisciplinary tandem-study, we have argued in detail that this\nis actually not the case because generative AI training fundamentally differs\nfrom TDM. In this article, we share our main findings and the implications for\nboth public and corporate research on generative models. We further discuss how\nthe phenomenon of training data memorization leads to copyright issues\nindependently from the \"fair use\" and TDM exceptions. Finally, we outline how\nthe ISMIR could contribute to the ongoing discussion about fair practices with\nrespect to generative AI that satisfy all stakeholders.",
      "tldr_zh": "该论文探讨了训练生成式 AI 模型时使用大量受版权保护数据的潜在侵权问题，特别是通过网络抓取获取数据。在美国，开发者依赖“fair use”例外，而在欧洲，普遍认为“Text and Data Mining”（TDM）例外适用，但作者基于一项跨学科联合研究，论证生成式 AI 训练与 TDM 有根本区别，因此这些例外并不真正适用。文章分享了主要发现，包括训练数据记忆化导致的独立版权风险，并讨论了其对公共和企业研究的启示，以及 ISMIR 如何为生成式 AI 的公平实践贡献。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "submitted as an overview article to the Transactions of the\n  International Society for Music Information Retrieval",
      "pdf_url": "http://arxiv.org/pdf/2502.15858v1",
      "published_date": "2025-02-21 08:45:14 UTC",
      "updated_date": "2025-02-21 08:45:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:09:42.240790"
    },
    {
      "arxiv_id": "2502.15296v1",
      "title": "Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Minbo Ma",
        "Kai Tang",
        "Huan Li",
        "Fei Teng",
        "Dalin Zhang",
        "Tianrui Li"
      ],
      "abstract": "Multivariate Time Series Forecasting (MTSF) has long been a key research\nfocus. Traditionally, these studies assume a fixed number of variables, but in\nreal-world applications, Cyber-Physical Systems often expand as new sensors are\ndeployed, increasing variables in MTSF. In light of this, we introduce a novel\ntask, Expanding-variate Time Series Forecasting (EVTSF). This task presents\nunique challenges, specifically (1) handling inconsistent data shapes caused by\nadding new variables, and (2) addressing imbalanced spatio-temporal learning,\nwhere expanding variables have limited observed data due to the necessity for\ntimely operation. To address these challenges, we propose STEV, a flexible\nspatio-temporal forecasting framework. STEV includes a new Flat Scheme to\ntackle the inconsistent data shape issue, which extends the graph-based\nspatio-temporal modeling architecture into 1D space by flattening the 2D\nsamples along the variable dimension, making the model variable-scale-agnostic\nwhile still preserving dynamic spatial correlations through a holistic graph.\nWe introduce a novel Spatio-temporal Focal Learning strategy that incorporates\na negative filter to resolve potential conflicts between contrastive learning\nand graph representation, and a focal contrastive loss as its core to guide the\nframework to focus on optimizing the expanding variables. We benchmark EVTSF\nperformance using three real-world datasets and compare it against three\npotential solutions employing SOTA MTSF models tailored for EVSTF. Experimental\nresults show that STEV significantly outperforms its competitors, particularly\non expanding variables. Notably, STEV, with only 5% of observations from the\nexpanding period, is on par with SOTA MTSF models trained with complete\nobservations. Further exploration of various expanding strategies underscores\nthe generalizability of STEV in real-world applications.",
      "tldr_zh": "该研究扩展了传统多变量时间序列预测（MTSF），引入了新任务Expanding-variate Time Series Forecasting (EVTSF)，以应对现实中变量数量动态增加带来的数据形状不一致和空间-时间学习不平衡问题。作者提出STEV框架，包括Flat Scheme方法，通过沿变量维度扁平化样本，将图-based 空间-时间建模扩展到1D空间，确保模型对变量规模不敏感并保留动态空间相关性；以及Spatio-temporal Focal Learning策略，利用负过滤器和焦点对比损失，专注于优化扩展变量。实验在三个真实数据集上显示，STEV显著优于SOTA MTSF模型，尤其在扩展变量上，即使仅使用5%的观察数据，其性能与完全数据训练的基准相当，并证明了其在各种扩展策略下的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15296v1",
      "published_date": "2025-02-21 08:43:26 UTC",
      "updated_date": "2025-02-21 08:43:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:09:55.757012"
    },
    {
      "arxiv_id": "2502.15294v2",
      "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Yaohua Tang",
        "Zhicheng Hu",
        "Kun Cheng",
        "Fan Mo",
        "Qiheng Lv",
        "Hua Wang",
        "Zhi Chen"
      ],
      "abstract": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
      "tldr_zh": "这篇论文针对大型语言模型(LLM)推理过程中，上下文窗口增大导致KV cache占用大量GPU内存的问题，提出了一种新型机制Round Attention。该机制通过分析真实用户对话数据，发现存在一个“watershed layer”，在此之后round-level attention的分布高度相似，从而只计算和回忆最相关rounds的KV cache，以加速推理过程。实验结果显示，Round Attention在不影响模型性能的情况下，节省了55%的内存使用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15294v2",
      "published_date": "2025-02-21 08:40:07 UTC",
      "updated_date": "2025-02-24 13:35:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:10:04.860203"
    },
    {
      "arxiv_id": "2503.05750v1",
      "title": "CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Mst. Fahmida Sultana Naznin",
        "Adnan Ibney Faruq",
        "Mostafa Rifat Tazwar",
        "Md Jobayer",
        "Md. Mehedi Hasan Shawon",
        "Md Rakibul Hasan"
      ],
      "abstract": "A radiology report comprises several sections, including the Findings and\nImpression of the diagnosis. Automatically generating the Impression from the\nFindings is crucial for reducing radiologists' workload and improving\ndiagnostic accuracy. Pretrained models that excel in common abstractive\nsummarization problems encounter challenges when applied to specialized medical\ndomains largely due to the complex terminology and the necessity for accurate\nclinical context. Such tasks in medical domains demand extracting core\ninformation, avoiding context shifts, and maintaining proper flow. Misuse of\nmedical terms can lead to drastic clinical errors. To address these issues, we\nintroduce a sequential transfer learning that ensures key content extraction\nand coherent summarization. Sequential transfer learning often faces challenges\nlike initial parameter decay and knowledge loss, which we resolve with the\nFisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model,\nCSTRL-Context-driven Sequential TRansfer Learning-achieved state-of-the-art\nperformance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in\nBLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over\nbenchmark studies. We also analyze factual consistency scores while preserving\nthe medical context. Our code is publicly available at TBA.",
      "tldr_zh": "该论文针对放射学报告摘要生成问题，提出CSTRL（Context-Driven Sequential Transfer Learning）方法，通过顺序转移学习从Findings部分自动生成Impression部分，以减少放射科医生的负担并提高诊断准确性。CSTRL采用Fisher matrix regularization来解决初始参数衰减和知识损失，确保关键内容的提取、上下文连贯性和医疗术语的准确使用。实验结果显示，在MIMIC-CXR和Open-I数据集上，该模型在BLEU-1、BLEU-2、BLEU-3、ROUGE-1、ROUGE-2和ROUGE-3指标上分别实现了56.2%、40.5%、84.3%、28.9%、41.0%和26.5%的显著改善，同时保持了事实一致性和临床上下文的完整性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "11-pages main paper with 2-pages appendices",
      "pdf_url": "http://arxiv.org/pdf/2503.05750v1",
      "published_date": "2025-02-21 08:32:11 UTC",
      "updated_date": "2025-02-21 08:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:10:20.307819"
    },
    {
      "arxiv_id": "2502.15287v1",
      "title": "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era",
      "title_zh": "时间扭曲：开发者理想与实际工作周之间的差距，在人工智能驱动时代",
      "authors": [
        "Sukrit Kumar",
        "Drishti Goel",
        "Thomas Zimmermann",
        "Brian Houck",
        "B. Ashok",
        "Chetan Bansal"
      ],
      "abstract": "Software developers balance a variety of different tasks in a workweek, yet\nthe allocation of time often differs from what they consider ideal. Identifying\nand addressing these deviations is crucial for organizations aiming to enhance\nthe productivity and well-being of the developers. In this paper, we present\nthe findings from a survey of 484 software developers at Microsoft, which aims\nto identify the key differences between how developers would like to allocate\ntheir time during an ideal workweek versus their actual workweek. Our analysis\nreveals significant deviations between a developer's ideal workweek and their\nactual workweek, with a clear correlation: as the gap between these two\nworkweeks widens, we observe a decline in both productivity and satisfaction.\nBy examining these deviations in specific activities, we assess their direct\nimpact on the developers' satisfaction and productivity. Additionally, given\nthe growing adoption of AI tools in software engineering, both in the industry\nand academia, we identify specific tasks and areas that could be strong\ncandidates for automation. In this paper, we make three key contributions: 1)\nWe quantify the impact of workweek deviations on developer productivity and\nsatisfaction 2) We identify individual tasks that disproportionately affect\nsatisfaction and productivity 3) We provide actual data-driven insights to\nguide future AI automation efforts in software engineering, aligning them with\nthe developers' requirements and ideal workflows for maximizing their\nproductivity and satisfaction.",
      "tldr_zh": "这篇论文通过对484名Microsoft软件开发者的调查，分析了他们在AI驱动时代理想工作周与实际工作周之间的差距。研究发现，这种差距会导致开发者的生产力(productivity)和满意度(satisfaction)显著下降，且特定任务的偏差会加剧这些负面影响。论文的主要贡献包括：量化工作周偏差对生产力和满意度的影响、识别出关键任务，以及提供数据驱动的见解来指导AI工具的自动化应用，以更好地匹配开发者的理想工作流程。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "ICSE SEIP 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15287v1",
      "published_date": "2025-02-21 08:29:49 UTC",
      "updated_date": "2025-02-21 08:29:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:10:30.605654"
    },
    {
      "arxiv_id": "2502.15285v3",
      "title": "Offload Rethinking by Cloud Assistance for Efficient Environmental Sound Recognition on LPWANs",
      "title_zh": "翻译失败",
      "authors": [
        "Le Zhang",
        "Quanling Zhao",
        "Run Wang",
        "Shirley Bian",
        "Onat Gungor",
        "Flavio Ponzina",
        "Tajana Rosing"
      ],
      "abstract": "Learning-based environmental sound recognition has emerged as a crucial\nmethod for ultra-low-power environmental monitoring in biological research and\ncity-scale sensing systems. These systems usually operate under limited\nresources and are often powered by harvested energy in remote areas. Recent\nefforts in on-device sound recognition suffer from low accuracy due to resource\nconstraints, whereas cloud offloading strategies are hindered by high\ncommunication costs. In this work, we introduce ORCA, a novel\nresource-efficient cloud-assisted environmental sound recognition system on\nbatteryless devices operating over the Low-Power Wide-Area Networks (LPWANs),\ntargeting wide-area audio sensing applications. We propose a cloud assistance\nstrategy that remedies the low accuracy of on-device inference while minimizing\nthe communication costs for cloud offloading. By leveraging a\nself-attention-based cloud sub-spectral feature selection method to facilitate\nefficient on-device inference, ORCA resolves three key challenges for\nresource-constrained cloud offloading over LPWANs: 1) high communication costs\nand low data rates, 2) dynamic wireless channel conditions, and 3) unreliable\noffloading. We implement ORCA on an energy-harvesting batteryless\nmicrocontroller and evaluate it in a real world urban sound testbed. Our\nresults show that ORCA outperforms state-of-the-art methods by up to $80\n\\times$ in energy savings and $220 \\times$ in latency reduction while\nmaintaining comparable accuracy.",
      "tldr_zh": "这篇论文提出了 ORCA 系统，一种针对 LPWANs 的资源高效云辅助环境声音识别方法，旨在解决基于学习的超低功耗环境监测中设备端准确率低和高通信成本的问题。ORCA 通过自注意力-based 云子频谱特征选择策略，优化了云卸载过程，缓解了高通信成本、低数据率、动态无线通道条件以及不可靠卸载的三大挑战。实验在能量采集的无电池微控制器上进行，结果显示 ORCA 比现有方法节省能量高达 80 倍、减少延迟 220 倍，同时保持可比的识别准确率。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.DC",
        "cs.NI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by The 23rd ACM Conference on Embedded Networked Sensor\n  Systems (SenSys '25)",
      "pdf_url": "http://arxiv.org/pdf/2502.15285v3",
      "published_date": "2025-02-21 08:23:32 UTC",
      "updated_date": "2025-03-21 11:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:10:41.765533"
    },
    {
      "arxiv_id": "2502.17499v2",
      "title": "Detecting Long QT Syndrome and First-Degree Atrioventricular Block using Single-Lead AI-ECG: A Multi-Center Real-World Study",
      "title_zh": "翻译失败",
      "authors": [
        "Sumei Fan",
        "Deyun Zhang",
        "Yue Wang",
        "Shijia Geng",
        "Kun Lu",
        "Meng Sang",
        "Weilun Xu",
        "Haixue Wang",
        "Qinghao Zhao",
        "Chuandong Cheng",
        "Peng Wang",
        "Shenda Hong"
      ],
      "abstract": "Home-based single-lead AI-ECG devices have enabled continuous, real-world\ncardiac monitoring. However, the accuracy of parameter calculations from\nsingle-lead AI-ECG algorithm remains to be fully validated, which is critical\nfor conditions such as Long QT Syndrome (LQTS) and First-Degree\nAtrioventricular Block (AVBI). In this multicenter study, we assessed\nFeatureDB, an ECG measurements computation algorithm, in the context of\nsingle-lead monitoring using three annotated datasets: PTB-XL+ (n=21,354), CSE\n(n=105), and HeartVoice-ECG-lite (n=369). FeatureDB showed strong correlation\nwith standard ECG machines (12SL and Uni-G) in key measurements (PR, QRS, QT,\nQTc), and high agreement confirmed by Bland-Altman analysis. In detecting LQTS\n(AUC=0.786) and AVBI (AUC=0.684), FeatureDB demonstrated diagnostic performance\ncomparable to commercial ECG systems (12SL: 0.859/0.716; Uni-G: 0.817/0.605),\nsignificantly outperforming ECGDeli (0.501/0.569). Notably, FeatureDB can\noperate locally on resource-limited devices, facilitating use in\nlow-connectivity settings. These findings confirm the clinical reliability of\nFeatureDB for single-lead ECG diagnostics and highlight its potential to bridge\ntraditional ECG diagnostics with wearable technology for scalable\ncardiovascular monitoring and early intervention.",
      "tldr_zh": "该研究评估了 FeatureDB 算法在单导联 AI-ECG 中的性能，用于检测 Long QT Syndrome (LQTS) 和 First-Degree Atrioventricular Block (AVBI)，通过多中心数据集（包括 PTB-XL+、CSE 和 HeartVoice-ECG-lite）进行验证。结果显示，FeatureDB 在关键测量（PR、QRS、QT、QTc）上与标准 ECG 机器（12SL 和 Uni-G）具有强相关性和高一致性，且在 LQTS (AUC=0.786) 和 AVBI (AUC=0.684) 检测中表现与商业系统相当，甚至优于 ECGDeli。FeatureDB 能本地运行于资源有限的设备，这为桥接传统 ECG 诊断与可穿戴技术提供了潜力，支持可扩展的心血管监测和早期干预。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "eess.SP",
      "comment": "29pages, 11 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.17499v2",
      "published_date": "2025-02-21 08:11:17 UTC",
      "updated_date": "2025-04-27 02:46:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:10:56.226179"
    },
    {
      "arxiv_id": "2502.15278v1",
      "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shunchang Liu",
        "Zhuan Shi",
        "Lingjuan Lyu",
        "Yaochu Jin",
        "Boi Faltings"
      ],
      "abstract": "Assessing whether AI-generated images are substantially similar to\ncopyrighted works is a crucial step in resolving copyright disputes. In this\npaper, we propose CopyJudge, an automated copyright infringement identification\nframework that leverages large vision-language models (LVLMs) to simulate\npractical court processes for determining substantial similarity between\ncopyrighted images and those generated by text-to-image diffusion models.\nSpecifically, we employ an abstraction-filtration-comparison test framework\nwith multi-LVLM debate to assess the likelihood of infringement and provide\ndetailed judgment rationales. Based on the judgments, we further introduce a\ngeneral LVLM-based mitigation strategy that automatically optimizes infringing\nprompts by avoiding sensitive expressions while preserving the non-infringing\ncontent. Besides, our approach can be enhanced by exploring non-infringing\nnoise vectors within the diffusion latent space via reinforcement learning,\neven without modifying the original prompts. Experimental results show that our\nidentification method achieves comparable state-of-the-art performance, while\noffering superior generalization and interpretability across various forms of\ninfringement, and that our mitigation method could more effectively mitigate\nmemorization and IP infringement without losing non-infringing expressions.",
      "tldr_zh": "本研究提出 CopyJudge，一种自动化框架，用于识别和缓解文本到图像扩散模型中的版权侵权问题。该框架利用大型视觉语言模型 (LVLMs) 模拟法院过程，通过 abstraction-filtration-comparison 测试框架和多 LVLM 辩论来评估生成图像与版权作品的实质相似性，并提供详细判断理由。基于这些判断，论文引入 LVLM 基于的缓解策略，可优化侵权提示以避免敏感表达，同时保留非侵权内容；此外，通过强化学习在扩散潜在空间中探索非侵权噪声向量，进一步提升缓解效果而不需修改原提示。实验结果显示，CopyJudge 在侵权识别方面达到最先进性能，并展现出更好的泛化性、可解释性和有效性，能显著减少记忆和 IP 侵权风险。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15278v1",
      "published_date": "2025-02-21 08:09:07 UTC",
      "updated_date": "2025-02-21 08:09:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:11:07.045129"
    },
    {
      "arxiv_id": "2502.18501v1",
      "title": "Deep Learning-based Dual Watermarking for Image Copyright Protection and Authentication",
      "title_zh": "基于深度学习的双重水印技术用于图像版权保护和认证",
      "authors": [
        "Sudev Kumar Padhi",
        "Archana Tiwari",
        "Sk. Subidh Ali"
      ],
      "abstract": "Advancements in digital technologies make it easy to modify the content of\ndigital images. Hence, ensuring digital images integrity and authenticity is\nnecessary to protect them against various attacks that manipulate them. We\npresent a Deep Learning (DL) based dual invisible watermarking technique for\nperforming source authentication, content authentication, and protecting\ndigital content copyright of images sent over the internet. Beyond securing\nimages, the proposed technique demonstrates robustness to content-preserving\nimage manipulations. It is also impossible to imitate or overwrite watermarks\nbecause the cryptographic hash of the image and the dominant features of the\nimage in the form of perceptual hash are used as watermarks. We highlighted the\nneed for source authentication to safeguard image integrity and authenticity,\nalong with identifying similar content for copyright protection. After\nexhaustive testing, we obtained a high peak signal-to-noise ratio (PSNR) and\nstructural similarity index measure (SSIM), which implies there is a minute\nchange in the original image after embedding our watermarks. Our trained model\nachieves high watermark extraction accuracy and to the best of our knowledge,\nthis is the first deep learning-based dual watermarking technique proposed in\nthe literature.",
      "tldr_zh": "该研究提出了一种基于Deep Learning的dual watermarking技术，用于图像版权保护和认证。该方法利用图像的cryptographic hash和perceptual hash作为隐形水印，实现源认证、内容认证，并对内容保留性图像操作（如旋转或压缩）具有鲁棒性。通过这种双重水印机制，水印难以模仿或覆盖，有效防范篡改攻击。实验结果显示，该模型在水印提取方面准确率高，并获得高PSNR和SSIM值，表明水印嵌入后图像质量几乎不受影响；这是在文献中首次提出的Deep Learning-based双重水印技术。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "IEEE Transactions on Artificial Intelligence. 2024 Oct 24",
      "pdf_url": "http://arxiv.org/pdf/2502.18501v1",
      "published_date": "2025-02-21 07:58:39 UTC",
      "updated_date": "2025-02-21 07:58:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:11:17.545805"
    },
    {
      "arxiv_id": "2502.17498v1",
      "title": "Improving Value-based Process Verifier via Structural Prior Injection",
      "title_zh": "通过结构先验注入改进基于价值的过程验证器",
      "authors": [
        "Zetian Sun",
        "Dongfang Li",
        "Baotian Hu",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "In the Large Language Model(LLM) reasoning scenario, people often estimate\nstate value via Monte Carlo sampling. Though Monte Carlo estimation is an\nelegant method with less inductive bias, noise and errors are inevitably\nintroduced due to the limited sampling. To handle the problem, we inject the\nstructural prior into the value representation and transfer the scalar value\ninto the expectation of a pre-defined categorical distribution, representing\nthe noise and errors from a distribution perspective. Specifically, by treating\nthe result of Monte Carlo sampling as a single sample from the prior\nground-truth Binomial distribution, we quantify the sampling error as the\nmismatch between posterior estimated distribution and ground-truth\ndistribution, which is thus optimized via distribution selection optimization.\nWe test the performance of value-based process verifiers on Best-of-N task and\nBeam search task. Compared with the scalar value representation, we show that\nreasonable structural prior injection induced by different objective functions\nor optimization methods can improve the performance of value-based process\nverifiers for about 1$\\sim$2 points at little-to-no cost. We also show that\nunder different structural prior, the verifiers' performances vary greatly\ndespite having the same optimal solution, indicating the importance of\nreasonable structural prior injection.",
      "tldr_zh": "本论文针对大型语言模型(LLM)推理中Monte Carlo采样估计状态值引入的噪声和错误问题，提出了一种通过注入structural prior的方法，将标量值转化为预定义分类分布（如Binomial distribution）的期望，从而从分布视角量化采样错误。作者将Monte Carlo采样结果视为来自真实分布的样本，并通过分布选择优化最小化后验估计分布与真实分布的失配，以提升值-based过程验证器的性能。在Best-of-N和Beam search任务上的实验表明，这种structural prior注入可将验证器性能提高约1~2点，且不同结构先验的设计会显著影响结果，突显了合理注入prior的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Under review",
      "pdf_url": "http://arxiv.org/pdf/2502.17498v1",
      "published_date": "2025-02-21 07:57:59 UTC",
      "updated_date": "2025-02-21 07:57:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:11:31.146050"
    },
    {
      "arxiv_id": "2504.05316v1",
      "title": "Scale Up Composed Image Retrieval Learning via Modification Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yinan Zhou",
        "Yaxiong Wang",
        "Haokun Lin",
        "Chen Ma",
        "Li Zhu",
        "Zhedong Zheng"
      ],
      "abstract": "Composed Image Retrieval (CIR) aims to search an image of interest using a\ncombination of a reference image and modification text as the query. Despite\nrecent advancements, this task remains challenging due to limited training data\nand laborious triplet annotation processes. To address this issue, this paper\nproposes to synthesize the training triplets to augment the training resource\nfor the CIR problem. Specifically, we commence by training a modification text\ngenerator exploiting large-scale multimodal models and scale up the CIR\nlearning throughout both the pretraining and fine-tuning stages. During\npretraining, we leverage the trained generator to directly create Modification\nText-oriented Synthetic Triplets(MTST) conditioned on pairs of images. For\nfine-tuning, we first synthesize reverse modification text to connect the\ntarget image back to the reference image. Subsequently, we devise a two-hop\nalignment strategy to incrementally close the semantic gap between the\nmultimodal pair and the target image. We initially learn an implicit prototype\nutilizing both the original triplet and its reversed version in a cycle manner,\nfollowed by combining the implicit prototype feature with the modification text\nto facilitate accurate alignment with the target image. Extensive experiments\nvalidate the efficacy of the generated triplets and confirm that our proposed\nmethodology attains competitive recall on both the CIRR and FashionIQ\nbenchmarks.",
      "tldr_zh": "该论文针对 Composed Image Retrieval (CIR) 任务提出一种通过修改文本生成来扩展训练数据的方法，以解决训练资源有限和三元组标注耗时的问题。具体而言，该方法训练一个修改文本生成器，并在预训练阶段生成基于图像对的 Modification Text-oriented Synthetic Triplets (MTST)，而在 fine-tuning 阶段采用两跳对齐策略，通过合成反向修改文本和隐式原型学习来缩小语义差距。实验结果显示，该方法在 CIRR 和 FashionIQ 基准上实现了竞争性的召回率，提升了 CIR 模型的性能。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05316v1",
      "published_date": "2025-02-21 07:48:55 UTC",
      "updated_date": "2025-02-21 07:48:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:11:42.879633"
    },
    {
      "arxiv_id": "2502.15261v1",
      "title": "Corrections Meet Explanations: A Unified Framework for Explainable Grammatical Error Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Jingheng Ye",
        "Shang Qin",
        "Yinghui Li",
        "Hai-Tao Zheng",
        "Shen Wang",
        "Qingsong Wen"
      ],
      "abstract": "Grammatical Error Correction (GEC) faces a critical challenge concerning\nexplainability, notably when GEC systems are designed for language learners.\nExisting research predominantly focuses on explaining grammatical errors\nextracted in advance, thus neglecting the relationship between explanations and\ncorrections. To address this gap, we introduce EXGEC, a unified explainable GEC\nframework that integrates explanation and correction tasks in a generative\nmanner, advocating that these tasks mutually reinforce each other. Experiments\nhave been conducted on EXPECT, a recent human-labeled dataset for explainable\nGEC, comprising around 20k samples. Moreover, we detect significant noise\nwithin EXPECT, potentially compromising model training and evaluation.\nTherefore, we introduce an alternative dataset named EXPECT-denoised, ensuring\na more objective framework for training and evaluation. Results on various NLP\nmodels (BART, T5, and Llama3) show that EXGEC models surpass single-task\nbaselines in both tasks, demonstrating the effectiveness of our approach.",
      "tldr_zh": "该论文针对 Grammatical Error Correction (GEC) 的可解释性挑战，特别是在语言学习者应用中，提出了 EXGEC 框架，该框架将解释和修正任务统一整合，通过生成方式让两者相互强化。实验在 EXPECT 数据集上进行，但作者发现数据集存在显著噪音问题，因此创建了改进版 EXPECT-denoised 数据集，以提供更可靠的训练和评估环境。结果显示，使用 BART、T5 和 Llama3 等模型的 EXGEC 模型在解释和修正任务上均超过了单任务基线，证明了这一统一框架的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 2 figures, and 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15261v1",
      "published_date": "2025-02-21 07:42:33 UTC",
      "updated_date": "2025-02-21 07:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:11:55.121128"
    },
    {
      "arxiv_id": "2502.15857v1",
      "title": "PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Fan",
        "Guoqiang Ma",
        "Yuanfeng Song",
        "Lixin Fan",
        "Kai Chen",
        "Qiang Yang"
      ],
      "abstract": "Compressing Large Language Models (LLMs) into task-specific Small Language\nModels (SLMs) encounters two significant challenges: safeguarding\ndomain-specific knowledge privacy and managing limited resources. To tackle\nthese challenges, we propose PPC-GPT, a innovative privacy-preserving federated\nframework specifically designed for compressing LLMs into task-specific SLMs\nvia pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a\nserver-client federated architecture, where the client sends differentially\nprivate (DP) perturbed task-specific data to the server's LLM. The LLM then\ngenerates synthetic data along with their corresponding rationales. This\nsynthetic data is subsequently used for both LLM pruning and retraining\nprocesses. Additionally, we harness COT knowledge distillation, leveraging the\nsynthetic data to further improve the retraining of structurally-pruned SLMs.\nOur experimental results demonstrate the effectiveness of PPC-GPT across\nvarious text generation tasks. By compressing LLMs into task-specific SLMs,\nPPC-GPT not only achieves competitive performance but also prioritizes data\nprivacy protection.",
      "tldr_zh": "本论文提出PPC-GPT，一种创新的隐私保护联邦框架，用于通过修剪(pruning)和Chain-of-Thought (COT)知识蒸馏将Large Language Models (LLMs)压缩成任务特定的Small Language Models (SLMs)，以解决知识隐私保护和资源限制的挑战。框架采用服务器-客户端架构，客户端发送差分隐私(DP)扰动的数据给服务器的LLM，生成合成数据及其推理路径，用于LLM的修剪和重新训练，同时利用COT知识蒸馏优化SLMs的表现。实验结果表明，PPC-GPT在多种文本生成任务中实现了竞争性的性能，同时优先保障数据隐私保护。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15857v1",
      "published_date": "2025-02-21 07:32:49 UTC",
      "updated_date": "2025-02-21 07:32:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:12:07.741003"
    },
    {
      "arxiv_id": "2502.15255v1",
      "title": "ComposeOn Academy: Transforming Melodic Ideas into Complete Compositions Integrating Music Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hongxi Pu",
        "Futian Jiang",
        "Zihao Chen",
        "Xingyue Song"
      ],
      "abstract": "Music composition has long been recognized as a significant art form.\nHowever, existing digital audio workstations and music production software\noften present high entry barriers for users lacking formal musical training. To\naddress this, we introduce ComposeOn, a music theory-based tool designed for\nusers with limited musical knowledge. ComposeOn enables users to easily extend\ntheir melodic ideas into complete compositions and offers simple editing\nfeatures. By integrating music theory, it explains music creation at beginner,\nintermediate, and advanced levels. Our user study (N=10) compared ComposeOn\nwith the baseline method, Suno AI, demonstrating that ComposeOn provides a more\naccessible and enjoyable composing and learning experience for individuals with\nlimited musical skills. ComposeOn bridges the gap between theory and practice,\noffering an innovative solution as both a composition aid and music education\nplatform. The study also explores the differences between theory-based music\ncreation and generative music, highlighting the former's advantages in personal\nexpression and learning.",
      "tldr_zh": "本研究引入了 ComposeOn Academy，一种基于音乐理论的工具，旨在帮助音乐知识有限的用户将旋律想法扩展成完整作曲，并提供简单编辑功能，同时从初级到高级水平解释音乐创作过程。相比基线方法 Suno AI，用户研究（N=10）显示 ComposeOn 提供更易访问和愉快的作曲与学习体验，尤其在提升个人表达和学习效果方面。ComposeOn 作为作曲辅助和音乐教育平台，有效桥接了音乐理论与实践，突出了理论-based 方法相对于生成式音乐的优势。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15255v1",
      "published_date": "2025-02-21 07:18:19 UTC",
      "updated_date": "2025-02-21 07:18:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:12:19.088445"
    },
    {
      "arxiv_id": "2502.15856v1",
      "title": "A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic Styles",
      "title_zh": "现代生成模型复制艺术风格能力的批判性评估",
      "authors": [
        "Andrea Asperti",
        "Franky George",
        "Tiberio Marras",
        "Razvan Ciprian Stricescu",
        "Fabio Zanotti"
      ],
      "abstract": "In recent years, advancements in generative artificial intelligence have led\nto the development of sophisticated tools capable of mimicking diverse artistic\nstyles, opening new possibilities for digital creativity and artistic\nexpression. This paper presents a critical assessment of the style replication\ncapabilities of contemporary generative models, evaluating their strengths and\nlimitations across multiple dimensions. We examine how effectively these models\nreproduce traditional artistic styles while maintaining structural integrity\nand compositional balance in the generated images.\n  The analysis is based on a new large dataset of AI-generated works imitating\nartistic styles of the past, holding potential for a wide range of\napplications: the \"AI-pastiche\" dataset.\n  The study is supported by extensive user surveys, collecting diverse opinions\non the dataset and investigation both technical and aesthetic challenges,\nincluding the ability to generate outputs that are realistic and visually\nconvincing, the versatility of models in handling a wide range of artistic\nstyles, and the extent to which they adhere to the content and stylistic\nspecifications outlined in prompts.\n  This paper aims to provide a comprehensive overview of the current state of\ngenerative tools in style replication, offering insights into their technical\nand artistic limitations, potential advancements in model design and training\nmethodologies, and emerging opportunities for enhancing digital artistry,\nhuman-AI collaboration, and the broader creative landscape.",
      "tldr_zh": "本研究对现代生成模型（generative models）复制艺术风格的能力进行了批判性评估，考察了这些模型在保持图像结构完整性和组成平衡方面的优缺点。研究基于一个新数据集“AI-pastiche”，该数据集包含AI生成的作品模仿历史艺术风格，并通过广泛的用户调查分析了技术与美学挑战，如生成真实视觉效果、处理多种风格以及遵守提示要求。结果显示，虽然生成模型在数字创意中展现潜力，但仍存在显著限制，论文提供了对模型设计训练方法的改进建议，并探讨了增强人类-AI协作和数字艺术领域的机遇。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T05",
        "I.2.10; I.5.4"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15856v1",
      "published_date": "2025-02-21 07:00:06 UTC",
      "updated_date": "2025-02-21 07:00:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:12:30.271693"
    },
    {
      "arxiv_id": "2502.15855v1",
      "title": "Non-Linear Flow Matching for Full-Atom Peptide Design",
      "title_zh": "翻译失败",
      "authors": [
        "Dengdeng Huang",
        "Shikui Tu"
      ],
      "abstract": "Peptide design plays a pivotal role in therapeutic applications, yet existing\nAI-assisted methods often struggle to generate stable peptides with high\naffinity due to their inability to accurately simulate the dynamic docking\nprocess. To address this challenge, we propose NLFlow, a novel multi-manifold\napproach based on non-linear flow matching. Specifically, we design a\npolynomial-based conditional vector field to accelerate the convergence of the\npeptide's position towards the target pocket, effectively capturing the\ntemporal inconsistencies across position, rotation, torsion, and amino acid\ntype manifolds. This enables the model to better align with the true\nconformational changes observed in biological docking processes. Additionally,\nwe incorporate interaction-related information, such as polarity, to enhance\nthe understanding of peptide-protein binding. Extensive experiments demonstrate\nthat NLFlow outperforms existing methods in generating peptides with superior\nstability, affinity, and diversity, offering a fast and efficient solution for\npeptide design and advancing the peptide-based therapeutic development.",
      "tldr_zh": "本研究针对肽设计在治疗应用中的挑战，提出NLFlow，一种基于Non-Linear Flow Matching的多流形方法，以解决现有AI方法在模拟动态对接过程中的不足。NLFlow采用多项式条件向量场加速肽位置向目标口袋的收敛，同时捕捉位置、旋转、扭转和氨基酸类型等流形上的时间不一致性，并整合极性等交互信息以提升肽-蛋白结合的理解。实验结果表明，NLFlow在生成肽的稳定性和亲和力方面优于现有方法，并显著提高了肽的多样性，为肽基治疗开发提供快速高效的解决方案。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15855v1",
      "published_date": "2025-02-21 06:49:49 UTC",
      "updated_date": "2025-02-21 06:49:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:12:44.218408"
    },
    {
      "arxiv_id": "2502.15854v1",
      "title": "Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data Generation and Evaluation using Reasoning Models",
      "title_zh": "增强特定领域的检索增强生成：合成数据生成和使用推理模型的评估",
      "authors": [
        "Aryan Jadon",
        "Avinash Patil",
        "Shashank Kumar"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems face significant performance\ngaps when applied to technical domains requiring precise information extraction\nfrom complex documents. Current evaluation methodologies relying on\ndocument-level metrics inadequately capture token-resolution retrieval accuracy\nthat is critical for domain-related documents. We propose a framework combining\ngranular evaluation metrics with synthetic data generation to optimize\ndomain-specific RAG performance. First, we introduce token-aware metrics\nPrecision $\\Omega$ and Intersection-over-Union (IoU) that quantify context\npreservation versus information density trade-offs inherent in technical texts.\nSecond, we develop a reasoning model-driven pipeline using instruction-tuned\nLLMs (DeepSeek-R1, DeepSeek-R1 distilled variants, and Phi-4) to generate\ncontext-anchored QA pairs with discontinuous reference spans across three\nspecialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed),\nand APT threat reports (cybersecurity).\n  Our empirical analysis reveals critical insights: smaller chunks (less than\n10 tokens) improve precision by 31-42% (IoU = 0.071 vs. baseline 0.053) at\nrecall costs (-18%), while domain-specific embedding strategies yield 22%\nvariance in optimal chunk sizing (5-20 tokens). The\nDeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment\n(+14% mean IoU over alternatives), though no configuration universally\ndominates. Financial texts favor larger chunks for risk factor coverage (Recall\n= 0.81 at size = 20), whereas cybersecurity content benefits from atomic\nsegmentation, Precision $\\Omega = 0.28$ at size = 5.\n  Our code is available on\nhttps://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model",
      "tldr_zh": "本论文针对Retrieval-Augmented Generation (RAG) 系统在技术领域中提取精确信息的性能不足问题，提出一个优化框架，结合粒度评估指标和合成数据生成方法。研究引入了token-aware 指标Precision Ω 和Intersection-over-Union (IoU)，并开发了一个基于指令调整的LLMs（如DeepSeek-R1及其变体）驱动的管道，用于生成上下文锚定的QA对，应用于SEC 10-K filings、金融、PubMed-生物医学和APT威胁报告-网络安全等领域语料。实验结果显示，小块文本（小于10 tokens）可提高Precision Ω 31-42%，但会降低召回率18%，而领域特定嵌入策略导致最佳块大小差异（5-20 tokens），DeepSeek-R1-Distill-Qwen-32B 模型表现出色，在概念对齐上提升14%的IoU。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "8 Pages",
      "pdf_url": "http://arxiv.org/pdf/2502.15854v1",
      "published_date": "2025-02-21 06:38:57 UTC",
      "updated_date": "2025-02-21 06:38:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:12:55.966515"
    },
    {
      "arxiv_id": "2502.15243v1",
      "title": "Comparative Analysis of Large Language Models for Context-Aware Code Completion using SAFIM Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Zhang",
        "Yanxin Shen",
        "Lun Wang",
        "Chuanqi Shi",
        "Shaoshuai Du",
        "Yiyi Tao",
        "Yixian Shen"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has revolutionized code\ncompletion, transforming it into a more intelligent and context-aware feature\nin modern integrated development environments. These advancements have\nsignificantly enhanced developers' ability to write efficient and error-free\ncode. This study evaluates the performance of several chat-based LLMs,\nincluding Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, GPT-4o-mini, and GPT-4\nTurbo, using the Syntax-Aware Fill-in-the-Middle (SAFIM) dataset. This\nbenchmark is specifically designed to assess models' capabilities in\nsyntax-sensitive code generation. Performance metrics, such as cosine\nsimilarity with ground-truth completions and latency, were employed to measure\nboth accuracy and efficiency. The findings reveal substantial differences in\nthe models' code completion abilities, offering valuable insights into their\nrespective strengths and weaknesses. This work provides a comparative analysis\nthat underscores the trade-offs between accuracy and speed, establishing a\nbenchmark for future advancements in LLM-based code completion.",
      "tldr_zh": "本研究比较分析了多种大型语言模型 (LLMs) 在上下文感知代码补全中的性能，包括 Gemini 1.5 Flash、Gemini 1.5 Pro、GPT-4o、GPT-4o-mini 和 GPT-4 Turbo。研究使用 Syntax-Aware Fill-in-the-Middle (SAFIM) 数据集评估模型在语法敏感代码生成方面的能力，并采用余弦相似度 (cosine similarity) 和延迟 (latency) 等指标来衡量准确性和效率。结果显示，这些模型在代码补全能力上存在显著差异，揭示了准确性与速度之间的权衡关系，并为未来 LLM-based 代码补全技术的发展提供了重要基准。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.15243v1",
      "published_date": "2025-02-21 06:32:31 UTC",
      "updated_date": "2025-02-21 06:32:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:13:06.726980"
    },
    {
      "arxiv_id": "2502.15228v1",
      "title": "AutoMR: A Universal Time Series Motion Recognition Pipeline",
      "title_zh": "翻译失败",
      "authors": [
        "Likun Zhang",
        "Sicheng Yang",
        "Zhuo Wang",
        "Haining Liang",
        "Junxiao Shen"
      ],
      "abstract": "In this paper, we present an end-to-end automated motion recognition (AutoMR)\npipeline designed for multimodal datasets. The proposed framework seamlessly\nintegrates data preprocessing, model training, hyperparameter tuning, and\nevaluation, enabling robust performance across diverse scenarios. Our approach\naddresses two primary challenges: 1) variability in sensor data formats and\nparameters across datasets, which traditionally requires task-specific machine\nlearning implementations, and 2) the complexity and time consumption of\nhyperparameter tuning for optimal model performance. Our library features an\nall-in-one solution incorporating QuartzNet as the core model, automated\nhyperparameter tuning, and comprehensive metrics tracking. Extensive\nexperiments demonstrate its effectiveness on 10 diverse datasets, achieving\nstate-of-the-art performance. This work lays a solid foundation for deploying\nmotion-capture solutions across varied real-world applications.",
      "tldr_zh": "本研究提出 AutoMR，一个通用的端到端时间序列动作识别管道，针对多模态数据集，集成了数据预处理、模型训练、hyperparameter tuning 和评估过程。AutoMR 解决了传感器数据格式变异和超参数调优的复杂性问题，使用 QuartzNet 作为核心模型，提供一个全集成解决方案。在 10 个多样数据集上的广泛实验中，AutoMR 实现了最先进性能，并为在真实世界应用中部署动作捕捉系统奠定了坚实基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15228v1",
      "published_date": "2025-02-21 05:59:41 UTC",
      "updated_date": "2025-02-21 05:59:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:13:18.279429"
    },
    {
      "arxiv_id": "2502.15226v1",
      "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews",
      "title_zh": "翻译失败",
      "authors": [
        "Mengqiao Liu",
        "Tevin Wang",
        "Cassandra A. Cohen",
        "Sarah Li",
        "Chenyan Xiong"
      ],
      "abstract": "Which large language model (LLM) is better? Every evaluation tells a story,\nbut what do users really think about current LLMs? This paper presents CLUE, an\nLLM-powered interviewer that conducts in-the-moment user experience interviews,\nright after users interacted with LLMs, and automatically gathers insights\nabout user opinions from massive interview logs. We conduct a study with\nthousands of users to understand user opinions on mainstream LLMs, recruiting\nusers to first chat with a target LLM and then interviewed by CLUE. Our\nexperiments demonstrate that CLUE captures interesting user opinions, for\nexample, the bipolar views on the displayed reasoning process of DeepSeek-R1\nand demands for information freshness and multi-modality. Our collected\nchat-and-interview logs will be released.",
      "tldr_zh": "本研究提出 CLUE，一种基于 Large Language Models (LLMs) 的即时用户体验采访工具，用于在用户与 LLMs 互动后自动收集和分析用户意见。该方法通过招募数千用户进行聊天和采访实验，捕捉了用户对主流 LLMs 的多样化观点，例如对 DeepSeek-R1 的双极化看法，以及对信息新鲜度和多模态功能的需求。实验结果证明 CLUE 有效揭示了用户真实反馈，并将收集的聊天和采访日志公开，以推动 LLM 评估的改进。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15226v1",
      "published_date": "2025-02-21 05:42:22 UTC",
      "updated_date": "2025-02-21 05:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:13:30.406949"
    },
    {
      "arxiv_id": "2502.15224v1",
      "title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Tingting Chen",
        "Srinivas Anumasa",
        "Beibei Lin",
        "Vedant Shah",
        "Anirudh Goyal",
        "Dianbo Liu"
      ],
      "abstract": "Given the remarkable performance of Large Language Models (LLMs), an\nimportant question arises: Can LLMs conduct human-like scientific research and\ndiscover new knowledge, and act as an AI scientist? Scientific discovery is an\niterative process that demands efficient knowledge updating and encoding. It\ninvolves understanding the environment, identifying new hypotheses, and\nreasoning about actions; however, no standardized benchmark specifically\ndesigned for scientific discovery exists for LLM agents. In response to these\nlimitations, we introduce a novel benchmark, \\textit{Auto-Bench}, that\nencompasses necessary aspects to evaluate LLMs for scientific discovery in both\nnatural and social sciences. Our benchmark is based on the principles of causal\ngraph discovery. It challenges models to uncover hidden structures and make\noptimal decisions, which includes generating valid justifications. By engaging\ninteractively with an oracle, the models iteratively refine their understanding\nof underlying interactions, the chemistry and social interactions, through\nstrategic interventions. We evaluate state-of-the-art LLMs, including GPT-4,\nGemini, Qwen, Claude, and Llama, and observe a significant performance drop as\nthe problem complexity increases, which suggests an important gap between\nmachine and human intelligence that future development of LLMs need to take\ninto consideration.",
      "tldr_zh": "这篇论文引入了 Auto-Bench，这是一个自动化基准，用于评估大型语言模型 (LLMs) 在自然和社会科学领域进行科学发现的能力。Auto-Bench 基于因果图发现 (causal graph discovery) 的原则，挑战模型通过与预言机 (oracle) 互动，进行迭代推理、战略干预和生成有效理由，以揭示隐藏结构并做出最佳决策。实验结果显示，在评估 GPT-4、Gemini、Qwen、Claude 和 Llama 等最先进 LLMs 时，随着问题复杂度的增加，模型性能显著下降，强调了未来 LLMs 发展需弥合机器与人类智能的差距。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.15224v1",
      "published_date": "2025-02-21 05:35:20 UTC",
      "updated_date": "2025-02-21 05:35:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:13:46.053779"
    },
    {
      "arxiv_id": "2502.17496v1",
      "title": "SpikeRL: A Scalable and Energy-efficient Framework for Deep Spiking Reinforcement Learning",
      "title_zh": "SpikeRL：一个可扩展且节能高效的框架，用于深度脉冲强化学习",
      "authors": [
        "Tokey Tahmid",
        "Mark Gates",
        "Piotr Luszczek",
        "Catherine D. Schuman"
      ],
      "abstract": "In this era of AI revolution, massive investments in large-scale data-driven\nAI systems demand high-performance computing, consuming tremendous energy and\nresources. This trend raises new challenges in optimizing sustainability\nwithout sacrificing scalability or performance. Among the energy-efficient\nalternatives of the traditional Von Neumann architecture, neuromorphic\ncomputing and its Spiking Neural Networks (SNNs) are a promising choice due to\ntheir inherent energy efficiency. However, in some real-world application\nscenarios such as complex continuous control tasks, SNNs often lack the\nperformance optimizations that traditional artificial neural networks have.\nResearchers have addressed this by combining SNNs with Deep Reinforcement\nLearning (DeepRL), yet scalability remains unexplored. In this paper, we extend\nour previous work on SpikeRL, which is a scalable and energy efficient\nframework for DeepRL-based SNNs for continuous control. In our initial\nimplementation of SpikeRL framework, we depended on the population encoding\nfrom the Population-coded Spiking Actor Network (PopSAN) method for our SNN\nmodel and implemented distributed training with Message Passing Interface (MPI)\nthrough mpi4py. Also, further optimizing our model training by using\nmixed-precision for parameter updates. In our new SpikeRL framework, we have\nimplemented our own DeepRL-SNN component with population encoding, and\ndistributed training with PyTorch Distributed package with NCCL backend while\nstill optimizing with mixed precision training. Our new SpikeRL implementation\nis 4.26X faster and 2.25X more energy efficient than state-of-the-art\nDeepRL-SNN methods. Our proposed SpikeRL framework demonstrates a truly\nscalable and sustainable solution for complex continuous control tasks in\nreal-world applications.",
      "tldr_zh": "该研究提出SpikeRL框架，这是一个可扩展且节能高效的框架，用于Deep Spiking Reinforcement Learning（DeepRL-SNN），旨在解决传统AI系统的高能耗问题。SpikeRL构建于人口编码（population encoding）的基础上，结合分布式训练（使用PyTorch Distributed和NCCL后端）以及混合精度训练（mixed-precision），以优化SNN在复杂连续控制任务中的性能。相比现有DeepRL-SNN方法，该框架训练速度提高了4.26倍，能效提升了2.25倍，为真实世界应用的可持续AI解决方案提供了可扩展途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17496v1",
      "published_date": "2025-02-21 05:28:42 UTC",
      "updated_date": "2025-02-21 05:28:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:13:54.213477"
    },
    {
      "arxiv_id": "2502.15217v1",
      "title": "FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Madhurima Chakraborty",
        "Peter Pirkelbauer",
        "Qing Yi"
      ],
      "abstract": "FormalSpecCpp is a dataset designed to fill the gap in standardized\nbenchmarks for verifying formal specifications in C++ programs. To the best of\nour knowledge, this is the first comprehensive collection of C++ programs with\nwell-defined preconditions and postconditions. It provides a structured\nbenchmark for evaluating specification inference tools and testing theaccuracy\nof generated specifications. Researchers and developers can use this dataset to\nbenchmark specification inference tools,fine-tune Large Language Models (LLMs)\nfor automated specification generation, and analyze the role of formal\nspecifications in improving program verification and automated testing. By\nmaking this dataset publicly available, we aim to advance research in program\nverification, specification inference, and AI-assisted software development.\nThe dataset and the code are available at\nhttps://github.com/MadhuNimmo/FormalSpecCpp.",
      "tldr_zh": "本研究引入了 FormalSpecCpp 数据集，这是首个全面的 C++ 程序集合，包含明确定义的 preconditions 和 postconditions，用于填补正式规范验证基准的空白。数据集利用 LLMs 创建，提供了一个结构化的基准，供研究者评估 specification inference 工具、微调 LLMs 以自动生成规范，并分析正式规范在程序验证和自动测试中的作用。通过公开数据集和代码（可在 https://github.com/MadhuNimmo/FormalSpecCpp 获取），该工作推动了程序验证、specification inference 以及 AI 辅助软件开发的进步。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at the 2025 IEEE/ACM 22nd International Conference on Mining\n  Software Repositories (MSR)",
      "pdf_url": "http://arxiv.org/pdf/2502.15217v1",
      "published_date": "2025-02-21 05:20:04 UTC",
      "updated_date": "2025-02-21 05:20:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:14:06.306995"
    },
    {
      "arxiv_id": "2502.15214v1",
      "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sheila Schoepp",
        "Masoud Jafaripour",
        "Yingyue Cao",
        "Tianpei Yang",
        "Fatemeh Abdollahi",
        "Shadan Golestan",
        "Zahin Sufiyan",
        "Osmar R. Zaiane",
        "Matthew E. Taylor"
      ],
      "abstract": "Reinforcement learning (RL) has shown impressive results in sequential\ndecision-making tasks. Meanwhile, Large Language Models (LLMs) and\nVision-Language Models (VLMs) have emerged, exhibiting impressive capabilities\nin multimodal understanding and reasoning. These advances have led to a surge\nof research integrating LLMs and VLMs into RL. In this survey, we review\nrepresentative works in which LLMs and VLMs are used to overcome key challenges\nin RL, such as lack of prior knowledge, long-horizon planning, and reward\ndesign. We present a taxonomy that categorizes these LLM/VLM-assisted RL\napproaches into three roles: agent, planner, and reward. We conclude by\nexploring open problems, including grounding, bias mitigation, improved\nrepresentations, and action advice. By consolidating existing research and\nidentifying future directions, this survey establishes a framework for\nintegrating LLMs and VLMs into RL, advancing approaches that unify natural\nlanguage and visual understanding with sequential decision-making.",
      "tldr_zh": "这篇调查论文回顾了将Large Language Models (LLMs) 和 Vision-Language Models (VLMs) 整合到 Reinforcement Learning (RL) 中的研究进展，旨在解决RL面临的挑战，如缺乏先验知识、长期规划和奖励设计。论文提出一个分类框架，将这些整合方法分为三种角色：代理（agent）、规划器（planner）和奖励（reward）。最终，论文总结了现有工作的贡献，建立了一个统一自然语言、视觉理解与顺序决策的框架，并指出了未来方向，包括grounding、bias mitigation、改进表示和行动建议。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15214v1",
      "published_date": "2025-02-21 05:01:30 UTC",
      "updated_date": "2025-02-21 05:01:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:14:18.605218"
    },
    {
      "arxiv_id": "2502.15212v1",
      "title": "Measuring AI agent autonomy: Towards a scalable approach with code inspection",
      "title_zh": "AI 代理自治性的测量：朝向一种基于代码检查的可扩展方法",
      "authors": [
        "Peter Cihon",
        "Merlin Stein",
        "Gagan Bansal",
        "Sam Manning",
        "Kevin Xu"
      ],
      "abstract": "AI agents are AI systems that can achieve complex goals autonomously.\nAssessing the level of agent autonomy is crucial for understanding both their\npotential benefits and risks. Current assessments of autonomy often focus on\nspecific risks and rely on run-time evaluations -- observations of agent\nactions during operation. We introduce a code-based assessment of autonomy that\neliminates the need to run an AI agent to perform specific tasks, thereby\nreducing the costs and risks associated with run-time evaluations. Using this\ncode-based framework, the orchestration code used to run an AI agent can be\nscored according to a taxonomy that assesses attributes of autonomy: impact and\noversight. We demonstrate this approach with the AutoGen framework and select\napplications.",
      "tldr_zh": "这篇论文提出了一种基于代码检查的可扩展方法，来评估AI agents的自主性，从而避免了传统运行时评估的成本和风险。方法通过分析orchestration code，并根据一个taxonomy评估自主性的关键属性，包括impact和oversight。该框架以AutoGen框架和选定应用为例进行演示，结果显示这种方法能更高效地理解AI agents的潜在益处和风险。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS Socially Responsible Language Modelling Research (SoLaR)\n  Workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2502.15212v1",
      "published_date": "2025-02-21 04:58:40 UTC",
      "updated_date": "2025-02-21 04:58:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:14:28.922803"
    },
    {
      "arxiv_id": "2503.10641v1",
      "title": "Estimating Control Barriers from Offline Data",
      "title_zh": "翻译失败",
      "authors": [
        "Hongzhan Yu",
        "Seth Farrell",
        "Ryo Yoshimitsu",
        "Zhizhen Qin",
        "Henrik I. Christensen",
        "Sicun Gao"
      ],
      "abstract": "Learning-based methods for constructing control barrier functions (CBFs) are\ngaining popularity for ensuring safe robot control. A major limitation of\nexisting methods is their reliance on extensive sampling over the state space\nor online system interaction in simulation. In this work we propose a novel\nframework for learning neural CBFs through a fixed, sparsely-labeled dataset\ncollected prior to training. Our approach introduces new annotation techniques\nbased on out-of-distribution analysis, enabling efficient knowledge propagation\nfrom the limited labeled data to the unlabeled data. We also eliminate the\ndependency on a high-performance expert controller, and allow multiple\nsub-optimal policies or even manual control during data collection. We evaluate\nthe proposed method on real-world platforms. With limited amount of offline\ndata, it achieves state-of-the-art performance for dynamic obstacle avoidance,\ndemonstrating statistically safer and less conservative maneuvers compared to\nexisting methods.",
      "tldr_zh": "这篇论文提出了一种新框架，用于从离线数据学习神经 control barrier functions (CBFs)，以确保机器人控制的安全，避免依赖于大规模状态空间采样或在线交互。方法引入基于 out-of-distribution 分析的标注技术，从稀疏标记数据集高效传播知识，并允许使用多个次优策略或手动控制来收集数据。实验结果显示，在真实世界平台上，该框架在动态障碍物避免任务中实现了 state-of-the-art 性能，提供更安全且不那么保守的操作。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.RO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "This paper has been accepted to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.10641v1",
      "published_date": "2025-02-21 04:55:20 UTC",
      "updated_date": "2025-02-21 04:55:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:14:42.173992"
    },
    {
      "arxiv_id": "2502.15210v2",
      "title": "PairBench: A Systematic Framework for Selecting Reliable Judge VLMs",
      "title_zh": "PairBench：用于选择可靠判断视觉语言模型的系统框架",
      "authors": [
        "Aarash Feizi",
        "Sai Rajeswar",
        "Adriana Romero-Soriano",
        "Reihaneh Rabbany",
        "Spandana Gella",
        "Valentina Zantedeschi",
        "João Monteiro"
      ],
      "abstract": "As large vision language models (VLMs) are increasingly used as automated\nevaluators, understanding their ability to effectively compare data pairs as\ninstructed in the prompt becomes essential. To address this, we present\nPairBench, a low-cost framework that systematically evaluates VLMs as\ncustomizable similarity tools across various modalities and scenarios. Through\nPairBench, we introduce four metrics that represent key desiderata of\nsimilarity scores: alignment with human annotations, consistency for data pairs\nirrespective of their order, smoothness of similarity distributions, and\ncontrollability through prompting. Our analysis demonstrates that no model,\nwhether closed- or open-source, is superior on all metrics; the optimal choice\ndepends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp\njudge), highlighting risks of widespread adoption of VLMs as evaluators without\nthorough assessment. For instance, the majority of VLMs struggle with\nmaintaining symmetric similarity scores regardless of order. Additionally, our\nresults show that the performance of VLMs on the metrics in PairBench closely\ncorrelates with popular benchmarks, showcasing its predictive power in ranking\nmodels.",
      "tldr_zh": "本研究引入了PairBench，一种低成本系统框架，用于评估视觉语言模型(VLMs)作为自动评估器的可靠性，专注于VLMs在比较数据对时的相似性能力。框架定义了四个关键指标，包括与人类标注的对齐、一致性（不受数据对顺序影响）、相似性分布的平滑性，以及通过提示的可控性。分析结果显示，没有任何封闭源或开放源模型在所有指标上均表现出色，最优选择取决于期望的行为（如平滑或锐利的判断），并强调了在广泛采用VLMs作为评估器前进行彻底评估的风险；此外，PairBench的性能指标与流行基准高度相关，具有预测模型排名的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15210v2",
      "published_date": "2025-02-21 04:53:11 UTC",
      "updated_date": "2025-02-24 15:01:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:14:54.739008"
    },
    {
      "arxiv_id": "2502.15851v1",
      "title": "Control Illusion: The Failure of Instruction Hierarchies in Large Language Models",
      "title_zh": "控制幻觉：大语言模型中指令层次结构的失败",
      "authors": [
        "Yilin Geng",
        "Haonan Li",
        "Honglin Mu",
        "Xudong Han",
        "Timothy Baldwin",
        "Omri Abend",
        "Eduard Hovy",
        "Lea Frermann"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed with hierarchical\ninstruction schemes, where certain instructions (e.g., system-level directives)\nare expected to take precedence over others (e.g., user messages). Yet, we lack\na systematic understanding of how effectively these hierarchical control\nmechanisms work. We introduce a systematic evaluation framework based on\nconstraint prioritization to assess how well LLMs enforce instruction\nhierarchies. Our experiments across six state-of-the-art LLMs reveal that\nmodels struggle with consistent instruction prioritization, even for simple\nformatting conflicts. We find that the widely-adopted system/user prompt\nseparation fails to establish a reliable instruction hierarchy, and models\nexhibit strong inherent biases toward certain constraint types regardless of\ntheir priority designation. While controlled prompt engineering and model\nfine-tuning show modest improvements, our results indicate that instruction\nhierarchy enforcement is not robustly realized, calling for deeper\narchitectural innovations beyond surface-level modifications.",
      "tldr_zh": "该研究揭示了大型语言模型（LLMs）在指令层次（如系统级指令优先于用户消息）上的失败问题，并引入了一个基于约束优先化的系统评估框架来评估其执行效果。实验涉及六种最先进的LLMs，结果显示模型在处理简单格式冲突时难以一致优先化指令，且系统/用户提示分离未能建立可靠的层次结构，同时存在对特定约束类型的固有偏见。尽管受控提示工程和模型微调带来 modest 改善，但指令层次执行仍不稳健，论文呼吁更深层的架构创新以解决这一问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15851v1",
      "published_date": "2025-02-21 04:51:37 UTC",
      "updated_date": "2025-02-21 04:51:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:15:08.695859"
    },
    {
      "arxiv_id": "2502.15203v1",
      "title": "FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Young Beom Woo",
        "Sun Eung Kim"
      ],
      "abstract": "Recently, methods that integrate multiple personalized concepts into a single\nimage have garnered significant attention in the field of text-to-image (T2I)\ngeneration. However, existing methods experience performance degradation in\ncomplex scenes with multiple objects due to distortions in non-personalized\nregions. To address this issue, we propose FlipConcept, a novel approach that\nseamlessly integrates multiple personalized concepts into a single image\nwithout requiring additional tuning. We introduce guided appearance attention\nto accurately mimic the appearance of a personalized concept as intended.\nAdditionally, we introduce mask-guided noise mixing to protect non-personalized\nregions during editing. Lastly, we apply background dilution to minimize\nattribute leakage, which is the undesired blending of personalized concept\nattributes with other objects in the image. In our experiments, we demonstrate\nthat the proposed method, despite not requiring tuning, outperforms existing\nmodels in both single and multiple personalized concept inference.",
      "tldr_zh": "该研究提出 FlipConcept，一种无需额外微调的多概念个性化方法，用于文本到图像 (T2I) 生成，旨在解决现有方法在复杂场景中非个性化区域失真的问题。该方法引入 guided appearance attention 来精确模仿个性化概念的外观、mask-guided noise mixing 来保护非个性化区域，以及 background dilution 来最小化属性泄漏。实验结果显示，FlipConcept 在单人和多概念推断任务上优于现有模型，提供更高质量的图像生成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15203v1",
      "published_date": "2025-02-21 04:37:18 UTC",
      "updated_date": "2025-02-21 04:37:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:15:18.667991"
    },
    {
      "arxiv_id": "2502.15197v1",
      "title": "TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaoxuan Wu",
        "Zijian Zhou",
        "Arun Verma",
        "Alok Prakash",
        "Daniela Rus",
        "Bryan Kian Hsiang Low"
      ],
      "abstract": "We propose TETRIS, a novel method that optimizes the total throughput of\nbatch speculative decoding in multi-request settings. Unlike existing methods\nthat optimize for a single request or a group of requests as a whole, TETRIS\nactively selects the most promising draft tokens (for every request in a batch)\nto be accepted when verified in parallel, resulting in fewer rejected tokens\nand hence less wasted computing resources. Such an effective resource\nutilization to achieve fast inference in large language models (LLMs) is\nespecially important to service providers with limited inference capacity.\nCompared to baseline speculative decoding, TETRIS yields a consistently higher\nacceptance rate and more effective utilization of the limited inference\ncapacity. We show theoretically and empirically that TETRIS outperforms\nbaseline speculative decoding and existing methods that dynamically select\ndraft tokens, leading to a more efficient batch inference in LLMs.",
      "tldr_zh": "本研究提出 TETRIS，一种优化批量推测解码（speculative decoding）的创新方法，旨在通过主动选择每个请求中最有前景的 draft tokens 来提高总吞吐量和资源利用效率。不同于现有方法，TETRIS 在多请求环境中并行验证并接受这些 tokens，从而减少被拒绝 tokens 和计算资源浪费，尤其适用于计算能力有限的服务提供者。实验和理论分析表明，TETRIS 相较于基线方法和动态选择 draft tokens 的技术，提供更高的接受率和更有效的 batch inference 性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 10 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15197v1",
      "published_date": "2025-02-21 04:19:24 UTC",
      "updated_date": "2025-02-21 04:19:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:15:31.199989"
    },
    {
      "arxiv_id": "2502.15189v1",
      "title": "Scale-Free Graph-Language Models",
      "title_zh": "无标度图语言模型",
      "authors": [
        "Jianglin Lu",
        "Yixuan Liu",
        "Yitian Zhang",
        "Yun Fu"
      ],
      "abstract": "Graph-language models (GLMs) have demonstrated great potential in graph-based\nsemi-supervised learning. A typical GLM consists of two key stages: graph\ngeneration and text embedding, which are usually implemented by inferring a\nlatent graph and finetuning a language model (LM), respectively. However, the\nformer often relies on artificial assumptions about the underlying edge\ndistribution, while the latter requires extensive data annotations. To tackle\nthese challenges, this paper introduces a novel GLM that integrates graph\ngeneration and text embedding within a unified framework. Specifically, for\ngraph generation, we leverage an inherent characteristic of real edge\ndistribution--the scale-free property--as a structural prior. We unexpectedly\nfind that this natural property can be effectively approximated by a simple\nk-nearest neighbor (KNN) graph. For text embedding, we develop a graph-based\npseudo-labeler that utilizes scale-free graphs to provide complementary\nsupervision for improved LM finetuning. Extensive experiments on representative\ndatasets validate our findings on the scale-free structural approximation of\nKNN graphs and demonstrate the effectiveness of integrating graph generation\nand text embedding with a real structural prior. Our code is available at\nhttps://github.com/Jianglin954/SFGL.",
      "tldr_zh": "本文提出了一种新型的 Scale-Free Graph-Language Models (GLMs)，通过整合图生成和文本嵌入于统一框架中，解决了传统 GLMs 依赖人工边分布假设和大量数据标注的挑战。具体而言，该方法利用真实边分布的 scale-free property 作为结构先验，并发现 k-nearest neighbor (KNN) 图能有效近似这一属性；同时，开发了基于图的伪标签器，利用 scale-free 图提供额外监督来改进 language model (LM) 的微调。在多个代表性数据集上的广泛实验验证了 KNN 图的近似效果，并证明了该框架在图-based 半监督学习中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15189v1",
      "published_date": "2025-02-21 03:41:43 UTC",
      "updated_date": "2025-02-21 03:41:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:15:43.904544"
    },
    {
      "arxiv_id": "2502.15186v1",
      "title": "LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise Adaptation Network for Image Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Namrah Siddiqua",
        "Kim Suneung"
      ],
      "abstract": "Low-light image enhancement (LLIE) is a crucial task in computer vision aimed\nto enhance the visual fidelity of images captured under low-illumination\nconditions. Conventional methods frequently struggle to mitigate pervasive\nshortcomings such as noise, over-exposure, and color distortion thereby\nprecipitating a pronounced degradation in image quality. To address these\nchallenges, we propose LUMINA-Net an advanced deep learning framework designed\nspecifically by integrating multi-stage illumination and reflectance modules.\nFirst, the illumination module intelligently adjusts brightness and contrast\nlevels while meticulously preserving intricate textural details. Second, the\nreflectance module incorporates a noise reduction mechanism that leverages\nspatial attention and channel-wise feature refinement to mitigate noise\ncontamination. Through a comprehensive suite of experiments conducted on LOL\nand SICE datasets using PSNR, SSIM and LPIPS metrics, surpassing\nstate-of-the-art methodologies and showcasing its efficacy in low-light image\nenhancement.",
      "tldr_zh": "该研究针对低光图像增强 (LLIE) 的挑战，如噪声、过曝和颜色失真问题，提出了一种先进的深度学习框架 LUMINA-Net。框架整合多阶段模块，包括照明模块（智能调整亮度和对比度，同时保留纹理细节）和反射模块（利用空间注意力和通道特征精炼机制减少噪声污染）。实验结果显示，在 LOL 和 SICE 数据集上，使用 PSNR、SSIM 和 LPIPS 指标，LUMINA-Net 超越了现有最先进方法，显著提升了图像增强效果。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15186v1",
      "published_date": "2025-02-21 03:37:58 UTC",
      "updated_date": "2025-02-21 03:37:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:15:55.719285"
    },
    {
      "arxiv_id": "2502.15185v1",
      "title": "Key Body Posture Characteristics of Short-distance Speed Skaters at the Start Based on Artificial Intelligence",
      "title_zh": "基于人工智能的短距离速滑运动员起跑关键身体姿势特征",
      "authors": [
        "Zhang Xueliana",
        "Fang Yingjieb",
        "Liu Hang"
      ],
      "abstract": "Objective To conduct biomechanical analysis on the starting technique of male\nshort-distance speed skating athletes in China and determine the key factors\naffecting the effectiveness of the starting movement. Methods 13 high-level\nmale short-distance speed skating athletes were selected as the test subjects,\nand kinematic data were collected using an artificial intelligence video\ncapture and analysis system. The body posture features and their effects on the\nstarting movement performance were analyzed in the three stages of starting\npreparation, starting, and sprinting. Results The post-stability angle,\nanterior knee angle of the front leg, posterior knee angle of the rear leg, and\nstride length showed moderate to high positive correlations with the starting\nspeed during the starting preparation stage. The trunk angle showed a high\nnegative correlation with the starting speed. The trunk angle (TO4, TD4, TO6,\nTD6), hip angle (TO1, TO4, TO6), and knee angle (TD1) showed moderate to high\nnegative correlations with the effectiveness of the starting movement during\nthe starting and sprinting stages. The knee angle (TD2), ice-contact angle\n(TD2, TD4, TD5, TD6), and propulsion angle (TO1, TO4, TO7) showed moderate\npositive correlations with the effectiveness of the starting movement.\nConclusion Stride length, left knee angle, and post-stability angle are the key\nfactors affecting the starting speed. The larger the post-stability angle and\nleft knee angle and the longer the stride length, the faster the starting\nspeed. During the starting and sprinting stages, the smaller the ice-contact\nangle and propulsion angle, the greater the trunk angle and hip angle changes,\nthe more effective the starting movement.",
      "tldr_zh": "本研究利用人工智能视频捕捉和分析系统，对13名中国高级男性短距离速滑运动员的起跑技术进行生物力学分析，旨在识别影响起跑有效性的关键身体姿势特征。结果显示，在起跑准备阶段，后稳定角（post-stability angle）、前腿前膝角、后腿后膝角和步幅长度（stride length）与起跑速度呈中度到高度正相关，而躯干角（trunk angle）呈高度负相关。起跑和冲刺阶段的关键因素包括躯干角、髋关节角（hip angle）和膝关节角（knee angle）的负相关，以及冰接触角（ice-contact angle）和推进角（propulsion angle）的正相关。结论是，增大后稳定角、左膝角并延长步幅长度可提升起跑速度，而减小冰接触角和推进角、增加躯干和髋关节角度变化能优化起跑运动的有效性。",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15185v1",
      "published_date": "2025-02-21 03:36:17 UTC",
      "updated_date": "2025-02-21 03:36:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:16:08.348422"
    },
    {
      "arxiv_id": "2502.15182v1",
      "title": "LEDD: Large Language Model-Empowered Data Discovery in Data Lakes",
      "title_zh": "翻译失败",
      "authors": [
        "Qi An",
        "Chihua Ying",
        "Yuqing Zhu",
        "Yihao Xu",
        "Manwei Zhang",
        "Jianmin Wang"
      ],
      "abstract": "Data discovery in data lakes with ever increasing datasets has long been\nrecognized as a big challenge in the realm of data management, especially for\nsemantic search of and hierarchical global catalog generation of tables. While\nlarge language models (LLMs) facilitate the processing of data semantics,\nchallenges remain in architecting an end-to-end system that comprehensively\nexploits LLMs for the two semantics-related tasks. In this demo, we propose\nLEDD, an end-to-end system with an extensible architecture that leverages LLMs\nto provide hierarchical global catalogs with semantic meanings and semantic\ntable search for data lakes. Specifically, LEDD can return semantically related\ntables based on natural-language specification. These features make LEDD an\nideal foundation for downstream tasks such as model training and schema linking\nfor text-to-SQL tasks. LEDD also provides a simple Python interface to\nfacilitate the extension and the replacement of data discovery algorithms.",
      "tldr_zh": "这篇论文介绍了LEDD，一种利用Large Language Models (LLMs)增强的数据湖数据发现系统，旨在解决数据湖中数据集不断增长带来的语义搜索和分层全局目录生成挑战。LEDD采用端到端架构，通过LLMs处理数据语义，提供基于自然语言规范的语义相关表搜索和分层目录功能。系统支持下游任务如模型训练和schema linking，并通过简单的Python接口实现算法的扩展和替换。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15182v1",
      "published_date": "2025-02-21 03:30:43 UTC",
      "updated_date": "2025-02-21 03:30:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:16:19.742820"
    },
    {
      "arxiv_id": "2502.15176v1",
      "title": "Methods and Trends in Detecting Generated Images: A Comprehensive Review",
      "title_zh": "检测生成图像的方法与趋势：全面综述",
      "authors": [
        "Arpan Mahara",
        "Naphtali Rishe"
      ],
      "abstract": "The proliferation of generative models, such as Generative Adversarial\nNetworks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has\nenabled the synthesis of high-quality multimedia data. However, these\nadvancements have also raised significant concerns regarding adversarial\nattacks, unethical usage, and societal harm. Recognizing these challenges,\nresearchers have increasingly focused on developing methodologies to detect\nsynthesized data effectively, aiming to mitigate potential risks. Prior reviews\nhave primarily focused on deepfake detection and often lack coverage of recent\nadvancements in synthetic image detection, particularly methods leveraging\nmultimodal frameworks for improved forensic analysis. To address this gap, the\npresent survey provides a comprehensive review of state-of-the-art methods for\ndetecting and classifying synthetic images generated by advanced generative AI\nmodels. This review systematically examines core detection methodologies,\nidentifies commonalities among approaches, and categorizes them into meaningful\ntaxonomies. Furthermore, given the crucial role of large-scale datasets in this\nfield, we present an overview of publicly available datasets that facilitate\nfurther research and benchmarking in synthetic data detection.",
      "tldr_zh": "这篇综述论文探讨了检测生成图像的方法和趋势，针对 Generative Adversarial Networks (GANs)、Diffusion Models 和 Variational Autoencoders (VAEs) 等生成模型带来的图像合成问题，以及潜在的对抗攻击和不道德使用风险。论文系统地审查了最先进的检测和分类方法，将它们归类到有意义的分类学中，并识别了这些方法之间的共同点，以填补现有综述（如deepfake检测）对最新多模态框架的覆盖不足。最终，论文概述了公开可用的数据集，用于支持合成数据检测的研究和基准测试，从而为该领域的发展提供全面指导。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "30 pages, 4 Figures, 10 Tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15176v1",
      "published_date": "2025-02-21 03:16:18 UTC",
      "updated_date": "2025-02-21 03:16:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:16:30.896407"
    },
    {
      "arxiv_id": "2502.15850v2",
      "title": "Forecasting Frontier Language Model Agent Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Govind Pimpale",
        "Axel Højmark",
        "Jérémy Scheurer",
        "Marius Hobbhahn"
      ],
      "abstract": "As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.",
      "tldr_zh": "这篇论文评估了六种预测方法，以准确预估 Language Models (LMs) 作为代理的下游能力，包括直接从输入指标（如计算量或发布日期）预测基准分数，或两步方法（如先预测中间指标 PC-1 和 Elo ratings 再预测性能）。研究通过在 OpenLLM 2 排行榜的 38 个 LMs 上回测，验证了这些方法的有效性，并使用两步方法（发布日期→Elo→基准）对前沿模型进行预测。预测结果显示，到 2026 年初，非专业化 LM 代理在 SWE-Bench Verified（软件开发基准）的成功率将达到 54%，而最先进的 LM 代理将达到 87%。然而，该预测可能因未考虑推理计算缩放的最新进展而显得过于保守。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15850v2",
      "published_date": "2025-02-21 02:34:17 UTC",
      "updated_date": "2025-03-03 17:11:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:16:45.044199"
    },
    {
      "arxiv_id": "2502.15849v2",
      "title": "Deriving Representative Structure from Music Corpora",
      "title_zh": "翻译失败",
      "authors": [
        "Ilana Shapiro",
        "Ruanqianqian Huang",
        "Zachary Novack",
        "Cheng-i Wang",
        "Hao-Wen Dong",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov",
        "Sorin Lerner"
      ],
      "abstract": "Western music is an innately hierarchical system of interacting levels of\nstructure, from fine-grained melody to high-level form. In order to analyze\nmusic compositions holistically and at multiple granularities, we propose a\nunified, hierarchical meta-representation of musical structure called the\nstructural temporal graph (STG). For a single piece, the STG is a data\nstructure that defines a hierarchy of progressively finer structural musical\nfeatures and the temporal relationships between them. We use the STG to enable\na novel approach for deriving a representative structural summary of a music\ncorpus, which we formalize as a dually NP-hard combinatorial optimization\nproblem extending the Generalized Median Graph problem. Our approach first\napplies simulated annealing to develop a measure of structural distance between\ntwo music pieces rooted in graph isomorphism. Our approach then combines the\nformal guarantees of SMT solvers with nested simulated annealing over\nstructural distances to produce a structurally sound, representative centroid\nSTG for an entire corpus of STGs from individual pieces. To evaluate our\napproach, we conduct experiments verifying that structural distance accurately\ndifferentiates between music pieces, and that derived centroids accurately\nstructurally characterize their corpora.",
      "tldr_zh": "本研究提出了一种统一的层次化元表示Structural Temporal Graph (STG)，用于整体分析西方音乐的多粒度结构，包括从细粒度的旋律到高层形式的层次关系。研究将从音乐语料库中推导代表性结构摘要形式化为一个dually NP-hard的组合优化问题，扩展了Generalized Median Graph问题，并通过simulated annealing计算基于图同构的结构距离。接着，结合SMT solvers和嵌套simulated annealing，生成语料库的代表性中心STG。实验结果显示，该结构距离能准确区分音乐作品，而派生出的中心STG有效地表征了语料库的整体结构。",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.SD",
        "G.1.6; I.2.4; J.5; G.2.2"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 8 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15849v2",
      "published_date": "2025-02-21 02:32:29 UTC",
      "updated_date": "2025-03-30 22:09:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:16:55.395411"
    },
    {
      "arxiv_id": "2502.15155v1",
      "title": "Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models",
      "title_zh": "极端言论分类在 LLMs 时代：探索开源和专有模型",
      "authors": [
        "Sarthak Mahajan",
        "Nimmi Rangaswamy"
      ],
      "abstract": "In recent years, widespread internet adoption and the growth in userbase of\nvarious social media platforms have led to an increase in the proliferation of\nextreme speech online. While traditional language models have demonstrated\nproficiency in distinguishing between neutral text and non-neutral text (i.e.\nextreme speech), categorizing the diverse types of extreme speech presents\nsignificant challenges. The task of extreme speech classification is\nparticularly nuanced, as it requires a deep understanding of socio-cultural\ncontexts to accurately interpret the intent of the language used by the\nspeaker. Even human annotators often disagree on the appropriate classification\nof such content, emphasizing the complex and subjective nature of this task.\nThe use of human moderators also presents a scaling issue, necessitating the\nneed for automated systems for extreme speech classification. The recent launch\nof ChatGPT has drawn global attention to the potential applications of Large\nLanguage Models (LLMs) across a diverse variety of tasks. Trained on vast and\ndiverse corpora, and demonstrating the ability to effectively capture and\nencode contextual information, LLMs emerge as highly promising tools for\ntackling this specific task of extreme speech classification. In this paper, we\nleverage the Indian subset of the extreme speech dataset from Maronikolakis et\nal. (2022) to develop an effective classification framework using LLMs. We\nevaluate open-source Llama models against closed-source OpenAI models, finding\nthat while pre-trained LLMs show moderate efficacy, fine-tuning with\ndomain-specific data significantly enhances performance, highlighting their\nadaptability to linguistic and contextual nuances. Although GPT-based models\noutperform Llama models in zero-shot settings, the performance gap disappears\nafter fine-tuning.",
      "tldr_zh": "互联网的普及导致极端言论在线上泛滥，而传统语言模型虽能区分中性与非中性文本，但分类多样类型极端言论仍面临社会文化上下文的复杂挑战。本文利用Marinikolakis et al. (2022)的印度子集数据集，评估开源Llama模型和专有OpenAI模型在极端言论分类中的表现，发现预训练LLMs的效果中等，但通过微调领域特定数据可显著提升性能。研究结果表明，在零样本设置中GPT模型优于Llama模型，但微调后性能差距消失，这突显了LLMs在处理语言细微差别的适应潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to 7th International Conference on information systems and\n  management science (ISMS), 2024",
      "pdf_url": "http://arxiv.org/pdf/2502.15155v1",
      "published_date": "2025-02-21 02:31:05 UTC",
      "updated_date": "2025-02-21 02:31:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:17:07.311265"
    },
    {
      "arxiv_id": "2502.15152v2",
      "title": "CW-BASS: Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic Segmentation",
      "title_zh": "CW-B",
      "authors": [
        "Ebenezer Tarubinga",
        "Jenifer Kalafatovich",
        "Seong-Whan Lee"
      ],
      "abstract": "Semi-supervised semantic segmentation (SSSS) aims to improve segmentation\nperformance by utilizing large amounts of unlabeled data with limited labeled\nsamples. Existing methods often suffer from coupling, where over-reliance on\ninitial labeled data leads to suboptimal learning; confirmation bias, where\nincorrect predictions reinforce themselves repeatedly; and boundary blur caused\nby limited boundary-awareness and ambiguous edge cues. To address these issues,\nwe propose CW-BASS, a novel framework for SSSS. In order to mitigate the impact\nof incorrect predictions, we assign confidence weights to pseudo-labels.\nAdditionally, we leverage boundary-delineation techniques, which, despite being\nextensively explored in weakly-supervised semantic segmentation (WSSS), remain\nunderutilized in SSSS. Specifically, our method: (1) reduces coupling via a\nconfidence-weighted loss that adjusts pseudo-label influence based on their\npredicted confidence scores, (2) mitigates confirmation bias with a dynamic\nthresholding mechanism that learns to filter out pseudo-labels based on model\nperformance, (3) tackles boundary blur using a boundary-aware module to refine\nsegmentation near object edges, and (4) reduces label noise through a\nconfidence decay strategy that progressively refines pseudo-labels during\ntraining. Extensive experiments on Pascal VOC 2012 and Cityscapes demonstrate\nthat CW-BASS achieves state-of-the-art performance. Notably, CW-BASS achieves a\n65.9% mIoU on Cityscapes under a challenging and underexplored 1/30 (3.3%)\nsplit (100 images), highlighting its effectiveness in limited-label settings.\nOur code is available at https://github.com/psychofict/CW-BASS.",
      "tldr_zh": "本文提出 CW-BASS，一种针对半监督语义分割 (SSSS) 的新框架，旨在解决现有方法中的耦合、确认偏差和边界模糊问题。该框架通过置信加权损失调整伪标签影响、动态阈值机制过滤低质量伪标签、边界感知模块精炼对象边缘，以及置信衰减策略减少标签噪声，来提升模型的鲁棒性和准确性。在 Pascal VOC 2012 和 Cityscapes 数据集上，CW-BASS 实现了最先进性能，尤其在标签有限的 Cityscapes 1/30 分割（100 张图像）下，mIoU 达到 65.9%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15152v2",
      "published_date": "2025-02-21 02:24:10 UTC",
      "updated_date": "2025-04-09 08:07:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:17:21.077007"
    },
    {
      "arxiv_id": "2502.15145v2",
      "title": "Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF",
      "title_zh": "翻译失败",
      "authors": [
        "Nuoya Xiong",
        "Aarti Singh"
      ],
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) is a widely used\nfine-tuning approach that aligns machine learning model, particularly Language\nModel (LM) with human preferences. There are typically multiple objectives\ndriving the preference, hence humans find it easier to express per-objective\ncomparisons rather than a global preference between two choices.\nMulti-Objective RLHF (MORLHF) aims to use per-objective preference feedback and\nachieve Pareto optimality among these objectives by aggregating them into a\nsingle unified objective for optimization. However, nearly all prior works rely\non linear aggregation, which rules out policies that favor specific objectives\nsuch as the worst one. The only existing approach using non-linear aggregation\nis computationally expensive due to its reward-based nature and the need for\nretraining whenever the aggregation parameters change. In this work, we address\nthis limitation by transforming the non-linear aggregation maximization problem\ninto a series of sub-problems. Each sub-problem involves only linear\naggregation, making it computationally efficient to solve. We further extend\nour framework to handle multi-group scenarios, where each group has distinct\nweights for the objectives. Our method enables achieving consensus or\nmaximizing the aggregated objective across all groups. Theoretically, we\ndemonstrate that our algorithmic framework achieves sublinear regret and can be\neasily adapted to a reward-free algorithm. Empirically, leveraging our\ntheoretical insights, we propose a nearly training-free algorithm once the\noptimal policies for individual objectives are obtained.",
      "tldr_zh": "这篇论文提出了一种通用的Projection Optimization框架，用于处理多目标和多组Reinforcement Learning with Human Feedback (RLHF)，旨在通过非线性聚合最大化问题转化为一系列线性子问题来提高计算效率。框架解决了现有方法的局限性，如线性聚合的限制和非线性方法的高成本，并扩展到多组场景，每个组具有不同的目标权重，以实现共识或最大化聚合目标。理论上，该框架证明了sublinear regret的性能，并可轻松适应reward-free算法。实证结果显示，一旦获得单个目标的最优策略，该方法几乎不需要额外训练，从而为复杂偏好下的RLHF优化提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15145v2",
      "published_date": "2025-02-21 01:56:52 UTC",
      "updated_date": "2025-02-24 06:06:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:17:34.238985"
    },
    {
      "arxiv_id": "2502.15134v1",
      "title": "Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG in Edge Device",
      "title_zh": "翻译失败",
      "authors": [
        "Juntae Lee",
        "Jihwan Bang",
        "Seunghan Yang",
        "Kyuhong Shim",
        "Simyung Chang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) with large language models (LLMs) is\nespecially valuable in specialized domains, where precision is critical. To\nmore specialize the LLMs into a target domain, domain-specific RAG has recently\nbeen developed by allowing the LLM to access the target domain early via\nfinetuning. The domain-specific RAG makes more sense in resource-constrained\nenvironments like edge devices, as they should perform a specific task (e.g.\npersonalization) reliably using only small-scale LLMs. While the\ndomain-specific RAG is well-aligned with edge devices in this respect, it often\nrelies on widely-used reasoning techniques like chain-of-thought (CoT). The\nreasoning step is useful to understand the given external knowledge, and yet it\nis computationally expensive and difficult for small-scale LLMs to learn it.\nTackling this, we propose the Chain of Rank (CoR) which shifts the focus from\nintricate lengthy reasoning to simple ranking of the reliability of input\nexternal documents. Then, CoR reduces computational complexity while\nmaintaining high accuracy, making it particularly suited for\nresource-constrained environments. We attain the state-of-the-art (SOTA)\nresults in benchmarks, and analyze its efficacy.",
      "tldr_zh": "这篇论文针对领域特定检索增强生成 (RAG) 在边缘设备上的应用，提出 Chain of Rank (CoR) 方法，以增强大型语言模型 (LLMs) 的性能。传统方法依赖链式思维 (CoT) 推理，但这会导致计算开销过大且不适合小规模 LLMs；CoR 通过简单排名输入外部文档的可靠性，减少了复杂性，同时保持高准确性。实验结果显示，CoR 在基准测试中达到了 state-of-the-art (SOTA) 水平，并证明了其在资源受限环境中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025 (Findings)",
      "pdf_url": "http://arxiv.org/pdf/2502.15134v1",
      "published_date": "2025-02-21 01:28:12 UTC",
      "updated_date": "2025-02-21 01:28:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:17:46.273041"
    },
    {
      "arxiv_id": "2502.15132v3",
      "title": "CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Vignesh Kothapalli",
        "Hamed Firooz",
        "Maziar Sanjabi"
      ],
      "abstract": "We introduce CoT-ICL Lab, a framework and methodology to generate synthetic\ntokenized datasets and systematically study chain-of-thought (CoT) in-context\nlearning (ICL) in language models. CoT-ICL Lab allows fine grained control over\nthe complexity of in-context examples by decoupling (1) the causal structure\ninvolved in chain token generation from (2) the underlying token processing\nfunctions. We train decoder-only transformers (up to 700M parameters) on these\ndatasets and show that CoT accelerates the accuracy transition to higher values\nacross model sizes. In particular, we find that model depth is crucial for\nleveraging CoT with limited in-context examples, while more examples help\nshallow models match deeper model performance. Additionally, limiting the\ndiversity of token processing functions throughout training improves causal\nstructure learning via ICL. We also interpret these transitions by analyzing\ntransformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a\nsimple yet powerful testbed for theoretical and empirical insights into ICL and\nCoT in language models.",
      "tldr_zh": "我们引入了 CoT-ICL Lab，这是一个合成框架，用于生成标记数据集并系统研究语言模型中的 Chain-of-Thought (CoT) 和 In-Context Learning (ICL)。该框架通过分离因果结构和底层标记处理函数，实现对上下文示例复杂性的精细控制，并在训练 decoder-only transformers（最多700M参数）时发现 CoT 能加速模型准确率的提升。研究表明，模型深度对利用有限的上下文示例至关重要，而增加示例数量可帮助浅层模型匹配深层模型性能，且限制标记处理函数多样性能改善因果结构学习。通过分析 transformer 的嵌入和注意力图，解释了这些学习转变，CoT-ICL Lab 因此成为 ICL 和 CoT 研究的强大测试平台。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL Main 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.15132v3",
      "published_date": "2025-02-21 01:24:54 UTC",
      "updated_date": "2025-05-21 21:21:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:18:00.160002"
    },
    {
      "arxiv_id": "2502.15127v1",
      "title": "The Imitation Game for Educational AI",
      "title_zh": "翻译失败",
      "authors": [
        "Shashank Sonkar",
        "Naiming Liu",
        "Xinghe Chen",
        "Richard G. Baraniuk"
      ],
      "abstract": "As artificial intelligence systems become increasingly prevalent in\neducation, a fundamental challenge emerges: how can we verify if an AI truly\nunderstands how students think and reason? Traditional evaluation methods like\nmeasuring learning gains require lengthy studies confounded by numerous\nvariables. We present a novel evaluation framework based on a two-phase\nTuring-like test. In Phase 1, students provide open-ended responses to\nquestions, revealing natural misconceptions. In Phase 2, both AI and human\nexperts, conditioned on each student's specific mistakes, generate distractors\nfor new related questions. By analyzing whether students select AI-generated\ndistractors at rates similar to human expert-generated ones, we can validate if\nthe AI models student cognition. We prove this evaluation must be conditioned\non individual responses - unconditioned approaches merely target common\nmisconceptions. Through rigorous statistical sampling theory, we establish\nprecise requirements for high-confidence validation. Our research positions\nconditioned distractor generation as a probe into an AI system's fundamental\nability to model student thinking - a capability that enables adapting\ntutoring, feedback, and assessments to each student's specific needs.",
      "tldr_zh": "这篇论文针对教育AI提出一个基于Turing-like测试的创新评估框架，用于验证AI是否能真正理解学生的思考和推理。框架分为两阶段：第一阶段，学生对问题提供开放式回答，以揭示自然误区；第二阶段，AI和人类专家基于学生的特定错误生成干扰项(distractors)，并通过分析学生选择AI生成干扰项的频率与人类专家相当来评估AI的认知建模能力。论文证明，这种评估必须以个体响应为基础，而非针对常见误区；同时，通过统计抽样理论(stochastic sampling theory)确立了高置信度验证的标准。该框架有助于AI实现个性化适应，如定制辅导、反馈和评估，以提升教育效果。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15127v1",
      "published_date": "2025-02-21 01:14:55 UTC",
      "updated_date": "2025-02-21 01:14:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:18:10.935543"
    },
    {
      "arxiv_id": "2502.15120v1",
      "title": "Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps",
      "title_zh": "翻译失败",
      "authors": [
        "Yen-Che Hsiao",
        "Abhishek Dutta"
      ],
      "abstract": "This study investigates the in-context learning capabilities of various\ndecoder-only transformer-based language models with different model sizes and\ntraining data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and\nGemma 2. We identify a critical parameter threshold (~1.6 billion), beyond\nwhich reasoning performance improves significantly in tasks such as commonsense\nreasoning in multiple-choice question answering and deductive reasoning.\nSpecifically, models above this threshold achieve better success rates in\nchain-of-thought (CoT) prompting for deductive reasoning tasks, especially\nthose requiring longer reasoning chains, such as proof by contradiction and\ndisjunction elimination. To address limitations in sub-threshold models, we\ndemonstrate that fine-tuning with task-specific exemplars substantially\nenhances reasoning performance, enabling accurate CoT generation even without\nadditional exemplars in the prompt for tasks with shorter reasoning chains.\nFinally, our analysis of attention maps reveals that models capable of\ngenerating correct CoTs exhibit higher token-level attention scores on\nsubsequent correct tokens and the correct parts of speech, providing\ninterpretability insights into reasoning processes. These findings collectively\nadvance understanding of reasoning capabilities in decoder-only\ntransformer-based models. The code is available at:\nhttps://github.com/AnnonymousForPapers/CoT_Reasoning_Test.",
      "tldr_zh": "本研究探讨了各种 decoder-only transformer-based 语言模型（如 GPT2 和 Gemma 2）的 in-context learning 能力，发现模型参数超过约 1.6 亿时，推理性能（如常识推理和演绎推理）显著提升，尤其在 chain-of-thought (CoT) 提示下处理长推理链任务时表现更好。针对参数低于阈值的模型，研究通过 fine-tuning 和任务特定示例，显著改善了其推理性能，甚至能在不使用额外示例的情况下处理较短推理链。分析 attention maps 结果显示，生成正确 CoT 的模型在后续正确 token 和词性部分表现出更高的 token-level attention scores，提供了对推理过程的解释性洞见。这些发现整体推进了对 decoder-only transformer-based 模型推理能力的理解，并开源了相关代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15120v1",
      "published_date": "2025-02-21 00:48:32 UTC",
      "updated_date": "2025-02-21 00:48:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:18:24.159137"
    },
    {
      "arxiv_id": "2502.15119v1",
      "title": "CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Sheng",
        "Zilin Huang",
        "Yansong Qu",
        "Yue Leng",
        "Sruthi Bhavanam",
        "Sikai Chen"
      ],
      "abstract": "Ensuring safety in autonomous driving systems remains a critical challenge,\nparticularly in handling rare but potentially catastrophic safety-critical\nscenarios. While existing research has explored generating safety-critical\nscenarios for autonomous vehicle (AV) testing, there is limited work on\neffectively incorporating these scenarios into policy learning to enhance\nsafety. Furthermore, developing training curricula that adapt to an AV's\nevolving behavioral patterns and performance bottlenecks remains largely\nunexplored. To address these challenges, we propose CurricuVLM, a novel\nframework that leverages Vision-Language Models (VLMs) to enable personalized\ncurriculum learning for autonomous driving agents. Our approach uniquely\nexploits VLMs' multimodal understanding capabilities to analyze agent behavior,\nidentify performance weaknesses, and dynamically generate tailored training\nscenarios for curriculum adaptation. Through comprehensive analysis of unsafe\ndriving situations with narrative descriptions, CurricuVLM performs in-depth\nreasoning to evaluate the AV's capabilities and identify critical behavioral\npatterns. The framework then synthesizes customized training scenarios\ntargeting these identified limitations, enabling effective and personalized\ncurriculum learning. Extensive experiments on the Waymo Open Motion Dataset\nshow that CurricuVLM outperforms state-of-the-art baselines across both regular\nand safety-critical scenarios, achieving superior performance in terms of\nnavigation success, driving efficiency, and safety metrics. Further analysis\nreveals that CurricuVLM serves as a general approach that can be integrated\nwith various RL algorithms to enhance autonomous driving systems. The code and\ndemo video are available at: https://zihaosheng.github.io/CurricuVLM/.",
      "tldr_zh": "该研究提出了一种名为 CurricuVLM 的框架，利用 Vision-Language Models (VLMs) 实现个性化的安全关键课程学习，以提升自动驾驶系统的安全性。该框架通过分析代理行为、识别性能弱点，并动态生成定制训练场景，帮助自动驾驶代理适应稀有灾难性场景。CurricuVLM 结合多模态理解能力，进行深度推理和场景合成，针对不安全驾驶情况进行个性化优化。在 Waymo Open Motion Dataset 的实验中，该框架超越了现有基线，在导航成功率、驾驶效率和安全指标上表现出色，并可与各种 RL algorithms 整合，增强自动驾驶系统的整体性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15119v1",
      "published_date": "2025-02-21 00:42:40 UTC",
      "updated_date": "2025-02-21 00:42:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:18:33.897168"
    },
    {
      "arxiv_id": "2502.15107v1",
      "title": "Assessing a Single Student's Concentration on Learning Platforms: A Machine Learning-Enhanced EEG-Based Framework",
      "title_zh": "评估单个学生在",
      "authors": [
        "Zewen Zhuo",
        "Mohamad Najafi",
        "Hazem Zein",
        "Amine Nait-Ali"
      ],
      "abstract": "This study introduces a specialized pipeline designed to classify the\nconcentration state of an individual student during online learning sessions by\ntraining a custom-tailored machine learning model. Detailed protocols for\nacquiring and preprocessing EEG data are outlined, along with the extraction of\nfifty statistical features from five EEG signal bands: alpha, beta, theta,\ndelta, and gamma. Following feature extraction, a thorough feature selection\nprocess was conducted to optimize the data inputs for a personalized analysis.\nThe study also explores the benefits of hyperparameter fine-tuning to enhance\nthe classification accuracy of the student's concentration state. EEG signals\nwere captured from the student using a Muse headband (Gen 2), equipped with\nfive electrodes (TP9, AF7, AF8, TP10, and a reference electrode NZ), during\nengagement with educational content on computer-based e-learning platforms.\nEmploying a random forest model customized to the student's data, we achieved\nremarkable classification performance, with test accuracies of 97.6% in the\ncomputer-based learning setting and 98% in the virtual reality setting. These\nresults underscore the effectiveness of our approach in delivering personalized\ninsights into student concentration during online educational activities.",
      "tldr_zh": "这篇论文提出了一种基于 machine learning 的 EEG 框架，用于评估单个学生在在线学习平台上的注意力状态，通过训练自定义模型实现个性化分类。研究详细描述了 EEG 数据采集和预处理过程，包括从 alpha、beta、theta、delta 和 gamma 五个频带提取五十个统计特征，并通过特征选择和超参数微调优化模型输入。采用随机森林模型进行分类，在计算机学习设置中达到97.6%的测试准确率，在虚拟现实设置中达到98%。这些结果突显了该框架在提供个性化学生注意力洞察方面的有效性，为在线教育优化提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15107v1",
      "published_date": "2025-02-21 00:02:28 UTC",
      "updated_date": "2025-02-21 00:02:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T17:18:46.493220"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 157,
  "processed_papers_count": 157,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-23T17:19:08.642129"
}
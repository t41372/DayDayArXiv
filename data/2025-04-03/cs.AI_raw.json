[
  {
    "arxiv_id": "2504.02828v1",
    "title": "Concept Lancet: Image Editing with Compositional Representation Transplant",
    "authors": [
      "Jinqi Luo",
      "Tianjiao Ding",
      "Kwan Ho Ryan Chan",
      "Hancheng Min",
      "Chris Callison-Burch",
      "René Vidal"
    ],
    "abstract": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan",
    "pdf_url": "http://arxiv.org/pdf/2504.02828v1",
    "published_date": "2025-04-03 17:59:58 UTC",
    "updated_date": "2025-04-03 17:59:58 UTC"
  },
  {
    "arxiv_id": "2504.02827v1",
    "title": "On Vanishing Variance in Transformer Length Generalization",
    "authors": [
      "Ruining Li",
      "Gabrijel Boduljak",
      "Jensen",
      "Zhou"
    ],
    "abstract": "It is a widely known issue that Transformers, when trained on shorter\nsequences, fail to generalize robustly to longer ones at test time. This raises\nthe question of whether Transformer models are real reasoning engines, despite\ntheir impressive abilities in mathematical problem solving and code synthesis.\nIn this paper, we offer a vanishing variance perspective on this issue. To the\nbest of our knowledge, we are the first to demonstrate that even for today's\nfrontier models, a longer sequence length results in a decrease in variance in\nthe output of the multi-head attention modules. On the argmax retrieval and\ndictionary lookup tasks, our experiments show that applying layer normalization\nafter the attention outputs leads to significantly better length\ngeneralization. Our analyses attribute this improvement to a reduction-though\nnot a complete elimination-of the distribution shift caused by vanishing\nvariance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page: https://ruiningli.com/vanishing-variance. The first two\n  authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2504.02827v1",
    "published_date": "2025-04-03 17:59:56 UTC",
    "updated_date": "2025-04-03 17:59:56 UTC"
  },
  {
    "arxiv_id": "2504.02822v1",
    "title": "Do Two AI Scientists Agree?",
    "authors": [
      "Xinghong Fu",
      "Ziming Liu",
      "Max Tegmark"
    ],
    "abstract": "When two AI models are trained on the same scientific task, do they learn the\nsame theory or two different theories? Throughout history of science, we have\nwitnessed the rise and fall of theories driven by experimental validation or\nfalsification: many theories may co-exist when experimental data is lacking,\nbut the space of survived theories become more constrained with more\nexperimental data becoming available. We show the same story is true for AI\nscientists. With increasingly more systems provided in training data, AI\nscientists tend to converge in the theories they learned, although sometimes\nthey form distinct groups corresponding to different theories. To\nmechanistically interpret what theories AI scientists learn and quantify their\nagreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI\nScientists, trained on standard problems in physics, aggregating training\nresults across many seeds simulating the different configurations of AI\nscientists. Our findings suggests for AI scientists switch from learning a\nHamiltonian theory in simple setups to a Lagrangian formulation when more\ncomplex systems are introduced. We also observe strong seed dependence of the\ntraining dynamics and final learned weights, controlling the rise and fall of\nrelevant theories. We finally demonstrate that not only can our neural networks\naid interpretability, it can also be applied to higher dimensional problems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02822v1",
    "published_date": "2025-04-03 17:58:44 UTC",
    "updated_date": "2025-04-03 17:58:44 UTC"
  },
  {
    "arxiv_id": "2504.02821v1",
    "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models",
    "authors": [
      "Mateusz Pach",
      "Shyamgopal Karthik",
      "Quentin Bouniot",
      "Serge Belongie",
      "Zeynep Akata"
    ],
    "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. The code is available at\n  https://github.com/ExplainableML/sae-for-vlm",
    "pdf_url": "http://arxiv.org/pdf/2504.02821v1",
    "published_date": "2025-04-03 17:58:35 UTC",
    "updated_date": "2025-04-03 17:58:35 UTC"
  },
  {
    "arxiv_id": "2504.02819v1",
    "title": "GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings",
    "authors": [
      "Yuexi Du",
      "Jiazhen Zhang",
      "Nicha C. Dvornek",
      "John A. Onofrey"
    ],
    "abstract": "Symmetry, where certain features remain invariant under geometric\ntransformations, can often serve as a powerful prior in designing convolutional\nneural networks (CNNs). While conventional CNNs inherently support\ntranslational equivariance, extending this property to rotation and reflection\nhas proven challenging, often forcing a compromise between equivariance,\nefficiency, and information loss. In this work, we introduce Gaussian Mixture\nRing Convolution (GMR-Conv), an efficient convolution kernel that smooths\nradial symmetry using a mixture of Gaussian-weighted rings. This design\nmitigates discretization errors of circular kernels, thereby preserving robust\nrotation and reflection equivariance without incurring computational overhead.\nWe further optimize both the space and speed efficiency of GMR-Conv via a novel\nparameterization and computation strategy, allowing larger kernels at an\nacceptable cost. Extensive experiments on eight classification and one\nsegmentation datasets demonstrate that GMR-Conv not only matches conventional\nCNNs' performance but can also surpass it in applications with orientation-less\ndata. GMR-Conv is also proven to be more robust and efficient than the\nstate-of-the-art equivariant learning methods. Our work provides inspiring\nempirical evidence that carefully applied radial symmetry can alleviate the\nchallenges of information loss, marking a promising advance in equivariant\nnetwork architectures. The code is available at\nhttps://github.com/XYPB/GMR-Conv.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02819v1",
    "published_date": "2025-04-03 17:58:18 UTC",
    "updated_date": "2025-04-03 17:58:18 UTC"
  },
  {
    "arxiv_id": "2504.02810v1",
    "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
    "authors": [
      "Haowei Lin",
      "Xiangyu Wang",
      "Ruilin Yan",
      "Baizhou Huang",
      "Haotian Ye",
      "Jianhua Zhu",
      "Zihao Wang",
      "James Zou",
      "Jianzhu Ma",
      "Yitao Liang"
    ],
    "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02810v1",
    "published_date": "2025-04-03 17:54:18 UTC",
    "updated_date": "2025-04-03 17:54:18 UTC"
  },
  {
    "arxiv_id": "2504.02807v1",
    "title": "MegaMath: Pushing the Limits of Open Math Corpora",
    "authors": [
      "Fan Zhou",
      "Zengzhi Wang",
      "Nikhil Ranjan",
      "Zhoujun Cheng",
      "Liping Tang",
      "Guowei He",
      "Zhengzhong Liu",
      "Eric P. Xing"
    ],
    "abstract": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 15 figures, 22 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.02807v1",
    "published_date": "2025-04-03 17:52:07 UTC",
    "updated_date": "2025-04-03 17:52:07 UTC"
  },
  {
    "arxiv_id": "2504.02799v1",
    "title": "Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence",
    "authors": [
      "Anita Rau",
      "Mark Endo",
      "Josiah Aklilu",
      "Jaewoo Heo",
      "Khaled Saab",
      "Alberto Paderno",
      "Jeffrey Jopling",
      "F. Christopher Holsinger",
      "Serena Yeung-Levy"
    ],
    "abstract": "Large Vision-Language Models offer a new paradigm for AI-driven image\nunderstanding, enabling models to perform tasks without task-specific training.\nThis flexibility holds particular promise across medicine, where\nexpert-annotated data is scarce. Yet, VLMs' practical utility in\nintervention-focused domains--especially surgery, where decision-making is\nsubjective and clinical scenarios are variable--remains uncertain. Here, we\npresent a comprehensive analysis of 11 state-of-the-art VLMs across 17 key\nvisual understanding tasks in surgical AI--from anatomy recognition to skill\nassessment--using 13 datasets spanning laparoscopic, robotic, and open\nprocedures. In our experiments, VLMs demonstrate promising generalizability, at\ntimes outperforming supervised models when deployed outside their training\nsetting. In-context learning, incorporating examples during testing, boosted\nperformance up to three-fold, suggesting adaptability as a key strength. Still,\ntasks requiring spatial or temporal reasoning remained difficult. Beyond\nsurgery, our findings offer insights into VLMs' potential for tackling complex\nand dynamic scenarios in clinical and broader real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02799v1",
    "published_date": "2025-04-03 17:42:56 UTC",
    "updated_date": "2025-04-03 17:42:56 UTC"
  },
  {
    "arxiv_id": "2504.02793v1",
    "title": "A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models",
    "authors": [
      "Gaurav Verma",
      "Jiawei Zhou",
      "Mohit Chandra",
      "Srijan Kumar",
      "Munmun De Choudhury"
    ],
    "abstract": "Large artificial intelligence (AI) models have garnered significant attention\nfor their remarkable, often \"superhuman\", performance on standardized\nbenchmarks. However, when these models are deployed in high-stakes verticals\nsuch as healthcare, education, and law, they often reveal notable limitations.\nFor instance, they exhibit brittleness to minor variations in input data,\npresent contextually uninformed decisions in critical settings, and undermine\nuser trust by confidently producing or reproducing inaccuracies. These\nchallenges in applying large models necessitate cross-disciplinary innovations\nto align the models' capabilities with the needs of real-world applications. We\nintroduce a framework that addresses this gap through a layer-wise abstraction\nof innovations aimed at meeting users' requirements with large models. Through\nmultiple case studies, we illustrate how researchers and practitioners across\nvarious fields can operationalize this framework. Beyond modularizing the\npipeline of transforming large models into useful \"vertical systems\", we also\nhighlight the dynamism that exists within different layers of the framework.\nFinally, we discuss how our framework can guide researchers and practitioners\nto (i) optimally situate their innovations (e.g., when vertical-specific\ninsights can empower broadly impactful vertical-agnostic innovations), (ii)\nuncover overlooked opportunities (e.g., spotting recurring problems across\nverticals to develop practically useful foundation models instead of chasing\nbenchmarks), and (iii) facilitate cross-disciplinary communication of critical\nchallenges (e.g., enabling a shared vocabulary for AI developers, domain\nexperts, and human-computer interaction scholars).",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "pre-print; 7 pages of main content, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2504.02793v1",
    "published_date": "2025-04-03 17:40:11 UTC",
    "updated_date": "2025-04-03 17:40:11 UTC"
  },
  {
    "arxiv_id": "2504.02792v1",
    "title": "Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets",
    "authors": [
      "Chuning Zhu",
      "Raymond Yu",
      "Siyuan Feng",
      "Benjamin Burchfiel",
      "Paarth Shah",
      "Abhishek Gupta"
    ],
    "abstract": "Imitation learning has emerged as a promising approach towards building\ngeneralist robots. However, scaling imitation learning for large robot\nfoundation models remains challenging due to its reliance on high-quality\nexpert demonstrations. Meanwhile, large amounts of video data depicting a wide\nrange of environments and diverse behaviors are readily available. This data\nprovides a rich source of information about real-world dynamics and\nagent-environment interactions. Leveraging this data directly for imitation\nlearning, however, has proven difficult due to the lack of action annotation\nrequired for most contemporary methods. In this work, we present Unified World\nModels (UWM), a framework that allows for leveraging both video and action data\nfor policy learning. Specifically, a UWM integrates an action diffusion process\nand a video diffusion process within a unified transformer architecture, where\nindependent diffusion timesteps govern each modality. We show that by simply\ncontrolling each diffusion timestep, UWM can flexibly represent a policy, a\nforward dynamics, an inverse dynamics, and a video generator. Through simulated\nand real-world experiments, we show that: (1) UWM enables effective pretraining\non large-scale multitask robot datasets with both dynamics and action\npredictions, resulting in more generalizable and robust policies than imitation\nlearning, (2) UWM naturally facilitates learning from action-free video data\nthrough independent control of modality-specific diffusion timesteps, further\nimproving the performance of finetuned policies. Our results suggest that UWM\noffers a promising step toward harnessing large, heterogeneous datasets for\nscalable robot learning, and provides a simple unification between the often\ndisparate paradigms of imitation learning and world modeling. Videos and code\nare available at https://weirdlabuw.github.io/uwm/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02792v1",
    "published_date": "2025-04-03 17:38:59 UTC",
    "updated_date": "2025-04-03 17:38:59 UTC"
  },
  {
    "arxiv_id": "2504.02781v1",
    "title": "Towards Green AI-Native Networks: Evaluation of Neural Circuit Policy for Estimating Energy Consumption of Base Stations",
    "authors": [
      "Selim Ickin",
      "Shruti Bothe",
      "Aman Raparia",
      "Nitin Khanna",
      "Erik Sanders"
    ],
    "abstract": "Optimization of radio hardware and AI-based network management software yield\nsignificant energy savings in radio access networks. The execution of\nunderlying Machine Learning (ML) models, which enable energy savings through\nrecommended actions, may require additional compute and energy, highlighting\nthe opportunity to explore and adopt accurate and energy-efficient ML\ntechnologies. This work evaluates the novel use of sparsely structured Neural\nCircuit Policies (NCPs) in a use case to estimate the energy consumption of\nbase stations. Sparsity in ML models yields reduced memory, computation and\nenergy demand, hence facilitating a low-cost and scalable solution. We also\nevaluate the generalization capability of NCPs in comparison to traditional and\nwidely used ML models such as Long Short Term Memory (LSTM), via quantifying\ntheir sensitivity to varying model hyper-parameters (HPs). NCPs demonstrated a\nclear reduction in computational overhead and energy consumption. Moreover,\nresults indicated that the NCPs are robust to varying HPs such as number of\nepochs and neurons in each layer, making them a suitable option to ease model\nmanagement and to reduce energy consumption in Machine Learning Operations\n(MLOps) in telecommunications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02781v1",
    "published_date": "2025-04-03 17:22:39 UTC",
    "updated_date": "2025-04-03 17:22:39 UTC"
  },
  {
    "arxiv_id": "2504.02780v1",
    "title": "From Consumption to Collaboration: Measuring Interaction Patterns to Augment Human Cognition in Open-Ended Tasks",
    "authors": [
      "Joshua Holstein",
      "Moritz Diener",
      "Philipp Spitzer"
    ],
    "abstract": "The rise of Generative AI, and Large Language Models (LLMs) in particular, is\nfundamentally changing cognitive processes in knowledge work, raising critical\nquestions about their impact on human reasoning and problem-solving\ncapabilities. As these AI systems become increasingly integrated into\nworkflows, they offer unprecedented opportunities for augmenting human thinking\nwhile simultaneously risking cognitive erosion through passive consumption of\ngenerated answers. This tension is particularly pronounced in open-ended tasks,\nwhere effective solutions require deep contextualization and integration of\ndomain knowledge. Unlike structured tasks with established metrics, measuring\nthe quality of human-LLM interaction in such open-ended tasks poses significant\nchallenges due to the absence of ground truth and the iterative nature of\nsolution development. To address this, we present a framework that analyzes\ninteraction patterns along two dimensions: cognitive activity mode (exploration\nvs. exploitation) and cognitive engagement mode (constructive vs. detrimental).\nThis framework provides systematic measurements to evaluate when LLMs are\neffective tools for thought rather than substitutes for human cognition,\nadvancing theoretical understanding and practical guidance for developing AI\nsystems that protect and augment human cognitive capabilities.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at Tools for Thought Workshop (CHI'25)",
    "pdf_url": "http://arxiv.org/pdf/2504.02780v1",
    "published_date": "2025-04-03 17:20:36 UTC",
    "updated_date": "2025-04-03 17:20:36 UTC"
  },
  {
    "arxiv_id": "2504.02778v1",
    "title": "Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition",
    "authors": [
      "Vincent Gbouna Zakka",
      "Luis J. Manso",
      "Zhuangzhuang Dai"
    ],
    "abstract": "Human activity recognition is increasingly vital for supporting independent\nliving, particularly for the elderly and those in need of assistance. Domestic\nservice robots with monitoring capabilities can enhance safety and provide\nessential support. Although image-based methods have advanced considerably in\nthe past decade, their adoption remains limited by concerns over privacy and\nsensitivity to low-light or dark conditions. As an alternative, millimetre-wave\n(mmWave) radar can produce point cloud data which is privacy-preserving.\nHowever, processing the sparse and noisy point clouds remains a long-standing\nchallenge. While graph-based methods and attention mechanisms show promise,\nthey predominantly rely on \"fixed\" kernels; kernels that are applied uniformly\nacross all neighbourhoods, highlighting the need for adaptive approaches that\ncan dynamically adjust their kernels to the specific geometry of each local\nneighbourhood in point cloud data. To overcome this limitation, we introduce an\nadaptive approach within the graph convolutional framework. Instead of a single\nshared weight function, our Multi-Head Adaptive Kernel (MAK) module generates\nmultiple dynamic kernels, each capturing different aspects of the local feature\nspace. By progressively refining local features while maintaining global\nspatial context, our method enables convolution kernels to adapt to varying\nlocal features. Experimental results on benchmark datasets confirm the\neffectiveness of our approach, achieving state-of-the-art performance in human\nactivity recognition. Our source code is made publicly available at:\nhttps://github.com/Gbouna/MAK-GCN",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02778v1",
    "published_date": "2025-04-03 17:19:20 UTC",
    "updated_date": "2025-04-03 17:19:20 UTC"
  },
  {
    "arxiv_id": "2504.02767v1",
    "title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
    "authors": [
      "Andres Algaba",
      "Vincent Holst",
      "Floriano Tori",
      "Melika Mobini",
      "Brecht Verbeken",
      "Sylvia Wenmackers",
      "Vincent Ginis"
    ],
    "abstract": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.DL",
    "comment": "32 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02767v1",
    "published_date": "2025-04-03 17:04:56 UTC",
    "updated_date": "2025-04-03 17:04:56 UTC"
  },
  {
    "arxiv_id": "2504.02764v1",
    "title": "Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model",
    "authors": [
      "Shengjun Zhang",
      "Jinzhao Li",
      "Xin Fei",
      "Hao Liu",
      "Yueqi Duan"
    ],
    "abstract": "In this paper, we propose Scene Splatter, a momentum-based paradigm for video\ndiffusion to generate generic scenes from single image. Existing methods, which\nemploy video generation models to synthesize novel views, suffer from limited\nvideo length and scene inconsistency, leading to artifacts and distortions\nduring further reconstruction. To address this issue, we construct noisy\nsamples from original features as momentum to enhance video details and\nmaintain scene consistency. However, for latent features with the perception\nfield that spans both known and unknown regions, such latent-level momentum\nrestricts the generative ability of video diffusion in unknown regions.\nTherefore, we further introduce the aforementioned consistent video as a\npixel-level momentum to a directly generated video without momentum for better\nrecovery of unseen regions. Our cascaded momentum enables video diffusion\nmodels to generate both high-fidelity and consistent novel views. We further\nfinetune the global Gaussian representations with enhanced frames and render\nnew frames for momentum update in the next step. In this manner, we can\niteratively recover a 3D scene, avoiding the limitation of video length.\nExtensive experiments demonstrate the generalization capability and superior\nperformance of our method in high-fidelity and consistent scene generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.02764v1",
    "published_date": "2025-04-03 17:00:44 UTC",
    "updated_date": "2025-04-03 17:00:44 UTC"
  },
  {
    "arxiv_id": "2504.02737v1",
    "title": "RBR4DNN: Requirements-based Testing of Neural Networks",
    "authors": [
      "Nusrat Jahan Mozumder",
      "Felipe Toledo",
      "Swaroopa Dola",
      "Matthew B. Dwyer"
    ],
    "abstract": "Deep neural network (DNN) testing is crucial for the reliability and safety\nof critical systems, where failures can have severe consequences. Although\nvarious techniques have been developed to create robustness test suites,\nrequirements-based testing for DNNs remains largely unexplored -- yet such\ntests are recognized as an essential component of software validation of\ncritical systems. In this work, we propose a requirements-based test suite\ngeneration method that uses structured natural language requirements formulated\nin a semantic feature space to create test suites by prompting text-conditional\nlatent diffusion models with the requirement precondition and then using the\nassociated postcondition to define a test oracle to judge outputs of the DNN\nunder test. We investigate the approach using fine-tuned variants of\npre-trained generative models. Our experiments on the MNIST, CelebA-HQ,\nImageNet, and autonomous car driving datasets demonstrate that the generated\ntest suites are realistic, diverse, consistent with preconditions, and capable\nof revealing faults.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02737v1",
    "published_date": "2025-04-03 16:24:49 UTC",
    "updated_date": "2025-04-03 16:24:49 UTC"
  },
  {
    "arxiv_id": "2504.02724v1",
    "title": "Autonomous Human-Robot Interaction via Operator Imitation",
    "authors": [
      "Sammy Christen",
      "David Müller",
      "Agon Serifi",
      "Ruben Grandia",
      "Georg Wiedebach",
      "Michael A. Hopkins",
      "Espen Knoop",
      "Moritz Bächer"
    ],
    "abstract": "Teleoperated robotic characters can perform expressive interactions with\nhumans, relying on the operators' experience and social intuition. In this\nwork, we propose to create autonomous interactive robots, by training a model\nto imitate operator data. Our model is trained on a dataset of human-robot\ninteractions, where an expert operator is asked to vary the interactions and\nmood of the robot, while the operator commands as well as the pose of the human\nand robot are recorded. Our approach learns to predict continuous operator\ncommands through a diffusion process and discrete commands through a\nclassifier, all unified within a single transformer architecture. We evaluate\nthe resulting model in simulation and with a user study on the real system. We\nshow that our method enables simple autonomous human-robot interactions that\nare comparable to the expert-operator baseline, and that users can recognize\nthe different robot moods as generated by our model. Finally, we demonstrate a\nzero-shot transfer of our model onto a different robotic platform with the same\noperator interface.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02724v1",
    "published_date": "2025-04-03 16:06:44 UTC",
    "updated_date": "2025-04-03 16:06:44 UTC"
  },
  {
    "arxiv_id": "2504.02701v1",
    "title": "Responsible Development of Offensive AI",
    "authors": [
      "Ryan Marinelli"
    ],
    "abstract": "As AI advances, broader consensus is needed to determine research priorities.\nThis endeavor discusses offensive AI and provides guidance by leveraging\nSustainable Development Goals (SDGs) and interpretability techniques. The\nobjective is to more effectively establish priorities that balance societal\nbenefits against risks. The two forms of offensive AI evaluated in this study\nare vulnerability detection agents, which solve Capture- The-Flag challenges,\nand AI-powered malware.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02701v1",
    "published_date": "2025-04-03 15:37:38 UTC",
    "updated_date": "2025-04-03 15:37:38 UTC"
  },
  {
    "arxiv_id": "2504.02698v1",
    "title": "SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions",
    "authors": [
      "Shengrui XU",
      "Tianchi Lu",
      "Zikun Wang",
      "Jixiu Zhai",
      "Jingwan Wang"
    ],
    "abstract": "Protein-Protein Interaction (PPI) prediction is a key task in uncovering\ncellular functional networks and disease mechanisms. However, traditional\nexperimental methods are time-consuming and costly, and existing computational\nmodels face challenges in cross-modal feature fusion, robustness, and\nfalse-negative suppression. In this paper, we propose a novel supervised\ncontrastive multimodal framework, SCMPPI, for PPI prediction. By integrating\nprotein sequence features (AAC, DPC, CKSAAP-ESMC) with PPI network topology\ninformation (Node2Vec graph embedding), and combining an improved supervised\ncontrastive learning strategy, SCMPPI significantly enhances PPI prediction\nperformance. For the PPI task, SCMPPI introduces a negative sample filtering\nmechanism and modifies the contrastive loss function, effectively optimizing\nmultimodal features. Experiments on eight benchmark datasets, including yeast,\nhuman, and H.pylori, show that SCMPPI outperforms existing state-of-the-art\nmethods (such as DF-PPI and TAGPPI) in key metrics such as accuracy ( 98.01%)\nand AUC (99.62%), and demonstrates strong generalization in cross-species\nprediction (AUC > 99% on multi-species datasets). Furthermore, SCMPPI has been\nsuccessfully applied to CD9 networks, the Wnt pathway, and cancer-specific\nnetworks, providing a reliable tool for disease target discovery. This\nframework also offers a new paradigm for multimodal biological information\nfusion and contrastive learning in collaborative optimization for various\ncombined predictions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM",
      "92C40, 68T07",
      "I.2.6; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages,11 figures,conference",
    "pdf_url": "http://arxiv.org/pdf/2504.02698v1",
    "published_date": "2025-04-03 15:34:02 UTC",
    "updated_date": "2025-04-03 15:34:02 UTC"
  },
  {
    "arxiv_id": "2504.02685v1",
    "title": "STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability",
    "authors": [
      "Iván Sevillano-García",
      "Julián Luengo",
      "Francisco Herrera"
    ],
    "abstract": "Out-of-Distribution (OOD) detection is a critical task in machine learning,\nparticularly in safety-sensitive applications where model failures can have\nserious consequences. However, current OOD detection methods often suffer from\nrestrictive distributional assumptions, limited scalability, and a lack of\ninterpretability. To address these challenges, we propose STOOD-X, a two-stage\nmethodology that combines a Statistical nonparametric Test for OOD Detection\nwith eXplainability enhancements. In the first stage, STOOD-X uses\nfeature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD\nsamples without assuming a specific feature distribution. In the second stage,\nit generates user-friendly, concept-based visual explanations that reveal the\nfeatures driving each decision, aligning with the BLUE XAI paradigm. Through\nextensive experiments on benchmark datasets and multiple architectures, STOOD-X\nachieves competitive performance against state-of-the-art post hoc OOD\ndetectors, particularly in high-dimensional and complex settings. In addition,\nits explainability framework enables human oversight, bias detection, and model\ndebugging, fostering trust and collaboration between humans and AI systems. The\nSTOOD-X methodology therefore offers a robust, explainable, and scalable\nsolution for real-world OOD detection tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 7 Figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02685v1",
    "published_date": "2025-04-03 15:26:03 UTC",
    "updated_date": "2025-04-03 15:26:03 UTC"
  },
  {
    "arxiv_id": "2504.02670v1",
    "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
    "authors": [
      "Maciej Besta",
      "Lorenzo Paleari",
      "Jia Hao Andrea Jiang",
      "Robert Gerstenberger",
      "You Wu",
      "Patrick Iff",
      "Ales Kubicek",
      "Piotr Nyczyk",
      "Diana Khimey",
      "Jón Gunnar Hannesson",
      "Grzegorz Kwaśniewski",
      "Marcin Copik",
      "Hubert Niewiadomski",
      "Torsten Hoefler"
    ],
    "abstract": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02670v1",
    "published_date": "2025-04-03 15:11:55 UTC",
    "updated_date": "2025-04-03 15:11:55 UTC"
  },
  {
    "arxiv_id": "2504.02654v1",
    "title": "SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning",
    "authors": [
      "Ivo Amador",
      "Nina Gierasimczuk"
    ],
    "abstract": "We propose a learning architecture that allows symbolic control and guidance\nin reinforcement learning with deep neural networks. We introduce SymDQN, a\nnovel modular approach that augments the existing Dueling Deep Q-Networks\n(DuelDQN) architecture with modules based on the neuro-symbolic framework of\nLogic Tensor Networks (LTNs). The modules guide action policy learning and\nallow reinforcement learning agents to display behaviour consistent with\nreasoning about the environment. Our experiment is an ablation study performed\non the modules. It is conducted in a reinforcement learning environment of a\n5x5 grid navigated by an agent that encounters various shapes, each associated\nwith a given reward. The underlying DuelDQN attempts to learn the optimal\nbehaviour of the agent in this environment, while the modules facilitate shape\nrecognition and reward prediction. We show that our architecture significantly\nimproves learning, both in terms of performance and the precision of the agent.\nThe modularity of SymDQN allows reflecting on the intricacies and complexities\nof combining neural and symbolic approaches in reinforcement learning.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.NE",
      "I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02654v1",
    "published_date": "2025-04-03 14:51:11 UTC",
    "updated_date": "2025-04-03 14:51:11 UTC"
  },
  {
    "arxiv_id": "2504.02646v1",
    "title": "Prompt Optimization with Logged Bandit Data",
    "authors": [
      "Haruka Kiyohara",
      "Daniel Yiming Cao",
      "Yuta Saito",
      "Thorsten Joachims"
    ],
    "abstract": "We study how to use naturally available user feedback, such as clicks, to\noptimize large language model (LLM) pipelines for generating personalized\nsentences using prompts. Naive approaches, which estimate the policy gradient\nin the prompt space, suffer either from variance caused by the large action\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\nthese challenges, we propose a novel kernel-based off-policy gradient method,\nwhich estimates the policy gradient by leveraging similarity among generated\nsentences, substantially reducing variance while suppressing the bias.\nEmpirical results on our newly established suite of benchmarks demonstrate the\neffectiveness of the proposed approach in generating personalized descriptions\nfor movie recommendations, particularly when the number of candidate prompts is\nlarge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2504.02646v1",
    "published_date": "2025-04-03 14:40:40 UTC",
    "updated_date": "2025-04-03 14:40:40 UTC"
  },
  {
    "arxiv_id": "2504.02623v1",
    "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions",
    "authors": [
      "PeiJie Yu",
      "Yifan Yang",
      "Jinjian Li",
      "Zelong Zhang",
      "Haorui Wang",
      "Xiao Feng",
      "Feng Zhang"
    ],
    "abstract": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02623v1",
    "published_date": "2025-04-03 14:21:33 UTC",
    "updated_date": "2025-04-03 14:21:33 UTC"
  },
  {
    "arxiv_id": "2504.02620v1",
    "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
    "authors": [
      "Leonardo Iurada",
      "Marco Ciccone",
      "Tatiana Tommasi"
    ],
    "abstract": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted ICLR 2025 - https://github.com/iurada/talos-task-arithmetic",
    "pdf_url": "http://arxiv.org/pdf/2504.02620v1",
    "published_date": "2025-04-03 14:20:06 UTC",
    "updated_date": "2025-04-03 14:20:06 UTC"
  },
  {
    "arxiv_id": "2504.02607v1",
    "title": "Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks",
    "authors": [
      "Samuel Tesfazgi",
      "Leonhard Sprandl",
      "Sandra Hirche"
    ],
    "abstract": "The practical deployment of learning-based autonomous systems would greatly\nbenefit from tools that flexibly obtain safety guarantees in the form of\ncertificate functions from data. While the geometrical properties of such\ncertificate functions are well understood, synthesizing them using machine\nlearning techniques still remains a challenge. To mitigate this issue, we\npropose a diffeomorphic function learning framework where prior structural\nknowledge of the desired output is encoded in the geometry of a simple\nsurrogate function, which is subsequently augmented through an expressive,\ntopology-preserving state-space transformation. Thereby, we achieve an indirect\nfunction approximation framework that is guaranteed to remain in the desired\nhypothesis space. To this end, we introduce a novel approach to construct\ndiffeomorphic maps based on RBF networks, which facilitate precise, local\ntransformations around data. Finally, we demonstrate our approach by learning\ndiffeomorphic Lyapunov functions from real-world data and apply our method to\ndifferent attractor systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02607v1",
    "published_date": "2025-04-03 14:09:17 UTC",
    "updated_date": "2025-04-03 14:09:17 UTC"
  },
  {
    "arxiv_id": "2504.02606v1",
    "title": "Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification",
    "authors": [
      "Jonas Teufel",
      "Annika Leinweber",
      "Pascal Friederich"
    ],
    "abstract": "Explainable AI (xAI) interventions aim to improve interpretability for\ncomplex black-box models, not only to improve user trust but also as a means to\nextract scientific insights from high-performing predictive systems. In\nmolecular property prediction, counterfactual explanations offer a way to\nunderstand predictive behavior by highlighting which minimal perturbations in\nthe input molecular structure cause the greatest deviation in the predicted\nproperty. However, such explanations only allow for meaningful scientific\ninsights if they reflect the distribution of the true underlying property -- a\nfeature we define as counterfactual truthfulness. To increase this\ntruthfulness, we propose the integration of uncertainty estimation techniques\nto filter counterfactual candidates with high predicted uncertainty. Through\ncomputational experiments with synthetic and real-world datasets, we\ndemonstrate that traditional uncertainty estimation methods, such as ensembles\nand mean-variance estimation, can already substantially reduce the average\nprediction error and increase counterfactual truthfulness, especially for\nout-of-distribution settings. Our results highlight the importance and\npotential impact of incorporating uncertainty estimation into explainability\nmethods, especially considering the relatively high effectiveness of low-effort\ninterventions like model ensembles.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 5 figures, 4 tabels, accepted at the 3rd xAI World\n  Conference",
    "pdf_url": "http://arxiv.org/pdf/2504.02606v1",
    "published_date": "2025-04-03 14:07:30 UTC",
    "updated_date": "2025-04-03 14:07:30 UTC"
  },
  {
    "arxiv_id": "2504.02605v1",
    "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
    "authors": [
      "Daoguang Zan",
      "Zhirong Huang",
      "Wei Liu",
      "Hanwu Chen",
      "Linhao Zhang",
      "Shulin Xin",
      "Lu Chen",
      "Qi Liu",
      "Xiaojian Zhong",
      "Aoyan Li",
      "Siyao Liu",
      "Yongsheng Xiao",
      "Liangqiang Chen",
      "Yuyu Zhang",
      "Jing Su",
      "Tianyu Liu",
      "Rui Long",
      "Kai Shen",
      "Liang Xiang"
    ],
    "abstract": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02605v1",
    "published_date": "2025-04-03 14:06:17 UTC",
    "updated_date": "2025-04-03 14:06:17 UTC"
  },
  {
    "arxiv_id": "2504.02589v1",
    "title": "Knowledge Graph Completion with Mixed Geometry Tensor Factorization",
    "authors": [
      "Viacheslav Yusupov",
      "Maxim Rakhuba",
      "Evgeny Frolov"
    ],
    "abstract": "In this paper, we propose a new geometric approach for knowledge graph\ncompletion via low rank tensor approximation. We augment a pretrained and\nwell-established Euclidean model based on a Tucker tensor decomposition with a\nnovel hyperbolic interaction term. This correction enables more nuanced\ncapturing of distributional properties in data better aligned with real-world\nknowledge graphs. By combining two geometries together, our approach improves\nexpressivity of the resulting model achieving new state-of-the-art link\nprediction accuracy with a significantly lower number of parameters compared to\nthe previous Euclidean and hyperbolic models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AISTATS 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.02589v1",
    "published_date": "2025-04-03 13:54:43 UTC",
    "updated_date": "2025-04-03 13:54:43 UTC"
  },
  {
    "arxiv_id": "2504.02586v1",
    "title": "Deep learning for music generation. Four approaches and their comparative evaluation",
    "authors": [
      "Razvan Paroiu",
      "Stefan Trausan-Matu"
    ],
    "abstract": "This paper introduces four different artificial intelligence algorithms for\nmusic generation and aims to compare these methods not only based on the\naesthetic quality of the generated music but also on their suitability for\nspecific applications. The first set of melodies is produced by a slightly\nmodified visual transformer neural network that is used as a language model.\nThe second set of melodies is generated by combining chat sonification with a\nclassic transformer neural network (the same method of music generation is\npresented in a previous research), the third set of melodies is generated by\ncombining the Schillinger rhythm theory together with a classic transformer\nneural network, and the fourth set of melodies is generated using GPT3\ntransformer provided by OpenAI. A comparative analysis is performed on the\nmelodies generated by these approaches and the results indicate that\nsignificant differences can be observed between them and regarding the\naesthetic value of them, GPT3 produced the most pleasing melodies, and the\nnewly introduced Schillinger method proved to generate better sounding music\nthan previous sonification methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02586v1",
    "published_date": "2025-04-03 13:51:07 UTC",
    "updated_date": "2025-04-03 13:51:07 UTC"
  },
  {
    "arxiv_id": "2504.02577v1",
    "title": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning",
    "authors": [
      "Erik Arakelyan"
    ],
    "abstract": "The recent advancements in Deep Learning models and techniques have led to\nsignificant strides in performance across diverse tasks and modalities.\nHowever, while the overall capabilities of models show promising growth, our\nunderstanding of their internal reasoning processes remains limited,\nparticularly concerning systematic inconsistencies or errors patterns of\nlogical or inferential flaws. These inconsistencies may manifest as\ncontradictory outputs, failure to generalize across similar tasks, or erroneous\nconclusions in specific contexts. Even detecting and measuring such reasoning\ndiscrepancies is challenging, as they may arise from opaque internal\nprocedures, biases and imbalances in training data, or the inherent complexity\nof the task. Without effective methods to detect, measure, and mitigate these\nerrors, there is a risk of deploying models that are biased, exploitable, or\nlogically unreliable. This thesis aims to address these issues by producing\nnovel methods for deep learning models that reason over knowledge graphs,\nnatural language, and images. The thesis contributes two techniques for\ndetecting and quantifying predictive inconsistencies originating from opaque\ninternal procedures in natural language and image processing models. To\nmitigate inconsistencies from biases in training data, this thesis presents a\ndata efficient sampling method to improve fairness and performance and a\nsynthetic dataset generation approach in low resource scenarios. Finally, the\nthesis offers two techniques to optimize the models for complex reasoning\ntasks. These methods enhance model performance while allowing for more faithful\nand interpretable exploration and exploitation during inference. Critically,\nthis thesis provides a comprehensive framework to improve the robustness,\nfairness, and interpretability of deep learning models across diverse tasks and\nmodalities.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2504.02577v1",
    "published_date": "2025-04-03 13:40:55 UTC",
    "updated_date": "2025-04-03 13:40:55 UTC"
  },
  {
    "arxiv_id": "2504.02558v1",
    "title": "Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results",
    "authors": [
      "Andrei Dumitriu",
      "Florin Tatui",
      "Florin Miron",
      "Radu Tudor Ionescu",
      "Radu Timofte"
    ],
    "abstract": "Rip currents are the leading cause of fatal accidents and injuries on many\nbeaches worldwide, emphasizing the importance of automatically detecting these\nhazardous surface water currents. In this paper, we address a novel task: rip\ncurrent instance segmentation. We introduce a comprehensive dataset containing\n$2,466$ images with newly created polygonal annotations for instance\nsegmentation, used for training and validation. Additionally, we present a\nnovel dataset comprising $17$ drone videos (comprising about $24K$ frames)\ncaptured at $30 FPS$, annotated with both polygons for instance segmentation\nand bounding boxes for object detection, employed for testing purposes. We\ntrain various versions of YOLOv8 for instance segmentation on static images and\nassess their performance on the test dataset (videos). The best results were\nachieved by the YOLOv8-nano model (runnable on a portable device), with an\nmAP50 of $88.94%$ on the validation dataset and $81.21%$ macro average on the\ntest dataset. The results provide a baseline for future research in rip current\nsegmentation. Our work contributes to the existing literature by introducing a\ndetailed, annotated dataset, and training a deep learning model for instance\nsegmentation of rip currents. The code, training details and the annotated\ndataset are made publicly available at https://github.com/Irikos/rip_currents.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.0; I.4.9"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2023 NTIRE Workshop",
    "pdf_url": "http://arxiv.org/pdf/2504.02558v1",
    "published_date": "2025-04-03 13:14:16 UTC",
    "updated_date": "2025-04-03 13:14:16 UTC"
  },
  {
    "arxiv_id": "2504.02546v1",
    "title": "GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning",
    "authors": [
      "Xiangxiang Chu",
      "Hailang Huang",
      "Xiao Zhang",
      "Fei Wei",
      "Yong Wang"
    ],
    "abstract": "Reinforcement Learning (RL) can directly enhance the reasoning capabilities\nof large language models without extensive reliance on Supervised Fine-Tuning\n(SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism\nand propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike\nconventional methods, GPG directly optimize the original RL objective, thus\nobviating the need for surrogate loss functions. As illustrated in our paper,\nby eliminating both the critic and reference models, and avoiding KL divergence\nconstraints, our approach significantly simplifies the training process when\ncompared to Group Relative Policy Optimization (GRPO). Our approach achieves\nsuperior performance without relying on auxiliary techniques or adjustments.\nExtensive experiments demonstrate that our method not only reduces\ncomputational costs but also consistently outperforms GRPO across various\nunimodal and multimodal tasks. Our code is available at\nhttps://github.com/AMAP-ML/GPG.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02546v1",
    "published_date": "2025-04-03 12:53:41 UTC",
    "updated_date": "2025-04-03 12:53:41 UTC"
  },
  {
    "arxiv_id": "2504.02544v1",
    "title": "Fourier Sliced-Wasserstein Embedding for Multisets and Measures",
    "authors": [
      "Tal Amir",
      "Nadav Dym"
    ],
    "abstract": "We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to\nembed multisets and measures over $\\mathbb{R}^d$ into Euclidean space.\n  Our proposed embedding approximately preserves the sliced Wasserstein\ndistance on distributions, thereby yielding geometrically meaningful\nrepresentations that better capture the structure of the input. Moreover, it is\ninjective on measures and bi-Lipschitz on multisets - a significant advantage\nover prevalent methods based on sum- or max-pooling, which are provably not\nbi-Lipschitz, and, in many cases, not even injective. The required output\ndimension for these guarantees is near-optimal: roughly $2 N d$, where $N$ is\nthe maximal input multiset size.\n  Furthermore, we prove that it is impossible to embed distributions over\n$\\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric\nproperties of our embedding are, in a sense, the best possible.\n  Through numerical experiments, we demonstrate that our method yields superior\nmultiset representations that improve performance in practical learning tasks.\nSpecifically, we show that (a) a simple combination of the FSW embedding with\nan MLP achieves state-of-the-art performance in learning the (non-sliced)\nWasserstein distance; and (b) replacing max-pooling with the FSW embedding\nmakes PointNet significantly more robust to parameter reduction, with only\nminor performance degradation even after a 40-fold reduction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 camera-ready. arXiv admin note: substantial text overlap\n  with arXiv:2405.16519",
    "pdf_url": "http://arxiv.org/pdf/2504.02544v1",
    "published_date": "2025-04-03 12:51:40 UTC",
    "updated_date": "2025-04-03 12:51:40 UTC"
  },
  {
    "arxiv_id": "2504.02526v1",
    "title": "Improving User Experience with FAICO: Towards a Framework for AI Communication in Human-AI Co-Creativity",
    "authors": [
      "Jeba Rezwana",
      "Corey Ford"
    ],
    "abstract": "How AI communicates with humans is crucial for effective human-AI\nco-creation. However, many existing co-creative AI tools cannot communicate\neffectively, limiting their potential as collaborators. This paper introduces\nour initial design of a Framework for designing AI Communication (FAICO) for\nco-creative AI based on a systematic review of 107 full-length papers. FAICO\npresents key aspects of AI communication and their impacts on user experience\nto guide the design of effective AI communication. We then show actionable ways\nto translate our framework into two practical tools: design cards for designers\nand a configuration tool for users. The design cards enable designers to\nconsider AI communication strategies that cater to a diverse range of users in\nco-creative contexts, while the configuration tool empowers users to customize\nAI communication based on their needs and creative workflows. This paper\ncontributes new insights within the literature on human-AI co-creativity and\nHuman-Computer Interaction, focusing on designing AI communication to enhance\nuser experience.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02526v1",
    "published_date": "2025-04-03 12:29:53 UTC",
    "updated_date": "2025-04-03 12:29:53 UTC"
  },
  {
    "arxiv_id": "2504.02512v1",
    "title": "Towards Generalizing Temporal Action Segmentation to Unseen Views",
    "authors": [
      "Emad Bahrami",
      "Olga Zatsarynna",
      "Gianpiero Francesca",
      "Juergen Gall"
    ],
    "abstract": "While there has been substantial progress in temporal action segmentation,\nthe challenge to generalize to unseen views remains unaddressed. Hence, we\ndefine a protocol for unseen view action segmentation where camera views for\nevaluating the model are unavailable during training. This includes changing\nfrom top-frontal views to a side view or even more challenging from exocentric\nto egocentric views. Furthermore, we present an approach for temporal action\nsegmentation that tackles this challenge. Our approach leverages a shared\nrepresentation at both the sequence and segment levels to reduce the impact of\nview differences during training. We achieve this by introducing a sequence\nloss and an action loss, which together facilitate consistent video and action\nrepresentations across different views. The evaluation on the Assembly101,\nIkeaASM, and EgoExoLearn datasets demonstrate significant improvements, with a\n12.8% increase in F1@50 for unseen exocentric views and a substantial 54%\nimprovement for unseen egocentric views.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02512v1",
    "published_date": "2025-04-03 11:53:59 UTC",
    "updated_date": "2025-04-03 11:53:59 UTC"
  },
  {
    "arxiv_id": "2504.02509v1",
    "title": "A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D Printing Work Orders",
    "authors": [
      "Yuhao Liu",
      "Maolin Yang",
      "Pingyu Jiang"
    ],
    "abstract": "With the rapid development of 3D printing, the demand for personalized and\ncustomized production on the manufacturing line is steadily increasing.\nEfficient merging of printing workpieces can significantly enhance the\nprocessing efficiency of the production line. Addressing the challenge, a Large\nLanguage Model (LLM)-driven method is established in this paper for the\nautonomous merging of 3D printing work orders, integrated with a\nmemory-augmented learning strategy. In industrial scenarios, both device and\norder features are modeled into LLM-readable natural language prompt templates,\nand develop an order-device matching tool along with a merging interference\nchecking module. By incorporating a self-memory learning strategy, an\nintelligent agent for autonomous order merging is constructed, resulting in\nimproved accuracy and precision in order allocation. The proposed method\neffectively leverages the strengths of LLMs in industrial applications while\nreducing hallucination.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02509v1",
    "published_date": "2025-04-03 11:50:29 UTC",
    "updated_date": "2025-04-03 11:50:29 UTC"
  },
  {
    "arxiv_id": "2504.02495v1",
    "title": "Inference-Time Scaling for Generalist Reward Modeling",
    "authors": [
      "Zijun Liu",
      "Peiyi Wang",
      "Runxin Xu",
      "Shirong Ma",
      "Chong Ruan",
      "Peng Li",
      "Yang Liu",
      "Yu Wu"
    ],
    "abstract": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint, under review. 42 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.02495v1",
    "published_date": "2025-04-03 11:19:49 UTC",
    "updated_date": "2025-04-03 11:19:49 UTC"
  },
  {
    "arxiv_id": "2504.02492v1",
    "title": "Industrial Internet Robot Collaboration System and Edge Computing Optimization",
    "authors": [
      "Qian Zuo",
      "Dajun Tao",
      "Tian Qi",
      "Jieyi Xie",
      "Zijie Zhou",
      "Zhen Tian",
      "Yu Mingyu"
    ],
    "abstract": "In a complex environment, for a mobile robot to safely and collision - free\navoid all obstacles, it poses high requirements for its intelligence level.\nGiven that the information such as the position and geometric characteristics\nof obstacles is random, the control parameters of the robot, such as velocity\nand angular velocity, are also prone to random deviations. To address this\nissue in the framework of the Industrial Internet Robot Collaboration System,\nthis paper proposes a global path control scheme for mobile robots based on\ndeep learning. First of all, the dynamic equation of the mobile robot is\nestablished. According to the linear velocity and angular velocity of the\nmobile robot, its motion behaviors are divided into obstacle - avoidance\nbehavior, target - turning behavior, and target approaching behavior.\nSubsequently, the neural network method in deep learning is used to build a\nglobal path planning model for the robot. On this basis, a fuzzy controller is\ndesigned with the help of a fuzzy control algorithm to correct the deviations\nthat occur during path planning, thereby achieving optimized control of the\nrobot's global path. In addition, considering edge computing optimization, the\nproposed model can process local data at the edge device, reducing the\ncommunication burden between the robot and the central server, and improving\nthe real time performance of path planning. The experimental results show that\nfor the mobile robot controlled by the research method in this paper, the\ndeviation distance of the path angle is within 5 cm, the deviation convergence\ncan be completed within 10 ms, and the planned path is shorter. This indicates\nthat the proposed scheme can effectively improve the global path planning\nability of mobile robots in the industrial Internet environment and promote the\ncollaborative operation of robots through edge computing optimization.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02492v1",
    "published_date": "2025-04-03 11:15:10 UTC",
    "updated_date": "2025-04-03 11:15:10 UTC"
  },
  {
    "arxiv_id": "2504.02489v1",
    "title": "The Self-Learning Agent with a Progressive Neural Network Integrated Transformer",
    "authors": [
      "Ajay Sivakumar",
      "Shalini",
      "Vasantha Raj",
      "Sebastian Sylvester"
    ],
    "abstract": "This paper introduces a self-learning agent that integrates LLaMA 3.2 with a\nProgressive Neural Network (PNN) for continual learning in conversational AI\nand code generation. The framework dynamically collects data, fine-tunes tasks\nwith minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA\noptimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances\nknowledge retention. Experimental results demonstrate improved adaptability and\nmemory stability, positioning this approach as a scalable step toward\nArtificial General Intelligence (AGI).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 2 figures, focuses on continual learning with PNN and LLaMA.\n  Experiments demonstrate scalability and lifelong learning capabilities",
    "pdf_url": "http://arxiv.org/pdf/2504.02489v1",
    "published_date": "2025-04-03 11:13:31 UTC",
    "updated_date": "2025-04-03 11:13:31 UTC"
  },
  {
    "arxiv_id": "2504.02486v1",
    "title": "We Need Improved Data Curation and Attribution in AI for Scientific Discovery",
    "authors": [
      "Mara Graziani",
      "Antonio Foncubierta",
      "Dimitrios Christofidellis",
      "Irina Espejo-Morales",
      "Malina Molnar",
      "Marvin Alberts",
      "Matteo Manica",
      "Jannis Born"
    ],
    "abstract": "As the interplay between human-generated and synthetic data evolves, new\nchallenges arise in scientific discovery concerning the integrity of the data\nand the stability of the models. In this work, we examine the role of synthetic\ndata as opposed to that of real experimental data for scientific research. Our\nanalyses indicate that nearly three-quarters of experimental datasets available\non open-access platforms have relatively low adoption rates, opening new\nopportunities to enhance their discoverability and usability by automated\nmethods. Additionally, we observe an increasing difficulty in distinguishing\nsynthetic from real experimental data. We propose supplementing ongoing efforts\nin automating synthetic data detection by increasing the focus on watermarking\nreal experimental data, thereby strengthening data traceability and integrity.\nOur estimates suggest that watermarking even less than half of the real world\ndata generated annually could help sustain model robustness, while promoting a\nbalanced integration of synthetic and human-generated content.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02486v1",
    "published_date": "2025-04-03 11:07:52 UTC",
    "updated_date": "2025-04-03 11:07:52 UTC"
  },
  {
    "arxiv_id": "2504.02480v1",
    "title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging",
    "authors": [
      "Kyungmin Choi",
      "JaKeoung Koo",
      "Stephen McLaughlin",
      "Abderrahim Halimi"
    ],
    "abstract": "Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02480v1",
    "published_date": "2025-04-03 10:57:26 UTC",
    "updated_date": "2025-04-03 10:57:26 UTC"
  },
  {
    "arxiv_id": "2504.02479v1",
    "title": "Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets",
    "authors": [
      "Stefano Covone",
      "Italo Napolitano",
      "Francesco De Lellis",
      "Mario di Bernardo"
    ],
    "abstract": "We propose a decentralized reinforcement learning solution for multi-agent\nshepherding of non-cohesive targets using policy-gradient methods. Our\narchitecture integrates target-selection with target-driving through Proximal\nPolicy Optimization, overcoming discrete-action constraints of previous Deep\nQ-Network approaches and enabling smoother agent trajectories. This model-free\nframework effectively solves the shepherding problem without prior dynamics\nknowledge. Experiments demonstrate our method's effectiveness and scalability\nwith increased target numbers and limited sensing capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02479v1",
    "published_date": "2025-04-03 10:56:57 UTC",
    "updated_date": "2025-04-03 10:56:57 UTC"
  },
  {
    "arxiv_id": "2504.02467v1",
    "title": "BOOST: Bootstrapping Strategy-Driven Reasoning Programs for Program-Guided Fact-Checking",
    "authors": [
      "Qisheng Hu",
      "Quanyu Long",
      "Wenya Wang"
    ],
    "abstract": "Program-guided reasoning has shown promise in complex claim fact-checking by\ndecomposing claims into function calls and executing reasoning programs.\nHowever, prior work primarily relies on few-shot in-context learning (ICL) with\nad-hoc demonstrations, which limit program diversity and require manual design\nwith substantial domain knowledge. Fundamentally, the underlying principles of\neffective reasoning program generation still remain underexplored, making it\nchallenging to construct effective demonstrations. To address this, we propose\nBOOST, a bootstrapping-based framework for few-shot reasoning program\ngeneration. BOOST explicitly integrates claim decomposition and\ninformation-gathering strategies as structural guidance for program generation,\niteratively refining bootstrapped demonstrations in a strategy-driven and\ndata-centric manner without human intervention. This enables a seamless\ntransition from zero-shot to few-shot strategic program-guided learning,\nenhancing interpretability and effectiveness. Experimental results show that\nBOOST outperforms prior few-shot baselines in both zero-shot and few-shot\nsettings for complex claim verification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02467v1",
    "published_date": "2025-04-03 10:38:45 UTC",
    "updated_date": "2025-04-03 10:38:45 UTC"
  },
  {
    "arxiv_id": "2504.02463v1",
    "title": "Evaluating AI Recruitment Sourcing Tools by Human Preference",
    "authors": [
      "Vladimir Slaykovskiy",
      "Maksim Zvegintsev",
      "Yury Sakhonchyk",
      "Hrachik Ajamian"
    ],
    "abstract": "This study introduces a benchmarking methodology designed to evaluate the\nperformance of AI-driven recruitment sourcing tools. We created and utilized a\ndataset to perform a comparative analysis of search results generated by\nleading AI-based solutions, LinkedIn Recruiter, and our proprietary system,\nPearch.ai. Human experts assessed the relevance of the returned candidates, and\nan Elo rating system was applied to quantitatively measure each tool's\ncomparative performance. Our findings indicate that AI-driven recruitment\nsourcing tools consistently outperform LinkedIn Recruiter in candidate\nrelevance, with Pearch.ai achieving the highest performance scores.\nFurthermore, we found a strong alignment between AI-based evaluations and human\njudgments, highlighting the potential for advanced AI technologies to\nsubstantially enhance talent acquisition effectiveness. Code and supporting\ndata are publicly available at\nhttps://github.com/vslaykovsky/ai-sourcing-benchmark",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02463v1",
    "published_date": "2025-04-03 10:33:43 UTC",
    "updated_date": "2025-04-03 10:33:43 UTC"
  },
  {
    "arxiv_id": "2504.02464v1",
    "title": "CornerPoint3D: Look at the Nearest Corner Instead of the Center",
    "authors": [
      "Ruixiao Zhang",
      "Runwei Guan",
      "Xiangyu Chen",
      "Adam Prugel-Bennett",
      "Xiaohao Cai"
    ],
    "abstract": "3D object detection aims to predict object centers, dimensions, and rotations\nfrom LiDAR point clouds. Despite its simplicity, LiDAR captures only the near\nside of objects, making center-based detectors prone to poor localization\naccuracy in cross-domain tasks with varying point distributions. Meanwhile,\nexisting evaluation metrics designed for single-domain assessment also suffer\nfrom overfitting due to dataset-specific size variations. A key question\narises: Do we really need models to maintain excellent performance in the\nentire 3D bounding boxes after being applied across domains? Actually, one of\nour main focuses is on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsizes is much more difficult. To address these issues, we rethink cross-domain\n3D object detection from a practical perspective. We propose two new metrics\nthat evaluate a model's ability to detect objects' closer-surfaces to the LiDAR\nsensor. Additionally, we introduce EdgeHead, a refinement head that guides\nmodels to focus more on learnable closer surfaces, significantly improving\ncross-domain performance under both our new and traditional BEV/3D metrics.\nFurthermore, we argue that predicting the nearest corner rather than the object\ncenter enhances robustness. We propose a novel 3D object detector, coined as\nCornerPoint3D, which is built upon CenterPoint and uses heatmaps to supervise\nthe learning and detection of the nearest corner of each object. Our proposed\nmethods realize a balanced trade-off between the detection quality of entire\nbounding boxes and the locating accuracy of closer surfaces to the LiDAR\nsensor, outperforming the traditional center-based detector CenterPoint in\nmultiple cross-domain tasks and providing a more practically reasonable and\nrobust cross-domain 3D object detection solution.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2407.04061",
    "pdf_url": "http://arxiv.org/pdf/2504.02464v1",
    "published_date": "2025-04-03 10:33:43 UTC",
    "updated_date": "2025-04-03 10:33:43 UTC"
  },
  {
    "arxiv_id": "2504.02461v1",
    "title": "Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness",
    "authors": [
      "Juliett Suárez Ferreira",
      "Marija Slavkovik",
      "Jorge Casillas"
    ],
    "abstract": "Current fairness metrics and mitigation techniques provide tools for\npractitioners to asses how non-discriminatory Automatic Decision Making (ADM)\nsystems are. What if I, as an individual facing a decision taken by an ADM\nsystem, would like to know: Am I being treated fairly? We explore how to create\nthe affordance for users to be able to ask this question of ADM. In this paper,\nwe argue for the reification of fairness not only as a property of ADM, but\nalso as an epistemic right of an individual to acquire information about the\ndecisions that affect them and use that information to contest and seek\neffective redress against those decisions, in case they are proven to be\ndiscriminatory. We examine key concepts from existing research not only in\nalgorithmic fairness but also in explainable artificial intelligence,\naccountability, and contestability. Integrating notions from these domains, we\npropose a conceptual framework to ascertain fairness by combining different\ntools that empower the end-users of ADM systems. Our framework shifts the focus\nfrom technical solutions aimed at practitioners to mechanisms that enable\nindividuals to understand, challenge, and verify the fairness of decisions, and\nalso serves as a blueprint for organizations and policymakers, bridging the gap\nbetween technical requirements and practical, user-centered accountability.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA",
      "I.2; J.4"
    ],
    "primary_category": "cs.CY",
    "comment": "21 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02461v1",
    "published_date": "2025-04-03 10:28:19 UTC",
    "updated_date": "2025-04-03 10:28:19 UTC"
  },
  {
    "arxiv_id": "2504.02458v1",
    "title": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation",
    "authors": [
      "Liangbo Ning",
      "Wenqi Fan",
      "Qing Li"
    ],
    "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02458v1",
    "published_date": "2025-04-03 10:22:30 UTC",
    "updated_date": "2025-04-03 10:22:30 UTC"
  },
  {
    "arxiv_id": "2504.02450v1",
    "title": "CHARMS: Cognitive Hierarchical Agent with Reasoning and Motion Styles",
    "authors": [
      "Jingyi Wang",
      "Duanfeng Chu",
      "Zejian Deng",
      "Liping Lu"
    ],
    "abstract": "To address the current challenges of low intelligence and simplistic vehicle\nbehavior modeling in autonomous driving simulation scenarios, this paper\nproposes the Cognitive Hierarchical Agent with Reasoning and Motion Styles\n(CHARMS). The model can reason about the behavior of other vehicles like a\nhuman driver and respond with different decision-making styles, thereby\nimproving the intelligence and diversity of the surrounding vehicles in the\ndriving scenario. By introducing the Level-k behavioral game theory, the paper\nmodels the decision-making process of human drivers and employs deep\nreinforcement learning to train the models with diverse decision styles,\nsimulating different reasoning approaches and behavioral characteristics.\nBuilding on the Poisson cognitive hierarchy theory, this paper also presents a\nnovel driving scenario generation method. The method controls the proportion of\nvehicles with different driving styles in the scenario using Poisson and\nbinomial distributions, thus generating controllable and diverse driving\nenvironments. Experimental results demonstrate that CHARMS not only exhibits\nsuperior decision-making capabilities as ego vehicles, but also generates more\ncomplex and diverse driving scenarios as surrounding vehicles. We will release\ncode for CHARMS at https://github.com/WUTAD-Wjy/CHARMS.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02450v1",
    "published_date": "2025-04-03 10:15:19 UTC",
    "updated_date": "2025-04-03 10:15:19 UTC"
  },
  {
    "arxiv_id": "2504.02441v1",
    "title": "Cognitive Memory in Large Language Models",
    "authors": [
      "Lianlei Shan",
      "Shixian Luo",
      "Zezhou Zhu",
      "Yu Yuan",
      "Yong Wu"
    ],
    "abstract": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "37 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02441v1",
    "published_date": "2025-04-03 09:58:19 UTC",
    "updated_date": "2025-04-03 09:58:19 UTC"
  },
  {
    "arxiv_id": "2504.02438v1",
    "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation",
    "authors": [
      "Chuanqi Cheng",
      "Jian Guan",
      "Wei Wu",
      "Rui Yan"
    ],
    "abstract": "Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLaMP, a hierarchical video-language model that\nprocesses hour-long videos at ``mixed precision'' through two key mechanisms:\n(1) differential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLaMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLaMP's superior performance\nacross four video understanding benchmarks, particularly on long-form content.\nNotably, ViLaMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02438v1",
    "published_date": "2025-04-03 09:55:09 UTC",
    "updated_date": "2025-04-03 09:55:09 UTC"
  },
  {
    "arxiv_id": "2504.02430v1",
    "title": "How Artificial Intelligence Leads to Knowledge Why: An Inquiry Inspired by Aristotle's Posterior Analytics",
    "authors": [
      "Guus Eelink",
      "Kilian Rückschloß",
      "Felix Weitkämper"
    ],
    "abstract": "Bayesian networks and causal models provide frameworks for handling queries\nabout external interventions and counterfactuals, enabling tasks that go beyond\nwhat probability distributions alone can address. While these formalisms are\noften informally described as capturing causal knowledge, there is a lack of a\nformal theory characterizing the type of knowledge required to predict the\neffects of external interventions. This work introduces the theoretical\nframework of causal systems to clarify Aristotle's distinction between\nknowledge that and knowledge why within artificial intelligence. By\ninterpreting existing artificial intelligence technologies as causal systems,\nit investigates the corresponding types of knowledge. Furthermore, it argues\nthat predicting the effects of external interventions is feasible only with\nknowledge why, providing a more precise understanding of the knowledge\nnecessary for such tasks.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02430v1",
    "published_date": "2025-04-03 09:37:05 UTC",
    "updated_date": "2025-04-03 09:37:05 UTC"
  },
  {
    "arxiv_id": "2504.02426v1",
    "title": "Narrative Studio: Visual narrative exploration using LLMs and Monte Carlo Tree Search",
    "authors": [
      "Parsa Ghaffari",
      "Chris Hokamp"
    ],
    "abstract": "Interactive storytelling benefits from planning and exploring multiple 'what\nif' scenarios. Modern LLMs are useful tools for ideation and exploration, but\ncurrent chat-based user interfaces restrict users to a single linear flow. To\naddress this limitation, we propose Narrative Studio -- a novel in-browser\nnarrative exploration environment featuring a tree-like interface that allows\nbranching exploration from user-defined points in a story. Each branch is\nextended via iterative LLM inference guided by system and user-defined prompts.\nAdditionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand\npromising narrative paths based on user-specified criteria, enabling more\ndiverse and robust story development. We also allow users to enhance narrative\ncoherence by grounding the generated text in an entity graph that represents\nthe actors and environment of the story.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02426v1",
    "published_date": "2025-04-03 09:31:07 UTC",
    "updated_date": "2025-04-03 09:31:07 UTC"
  },
  {
    "arxiv_id": "2504.02417v1",
    "title": "Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering",
    "authors": [
      "Lili Liang",
      "Guanglu Sun"
    ],
    "abstract": "Video Question Answering (VideoQA) is an important research direction in the\nfield of artificial intelligence, enabling machines to understand video content\nand perform reasoning and answering based on natural language questions.\nAlthough methods based on static relationship reasoning have made certain\nprogress, there are still deficiencies in the accuracy of static relationship\nrecognition and representation, and they have not fully utilized the static\nrelationship information in videos for in-depth reasoning and analysis.\nTherefore, this paper proposes a reasoning method for intra-type and inter-type\nmessage passing based on static relationships. This method constructs a dual\ngraph for intra-type message passing reasoning and builds a heterogeneous graph\nbased on static relationships for inter-type message passing reasoning. The\nintra-type message passing reasoning model captures the neighborhood\ninformation of targets and relationships related to the question in the dual\ngraph, updating the dual graph to obtain intra-type clues for answering the\nquestion. The inter-type message passing reasoning model captures the\nneighborhood information of targets and relationships from different categories\nrelated to the question in the heterogeneous graph, updating the heterogeneous\ngraph to obtain inter-type clues for answering the question. Finally, the\nanswers are inferred by combining the intra-type and inter-type clues based on\nstatic relationships. Experimental results on the ANetQA and Next-QA datasets\ndemonstrate the effectiveness of this method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02417v1",
    "published_date": "2025-04-03 09:14:41 UTC",
    "updated_date": "2025-04-03 09:14:41 UTC"
  },
  {
    "arxiv_id": "2504.02408v1",
    "title": "Translation of Fetal Brain Ultrasound Images into Pseudo-MRI Images using Artificial Intelligence",
    "authors": [
      "Naomi Silverstein",
      "Efrat Leibowitz",
      "Ron Beloosesky",
      "Haim Azhari"
    ],
    "abstract": "Ultrasound is a widely accessible and cost-effective medical imaging tool\ncommonly used for prenatal evaluation of the fetal brain. However, it has\nlimitations, particularly in the third trimester, where the complexity of the\nfetal brain requires high image quality for extracting quantitative data. In\ncontrast, magnetic resonance imaging (MRI) offers superior image quality and\ntissue differentiation but is less available, expensive, and requires\ntime-consuming acquisition. Thus, transforming ultrasonic images into an\nMRI-mimicking display may be advantageous and allow better tissue anatomy\npresentation. To address this goal, we have examined the use of artificial\nintelligence, implementing a diffusion model renowned for generating\nhigh-quality images. The proposed method, termed \"Dual Diffusion Imposed\nCorrelation\" (DDIC), leverages a diffusion-based translation methodology,\nassuming a shared latent space between ultrasound and MRI domains. Model\ntraining was obtained utilizing the \"HC18\" dataset for ultrasound and the \"CRL\nfetal brain atlas\" along with the \"FeTA \" datasets for MRI. The generated\npseudo-MRI images provide notable improvements in visual discrimination of\nbrain tissue, especially in the lateral ventricles and the Sylvian fissure,\ncharacterized by enhanced contrast clarity. Improvement was demonstrated in\nMutual information, Peak signal-to-noise ratio, Fr\\'echet Inception Distance,\nand Contrast-to-noise ratio. Findings from these evaluations indicate\nstatistically significant superior performance of the DDIC compared to other\ntranslation methodologies. In addition, a Medical Opinion Test was obtained\nfrom 5 gynecologists. The results demonstrated display improvement in 81% of\nthe tested images. In conclusion, the presented pseudo-MRI images hold the\npotential for streamlining diagnosis and enhancing clinical outcomes through\nimproved representation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.02408v1",
    "published_date": "2025-04-03 08:59:33 UTC",
    "updated_date": "2025-04-03 08:59:33 UTC"
  },
  {
    "arxiv_id": "2504.02402v1",
    "title": "EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling",
    "authors": [
      "Hao Yin",
      "Shi Guo",
      "Xu Jia",
      "Xudong XU",
      "Lu Zhang",
      "Si Liu",
      "Dong Wang",
      "Huchuan Lu",
      "Tianfan Xue"
    ],
    "abstract": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Our project page: https://yyzq1.github.io/EvMic/",
    "pdf_url": "http://arxiv.org/pdf/2504.02402v1",
    "published_date": "2025-04-03 08:51:17 UTC",
    "updated_date": "2025-04-03 08:51:17 UTC"
  },
  {
    "arxiv_id": "2504.02388v1",
    "title": "Steiner Traveling Salesman Problem with Quantum Annealing",
    "authors": [
      "Alessia Ciacco",
      "Francesca Guerriero",
      "Eneko Osaba"
    ],
    "abstract": "The Steiner Traveling Salesman Problem (STSP) is a variant of the classical\nTraveling Salesman Problem. The STSP involves incorporating steiner nodes,\nwhich are extra nodes not originally part of the required visit set but that\ncan be added to the route to enhance the overall solution and minimize the\ntotal travel cost. Given the NP-hard nature of the STSP, we propose a quantum\napproach to address it. Specifically, we employ quantum annealing using\nD-Wave's hardware to explore its potential for solving this problem. To enhance\ncomputational feasibility, we develop a preprocessing method that effectively\nreduces the network size. Our experimental results demonstrate that this\nreduction technique significantly decreases the problem complexity, making the\nQuadratic Unconstrained Binary Optimization formulation, the standard input for\nquantum annealers, better suited for existing quantum hardware. Furthermore,\nthe results highlight the potential of quantum annealing as a promising and\ninnovative approach for solving the STSP.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "quant-ph",
    "comment": "7 pages, 1 figure, 6 tables. Paper submitted to The Genetic and\n  Evolutionary Computation Conference (GECCO 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.02388v1",
    "published_date": "2025-04-03 08:29:57 UTC",
    "updated_date": "2025-04-03 08:29:57 UTC"
  },
  {
    "arxiv_id": "2504.02382v1",
    "title": "Benchmark of Segmentation Techniques for Pelvic Fracture in CT and X-ray: Summary of the PENGWIN 2024 Challenge",
    "authors": [
      "Yudi Sang",
      "Yanzhen Liu",
      "Sutuke Yibulayimu",
      "Yunning Wang",
      "Benjamin D. Killeen",
      "Mingxu Liu",
      "Ping-Cheng Ku",
      "Ole Johannsen",
      "Karol Gotkowski",
      "Maximilian Zenk",
      "Klaus Maier-Hein",
      "Fabian Isensee",
      "Peiyan Yue",
      "Yi Wang",
      "Haidong Yu",
      "Zhaohong Pan",
      "Yutong He",
      "Xiaokun Liang",
      "Daiqi Liu",
      "Fuxin Fan",
      "Artur Jurgas",
      "Andrzej Skalski",
      "Yuxi Ma",
      "Jing Yang",
      "Szymon Płotka",
      "Rafał Litka",
      "Gang Zhu",
      "Yingchun Song",
      "Mathias Unberath",
      "Mehran Armand",
      "Dan Ruan",
      "S. Kevin Zhou",
      "Qiyong Cao",
      "Chunpeng Zhao",
      "Xinbao Wu",
      "Yu Wang"
    ],
    "abstract": "The segmentation of pelvic fracture fragments in CT and X-ray images is\ncrucial for trauma diagnosis, surgical planning, and intraoperative guidance.\nHowever, accurately and efficiently delineating the bone fragments remains a\nsignificant challenge due to complex anatomy and imaging limitations. The\nPENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance\nautomated fracture segmentation by benchmarking state-of-the-art algorithms on\nthese complex tasks. A diverse dataset of 150 CT scans was collected from\nmultiple clinical centers, and a large set of simulated X-ray images was\ngenerated using the DeepDRR method. Final submissions from 16 teams worldwide\nwere evaluated under a rigorous multi-metric testing scheme. The top-performing\nCT algorithm achieved an average fragment-wise intersection over union (IoU) of\n0.930, demonstrating satisfactory accuracy. However, in the X-ray task, the\nbest algorithm attained an IoU of 0.774, highlighting the greater challenges\nposed by overlapping anatomical structures. Beyond the quantitative evaluation,\nthe challenge revealed methodological diversity in algorithm design. Variations\nin instance representation, such as primary-secondary classification versus\nboundary-core separation, led to differing segmentation strategies. Despite\npromising results, the challenge also exposed inherent uncertainties in\nfragment definition, particularly in cases of incomplete fractures. These\nfindings suggest that interactive segmentation approaches, integrating human\ndecision-making with task-relevant information, may be essential for improving\nmodel reliability and clinical applicability.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "PENGWIN 2024 Challenge Report",
    "pdf_url": "http://arxiv.org/pdf/2504.02382v1",
    "published_date": "2025-04-03 08:19:36 UTC",
    "updated_date": "2025-04-03 08:19:36 UTC"
  },
  {
    "arxiv_id": "2504.02351v1",
    "title": "Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation",
    "authors": [
      "Chengxi Zeng",
      "Yuxuan Jiang",
      "Fan Zhang",
      "Alberto Gambaruto",
      "Tilo Burghardt"
    ],
    "abstract": "The deployment of foundation models for medical imaging has demonstrated\nconsiderable success. However, their training overheads associated with\ndownstream tasks remain substantial due to the size of the image encoders\nemployed, and the inference complexity is also significantly high. Although\nlightweight variants have been obtained for these foundation models, their\nperformance is constrained by their limited model capacity and suboptimal\ntraining strategies. In order to achieve an improved tradeoff between\ncomplexity and performance, we propose a new framework to improve the\nperformance of low complexity models via knowledge distillation from multiple\nlarge medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each\nspecializing in different vision tasks, with the goal to effectively bridge the\nperformance gap for medical image segmentation tasks. The agglomerated model\ndemonstrates superior generalization across 12 segmentation tasks, whereas\nspecialized models require explicit training for each task. Our approach\nachieved an average performance gain of 2\\% in Dice coefficient compared to\nsimple distillation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02351v1",
    "published_date": "2025-04-03 07:38:09 UTC",
    "updated_date": "2025-04-03 07:38:09 UTC"
  },
  {
    "arxiv_id": "2504.02317v1",
    "title": "Temporal Gaussian Copula For Clinical Multivariate Time Series Data Imputation",
    "authors": [
      "Ye Su",
      "Hezhe Qiao",
      "Di Wu",
      "Yuwen Chen",
      "Lin Chen"
    ],
    "abstract": "The imputation of the Multivariate time series (MTS) is particularly\nchallenging since the MTS typically contains irregular patterns of missing\nvalues due to various factors such as instrument failures, interference from\nirrelevant data, and privacy regulations. Existing statistical methods and deep\nlearning methods have shown promising results in time series imputation. In\nthis paper, we propose a Temporal Gaussian Copula Model (TGC) for three-order\nMTS imputation. The key idea is to leverage the Gaussian Copula to explore the\ncross-variable and temporal relationships based on the latent Gaussian\nrepresentation. Subsequently, we employ an Expectation-Maximization (EM)\nalgorithm to improve robustness in managing data with varying missing rates.\nComprehensive experiments were conducted on three real-world MTS datasets. The\nresults demonstrate that our TGC substantially outperforms the state-of-the-art\nimputation methods. Additionally, the TGC model exhibits stronger robustness to\nthe varying missing ratios in the test dataset. Our code is available at\nhttps://github.com/MVL-Lab/TGC-MTS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in BIBM2024",
    "pdf_url": "http://arxiv.org/pdf/2504.02317v1",
    "published_date": "2025-04-03 06:44:05 UTC",
    "updated_date": "2025-04-03 06:44:05 UTC"
  },
  {
    "arxiv_id": "2504.02316v1",
    "title": "ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation",
    "authors": [
      "Yuan Zhou",
      "Shilong Jin",
      "Litao Hua",
      "Wanjun Lv",
      "Haoran Duan",
      "Jungong Han"
    ],
    "abstract": "Recent advances in zero-shot text-to-3D generation have revolutionized 3D\ncontent creation by enabling direct synthesis from textual descriptions. While\nstate-of-the-art methods leverage 3D Gaussian Splatting with score distillation\nto enhance multi-view rendering through pre-trained text-to-image (T2I) models,\nthey suffer from inherent view biases in T2I priors. These biases lead to\ninconsistent 3D generation, particularly manifesting as the multi-face Janus\nproblem, where objects exhibit conflicting features across views. To address\nthis fundamental challenge, we propose ConsDreamer, a novel framework that\nmitigates view bias by refining both the conditional and unconditional terms in\nthe score distillation process: (1) a View Disentanglement Module (VDM) that\neliminates viewpoint biases in conditional prompts by decoupling irrelevant\nview components and injecting precise camera parameters; and (2) a\nsimilarity-based partial order loss that enforces geometric consistency in the\nunconditional term by aligning cosine similarities with azimuth relationships.\nExtensive experiments demonstrate that ConsDreamer effectively mitigates the\nmulti-face Janus problem in text-to-3D generation, outperforming existing\nmethods in both visual quality and consistency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 11 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.02316v1",
    "published_date": "2025-04-03 06:43:23 UTC",
    "updated_date": "2025-04-03 06:43:23 UTC"
  },
  {
    "arxiv_id": "2504.02312v1",
    "title": "OmniCam: Unified Multimodal Video Generation via Camera Control",
    "authors": [
      "Xiaoda Yang",
      "Jiayang Xu",
      "Kaixuan Luan",
      "Xinyu Zhan",
      "Hongshun Qiu",
      "Shijun Shi",
      "Hao Li",
      "Shuai Yang",
      "Li Zhang",
      "Checheng Yu",
      "Cewu Lu",
      "Lixin Yang"
    ],
    "abstract": "Camera control, which achieves diverse visual effects by changing camera\nposition and pose, has attracted widespread attention. However, existing\nmethods face challenges such as complex interaction and limited control\ncapabilities. To address these issues, we present OmniCam, a unified multimodal\ncamera control framework. Leveraging large language models and video diffusion\nmodels, OmniCam generates spatio-temporally consistent videos. It supports\nvarious combinations of input modalities: the user can provide text or video\nwith expected trajectory as camera path guidance, and image or video as content\nreference, enabling precise control over camera motion. To facilitate the\ntraining of OmniCam, we introduce the OmniTr dataset, which contains a large\ncollection of high-quality long-sequence trajectories, videos, and\ncorresponding descriptions. Experimental results demonstrate that our model\nachieves state-of-the-art performance in high-quality camera-controlled video\ngeneration across various metrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02312v1",
    "published_date": "2025-04-03 06:38:30 UTC",
    "updated_date": "2025-04-03 06:38:30 UTC"
  },
  {
    "arxiv_id": "2504.02293v1",
    "title": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla",
    "authors": [
      "Sharif Md. Abdullah",
      "Abhijit Paul",
      "Shebuti Rayana",
      "Ahmedul Kabir",
      "Zarif Masud"
    ],
    "abstract": "Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language\n(BdSL) remains a understudied domain. Specifically, there are no works on\nBangla text-to-gloss translation task. To address this gap, we begin by\naddressing the dataset problem. We take inspiration from grammatical rule based\ngloss generation used in Germany and American sign langauage (ASL) and adapt it\nfor BdSL. We also leverage LLM to generate synthetic data and use\nback-translation, text generation for data augmentation. With dataset prepared,\nwe started experimentation. We fine-tuned pretrained mBART-50 and\nmBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a\nnovel seq-to-seq model with multi-head attention. We observe significant high\nperformance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual\nmodel from Facebook. We then explored why we observe such high performance with\nmBART. We soon notice an interesting property of mBART -- it was trained on\nshuffled and masked text data. And as we know, gloss form has shuffling\nproperty. So we hypothesize that mBART is inherently good at text-to-gloss\ntasks. To find support against this hypothesis, we trained mBART-50 on\nPHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50\nfinetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark,\nfar outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 =\n55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on\nthe results, this study proposes a new paradigm for text-to-gloss task using\nmBART models. Additionally, our results show that BdSL text-to-gloss task can\ngreatly benefit from rule-based synthetic dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Initial Version",
    "pdf_url": "http://arxiv.org/pdf/2504.02293v1",
    "published_date": "2025-04-03 05:47:51 UTC",
    "updated_date": "2025-04-03 05:47:51 UTC"
  },
  {
    "arxiv_id": "2504.02285v1",
    "title": "Tree-based Models for Vertical Federated Learning: A Survey",
    "authors": [
      "Bingchen Qian",
      "Yuexiang Xie",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "Tree-based models have achieved great success in a wide range of real-world\napplications due to their effectiveness, robustness, and interpretability,\nwhich inspired people to apply them in vertical federated learning (VFL)\nscenarios in recent years. In this paper, we conduct a comprehensive study to\ngive an overall picture of applying tree-based models in VFL, from the\nperspective of their communication and computation protocols. We categorize\ntree-based models in VFL into two types, i.e., feature-gathering models and\nlabel-scattering models, and provide a detailed discussion regarding their\ncharacteristics, advantages, privacy protection mechanisms, and applications.\nThis study also focuses on the implementation of tree-based models in VFL,\nsummarizing several design principles for better satisfying various\nrequirements from both academic research and industrial deployment. We conduct\na series of experiments to provide empirical observations on the differences\nand advances of different types of tree-based models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACM Computing Surveys (CSUR)",
    "pdf_url": "http://arxiv.org/pdf/2504.02285v1",
    "published_date": "2025-04-03 05:16:09 UTC",
    "updated_date": "2025-04-03 05:16:09 UTC"
  },
  {
    "arxiv_id": "2504.02277v1",
    "title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation",
    "authors": [
      "Amit Rand",
      "Hadi Ibrahim"
    ],
    "abstract": "Medical imaging, particularly X-ray analysis, often involves detecting\nmultiple conditions simultaneously within a single scan, making multi-label\nclassification crucial for real-world clinical applications. We present the\nMedical X-ray Attention (MXA) block, a novel attention mechanism tailored\nspecifically to address the unique challenges of X-ray abnormality detection.\nThe MXA block enhances traditional Multi-Head Self Attention (MHSA) by\nintegrating a specialized module that efficiently captures both detailed local\ninformation and broader global context. To the best of our knowledge, this is\nthe first work to propose a task-specific attention mechanism for diagnosing\nchest X-rays, as well as to attempt multi-label classification using an\nEfficient Vision Transformer (EfficientViT). By embedding the MXA block within\nthe EfficientViT architecture and employing knowledge distillation, our\nproposed model significantly improves performance on the CheXpert dataset, a\nwidely used benchmark for multi-label chest X-ray abnormality detection. Our\napproach achieves an area under the curve (AUC) of 0.85, an absolute\nimprovement of 0.19 compared to our baseline model's AUC of 0.66, corresponding\nto a substantial approximate 233% relative improvement over random guessing\n(AUC = 0.5).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 4 figures, 5 tables. For supplementary material and code,\n  see https://github.com/Hadi-M-Ibrahim/Beyond-Conventional-Transformers/",
    "pdf_url": "http://arxiv.org/pdf/2504.02277v1",
    "published_date": "2025-04-03 04:55:42 UTC",
    "updated_date": "2025-04-03 04:55:42 UTC"
  },
  {
    "arxiv_id": "2504.02269v1",
    "title": "Engineering Artificial Intelligence: Framework, Challenges, and Future Direction",
    "authors": [
      "Jay Lee",
      "Hanqi Su",
      "Dai-Yan Ji",
      "Takanobu Minami"
    ],
    "abstract": "Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and nine future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02269v1",
    "published_date": "2025-04-03 04:30:10 UTC",
    "updated_date": "2025-04-03 04:30:10 UTC"
  },
  {
    "arxiv_id": "2504.02260v1",
    "title": "Implicit Neural Differential Model for Spatiotemporal Dynamics",
    "authors": [
      "Deepak Akhare",
      "Pan Du",
      "Tengfei Luo",
      "Jian-Xun Wang"
    ],
    "abstract": "Hybrid neural-physics modeling frameworks through differentiable programming\nhave emerged as powerful tools in scientific machine learning, enabling the\nintegration of known physics with data-driven learning to improve prediction\naccuracy and generalizability. However, most existing hybrid frameworks rely on\nexplicit recurrent formulations, which suffer from numerical instability and\nerror accumulation during long-horizon forecasting. In this work, we introduce\nIm-PiNDiff, a novel implicit physics-integrated neural differentiable solver\nfor stable and accurate modeling of spatiotemporal dynamics. Inspired by deep\nequilibrium models, Im-PiNDiff advances the state using implicit fixed-point\nlayers, enabling robust long-term simulation while remaining fully end-to-end\ndifferentiable. To enable scalable training, we introduce a hybrid gradient\npropagation strategy that integrates adjoint-state methods with reverse-mode\nautomatic differentiation. This approach eliminates the need to store\nintermediate solver states and decouples memory complexity from the number of\nsolver iterations, significantly reducing training overhead. We further\nincorporate checkpointing techniques to manage memory in long-horizon rollouts.\nNumerical experiments on various spatiotemporal PDE systems, including\nadvection-diffusion processes, Burgers' dynamics, and multi-physics chemical\nvapor infiltration processes, demonstrate that Im-PiNDiff achieves superior\npredictive performance, enhanced numerical stability, and substantial\nreductions in memory and runtime cost relative to explicit and naive implicit\nbaselines. This work provides a principled, efficient, and scalable framework\nfor hybrid neural-physics modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02260v1",
    "published_date": "2025-04-03 04:07:18 UTC",
    "updated_date": "2025-04-03 04:07:18 UTC"
  },
  {
    "arxiv_id": "2504.02254v1",
    "title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks",
    "authors": [
      "Seunghyun Yoo"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have not only showcased\nimpressive creative capabilities but also revealed emerging agentic behaviors\nthat exploit linguistic ambiguity in adversarial settings. In this study, we\ninvestigate how an LLM, acting as an autonomous agent, leverages semantic\nambiguity to generate deceptive puzzles that mislead and challenge human users.\nInspired by the popular puzzle game \"Connections\", we systematically compare\npuzzles produced through zero-shot prompting, role-injected adversarial\nprompts, and human-crafted examples, with an emphasis on understanding the\nunderlying agent decision-making processes. Employing computational analyses\nwith HateBERT to quantify semantic ambiguity, alongside subjective human\nevaluations, we demonstrate that explicit adversarial agent behaviors\nsignificantly heighten semantic ambiguity -- thereby increasing cognitive load\nand reducing fairness in puzzle solving. These findings provide critical\ninsights into the emergent agentic qualities of LLMs and underscore important\nethical considerations for evaluating and safely deploying autonomous language\nsystems in both educational technologies and entertainment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50, 68T05, 68U35"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2504.02254v1",
    "published_date": "2025-04-03 03:45:58 UTC",
    "updated_date": "2025-04-03 03:45:58 UTC"
  },
  {
    "arxiv_id": "2504.02252v1",
    "title": "Adapting World Models with Latent-State Dynamics Residuals",
    "authors": [
      "JB Lanier",
      "Kyungmin Kim",
      "Armin Karamzade",
      "Yifei Liu",
      "Ankita Sinha",
      "Kat He",
      "Davide Corsi",
      "Roy Fox"
    ],
    "abstract": "Simulation-to-reality reinforcement learning (RL) faces the critical\nchallenge of reconciling discrepancies between simulated and real-world\ndynamics, which can severely degrade agent performance. A promising approach\ninvolves learning corrections to simulator forward dynamics represented as a\nresidual error function, however this operation is impractical with\nhigh-dimensional states such as images. To overcome this, we propose ReDRAW, a\nlatent-state autoregressive world model pretrained in simulation and calibrated\nto target environments through residual corrections of latent-state dynamics\nrather than of explicit observed states. Using this adapted world model, ReDRAW\nenables RL agents to be optimized with imagined rollouts under corrected\ndynamics and then deployed in the real world. In multiple vision-based MuJoCo\ndomains and a physical robot visual lane-following task, ReDRAW effectively\nmodels changes to dynamics and avoids overfitting in low data regimes where\ntraditional transfer methods fail.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 11 figures. Project website at https://redraw.jblanier.net/",
    "pdf_url": "http://arxiv.org/pdf/2504.02252v1",
    "published_date": "2025-04-03 03:41:30 UTC",
    "updated_date": "2025-04-03 03:41:30 UTC"
  },
  {
    "arxiv_id": "2504.02234v1",
    "title": "LLM Social Simulations Are a Promising Research Method",
    "authors": [
      "Jacy Reese Anthis",
      "Ryan Liu",
      "Sean M. Richardson",
      "Austin C. Kozlowski",
      "Bernard Koch",
      "James Evans",
      "Erik Brynjolfsson",
      "Michael Bernstein"
    ],
    "abstract": "Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted these methods. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a literature\nsurvey of empirical comparisons between LLMs and human research subjects,\ncommentaries on the topic, and related work. We identify promising directions\nwith prompting, fine-tuning, and complementary methods. We believe that LLM\nsocial simulations can already be used for exploratory research, such as pilot\nexperiments for psychology, economics, sociology, and marketing. More\nwidespread use may soon be possible with rapidly advancing LLM capabilities,\nand researchers should prioritize developing conceptual models and evaluations\nthat can be iteratively deployed and refined at pace with ongoing AI advances.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02234v1",
    "published_date": "2025-04-03 03:01:26 UTC",
    "updated_date": "2025-04-03 03:01:26 UTC"
  },
  {
    "arxiv_id": "2504.02231v1",
    "title": "AC-LoRA: Auto Component LoRA for Personalized Artistic Style Image Generation",
    "authors": [
      "Zhipu Cui",
      "Andong Tian",
      "Zhi Ying",
      "Jialiang Lu"
    ],
    "abstract": "Personalized image generation allows users to preserve styles or subjects of\na provided small set of images for further image generation. With the\nadvancement in large text-to-image models, many techniques have been developed\nto efficiently fine-tune those models for personalization, such as Low Rank\nAdaptation (LoRA). However, LoRA-based methods often face the challenge of\nadjusting the rank parameter to achieve satisfactory results. To address this\nchallenge, AutoComponent-LoRA (AC-LoRA) is proposed, which is able to\nautomatically separate the signal component and noise component of the LoRA\nmatrices for fast and efficient personalized artistic style image generation.\nThis method is based on Singular Value Decomposition (SVD) and dynamic\nheuristics to update the hyperparameters during training. Superior performance\nover existing methods in overcoming model underfitting or overfitting problems\nis demonstrated. The results were validated using FID, CLIP, DINO, and\nImageReward, achieving an average of 9% improvement.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T05, 68U10",
      "I.2.6; I.4.0"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 4 figures, ICCGV 2025, SPIE",
    "pdf_url": "http://arxiv.org/pdf/2504.02231v1",
    "published_date": "2025-04-03 02:56:01 UTC",
    "updated_date": "2025-04-03 02:56:01 UTC"
  },
  {
    "arxiv_id": "2504.02227v1",
    "title": "VEGAS: Towards Visually Explainable and Grounded Artificial Social Intelligence",
    "authors": [
      "Hao Li",
      "Hao Fei",
      "Zechao Hu",
      "Zhengwei Yang",
      "Zheng Wang"
    ],
    "abstract": "Social Intelligence Queries (Social-IQ) serve as the primary multimodal\nbenchmark for evaluating a model's social intelligence level. While impressive\nmultiple-choice question(MCQ) accuracy is achieved by current solutions,\nincreasing evidence shows that they are largely, and in some cases entirely,\ndependent on language modality, overlooking visual context. Additionally, the\nclosed-set nature further prevents the exploration of whether and to what\nextent the reasoning path behind selection is correct. To address these\nlimitations, we propose the Visually Explainable and Grounded Artificial Social\nIntelligence (VEGAS) model. As a generative multimodal model, VEGAS leverages\nopen-ended answering to provide explainable responses, which enhances the\nclarity and evaluation of reasoning paths. To enable visually grounded\nanswering, we propose a novel sampling strategy to provide the model with more\nrelevant visual frames. We then enhance the model's interpretation of these\nframes through Generalist Instruction Fine-Tuning (GIFT), which aims to: i)\nlearn multimodal-language transformations for fundamental emotional social\ntraits, and ii) establish multimodal joint reasoning capabilities. Extensive\nexperiments, comprising modality ablation, open-ended assessments, and\nsupervised MCQ evaluations, consistently show that VEGAS effectively utilizes\nvisual information in reasoning to produce correct and also credible answers.\nWe expect this work to of fer a new perspective on Social-IQ and advance the\ndevelopment of human-like social AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 5 figures, AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.02227v1",
    "published_date": "2025-04-03 02:48:21 UTC",
    "updated_date": "2025-04-03 02:48:21 UTC"
  },
  {
    "arxiv_id": "2504.02221v1",
    "title": "Learning and Improving Backgammon Strategy",
    "authors": [
      "Gregory R. Galperin"
    ],
    "abstract": "A novel approach to learning is presented, combining features of on-line and\noff-line methods to achieve considerable performance in the task of learning a\nbackgammon value function in a process that exploits the processing power of\nparallel supercomputers. The off-line methods comprise a set of techniques for\nparallelizing neural network training and $TD(\\lambda)$ reinforcement learning;\nhere Monte-Carlo ``Rollouts'' are introduced as a massively parallel on-line\npolicy improvement technique which applies resources to the decision points\nencountered during the search of the game tree to further augment the learned\nvalue function estimate. A level of play roughly as good as, or possibly better\nthan, the current champion human and computer backgammon players has been\nachieved in a short period of learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accompanied by oral presentation by Gregory Galperin at the CBCL\n  Learning Day 1994",
    "pdf_url": "http://arxiv.org/pdf/2504.02221v1",
    "published_date": "2025-04-03 02:27:22 UTC",
    "updated_date": "2025-04-03 02:27:22 UTC"
  },
  {
    "arxiv_id": "2504.02211v1",
    "title": "FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention",
    "authors": [
      "Huangliang Dai",
      "Shixun Wu",
      "Hairui Zhao",
      "Jiajun Huang",
      "Zizhe Jian",
      "Yue Zhu",
      "Haiyang Hu",
      "Zizhong Chen"
    ],
    "abstract": "Transformer models leverage self-attention mechanisms to capture complex\ndependencies, demonstrating exceptional performance in various applications.\nHowever, the long-duration high-load computations required for model inference\nimpose stringent reliability demands on the computing platform, as soft errors\nthat occur during execution can significantly degrade model performance.\nExisting fault tolerance methods protect each operation separately using\ndecoupled kernels, incurring substantial computational and memory overhead. In\nthis paper, we propose a novel error-resilient framework for Transformer\nmodels, integrating end-to-end fault tolerant attention (EFTA) to improve\ninference reliability against soft errors. Our approach enables error detection\nand correction within a fully fused attention kernel, reducing redundant data\naccess and thereby mitigating memory faults. To further enhance error coverage\nand reduce overhead, we design a hybrid fault tolerance scheme tailored for the\nEFTA, introducing for the first time: 1) architecture-aware algorithm-based\nfault tolerance (ABFT) using tensor checksum, which minimizes inter-thread\ncommunication overhead on tensor cores during error detection; 2) selective\nneuron value restriction, which selectively applies adaptive fault tolerance\nconstraints to neuron values, balancing error coverage and overhead; 3) unified\nverification, reusing checksums to streamline multiple computation steps into a\nsingle verification process. Experimental results show that EFTA achieves up to\n7.56x speedup over traditional methods with an average fault tolerance overhead\nof 13.9%.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02211v1",
    "published_date": "2025-04-03 02:05:08 UTC",
    "updated_date": "2025-04-03 02:05:08 UTC"
  },
  {
    "arxiv_id": "2504.02199v1",
    "title": "ESC: Erasing Space Concept for Knowledge Deletion",
    "authors": [
      "Tae-Young Lee",
      "Sundong Park",
      "Minwoo Jeon",
      "Hyoseok Hwang",
      "Gyeong-Moon Park"
    ],
    "abstract": "As concerns regarding privacy in deep learning continue to grow, individuals\nare increasingly apprehensive about the potential exploitation of their\npersonal knowledge in trained models. Despite several research efforts to\naddress this, they often fail to consider the real-world demand from users for\ncomplete knowledge erasure. Furthermore, our investigation reveals that\nexisting methods have a risk of leaking personal knowledge through embedding\nfeatures. To address these issues, we introduce a novel concept of Knowledge\nDeletion (KD), an advanced task that considers both concerns, and provides an\nappropriate metric, named Knowledge Retention score (KR), for assessing\nknowledge retention in feature space. To achieve this, we propose a novel\ntraining-free erasing approach named Erasing Space Concept (ESC), which\nrestricts the important subspace for the forgetting knowledge by eliminating\nthe relevant activations in the feature. In addition, we suggest ESC with\nTraining (ESC-T), which uses a learnable mask to better balance the trade-off\nbetween forgetting and preserving knowledge in KD. Our extensive experiments on\nvarious datasets and models demonstrate that our proposed methods achieve the\nfastest and state-of-the-art performance. Notably, our methods are applicable\nto diverse forgetting scenarios, such as facial domain setting, demonstrating\nthe generalizability of our methods. The code is available at\nhttp://github.com/KU-VGI/ESC .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 14 figures, 18 tables, CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.02199v1",
    "published_date": "2025-04-03 00:53:09 UTC",
    "updated_date": "2025-04-03 00:53:09 UTC"
  },
  {
    "arxiv_id": "2504.02193v1",
    "title": "More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment",
    "authors": [
      "Yifan Wang",
      "Runjin Chen",
      "Bolian Li",
      "David Cho",
      "Yihe Deng",
      "Ruqi Zhang",
      "Tianlong Chen",
      "Zhangyang Wang",
      "Ananth Grama",
      "Junyuan Hong"
    ],
    "abstract": "Aligning large language models (LLMs) with human values is an increasingly\ncritical step in post-training. Direct Preference Optimization (DPO) has\nemerged as a simple, yet effective alternative to reinforcement learning from\nhuman feedback (RLHF). Synthetic preference data with its low cost and high\nquality enable effective alignment through single- or multi-model generated\npreference data. Our study reveals a striking, safety-specific phenomenon\nassociated with DPO alignment: Although multi-model generated data enhances\nperformance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by\nproviding diverse responses, it also tends to facilitate reward hacking during\ntraining. This can lead to a high attack success rate (ASR) when models\nencounter jailbreaking prompts. The issue is particularly pronounced when\nemploying stronger models like GPT-4o or larger models in the same family to\ngenerate chosen responses paired with target model self-generated rejected\nresponses, resulting in dramatically poorer safety outcomes. Furthermore, with\nrespect to safety, using solely self-generated responses (single-model\ngeneration) for both chosen and rejected pairs significantly outperforms\nconfigurations that incorporate responses from stronger models, whether used\ndirectly as chosen data or as part of a multi-model response pool. We\ndemonstrate that multi-model preference data exhibits high linear separability\nbetween chosen and rejected responses, which allows models to exploit\nsuperficial cues rather than internalizing robust safety constraints. Our\nexperiments, conducted on models from the Llama, Mistral, and Qwen families,\nconsistently validate these findings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02193v1",
    "published_date": "2025-04-03 00:36:40 UTC",
    "updated_date": "2025-04-03 00:36:40 UTC"
  }
]
{
  "date": "2024-08-25",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-25 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 35 篇论文，主要聚焦于 AI 和机器学习的创新应用，包括大型语言模型 (LLM) 在代码生成、图推理和医学诊断中的提升，以及教育数据分析和多模态模型的进展。其中，令人印象深刻的包括 CodeGraph 使用 LLM 增强图推理，以及 PAM 在医疗图像分割中的高效表现；知名学者如 Nadav Cohen 的线性神经网络理论论文也值得关注。\n\n下面，我将挑选并分组讨论最具话题性和影响力的论文，先从 LLM 和 AI 推理相关的内容入手，再聊医疗应用，最后快速掠过其他领域。每个条目会列出论文标题（中文 + 英文），并简要概述核心贡献和发现。\n\n### LLM 和 AI 推理领域的亮点\n- **CodeGraph: Enhancing Graph Reasoning of LLMs with Code** (中文：CodeGraph：使用代码增强 LLM 的图推理 / 英文：CodeGraph: Enhancing Graph Reasoning of LLMs with Code)  \n  这篇论文提出 CodeGraph 方法，通过将图问题转化为代码形式，结合 LLM 和程序解释器，提升图推理任务的准确性。在 GraphQA 数据集上，实验显示性能提升 1.3% 到 58.6%，特别在算术问题上更可靠，展示了 LLM 在图结构任务中的潜力。\n\n- **LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback** (中文：LLM 作为优秀的反馈提供者：使用自生成反馈引导的谎言检测推理 / 英文：LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback)  \n  作者使用 LLM 生成反馈来提升谎言检测性能，通过三阶段框架（建议、反馈收集、修改），在 Diplomacy 游戏中实现 39% 的 F1 分数提升，媲美监督学习方法，突显 LLM 在复杂对话推理中的优势。\n\n- **Path-Consistency: Prefix Enhancement for Efficient Inference in LLM** (中文：路径一致性：用于 LLM 高效推理的前缀增强 / 英文：Path-Consistency: Prefix Enhancement for Efficient Inference in LLM)  \n  这篇创新工作引入路径一致性方法，通过分析早期分支的置信度优化后续生成，减少冗余采样。在数学和代码生成任务中，推理延迟减少 7.8% 到 40.5%，同时保持或提升准确性，显著提高了 LLM 的推理效率。\n\n- **StockTime: A Time Series Specialized Large Language Model Architecture for Stock Price Prediction** (中文：StockTime：专为股票价格预测设计的时间序列 LLM 架构 / 英文：StockTime: A Time Series Specialized Large Language Model Architecture for Stock Price Prediction)  \n  论文设计 StockTime 架构，将股票价格视为连续 token，融合时间序列和文本数据，实现任意回溯期的预测。相比其他 LLM，它在准确性上更胜一筹，同时降低内存和运行成本，适用于金融预测。\n\n- **LogParser-LLM: Advancing Efficient Log Parsing with Large Language Model** (中文：LogParser-LLM：使用 LLM 提升日志解析效率 / 英文：LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models)  \n  该方法结合 LLM 和统计分析，实现无标注数据的日志解析，在 LogPub 基准上达到 90.6% 分组准确率，仅需少量 LLM 调用，高效处理大规模日志，适用于系统诊断和优化。\n\n### 医疗和教育应用的进展\n- **PAM: A Propagation-Based Model for Segmenting Any 3D Objects across Multi-Modal Medical Images** (中文：PAM：基于传播的模型，用于多模态医疗图像的 3D 对象分割 / 英文：PAM: A Propagation-Based Model for Segmenting Any 3D Objects across Multi-Modal Medical Images)  \n  PAM 模型使用 2D 提示（如边界框）生成 3D 医疗图像分割，结合 CNN 和 Transformer，提升泛化能力。在 44 个数据集上，Dice 相似系数提高 18.1%，并减少手动标注需求，是医疗图像分析的重大进步。\n\n- **FedGlu: A personalized federated learning-based glucose forecasting algorithm** (中文：FedGlu：基于个性化联邦学习的葡萄糖预测算法 / 英文：FedGlu: A personalized federated learning-based glucose forecasting algorithm)  \n  作者提出 FedGlu 和 HH 损失函数，提升糖尿病患者葡萄糖异常检测率 35%，并解决数据隐私问题。通过联邦学习框架，在 125 名患者中表现出色，适用于可穿戴设备。\n\n- **Time Series Analysis for Education: Methods, Applications, and Future Directions** (中文：教育中的时间序列分析：方法、应用和未来方向 / 英文：Time Series Analysis for Education: Methods, Applications, and Future Directions)  \n  这篇综述首次系统总结教育数据的时间序列方法，包括预测、分类和异常检测，讨论 LLM 在个性化学习中的潜力，提供详尽分类和未来趋势，适合教育研究者。\n\n### 其他领域的快速掠过\n其他论文多为特定应用或技术细节，这里仅简要提及那些有潜在影响的：\n- **Learning to Move Like Professional Counter-Strike Players** (中文：学习像专业 CS 玩家一样移动 / 英文：Learning to Move Like Professional Counter-Strike Players)：使用 Transformer 模型模拟游戏团队移动，性能优于传统算法，提升游戏 AI 的人类化水平。\n- **Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation** (中文：Geo-Llama：使用 LLM 生成受时空约束的人类移动轨迹 / 英文：Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation)：LLM 框架生成受约束的轨迹，适用于城市规划。\n- **Lecture Notes on Linear Neural Networks** (中文：线性神经网络讲义：深度学习中优化和泛化的故事 / 英文：Lecture Notes on Linear Neural Networks: A Tale of Optimization and Generalization in Deep Learning)：知名学者 Nadav Cohen 的讲义，探讨线性神经网络的理论基础，对理解深度学习优化有启发。\n- 其余如图像处理、代码生成和游戏 AI 的论文（如 AlphaViT、ConVis），虽有贡献（如高效的图神经网络或多模态融合），但影响力较小，故不展开讨论。\n\n总之，今天的 arXiv 论文突显了 LLM 在跨领域应用的潜力，同时强调了 AI 在医疗和教育的实际价值。感兴趣的读者可关注上述亮点论文，探索更多创新方向！（本快报基于摘要总结，完整内容请查阅 arXiv。）",
  "papers": [
    {
      "arxiv_id": "2408.13960v2",
      "title": "Time Series Analysis for Education: Methods, Applications, and Future Directions",
      "title_zh": "教育的时间序列分析：方法、应用和未来方向",
      "authors": [
        "Shengzhong Mao",
        "Chaoli Zhang",
        "Yichi Song",
        "Jindong Wang",
        "Xiao-Jun Zeng",
        "Zenglin Xu",
        "Qingsong Wen"
      ],
      "abstract": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.",
      "tldr_zh": "这篇论文对时间序列分析在教育领域的应用进行了全面综述，填补了相关研究总结的空白。论文首先分类了教育数据来源和类型，然后回顾了四种主要方法——forecasting、classification、clustering 和 anomaly detection——并说明了它们在教育场景中的具体应用，如处理复杂教育任务。最终，论文讨论了未来方向，包括 personalized learning analytics、multimodal data fusion，以及 large language models (LLMs) 在教育时间序列分析中的潜力，为数据驱动的教育决策提供了宝贵见解和研究机会。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 3 figures, 6 tables, project page: see\n  https://github.com/ai-for-edu/time-series-analysis-for-education",
      "pdf_url": "http://arxiv.org/pdf/2408.13960v2",
      "published_date": "2024-08-25 23:48:11 UTC",
      "updated_date": "2024-08-27 15:06:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:35:24.033917"
    },
    {
      "arxiv_id": "2408.13950v1",
      "title": "Bridging the Gap between Real-world and Synthetic Images for Testing Autonomous Driving Systems",
      "title_zh": "弥合现实世界图像与合成图像之间的差距，用于测试自动驾驶系统",
      "authors": [
        "Mohammad Hossein Amini",
        "Shiva Nejati"
      ],
      "abstract": "Deep Neural Networks (DNNs) for Autonomous Driving Systems (ADS) are\ntypically trained on real-world images and tested using synthetic simulator\nimages. This approach results in training and test datasets with dissimilar\ndistributions, which can potentially lead to erroneously decreased test\naccuracy. To address this issue, the literature suggests applying\ndomain-to-domain translators to test datasets to bring them closer to the\ntraining datasets. However, translating images used for testing may\nunpredictably affect the reliability, effectiveness and efficiency of the\ntesting process. Hence, this paper investigates the following questions in the\ncontext of ADS: Could translators reduce the effectiveness of images used for\nADS-DNN testing and their ability to reveal faults in ADS-DNNs? Can translators\nresult in excessive time overhead during simulation-based testing? To address\nthese questions, we consider three domain-to-domain translators: CycleGAN and\nneural style transfer, from the literature, and SAEVAE, our proposed\ntranslator. Our results for two critical ADS tasks -- lane keeping and object\ndetection -- indicate that translators significantly narrow the gap in ADS test\naccuracy caused by distribution dissimilarities between training and test data,\nwith SAEVAE outperforming the other two translators. We show that, based on the\nrecent diversity, coverage, and fault-revealing ability metrics for testing\ndeep-learning systems, translators do not compromise the diversity and the\ncoverage of test data, nor do they lead to revealing fewer faults in ADS-DNNs.\nFurther, among the translators considered, SAEVAE incurs a negligible overhead\nin simulation time and can be efficiently integrated into simulation-based\ntesting. Finally, we show that translators increase the correlation between\noffline and simulation-based testing results, which can help reduce the cost of\nsimulation-based testing.",
      "tldr_zh": "这篇论文探讨了在自主驾驶系统(ADS)测试中，桥接真实图像和合成图像分布差异的问题，以避免测试准确率下降。研究者评估了三种域到域翻译器：CycleGAN、神经风格转移和提出的SAEVAE，应用于车道保持和物体检测任务。结果表明，这些翻译器显著提高了测试准确率，同时不影响测试数据的多样性、覆盖率和故障揭示能力，其中SAEVAE表现出色，时间开销微不足道。最终，翻译器增强了离线测试和模拟测试的相关性，有助于降低模拟测试成本。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication by the International Conference on Automated\n  Software Engineering (ASE 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.13950v1",
      "published_date": "2024-08-25 22:07:41 UTC",
      "updated_date": "2024-08-25 22:07:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:35:37.031151"
    },
    {
      "arxiv_id": "2408.13934v1",
      "title": "Learning to Move Like Professional Counter-Strike Players",
      "title_zh": "翻译失败",
      "authors": [
        "David Durst",
        "Feng Xie",
        "Vishnu Sarukkai",
        "Brennan Shacklett",
        "Iuri Frosio",
        "Chen Tessler",
        "Joohwan Kim",
        "Carly Taylor",
        "Gilbert Bernstein",
        "Sanjiban Choudhury",
        "Pat Hanrahan",
        "Kayvon Fatahalian"
      ],
      "abstract": "In multiplayer, first-person shooter games like Counter-Strike: Global\nOffensive (CS:GO), coordinated movement is a critical component of high-level\nstrategic play. However, the complexity of team coordination and the variety of\nconditions present in popular game maps make it impractical to author\nhand-crafted movement policies for every scenario. We show that it is possible\nto take a data-driven approach to creating human-like movement controllers for\nCS:GO. We curate a team movement dataset comprising 123 hours of professional\ngame play traces, and use this dataset to train a transformer-based movement\nmodel that generates human-like team movement for all players in a \"Retakes\"\nround of the game. Importantly, the movement prediction model is efficient.\nPerforming inference for all players takes less than 0.5 ms per game step\n(amortized cost) on a single CPU core, making it plausible for use in\ncommercial games today. Human evaluators assess that our model behaves more\nlike humans than both commercially-available bots and procedural movement\ncontrollers scripted by experts (16% to 59% higher by TrueSkill rating of\n\"human-like\"). Using experiments involving in-game bot vs. bot self-play, we\ndemonstrate that our model performs simple forms of teamwork, makes fewer\ncommon movement mistakes, and yields movement distributions, player lifetimes,\nand kill locations similar to those observed in professional CS:GO match play.",
      "tldr_zh": "本研究针对 CS:GO 等第一人称射击游戏中团队协调移动的复杂性，提出了一种数据驱动方法，使用 123 小时专业游戏数据训练基于 Transformer 的移动模型，以生成人类般的团队移动策略。模型专注于“Retakes”回合，能够高效推理，每个游戏步的计算成本不到 0.5 ms，在单 CPU 核心上运行。实验结果显示，该模型在人类评估中比商业机器人和专家脚本更接近人类行为（TrueSkill 评分提高 16% 到 59%），并在 bot 自对战中展示简单团队合作、减少常见错误，以及与专业比赛相似的移动分布、玩家寿命和击杀位置。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.LG",
      "comment": "The project website is at https://davidbdurst.com/mlmove/",
      "pdf_url": "http://arxiv.org/pdf/2408.13934v1",
      "published_date": "2024-08-25 20:43:34 UTC",
      "updated_date": "2024-08-25 20:43:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:35:46.957288"
    },
    {
      "arxiv_id": "2408.13926v1",
      "title": "FedGlu: A personalized federated learning-based glucose forecasting algorithm for improved performance in glycemic excursion regions",
      "title_zh": "FedGlu：",
      "authors": [
        "Darpit Dave",
        "Kathan Vyas",
        "Jagadish Kumaran Jayagopal",
        "Alfredo Garcia",
        "Madhav Erraguntla",
        "Mark Lawley"
      ],
      "abstract": "Continuous glucose monitoring (CGM) devices provide real-time glucose\nmonitoring and timely alerts for glycemic excursions, improving glycemic\ncontrol among patients with diabetes. However, identifying rare events like\nhypoglycemia and hyperglycemia remain challenging due to their infrequency.\nMoreover, limited access to sensitive patient data hampers the development of\nrobust machine learning models. Our objective is to accurately predict glycemic\nexcursions while addressing data privacy concerns. To tackle excursion\nprediction, we propose a novel Hypo-Hyper (HH) loss function, which\nsignificantly improves performance in the glycemic excursion regions. The HH\nloss function demonstrates a 46% improvement over mean-squared error (MSE) loss\nacross 125 patients. To address privacy concerns, we propose FedGlu, a machine\nlearning model trained in a federated learning (FL) framework. FL allows\ncollaborative learning without sharing sensitive data by training models\nlocally and sharing only model parameters across other patients. FedGlu\nachieves a 35% superior glycemic excursion detection rate compared to local\nmodels. This improvement translates to enhanced performance in predicting both,\nhypoglycemia and hyperglycemia, for 105 out of 125 patients. These results\nunderscore the effectiveness of the proposed HH loss function in augmenting the\npredictive capabilities of glucose predictions. Moreover, implementing models\nwithin a federated learning framework not only ensures better predictive\ncapabilities but also safeguards sensitive data concurrently.",
      "tldr_zh": "本文提出 FedGlu，一种基于联邦学习 (FL) 的个性化葡萄糖预测算法，旨在准确预测糖尿病患者的血糖异常 (glycemic excursions) 如低血糖和高血糖，同时解决数据隐私问题。算法引入了新型 Hypo-Hyper (HH) 损失函数，与传统均方误差 (MSE) 损失相比，在血糖异常区域的性能提高了 46%。实验结果显示，FedGlu 比本地模型提升了 35% 的异常检测率，并在 105 名患者中显著改善了低血糖和高血糖的预测准确性。这些创新不仅增强了预测能力，还确保了敏感数据的安全保护。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13926v1",
      "published_date": "2024-08-25 19:51:27 UTC",
      "updated_date": "2024-08-25 19:51:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:36:01.574680"
    },
    {
      "arxiv_id": "2408.13918v4",
      "title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with Spatiotemporal Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Siyu Li",
        "Toan Tran",
        "Haowen Lin",
        "John Krumm",
        "Cyrus Shahabi",
        "Lingyi Zhao",
        "Khurram Shafique",
        "Li Xiong"
      ],
      "abstract": "Generating realistic human mobility data is essential for various application\ndomains, including transportation, urban planning, and epidemic control, as\nreal data is often inaccessible to researchers due to high costs and privacy\nconcerns. Existing deep generative models learn from real trajectories to\ngenerate synthetic ones. Despite the progress, most of them suffer from\ntraining stability issues and scale poorly with increasing data size. More\nimportantly, they often lack control mechanisms to guide the generated\ntrajectories under constraints such as enforcing specific visits. To address\nthese limitations, we formally define the controlled trajectory generation\nproblem for effectively handling multiple spatiotemporal constraints. We\nintroduce Geo-Llama, a novel LLM finetuning framework that can enforce multiple\nexplicit visit constraints while maintaining contextual coherence of the\ngenerated trajectories. In this approach, pre-trained LLMs are fine-tuned on\ntrajectory data with a visit-wise permutation strategy where each visit\ncorresponds to a specific time and location. This strategy enables the model to\ncapture spatiotemporal patterns regardless of visit orders while maintaining\nflexible and in-context constraint integration through prompts during\ngeneration. Extensive experiments on real-world and synthetic datasets validate\nthe effectiveness of Geo-Llama, demonstrating its versatility and robustness in\nhandling a broad range of constraints to generate more realistic trajectories\ncompared to existing methods.",
      "tldr_zh": "该论文针对生成人类移动轨迹数据面临的隐私和控制问题，正式定义了受控轨迹生成问题，以处理多个 spatiotemporal constraints。研究引入 Geo-Llama，一种基于 LLM finetuning 的框架，通过 visit-wise permutation 策略微调预训练 LLMs，使模型能捕获时空模式并在生成过程中灵活集成约束，确保轨迹的上下文连贯性。与现有方法相比，Geo-Llama 在真实和合成数据集上的实验证明了其有效性，提高了轨迹的真实性和鲁棒性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13918v4",
      "published_date": "2024-08-25 19:03:46 UTC",
      "updated_date": "2025-04-28 09:22:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:36:12.248246"
    },
    {
      "arxiv_id": "2408.13915v1",
      "title": "LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Tanushree Banerjee",
        "Richard Zhu",
        "Runzhe Yang",
        "Karthik Narasimhan"
      ],
      "abstract": "Large Language Models (LLMs) excel at generating human-like dialogues and\ncomprehending text. However, understanding the subtleties of complex exchanges\nin language remains a challenge. We propose a bootstrapping framework that\nleverages self-generated feedback to enhance LLM reasoning capabilities for lie\ndetection. The framework consists of three stages: suggestion, feedback\ncollection, and modification. In the suggestion stage, a cost-effective\nlanguage model generates initial predictions based on game state and dialogue.\nThe feedback-collection stage involves a language model providing feedback on\nthese predictions. In the modification stage, a more advanced language model\nrefines the initial predictions using the auto-generated feedback. We\ninvestigate the application of the proposed framework for detecting betrayal\nand deception in Diplomacy games, and compare it with feedback from\nprofessional human players. The LLM-generated feedback exhibits superior\nquality and significantly enhances the performance of the model. Our approach\nachieves a 39% improvement over the zero-shot baseline in lying-F1 without the\nneed for any training data, rivaling state-of-the-art supervised learning\nresults.",
      "tldr_zh": "这篇论文提出了一种引导框架（bootstrapping framework），利用大语言模型（LLMs）自我生成的反馈来提升其在谎言检测中的推理能力，而无需任何训练数据。框架包括三个阶段：suggestion 阶段使用成本有效的模型生成初始预测、feedback collection 阶段由语言模型提供反馈，以及 modification 阶段由高级模型根据反馈优化预测。该方法应用于 Diplomacy 游戏中的背叛和欺骗检测，与人类专业玩家反馈相比，LLM 生成的反馈质量更高，并实现了 lying-F1 得分较 zero-shot baseline 提升 39%，性能媲美最先进的监督学习结果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 18 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.13915v1",
      "published_date": "2024-08-25 18:47:55 UTC",
      "updated_date": "2024-08-25 18:47:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:36:24.226892"
    },
    {
      "arxiv_id": "2408.13906v1",
      "title": "ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yeji Park",
        "Deokyeong Lee",
        "Junsuk Choe",
        "Buru Chang"
      ],
      "abstract": "Hallucinations in Multimodal Large Language Models (MLLMs) where generated\nresponses fail to accurately reflect the given image pose a significant\nchallenge to their reliability. To address this, we introduce ConVis, a novel\ntraining-free contrastive decoding method. ConVis leverages a text-to-image\n(T2I) generation model to semantically reconstruct the given image from\nhallucinated captions. By comparing the contrasting probability distributions\nproduced by the original and reconstructed images, ConVis enables MLLMs to\ncapture visual contrastive signals that penalize hallucination generation.\nNotably, this method operates purely within the decoding process, eliminating\nthe need for additional data or model updates. Our extensive experiments on\nfive popular benchmarks demonstrate that ConVis effectively reduces\nhallucinations across various MLLMs, highlighting its potential to enhance\nmodel reliability.",
      "tldr_zh": "这篇论文针对 Multimodal Large Language Models (MLLMs) 中生成的响应未能准确反映图像的幻觉问题，提出了 ConVis，一种无需训练的对比解码方法。ConVis 利用 text-to-image (T2I) 生成模型从幻觉标题中语义重建图像，并通过比较原始图像和重建图像的对比概率分布，捕获视觉对比信号以惩罚幻觉生成。该方法仅在解码过程中操作，不需要额外数据或模型更新。在五个流行基准上的实验显示，ConVis 显著减少了各种 MLLMs 的幻觉，提升了模型的可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "First two authors contributed equally. Source code is available at\n  https://github.com/yejipark-m/ConVis",
      "pdf_url": "http://arxiv.org/pdf/2408.13906v1",
      "published_date": "2024-08-25 18:02:36 UTC",
      "updated_date": "2024-08-25 18:02:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:36:36.128696"
    },
    {
      "arxiv_id": "2408.16018v1",
      "title": "SPICED: Syntactical Bug and Trojan Pattern Identification in A/MS Circuits using LLM-Enhanced Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jayeeta Chaudhuri",
        "Dhruv Thapar",
        "Arjun Chaudhuri",
        "Farshad Firouzi",
        "Krishnendu Chakrabarty"
      ],
      "abstract": "Analog and mixed-signal (A/MS) integrated circuits (ICs) are crucial in\nmodern electronics, playing key roles in signal processing, amplification,\nsensing, and power management. Many IC companies outsource manufacturing to\nthird-party foundries, creating security risks such as stealthy analog Trojans.\nTraditional detection methods, including embedding circuit watermarks or\nconducting hardware-based monitoring, often impose significant area and power\noverheads, and may not effectively identify all types of Trojans. To address\nthese shortcomings, we propose SPICED, a Large Language Model (LLM)-based\nframework that operates within the software domain, eliminating the need for\nhardware modifications for Trojan detection and localization. This is the first\nwork using LLM-aided techniques for detecting and localizing syntactical bugs\nand analog Trojans in circuit netlists, requiring no explicit training and\nincurring zero area overhead. Our framework employs chain-of-thought reasoning\nand few-shot examples to teach anomaly detection rules to LLMs. With the\nproposed method, we achieve an average Trojan coverage of 93.32% and an average\ntrue positive rate of 93.4% in identifying Trojan-impacted nodes for the\nevaluated analog benchmark circuits. These experimental results validate the\neffectiveness of LLMs in detecting and locating both syntactical bugs and\nTrojans within analog netlists.",
      "tldr_zh": "本文提出 SPICED 框架，一种基于 Large Language Model (LLM) 的软件域方法，用于检测和定位 Analog and Mixed-Signal (A/MS) Circuits 中的 Syntactical Bug 和 Trojan Pattern，避免了传统方法的面积和功耗开销。SPICED 利用 Chain-of-Thought Reasoning 和 Few-Shot Examples 教导 LLMs 识别异常规则，无需显式训练或硬件修改。实验结果显示，在评估的模拟基准电路上，该框架实现了 93.32% 的平均 Trojan 覆盖率和 93.4% 的真阳性率，证明了其在安全风险识别方面的有效性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at PAINE'24",
      "pdf_url": "http://arxiv.org/pdf/2408.16018v1",
      "published_date": "2024-08-25 17:07:08 UTC",
      "updated_date": "2024-08-25 17:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:36:48.214764"
    },
    {
      "arxiv_id": "2408.13888v1",
      "title": "Enhancing SQL Query Generation with Neurosymbolic Reasoning",
      "title_zh": "利用神经符号推理增强",
      "authors": [
        "Henrijs Princis",
        "Cristina David",
        "Alan Mycroft"
      ],
      "abstract": "Neurosymbolic approaches blend the effectiveness of symbolic reasoning with\nthe flexibility of neural networks. In this work, we propose a neurosymbolic\narchitecture for generating SQL queries that builds and explores a solution\ntree using Best-First Search, with the possibility of backtracking. For this\npurpose, it integrates a Language Model (LM) with symbolic modules that help\ncatch and correct errors made by the LM on SQL queries, as well as guiding the\nexploration of the solution tree. We focus on improving the performance of\nsmaller open-source LMs, and we find that our tool, Xander, increases accuracy\nby an average of 10.9% and reduces runtime by an average of 28% compared to the\nLM without Xander, enabling a smaller LM (with Xander) to outperform its\nfour-times larger counterpart (without Xander).",
      "tldr_zh": "这篇论文提出了一种神经符号(neurosymbolic)架构Xander，用于提升SQL查询生成，通过整合Language Model (LM)和符号模块，利用Best-First Search探索解决方案树并支持回溯，以捕获和纠正LM的错误。Xander专注于优化小型开源LM的性能，实验结果显示其准确率平均提高了10.9%，运行时间减少了28%。此外，该方法使小型LM的性能超过了其四倍大的对应模型，提供了一种高效的查询生成策略。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.SE",
        "I.2"
      ],
      "primary_category": "cs.DB",
      "comment": "11 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.13888v1",
      "published_date": "2024-08-25 16:37:26 UTC",
      "updated_date": "2024-08-25 16:37:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:36:59.537598"
    },
    {
      "arxiv_id": "2408.13871v2",
      "title": "AlphaViT: A Flexible Game-Playing AI for Multiple Games and Variable Board Sizes",
      "title_zh": "AlphaViT：一种适用于多种游戏和可变棋盘大小的灵活游戏AI",
      "authors": [
        "Kazuhisa Fujita"
      ],
      "abstract": "This paper presents novel game-playing AI agents based on the AlphaZero\nframework, enhanced with Vision Transformer (ViT): AlphaViT, AlphaViD, and\nAlphaVDA. These agents are designed to play multiple board games of various\nsizes using a single network with shared weights, thereby overcoming\nAlphaZero's limitation of fixed-board-size constraints. AlphaViT employs only a\ntransformer encoder, whereas AlphaViD and AlphaVDA incorporate both transformer\nencoders and decoders. In AlphaViD, the decoder processes outputs from the\nencoder, whereas AlphaVDA uses a learnable embeddings as the decoder input. The\nadditional decoder layers in AlphaViD and AlphaVDA provide flexibility to adapt\nto various action spaces and board sizes. Experimental results show that the\nproposed agents, trained on either individual games or multiple games\nsimultaneously, consistently outperform traditional algorithms such as Minimax\nand Monte Carlo Tree Search and approach the performance of AlphaZero, despite\nusing a single deep neural network (DNN) with shared weights. In particular,\nAlphaViT shows strong performance across all tested games. Furthermore,\nfine-tuning the DNN using pre-trained weights from small-board games\naccelerates convergence and improves performance, particularly in Gomoku.\nInterestingly, simultaneous training on multiple games yields performance\ncomparable to, or even surpassing, single-game training. These results indicate\nthe potential of transformer-based architectures to develop more flexible and\nrobust game-playing AI agents that excel in multiple games and dynamic\nenvironments.",
      "tldr_zh": "本文提出 AlphaViT、AlphaViD 和 AlphaVDA 等新型游戏 AI 代理，这些基于 AlphaZero 框架并整合 Vision Transformer (ViT)，能使用单一共享权重的网络适应多种棋盘游戏和不同棋盘大小，克服了 AlphaZero 的固定棋盘限制。AlphaViT 仅采用 transformer encoder，而 AlphaViD 和 AlphaVDA 结合 encoder 和 decoder，以灵活处理各种行动空间。实验结果显示，这些代理在单个或多个游戏训练中，性能优于传统算法如 Minimax 和 Monte Carlo Tree Search，并接近 AlphaZero；此外，微调预训练权重加速了收敛，尤其在 Gomoku 上，同时训练多个游戏还能实现或超越单游戏训练的水平。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13871v2",
      "published_date": "2024-08-25 15:40:21 UTC",
      "updated_date": "2024-11-29 07:00:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:37:12.841816"
    },
    {
      "arxiv_id": "2408.13863v1",
      "title": "CodeGraph: Enhancing Graph Reasoning of LLMs with Code",
      "title_zh": "CodeGraph：使用代码增强大型语言模型的图推理",
      "authors": [
        "Qiaolong Cai",
        "Zhaowei Wang",
        "Shizhe Diao",
        "James Kwok",
        "Yangqiu Song"
      ],
      "abstract": "With the increasing popularity of large language models (LLMs), reasoning on\nbasic graph algorithm problems is an essential intermediate step in assessing\ntheir abilities to process and infer complex graph reasoning tasks. Existing\nmethods usually convert graph-structured data to textual descriptions and then\nuse LLMs for reasoning and computation. However, LLMs often produce computation\nerrors on arithmetic parts in basic graph algorithm problems, such as counting\nnumber of edges. In addition, they struggle to control or understand the output\nof the reasoning process, raising concerns about whether LLMs are simply\nguessing. In this paper, we introduce CodeGraph, a method that encodes graph\nproblem solutions as code. The methods solve new graph problems by learning\nfrom exemplars, generating programs, and executing them via a program\ninterpreter. Using the few-shot setting, we evaluate CodeGraph with the base\nLLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and\nMixtral-8x7B Instruct. Experimental results on six tasks with six graph\nencoding methods in the GraphQA dataset demonstrate that CodeGraph can boost\nperformance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on\nthe task. Compared to the existing methods, CodeGraph demonstrates strong\nperformance on arithmetic problems in graph tasks and offers a more\ncontrollable and interpretable approach to the reasoning process.",
      "tldr_zh": "本论文提出 CodeGraph 方法，通过将图问题解决方案编码为代码，来提升大语言模型 (LLMs) 在图推理任务中的性能，解决现有方法在算术计算（如边数计数）和输出控制方面的不足。CodeGraph 利用 few-shot 学习从示例中生成程序，并通过程序解释器执行，以实现更准确和可解释的推理过程。在 GraphQA 数据集的六种任务和六种图编码方法上，实验结果显示，使用 GPT-3.5 Turbo、Llama3-70B Instruct 等基线模型时，CodeGraph 的性能提升幅度达 1.3% 到 58.6%。相比现有方法，该方法在算术问题上表现出色，并增强了推理过程的可控性和可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "In Progress",
      "pdf_url": "http://arxiv.org/pdf/2408.13863v1",
      "published_date": "2024-08-25 15:27:21 UTC",
      "updated_date": "2024-08-25 15:27:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:37:26.679940"
    },
    {
      "arxiv_id": "2409.00084v2",
      "title": "Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models",
      "title_zh": "翻译失败",
      "authors": [
        "Seyed Amir Ahmad Safavi-Naini",
        "Shuhaib Ali",
        "Omer Shahab",
        "Zahra Shahhoseini",
        "Thomas Savage",
        "Sara Rafiee",
        "Jamil S Samaan",
        "Reem Al Shabeeb",
        "Farah Ladak",
        "Jamie O Yang",
        "Juan Echavarria",
        "Sumbal Babar",
        "Aasma Shaukat",
        "Samuel Margolis",
        "Nicholas P Tatonetti",
        "Girish Nadkarni",
        "Bara El Kurdi",
        "Ali Soroush"
      ],
      "abstract": "Background and Aims: This study evaluates the medical reasoning performance\nof large language models (LLMs) and vision language models (VLMs) in\ngastroenterology.\n  Methods: We used 300 gastroenterology board exam-style multiple-choice\nquestions, 138 of which contain images to systematically assess the impact of\nmodel configurations and parameters and prompt engineering strategies utilizing\nGPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs\n(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),\nMistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces\n(web and API), computing environments (cloud and local), and model precisions\n(with and without quantization). Finally, we assessed accuracy using a\nsemiautomated pipeline.\n  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet\n(74.0%) achieved the highest accuracy, outperforming the top open-source\nmodels: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).\nAmong the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)\nperformed best. The scores of the quantized models were comparable to those of\nthe full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM\nperformance on image-containing questions did not improve when the images were\nprovided and worsened when LLM-generated captions were provided. In contrast, a\n10% increase in accuracy was observed when images were accompanied by\nhuman-crafted image descriptions.\n  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in\nmedical reasoning, the integration of visual data remains a challenge for VLMs.\nEffective deployment involves carefully determining optimal model\nconfigurations, encouraging users to consider either the high performance of\nproprietary models or the flexible adaptability of open-source models.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）和视觉语言模型（VLMs）在胃肠病学领域的医疗推理性能，使用300道考试风格多选题（其中138道包含图像）测试了如GPT-4o、Claude3.5-Sonnet、Llama3.1-405b等专有和开源模型的配置、参数及提示工程策略。结果显示，专有模型GPT-4o（73.7%）和Claude3.5-Sonnet（74.0%）准确率最高，而开源模型中Llama3.1-405b（64%）领先，量化模型如6-bit Phi3-14b（48.7%）性能可与某些全精度模型相当。VLMs在处理图像时表现不佳，提供图像或LLM生成的标题会降低准确率，但使用人工制作的图像描述可提高10%的准确率。总之，LLMs展现出强劲的零样本性能，而VLMs整合视觉数据仍面临挑战，建议根据需求选择高性能的专有模型或灵活的开源模型。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "92C50, 68T50",
        "J.3"
      ],
      "primary_category": "cs.CL",
      "comment": "Manuscript Pages: 34, Figures: 7, Tables: 2, Supplementary File\n  Pages: 35, Data Transparency Statement: Code is available at:\n  https://github.com/Sdamirsa/LLM-VLM-in-Gastroenterology . Study data from\n  American College of Gastroenterology (ACG) are restricted and available upon\n  request with ACG permission. Correction: updated abstract considering\n  Llama3.1 results",
      "pdf_url": "http://arxiv.org/pdf/2409.00084v2",
      "published_date": "2024-08-25 14:50:47 UTC",
      "updated_date": "2024-09-04 08:22:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:37:41.104549"
    },
    {
      "arxiv_id": "2408.13854v2",
      "title": "Tangram: Benchmark for Evaluating Geometric Element Recognition in Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chao Zhang",
        "Jiamin Tang",
        "Jing Xiao"
      ],
      "abstract": "Significant advancements in Large Multimodal Models (LMMs) have enabled them\nto tackle complex problems involving visual-mathematical reasoning. However,\ntheir ability to identify geometric elements remains underexplored. To address\nthis gap, we introduce Tangram, a novel benchmark designed to evaluate the\nperformance of LMMs on geometric element recognition. Tangram comprises 1,080\ndiverse geometric diagrams sourced from primary and secondary school exams,\ncompetitions, and textbooks, ranging from simple geometric shapes to complex\ncombinations. Each diagram is paired with four questions, resulting in 4,320\nvisual-question-answer pairs. Unlike existing benchmarks that emphasize\nhigher-level cognition and reasoning, Tangram focuses on understanding\ngeometric elements, requiring models to perform a ``simple yet challenging\"\ncounting task. Systematic evaluation of 13 prominent LMMs, such as GPT-4o and\nClaude 3.5 Sonnet, reveals that these models face significant challenges even\nin seemingly straightforward tasks. The top-performing model achieves an\naccuracy of only 53.0%, highlighting a substantial gap compared to human\nperformance. These findings underscore the limitations of current multimodal AI\nsystems in handling basic perception tasks and serve to inspire the development\nof the next generation of expert-level multimodal foundational models. The data\nand code will be released soon.",
      "tldr_zh": "本研究引入了Tangram基准，用于评估大型多模态模型(LMMs)在几何元素识别方面的性能。Tangram包含1080个多样化的几何图表（ sourced from primary and secondary school exams, competitions, and textbooks），每个图表配有4个问题，总计4320个视觉-问题-答案对，重点在于进行“simple yet challenging”的计数任务。对13个著名LMMs（如GPT-4o和Claude 3.5 Sonnet）的系统评估显示，最顶尖模型的准确率仅为53.0%，远低于人类表现，这突显了当前多模态AI系统在基本感知任务上的局限性，并激励开发下一代专家级多模态基础模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.13854v2",
      "published_date": "2024-08-25 14:47:25 UTC",
      "updated_date": "2024-12-17 08:12:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:37:52.346005"
    },
    {
      "arxiv_id": "2408.13850v1",
      "title": "Condensed Sample-Guided Model Inversion for Knowledge Distillation",
      "title_zh": "用于知识蒸馏的浓缩样本引导模型反转",
      "authors": [
        "Kuluhan Binici",
        "Shivam Aggarwal",
        "Cihan Acar",
        "Nam Trung Pham",
        "Karianto Leman",
        "Gim Hee Lee",
        "Tulika Mitra"
      ],
      "abstract": "Knowledge distillation (KD) is a key element in neural network compression\nthat allows knowledge transfer from a pre-trained teacher model to a more\ncompact student model. KD relies on access to the training dataset, which may\nnot always be fully available due to privacy concerns or logistical issues\nrelated to the size of the data. To address this, \"data-free\" KD methods use\nsynthetic data, generated through model inversion, to mimic the target data\ndistribution. However, conventional model inversion methods are not designed to\nutilize supplementary information from the target dataset, and thus, cannot\nleverage it to improve performance, even when it is available. In this paper,\nwe consider condensed samples, as a form of supplementary information, and\nintroduce a method for using them to better approximate the target data\ndistribution, thereby enhancing the KD performance. Our approach is versatile,\nevidenced by improvements of up to 11.4% in KD accuracy across various datasets\nand model inversion-based methods. Importantly, it remains effective even when\nusing as few as one condensed sample per class, and can also enhance\nperformance in few-shot scenarios where only limited real data samples are\navailable.",
      "tldr_zh": "这篇论文针对 Knowledge Distillation (KD) 的数据可用性问题，提出了一种使用 condensed samples 作为补充信息的模型反演方法，以更好地逼近目标数据分布并提升 KD 性能。该方法通过指导模型 inversion 生成更有效的合成数据，适用于各种数据集和反演技术，实现了高达 11.4% 的 KD 准确率提升。即使每个类别仅有一个 condensed sample，或在少样本场景下，该方法也能保持显著效果，为数据-free KD 提供了更灵活的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13850v1",
      "published_date": "2024-08-25 14:43:27 UTC",
      "updated_date": "2024-08-25 14:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:38:00.777593"
    },
    {
      "arxiv_id": "2408.13836v2",
      "title": "PAM: A Propagation-Based Model for Segmenting Any 3D Objects across Multi-Modal Medical Images",
      "title_zh": "翻译失败",
      "authors": [
        "Zifan Chen",
        "Xinyu Nan",
        "Jiazheng Li",
        "Jie Zhao",
        "Haifeng Li",
        "Ziling Lin",
        "Haoshen Li",
        "Heyun Chen",
        "Yiting Liu",
        "Lei Tang",
        "Li Zhang",
        "Bin Dong"
      ],
      "abstract": "Volumetric segmentation is important in medical imaging, but current methods\nface challenges like requiring lots of manual annotations and being tailored to\nspecific tasks, which limits their versatility. General segmentation models\nused for natural images don't perform well with the unique features of medical\nimages. There's a strong need for an adaptable approach that can effectively\nhandle different 3D medical structures and imaging modalities. In this study,\nwe present PAM (Propagating Anything Model), a segmentation approach that uses\na 2D prompt, like a bounding box or sketch, to create a complete 3D\nsegmentation of medical image volumes. PAM works by modeling relationships\nbetween slices, maintaining information flow across the 3D structure. It\ncombines a CNN-based UNet for processing within slices and a Transformer-based\nattention module for propagating information between slices, leading to better\ngeneralizability across various imaging modalities. PAM significantly\noutperformed existing models like MedSAM and SegVol, with an average\nimprovement of over 18.1% in dice similarity coefficient (DSC) across 44\nmedical datasets and various object types. It also showed stable performance\ndespite prompt deviations and different propagation setups, and faster\ninference speeds compared to other models. PAM's one-view prompt design made it\nmore efficient, reducing interaction time by about 63.6% compared to two-view\nprompts. Thanks to its focus on structural relationships, PAM handled unseen\nand complex objects well, showing a unique ability to generalize to new\nsituations. PAM represents an advancement in medical image segmentation,\neffectively reducing the need for extensive manual work and specialized\ntraining. Its adaptability makes it a promising tool for more automated and\nreliable analysis in clinical settings.",
      "tldr_zh": "本文提出PAM（Propagating Anything Model），一种基于传播机制的模型，用于多模态医学图像中任意3D对象的分割，解决了现有方法依赖大量手动标注和任务特定性的局限性。PAM通过2D提示（如bounding box或sketch）建模切片间关系，结合CNN-based UNet处理单个切片和Transformer-based attention模块传播信息，从而提升了跨模态的泛化能力。在44个数据集上，PAM比MedSAM和SegVol模型平均Dice Similarity Coefficient (DSC)提升18.1%，并显著减少交互时间（约63.6%），为临床图像分析提供更高效、可靠的自动化工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.13836v2",
      "published_date": "2024-08-25 13:42:47 UTC",
      "updated_date": "2024-10-25 08:31:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:38:13.618296"
    },
    {
      "arxiv_id": "2408.13831v1",
      "title": "Guardians of the Machine Translation Meta-Evaluation: Sentinel Metrics Fall In!",
      "title_zh": "翻译失败",
      "authors": [
        "Stefano Perrella",
        "Lorenzo Proietti",
        "Alessandro Scirè",
        "Edoardo Barba",
        "Roberto Navigli"
      ],
      "abstract": "Annually, at the Conference of Machine Translation (WMT), the Metrics Shared\nTask organizers conduct the meta-evaluation of Machine Translation (MT)\nmetrics, ranking them according to their correlation with human judgments.\nTheir results guide researchers toward enhancing the next generation of metrics\nand MT systems. With the recent introduction of neural metrics, the field has\nwitnessed notable advancements. Nevertheless, the inherent opacity of these\nmetrics has posed substantial challenges to the meta-evaluation process. This\nwork highlights two issues with the meta-evaluation framework currently\nemployed in WMT, and assesses their impact on the metrics rankings. To do this,\nwe introduce the concept of sentinel metrics, which are designed explicitly to\nscrutinize the meta-evaluation process's accuracy, robustness, and fairness. By\nemploying sentinel metrics, we aim to validate our findings, and shed light on\nand monitor the potential biases or inconsistencies in the rankings. We\ndiscover that the present meta-evaluation framework favors two categories of\nmetrics: i) those explicitly trained to mimic human quality assessments, and\nii) continuous metrics. Finally, we raise concerns regarding the evaluation\ncapabilities of state-of-the-art metrics, emphasizing that they might be basing\ntheir assessments on spurious correlations found in their training data.",
      "tldr_zh": "这篇论文分析了 WMT 会议中机器翻译 (MT) 指标的元评估框架，揭示了其存在的两个主要问题：评估的不透明性和潜在偏差。作者引入了 sentinel metrics 作为测试工具，来评估该框架的准确性、鲁棒性和公平性。研究发现，该框架偏好两种指标：(i) 那些显式训练来模仿人类质量评估的指标，以及 (ii) 连续指标。同时，论文质疑了当前 state-of-the-art 指标的可靠性，指出它们可能依赖于训练数据中的虚假相关性 (spurious correlations)，从而影响评估结果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at ACL 2024 Main Conference. 29 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.13831v1",
      "published_date": "2024-08-25 13:29:34 UTC",
      "updated_date": "2024-08-25 13:29:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:38:26.132538"
    },
    {
      "arxiv_id": "2408.13825v2",
      "title": "RoCP-GNN: Robust Conformal Prediction for Graph Neural Networks in Node-Classification",
      "title_zh": "RoCP-GNN：节点分类中图神经网络的鲁棒保形预测",
      "authors": [
        "S. Akansha"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for predicting\noutcomes in graph-structured data. However, a notable limitation of GNNs is\ntheir inability to provide robust uncertainty estimates, which undermines their\nreliability in contexts where errors are costly. One way to address this issue\nis by providing prediction sets that contain the true label with a predefined\nprobability margin. Our approach builds upon conformal prediction (CP), a\nframework that promises to construct statistically robust prediction sets or\nintervals. There are two primary challenges: first, given dependent data like\ngraphs, it is unclear whether the critical assumption in CP - exchangeability -\nstill holds when applied to node classification. Second, even if the\nexchangeability assumption is valid for conformalized link prediction, we need\nto ensure high efficiency, i.e., the resulting prediction set or the interval\nlength is small enough to provide useful information. In this article, we\npropose a novel approach termed Robust Conformal Prediction for GNNs\n(RoCP-GNN), which integrates conformal prediction (CP) directly into the GNN\ntraining process. This method generates prediction sets, instead of just point\npredictions, that are valid at a user-defined confidence level, assuming only\nexchangeability. Our approach robustly predicts outcomes with any predictive\nGNN model while quantifying the uncertainty in predictions within the realm of\ngraph-based semi-supervised learning (SSL). Experimental results demonstrate\nthat GNN models with size loss provide a statistically significant increase in\nperformance. We validate our approach on standard graph benchmark datasets by\ncoupling it with various state-of-the-art GNNs in node classification. The code\nwill be made available after publication.",
      "tldr_zh": "该研究针对图神经网络(GNNs)在节点分类任务中缺乏鲁棒不确定性估计的问题，提出了一种新方法RoCP-GNN，将Conformal Prediction(CP)直接整合到GNN训练过程中。RoCP-GNN生成置信度可控的预测集，而不是单一点预测，假设数据满足exchangeability条件，同时优化预测集的效率以提供有用信息。实验结果显示，使用size loss的GNN模型在标准图基准数据集上与多种SOTA GNNs结合时，性能显著提升，为图-based semi-supervised learning(SSL)中的不确定性量化提供了可靠解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "12, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.13825v2",
      "published_date": "2024-08-25 12:51:19 UTC",
      "updated_date": "2024-10-09 06:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:38:37.851790"
    },
    {
      "arxiv_id": "2408.14515v2",
      "title": "A Joint Learning Model with Variational Interaction for Multilingual Program Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Yali Du",
        "Hui Sun",
        "Ming Li"
      ],
      "abstract": "Programs implemented in various programming languages form the foundation of\nsoftware applications. To alleviate the burden of program migration and\nfacilitate the development of software systems, automated program translation\nacross languages has garnered significant attention. Previous approaches\nprimarily focus on pairwise translation paradigms, learning translation between\npairs of languages using bilingual parallel data. However, parallel data is\ndifficult to collect for some language pairs, and the distribution of program\nsemantics across languages can shift, posing challenges for pairwise program\ntranslation. In this paper, we argue that jointly learning a unified model to\ntranslate code across multiple programming languages is superior to separately\nlearning from bilingual parallel data. We propose Variational Interaction for\nMultilingual Program Translation~(VIM-PT), a disentanglement-based generative\napproach that jointly trains a unified model for multilingual program\ntranslation across multiple languages. VIM-PT disentangles code into\nlanguage-shared and language-specific features, using variational inference and\ninteraction information with a novel lower bound, then achieves program\ntranslation through conditional generation. VIM-PT demonstrates four\nadvantages: 1) captures language-shared information more accurately from\nvarious implementations and improves the quality of multilingual program\ntranslation, 2) mines and leverages the capability of non-parallel data, 3)\naddresses the distribution shift of program semantics across languages, 4) and\nserves as a unified model, reducing deployment complexity.",
      "tldr_zh": "本论文提出 VIM-PT（Variational Interaction for Multilingual Program Translation），一种基于变分交互的联合学习模型，用于多语言程序翻译。该模型通过解缠结代码的语言共享和语言特定特征，并结合变分推理和交互信息的新下界，实现统一的多语言翻译训练，而非依赖传统的配对翻译方法。VIM-PT 能够更准确捕获语言共享信息、利用非平行数据、解决程序语义分布偏移问题，并作为单一模型减少部署复杂性，从而显著提升多语言程序翻译的质量。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.14515v2",
      "published_date": "2024-08-25 11:33:52 UTC",
      "updated_date": "2024-09-13 04:25:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:38:49.879927"
    },
    {
      "arxiv_id": "2408.13786v1",
      "title": "Localization of Synthetic Manipulations in Western Blot Images",
      "title_zh": "翻译失败",
      "authors": [
        "Anmol Manjunath",
        "Viola Negroni",
        "Sara Mandelli",
        "Daniel Moreira",
        "Paolo Bestagini"
      ],
      "abstract": "Recent breakthroughs in deep learning and generative systems have\nsignificantly fostered the creation of synthetic media, as well as the local\nalteration of real content via the insertion of highly realistic synthetic\nmanipulations. Local image manipulation, in particular, poses serious\nchallenges to the integrity of digital content and societal trust. This problem\nis not only confined to multimedia data, but also extends to biological images\nincluded in scientific publications, like images depicting Western blots. In\nthis work, we address the task of localizing synthetic manipulations in Western\nblot images. To discriminate between pristine and synthetic pixels of an\nanalyzed image, we propose a synthetic detector that operates on small patches\nextracted from the image. We aggregate patch contributions to estimate a\ntampering heatmap, highlighting synthetic pixels out of pristine ones. Our\nmethodology proves effective when tested over two manipulated Western blot\nimage datasets, one altered automatically and the other manually by exploiting\nadvanced AI-based image manipulation tools that are unknown at our training\nstage. We also explore the robustness of our method over an external dataset of\nother scientific images depicting different semantics, manipulated through\nunseen generation techniques.",
      "tldr_zh": "本研究针对深度学习和生成系统导致的图像操纵问题，专注于定位 Western blot 图像中的合成篡改，以维护数字内容完整性和社会信任。研究提出了一种合成检测器，该方法通过分析图像的小 patch，并聚合这些 patch 的贡献来生成 tampering heatmap，从而精确区分合成像素和原始像素。在两个数据集上的实验中，该方法在自动和手动篡改的 Western blot 图像中表现出色，即使面对训练阶段未知的 AI 工具和外部科学图像数据集，也证明了其鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13786v1",
      "published_date": "2024-08-25 09:29:20 UTC",
      "updated_date": "2024-08-25 09:29:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:39:02.466063"
    },
    {
      "arxiv_id": "2408.13784v1",
      "title": "Analyzing the Impact of Splicing Artifacts in Partially Fake Speech Signals",
      "title_zh": "分析拼接伪像在部分伪造语音信号中的影响",
      "authors": [
        "Viola Negroni",
        "Davide Salvi",
        "Paolo Bestagini",
        "Stefano Tubaro"
      ],
      "abstract": "Speech deepfake detection has recently gained significant attention within\nthe multimedia forensics community. Related issues have also been explored,\nsuch as the identification of partially fake signals, i.e., tracks that include\nboth real and fake speech segments. However, generating high-quality spliced\naudio is not as straightforward as it may appear. Spliced signals are typically\ncreated through basic signal concatenation. This process could introduce\nnoticeable artifacts that can make the generated data easier to detect. We\nanalyze spliced audio tracks resulting from signal concatenation, investigate\ntheir artifacts and assess whether such artifacts introduce any bias in\nexisting datasets. Our findings reveal that by analyzing splicing artifacts, we\ncan achieve a detection EER of 6.16% and 7.36% on PartialSpoof and HAD\ndatasets, respectively, without needing to train any detector. These results\nunderscore the complexities of generating reliable spliced audio data and lead\nto discussions that can help improve future research in this area.",
      "tldr_zh": "本研究分析了拼接 artifacts 在部分假语音信号（partially fake speech signals）中的影响，这些信号由真实和假冒语音段组成。研究发现，通过基本信号连接创建的拼接音频通常会引入明显的 artifacts，从而导致现有数据集（如 PartialSpoof 和 HAD）中的检测偏差。无需训练任何检测器，仅通过分析这些 artifacts，即可实现 6.16% 和 7.36% 的检测 EER（Equal Error Rate）。这些结果突出了生成可靠拼接音频的复杂性，并为改进语音 deepfake detection 领域的未来研究提供了重要启示。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at ASVspoof 5 Workshop (Interspeech2024 Satellite)",
      "pdf_url": "http://arxiv.org/pdf/2408.13784v1",
      "published_date": "2024-08-25 09:28:04 UTC",
      "updated_date": "2024-08-25 09:28:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:39:12.698010"
    },
    {
      "arxiv_id": "2408.14513v1",
      "title": "Variational autoencoder-based neural network model compression",
      "title_zh": "基于变分自编码器的神经网络模型压缩",
      "authors": [
        "Liang Cheng",
        "Peiyuan Guan",
        "Amir Taherkordi",
        "Lei Liu",
        "Dapeng Lan"
      ],
      "abstract": "Variational Autoencoders (VAEs), as a form of deep generative model, have\nbeen widely used in recent years, and shown great great peformance in a number\nof different domains, including image generation and anomaly detection, etc..\nThis paper aims to explore neural network model compression method based on\nVAE. The experiment uses different neural network models for MNIST recognition\nas compression targets, including Feedforward Neural Network (FNN),\nConvolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Long\nShort-Term Memory (LSTM). These models are the most basic models in deep\nlearning, and other more complex and advanced models are based on them or\ninherit their features and evolve. In the experiment, the first step is to\ntrain the models mentioned above, each trained model will have different\naccuracy and number of total parameters. And then the variants of parameters\nfor each model are processed as training data in VAEs separately, and the\ntrained VAEs are tested by the true model parameters. The experimental results\nshow that using the latent space as a representation of the model compression\ncan improve the compression rate compared to some traditional methods such as\npruning and quantization, meanwhile the accuracy is not greatly affected using\nthe model parameters reconstructed based on the latent space. In the future, a\nvariety of different large-scale deep learning models will be used more widely,\nso exploring different ways to save time and space on saving or transferring\nmodels will become necessary, and the use of VAE in this paper can provide a\nbasis for these further explorations.",
      "tldr_zh": "本论文提出了一种基于 Variational Autoencoders (VAEs) 的神经网络模型压缩方法，旨在通过潜在空间(latent space)高效压缩模型参数，同时保持准确性。研究者使用 Feedforward Neural Network (FNN)、Convolutional Neural Network (CNN)、Recurrent Neural Network (RNN) 和 Long Short-Term Memory (LSTM) 等模型在 MNIST 识别任务上进行训练，并将这些模型的参数作为 VAEs 的训练数据。实验结果显示，该方法相较于传统 pruning 和 quantization 技术，提高了压缩率，且使用重建参数的模型准确性影响不大，为未来大规模深度学习模型的保存和传输提供新基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14513v1",
      "published_date": "2024-08-25 09:06:22 UTC",
      "updated_date": "2024-08-25 09:06:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:39:26.203303"
    },
    {
      "arxiv_id": "2408.13773v1",
      "title": "SAB:A Stealing and Robust Backdoor Attack based on Steganographic Algorithm against Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Weida Xu",
        "Yang Xu",
        "Sicong Zhang"
      ],
      "abstract": "Federated learning, an innovative network architecture designed to safeguard\nuser privacy, is gaining widespread adoption in the realm of technology.\nHowever, given the existence of backdoor attacks in federated learning,\nexploring the security of federated learning is significance. Nevertheless, the\nbackdoors investigated in current federated learning research can be readily\ndetected by human inspection or resisted by detection algorithms. Accordingly,\na new goal has been set to develop stealing and robust federated learning\nbackdoor attacks. In this paper, we introduce a novel approach, SAB, tailored\nspecifically for backdoor attacks in federated learning, presenting an\nalternative gradient updating mechanism. SAB attack based on steganographic\nalgorithm, using image steganographic algorithm to build a full-size trigger to\nimprove the accuracy of backdoors and use multiple loss joint computation to\nproduce triggers. SAB exhibits smaller distances to benign samples and greater\nimperceptibility to the human eye. As such, our triggers are capable of\nmitigating or evading specific backdoor defense methods. In SAB, the\nbottom-95\\% method is applied to extend the lifespan of backdoor attacks. It\nupdates the gradient on minor value points to reduce the probability of being\ncleaned. Finally, the generalization of backdoors is enhanced with\nSparse-update to improve the backdoor accuracy.",
      "tldr_zh": "该论文提出了一种针对联邦学习的窃取和鲁棒后门攻击方法SAB，利用图像隐写算法（steganographic algorithm）构建全尺寸触发器，并通过多个损失联合计算来提高后门攻击的准确性和隐蔽性。SAB的触发器与良性样本距离更小，更难被人类察觉，从而能缓解或规避特定后门防御机制，同时引入bottom-95%方法延长攻击寿命，并使用Sparse-update增强后门的泛化性。实验结果表明，SAB显著提高了后门攻击的鲁棒性和有效性，为探索联邦学习（Federated Learning）的安全漏洞提供了新视角。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13773v1",
      "published_date": "2024-08-25 08:54:08 UTC",
      "updated_date": "2024-08-25 08:54:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:39:38.044764"
    },
    {
      "arxiv_id": "2408.13767v2",
      "title": "Lecture Notes on Linear Neural Networks: A Tale of Optimization and Generalization in Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nadav Cohen",
        "Noam Razin"
      ],
      "abstract": "These notes are based on a lecture delivered by NC on March 2021, as part of\nan advanced course in Princeton University on the mathematical understanding of\ndeep learning. They present a theory (developed by NC, NR and collaborators) of\nlinear neural networks -- a fundamental model in the study of optimization and\ngeneralization in deep learning. Practical applications born from the presented\ntheory are also discussed. The theory is based on mathematical tools that are\ndynamical in nature. It showcases the potential of such tools to push the\nenvelope of our understanding of optimization and generalization in deep\nlearning. The text assumes familiarity with the basics of statistical learning\ntheory. Exercises (without solutions) are included.",
      "tldr_zh": "这些笔记基于2021年NC在Princeton大学高级深度学习课程中的讲座，介绍了线性神经网络作为深度学习优化和泛化研究的根本模型。该理论由NC、NR和合作者开发，利用动态数学工具来深入剖析这些问题，并讨论了从理论中衍生出的实际应用。笔记假设读者熟悉统计学习理论的基本知识，并包含练习题（无解答），旨在推动对深度学习优化的数学理解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Lecture notes",
      "pdf_url": "http://arxiv.org/pdf/2408.13767v2",
      "published_date": "2024-08-25 08:24:48 UTC",
      "updated_date": "2024-11-06 15:02:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:39:49.055574"
    },
    {
      "arxiv_id": "2409.00083v1",
      "title": "On-device Learning of EEGNet-based Network For Wearable Motor Imagery Brain-Computer Interface",
      "title_zh": "翻译失败",
      "authors": [
        "Sizhen Bian",
        "Pixi Kang",
        "Julian Moosmann",
        "Mengxi Liu",
        "Pietro Bonazzi",
        "Roman Rosipal",
        "Michele Magno"
      ],
      "abstract": "Electroencephalogram (EEG)-based Brain-Computer Interfaces (BCIs) have\ngarnered significant interest across various domains, including rehabilitation\nand robotics. Despite advancements in neural network-based EEG decoding,\nmaintaining performance across diverse user populations remains challenging due\nto feature distribution drift. This paper presents an effective approach to\naddress this challenge by implementing a lightweight and efficient on-device\nlearning engine for wearable motor imagery recognition. The proposed approach,\napplied to the well-established EEGNet architecture, enables real-time and\naccurate adaptation to EEG signals from unregistered users. Leveraging the\nnewly released low-power parallel RISC-V-based processor, GAP9 from\nGreeenwaves, and the Physionet EEG Motor Imagery dataset, we demonstrate a\nremarkable accuracy gain of up to 7.31\\% with respect to the baseline with a\nmemory footprint of 15.6 KByte. Furthermore, by optimizing the input stream, we\nachieve enhanced real-time performance without compromising inference accuracy.\nOur tailored approach exhibits inference time of 14.9 ms and 0.76 mJ per single\ninference and 20 us and 0.83 uJ per single update during online training. These\nfindings highlight the feasibility of our method for edge EEG devices as well\nas other battery-powered wearable AI systems suffering from subject-dependant\nfeature distribution drift.",
      "tldr_zh": "本研究针对基于EEG的脑机接口(BCIs) 在不同用户间因特征分布漂移导致的性能下降问题，提出了一种轻量级设备端学习引擎，应用于EEGNet架构，以实现可穿戴运动想象识别的实时适应。方法利用低功耗RISC-V处理器（GAP9）和Physionet数据集，对未注册用户的EEG信号进行高效在线训练和推理，显著提升准确率高达7.31%，并将内存占用控制在15.6 KByte。实验结果显示，该方法在优化输入流后，推理时间为14.9 ms、能量消耗为0.76 mJ每次推理，以及训练时20 us和0.83 uJ每次更新，证明其适用于边缘EEG设备和其他受主体相关漂移影响的电池供电可穿戴AI系统。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00083v1",
      "published_date": "2024-08-25 08:23:51 UTC",
      "updated_date": "2024-08-25 08:23:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:40:02.495902"
    },
    {
      "arxiv_id": "2408.13754v1",
      "title": "Multimodal Ensemble with Conditional Feature Fusion for Dysgraphia Diagnosis in Children from Handwriting Samples",
      "title_zh": "多模态集成与条件特征融合用于从手写样本诊断儿童书写障碍",
      "authors": [
        "Jayakanth Kunhoth",
        "Somaya Al-Maadeed",
        "Moutaz Saleh",
        "Younes Akbari"
      ],
      "abstract": "Developmental dysgraphia is a neurological disorder that hinders children's\nwriting skills. In recent years, researchers have increasingly explored machine\nlearning methods to support the diagnosis of dysgraphia based on offline and\nonline handwriting. In most previous studies, the two types of handwriting have\nbeen analysed separately, which does not necessarily lead to promising results.\nIn this way, the relationship between online and offline data cannot be\nexplored. To address this limitation, we propose a novel multimodal machine\nlearning approach utilizing both online and offline handwriting data. We\ncreated a new dataset by transforming an existing online handwritten dataset,\ngenerating corresponding offline handwriting images. We considered only\ndifferent types of word data (simple word, pseudoword & difficult word) in our\nmultimodal analysis. We trained SVM and XGBoost classifiers separately on\nonline and offline features as well as implemented multimodal feature fusion\nand soft-voted ensemble. Furthermore, we proposed a novel ensemble with\nconditional feature fusion method which intelligently combines predictions from\nonline and offline classifiers, selectively incorporating feature fusion when\nconfidence scores fall below a threshold. Our novel approach achieves an\naccuracy of 88.8%, outperforming SVMs for single modalities by 12-14%, existing\nmethods by 8-9%, and traditional multimodal approaches (soft-vote ensemble and\nfeature fusion) by 3% and 5%, respectively. Our methodology contributes to the\ndevelopment of accurate and efficient dysgraphia diagnosis tools, requiring\nonly a single instance of multimodal word/pseudoword data to determine the\nhandwriting impairment. This work highlights the potential of multimodal\nlearning in enhancing dysgraphia diagnosis, paving the way for accessible and\npractical diagnostic tools.",
      "tldr_zh": "本文提出了一种多模态集成方法，用于从手写样本诊断儿童发育性书写障碍（dysgraphia），通过结合在线和离线手写数据来克服传统单独分析的局限性。方法包括训练 SVM 和 XGBoost 分类器分别处理在线和离线特征，并引入条件特征融合集成（conditional feature fusion），根据置信度阈值智能融合预测以提升准确性。实验在新建的数据集上实现了88.8%的准确率，比单模态 SVM 高12-14%、现有方法高8-9%、以及传统多模态方法高3-5%。这项研究为高效、仅需单一多模态实例的书写障碍诊断工具奠定基础，突显多模态学习在临床应用中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.6; I.2.10; I.4.9; I.5.1; I.5.4"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13754v1",
      "published_date": "2024-08-25 07:42:54 UTC",
      "updated_date": "2024-08-25 07:42:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:40:15.366828"
    },
    {
      "arxiv_id": "2408.13750v3",
      "title": "Multi-Agent Target Assignment and Path Finding for Intelligent Warehouse: A Cooperative Multi-Agent Deep Reinforcement Learning Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Liu",
        "Jianqi Gao",
        "Dongjie Zhu",
        "Zhongjian Qiao",
        "Pengbin Chen",
        "Jingxiang Guo",
        "Yanjie Li"
      ],
      "abstract": "Multi-agent target assignment and path planning (TAPF) are two key problems\nin intelligent warehouse. However, most literature only addresses one of these\ntwo problems separately. In this study, we propose a method to simultaneously\nsolve target assignment and path planning from a perspective of cooperative\nmulti-agent deep reinforcement learning (RL). To the best of our knowledge,\nthis is the first work to model the TAPF problem for intelligent warehouse to\ncooperative multi-agent deep RL, and the first to simultaneously address TAPF\nbased on multi-agent deep RL. Furthermore, previous literature rarely considers\nthe physical dynamics of agents. In this study, the physical dynamics of the\nagents is considered. Experimental results show that our method performs well\nin various task settings, which means that the target assignment is solved\nreasonably well and the planned path is almost shortest. Moreover, our method\nis more time-efficient than baselines.",
      "tldr_zh": "本文提出了一种基于合作多智能体深度强化学习（cooperative multi-agent deep reinforcement learning）的框架，用于同时解决智能仓库中的多智能体目标分配和路径规划（TAPF）问题，这是首次将TAPF建模为该框架并考虑智能体的物理动态。相比现有文献，该方法能够高效处理两者，实验结果显示在各种任务设置下，目标分配合理、规划路径几乎最短，且比基线方法更具时间效率。总体上，该研究为智能仓库优化提供了可扩展的解决方案。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13750v3",
      "published_date": "2024-08-25 07:32:58 UTC",
      "updated_date": "2024-10-27 09:09:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:40:26.619876"
    },
    {
      "arxiv_id": "2408.13745v4",
      "title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Haau-Sing Li",
        "Patrick Fernandes",
        "Iryna Gurevych",
        "André F. T. Martins"
      ],
      "abstract": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.",
      "tldr_zh": "该研究提出DOCE框架，用于优化LLM-based代码生成，通过整合候选生成、n-best reranking、minimum Bayes risk (MBR)解码和self-debugging等组件，进行执行-based评估。实验结果强调了执行-based方法相对于执行-free方法的显著优势，并证明了基于试用单元测试的过滤策略的有效性。作者进一步创新性地实现了self-debugging on multiple candidates，达到了代码生成重排序的state-of-the-art性能，并为未来代码生成研究提供了全面指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers",
      "pdf_url": "http://arxiv.org/pdf/2408.13745v4",
      "published_date": "2024-08-25 07:10:36 UTC",
      "updated_date": "2024-10-16 15:07:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:40:36.678035"
    },
    {
      "arxiv_id": "2408.13727v1",
      "title": "LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models",
      "title_zh": "LogParser-LLM：利用大型语言模型推进高效日志解析",
      "authors": [
        "Aoxiao Zhong",
        "Dengyao Mo",
        "Guiyang Liu",
        "Jinbu Liu",
        "Qingda Lu",
        "Qi Zhou",
        "Jiesheng Wu",
        "Quanzheng Li",
        "Qingsong Wen"
      ],
      "abstract": "Logs are ubiquitous digital footprints, playing an indispensable role in\nsystem diagnostics, security analysis, and performance optimization. The\nextraction of actionable insights from logs is critically dependent on the log\nparsing process, which converts raw logs into structured formats for downstream\nanalysis. Yet, the complexities of contemporary systems and the dynamic nature\nof logs pose significant challenges to existing automatic parsing techniques.\nThe emergence of Large Language Models (LLM) offers new horizons. With their\nexpansive knowledge and contextual prowess, LLMs have been transformative\nacross diverse applications. Building on this, we introduce LogParser-LLM, a\nnovel log parser integrated with LLM capabilities. This union seamlessly blends\nsemantic insights with statistical nuances, obviating the need for\nhyper-parameter tuning and labeled training data, while ensuring rapid\nadaptability through online parsing. Further deepening our exploration, we\naddress the intricate challenge of parsing granularity, proposing a new metric\nand integrating human interactions to allow users to calibrate granularity to\ntheir specific needs. Our method's efficacy is empirically demonstrated through\nevaluations on the Loghub-2k and the large-scale LogPub benchmark. In\nevaluations on the LogPub benchmark, involving an average of 3.6 million logs\nper dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM\ninvocations on average, achieving a 90.6% F1 score for grouping accuracy and an\n81.1% for parsing accuracy. These results demonstrate the method's high\nefficiency and accuracy, outperforming current state-of-the-art log parsers,\nincluding pattern-based, neural network-based, and existing LLM-enhanced\napproaches.",
      "tldr_zh": "本文提出 LogParser-LLM，一种利用大型语言模型 (LLM) 提升日志解析效率的新方法，旨在解决传统解析技术在复杂系统中的挑战，如无需超参数调整和标记训练数据，并支持在线解析和用户交互以调整解析粒度。LogParser-LLM 结合语义洞见与统计分析，引入新粒度指标以优化解析过程。在 Loghub-2k 和 LogPub 基准测试中，该方法平均仅需 272.5 次 LLM 调用，即实现 90.6% 的分组准确率 F1 score 和 81.1% 的解析准确率，显著优于基于模式、神经网络和现有 LLM 增强的日志解析器。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ACM KDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.13727v1",
      "published_date": "2024-08-25 05:34:24 UTC",
      "updated_date": "2024-08-25 05:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:40:51.359657"
    },
    {
      "arxiv_id": "2409.06709v1",
      "title": "Unveiling Visual Biases in Audio-Visual Localization Benchmarks",
      "title_zh": "翻译失败",
      "authors": [
        "Liangyu Chen",
        "Zihao Yue",
        "Boshen Xu",
        "Qin Jin"
      ],
      "abstract": "Audio-Visual Source Localization (AVSL) aims to localize the source of sound\nwithin a video. In this paper, we identify a significant issue in existing\nbenchmarks: the sounding objects are often easily recognized based solely on\nvisual cues, which we refer to as visual bias. Such biases hinder these\nbenchmarks from effectively evaluating AVSL models. To further validate our\nhypothesis regarding visual biases, we examine two representative AVSL\nbenchmarks, VGG-SS and EpicSounding-Object, where the vision-only models\noutperform all audiovisual baselines. Our findings suggest that existing AVSL\nbenchmarks need further refinement to facilitate audio-visual learning.",
      "tldr_zh": "本研究揭示了音频-视觉源定位 (Audio-Visual Source Localization, AVSL) 基准中的 visual bias 问题，即声音对象往往可以通过视觉线索 alone 轻易识别，导致基准无法有效评估 AVSL 模型。该团队通过检查两个代表性基准 VGG-SS 和 EpicSounding-Object，发现 vision-only 模型的表现优于所有 audiovisual 基准，验证了这一偏差的存在。最终，研究建议现有 AVSL 基准需要进一步改进，以更好地促进音频-视觉学习。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted by ECCV24 AVGenL Workshop",
      "pdf_url": "http://arxiv.org/pdf/2409.06709v1",
      "published_date": "2024-08-25 04:56:08 UTC",
      "updated_date": "2024-08-25 04:56:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:41:00.937714"
    },
    {
      "arxiv_id": "2408.14512v3",
      "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings",
      "title_zh": "大型语言模型作为零样本图学习器：图神经网络表示与大型语言模型标记嵌入的对",
      "authors": [
        "Duo Wang",
        "Yuan Zuo",
        "Fengzhi Li",
        "Junjie Wu"
      ],
      "abstract": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.",
      "tldr_zh": "该论文提出了一种名为 TEA-GLM 的框架，将大型语言模型 (LLMs) 作为零样本图机器学习器，通过对齐图神经网络 (GNN) 表示与 LLM token 嵌入来解决标签数据稀缺的问题。该框架预训练 GNN，并使用线性投影器将 GNN 表示转化为固定数量的图 token 嵌入，而无需调整 LLM，同时设计统一的指令支持不同级别的图任务，如节点分类和链接预测。与现有方法相比，这种设计增强了零样本学习的效果。实验结果表明，TEA-GLM 在未见数据集和任务上实现了最先进的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14512v3",
      "published_date": "2024-08-25 04:32:45 UTC",
      "updated_date": "2024-12-19 16:37:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:41:13.305006"
    },
    {
      "arxiv_id": "2408.13719v1",
      "title": "Count-based Novelty Exploration in Classical Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Giacomo Rosa",
        "Nir Lipovetzky"
      ],
      "abstract": "Count-based exploration methods are widely employed to improve the\nexploratory behavior of learning agents over sequential decision problems.\nMeanwhile, Novelty search has achieved success in Classical Planning through\nrecording of the first, but not successive, occurrences of tuples. In order to\nstructure the exploration, however, the number of tuples considered needs to\ngrow exponentially as the search progresses. We propose a new novelty\ntechnique, classical count-based novelty, which aims to explore the state space\nwith a constant number of tuples, by leveraging the frequency of each tuple's\nappearance in a search tree. We then justify the mechanisms through which lower\ntuple counts lead the search towards novel tuples. We also introduce\nalgorithmic contributions in the form of a trimmed open list that maintains a\nconstant size by pruning nodes with bad novelty values. These techniques are\nshown to complement existing novelty heuristics when integrated in a classical\nsolver, achieving competitive results in challenging benchmarks from recent\nInternational Planning Competitions. Moreover, adapting our solver as the\nfrontend planner in dual configurations that utilize both memory and time\nthresholds demonstrates a significant increase in instance coverage, surpassing\ncurrent state-of-the-art solvers.",
      "tldr_zh": "本文提出了一种新的count-based novelty技术，用于Classical Planning中的状态空间探索，该方法通过记录元组在搜索树中的出现频率，保持元组数量恒定，从而避免了传统Novelty search需要指数级增长的元组问题，并证明了较低元组计数能引导搜索向新元组。算法贡献包括引入trimmed open list，通过修剪不良新颖性值的节点来维持列表大小恒定，与现有新颖性启发式结合后，在国际规划竞赛的挑战基准上实现了竞争性性能。最终，该求解器在双配置（使用内存和时间阈值）中显著提升了实例覆盖率，超过了当前最先进求解器。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of paper accepted for publication at ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.13719v1",
      "published_date": "2024-08-25 04:25:10 UTC",
      "updated_date": "2024-08-25 04:25:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:41:26.644009"
    },
    {
      "arxiv_id": "2408.14511v2",
      "title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods",
      "title_zh": "揭示链式思维提示方法的统计基础",
      "authors": [
        "Xinyang Hu",
        "Fengzhuo Zhang",
        "Siyu Chen",
        "Zhuoran Yang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings.",
      "tldr_zh": "本研究从统计估计角度分析了Chain-of-Thought (CoT) 提示及其变体在解决多步推理问题时的样本复杂度，引入了一个多步潜在变量模型来封装推理过程。论文证明，当预训练数据集足够大时，CoT 提示等价于一个Bayesian estimator，通过聚合提示中的演示示例后验分布来有效处理推理任务。统计错误可分解为提示错误（随演示数量指数衰减）和预训练LLM的统计错误，且构建的transformer模型能使近似错误随块数指数减少。该分析扩展到Self-Consistent CoT、Tree-of-Thought和Selection-Inference等变体，并通过数值实验验证了理论结果。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.AI",
      "comment": "150 pages, 18 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.14511v2",
      "published_date": "2024-08-25 04:07:18 UTC",
      "updated_date": "2024-08-28 14:13:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:41:38.490754"
    },
    {
      "arxiv_id": "2408.13704v2",
      "title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
      "title_zh": "DHP 基准：LLMs 是否是好的 NLG 评估者？",
      "authors": [
        "Yicheng Wang",
        "Jiayi Yuan",
        "Yu-Neng Chuang",
        "Zhuoer Wang",
        "Yingchi Liu",
        "Mark Cusick",
        "Param Kulkarni",
        "Zhengping Ji",
        "Yasser Ibrahim",
        "Xia Hu"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly serving as evaluators in\nNatural Language Generation (NLG) tasks; this is often referred to as\n``LLM-as-a-judge'' paradigm. However, the capabilities of LLMs in evaluating\nNLG quality remain underexplored. Current studies depend on human assessments\nand simple metrics that fail to capture the discernment of LLMs across diverse\nNLG tasks. To address this gap, we propose the Discernment of Hierarchical\nPerturbation (DHP) benchmarking framework, which provides quantitative\ndiscernment scores for LLMs. This framework leverages hierarchically perturbed\ntext data and statistical tests to systematically measure the NLG evaluation\ncapabilities of LLMs. We re-established six evaluation datasets for this\nbenchmark, covering four NLG tasks: Summarization, Story Completion, Question\nAnswering, and Translation. Our comprehensive benchmarking of five major LLM\nfamilies provides critical insight into their strengths and limitations as NLG\nevaluators. Our dataset is available at\nhttps://huggingface.co/datasets/YCWANGVINCE/DHP_Benchmark.",
      "tldr_zh": "该论文探讨大型语言模型 (LLMs) 作为自然语言生成 (NLG) 评估者的能力，针对现有依赖人类评估和简单指标的不足，提出 Discernment of Hierarchical Perturbation (DHP) 基准框架，用于量化 LLMs 的辨别分数。DHP 框架通过分层扰动文本数据和统计测试，重新建立了六个数据集，涵盖总结、故事完成、问答和翻译等四种 NLG 任务。对五个主要 LLM 系列进行全面基准测试，揭示了它们作为 NLG 评估者的优势和局限性，并提供了公开数据集以供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13704v2",
      "published_date": "2024-08-25 02:01:38 UTC",
      "updated_date": "2025-02-25 01:51:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:42:00.735938"
    },
    {
      "arxiv_id": "2409.01281v2",
      "title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Jiace Zhu",
        "Yingtao Shen",
        "Jie Zhao",
        "An Zou"
      ],
      "abstract": "To enhance the reasoning capabilities of large language models (LLMs),\nself-consistency has gained significant popularity by combining multiple\nsampling with majority voting. However, the state-of-the-art self-consistency\napproaches consume substantial computational resources and lead to significant\nadditional time costs due to the multiple sampling. This prevents its full\npotential from being realized in scenarios where computational resources are\ncritical. To improve the inference efficiency, this paper introduces\n\\textit{path-consistency}, a method that leverages the confidence of answers\ngenerated in earlier branches to identify the prefix of the most promising\npath. By dynamically guiding the generation of subsequent branches based on\nthis prefix, the \\textit{path-consistency} mitigates both the errors and\nredundancies from random or less useful sampling in self-consistency. As a\nresult, it can significantly accelerate the inference process by reducing the\nnumber of tokens generated. Our extensive empirical evaluation shows that the\n\\textit{path-consistency} achieves significant acceleration in inference\nlatency ranging from $7.8\\%$ to $40.5\\%$, while maintaining or even improving\ntask accuracy across different datasets, including mathematical reasoning,\ncommon sense reasoning, symbolic reasoning, and code generation.",
      "tldr_zh": "本文提出path-consistency方法，以提升大型语言模型(LLM)的推理效率。该方法通过利用早期分支生成的答案置信度，识别最有前景的路径前缀，并动态引导后续分支的生成，从而减少self-consistency中随机采样的错误和冗余。相比传统方法，path-consistency显著加速推理延迟（7.8%至40.5%），同时在数学推理、常识推理、符号推理和代码生成等任务上维持或提升准确率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.01281v2",
      "published_date": "2024-08-25 01:45:53 UTC",
      "updated_date": "2025-03-02 09:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:42:03.286902"
    },
    {
      "arxiv_id": "2409.08281v1",
      "title": "StockTime: A Time Series Specialized Large Language Model Architecture for Stock Price Prediction",
      "title_zh": "StockTime：一种针对时间序列的股票价格预测专用大语言模型架构",
      "authors": [
        "Shengkun Wang",
        "Taoran Ji",
        "Linhan Wang",
        "Yanshen Sun",
        "Shang-Ching Liu",
        "Amit Kumar",
        "Chang-Tien Lu"
      ],
      "abstract": "The stock price prediction task holds a significant role in the financial\ndomain and has been studied for a long time. Recently, large language models\n(LLMs) have brought new ways to improve these predictions. While recent\nfinancial large language models (FinLLMs) have shown considerable progress in\nfinancial NLP tasks compared to smaller pre-trained language models (PLMs),\nchallenges persist in stock price forecasting. Firstly, effectively integrating\nthe modalities of time series data and natural language to fully leverage these\ncapabilities remains complex. Secondly, FinLLMs focus more on analysis and\ninterpretability, which can overlook the essential features of time series\ndata. Moreover, due to the abundance of false and redundant information in\nfinancial markets, models often produce less accurate predictions when faced\nwith such input data. In this paper, we introduce StockTime, a novel LLM-based\narchitecture designed specifically for stock price data. Unlike recent FinLLMs,\nStockTime is specifically designed for stock price time series data. It\nleverages the natural ability of LLMs to predict the next token by treating\nstock prices as consecutive tokens, extracting textual information such as\nstock correlations, statistical trends and timestamps directly from these stock\nprices. StockTime then integrates both textual and time series data into the\nembedding space. By fusing this multimodal data, StockTime effectively predicts\nstock prices across arbitrary look-back periods. Our experiments demonstrate\nthat StockTime outperforms recent LLMs, as it gives more accurate predictions\nwhile reducing memory usage and runtime costs.",
      "tldr_zh": "本文提出 StockTime，一种专为股票价格预测设计的大型语言模型 (LLMs) 架构，旨在解决现有金融大型语言模型 (FinLLMs) 在整合时间序列数据和自然语言时的挑战，包括数据模态融合困难和对冗余信息的敏感性。StockTime 通过将股票价格视为连续 tokens，利用 LLMs 的下一 token 预测能力，从中提取文本信息（如股票相关性、统计趋势和时间戳），并将这些多模态数据融合到嵌入空间中，实现任意回溯期的准确预测。实验表明，StockTime 相较于现有 LLMs 提供了更精确的预测，同时显著降低了内存使用和运行时成本。",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.08281v1",
      "published_date": "2024-08-25 00:50:33 UTC",
      "updated_date": "2024-08-25 00:50:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:42:24.520020"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 35,
  "processed_papers_count": 35,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T18:42:45.717312"
}
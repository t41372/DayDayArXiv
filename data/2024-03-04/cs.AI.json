{
  "date": "2024-03-04",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-04 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 104 篇论文，主要聚焦于 AI 领域，包括大型语言模型（LLMs）的创新应用、强化学习、图像生成和处理，以及医疗和机器人学等实际问题。其中，LLMs 在神经科学预测和不确定性量化方面的文章最令人印象深刻，展示了 LLMs 超越人类专家的潜力，同时一些知名学者如 Jitendra Malik 和 Pieter Abbeel 的相关工作也值得关注。\n\n下面，我挑选了今天几篇重要的、话题度高的论文进行详细讨论，先从影响力大的 LLMs 相关文章开始，然后快速概述其他有价值的论文。对于一些较基础或重复性的文章（如部分优化算法或小数据集分析），我将简要掠过，以控制篇幅。\n\n### 1. Large Language Models surpass human experts in predicting neuroscience results  \n**标题（中文 + 英文）**：大型语言模型在预测神经科学结果中超越人类专家（Large Language Models surpass human experts in predicting neuroscience results）  \n**主要贡献和发现**：这篇论文展示了 LLMs（如 GPT-4）在预测神经科学实验结果时超越人类专家的性能，通过 BrainBench 基准测试，LLMs 在处理复杂科学数据时表现出更高的准确性和效率。这不仅验证了 LLMs 在知识密集型任务中的潜力，还为未来的人机协作提供洞见，强调了 LLMs 在科学领域的实际应用价值。\n\n### 2. SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models  \n**标题（中文 + 英文）**：SPUQ：基于扰动的LLM不确定性量化（SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models）  \n**主要贡献和发现**：论文提出 SPUQ 方法，用于量化 LLMs 中的不确定性（包括 aleatoric 和 epistemic），通过扰动输入和聚合输出显著降低了预期校准误差（ECE 平均减少 50%）。这增强了 LLMs 的可靠性和鲁棒性，对于高风险应用（如医疗决策）有重要意义。\n\n### 3. Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents  \n**标题（中文 + 英文）**：试错：基于探索的LLM代理轨迹优化（Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents）  \n**主要贡献和发现**：作者引入 ETO 框架，让 LLMs 通过探索失败轨迹来优化策略，使用对比学习（如 DPO）迭代改进代理性能，在复杂任务上大幅超越基线。该方法强调从失败中学习，适用于强化学习场景，并已在多个任务中验证其有效性。\n\n### 4. Wukong: Towards a Scaling Law for Large-Scale Recommendation  \n**标题（中文 + 英文）**：Wukong：针对大规模推荐的缩放定律（Wukong: Towards a Scaling Law for Large-Scale Recommendation）  \n**主要贡献和发现**：论文提出 Wukong 架构，使用堆叠因子机捕获高阶交互，并建立推荐系统的缩放定律，在多个数据集上超越 SOTA 模型。该工作解决了推荐模型的扩展性问题，对于实际推荐系统有直接启发。\n\n### 5. HeAR - Health Acoustic Representations  \n**标题（中文 + 英文）**：HeAR - 健康声学表示（HeAR - Health Acoustic Representations）  \n**主要贡献和发现**：该研究开发了 HeAR 模型，使用自监督学习从海量音频数据中提取健康信号表示，在 33 个健康音频任务上达到 SOTA 性能。这为非侵入式健康监测提供了新工具，扩展了医疗 AI 的应用。\n\n其他论文中，一些涉及机器人和图像生成的文章也有亮点，但不那么核心。例如：\n- **Twisting Lids Off with Two Hands**（拧开盖子用两手）：提出强化学习框架用于双臂机器人操作，实现了模拟到真实世界的转移，贡献在于动态行为建模。\n- **Diffusion-TS**（扩散时间序列）：引入扩散模型用于时间序列生成，改进了多变量预测的准确性。\n- **Anatomically Constrained Tractography of the Fetal Brain**（胎儿大脑的解剖约束轨迹追踪）：使用深度学习提升胎儿脑部图像分析，适用于医疗成像。\n- 部分较基础的论文，如一些 SVM 或优化算法的改进（e.g., MORBDD），我快速掠过，因为它们对整体领域影响有限。\n\n总之，今天的论文突显了 LLMs 在科学和实际应用中的潜力，同时强化学习和多模态方法也在快速发展。如果你对 LLMs 在神经科学或不确定性方面的应用感兴趣，建议优先阅读以上几篇。明天的快报再见！",
  "papers": [
    {
      "arxiv_id": "2403.02545v4",
      "title": "Wukong: Towards a Scaling Law for Large-Scale Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Buyun Zhang",
        "Liang Luo",
        "Yuxin Chen",
        "Jade Nie",
        "Xi Liu",
        "Daifeng Guo",
        "Yanli Zhao",
        "Shen Li",
        "Yuchen Hao",
        "Yantao Yao",
        "Guna Lakshminarayanan",
        "Ellie Dingqiao Wen",
        "Jongsoo Park",
        "Maxim Naumov",
        "Wenlin Chen"
      ],
      "abstract": "Scaling laws play an instrumental role in the sustainable improvement in\nmodel quality. Unfortunately, recommendation models to date do not exhibit such\nlaws similar to those observed in the domain of large language models, due to\nthe inefficiencies of their upscaling mechanisms. This limitation poses\nsignificant challenges in adapting these models to increasingly more complex\nreal-world datasets. In this paper, we propose an effective network\narchitecture based purely on stacked factorization machines, and a synergistic\nupscaling strategy, collectively dubbed Wukong, to establish a scaling law in\nthe domain of recommendation. Wukong's unique design makes it possible to\ncapture diverse, any-order of interactions simply through taller and wider\nlayers. We conducted extensive evaluations on six public datasets, and our\nresults demonstrate that Wukong consistently outperforms state-of-the-art\nmodels quality-wise. Further, we assessed Wukong's scalability on an internal,\nlarge-scale dataset. The results show that Wukong retains its superiority in\nquality over state-of-the-art models, while holding the scaling law across two\norders of magnitude in model complexity, extending beyond 100 GFLOP/example,\nwhere prior arts fall short.",
      "tldr_zh": "该论文指出，现有的推荐模型缺乏类似于大语言模型的缩放定律（scaling law），这阻碍了它们适应复杂数据集的能力。作者提出 Wukong，一种基于堆叠因子分解机（stacked factorization machines）的网络架构和协同缩放策略，通过更深更宽的层来捕获各种阶交互。实验结果显示，Wukong 在六个公共数据集上优于最先进模型，并在大型内部数据集上证明了其缩放定律，模型复杂度超过 100 GFLOP/example 时仍保持性能优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.02545v4",
      "published_date": "2024-03-04 23:40:20 UTC",
      "updated_date": "2024-06-04 04:29:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:08:28.369499"
    },
    {
      "arxiv_id": "2403.02528v2",
      "title": "DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Xueqing Wu",
        "Rui Zheng",
        "Jingzhen Sha",
        "Te-Lin Wu",
        "Hanyu Zhou",
        "Mohan Tang",
        "Kai-Wei Chang",
        "Nanyun Peng",
        "Haoran Huang"
      ],
      "abstract": "Data analysis is a crucial analytical process to generate in-depth studies\nand conclusive insights to comprehensively answer a given user query for\ntabular data. In this work, we aim to propose new resources and benchmarks to\ninspire future research on this crucial yet challenging and under-explored\ntask. However, collecting data analysis annotations curated by experts can be\nprohibitively expensive. We propose to automatically generate high-quality\nanswer annotations leveraging the code-generation capabilities of LLMs with a\nmulti-turn prompting technique. We construct the DACO dataset, containing (1)\n440 databases (of tabular data) collected from real-world scenarios, (2) ~2k\nquery-answer pairs that can serve as weak supervision for model training, and\n(3) a concentrated but high-quality test set with human refined annotations\nthat serves as our main evaluation benchmark. We train a 6B supervised\nfine-tuning (SFT) model on DACO dataset, and find that the SFT model learns\nreasonable data analysis capabilities. To further align the models with human\npreference, we use reinforcement learning to encourage generating analysis\nperceived by human as helpful, and design a set of dense rewards to propagate\nthe sparse human preference reward to intermediate code generation steps. Our\nDACO-RL algorithm is evaluated by human annotators to produce more helpful\nanswers than SFT model in 57.72% cases, validating the effectiveness of our\nproposed algorithm. Data and code are released at\nhttps://github.com/shirley-wu/daco",
      "tldr_zh": "该论文提出DACO数据集和基准，旨在推动应用驱动的全面数据分析，通过LLMs的代码生成能力自动生成高质量的查询-答案注释。研究构建了包含440个真实场景数据库、约2k查询-答案对以及一个高质量人工精炼测试集的DACO数据集，并训练了一个6B SFT模型，使其学会合理的数据分析能力。为进一步与人类偏好对齐，他们设计了DACO-RL算法，使用强化学习(Reinforcement Learning)结合密集奖励来优化中间代码生成步骤。实验结果显示，DACO-RL在人类评估中比SFT模型在57.72%的案例中产生更有效的答案，从而验证了该算法的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024 Dataset and Benchmark Track",
      "pdf_url": "http://arxiv.org/pdf/2403.02528v2",
      "published_date": "2024-03-04 22:47:58 UTC",
      "updated_date": "2024-10-28 22:07:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:08:39.980866"
    },
    {
      "arxiv_id": "2403.02523v1",
      "title": "Transformer for Times Series: an Application to the S&P500",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Brugiere",
        "Gabriel Turinici"
      ],
      "abstract": "The transformer models have been extensively used with good results in a wide\narea of machine learning applications including Large Language Models and image\ngeneration. Here, we inquire on the applicability of this approach to financial\ntime series. We first describe the dataset construction for two prototypical\nsituations: a mean reverting synthetic Ornstein-Uhlenbeck process on one hand\nand real S&P500 data on the other hand. Then, we present in detail the proposed\nTransformer architecture and finally we discuss some encouraging results. For\nthe synthetic data we predict rather accurately the next move, and for the\nS&P500 we get some interesting results related to quadratic variation and\nvolatility prediction.",
      "tldr_zh": "这篇论文探讨了Transformer模型在金融时间序列分析中的应用，特别针对S&P500数据。作者构建了数据集，包括合成Ornstein-Uhlenbeck过程和真实S&P500数据，并详细设计了一个Transformer架构来处理这些序列。实验结果显示，该模型对合成数据实现了准确的下一步预测，而在S&P500数据上，取得了与二次变差和波动率预测相关的积极成果。",
      "categories": [
        "cs.AI",
        "q-fin.PM",
        "q-fin.ST",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02523v1",
      "published_date": "2024-03-04 22:27:11 UTC",
      "updated_date": "2024-03-04 22:27:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:08:51.034539"
    },
    {
      "arxiv_id": "2403.02522v1",
      "title": "HeAR -- Health Acoustic Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Sebastien Baur",
        "Zaid Nabulsi",
        "Wei-Hung Weng",
        "Jake Garrison",
        "Louis Blankemeier",
        "Sam Fishman",
        "Christina Chen",
        "Sujay Kakarmath",
        "Minyoi Maimbolwa",
        "Nsala Sanjase",
        "Brian Shuma",
        "Yossi Matias",
        "Greg S. Corrado",
        "Shwetak Patel",
        "Shravya Shetty",
        "Shruthi Prabhakara",
        "Monde Muyoyeta",
        "Diego Ardila"
      ],
      "abstract": "Health acoustic sounds such as coughs and breaths are known to contain useful\nhealth signals with significant potential for monitoring health and disease,\nyet are underexplored in the medical machine learning community. The existing\ndeep learning systems for health acoustics are often narrowly trained and\nevaluated on a single task, which is limited by data and may hinder\ngeneralization to other tasks. To mitigate these gaps, we develop HeAR, a\nscalable self-supervised learning-based deep learning system using masked\nautoencoders trained on a large dataset of 313 million two-second long audio\nclips. Through linear probes, we establish HeAR as a state-of-the-art health\naudio embedding model on a benchmark of 33 health acoustic tasks across 6\ndatasets. By introducing this work, we hope to enable and accelerate further\nhealth acoustics research.",
      "tldr_zh": "该研究针对健康声学声音（如咳嗽和呼吸）中潜在的健康信号，开发了 HeAR，这是一个可扩展的自监督学习系统，使用 masked autoencoders 在 3.13 亿个 2 秒音频片段上进行训练，以解决现有模型任务单一和泛化能力不足的问题。通过线性 probes 测试，HeAR 在跨越 6 个数据集的 33 个健康声学任务基准上，确立为最先进的健康音频嵌入模型。该工作旨在推动健康声学领域的研究，促进更多应用和创新。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 tables, 4 figures, 6 supplementary tables, 3 supplementary figures",
      "pdf_url": "http://arxiv.org/pdf/2403.02522v1",
      "published_date": "2024-03-04 22:26:25 UTC",
      "updated_date": "2024-03-04 22:26:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:09:03.456729"
    },
    {
      "arxiv_id": "2403.02514v2",
      "title": "A Formalisation of the Purpose Framework: the Autonomy-Alignment Problem in Open-Ended Learning Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Gianluca Baldassarre",
        "Richard J. Duro",
        "Emilio Cartoni",
        "Mehdi Khamassi",
        "Alejandro Romero",
        "Vieri Giuliano Santucci"
      ],
      "abstract": "The unprecedented advancement of artificial intelligence enables the\ndevelopment of increasingly autonomous robots. These robots hold significant\npotential, particularly in moving beyond engineered factory settings to operate\nin the unstructured environments inhabited by humans. However, this possibility\nalso generates a relevant autonomy-alignment problem to ensure that robots'\nautonomous learning processes still focus on acquiring knowledge relevant to\naccomplish human practical purposes, while their behaviour still aligns with\ntheir broader purposes. The literature has only begun to address this problem,\nand a conceptual, terminological, and formal framework is still lacking. Here\nwe address one of the most challenging instances of the problem: autonomous\nopen-ended learning (OEL) robots, capable of cumulatively acquiring new skills\nand knowledge through direct interaction with the environment, guided by\nself-generated goals and intrinsic motivations. In particular, we propose a\ncomputational framework, first introduced qualitatively and then formalised, to\nsupport the design of OEL robot architectures that balance autonomy and\ncontrol. The framework pivots on the novel concept of purpose. A human purpose\nspecifies what humans (e.g., designers or users) want the robot to learn, do or\nnot do, within a certain boundary of autonomy and independently of the domains\nin which it operates.The framework decomposes the autonomy-alignment problem\ninto more tractable sub-problems: the alignment of `robot purposes' with human\npurposes, either by hardwiring or through learning; the arbitration between\nmultiple purposes; the grounding of purposes into specific domain-dependent\nrobot goals; and the competence acquisition needed to accomplish these goals.\nThe framework and its potential utility are further elucidated through the\ndiscussion of hypothetical example scenarios framed within it.",
      "tldr_zh": "该论文探讨了人工智能在自主机器人领域的进展，特别是开放式学习（open-ended learning）机器人的自治-对齐问题（autonomy-alignment problem），即确保机器人在自主学习时仍能聚焦于人类实际目的并保持行为一致。作者提出一个基于“purpose”的计算框架，先进行定性介绍后形式化，以支持设计能平衡自治和控制的机器人架构。框架将问题分解为几个子问题，包括对齐“robot purposes”与人类目的（通过硬编码或学习）、仲裁多个目的、将目的转化为特定领域的机器人目标，以及获取必要技能。最后，通过假设场景阐释了框架的潜在实用性，为开发可信赖的自主机器人提供了理论基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.02514v2",
      "published_date": "2024-03-04 22:03:49 UTC",
      "updated_date": "2025-04-07 17:46:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:09:16.118699"
    },
    {
      "arxiv_id": "2403.02509v1",
      "title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Gao",
        "Jiaxin Zhang",
        "Lalla Mouatadid",
        "Kamalika Das"
      ],
      "abstract": "In recent years, large language models (LLMs) have become increasingly\nprevalent, offering remarkable text generation capabilities. However, a\npressing challenge is their tendency to make confidently wrong predictions,\nhighlighting the critical need for uncertainty quantification (UQ) in LLMs.\nWhile previous works have mainly focused on addressing aleatoric uncertainty,\nthe full spectrum of uncertainties, including epistemic, remains inadequately\nexplored. Motivated by this gap, we introduce a novel UQ method, sampling with\nperturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic\nuncertainties. The method entails generating a set of perturbations for LLM\ninputs, sampling outputs for each perturbation, and incorporating an\naggregation module that generalizes the sampling uncertainty approach for text\ngeneration tasks. Through extensive experiments on various datasets, we\ninvestigated different perturbation and aggregation techniques. Our findings\nshow a substantial improvement in model uncertainty calibration, with a\nreduction in Expected Calibration Error (ECE) by 50\\% on average. Our findings\nsuggest that our proposed UQ method offers promising steps toward enhancing the\nreliability and trustworthiness of LLMs.",
      "tldr_zh": "本论文针对大型语言模型（Large Language Models, LLMs）的自信错误预测问题，提出了一种新的不确定性量化（Uncertainty Quantification, UQ）方法SPUQ，以同时处理aleatoric uncertainty和epistemic uncertainty。SPUQ方法包括为LLM输入生成一组扰动（perturbations）、对每个扰动进行输出采样，并使用聚合模块来泛化采样不确定性策略，从而提升模型在文本生成任务中的不确定性校准。实验在多种数据集上验证了不同扰动和聚合技术的效果，结果显示SPUQ平均降低了Expected Calibration Error (ECE) 50%，为提高LLMs的可靠性和可信度提供了重要进展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to appear at EACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02509v1",
      "published_date": "2024-03-04 21:55:22 UTC",
      "updated_date": "2024-03-04 21:55:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:09:28.578984"
    },
    {
      "arxiv_id": "2403.02504v3",
      "title": "A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Wang",
        "Wen Qu"
      ],
      "abstract": "Given that natural language serves as the primary conduit for expressing\nthoughts and emotions, text analysis has become a key technique in\npsychological research. It enables the extraction of valuable insights from\nnatural language, facilitating endeavors like personality traits assessment,\nmental health monitoring, and sentiment analysis in interpersonal\ncommunications. In text analysis, existing studies often resort to either human\ncoding, which is time-consuming, using pre-built dictionaries, which often\nfails to cover all possible scenarios, or training models from scratch, which\nrequires large amounts of labeled data. In this tutorial, we introduce the\npretrain-finetune paradigm. The pretrain-finetune paradigm represents a\ntransformative approach in text analysis and natural language processing. This\nparadigm distinguishes itself through the use of large pretrained language\nmodels, demonstrating remarkable efficiency in finetuning tasks, even with\nlimited training data. This efficiency is especially beneficial for research in\nsocial sciences, where the number of annotated samples is often quite limited.\nOur tutorial offers a comprehensive introduction to the pretrain-finetune\nparadigm. We first delve into the fundamental concepts of pretraining and\nfinetuning, followed by practical exercises using real-world applications. We\ndemonstrate the application of the paradigm across various tasks, including\nmulti-class classification and regression. Emphasizing its efficacy and\nuser-friendliness, the tutorial aims to encourage broader adoption of this\nparadigm. To this end, we have provided open access to all our code and\ndatasets. The tutorial is highly beneficial across various psychology\ndisciplines, providing a comprehensive guide to employing text analysis in\ndiverse research settings.",
      "tldr_zh": "本教程介绍了预训练-微调（pretrain-finetune paradigm）范式在自然语言处理（NLP）中的应用，特别是针对文本分析的挑战，如人工编码耗时、预建字典覆盖不足或从零训练数据需求过高。该范式利用大型预训练语言模型（pretrained language models），通过预训练和微调过程，实现高效处理任务，即使在标注数据有限的社会科学研究中也能表现出色。教程涵盖基础概念、实际练习（如多类分类和回归任务），并提供开源代码和数据集，以鼓励其在心理学科中的广泛采用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "29 pages, 6 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.02504v3",
      "published_date": "2024-03-04 21:51:11 UTC",
      "updated_date": "2024-08-02 04:44:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:09:40.439969"
    },
    {
      "arxiv_id": "2403.02502v2",
      "title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Song",
        "Da Yin",
        "Xiang Yue",
        "Jie Huang",
        "Sujian Li",
        "Bill Yuchen Lin"
      ],
      "abstract": "Large Language Models (LLMs) have become integral components in various\nautonomous agent systems. In this study, we present an exploration-based\ntrajectory optimization approach, referred to as ETO. This learning method is\ndesigned to enhance the performance of open LLM agents. Contrary to previous\nstudies that exclusively train on successful expert trajectories, our method\nallows agents to learn from their exploration failures. This leads to improved\nperformance through an iterative optimization framework. During the exploration\nphase, the agent interacts with the environment while completing given tasks,\ngathering failure trajectories to create contrastive trajectory pairs. In the\nsubsequent training phase, the agent utilizes these trajectory preference pairs\nto update its policy using contrastive learning methods like DPO. This\niterative cycle of exploration and training fosters continued improvement in\nthe agents. Our experiments on three complex tasks demonstrate that ETO\nconsistently surpasses baseline performance by a large margin. Furthermore, an\nexamination of task-solving efficiency and potential in scenarios lacking\nexpert trajectory underscores the effectiveness of our approach.",
      "tldr_zh": "本研究提出了一种名为 ETO 的探索-based 轨迹优化方法，旨在提升大型语言模型（LLM）代理的性能。不同于以往仅依赖成功专家轨迹的训练，ETO 允许代理通过与环境互动收集失败轨迹，形成对比轨迹对，并在后续训练阶段使用对比学习方法如 DPO 迭代更新策略，从而实现持续性能改进。在三个复杂任务的实验中，ETO 显著超过了基线性能，并在缺乏专家轨迹的场景下展示了更高的任务解决效率和潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024 Main Conference; Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2403.02502v2",
      "published_date": "2024-03-04 21:50:29 UTC",
      "updated_date": "2024-07-10 17:36:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:09:51.608449"
    },
    {
      "arxiv_id": "2403.02495v1",
      "title": "Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking",
      "title_zh": "翻译失败",
      "authors": [
        "Huy Le",
        "Philipp Schillinger",
        "Miroslav Gabriel",
        "Alexander Qualmann",
        "Ngo Anh Vien"
      ],
      "abstract": "The prevailing grasp prediction methods predominantly rely on offline\nlearning, overlooking the dynamic grasp learning that occurs during real-time\nadaptation to novel picking scenarios. These scenarios may involve previously\nunseen objects, variations in camera perspectives, and bin configurations,\namong other factors. In this paper, we introduce a novel approach, SSL-ConvSAC,\nthat combines semi-supervised learning and reinforcement learning for online\ngrasp learning. By treating pixels with reward feedback as labeled data and\nothers as unlabeled, it efficiently exploits unlabeled data to enhance\nlearning. In addition, we address the imbalance between labeled and unlabeled\ndata by proposing a contextual curriculum-based method. We ablate the proposed\napproach on real-world evaluation data and demonstrate promise for improving\nonline grasp learning on bin picking tasks using a physical 7-DoF Franka Emika\nrobot arm with a suction gripper. Video: https://youtu.be/OAro5pg8I9U",
      "tldr_zh": "本文提出SSL-ConvSAC方法，结合半监督学习和强化学习，用于机器人箱子捡取任务中的在线抓取学习，以应对新物体、相机视角变化等动态场景。方法通过将有奖励反馈的像素视为标记数据，并利用伪标签（Pseudo-Labeling）高效处理未标记数据，同时引入上下文课程学习（Contextual Curriculum Learning）来平衡数据不平衡问题。实验在真实7-DoF Franka Emika机器人臂上验证，展示了该方法显著提升在线抓取学习的性能和适应性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ICRA 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02495v1",
      "published_date": "2024-03-04 21:41:27 UTC",
      "updated_date": "2024-03-04 21:41:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:10:04.217815"
    },
    {
      "arxiv_id": "2403.02484v1",
      "title": "Encodings for Prediction-based Neural Architecture Search",
      "title_zh": "翻译失败",
      "authors": [
        "Yash Akhauri",
        "Mohamed S. Abdelfattah"
      ],
      "abstract": "Predictor-based methods have substantially enhanced Neural Architecture\nSearch (NAS) optimization. The efficacy of these predictors is largely\ninfluenced by the method of encoding neural network architectures. While\ntraditional encodings used an adjacency matrix describing the graph structure\nof a neural network, novel encodings embrace a variety of approaches from\nunsupervised pretraining of latent representations to vectors of zero-cost\nproxies. In this paper, we categorize and investigate neural encodings from\nthree main types: structural, learned, and score-based. Furthermore, we extend\nthese encodings and introduce \\textit{unified encodings}, that extend NAS\npredictors to multiple search spaces. Our analysis draws from experiments\nconducted on over 1.5 million neural network architectures on NAS spaces such\nas NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and\nTransNASBench-101. Building on our study, we present our predictor\n\\textbf{FLAN}: \\textbf{Fl}ow \\textbf{A}ttention for \\textbf{N}AS. FLAN\nintegrates critical insights on predictor design, transfer learning, and\n\\textit{unified encodings} to enable more than an order of magnitude cost\nreduction for training NAS accuracy predictors. Our implementation and\nencodings for all neural networks are open-sourced at\n\\href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan\\_nas}.",
      "tldr_zh": "本论文探讨了基于预测器的 Neural Architecture Search (NAS) 中的编码方法，将其分类为 structural（结构化）、learned（学习型）和 score-based（基于分数）三类，并通过实验分析这些编码在 NASBench-101 (NB101)、NB201、NB301、NDS 和 TransNASBench-101 等空间上的表现。作者扩展了这些编码，引入了 unified encodings，以使 NAS 预测器适用于多个搜索空间。论文提出了一种新预测器 FLAN（Flow Attention for NAS），它整合了预测器设计、转移学习和 unified encodings 的关键见解，实现训练 NAS 准确性预测器的成本减少一个数量级以上，所有实现已开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02484v1",
      "published_date": "2024-03-04 21:05:52 UTC",
      "updated_date": "2024-03-04 21:05:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:10:17.091009"
    },
    {
      "arxiv_id": "2403.02482v1",
      "title": "MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify",
      "title_zh": "MORBDD：通过学习稀疏化实现的多目标受限二进制决策图",
      "authors": [
        "Rahul Patel",
        "Elias B. Khalil",
        "David Bergman"
      ],
      "abstract": "In multicriteria decision-making, a user seeks a set of non-dominated\nsolutions to a (constrained) multiobjective optimization problem, the so-called\nPareto frontier. In this work, we seek to bring a state-of-the-art method for\nexact multiobjective integer linear programming into the heuristic realm. We\nfocus on binary decision diagrams (BDDs) which first construct a graph that\nrepresents all feasible solutions to the problem and then traverse the graph to\nextract the Pareto frontier. Because the Pareto frontier may be exponentially\nlarge, enumerating it over the BDD can be time-consuming. We explore how\nrestricted BDDs, which have already been shown to be effective as heuristics\nfor single-objective problems, can be adapted to multiobjective optimization\nthrough the use of machine learning (ML). MORBDD, our ML-based BDD sparsifier,\nfirst trains a binary classifier to eliminate BDD nodes that are unlikely to\ncontribute to Pareto solutions, then post-processes the sparse BDD to ensure\nits connectivity via optimization. Experimental results on multiobjective\nknapsack problems show that MORBDD is highly effective at producing very small\nrestricted BDDs with excellent approximation quality, outperforming\nwidth-limited restricted BDDs and the well-known evolutionary algorithm\nNSGA-II.",
      "tldr_zh": "本研究针对多目标优化问题，提出MORBDD框架，通过机器学习(Machine Learning, ML)来稀疏化Binary Decision Diagrams (BDDs)，以高效提取Pareto前沿，同时减少计算开销。MORBDD首先训练一个二元分类器识别并消除不太可能贡献到Pareto解的节点，然后通过优化确保稀疏BDD的连通性。实验结果显示，在多目标背包问题上，MORBDD比width-limited restricted BDDs和NSGA-II更有效，生成更小的受限BDD同时保持优秀的近似质量。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02482v1",
      "published_date": "2024-03-04 21:04:54 UTC",
      "updated_date": "2024-03-04 21:04:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:10:30.601985"
    },
    {
      "arxiv_id": "2403.02454v1",
      "title": "The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer",
      "title_zh": "翻译失败",
      "authors": [
        "Asad Anjum",
        "Yuting Li",
        "Noelle Law",
        "M Charity",
        "Julian Togelius"
      ],
      "abstract": "This paper studies how large language models (LLMs) can act as effective,\nhigh-level creative collaborators and ``muses'' for game design. We model the\ndesign of this study after the exercises artists use by looking at amorphous\nink splotches for creative inspiration. Our goal is to determine whether\nAI-assistance can improve, hinder, or provide an alternative quality to games\nwhen compared to the creative intents implemented by human designers. The\ncapabilities of LLMs as game designers are stress tested by placing it at the\nforefront of the decision making process. Three prototype games are designed\nacross 3 different genres: (1) a minimalist base game, (2) a game with features\nand game feel elements added by a human game designer, and (3) a game with\nfeatures and feel elements directly implemented from prompted outputs of the\nLLM, ChatGPT. A user study was conducted and participants were asked to blindly\nevaluate the quality and their preference of these games. We discuss both the\ndevelopment process of communicating creative intent to an AI chatbot and the\nsynthesized open feedback of the participants. We use this data to determine\nboth the benefits and shortcomings of AI in a more design-centric role.",
      "tldr_zh": "本研究探讨大型语言模型（LLMs）如 ChatGPT 在游戏设计中的创意合作作用，通过模拟艺术家从墨渍中获取灵感的方法，评估 AI 是否能提升、阻碍或提供游戏质量的替代方案。研究设计了三个原型游戏：一个基础版本、一个由人类设计师添加特征的版本，以及一个直接基于 ChatGPT 输出实现的版本。用户研究中，参与者盲目评估这些游戏的质量和偏好，结果显示 AI 在创意沟通和生成方面有优势，但也存在理解意图和原创性不足的缺点，为 LLMs 在设计领域的应用提供了宝贵见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.02454v1",
      "published_date": "2024-03-04 20:14:38 UTC",
      "updated_date": "2024-03-04 20:14:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:10:40.577220"
    },
    {
      "arxiv_id": "2403.02444v1",
      "title": "Anatomically Constrained Tractography of the Fetal Brain",
      "title_zh": "解剖约束的胎儿大脑纤维束追踪",
      "authors": [
        "Camilo Calixto",
        "Camilo Jaimes",
        "Matheus D. Soldatelli",
        "Simon K. Warfield",
        "Ali Gholipour",
        "Davood Karimi"
      ],
      "abstract": "Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to\nstudy the fetal brain in utero. An important computation enabled by dMRI is\nstreamline tractography, which has unique applications such as tract-specific\nanalysis of the brain white matter and structural connectivity assessment.\nHowever, due to the low fetal dMRI data quality and the challenging nature of\ntractography, existing methods tend to produce highly inaccurate results. They\ngenerate many false streamlines while failing to reconstruct streamlines that\nconstitute the major white matter tracts. In this paper, we advocate for\nanatomically constrained tractography based on an accurate segmentation of the\nfetal brain tissue directly in the dMRI space. We develop a deep learning\nmethod to compute the segmentation automatically. Experiments on independent\ntest data show that this method can accurately segment the fetal brain tissue\nand drastically improve tractography results. It enables the reconstruction of\nhighly curved tracts such as optic radiations. Importantly, our method infers\nthe tissue segmentation and streamline propagation direction from a diffusion\ntensor fit to the dMRI data, making it applicable to routine fetal dMRI scans.\nThe proposed method can lead to significant improvements in the accuracy and\nreproducibility of quantitative assessment of the fetal brain with dMRI.",
      "tldr_zh": "本文提出一种基于解剖约束的轨迹追踪方法，用于改善胎儿大脑的 diffusion-weighted Magnetic Resonance Imaging (dMRI) 分析。该方法利用深度学习自动分割 dMRI 空间中的胎儿大脑组织，从而减少假流线生成并准确重建主要白质束，如高度弯曲的 optic radiations。实验结果显示，该方法在独立测试数据上显著提升了 tractography 的准确性和可重复性，为胎儿大脑结构连接评估提供更可靠的定量工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02444v1",
      "published_date": "2024-03-04 19:56:19 UTC",
      "updated_date": "2024-03-04 19:56:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:10:53.223930"
    },
    {
      "arxiv_id": "2403.02439v1",
      "title": "Root Causing Prediction Anomalies Using Explainable AI",
      "title_zh": "利用",
      "authors": [
        "Ramanathan Vishnampet",
        "Rajesh Shenoy",
        "Jianhui Chen",
        "Anuj Gupta"
      ],
      "abstract": "This paper presents a novel application of explainable AI (XAI) for\nroot-causing performance degradation in machine learning models that learn\ncontinuously from user engagement data. In such systems a single feature\ncorruption can cause cascading feature, label and concept drifts. We have\nsuccessfully applied this technique to improve the reliability of models used\nin personalized advertising. Performance degradation in such systems manifest\nas prediction anomalies in the models. These models are typically trained\ncontinuously using features that are produced by hundreds of real time data\nprocessing pipelines or derived from other upstream models. A failure in any of\nthese pipelines or an instability in any of the upstream models can cause\nfeature corruption, causing the model's predicted output to deviate from the\nactual output and the training data to become corrupted. The causal\nrelationship between the features and the predicted output is complex, and\nroot-causing is challenging due to the scale and dynamism of the system. We\ndemonstrate how temporal shifts in the global feature importance distribution\ncan effectively isolate the cause of a prediction anomaly, with better recall\nthan model-to-feature correlation methods. The technique appears to be\neffective even when approximating the local feature importance using a simple\nperturbation-based method, and aggregating over a few thousand examples. We\nhave found this technique to be a model-agnostic, cheap and effective way to\nmonitor complex data pipelines in production and have deployed a system for\ncontinuously analyzing the global feature importance distribution of\ncontinuously trained models.",
      "tldr_zh": "这篇论文提出了一种使用 Explainable AI (XAI) 来根因机器学习模型预测异常的新方法，针对从用户互动数据中持续学习的系统，解决特征损坏导致的级联特征、标签和概念漂移问题。方法通过分析全局特征重要性的时间变化来有效隔离异常原因，比传统的模型到特征相关性方法具有更高的召回率，即使使用简单的扰动-based 近似和少量样本聚合。作者已在个性化广告模型中成功应用此技术，并部署了系统用于持续监控复杂数据管道，提高了模型的可靠性和生产效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to The 2nd World Conference on eXplainable Artificial\n  Intelligence, 17-19 July, 2024, Malta, Valletta",
      "pdf_url": "http://arxiv.org/pdf/2403.02439v1",
      "published_date": "2024-03-04 19:38:50 UTC",
      "updated_date": "2024-03-04 19:38:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:11:06.493084"
    },
    {
      "arxiv_id": "2403.02437v3",
      "title": "A Survey on Federated Unlearning: Challenges and Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Hyejun Jeong",
        "Shiqing Ma",
        "Amir Houmansadr"
      ],
      "abstract": "Federated learning (FL), introduced in 2017, facilitates collaborative\nlearning between non-trusting parties with no need for the parties to\nexplicitly share their data among themselves. This allows training models on\nuser data while respecting privacy regulations such as GDPR and CPRA. However,\nemerging privacy requirements may mandate model owners to be able to\n\\emph{forget} some learned data, e.g., when requested by data owners or law\nenforcement. This has given birth to an active field of research called\n\\emph{machine unlearning}. In the context of FL, many techniques developed for\nunlearning in centralized settings are not trivially applicable! This is due to\nthe unique differences between centralized and distributed learning, in\nparticular, interactivity, stochasticity, heterogeneity, and limited\naccessibility in FL. In response, a recent line of work has focused on\ndeveloping unlearning mechanisms tailored to FL.\n  This SoK paper aims to take a deep look at the \\emph{federated unlearning}\nliterature, with the goal of identifying research trends and challenges in this\nemerging field. By carefully categorizing papers published on FL unlearning\n(since 2020), we aim to pinpoint the unique complexities of federated\nunlearning, highlighting limitations on directly applying centralized\nunlearning methods. We compare existing federated unlearning methods regarding\ninfluence removal and performance recovery, compare their threat models and\nassumptions, and discuss their implications and limitations. For instance, we\nanalyze the experimental setup of FL unlearning studies from various\nperspectives, including data heterogeneity and its simulation, the datasets\nused for demonstration, and evaluation metrics. Our work aims to offer insights\nand suggestions for future research on federated unlearning.",
      "tldr_zh": "这篇调查论文探讨了Federated Learning (FL)中机器遗忘（Machine Unlearning）的挑战与机会，FL允许各方在不共享数据的情况下进行协作学习，但传统中心化遗忘方法难以直接应用于FL，由于其交互性、随机性、异质性和有限可访问性等独特特性。论文系统化地分类了自2020年以来发布的FL遗忘相关文献，比较了现有方法的性能恢复和影响移除效果，并分析了它们的威胁模型、假设以及实验设置（如数据异质性模拟、数据集选择和评估指标）。最终，该研究揭示了FL遗忘的复杂性，提供见解和建议，以指导未来研究提升隐私保护和模型适应性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02437v3",
      "published_date": "2024-03-04 19:35:08 UTC",
      "updated_date": "2025-04-07 19:55:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:11:18.082622"
    },
    {
      "arxiv_id": "2403.02429v1",
      "title": "Towards efficient deep autoencoders for multivariate time series anomaly detection",
      "title_zh": "翻译失败",
      "authors": [
        "Marcin Pietroń",
        "Dominik Żurek",
        "Kamil Faber",
        "Roberto Corizzo"
      ],
      "abstract": "Multivariate time series anomaly detection is a crucial problem in many\nindustrial and research applications. Timely detection of anomalies allows, for\ninstance, to prevent defects in manufacturing processes and failures in\ncyberphysical systems. Deep learning methods are preferred among others for\ntheir accuracy and robustness for the analysis of complex multivariate data.\nHowever, a key aspect is being able to extract predictions in a timely manner,\nto accommodate real-time requirements in different applications. In the case of\ndeep learning models, model reduction is extremely important to achieve optimal\nresults in real-time systems with limited time and memory constraints. In this\npaper, we address this issue by proposing a novel compression method for deep\nautoencoders that involves three key factors. First, pruning reduces the number\nof weights, while preventing catastrophic drops in accuracy by means of a fast\nsearch process that identifies high sparsity levels. Second, linear and\nnon-linear quantization reduces model complexity by reducing the number of bits\nfor every single weight. The combined contribution of these three aspects allow\nthe model size to be reduced, by removing a subset of the weights (pruning),\nand decreasing their bit-width (quantization). As a result, the compressed\nmodel is faster and easier to adopt in highly constrained hardware\nenvironments. Experiments performed on popular multivariate anomaly detection\nbenchmarks, show that our method is capable of achieving significant model\ncompression ratio (between 80% and 95%) without a significant reduction in the\nanomaly detection performance.",
      "tldr_zh": "该论文针对多变量时间序列异常检测（multivariate time series anomaly detection）问题，提出了一种高效的深层自编码器（deep autoencoders）压缩方法，以满足实时系统的内存和时间约束。方法包括三个关键因素：修剪（pruning）通过快速搜索减少权重数量并维持准确率、非线性或线性量化（quantization）降低每个权重的位数，从而显著减少模型大小。实验结果显示，在热门基准数据集上，该方法实现了80%至95%的模型压缩率，同时异常检测性能几乎没有下降，为资源受限环境下的应用提供了实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02429v1",
      "published_date": "2024-03-04 19:22:09 UTC",
      "updated_date": "2024-03-04 19:22:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:11:29.684386"
    },
    {
      "arxiv_id": "2403.02419v2",
      "title": "Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems",
      "title_zh": "更多的LLM调用就是你所需要的全部吗？ 迈向复合推理系统的缩放定律",
      "authors": [
        "Lingjiao Chen",
        "Jared Quincy Davis",
        "Boris Hanin",
        "Peter Bailis",
        "Ion Stoica",
        "Matei Zaharia",
        "James Zou"
      ],
      "abstract": "Many recent state-of-the-art results in language tasks were achieved using\ncompound systems that perform multiple Language Model (LM) calls and aggregate\ntheir responses. However, there is little understanding of how the number of LM\ncalls - e.g., when asking the LM to answer each question multiple times and\ntaking a majority vote - affects such a compound system's performance. In this\npaper, we initiate the study of scaling properties of compound inference\nsystems. We analyze, theoretically and empirically, how the number of LM calls\naffects the performance of Vote and Filter-Vote, two of the simplest compound\nsystem designs, which aggregate LM responses via majority voting, optionally\napplying LM filters. We find, surprisingly, that across multiple language\ntasks, the performance of both Vote and Filter-Vote can first increase but then\ndecrease as a function of the number of LM calls. Our theoretical results\nsuggest that this non-monotonicity is due to the diversity of query\ndifficulties within a task: more LM calls lead to higher performance on \"easy\"\nqueries, but lower performance on \"hard\" queries, and non-monotone behavior can\nemerge when a task contains both types of queries. This insight then allows us\nto compute, from a small number of samples, the number of LM calls that\nmaximizes system performance, and define an analytical scaling model for both\nsystems. Experiments show that our scaling model can accurately predict the\nperformance of Vote and Filter-Vote systems and thus find the optimal number of\nLM calls to make.",
      "tldr_zh": "这篇论文探讨了在复合推理系统中增加 Language Model (LM) 调用次数是否能持续提升性能，焦点在于 Vote 和 Filter-Vote 这两种通过多数投票聚合响应的简单设计。研究发现，随着 LM 调用次数增加，系统性能并非单调上升，而是先提升后下降，这主要是由于任务中查询难度的多样性：容易查询受益于更多调用，而困难查询则可能受负面影响。论文通过理论分析和实验验证，开发了一个分析性缩放模型，能从少量样本计算出最优调用次数，并准确预测系统性能，从而为优化复合推理系统提供了指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02419v2",
      "published_date": "2024-03-04 19:12:48 UTC",
      "updated_date": "2024-06-04 21:20:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:11:42.131669"
    },
    {
      "arxiv_id": "2403.02338v2",
      "title": "Twisting Lids Off with Two Hands",
      "title_zh": "翻译失败",
      "authors": [
        "Toru Lin",
        "Zhao-Heng Yin",
        "Haozhi Qi",
        "Pieter Abbeel",
        "Jitendra Malik"
      ],
      "abstract": "Manipulating objects with two multi-fingered hands has been a long-standing\nchallenge in robotics, due to the contact-rich nature of many manipulation\ntasks and the complexity inherent in coordinating a high-dimensional bimanual\nsystem. In this work, we share novel insights into physical modeling, real-time\nperception, and reward design that enable policies trained in simulation using\ndeep reinforcement learning (RL) to be effectively and efficiently transferred\nto the real world. Specifically, we consider the problem of twisting lids of\nvarious bottle-like objects with two hands, demonstrating policies with\ngeneralization capabilities across a diverse set of unseen objects as well as\ndynamic and dexterous behaviors. To the best of our knowledge, this is the\nfirst sim-to-real RL system that enables such capabilities on bimanual\nmulti-fingered hands.",
      "tldr_zh": "该论文探讨了机器人使用两个多指手的双臂系统（bimanual multi-fingered hands）进行接触丰富的操作任务的挑战，并分享了物理建模、实时感知和奖励设计的创新见解。研究者通过在模拟环境中运用deep reinforcement learning (RL)训练策略，并实现sim-to-real转移，成功开发了能拧开各种瓶状物体盖子的政策，这些政策展示了在未见过物体上的泛化能力以及动态和灵巧的行为。作为首次在双臂多指手上实现这种能力的sim-to-real RL系统，该工作为复杂机器人操作奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page can be found at https://toruowo.github.io/bimanual-twist",
      "pdf_url": "http://arxiv.org/pdf/2403.02338v2",
      "published_date": "2024-03-04 18:59:30 UTC",
      "updated_date": "2024-10-14 06:02:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:11:54.774623"
    },
    {
      "arxiv_id": "2403.02336v1",
      "title": "Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis",
      "title_zh": "包装中的品牌可见性：一种用于标志检测、显著性图预测以及标志放置分析的",
      "authors": [
        "Alireza Hosseini",
        "Kiana Hooshanfar",
        "Pouria Omrani",
        "Reza Toosi",
        "Ramin Toosi",
        "Zahra Ebrahimian",
        "Mohammad Ali Akhaee"
      ],
      "abstract": "In the highly competitive area of product marketing, the visibility of brand\nlogos on packaging plays a crucial role in shaping consumer perception,\ndirectly influencing the success of the product. This paper introduces a\ncomprehensive framework to measure the brand logo's attention on a packaging\ndesign. The proposed method consists of three steps. The first step leverages\nYOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500\nand LogoDet-3K. The second step involves modeling the user's visual attention\nwith a novel saliency prediction model tailored for the packaging context. The\nproposed saliency model combines the visual elements with text maps employing a\ntransformers-based architecture to predict user attention maps. In the third\nstep, by integrating logo detection with a saliency map generation, the\nframework provides a comprehensive brand attention score. The effectiveness of\nthe proposed method is assessed module by module, ensuring a thorough\nevaluation of each component. Comparing logo detection and saliency map\nprediction with state-of-the-art models shows the superiority of the proposed\nmethods. To investigate the robustness of the proposed brand attention score,\nwe collected a unique dataset to examine previous psychophysical hypotheses\nrelated to brand visibility. the results show that the brand attention score is\nin line with all previous studies. Also, we introduced seven new hypotheses to\ncheck the impact of position, orientation, presence of person, and other visual\nelements on brand attention. This research marks a significant stride in the\nintersection of cognitive psychology, computer vision, and marketing, paving\nthe way for advanced, consumer-centric packaging designs.",
      "tldr_zh": "本研究提出了一种深度学习框架，用于评估品牌标志在包装设计中的可见性，从而影响消费者感知。该框架包括三个步骤：首先，使用 YOLOv8 在 FoodLogoDet-1500 和 LogoDet-3K 数据集上进行精确标志检测；其次，开发一个基于 transformers 的显著性预测模型，结合视觉元素和文本地图来模拟用户注意力；最后，通过整合标志检测和 saliency map 生成，计算出全面的品牌注意力分数。实验结果显示，该方法在标志检测和显著性预测方面优于现有模型，并在新数据集上验证了心理物理学假设，并引入七个新假设，探讨位置、方向、人存在等因素对品牌注意力的影响。该研究为认知心理学、计算机视觉和营销的交叉领域提供了新进展，推动消费者导向的包装设计。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02336v1",
      "published_date": "2024-03-04 18:58:53 UTC",
      "updated_date": "2024-03-04 18:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:12:05.899917"
    },
    {
      "arxiv_id": "2403.02334v1",
      "title": "Gradient Correlation Subspace Learning against Catastrophic Forgetting",
      "title_zh": "梯度相关子空间学习对抗灾难性",
      "authors": [
        "Tammuz Dubnov",
        "Vishal Thengane"
      ],
      "abstract": "Efficient continual learning techniques have been a topic of significant\nresearch over the last few years. A fundamental problem with such learning is\nsevere degradation of performance on previously learned tasks, known also as\ncatastrophic forgetting. This paper introduces a novel method to reduce\ncatastrophic forgetting in the context of incremental class learning called\nGradient Correlation Subspace Learning (GCSL). The method detects a subspace of\nthe weights that is least affected by previous tasks and projects the weights\nto train for the new task into said subspace. The method can be applied to one\nor more layers of a given network architectures and the size of the subspace\nused can be altered from layer to layer and task to task. Code will be\navailable at\n\\href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}",
      "tldr_zh": "本文提出了一种名为 Gradient Correlation Subspace Learning (GCSL) 的新方法，用于缓解持续学习（continual learning）中的 catastrophic forgetting 问题，即在新任务学习时避免对先前任务性能的严重退化。GCSL 通过检测权重子空间，该子空间对之前任务影响最小，并将新任务的权重投影到该子空间中进行训练，从而实现高效的增量类学习（incremental class learning）。该方法可灵活应用于网络的一个或多个层，并根据层级和任务调整子空间大小，提供更可适应的学习框架。代码已在 GitHub 上开源，供进一步验证和应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "5 figures; Code will be available here:\n  https://github.com/vgthengane/GCSL",
      "pdf_url": "http://arxiv.org/pdf/2403.02334v1",
      "published_date": "2024-03-04 18:58:46 UTC",
      "updated_date": "2024-03-04 18:58:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:12:19.443794"
    },
    {
      "arxiv_id": "2403.02333v3",
      "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning",
      "title_zh": "关键点驱动的数据合成及其对数学推理的增强",
      "authors": [
        "Yiming Huang",
        "Xiao Liu",
        "Yeyun Gong",
        "Zhibin Gou",
        "Yelong Shen",
        "Nan Duan",
        "Weizhu Chen"
      ],
      "abstract": "Large language models (LLMs) have shown great potential in complex reasoning\ntasks, yet their performance is often hampered by the scarcity of high-quality\nand reasoning-focused training datasets. Addressing this challenge, we propose\nKey-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that\nsynthesizes question-answer pairs by leveraging key points and exemplar\npractices from authentic data sources. KPDDS ensures the generation of novel\nquestions with rigorous quality control and substantial scalability. As a\nresult, we present KPMath, an extensive synthetic dataset tailored for\nmathematical reasoning, comprising over 800K question-answer pairs. Utilizing\nKPMath and augmenting it with additional reasoning-intensive corpora, we create\nthe comprehensive KPMath-Plus dataset. The Qwen1.5-72B model, fine-tuned on\nKPMath-Plus, achieves 87.0% PASS@1 accuracy on GSM8K and 58.3% on MATH,\nsurpassing competitors in the 7B to 70B range and best commercial models like\nGPT-4 across multiple math reasoning datasets.",
      "tldr_zh": "本研究提出 Key-Point-Driven Data Synthesis (KPDDS)，一种创新的数据合成框架，通过利用关键点和示例实践从真实数据源生成高质量问题-答案对，以解决大型语言模型(LLMs)在复杂推理任务中数据稀缺的问题。KPDDS 确保了合成的问答对具有新颖性、严格的质量控制和高度可扩展性，并据此创建了 KPMath 数据集，包含超过80万对数学推理问题-答案，以及进一步扩展的 KPMath-Plus 数据集。实验结果显示，在 KPMath-Plus 上微调 Qwen1.5-72B 模型后，其在 GSM8K 数据集上达到87.0% PASS@1 准确率，在 MATH 数据集上达到58.3%，超过了7B至70B范围的竞争模型和 GPT-4 在多个数学推理任务中的表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "In progress",
      "pdf_url": "http://arxiv.org/pdf/2403.02333v3",
      "published_date": "2024-03-04 18:58:30 UTC",
      "updated_date": "2024-05-08 01:48:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:12:33.542283"
    },
    {
      "arxiv_id": "2403.02327v2",
      "title": "Model Lakes",
      "title_zh": "模型湖",
      "authors": [
        "Koyena Pal",
        "David Bau",
        "Renée J. Miller"
      ],
      "abstract": "Given a set of deep learning models, it can be hard to find models\nappropriate to a task, understand the models, and characterize how models are\ndifferent one from another. Currently, practitioners rely on manually-written\ndocumentation to understand and choose models. However, not all models have\ncomplete and reliable documentation. As the number of models increases, the\nchallenges of finding, differentiating, and understanding models become\nincreasingly crucial. Inspired from research on data lakes, we introduce the\nconcept of model lakes. We formalize key model lake tasks, including model\nattribution, versioning, search, and benchmarking, and discuss fundamental\nresearch challenges in the management of large models. We also explore what\ndata management techniques can be brought to bear on the study of large model\nmanagement.",
      "tldr_zh": "该研究探讨了管理深度学习模型的挑战，包括难以找到合适模型、理解模型差异以及依赖不完整的手动文档。随着模型数量增加，这些问题变得更加突出。论文引入了“model lakes”概念，借鉴“data lakes”研究，形式化了关键任务如“model attribution”、版本控制（versioning）、搜索（search）和基准测试（benchmarking）。此外，它讨论了大型模型管理的根本研究挑战，并探讨了数据管理技术在这一领域的潜在应用。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted to EDBT 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.02327v2",
      "published_date": "2024-03-04 18:55:50 UTC",
      "updated_date": "2025-02-21 16:46:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:12:42.857758"
    },
    {
      "arxiv_id": "2403.02325v1",
      "title": "Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training",
      "title_zh": "翻译失败",
      "authors": [
        "David Wan",
        "Jaemin Cho",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Highlighting particularly relevant regions of an image can improve the\nperformance of vision-language models (VLMs) on various vision-language (VL)\ntasks by guiding the model to attend more closely to these regions of interest.\nFor example, VLMs can be given a \"visual prompt\", where visual markers such as\nbounding boxes delineate key image regions. However, current VLMs that can\nincorporate visual guidance are either proprietary and expensive or require\ncostly training on curated data that includes visual prompts. We introduce\nContrastive Region Guidance (CRG), a training-free guidance method that enables\nopen-source VLMs to respond to visual prompts. CRG contrasts model outputs\nproduced with and without visual prompts, factoring out biases revealed by the\nmodel when answering without the information required to produce a correct\nanswer (i.e., the model's prior). CRG achieves substantial improvements in a\nwide variety of VL tasks: When region annotations are provided, CRG increases\nabsolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse\nregion-based tasks such as recognition, math, and object relationship\nreasoning. We also show CRG's applicability to spatial reasoning, with 10%\nimprovement on What'sUp, as well as to compositional generalization --\nimproving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe\n-- and to image-text alignment for generated images, where we improve by up to\n8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG\nallows us to re-rank proposed regions in referring expression comprehension and\nphrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an\naverage gain of 3.2% in accuracy. Our analysis explores alternative masking\nstrategies for CRG, quantifies CRG's probability shift, and evaluates the role\nof region guidance strength, empirically validating CRG's design choices.",
      "tldr_zh": "本论文提出了一种无需训练的 Contrastive Region Guidance (CRG) 方法，用于提升开源 Vision-Language Models (VLMs) 在视觉语言任务中的区域引导性能。CRG 通过对比模型在带视觉提示（如边界框）和不带提示下的输出，消除模型的先验偏差，从而更好地关注图像关键区域。实验结果显示，CRG 在 ViP-Bench 等基准上使准确率提高高达 11.1%，并在空间推理（如 What'sUp，提升 10%）和组合泛化（如 SugarCrepe，提升 11.5% 和 7.5%）任务中表现出显著改善。论文还分析了 CRG 的替代策略和概率偏移，验证了其有效性和设计选择。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://contrastive-region-guidance.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2403.02325v1",
      "published_date": "2024-03-04 18:55:30 UTC",
      "updated_date": "2024-03-04 18:55:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:12:57.992850"
    },
    {
      "arxiv_id": "2403.02302v4",
      "title": "Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation",
      "title_zh": "超越专业化：",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have recently gained immense\npopularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as\nopen-source ones such as LLaVA, are essentially general-purpose models and are\napplied to solve a wide variety of tasks, including those in computer vision.\nThese neural networks possess such strong general knowledge and reasoning\nabilities that they have proven capable of working even on tasks for which they\nwere not specifically trained. We compared the capabilities of the most\npowerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task\nof age and gender estimation with our state-of-the-art specialized model,\nMiVOLO. We also updated MiVOLO and provide details and new metrics in this\narticle. This comparison has yielded some interesting results and insights\nabout the strengths and weaknesses of the participating models. Furthermore, we\nattempted various ways to fine-tune the ShareGPT4V model for this specific\ntask, aiming to achieve state-of-the-art results in this particular challenge.\nAlthough such a model would not be practical in production, as it is incredibly\nexpensive compared to a specialized model like MiVOLO, it could be very useful\nin some tasks, like data annotation.",
      "tldr_zh": "本文评估了 Multimodal Large Language Models (MLLMs) 如 ShareGPT4V、ChatGPT 和 LLaVA-Next 在年龄和性别估计任务上的能力，与专业模型 MiVOLO 进行比较。研究者更新了 MiVOLO 并引入新指标，揭示了这些通用模型在未专门训练任务中的优势和局限性。实验结果显示，MLLMs 表现出较强的泛化能力，但专业模型在准确性和效率上更胜一筹；此外，尝试微调 ShareGPT4V 虽可提升特定任务表现，但由于成本高昂，仅适用于如数据标注的辅助场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2.0; I.4.0; I.4.9"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02302v4",
      "published_date": "2024-03-04 18:32:12 UTC",
      "updated_date": "2025-01-21 14:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:13:08.354067"
    },
    {
      "arxiv_id": "2403.02372v1",
      "title": "OTClean: Data Cleaning for Conditional Independence Violations using Optimal Transport",
      "title_zh": "翻译失败",
      "authors": [
        "Alireza Pirhadi",
        "Mohammad Hossein Moslemi",
        "Alexander Cloninger",
        "Mostafa Milani",
        "Babak Salimi"
      ],
      "abstract": "Ensuring Conditional Independence (CI) constraints is pivotal for the\ndevelopment of fair and trustworthy machine learning models. In this paper, we\nintroduce \\sys, a framework that harnesses optimal transport theory for data\nrepair under CI constraints. Optimal transport theory provides a rigorous\nframework for measuring the discrepancy between probability distributions,\nthereby ensuring control over data utility. We formulate the data repair\nproblem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and\npropose an alternating method for its solution. However, this approach faces\nscalability issues due to the computational cost associated with computing\noptimal transport distances, such as the Wasserstein distance. To overcome\nthese scalability challenges, we reframe our problem as a regularized\noptimization problem, enabling us to develop an iterative algorithm inspired by\nSinkhorn's matrix scaling algorithm, which efficiently addresses\nhigh-dimensional and large-scale data. Through extensive experiments, we\ndemonstrate the efficacy and efficiency of our proposed methods, showcasing\ntheir practical utility in real-world data cleaning and preprocessing tasks.\nFurthermore, we provide comparisons with traditional approaches, highlighting\nthe superiority of our techniques in terms of preserving data utility while\nensuring adherence to the desired CI constraints.",
      "tldr_zh": "本论文引入了 OTClean 框架，利用 Optimal Transport 理论来修复数据中的 Conditional Independence (CI) 违反问题，从而提升机器学习模型的公平性和可信度。研究将数据修复问题表述为 Quadratically Constrained Linear Program (QCLP)，并提出交替方法来求解，但为解决计算开销和可扩展性问题，重构为正则化优化问题，并开发基于 Sinkhorn 算法的迭代算法，以高效处理高维大规模数据。通过广泛实验验证，OTClean 在保持数据效用同时确保 CI 约束的遵守，显著优于传统方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02372v1",
      "published_date": "2024-03-04 18:23:55 UTC",
      "updated_date": "2024-03-04 18:23:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:13:20.873780"
    },
    {
      "arxiv_id": "2403.02290v1",
      "title": "Koopman-Assisted Reinforcement Learning",
      "title_zh": "Koopman辅助强化学习",
      "authors": [
        "Preston Rozwood",
        "Edward Mehrez",
        "Ludger Paehler",
        "Wen Sun",
        "Steven L. Brunton"
      ],
      "abstract": "The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman\n(HJB) equation, are ubiquitous in reinforcement learning (RL) and control\ntheory. However, these equations quickly become intractable for systems with\nhigh-dimensional states and nonlinearity. This paper explores the connection\nbetween the data-driven Koopman operator and Markov Decision Processes (MDPs),\nresulting in the development of two new RL algorithms to address these\nlimitations. We leverage Koopman operator techniques to lift a nonlinear system\ninto new coordinates where the dynamics become approximately linear, and where\nHJB-based methods are more tractable. In particular, the Koopman operator is\nable to capture the expectation of the time evolution of the value function of\na given system via linear dynamics in the lifted coordinates. By parameterizing\nthe Koopman operator with the control actions, we construct a ``Koopman\ntensor'' that facilitates the estimation of the optimal value function. Then, a\ntransformation of Bellman's framework in terms of the Koopman tensor enables us\nto reformulate two max-entropy RL algorithms: soft value iteration and soft\nactor-critic (SAC). This highly flexible framework can be used for\ndeterministic or stochastic systems as well as for discrete or continuous-time\ndynamics. Finally, we show that these Koopman Assisted Reinforcement Learning\n(KARL) algorithms attain state-of-the-art (SOTA) performance with respect to\ntraditional neural network-based SAC and linear quadratic regulator (LQR)\nbaselines on four controlled dynamical systems: a linear state-space system,\nthe Lorenz system, fluid flow past a cylinder, and a double-well potential with\nnon-isotropic stochastic forcing.",
      "tldr_zh": "这篇论文探讨了 Bellman 方程和 Hamilton-Jacobi-Bellman (HJB) 方程在强化学习 (RL) 中处理高维非线性系统的挑战，通过引入 Koopman operator 与 Markov Decision Processes (MDPs) 的联系，开发了两个新算法。作者利用 Koopman operator 将非线性系统提升到近似线性坐标系，并构建 Koopman tensor 来估计最优价值函数，从而改进了软值迭代和软演员-评论家 (SAC) 算法，使其适用于确定性或随机系统，以及离散或连续时间动态。实验结果显示，这些 Koopman Assisted Reinforcement Learning (KARL) 算法在四个动态系统中（如 Lorenz 系统和流体流动）达到了 state-of-the-art (SOTA) 性能，优于传统 SAC 和线性二次调节器 (LQR) 基线。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.DS",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "35 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.02290v1",
      "published_date": "2024-03-04 18:19:48 UTC",
      "updated_date": "2024-03-04 18:19:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:13:35.129818"
    },
    {
      "arxiv_id": "2403.02268v1",
      "title": "Subjective $\\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Amanda Cercas Curry",
        "Gavin Abercrombie",
        "Zeerak Talat"
      ],
      "abstract": "Natural language processing research has begun to embrace the notion of\nannotator subjectivity, motivated by variations in labelling. This approach\nunderstands each annotator's view as valid, which can be highly suitable for\ntasks that embed subjectivity, e.g., sentiment analysis. However, this\nconstruction may be inappropriate for tasks such as hate speech detection, as\nit affords equal validity to all positions on e.g., sexism or racism. We argue\nthat the conflation of hate and offence can invalidate findings on hate speech,\nand call for future work to be situated in theory, disentangling hate from its\northogonal concept, offence.",
      "tldr_zh": "该论文探讨了在滥用语言检测中，将仇恨(hate speech)和冒犯(offence)混为一谈的潜在危险。作者批评了自然语言处理(NLP)研究中对标注者主观性(annotator subjectivity)的过度强调，因为这赋予了性别歧视(sexism)和种族歧视(racism)等歧视性观点同等有效性，从而可能使仇恨言论研究结果无效。论文呼吁未来工作需基于理论框架，将仇恨与冒犯这两个正交概念区分开来，以提升研究的准确性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02268v1",
      "published_date": "2024-03-04 17:56:28 UTC",
      "updated_date": "2024-03-04 17:56:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:13:44.644721"
    },
    {
      "arxiv_id": "2403.05589v4",
      "title": "Ergonomic Design of Computer Laboratory Furniture: Mismatch Analysis Utilizing Anthropometric Data of University Students",
      "title_zh": "翻译失败",
      "authors": [
        "Anik Kumar Saha",
        "Md Abrar Jahin",
        "Md. Rafiquzzaman",
        "M. F. Mridha"
      ],
      "abstract": "Many studies have shown how ergonomically designed furniture improves\nproductivity and well-being. As computers have become a part of students'\nacademic lives, they will grow further in the future. We propose\nanthropometric-based furniture dimensions suitable for university students to\nimprove computer laboratory ergonomics. We collected data from 380 participants\nand analyzed 11 anthropometric measurements, correlating them to 11 furniture\ndimensions. Two types of furniture were studied: a non-adjustable chair with a\nnon-adjustable table and an adjustable chair with a non-adjustable table. The\nmismatch calculation showed a significant difference between furniture\ndimensions and anthropometric measurements. The one-way ANOVA test with a\nsignificance level of 5% also showed a significant difference between proposed\nand existing furniture dimensions. The proposed dimensions were found to be\nmore compatible and reduced mismatch percentages for both males and females\ncompared to existing furniture. The proposed dimensions of the furniture set\nwith adjustable seat height showed slightly improved results compared to the\nnon-adjustable furniture set. This suggests that the proposed dimensions can\nimprove comfort levels and reduce the risk of musculoskeletal disorders among\nstudents. Further studies on the implementation and long-term effects of these\nproposed dimensions in real-world computer laboratory settings are recommended.",
      "tldr_zh": "本研究针对大学学生的计算机实验室家具设计，进行了基于anthropometric data的人体工程学（ergonomics）分析，以解决现有家具尺寸与人体测量间的mismatch问题。研究者收集了380名参与者的11个anthropometric measurements，并将其与11个家具维度相关联，比较了非可调椅子+非可调桌子和可调椅子+非可调桌子两种类型。结果显示，通过mismatch计算和单因素ANOVA测试（显著性水平5%），现有家具尺寸与人体数据存在显著差异，而提出的新尺寸显著降低了男性和女性的mismatch百分比，尤其在可调座高家具上效果更佳。该方法可提升学生舒适度并降低肌肉骨骼疾病风险，建议进一步研究其在实际环境中的实施和长期影响。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05589v4",
      "published_date": "2024-03-04 17:44:18 UTC",
      "updated_date": "2024-11-18 23:21:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:13:57.353598"
    },
    {
      "arxiv_id": "2403.02253v2",
      "title": "KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yuexin Li",
        "Chengyu Huang",
        "Shumin Deng",
        "Mei Lin Lock",
        "Tri Cao",
        "Nay Oo",
        "Hoon Wei Lim",
        "Bryan Hooi"
      ],
      "abstract": "Phishing attacks have inflicted substantial losses on individuals and\nbusinesses alike, necessitating the development of robust and efficient\nautomated phishing detection approaches. Reference-based phishing detectors\n(RBPDs), which compare the logos on a target webpage to a known set of logos,\nhave emerged as the state-of-the-art approach. However, a major limitation of\nexisting RBPDs is that they rely on a manually constructed brand knowledge\nbase, making it infeasible to scale to a large number of brands, which results\nin false negative errors due to the insufficient brand coverage of the\nknowledge base. To address this issue, we propose an automated knowledge\ncollection pipeline, using which we collect a large-scale multimodal brand\nknowledge base, KnowPhish, containing 20k brands with rich information about\neach brand. KnowPhish can be used to boost the performance of existing RBPDs in\na plug-and-play manner. A second limitation of existing RBPDs is that they\nsolely rely on the image modality, ignoring useful textual information present\nin the webpage HTML. To utilize this textual information, we propose a Large\nLanguage Model (LLM)-based approach to extract brand information of webpages\nfrom text. Our resulting multimodal phishing detection approach, KnowPhish\nDetector (KPD), can detect phishing webpages with or without logos. We evaluate\nKnowPhish and KPD on a manually validated dataset, and a field study under\nSingapore's local context, showing substantial improvements in effectiveness\nand efficiency compared to state-of-the-art baselines.",
      "tldr_zh": "本文提出 KnowPhish，一种结合 Large Language Models (LLMs) 和 Multimodal Knowledge Graphs 的框架，用于提升 Reference-based Phishing Detection (RBPDs)，以解决现有方法的知识库覆盖不足和仅依赖图像模态的问题。研究团队开发了一个自动知识收集管道，构建了包含 20k 品牌的 KnowPhish 知识库，并利用 LLMs 从网页 HTML 文本中提取品牌信息，从而实现多模态检测。实验结果显示，KnowPhish Detector (KPD) 在手动验证数据集和新加坡实地研究中，比现有基线方法显著提高了检测准确性和效率。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by USENIX Security 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02253v2",
      "published_date": "2024-03-04 17:38:32 UTC",
      "updated_date": "2024-06-15 11:34:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:14:09.143413"
    },
    {
      "arxiv_id": "2403.02249v2",
      "title": "Non-autoregressive Sequence-to-Sequence Vision-Language Models",
      "title_zh": "非自回归序列到序列视觉语言模型",
      "authors": [
        "Kunyu Shi",
        "Qi Dong",
        "Luis Goncalves",
        "Zhuowen Tu",
        "Stefano Soatto"
      ],
      "abstract": "Sequence-to-sequence vision-language models are showing promise, but their\napplicability is limited by their inference latency due to their autoregressive\nway of generating predictions. We propose a parallel decoding\nsequence-to-sequence vision-language model, trained with a Query-CTC loss, that\nmarginalizes over multiple inference paths in the decoder. This allows us to\nmodel the joint distribution of tokens, rather than restricting to conditional\ndistribution as in an autoregressive model. The resulting model, NARVL,\nachieves performance on-par with its state-of-the-art autoregressive\ncounterpart, but is faster at inference time, reducing from the linear\ncomplexity associated with the sequential generation of tokens to a paradigm of\nconstant time joint inference.",
      "tldr_zh": "这篇论文提出了NARVL，一种非自回归（Non-autoregressive）的Sequence-to-Sequence Vision-Language模型，旨在解决传统模型因自回归生成导致的推断延迟问题。模型通过Query-CTC loss训练，并对解码器中的多个推断路径进行边缘化，以建模tokens的联合分布，而非仅限于条件分布。实验结果显示，NARVL的性能与最先进的自回归模型相当，但在推断时显著更快，将线性复杂度降低为常量时间的联合推断。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02249v2",
      "published_date": "2024-03-04 17:34:59 UTC",
      "updated_date": "2025-03-13 00:22:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:14:21.469367"
    },
    {
      "arxiv_id": "2403.02243v1",
      "title": "Better Schedules for Low Precision Training of Deep Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Cameron R. Wolfe",
        "Anastasios Kyrillidis"
      ],
      "abstract": "Low precision training can significantly reduce the computational overhead of\ntraining deep neural networks (DNNs). Though many such techniques exist, cyclic\nprecision training (CPT), which dynamically adjusts precision throughout\ntraining according to a cyclic schedule, achieves particularly impressive\nimprovements in training efficiency, while actually improving DNN performance.\nExisting CPT implementations take common learning rate schedules (e.g.,\ncyclical cosine schedules) and use them for low precision training without\nadequate comparisons to alternative scheduling options. We define a diverse\nsuite of CPT schedules and analyze their performance across a variety of DNN\ntraining regimes, some of which are unexplored in the low precision training\nliterature (e.g., node classification with graph neural networks). From these\nexperiments, we discover alternative CPT schedules that offer further\nimprovements in training efficiency and model performance, as well as derive a\nset of best practices for choosing CPT schedules. Going further, we find that a\ncorrelation exists between model performance and training cost, and that\nchanging the underlying CPT schedule can control the tradeoff between these two\nvariables. To explain the direct correlation between model performance and\ntraining cost, we draw a connection between quantized training and critical\nlearning periods, suggesting that aggressive quantization is a form of learning\nimpairment that can permanently damage model performance.",
      "tldr_zh": "该论文探讨了低精度训练（Low precision training）在深度神经网络（DNNs）训练中的应用，重点优化循环精度训练（CPT）调度以提高效率和性能。研究者定义了一系列CPT调度，并通过实验在多种训练场景（如图神经网络的节点分类）中评估其效果，发现某些替代调度能进一步提升训练效率和模型准确率，并总结了选择CPT调度的最佳实践。论文还揭示了模型性能与训练成本之间的相关性，表明激进的量化训练可能作为一种学习损伤永久损害模型表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6; I.2.10; I.4.0"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 8 figures, 1 table, ACML 2023",
      "pdf_url": "http://arxiv.org/pdf/2403.02243v1",
      "published_date": "2024-03-04 17:33:39 UTC",
      "updated_date": "2024-03-04 17:33:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:14:34.386534"
    },
    {
      "arxiv_id": "2403.02241v3",
      "title": "Neural Redshift: Random Networks are not Random Functions",
      "title_zh": "Neural",
      "authors": [
        "Damien Teney",
        "Armand Nicolicioiu",
        "Valentin Hartmann",
        "Ehsan Abbasnejad"
      ],
      "abstract": "Our understanding of the generalization capabilities of neural networks (NNs)\nis still incomplete. Prevailing explanations are based on implicit biases of\ngradient descent (GD) but they cannot account for the capabilities of models\nfrom gradient-free methods nor the simplicity bias recently observed in\nuntrained networks. This paper seeks other sources of generalization in NNs.\n  Findings. To understand the inductive biases provided by architectures\nindependently from GD, we examine untrained, random-weight networks. Even\nsimple MLPs show strong inductive biases: uniform sampling in weight space\nyields a very biased distribution of functions in terms of complexity. But\nunlike common wisdom, NNs do not have an inherent \"simplicity bias\". This\nproperty depends on components such as ReLUs, residual connections, and layer\nnormalizations. Alternative architectures can be built with a bias for any\nlevel of complexity. Transformers also inherit all these properties from their\nbuilding blocks.\n  Implications. We provide a fresh explanation for the success of deep learning\nindependent from gradient-based training. It points at promising avenues for\ncontrolling the solutions implemented by trained models.",
      "tldr_zh": "这篇论文“Neural Redshift: Random Networks are not Random Functions”揭示了神经网络(NNs)的泛化能力源于架构本身的归纳偏差，而非仅依赖梯度下降(GD)。研究通过分析未训练的随机权重网络（如简单多层感知机(MLPs)）发现，这些网络在权重空间的均匀采样下表现出复杂度的强烈偏差，但这种偏差取决于特定组件，如 ReLU、残差连接和层归一化。Transformers 也继承这些特性，为深度学习成功提供独立于梯度训练的解释，并为控制训练模型输出开辟新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02241v3",
      "published_date": "2024-03-04 17:33:20 UTC",
      "updated_date": "2025-04-29 19:25:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:14:45.967062"
    },
    {
      "arxiv_id": "2403.02238v2",
      "title": "Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitrios Michael Manias",
        "Ali Chouman",
        "Abdallah Shami"
      ],
      "abstract": "The integration of Machine Learning and Artificial Intelligence (ML/AI) into\nfifth-generation (5G) networks has made evident the limitations of network\nintelligence with ever-increasing, strenuous requirements for current and\nnext-generation devices. This transition to ubiquitous intelligence demands\nhigh connectivity, synchronicity, and end-to-end communication between users\nand network operators, and will pave the way towards full network automation\nwithout human intervention. Intent-based networking is a key factor in the\nreduction of human actions, roles, and responsibilities while shifting towards\nnovel extraction and interpretation of automated network management. This paper\npresents the development of a custom Large Language Model (LLM) for 5G and\nnext-generation intent-based networking and provides insights into future LLM\ndevelopments and integrations to realize end-to-end intent-based networking for\nfully automated network intelligence.",
      "tldr_zh": "本论文探讨了将机器学习和人工智能（ML/AI）整合到第五代（5G）网络中的挑战，强调了实现高连接性、同期性和端到端通信以推动全自动化网络智能的需求。作者提出开发一个自定义Large Language Model (LLM)，用于从5G核心网络中提取意图，从而减少人为干预并实现Intent-Based Networking。实验和见解表明，这种方法有助于构建端到端的意图驱动网络管理，为未来LLM在自动化网络智能中的应用奠定基础。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "Presented at DRCN 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02238v2",
      "published_date": "2024-03-04 17:29:57 UTC",
      "updated_date": "2024-05-22 13:34:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:14:57.510033"
    },
    {
      "arxiv_id": "2403.02232v2",
      "title": "Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenglin Li",
        "Haibei Zhu",
        "Houze Liu",
        "Jintong Song",
        "Qishuo Cheng"
      ],
      "abstract": "This study conducts a thorough examination of malware detection using machine\nlearning techniques, focusing on the evaluation of various classification\nmodels using the Mal-API-2019 dataset. The aim is to advance cybersecurity\ncapabilities by identifying and mitigating threats more effectively. Both\nensemble and non-ensemble machine learning methods, such as Random Forest,\nXGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special\nemphasis is placed on the importance of data pre-processing techniques,\nparticularly TF-IDF representation and Principal Component Analysis, in\nimproving model performance. Results indicate that ensemble methods,\nparticularly Random Forest and XGBoost, exhibit superior accuracy, precision,\nand recall compared to others, highlighting their effectiveness in malware\ndetection. The paper also discusses limitations and potential future\ndirections, emphasizing the need for continuous adaptation to address the\nevolving nature of malware. This research contributes to ongoing discussions in\ncybersecurity and provides practical insights for developing more robust\nmalware detection systems in the digital era.",
      "tldr_zh": "这篇论文对 Mal-API-2019 数据集进行了全面评估，使用 machine learning 技术来提升恶意软件检测能力。研究探索了多种分类模型，包括 ensemble 方法（如 Random Forest 和 XGBoost）以及非-ensemble 方法（如 KNN 和 Neural Networks），并强调了数据预处理技术如 TF-IDF 和 Principal Component Analysis 在提高模型性能方面的作用。结果表明，Random Forest 和 XGBoost 等集成方法在准确率、精确率和召回率上表现出色，优于其他模型。该研究为网络安全领域提供了实用洞见，并讨论了潜在限制及未来方向，以应对恶意软件的持续演变。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02232v2",
      "published_date": "2024-03-04 17:22:43 UTC",
      "updated_date": "2024-03-25 21:33:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:15:11.322722"
    },
    {
      "arxiv_id": "2403.02227v2",
      "title": "Policy Space Response Oracles: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Ariyan Bighashdel",
        "Yongzhao Wang",
        "Stephen McAleer",
        "Rahul Savani",
        "Frans A. Oliehoek"
      ],
      "abstract": "Game theory provides a mathematical way to study the interaction between\nmultiple decision makers. However, classical game-theoretic analysis is limited\nin scalability due to the large number of strategies, precluding direct\napplication to more complex scenarios. This survey provides a comprehensive\noverview of a framework for large games, known as Policy Space Response Oracles\n(PSRO), which holds promise to improve scalability by focusing attention on\nsufficient subsets of strategies. We first motivate PSRO and provide historical\ncontext. We then focus on the strategy exploration problem for PSRO: the\nchallenge of assembling effective subsets of strategies that still represent\nthe original game well with minimum computational cost. We survey current\nresearch directions for enhancing the efficiency of PSRO, and explore the\napplications of PSRO across various domains. We conclude by discussing open\nquestions and future research.",
      "tldr_zh": "这篇调查论文综述了Policy Space Response Oracles (PSRO)框架，该框架通过聚焦于策略的足够子集来提升游戏理论分析的可扩展性，从而适用于大规模多决策者互动场景。论文首先阐述了PSRO的动机和历史背景，然后深入探讨策略探索问题，即如何以最低计算成本组装有效策略子集，同时调研了提升PSRO效率的当前研究方向及其在各种领域的应用。最终，论文指出了开放问题和未来研究方向，为扩展游戏理论的应用奠定基础。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "Ariyan Bighashdel and Yongzhao Wang contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2403.02227v2",
      "published_date": "2024-03-04 17:15:09 UTC",
      "updated_date": "2024-05-27 16:49:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:15:21.434118"
    },
    {
      "arxiv_id": "2403.02181v3",
      "title": "Not All Layers of LLMs Are Necessary During Inference",
      "title_zh": "并非所有大型语言模型的层在推理过程中都是必要的",
      "authors": [
        "Siqi Fan",
        "Xin Jiang",
        "Xiang Li",
        "Xuying Meng",
        "Peng Han",
        "Shuo Shang",
        "Aixin Sun",
        "Yequan Wang",
        "Zhongyuan Wang"
      ],
      "abstract": "Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. However, not all requests posed to LLMs\nare equally difficult to handle. Through analysis, we show that for some tasks,\nLLMs can achieve results comparable to the final output at some intermediate\nlayers. That is, not all layers of LLMs are necessary during inference. If we\ncan predict at which layer the inferred results match the final results\n(produced by evaluating all layers), we could significantly reduce the\ninference cost. To this end, we propose a simple yet effective algorithm named\nAdaInfer to adaptively terminate the inference process for an input instance.\nAdaInfer relies on easily obtainable statistical features and classic\nclassifiers like SVM. Experiments on well-known LLMs like the Llama2 series and\nOPT, show that AdaInfer can achieve an average of 17.8% pruning ratio, and up\nto 43% on sentiment tasks, with nearly no performance drop (<1%). Because\nAdaInfer does not alter LLM parameters, the LLMs incorporated with AdaInfer\nmaintain generalizability across tasks.",
      "tldr_zh": "该研究发现，大型语言模型（LLMs）的推理过程并非所有层都必需，因为某些任务在中间层即可产生与最终输出相当的结果，从而减少资源消耗。作者提出AdaInfer算法，使用统计特征和经典分类器如SVM来预测输入实例的推理终止点，实现自适应推理。实验在Llama2系列和OPT等模型上显示，AdaInfer平均实现了17.8%的层剪枝比例，并在情感任务上高达43%，而性能下降不到1%，并保持了LLMs在不同任务上的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02181v3",
      "published_date": "2024-03-04 16:23:58 UTC",
      "updated_date": "2024-07-09 11:59:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:15:33.955837"
    },
    {
      "arxiv_id": "2403.02178v2",
      "title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models",
      "title_zh": "Masked Thought：简单地掩码部分推理步骤可以改善",
      "authors": [
        "Changyu Chen",
        "Xiting Wang",
        "Ting-En Lin",
        "Ang Lv",
        "Yuchuan Wu",
        "Xin Gao",
        "Ji-Rong Wen",
        "Rui Yan",
        "Yongbin Li"
      ],
      "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results,\nleading to suboptimal performance of large language models in such domains.\nEarlier fine-tuning approaches sought to mitigate this by leveraging more\nprecise supervisory signals from human labeling, larger models, or\nself-sampling, although at a high cost. Conversely, we develop a method that\navoids external resources, relying instead on introducing perturbations to the\ninput. Our training approach randomly masks certain tokens within the chain of\nthought, a technique we found to be particularly effective for reasoning tasks.\nWhen applied to fine-tuning with GSM8K on Llama-2-7B, this method achieved a\n5\\% improvement in GSM8K accuracy and a 10\\% improvement in GSM-IC accuracy\nover standard supervised fine-tuning with a few codes modified. Furthermore, it\nis complementary to existing methods. When integrated with related explicit\ndata augmentation methods, it leads to improvements across five datasets of\nvarious augmentation methods, as well as two different base models. We further\ninvestigate the mechanisms behind this improvement through case studies and\nquantitative analysis, suggesting that our approach may provide superior\nsupport for the model in capturing long-distance dependencies, especially those\nrelated to questions. This enhancement could deepen understanding of the\npremises in questions and prior steps. Our code is available at Github.",
      "tldr_zh": "该研究提出了一种名为 Masked Thought 的简单方法，通过在 chain of thought 推理链中随机掩码部分 tokens，来提升语言模型在数学推理任务中的学习能力，而无需外部资源。相比标准监督微调，该方法在 Llama-2-7B 模型上应用于 GSM8K 数据集时，准确率提升了 5%，在 GSM-IC 数据集上提升了 10%。此外，该方法与现有数据增强技术互补，在五个数据集和两种基模型上实现了进一步改进，并通过案例研究和定量分析显示，它有助于模型更好地捕捉长距离依赖，从而加深对问题和先前步骤的理解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02178v2",
      "published_date": "2024-03-04 16:21:54 UTC",
      "updated_date": "2024-07-10 19:15:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:15:47.108131"
    },
    {
      "arxiv_id": "2403.04795v1",
      "title": "Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Haley Hostetter",
        "M. Z. Naser",
        "Xinyan Huang",
        "John Gales"
      ],
      "abstract": "This communication presents preliminary findings from comparing two recent\nchatbots, OpenAI's ChatGPT and Google's Bard, in the context of fire\nengineering by evaluating their responses in handling fire safety related\nqueries. A diverse range of fire engineering questions and scenarios were\ncreated and examined, including structural fire design, fire prevention\nstrategies, evacuation, building code compliance, and fire suppression systems\n(some of which resemble those commonly present in the Fire Protection exam\n(FPE)). The results reveal some key differences in the performance of the\nchatbots, with ChatGPT demonstrating a relatively superior performance. Then,\nthis communication highlights the potential for chatbot technology to\nrevolutionize fire engineering practices by providing instant access to\ncritical information while outlining areas for further improvement and\nresearch. Evidently, and when it matures, this technology will likely be\nelemental to our engineers' practice and education.",
      "tldr_zh": "本研究评估了大型语言模型（Large Language Models）ChatGPT和Bard在消防工程领域的表现，通过测试它们对结构防火设计、防火策略、疏散、建筑规范合规和灭火系统的响应。结果显示，ChatGPT在处理这些技术查询时表现出色，整体性能优于Bard。论文强调，聊天机器人技术有潜力革新消防工程实践，提供即时信息访问，但需进一步改进和研究，最终可能成为工程师教育和实践的核心工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04795v1",
      "published_date": "2024-03-04 16:18:36 UTC",
      "updated_date": "2024-03-04 16:18:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:15:57.281160"
    },
    {
      "arxiv_id": "2403.02371v3",
      "title": "NeuroVoz: a Castillian Spanish corpus of parkinsonian speech",
      "title_zh": "翻译失败",
      "authors": [
        "Janaína Mendes-Laureano",
        "Jorge A. Gómez-García",
        "Alejandro Guerrero-López",
        "Elisa Luque-Buzo",
        "Julián D. Arias-Londoño",
        "Francisco J. Grandas-Pérez",
        "Juan I. Godino-Llorente"
      ],
      "abstract": "The screening of Parkinson's Disease (PD) through speech is hindered by a\nnotable lack of publicly available datasets in different languages. This fact\nlimits the reproducibility and further exploration of existing research.\n  To address this gap, this manuscript presents the NeuroVoz corpus consisting\nof 112 native Castilian-Spanish speakers, including 58 healthy controls and 54\nindividuals with PD, all recorded in ON state. The corpus showcases a diverse\narray of speech tasks: sustained vowels; diadochokinetic tests; 16\nListen-and-Repeat utterances; and spontaneous monologues.\n  The dataset is also complemented with subjective assessments of voice quality\nperformed by an expert according to the GRBAS scale\n(Grade/Roughness/Breathiness/Asthenia/Strain), as well as annotations with a\nthorough examination of phonation quality, intensity, speed, resonance,\nintelligibility, and prosody.\n  The corpus offers a substantial resource for the exploration of the impact of\nPD on speech. This data set has already supported several studies, achieving a\nbenchmark accuracy of 89% for the screening of PD. Despite these advances, the\nbroader challenge of conducting a language-agnostic, cross-corpora analysis of\nParkinsonian speech patterns remains open.",
      "tldr_zh": "本研究介绍了NeuroVoz语料库，这是一个针对帕金森病(PD)语音的公开数据集，包含112名本土Castilian-Spanish说话者，包括58名健康对照组和54名PD患者（在ON状态下录制）。语料库涵盖多种语音任务，如持续元音、diadochokinetic tests、16个Listen-and-Repeat语句以及自发独白，并辅以专家根据GRBAS scale进行的语音质量评估，以及对发音质量、强度、速度、共振、可懂度和韵律的详细注释。数据集已支持多项研究，在PD筛查中实现了89%的基准准确率，为探索PD对语音的影响提供了宝贵资源。尽管取得了进展，进行语言无关的跨语料库分析帕金森语音模式仍面临挑战。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Paper accepted at Scientific Data",
      "pdf_url": "http://arxiv.org/pdf/2403.02371v3",
      "published_date": "2024-03-04 16:17:39 UTC",
      "updated_date": "2025-02-26 15:42:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:16:11.489070"
    },
    {
      "arxiv_id": "2403.02167v3",
      "title": "EMOVOME: A Dataset for Emotion Recognition in Spontaneous Real-Life Speech",
      "title_zh": "EMOVOME：一种用于自发真实生活语音的情感识别数据集",
      "authors": [
        "Lucía Gómez-Zaragozá",
        "Rocío del Amor",
        "María José Castro-Bleda",
        "Valery Naranjo",
        "Mariano Alcañiz Raya",
        "Javier Marín-Morales"
      ],
      "abstract": "Spontaneous datasets for Speech Emotion Recognition (SER) are scarce and\nfrequently derived from laboratory environments or staged scenarios, such as TV\nshows, limiting their application in real-world contexts. We developed and\npublicly released the Emotional Voice Messages (EMOVOME) dataset, including 999\nvoice messages from real conversations of 100 Spanish speakers on a messaging\napp, labeled in continuous and discrete emotions by expert and non-expert\nannotators. We evaluated speaker-independent SER models using acoustic features\nas baseline and transformer-based models. We compared the results with\nreference datasets including acted and elicited speech, and analyzed the\ninfluence of annotators and gender fairness. The pre-trained\nUniSpeech-SAT-Large model achieved the highest results, 61.64% and 55.57%\nUnweighted Accuracy (UA) for 3-class valence and arousal prediction\nrespectively on EMOVOME, a 10% improvement over baseline models. For the\nemotion categories, 42.58% UA was obtained. EMOVOME performed lower than the\nacted RAVDESS dataset. The elicited IEMOCAP dataset also outperformed EMOVOME\nin predicting emotion categories, while similar results were obtained in\nvalence and arousal. EMOVOME outcomes varied with annotator labels, showing\nbetter results and fairness when combining expert and non-expert annotations.\nThis study highlights the gap between controlled and real-life scenarios,\nsupporting further advancements in recognizing genuine emotions.",
      "tldr_zh": "本文开发并公开了 EMOVOME 数据集，这是一个包含 999 条真实生活自发语音消息的 Speech Emotion Recognition (SER) 资源，由 100 名西班牙语说话者提供，并由专家和非专家标注了连续和离散情绪。研究评估了基于声学特征的基线模型和 Transformer 模型，如 UniSpeech-SAT-Large，在 EMOVOME 上取得了 61.64% 和 55.57% Unweighted Accuracy (UA) 在 3 类 valence 和 arousal 预测上的最佳表现，比基线提高了 10%。与 acted 数据集如 RAVDESS 相比，EMOVOME 的性能较低，但在 valence 和 arousal 上与 elicited 数据集 IEMOCAP 类似；此外，结合专家和非专家标注能提升结果准确性和性别公平性。该研究强调了真实场景与控制环境间的差距，推动了 SER 在实际应用中的进步。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD",
        "I.5.1; I.5.4"
      ],
      "primary_category": "eess.AS",
      "comment": "This article is a merged version of the description of the EMOVOME\n  database in arXiv:2402.17496v1 and the speech emotion recognition models in\n  arXiv:2403.02167v1. This work has been submitted to the IEEE for possible\n  publication",
      "pdf_url": "http://arxiv.org/pdf/2403.02167v3",
      "published_date": "2024-03-04 16:13:39 UTC",
      "updated_date": "2024-12-04 02:08:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:16:25.302706"
    },
    {
      "arxiv_id": "2403.02164v2",
      "title": "Cognition is All You Need -- The Next Layer of AI Above Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Nova Spivack",
        "Sam Douglas",
        "Michelle Crames",
        "Tim Connors"
      ],
      "abstract": "Recent studies of the applications of conversational AI tools, such as\nchatbots powered by large language models, to complex real-world knowledge work\nhave shown limitations related to reasoning and multi-step problem solving.\nSpecifically, while existing chatbots simulate shallow reasoning and\nunderstanding they are prone to errors as problem complexity increases. The\nfailure of these systems to address complex knowledge work is due to the fact\nthat they do not perform any actual cognition. In this position paper, we\npresent Cognitive AI, a higher-level framework for implementing\nprogrammatically defined neuro-symbolic cognition above and outside of large\nlanguage models. Specifically, we propose a dual-layer functional architecture\nfor Cognitive AI that serves as a roadmap for AI systems that can perform\ncomplex multi-step knowledge work. We propose that Cognitive AI is a necessary\nprecursor for the evolution of higher forms of AI, such as AGI, and\nspecifically claim that AGI cannot be achieved by probabilistic approaches on\ntheir own. We conclude with a discussion of the implications for large language\nmodels, adoption cycles in AI, and commercial Cognitive AI development.",
      "tldr_zh": "最近的研究表明，大型语言模型(LLMs)驱动的聊天机器人在处理复杂真实世界知识工作时，存在推理和多步问题解决的局限性，因为它们仅模拟浅层理解而非实际认知，导致错误率增加。论文提出 Cognitive AI 框架，这是一个在 LLMs 之上构建的程序定义神经符号认知系统，采用双层功能架构作为路线图，以支持复杂多步知识工作。作者主张，Cognitive AI 是实现通用人工智能(AGI)的必要前驱，且仅靠概率方法无法达到 AGI，并讨论了其对 LLMs 的启示、AI 采用周期以及商业开发潜力。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "63 pages, 18 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.02164v2",
      "published_date": "2024-03-04 16:11:57 UTC",
      "updated_date": "2024-03-05 10:23:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:16:39.152916"
    },
    {
      "arxiv_id": "2403.02131v3",
      "title": "Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution",
      "title_zh": "深度强化学习用于动态算法选择：Differential Evolution 的原理证明研究",
      "authors": [
        "Hongshu Guo",
        "Yining Ma",
        "Zeyuan Ma",
        "Jiacheng Chen",
        "Xinglin Zhang",
        "Zhiguang Cao",
        "Jun Zhang",
        "Yue-Jiao Gong"
      ],
      "abstract": "Evolutionary algorithms, such as Differential Evolution, excel in solving\nreal-parameter optimization challenges. However, the effectiveness of a single\nalgorithm varies across different problem instances, necessitating considerable\nefforts in algorithm selection or configuration. This paper aims to address the\nlimitation by leveraging the complementary strengths of a group of algorithms\nand dynamically scheduling them throughout the optimization progress for\nspecific problems. We propose a deep reinforcement learning-based dynamic\nalgorithm selection framework to accomplish this task. Our approach models the\ndynamic algorithm selection a Markov Decision Process, training an agent in a\npolicy gradient manner to select the most suitable algorithm according to the\nfeatures observed during the optimization process. To empower the agent with\nthe necessary information, our framework incorporates a thoughtful design of\nlandscape and algorithmic features. Meanwhile, we employ a sophisticated deep\nneural network model to infer the optimal action, ensuring informed algorithm\nselections. Additionally, an algorithm context restoration mechanism is\nembedded to facilitate smooth switching among different algorithms. These\nmechanisms together enable our framework to seamlessly select and switch\nalgorithms in a dynamic online fashion. Notably, the proposed framework is\nsimple and generic, offering potential improvements across a broad spectrum of\nevolutionary algorithms. As a proof-of-principle study, we apply this framework\nto a group of Differential Evolution algorithms. The experimental results\nshowcase the remarkable effectiveness of the proposed framework, not only\nenhancing the overall optimization performance but also demonstrating favorable\ngeneralization ability across different problem classes.",
      "tldr_zh": "这篇论文提出了一种基于Deep Reinforcement Learning的动态算法选择框架，旨在解决进化算法如Differential Evolution在不同问题实例上的变异性问题，通过动态调度一组算法来优化特定任务。该框架将算法选择建模为Markov Decision Process，并使用策略梯度方法训练代理，根据优化过程中的landscape和algorithmic features进行智能决策，同时嵌入算法上下文恢复机制以实现平滑切换。实验结果显示，在Differential Evolution算法组上应用该框架后，优化性能显著提升，并展示了良好的泛化能力。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted by IEEE Transactions on Systems, Man, and Cybernetics:\n  Systems at Thu, Feb 29, 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02131v3",
      "published_date": "2024-03-04 15:40:28 UTC",
      "updated_date": "2024-03-07 09:42:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:16:49.907832"
    },
    {
      "arxiv_id": "2403.02127v1",
      "title": "LOCR: Location-Guided Transformer for Optical Character Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Sun",
        "Dongzhan Zhou",
        "Chen Lin",
        "Conghui He",
        "Wanli Ouyang",
        "Han-Sen Zhong"
      ],
      "abstract": "Academic documents are packed with texts, equations, tables, and figures,\nrequiring comprehensive understanding for accurate Optical Character\nRecognition (OCR). While end-to-end OCR methods offer improved accuracy over\nlayout-based approaches, they often grapple with significant repetition issues,\nespecially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this\nissue, we propose LOCR, a model that integrates location guiding into the\ntransformer architecture during autoregression. We train the model on a dataset\ncomprising over 77M text-location pairs from 125K academic document pages,\nincluding bounding boxes for words, tables and mathematical symbols. LOCR\nadeptly handles various formatting elements and generates content in Markdown\nlanguage. It outperforms all existing methods in our test set constructed from\narXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also\nreduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset,\nfrom 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in\nOOD marketing documents. Additionally, LOCR features an interactive OCR mode,\nfacilitating the generation of complex documents through a few location prompts\nfrom human.",
      "tldr_zh": "本论文提出LOCR，一种位置引导的Transformer模型，用于提升Optical Character Recognition (OCR) 在学术文档中的准确性，针对复杂布局和Out-Of-Domain (OOD) 文档的重复问题。LOCR 通过在自回归过程中集成位置引导机制，并在超过77M文本-位置对的训练数据集上训练（包括单词、表格和数学符号的边界框），能够处理各种格式元素并生成Markdown语言输出。实验结果显示，LOCR 在arXiv测试集上优于现有方法，在编辑距离、BLEU、METEOR和F-measure指标上表现突出，并将重复频率显著降低（如arXiv数据集从4.4%降至0.5%）。此外，该模型支持交互式OCR模式，通过少量位置提示实现复杂文档的生成。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02127v1",
      "published_date": "2024-03-04 15:34:12 UTC",
      "updated_date": "2024-03-04 15:34:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:17:03.342291"
    },
    {
      "arxiv_id": "2403.03230v4",
      "title": "Large language models surpass human experts in predicting neuroscience results",
      "title_zh": "大型语言模型在预测神经科学结果方面优于人类专家",
      "authors": [
        "Xiaoliang Luo",
        "Akilles Rechardt",
        "Guangzhi Sun",
        "Kevin K. Nejad",
        "Felipe Yáñez",
        "Bati Yilmaz",
        "Kangjoo Lee",
        "Alexandra O. Cohen",
        "Valentina Borghesani",
        "Anton Pashkov",
        "Daniele Marinazzo",
        "Jonathan Nicholas",
        "Alessandro Salatiello",
        "Ilia Sucholutsky",
        "Pasquale Minervini",
        "Sepehr Razavi",
        "Roberta Rocca",
        "Elkhan Yusifov",
        "Tereza Okalova",
        "Nianlong Gu",
        "Martin Ferianc",
        "Mikail Khona",
        "Kaustubh R. Patil",
        "Pui-Shee Lee",
        "Rui Mata",
        "Nicholas E. Myers",
        "Jennifer K Bizley",
        "Sebastian Musslick",
        "Isil Poyraz Bilgin",
        "Guiomar Niso",
        "Justin M. Ales",
        "Michael Gaebler",
        "N Apurva Ratan Murty",
        "Leyla Loued-Khenissi",
        "Anna Behler",
        "Chloe M. Hall",
        "Jessica Dafflon",
        "Sherry Dongqi Bao",
        "Bradley C. Love"
      ],
      "abstract": "Scientific discoveries often hinge on synthesizing decades of research, a\ntask that potentially outstrips human information processing capacities. Large\nlanguage models (LLMs) offer a solution. LLMs trained on the vast scientific\nliterature could potentially integrate noisy yet interrelated findings to\nforecast novel results better than human experts. To evaluate this possibility,\nwe created BrainBench, a forward-looking benchmark for predicting neuroscience\nresults. We find that LLMs surpass experts in predicting experimental outcomes.\nBrainGPT, an LLM we tuned on the neuroscience literature, performed better yet.\nLike human experts, when LLMs were confident in their predictions, they were\nmore likely to be correct, which presages a future where humans and LLMs team\ntogether to make discoveries. Our approach is not neuroscience-specific and is\ntransferable to other knowledge-intensive endeavors.",
      "tldr_zh": "本研究发现，大型语言模型（LLMs）在预测神经科学实验结果方面超过了人类专家，因为LLMs能更好地整合庞大科学文献。研究团队开发了BrainBench基准测试，并调整了BrainGPT模型，使其在神经科学预测中表现出色。结果显示，BrainGPT的性能优于标准LLMs，且当LLMs对预测有信心时，准确率更高，这种方法可扩展到其他知识密集型领域，并为未来人类与LLMs的合作探索铺平道路。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "The latest version of this paper has been published at Nature Human\n  Behaviour, please see https://www.nature.com/articles/s41562-024-02046-9",
      "pdf_url": "http://arxiv.org/pdf/2403.03230v4",
      "published_date": "2024-03-04 15:27:59 UTC",
      "updated_date": "2024-11-28 08:49:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:17:13.359907"
    },
    {
      "arxiv_id": "2403.02121v1",
      "title": "Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sargam Yadav",
        "Abhishek Kaushik",
        "Kevin McDaid"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has advanced the benchmark in\nvarious Natural Language Processing (NLP) tasks. However, large amounts of\nlabelled training data are required to train LLMs. Furthermore, data annotation\nand training are computationally expensive and time-consuming. Zero and\nfew-shot learning have recently emerged as viable options for labelling data\nusing large pre-trained models. Hate speech detection in mix-code low-resource\nlanguages is an active problem area where the use of LLMs has proven\nbeneficial. In this study, we have compiled a dataset of 100 YouTube comments,\nand weakly labelled them for coarse and fine-grained misogyny classification in\nmix-code Hinglish. Weak annotation was applied due to the labor-intensive\nannotation process. Zero-shot learning, one-shot learning, and few-shot\nlearning and prompting approaches have then been applied to assign labels to\nthe comments and compare them to human-assigned labels. Out of all the\napproaches, zero-shot classification using the Bidirectional Auto-Regressive\nTransformers (BART) large model and few-shot prompting using Generative\nPre-trained Transformer- 3 (ChatGPT-3) achieve the best results",
      "tldr_zh": "本研究探讨了利用弱标注数据和大型语言模型（LLMs）的转移学习方法，来检测混合代码 Hinglish 中的仇恨言论。研究者编译了一个包含100条YouTube评论的数据集，并采用弱标注进行粗粒度和细粒度厌女症分类，以减少标注工作量。随后，通过零样本学习、一样本学习和少样本学习及提示方法（如使用Bidirectional Auto-Regressive Transformers (BART) large模型和Generative Pre-trained Transformer-3 (ChatGPT-3)）对数据进行标签分配，并与人工标签比较。结果显示，BART large的零样本分类和ChatGPT-3的少样本提示方法取得了最佳性能，为低资源语言的仇恨言论检测提供了可行性驱动的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper is accepted in the 16th ISDSI-Global Conference 2023\n  https://isdsi2023.iimranchi.ac.in",
      "pdf_url": "http://arxiv.org/pdf/2403.02121v1",
      "published_date": "2024-03-04 15:27:49 UTC",
      "updated_date": "2024-03-04 15:27:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:17:25.820220"
    },
    {
      "arxiv_id": "2403.02118v4",
      "title": "Position: Towards Implicit Prompt For Text-To-Image Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Yang",
        "Yuqi Lin",
        "Hong Liu",
        "Wenqi Shao",
        "Runjian Chen",
        "Hailong Shang",
        "Yu Wang",
        "Yu Qiao",
        "Kaipeng Zhang",
        "Ping Luo"
      ],
      "abstract": "Recent text-to-image (T2I) models have had great success, and many benchmarks\nhave been proposed to evaluate their performance and safety. However, they only\nconsider explicit prompts while neglecting implicit prompts (hint at a target\nwithout explicitly mentioning it). These prompts may get rid of safety\nconstraints and pose potential threats to the applications of these models.\nThis position paper highlights the current state of T2I models toward implicit\nprompts. We present a benchmark named ImplicitBench and conduct an\ninvestigation on the performance and impacts of implicit prompts with popular\nT2I models. Specifically, we design and collect more than 2,000 implicit\nprompts of three aspects: General Symbols, Celebrity Privacy, and\nNot-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models'\ncapabilities under these implicit prompts. Experiment results show that (1) T2I\nmodels are able to accurately create various target symbols indicated by\nimplicit prompts; (2) Implicit prompts bring potential risks of privacy leakage\nfor T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can\nbe bypassed with implicit prompts. We call for increased attention to the\npotential and risks of implicit prompts in the T2I community and further\ninvestigation into the capabilities and impacts of implicit prompts, advocating\nfor a balanced approach that harnesses their benefits while mitigating their\nrisks.",
      "tldr_zh": "这篇论文探讨了文本到图像 (T2I) 模型对隐式提示的响应问题，这些提示暗示目标而不直接提及，可能绕过安全机制并带来风险。作者提出了一个名为 ImplicitBench 的基准，并收集了超过 2000 个隐式提示，涵盖一般符号、名人隐私和 NSFW 问题，以评估六种流行 T2I 模型的表现。实验结果显示，T2I 模型能准确生成隐式提示指示的符号，但也暴露了潜在风险，如隐私泄露和绕过 NSFW 约束。论文呼吁 T2I 社区加强对隐式提示的关注，进行进一步研究，以平衡其益处和风险。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02118v4",
      "published_date": "2024-03-04 15:21:51 UTC",
      "updated_date": "2024-05-28 04:24:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:17:38.255067"
    },
    {
      "arxiv_id": "2403.02107v6",
      "title": "Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Théo Vincent",
        "Daniel Palenicek",
        "Boris Belousov",
        "Jan Peters",
        "Carlo D'Eramo"
      ],
      "abstract": "The vast majority of Reinforcement Learning methods is largely impacted by\nthe computation effort and data requirements needed to obtain effective\nestimates of action-value functions, which in turn determine the quality of the\noverall performance and the sample-efficiency of the learning procedure.\nTypically, action-value functions are estimated through an iterative scheme\nthat alternates the application of an empirical approximation of the Bellman\noperator and a subsequent projection step onto a considered function space. It\nhas been observed that this scheme can be potentially generalized to carry out\nmultiple iterations of the Bellman operator at once, benefiting the underlying\nlearning algorithm. However, till now, it has been challenging to effectively\nimplement this idea, especially in high-dimensional problems. In this paper, we\nintroduce iterated $Q$-Network (i-QN), a novel principled approach that enables\nmultiple consecutive Bellman updates by learning a tailored sequence of\naction-value functions where each serves as the target for the next. We show\nthat i-QN is theoretically grounded and that it can be seamlessly used in\nvalue-based and actor-critic methods. We empirically demonstrate the advantages\nof i-QN in Atari $2600$ games and MuJoCo continuous control problems.",
      "tldr_zh": "本文提出 Iterated $Q$-Network (i-QN)，一种超越传统单步 Bellman updates 的方法，用于提升深度强化学习中的行动价值函数估计效率和样本效率。i-QN 通过学习一个定制的行动价值函数序列，每个函数作为下一个的目标，实现多个连续 Bellman updates，从而解决高维问题中的计算挑战。该方法理论上严谨，并可无缝整合到基于价值的和 actor-critic 算法中。实验结果显示，i-QN 在 Atari 2600 游戏和 MuJoCo 连续控制任务上表现出显著优势，证明了其实际效能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at TMLR: https://openreview.net/forum?id=Lt2H8Bd8jF",
      "pdf_url": "http://arxiv.org/pdf/2403.02107v6",
      "published_date": "2024-03-04 15:07:33 UTC",
      "updated_date": "2025-04-03 13:58:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:17:50.702744"
    },
    {
      "arxiv_id": "2406.18535v1",
      "title": "DRAK: Unlocking Molecular Insights with Domain-Specific Retrieval-Augmented Knowledge in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Jinzhe Liu",
        "Xiangsheng Huang",
        "Zhuo Chen",
        "Yin Fang"
      ],
      "abstract": "Large Language Models (LLMs) encounter challenges with the unique syntax of\nspecific domains, such as biomolecules. Existing fine-tuning or modality\nalignment techniques struggle to bridge the domain knowledge gap and understand\ncomplex molecular data, limiting LLMs' progress in specialized fields. To\novercome these limitations, we propose an expandable and adaptable\nnon-parametric knowledge injection framework named Domain-specific\nRetrieval-Augmented Knowledge (DRAK), aimed at enhancing reasoning capabilities\nin specific domains. Utilizing knowledge-aware prompts and gold label-induced\nreasoning, DRAK has developed profound expertise in the molecular domain and\nthe capability to handle a broad spectrum of analysis tasks. We evaluated two\ndistinct forms of DRAK variants, proving that DRAK exceeds previous benchmarks\non six molecular tasks within the Mol-Instructions dataset. Extensive\nexperiments have underscored DRAK's formidable performance and its potential to\nunlock molecular insights, offering a unified paradigm for LLMs to tackle\nknowledge-intensive tasks in specific domains. Our code will be available soon.",
      "tldr_zh": "该研究指出，大型语言模型（LLMs）在处理特定领域如生物分子的独特语法时面临知识差距，现有的微调或模态对齐技术难以有效应对。为解决这一问题，研究提出DRAK框架，这是一个可扩展的非参数知识注入系统，利用领域特定检索增强知识（Domain-specific Retrieval-Augmented Knowledge）、知识感知提示和金标准标签诱导推理，提升LLMs在分子领域的推理能力。在Mol-Instructions数据集的六个分子任务上，DRAK的表现超过了现有基准，展示了其强大的性能，并为LLMs处理知识密集型任务提供了一个统一的范式。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Ongoing work; 11 pages, 6 Figures, 2 Tables",
      "pdf_url": "http://arxiv.org/pdf/2406.18535v1",
      "published_date": "2024-03-04 15:04:05 UTC",
      "updated_date": "2024-03-04 15:04:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:18:02.043132"
    },
    {
      "arxiv_id": "2403.02370v1",
      "title": "adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds",
      "title_zh": "翻译失败",
      "authors": [
        "Séamus Lankford",
        "Haithem Afli",
        "Andy Way"
      ],
      "abstract": "The advent of Multilingual Language Models (MLLMs) and Large Language Models\nhas spawned innovation in many areas of natural language processing. Despite\nthe exciting potential of this technology, its impact on developing\nhigh-quality Machine Translation (MT) outputs for low-resource languages\nremains relatively under-explored. Furthermore, an open-source application,\ndedicated to both fine-tuning MLLMs and managing the complete MT workflow for\nlow-resources languages, remains unavailable. We aim to address these\nimbalances through the development of adaptMLLM, which streamlines all\nprocesses involved in the fine-tuning of MLLMs for MT. This open-source\napplication is tailored for developers, translators, and users who are engaged\nin MT. An intuitive interface allows for easy customisation of hyperparameters,\nand the application offers a range of metrics for model evaluation and the\ncapability to deploy models as a translation service directly within the\napplication. As a multilingual tool, we used adaptMLLM to fine-tune models for\ntwo low-resource language pairs: English to Irish (EN$\\leftrightarrow$GA) and\nEnglish to Marathi (EN$\\leftrightarrow$MR). Compared with baselines from the\nLoResMT2021 Shared Task, the adaptMLLM system demonstrated significant\nimprovements. In the EN$\\rightarrow$GA direction, an improvement of 5.2 BLEU\npoints was observed and an increase of 40.5 BLEU points was recorded in the\nGA$\\rightarrow$EN direction. Significant improvements in the translation\nperformance of the EN$\\leftrightarrow$MR pair were also observed notably in the\nMR$\\rightarrow$EN direction with an increase of 21.3 BLEU points. Finally, a\nfine-grained human evaluation of the MLLM output on the EN$\\rightarrow$GA pair\nwas conducted using the Multidimensional Quality Metrics and Scalar Quality\nMetrics error taxonomies. The application and models are freely available.",
      "tldr_zh": "该论文介绍了 adaptMLLM，一种开源应用，用于微调 Multilingual Language Models (MLLMs) 以提升低资源语言的 Machine Translation (MT) 性能。该工具整合了 LLM Playgrounds，提供直观的界面来自定义超参数、评估模型（如 BLEU 指标）和直接部署翻译服务。实验结果显示，在 EN↔GA 和 EN↔MR 语言对上，adaptMLLM 比 LoResMT2021 Shared Task 的基线模型显著改善，例如 EN→GA 提升 5.2 BLEU 点，GA→EN 提升 40.5 BLEU 点，并通过 Multidimensional Quality Metrics 和 Scalar Quality Metrics 进行了细粒度的人类评估。该应用和模型已免费开源，适用于开发者、翻译者和相关用户。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02370v1",
      "published_date": "2024-03-04 14:49:18 UTC",
      "updated_date": "2024-03-04 14:49:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:18:15.117205"
    },
    {
      "arxiv_id": "2403.02076v1",
      "title": "VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT",
      "title_zh": "翻译失败",
      "authors": [
        "Yifang Xu",
        "Yunzhuo Sun",
        "Zien Xie",
        "Benxiang Zhai",
        "Sidan Du"
      ],
      "abstract": "Video temporal grounding (VTG) aims to locate specific temporal segments from\nan untrimmed video based on a linguistic query. Most existing VTG models are\ntrained on extensive annotated video-text pairs, a process that not only\nintroduces human biases from the queries but also incurs significant\ncomputational costs. To tackle these challenges, we propose VTG-GPT, a\nGPT-based method for zero-shot VTG without training or fine-tuning. To reduce\nprejudice in the original query, we employ Baichuan2 to generate debiased\nqueries. To lessen redundant information in videos, we apply MiniGPT-v2 to\ntransform visual content into more precise captions. Finally, we devise the\nproposal generator and post-processing to produce accurate segments from\ndebiased queries and image captions. Extensive experiments demonstrate that\nVTG-GPT significantly outperforms SOTA methods in zero-shot settings and\nsurpasses unsupervised approaches. More notably, it achieves competitive\nperformance comparable to supervised methods. The code is available on\nhttps://github.com/YoucanBaby/VTG-GPT",
      "tldr_zh": "该论文提出 VTG-GPT，一种基于 GPT 的零样本视频时间定位 (Video Temporal Grounding, VTG) 方法，无需训练或微调，从而避免了传统模型依赖大量标注数据带来的偏见和计算成本。方法包括使用 Baichuan2 生成去偏置查询 (debiased queries)，MiniGPT-v2 将视频内容转化为精确字幕，以及设计提案生成器和后处理模块来从这些查询和字幕中提取准确的视频段。实验结果显示，VTG-GPT 在零样本设置下显著优于现有最先进 (SOTA) 方法，超越无监督方法，并与监督方法性能相当。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.02076v1",
      "published_date": "2024-03-04 14:22:02 UTC",
      "updated_date": "2024-03-04 14:22:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:18:27.614065"
    },
    {
      "arxiv_id": "2403.02074v1",
      "title": "Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongzhen Huang",
        "Linda Wei",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Combining images from multi-modalities is beneficial to explore various\ninformation in computer vision, especially in the medical domain. As an\nessential part of clinical diagnosis, multi-modal brain tumor segmentation aims\nto delineate the malignant entity involving multiple modalities. Although\nexisting methods have shown remarkable performance in the task, the information\nexchange for cross-scale and high-level representations fusion in spatial and\nmodality are limited in these methods. In this paper, we present a novel\nModality Aware and Shift Mixer that integrates intra-modality and\ninter-modality dependencies of multi-modal images for effective and robust\nbrain tumor segmentation. Specifically, we introduce a Modality-Aware module\naccording to neuroimaging studies for modeling the specific modality pair\nrelationships at low levels, and a Modality-Shift module with specific mosaic\npatterns is developed to explore the complex relationships across modalities at\nhigh levels via the self-attention. Experimentally, we outperform previous\nstate-of-the-art approaches on the public Brain Tumor Segmentation (BraTS 2021\nsegmentation) dataset. Further qualitative experiments demonstrate the efficacy\nand robustness of MASM.",
      "tldr_zh": "这篇论文针对多模态脑肿瘤分割任务，提出了一种新型的 Modality-Aware and Shift Mixer (MASM) 框架，以整合多模态图像的 intra-modality 和 inter-modality 依赖性，提升信息交换效率。框架包括 Modality-Aware 模块，用于在低水平根据神经影像学研究建模特定模态对关系，以及 Modality-Shift 模块，通过自注意力机制和特定马赛克模式在高水平探索跨模态复杂关系。在 BraTS 2021 数据集上，MASM 超过了现有 state-of-the-art 方法，并在定性实验中证明了其有效性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02074v1",
      "published_date": "2024-03-04 14:21:51 UTC",
      "updated_date": "2024-03-04 14:21:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:18:41.963943"
    },
    {
      "arxiv_id": "2403.04793v1",
      "title": "A Data-Driven Two-Phase Multi-Split Causal Ensemble Model for Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Zhipeng Ma",
        "Marco Kemmerling",
        "Daniel Buschmann",
        "Chrismarie Enslin",
        "Daniel Lütticke",
        "Robert H. Schmitt"
      ],
      "abstract": "Causal inference is a fundamental research topic for discovering the\ncause-effect relationships in many disciplines. However, not all algorithms are\nequally well-suited for a given dataset. For instance, some approaches may only\nbe able to identify linear relationships, while others are applicable for\nnon-linearities. Algorithms further vary in their sensitivity to noise and\ntheir ability to infer causal information from coupled vs. non-coupled time\nseries. Therefore, different algorithms often generate different causal\nrelationships for the same input. To achieve a more robust causal inference\nresult, this publication proposes a novel data-driven two-phase multi-split\ncausal ensemble model to combine the strengths of different causality base\nalgorithms. In comparison to existing approaches, the proposed ensemble method\nreduces the influence of noise through a data partitioning scheme in the first\nphase. To achieve this, the data are initially divided into several partitions\nand the base algorithms are applied to each partition. Subsequently, Gaussian\nmixture models are used to identify the causal relationships derived from the\ndifferent partitions that are likely to be valid. In the second phase, the\nidentified relationships from each base algorithm are then merged based on\nthree combination rules. The proposed ensemble approach is evaluated using\nmultiple metrics, among them a newly developed evaluation index for causal\nensemble approaches. We perform experiments using three synthetic datasets with\ndifferent volumes and complexity, which are specifically designed to test\ncausality detection methods under different circumstances while knowing the\nground truth causal relationships. In these experiments, our causality ensemble\noutperforms each of its base algorithms. In practical applications, the use of\nthe proposed method could hence lead to more robust and reliable causality\nresults.",
      "tldr_zh": "这篇论文提出了一种数据驱动的两阶段多分割因果集成模型，用于时间序列分析，旨在结合不同因果基础算法的优势以获得更稳健的因果推断结果。\n在第一阶段，该模型通过数据分区方案减少噪声影响，将数据分成多个分区后应用基础算法，并使用Gaussian mixture models识别可能有效的因果关系。\n第二阶段则基于三个组合规则合并从各分区和算法得出的关系。\n实验在三个合成数据集上进行，使用多种指标（包括一个新开发的评估指数）评估，结果显示该集成模型在不同复杂度和数据量下均优于单个基础算法，提供更可靠的因果关系发现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04793v1",
      "published_date": "2024-03-04 14:20:41 UTC",
      "updated_date": "2024-03-04 14:20:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:18:52.644852"
    },
    {
      "arxiv_id": "2403.02054v1",
      "title": "Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism",
      "title_zh": "翻译失败",
      "authors": [
        "Shuvayan Brahmachary",
        "Subodh M. Joshi",
        "Aniruddha Panda",
        "Kaushik Koneripalli",
        "Arun Kumar Sagotra",
        "Harshil Patel",
        "Ankush Sharma",
        "Ameya D. Jagtap",
        "Kaushic Kalyanaraman"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning\nabilities, prompting interest in their application as black-box optimizers.\nThis paper asserts that LLMs possess the capability for zero-shot optimization\nacross diverse scenarios, including multi-objective and high-dimensional\nproblems. We introduce a novel population-based method for numerical\noptimization using LLMs called Language-Model-Based Evolutionary Optimizer\n(LEO). Our hypothesis is supported through numerical examples, spanning\nbenchmark and industrial engineering problems such as supersonic nozzle shape\noptimization, heat transfer, and windfarm layout optimization. We compare our\nmethod to several gradient-based and gradient-free optimization approaches.\nWhile LLMs yield comparable results to state-of-the-art methods, their\nimaginative nature and propensity to hallucinate demand careful handling. We\nprovide practical guidelines for obtaining reliable answers from LLMs and\ndiscuss method limitations and potential research directions.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)作为黑箱优化器的潜力，提出了一种新型方法Language-Model-Based Evolutionary Optimizer (LEO)，利用LLMs的推理能力实现零-shot优化，包括多目标和高维问题。LEO采用基于种群的进化优化策略，并在基准测试和工业工程场景（如超音速喷嘴形状优化、热传输及风电场布局）中验证其有效性，与梯度-based和梯度-free方法相比表现出可比性能。论文强调了LLMs的幻觉问题，并提供了获取可靠答案的实用指南，同时讨论了方法的局限性和未来研究方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02054v1",
      "published_date": "2024-03-04 13:57:37 UTC",
      "updated_date": "2024-03-04 13:57:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:19:05.657568"
    },
    {
      "arxiv_id": "2403.02053v1",
      "title": "A Scoping Review of Energy-Efficient Driving Behaviors and Applied State-of-the-Art AI Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Zhipeng Ma",
        "Bo Nørregaard Jørgensen",
        "Zheng Ma"
      ],
      "abstract": "The transportation sector remains a major contributor to greenhouse gas\nemissions. The understanding of energy-efficient driving behaviors and\nutilization of energy-efficient driving strategies are essential to reduce\nvehicles' fuel consumption. However, there is no comprehensive investigation\ninto energy-efficient driving behaviors and strategies. Furthermore, many\nstate-of-the-art AI models have been applied for the analysis of eco-friendly\ndriving styles, but no overview is available. To fill the gap, this paper\nconducts a thorough literature review on ecological driving behaviors and\nstyles and analyzes the driving factors influencing energy consumption and\nstate-of-the-art methodologies. With a thorough scoping review process, the\nmethodological and related data are compared. The results show that the factors\nthat impact driving behaviors can be summarized into eleven features including\nspeed, acceleration, deceleration, pedal, and so on. This paper finds that\nsupervised/unsupervised learning algorithms and reinforcement learning\nframeworks have been popularly used to model the vehicle's energy consumption\nwith multi-dimensional data. Furthermore, the literature shows that the driving\ndata are collected from either simulators or real-world experiments, and the\nreal-world data are mainly stored and transmitted by meters, controller area\nnetworks, onboard data services, smartphones, and additional sensors installed\nin the vehicle. Based on driving behavior factors, driver characteristics, and\nsafety rules, this paper recommends nine energy-efficient driving styles\nincluding four guidelines for the drivers' selection and adjustment of the\nvehicle parameters, three recommendations for the energy-efficient driving\nstyles in different driving scenarios, and two subjective suggestions for\ndifferent types of drivers and employers.",
      "tldr_zh": "这篇论文通过文献综述，探讨了能源高效驾驶行为及其影响因素，旨在减少交通部门对温室气体排放的贡献。研究总结了11个关键特征（如速度、加速度和减速）对车辆能耗的影响，并分析了State-of-the-Art AI方法的应用，包括监督/无监督学习算法和强化学习框架，用于基于多维数据建模能耗。数据来源包括模拟器或真实世界实验，通过仪表、Controller Area Networks、Onboard Data Services、智能手机和附加传感器收集。最后，论文推荐了九种能源高效驾驶风格，涵盖车辆参数调整、不同场景的驾驶建议，以及针对不同类型驾驶员和雇主的指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02053v1",
      "published_date": "2024-03-04 13:57:34 UTC",
      "updated_date": "2024-03-04 13:57:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:19:17.385042"
    },
    {
      "arxiv_id": "2403.12988v1",
      "title": "Improving the Robustness of Object Detection and Classification AI models against Adversarial Patch Attacks",
      "title_zh": "提升对象检测和分类AI模型针对对抗性补丁攻击的鲁",
      "authors": [
        "Roie Kazoom",
        "Raz Birman",
        "Ofer Hadar"
      ],
      "abstract": "Adversarial patch attacks, crafted to compromise the integrity of Deep Neural\nNetworks (DNNs), significantly impact Artificial Intelligence (AI) systems\ndesigned for object detection and classification tasks. The primary purpose of\nthis work is to defend models against real-world physical attacks that target\nobject detection and classification. We analyze attack techniques and propose a\nrobust defense approach. We successfully reduce model confidence by over 20%\nusing adversarial patch attacks that exploit object shape, texture and\nposition. Leveraging the inpainting pre-processing technique, we effectively\nrestore the original confidence levels, demonstrating the importance of robust\ndefenses in mitigating these threats. Following fine-tuning of an AI model for\ntraffic sign classification, we subjected it to a simulated pixelized\npatch-based physical adversarial attack, resulting in misclassifications. Our\ninpainting defense approach significantly enhances model resilience, achieving\nhigh accuracy and reliable localization despite the adversarial attacks. This\ncontribution advances the resilience and reliability of object detection and\nclassification networks against adversarial challenges, providing a robust\nfoundation for critical applications.",
      "tldr_zh": "这篇论文针对对抗性补丁攻击（Adversarial Patch Attacks）对深度神经网络（DNNs）在物体检测和分类任务中的影响，提出了一种鲁棒防御方法，以提升模型在真实世界物理攻击下的可靠性。研究者分析了攻击技术，利用物体形状、纹理和位置来降低模型置信度超过20%，并通过Inpainting预处理技术有效恢复原始置信度。实验结果显示，在交通标志分类模型上应用该防御方法后，模型实现了高准确性和可靠定位，显著提高了物体检测和分类网络的韧性，为关键应用提供了坚实基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12988v1",
      "published_date": "2024-03-04 13:32:48 UTC",
      "updated_date": "2024-03-04 13:32:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:19:29.597138"
    },
    {
      "arxiv_id": "2403.02368v1",
      "title": "A Novel Hybrid Feature Importance and Feature Interaction Detection Framework for Predictive Optimization in Industry 4.0 Applications",
      "title_zh": "一种新",
      "authors": [
        "Zhipeng Ma",
        "Bo Nørregaard Jørgensen",
        "Zheng Grace Ma"
      ],
      "abstract": "Advanced machine learning algorithms are increasingly utilized to provide\ndata-based prediction and decision-making support in Industry 4.0. However, the\nprediction accuracy achieved by the existing models is insufficient to warrant\npractical implementation in real-world applications. This is because not all\nfeatures present in real-world datasets possess a direct relevance to the\npredictive analysis being conducted. Consequently, the careful incorporation of\nselect features has the potential to yield a substantial positive impact on the\noutcome. To address the research gap, this paper proposes a novel hybrid\nframework that combines the feature importance detector - local interpretable\nmodel-agnostic explanations (LIME) and the feature interaction detector -\nneural interaction detection (NID), to improve prediction accuracy. By applying\nthe proposed framework, unnecessary features can be eliminated, and\ninteractions are encoded to generate a more conducive dataset for predictive\npurposes. Subsequently, the proposed model is deployed to refine the prediction\nof electricity consumption in foundry processing. The experimental outcomes\nreveal an augmentation of up to 9.56% in the R2 score, and a diminution of up\nto 24.05% in the root mean square error.",
      "tldr_zh": "这篇论文针对 Industry 4.0 中的预测优化问题，提出一个新颖的混合框架，结合 LIME（局部可解释模型无关解释）和 NID（神经交互检测）来检测特征重要性和特征交互，从而消除不必要特征并编码交互以提升预测准确性。该框架应用于铸造加工的电力消耗预测中，实验结果显示 R2 score 提高了高达 9.56%，root mean square error 降低了高达 24.05%。这项工作为机器学习在工业应用中的实际部署提供了有效的改进路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02368v1",
      "published_date": "2024-03-04 13:22:53 UTC",
      "updated_date": "2024-03-04 13:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:19:40.391062"
    },
    {
      "arxiv_id": "2403.02018v1",
      "title": "Cross Domain Policy Transfer with Effect Cycle-Consistency",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiqi Zhu",
        "Tianhong Dai",
        "Oya Celiktutan"
      ],
      "abstract": "Training a robotic policy from scratch using deep reinforcement learning\nmethods can be prohibitively expensive due to sample inefficiency. To address\nthis challenge, transferring policies trained in the source domain to the\ntarget domain becomes an attractive paradigm. Previous research has typically\nfocused on domains with similar state and action spaces but differing in other\naspects. In this paper, our primary focus lies in domains with different state\nand action spaces, which has broader practical implications, i.e. transfer the\npolicy from robot A to robot B. Unlike prior methods that rely on paired data,\nwe propose a novel approach for learning the mapping functions between state\nand action spaces across domains using unpaired data. We propose effect cycle\nconsistency, which aligns the effects of transitions across two domains through\na symmetrical optimization structure for learning these mapping functions. Once\nthe mapping functions are learned, we can seamlessly transfer the policy from\nthe source domain to the target domain. Our approach has been tested on three\nlocomotion tasks and two robotic manipulation tasks. The empirical results\ndemonstrate that our method can reduce alignment errors significantly and\nachieve better performance compared to the state-of-the-art method.",
      "tldr_zh": "本论文针对深度强化学习（deep reinforcement learning）在机器人策略训练中的样例效率低问题，提出一种跨域策略转移方法，专注于状态和动作空间不同的域（如从机器人 A 转移到机器人 B）。该方法使用 unpaired data 学习状态和动作空间之间的映射函数，通过 effect cycle-consistency 的对称优化结构对齐两个域的转移效果，从而实现策略的无缝转移。实验在三个运动任务和两个机器人操作任务上验证，该方法显著减少了对齐错误，并比 state-of-the-art 方法表现出更好的性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to International Conference on Robotics and Automation\n  (ICRA), 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.02018v1",
      "published_date": "2024-03-04 13:20:07 UTC",
      "updated_date": "2024-03-04 13:20:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:19:51.344735"
    },
    {
      "arxiv_id": "2403.02014v1",
      "title": "Unveiling Hidden Links Between Unseen Security Entities",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Alfasi",
        "Tal Shapira",
        "Anat Bremler Barr"
      ],
      "abstract": "The proliferation of software vulnerabilities poses a significant challenge\nfor security databases and analysts tasked with their timely identification,\nclassification, and remediation. With the National Vulnerability Database (NVD)\nreporting an ever-increasing number of vulnerabilities, the traditional manual\nanalysis becomes untenably time-consuming and prone to errors. This paper\nintroduces VulnScopper, an innovative approach that utilizes multi-modal\nrepresentation learning, combining Knowledge Graphs (KG) and Natural Language\nProcessing (NLP), to automate and enhance the analysis of software\nvulnerabilities. Leveraging ULTRA, a knowledge graph foundation model, combined\nwith a Large Language Model (LLM), VulnScopper effectively handles unseen\nentities, overcoming the limitations of previous KG approaches. We evaluate\nVulnScopper on two major security datasets, the NVD and the Red Hat CVE\ndatabase. Our method significantly improves the link prediction accuracy\nbetween Common Vulnerabilities and Exposures (CVEs), Common Weakness\nEnumeration (CWEs), and Common Platform Enumerations (CPEs). Our results show\nthat VulnScopper outperforms existing methods, achieving up to 78% Hits@10\naccuracy in linking CVEs to CPEs and CWEs and presenting an 11.7% improvement\nover large language models in predicting CWE labels based on the Red Hat\ndatabase. Based on the NVD, only 6.37% of the linked CPEs are being published\nduring the first 30 days; many of them are related to critical and high-risk\nvulnerabilities which, according to multiple compliance frameworks (such as\nCISA and PCI), should be remediated within 15-30 days. Our model can uncover\nnew products linked to vulnerabilities, reducing remediation time and improving\nvulnerability management. We analyzed several CVEs from 2023 to showcase this\nability.",
      "tldr_zh": "该研究提出了一种创新方法VulnScopper，利用多模态表示学习结合知识图谱(KG)和自然语言处理(NLP)，通过ULTRA知识图谱基础模型和大型语言模型(LLM)，自动分析软件漏洞并处理未见实体。实验在NVD和Red Hat CVE数据库上评估，VulnScopper显著提升了链接预测准确率，达到78% Hits@10在CVEs与CPEs和CWEs之间的链接，并比现有LLM方法提高了11.7%的CWE标签预测性能。结果显示，该模型能发现新产品与漏洞的关联，减少修复时间，并针对2023年CVE案例证明其在漏洞管理中的实际价值。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02014v1",
      "published_date": "2024-03-04 13:14:39 UTC",
      "updated_date": "2024-03-04 13:14:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:20:05.147871"
    },
    {
      "arxiv_id": "2403.01985v1",
      "title": "Transformers for Low-Resource Languages: Is Féidir Linn!",
      "title_zh": "Transformer 模型用于低资源语言：Is Féidir Linn!",
      "authors": [
        "Séamus Lankford",
        "Haithem Afli",
        "Andy Way"
      ],
      "abstract": "The Transformer model is the state-of-the-art in Machine Translation.\nHowever, in general, neural translation models often under perform on language\npairs with insufficient training data. As a consequence, relatively few\nexperiments have been carried out using this architecture on low-resource\nlanguage pairs. In this study, hyperparameter optimization of Transformer\nmodels in translating the low-resource English-Irish language pair is\nevaluated. We demonstrate that choosing appropriate parameters leads to\nconsiderable performance improvements. Most importantly, the correct choice of\nsubword model is shown to be the biggest driver of translation performance.\nSentencePiece models using both unigram and BPE approaches were appraised.\nVariations on model architectures included modifying the number of layers,\ntesting various regularisation techniques and evaluating the optimal number of\nheads for attention. A generic 55k DGT corpus and an in-domain 88k public admin\ncorpus were used for evaluation. A Transformer optimized model demonstrated a\nBLEU score improvement of 7.8 points when compared with a baseline RNN model.\nImprovements were observed across a range of metrics, including TER, indicating\na substantially reduced post editing effort for Transformer optimized models\nwith 16k BPE subword models. Bench-marked against Google Translate, our\ntranslation engines demonstrated significant improvements. The question of\nwhether or not Transformers can be used effectively in a low-resource setting\nof English-Irish translation has been addressed. Is f\\'eidir linn - yes we can.",
      "tldr_zh": "该研究评估了 Transformer 模型在低资源语言对（如英语-爱尔兰语）机器翻译中的性能，通过超参数优化来解决训练数据不足的问题。方法包括测试 SentencePiece 的 unigram 和 BPE 子词模型、调整层数、正则化技术以及注意力头数，结果显示 BPE 子词模型是性能提升的关键驱动因素。实验使用 DGT 和公共管理语料库表明，优化后的 Transformer 模型比基线 RNN 模型的 BLEU 分数提高了 7.8 点，并在 TER 等指标上显著减少了后期编辑工作，最终优于 Google Translate，证明了其在低资源场景中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.01985v1",
      "published_date": "2024-03-04 12:29:59 UTC",
      "updated_date": "2024-03-04 12:29:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:20:17.605864"
    },
    {
      "arxiv_id": "2403.01977v2",
      "title": "TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions",
      "title_zh": "TTA-Nav：测试时自适应重建用于视觉干扰下的点目标导航",
      "authors": [
        "Maytus Piriyajitakonkij",
        "Mingfei Sun",
        "Mengmi Zhang",
        "Wei Pan"
      ],
      "abstract": "Robot navigation under visual corruption presents a formidable challenge. To\naddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\nfor point-goal navigation under visual corruptions. Our \"plug-and-play\" method\nincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\npre-trained navigation model gets a corrupted image and extracts features.\nSecondly, the top-down decoder produces the reconstruction given the high-level\nfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\nof a corrupted image back to the pre-trained model. Finally, the pre-trained\nmodel does forward pass again to output action. Despite being trained solely on\nclean images, the top-down decoder can reconstruct cleaner images from\ncorrupted ones without the need for gradient-based adaptation. The pre-trained\nnavigation model with our top-down decoder significantly enhances navigation\nperformance across almost all visual corruptions in our benchmarks. Our method\nimproves the success rate of point-goal navigation from the state-of-the-art\nresult of 46% to 94% on the most severe corruption. This suggests its potential\nfor broader application in robotic visual navigation. Project page:\nhttps://sites.google.com/view/tta-nav",
      "tldr_zh": "本研究提出了一种Test-time Adaptation (TTA) 方法，名为TTA-Nav，用于在visual corruptions下提升点目标导航(point-goal navigation)的性能。该方法采用“plug-and-play”方式，将top-down decoder添加到预训练导航模型中，通过提取图像特征并重建更干净的图像，再反馈到模型以输出动作，尽管decoder仅在干净图像上训练。实验结果显示，TTA-Nav在各种视觉干扰基准上显著改善导航表现，将最严重干扰下的成功率从46%提高到94%。这项创新为机器人视觉导航提供了更鲁棒的解决方案，具有广泛应用潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to IROS2024",
      "pdf_url": "http://arxiv.org/pdf/2403.01977v2",
      "published_date": "2024-03-04 12:20:29 UTC",
      "updated_date": "2024-03-14 16:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:20:28.042566"
    },
    {
      "arxiv_id": "2403.02367v1",
      "title": "adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Séamus Lankford",
        "Haithem Afli",
        "Andy Way"
      ],
      "abstract": "adaptNMT streamlines all processes involved in the development and deployment\nof RNN and Transformer neural translation models. As an open-source\napplication, it is designed for both technical and non-technical users who work\nin the field of machine translation. Built upon the widely-adopted OpenNMT\necosystem, the application is particularly useful for new entrants to the field\nsince the setup of the development environment and creation of train,\nvalidation and test splits is greatly simplified. Graphing, embedded within the\napplication, illustrates the progress of model training, and SentencePiece is\nused for creating subword segmentation models. Hyperparameter customization is\nfacilitated through an intuitive user interface, and a single-click model\ndevelopment approach has been implemented. Models developed by adaptNMT can be\nevaluated using a range of metrics, and deployed as a translation service\nwithin the application. To support eco-friendly research in the NLP space, a\ngreen report also flags the power consumption and kgCO$_{2}$ emissions\ngenerated during model development. The application is freely available.",
      "tldr_zh": "adaptNMT是一个开源、语言无关的开发环境，旨在简化RNN和Transformer神经机器翻译（Neural Machine Translation）模型的开发和部署过程。该工具基于OpenNMT生态系统，提供直观的界面来处理环境设置、数据分割、训练进度图形化展示以及SentencePiece子词分割，支持超参数自定义和一键式模型开发。用户可以通过多种评估指标评估模型，并直接部署为翻译服务，同时绿色报告功能显示了模型开发中的功率消耗和CO2排放，以促进环保型NLP研究。该应用免费提供，特别适合技术和非技术用户入门机器翻译领域。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02367v1",
      "published_date": "2024-03-04 12:10:17 UTC",
      "updated_date": "2024-03-04 12:10:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:20:40.751749"
    },
    {
      "arxiv_id": "2403.01964v2",
      "title": "The Heterogeneous Productivity Effects of Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "David Kreitmeir",
        "Paul A. Raschky"
      ],
      "abstract": "We analyse the individual productivity effects of Italy's ban on ChatGPT, a\ngenerative pretrained transformer chatbot. We compile data on the daily coding\noutput quantity and quality of over 36,000 GitHub users in Italy and other\nEuropean countries and combine these data with the sudden announcement of the\nban in a difference-in-differences framework. Among the affected users in\nItaly, we find a short-term increase in output quantity and quality for less\nexperienced users and a decrease in productivity on more routine tasks for\nexperienced users.",
      "tldr_zh": "该研究考察了生成式 AI（如 ChatGPT）对个体生产力的异质性影响，通过分析意大利禁令作为自然实验。研究者收集了超过 36,000 名 GitHub 用户在意大利和其他欧洲国家的日常编码输出数据（包括数量和质量），并采用 difference-in-differences 框架进行分析。结果显示，受影响的意大利用户中，经验较少的用户短期内输出量和质量有所提升，而经验较多的用户在更常规任务上生产力出现下降。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "q-fin.EC"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01964v2",
      "published_date": "2024-03-04 12:07:28 UTC",
      "updated_date": "2024-06-03 01:21:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:20:52.211386"
    },
    {
      "arxiv_id": "2403.01954v4",
      "title": "DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Xu",
        "Tian Lan",
        "Yu Ji",
        "Changlong Yu",
        "Wei Wang",
        "Jun Gao",
        "Qunxi Dong",
        "Kun Qian",
        "Piji Li",
        "Wei Bi",
        "Bin Hu"
      ],
      "abstract": "Constrained decoding approaches aim to control the meaning or style of text\ngenerated by the pre-trained large language models (LLMs or also PLMs) for\nvarious tasks at inference time. However, these methods often guide plausible\ncontinuations by greedily and explicitly selecting targets. Though fulfilling\nthe task requirements, these methods may overlook certain general and natural\nlogics that humans would implicitly follow towards such targets. Inspired by\ncognitive dual-process theory, in this work, we propose a novel decoding\nframework DECIDER where the base LLMs are equipped with a First-Order Logic\n(FOL) reasoner to express and evaluate the rules, along with a decision\nfunction that merges the outputs of both systems to guide the generation.\nUnlike previous constrained decodings, DECIDER transforms the encouragement of\ntarget-specific words into all words that satisfy several high-level rules,\nenabling us to programmatically integrate our logic into LLMs. Experiments on\nCommonGen and PersonaChat demonstrate that DECIDER effectively follows given\nFOL rules to guide LLMs in a more human-like and logic-controlled manner.",
      "tldr_zh": "该论文提出 DECIDER 框架，一种受认知双过程理论启发的双系统解码方法，用于控制大语言模型 (LLMs) 在文本生成中的规则遵守。DECIDER 将基础 LLMs 与一阶逻辑 (FOL) 推理器相结合，通过一个决策函数合并两者的输出，将目标词汇的鼓励转化为满足高水平规则的所有词汇，从而以编程方式整合逻辑。相比传统约束解码方法，该框架能更自然地遵循人类隐含逻辑，并在 CommonGen 和 PersonaChat 数据集的实验中，实现了更人性化和逻辑控制的文本生成。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IEEE TKDE 2025, 14 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.01954v4",
      "published_date": "2024-03-04 11:49:08 UTC",
      "updated_date": "2025-05-04 06:48:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:21:04.870583"
    },
    {
      "arxiv_id": "2403.02366v1",
      "title": "Human Evaluation of English--Irish Transformer-Based NMT",
      "title_zh": "翻译失败",
      "authors": [
        "Séamus Lankford",
        "Haithem Afli",
        "Andy Way"
      ],
      "abstract": "In this study, a human evaluation is carried out on how hyperparameter\nsettings impact the quality of Transformer-based Neural Machine Translation\n(NMT) for the low-resourced English--Irish pair. SentencePiece models using\nboth Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations\nin model architectures included modifying the number of layers, evaluating the\noptimal number of heads for attention and testing various regularisation\ntechniques. The greatest performance improvement was recorded for a\nTransformer-optimized model with a 16k BPE subword model. Compared with a\nbaseline Recurrent Neural Network (RNN) model, a Transformer-optimized model\ndemonstrated a BLEU score improvement of 7.8 points. When benchmarked against\nGoogle Translate, our translation engines demonstrated significant\nimprovements. Furthermore, a quantitative fine-grained manual evaluation was\nconducted which compared the performance of machine translation systems. Using\nthe Multidimensional Quality Metrics (MQM) error taxonomy, a human evaluation\nof the error types generated by an RNN-based system and a Transformer-based\nsystem was explored. Our findings show the best-performing Transformer system\nsignificantly reduces both accuracy and fluency errors when compared with an\nRNN-based model.",
      "tldr_zh": "本研究评估了超参数设置对 Transformer-based NMT 在低资源英语-爱尔兰语翻译中的影响，测试了 SentencePiece 模型（包括 Byte Pair Encoding (BPE) 和 unigram 方法）、层数、注意力头数以及各种正则化技术。结果显示，优化后的 Transformer 模型较 RNN 基准模型提高了 7.8 BLEU score，并在与 Google Translate 的比较中表现出显著优势。通过 Multidimensional Quality Metrics (MQM) 错误分类的人工细粒度评估，发现最佳 Transformer 系统显著减少了准确性和流畅性错误，为低资源语言翻译优化提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2403.01985",
      "pdf_url": "http://arxiv.org/pdf/2403.02366v1",
      "published_date": "2024-03-04 11:45:46 UTC",
      "updated_date": "2024-03-04 11:45:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:21:17.829694"
    },
    {
      "arxiv_id": "2403.06999v1",
      "title": "Survival modeling using deep learning, machine learning and statistical methods: A comparative analysis for predicting mortality after hospital admission",
      "title_zh": "翻译失败",
      "authors": [
        "Ziwen Wang",
        "Jin Wee Lee",
        "Tanujit Chakraborty",
        "Yilin Ning",
        "Mingxuan Liu",
        "Feng Xie",
        "Marcus Eng Hock Ong",
        "Nan Liu"
      ],
      "abstract": "Survival analysis is essential for studying time-to-event outcomes and\nproviding a dynamic understanding of the probability of an event occurring over\ntime. Various survival analysis techniques, from traditional statistical models\nto state-of-the-art machine learning algorithms, support healthcare\nintervention and policy decisions. However, there remains ongoing discussion\nabout their comparative performance. We conducted a comparative study of\nseveral survival analysis methods, including Cox proportional hazards (CoxPH),\nstepwise CoxPH, elastic net penalized Cox model, Random Survival Forests (RSF),\nGradient Boosting machine (GBM) learning, AutoScore-Survival, DeepSurv,\ntime-dependent Cox model based on neural network (CoxTime), and DeepHit\nsurvival neural network. We applied the concordance index (C-index) for model\ngoodness-of-fit, and integral Brier scores (IBS) for calibration, and\nconsidered the model interpretability. As a case study, we performed a\nretrospective analysis of patients admitted through the emergency department of\na tertiary hospital from 2017 to 2019, predicting 90-day all-cause mortality\nbased on patient demographics, clinicopathological features, and historical\ndata. The results of the C-index indicate that deep learning achieved\ncomparable performance, with DeepSurv producing the best discrimination\n(DeepSurv: 0.893; CoxTime: 0.892; DeepHit: 0.891). The calibration of DeepSurv\n(IBS: 0.041) performed the best, followed by RSF (IBS: 0.042) and GBM (IBS:\n0.0421), all using the full variables. Moreover, AutoScore-Survival, using a\nminimal variable subset, is easy to interpret, and can achieve good\ndiscrimination and calibration (C-index: 0.867; IBS: 0.044). While all models\nwere satisfactory, DeepSurv exhibited the best discrimination and calibration.\nIn addition, AutoScore-Survival offers a more parsimonious model and excellent\ninterpretability.",
      "tldr_zh": "本研究比较了多种生存分析方法，包括统计模型（如CoxPH、stepwise CoxPH和弹性网惩罚Cox模型）、机器学习算法（如RSF和GBM）以及深度学习方法（如DeepSurv、CoxTime和DeepHit），以预测医院入院后患者的90天全因死亡率。研究使用C-index评估模型的区分能力、IBS评估校准度，并考虑模型可解释性，通过对2017-2019年某三级医院紧急部门患者数据的回顾性分析进行案例研究。结果显示，DeepSurv表现出最佳性能（C-index: 0.893；IBS: 0.041），而AutoScore-Survival虽使用最少变量但仍具有良好效果（C-index: 0.867；IBS: 0.044），为医疗干预决策提供了更可解释的选项。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06999v1",
      "published_date": "2024-03-04 10:46:02 UTC",
      "updated_date": "2024-03-04 10:46:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:21:29.837266"
    },
    {
      "arxiv_id": "2403.01924v2",
      "title": "To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Giacomo Frisoni",
        "Alessio Cocchieri",
        "Alex Presepi",
        "Gianluca Moro",
        "Zaiqiao Meng"
      ],
      "abstract": "Medical open-domain question answering demands substantial access to\nspecialized knowledge. Recent efforts have sought to decouple knowledge from\nmodel parameters, counteracting architectural scaling and allowing for training\non common low-resource hardware. The retrieve-then-read paradigm has become\nubiquitous, with model predictions grounded on relevant knowledge pieces from\nexternal repositories such as PubMed, textbooks, and UMLS. An alternative path,\nstill under-explored but made possible by the advent of domain-specific large\nlanguage models, entails constructing artificial contexts through prompting. As\na result, \"to generate or to retrieve\" is the modern equivalent of Hamlet's\ndilemma. This paper presents MedGENIE, the first generate-then-read framework\nfor multiple-choice question answering in medicine. We conduct extensive\nexperiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical\nperspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new\nstate-of-the-art in the open-book setting of each testbed, allowing a\nsmall-scale reader to outcompete zero-shot closed-book 175B baselines while\nusing up to 706$\\times$ fewer parameters. Our findings reveal that generated\npassages are more effective than retrieved ones in attaining higher accuracy.",
      "tldr_zh": "本研究探讨了在医疗开放域问答中，是生成人工上下文还是检索外部知识更有效的问题，针对模型参数规模和硬件资源（如最大24GB VRAM）的限制。论文提出MedGENIE框架，这是一种generate-then-read方法，通过提示生成相关上下文，然后进行多选题问答，在MedQA-USMLE、MedMCQA和MMLU数据集上实现了open-book设置的新state-of-the-art。实验结果显示，MedGENIE让小规模阅读器超越了零样本closed-book 175B基线，使用最多706倍更少的参数，且生成的段落比retrieved passages更能提升准确率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 (camera-ready paper)",
      "pdf_url": "http://arxiv.org/pdf/2403.01924v2",
      "published_date": "2024-03-04 10:41:52 UTC",
      "updated_date": "2024-06-13 08:42:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:21:41.132959"
    },
    {
      "arxiv_id": "2404.02912v1",
      "title": "Probabilistic Generating Circuits -- Demystified",
      "title_zh": "概率生成电路——去神秘",
      "authors": [
        "Sanyam Agarwal",
        "Markus Bläser"
      ],
      "abstract": "Zhang et al. (ICML 2021, PLMR 139, pp. 12447-1245) introduced probabilistic\ngenerating circuits (PGCs) as a probabilistic model to unify probabilistic\ncircuits (PCs) and determinantal point processes (DPPs). At a first glance,\nPGCs store a distribution in a very different way, they compute the probability\ngenerating polynomial instead of the probability mass function and it seems\nthat this is the main reason why PGCs are more powerful than PCs or DPPs.\nHowever, PGCs also allow for negative weights, whereas classical PCs assume\nthat all weights are nonnegative. One of the main insights of our paper is that\nthe negative weights are responsible for the power of PGCs and not the\ndifferent representation. PGCs are PCs in disguise, in particular, we show how\nto transform any PGC into a PC with negative weights with only polynomial\nblowup.\n  PGCs were defined by Zhang et al. only for binary random variables. As our\nsecond main result, we show that there is a good reason for this: we prove that\nPGCs for categorial variables with larger image size do not support tractable\nmarginalization unless NP = P. On the other hand, we show that we can model\ncategorial variables with larger image size as PC with negative weights\ncomputing set-multilinear polynomials. These allow for tractable\nmarginalization. In this sense, PCs with negative weights strictly subsume\nPGCs.",
      "tldr_zh": "本论文剖析了 Probabilistic Generating Circuits (PGCs)，一种统一 Probabilistic Circuits (PCs) 和 Determinantal Point Processes (DPPs) 的概率模型。研究发现，PGCs 的强大在于允许负权重，而非其概率生成多项式的不同表示形式；作者展示了如何将任何 PGC 转化为带有负权重的 PCs，仅需多项式级别的膨胀。论文证明，PGCs 仅适用于二元随机变量，因为对于类别变量（categorial variables）更大的图像大小，PGCs 不支持可处理的边缘化，除非 NP = P。同时，作者扩展了带有负权重的 PCs 来计算集合多项式，从而能处理更大类别变量并支持可处理的边缘化，最终表明带有负权重的 PCs 严格包含了 PGCs。",
      "categories": [
        "cs.CC",
        "cs.AI"
      ],
      "primary_category": "cs.CC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02912v1",
      "published_date": "2024-03-04 10:40:09 UTC",
      "updated_date": "2024-03-04 10:40:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:21:53.339858"
    },
    {
      "arxiv_id": "2403.01915v2",
      "title": "xT: Nested Tokenization for Larger Context in Large Images",
      "title_zh": "翻译失败",
      "authors": [
        "Ritwik Gupta",
        "Shufan Li",
        "Tyler Zhu",
        "Jitendra Malik",
        "Trevor Darrell",
        "Karttikeya Mangalam"
      ],
      "abstract": "Modern computer vision pipelines handle large images in one of two\nsub-optimal ways: down-sampling or cropping. These two methods incur\nsignificant losses in the amount of information and context present in an\nimage. There are many downstream applications in which global context matters\nas much as high frequency details, such as in real-world satellite imagery; in\nsuch cases researchers have to make the uncomfortable choice of which\ninformation to discard. We introduce xT, a simple framework for vision\ntransformers which effectively aggregates global context with local details and\ncan model large images end-to-end on contemporary GPUs. We select a set of\nbenchmark datasets across classic vision tasks which accurately reflect a\nvision model's ability to understand truly large images and incorporate fine\ndetails over large scales and assess our method's improvement on them. xT is a\nstreaming, two-stage architecture that adapts existing vision backbones and\nlong sequence language models to effectively model large images without\nquadratic memory growth. We are able to increase accuracy by up to 8.6% on\nchallenging classification tasks and $F_1$ score by 11.6 on context-dependent\nsegmentation on images as large as 29,000 x 29,000 pixels.",
      "tldr_zh": "本论文提出 xT 框架，通过 nested tokenization 技术处理大图像问题，避免传统 down-sampling 或 cropping 方法导致的信息和上下文损失。xT 是一个流式两阶段架构，适应现有 vision transformers 和长序列语言模型，能够在当代 GPU 上端到端聚合全局上下文与局部细节，而不引起二次内存增长。在基准数据集上，xT 在挑战性分类任务上准确率提高多达 8.6%，在上下文相关的分割任务上 F1 分数提高 11.6%，成功处理高达 29,000 x 29,000 像素的图像。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the 2024 International Conference on Machine Learning\n  (ICML)",
      "pdf_url": "http://arxiv.org/pdf/2403.01915v2",
      "published_date": "2024-03-04 10:29:58 UTC",
      "updated_date": "2024-07-21 02:33:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:22:06.256891"
    },
    {
      "arxiv_id": "2403.01909v3",
      "title": "Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey",
      "title_zh": "基于伪标签的半监督语义分割：综述",
      "authors": [
        "Lingyan Ran",
        "Yali Li",
        "Guoqiang Liang",
        "Yanning Zhang"
      ],
      "abstract": "Semantic segmentation is an important and popular research area in computer\nvision that focuses on classifying pixels in an image based on their semantics.\nHowever, supervised deep learning requires large amounts of data to train\nmodels and the process of labeling images pixel by pixel is time-consuming and\nlaborious. This review aims to provide a first comprehensive and organized\noverview of the state-of-the-art research results on pseudo-label methods in\nthe field of semi-supervised semantic segmentation, which we categorize from\ndifferent perspectives and present specific methods for specific application\nareas. In addition, we explore the application of pseudo-label technology in\nmedical and remote-sensing image segmentation. Finally, we also propose some\nfeasible future research directions to address the existing challenges.",
      "tldr_zh": "这篇调查论文综述了基于伪标签（Pseudo-Labels）的半监督语义分割（Semi-Supervised Semantic Segmentation）方法，以解决监督学习中标注图像像素所需的大量数据和劳动力问题。论文从不同视角对现有研究进行分类，并详细介绍了这些方法在特定应用领域的实现，如医疗和遥感图像分割。最终，论文提出了可行的未来研究方向，以应对当前挑战并推动该领域的进展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology(TCSVT)",
      "pdf_url": "http://arxiv.org/pdf/2403.01909v3",
      "published_date": "2024-03-04 10:18:38 UTC",
      "updated_date": "2025-03-18 05:27:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:22:16.428860"
    },
    {
      "arxiv_id": "2403.04790v1",
      "title": "Online Training of Large Language Models: Learn while chatting",
      "title_zh": "大语言模型的在线训练：边聊天边学习",
      "authors": [
        "Juhao Liang",
        "Ziwei Wang",
        "Zhuoheng Ma",
        "Jianquan Li",
        "Zhiyi Zhang",
        "Xiangbo Wu",
        "Benyou Wang"
      ],
      "abstract": "Large Language Models(LLMs) have dramatically revolutionized the field of\nNatural Language Processing(NLP), offering remarkable capabilities that have\ngarnered widespread usage. However, existing interaction paradigms between LLMs\nand users are constrained by either inflexibility, limitations in\ncustomization, or a lack of persistent learning. This inflexibility is\nparticularly evident as users, especially those without programming skills,\nhave restricted avenues to enhance or personalize the model. Existing\nframeworks further complicate the model training and deployment process due to\ntheir computational inefficiencies and lack of user-friendly interfaces. To\novercome these challenges, this paper introduces a novel interaction\nparadigm-'Online Training using External Interactions'-that merges the benefits\nof persistent, real-time model updates with the flexibility for individual\ncustomization through external interactions such as AI agents or online/offline\nknowledge bases.",
      "tldr_zh": "本论文探讨了Large Language Models (LLMs) 在交互范式上的局限性，包括缺乏灵活性、自定义困难以及持久学习缺失，导致用户尤其是非编程人员难以个性化模型，且现有框架在训练和部署方面存在计算效率和用户界面问题。为解决这些挑战，论文提出了一种新交互范式——Online Training using External Interactions，通过外部交互（如AI agents或在线/离线知识库）实现实时模型更新和个性化定制。这种方法提升了LLMs 的实用性，融合了持久学习与灵活性，为用户友好型NLP 应用提供了更高效的框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04790v1",
      "published_date": "2024-03-04 10:00:55 UTC",
      "updated_date": "2024-03-04 10:00:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:22:27.814490"
    },
    {
      "arxiv_id": "2403.01895v1",
      "title": "Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyang Yuan",
        "Qinglin Cai",
        "Keting Yin"
      ],
      "abstract": "Distance-based time series anomaly detection methods are prevalent due to\ntheir relative non-parametric nature and interpretability. However, the\ncommonly used Euclidean distance is sensitive to noise. While existing works\nhave explored dynamic time warping (DTW) for its robustness, they only support\nsupervised tasks over multivariate time series (MTS), leaving a scarcity of\nunsupervised methods. In this work, we propose FCM-wDTW, an unsupervised\ndistance metric learning method for anomaly detection over MTS, which encodes\nraw data into latent space and reveals normal dimension relationships through\ncluster centers. FCM-wDTW introduces locally weighted DTW into fuzzy C-means\nclustering and learns the optimal latent space efficiently, enabling anomaly\nidentification via data reconstruction. Experiments with 11 different types of\nbenchmarks demonstrate our method's competitive accuracy and efficiency.",
      "tldr_zh": "本研究针对基于距离的多变量时间序列 (MTS) 异常检测问题，指出传统 Euclidean distance 易受噪声影响，而现有动态时间扭曲 (DTW) 方法仅限于监督任务，导致无监督方法缺乏。  \n为此，提出 FCM-wDTW，一种无监督的距离度量学习方法，将原始数据编码到潜在空间，通过模糊 C-means 聚类结合局部加权 DTW，揭示正常维度关系并高效学习最优潜在空间。  \n实验在 11 种不同基准上证明，FCM-wDTW 在准确性和效率上均表现出竞争性优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01895v1",
      "published_date": "2024-03-04 09:55:16 UTC",
      "updated_date": "2024-03-04 09:55:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:22:41.501343"
    },
    {
      "arxiv_id": "2403.01888v3",
      "title": "Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks",
      "title_zh": "翻译失败",
      "authors": [
        "Shuhei Watanabe",
        "Neeratyoy Mallik",
        "Edward Bergman",
        "Frank Hutter"
      ],
      "abstract": "While deep learning has celebrated many successes, its results often hinge on\nthe meticulous selection of hyperparameters (HPs). However, the time-consuming\nnature of deep learning training makes HP optimization (HPO) a costly endeavor,\nslowing down the development of efficient HPO tools. While zero-cost\nbenchmarks, which provide performance and runtime without actual training,\noffer a solution for non-parallel setups, they fall short in parallel setups as\neach worker must communicate its queried runtime to return its evaluation in\nthe exact order. This work addresses this challenge by introducing a\nuser-friendly Python package that facilitates efficient parallel HPO with\nzero-cost benchmarks. Our approach calculates the exact return order based on\nthe information stored in file system, eliminating the need for long waiting\ntimes and enabling much faster HPO evaluations. We first verify the correctness\nof our approach through extensive testing and the experiments with 6 popular\nHPO libraries show its applicability to diverse libraries and its ability to\nachieve over 1000x speedup compared to a traditional approach. Our package can\nbe installed via pip install mfhpo-simulator.",
      "tldr_zh": "这篇论文针对深度学习的超参数优化（HPO）过程耗时问题，提出了一种在零成本基准（zero-cost benchmarks）上进行异步多保真度优化的快速基准测试方法。该方法通过一个用户友好的 Python 包，利用文件系统存储信息来计算确切的返回顺序，从而避免并行设置中的等待时间，实现高效的并行 HPO 评估。实验验证了该方法的正确性，并在 6 个流行 HPO 库上实现了超过 1000 倍的速度提升。该包可通过 pip install mfhpo-simulator 轻松安装，为加速 HPO 开发提供了实用工具。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AutoML Conference 2024 ABCD Track",
      "pdf_url": "http://arxiv.org/pdf/2403.01888v3",
      "published_date": "2024-03-04 09:49:35 UTC",
      "updated_date": "2024-08-19 08:07:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:22:53.305990"
    },
    {
      "arxiv_id": "2403.01886v1",
      "title": "FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction",
      "title_zh": "FCDS：将成分句法和依存句法融入文档级关系抽取",
      "authors": [
        "Xudong Zhu",
        "Zhao Kang",
        "Bei Hui"
      ],
      "abstract": "Document-level Relation Extraction (DocRE) aims to identify relation labels\nbetween entities within a single document. It requires handling several\nsentences and reasoning over them. State-of-the-art DocRE methods use a graph\nstructure to connect entities across the document to capture dependency syntax\ninformation. However, this is insufficient to fully exploit the rich syntax\ninformation in the document. In this work, we propose to fuse constituency and\ndependency syntax into DocRE. It uses constituency syntax to aggregate the\nwhole sentence information and select the instructive sentences for the pairs\nof targets. It exploits the dependency syntax in a graph structure with\nconstituency syntax enhancement and chooses the path between entity pairs based\non the dependency graph. The experimental results on datasets from various\ndomains demonstrate the effectiveness of the proposed method. The code is\npublicly available at this url.",
      "tldr_zh": "本文提出 FCDS 方法，将成分句法 (constituency syntax) 和依赖句法 (dependency syntax) 融合到文档级关系抽取 (DocRE) 中，以解决现有方法仅依赖图结构不足以充分利用文档句法信息的问题。该方法利用成分句法聚合整个句子的信息，并选择针对实体对的有指导性句子；同时，在依赖句法图结构中进行成分句法增强，并基于依赖图选择实体对之间的路径。实验结果在各种领域的数据集上证明了 FCDS 的有效性，代码已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Appear in COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.01886v1",
      "published_date": "2024-03-04 09:48:55 UTC",
      "updated_date": "2024-03-04 09:48:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:23:05.247512"
    },
    {
      "arxiv_id": "2403.01875v4",
      "title": "Locally Convex Global Loss Network for Decision-Focused Learning",
      "title_zh": "局部凸全局损失网络用于决策导向学习",
      "authors": [
        "Haeun Jeon",
        "Hyunglip Bae",
        "Minsu Park",
        "Chanyeong Kim",
        "Woo Chang Kim"
      ],
      "abstract": "In decision-making problems under uncertainty, predicting unknown parameters\nis often considered independent of the optimization part. Decision-focused\nlearning (DFL) is a task-oriented framework that integrates prediction and\noptimization by adapting the predictive model to give better decisions for the\ncorresponding task. Here, an inevitable challenge arises when computing the\ngradients of the optimal decision with respect to the parameters. Existing\nresearch copes with this issue by smoothly reforming surrogate optimization or\nconstructing surrogate loss functions that mimic task loss. However, they are\napplied to restricted optimization domains. In this paper, we propose Locally\nConvex Global Loss Network (LCGLN), a global surrogate loss model that can be\nimplemented in a general DFL paradigm. LCGLN learns task loss via a partial\ninput convex neural network which is guaranteed to be convex for chosen inputs\nwhile keeping the non-convex global structure for the other inputs. This\nenables LCGLN to admit general DFL through only a single surrogate loss without\nany sense for choosing appropriate parametric forms. We confirm the\neffectiveness and flexibility of LCGLN by evaluating our proposed model with\nthree stochastic decision-making problems.",
      "tldr_zh": "这篇论文针对不确定性决策问题，提出 Locally Convex Global Loss Network (LCGLN) 作为一种全局代理损失模型，用于 Decision-Focused Learning (DFL) 框架，以整合预测和优化过程。LCGLN 采用部分输入凸神经网络（partial input convex neural network），确保在特定输入下保持凸性，同时保留全局非凸结构，从而避免了现有方法对优化域的限制，并无需选择特定参数形式。实验结果显示，LCGLN 在三个随机决策问题上表现出色，验证了其有效性和灵活性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI-25 (Oral Presentation)",
      "pdf_url": "http://arxiv.org/pdf/2403.01875v4",
      "published_date": "2024-03-04 09:31:56 UTC",
      "updated_date": "2025-02-10 10:48:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:23:17.338925"
    },
    {
      "arxiv_id": "2403.01861v1",
      "title": "AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes",
      "title_zh": "翻译失败",
      "authors": [
        "Jaehoon Jang",
        "Inha Lee",
        "Minje Kim",
        "Kyungdon Joo"
      ],
      "abstract": "Indoor scenes we are living in are visually homogenous or textureless, while\nthey inherently have structural forms and provide enough structural priors for\n3D scene reconstruction. Motivated by this fact, we propose a structure-aware\nonline signed distance fields (SDF) reconstruction framework in indoor scenes,\nespecially under the Atlanta world (AW) assumption. Thus, we dub this\nincremental SDF reconstruction for AW as AiSDF. Within the online framework, we\ninfer the underlying Atlanta structure of a given scene and then estimate\nplanar surfel regions supporting the Atlanta structure. This Atlanta-aware\nsurfel representation provides an explicit planar map for a given scene. In\naddition, based on these Atlanta planar surfel regions, we adaptively sample\nand constrain the structural regularity in the SDF reconstruction, which\nenables us to improve the reconstruction quality by maintaining a high-level\nstructure while enhancing the details of a given scene. We evaluate the\nproposed AiSDF on the ScanNet and ReplicaCAD datasets, where we demonstrate\nthat the proposed framework is capable of reconstructing fine details of\nobjects implicitly, as well as structures explicitly in room-scale scenes.",
      "tldr_zh": "本研究提出 AiSDF，一种结构感知的神经 Signed Distance Fields (SDF) 重建框架，针对室内场景的 Atlanta world (AW) 假设，旨在利用场景的内在结构形式提升 3D 重建质量。该框架通过推断场景的 Atlanta 结构、估计平面 surfel 区域并提供显式平面地图，同时自适应采样和约束 SDF 重建，以保持高层结构并增强细节。在 ScanNet 和 ReplicaCAD 数据集上的实验表明，AiSDF 能够显式重建结构并隐式捕捉物体细节，显著提高了重建效果。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 6 figures, Accepted to IEEE RA-L (First two authors\n  contributed equally)",
      "pdf_url": "http://arxiv.org/pdf/2403.01861v1",
      "published_date": "2024-03-04 09:18:13 UTC",
      "updated_date": "2024-03-04 09:18:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:23:29.914921"
    },
    {
      "arxiv_id": "2403.01851v1",
      "title": "Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral",
      "title_zh": "重新思考大语言模型语言适应：中文 Mixtral 的案例研究",
      "authors": [
        "Yiming Cui",
        "Xin Yao"
      ],
      "abstract": "Mixtral, a representative sparse mixture of experts (SMoE) language model,\nhas received significant attention due to its unique model design and superior\nperformance. Based on Mixtral-8x7B-v0.1, in this paper, we propose\nChinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language\nabilities by adopting further pre-training and instruction fine-tuning.\nExperimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct\nsuccessfully improve Chinese understanding and generation performance while\nretaining the original English abilities. Then, we discuss several key\nquestions when performing language adaptation on large language models,\nincluding the necessity of extending the language-specific vocabulary and the\nchoice of the initialization model (foundation model v.s. instruction model),\nby providing empirical results and analysis. We also present the visualizations\nof each expert to examine their importance on downstream tasks. Our resources\nare publicly available through \\url{https://github.com/ymcui/Chinese-Mixtral}.",
      "tldr_zh": "本论文重新审视大型语言模型(LLM)的语言适配问题，以 Mixtral-8x7B-v0.1 为基础，提出 Chinese-Mixtral 和 Chinese-Mixtral-Instruct，通过 further pre-training 和 instruction fine-tuning 来提升中文理解和生成性能，同时保留原有英文能力。实验结果显示，这些模型在中文任务上表现出色，并通过实证分析讨论了关键问题，如扩展语言特定词汇的必要性和初始化模型的选择（foundation model vs. instruction model）。此外，论文提供了专家的可视化分析，并公开了相关资源以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.01851v1",
      "published_date": "2024-03-04 09:01:10 UTC",
      "updated_date": "2024-03-04 09:01:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:23:40.836141"
    },
    {
      "arxiv_id": "2403.01849v1",
      "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
      "title_zh": "一个提示词就足以提升预训练视觉语言模型的对抗鲁棒性",
      "authors": [
        "Lin Li",
        "Haoyan Guan",
        "Jianing Qiu",
        "Michael Spratling"
      ],
      "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having\nremarkable generalization ability, are highly vulnerable to adversarial\nexamples. This work studies the adversarial robustness of VLMs from the novel\nperspective of the text prompt instead of the extensively studied model weights\n(frozen in this work). We first show that the effectiveness of both adversarial\nattack and defense are sensitive to the used text prompt. Inspired by this, we\npropose a method to improve resilience to adversarial attacks by learning a\nrobust text prompt for VLMs. The proposed method, named Adversarial Prompt\nTuning (APT), is effective while being both computationally and data efficient.\nExtensive experiments are conducted across 15 datasets and 4 data sparsity\nschemes (from 1-shot to full training data settings) to show APT's superiority\nover hand-engineered prompts and other state-of-the-art adaption methods. APT\ndemonstrated excellent abilities in terms of the in-distribution performance\nand the generalization under input distribution shift and across datasets.\nSurprisingly, by simply adding one learned word to the prompts, APT can\nsignificantly boost the accuracy and robustness (epsilon=4/255) over the\nhand-engineered prompts by +13% and +8.5% on average respectively. The\nimprovement further increases, in our most effective setting, to +26.4% for\naccuracy and +16.7% for robustness. Code is available at\nhttps://github.com/TreeLLi/APT.",
      "tldr_zh": "这篇论文从文本提示的视角研究了预训练视觉语言模型(VLMs)如 CLIP 的对抗鲁棒性，发现提示的设计对攻击和防御的有效性高度敏感。作者提出了一种高效方法Adversarial Prompt Tuning (APT)，通过学习一个鲁棒的文本提示（仅需添加一个单词）来提升模型的抵抗力，而无需修改冻结的模型权重。实验在15个数据集和多种数据稀疏方案下证明，APT 相较手工提示和现有SOTA方法，平均提升准确率13%和鲁棒性（epsilon=4/255）8.5%，在最优设置下进一步提升至准确率26.4%和鲁棒性16.7%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR2024",
      "pdf_url": "http://arxiv.org/pdf/2403.01849v1",
      "published_date": "2024-03-04 08:59:32 UTC",
      "updated_date": "2024-03-04 08:59:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:23:54.462167"
    },
    {
      "arxiv_id": "2403.01845v2",
      "title": "NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models",
      "title_zh": "NASH：针对硬件优化的机器学习模型的神经架构搜索",
      "authors": [
        "Mengfei Ji",
        "Yuchun Chang",
        "Baolin Zhang",
        "Zaid Al-Ars"
      ],
      "abstract": "As machine learning (ML) algorithms get deployed in an ever-increasing number\nof applications, these algorithms need to achieve better trade-offs between\nhigh accuracy, high throughput and low latency. This paper introduces NASH, a\nnovel approach that applies neural architecture search to machine learning\nhardware. Using NASH, hardware designs can achieve not only high throughput and\nlow latency but also superior accuracy performance. We present four versions of\nthe NASH strategy in this paper, all of which show higher accuracy than the\noriginal models. The strategy can be applied to various convolutional neural\nnetworks, selecting specific model operations among many to guide the training\nprocess toward higher accuracy. Experimental results show that applying NASH on\nResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top\n5 accuracy increase of up to 2.2% compared to the non-NASH version when tested\non the ImageNet data set. We also integrated this approach into the FINN\nhardware model synthesis tool to automate the application of our approach and\nthe generation of the hardware model. Results show that using FINN can achieve\na maximum throughput of 324.5 fps. In addition, NASH models can also result in\na better trade-off between accuracy and hardware resource utilization. The\naccuracy-hardware (HW) Pareto curve shows that the models with the four NASH\nversions represent the best trade-offs achieving the highest accuracy for a\ngiven HW utilization. The code for our implementation is open-source and\npublicly available on GitHub at https://github.com/MFJI/NASH.",
      "tldr_zh": "本论文提出 NASH，一种将 Neural Architecture Search 应用于硬件优化的机器学习模型方法，旨在实现高准确性、高吞吐量和低延迟的平衡。NASH 通过选择特定模型操作指导训练过程，并在 ResNet18 和 ResNet34 等卷积神经网络上测试，实验结果显示在 ImageNet 数据集上，top-1 准确率提高高达 3.1%，top-5 准确率提高高达 2.2%。此外，该方法集成 FINN 硬件工具，达到最大吞吐量 324.5 fps，并优化准确性与硬件资源利用的 Pareto 曲线，提供更好的性能权衡，相关代码已在 GitHub 开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01845v2",
      "published_date": "2024-03-04 08:51:38 UTC",
      "updated_date": "2024-03-10 05:49:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:24:05.977739"
    },
    {
      "arxiv_id": "2404.01308v1",
      "title": "Learning to Solve Job Shop Scheduling under Uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Guillaume Infantes",
        "Stéphanie Roussel",
        "Pierre Pereira",
        "Antoine Jacquet",
        "Emmanuel Benazera"
      ],
      "abstract": "Job-Shop Scheduling Problem (JSSP) is a combinatorial optimization problem\nwhere tasks need to be scheduled on machines in order to minimize criteria such\nas makespan or delay. To address more realistic scenarios, we associate a\nprobability distribution with the duration of each task. Our objective is to\ngenerate a robust schedule, i.e. that minimizes the average makespan. This\npaper introduces a new approach that leverages Deep Reinforcement Learning\n(DRL) techniques to search for robust solutions, emphasizing JSSPs with\nuncertain durations. Key contributions of this research include: (1)\nadvancements in DRL applications to JSSPs, enhancing generalization and\nscalability, (2) a novel method for addressing JSSPs with uncertain durations.\nThe Wheatley approach, which integrates Graph Neural Networks (GNNs) and DRL,\nis made publicly available for further research and applications.",
      "tldr_zh": "本研究针对 Job-Shop Scheduling Problem (JSSP) 在不确定任务持续时间下的优化问题，使用 Deep Reinforcement Learning (DRL) 技术来生成鲁棒调度，以最小化平均 makespan。方法引入了 Wheatley 框架，该框架整合 Graph Neural Networks (GNNs) 和 DRL，提升了解决方案的泛化和可扩展性。关键贡献包括 DRL 在 JSSP 应用的进展，以及一个新颖的处理不确定持续时间的方法，并公开了 Wheatley 框架以供进一步研究和应用。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "To be published at CPAIOR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01308v1",
      "published_date": "2024-03-04 08:38:55 UTC",
      "updated_date": "2024-03-04 08:38:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:24:17.575277"
    },
    {
      "arxiv_id": "2403.04789v2",
      "title": "TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jiamin Luo",
        "Jingjing Wang",
        "Guodong Zhou"
      ],
      "abstract": "Multimodal Conversational Emotion (MCE) detection, generally spanning across\nthe acoustic, vision and language modalities, has attracted increasing interest\nin the multimedia community. Previous studies predominantly focus on learning\ncontextual information in conversations with only a few considering the topic\ninformation in single language modality, while always neglecting the acoustic\nand vision topic information. On this basis, we propose a model-agnostic\nTopic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic\ninformation in MCE tasks. Particularly, we integrate the diffusion model into\nneural topic model to alleviate the diversity deficiency problem of neural\ntopic model in capturing topic information. Detailed evaluations demonstrate\nthe significant improvements of TopicDiff over the state-of-the-art MCE\nbaselines, justifying the importance of multimodal topic information to MCE and\nthe effectiveness of TopicDiff in capturing such information. Furthermore, we\nobserve an interesting finding that the topic information in acoustic and\nvision is more discriminative and robust compared to the language.",
      "tldr_zh": "本文提出了一种模型无关的 Topic-enriched Diffusion (TopicDiff) 方法，用于 Multimodal Conversational Emotion (MCE) 检测，旨在捕捉声学、视觉和语言模态中的主题信息，以弥补现有方法忽略多模态主题的不足。TopicDiff 通过将扩散模型集成到神经主题模型中，缓解了神经主题模型在主题信息捕捉时的多样性问题。实验结果显示，该方法在 MCE 任务上显著优于现有基准模型，并发现声学和视觉模态的主题信息比语言模态更具区分性和鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04789v2",
      "published_date": "2024-03-04 08:38:53 UTC",
      "updated_date": "2024-03-11 01:04:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:24:29.853810"
    },
    {
      "arxiv_id": "2403.01840v1",
      "title": "FreeA: Human-object Interaction Detection using Free Annotation Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiao Wang",
        "Zhenao Wei",
        "Xinyu Jiang",
        "Yu Lei",
        "Weiying Xue",
        "Jinxiu Liu",
        "Qi Liu"
      ],
      "abstract": "Recent human-object interaction (HOI) detection approaches rely on high cost\nof manpower and require comprehensive annotated image datasets. In this paper,\nwe propose a novel self-adaption language-driven HOI detection method, termed\nas FreeA, without labeling by leveraging the adaptability of CLIP to generate\nlatent HOI labels. To be specific, FreeA matches image features of human-object\npairs with HOI text templates, and a priori knowledge-based mask method is\ndeveloped to suppress improbable interactions. In addition, FreeA utilizes the\nproposed interaction correlation matching method to enhance the likelihood of\nactions related to a specified action, further refine the generated HOI labels.\nExperiments on two benchmark datasets show that FreeA achieves state-of-the-art\nperformance among weakly supervised HOI models. Our approach is +8.58 mean\nAverage Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in\nlocalizing and classifying the interactive actions than the newest weakly\nmodel, and +1.68 mAP and +7.28 mAP than the latest weakly+ model, respectively.\nCode will be available at https://drliuqi.github.io/.",
      "tldr_zh": "该论文提出了一种名为 FreeA 的自适应语言驱动方法，用于人类物体交互（HOI）检测，无需手动标注，而是利用 CLIP 的适应性生成潜在 HOI 标签。FreeA 通过将图像特征与 HOI 文本模板匹配，并结合基于先验知识的掩码方法抑制不可能的交互，以及交互相关性匹配方法来增强和完善生成的标签，从而提高检测准确性。在 HICO-DET 和 V-COCO 等基准数据集上的实验表明，FreeA 比最新弱监督模型提升了 +8.58 mAP 和 +1.23 mAP，分别比弱+模型高 +1.68 mAP 和 +7.28 mAP，展示了其在弱监督 HOI 检测中的最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 7 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.01840v1",
      "published_date": "2024-03-04 08:38:15 UTC",
      "updated_date": "2024-03-04 08:38:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:24:43.475766"
    },
    {
      "arxiv_id": "2403.01832v1",
      "title": "Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals and Industrial Pragmatism",
      "title_zh": "基于模型的数据中心 AI：弥合学术理想与工业实用主义之间的分歧",
      "authors": [
        "Chanjun Park",
        "Minsoo Khang",
        "Dahyun Kim"
      ],
      "abstract": "This paper delves into the contrasting roles of data within academic and\nindustrial spheres, highlighting the divergence between Data-Centric AI and\nModel-Agnostic AI approaches. We argue that while Data-Centric AI focuses on\nthe primacy of high-quality data for model performance, Model-Agnostic AI\nprioritizes algorithmic flexibility, often at the expense of data quality\nconsiderations. This distinction reveals that academic standards for data\nquality frequently do not meet the rigorous demands of industrial applications,\nleading to potential pitfalls in deploying academic models in real-world\nsettings. Through a comprehensive analysis, we address these disparities,\npresenting both the challenges they pose and strategies for bridging the gap.\nFurthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which\naims to reconcile these differences by integrating model considerations into\ndata optimization processes. This approach underscores the necessity for\nevolving data requirements that are sensitive to the nuances of both academic\nresearch and industrial deployment. By exploring these discrepancies, we aim to\nfoster a more nuanced understanding of data's role in AI development and\nencourage a convergence of academic and industrial standards to enhance AI's\nreal-world applicability.",
      "tldr_zh": "这篇论文探讨了学术和工业领域在AI数据处理上的分歧，指出Data-Centric AI强调高质量数据的重要性，而Model-Agnostic AI更注重算法灵活性，往往忽略数据质量，导致学术标准难以满足工业应用的严格要求。作者通过全面分析这些差异，提出了Model-Based Data-Centric AI的新范式，该方法将模型考虑整合到数据优化过程中，以桥接学术理想与工业实际的鸿沟。最终，该框架旨在促进学术和工业标准的融合，提升AI在真实世界的适用性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for Data-centric Machine Learning Research (DMLR) Workshop\n  at ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.01832v1",
      "published_date": "2024-03-04 08:29:15 UTC",
      "updated_date": "2024-03-04 08:29:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:24:55.027069"
    },
    {
      "arxiv_id": "2403.01827v3",
      "title": "Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification",
      "title_zh": "时序数据分类的分析与完全基于忆阻器的Reservoir计算",
      "authors": [
        "Ankur Singh",
        "Sanghyeon Choi",
        "Gunuk Wang",
        "Maryaradhiya Daimari",
        "Byung-Geun Lee"
      ],
      "abstract": "Reservoir computing (RC) offers a neuromorphic framework that is particularly\neffective for processing spatiotemporal signals. Known for its temporal\nprocessing prowess, RC significantly lowers training costs compared to\nconventional recurrent neural networks. A key component in its hardware\ndeployment is the ability to generate dynamic reservoir states. Our research\nintroduces a novel dual-memory RC system, integrating a short-term memory via a\nWOx-based memristor, capable of achieving 16 distinct states encoded over 4\nbits, and a long-term memory component using a TiOx-based memristor within the\nreadout layer. We thoroughly examine both memristor types and leverage the RC\nsystem to process temporal data sets. The performance of the proposed RC system\nis validated through two benchmark tasks: isolated spoken digit recognition\nwith incomplete inputs and Mackey-Glass time series prediction. The system\ndelivered an impressive 98.84% accuracy in digit recognition and sustained a\nlow normalized root mean square error (NRMSE) of 0.036 in the time series\nprediction task, underscoring its capability. This study illuminates the\nadeptness of memristor-based RC systems in managing intricate temporal\nchallenges, laying the groundwork for further innovations in neuromorphic\ncomputing.",
      "tldr_zh": "本文分析并提出了一种全 memristor 基础的 Reservoir Computing (RC) 系统，用于时间数据分类。该系统整合了短时记忆（使用 WOx-based memristor 实现 16 个状态编码于 4 bits）和长时记忆（使用 TiOx-based memristor 在 readout 层），以生成动态 reservoir 状态并降低训练成本。通过实验验证，该系统在孤立口语数字识别任务中达到 98.84% 的准确率，并在 Mackey-Glass 时间序列预测任务中获得 0.036 的 NRMSE，展示了其处理复杂时空信号的能力。该研究为神经形态计算领域的创新奠定了基础。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "22 pages, 20 figures, Journal, Typo corrected and updated reference",
      "pdf_url": "http://arxiv.org/pdf/2403.01827v3",
      "published_date": "2024-03-04 08:22:29 UTC",
      "updated_date": "2025-03-21 06:52:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:25:07.344892"
    },
    {
      "arxiv_id": "2403.01823v2",
      "title": "RT-H: Action Hierarchies Using Language",
      "title_zh": "RT-H：使用语言的动作层次结构",
      "authors": [
        "Suneel Belkhale",
        "Tianli Ding",
        "Ted Xiao",
        "Pierre Sermanet",
        "Quon Vuong",
        "Jonathan Tompson",
        "Yevgen Chebotar",
        "Debidatta Dwibedi",
        "Dorsa Sadigh"
      ],
      "abstract": "Language provides a way to break down complex concepts into digestible\npieces. Recent works in robot imitation learning use language-conditioned\npolicies that predict actions given visual observations and the high-level task\nspecified in language. These methods leverage the structure of natural language\nto share data between semantically similar tasks (e.g., \"pick coke can\" and\n\"pick an apple\") in multi-task datasets. However, as tasks become more\nsemantically diverse (e.g., \"pick coke can\" and \"pour cup\"), sharing data\nbetween tasks becomes harder, so learning to map high-level tasks to actions\nrequires much more demonstration data. To bridge tasks and actions, our insight\nis to teach the robot the language of actions, describing low-level motions\nwith more fine-grained phrases like \"move arm forward\". Predicting these\nlanguage motions as an intermediate step between tasks and actions forces the\npolicy to learn the shared structure of low-level motions across seemingly\ndisparate tasks. Furthermore, a policy that is conditioned on language motions\ncan easily be corrected during execution through human-specified language\nmotions. This enables a new paradigm for flexible policies that can learn from\nhuman intervention in language. Our method RT-H builds an action hierarchy\nusing language motions: it first learns to predict language motions, and\nconditioned on this and the high-level task, it predicts actions, using visual\ncontext at all stages. We show that RT-H leverages this language-action\nhierarchy to learn policies that are more robust and flexible by effectively\ntapping into multi-task datasets. We show that these policies not only allow\nfor responding to language interventions, but can also learn from such\ninterventions and outperform methods that learn from teleoperated\ninterventions. Our website and videos are found at\nhttps://rt-hierarchy.github.io.",
      "tldr_zh": "该研究提出 RT-H 方法，利用语言构建动作层次，以解决机器人模仿学习（imitation learning）在语义多样任务间数据共享的挑战。RT-H 先预测细粒度语言动作（如“move arm forward”）作为任务和实际动作之间的中间步骤，然后结合高水平任务和视觉上下文预测最终动作。这种层次化方法允许策略学习低级动作的共享结构，提高了多任务数据集的利用效率。实验结果显示，RT-H 使机器人策略更鲁棒，能够响应和学习人类语言干预，并优于基于遥操作干预的方法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01823v2",
      "published_date": "2024-03-04 08:16:11 UTC",
      "updated_date": "2024-06-01 01:54:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:25:18.543234"
    },
    {
      "arxiv_id": "2403.14668v1",
      "title": "Predicting Learning Performance with Large Language Models: A Study in Adult Literacy",
      "title_zh": "使用大型语言模型预测学习表现：成人识字领域的研究",
      "authors": [
        "Liang Zhang",
        "Jionghao Lin",
        "Conrad Borchers",
        "John Sabatini",
        "John Hollander",
        "Meng Cao",
        "Xiangen Hu"
      ],
      "abstract": "Intelligent Tutoring Systems (ITSs) have significantly enhanced adult\nliteracy training, a key factor for societal participation, employment\nopportunities, and lifelong learning. Our study investigates the application of\nadvanced AI models, including Large Language Models (LLMs) like GPT-4, for\npredicting learning performance in adult literacy programs in ITSs. This\nresearch is motivated by the potential of LLMs to predict learning performance\nbased on its inherent reasoning and computational capabilities. By using\nreading comprehension datasets from the ITS, AutoTutor, we evaluate the\npredictive capabilities of GPT-4 versus traditional machine learning methods in\npredicting learning performance through five-fold cross-validation techniques.\nOur findings show that the GPT-4 presents the competitive predictive abilities\nwith traditional machine learning methods such as Bayesian Knowledge Tracing,\nPerformance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor\nfactorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained\non local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected\nXGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior\nperformance compared to local machine execution. Moreover, our investigation\ninto hyper-parameter tuning by GPT-4 versus grid-search suggests comparable\nperformance, albeit with less stability in the automated approach, using\nXGBoost as the case study. Our study contributes to the field by highlighting\nthe potential of integrating LLMs with traditional machine learning models to\nenhance predictive accuracy and personalize adult literacy education, setting a\nfoundation for future research in applying LLMs within ITSs.",
      "tldr_zh": "本研究探讨了使用Large Language Models (LLMs)如GPT-4来预测成人识字程序中学习表现的可能性，旨在提升Intelligent Tutoring Systems (ITSs)对成人教育的支持。研究采用AutoTutor的阅读理解数据集，通过五折交叉验证比较GPT-4与传统机器学习方法（如Bayesian Knowledge Tracing、XGBoost等）的预测能力，发现GPT-4的性能与这些方法相当，而GPT-4优化后的XGBoost模型在准确性上优于本地训练版本。总体而言，该工作突出了LLMs与传统模型整合的潜力，能够提高预测准确性和个性化教育，为未来ITSs中的AI应用奠定基础。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "26TH International Conference on Human-Computer Interaction",
      "pdf_url": "http://arxiv.org/pdf/2403.14668v1",
      "published_date": "2024-03-04 08:14:07 UTC",
      "updated_date": "2024-03-04 08:14:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:25:31.125035"
    },
    {
      "arxiv_id": "2403.02363v1",
      "title": "Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity",
      "title_zh": "翻译失败",
      "authors": [
        "Ying-Hsuan Wu",
        "Jun-Wei Hsieh",
        "Li Xin",
        "Shin-You Teng",
        "Yi-Kuan Hsieh",
        "Ming-Ching Chang"
      ],
      "abstract": "Real-world datasets commonly exhibit noisy labels and class imbalance, such\nas long-tailed distributions. While previous research addresses this issue by\ndifferentiating noisy and clean samples, reliance on information from\npredictions based on noisy long-tailed data introduces potential errors. To\novercome the limitations of prior works, we introduce an effective two-stage\napproach by combining soft-label refurbishing with multi-expert ensemble\nlearning. In the first stage of robust soft label refurbishing, we acquire\nunbiased features through contrastive learning, making preliminary predictions\nusing a classifier trained with a carefully designed BAlanced Noise-tolerant\nCross-entropy (BANC) loss. In the second stage, our label refurbishment method\nis applied to obtain soft labels for multi-expert ensemble learning, providing\na principled solution to the long-tail noisy label problem. Experiments\nconducted across multiple benchmarks validate the superiority of our approach,\nLabel Refurbishment considering Label Rarity (LR^2), achieving remarkable\naccuracies of 94.19% and 77.05% on simulated noisy CIFAR-10 and CIFAR-100\nlong-tail datasets, as well as 77.74% and 81.40% on real-noise long-tail\ndatasets, Food-101N and Animal-10N, surpassing existing state-of-the-art\nmethods.",
      "tldr_zh": "本文提出了一种名为 LR^2 的两阶段方法，用于解决长尾噪声标签学习问题，该方法考虑标签稀有性，通过软标签修复和多专家集成学习来改善模型性能。第一阶段利用对比学习（contrastive learning）获取无偏特征，并采用 BAlanced Noise-tolerant Cross-entropy (BANC) 损失训练分类器进行初步预测；第二阶段则应用标签修复技术生成软标签，支持多专家集成学习，从而缓解噪声和类别不平衡的影响。实验在多个基准数据集上验证了该方法的优越性，包括在模拟噪声的 CIFAR-10 和 CIFAR-100 上分别达到 94.19% 和 77.05% 的准确率，以及在真实噪声长尾数据集 Food-101N 和 Animal-10N 上取得 77.74% 和 81.40% 的表现，超越了现有最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02363v1",
      "published_date": "2024-03-04 08:06:57 UTC",
      "updated_date": "2024-03-04 08:06:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:25:43.758014"
    },
    {
      "arxiv_id": "2403.01818v3",
      "title": "AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Haonan Wang",
        "Qixiang Zhang",
        "Yi Li",
        "Xiaomeng Li"
      ],
      "abstract": "Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate\nthe burden of time-consuming pixel-level manual labeling, which leverages\nlimited labeled data along with larger amounts of unlabeled data. Current\nstate-of-the-art methods train the labeled data with ground truths and\nunlabeled data with pseudo labels. However, the two training flows are\nseparate, which allows labeled data to dominate the training process, resulting\nin low-quality pseudo labels and, consequently, sub-optimal results. To\nalleviate this issue, we present AllSpark, which reborns the labeled features\nfrom unlabeled ones with the channel-wise cross-attention mechanism. We further\nintroduce a Semantic Memory along with a Channel Semantic Grouping strategy to\nensure that unlabeled features adequately represent labeled features. The\nAllSpark shed new light on the architecture level designs of SSSS rather than\nframework level, which avoids increasingly complicated training pipeline\ndesigns. It can also be regarded as a flexible bottleneck module that can be\nseamlessly integrated into a general transformer-based segmentation model. The\nproposed AllSpark outperforms existing methods across all evaluation protocols\non Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and\nmodel weights are available at: https://github.com/xmed-lab/AllSpark.",
      "tldr_zh": "本论文提出 AllSpark 框架，用于改进半监督语义分割 (Semi-Supervised Semantic Segmentation)，通过通道-wise 交叉注意力机制从未标注特征中“重生”标注特征，从而解决现有方法中训练流程分离导致伪标签质量低的问题。AllSpark 还引入 Semantic Memory 和 Channel Semantic Grouping 策略，确保未标注特征能充分代表标注特征，并作为灵活的瓶颈模块无缝集成到 Transformer-based 模型中。该框架在架构层面创新，避免复杂训练管道，在 Pascal、Cityscapes 和 COCO 基准测试中超越现有方法，提供高效的半监督训练方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024; correct typos; this is not the camera-ready\n  version",
      "pdf_url": "http://arxiv.org/pdf/2403.01818v3",
      "published_date": "2024-03-04 08:06:41 UTC",
      "updated_date": "2024-03-14 15:39:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:25:54.699026"
    },
    {
      "arxiv_id": "2403.01816v1",
      "title": "SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition",
      "title_zh": "SMAUG：一种基于滑动多维任务窗口的多智能体强化学习框架，用于自适应",
      "authors": [
        "Wenjing Zhang",
        "Wei Zhang"
      ],
      "abstract": "Instead of making behavioral decisions directly from the exponentially\nexpanding joint observational-action space, subtask-based multi-agent\nreinforcement learning (MARL) methods enable agents to learn how to tackle\ndifferent subtasks. Most existing subtask-based MARL methods are based on\nhierarchical reinforcement learning (HRL). However, these approaches often\nlimit the number of subtasks, perform subtask recognition periodically, and can\nonly identify and execute a specific subtask within the predefined fixed time\nperiod, which makes them inflexible and not suitable for diverse and dynamic\nscenarios with constantly changing subtasks. To break through above\nrestrictions, a \\textbf{S}liding \\textbf{M}ultidimensional t\\textbf{A}sk window\nbased m\\textbf{U}ti-agent reinforcement learnin\\textbf{G} framework (SMAUG) is\nproposed for adaptive real-time subtask recognition. It leverages a sliding\nmultidimensional task window to extract essential information of subtasks from\ntrajectory segments concatenated based on observed and predicted trajectories\nin varying lengths. An inference network is designed to iteratively predict\nfuture trajectories with the subtask-oriented policy network. Furthermore,\nintrinsic motivation rewards are defined to promote subtask exploration and\nbehavior diversity. SMAUG can be integrated with any Q-learning-based approach.\nExperiments on StarCraft II show that SMAUG not only demonstrates performance\nsuperiority in comparison with all baselines but also presents a more prominent\nand swift rise in rewards during the initial training stage.",
      "tldr_zh": "该论文提出SMAUG框架，一种基于滑动多维任务窗口的MARL（Multi-Agent Reinforcement Learning）方法，用于实现自适应实时子任务识别，以克服传统HRL（Hierarchical Reinforcement Learning）方法的局限，如子任务数量限制和不灵活性。SMAUG通过滑动多维任务窗口从观察和预测轨迹段中提取子任务信息，并结合推理网络、子任务导向政策网络以及内在动机奖励来促进子任务探索和行为多样性，同时可与任何Q-learning-based方法整合。实验结果显示，在StarCraft II环境中，SMAUG比基线方法表现出色，并在初始训练阶段实现更显著和快速的奖励提升。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01816v1",
      "published_date": "2024-03-04 08:04:41 UTC",
      "updated_date": "2024-03-04 08:04:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:26:07.489153"
    },
    {
      "arxiv_id": "2403.01801v1",
      "title": "COLA: Cross-city Mobility Transformer for Human Trajectory Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Wang",
        "Tongya Zheng",
        "Yuxuan Liang",
        "Shunyu Liu",
        "Mingli Song"
      ],
      "abstract": "Human trajectory data produced by daily mobile devices has proven its\nusefulness in various substantial fields such as urban planning and epidemic\nprevention. In terms of the individual privacy concern, human trajectory\nsimulation has attracted increasing attention from researchers, targeting at\noffering numerous realistic mobility data for downstream tasks. Nevertheless,\nthe prevalent issue of data scarcity undoubtedly degrades the reliability of\nexisting deep learning models. In this paper, we are motivated to explore the\nintriguing problem of mobility transfer across cities, grasping the universal\npatterns of human trajectories to augment the powerful Transformer with\nexternal mobility data. There are two crucial challenges arising in the\nknowledge transfer across cities: 1) how to transfer the Transformer to adapt\nfor domain heterogeneity; 2) how to calibrate the Transformer to adapt for\nsubtly different long-tail frequency distributions of locations. To address\nthese challenges, we have tailored a Cross-city mObiLity trAnsformer (COLA)\nwith a dedicated model-agnostic transfer framework by effectively transferring\ncross-city knowledge for human trajectory simulation. Firstly, COLA divides the\nTransformer into the private modules for city-specific characteristics and the\nshared modules for city-universal mobility patterns. Secondly, COLA leverages a\nlightweight yet effective post-hoc adjustment strategy for trajectory\nsimulation, without disturbing the complex bi-level optimization of\nmodel-agnostic knowledge transfer. Extensive experiments of COLA compared to\nstate-of-the-art single-city baselines and our implemented cross-city baselines\nhave demonstrated its superiority and effectiveness. The code is available at\nhttps://github.com/Star607/Cross-city-Mobility-Transformer.",
      "tldr_zh": "这篇论文提出 COLA，一种跨城市迁移 Transformer，用于模拟人类轨迹，以解决数据稀缺导致的模型可靠性问题，并捕捉城市间通用移动模式。COLA 通过将 Transformer 分解为私有模块（处理城市特定特征）和共享模块（捕捉通用模式），并采用模型无关的知识转移框架来适应域异质性。论文还引入轻量级后处理调整策略，以校准长尾频率分布，而不干扰优化过程。实验结果显示，COLA 比现有单城市和跨城市基线模型表现出色，提升了轨迹模拟的准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WWW 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.01801v1",
      "published_date": "2024-03-04 07:45:29 UTC",
      "updated_date": "2024-03-04 07:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:26:18.360920"
    },
    {
      "arxiv_id": "2403.01791v1",
      "title": "Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making",
      "title_zh": "翻译失败",
      "authors": [
        "Shuai Ma",
        "Chenyi Zhang",
        "Xinru Wang",
        "Xiaojuan Ma",
        "Ming Yin"
      ],
      "abstract": "Artificial Intelligence (AI) is increasingly employed in various\ndecision-making tasks, typically as a Recommender, providing recommendations\nthat the AI deems correct. However, recent studies suggest this may diminish\nhuman analytical thinking and lead to humans' inappropriate reliance on AI,\nimpairing the synergy in human-AI teams. In contrast, human advisors in group\ndecision-making perform various roles, such as analyzing alternative options or\ncriticizing decision-makers to encourage their critical thinking. This\ndiversity of roles has not yet been empirically explored in AI assistance. In\nthis paper, we examine three AI roles: Recommender, Analyzer, and Devil's\nAdvocate, and evaluate their effects across two AI performance levels. Our\nresults show each role's distinct strengths and limitations in task\nperformance, reliance appropriateness, and user experience. Notably, the\nRecommender role is not always the most effective, especially if the AI\nperformance level is low, the Analyzer role may be preferable. These insights\noffer valuable implications for designing AI assistants with adaptive\nfunctional roles according to different situations.",
      "tldr_zh": "这篇论文探索了 AI 在决策支持中的不同角色，包括 Recommender、Analyzer 和 Devil's Advocate，以解决传统 Recommender 模式可能削弱人类分析思考和导致不当依赖的问题。研究通过实验评估了这些角色在不同 AI 性能水平下的影响，结果显示每个角色具有独特的优势和局限性，例如在 AI 性能较低时，Analyzer 角色可能比 Recommender 更有效。这些发现为设计适应性 AI 助手提供了重要启示，帮助优化人类-AI 团队的协同效果。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01791v1",
      "published_date": "2024-03-04 07:32:28 UTC",
      "updated_date": "2024-03-04 07:32:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:26:30.726793"
    },
    {
      "arxiv_id": "2403.01784v1",
      "title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenru Lin",
        "Yiqun Yao",
        "Yang Yuan"
      ],
      "abstract": "Large language models (LLMs) such as ChatGPT are increasingly proficient in\nunderstanding and generating a mixture of code and text. Evaluation based on\nsuch $\\textit{mixture}$ can lead to a more comprehensive understanding of the\nmodels' abilities in solving coding problems. However, in this context, current\nevaluation methods are either limited in task coverage or lack standardization.\nTo address this issue, we propose using category theory as a framework for\nevaluation. Specifically, morphisms within a code category can represent code\ndebugging and transformation, functors between two categories represent code\ntranslation, and functors between a code category and a natural language\ncategory represent code generation, explanation, and reproduction. We present\nan automatic evaluation framework called $\\textbf{CatCode}$\n($\\textbf{Cat}$egory $\\textbf{Code}$) that can comprehensively assess the\ncoding abilities of LLMs, including ChatGPT, Text-Davinci, and CodeGeeX.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）如ChatGPT在处理代码和文本混合时的能力评估问题，提出CatCode框架，以category theory作为标准化评估基础。框架将morphisms用于表示代码调试和转换，functors用于代码翻译，以及代码范畴和自然语言范畴间的functors用于代码生成、解释和再现，从而全面覆盖评估任务。CatCode支持自动评估多种LLMs模型，包括ChatGPT、Text-Davinci和CodeGeeX，提供更可靠的编码能力分析。",
      "categories": [
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.01784v1",
      "published_date": "2024-03-04 07:26:07 UTC",
      "updated_date": "2024-03-04 07:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:26:44.393297"
    },
    {
      "arxiv_id": "2403.01781v1",
      "title": "Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning",
      "title_zh": "整合高效最优传输和功能映射用于无监督形状对应学习",
      "authors": [
        "Tung Le",
        "Khai Nguyen",
        "Shanlin Sun",
        "Nhat Ho",
        "Xiaohui Xie"
      ],
      "abstract": "In the realm of computer vision and graphics, accurately establishing\ncorrespondences between geometric 3D shapes is pivotal for applications like\nobject tracking, registration, texture transfer, and statistical shape\nanalysis. Moving beyond traditional hand-crafted and data-driven feature\nlearning methods, we incorporate spectral methods with deep learning, focusing\non functional maps (FMs) and optimal transport (OT). Traditional OT-based\napproaches, often reliant on entropy regularization OT in learning-based\nframework, face computational challenges due to their quadratic cost. Our key\ncontribution is to employ the sliced Wasserstein distance (SWD) for OT, which\nis a valid fast optimal transport metric in an unsupervised shape matching\nframework. This unsupervised framework integrates functional map regularizers\nwith a novel OT-based loss derived from SWD, enhancing feature alignment\nbetween shapes treated as discrete probability measures. We also introduce an\nadaptive refinement process utilizing entropy regularized OT, further refining\nfeature alignments for accurate point-to-point correspondences. Our method\ndemonstrates superior performance in non-rigid shape matching, including\nnear-isometric and non-isometric scenarios, and excels in downstream tasks like\nsegmentation transfer. The empirical results on diverse datasets highlight our\nframework's effectiveness and generalization capabilities, setting new\nstandards in non-rigid shape matching with efficient OT metrics and an adaptive\nrefinement module.",
      "tldr_zh": "本研究提出了一种无监督形状对应学习框架，将高效的Optimal Transport (OT)与Functional Maps (FMs)整合，用于计算机视觉和图形学中的3D形状匹配任务，如对象跟踪和纹理转移。核心创新在于采用Sliced Wasserstein Distance (SWD)作为快速OT指标，结合FMs正则化和基于SWD的损失函数，提升形状作为离散概率测度的特征对齐，并引入自适应精炼过程利用熵正则化OT来优化点到点对应。实验结果显示，该方法在非刚性形状匹配（包括近等距和非等距场景）中表现出色，并在下游任务如分割转移上实现优越性能，并在多种数据集上证明了其有效性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.01781v1",
      "published_date": "2024-03-04 07:21:07 UTC",
      "updated_date": "2024-03-04 07:21:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:26:56.229872"
    },
    {
      "arxiv_id": "2403.01773v2",
      "title": "Improving out-of-distribution generalization in graphs via hierarchical semantic environments",
      "title_zh": "翻译失败",
      "authors": [
        "Yinhua Piao",
        "Sangseon Lee",
        "Yijingxiu Lu",
        "Sun Kim"
      ],
      "abstract": "Out-of-distribution (OOD) generalization in the graph domain is challenging\ndue to complex distribution shifts and a lack of environmental contexts. Recent\nmethods attempt to enhance graph OOD generalization by generating flat\nenvironments. However, such flat environments come with inherent limitations to\ncapture more complex data distributions. Considering the DrugOOD dataset, which\ncontains diverse training environments (e.g., scaffold, size, etc.), flat\ncontexts cannot sufficiently address its high heterogeneity. Thus, a new\nchallenge is posed to generate more semantically enriched environments to\nenhance graph invariant learning for handling distribution shifts. In this\npaper, we propose a novel approach to generate hierarchical semantic\nenvironments for each graph. Firstly, given an input graph, we explicitly\nextract variant subgraphs from the input graph to generate proxy predictions on\nlocal environments. Then, stochastic attention mechanisms are employed to\nre-extract the subgraphs for regenerating global environments in a hierarchical\nmanner. In addition, we introduce a new learning objective that guides our\nmodel to learn the diversity of environments within the same hierarchy while\nmaintaining consistency across different hierarchies. This approach enables our\nmodel to consider the relationships between environments and facilitates robust\ngraph invariant learning. Extensive experiments on real-world graph data have\ndemonstrated the effectiveness of our framework. Particularly, in the\nchallenging dataset DrugOOD, our method achieves up to 1.29% and 2.83%\nimprovement over the best baselines on IC50 and EC50 prediction tasks,\nrespectively.",
      "tldr_zh": "该论文针对图领域中 out-of-distribution (OOD) 泛化的挑战，提出一种通过分层语义环境（hierarchical semantic environments）提升模型鲁棒性的方法，以应对复杂分布偏移和环境上下文不足的问题。方法首先从输入图中提取变异子图（variant subgraphs）生成局部环境，然后使用随机注意力机制（stochastic attention mechanisms）分层重新提取子图以创建全局环境，并引入新学习目标来确保同一层次环境的多样性以及不同层次的一致性。实验结果显示，在 DrugOOD 数据集上，该框架在 IC50 和 EC50 预测任务上分别比最佳基线提高了 1.29% 和 2.83%，证明了其在真实世界图数据上的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.01773v2",
      "published_date": "2024-03-04 07:03:10 UTC",
      "updated_date": "2024-06-03 05:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:27:09.502989"
    },
    {
      "arxiv_id": "2403.01769v1",
      "title": "A Safe Screening Rule with Bi-level Optimization of $ν$ Support Vector Machine",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiji Yang",
        "Wanyi Chen",
        "Huan Zhang",
        "Yitian Xu",
        "Lei Shi",
        "Jianhua Zhao"
      ],
      "abstract": "Support vector machine (SVM) has achieved many successes in machine learning,\nespecially for a small sample problem. As a famous extension of the traditional\nSVM, the $\\nu$ support vector machine ($\\nu$-SVM) has shown outstanding\nperformance due to its great model interpretability. However, it still faces\nchallenges in training overhead for large-scale problems. To address this\nissue, we propose a safe screening rule with bi-level optimization for\n$\\nu$-SVM (SRBO-$\\nu$-SVM) which can screen out inactive samples before\ntraining and reduce the computational cost without sacrificing the prediction\naccuracy. Our SRBO-$\\nu$-SVM is strictly deduced by integrating the\nKarush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex\nproblems and the $\\nu$-property. Furthermore, we develop an efficient dual\ncoordinate descent method (DCDM) to further improve computational speed.\nFinally, a unified framework for SRBO is proposed to accelerate many SVM-type\nmodels, and it is successfully applied to one-class SVM. Experimental results\non 6 artificial data sets and 30 benchmark data sets have verified the\neffectiveness and safety of our proposed methods in supervised and unsupervised\ntasks.",
      "tldr_zh": "该论文针对 $ν$-SVM 在大规模问题上的训练开销问题，提出了一种安全筛选规则 SRBO-$ν$-SVM 方法，通过双层优化在训练前筛选出非活跃样本，从而减少计算成本而不影响预测准确性。方法基于 Karush-Kuhn-Tucker (KKT) 条件、凸问题的变分不等式和 $ν$-property 进行严格推导，并结合高效的双坐标下降方法 (DCDM) 进一步提升计算速度。论文还构建了一个统一的 SRBO 框架，以加速多种 SVM 类型模型，包括应用于 one-class SVM。实验在 6 个人工数据集和 30 个基准数据集上验证了该方法的有效性和安全性，在监督和无监督任务中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01769v1",
      "published_date": "2024-03-04 06:55:57 UTC",
      "updated_date": "2024-03-04 06:55:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:27:21.110381"
    },
    {
      "arxiv_id": "2403.01768v1",
      "title": "Canonical Form of Datatic Description in Control Systems",
      "title_zh": "控制系统中的数据驱动描述标准形式",
      "authors": [
        "Guojian Zhan",
        "Ziang Zheng",
        "Shengbo Eben Li"
      ],
      "abstract": "The design of feedback controllers is undergoing a paradigm shift from\nmodelic (i.e., model-driven) control to datatic (i.e., data-driven) control.\nCanonical form of state space model is an important concept in modelic control\nsystems, exemplified by Jordan form, controllable form and observable form,\nwhose purpose is to facilitate system analysis and controller synthesis. In the\nrealm of datatic control, there is a notable absence in the standardization of\ndata-based system representation. This paper for the first time introduces the\nconcept of canonical data form for the purpose of achieving more effective\ndesign of datatic controllers. In a control system, the data sample in\ncanonical form consists of a transition component and an attribute component.\nThe former encapsulates the plant dynamics at the sampling time independently,\nwhich is a tuple containing three elements: a state, an action and their\ncorresponding next state. The latter describes one or some artificial\ncharacteristics of the current sample, whose calculation must be performed in\nan online manner. The attribute of each sample must adhere to two requirements:\n(1) causality, ensuring independence from any future samples; and (2) locality,\nallowing dependence on historical samples but constrained to a finite\nneighboring set. The purpose of adding attribute is to offer some kinds of\nbenefits for controller design in terms of effectiveness and efficiency. To\nprovide a more close-up illustration, we present two canonical data forms:\ntemporal form and spatial form, and demonstrate their advantages in reducing\ninstability and enhancing training efficiency in two datatic control systems.",
      "tldr_zh": "本论文首次引入了“canonical data form”的概念，用于数据驱动（datatic）控制系统，以标准化数据表示并提升控制器设计效率。不同于模型驱动（modelic）控制中的规范形式（如 Jordan form 和 controllable form），canonical data form 由 transition component（包含状态、动作和下一个状态的元组）和 attribute component（描述样本人工特征）组成。attribute component 必须满足 causality 和 locality 要求，即在线计算且仅依赖有限的历史样本，以提高控制系统的有效性和效率。论文通过 temporal form 和 spatial form 的示例，展示了这种形式在减少系统不稳定性并提升训练效率方面的优势。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01768v1",
      "published_date": "2024-03-04 06:53:38 UTC",
      "updated_date": "2024-03-04 06:53:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:27:32.755977"
    },
    {
      "arxiv_id": "2403.01757v1",
      "title": "How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiao Huang",
        "Wenjie Zhang",
        "Liang Feng",
        "Xingyu Wu",
        "Kay Chen Tan"
      ],
      "abstract": "Recently, large language models (LLMs) have notably positioned them as\ncapable tools for addressing complex optimization challenges. Despite this\nrecognition, a predominant limitation of existing LLM-based optimization\nmethods is their struggle to capture the relationships among decision variables\nwhen relying exclusively on numerical text prompts, especially in\nhigh-dimensional problems. Keeping this in mind, we first propose to enhance\nthe optimization performance using multimodal LLM capable of processing both\ntextual and visual prompts for deeper insights of the processed optimization\nproblem. This integration allows for a more comprehensive understanding of\noptimization problems, akin to human cognitive processes. We have developed a\nmultimodal LLM-based optimization framework that simulates human\nproblem-solving workflows, thereby offering a more nuanced and effective\nanalysis. The efficacy of this method is evaluated through extensive empirical\nstudies focused on a well-known combinatorial optimization problem, i.e.,\ncapacitated vehicle routing problem. The results are compared against those\nobtained from the LLM-based optimization algorithms that rely solely on textual\nprompts, demonstrating the significant advantages of our multimodal approach.",
      "tldr_zh": "该研究指出，现有的LLM优化方法在处理高维问题时，仅依赖数字文本提示难以捕捉决策变量之间的关系。为提升性能，提出一种多模态LLM框架，能够同时处理文本和视觉提示，以模拟人类认知流程，提供更全面的优化问题分析。研究通过实证实验在Capacitated Vehicle Routing Problems上进行评估，结果显示，该多模态方法相较于仅使用文本提示的LLM算法，显著提高了优化效果。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.NE",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "8pages,3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.01757v1",
      "published_date": "2024-03-04 06:24:21 UTC",
      "updated_date": "2024-03-04 06:24:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:27:43.637057"
    },
    {
      "arxiv_id": "2403.01748v3",
      "title": "NeuSpeech: Decode Neural signal as Speech",
      "title_zh": "NeuSpeech: 将神经信号解",
      "authors": [
        "Yiqian Yang",
        "Yiqun Duan",
        "Qiang Zhang",
        "Hyejeong Jo",
        "Jinni Zhou",
        "Won Hee Lee",
        "Renjing Xu",
        "Hui Xiong"
      ],
      "abstract": "Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used $``teacher-forcing\"$ during generative decoding, which is\nimpractical; 3) prior works are mostly $``BART-based\"$ not fully\nauto-regressive, which performs better in other sequence tasks. In this paper,\nwe explore the brain-to-text translation of MEG signals in a speech-decoding\nformation. Here we are the first to investigate a cross-attention-based\n``whisper\" model for generating text directly from MEG signals without teacher\nforcing. Our model achieves impressive BLEU-1 scores of 60.30 and 52.89 without\npretraining $\\&$ teacher-forcing on two major datasets ($\\textit{GWilliams}$\nand $\\textit{Schoffelen}$). This paper conducts a comprehensive review to\nunderstand how speech decoding formation performs on the neural decoding tasks,\nincluding pretraining initialization, training $\\&$ evaluation set splitting,\naugmentation, and scaling law. Code is available at\nhttps://github.com/NeuSpeech/NeuSpeech1$.",
      "tldr_zh": "该研究提出NeuSpeech框架，旨在从脑部动态解码语言以支持脑机接口(BCI)，特别关注非侵入性神经信号如EEG和MEG的安全优势。论文首次在MEG信号上探索基于cross-attention的“whisper”模型，直接生成文本而非使用teacher-forcing或BART-based方法，从而实现完全自回归的脑到文本翻译。实验结果显示，该模型在GWilliams和Schoffelen数据集上分别取得BLEU-1分数60.30和52.89，同时论文还对预训练初始化、数据集分割、增强和缩放定律进行了全面审查，为非侵入性BCI应用奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01748v3",
      "published_date": "2024-03-04 05:55:01 UTC",
      "updated_date": "2024-06-03 16:58:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:27:57.183168"
    },
    {
      "arxiv_id": "2403.01742v3",
      "title": "Diffusion-TS: Interpretable Diffusion for General Time Series Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Yuan",
        "Yan Qiao"
      ],
      "abstract": "Denoising diffusion probabilistic models (DDPMs) are becoming the leading\nparadigm for generative models. It has recently shown breakthroughs in audio\nsynthesis, time series imputation and forecasting. In this paper, we propose\nDiffusion-TS, a novel diffusion-based framework that generates multivariate\ntime series samples of high quality by using an encoder-decoder transformer\nwith disentangled temporal representations, in which the decomposition\ntechnique guides Diffusion-TS to capture the semantic meaning of time series\nwhile transformers mine detailed sequential information from the noisy model\ninput. Different from existing diffusion-based approaches, we train the model\nto directly reconstruct the sample instead of the noise in each diffusion step,\ncombining a Fourier-based loss term. Diffusion-TS is expected to generate time\nseries satisfying both interpretablity and realness. In addition, it is shown\nthat the proposed Diffusion-TS can be easily extended to conditional generation\ntasks, such as forecasting and imputation, without any model changes. This also\nmotivates us to further explore the performance of Diffusion-TS under irregular\nsettings. Finally, through qualitative and quantitative experiments, results\nshow that Diffusion-TS achieves the state-of-the-art results on various\nrealistic analyses of time series.",
      "tldr_zh": "本研究提出Diffusion-TS，一种基于去噪扩散概率模型(DDPMs)的框架，用于生成高质量的多变量时间序列，通过编码器-解码器Transformer和分离的时间表示来捕捉语义含义和顺序信息。不同于现有方法，该框架训练模型直接重建样本而非噪声，并结合Fourier-based损失，以提升生成结果的可解释性和真实性。Diffusion-TS可轻松扩展到条件生成任务，如预测和插值，并在不规则设置下表现出色；实验结果显示，它在各种时间序列分析中达到了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01742v3",
      "published_date": "2024-03-04 05:39:23 UTC",
      "updated_date": "2024-10-21 04:38:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:28:06.612144"
    },
    {
      "arxiv_id": "2403.01734v1",
      "title": "Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyang Cao",
        "Zichen Yan",
        "Renhao Lu",
        "Junbo Tan",
        "Xueqian Wang"
      ],
      "abstract": "Offline goal-conditioned reinforcement learning (GCRL) aims at solving\ngoal-reaching tasks with sparse rewards from an offline dataset. While prior\nwork has demonstrated various approaches for agents to learn near-optimal\npolicies, these methods encounter limitations when dealing with diverse\nconstraints in complex environments, such as safety constraints. Some of these\napproaches prioritize goal attainment without considering safety, while others\nexcessively focus on safety at the expense of training efficiency. In this\npaper, we study the problem of constrained offline GCRL and propose a new\nmethod called Recovery-based Supervised Learning (RbSL) to accomplish\nsafety-critical tasks with various goals. To evaluate the method performance,\nwe build a benchmark based on the robot-fetching environment with a randomly\npositioned obstacle and use expert or random policies to generate an offline\ndataset. We compare RbSL with three offline GCRL algorithms and one offline\nsafe RL algorithm. As a result, our method outperforms the existing\nstate-of-the-art methods to a large extent. Furthermore, we validate the\npracticality and effectiveness of RbSL by deploying it on a real Panda\nmanipulator. Code is available at https://github.com/Sunlighted/RbSL.git.",
      "tldr_zh": "本文研究了离线目标条件强化学习（Offline Goal-Conditioned Reinforcement Learning, GCRL）在安全关键任务中的挑战，提出了一种新方法Recovery-based Supervised Learning (RbSL)，通过恢复策略平衡目标达成与安全约束，提高训练效率。RbSL 在基于机器人抓取环境的基准测试中，使用专家或随机策略生成离线数据集，并与现有三种GCRL算法和一种安全强化学习算法相比，表现出显著优越性。实验结果显示，RbSL 在真实Panda机械臂部署中验证了其实际有效性，代码已在GitHub上公开。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "68T40"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by ICRA24",
      "pdf_url": "http://arxiv.org/pdf/2403.01734v1",
      "published_date": "2024-03-04 05:20:57 UTC",
      "updated_date": "2024-03-04 05:20:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:28:20.806390"
    },
    {
      "arxiv_id": "2403.02360v1",
      "title": "Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling",
      "title_zh": "翻译失败",
      "authors": [
        "Xingyan Chen",
        "Tian Du",
        "Mu Wang",
        "Tiancheng Gu",
        "Yu Zhao",
        "Gang Kou",
        "Changqiao Xu",
        "Dapeng Oliver Wu"
      ],
      "abstract": "Federated learning, as a promising distributed learning paradigm, enables\ncollaborative training of a global model across multiple network edge clients\nwithout the need for central data collecting. However, the heterogeneity of\nedge data distribution drags the model towards the local minima, which can be\ndistant from the global optimum. Such heterogeneity often leads to slow\nconvergence and substantial communication overhead. To address these issues, we\npropose a novel federated learning framework called FedCMD, a model decoupling\ntailored to the Cloud-edge supported federated learning that separates deep\nneural networks into a body for capturing shared representations in Cloud and a\npersonalized head for migrating data heterogeneity. Our motivation is that, by\nthe deep investigation of the performance of selecting different neural network\nlayers as the personalized head, we found rigidly assigning the last layer as\nthe personalized head in current studies is not always optimal. Instead, it is\nnecessary to dynamically select the personalized layer that maximizes the\ntraining performance by taking the representation difference between neighbor\nlayers into account. To find the optimal personalized layer, we utilize the\nlow-dimensional representation of each layer to contrast feature distribution\ntransfer and introduce a Wasserstein-based layer selection method, aimed at\nidentifying the best-match layer for personalization. Additionally, a weighted\nglobal aggregation algorithm is proposed based on the selected personalized\nlayer for the practical application of FedCMD. Extensive experiments on ten\nbenchmarks demonstrate the efficiency and superior performance of our solution\ncompared with nine state-of-the-art solutions. All code and results are\navailable at https://github.com/elegy112138/FedCMD.",
      "tldr_zh": "本研究针对异质联邦学习（Federated Learning）中数据分布差异导致的模型收敛缓慢和通信开销大的问题，提出了一种新型框架FedCMD。该框架通过Contrastive Cloud-Edge Model Decoupling将深度神经网络解耦为Cloud中的共享主体和边缘的个性化头部，并动态选择最优个性化层，利用Wasserstein-based layer selection方法对比邻层特征分布以最大化训练性能。此外，引入基于选定层的加权全局聚合算法进行实际应用，实验在十个基准上显示FedCMD比九个最先进方案更高效和表现更优越。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.02360v1",
      "published_date": "2024-03-04 05:10:28 UTC",
      "updated_date": "2024-03-04 05:10:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:28:31.783135"
    },
    {
      "arxiv_id": "2403.01709v1",
      "title": "Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study",
      "title_zh": "翻译失败",
      "authors": [
        "Rudra Dhar",
        "Karthik Vaidhyanathan",
        "Vasudeva Varma"
      ],
      "abstract": "Architectural Knowledge Management (AKM) involves the organized handling of\ninformation related to architectural decisions and design within a project or\norganization. An essential artifact of AKM is the Architecture Decision Records\n(ADR), which documents key design decisions. ADRs are documents that capture\ndecision context, decision made and various aspects related to a design\ndecision, thereby promoting transparency, collaboration, and understanding.\nDespite their benefits, ADR adoption in software development has been slow due\nto challenges like time constraints and inconsistent uptake. Recent\nadvancements in Large Language Models (LLMs) may help bridge this adoption gap\nby facilitating ADR generation. However, the effectiveness of LLM for ADR\ngeneration or understanding is something that has not been explored. To this\nend, in this work, we perform an exploratory study that aims to investigate the\nfeasibility of using LLM for the generation of ADRs given the decision context.\nIn our exploratory study, we utilize GPT and T5-based models with 0-shot,\nfew-shot, and fine-tuning approaches to generate the Decision of an ADR given\nits Context. Our results indicate that in a 0-shot setting, state-of-the-art\nmodels such as GPT-4 generate relevant and accurate Design Decisions, although\nthey fall short of human-level performance. Additionally, we observe that more\ncost-effective models like GPT-3.5 can achieve similar outcomes in a few-shot\nsetting, and smaller models such as Flan-T5 can yield comparable results after\nfine-tuning. To conclude, this exploratory study suggests that LLM can generate\nDesign Decisions, but further research is required to attain human-level\ngeneration and establish standardized widespread adoption.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在生成架构设计决策（Architectural Design Decisions）方面的可行性，针对 Architectural Knowledge Management (AKM) 中的 Architecture Decision Records (ADRs) 文档生成问题。研究采用 GPT 和 T5-based models，通过 0-shot、few-shot 和 fine-tuning 方法，基于决策上下文生成 ADRs 的决策内容。结果显示，GPT-4 在 0-shot 设置下能产生相关准确的决策，但仍低于人类水平，而更经济的模型如 GPT-3.5 在 few-shot 设置下表现类似，Flan-T5 通过 fine-tuning 也可达到可比效果。该研究表明 LLMs 有潜力辅助 ADRs 生成，但需进一步优化以实现人类水平和广泛采用。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "This paper has been accepted to IEEE ICSA 2024 (Main Track - Research\n  Track)",
      "pdf_url": "http://arxiv.org/pdf/2403.01709v1",
      "published_date": "2024-03-04 03:56:14 UTC",
      "updated_date": "2024-03-04 03:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:28:44.444985"
    },
    {
      "arxiv_id": "2403.07932v1",
      "title": "Feint in Multi-Player Games",
      "title_zh": "翻译失败",
      "authors": [
        "Junyu Liu",
        "Wangkai Jin",
        "Xiangjun Peng"
      ],
      "abstract": "This paper introduces the first formalization, implementation and\nquantitative evaluation of Feint in Multi-Player Games. Our work first\nformalizes Feint from the perspective of Multi-Player Games, in terms of the\ntemporal, spatial, and their collective impacts. The formalization is built\nupon Non-transitive Active Markov Game Model, where Feint can have a\nconsiderable amount of impacts. Then, our work considers practical\nimplementation details of Feint in Multi-Player Games, under the\nstate-of-the-art progress of multi-agent modeling to date (namely Multi-Agent\nReinforcement Learning). Finally, our work quantitatively examines the\neffectiveness of our design, and the results show that our design of Feint can\n(1) greatly improve the reward gains from the game; (2) significantly improve\nthe diversity of Multi-Player Games; and (3) only incur negligible overheads in\nterms of time consumption. We conclude that our design of Feint is effective\nand practical, to make Multi-Player Games more interesting.",
      "tldr_zh": "这篇论文首次正式化、实现并定量评估了 Feint 在 Multi-Player Games 中的应用，从时间、空间及其整体影响的角度出发，并基于 Non-transitive Active Markov Game Model 构建框架。作者考虑了在 Multi-Agent Reinforcement Learning 的最新进展下，Feint 的实际实现细节，以提升游戏策略的有效性。实验结果表明，该设计显著提高了游戏奖励收益和多样性，同时仅带来了微不足道的计算开销。总之，Feint 的引入使 Multi-Player Games 更具趣味性和实用价值。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.07932v1",
      "published_date": "2024-03-04 03:43:45 UTC",
      "updated_date": "2024-03-04 03:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:28:55.944614"
    },
    {
      "arxiv_id": "2403.01699v3",
      "title": "Brilla AI: AI Contestant for the National Science and Maths Quiz",
      "title_zh": "翻译失败",
      "authors": [
        "George Boateng",
        "Jonathan Abrefah Mensah",
        "Kevin Takyi Yeboah",
        "William Edor",
        "Andrew Kojo Mensah-Onumah",
        "Naafi Dasana Ibrahim",
        "Nana Sam Yeboah"
      ],
      "abstract": "The African continent lacks enough qualified teachers which hampers the\nprovision of adequate learning support. An AI could potentially augment the\nefforts of the limited number of teachers, leading to better learning outcomes.\nTowards that end, this work describes and evaluates the first key output for\nthe NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for\nsuch an AI: \"Build an AI to compete live in Ghana's National Science and Maths\nQuiz (NSMQ) competition and win - performing better than the best contestants\nin all rounds and stages of the competition\". The NSMQ is an annual live\nscience and mathematics competition for senior secondary school students in\nGhana in which 3 teams of 2 students compete by answering questions across\nbiology, chemistry, physics, and math in 5 rounds over 5 progressive stages\nuntil a winning team is crowned for that year. In this work, we built Brilla\nAI, an AI contestant that we deployed to unofficially compete remotely and live\nin the Riddles round of the 2023 NSMQ Grand Finale, the first of its kind in\nthe 30-year history of the competition. Brilla AI is currently available as a\nweb app that livestreams the Riddles round of the contest, and runs 4 machine\nlearning systems: (1) speech to text (2) question extraction (3) question\nanswering and (4) text to speech that work together in real-time to quickly and\naccurately provide an answer, and then say it with a Ghanaian accent. In its\ndebut, our AI answered one of the 4 riddles ahead of the 3 human contesting\nteams, unofficially placing second (tied). Improvements and extensions of this\nAI could potentially be deployed to offer science tutoring to students and\neventually enable millions across Africa to have one-on-one learning\ninteractions, democratizing science education.",
      "tldr_zh": "这篇论文介绍了 Brilla AI，一种 AI 系统，旨在通过参与加纳的 National Science and Maths Quiz (NSMQ) 比赛来解决非洲教师短缺问题，并提升科学教育成果。Brilla AI 整合了四个机器学习系统，包括 speech to text、问题提取、question answering 和 text to speech，实现实时回答问题，并在 2023 NSMQ Grand Finale 的 Riddles 回合中 unofficial 排名第二。实验结果显示，该 AI 在首秀中比人类团队更快地回答了一个谜语，证明了其在真实竞赛环境中的可行性。未来，Brilla AI 的改进可用于提供科学辅导，实现一对一学习并民主化非洲的教育资源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages. Accepted for the WideAIED track at the 25th International\n  Conference on AI in Education (AIED 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.01699v3",
      "published_date": "2024-03-04 03:24:18 UTC",
      "updated_date": "2024-04-30 19:22:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:29:09.480832"
    },
    {
      "arxiv_id": "2403.01698v1",
      "title": "Hypertext Entity Extraction in Webpage",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Yang",
        "Tianqiao Liu",
        "Bo Shao",
        "Hai Zhao",
        "Linjun Shou",
        "Ming Gong",
        "Daxin Jiang"
      ],
      "abstract": "Webpage entity extraction is a fundamental natural language processing task\nin both research and applications. Nowadays, the majority of webpage entity\nextraction models are trained on structured datasets which strive to retain\ntextual content and its structure information. However, existing datasets all\noverlook the rich hypertext features (e.g., font color, font size) which show\ntheir effectiveness in previous works. To this end, we first collect a\n\\textbf{H}ypertext \\textbf{E}ntity \\textbf{E}xtraction \\textbf{D}ataset\n(\\textit{HEED}) from the e-commerce domains, scraping both the text and the\ncorresponding explicit hypertext features with high-quality manual entity\nannotations. Furthermore, we present the \\textbf{Mo}E-based \\textbf{E}ntity\n\\textbf{E}xtraction \\textbf{F}ramework (\\textit{MoEEF}), which efficiently\nintegrates multiple features to enhance model performance by Mixture of Experts\nand outperforms strong baselines, including the state-of-the-art small-scale\nmodels and GPT-3.5-turbo. Moreover, the effectiveness of hypertext features in\n\\textit{HEED} and several model components in \\textit{MoEEF} are analyzed.",
      "tldr_zh": "这篇论文针对网页实体提取任务，指出现有模型忽略了超文本特征（如字体颜色和大小），从而提出收集\\textbf{HEED}数据集，该数据集从电商领域获取文本和相关超文本特征，并提供高质量的手动实体标注。作者开发了基于Mixture of Experts的\\textbf{MoEEF}框架，通过高效整合多种特征提升模型性能，并超过了强基线模型，包括小规模SOTA模型和GPT-3.5-turbo。实验分析证实了超文本特征的有效性及其在框架中的关键作用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01698v1",
      "published_date": "2024-03-04 03:21:40 UTC",
      "updated_date": "2024-03-04 03:21:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:29:20.680622"
    },
    {
      "arxiv_id": "2403.01695v3",
      "title": "DyCE: Dynamically Configurable Exiting for Deep Learning Compression and Real-time Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Qingyuan Wang",
        "Barry Cardiff",
        "Antoine Frappé",
        "Benoit Larras",
        "Deepu John"
      ],
      "abstract": "Conventional deep learning (DL) model compression and scaling methods focus\non altering the model's components, impacting the results across all samples\nuniformly. However, since samples vary in difficulty, a dynamic model that\nadapts computation based on sample complexity offers a novel perspective for\ncompression and scaling. Despite this potential, existing dynamic models are\ntypically monolithic and model-specific, limiting their generalizability as\nbroad compression and scaling methods. Additionally, most deployed DL systems\nare fixed, unable to adjust their scale once deployed and, therefore, cannot\nadapt to the varying real-time demands. This paper introduces DyCE, a\ndynamically configurable system that can adjust the performance-complexity\ntrade-off of a DL model at runtime without requiring re-initialization or\nredeployment on inference hardware. DyCE achieves this by adding small exit\nnetworks to intermediate layers of the original model, allowing computation to\nterminate early if acceptable results are obtained. DyCE also decouples the\ndesign of an efficient dynamic model, facilitating easy adaptation to new base\nmodels and potential general use in compression and scaling. We also propose\nmethods for generating optimized configurations and determining the types and\npositions of exit networks to achieve desired performance and complexity\ntrade-offs. By enabling simple configuration switching, DyCE provides\nfine-grained performance tuning in real-time. We demonstrate the effectiveness\nof DyCE through image classification tasks using deep convolutional neural\nnetworks (CNNs). DyCE significantly reduces computational complexity by 23.5%\nfor ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with accuracy\nreductions of less than 0.5%.",
      "tldr_zh": "本研究提出了一种动态可配置退出系统(DyCE)，旨在解决传统deep learning模型压缩和缩放方法的局限性，这些方法无法根据样本难度动态调整计算量。DyCE通过在原模型的中间层添加小型退出网络(exit networks)，允许在达到可接受结果时提前终止计算，从而实现运行时性能-复杂度权衡，而无需重新初始化或重新部署。研究还引入了优化配置生成方法和退出网络的类型及位置确定策略，便于DyCE适应新基模型并实现实时细粒度调整。在ImageNet图像分类任务上，DyCE使ResNet152的计算复杂度减少23.5%、ConvNextv2-tiny减少25.9%，同时准确率损失小于0.5%。这为通用deep learning压缩和缩放提供了新框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01695v3",
      "published_date": "2024-03-04 03:09:28 UTC",
      "updated_date": "2025-05-07 23:56:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:29:32.715804"
    },
    {
      "arxiv_id": "2403.01693v3",
      "title": "HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances",
      "title_zh": "翻译失败",
      "authors": [
        "Supreeth Narasimhaswamy",
        "Uttaran Bhattacharya",
        "Xiang Chen",
        "Ishita Dasgupta",
        "Saayan Mitra",
        "Minh Hoai"
      ],
      "abstract": "Text-to-image generative models can generate high-quality humans, but realism\nis lost when generating hands. Common artifacts include irregular hand poses,\nshapes, incorrect numbers of fingers, and physically implausible finger\norientations. To generate images with realistic hands, we propose a novel\ndiffusion-based architecture called HanDiffuser that achieves realism by\ninjecting hand embeddings in the generative process. HanDiffuser consists of\ntwo components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and\nMANO-Hand parameters from input text prompts, and a Text-Guided\nHand-Params-to-Image diffusion model to synthesize images by conditioning on\nthe prompts and hand parameters generated by the previous component. We\nincorporate multiple aspects of hand representation, including 3D shapes and\njoint-level finger positions, orientations and articulations, for robust\nlearning and reliable performance during inference. We conduct extensive\nquantitative and qualitative experiments and perform user studies to\ndemonstrate the efficacy of our method in generating images with high-quality\nhands.",
      "tldr_zh": "该论文针对文本到图像生成模型在手部生成中存在的缺陷（如不规则姿势、形状、指数量错误和不合理的指尖方向），提出了一种新型基于扩散模型的架构HanDiffuser，通过注入手部嵌入来提升手部真实性。HanDiffuser由两个组件组成：Text-to-Hand-Params扩散模型，用于从文本提示生成SMPL-Body和MANO-Hand参数；以及Text-Guided Hand-Params-to-Image扩散模型，用于基于这些参数和提示合成图像。该方法整合了手部的多种表示，包括3D形状、关节级别的指尖位置、方向和关节运动，通过定量、定性和用户研究证明了其在生成高质量手部图像方面的显著有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Revisions: 1. Added a link to project page in the abstract, 2.\n  Updated references and related work, 3. Fixed some grammatical errors",
      "pdf_url": "http://arxiv.org/pdf/2403.01693v3",
      "published_date": "2024-03-04 03:00:22 UTC",
      "updated_date": "2024-11-22 22:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:29:45.794833"
    },
    {
      "arxiv_id": "2403.01673v1",
      "title": "CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables",
      "title_zh": "翻译失败",
      "authors": [
        "Jiecheng Lu",
        "Xu Han",
        "Yan Sun",
        "Shihao Yang"
      ],
      "abstract": "For Multivariate Time Series Forecasting (MTSF), recent deep learning\napplications show that univariate models frequently outperform multivariate\nones. To address the difficiency in multivariate models, we introduce a method\nto Construct Auxiliary Time Series (CATS) that functions like a 2D\ntemporal-contextual attention mechanism, which generates Auxiliary Time Series\n(ATS) from Original Time Series (OTS) to effectively represent and incorporate\ninter-series relationships for forecasting. Key principles of ATS - continuity,\nsparsity, and variability - are identified and implemented through different\nmodules. Even with a basic 2-layer MLP as core predictor, CATS achieves\nstate-of-the-art, significantly reducing complexity and parameters compared to\nprevious multivariate models, marking it an efficient and transferable MTSF\nsolution.",
      "tldr_zh": "本文提出 CATS 方法，通过构建辅助时间序列 (ATS) 作为外生变量，来增强多变量时间序列预测 (MTSF) 的性能，旨在解决多变量模型不如单变量模型有效的不足。CATS 类似于一个 2D temporal-contextual attention 机制，从原始时间序列 (OTS) 生成 ATS，并通过 continuity、sparsity 和 variability 等关键原则来表示和整合序列间关系。实验表明，即使使用简单的 2 层 MLP 作为核心预测器，CATS 也达到了 state-of-the-art 水平，显著降低了模型复杂性和参数量，使其成为高效且可转移的 MTSF 解决方案。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01673v1",
      "published_date": "2024-03-04 01:52:40 UTC",
      "updated_date": "2024-03-04 01:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:29:57.254746"
    },
    {
      "arxiv_id": "2403.01649v1",
      "title": "Recommendations for Government Development and Use of Advanced Automated Systems to Make Decisions about Individuals",
      "title_zh": "翻译失败",
      "authors": [
        "Susan Landau",
        "James X. Dempsey",
        "Ece Kamar",
        "Steven M. Bellovin"
      ],
      "abstract": "Contestability -- the ability to effectively challenge a decision -- is\ncritical to the implementation of fairness. In the context of governmental\ndecision making about individuals, contestability is often constitutionally\nrequired as an element of due process; specific procedures may be required by\nstate or federal law relevant to a particular program. In addition,\ncontestability can be a valuable way to discover systemic errors, contributing\nto ongoing assessments and system improvement.\n  On January 24-25, 2024, with support from the National Science Foundation and\nthe William and Flora Hewlett Foundation, we convened a diverse group of\ngovernment officials, representatives of leading technology companies,\ntechnology and policy experts from academia and the non-profit sector,\nadvocates, and stakeholders for a workshop on advanced automated decision\nmaking, contestability, and the law. Informed by the workshop's rich and\nwide-ranging discussion, we offer these recommendations. A full report\nsummarizing the discussion is in preparation.",
      "tldr_zh": "这篇论文针对政府开发和使用高级自动化系统进行个人决策，提供了一系列推荐。论文强调了 contestability（可争辩性）的关键作用，这有助于确保决策公平、符合 due process（正当程序），并通过发现系统性错误来促进系统改进。基于2024年1月24-25日召开的研讨会，该研讨会汇集了政府官员、技术公司代表、学术专家和利益相关者进行广泛讨论，最终形成了这些推荐，一个完整的报告正在准备中。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.4; K.5; I.2"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01649v1",
      "published_date": "2024-03-04 00:03:00 UTC",
      "updated_date": "2024-03-04 00:03:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:30:09.018072"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 108,
  "processed_papers_count": 108,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T12:30:29.608235"
}
[
  {
    "arxiv_id": "2409.06916v1",
    "title": "Interactive Counterfactual Exploration of Algorithmic Harms in Recommender Systems",
    "authors": [
      "Yongsu Ahn",
      "Quinn K Wolter",
      "Jonilyn Dick",
      "Janet Dick",
      "Yu-Ru Lin"
    ],
    "abstract": "Recommender systems have become integral to digital experiences, shaping user\ninteractions and preferences across various platforms. Despite their widespread\nuse, these systems often suffer from algorithmic biases that can lead to unfair\nand unsatisfactory user experiences. This study introduces an interactive tool\ndesigned to help users comprehend and explore the impacts of algorithmic harms\nin recommender systems. By leveraging visualizations, counterfactual\nexplanations, and interactive modules, the tool allows users to investigate how\nbiases such as miscalibration, stereotypes, and filter bubbles affect their\nrecommendations. Informed by in-depth user interviews, this tool benefits both\ngeneral users and researchers by increasing transparency and offering\npersonalized impact assessments, ultimately fostering a better understanding of\nalgorithmic biases and contributing to more equitable recommendation outcomes.\nThis work provides valuable insights for future research and practical\napplications in mitigating bias and enhancing fairness in machine learning\nalgorithms.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06916v1",
    "published_date": "2024-09-10 23:58:27 UTC",
    "updated_date": "2024-09-10 23:58:27 UTC"
  },
  {
    "arxiv_id": "2409.06912v3",
    "title": "A Bayesian Framework for Active Tactile Object Recognition, Pose Estimation and Shape Transfer Learning",
    "authors": [
      "Haodong Zheng",
      "Andrei Jalba",
      "Raymond H. Cuijpers",
      "Wijnand IJsselsteijn",
      "Sanne Schoenmakers"
    ],
    "abstract": "As humans can explore and understand the world through active touch, similar\ncapability is desired for robots. In this paper, we address the problem of\nactive tactile object recognition, pose estimation and shape transfer learning,\nwhere a customized particle filter (PF) and Gaussian process implicit surface\n(GPIS) is combined in a unified Bayesian framework. Upon new tactile input, the\ncustomized PF updates the joint distribution of the object class and object\npose while tracking the novelty of the object. Once a novel object is\nidentified, its shape will be reconstructed using GPIS. By grounding the prior\nof the GPIS with the maximum-a-posteriori (MAP) estimation from the PF, the\nknowledge about known shapes can be transferred to learn novel shapes. An\nexploration procedure based on global shape estimation is proposed to guide\nactive data acquisition and terminate the exploration upon sufficient\ninformation. Through experiments in simulation, the proposed framework\ndemonstrated its effectiveness and efficiency in estimating object class and\npose for known objects and learning novel shapes. Furthermore, it can recognize\npreviously learned shapes reliably.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06912v3",
    "published_date": "2024-09-10 23:35:30 UTC",
    "updated_date": "2024-10-11 09:49:15 UTC"
  },
  {
    "arxiv_id": "2409.16307v1",
    "title": "DeepScore: A Comprehensive Approach to Measuring Quality in AI-Generated Clinical Documentation",
    "authors": [
      "Jon Oleson"
    ],
    "abstract": "Medical practitioners are rapidly adopting generative AI solutions for\nclinical documentation, leading to significant time savings and reduced stress.\nHowever, evaluating the quality of AI-generated documentation is a complex and\nongoing challenge. This paper presents an overview of DeepScribe's\nmethodologies for assessing and managing note quality, focusing on various\nmetrics and the composite \"DeepScore\", an overall index of quality and\naccuracy. These methodologies aim to enhance the quality of patient care\ndocumentation through accountability and continuous improvement.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 5 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.16307v1",
    "published_date": "2024-09-10 23:06:48 UTC",
    "updated_date": "2024-09-10 23:06:48 UTC"
  },
  {
    "arxiv_id": "2409.06904v1",
    "title": "Applied Federated Model Personalisation in the Industrial Domain: A Comparative Study",
    "authors": [
      "Ilias Siniosoglou",
      "Vasileios Argyriou",
      "George Fragulis",
      "Panagiotis Fouliras",
      "Georgios Th. Papadopoulos",
      "Anastasios Lytos",
      "Panagiotis Sarigiannidis"
    ],
    "abstract": "The time-consuming nature of training and deploying complicated Machine and\nDeep Learning (DL) models for a variety of applications continues to pose\nsignificant challenges in the field of Machine Learning (ML). These challenges\nare particularly pronounced in the federated domain, where optimizing models\nfor individual nodes poses significant difficulty. Many methods have been\ndeveloped to tackle this problem, aiming to reduce training expenses and time\nwhile maintaining efficient optimisation. Three suggested strategies to tackle\nthis challenge include Active Learning, Knowledge Distillation, and Local\nMemorization. These methods enable the adoption of smaller models that require\nfewer computational resources and allow for model personalization with local\ninsights, thereby improving the effectiveness of current models. The present\nstudy delves into the fundamental principles of these three approaches and\nproposes an advanced Federated Learning System that utilises different\nPersonalisation methods towards improving the accuracy of AI models and\nenhancing user experience in real-time NG-IoT applications, investigating the\nefficacy of these techniques in the local and federated domain. The results of\nthe original and optimised models are then compared in both local and federated\ncontexts using a comparison analysis. The post-analysis shows encouraging\noutcomes when it comes to optimising and personalising the models with the\nsuggested techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06904v1",
    "published_date": "2024-09-10 23:00:19 UTC",
    "updated_date": "2024-09-10 23:00:19 UTC"
  },
  {
    "arxiv_id": "2409.06892v1",
    "title": "Formative Study for AI-assisted Data Visualization",
    "authors": [
      "Rania Saber",
      "Anna Fariha"
    ],
    "abstract": "This formative study investigates the impact of data quality on AI-assisted\ndata visualizations, focusing on how uncleaned datasets influence the outcomes\nof these tools. By generating visualizations from datasets with inherent\nquality issues, the research aims to identify and categorize the specific\nvisualization problems that arise. The study further explores potential methods\nand tools to address these visualization challenges efficiently and\neffectively. Although tool development has not yet been undertaken, the\nfindings emphasize enhancing AI visualization tools to handle flawed data\nbetter. This research underscores the critical need for more robust,\nuser-friendly solutions that facilitate quicker and easier correction of data\nand visualization errors, thereby improving the overall reliability and\nusability of AI-assisted data visualization processes.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06892v1",
    "published_date": "2024-09-10 22:20:28 UTC",
    "updated_date": "2024-09-10 22:20:28 UTC"
  },
  {
    "arxiv_id": "2409.06883v1",
    "title": "A Dataset for Evaluating LLM-based Evaluation Functions for Research Question Extraction Task",
    "authors": [
      "Yuya Fujisaki",
      "Shiro Takagi",
      "Hideki Asoh",
      "Wataru Kumagai"
    ],
    "abstract": "The progress in text summarization techniques has been remarkable. However\nthe task of accurately extracting and summarizing necessary information from\nhighly specialized documents such as research papers has not been sufficiently\ninvestigated. We are focusing on the task of extracting research questions (RQ)\nfrom research papers and construct a new dataset consisting of machine learning\npapers, RQ extracted from these papers by GPT-4, and human evaluations of the\nextracted RQ from multiple perspectives. Using this dataset, we systematically\ncompared recently proposed LLM-based evaluation functions for summarizations,\nand found that none of the functions showed sufficiently high correlations with\nhuman evaluations. We expect our dataset provides a foundation for further\nresearch on developing better evaluation functions tailored to the RQ\nextraction task, and contribute to enhance the performance of the task. The\ndataset is available at https://github.com/auto-res/PaperRQ-HumanAnno-Dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06883v1",
    "published_date": "2024-09-10 21:54:46 UTC",
    "updated_date": "2024-09-10 21:54:46 UTC"
  },
  {
    "arxiv_id": "2409.06859v2",
    "title": "NSP: A Neuro-Symbolic Natural Language Navigational Planner",
    "authors": [
      "William English",
      "Dominic Simon",
      "Sumit Jha",
      "Rickard Ewetz"
    ],
    "abstract": "Path planners that can interpret free-form natural language instructions hold\npromise to automate a wide range of robotics applications. These planners\nsimplify user interactions and enable intuitive control over complex\nsemi-autonomous systems. While existing symbolic approaches offer guarantees on\nthe correctness and efficiency, they struggle to parse free-form natural\nlanguage inputs. Conversely, neural approaches based on pre-trained Large\nLanguage Models (LLMs) can manage natural language inputs but lack performance\nguarantees. In this paper, we propose a neuro-symbolic framework for path\nplanning from natural language inputs called NSP. The framework leverages the\nneural reasoning abilities of LLMs to i) craft symbolic representations of the\nenvironment and ii) a symbolic path planning algorithm. Next, a solution to the\npath planning problem is obtained by executing the algorithm on the environment\nrepresentation. The framework uses a feedback loop from the symbolic execution\nenvironment to the neural generation process to self-correct syntax errors and\nsatisfy execution time constraints. We evaluate our neuro-symbolic approach\nusing a benchmark suite with 1500 path-planning problems. The experimental\nevaluation shows that our neuro-symbolic approach produces 90.1% valid paths\nthat are on average 19-77% shorter than state-of-the-art neural approaches.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, Preprint of paper accepted at 23rd International Conference\n  on Machine Learning and Applications (ICMLA) 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.06859v2",
    "published_date": "2024-09-10 20:49:05 UTC",
    "updated_date": "2024-09-13 22:13:01 UTC"
  },
  {
    "arxiv_id": "2409.17158v1",
    "title": "Cross Dataset Analysis and Network Architecture Repair for Autonomous Car Lane Detection",
    "authors": [
      "Parth Ganeriwala",
      "Siddhartha Bhattacharyya",
      "Raja Muthalagu"
    ],
    "abstract": "Transfer Learning has become one of the standard methods to solve problems to\novercome the isolated learning paradigm by utilizing knowledge acquired for one\ntask to solve another related one. However, research needs to be done, to\nidentify the initial steps before inducing transfer learning to applications\nfor further verification and explainablity. In this research, we have performed\ncross dataset analysis and network architecture repair for the lane detection\napplication in autonomous vehicles. Lane detection is an important aspect of\nautonomous vehicles driving assistance system. In most circumstances, modern\ndeep-learning-based lane recognition systems are successful, but they struggle\nwith lanes with complex topologies. The proposed architecture, ERFCondLaneNet\nis an enhancement to the CondlaneNet used for lane identification framework to\nsolve the difficulty of detecting lane lines with complex topologies like\ndense, curved and fork lines. The newly proposed technique was tested on two\ncommon lane detecting benchmarks, CULane and CurveLanes respectively, and two\ndifferent backbones, ResNet and ERFNet. The researched technique with\nERFCondLaneNet, exhibited similar performance in comparison to\nResnetCondLaneNet, while using 33% less features, resulting in a reduction of\nmodel size by 46%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17158v1",
    "published_date": "2024-09-10 20:27:49 UTC",
    "updated_date": "2024-09-10 20:27:49 UTC"
  },
  {
    "arxiv_id": "2409.06851v3",
    "title": "LIME: Less Is More for MLLM Evaluation",
    "authors": [
      "King Zhu",
      "Qianbo Zang",
      "Shian Jia",
      "Siwei Wu",
      "Feiteng Fang",
      "Yizhi Li",
      "Shawn Gavin",
      "Tuney Zheng",
      "Jiawei Guo",
      "Bo Li",
      "Haoning Wu",
      "Xingwei Qu",
      "Jian Yang",
      "Zachary Liu",
      "Xiang Yue",
      "J. H. Liu",
      "Chenghua Lin",
      "Min Yang",
      "Shiwen Ni",
      "Wenhao Huang",
      "Ge Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are evaluated on various benchmarks,\nsuch as image captioning, visual question answering, and reasoning. However,\nmany of these benchmarks include overly simple or uninformative samples,\ncomplicating the effective distinction of different MLLMs' performance.\nFurthermore, evaluating models across numerous benchmarks incurs a significant\ncomputational burden. To address these issues, we propose LIME (Less Is More\nfor MLLM Evaluation), a refined and efficient benchmark curated through a\nsemi-automated pipeline. This pipeline filters out uninformative samples and\neliminates answer leakage by focusing on tasks that necessitate image-based\nunderstanding. Our experiments indicate that LIME reduces the number of samples\nby 76% and evaluation time by 77%, while also providing a more effective means\nof distinguishing the capabilities of different models. Notably, we find that\ntraditional automatic metrics, such as CIDEr, are inadequate for assessing\nMLLMs' captioning performance; excluding the caption task score yields a more\naccurate reflection of overall model performance. All code and data are\navailable at https://github.com/kangreen0210/LIME.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06851v3",
    "published_date": "2024-09-10 20:19:14 UTC",
    "updated_date": "2024-10-13 18:11:26 UTC"
  },
  {
    "arxiv_id": "2409.06817v1",
    "title": "Bifurcation Identification for Ultrasound-driven Robotic Cannulation",
    "authors": [
      "Cecilia G. Morales",
      "Dhruv Srikanth",
      "Jack H. Good",
      "Keith A. Dufendach",
      "Artur Dubrawski"
    ],
    "abstract": "In trauma and critical care settings, rapid and precise intravascular access\nis key to patients' survival. Our research aims at ensuring this access, even\nwhen skilled medical personnel are not readily available. Vessel bifurcations\nare anatomical landmarks that can guide the safe placement of catheters or\nneedles during medical procedures. Although ultrasound is advantageous in\nnavigating anatomical landmarks in emergency scenarios due to its portability\nand safety, to our knowledge no existing algorithm can autonomously extract\nvessel bifurcations using ultrasound images. This is primarily due to the\nlimited availability of ground truth data, in particular, data from live\nsubjects, needed for training and validating reliable models. Researchers often\nresort to using data from anatomical phantoms or simulations. We introduce\nBIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a\nnovel algorithm that identifies vessel bifurcations and provides optimal needle\ninsertion sites for an autonomous robotic cannulation system. BIFURC integrates\nexpert knowledge with deep learning techniques to efficiently detect vessel\nbifurcations within the femoral region and can be trained on a limited amount\nof in-vivo data. We evaluated our algorithm using a medical phantom as well as\nreal-world experiments involving live pigs. In all cases, BIFURC consistently\nidentified bifurcation points and needle insertion locations in alignment with\nthose identified by expert clinicians.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06817v1",
    "published_date": "2024-09-10 18:53:52 UTC",
    "updated_date": "2024-09-10 18:53:52 UTC"
  },
  {
    "arxiv_id": "2409.09082v1",
    "title": "Shadowed AHP for multi-criteria supplier selection",
    "authors": [
      "Mohamed Abdel Hameed El-Hawy"
    ],
    "abstract": "Numerous techniques of multi-criteria decision-making (MCDM) have been\nproposed in a variety of business domains. One of the well-known methods is the\nAnalytical Hierarchical Process (AHP). Various uncertain numbers are commonly\nused to represent preference values in AHP problems. In the case of\nmulti-granularity linguistic information, several methods have been proposed to\naddress this type of AHP problem. This paper introduces a novel method to solve\nthis problem using shadowed fuzzy numbers (SFNs). These numbers are\ncharacterized by approximating different types of fuzzy numbers and preserving\ntheir uncertainty properties. The new Shadowed AHP method is proposed to handle\npreference values which are represented by multi-types of uncertain numbers.\nThe new approach converts multi-granular preference values into unified model\nof shadowed fuzzy numbers and utilizes their properties. A new ranking approach\nis introduced to order the results of aggregation preferences. The new approach\nis applied to solve a supplier selection problem in which multi-granular\ninformation are used. The features of the new approach are significant for\ndecision-making applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.09082v1",
    "published_date": "2024-09-10 18:44:54 UTC",
    "updated_date": "2024-09-10 18:44:54 UTC"
  },
  {
    "arxiv_id": "2409.06805v1",
    "title": "Personalized Federated Learning Techniques: Empirical Analysis",
    "authors": [
      "Azal Ahmad Khan",
      "Ahmad Faraz Khan",
      "Haider Ali",
      "Ali Anwar"
    ],
    "abstract": "Personalized Federated Learning (pFL) holds immense promise for tailoring\nmachine learning models to individual users while preserving data privacy.\nHowever, achieving optimal performance in pFL often requires a careful\nbalancing act between memory overhead costs and model accuracy. This paper\ndelves into the trade-offs inherent in pFL, offering valuable insights for\nselecting the right algorithms for diverse real-world scenarios. We empirically\nevaluate ten prominent pFL techniques across various datasets and data splits,\nuncovering significant differences in their performance. Our study reveals\ninteresting insights into how pFL methods that utilize personalized (local)\naggregation exhibit the fastest convergence due to their efficiency in\ncommunication and computation. Conversely, fine-tuning methods face limitations\nin handling data heterogeneity and potential adversarial attacks while\nmulti-objective learning methods achieve higher accuracy at the cost of\nadditional training and resource consumption. Our study emphasizes the critical\nrole of communication efficiency in scaling pFL, demonstrating how it can\nsignificantly affect resource usage in real-world deployments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06805v1",
    "published_date": "2024-09-10 18:16:28 UTC",
    "updated_date": "2024-09-10 18:16:28 UTC"
  },
  {
    "arxiv_id": "2409.06800v1",
    "title": "Adaptive Meta-Domain Transfer Learning (AMDTL): A Novel Approach for Knowledge Transfer in AI",
    "authors": [
      "Michele Laurelli"
    ],
    "abstract": "This paper presents Adaptive Meta-Domain Transfer Learning (AMDTL), a novel\nmethodology that combines principles of meta-learning with domain-specific\nadaptations to enhance the transferability of artificial intelligence models\nacross diverse and unknown domains. AMDTL aims to address the main challenges\nof transfer learning, such as domain misalignment, negative transfer, and\ncatastrophic forgetting, through a hybrid framework that emphasizes both\ngeneralization and contextual specialization. The framework integrates a\nmeta-learner trained on a diverse distribution of tasks, adversarial training\ntechniques for aligning domain feature distributions, and dynamic feature\nregulation mechanisms based on contextual domain embeddings. Experimental\nresults on benchmark datasets demonstrate that AMDTL outperforms existing\ntransfer learning methodologies in terms of accuracy, adaptation efficiency,\nand robustness. This research provides a solid theoretical and practical\nfoundation for the application of AMDTL in various fields, opening new\nperspectives for the development of more adaptable and inclusive AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06800v1",
    "published_date": "2024-09-10 18:11:48 UTC",
    "updated_date": "2024-09-10 18:11:48 UTC"
  },
  {
    "arxiv_id": "2409.06702v1",
    "title": "Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving",
    "authors": [
      "Kairui Ding",
      "Boyuan Chen",
      "Yuchen Su",
      "Huan-ang Gao",
      "Bu Jin",
      "Chonghao Sima",
      "Wuqiang Zhang",
      "Xiaohui Li",
      "Paul Barsch",
      "Hongyang Li",
      "Hao Zhao"
    ],
    "abstract": "End-to-end architectures in autonomous driving (AD) face a significant\nchallenge in interpretability, impeding human-AI trust. Human-friendly natural\nlanguage has been explored for tasks such as driving explanation and 3D\ncaptioning. However, previous works primarily focused on the paradigm of\ndeclarative interpretability, where the natural language interpretations are\nnot grounded in the intermediate outputs of AD systems, making the\ninterpretations only declarative. In contrast, aligned interpretability\nestablishes a connection between language and the intermediate outputs of AD\nsystems. Here we introduce Hint-AD, an integrated AD-language system that\ngenerates language aligned with the holistic perception-prediction-planning\noutputs of the AD model. By incorporating the intermediate outputs and a\nholistic token mixer sub-network for effective feature adaptation, Hint-AD\nachieves desirable accuracy, achieving state-of-the-art results in driving\nlanguage tasks including driving explanation, 3D dense captioning, and command\nprediction. To facilitate further study on driving explanation task on\nnuScenes, we also introduce a human-labeled dataset, Nu-X. Codes, dataset, and\nmodels will be publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CoRL 2024, Project Page: https://air-discover.github.io/Hint-AD/",
    "pdf_url": "http://arxiv.org/pdf/2409.06702v1",
    "published_date": "2024-09-10 17:59:40 UTC",
    "updated_date": "2024-09-10 17:59:40 UTC"
  },
  {
    "arxiv_id": "2409.06764v1",
    "title": "Modeling Image Tone Dichotomy with the Power Function",
    "authors": [
      "Axel Martinez",
      "Gustavo Olague",
      "Emilio Hernandez"
    ],
    "abstract": "The primary purpose of this paper is to present the concept of dichotomy in\nimage illumination modeling based on the power function. In particular, we\nreview several mathematical properties of the power function to identify the\nlimitations and propose a new mathematical model capable of abstracting\nillumination dichotomy. The simplicity of the equation opens new avenues for\nclassical and modern image analysis and processing. The article provides\npractical and illustrative image examples to explain how the new model manages\ndichotomy in image perception. The article shows dichotomy image space as a\nviable way to extract rich information from images despite poor contrast linked\nto tone, lightness, and color perception. Moreover, a comparison with\nstate-of-the-art methods in image enhancement provides evidence of the method's\nvalue.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "49 pages, 11 figures and 36 references",
    "pdf_url": "http://arxiv.org/pdf/2409.06764v1",
    "published_date": "2024-09-10 17:55:09 UTC",
    "updated_date": "2024-09-10 17:55:09 UTC"
  },
  {
    "arxiv_id": "2409.06692v1",
    "title": "HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs",
    "authors": [
      "Umair Qudus",
      "Michael Roeder",
      "Muhammad Saleem",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "abstract": "We consider fact-checking approaches that aim to predict the veracity of\nassertions in knowledge graphs. Five main categories of fact-checking\napproaches for knowledge graphs have been proposed in the recent literature, of\nwhich each is subject to partially overlapping limitations. In particular,\ncurrent text-based approaches are limited by manual feature engineering.\nPath-based and rule-based approaches are limited by their exclusive use of\nknowledge graphs as background knowledge, and embedding-based approaches suffer\nfrom low accuracy scores on current fact-checking tasks. We propose a hybrid\napproach -- dubbed HybridFC -- that exploits the diversity of existing\ncategories of fact-checking approaches within an ensemble learning setting to\nachieve a significantly better prediction performance. In particular, our\napproach outperforms the state of the art by 0.14 to 0.27 in terms of Area\nUnder the Receiver Operating Characteristic curve on the FactBench dataset. Our\ncode is open-source and can be found at https://github.com/dice-group/HybridFC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06692v1",
    "published_date": "2024-09-10 17:55:00 UTC",
    "updated_date": "2024-09-10 17:55:00 UTC"
  },
  {
    "arxiv_id": "2409.06691v3",
    "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
    "authors": [
      "Hiroki Furuta",
      "Kuang-Huei Lee",
      "Shixiang Shane Gu",
      "Yutaka Matsuo",
      "Aleksandra Faust",
      "Heiga Zen",
      "Izzeddin Gur"
    ],
    "abstract": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, human preferences can vary\nacross individuals, and therefore should be represented distributionally. In\nthis work, we introduce the distributional soft preference labels and improve\nDirect Preference Optimization (DPO) with a weighted geometric average of the\nLLM output likelihood in the loss function. This approach adjusts the scale of\nlearning loss based on the soft labels such that the loss would approach zero\nwhen the responses are closer to equally preferred. This simple modification\ncan be easily applied to any DPO-based methods and mitigate over-optimization\nand objective mismatch, which prior works suffer from. Our experiments simulate\nthe soft preference labels with AI feedback from LLMs and demonstrate that\ngeometric averaging consistently improves performance on standard benchmarks\nfor alignment research. In particular, we observe more preferable responses\nthan binary labels and significant improvements where modestly-confident labels\nare in the majority.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.06691v3",
    "published_date": "2024-09-10 17:54:28 UTC",
    "updated_date": "2024-12-30 11:05:20 UTC"
  },
  {
    "arxiv_id": "2409.06690v1",
    "title": "Benchmarking Sub-Genre Classification For Mainstage Dance Music",
    "authors": [
      "Hongzhi Shu",
      "Xinglin Li",
      "Hongyu Jiang",
      "Minghao Fu",
      "Xinyu Li"
    ],
    "abstract": "Music classification, with a wide range of applications, is one of the most\nprominent tasks in music information retrieval. To address the absence of\ncomprehensive datasets and high-performing methods in the classification of\nmainstage dance music, this work introduces a novel benchmark comprising a new\ndataset and a baseline. Our dataset extends the number of sub-genres to cover\nmost recent mainstage live sets by top DJs worldwide in music festivals. A\ncontinuous soft labeling approach is employed to account for tracks that span\nmultiple sub-genres, preserving the inherent sophistication. For the baseline,\nwe developed deep learning models that outperform current state-of-the-art\nmultimodel language models, which struggle to identify house music sub-genres,\nemphasizing the need for specialized models trained on fine-grained datasets.\nOur benchmark is applicable to serve for application scenarios such as music\nrecommendation, DJ set curation, and interactive multimedia, where we also\nprovide video demos. Our code is on\n\\url{https://anonymous.4open.science/r/Mainstage-EDM-Benchmark/}.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "I.2.1"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.06690v1",
    "published_date": "2024-09-10 17:54:00 UTC",
    "updated_date": "2024-09-10 17:54:00 UTC"
  },
  {
    "arxiv_id": "2409.06762v1",
    "title": "Generative Hierarchical Materials Search",
    "authors": [
      "Sherry Yang",
      "Simon Batzner",
      "Ruiqi Gao",
      "Muratahan Aykol",
      "Alexander L. Gaunt",
      "Brendan McMorrow",
      "Danilo J. Rezende",
      "Dale Schuurmans",
      "Igor Mordatch",
      "Ekin D. Cubuk"
    ],
    "abstract": "Generative models trained at scale can now produce text, video, and more\nrecently, scientific data such as crystal structures. In applications of\ngenerative approaches to materials science, and in particular to crystal\nstructures, the guidance from the domain expert in the form of high-level\ninstructions can be essential for an automated system to output candidate\ncrystals that are viable for downstream research. In this work, we formulate\nend-to-end language-to-structure generation as a multi-objective optimization\nproblem, and propose Generative Hierarchical Materials Search (GenMS) for\ncontrollable generation of crystal structures. GenMS consists of (1) a language\nmodel that takes high-level natural language as input and generates\nintermediate textual information about a crystal (e.g., chemical formulae), and\n(2) a diffusion model that takes intermediate information as input and\ngenerates low-level continuous value crystal structures. GenMS additionally\nuses a graph neural network to predict properties (e.g., formation energy) from\nthe generated crystal structures. During inference, GenMS leverages all three\ncomponents to conduct a forward tree search over the space of possible\nstructures. Experiments show that GenMS outperforms other alternatives of\ndirectly using language models to generate structures both in satisfying user\nrequest and in generating low-energy structures. We confirm that GenMS is able\nto generate common crystal structures such as double perovskites, or spinels,\nsolely from natural language input, and hence can form the foundation for more\ncomplex structure generation in near future.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "https://generative-materials.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2409.06762v1",
    "published_date": "2024-09-10 17:51:28 UTC",
    "updated_date": "2024-09-10 17:51:28 UTC"
  },
  {
    "arxiv_id": "2409.06673v1",
    "title": "Liability and Insurance for Catastrophic Losses: the Nuclear Power Precedent and Lessons for AI",
    "authors": [
      "Cristian Trout"
    ],
    "abstract": "As AI systems become more autonomous and capable, experts warn of them\npotentially causing catastrophic losses. Drawing on the successful precedent\nset by the nuclear power industry, this paper argues that developers of\nfrontier AI models should be assigned limited, strict, and exclusive third\nparty liability for harms resulting from Critical AI Occurrences (CAIOs) -\nevents that cause or easily could have caused catastrophic losses. Mandatory\ninsurance for CAIO liability is recommended to overcome developers'\njudgment-proofness, mitigate winner's curse dynamics, and leverage insurers'\nquasi-regulatory abilities. Based on theoretical arguments and observations\nfrom the analogous nuclear power context, insurers are expected to engage in a\nmix of causal risk-modeling, monitoring, lobbying for stricter regulation, and\nproviding loss prevention guidance in the context of insuring against\nheavy-tail risks from AI. While not a substitute for regulation, clear\nliability assignment and mandatory insurance can help efficiently allocate\nresources to risk-modeling and safe design, facilitating future regulatory\nefforts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to Generative AI and Law Workshop at the International\n  Conference on Machine Learning (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.06673v1",
    "published_date": "2024-09-10 17:41:31 UTC",
    "updated_date": "2024-09-10 17:41:31 UTC"
  },
  {
    "arxiv_id": "2409.06672v1",
    "title": "Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort",
    "authors": [
      "Cristian Trout"
    ],
    "abstract": "Many experts believe that AI systems will sooner or later pose uninsurable\nrisks, including existential risks. This creates an extreme judgment-proof\nproblem: few if any parties can be held accountable ex post in the event of\nsuch a catastrophe. This paper proposes a novel solution: a\ngovernment-provided, mandatory indemnification program for AI developers. The\nprogram uses risk-priced indemnity fees to induce socially optimal levels of\ncare. Risk-estimates are determined by surveying experts, including indemnified\ndevelopers. The Bayesian Truth Serum mechanism is employed to incent honest and\neffortful responses. Compared to alternatives, this approach arguably better\nleverages all private information, and provides a clearer signal to indemnified\ndevelopers regarding what risks they must mitigate to lower their fees. It's\nrecommended that collected fees be used to help fund the safety research\ndevelopers need, employing a fund matching mechanism (Quadratic Financing) to\ninduce an optimal supply of this public good. Under Quadratic Financing, safety\nresearch projects would compete for private contributions from developers,\nsignaling how much each is to be supplemented with public funds.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "q-fin.RM"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to Generative AI and Law Workshop at the International\n  Conference on Machine Learning (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.06672v1",
    "published_date": "2024-09-10 17:41:24 UTC",
    "updated_date": "2024-09-10 17:41:24 UTC"
  },
  {
    "arxiv_id": "2409.06666v2",
    "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
    "authors": [
      "Qingkai Fang",
      "Shoutao Guo",
      "Yan Zhou",
      "Zhengrui Ma",
      "Shaolei Zhang",
      "Yang Feng"
    ],
    "abstract": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.06666v2",
    "published_date": "2024-09-10 17:34:34 UTC",
    "updated_date": "2025-03-01 12:59:49 UTC"
  },
  {
    "arxiv_id": "2409.06662v1",
    "title": "World-Grounded Human Motion Recovery via Gravity-View Coordinates",
    "authors": [
      "Zehong Shen",
      "Huaijin Pi",
      "Yan Xia",
      "Zhi Cen",
      "Sida Peng",
      "Zechen Hu",
      "Hujun Bao",
      "Ruizhen Hu",
      "Xiaowei Zhou"
    ],
    "abstract": "We present a novel method for recovering world-grounded human motion from\nmonocular video. The main challenge lies in the ambiguity of defining the world\ncoordinate system, which varies between sequences. Previous approaches attempt\nto alleviate this issue by predicting relative motion in an autoregressive\nmanner, but are prone to accumulating errors. Instead, we propose estimating\nhuman poses in a novel Gravity-View (GV) coordinate system, which is defined by\nthe world gravity and the camera view direction. The proposed GV system is\nnaturally gravity-aligned and uniquely defined for each video frame, largely\nreducing the ambiguity of learning image-pose mapping. The estimated poses can\nbe transformed back to the world coordinate system using camera rotations,\nforming a global motion sequence. Additionally, the per-frame estimation avoids\nerror accumulation in the autoregressive methods. Experiments on in-the-wild\nbenchmarks demonstrate that our method recovers more realistic motion in both\nthe camera space and world-grounded settings, outperforming state-of-the-art\nmethods in both accuracy and speed. The code is available at\nhttps://zju3dv.github.io/gvhmr/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at SIGGRAPH Asia 2024 (Conference Track). Project page:\n  https://zju3dv.github.io/gvhmr/",
    "pdf_url": "http://arxiv.org/pdf/2409.06662v1",
    "published_date": "2024-09-10 17:25:47 UTC",
    "updated_date": "2024-09-10 17:25:47 UTC"
  },
  {
    "arxiv_id": "2409.06644v2",
    "title": "EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis",
    "authors": [
      "Danli Shi",
      "Weiyi Zhang",
      "Jiancheng Yang",
      "Siyu Huang",
      "Xiaolan Chen",
      "Mayinuer Yusufu",
      "Kai Jin",
      "Shan Lin",
      "Shunming Liu",
      "Qing Zhang",
      "Mingguang He"
    ],
    "abstract": "Early detection of eye diseases like glaucoma, macular degeneration, and\ndiabetic retinopathy is crucial for preventing vision loss. While artificial\nintelligence (AI) foundation models hold significant promise for addressing\nthese challenges, existing ophthalmic foundation models primarily focus on a\nsingle modality, whereas diagnosing eye diseases requires multiple modalities.\nA critical yet often overlooked aspect is harnessing the multi-view information\nacross various modalities for the same patient. Additionally, due to the\nlong-tail nature of ophthalmic diseases, standard fully supervised or\nunsupervised learning approaches often struggle. Therefore, it is essential to\nintegrate clinical text to capture a broader spectrum of diseases. We propose\nEyeCLIP, a visual-language foundation model developed using over 2.77 million\nmulti-modal ophthalmology images with partial text data. To fully leverage the\nlarge multi-modal unlabeled and labeled data, we introduced a pretraining\nstrategy that combines self-supervised reconstructions, multi-modal image\ncontrastive learning, and image-text contrastive learning to learn a shared\nrepresentation of multiple modalities. Through evaluation using 14 benchmark\ndatasets, EyeCLIP can be transferred to a wide range of downstream tasks\ninvolving ocular and systemic diseases, achieving state-of-the-art performance\nin disease classification, visual question answering, and cross-modal\nretrieval. EyeCLIP represents a significant advancement over previous methods,\nespecially showcasing few-shot, even zero-shot capabilities in real-world\nlong-tail scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06644v2",
    "published_date": "2024-09-10 17:00:19 UTC",
    "updated_date": "2024-09-11 17:00:09 UTC"
  },
  {
    "arxiv_id": "2409.06635v4",
    "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders",
    "authors": [
      "Wenyu Zhang",
      "Shuo Sun",
      "Bin Wang",
      "Xunlong Zou",
      "Zhuohan Liu",
      "Yingxu He",
      "Geyu Lin",
      "Nancy F. Chen",
      "Ai Ti Aw"
    ],
    "abstract": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.06635v4",
    "published_date": "2024-09-10 16:46:18 UTC",
    "updated_date": "2025-04-21 09:48:05 UTC"
  },
  {
    "arxiv_id": "2409.13740v2",
    "title": "Language agents achieve superhuman synthesis of scientific knowledge",
    "authors": [
      "Michael D. Skarlinski",
      "Sam Cox",
      "Jon M. Laurent",
      "James D. Braza",
      "Michaela Hinks",
      "Michael J. Hammerling",
      "Manvitha Ponnapati",
      "Samuel G. Rodriques",
      "Andrew D. White"
    ],
    "abstract": "Language models are known to hallucinate incorrect information, and it is\nunclear if they are sufficiently accurate and reliable for use in scientific\nresearch. We developed a rigorous human-AI comparison methodology to evaluate\nlanguage model agents on real-world literature search tasks covering\ninformation retrieval, summarization, and contradiction detection tasks. We\nshow that PaperQA2, a frontier language model agent optimized for improved\nfactuality, matches or exceeds subject matter expert performance on three\nrealistic literature research tasks without any restrictions on humans (i.e.,\nfull access to internet, search tools, and time). PaperQA2 writes cited,\nWikipedia-style summaries of scientific topics that are significantly more\naccurate than existing, human-written Wikipedia articles. We also introduce a\nhard benchmark for scientific literature research called LitQA2 that guided\ndesign of PaperQA2, leading to it exceeding human performance. Finally, we\napply PaperQA2 to identify contradictions within the scientific literature, an\nimportant scientific task that is challenging for humans. PaperQA2 identifies\n2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of\nwhich 70% are validated by human experts. These results demonstrate that\nlanguage model agents are now capable of exceeding domain experts across\nmeaningful tasks on scientific literature.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13740v2",
    "published_date": "2024-09-10 16:37:58 UTC",
    "updated_date": "2024-09-26 15:27:08 UTC"
  },
  {
    "arxiv_id": "2409.06756v1",
    "title": "Beyond designer's knowledge: Generating materials design hypotheses via large language models",
    "authors": [
      "Quanliang Liu",
      "Maciej P. Polak",
      "So Yeon Kim",
      "MD Al Amin Shuvo",
      "Hrishikesh Shridhar Deodhar",
      "Jeongsoo Han",
      "Dane Morgan",
      "Hyunseok Oh"
    ],
    "abstract": "Materials design often relies on human-generated hypotheses, a process\ninherently limited by cognitive constraints such as knowledge gaps and limited\nability to integrate and extract knowledge implications, particularly when\nmultidisciplinary expertise is required. This work demonstrates that large\nlanguage models (LLMs), coupled with prompt engineering, can effectively\ngenerate non-trivial materials hypotheses by integrating scientific principles\nfrom diverse sources without explicit design guidance by human experts. These\ninclude design ideas for high-entropy alloys with superior cryogenic properties\nand halide solid electrolytes with enhanced ionic conductivity and formability.\nThese design ideas have been experimentally validated in high-impact\npublications in 2023 not available in the LLM training data, demonstrating the\nLLM's ability to generate highly valuable and realizable innovative ideas not\nestablished in the literature. Our approach primarily leverages materials\nsystem charts encoding processing-structure-property relationships, enabling\nmore effective data integration by condensing key information from numerous\npapers, and evaluation and categorization of numerous hypotheses for human\ncognition, both through the LLM. This LLM-driven approach opens the door to new\navenues of artificial intelligence-driven materials discovery by accelerating\ndesign, democratizing innovation, and expanding capabilities beyond the\ndesigner's direct knowledge.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06756v1",
    "published_date": "2024-09-10 16:28:50 UTC",
    "updated_date": "2024-09-10 16:28:50 UTC"
  },
  {
    "arxiv_id": "2409.06624v1",
    "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio",
    "authors": [
      "Ningyuan Xi",
      "Yetao Wu",
      "Kun Fan",
      "Teng Chen",
      "Qingqing Gu",
      "Peng Yu",
      "Jinxian Qu",
      "Chenxi Liu",
      "Zhonglin Jiang",
      "Yong Chen",
      "Luo Ji"
    ],
    "abstract": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.06624v1",
    "published_date": "2024-09-10 16:26:43 UTC",
    "updated_date": "2024-09-10 16:26:43 UTC"
  },
  {
    "arxiv_id": "2409.06615v6",
    "title": "One-Shot Imitation under Mismatched Execution",
    "authors": [
      "Kushal Kedia",
      "Prithwish Dan",
      "Angela Chao",
      "Maximus Adrian Pace",
      "Sanjiban Choudhury"
    ],
    "abstract": "Human demonstrations as prompts are a powerful way to program robots to do\nlong-horizon manipulation tasks. However, translating these demonstrations into\nrobot-executable actions presents significant challenges due to execution\nmismatches in movement styles and physical capabilities. Existing methods for\nhuman-robot translation either depend on paired data, which is infeasible to\nscale, or rely heavily on frame-level visual similarities that often break down\nin practice. To address these challenges, we propose RHyME, a novel framework\nthat automatically pairs human and robot trajectories using sequence-level\noptimal transport cost functions. Given long-horizon robot demonstrations,\nRHyME synthesizes semantically equivalent human videos by retrieving and\ncomposing short-horizon human clips. This approach facilitates effective policy\ntraining without the need for paired data. RHyME successfully imitates a range\nof cross-embodiment demonstrators, both in simulation and with a real human\nhand, achieving over 50% increase in task success compared to previous methods.\nWe release our code and datasets at https://portal-cornell.github.io/rhyme/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06615v6",
    "published_date": "2024-09-10 16:11:57 UTC",
    "updated_date": "2025-03-28 22:16:37 UTC"
  },
  {
    "arxiv_id": "2409.06754v4",
    "title": "Scaling Law Hypothesis for Multimodal Model",
    "authors": [
      "Qingyun Sun",
      "Zhen Guo",
      "PIN AI Team"
    ],
    "abstract": "We propose a scaling law hypothesis for multimodal models processing text,\naudio, images, and video within a shared token and embedding space. Our\nframework predicts model performance based on modality-specific compression and\ntokenization efficiency, extending established scaling laws from text-based\ndecoder models to mixed-modality systems. We explore whether leveraging more\ntraining data in multiple modalities can reduce the size of the multimodal\nmodel, enabling efficient deployment on resource-constrained devices.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06754v4",
    "published_date": "2024-09-10 16:05:02 UTC",
    "updated_date": "2024-11-11 18:32:16 UTC"
  },
  {
    "arxiv_id": "2409.06612v1",
    "title": "Label-free Monitoring of Self-Supervised Learning Progress",
    "authors": [
      "Isaac Xu",
      "Scott Lowe",
      "Thomas Trappenberg"
    ],
    "abstract": "Self-supervised learning (SSL) is an effective method for exploiting\nunlabelled data to learn a high-level embedding space that can be used for\nvarious downstream tasks. However, existing methods to monitor the quality of\nthe encoder -- either during training for one model or to compare several\ntrained models -- still rely on access to annotated data. When SSL\nmethodologies are applied to new data domains, a sufficiently large labelled\ndataset may not always be available. In this study, we propose several\nevaluation metrics which can be applied on the embeddings of unlabelled data\nand investigate their viability by comparing them to linear probe accuracy (a\ncommon metric which utilizes an annotated dataset). In particular, we apply\n$k$-means clustering and measure the clustering quality with the silhouette\nscore and clustering agreement. We also measure the entropy of the embedding\ndistribution. We find that while the clusters did correspond better to the\nground truth annotations as training of the network progressed, label-free\nclustering metrics correlated with the linear probe accuracy only when training\nwith SSL methods SimCLR and MoCo-v2, but not with SimSiam. Additionally,\nalthough entropy did not always have strong correlations with LP accuracy, this\nappears to be due to instability arising from early training, with the metric\nstabilizing and becoming more reliable at later stages of learning.\nFurthermore, while entropy generally decreases as learning progresses, this\ntrend reverses for SimSiam. More research is required to establish the cause\nfor this unexpected behaviour. Lastly, we find that while clustering based\napproaches are likely only viable for same-architecture comparisons, entropy\nmay be architecture-independent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06612v1",
    "published_date": "2024-09-10 16:04:10 UTC",
    "updated_date": "2024-09-10 16:04:10 UTC"
  },
  {
    "arxiv_id": "2409.06608v1",
    "title": "Simulation-based Scenario Generation for Robust Hybrid AI for Autonomy",
    "authors": [
      "Hambisa Keno",
      "Nicholas J. Pioch",
      "Christopher Guagliano",
      "Timothy H. Chung"
    ],
    "abstract": "Application of Unmanned Aerial Vehicles (UAVs) in search and rescue,\nemergency management, and law enforcement has gained traction with the advent\nof low-cost platforms and sensor payloads. The emergence of hybrid neural and\nsymbolic AI approaches for complex reasoning is expected to further push the\nboundaries of these applications with decreasing levels of human intervention.\nHowever, current UAV simulation environments lack semantic context suited to\nthis hybrid approach. To address this gap, HAMERITT (Hybrid Ai Mission\nEnvironment for RapId Training and Testing) provides a simulation-based\nautonomy software framework that supports the training, testing and assurance\nof neuro-symbolic algorithms for autonomous maneuver and perception reasoning.\nHAMERITT includes scenario generation capabilities that offer mission-relevant\ncontextual symbolic information in addition to raw sensor data. Scenarios\ninclude symbolic descriptions for entities of interest and their relations to\nscene elements, as well as spatial-temporal constraints in the form of\ntime-bounded areas of interest with prior probabilities and restricted zones\nwithin those areas. HAMERITT also features support for training distinct\nalgorithm threads for maneuver vs. perception within an end-to-end mission run.\nFuture work includes improving scenario realism and scaling symbolic context\ngeneration through automated workflow.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T20, 68T45, 68T40",
      "J.7; C.3"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2409.06608v1",
    "published_date": "2024-09-10 16:00:26 UTC",
    "updated_date": "2024-09-10 16:00:26 UTC"
  },
  {
    "arxiv_id": "2409.06607v3",
    "title": "An Ontology-based Approach Towards Traceable Behavior Specifications in Automated Driving",
    "authors": [
      "Nayel Fabian Salem",
      "Marcus Nolte",
      "Veronica Haber",
      "Till Menzel",
      "Hans Steege",
      "Robert Graubohm",
      "Markus Maurer"
    ],
    "abstract": "Vehicles in public traffic that are equipped with Automated Driving Systems\nare subject to a number of expectations: Among other aspects, their behavior\nshould be safe, conforming to the rules of the road and provide mobility to\ntheir users. This poses challenges for the developers of such systems:\nDevelopers are responsible for specifying this behavior, for example, in terms\nof requirements at system design time. As we will discuss in the article, this\nspecification always involves the need for assumptions and trade-offs. As a\nresult, insufficiencies in such a behavior specification can occur that can\npotentially lead to unsafe system behavior. In order to support the\nidentification of specification insufficiencies, requirements and respective\nassumptions need to be made explicit. In this article, we propose the Semantic\nNorm Behavior Analysis as an ontology-based approach to specify the behavior\nfor an Automated Driving System equipped vehicle. We use ontologies to formally\nrepresent specified behavior for a targeted operational environment, and to\nestablish traceability between specified behavior and the addressed stakeholder\nneeds. Furthermore, we illustrate the application of the Semantic Norm Behavior\nAnalysis in a German legal context with two example scenarios and evaluate our\nresults. Our evaluation shows that the explicit documentation of assumptions in\nthe behavior specification supports both the identification of specification\ninsufficiencies and their treatment. Therefore, this article provides\nrequirements, terminology and an according methodology to facilitate\nontology-based behavior specifications in automated driving.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.SE",
    "comment": "24 pages, 12 figures, submitted for publication",
    "pdf_url": "http://arxiv.org/pdf/2409.06607v3",
    "published_date": "2024-09-10 16:00:22 UTC",
    "updated_date": "2024-11-15 14:09:05 UTC"
  },
  {
    "arxiv_id": "2409.06585v1",
    "title": "Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records",
    "authors": [
      "Zoe Hancox",
      "Sarah R. Kingsbury",
      "Andrew Clegg",
      "Philip G. Conaghan",
      "Samuel D. Relton"
    ],
    "abstract": "Background: Hip replacement procedures improve patient lives by relieving\npain and restoring mobility. Predicting hip replacement in advance could reduce\npain by enabling timely interventions, prioritising individuals for surgery or\nrehabilitation, and utilising physiotherapy to potentially delay the need for\njoint replacement. This study predicts hip replacement a year in advance to\nenhance quality of life and health service efficiency. Methods: Adapting\nprevious work using Temporal Graph Convolutional Neural Network (TG-CNN)\nmodels, we construct temporal graphs from primary care medical event codes,\nsourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip\nreplacement risk. We match hip replacement cases to controls by age, sex, and\nIndex of Multiple Deprivation. The model, trained on 9,187 cases and 9,187\ncontrols, predicts hip replacement one year in advance. We validate the model\non two unseen datasets, recalibrating for class imbalance. Additionally, we\nconduct an ablation study and compare against four baseline models. Results:\nOur best model predicts hip replacement risk one year in advance with an AUROC\nof 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),\nachieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after\nrecalibration. Conclusions: The TG-CNN model effectively predicts hip\nreplacement risk by identifying patterns in patient trajectories, potentially\nimproving understanding and management of hip-related conditions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 2024 International Conference on Machine Learning and\n  Applications (ICMLA). 8 pages, 3 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.06585v1",
    "published_date": "2024-09-10 15:26:58 UTC",
    "updated_date": "2024-09-10 15:26:58 UTC"
  },
  {
    "arxiv_id": "2409.06579v1",
    "title": "Quantifying and Enabling the Interpretability of CLIP-like Models",
    "authors": [
      "Avinash Madasu",
      "Yossi Gandelsman",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "abstract": "CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. To bridge this gap we propose a study to quantify the interpretability\nin CLIP like models. We conduct this study on six different CLIP models from\nOpenAI and OpenCLIP which vary by size, type of pre-training data and patch\nsize. Our approach begins with using the TEXTSPAN algorithm and in-context\nlearning to break down individual attention heads into specific properties. We\nthen evaluate how easily these heads can be interpreted using new metrics which\nmeasure property consistency within heads and property disentanglement across\nheads. Our findings reveal that larger CLIP models are generally more\ninterpretable than their smaller counterparts. To further assist users in\nunderstanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a\ntool designed for interpretability analysis. CLIP-InterpreT offers five types\nof analyses: property-based nearest neighbor search, per-head topic\nsegmentation, contrastive segmentation, per-head nearest neighbors of an image,\nand per-head nearest neighbors of text.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06579v1",
    "published_date": "2024-09-10 15:19:40 UTC",
    "updated_date": "2024-09-10 15:19:40 UTC"
  },
  {
    "arxiv_id": "2409.06566v1",
    "title": "Indirect Dynamic Negotiation in the Nash Demand Game",
    "authors": [
      "Tatiana V. Guy",
      "Jitka Homolová",
      "Aleksej Gaj"
    ],
    "abstract": "The paper addresses a problem of sequential bilateral bargaining with\nincomplete information. We proposed a decision model that helps agents to\nsuccessfully bargain by performing indirect negotiation and learning the\nopponent's model. Methodologically the paper casts heuristically-motivated\nbargaining of a self-interested independent player into a framework of Bayesian\nlearning and Markov decision processes. The special form of the reward\nimplicitly motivates the players to negotiate indirectly, via closed-loop\ninteraction. We illustrate the approach by applying our model to the Nash\ndemand game, which is an abstract model of bargaining. The results indicate\nthat the established negotiation: i) leads to coordinating players' actions;\nii) results in maximising success rate of the game and iii) brings more\nindividual profit to the players.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.GT",
    "comment": "Appears in IEEE Access",
    "pdf_url": "http://arxiv.org/pdf/2409.06566v1",
    "published_date": "2024-09-10 14:58:00 UTC",
    "updated_date": "2024-09-10 14:58:00 UTC"
  },
  {
    "arxiv_id": "2409.06561v1",
    "title": "ChatGPT's Potential in Cryptography Misuse Detection: A Comparative Analysis with Static Analysis Tools",
    "authors": [
      "Ehsan Firouzi",
      "Mohammad Ghafari",
      "Mike Ebrahimi"
    ],
    "abstract": "The correct adoption of cryptography APIs is challenging for mainstream\ndevelopers, often resulting in widespread API misuse. Meanwhile, cryptography\nmisuse detectors have demonstrated inconsistent performance and remain largely\ninaccessible to most developers. We investigated the extent to which ChatGPT\ncan detect cryptography misuses and compared its performance with that of the\nstate-of-the-art static analysis tools. Our investigation, mainly based on the\nCryptoAPI-Bench benchmark, demonstrated that ChatGPT is effective in\nidentifying cryptography API misuses, and with the use of prompt engineering,\nit can even outperform leading static cryptography misuse detectors.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "ESEM 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.06561v1",
    "published_date": "2024-09-10 14:50:12 UTC",
    "updated_date": "2024-09-10 14:50:12 UTC"
  },
  {
    "arxiv_id": "2409.06518v1",
    "title": "Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games",
    "authors": [
      "Juhwan Choi",
      "YoungBin Kim"
    ],
    "abstract": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06518v1",
    "published_date": "2024-09-10 13:54:04 UTC",
    "updated_date": "2024-09-10 13:54:04 UTC"
  },
  {
    "arxiv_id": "2409.06513v3",
    "title": "Sines, Transient, Noise Neural Modeling of Piano Notes",
    "authors": [
      "Riccardo Simionato",
      "Stefano Fasciani"
    ],
    "abstract": "This paper introduces a novel method for emulating piano sounds. We propose\nto exploit the sines, transient, and noise decomposition to design a\ndifferentiable spectral modeling synthesizer replicating piano notes. Three\nsub-modules learn these components from piano recordings and generate the\ncorresponding harmonic, transient, and noise signals. Splitting the emulation\ninto three independently trainable models reduces the modeling tasks'\ncomplexity. The quasi-harmonic content is produced using a differentiable\nsinusoidal model guided by physics-derived formulas, whose parameters are\nautomatically estimated from audio recordings. The noise sub-module uses a\nlearnable time-varying filter, and the transients are generated using a deep\nconvolutional network. From singular notes, we emulate the coupling between\ndifferent keys in trichords with a convolutional-based network. Results show\nthe model matches the partial distribution of the target while predicting the\nenergy in the higher part of the spectrum presents more challenges. The energy\ndistribution in the spectra of the transient and noise components is accurate\noverall. While the model is more computationally and memory efficient,\nperceptual tests reveal limitations in accurately modeling the attack phase of\nnotes. Despite this, it generally achieves perceptual accuracy in emulating\nsingle notes and trichords.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06513v3",
    "published_date": "2024-09-10 13:48:18 UTC",
    "updated_date": "2025-02-01 13:18:06 UTC"
  },
  {
    "arxiv_id": "2409.06509v3",
    "title": "Aligning Machine and Human Visual Representations across Abstraction Levels",
    "authors": [
      "Lukas Muttenthaler",
      "Klaus Greff",
      "Frieda Born",
      "Bernhard Spitzer",
      "Simon Kornblith",
      "Michael C. Mozer",
      "Klaus-Robert Müller",
      "Thomas Unterthiner",
      "Andrew K. Lampinen"
    ],
    "abstract": "Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior in vision tasks. However,\nneural network training and human learning differ in fundamental ways, and\nneural networks often fail to generalize as robustly as humans do, raising\nquestions regarding the similarity of their underlying representations. What is\nmissing for modern learning systems to exhibit more human-like behavior? We\nhighlight a key misalignment between vision models and humans: whereas human\nconceptual knowledge is hierarchically organized from fine- to coarse-scale\ndistinctions, model representations do not accurately capture all these levels\nof abstraction. To address this misalignment, we first train a teacher model to\nimitate human judgments, then transfer human-like structure from its\nrepresentations into pretrained state-of-the-art vision foundation models.\nThese human-aligned models more accurately approximate human behavior and\nuncertainty across a wide range of similarity tasks, including a new dataset of\nhuman judgments spanning multiple levels of semantic abstractions. They also\nperform better on a diverse set of machine learning tasks, increasing\ngeneralization and out-of-distribution robustness. Thus, infusing neural\nnetworks with additional human knowledge yields a best-of-both-worlds\nrepresentation that is both more consistent with human cognition and more\npractically useful, thus paving the way toward more robust, interpretable, and\nhuman-like artificial intelligence systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "54 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.06509v3",
    "published_date": "2024-09-10 13:41:08 UTC",
    "updated_date": "2024-10-29 09:30:12 UTC"
  },
  {
    "arxiv_id": "2409.06750v2",
    "title": "Can Agents Spontaneously Form a Society? Introducing a Novel Architecture for Generative Multi-Agents to Elicit Social Emergence",
    "authors": [
      "H. Zhang",
      "J. Yin",
      "M. Jiang",
      "C. Su"
    ],
    "abstract": "Generative agents have demonstrated impressive capabilities in specific\ntasks, but most of these frameworks focus on independent tasks and lack\nattention to social interactions. We introduce a generative agent architecture\ncalled ITCMA-S, which includes a basic framework for individual agents and a\nframework called LTRHA that supports social interactions among multi-agents.\nThis architecture enables agents to identify and filter out behaviors that are\ndetrimental to social interactions, guiding them to choose more favorable\nactions. We designed a sandbox environment to simulate the natural evolution of\nsocial relationships among multiple identity-less agents for experimental\nevaluation. The results showed that ITCMA-S performed well on multiple\nevaluation indicators, demonstrating its ability to actively explore the\nenvironment, recognize new agents, and acquire new information through\ncontinuous actions and dialogue. Observations show that as agents establish\nconnections with each other, they spontaneously form cliques with internal\nhierarchies around a selected leader and organize collective activities.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "68T42",
      "I.2.7; J.4"
    ],
    "primary_category": "cs.MA",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.06750v2",
    "published_date": "2024-09-10 13:39:29 UTC",
    "updated_date": "2024-11-19 15:44:30 UTC"
  },
  {
    "arxiv_id": "2409.06477v1",
    "title": "Superior Computer Chess with Model Predictive Control, Reinforcement Learning, and Rollout",
    "authors": [
      "Atharva Gundawar",
      "Yuchao Li",
      "Dimitri Bertsekas"
    ],
    "abstract": "In this paper we apply model predictive control (MPC), rollout, and\nreinforcement learning (RL) methodologies to computer chess. We introduce a new\narchitecture for move selection, within which available chess engines are used\nas components. One engine is used to provide position evaluations in an\napproximation in value space MPC/RL scheme, while a second engine is used as\nnominal opponent, to emulate or approximate the moves of the true opponent\nplayer.\n  We show that our architecture improves substantially the performance of the\nposition evaluation engine. In other words our architecture provides an\nadditional layer of intelligence, on top of the intelligence of the engines on\nwhich it is based. This is true for any engine, regardless of its strength: top\nengines such as Stockfish and Komodo Dragon (of varying strengths), as well as\nweaker engines.\n  Structurally, our basic architecture selects moves by a one-move lookahead\nsearch, with an intermediate move generated by a nominal opponent engine, and\nfollowed by a position evaluation by another chess engine. Simpler schemes that\nforego the use of the nominal opponent, also perform better than the position\nevaluator, but not quite by as much. More complex schemes, involving multistep\nlookahead, may also be used and generally tend to perform better as the length\nof the lookahead increases.\n  Theoretically, our methodology relies on generic cost improvement properties\nand the superlinear convergence framework of Newton's method, which\nfundamentally underlies approximation in value space, and related MPC/RL and\nrollout/policy iteration schemes. A critical requirement of this framework is\nthat the first lookahead step should be executed exactly. This fact has guided\nour architectural choices, and is apparently an important factor in improving\nthe performance of even the best available chess engines.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06477v1",
    "published_date": "2024-09-10 13:05:45 UTC",
    "updated_date": "2024-09-10 13:05:45 UTC"
  },
  {
    "arxiv_id": "2409.17157v1",
    "title": "Confident Teacher, Confident Student? A Novel User Study Design for Investigating the Didactic Potential of Explanations and their Impact on Uncertainty",
    "authors": [
      "Teodor Chiaburu",
      "Frank Haußer",
      "Felix Bießmann"
    ],
    "abstract": "Evaluating the quality of explanations in Explainable Artificial Intelligence\n(XAI) is to this day a challenging problem, with ongoing debate in the research\ncommunity. While some advocate for establishing standardized offline metrics,\nothers emphasize the importance of human-in-the-loop (HIL) evaluation. Here we\npropose an experimental design to evaluate the potential of XAI in human-AI\ncollaborative settings as well as the potential of XAI for didactics. In a user\nstudy with 1200 participants we investigate the impact of explanations on human\nperformance on a challenging visual task - annotation of biological species in\ncomplex taxonomies. Our results demonstrate the potential of XAI in complex\nvisual annotation tasks: users become more accurate in their annotations and\ndemonstrate less uncertainty with AI assistance. The increase in accuracy was,\nhowever, not significantly different when users were shown the mere prediction\nof the model compared to when also providing an explanation. We also find\nnegative effects of explanations: users tend to replicate the model's\npredictions more often when shown explanations, even when those predictions are\nwrong. When evaluating the didactic effects of explanations in collaborative\nhuman-AI settings, we find that users' annotations are not significantly better\nafter performing annotation with AI assistance. This suggests that explanations\nin visual human-AI collaboration do not appear to induce lasting learning\neffects. All code and experimental data can be found in our GitHub repository:\nhttps://github.com/TeodorChiaburu/beexplainable.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "15 pages, 5 figures, 1 table, presented at ECML 2024, AIMLAI\n  Workshop, Vilnius",
    "pdf_url": "http://arxiv.org/pdf/2409.17157v1",
    "published_date": "2024-09-10 12:59:50 UTC",
    "updated_date": "2024-09-10 12:59:50 UTC"
  },
  {
    "arxiv_id": "2409.06468v1",
    "title": "An Effective Context-Balanced Adaptation Approach for Long-Tailed Speech Recognition",
    "authors": [
      "Yi-Cheng Wang",
      "Li-Ting Pai",
      "Bi-Cheng Yan",
      "Hsin-Wei Wang",
      "Chi-Han Lin",
      "Berlin Chen"
    ],
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have become\nstandard practice for various commercial applications. However, in real-world\nscenarios, the long-tailed nature of word distribution often leads E2E ASR\nmodels to perform well on common words but fall short in recognizing uncommon\nones. Recently, the notion of a contextual adapter (CA) was proposed to infuse\nexternal knowledge represented by a context word list into E2E ASR models.\nAlthough CA can improve recognition performance on rare words, two crucial data\nimbalance problems remain. First, when using low-frequency words as context\nwords during training, since these words rarely occur in the utterance, CA\nbecomes prone to overfit on attending to the <no-context> token due to\nhigher-frequency words not being present in the context list. Second, the\nlong-tailed distribution within the context list itself still causes the model\nto perform poorly on low-frequency context words. In light of this, we explore\nin-depth the impact of altering the context list to have words with different\nfrequency distributions on model performance, and meanwhile extend CA with a\nsimple yet effective context-balanced learning objective. A series of\nexperiments conducted on the AISHELL-1 benchmark dataset suggests that using\nall vocabulary words from the training corpus as the context list and pairing\nthem with our balanced objective yields the best performance, demonstrating a\nsignificant reduction in character error rate (CER) by up to 1.21% and a more\npronounced 9.44% reduction in the error rate of zero-shot words.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.06468v1",
    "published_date": "2024-09-10 12:52:36 UTC",
    "updated_date": "2024-09-10 12:52:36 UTC"
  },
  {
    "arxiv_id": "2409.06450v1",
    "title": "Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles",
    "authors": [
      "Qiujing Lu",
      "Xuanhan Wang",
      "Yiwei Jiang",
      "Guangming Zhao",
      "Mingyue Ma",
      "Shuo Feng"
    ],
    "abstract": "The generation of corner cases has become increasingly crucial for\nefficiently testing autonomous vehicles prior to road deployment. However,\nexisting methods struggle to accommodate diverse testing requirements and often\nlack the ability to generalize to unseen situations, thereby reducing the\nconvenience and usability of the generated scenarios. A method that facilitates\neasily controllable scenario generation for efficient autonomous vehicles (AV)\ntesting with realistic and challenging situations is greatly needed. To address\nthis, we proposed OmniTester: a multimodal Large Language Model (LLM) based\nframework that fully leverages the extensive world knowledge and reasoning\ncapabilities of LLMs. OmniTester is designed to generate realistic and diverse\nscenarios within a simulation environment, offering a robust solution for\ntesting and evaluating AVs. In addition to prompt engineering, we employ tools\nfrom Simulation of Urban Mobility to simplify the complexity of codes generated\nby LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a\nself-improvement mechanism to enhance the LLM's understanding of scenarios,\nthereby increasing its ability to produce more realistic scenes. In the\nexperiments, we demonstrated the controllability and realism of our approaches\nin generating three types of challenging and complex scenarios. Additionally,\nwe showcased its effectiveness in reconstructing new scenarios described in\ncrash report, driven by the generalization capability of LLMs.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06450v1",
    "published_date": "2024-09-10 12:12:09 UTC",
    "updated_date": "2024-09-10 12:12:09 UTC"
  },
  {
    "arxiv_id": "2409.06446v1",
    "title": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training Data",
    "authors": [
      "Hossein Hajipour",
      "Lea Schönherr",
      "Thorsten Holz",
      "Mario Fritz"
    ],
    "abstract": "Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "24 pages, 16 tables, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.06446v1",
    "published_date": "2024-09-10 12:01:43 UTC",
    "updated_date": "2024-09-10 12:01:43 UTC"
  },
  {
    "arxiv_id": "2409.06445v2",
    "title": "Learning Generative Interactive Environments By Trained Agent Exploration",
    "authors": [
      "Naser Kazemi",
      "Nedko Savov",
      "Danda Paudel",
      "Luc Van Gool"
    ],
    "abstract": "World models are increasingly pivotal in interpreting and simulating the\nrules and actions of complex environments. Genie, a recent model, excels at\nlearning from visually diverse environments but relies on costly\nhuman-collected data. We observe that their alternative method of using random\nagents is too limited to explore the environment. We propose to improve the\nmodel by employing reinforcement learning based agents for data generation.\nThis approach produces diverse datasets that enhance the model's ability to\nadapt and perform well across various scenarios and realistic actions within\nthe environment. In this paper, we first release the model GenieRedux - an\nimplementation based on Genie. Additionally, we introduce GenieRedux-G, a\nvariant that uses the agent's readily available actions to factor out action\nprediction uncertainty during validation. Our evaluation, including a\nreplication of the Coinrun case study, shows that GenieRedux-G achieves\nsuperior visual fidelity and controllability using the trained agent\nexploration. The proposed approach is reproducable, scalable and adaptable to\nnew types of environments. Our codebase is available at\nhttps://github.com/insait-institute/GenieRedux .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06445v2",
    "published_date": "2024-09-10 12:00:40 UTC",
    "updated_date": "2024-10-18 17:37:51 UTC"
  },
  {
    "arxiv_id": "2409.06748v1",
    "title": "EasyST: A Simple Framework for Spatio-Temporal Prediction",
    "authors": [
      "Jiabin Tang",
      "Wei Wei",
      "Lianghao Xia",
      "Chao Huang"
    ],
    "abstract": "Spatio-temporal prediction is a crucial research area in data-driven urban\ncomputing, with implications for transportation, public safety, and\nenvironmental monitoring. However, scalability and generalization challenges\nremain significant obstacles. Advanced models often rely on Graph Neural\nNetworks to encode spatial and temporal correlations, but struggle with the\nincreased complexity of large-scale datasets. The recursive GNN-based message\npassing schemes used in these models hinder their training and deployment in\nreal-life urban sensing scenarios. Moreover, long-spanning large-scale\nspatio-temporal data introduce distribution shifts, necessitating improved\ngeneralization performance. To address these challenges, we propose a simple\nframework for spatio-temporal prediction - EasyST paradigm. It learns\nlightweight and robust Multi-Layer Perceptrons (MLPs) by effectively distilling\nknowledge from complex spatio-temporal GNNs. We ensure robust knowledge\ndistillation by integrating the spatio-temporal information bottleneck with\nteacher-bounded regression loss, filtering out task-irrelevant noise and\navoiding erroneous guidance. We further enhance the generalization ability of\nthe student model by incorporating spatial and temporal prompts to provide\ndownstream task contexts. Evaluation on three spatio-temporal datasets for\nurban computing tasks demonstrates that EasyST surpasses state-of-the-art\napproaches in terms of efficiency and accuracy. The implementation code is\navailable at: https://github.com/HKUDS/EasyST.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by CIKM'2024, full paper",
    "pdf_url": "http://arxiv.org/pdf/2409.06748v1",
    "published_date": "2024-09-10 11:40:01 UTC",
    "updated_date": "2024-09-10 11:40:01 UTC"
  },
  {
    "arxiv_id": "2409.06427v1",
    "title": "GeMuCo: Generalized Multisensory Correlational Model for Body Schema Learning",
    "authors": [
      "Kento Kawaharazuka",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "abstract": "Humans can autonomously learn the relationship between sensation and motion\nin their own bodies, estimate and control their own body states, and move while\ncontinuously adapting to the current environment. On the other hand, current\nrobots control their bodies by learning the network structure described by\nhumans from their experiences, making certain assumptions on the relationship\nbetween sensors and actuators. In addition, the network model does not adapt to\nchanges in the robot's body, the tools that are grasped, or the environment,\nand there is no unified theory, not only for control but also for state\nestimation, anomaly detection, simulation, and so on. In this study, we propose\na Generalized Multisensory Correlational Model (GeMuCo), in which the robot\nitself acquires a body schema describing the correlation between sensors and\nactuators from its own experience, including model structures such as network\ninput/output. The robot adapts to the current environment by updating this body\nschema model online, estimates and controls its body state, and even performs\nanomaly detection and simulation. We demonstrate the effectiveness of this\nmethod by applying it to tool-use considering changes in grasping state for an\naxis-driven robot, to joint-muscle mapping learning for a musculoskeletal\nrobot, and to full-body tool manipulation for a low-rigidity plastic-made\nhumanoid.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at IEEE Robotics and Automation Magazine",
    "pdf_url": "http://arxiv.org/pdf/2409.06427v1",
    "published_date": "2024-09-10 11:19:13 UTC",
    "updated_date": "2024-09-10 11:19:13 UTC"
  },
  {
    "arxiv_id": "2409.09079v1",
    "title": "D3-GNN: Dynamic Distributed Dataflow for Streaming Graph Neural Networks",
    "authors": [
      "Rustam Guliyev",
      "Aparajita Haldar",
      "Hakan Ferhatosmanoglu"
    ],
    "abstract": "Graph Neural Network (GNN) models on streaming graphs entail algorithmic\nchallenges to continuously capture its dynamic state, as well as systems\nchallenges to optimize latency, memory, and throughput during both inference\nand training. We present D3-GNN, the first distributed, hybrid-parallel,\nstreaming GNN system designed to handle real-time graph updates under online\nquery setting. Our system addresses data management, algorithmic, and systems\nchallenges, enabling continuous capturing of the dynamic state of the graph and\nupdating node representations with fault-tolerance and optimal latency,\nload-balance, and throughput. D3-GNN utilizes streaming GNN aggregators and an\nunrolled, distributed computation graph architecture to handle cascading graph\nupdates. To counteract data skew and neighborhood explosion issues, we\nintroduce inter-layer and intra-layer windowed forward pass solutions.\nExperiments on large-scale graph streams demonstrate that D3-GNN achieves high\nefficiency and scalability. Compared to DGL, D3-GNN achieves a significant\nthroughput improvement of about 76x for streaming workloads. The windowed\nenhancement further reduces running times by around 10x and message volumes by\nup to 15x at higher parallelism.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "14 pages, 7 figures, published at VLDB'24",
    "pdf_url": "http://arxiv.org/pdf/2409.09079v1",
    "published_date": "2024-09-10 11:00:43 UTC",
    "updated_date": "2024-09-10 11:00:43 UTC"
  },
  {
    "arxiv_id": "2409.06416v1",
    "title": "Exploring the Integration of Large Language Models in Industrial Test Maintenance Processes",
    "authors": [
      "Ludvig Lemner",
      "Linnea Wahlgren",
      "Gregory Gay",
      "Nasser Mohammadiha",
      "Jingxiong Liu",
      "Joakim Wennerberg"
    ],
    "abstract": "Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplored the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also proposed and demonstrated\nimplementations of two multi-agent architectures that can predict which test\ncases require maintenance following a change to the source code. Collectively,\nthese contributions advance our theoretical and practical understanding of how\nLLMs can be deployed to benefit industrial test maintenance processes.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Under submission to ACM TOSEM",
    "pdf_url": "http://arxiv.org/pdf/2409.06416v1",
    "published_date": "2024-09-10 10:55:48 UTC",
    "updated_date": "2024-09-10 10:55:48 UTC"
  },
  {
    "arxiv_id": "2409.06402v2",
    "title": "Symmetry Breaking in Neural Network Optimization: Insights from Input Dimension Expansion",
    "authors": [
      "Jun-Jie Zhang",
      "Nan Cheng",
      "Fu-Peng Li",
      "Xiu-Cheng Wang",
      "Jian-Nan Chen",
      "Long-Gang Pang",
      "Deyu Meng"
    ],
    "abstract": "Understanding the mechanisms behind neural network optimization is crucial\nfor improving network design and performance. While various optimization\ntechniques have been developed, a comprehensive understanding of the underlying\nprinciples that govern these techniques remains elusive. Specifically, the role\nof symmetry breaking, a fundamental concept in physics, has not been fully\nexplored in neural network optimization. This gap in knowledge limits our\nability to design networks that are both efficient and effective. Here, we\npropose the symmetry breaking hypothesis to elucidate the significance of\nsymmetry breaking in enhancing neural network optimization. We demonstrate that\na simple input expansion can significantly improve network performance across\nvarious tasks, and we show that this improvement can be attributed to the\nunderlying symmetry breaking mechanism. We further develop a metric to quantify\nthe degree of symmetry breaking in neural networks, providing a practical\napproach to evaluate and guide network design. Our findings confirm that\nsymmetry breaking is a fundamental principle that underpins various\noptimization techniques, including dropout, batch normalization, and\nequivariance. By quantifying the degree of symmetry breaking, our work offers a\npractical technique for performance enhancement and a metric to guide network\ndesign without the need for complete datasets and extensive training processes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.06402v2",
    "published_date": "2024-09-10 10:36:40 UTC",
    "updated_date": "2024-09-12 10:47:35 UTC"
  },
  {
    "arxiv_id": "2409.06371v1",
    "title": "Distilling Generative-Discriminative Representations for Very Low-Resolution Face Recognition",
    "authors": [
      "Junzheng Zhang",
      "Weijia Guo",
      "Bochao Liu",
      "Ruixin Shi",
      "Yong Li",
      "Shiming Ge"
    ],
    "abstract": "Very low-resolution face recognition is challenging due to the serious loss\nof informative facial details in resolution degradation. In this paper, we\npropose a generative-discriminative representation distillation approach that\ncombines generative representation with cross-resolution aligned knowledge\ndistillation. This approach facilitates very low-resolution face recognition by\njointly distilling generative and discriminative models via two distillation\nmodules. Firstly, the generative representation distillation takes the encoder\nof a diffusion model pretrained for face super-resolution as the generative\nteacher to supervise the learning of the student backbone via feature\nregression, and then freezes the student backbone. After that, the\ndiscriminative representation distillation further considers a pretrained face\nrecognizer as the discriminative teacher to supervise the learning of the\nstudent head via cross-resolution relational contrastive distillation. In this\nway, the general backbone representation can be transformed into discriminative\nhead representation, leading to a robust and discriminative student model for\nvery low-resolution face recognition. Our approach improves the recovery of the\nmissing details in very low-resolution faces and achieves better knowledge\ntransfer. Extensive experiments on face datasets demonstrate that our approach\nenhances the recognition accuracy of very low-resolution faces, showcasing its\neffectiveness and adaptability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06371v1",
    "published_date": "2024-09-10 09:53:06 UTC",
    "updated_date": "2024-09-10 09:53:06 UTC"
  },
  {
    "arxiv_id": "2409.06367v1",
    "title": "Texture-AD: An Anomaly Detection Dataset and Benchmark for Real Algorithm Development",
    "authors": [
      "Tianwu Lei",
      "Bohan Wang",
      "Silin Chen",
      "Shurong Cao",
      "Ningmu Zou"
    ],
    "abstract": "Anomaly detection is a crucial process in industrial manufacturing and has\nmade significant advancements recently. However, there is a large variance\nbetween the data used in the development and the data collected by the\nproduction environment. Therefore, we present the Texture-AD benchmark based on\nrepresentative texture-based anomaly detection to evaluate the effectiveness of\nunsupervised anomaly detection algorithms in real-world applications. This\ndataset includes images of 15 different cloth, 14 semiconductor wafers and 10\nmetal plates acquired under different optical schemes. In addition, it includes\nmore than 10 different types of defects produced during real manufacturing\nprocesses, such as scratches, wrinkles, color variations and point defects,\nwhich are often more difficult to detect than existing datasets. All anomalous\nareas are provided with pixel-level annotations to facilitate comprehensive\nevaluation using anomaly detection models. Specifically, to adapt to diverse\nproducts in automated pipelines, we present a new evaluation method and results\nof baseline algorithms. The experimental results show that Texture-AD is a\ndifficult challenge for state-of-the-art algorithms. To our knowledge,\nTexture-AD is the first dataset to be devoted to evaluating industrial defect\ndetection algorithms in the real world. The dataset is available at\nhttps://XXX.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06367v1",
    "published_date": "2024-09-10 09:44:38 UTC",
    "updated_date": "2024-09-10 09:44:38 UTC"
  },
  {
    "arxiv_id": "2409.06362v1",
    "title": "Connecting Concept Convexity and Human-Machine Alignment in Deep Neural Networks",
    "authors": [
      "Teresa Dorszewski",
      "Lenka Tětková",
      "Lorenz Linhardt",
      "Lars Kai Hansen"
    ],
    "abstract": "Understanding how neural networks align with human cognitive processes is a\ncrucial step toward developing more interpretable and reliable AI systems.\nMotivated by theories of human cognition, this study examines the relationship\nbetween \\emph{convexity} in neural network representations and\n\\emph{human-machine alignment} based on behavioral data. We identify a\ncorrelation between these two dimensions in pretrained and fine-tuned vision\ntransformer models. Our findings suggest that the convex regions formed in\nlatent spaces of neural networks to some extent align with human-defined\ncategories and reflect the similarity relations humans use in cognitive tasks.\nWhile optimizing for alignment generally enhances convexity, increasing\nconvexity through fine-tuning yields inconsistent effects on alignment, which\nsuggests a complex relationship between the two. This study presents a first\nstep toward understanding the relationship between the convexity of latent\nrepresentations and human-machine alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "First two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2409.06362v1",
    "published_date": "2024-09-10 09:32:16 UTC",
    "updated_date": "2024-09-10 09:32:16 UTC"
  },
  {
    "arxiv_id": "2409.06356v2",
    "title": "Double Successive Over-Relaxation Q-Learning with an Extension to Deep Reinforcement Learning",
    "authors": [
      "Shreyas S R"
    ],
    "abstract": "Q-learning is a widely used algorithm in reinforcement learning (RL), but its\nconvergence can be slow, especially when the discount factor is close to one.\nSuccessive Over-Relaxation (SOR) Q-learning, which introduces a relaxation\nfactor to speed up convergence, addresses this issue but has two major\nlimitations: In the tabular setting, the relaxation parameter depends on\ntransition probability, making it not entirely model-free, and it suffers from\noverestimation bias. To overcome these limitations, we propose a sample-based,\nmodel-free double SOR Q-learning algorithm. Theoretically and empirically, this\nalgorithm is shown to be less biased than SOR Q-learning. Further, in the\ntabular setting, the convergence analysis under boundedness assumptions on\niterates is discussed. The proposed algorithm is extended to large-scale\nproblems using deep RL. Finally, the tabular version of the proposed algorithm\nis compared using roulette and grid world environments, while the deep RL\nversion is tested on a maximization bias example and OpenAI Gym environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06356v2",
    "published_date": "2024-09-10 09:23:03 UTC",
    "updated_date": "2025-05-15 15:16:33 UTC"
  },
  {
    "arxiv_id": "2409.06351v1",
    "title": "MAGDA: Multi-agent guideline-driven diagnostic assistance",
    "authors": [
      "David Bani-Harouni",
      "Nassir Navab",
      "Matthias Keicher"
    ],
    "abstract": "In emergency departments, rural hospitals, or clinics in less developed\nregions, clinicians often lack fast image analysis by trained radiologists,\nwhich can have a detrimental effect on patients' healthcare. Large Language\nModels (LLMs) have the potential to alleviate some pressure from these\nclinicians by providing insights that can help them in their decision-making.\nWhile these LLMs achieve high test results on medical exams showcasing their\ngreat theoretical medical knowledge, they tend not to follow medical\nguidelines. In this work, we introduce a new approach for zero-shot\nguideline-driven decision support. We model a system of multiple LLM agents\naugmented with a contrastive vision-language model that collaborate to reach a\npatient diagnosis. After providing the agents with simple diagnostic\nguidelines, they will synthesize prompts and screen the image for findings\nfollowing these guidelines. Finally, they provide understandable\nchain-of-thought reasoning for their diagnosis, which is then self-refined to\nconsider inter-dependencies between diseases. As our method is zero-shot, it is\nadaptable to settings with rare diseases, where training data is limited, but\nexpert-crafted disease descriptions are available. We evaluate our method on\ntwo chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing\nperformance improvement over existing zero-shot methods and generalizability to\nrare diseases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06351v1",
    "published_date": "2024-09-10 09:10:30 UTC",
    "updated_date": "2024-09-10 09:10:30 UTC"
  },
  {
    "arxiv_id": "2409.06348v1",
    "title": "VoiceWukong: Benchmarking Deepfake Voice Detection",
    "authors": [
      "Ziwei Yan",
      "Yanjie Zhao",
      "Haoyu Wang"
    ],
    "abstract": "With the rapid advancement of technologies like text-to-speech (TTS) and\nvoice conversion (VC), detecting deepfake voices has become increasingly\ncrucial. However, both academia and industry lack a comprehensive and intuitive\nbenchmark for evaluating detectors. Existing datasets are limited in language\ndiversity and lack many manipulations encountered in real-world production\nenvironments.\n  To fill this gap, we propose VoiceWukong, a benchmark designed to evaluate\nthe performance of deepfake voice detectors. To build the dataset, we first\ncollected deepfake voices generated by 19 advanced and widely recognized\ncommercial tools and 15 open-source tools. We then created 38 data variants\ncovering six types of manipulations, constructing the evaluation dataset for\ndeepfake voice detection. VoiceWukong thus includes 265,200 English and 148,200\nChinese deepfake voice samples. Using VoiceWukong, we evaluated 12\nstate-of-the-art detectors. AASIST2 achieved the best equal error rate (EER) of\n13.50%, while all others exceeded 20%. Our findings reveal that these detectors\nface significant challenges in real-world applications, with dramatically\ndeclining performance. In addition, we conducted a user study with more than\n300 participants. The results are compared with the performance of the 12\ndetectors and a multimodel large language model (MLLM), i.e., Qwen2-Audio,\nwhere different detectors and humans exhibit varying identification\ncapabilities for deepfake voices at different deception levels, while the LALM\ndemonstrates no detection ability at all. Furthermore, we provide a leaderboard\nfor deepfake voice detection, publicly available at\n{https://voicewukong.github.io}.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06348v1",
    "published_date": "2024-09-10 09:07:12 UTC",
    "updated_date": "2024-09-10 09:07:12 UTC"
  },
  {
    "arxiv_id": "2409.06343v2",
    "title": "Compute-Update Federated Learning: A Lattice Coding Approach Over-the-Air",
    "authors": [
      "Seyed Mohammad Azimi-Abarghouyi",
      "Lav R. Varshney"
    ],
    "abstract": "This paper introduces a federated learning framework that enables\nover-the-air computation via digital communications, using a new joint\nsource-channel coding scheme. Without relying on channel state information at\ndevices, this scheme employs lattice codes to both quantize model parameters\nand exploit interference from the devices. We propose a novel receiver\nstructure at the server, designed to reliably decode an integer combination of\nthe quantized model parameters as a lattice point for the purpose of\naggregation. We present a mathematical approach to derive a convergence bound\nfor the proposed scheme and offer design remarks. In this context, we suggest\nan aggregation metric and a corresponding algorithm to determine effective\ninteger coefficients for the aggregation in each communication round. Our\nresults illustrate that, regardless of channel dynamics and data heterogeneity,\nour scheme consistently delivers superior learning accuracy across various\nparameters and markedly surpasses other over-the-air methodologies.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "Extended version of the preprint available at arXiv:2403.01023",
    "pdf_url": "http://arxiv.org/pdf/2409.06343v2",
    "published_date": "2024-09-10 08:52:24 UTC",
    "updated_date": "2024-11-05 21:17:46 UTC"
  },
  {
    "arxiv_id": "2409.06336v3",
    "title": "Towards Agentic AI on Particle Accelerators",
    "authors": [
      "Antonin Sulc",
      "Thorsten Hellert",
      "Raimund Kammering",
      "Hayden Hoschouer",
      "Jason St. John"
    ],
    "abstract": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized to control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show three examples, where we demonstrate the viability of such\narchitecture.",
    "categories": [
      "physics.acc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.acc-ph",
    "comment": "5 pages, 3 figures, Machine Learning and the Physical Sciences at\n  Workshop at the 38th conference on Neural Information Processing Systems\n  (NeurIPS)",
    "pdf_url": "http://arxiv.org/pdf/2409.06336v3",
    "published_date": "2024-09-10 08:47:23 UTC",
    "updated_date": "2024-12-22 09:30:47 UTC"
  },
  {
    "arxiv_id": "2409.06323v1",
    "title": "LAMP: Learnable Meta-Path Guided Adversarial Contrastive Learning for Heterogeneous Graphs",
    "authors": [
      "Siqing Li",
      "Jin-Duk Park",
      "Wei Huang",
      "Xin Cao",
      "Won-Yong Shin",
      "Zhiqiang Xu"
    ],
    "abstract": "Heterogeneous graph neural networks (HGNNs) have significantly propelled the\ninformation retrieval (IR) field. Still, the effectiveness of HGNNs heavily\nrelies on high-quality labels, which are often expensive to acquire. This\nchallenge has shifted attention towards Heterogeneous Graph Contrastive\nLearning (HGCL), which usually requires pre-defined meta-paths. However, our\nfindings reveal that meta-path combinations significantly affect performance in\nunsupervised settings, an aspect often overlooked in current literature.\nExisting HGCL methods have considerable variability in outcomes across\ndifferent meta-path combinations, thereby challenging the optimization process\nto achieve consistent and high performance. In response, we introduce\n\\textsf{LAMP} (\\underline{\\textbf{L}}earn\\underline{\\textbf{A}}ble\n\\underline{\\textbf{M}}eta-\\underline{\\textbf{P}}ath), a novel adversarial\ncontrastive learning approach that integrates various meta-path sub-graphs into\na unified and stable structure, leveraging the overlap among these sub-graphs.\nTo address the denseness of this integrated sub-graph, we propose an\nadversarial training strategy for edge pruning, maintaining sparsity to enhance\nmodel performance and robustness. \\textsf{LAMP} aims to maximize the difference\nbetween meta-path and network schema views for guiding contrastive learning to\ncapture the most meaningful information. Our extensive experimental study\nconducted on four diverse datasets from the Heterogeneous Graph Benchmark (HGB)\ndemonstrates that \\textsf{LAMP} significantly outperforms existing\nstate-of-the-art unsupervised models in terms of accuracy and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.06323v1",
    "published_date": "2024-09-10 08:27:39 UTC",
    "updated_date": "2024-09-10 08:27:39 UTC"
  },
  {
    "arxiv_id": "2409.06316v2",
    "title": "PharmacoMatch: Efficient 3D Pharmacophore Screening via Neural Subgraph Matching",
    "authors": [
      "Daniel Rose",
      "Oliver Wieder",
      "Thomas Seidel",
      "Thierry Langer"
    ],
    "abstract": "The increasing size of screening libraries poses a significant challenge for\nthe development of virtual screening methods for drug discovery, necessitating\na re-evaluation of traditional approaches in the era of big data. Although 3D\npharmacophore screening remains a prevalent technique, its application to very\nlarge datasets is limited by the computational cost associated with matching\nquery pharmacophores to database molecules. In this study, we introduce\nPharmacoMatch, a novel contrastive learning approach based on neural subgraph\nmatching. Our method reinterprets pharmacophore screening as an approximate\nsubgraph matching problem and enables efficient querying of conformational\ndatabases by encoding query-target relationships in the embedding space. We\nconduct comprehensive investigations of the learned representations and\nevaluate PharmacoMatch as pre-screening tool in a zero-shot setting. We\ndemonstrate significantly shorter runtimes and comparable performance metrics\nto existing solutions, providing a promising speed-up for screening very large\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06316v2",
    "published_date": "2024-09-10 08:17:06 UTC",
    "updated_date": "2025-03-14 09:51:43 UTC"
  },
  {
    "arxiv_id": "2409.06307v1",
    "title": "An End-to-End Approach for Chord-Conditioned Song Generation",
    "authors": [
      "Shuochen Gao",
      "Shun Lei",
      "Fan Zhuo",
      "Hangyu Liu",
      "Feng Liu",
      "Boshi Tang",
      "Qiaochu Huang",
      "Shiyin Kang",
      "Zhiyong Wu"
    ],
    "abstract": "The Song Generation task aims to synthesize music composed of vocals and\naccompaniment from given lyrics. While the existing method, Jukebox, has\nexplored this task, its constrained control over the generations often leads to\ndeficiency in music performance. To mitigate the issue, we introduce an\nimportant concept from music composition, namely chords, to song generation\nnetworks. Chords form the foundation of accompaniment and provide vocal melody\nwith associated harmony. Given the inaccuracy of automatic chord extractors, we\ndevise a robust cross-attention mechanism augmented with dynamic weight\nsequence to integrate extracted chord information into song generations and\nreduce frame-level flaws, and propose a novel model termed Chord-Conditioned\nSong Generator (CSG) based on it. Experimental evidence demonstrates our\nproposed method outperforms other approaches in terms of musical performance\nand control precision of generated songs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06307v1",
    "published_date": "2024-09-10 08:07:43 UTC",
    "updated_date": "2024-09-10 08:07:43 UTC"
  },
  {
    "arxiv_id": "2409.06299v1",
    "title": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory",
    "authors": [
      "Dingxin Cheng",
      "Mingda Li",
      "Jingyu Liu",
      "Yongxin Guo",
      "Bin Jiang",
      "Qingbin Liu",
      "Xi Chen",
      "Bo Zhao"
    ],
    "abstract": "Recently, integrating visual foundation models into large language models\n(LLMs) to form video understanding systems has attracted widespread attention.\nMost of the existing models compress diverse semantic information within the\nwhole video and feed it into LLMs for content comprehension. While this method\nexcels in short video understanding, it may result in a blend of multiple event\ninformation in long videos due to coarse compression, which causes information\nredundancy. Consequently, the semantics of key events might be obscured within\nthe vast information that hinders the model's understanding capabilities. To\naddress this issue, we propose a Hierarchical Event-based Memory-enhanced LLM\n(HEM-LLM) for better understanding of long videos. Firstly, we design a novel\nadaptive sequence segmentation scheme to divide multiple events within long\nvideos. In this way, we can perform individual memory modeling for each event\nto establish intra-event contextual connections, thereby reducing information\nredundancy. Secondly, while modeling current event, we compress and inject the\ninformation of the previous event to enhance the long-term inter-event\ndependencies in videos. Finally, we perform extensive experiments on various\nvideo understanding tasks and the results show that our model achieves\nstate-of-the-art performances.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06299v1",
    "published_date": "2024-09-10 07:53:10 UTC",
    "updated_date": "2024-09-10 07:53:10 UTC"
  },
  {
    "arxiv_id": "2409.06280v2",
    "title": "Catch Me if You Can: Detecting Unauthorized Data Use in Deep Learning Models",
    "authors": [
      "Zitao Chen",
      "Karthik Pattabiraman"
    ],
    "abstract": "The rise of deep learning (DL) has led to a surging demand for training data,\nwhich incentivizes the creators of DL models to trawl through the Internet for\ntraining materials. Meanwhile, users often have limited control over whether\ntheir data (e.g., facial images) are used to train DL models without their\nconsent, which has engendered pressing concerns.\n  This work proposes MembershipTracker, a practical data auditing tool that can\nempower ordinary users to reliably detect the unauthorized use of their data in\ntraining DL models. We view data auditing through the lens of membership\ninference (MI). MembershipTracker consists of a lightweight data marking\ncomponent to mark the target data with small and targeted changes, which can be\nstrongly memorized by the model trained on them; and a specialized MI-based\nverification process to audit whether the model exhibits strong memorization on\nthe target samples.\n  MembershipTracker only requires the users to mark a small fraction of data\n(0.005% to 0.1% in proportion to the training set), and it enables the users to\nreliably detect the unauthorized use of their data (average 0% FPR@100% TPR).\nWe show that MembershipTracker is highly effective across various settings,\nincluding industry-scale training on the full-size ImageNet-1k dataset. We\nfinally evaluate MembershipTracker under multiple classes of countermeasures.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06280v2",
    "published_date": "2024-09-10 07:31:56 UTC",
    "updated_date": "2024-12-23 02:44:47 UTC"
  },
  {
    "arxiv_id": "2409.06277v2",
    "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models",
    "authors": [
      "Yao Shu",
      "Wenyang Hu",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low",
      "Fei Richard Yu"
    ],
    "abstract": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06277v2",
    "published_date": "2024-09-10 07:28:13 UTC",
    "updated_date": "2024-09-11 01:47:48 UTC"
  },
  {
    "arxiv_id": "2409.06270v1",
    "title": "Towards Robust Uncertainty-Aware Incomplete Multi-View Classification",
    "authors": [
      "Mulin Chen",
      "Haojian Huang",
      "Qiang Li"
    ],
    "abstract": "Handling incomplete data in multi-view classification is challenging,\nespecially when traditional imputation methods introduce biases that compromise\nuncertainty estimation. Existing Evidential Deep Learning (EDL) based\napproaches attempt to address these issues, but they often struggle with\nconflicting evidence due to the limitations of the Dempster-Shafer combination\nrule, leading to unreliable decisions. To address these challenges, we propose\nthe Alternating Progressive Learning Network (APLN), specifically designed to\nenhance EDL-based methods in incomplete MVC scenarios. Our approach mitigates\nbias from corrupted observed data by first applying coarse imputation, followed\nby mapping the data to a latent space. In this latent space, we progressively\nlearn an evidence distribution aligned with the target domain, incorporating\nuncertainty considerations through EDL. Additionally, we introduce a\nconflict-aware Dempster-Shafer combination rule (DSCR) to better handle\nconflicting evidence. By sampling from the learned distribution, we optimize\nthe latent representations of missing views, reducing bias and enhancing\ndecision-making robustness. Extensive experiments demonstrate that APLN,\ncombined with DSCR, significantly outperforms traditional methods, particularly\nin environments characterized by high uncertainty and conflicting evidence,\nestablishing it as a promising solution for incomplete multi-view\nclassification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Ongoing work: 9 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.06270v1",
    "published_date": "2024-09-10 07:18:57 UTC",
    "updated_date": "2024-09-10 07:18:57 UTC"
  },
  {
    "arxiv_id": "2409.06263v1",
    "title": "Keyword-Aware ASR Error Augmentation for Robust Dialogue State Tracking",
    "authors": [
      "Jihyun Lee",
      "Solee Im",
      "Wonjun Lee",
      "Gary Geunbae Lee"
    ],
    "abstract": "Dialogue State Tracking (DST) is a key part of task-oriented dialogue\nsystems, identifying important information in conversations. However, its\naccuracy drops significantly in spoken dialogue environments due to named\nentity errors from Automatic Speech Recognition (ASR) systems. We introduce a\nsimple yet effective data augmentation method that targets those entities to\nimprove the robustness of DST model. Our novel method can control the placement\nof errors using keyword-highlighted prompts while introducing phonetically\nsimilar errors. As a result, our method generated sufficient error patterns on\nkeywords, leading to improved accuracy in noised and low-accuracy ASR\nenvironments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06263v1",
    "published_date": "2024-09-10 07:06:40 UTC",
    "updated_date": "2024-09-10 07:06:40 UTC"
  },
  {
    "arxiv_id": "2409.06745v1",
    "title": "Personalized Knowledge Tracing through Student Representation Reconstruction and Class Imbalance Mitigation",
    "authors": [
      "Zhiyu Chen",
      "Wei Ji",
      "Jing Xiao",
      "Zitao Liu"
    ],
    "abstract": "Knowledge tracing is a technique that predicts students' future performance\nby analyzing their learning process through historical interactions with\nintelligent educational platforms, enabling a precise evaluation of their\nknowledge mastery. Recent studies have achieved significant progress by\nleveraging powerful deep neural networks. These models construct complex input\nrepresentations using questions, skills, and other auxiliary information but\noverlook individual student characteristics, which limits the capability for\npersonalized assessment. Additionally, the available datasets in the field\nexhibit class imbalance issues. The models that simply predict all responses as\ncorrect without substantial effort can yield impressive accuracy. In this\npaper, we propose PKT, a novel approach for personalized knowledge tracing. PKT\nreconstructs representations from sequences of interactions with a tutoring\nplatform to capture latent information about the students. Moreover, PKT\nincorporates focal loss to improve prioritize minority classes, thereby\nachieving more balanced predictions. Extensive experimental results on four\npublicly available educational datasets demonstrate the advanced predictive\nperformance of PKT in comparison with 16 state-of-the-art models. To ensure the\nreproducibility of our research, the code is publicly available at\nhttps://anonymous.4open.science/r/PKT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06745v1",
    "published_date": "2024-09-10 07:02:46 UTC",
    "updated_date": "2024-09-10 07:02:46 UTC"
  },
  {
    "arxiv_id": "2409.06744v2",
    "title": "ProteinBench: A Holistic Evaluation of Protein Foundation Models",
    "authors": [
      "Fei Ye",
      "Zaixiang Zheng",
      "Dongyu Xue",
      "Yuning Shen",
      "Lihao Wang",
      "Yiming Ma",
      "Yan Wang",
      "Xinyou Wang",
      "Xiangxin Zhou",
      "Quanquan Gu"
    ],
    "abstract": "Recent years have witnessed a surge in the development of protein foundation\nmodels, significantly improving performance in protein prediction and\ngenerative tasks ranging from 3D structure prediction and protein design to\nconformational dynamics. However, the capabilities and limitations associated\nwith these models remain poorly understood due to the absence of a unified\nevaluation framework. To fill this gap, we introduce ProteinBench, a holistic\nevaluation framework designed to enhance the transparency of protein foundation\nmodels. Our approach consists of three key components: (i) A taxonomic\nclassification of tasks that broadly encompass the main challenges in the\nprotein domain, based on the relationships between different protein\nmodalities; (ii) A multi-metric evaluation approach that assesses performance\nacross four key dimensions: quality, novelty, diversity, and robustness; and\n(iii) In-depth analyses from various user objectives, providing a holistic view\nof model performance. Our comprehensive evaluation of protein foundation models\nreveals several key findings that shed light on their current capabilities and\nlimitations. To promote transparency and facilitate further research, we\nrelease the evaluation dataset, code, and a public leaderboard publicly for\nfurther analysis and a general modular toolkit. We intend for ProteinBench to\nbe a living benchmark for establishing a standardized, in-depth evaluation\nframework for protein foundation models, driving their development and\napplication while fostering collaboration within the field.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "q-bio.QM",
    "comment": "30 pages, 2 figures and 15 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.06744v2",
    "published_date": "2024-09-10 06:52:33 UTC",
    "updated_date": "2024-10-07 08:20:32 UTC"
  },
  {
    "arxiv_id": "2409.06241v1",
    "title": "DiPT: Enhancing LLM reasoning through diversified perspective-taking",
    "authors": [
      "Hoang Anh Just",
      "Mahavir Dabas",
      "Lifu Huang",
      "Ming Jin",
      "Ruoxi Jia"
    ],
    "abstract": "Existing work on improving language model reasoning typically explores a\nsingle solution path, which can be prone to errors. Inspired by\nperspective-taking in social studies, this paper introduces DiPT, a novel\napproach that complements current reasoning methods by explicitly incorporating\ndiversified viewpoints. This approach allows the model to gain a deeper\nunderstanding of the problem's context and identify the most effective solution\npath during the inference stage. Additionally, it provides a general\ndata-centric AI recipe for augmenting existing data to improve their quality\nfor fine-tuning.\n  Our empirical results demonstrate that DiPT can be flexibly integrated into\nexisting methods that focus on a single reasoning approach, enhancing their\nreasoning performance and stability when presented with paraphrased problems.\nFurthermore, we illustrate improved context understanding by maintaining the\nmodel's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning\nwith data enriched with diverse perspectives can boost the reasoning\ncapabilities of the model compared to fine-tuning with raw data alone.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "LLM Reasoning with Perspectives, Preprint",
    "pdf_url": "http://arxiv.org/pdf/2409.06241v1",
    "published_date": "2024-09-10 06:17:27 UTC",
    "updated_date": "2024-09-10 06:17:27 UTC"
  },
  {
    "arxiv_id": "2409.13733v1",
    "title": "RNR: Teaching Large Language Models to Follow Roles and Rules",
    "authors": [
      "Kuan Wang",
      "Alexander Bukharin",
      "Haoming Jiang",
      "Qingyu Yin",
      "Zhengyang Wang",
      "Tuo Zhao",
      "Jingbo Shang",
      "Chao Zhang",
      "Bing Yin",
      "Xian Li",
      "Jianshu Chen",
      "Shiyang Li"
    ],
    "abstract": "Instruction fine-tuning (IFT) elicits instruction following capabilities and\nsteers the behavior of large language models (LLMs) via supervised learning.\nHowever, existing models trained on open-source IFT datasets only have the\nability to follow instructions from users, and often fail to follow complex\nrole and rules specified by developers, a.k.a. system prompts. The ability to\nfollow these roles and rules is essential for deployment, as it ensures that\nthe model safely interacts with users within developer defined guidelines. To\nimprove such role and rule following ability, we propose \\model, an automated\ndata generation pipeline that generates diverse roles and rules from existing\nIFT instructions, along with corresponding responses. This data can then be\nused to train models that follow complex system prompts. The models are\nevaluated on our newly created benchmarks for role and rule following ability,\nas well as standard instruction-following benchmarks and general NLP tasks. Our\nframework significantly improves role and rule following capability in LLMs, as\nevidenced by over 25% increase in pass-rate on rule adherence, i.e. following\nall requirements, in our experiments with the Alpaca and Ultrachat datasets.\nMoreover, our models achieves this increase without any regression on popular\ninstruction following benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13733v1",
    "published_date": "2024-09-10 06:07:32 UTC",
    "updated_date": "2024-09-10 06:07:32 UTC"
  },
  {
    "arxiv_id": "2409.06220v2",
    "title": "CerviXpert: A Multi-Structural Convolutional Neural Network for Predicting Cervix Type and Cervical Cell Abnormalities",
    "authors": [
      "Rashik Shahriar Akash",
      "Radiful Islam",
      "S. M. Saiful Islam Badhon",
      "K. S. M. Tozammel Hossain"
    ],
    "abstract": "Cervical cancer is a major cause of cancer-related mortality among women\nworldwide, and its survival rate improves significantly with early detection.\nTraditional diagnostic methods such as Pap smears and cervical biopsies rely\nheavily on cytologist expertise, making the process prone to human error. This\nstudy introduces CerviXpert, a multi-structural convolutional neural network\nmodel designed to efficiently classify cervix types and detect cervical cell\nabnormalities. CerviXpert is built as a computationally efficient model that\nclassifies cervical cancer using images from the publicly available SiPaKMeD\ndataset. The model architecture emphasizes simplicity, using a limited number\nof convolutional layers followed by max pooling and dense layers, trained from\nscratch.\n  We assessed the performance of CerviXpert against other state of the art\nconvolutional neural network models including ResNet50, VGG16, MobileNetV2, and\nInceptionV3, evaluating them on accuracy, computational efficiency, and\nrobustness using five fold cross validation. CerviXpert achieved an accuracy of\n98.04 percent in classifying cervical cell abnormalities into three classes and\n98.60 percent for five class cervix type classification, outperforming\nMobileNetV2 and InceptionV3 in both accuracy and computational requirements. It\nshowed comparable results to ResNet50 and VGG16 while reducing computational\ncomplexity and resource needs.\n  CerviXpert provides an effective solution for cervical cancer screening and\ndiagnosis, balancing accuracy with computational efficiency. Its streamlined\ndesign enables deployment in resource constrained environments, potentially\nenhancing early detection and management of cervical cancer.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "11 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.06220v2",
    "published_date": "2024-09-10 05:08:26 UTC",
    "updated_date": "2024-11-18 05:00:58 UTC"
  },
  {
    "arxiv_id": "2409.06214v4",
    "title": "Towards Generalizable Scene Change Detection",
    "authors": [
      "Jaewoo Kim",
      "Uehwan Kim"
    ],
    "abstract": "While current state-of-the-art Scene Change Detection (SCD) approaches\nachieve impressive results in well-trained research data, they become\nunreliable under unseen environments and different temporal conditions;\nin-domain performance drops from 77.6% to 8.0% in a previously unseen\nenvironment and to 4.6% under a different temporal condition -- calling for\ngeneralizable SCD and benchmark. In this work, we propose the Generalizable\nScene Change Detection Framework (GeSCF), which addresses unseen domain\nperformance and temporal consistency -- to meet the growing demand for anything\nSCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a\nzero-shot manner. For this, we design Initial Pseudo-mask Generation and\nGeometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and\nsingle-image based segmentation into scene change detection for a pair of\ninputs without guidance. Furthermore, we define the Generalizable Scene Change\nDetection (GeSCD) benchmark along with novel metrics and an evaluation protocol\nto facilitate SCD research in generalizability. In the process, we introduce\nthe ChangeVPR dataset, a collection of challenging image pairs with diverse\nenvironmental scenarios -- including urban, suburban, and rural settings.\nExtensive experiments across various datasets demonstrate that GeSCF achieves\nan average performance gain of 19.2% on existing SCD datasets and 30.0% on the\nChangeVPR dataset, nearly doubling the prior art performance. We believe our\nwork can lay a solid foundation for robust and generalizable SCD research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Camera-ready version. Accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.06214v4",
    "published_date": "2024-09-10 04:45:25 UTC",
    "updated_date": "2025-03-13 13:55:30 UTC"
  },
  {
    "arxiv_id": "2409.06209v1",
    "title": "Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis",
    "authors": [
      "Xin Zhang",
      "Deval Mehta",
      "Yanan Hu",
      "Chao Zhu",
      "David Darby",
      "Zhen Yu",
      "Daniel Merlo",
      "Melissa Gresle",
      "Anneke Van Der Walt",
      "Helmut Butzkueven",
      "Zongyuan Ge"
    ],
    "abstract": "Survival analysis holds a crucial role across diverse disciplines, such as\neconomics, engineering and healthcare. It empowers researchers to analyze both\ntime-invariant and time-varying data, encompassing phenomena like customer\nchurn, material degradation and various medical outcomes. Given the complexity\nand heterogeneity of such data, recent endeavors have demonstrated successful\nintegration of deep learning methodologies to address limitations in\nconventional statistical approaches. However, current methods typically involve\ncluttered probability distribution function (PDF), have lower sensitivity in\ncensoring prediction, only model static datasets, or only rely on recurrent\nneural networks for dynamic modelling. In this paper, we propose a novel\nsurvival regression method capable of producing high-quality unimodal PDFs\nwithout any prior distribution assumption, by optimizing novel\nMargin-Mean-Variance loss and leveraging the flexibility of Transformer to\nhandle both temporal and non-temporal data, coined UniSurv. Extensive\nexperiments on several datasets demonstrate that UniSurv places a significantly\nhigher emphasis on censoring compared to other methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06209v1",
    "published_date": "2024-09-10 04:29:59 UTC",
    "updated_date": "2024-09-10 04:29:59 UTC"
  },
  {
    "arxiv_id": "2409.06192v1",
    "title": "NOVI : Chatbot System for University Novice with BERT and LLMs",
    "authors": [
      "Yoonji Nam",
      "TaeWoong Seo",
      "Gyeongcheol Shin",
      "Sangji Lee",
      "JaeEun Im"
    ],
    "abstract": "To mitigate the difficulties of university freshmen in adapting to university\nlife, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes\npost and comment data from SKKU 'Everytime', a university community site.\nDeveloped using LangChain, NOVI's performance has been evaluated with a BLEU\nscore, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR\nscore. This approach is not only limited to help university freshmen but is\nalso expected to help various people adapting to new environments with\ndifferent data. This research explores the development and potential\napplication of new educational technology tools, contributing to easier social\nadaptation for beginners and settling a foundation for future advancement in\nLLM studies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06192v1",
    "published_date": "2024-09-10 03:43:26 UTC",
    "updated_date": "2024-09-10 03:43:26 UTC"
  },
  {
    "arxiv_id": "2409.06185v1",
    "title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
    "authors": [
      "Sandeep Kumar",
      "Tirthankar Ghosal",
      "Vinayak Goyal",
      "Asif Ekbal"
    ],
    "abstract": "\"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 12 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.06185v1",
    "published_date": "2024-09-10 03:26:42 UTC",
    "updated_date": "2024-09-10 03:26:42 UTC"
  },
  {
    "arxiv_id": "2409.06173v3",
    "title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks",
    "authors": [
      "Georgios Chochlakis",
      "Niyantha Maruthu Pandiyan",
      "Kristina Lerman",
      "Shrikanth Narayanan"
    ],
    "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 2 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2403.17125",
    "pdf_url": "http://arxiv.org/pdf/2409.06173v3",
    "published_date": "2024-09-10 03:06:17 UTC",
    "updated_date": "2024-10-17 17:02:02 UTC"
  },
  {
    "arxiv_id": "2409.06741v2",
    "title": "Generative AI for Requirements Engineering: A Systematic Literature Review",
    "authors": [
      "Haowei Cheng",
      "Jati H. Husen",
      "Yijun Lu",
      "Teeradaj Racharak",
      "Nobukazu Yoshioka",
      "Naoyasu Ubayashi",
      "Hironori Washizaki"
    ],
    "abstract": "Context: Requirements engineering (RE) faces mounting challenges in handling\nincreasingly complex software systems. The emergence of generative AI (GenAI)\noffers new opportunities and challenges in RE. Objective: This systematic\nliterature review aims to analyze and synthesize current research on GenAI\napplications in RE, focusing on identifying research trends, methodologies,\nchallenges, and future directions. Method: We conducted a comprehensive review\nof 105 articles published between 2019 and 2024 obtained from major academic\ndatabases, using a systematic methodology for paper selection, data extraction,\nand feature analysis. Results: Analysis revealed the following. (1) While GPT\nseries models dominate current applications by 67.3% of studies, the existing\narchitectures face technical challenges-interpretability (61.9%),\nreproducibility (52.4%), and controllability (47.6%), which demonstrate strong\ncorrelations (>35% co-occurrence). (2) Reproducibility is identified as a major\nconcern by 52.4% of studies, which highlights challenges in achieving\nconsistent results due to the stochastic nature and parameter sensitivity of\nGenAI. (3) Governance-related issues (e.g., ethics and security) form a\ndistinct cluster of challenges that requires coordinated solutions, yet they\nare addressed by less than 20% of studies. Conclusions: While GenAI exhibits\npotential in RE, our findings reveal critical issues: (1) the high correlations\namong interpretability, reproducibility, and controllability imply the\nrequirement for more specialized architectures that target interdependencies of\nthese attributes. (2) The widespread concern about result consistency and\nreproducibility demands standardized evaluation frameworks. (3) The emergence\nof challenges related to interconnected governance demands comprehensive\ngovernance structures.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06741v2",
    "published_date": "2024-09-10 02:44:39 UTC",
    "updated_date": "2025-01-23 11:12:26 UTC"
  },
  {
    "arxiv_id": "2409.06163v1",
    "title": "MCDGLN: Masked Connection-based Dynamic Graph Learning Network for Autism Spectrum Disorder",
    "authors": [
      "Peng Wang",
      "Xin Wen",
      "Ruochen Cao",
      "Chengxin Gao",
      "Yanrong Hao",
      "Rui Cao"
    ],
    "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized\nby complex physiological processes. Previous research has predominantly focused\non static cerebral interactions, often neglecting the brain's dynamic nature\nand the challenges posed by network noise. To address these gaps, we introduce\nthe Masked Connection-based Dynamic Graph Learning Network (MCDGLN). Our\napproach first segments BOLD signals using sliding temporal windows to capture\ndynamic brain characteristics. We then employ a specialized weighted edge\naggregation (WEA) module, which uses the cross convolution with channel-wise\nelement-wise convolutional kernel, to integrate dynamic functional connectivity\nand to isolating task-relevant connections. This is followed by topological\nfeature extraction via a hierarchical graph convolutional network (HGCN), with\nkey attributes highlighted by a self-attention module. Crucially, we refine\nstatic functional connections using a customized task-specific mask, reducing\nnoise and pruning irrelevant links. The attention-based connection encoder\n(ACE) then enhances critical connections and compresses static features. The\ncombined features are subsequently used for classification. Applied to the\nAutism Brain Imaging Data Exchange I (ABIDE I) dataset, our framework achieves\na 73.3\\% classification accuracy between ASD and Typical Control (TC) groups\namong 1,035 subjects. The pivotal roles of WEA and ACE in refining connectivity\nand enhancing classification accuracy underscore their importance in capturing\nASD-specific features, offering new insights into the disorder.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.06163v1",
    "published_date": "2024-09-10 02:21:29 UTC",
    "updated_date": "2024-09-10 02:21:29 UTC"
  },
  {
    "arxiv_id": "2409.13731v3",
    "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation",
    "authors": [
      "Lei Liang",
      "Mengshu Sun",
      "Zhengke Gui",
      "Zhongshu Zhu",
      "Zhouyu Jiang",
      "Ling Zhong",
      "Yuan Qu",
      "Peilong Zhao",
      "Zhongpu Bo",
      "Jin Yang",
      "Huaidong Xiong",
      "Lin Yuan",
      "Jun Xu",
      "Zaoyang Wang",
      "Zhiqiang Zhang",
      "Wen Zhang",
      "Huajun Chen",
      "Wenguang Chen",
      "Jun Zhou"
    ],
    "abstract": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.13731v3",
    "published_date": "2024-09-10 02:00:28 UTC",
    "updated_date": "2024-09-26 16:34:35 UTC"
  },
  {
    "arxiv_id": "2409.06147v1",
    "title": "Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings",
    "authors": [
      "Dong Han",
      "Jihye Moon",
      "Luís Roberto Mercado Díaz",
      "Darren Chen",
      "Devan Williams",
      "Eric Y. Ding",
      "Khanh-Van Tran",
      "David D. McManus",
      "Ki H. Chon"
    ],
    "abstract": "Most deep learning models of multiclass arrhythmia classification are tested\non fingertip photoplethysmographic (PPG) data, which has higher signal-to-noise\nratios compared to smartwatch-derived PPG, and the best reported sensitivity\nvalue for premature atrial/ventricular contraction (PAC/PVC) detection is only\n75%. To improve upon PAC/PVC detection sensitivity while maintaining high AF\ndetection, we use multi-modal data which incorporates 1D PPG, accelerometers,\nand heart rate data as the inputs to a computationally efficient 1D\nbi-directional Gated Recurrent Unit (1D-Bi-GRU) model to detect three\narrhythmia classes. We used motion-artifact prone smartwatch PPG data from the\nNIH-funded Pulsewatch clinical trial. Our multimodal model tested on 72\nsubjects achieved an unprecedented 83% sensitivity for PAC/PVC detection while\nmaintaining a high accuracy of 97.31% for AF detection. These results\noutperformed the best state-of-the-art model by 20.81% for PAC/PVC and 2.55%\nfor AF detection even while our model was computationally more efficient (14\ntimes lighter and 2.7 faster).",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06147v1",
    "published_date": "2024-09-10 01:44:56 UTC",
    "updated_date": "2024-09-10 01:44:56 UTC"
  },
  {
    "arxiv_id": "2409.13730v2",
    "title": "VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning",
    "authors": [
      "Zhihuan Jiang",
      "Zhen Yang",
      "Jinhao Chen",
      "Zhengxiao Du",
      "Weihan Wang",
      "Bin Xu",
      "Jie Tang"
    ],
    "abstract": "Multi-modal large language models (MLLMs) have demonstrated promising\ncapabilities across various tasks by integrating textual and visual information\nto achieve visual understanding in complex scenarios. Despite the availability\nof several benchmarks aims to evaluating MLLMs in tasks from visual question\nanswering to complex problem-solving, most focus predominantly on mathematics\nor general visual understanding tasks. This reveals a critical gap in current\nbenchmarks, which often overlook the inclusion of other key scientific\ndisciplines such as physics and chemistry. To address this gap, we meticulously\nconstruct a comprehensive benchmark, named VisScience, which is utilized to\nassess the multi-modal scientific reasoning across the three disciplines of\nmathematics, physics, and chemistry. This benchmark comprises 3,000 questions\ndrawn from K12 education - spanning elementary school through high school -\nequally distributed across three disciplines, with 1,000 questions per\ndiscipline. The questions within VisScience span 21 distinct subjects and are\ncategorized into five difficulty levels, offering a broad spectrum of topics\nwithin each discipline. With VisScience, we present a detailed evaluation of\nthe performance of 25 representative MLLMs in scientific reasoning.\nExperimental results demonstrate that closed-source MLLMs generally outperform\nopen-source models. The best performance observed include a 53.4\\% accuracy in\nmathematics by Claude3.5-Sonnet, 38.2\\% in physics by GPT-4o, and 47.0\\% in\nchemistry by Gemini-1.5-Pro. These results underscore the strengths and\nlimitations of MLLMs, suggesting areas for future improvement and highlighting\nthe importance of developing models that can effectively handle the diverse\ndemands of multi-modal scientific reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "89 pages, 70 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13730v2",
    "published_date": "2024-09-10 01:20:26 UTC",
    "updated_date": "2024-12-02 15:11:23 UTC"
  },
  {
    "arxiv_id": "2409.13729v2",
    "title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model",
    "authors": [
      "Zhen Yang",
      "Jinhao Chen",
      "Zhengxiao Du",
      "Wenmeng Yu",
      "Weihan Wang",
      "Wenyi Hong",
      "Zhihuan Jiang",
      "Bin Xu",
      "Jie Tang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "30 pages,19 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13729v2",
    "published_date": "2024-09-10 01:20:22 UTC",
    "updated_date": "2024-12-02 14:59:08 UTC"
  },
  {
    "arxiv_id": "2409.06131v2",
    "title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review",
    "authors": [
      "Neha Prakriya",
      "Jui-Nan Yen",
      "Cho-Jui Hsieh",
      "Jason Cong"
    ],
    "abstract": "Traditional Large Language Model (LLM) pretraining relies on autoregressive\nlanguage modeling with randomly sampled data from web-scale datasets. Inspired\nby human learning techniques like spaced repetition, we hypothesize that random\nsampling leads to high training costs, lower-quality models, and significant\ndata forgetting. To address these inefficiencies, we propose the\nLearn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to\nthe model's learning progress. LFR tracks the model's learning performance\nacross data blocks (sequences of tokens) and prioritizes revisiting challenging\nregions of the dataset that are more prone to being forgotten, enabling better\nretention and more efficient learning. Using the LFR paradigm, we pretrained\nLlama and GPT models on the SlimPajama and OpenWebText datasets, respectively.\nThese models were evaluated on downstream tasks across various domains,\nincluding question answering, problem-solving, commonsense reasoning, language\nmodeling, and translation. Compared to baseline models trained on the full\ndatasets, LFR consistently achieved lower perplexity and higher accuracy, while\nusing only 5%--19% of the training tokens. Furthermore, LFR matched the\nperformance of industry-standard Pythia models with up to 2$\\times$ the\nparameter count, using just 3.2% of the training tokens, demonstrating its\neffectiveness and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06131v2",
    "published_date": "2024-09-10 00:59:18 UTC",
    "updated_date": "2025-01-28 19:18:51 UTC"
  },
  {
    "arxiv_id": "2409.06130v1",
    "title": "On the Weaknesses of Backdoor-based Model Watermarking: An Information-theoretic Perspective",
    "authors": [
      "Aoting Hu",
      "Yanzhi Chen",
      "Renjie Xie",
      "Adrian Weller"
    ],
    "abstract": "Safeguarding the intellectual property of machine learning models has emerged\nas a pressing concern in AI security. Model watermarking is a powerful\ntechnique for protecting ownership of machine learning models, yet its\nreliability has been recently challenged by recent watermark removal attacks.\nIn this work, we investigate why existing watermark embedding techniques\nparticularly those based on backdooring are vulnerable. Through an\ninformation-theoretic analysis, we show that the resilience of watermarking\nagainst erasure attacks hinges on the choice of trigger-set samples, where\ncurrent uses of out-distribution trigger-set are inherently vulnerable to\nwhite-box adversaries. Based on this discovery, we propose a novel model\nwatermarking scheme, In-distribution Watermark Embedding (IWE), to overcome the\nlimitations of existing method. To further minimise the gap to clean models, we\nanalyze the role of logits as watermark information carriers and propose a new\napproach to better conceal watermark information within the logits. Experiments\non real-world datasets including CIFAR-100 and Caltech-101 demonstrate that our\nmethod robustly defends against various adversaries with negligible accuracy\nloss (< 0.1%).",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06130v1",
    "published_date": "2024-09-10 00:55:21 UTC",
    "updated_date": "2024-09-10 00:55:21 UTC"
  },
  {
    "arxiv_id": "2409.06122v1",
    "title": "Case Study: Leveraging GenAI to Build AI-based Surrogates and Regressors for Modeling Radio Frequency Heating in Fusion Energy Science",
    "authors": [
      "E. Wes Bethel",
      "Vianna Cramer",
      "Alexander del Rio",
      "Lothar Narins",
      "Chris Pestano",
      "Satvik Verma",
      "Erick Arias",
      "Nicola Bertelli",
      "Talita Perciano",
      "Syun'ichi Shiraiwa",
      "Álvaro Sánchez Villar",
      "Greg Wallace",
      "John C. Wright"
    ],
    "abstract": "This work presents a detailed case study on using Generative AI (GenAI) to\ndevelop AI surrogates for simulation models in fusion energy research. The\nscope includes the methodology, implementation, and results of using GenAI to\nassist in model development and optimization, comparing these results with\nprevious manually developed models.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06122v1",
    "published_date": "2024-09-10 00:22:19 UTC",
    "updated_date": "2024-09-10 00:22:19 UTC"
  }
]
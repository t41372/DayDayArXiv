[
  {
    "arxiv_id": "2405.12236v2",
    "title": "Fully Distributed Fog Load Balancing with Multi-Agent Reinforcement Learning",
    "authors": [
      "Maad Ebrahim",
      "Abdelhakim Hafid"
    ],
    "abstract": "Real-time Internet of Things (IoT) applications require real-time support to\nhandle the ever-growing demand for computing resources to process IoT\nworkloads. Fog Computing provides high availability of such resources in a\ndistributed manner. However, these resources must be efficiently managed to\ndistribute unpredictable traffic demands among heterogeneous Fog resources.\nThis paper proposes a fully distributed load-balancing solution with\nMulti-Agent Reinforcement Learning (MARL) that intelligently distributes IoT\nworkloads to optimize the waiting time while providing fair resource\nutilization in the Fog network. These agents use transfer learning for\nlife-long self-adaptation to dynamic changes in the environment. By leveraging\ndistributed decision-making, MARL agents effectively minimize the waiting time\ncompared to a single centralized agent solution and other baselines, enhancing\nend-to-end execution delay. Besides performance gain, a fully distributed\nsolution allows for a global-scale implementation where agents can work\nindependently in small collaboration regions, leveraging nearby local\nresources. Furthermore, we analyze the impact of a realistic frequency to\nobserve the state of the environment, unlike the unrealistic common assumption\nin the literature of having observations readily available in real-time for\nevery required action. The findings highlight the trade-off between realism and\nperformance using an interval-based Gossip-based multi-casting protocol against\nassuming real-time observation availability for every generated workload.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to IEEE TNSM with 14 pages, 11 figures, and 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.12236v2",
    "published_date": "2024-05-15 23:44:06 UTC",
    "updated_date": "2025-03-26 14:25:26 UTC"
  },
  {
    "arxiv_id": "2405.13025v2",
    "title": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
    "authors": [
      "Qingyang Ren",
      "Zilin Jiang",
      "Jinghan Cao",
      "Sijia Li",
      "Chiqu Li",
      "Yiyang Liu",
      "Shuning Huo",
      "Tiange He",
      "Yuan Chen"
    ],
    "abstract": "This survey explores the fairness of large language models (LLMs) in\ne-commerce, examining their progress, applications, and the challenges they\nface. LLMs have become pivotal in the e-commerce domain, offering innovative\nsolutions and enhancing customer experiences. This work presents a\ncomprehensive survey on the applications and challenges of LLMs in e-commerce.\nThe paper begins by introducing the key principles underlying the use of LLMs\nin e-commerce, detailing the processes of pretraining, fine-tuning, and\nprompting that tailor these models to specific needs. It then explores the\nvaried applications of LLMs in e-commerce, including product reviews, where\nthey synthesize and analyze customer feedback; product recommendations, where\nthey leverage consumer data to suggest relevant items; product information\ntranslation, enhancing global accessibility; and product question and answer\nsections, where they automate customer support. The paper critically addresses\nthe fairness challenges in e-commerce, highlighting how biases in training data\nand algorithms can lead to unfair outcomes, such as reinforcing stereotypes or\ndiscriminating against certain groups. These issues not only undermine consumer\ntrust, but also raise ethical and legal concerns. Finally, the work outlines\nfuture research directions, emphasizing the need for more equitable and\ntransparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate\nbiases and improve the fairness of these systems, ensuring they serve diverse\nglobal markets effectively and ethically. Through this comprehensive analysis,\nthe survey provides a holistic view of the current landscape of LLMs in\ne-commerce, offering insights into their potential and limitations, and guiding\nfuture endeavors in creating fairer and more inclusive e-commerce environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.13025v2",
    "published_date": "2024-05-15 23:25:19 UTC",
    "updated_date": "2024-06-21 21:26:03 UTC"
  },
  {
    "arxiv_id": "2405.17442v1",
    "title": "Leveraging Machine Learning for Accurate IoT Device Identification in Dynamic Wireless Contexts",
    "authors": [
      "Bhagyashri Tushir",
      "Vikram K Ramanna",
      "Yuhong Liu",
      "Behnam Dezfouli"
    ],
    "abstract": "Identifying IoT devices is crucial for network monitoring, security\nenforcement, and inventory tracking. However, most existing identification\nmethods rely on deep packet inspection, which raises privacy concerns and adds\ncomputational complexity. More importantly, existing works overlook the impact\nof wireless channel dynamics on the accuracy of layer-2 features, thereby\nlimiting their effectiveness in real-world scenarios. In this work, we define\nand use the latency of specific probe-response packet exchanges, referred to as\n\"device latency,\" as the main feature for device identification. Additionally,\nwe reveal the critical impact of wireless channel dynamics on the accuracy of\ndevice identification based on device latency. Specifically, this work\nintroduces \"accumulation score\" as a novel approach to capturing fine-grained\nchannel dynamics and their impact on device latency when training machine\nlearning models. We implement the proposed methods and measure the accuracy and\noverhead of device identification in real-world scenarios. The results confirm\nthat by incorporating the accumulation score for balanced data collection and\ntraining machine learning algorithms, we achieve an F1 score of over 97% for\ndevice identification, even amidst wireless channel dynamics, a significant\nimprovement over the 75% F1 score achieved by disregarding the impact of\nchannel dynamics on data collection and device latency.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "cs.OS"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17442v1",
    "published_date": "2024-05-15 22:34:52 UTC",
    "updated_date": "2024-05-15 22:34:52 UTC"
  },
  {
    "arxiv_id": "2405.09719v3",
    "title": "Spectral Editing of Activations for Large Language Model Alignment",
    "authors": [
      "Yifu Qiu",
      "Zheng Zhao",
      "Yftah Ziser",
      "Anna Korhonen",
      "Edoardo M. Ponti",
      "Shay B. Cohen"
    ],
    "abstract": "Large language models (LLMs) often exhibit undesirable behaviours, such as\ngenerating untruthful or biased content. Editing their internal representations\nhas been shown to be effective in mitigating such behaviours on top of the\nexisting alignment methods. We propose a novel inference-time editing method,\nnamely spectral editing of activations (SEA), to project the input\nrepresentations into directions with maximal covariance with the positive\ndemonstrations (e.g., truthful) while minimising covariance with the negative\ndemonstrations (e.g., hallucinated). We also extend our method to non-linear\nediting using feature functions. We run extensive experiments on benchmarks\nconcerning truthfulness and bias with six open-source LLMs of different sizes\nand model families. The results demonstrate the superiority of SEA in\neffectiveness, generalisation to similar tasks, as well as computation and data\nefficiency. We also show that SEA editing only has a limited negative impact on\nother model capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09719v3",
    "published_date": "2024-05-15 22:28:23 UTC",
    "updated_date": "2024-11-03 11:12:14 UTC"
  },
  {
    "arxiv_id": "2405.09713v2",
    "title": "SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge",
    "authors": [
      "Andong Wang",
      "Bo Wu",
      "Sunli Chen",
      "Zhenfang Chen",
      "Haotian Guan",
      "Wei-Ning Lee",
      "Li Erran Li",
      "Chuang Gan"
    ],
    "abstract": "Learning commonsense reasoning from visual contexts and scenes in real-world\nis a crucial step toward advanced artificial intelligence. However, existing\nvideo reasoning benchmarks are still inadequate since they were mainly designed\nfor factual or situated reasoning and rarely involve broader knowledge in the\nreal world. Our work aims to delve deeper into reasoning evaluations,\nspecifically within dynamic, open-world, and structured context knowledge. We\npropose a new benchmark (SOK-Bench), consisting of 44K questions and 10K\nsituations with instance-level annotations depicted in the videos. The\nreasoning process is required to understand and apply situated knowledge and\ngeneral knowledge for problem-solving. To create such a dataset, we propose an\nautomatic and scalable generation method to generate question-answer pairs,\nknowledge graphs, and rationales by instructing the combinations of LLMs and\nMLLMs. Concretely, we first extract observable situated entities, relations,\nand processes from videos for situated knowledge and then extend to open-world\nknowledge beyond the visible content. The task generation is facilitated\nthrough multiple dialogues as iterations and subsequently corrected and refined\nby our designed self-promptings and demonstrations. With a corpus of both\nexplicit situated facts and implicit commonsense, we generate associated\nquestion-answer pairs and reasoning processes, finally followed by manual\nreviews for quality assurance. We evaluated recent mainstream large\nvision-language models on the benchmark and found several insightful\nconclusions. For more information, please refer to our benchmark at\nwww.bobbywu.com/SOKBench.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR",
    "pdf_url": "http://arxiv.org/pdf/2405.09713v2",
    "published_date": "2024-05-15 21:55:31 UTC",
    "updated_date": "2024-05-17 02:18:16 UTC"
  },
  {
    "arxiv_id": "2405.09711v1",
    "title": "STAR: A Benchmark for Situated Reasoning in Real-World Videos",
    "authors": [
      "Bo Wu",
      "Shoubin Yu",
      "Zhenfang Chen",
      "Joshua B Tenenbaum",
      "Chuang Gan"
    ],
    "abstract": "Reasoning in the real world is not divorced from situations. How to capture\nthe present knowledge from surrounding situations and perform reasoning\naccordingly is crucial and challenging for machine intelligence. This paper\nintroduces a new benchmark that evaluates the situated reasoning ability via\nsituation abstraction and logic-grounded question answering for real-world\nvideos, called Situated Reasoning in Real-World Videos (STAR Benchmark). This\nbenchmark is built upon the real-world videos associated with human actions or\ninteractions, which are naturally dynamic, compositional, and logical. The\ndataset includes four types of questions, including interaction, sequence,\nprediction, and feasibility. We represent the situations in real-world videos\nby hyper-graphs connecting extracted atomic entities and relations (e.g.,\nactions, persons, objects, and relationships). Besides visual perception,\nsituated reasoning also requires structured situation comprehension and logical\nreasoning. Questions and answers are procedurally generated. The answering\nlogic of each question is represented by a functional program based on a\nsituation hyper-graph. We compare various existing video reasoning models and\nfind that they all struggle on this challenging situated reasoning task. We\nfurther propose a diagnostic neuro-symbolic model that can disentangle visual\nperception, situation abstraction, language understanding, and functional\nreasoning to understand the challenges of this benchmark.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS",
    "pdf_url": "http://arxiv.org/pdf/2405.09711v1",
    "published_date": "2024-05-15 21:53:54 UTC",
    "updated_date": "2024-05-15 21:53:54 UTC"
  },
  {
    "arxiv_id": "2405.09708v1",
    "title": "No More Mumbles: Enhancing Robot Intelligibility through Speech Adaptation",
    "authors": [
      "Qiaoqiao Ren",
      "Yuanbo Hou",
      "Dick Botteldooren",
      "Tony Belpaeme"
    ],
    "abstract": "Spoken language interaction is at the heart of interpersonal communication,\nand people flexibly adapt their speech to different individuals and\nenvironments. It is surprising that robots, and by extension other digital\ndevices, are not equipped to adapt their speech and instead rely on fixed\nspeech parameters, which often hinder comprehension by the user. We conducted a\nspeech comprehension study involving 39 participants who were exposed to\ndifferent environmental and contextual conditions. During the experiment, the\nrobot articulated words using different vocal parameters, and the participants\nwere tasked with both recognising the spoken words and rating their subjective\nimpression of the robot's speech. The experiment's primary outcome shows that\nspaces with good acoustic quality positively correlate with intelligibility and\nuser experience. However, increasing the distance between the user and the\nrobot exacerbated the user experience, while distracting background sounds\nsignificantly reduced speech recognition accuracy and user satisfaction. We\nnext built an adaptive voice for the robot. For this, the robot needs to know\nhow difficult it is for a user to understand spoken language in a particular\nsetting. We present a prediction model that rates how annoying the ambient\nacoustic environment is and, consequentially, how hard it is to understand\nsomeone in this setting. Then, we develop a convolutional neural network model\nto adapt the robot's speech parameters to different users and spaces, while\ntaking into account the influence of ambient acoustics on intelligibility.\nFinally, we present an evaluation with 27 users, demonstrating superior\nintelligibility and user experience with adaptive voice parameters compared to\nfixed voice.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "stat.CO"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE Robotics and Automation Letters (IEEE RAL)",
    "pdf_url": "http://arxiv.org/pdf/2405.09708v1",
    "published_date": "2024-05-15 21:28:55 UTC",
    "updated_date": "2024-05-15 21:28:55 UTC"
  },
  {
    "arxiv_id": "2405.09691v2",
    "title": "Modeling User Preferences via Brain-Computer Interfacing",
    "authors": [
      "Luis A. Leiva",
      "V. Javier Traver",
      "Alexandra Kawala-Sterniuk",
      "Tuukka Ruotsalo"
    ],
    "abstract": "Present Brain-Computer Interfacing (BCI) technology allows inference and\ndetection of cognitive and affective states, but fairly little has been done to\nstudy scenarios in which such information can facilitate new applications that\nrely on modeling human cognition. One state that can be quantified from various\nphysiological signals is attention. Estimates of human attention can be used to\nreveal preferences and novel dimensions of user experience. Previous approaches\nhave tackled these incredibly challenging tasks using a variety of behavioral\nsignals, from dwell-time to click-through data, and computational models of\nvisual correspondence to these behavioral signals. However, behavioral signals\nare only rough estimations of the real underlying attention and affective\npreferences of the users. Indeed, users may attend to some content simply\nbecause it is salient, but not because it is really interesting, or simply\nbecause it is outrageous. With this paper, we put forward a research agenda and\nexample work using BCI to infer users' preferences, their attentional\ncorrelates towards visual content, and their associations with affective\nexperience. Subsequently, we link these to relevant applications, such as\ninformation retrieval, personalized steering of generative models, and\ncrowdsourcing population estimates of affective experiences.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09691v2",
    "published_date": "2024-05-15 20:41:46 UTC",
    "updated_date": "2024-05-31 16:57:30 UTC"
  },
  {
    "arxiv_id": "2405.09689v1",
    "title": "Generalized Holographic Reduced Representations",
    "authors": [
      "Calvin Yeung",
      "Zhuowen Zou",
      "Mohsen Imani"
    ],
    "abstract": "Deep learning has achieved remarkable success in recent years. Central to its\nsuccess is its ability to learn representations that preserve task-relevant\nstructure. However, massive energy, compute, and data costs are required to\nlearn general representations. This paper explores Hyperdimensional Computing\n(HDC), a computationally and data-efficient brain-inspired alternative. HDC\nacts as a bridge between connectionist and symbolic approaches to artificial\nintelligence (AI), allowing explicit specification of representational\nstructure as in symbolic approaches while retaining the flexibility of\nconnectionist approaches. However, HDC's simplicity poses challenges for\nencoding complex compositional structures, especially in its binding operation.\nTo address this, we propose Generalized Holographic Reduced Representations\n(GHRR), an extension of Fourier Holographic Reduced Representations (FHRR), a\nspecific HDC implementation. GHRR introduces a flexible, non-commutative\nbinding operation, enabling improved encoding of complex data structures while\npreserving HDC's desirable properties of robustness and transparency. In this\nwork, we introduce the GHRR framework, prove its theoretical properties and its\nadherence to HDC properties, explore its kernel and binding characteristics,\nand perform empirical experiments showcasing its flexible non-commutativity,\nenhanced decoding accuracy for compositional structures, and improved\nmemorization capacity compared to FHRR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09689v1",
    "published_date": "2024-05-15 20:37:48 UTC",
    "updated_date": "2024-05-15 20:37:48 UTC"
  },
  {
    "arxiv_id": "2405.10986v1",
    "title": "Benchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models",
    "authors": [
      "Anthony M. Barrett",
      "Krystal Jackson",
      "Evan R. Murphy",
      "Nada Madkour",
      "Jessica Newman"
    ],
    "abstract": "A concern about cutting-edge or \"frontier\" AI foundation models is that an\nadversary may use the models for preparing chemical, biological, radiological,\nnuclear, (CBRN), cyber, or other attacks. At least two methods can identify\nfoundation models with potential dual-use capability; each has advantages and\ndisadvantages: A. Open benchmarks (based on openly available questions and\nanswers), which are low-cost but accuracy-limited by the need to omit\nsecurity-sensitive details; and B. Closed red team evaluations (based on\nprivate evaluation by CBRN and cyber experts), which are higher-cost but can\nachieve higher accuracy by incorporating sensitive details. We propose a\nresearch and risk-management approach using a combination of methods including\nboth open benchmarks and closed red team evaluations, in a way that leverages\nadvantages of both methods. We recommend that one or more groups of researchers\nwith sufficient resources and access to a range of near-frontier and frontier\nfoundation models run a set of foundation models through dual-use capability\nevaluation benchmarks and red team evaluations, then analyze the resulting sets\nof models' scores on benchmark and red team evaluations to see how correlated\nthose are. If, as we expect, there is substantial correlation between the\ndual-use potential benchmark scores and the red team evaluation scores, then\nimplications include the following: The open benchmarks should be used\nfrequently during foundation model development as a quick, low-cost measure of\na model's dual-use potential; and if a particular model gets a high score on\nthe dual-use potential benchmark, then more in-depth red team assessments of\nthat model's dual-use capability should be performed. We also discuss\nlimitations and mitigations for our approach, e.g., if model developers try to\ngame benchmarks by including a version of benchmark test data in a model's\ntraining data.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "62 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.10986v1",
    "published_date": "2024-05-15 20:28:15 UTC",
    "updated_date": "2024-05-15 20:28:15 UTC"
  },
  {
    "arxiv_id": "2405.09688v4",
    "title": "A Theory of Synaptic Neural Balance: From Local to Global Order",
    "authors": [
      "Pierre Baldi",
      "Antonios Alexos",
      "Ian Domingo",
      "Alireza Rahmansetayesh"
    ],
    "abstract": "We develop a general theory of synaptic neural balance and how it can emerge\nor be enforced in neural networks. For a given regularizer, a neuron is said to\nbe in balance if the total cost of its input weights is equal to the total cost\nof its output weights. The basic example is provided by feedforward networks of\nReLU units trained with $L_2$ regularizers, which exhibit balance after proper\ntraining. The theory explains this phenomenon and extends it in several\ndirections. The first direction is the extension to bilinear and other\nactivation functions. The second direction is the extension to more general\nregularizers, including all $L_p$ regularizers. The third direction is the\nextension to non-layered architectures, recurrent architectures, convolutional\narchitectures, as well as architectures with mixed activation functions.\nGradient descent on the error function alone does not converge in general to a\nbalanced state, where every neuron is in balance, even when starting from a\nbalanced state. However, gradient descent on the regularized error function\nought to converge to a balanced state, and thus network balance can be used to\nassess learning progress. The theory is based on two local neuronal operations:\nscaling which is commutative, and balancing which is not commutative. Given any\ninitial set of weights, when local balancing operations are applied to each\nneuron in a stochastic manner, global order always emerges through the\nconvergence of the stochastic balancing algorithm to the same unique set of\nbalanced weights. The reason for this is the existence of an underlying\nstrictly convex optimization problem where the relevant variables are\nconstrained to a linear, only architecture-dependent, manifold. Simulations\nshow that balancing neurons prior to learning, or during learning in\nalternation with gradient descent steps, can improve learning speed and final\nperformance.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09688v4",
    "published_date": "2024-05-15 20:27:56 UTC",
    "updated_date": "2024-10-31 02:01:41 UTC"
  },
  {
    "arxiv_id": "2407.04712v1",
    "title": "Sensing technologies and machine learning methods for emotion recognition in autism: Systematic review",
    "authors": [
      "Oresti Banos",
      "Zhoe Comas-González",
      "Javier Medina",
      "Aurora Polo-Rodríguez",
      "David Gil",
      "Jesús Peral",
      "Sandra Amador",
      "Claudia Villalonga"
    ],
    "abstract": "Background: Human Emotion Recognition (HER) has been a popular field of study\nin the past years. Despite the great progresses made so far, relatively little\nattention has been paid to the use of HER in autism. People with autism are\nknown to face problems with daily social communication and the prototypical\ninterpretation of emotional responses, which are most frequently exerted via\nfacial expressions. This poses significant practical challenges to the\napplication of regular HER systems, which are normally developed for and by\nneurotypical people. Objective: This study reviews the literature on the use of\nHER systems in autism, particularly with respect to sensing technologies and\nmachine learning methods, as to identify existing barriers and possible future\ndirections. Methods: We conducted a systematic review of articles published\nbetween January 2011 and June 2023 according to the 2020 PRISMA guidelines.\nManuscripts were identified through searching Web of Science and Scopus\ndatabases. Manuscripts were included when related to emotion recognition, used\nsensors and machine learning techniques, and involved children with autism,\nyoung, or adults. Results: The search yielded 346 articles. A total of 65\npublications met the eligibility criteria and were included in the review.\nConclusions: Studies predominantly used facial expression techniques as the\nemotion recognition method. Consequently, video cameras were the most widely\nused devices across studies, although a growing trend in the use of\nphysiological sensors was observed lately. Happiness, sadness, anger, fear,\ndisgust, and surprise were most frequently addressed. Classical supervised\nmachine learning techniques were primarily used at the expense of unsupervised\napproaches or more recent deep learning models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.04712v1",
    "published_date": "2024-05-15 19:48:04 UTC",
    "updated_date": "2024-05-15 19:48:04 UTC"
  },
  {
    "arxiv_id": "2405.09679v2",
    "title": "Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation",
    "authors": [
      "Julia Barnett",
      "Kimon Kieslich",
      "Nicholas Diakopoulos"
    ],
    "abstract": "The rapid advancement of AI technologies yields numerous future impacts on\nindividuals and society. Policymakers are tasked to react quickly and establish\npolicies that mitigate those impacts. However, anticipating the effectiveness\nof policies is a difficult task, as some impacts might only be observable in\nthe future and respective policies might not be applicable to the future\ndevelopment of AI. In this work we develop a method for using large language\nmodels (LLMs) to evaluate the efficacy of a given piece of policy at mitigating\nspecified negative impacts. We do so by using GPT-4 to generate scenarios both\npre- and post-introduction of policy and translating these vivid stories into\nmetrics based on human perceptions of impacts. We leverage an already\nestablished taxonomy of impacts of generative AI in the media environment to\ngenerate a set of scenario pairs both mitigated and non-mitigated by the\ntransparency policy in Article 50 of the EU AI Act. We then run a user study\n(n=234) to evaluate these scenarios across four risk-assessment dimensions:\nseverity, plausibility, magnitude, and specificity to vulnerable populations.\nWe find that this transparency legislation is perceived to be effective at\nmitigating harms in areas such as labor and well-being, but largely ineffective\nin areas such as social cohesion and security. Through this case study we\ndemonstrate the efficacy of our method as a tool to iterate on the\neffectiveness of policy for mitigating various negative impacts. We expect this\nmethod to be useful to researchers or other stakeholders who want to brainstorm\nthe potential utility of different pieces of policy or other mitigation\nstrategies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in the proceedings of the Seventh AAAI/ACM Conference\n  on AI, Ethics, and Society",
    "pdf_url": "http://arxiv.org/pdf/2405.09679v2",
    "published_date": "2024-05-15 19:44:54 UTC",
    "updated_date": "2024-07-26 21:23:14 UTC"
  },
  {
    "arxiv_id": "2405.09673v2",
    "title": "LoRA Learns Less and Forgets Less",
    "authors": [
      "Dan Biderman",
      "Jacob Portes",
      "Jose Javier Gonzalez Ortiz",
      "Mansheej Paul",
      "Philip Greengard",
      "Connor Jennings",
      "Daniel King",
      "Sam Havens",
      "Vitaliy Chiley",
      "Jonathan Frankle",
      "Cody Blakeney",
      "John P. Cunningham"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning\nmethod for large language models. LoRA saves memory by training only low rank\nperturbations to selected weight matrices. In this work, we compare the\nperformance of LoRA and full finetuning on two target domains, programming and\nmathematics. We consider both the instruction finetuning (approximately 100K\nprompt-response pairs) and continued pretraining (20B unstructured tokens) data\nregimes. Our results show that, in the standard low-rank settings, LoRA\nsubstantially underperforms full finetuning. Nevertheless, LoRA better\nmaintains the base model's performance on tasks outside the target domain. We\nshow that LoRA mitigates forgetting more than common regularization techniques\nsuch as weight decay and dropout; it also helps maintain more diverse\ngenerations. Finally, we show that full finetuning learns perturbations with a\nrank that is 10-100X greater than typical LoRA configurations, possibly\nexplaining some of the reported gaps. We conclude by proposing best practices\nfor finetuning with LoRA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Final version with new experiments and analyses, as accepted to\n  Transactions on Machine Learning Research, August 2024 (Featured\n  Certification). https://openreview.net/forum?id=aloEru2qCG&noteId=Jb3PQNQDI2",
    "pdf_url": "http://arxiv.org/pdf/2405.09673v2",
    "published_date": "2024-05-15 19:27:45 UTC",
    "updated_date": "2024-09-20 21:21:56 UTC"
  },
  {
    "arxiv_id": "2405.09657v1",
    "title": "Detecting Continuous Integration Skip : A Reinforcement Learning-based Approach",
    "authors": [
      "Hajer Mhalla",
      "Mohamed Aymen Saied"
    ],
    "abstract": "The software industry is experiencing a surge in the adoption of Continuous\nIntegration (CI) practices, both in commercial and open-source environments. CI\npractices facilitate the seamless integration of code changes by employing\nautomated building and testing processes. Some frameworks, such as Travis CI\nand GitHub Actions have significantly contributed to simplifying and enhancing\nthe CI process, rendering it more accessible and efficient for development\nteams. Despite the availability these CI tools , developers continue to\nencounter difficulties in accurately flagging commits as either suitable for CI\nexecution or as candidates for skipping especially for large projects with many\ndependencies. Inaccurate flagging of commits can lead to resource-intensive\ntest and build processes, as even minor commits may inadvertently trigger the\nContinuous Integration process. The problem of detecting CI-skip commits, can\nbe modeled as binary classification task where we decide to either build a\ncommit or to skip it. This study proposes a novel solution that leverages Deep\nReinforcement Learning techniques to construct an optimal Decision Tree\nclassifier that addresses the imbalanced nature of the data. We evaluate our\nsolution by running a within and a cross project validation benchmark on\ndiverse range of Open-Source projects hosted on GitHub which showcased superior\nresults when compared with existing state-of-the-art methods.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09657v1",
    "published_date": "2024-05-15 18:48:57 UTC",
    "updated_date": "2024-05-15 18:48:57 UTC"
  },
  {
    "arxiv_id": "2405.09528v1",
    "title": "Energy-Efficient Sleep Mode Optimization of 5G mmWave Networks Using Deep Contextual MAB",
    "authors": [
      "Saad Masrur",
      "Ismail Guvenc",
      "David Lopez-Perez"
    ],
    "abstract": "Millimeter-wave (mmWave) networks, integral to 5G communication, offer a vast\nspectrum that addresses the issue of spectrum scarcity and enhances peak rate\nand capacity. However, their dense deployment, necessary to counteract\npropagation losses, leads to high power consumption. An effective strategy to\nreduce this energy consumption in mobile networks is the sleep mode\noptimization (SMO) of base stations (BSs). In this paper, we propose a novel\nSMO approach for mmWave BSs in a 3D urban environment. This approach, which\nincorporates a neural network (NN) based contextual multi-armed bandit (C-MAB)\nwith an epsilon decay algorithm, accommodates the dynamic and diverse traffic\nof user equipment (UE) by clustering the UEs in their respective tracking areas\n(TAs). Our strategy includes beamforming, which helps reduce energy consumption\nfrom the UE side, while SMO minimizes energy use from the BS perspective. We\nextended our investigation to include Random, Epsilon Greedy, Upper Confidence\nBound (UCB), and Load Based sleep mode (SM) strategies. We compared the\nperformance of our proposed C-MAB based SM algorithm with those of All On and\nother alternative approaches. Simulation results show that our proposed method\noutperforms all other SM strategies in terms of the $10^{th}$ percentile of\nuser rate and average throughput while demonstrating comparable average\nthroughput to the All On approach. Importantly, it outperforms all approaches\nin terms of energy efficiency (EE).",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09528v1",
    "published_date": "2024-05-15 17:37:28 UTC",
    "updated_date": "2024-05-15 17:37:28 UTC"
  },
  {
    "arxiv_id": "2405.09521v3",
    "title": "Declarative Design of Neural Predicates in Neuro-Symbolic Systems",
    "authors": [
      "Tilman Hinnerichs",
      "Robin Manhaeve",
      "Giuseppe Marra",
      "Sebastijan Dumancic"
    ],
    "abstract": "Neuro-symbolic systems (NeSy), which claim to combine the best of both\nlearning and reasoning capabilities of artificial intelligence, are missing a\ncore property of reasoning systems: Declarativeness. The lack of\ndeclarativeness is caused by the functional nature of neural predicates\ninherited from neural networks. We propose and implement a general framework\nfor fully declarative neural predicates, which hence extends to fully\ndeclarative NeSy frameworks. We first show that the declarative extension\npreserves the learning and reasoning capabilities while being able to answer\narbitrary queries while only being trained on a single query type.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09521v3",
    "published_date": "2024-05-15 17:24:34 UTC",
    "updated_date": "2025-01-30 13:51:30 UTC"
  },
  {
    "arxiv_id": "2405.09605v1",
    "title": "Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models",
    "authors": [
      "Anna A. Ivanova",
      "Aalok Sathe",
      "Benjamin Lipkin",
      "Unnathi Kumar",
      "Setayesh Radkani",
      "Thomas H. Clark",
      "Carina Kauf",
      "Jennifer Hu",
      "R. T. Pramod",
      "Gabriel Grand",
      "Vivian Paulun",
      "Maria Ryskina",
      "Ekin Akyürek",
      "Ethan Wilcox",
      "Nafisa Rashid",
      "Leshem Choshen",
      "Roger Levy",
      "Evelina Fedorenko",
      "Joshua Tenenbaum",
      "Jacob Andreas"
    ],
    "abstract": "The ability to build and leverage world models is essential for a\ngeneral-purpose AI agent. Testing such capabilities is hard, in part because\nthe building blocks of world models are ill-defined. We present Elements of\nWorld Knowledge (EWOK), a framework for evaluating world modeling in language\nmodels by testing their ability to use knowledge of a concept to match a target\ntext with a plausible/implausible context. EWOK targets specific concepts from\nmultiple knowledge domains known to be vital for world modeling in humans.\nDomains range from social interactions (help/hinder) to spatial relations\n(left/right). Both, contexts and targets are minimal pairs. Objects, agents,\nand locations in the items can be flexibly filled in enabling easy generation\nof multiple controlled datasets. We then introduce EWOK-CORE-1.0, a dataset of\n4,374 items covering 11 world knowledge domains. We evaluate 20 openweights\nlarge language models (1.3B--70B parameters) across a battery of evaluation\nparadigms along with a human norming study comprising 12,480 measurements. The\noverall performance of all tested models is worse than human performance, with\nresults varying drastically across domains. These data highlight simple cases\nwhere even large models fail and present rich avenues for targeted research on\nLLM world modeling capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages (11 main), 7 figures. Authors Anna Ivanova, Aalok Sathe,\n  Benjamin Lipkin contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2405.09605v1",
    "published_date": "2024-05-15 17:19:42 UTC",
    "updated_date": "2024-05-15 17:19:42 UTC"
  },
  {
    "arxiv_id": "2405.09507v1",
    "title": "QueryNER: Segmentation of E-commerce Queries",
    "authors": [
      "Chester Palen-Michel",
      "Lizzie Liang",
      "Zhe Wu",
      "Constantine Lignos"
    ],
    "abstract": "We present QueryNER, a manually-annotated dataset and accompanying model for\ne-commerce query segmentation. Prior work in sequence labeling for e-commerce\nhas largely addressed aspect-value extraction which focuses on extracting\nportions of a product title or query for narrowly defined aspects. Our work\ninstead focuses on the goal of dividing a query into meaningful chunks with\nbroadly applicable types. We report baseline tagging results and conduct\nexperiments comparing token and entity dropping for null and low recall query\nrecovery. Challenging test sets are created using automatic transformations and\nshow how simple data augmentation techniques can make the models more robust to\nnoise. We make the QueryNER dataset publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09507v1",
    "published_date": "2024-05-15 16:58:35 UTC",
    "updated_date": "2024-05-15 16:58:35 UTC"
  },
  {
    "arxiv_id": "2405.09496v1",
    "title": "ParaNames 1.0: Creating an Entity Name Corpus for 400+ Languages using Wikidata",
    "authors": [
      "Jonne Sälevä",
      "Constantine Lignos"
    ],
    "abstract": "We introduce ParaNames, a massively multilingual parallel name resource\nconsisting of 140 million names spanning over 400 languages. Names are provided\nfor 16.8 million entities, and each entity is mapped from a complex type\nhierarchy to a standard type (PER/LOC/ORG). Using Wikidata as a source, we\ncreate the largest resource of this type to date. We describe our approach to\nfiltering and standardizing the data to provide the best quality possible.\nParaNames is useful for multilingual language processing, both in defining\ntasks for name translation/transliteration and as supplementary data for tasks\nsuch as named entity recognition and linking. We demonstrate the usefulness of\nParaNames on two tasks. First, we perform canonical name translation between\nEnglish and 17 other languages. Second, we use it as a gazetteer for\nmultilingual named entity recognition, obtaining performance improvements on\nall 10 languages evaluated.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024. arXiv admin note: text overlap with\n  arXiv:2202.14035",
    "pdf_url": "http://arxiv.org/pdf/2405.09496v1",
    "published_date": "2024-05-15 16:44:54 UTC",
    "updated_date": "2024-05-15 16:44:54 UTC"
  },
  {
    "arxiv_id": "2405.09477v1",
    "title": "Harmonizing Human Insights and AI Precision: Hand in Hand for Advancing Knowledge Graph Task",
    "authors": [
      "Shurong Wang",
      "Yufei Zhang",
      "Xuliang Huang",
      "Hongwei Wang"
    ],
    "abstract": "Knowledge graph embedding (KGE) has caught significant interest for its\neffectiveness in knowledge graph completion (KGC), specifically link prediction\n(LP), with recent KGE models cracking the LP benchmarks. Despite the rapidly\ngrowing literature, insufficient attention has been paid to the cooperation\nbetween humans and AI on KG. However, humans' capability to analyze graphs\nconceptually may further improve the efficacy of KGE models with semantic\ninformation. To this effect, we carefully designed a human-AI team (HAIT)\nsystem dubbed KG-HAIT, which harnesses the human insights on KG by leveraging\nfully human-designed ad-hoc dynamic programming (DP) on KG to produce human\ninsightful feature (HIF) vectors that capture the subgraph structural feature\nand semantic similarities. By integrating HIF vectors into the training of KGE\nmodels, notable improvements are observed across various benchmarks and\nmetrics, accompanied by accelerated model convergence. Our results underscore\nthe effectiveness of human-designed DP in the task of LP, emphasizing the\npivotal role of collaboration between humans and AI on KG. We open avenues for\nfurther exploration and innovation through KG-HAIT, paving the way towards more\neffective and insightful KG analysis techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09477v1",
    "published_date": "2024-05-15 16:16:37 UTC",
    "updated_date": "2024-05-15 16:16:37 UTC"
  },
  {
    "arxiv_id": "2405.09459v2",
    "title": "Fourier Boundary Features Network with Wider Catchers for Glass Segmentation",
    "authors": [
      "Xiaolin Qin",
      "Jiacen Liu",
      "Qianlei Wang",
      "Shaolin Zhang",
      "Fei Zhu",
      "Zhang Yi"
    ],
    "abstract": "Glass largely blurs the boundary between the real world and the reflection.\nThe special transmittance and reflectance quality have confused the semantic\ntasks related to machine vision. Therefore, how to clear the boundary built by\nglass, and avoid over-capturing features as false positive information in deep\nstructure, matters for constraining the segmentation of reflection surface and\npenetrating glass. We proposed the Fourier Boundary Features Network with Wider\nCatchers (FBWC), which might be the first attempt to utilize sufficiently wide\nhorizontal shallow branches without vertical deepening for guiding the fine\ngranularity segmentation boundary through primary glass semantic information.\nSpecifically, we designed the Wider Coarse-Catchers (WCC) for anchoring large\narea segmentation and reducing excessive extraction from a structural\nperspective. We embed fine-grained features by Cross Transpose Attention (CTA),\nwhich is introduced to avoid the incomplete area within the boundary caused by\nreflection noise. For excavating glass features and balancing high-low layers\ncontext, a learnable Fourier Convolution Controller (FCC) is proposed to\nregulate information integration robustly. The proposed method has been\nvalidated on three different public glass segmentation datasets. Experimental\nresults reveal that the proposed method yields better segmentation performance\ncompared with the state-of-the-art (SOTA) methods in glass image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09459v2",
    "published_date": "2024-05-15 15:52:27 UTC",
    "updated_date": "2024-12-05 05:37:23 UTC"
  },
  {
    "arxiv_id": "2405.09444v1",
    "title": "Desk-AId: Humanitarian Aid Desk Assessment with Geospatial AI for Predicting Landmine Areas",
    "authors": [
      "Flavio Cirillo",
      "Gürkan Solmaz",
      "Yi-Hsuan Peng",
      "Christian Bizer",
      "Martin Jebens"
    ],
    "abstract": "The process of clearing areas, namely demining, starts by assessing and\nprioritizing potential hazardous areas (i.e., desk assessment) to go under\nthorough investigation of experts, who confirm the risk and proceed with the\nmines clearance operations. This paper presents Desk-AId that supports the desk\nassessment phase by estimating landmine risks using geospatial data and\nsocioeconomic information. Desk-AId uses a Geospatial AI approach specialized\nto landmines. The approach includes mixed data sampling strategies and\ncontext-enrichment by historical conflicts and key multi-domain facilities\n(e.g., buildings, roads, health sites). The proposed system addresses the issue\nof having only ground-truth for confirmed hazardous areas by implementing a new\nhard-negative data sampling strategy, where negative points are sampled in the\nvicinity of hazardous areas. Experiments validate Desk-Aid in two domains for\nlandmine risk assessment: 1) country-wide, and 2) uncharted study areas). The\nproposed approach increases the estimation accuracies up to 92%, for different\nclassification models such as RandomForest (RF), Feedforward Neural Networks\n(FNN), and Graph Neural Networks (GNN).",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09444v1",
    "published_date": "2024-05-15 15:39:35 UTC",
    "updated_date": "2024-05-15 15:39:35 UTC"
  },
  {
    "arxiv_id": "2405.09439v1",
    "title": "Facilitating Opinion Diversity through Hybrid NLP Approaches",
    "authors": [
      "Michiel van der Meer"
    ],
    "abstract": "Modern democracies face a critical issue of declining citizen participation\nin decision-making. Online discussion forums are an important avenue for\nenhancing citizen participation. This thesis proposal 1) identifies the\nchallenges involved in facilitating large-scale online discussions with Natural\nLanguage Processing (NLP), 2) suggests solutions to these challenges by\nincorporating hybrid human-AI technologies, and 3) investigates what these\ntechnologies can reveal about individual perspectives in online discussions. We\npropose a three-layered hierarchy for representing perspectives that can be\nobtained by a mixture of human intelligence and large language models. We\nillustrate how these representations can draw insights into the diversity of\nperspectives and allow us to investigate interactions in online discussions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2024, Student Research Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.09439v1",
    "published_date": "2024-05-15 15:30:17 UTC",
    "updated_date": "2024-05-15 15:30:17 UTC"
  },
  {
    "arxiv_id": "2405.09602v1",
    "title": "Improving Label Error Detection and Elimination with Uncertainty Quantification",
    "authors": [
      "Johannes Jakubik",
      "Michael Vössing",
      "Manil Maskey",
      "Christopher Wölfle",
      "Gerhard Satzger"
    ],
    "abstract": "Identifying and handling label errors can significantly enhance the accuracy\nof supervised machine learning models. Recent approaches for identifying label\nerrors demonstrate that a low self-confidence of models with respect to a\ncertain label represents a good indicator of an erroneous label. However,\nlatest work has built on softmax probabilities to measure self-confidence. In\nthis paper, we argue that -- as softmax probabilities do not reflect a model's\npredictive uncertainty accurately -- label error detection requires more\nsophisticated measures of model uncertainty. Therefore, we develop a range of\nnovel, model-agnostic algorithms for Uncertainty Quantification-Based Label\nError Detection (UQ-LED), which combine the techniques of confident learning\n(CL), Monte Carlo Dropout (MCD), model uncertainty measures (e.g., entropy),\nand ensemble learning to enhance label error detection. We comprehensively\nevaluate our algorithms on four image classification benchmark datasets in two\nstages. In the first stage, we demonstrate that our UQ-LED algorithms\noutperform state-of-the-art confident learning in identifying label errors. In\nthe second stage, we show that removing all identified errors from the training\ndata based on our approach results in higher accuracies than training on all\navailable labeled data. Importantly, besides our contributions to the detection\nof label errors, we particularly propose a novel approach to generate\nrealistic, class-dependent label errors synthetically. Overall, our study\ndemonstrates that selectively cleaning datasets with UQ-LED algorithms leads to\nmore accurate classifications than using larger, noisier datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under single blinded review",
    "pdf_url": "http://arxiv.org/pdf/2405.09602v1",
    "published_date": "2024-05-15 15:17:52 UTC",
    "updated_date": "2024-05-15 15:17:52 UTC"
  },
  {
    "arxiv_id": "2405.09415v3",
    "title": "On the Correspondence of Non-flat Assumption-based Argumentation and Logic Programming with Negation as Failure in the Head",
    "authors": [
      "Anna Rapberger",
      "Markus Ulbricht",
      "Francesca Toni"
    ],
    "abstract": "The relation between (a fragment of) assumption-based argumentation (ABA) and\nlogic programs (LPs) under stable model semantics is well-studied. However, for\nobtaining this relation, the ABA framework needs to be restricted to being\nflat, i.e., a fragment where the (defeasible) assumptions can never be\nentailed, only assumed to be true or false. Here, we remove this restriction\nand show a correspondence between non-flat ABA and LPs with negation as failure\nin their head. We then extend this result to so-called set-stable ABA\nsemantics, originally defined for the fragment of non-flat ABA called bipolar\nABA. We showcase how to define set-stable semantics for LPs with negation as\nfailure in their head and show the correspondence to set-stable ABA semantics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09415v3",
    "published_date": "2024-05-15 15:10:03 UTC",
    "updated_date": "2024-08-13 15:32:51 UTC"
  },
  {
    "arxiv_id": "2405.13024v1",
    "title": "Intelligent Tutor: Leveraging ChatGPT and Microsoft Copilot Studio to Deliver a Generative AI Student Support and Feedback System within Teams",
    "authors": [
      "Wei-Yu Chen"
    ],
    "abstract": "This study explores the integration of the ChatGPT API with GPT-4 model and\nMicrosoft Copilot Studio on the Microsoft Teams platform to develop an\nintelligent tutoring system. Designed to provide instant support to students,\nthe system dynamically adjusts educational content in response to the learners'\nprogress and feedback. Utilizing advancements in natural language processing\nand machine learning, it interprets student inquiries, offers tailored\nfeedback, and facilitates the educational journey. Initial implementation\nhighlights the system's potential in boosting students' motivation and\nengagement, while equipping educators with critical insights into the learning\nprocess, thus promoting tailored educational experiences and enhancing\ninstructional effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13024v1",
    "published_date": "2024-05-15 15:09:41 UTC",
    "updated_date": "2024-05-15 15:09:41 UTC"
  },
  {
    "arxiv_id": "2405.09396v1",
    "title": "$O_2$ is a multiple context-free grammar: an implementation-, formalisation-friendly proof",
    "authors": [
      "Marco B. Caminati"
    ],
    "abstract": "Classifying formal languages according to the expressiveness of grammars able\nto generate them is a fundamental problem in computational linguistics and,\ntherefore, in the theory of computation. Furthermore, such kind of analysis can\ngive insight into the classification of abstract algebraic structure such as\ngroups, for example through the correspondence given by the word problem. While\nmany such classification problems remain open, others have been settled.\nRecently, it was proved that $n$-balanced languages (i.e., whose strings\ncontain the same occurrences of letters $a_i$ and $A_i$ with $1\\leq i \\leq n$)\ncan be generated by multiple context-free grammars (MCFGs), which are one of\nthe several slight extensions of context free grammars added to the classical\nChomsky hierarchy to make the mentioned classification more precise. This paper\nanalyses the existing proofs from the computational and the proof-theoretical\npoint of views, systematically studying whether each proof can lead to a\nverified (i.e., checked by a proof assistant) algorithm parsing balanced\nlanguages via MCFGs. We conclude that none of the existing proofs is\nrealistically suitable against this practical goal, and proceed to provide a\nradically new, elementary, extremely short proof for the crucial case $n \\leq\n2$. A comparative analysis with respect to the existing proofs is finally\nperformed to justify why the proposed proof is a substantial step towards\nconcretely obtaining a verified parsing algorithm for $O_2$.",
    "categories": [
      "cs.FL",
      "cs.AI",
      "cs.LO",
      "math.LO"
    ],
    "primary_category": "cs.FL",
    "comment": "dlt 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09396v1",
    "published_date": "2024-05-15 14:51:11 UTC",
    "updated_date": "2024-05-15 14:51:11 UTC"
  },
  {
    "arxiv_id": "2405.09395v2",
    "title": "Matching domain experts by training from scratch on domain knowledge",
    "authors": [
      "Xiaoliang Luo",
      "Guangzhi Sun",
      "Bradley C. Love"
    ],
    "abstract": "Recently, large language models (LLMs) have outperformed human experts in\npredicting the results of neuroscience experiments (Luo et al., 2024). What is\nthe basis for this performance? One possibility is that statistical patterns in\nthat specific scientific literature, as opposed to emergent reasoning abilities\narising from broader training, underlie LLMs' performance. To evaluate this\npossibility, we trained (next word prediction) a relatively small\n124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.\nDespite being orders of magnitude smaller than larger LLMs trained on trillions\nof tokens, small models achieved expert-level performance in predicting\nneuroscience results. Small models trained on the neuroscience literature\nsucceeded when they were trained from scratch using a tokenizer specifically\ntrained on neuroscience text or when the neuroscience literature was used to\nfinetune a pretrained GPT-2. Our results indicate that expert-level performance\nmay be attained by even small LLMs through domain-specific, auto-regressive\ntraining approaches.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "q-bio.NC",
    "comment": "ICML 2024 (Large Language Models and Cognition)",
    "pdf_url": "http://arxiv.org/pdf/2405.09395v2",
    "published_date": "2024-05-15 14:50:51 UTC",
    "updated_date": "2024-07-02 16:42:48 UTC"
  },
  {
    "arxiv_id": "2405.09600v1",
    "title": "Aggregate Representation Measure for Predictive Model Reusability",
    "authors": [
      "Vishwesh Sangarya",
      "Richard Bradford",
      "Jung-Eun Kim"
    ],
    "abstract": "In this paper, we propose a predictive quantifier to estimate the retraining\ncost of a trained model in distribution shifts. The proposed Aggregated\nRepresentation Measure (ARM) quantifies the change in the model's\nrepresentation from the old to new data distribution. It provides, before\nactually retraining the model, a single concise index of resources - epochs,\nenergy, and carbon emissions - required for the retraining. This enables reuse\nof a model with a much lower cost than training a new model from scratch. The\nexperimental results indicate that ARM reasonably predicts retraining costs for\nvarying noise intensities and enables comparisons among multiple model\narchitectures to determine the most cost-effective and sustainable option.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09600v1",
    "published_date": "2024-05-15 14:14:34 UTC",
    "updated_date": "2024-05-15 14:14:34 UTC"
  },
  {
    "arxiv_id": "2405.09355v1",
    "title": "Vision-Based Neurosurgical Guidance: Unsupervised Localization and Camera-Pose Prediction",
    "authors": [
      "Gary Sarwin",
      "Alessandro Carretta",
      "Victor Staartjes",
      "Matteo Zoli",
      "Diego Mazzatenta",
      "Luca Regli",
      "Carlo Serra",
      "Ender Konukoglu"
    ],
    "abstract": "Localizing oneself during endoscopic procedures can be problematic due to the\nlack of distinguishable textures and landmarks, as well as difficulties due to\nthe endoscopic device such as a limited field of view and challenging lighting\nconditions. Expert knowledge shaped by years of experience is required for\nlocalization within the human body during endoscopic procedures. In this work,\nwe present a deep learning method based on anatomy recognition, that constructs\na surgical path in an unsupervised manner from surgical videos, modelling\nrelative location and variations due to different viewing angles. At inference\ntime, the model can map an unseen video's frames on the path and estimate the\nviewing angle, aiming to provide guidance, for instance, to reach a particular\ndestination. We test the method on a dataset consisting of surgical videos of\ntranssphenoidal adenomectomies, as well as on a synthetic dataset. An online\ntool that lets researchers upload their surgical videos to obtain anatomy\ndetections and the weights of the trained YOLOv7 model are available at:\nhttps://surgicalvision.bmic.ethz.ch.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Early Accept at MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09355v1",
    "published_date": "2024-05-15 14:09:11 UTC",
    "updated_date": "2024-05-15 14:09:11 UTC"
  },
  {
    "arxiv_id": "2405.09598v1",
    "title": "Properties that allow or prohibit transferability of adversarial attacks among quantized networks",
    "authors": [
      "Abhishek Shrestha",
      "Jürgen Großmann"
    ],
    "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to adversarial\nexamples. Further, these adversarial examples are found to be transferable from\nthe source network in which they are crafted to a black-box target network. As\nthe trend of using deep learning on embedded devices grows, it becomes relevant\nto study the transferability properties of adversarial examples among\ncompressed networks. In this paper, we consider quantization as a network\ncompression technique and evaluate the performance of transfer-based attacks\nwhen the source and target networks are quantized at different bitwidths. We\nexplore how algorithm specific properties affect transferability by considering\nvarious adversarial example generation algorithms. Furthermore, we examine\ntransferability in a more realistic scenario where the source and target\nnetworks may differ in bitwidth and other model-related properties like\ncapacity and architecture. We find that although quantization reduces\ntransferability, certain attack types demonstrate an ability to enhance it.\nAdditionally, the average transferability of adversarial examples among\nquantized versions of a network can be used to estimate the transferability to\nquantized target networks with varying capacity and architecture.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09598v1",
    "published_date": "2024-05-15 14:06:28 UTC",
    "updated_date": "2024-05-15 14:06:28 UTC"
  },
  {
    "arxiv_id": "2406.16895v1",
    "title": "Coronary Artery Disease Classification Using One-dimensional Convolutional Neural Network",
    "authors": [
      "Atitaya Phoemsuk",
      "Vahid Abolghasemi"
    ],
    "abstract": "Coronary Artery Disease (CAD) diagnostic to be a major global cause of death,\nnecessitating innovative solutions. Addressing the critical importance of early\nCAD detection and its impact on the mortality rate, we propose the potential of\none-dimensional convolutional neural networks (1D-CNN) to enhance detection\naccuracy and reduce network complexity. This study goes beyond traditional\ndiagnostic methodologies, leveraging the remarkable ability of 1D-CNN to\ninterpret complex patterns within Electrocardiogram (ECG) signals without\ndepending on feature extraction techniques. We explore the impact of varying\nsample lengths on model performance and conduct experiments involving layers\nreduction. The ECG data employed were obtained from the PhysioNet databases,\nnamely the MIMIC III and Fantasia datasets, with respective sampling\nfrequencies of 125 Hz and 250 Hz. The highest accuracy for unseen data obtained\nwith a sample length of 250. These initial findings demonstrate the potential\nof 1D-CNNs in CAD diagnosis using ECG signals and highlight the sample size's\nrole in achieving high accuracy.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16895v1",
    "published_date": "2024-05-15 13:51:02 UTC",
    "updated_date": "2024-05-15 13:51:02 UTC"
  },
  {
    "arxiv_id": "2405.09597v3",
    "title": "When AI Eats Itself: On the Caveats of AI Autophagy",
    "authors": [
      "Xiaodan Xing",
      "Fadong Shi",
      "Jiahao Huang",
      "Yinzhe Wu",
      "Yang Nan",
      "Sheng Zhang",
      "Yingying Fang",
      "Mike Roberts",
      "Carola-Bibiane Schönlieb",
      "Javier Del Ser",
      "Guang Yang"
    ],
    "abstract": "Generative Artificial Intelligence (AI) technologies and large models are\nproducing realistic outputs across various domains, such as images, text,\nspeech, and music. Creating these advanced generative models requires\nsignificant resources, particularly large and high-quality datasets. To\nminimise training expenses, many algorithm developers use data created by the\nmodels themselves as a cost-effective training solution. However, not all\nsynthetic data effectively improve model performance, necessitating a strategic\nbalance in the use of real versus synthetic data to optimise outcomes.\nCurrently, the previously well-controlled integration of real and synthetic\ndata is becoming uncontrollable. The widespread and unregulated dissemination\nof synthetic data online leads to the contamination of datasets traditionally\ncompiled through web scraping, now mixed with unlabeled synthetic data. This\ntrend, known as the AI autophagy phenomenon, suggests a future where generative\nAI systems may increasingly consume their own outputs without discernment,\nraising concerns about model performance, reliability, and ethical\nimplications. What will happen if generative AI continuously consumes itself\nwithout discernment? What measures can we take to mitigate the potential\nadverse effects? To address these research questions, this study examines the\nexisting literature, delving into the consequences of AI autophagy, analyzing\nthe associated risks, and exploring strategies to mitigate its impact. Our aim\nis to provide a comprehensive perspective on this phenomenon advocating for a\nbalanced approach that promotes the sustainable development of generative AI\ntechnologies in the era of large models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09597v3",
    "published_date": "2024-05-15 13:50:23 UTC",
    "updated_date": "2024-11-08 10:51:40 UTC"
  },
  {
    "arxiv_id": "2405.09341v2",
    "title": "Large Language Model Bias Mitigation from the Perspective of Knowledge Editing",
    "authors": [
      "Ruizhe Chen",
      "Yichen Li",
      "Zikai Xiao",
      "Zuozhu Liu"
    ],
    "abstract": "Existing debiasing methods inevitably make unreasonable or undesired\npredictions as they are designated and evaluated to achieve parity across\ndifferent social groups but leave aside individual facts, resulting in modified\nexisting knowledge. In this paper, we first establish a new bias mitigation\nbenchmark BiasKE leveraging existing and additional constructed datasets, which\nsystematically assesses debiasing performance by complementary metrics on\nfairness, specificity, and generalization. Meanwhile, we propose a novel\ndebiasing method, Fairness Stamp (FAST), which enables editable fairness\nthrough fine-grained calibration on individual biased knowledge. Comprehensive\nexperiments demonstrate that FAST surpasses state-of-the-art baselines with\nremarkable debiasing performance while not hampering overall model capability\nfor knowledge preservation, highlighting the prospect of fine-grained debiasing\nstrategies for editable fairness in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09341v2",
    "published_date": "2024-05-15 13:44:13 UTC",
    "updated_date": "2024-06-29 05:50:28 UTC"
  },
  {
    "arxiv_id": "2405.09596v2",
    "title": "Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)",
    "authors": [
      "Nicolas Drapier",
      "Aladine Chetouani",
      "Aurélien Chateigner"
    ],
    "abstract": "The prediction of ship trajectories is a growing field of study in artificial\nintelligence. Traditional methods rely on the use of LSTM, GRU networks, and\neven Transformer architectures for the prediction of spatio-temporal series.\nThis study proposes a viable alternative for predicting these trajectories\nusing only GNSS positions. It considers this spatio-temporal problem as a\nnatural language processing problem. The latitude/longitude coordinates of AIS\nmessages are transformed into cell identifiers using the H3 index. Thanks to\nthe pseudo-octal representation, it becomes easier for language models to learn\nthe spatial hierarchy of the H3 index. The method is compared with a classical\nKalman filter, widely used in the maritime domain, and introduces the Fr\\'echet\ndistance as the main evaluation metric. We show that it is possible to predict\nship trajectories quite precisely up to 8 hours ahead with 30 minutes of\ncontext, using solely GNSS positions, without relying on any additional\ninformation such as speed, course, or external conditions - unlike many\ntraditional methods. We demonstrate that this alternative works well enough to\npredict trajectories worldwide.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09596v2",
    "published_date": "2024-05-15 13:43:07 UTC",
    "updated_date": "2024-11-14 18:57:09 UTC"
  },
  {
    "arxiv_id": "2405.09334v2",
    "title": "Content-Based Image Retrieval for Multi-Class Volumetric Radiology Images: A Benchmark Study",
    "authors": [
      "Farnaz Khun Jush",
      "Steffen Vogler",
      "Tuan Truong",
      "Matthias Lenga"
    ],
    "abstract": "While content-based image retrieval (CBIR) has been extensively studied in\nnatural image retrieval, its application to medical images presents ongoing\nchallenges, primarily due to the 3D nature of medical images. Recent studies\nhave shown the potential use of pre-trained vision embeddings for CBIR in the\ncontext of radiology image retrieval. However, a benchmark for the retrieval of\n3D volumetric medical images is still lacking, hindering the ability to\nobjectively evaluate and compare the efficiency of proposed CBIR approaches in\nmedical imaging. In this study, we extend previous work and establish a\nbenchmark for region-based and localized multi-organ retrieval using the\nTotalSegmentator dataset (TS) with detailed multi-organ annotations. We\nbenchmark embeddings derived from pre-trained supervised models on medical\nimages against embeddings derived from pre-trained unsupervised models on\nnon-medical images for 29 coarse and 104 detailed anatomical structures in\nvolume and region levels. For volumetric image retrieval, we adopt a late\ninteraction re-ranking method inspired by text matching. We compare it against\nthe original method proposed for volume and region retrieval and achieve a\nretrieval recall of 1.0 for diverse anatomical regions with a wide size range.\nThe findings and methodologies presented in this paper provide insights and\nbenchmarks for further development and evaluation of CBIR approaches in the\ncontext of medical imaging.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "34 pages, 12 Figures, 22 Tables",
    "pdf_url": "http://arxiv.org/pdf/2405.09334v2",
    "published_date": "2024-05-15 13:34:07 UTC",
    "updated_date": "2024-07-04 09:00:32 UTC"
  },
  {
    "arxiv_id": "2405.09595v1",
    "title": "Simplicity within biological complexity",
    "authors": [
      "Natasa Przulj",
      "Noel Malod-Dognin"
    ],
    "abstract": "Heterogeneous, interconnected, systems-level, molecular data have become\nincreasingly available and key in precision medicine. We need to utilize them\nto better stratify patients into risk groups, discover new biomarkers and\ntargets, repurpose known and discover new drugs to personalize medical\ntreatment. Existing methodologies are limited and a paradigm shift is needed to\nachieve quantitative and qualitative breakthroughs. In this perspective paper,\nwe survey the literature and argue for the development of a comprehensive,\ngeneral framework for embedding of multi-scale molecular network data that\nwould enable their explainable exploitation in precision medicine in linear\ntime. Network embedding methods map nodes to points in low-dimensional space,\nso that proximity in the learned space reflects the network's topology-function\nrelationships. They have recently achieved unprecedented performance on hard\nproblems of utilizing few omic data in various biomedical applications.\nHowever, research thus far has been limited to special variants of the problems\nand data, with the performance depending on the underlying topology-function\nnetwork biology hypotheses, the biomedical applications and evaluation metrics.\nThe availability of multi-omic data, modern graph embedding paradigms and\ncompute power call for a creation and training of efficient, explainable and\ncontrollable models, having no potentially dangerous, unexpected behaviour,\nthat make a qualitative breakthrough. We propose to develop a general,\ncomprehensive embedding framework for multi-omic network data, from models to\nefficient and scalable software implementation, and to apply it to biomedical\ninformatics. It will lead to a paradigm shift in computational and biomedical\nunderstanding of data and diseases that will open up ways to solving some of\nthe major bottlenecks in precision medicine and other domains.",
    "categories": [
      "q-bio.OT",
      "cs.AI",
      "I.2; J.3"
    ],
    "primary_category": "q-bio.OT",
    "comment": "29 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09595v1",
    "published_date": "2024-05-15 13:32:45 UTC",
    "updated_date": "2024-05-15 13:32:45 UTC"
  },
  {
    "arxiv_id": "2405.09321v1",
    "title": "ReconBoost: Boosting Can Achieve Modality Reconcilement",
    "authors": [
      "Cong Hua",
      "Qianqian Xu",
      "Shilong Bao",
      "Zhiyong Yang",
      "Qingming Huang"
    ],
    "abstract": "This paper explores a novel multi-modal alternating learning paradigm\npursuing a reconciliation between the exploitation of uni-modal features and\nthe exploration of cross-modal interactions. This is motivated by the fact that\ncurrent paradigms of multi-modal learning tend to explore multi-modal features\nsimultaneously. The resulting gradient prohibits further exploitation of the\nfeatures in the weak modality, leading to modality competition, where the\ndominant modality overpowers the learning process. To address this issue, we\nstudy the modality-alternating learning paradigm to achieve reconcilement.\nSpecifically, we propose a new method called ReconBoost to update a fixed\nmodality each time. Herein, the learning objective is dynamically adjusted with\na reconcilement regularization against competition with the historical models.\nBy choosing a KL-based reconcilement, we show that the proposed method\nresembles Friedman's Gradient-Boosting (GB) algorithm, where the updated\nlearner can correct errors made by others and help enhance the overall\nperformance. The major difference with the classic GB is that we only preserve\nthe newest model for each modality to avoid overfitting caused by ensembling\nstrong learners. Furthermore, we propose a memory consolidation scheme and a\nglobal rectification scheme to make this strategy more effective. Experiments\nover six multi-modal benchmarks speak to the efficacy of the method. We release\nthe code at https://github.com/huacong/ReconBoost.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09321v1",
    "published_date": "2024-05-15 13:22:39 UTC",
    "updated_date": "2024-05-15 13:22:39 UTC"
  },
  {
    "arxiv_id": "2405.09308v1",
    "title": "TimeX++: Learning Time-Series Explanations with Information Bottleneck",
    "authors": [
      "Zichuan Liu",
      "Tianchun Wang",
      "Jimeng Shi",
      "Xu Zheng",
      "Zhuomin Chen",
      "Lei Song",
      "Wenqian Dong",
      "Jayantha Obeysekera",
      "Farhad Shirani",
      "Dongsheng Luo"
    ],
    "abstract": "Explaining deep learning models operating on time series data is crucial in\nvarious applications of interest which require interpretable and transparent\ninsights from time series signals. In this work, we investigate this problem\nfrom an information theoretic perspective and show that most existing measures\nof explainability may suffer from trivial solutions and distributional shift\nissues. To address these issues, we introduce a simple yet practical objective\nfunction for time series explainable learning. The design of the objective\nfunction builds upon the principle of information bottleneck (IB), and modifies\nthe IB objective function to avoid trivial solutions and distributional shift\nissues. We further present TimeX++, a novel explanation framework that\nleverages a parametric network to produce explanation-embedded instances that\nare both in-distributed and label-preserving. We evaluate TimeX++ on both\nsynthetic and real-world datasets comparing its performance against leading\nbaselines, and validate its practical efficacy through case studies in a\nreal-world environmental application. Quantitative and qualitative evaluations\nshow that TimeX++ outperforms baselines across all datasets, demonstrating a\nsubstantial improvement in explanation quality for time series data. The source\ncode is available at \\url{https://github.com/zichuan-liu/TimeXplusplus}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by International Conference on Machine Learning (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.09308v1",
    "published_date": "2024-05-15 13:03:41 UTC",
    "updated_date": "2024-05-15 13:03:41 UTC"
  },
  {
    "arxiv_id": "2407.12135v1",
    "title": "Trustworthy AI in practice: an analysis of practitioners' needs and challenges",
    "authors": [
      "Maria Teresa Baldassarre",
      "Domenico Gigante",
      "Marcos Kalinowski",
      "Azzurra Ragone",
      "Sara Tibidò"
    ],
    "abstract": "Recently, there has been growing attention on behalf of both academic and\npractice communities towards the ability of Artificial Intelligence (AI)\nsystems to operate responsibly and ethically. As a result, a plethora of\nframeworks and guidelines have appeared to support practitioners in\nimplementing Trustworthy AI applications (TAI). However, little research has\nbeen done to investigate whether such frameworks are being used and how. In\nthis work, we study the vision AI practitioners have on TAI principles, how\nthey address them, and what they would like to have - in terms of tools,\nknowledge, or guidelines - when they attempt to incorporate such principles\ninto the systems they develop. Through a survey and semi-structured interviews,\nwe systematically investigated practitioners' challenges and needs in\ndeveloping TAI systems. Based on these practical findings, we highlight\nrecommendations to help AI practitioners develop Trustworthy AI applications.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12135v1",
    "published_date": "2024-05-15 13:02:46 UTC",
    "updated_date": "2024-05-15 13:02:46 UTC"
  },
  {
    "arxiv_id": "2405.09300v1",
    "title": "Comparing the Efficacy of GPT-4 and Chat-GPT in Mental Health Care: A Blind Assessment of Large Language Models for Psychological Support",
    "authors": [
      "Birger Moell"
    ],
    "abstract": "Background: Rapid advancements in natural language processing have led to the\ndevelopment of large language models with the potential to revolutionize mental\nhealth care. These models have shown promise in assisting clinicians and\nproviding support to individuals experiencing various psychological challenges.\n  Objective: This study aims to compare the performance of two large language\nmodels, GPT-4 and Chat-GPT, in responding to a set of 18 psychological prompts,\nto assess their potential applicability in mental health care settings.\n  Methods: A blind methodology was employed, with a clinical psychologist\nevaluating the models' responses without knowledge of their origins. The\nprompts encompassed a diverse range of mental health topics, including\ndepression, anxiety, and trauma, to ensure a comprehensive assessment.\n  Results: The results demonstrated a significant difference in performance\nbetween the two models (p > 0.05). GPT-4 achieved an average rating of 8.29 out\nof 10, while Chat-GPT received an average rating of 6.52. The clinical\npsychologist's evaluation suggested that GPT-4 was more effective at generating\nclinically relevant and empathetic responses, thereby providing better support\nand guidance to potential users.\n  Conclusions: This study contributes to the growing body of literature on the\napplicability of large language models in mental health care settings. The\nfindings underscore the importance of continued research and development in the\nfield to optimize these models for clinical use. Further investigation is\nnecessary to understand the specific factors underlying the performance\ndifferences between the two models and to explore their generalizability across\nvarious populations and mental health conditions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09300v1",
    "published_date": "2024-05-15 12:44:54 UTC",
    "updated_date": "2024-05-15 12:44:54 UTC"
  },
  {
    "arxiv_id": "2405.13021v1",
    "title": "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues",
    "authors": [
      "Diji Yang",
      "Jinmeng Rao",
      "Kezhen Chen",
      "Xiaoyuan Guo",
      "Yawen Zhang",
      "Jie Yang",
      "Yi Zhang"
    ],
    "abstract": "Although the Retrieval-Augmented Generation (RAG) paradigms can use external\nknowledge to enhance and ground the outputs of Large Language Models (LLMs) to\nmitigate generative hallucinations and static knowledge base problems, they\nstill suffer from limited flexibility in adopting Information Retrieval (IR)\nsystems with varying capabilities, constrained interpretability during the\nmulti-round retrieval process, and a lack of end-to-end optimization. To\naddress these challenges, we propose a novel LLM-centric approach, IM-RAG, that\nintegrates IR systems with LLMs to support multi-round RAG through learning\nInner Monologues (IM, i.e., the human inner voice that narrates one's\nthoughts). During the IM process, the LLM serves as the core reasoning model\n(i.e., Reasoner) to either propose queries to collect more information via the\nRetriever or to provide a final answer based on the conversational context. We\nalso introduce a Refiner that improves the outputs from the Retriever,\neffectively bridging the gap between the Reasoner and IR modules with varying\ncapabilities and fostering multi-round communications. The entire IM process is\noptimized via Reinforcement Learning (RL) where a Progress Tracker is\nincorporated to provide mid-step rewards, and the answer prediction is further\nseparately optimized via Supervised Fine-Tuning (SFT). We conduct extensive\nexperiments with the HotPotQA dataset, a popular benchmark for retrieval-based,\nmulti-step question-answering. The results show that our approach achieves\nstate-of-the-art (SOTA) performance while providing high flexibility in\nintegrating IR modules as well as strong interpretability exhibited in the\nlearned inner monologues.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the 47th International ACM SIGIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.13021v1",
    "published_date": "2024-05-15 12:41:20 UTC",
    "updated_date": "2024-05-15 12:41:20 UTC"
  },
  {
    "arxiv_id": "2405.09293v1",
    "title": "Do language models capture implied discourse meanings? An investigation with exhaustivity implicatures of Korean morphology",
    "authors": [
      "Hagyeong Shin",
      "Sean Trott"
    ],
    "abstract": "Markedness in natural language is often associated with non-literal meanings\nin discourse. Differential Object Marking (DOM) in Korean is one instance of\nthis phenomenon, where post-positional markers are selected based on both the\nsemantic features of the noun phrases and the discourse features that are\northogonal to the semantic features. Previous work has shown that\ndistributional models of language recover certain semantic features of words --\ndo these models capture implied discourse-level meanings as well? We evaluate\nwhether a set of large language models are capable of associating discourse\nmeanings with different object markings in Korean. Results suggest that\ndiscourse meanings of a grammatical marker can be more challenging to encode\nthan that of a discourse marker.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the Society for Computation in Linguistics (SCiL)\n  2024, Association for Computational Linguistics (ACL) Anthology",
    "pdf_url": "http://arxiv.org/pdf/2405.09293v1",
    "published_date": "2024-05-15 12:34:40 UTC",
    "updated_date": "2024-05-15 12:34:40 UTC"
  },
  {
    "arxiv_id": "2405.09292v1",
    "title": "Attribute reduction algorithm of rough sets based on spatial optimization",
    "authors": [
      "Xuchang Guo",
      "Houbiao Li"
    ],
    "abstract": "Rough set is one of the important methods for rule acquisition and attribute\nreduction. The current goal of rough set attribute reduction focuses more on\nminimizing the number of reduced attributes, but ignores the spatial similarity\nbetween reduced and decision attributes, which may lead to problems such as\nincreased number of rules and limited generality. In this paper, a rough set\nattribute reduction algorithm based on spatial optimization is proposed. By\nintroducing the concept of spatial similarity, to find the reduction with the\nhighest spatial similarity, so that the spatial similarity between reduction\nand decision attributes is higher, and more concise and widespread rules are\nobtained. In addition, a comparative experiment with the traditional rough set\nattribute reduction algorithms is designed to prove the effectiveness of the\nrough set attribute reduction algorithm based on spatial optimization, which\nhas made significant improvements on many datasets.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2405.09292v1",
    "published_date": "2024-05-15 12:30:19 UTC",
    "updated_date": "2024-05-15 12:30:19 UTC"
  },
  {
    "arxiv_id": "2405.09291v1",
    "title": "Sensitivity Decouple Learning for Image Compression Artifacts Reduction",
    "authors": [
      "Li Ma",
      "Yifan Zhao",
      "Peixi Peng",
      "Yonghong Tian"
    ],
    "abstract": "With the benefit of deep learning techniques, recent researches have made\nsignificant progress in image compression artifacts reduction. Despite their\nimproved performances, prevailing methods only focus on learning a mapping from\nthe compressed image to the original one but ignore the intrinsic attributes of\nthe given compressed images, which greatly harms the performance of downstream\nparsing tasks. Different from these methods, we propose to decouple the\nintrinsic attributes into two complementary features for artifacts\nreduction,ie, the compression-insensitive features to regularize the high-level\nsemantic representations during training and the compression-sensitive features\nto be aware of the compression degree. To achieve this, we first employ\nadversarial training to regularize the compressed and original encoded features\nfor retaining high-level semantics, and we then develop the compression\nquality-aware feature encoder for compression-sensitive features. Based on\nthese dual complementary features, we propose a Dual Awareness Guidance Network\n(DAGN) to utilize these awareness features as transformation guidance during\nthe decoding phase. In our proposed DAGN, we develop a cross-feature fusion\nmodule to maintain the consistency of compression-insensitive features by\nfusing compression-insensitive features into the artifacts reduction baseline.\nOur method achieves an average 2.06 dB PSNR gains on BSD500, outperforming\nstate-of-the-art methods, and only requires 29.7 ms to process one image on\nBSD500. Besides, the experimental results on LIVE1 and LIU4K also demonstrate\nthe efficiency, effectiveness, and superiority of the proposed method in terms\nof quantitative metrics, visual quality, and downstream machine vision tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by Transactions on Image Processing",
    "pdf_url": "http://arxiv.org/pdf/2405.09291v1",
    "published_date": "2024-05-15 12:29:35 UTC",
    "updated_date": "2024-05-15 12:29:35 UTC"
  },
  {
    "arxiv_id": "2405.09593v1",
    "title": "SQL-to-Schema Enhances Schema Linking in Text-to-SQL",
    "authors": [
      "Sun Yang",
      "Qiong Su",
      "Zhishuai Li",
      "Ziyue Li",
      "Hangyu Mao",
      "Chenxi Liu",
      "Rui Zhao"
    ],
    "abstract": "In sophisticated existing Text-to-SQL methods exhibit errors in various\nproportions, including schema-linking errors (incorrect columns, tables, or\nextra columns), join errors, nested errors, and group-by errors. Consequently,\nthere is a critical need to filter out unnecessary tables and columns,\ndirecting the language models attention to relevant tables and columns with\nschema-linking, to reduce errors during SQL generation. Previous approaches\nhave involved sorting tables and columns based on their relevance to the\nquestion, selecting the top-ranked ones for sorting, or directly identifying\nthe necessary tables and columns for SQL generation. However, these methods\nface challenges such as lengthy model training times, high consumption of\nexpensive GPT-4 tokens in few-shot prompts, or suboptimal performance in schema\nlinking. Therefore, we propose an inventive schema linking method in two steps:\nFirstly, generate an initial SQL query by utilizing the complete database\nschema. Subsequently, extract tables and columns from the initial SQL query to\ncreate a concise schema. Using CodeLlama-34B, when comparing the schemas\nobtained by mainstream methods with ours for SQL generation, our schema\nperforms optimally. Leveraging GPT4, our SQL generation method achieved results\nthat are comparable to mainstream Text-to-SQL methods on the Spider dataset.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09593v1",
    "published_date": "2024-05-15 12:22:48 UTC",
    "updated_date": "2024-05-15 12:22:48 UTC"
  },
  {
    "arxiv_id": "2405.09592v1",
    "title": "A Survey of Generative Techniques for Spatial-Temporal Data Mining",
    "authors": [
      "Qianru Zhang",
      "Haixin Wang",
      "Cheng Long",
      "Liangcai Su",
      "Xingwei He",
      "Jianlong Chang",
      "Tailin Wu",
      "Hongzhi Yin",
      "Siu-Ming Yiu",
      "Qi Tian",
      "Christian S. Jensen"
    ],
    "abstract": "This paper focuses on the integration of generative techniques into\nspatial-temporal data mining, considering the significant growth and diverse\nnature of spatial-temporal data. With the advancements in RNNs, CNNs, and other\nnon-generative techniques, researchers have explored their application in\ncapturing temporal and spatial dependencies within spatial-temporal data.\nHowever, the emergence of generative techniques such as LLMs, SSL, Seq2Seq and\ndiffusion models has opened up new possibilities for enhancing spatial-temporal\ndata mining further. The paper provides a comprehensive analysis of generative\ntechnique-based spatial-temporal methods and introduces a standardized\nframework specifically designed for the spatial-temporal data mining pipeline.\nBy offering a detailed review and a novel taxonomy of spatial-temporal\nmethodology utilizing generative techniques, the paper enables a deeper\nunderstanding of the various techniques employed in this field. Furthermore,\nthe paper highlights promising future research directions, urging researchers\nto delve deeper into spatial-temporal data mining. It emphasizes the need to\nexplore untapped opportunities and push the boundaries of knowledge to unlock\nnew insights and improve the effectiveness and efficiency of spatial-temporal\ndata mining. By integrating generative techniques and providing a standardized\nframework, the paper contributes to advancing the field and encourages\nresearchers to explore the vast potential of generative techniques in\nspatial-temporal data mining.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.09592v1",
    "published_date": "2024-05-15 12:07:43 UTC",
    "updated_date": "2024-05-15 12:07:43 UTC"
  },
  {
    "arxiv_id": "2405.09591v3",
    "title": "A Comprehensive Survey on Data Augmentation",
    "authors": [
      "Zaitian Wang",
      "Pengfei Wang",
      "Kunpeng Liu",
      "Pengyang Wang",
      "Yanjie Fu",
      "Chang-Tien Lu",
      "Charu C. Aggarwal",
      "Jian Pei",
      "Yuanchun Zhou"
    ],
    "abstract": "Data augmentation is a series of techniques that generate high-quality\nartificial data by manipulating existing data samples. By leveraging data\naugmentation techniques, AI models can achieve significantly improved\napplicability in tasks involving scarce or imbalanced datasets, thereby\nsubstantially enhancing AI models' generalization capabilities. Existing\nliterature surveys only focus on a certain type of specific modality data, and\ncategorize these methods from modality-specific and operation-centric\nperspectives, which lacks a consistent summary of data augmentation methods\nacross multiple modalities and limits the comprehension of how existing data\nsamples serve the data augmentation process. To bridge this gap, we propose a\nmore enlightening taxonomy that encompasses data augmentation techniques for\ndifferent common data modalities. Specifically, from a data-centric\nperspective, this survey proposes a modality-independent taxonomy by\ninvestigating how to take advantage of the intrinsic relationship between data\nsamples, including single-wise, pair-wise, and population-wise sample data\naugmentation methods. Additionally, we categorize data augmentation methods\nacross five data modalities through a unified inductive approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09591v3",
    "published_date": "2024-05-15 11:58:08 UTC",
    "updated_date": "2025-05-02 03:47:29 UTC"
  },
  {
    "arxiv_id": "2405.09279v1",
    "title": "Sign of the Times: Evaluating the use of Large Language Models for Idiomaticity Detection",
    "authors": [
      "Dylan Phelps",
      "Thomas Pickard",
      "Maggie Mi",
      "Edward Gow-Smith",
      "Aline Villavicencio"
    ],
    "abstract": "Despite the recent ubiquity of large language models and their high zero-shot\nprompted performance across a wide range of tasks, it is still not known how\nwell they perform on tasks which require processing of potentially idiomatic\nlanguage. In particular, how well do such models perform in comparison to\nencoder-only models fine-tuned specifically for idiomaticity tasks? In this\nwork, we attempt to answer this question by looking at the performance of a\nrange of LLMs (both local and software-as-a-service models) on three\nidiomaticity datasets: SemEval 2022 Task 2a, FLUTE, and MAGPIE. Overall, we\nfind that whilst these models do give competitive performance, they do not\nmatch the results of fine-tuned task-specific models, even at the largest\nscales (e.g. for GPT-4). Nevertheless, we do see consistent performance\nimprovements across model scale. Additionally, we investigate prompting\napproaches to improve performance, and discuss the practicalities of using LLMs\nfor these tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at the MWE-UD Workshop at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09279v1",
    "published_date": "2024-05-15 11:55:14 UTC",
    "updated_date": "2024-05-15 11:55:14 UTC"
  },
  {
    "arxiv_id": "2405.09276v2",
    "title": "Dual-Segment Clustering Strategy for Hierarchical Federated Learning in Heterogeneous Wireless Environments",
    "authors": [
      "Pengcheng Sun",
      "Erwu Liu",
      "Wei Ni",
      "Kanglei Yu",
      "Xinyu Qu",
      "Rui Wang",
      "Yanlong Bi",
      "Chuanchun Zhang",
      "Abbas Jamalipour"
    ],
    "abstract": "Non-independent and identically distributed (Non- IID) data adversely affects\nfederated learning (FL) while heterogeneity in communication quality can\nundermine the reliability of model parameter transmission, potentially\ndegrading wireless FL convergence. This paper proposes a novel dual-segment\nclustering (DSC) strategy that jointly addresses communication and data\nheterogeneity in FL. This is achieved by defining a new signal-to-noise ratio\n(SNR) matrix and information quantity matrix to capture the communication and\ndata heterogeneity, respectively. The celebrated affinity propagation algorithm\nis leveraged to iteratively refine the clustering of clients based on the newly\ndefined matrices effectively enhancing model aggregation in heterogeneous\nenvironments. The convergence analysis and experimental results show that the\nDSC strategy can improve the convergence rate of wireless FL and demonstrate\nsuperior accuracy in heterogeneous environments compared to classical\nclustering methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09276v2",
    "published_date": "2024-05-15 11:46:47 UTC",
    "updated_date": "2024-11-14 08:06:46 UTC"
  },
  {
    "arxiv_id": "2405.09266v3",
    "title": "Dance Any Beat: Blending Beats with Visuals in Dance Video Generation",
    "authors": [
      "Xuanchen Wang",
      "Heng Wang",
      "Dongnan Liu",
      "Weidong Cai"
    ],
    "abstract": "Generating dance from music is crucial for advancing automated choreography.\nCurrent methods typically produce skeleton keypoint sequences instead of dance\nvideos and lack the capability to make specific individuals dance, which\nreduces their real-world applicability. These methods also require precise\nkeypoint annotations, complicating data collection and limiting the use of\nself-collected video datasets. To overcome these challenges, we introduce a\nnovel task: generating dance videos directly from images of individuals guided\nby music. This task enables the dance generation of specific individuals\nwithout requiring keypoint annotations, making it more versatile and applicable\nto various situations. Our solution, the Dance Any Beat Diffusion model\n(DabFusion), utilizes a reference image and a music piece to generate dance\nvideos featuring various dance types and choreographies. The music is analyzed\nby our specially designed music encoder, which identifies essential features\nincluding dance style, movement, and rhythm. DabFusion excels in generating\ndance videos not only for individuals in the training dataset but also for any\npreviously unseen person. This versatility stems from its approach of\ngenerating latent optical flow, which contains all necessary motion information\nto animate any person in the image. We evaluate DabFusion's performance using\nthe AIST++ dataset, focusing on video quality, audio-video synchronization, and\nmotion-music alignment. We propose a 2D Motion-Music Alignment Score (2D-MM\nAlign), which builds on the Beat Alignment Score to more effectively evaluate\nmotion-music alignment for this new task. Experiments show that our DabFusion\nestablishes a solid baseline for this innovative task. Video results can be\nfound on our project page: https://DabFusion.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "WACV2025, 11 pages, 7 figures, demo page: https://DabFusion.github.io",
    "pdf_url": "http://arxiv.org/pdf/2405.09266v3",
    "published_date": "2024-05-15 11:33:07 UTC",
    "updated_date": "2024-11-28 10:30:14 UTC"
  },
  {
    "arxiv_id": "2405.13020v1",
    "title": "Using Combinatorial Optimization to Design a High quality LLM Solution",
    "authors": [
      "Samuel Ackerman",
      "Eitan Farchi",
      "Rami Katan",
      "Orna Raz"
    ],
    "abstract": "We introduce a novel LLM based solution design approach that utilizes\ncombinatorial optimization and sampling. Specifically, a set of factors that\ninfluence the quality of the solution are identified. They typically include\nfactors that represent prompt types, LLM inputs alternatives, and parameters\ngoverning the generation and design alternatives. Identifying the factors that\ngovern the LLM solution quality enables the infusion of subject matter expert\nknowledge. Next, a set of interactions between the factors are defined and\ncombinatorial optimization is used to create a small subset $P$ that ensures\nall desired interactions occur in $P$. Each element $p \\in P$ is then developed\ninto an appropriate benchmark. Applying the alternative solutions on each\ncombination, $p \\in P$ and evaluating the results facilitate the design of a\nhigh quality LLM solution pipeline. The approach is especially applicable when\nthe design and evaluation of each benchmark in $P$ is time-consuming and\ninvolves manual steps and human evaluation. Given its efficiency the approach\ncan also be used as a baseline to compare and validate an autoML approach that\nsearches over the factors governing the solution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13020v1",
    "published_date": "2024-05-15 11:13:39 UTC",
    "updated_date": "2024-05-15 11:13:39 UTC"
  },
  {
    "arxiv_id": "2406.16893v1",
    "title": "A Survey on Transformers in NLP with Focus on Efficiency",
    "authors": [
      "Wazib Ansar",
      "Saptarsi Goswami",
      "Amlan Chakrabarti"
    ],
    "abstract": "The advent of transformers with attention mechanisms and associated\npre-trained models have revolutionized the field of Natural Language Processing\n(NLP). However, such models are resource-intensive due to highly complex\narchitecture. This limits their application to resource-constrained\nenvironments. While choosing an appropriate NLP model, a major trade-off exists\nover choosing accuracy over efficiency and vice versa. This paper presents a\ncommentary on the evolution of NLP and its applications with emphasis on their\naccuracy as-well-as efficiency. Following this, a survey of research\ncontributions towards enhancing the efficiency of transformer-based models at\nvarious stages of model development along with hardware considerations has been\nconducted. The goal of this survey is to determine how current NLP techniques\ncontribute towards a sustainable society and to establish a foundation for\nfuture research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16893v1",
    "published_date": "2024-05-15 10:32:41 UTC",
    "updated_date": "2024-05-15 10:32:41 UTC"
  },
  {
    "arxiv_id": "2407.06115v1",
    "title": "Infer Induced Sentiment of Comment Response to Video: A New Task, Dataset and Baseline",
    "authors": [
      "Qi Jia",
      "Baoyu Fan",
      "Cong Xu",
      "Lu Liu",
      "Liang Jin",
      "Guoguang Du",
      "Zhenhua Guo",
      "Yaqian Zhao",
      "Xuanjing Huang",
      "Rengang Li"
    ],
    "abstract": "Existing video multi-modal sentiment analysis mainly focuses on the sentiment\nexpression of people within the video, yet often neglects the induced sentiment\nof viewers while watching the videos. Induced sentiment of viewers is essential\nfor inferring the public response to videos, has broad application in analyzing\npublic societal sentiment, effectiveness of advertising and other areas. The\nmicro videos and the related comments provide a rich application scenario for\nviewers induced sentiment analysis. In light of this, we introduces a novel\nresearch task, Multi-modal Sentiment Analysis for Comment Response of Video\nInduced(MSA-CRVI), aims to inferring opinions and emotions according to the\ncomments response to micro video. Meanwhile, we manually annotate a dataset\nnamed Comment Sentiment toward to Micro Video (CSMV) to support this research.\nIt is the largest video multi-modal sentiment dataset in terms of scale and\nvideo duration to our knowledge, containing 107,267 comments and 8,210 micro\nvideos with a video duration of 68.83 hours. To infer the induced sentiment of\ncomment should leverage the video content, so we propose the Video\nContent-aware Comment Sentiment Analysis (VC-CSA) method as baseline to address\nthe challenges inherent in this new task. Extensive experiments demonstrate\nthat our method is showing significant improvements over other established\nbaselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06115v1",
    "published_date": "2024-05-15 10:24:54 UTC",
    "updated_date": "2024-05-15 10:24:54 UTC"
  },
  {
    "arxiv_id": "2405.09589v4",
    "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models",
    "authors": [
      "Pranab Sahoo",
      "Prabhash Meharia",
      "Akash Ghosh",
      "Sriparna Saha",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "abstract": "The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2405.09589v4",
    "published_date": "2024-05-15 10:16:25 UTC",
    "updated_date": "2024-10-03 09:00:35 UTC"
  },
  {
    "arxiv_id": "2405.09224v1",
    "title": "Perception-Inspired Graph Convolution for Music Understanding Tasks",
    "authors": [
      "Emmanouil Karystinaios",
      "Francesco Foscarin",
      "Gerhard Widmer"
    ],
    "abstract": "We propose a new graph convolutional block, called MusGConv, specifically\ndesigned for the efficient processing of musical score data and motivated by\ngeneral perceptual principles. It focuses on two fundamental dimensions of\nmusic, pitch and rhythm, and considers both relative and absolute\nrepresentations of these components. We evaluate our approach on four different\nmusical understanding problems: monophonic voice separation, harmonic analysis,\ncadence detection, and composer identification which, in abstract terms,\ntranslate to different graph learning problems, namely, node classification,\nlink prediction, and graph classification. Our experiments demonstrate that\nMusGConv improves the performance on three of the aforementioned tasks while\nbeing conceptually very simple and efficient. We interpret this as evidence\nthat it is beneficial to include perception-informed processing of fundamental\nmusical concepts when developing graph network applications on musical score\ndata.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at the 33rd International Joint Conference on Artificial\n  Intelligence (IJCAI-24)",
    "pdf_url": "http://arxiv.org/pdf/2405.09224v1",
    "published_date": "2024-05-15 10:04:44 UTC",
    "updated_date": "2024-05-15 10:04:44 UTC"
  },
  {
    "arxiv_id": "2405.09223v2",
    "title": "Word Alignment as Preference for Machine Translation",
    "authors": [
      "Qiyu Wu",
      "Masaaki Nagata",
      "Zhongtao Miao",
      "Yoshimasa Tsuruoka"
    ],
    "abstract": "The problem of hallucination and omission, a long-standing problem in machine\ntranslation (MT), is more pronounced when a large language model (LLM) is used\nin MT because an LLM itself is susceptible to these phenomena. In this work, we\nmitigate the problem in an LLM-based MT model by guiding it to better word\nalignment. We first study the correlation between word alignment and the\nphenomena of hallucination and omission in MT. Then we propose to utilize word\nalignment as preference to optimize the LLM-based MT model. The preference data\nare constructed by selecting chosen and rejected translations from multiple MT\ntools. Subsequently, direct preference optimization is used to optimize the\nLLM-based model towards the preference signal. Given the absence of evaluators\nspecifically designed for hallucination and omission in MT, we further propose\nselecting hard instances and utilizing GPT-4 to directly evaluate the\nperformance of the models in mitigating these issues. We verify the rationality\nof these designed evaluation methods by experiments, followed by extensive\nresults demonstrating the effectiveness of word alignment-based preference\noptimization to mitigate hallucination and omission. On the other hand,\nalthough it shows promise in mitigating hallucination and omission, the overall\nperformance of MT in different language directions remains mixed, with slight\nincreases in BLEU and decreases in COMET.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2405.09223v2",
    "published_date": "2024-05-15 10:04:19 UTC",
    "updated_date": "2024-11-20 23:06:56 UTC"
  },
  {
    "arxiv_id": "2405.09221v1",
    "title": "Bridging the gap in online hate speech detection: a comparative analysis of BERT and traditional models for homophobic content identification on X/Twitter",
    "authors": [
      "Josh McGiff",
      "Nikola S. Nikolov"
    ],
    "abstract": "Our study addresses a significant gap in online hate speech detection\nresearch by focusing on homophobia, an area often neglected in sentiment\nanalysis research. Utilising advanced sentiment analysis models, particularly\nBERT, and traditional machine learning methods, we developed a nuanced approach\nto identify homophobic content on X/Twitter. This research is pivotal due to\nthe persistent underrepresentation of homophobia in detection models. Our\nfindings reveal that while BERT outperforms traditional methods, the choice of\nvalidation technique can impact model performance. This underscores the\nimportance of contextual understanding in detecting nuanced hate speech. By\nreleasing the largest open-source labelled English dataset for homophobia\ndetection known to us, an analysis of various models' performance and our\nstrongest BERT-based model, we aim to enhance online safety and inclusivity.\nFuture work will extend to broader LGBTQIA+ hate speech detection, addressing\nthe challenges of sourcing diverse datasets. Through this endeavour, we\ncontribute to the larger effort against online hate, advocating for a more\ninclusive digital landscape. Our study not only offers insights into the\neffective detection of homophobic content by improving on previous research\nresults, but it also lays groundwork for future advancements in hate speech\nanalysis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "H.5; I.2; J.5"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, Homophobia detection model available at:\n  https://huggingface.co/JoshMcGiff/homophobiaBERT. The dataset used for this\n  study is available at:\n  https://huggingface.co/datasets/JoshMcGiff/HomophobiaDetectionTwitterX - This\n  paper has been accepted by the 6th International Conference on Computing and\n  Data Science (CONF-CDS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.09221v1",
    "published_date": "2024-05-15 10:02:47 UTC",
    "updated_date": "2024-05-15 10:02:47 UTC"
  },
  {
    "arxiv_id": "2405.09220v3",
    "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models",
    "authors": [
      "Siwei Wang",
      "Yifei Shen",
      "Shi Feng",
      "Haoran Sun",
      "Shang-Hua Teng",
      "Wei Chen"
    ],
    "abstract": "Planning is a crucial element of both human intelligence and contemporary\nlarge language models (LLMs). In this paper, we initiate a theoretical\ninvestigation into the emergence of planning capabilities in Transformer-based\nLLMs via their next-word prediction mechanisms. We model planning as a network\npath-finding task, where the objective is to generate a valid path from a\nspecified source node to a designated target node. Our mathematical\ncharacterization shows that Transformer architectures can execute path-finding\nby embedding the adjacency and reachability matrices within their weights.\nFurthermore, our theoretical analysis of gradient-based learning dynamics\nreveals that LLMs can learn both the adjacency and a limited form of the\nreachability matrices. These theoretical insights are then validated through\nexperiments, which demonstrate that Transformer architectures indeed learn the\nadjacency and an incomplete reachability matrices, consistent with our\ntheoretical predictions. When applying our methodology to the real-world\nplanning benchmark Blocksworld, our observations remain consistent.\nAdditionally, our analyses uncover a fundamental limitation of current\nTransformer architectures in path-finding: these architectures cannot identify\nreachability relationships through transitivity, which leads to failures in\ngenerating paths when concatenation is required. These findings provide new\ninsights into how the internal mechanisms of autoregressive learning facilitate\nintelligent planning and deepen our understanding of how future LLMs might\nachieve more advanced and general planning-and-reasoning capabilities across\ndiverse applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09220v3",
    "published_date": "2024-05-15 09:59:37 UTC",
    "updated_date": "2024-11-11 09:16:56 UTC"
  },
  {
    "arxiv_id": "2405.09215v3",
    "title": "Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model",
    "authors": [
      "Wanting Xu",
      "Yang Liu",
      "Langping He",
      "Xucheng Huang",
      "Ling Jiang"
    ],
    "abstract": "We introduce Xmodel-VLM, a cutting-edge multimodal vision language model. It\nis designed for efficient deployment on consumer GPU servers. Our work directly\nconfronts a pivotal industry issue by grappling with the prohibitive service\ncosts that hinder the broad adoption of large-scale multimodal systems. Through\nrigorous training, we have developed a 1B-scale language model from the ground\nup, employing the LLaVA paradigm for modal alignment. The result, which we call\nXmodel-VLM, is a lightweight yet powerful multimodal vision language model.\nExtensive testing across numerous classic multimodal benchmarks has revealed\nthat despite its smaller size and faster execution, Xmodel-VLM delivers\nperformance comparable to that of larger models. Our model checkpoints and code\nare publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09215v3",
    "published_date": "2024-05-15 09:47:59 UTC",
    "updated_date": "2024-06-20 07:31:13 UTC"
  },
  {
    "arxiv_id": "2405.09588v1",
    "title": "Training Deep Learning Models with Hybrid Datasets for Robust Automatic Target Detection on real SAR images",
    "authors": [
      "Benjamin Camus",
      "Théo Voillemin",
      "Corentin Le Barbu",
      "Jean-Christophe Louvigné",
      "Carole Belloni",
      "Emmanuel Vallée"
    ],
    "abstract": "In this work, we propose to tackle several challenges hindering the\ndevelopment of Automatic Target Detection (ATD) algorithms for ground targets\nin SAR images. To address the lack of representative training data, we propose\na Deep Learning approach to train ATD models with synthetic target signatures\nproduced with the MOCEM simulator. We define an incrustation pipeline to\nincorporate synthetic targets into real backgrounds. Using this hybrid dataset,\nwe train ATD models specifically tailored to bridge the domain gap between\nsynthetic and real data. Our approach notably relies on massive physics-based\ndata augmentation techniques and Adversarial Training of two deep-learning\ndetection architectures. We then test these models on several datasets,\nincluding (1) patchworks of real SAR images, (2) images with the incrustation\nof real targets in real backgrounds, and (3) images with the incrustation of\nsynthetic background objects in real backgrounds. Results show that the\nproduced hybrid datasets are exempt from image overlay bias. Our approach can\nreach up to 90% of Average Precision on real data while exclusively using\nsynthetic targets for training.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09588v1",
    "published_date": "2024-05-15 09:26:24 UTC",
    "updated_date": "2024-05-15 09:26:24 UTC"
  },
  {
    "arxiv_id": "2405.10343v1",
    "title": "UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning",
    "authors": [
      "Shikun Feng",
      "Yuyan Ni",
      "Minghao Li",
      "Yanwen Huang",
      "Zhi-Ming Ma",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ],
    "abstract": "Recently, a noticeable trend has emerged in developing pre-trained foundation\nmodels in the domains of CV and NLP. However, for molecular pre-training, there\nlacks a universal model capable of effectively applying to various categories\nof molecular tasks, since existing prevalent pre-training methods exhibit\neffectiveness for specific types of downstream tasks. Furthermore, the lack of\nprofound understanding of existing pre-training methods, including 2D graph\nmasking, 2D-3D contrastive learning, and 3D denoising, hampers the advancement\nof molecular foundation models. In this work, we provide a unified\ncomprehension of existing pre-training methods through the lens of contrastive\nlearning. Thus their distinctions lie in clustering different views of\nmolecules, which is shown beneficial to specific downstream tasks. To achieve a\ncomplete and general-purpose molecular representation, we propose a novel\npre-training framework, named UniCorn, that inherits the merits of the three\nmethods, depicting molecular views in three different levels. SOTA performance\nacross quantum, physicochemical, and biological tasks, along with comprehensive\nablation study, validate the universality and effectiveness of UniCorn.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10343v1",
    "published_date": "2024-05-15 09:20:02 UTC",
    "updated_date": "2024-05-15 09:20:02 UTC"
  },
  {
    "arxiv_id": "2405.09190v1",
    "title": "Advancing Explainable AI with Causal Analysis in Large-Scale Fuzzy Cognitive Maps",
    "authors": [
      "Marios Tyrovolas",
      "Nikolaos D. Kallimanis",
      "Chrysostomos Stylios"
    ],
    "abstract": "In the quest for accurate and interpretable AI models, eXplainable AI (XAI)\nhas become crucial. Fuzzy Cognitive Maps (FCMs) stand out as an advanced XAI\nmethod because of their ability to synergistically combine and exploit both\nexpert knowledge and data-driven insights, providing transparency and intrinsic\ninterpretability. This letter introduces and investigates the \"Total Causal\nEffect Calculation for FCMs\" (TCEC-FCM) algorithm, an innovative approach that,\nfor the first time, enables the efficient calculation of total causal effects\namong concepts in large-scale FCMs by leveraging binary search and graph\ntraversal techniques, thereby overcoming the challenge of exhaustive causal\npath exploration that hinder existing methods. We evaluate the proposed method\nacross various synthetic FCMs that demonstrate TCEC-FCM's superior performance\nover exhaustive methods, marking a significant advancement in causal effect\nanalysis within FCMs, thus broadening their usability for modern complex XAI\napplications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09190v1",
    "published_date": "2024-05-15 08:53:47 UTC",
    "updated_date": "2024-05-15 08:53:47 UTC"
  },
  {
    "arxiv_id": "2405.09586v2",
    "title": "Factual Serialization Enhancement: A Key Innovation for Chest X-ray Report Generation",
    "authors": [
      "Kang Liu",
      "Zhuoqi Ma",
      "Mengmeng Liu",
      "Zhicheng Jiao",
      "Xiaolu Kang",
      "Qiguang Miao",
      "Kun Xie"
    ],
    "abstract": "A radiology report comprises presentation-style vocabulary, which ensures\nclarity and organization, and factual vocabulary, which provides accurate and\nobjective descriptions based on observable findings. While manually writing\nthese reports is time-consuming and labor-intensive, automatic report\ngeneration offers a promising alternative. A critical step in this process is\nto align radiographs with their corresponding reports. However, existing\nmethods often rely on complete reports for alignment, overlooking the impact of\npresentation-style vocabulary. To address this issue, we propose FSE, a\ntwo-stage Factual Serialization Enhancement method. In Stage 1, we introduce\nfactuality-guided contrastive learning for visual representation by maximizing\nthe semantic correspondence between radiographs and corresponding factual\ndescriptions. In Stage 2, we present evidence-driven report generation that\nenhances diagnostic accuracy by integrating insights from similar historical\ncases structured as factual serialization. Experiments on MIMIC-CXR and IU\nX-ray datasets across specific and general scenarios demonstrate that FSE\noutperforms state-of-the-art approaches in both natural language generation and\nclinical efficacy metrics. Ablation studies further emphasize the positive\neffects of factual serialization in Stage 1 and Stage 2. The code is available\nat https://github.com/mk-runner/FSE.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "code is available at https://github.com/mk-runner/FSE",
    "pdf_url": "http://arxiv.org/pdf/2405.09586v2",
    "published_date": "2024-05-15 07:56:38 UTC",
    "updated_date": "2024-09-12 03:11:41 UTC"
  },
  {
    "arxiv_id": "2405.13019v2",
    "title": "A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models",
    "authors": [
      "Mahsa Khoshnoodi",
      "Vinija Jain",
      "Mingye Gao",
      "Malavika Srikanth",
      "Aman Chadha"
    ],
    "abstract": "Despite the crucial importance of accelerating text generation in large\nlanguage models (LLMs) for efficiently producing content, the sequential nature\nof this process often leads to high inference latency, posing challenges for\nreal-time applications. Various techniques have been proposed and developed to\naddress these challenges and improve efficiency. This paper presents a\ncomprehensive survey of accelerated generation techniques in autoregressive\nlanguage models, aiming to understand the state-of-the-art methods and their\napplications. We categorize these techniques into several key areas:\nspeculative decoding, early exiting mechanisms, and non-autoregressive methods.\nWe discuss each category's underlying principles, advantages, limitations, and\nrecent advancements. Through this survey, we aim to offer insights into the\ncurrent landscape of techniques in LLMs and provide guidance for future\nresearch directions in this critical area of natural language processing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13019v2",
    "published_date": "2024-05-15 07:36:56 UTC",
    "updated_date": "2024-05-24 07:40:27 UTC"
  },
  {
    "arxiv_id": "2405.09585v3",
    "title": "An Embarrassingly Simple Approach to Enhance Transformer Performance in Genomic Selection for Crop Breeding",
    "authors": [
      "Renqi Chen",
      "Wenwei Han",
      "Haohao Zhang",
      "Haoyang Su",
      "Zhefan Wang",
      "Xiaolei Liu",
      "Hao Jiang",
      "Wanli Ouyang",
      "Nanqing Dong"
    ],
    "abstract": "Genomic selection (GS), as a critical crop breeding strategy, plays a key\nrole in enhancing food production and addressing the global hunger crisis. The\npredominant approaches in GS currently revolve around employing statistical\nmethods for prediction. However, statistical methods often come with two main\nlimitations: strong statistical priors and linear assumptions. A recent trend\nis to capture the non-linear relationships between markers by deep learning.\nHowever, as crop datasets are commonly long sequences with limited samples, the\nrobustness of deep learning models, especially Transformers, remains a\nchallenge. In this work, to unleash the unexplored potential of attention\nmechanism for the task of interest, we propose a simple yet effective\nTransformer-based framework that enables end-to-end training of the whole\nsequence. Via experiments on rice3k and wheat3k datasets, we show that, with\nsimple tricks such as k-mer tokenization and random masking, Transformer can\nachieve overall superior performance against seminal methods on GS tasks of\ninterest.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI2024. Code is available at\n  https://github.com/RenqiChen/Genomic-Selection",
    "pdf_url": "http://arxiv.org/pdf/2405.09585v3",
    "published_date": "2024-05-15 07:31:06 UTC",
    "updated_date": "2024-06-24 09:56:35 UTC"
  },
  {
    "arxiv_id": "2405.13018v1",
    "title": "Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings",
    "authors": [
      "Ahmed Adel Attia",
      "Dorottya Demszky",
      "Tolulope Ogunremi",
      "Jing Liu",
      "Carol Espy-Wilson"
    ],
    "abstract": "Creating Automatic Speech Recognition (ASR) systems that are robust and\nresilient to classroom conditions is paramount to the development of AI tools\nto aid teachers and students. In this work, we study the efficacy of continued\npretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that\nCPT is a powerful tool in that regard and reduces the Word Error Rate (WER) of\nWav2vec2.0-based models by upwards of 10%. More specifically, CPT improves the\nmodel's robustness to different noises, microphones, classroom conditions as\nwell as classroom demographics. Our CPT models show improved ability to\ngeneralize to different demographics unseen in the labeled finetuning data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13018v1",
    "published_date": "2024-05-15 06:59:33 UTC",
    "updated_date": "2024-05-15 06:59:33 UTC"
  },
  {
    "arxiv_id": "2405.09125v1",
    "title": "HAAP: Vision-context Hierarchical Attention Autoregressive with Adaptive Permutation for Scene Text Recognition",
    "authors": [
      "Honghui Chen",
      "Yuhang Qiu",
      "Jiabao Wang",
      "Pingping Chen",
      "Nam Ling"
    ],
    "abstract": "Internal Language Model (LM)-based methods use permutation language modeling\n(PLM) to solve the error correction caused by conditional independence in\nexternal LM-based methods. However, random permutations of human interference\ncause fit oscillations in the model training, and Iterative Refinement (IR)\noperation to improve multimodal information decoupling also introduces\nadditional overhead. To address these issues, this paper proposes the\nHierarchical Attention autoregressive Model with Adaptive Permutation (HAAP) to\nenhance the location-context-image interaction capability, improving\nautoregressive generalization with internal LM. First, we propose Implicit\nPermutation Neurons (IPN) to generate adaptive attention masks to dynamically\nexploit token dependencies. The adaptive masks increase the diversity of\ntraining data and prevent model dependency on a specific order. It reduces the\ntraining overhead of PLM while avoiding training fit oscillations. Second, we\ndevelop Cross-modal Hierarchical Attention mechanism (CHA) to couple context\nand image features. This processing establishes rich positional semantic\ndependencies between context and image while avoiding IR. Extensive\nexperimental results show the proposed HAAP achieves state-of-the-art (SOTA)\nperformance in terms of accuracy, complexity, and latency on several datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T01",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09125v1",
    "published_date": "2024-05-15 06:41:43 UTC",
    "updated_date": "2024-05-15 06:41:43 UTC"
  },
  {
    "arxiv_id": "2405.09118v2",
    "title": "BonnBot-I Plus: A Bio-diversity Aware Precise Weed Management Robotic Platform",
    "authors": [
      "Alireza Ahmadi",
      "Michael Halstead",
      "Claus Smitt",
      "Chris McCool"
    ],
    "abstract": "In this article, we focus on the critical tasks of plant protection in arable\nfarms, addressing a modern challenge in agriculture: integrating ecological\nconsiderations into the operational strategy of precision weeding robots like\n\\bbot. This article presents the recent advancements in weed management\nalgorithms and the real-world performance of \\bbot\\ at the University of Bonn's\nKlein-Altendorf campus. We present a novel Rolling-view observation model for\nthe BonnBot-Is weed monitoring section which leads to an average absolute\nweeding performance enhancement of $3.4\\%$. Furthermore, for the first time, we\nshow how precision weeding robots could consider bio-diversity-aware concerns\nin challenging weeding scenarios. We carried out comprehensive weeding\nexperiments in sugar-beet fields, covering both weed-only and mixed crop-weed\nsituations, and introduced a new dataset compatible with precision weeding. Our\nreal-field experiments revealed that our weeding approach is capable of\nhandling diverse weed distributions, with a minimal loss of only $11.66\\%$\nattributable to intervention planning and $14.7\\%$ to vision system limitations\nhighlighting required improvements of the vision system.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09118v2",
    "published_date": "2024-05-15 06:23:59 UTC",
    "updated_date": "2024-07-04 20:49:51 UTC"
  },
  {
    "arxiv_id": "2405.09111v2",
    "title": "CarDreamer: Open-Source Learning Platform for World Model based Autonomous Driving",
    "authors": [
      "Dechen Gao",
      "Shuangyu Cai",
      "Hanchu Zhou",
      "Hang Wang",
      "Iman Soltani",
      "Junshan Zhang"
    ],
    "abstract": "To safely navigate intricate real-world scenarios, autonomous vehicles must\nbe able to adapt to diverse road conditions and anticipate future events. World\nmodel (WM) based reinforcement learning (RL) has emerged as a promising\napproach by learning and predicting the complex dynamics of various\nenvironments. Nevertheless, to the best of our knowledge, there does not exist\nan accessible platform for training and testing such algorithms in\nsophisticated driving environments. To fill this void, we introduce CarDreamer,\nthe first open-source learning platform designed specifically for developing WM\nbased autonomous driving algorithms. It comprises three key components: 1)\nWorld model backbone: CarDreamer has integrated some state-of-the-art WMs,\nwhich simplifies the reproduction of RL algorithms. The backbone is decoupled\nfrom the rest and communicates using the standard Gym interface, so that users\ncan easily integrate and test their own algorithms. 2) Built-in tasks:\nCarDreamer offers a comprehensive set of highly configurable driving tasks\nwhich are compatible with Gym interfaces and are equipped with empirically\noptimized reward functions. 3) Task development suite: This suite streamlines\nthe creation of driving tasks, enabling easy definition of traffic flows and\nvehicle routes, along with automatic collection of multi-modal observation\ndata. A visualization server allows users to trace real-time agent driving\nvideos and performance metrics through a browser. Furthermore, we conduct\nextensive experiments using built-in tasks to evaluate the performance and\npotential of WMs in autonomous driving. Thanks to the richness and flexibility\nof CarDreamer, we also systematically study the impact of observation modality,\nobservability, and sharing of vehicle intentions on AV safety and efficiency.\nAll code and documents are accessible on\nhttps://github.com/ucd-dare/CarDreamer.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Dechen Gao, Shuangyu Cai, Hanchu Zhou, Hang Wang contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2405.09111v2",
    "published_date": "2024-05-15 05:57:20 UTC",
    "updated_date": "2024-07-25 23:02:27 UTC"
  },
  {
    "arxiv_id": "2405.09109v2",
    "title": "Motion Prediction with Gaussian Processes for Safe Human-Robot Interaction in Virtual Environments",
    "authors": [
      "Stanley Mugisha",
      "Vamsi Krishna Guda",
      "Christine Chevallereau",
      "Damien Chablat",
      "Matteo Zoppi"
    ],
    "abstract": "Humans use collaborative robots as tools for accomplishing various tasks. The\ninteraction between humans and robots happens in tight shared workspaces.\nHowever, these machines must be safe to operate alongside humans to minimize\nthe risk of accidental collisions. Ensuring safety imposes many constraints,\nsuch as reduced torque and velocity limits during operation, thus increasing\nthe time to accomplish many tasks. However, for applications such as using\ncollaborative robots as haptic interfaces with intermittent contacts for\nvirtual reality applications, speed limitations result in poor user\nexperiences. This research aims to improve the efficiency of a collaborative\nrobot while improving the safety of the human user. We used Gaussian process\nmodels to predict human hand motion and developed strategies for human\nintention detection based on hand motion and gaze to improve the time for the\nrobot and human security in a virtual environment. We then studied the effect\nof prediction. Results from comparisons show that the prediction models\nimproved the robot time by 3\\% and safety by 17\\%. When used alongside gaze,\nprediction with Gaussian process models resulted in an improvement of the robot\ntime by 2\\% and the safety by 13\\%.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.9; I.3.2; H.5.2"
    ],
    "primary_category": "cs.RO",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.09109v2",
    "published_date": "2024-05-15 05:51:41 UTC",
    "updated_date": "2024-05-18 17:47:42 UTC"
  },
  {
    "arxiv_id": "2405.09086v1",
    "title": "Chaos-based reinforcement learning with TD3",
    "authors": [
      "Toshitaka Matsuki",
      "Yusuke Sakemi",
      "Kazuyuki Aihara"
    ],
    "abstract": "Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. This approach offers a model for\nconsidering how the biological brain can create variability in its behavior and\nlearn in an exploratory manner. At the same time, it is a learning model that\nhas the ability to automatically switch between exploration and exploitation\nmodes and the potential to realize higher explorations that reflect what it has\nlearned so far. However, the learning algorithms in CBRL have not been\nwell-established in previous studies and have yet to incorporate recent\nadvances in reinforcement learning. This study introduced Twin Delayed Deep\nDeterministic Policy Gradients (TD3), which is one of the state-of-the-art deep\nreinforcement learning algorithms that can treat deterministic and continuous\naction spaces, to CBRL. The validation results provide several insights. First,\nTD3 works as a learning algorithm for CBRL in a simple goal-reaching task.\nSecond, CBRL agents with TD3 can autonomously suppress their exploratory\nbehavior as learning progresses and resume exploration when the environment\nchanges. Finally, examining the effect of the agent's chaoticity on learning\nshows that extremely strong chaos negatively impacts the flexible switching\nbetween exploration and exploitation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09086v1",
    "published_date": "2024-05-15 04:47:31 UTC",
    "updated_date": "2024-05-15 04:47:31 UTC"
  },
  {
    "arxiv_id": "2405.09081v2",
    "title": "Explainable AI for Ship Collision Avoidance: Decoding Decision-Making Processes and Behavioral Intentions",
    "authors": [
      "Hitoshi Yoshioka",
      "Hirotada Hashimoto"
    ],
    "abstract": "This study developed an explainable AI for ship collision avoidance.\nInitially, a critic network composed of sub-task critic networks was proposed\nto individually evaluate each sub-task in collision avoidance to clarify the AI\ndecision-making processes involved. Additionally, an attempt was made to\ndiscern behavioral intentions through a Q-value analysis and an Attention\nmechanism. The former focused on interpreting intentions by examining the\nincrement of the Q-value resulting from AI actions, while the latter\nincorporated the significance of other ships in the decision-making process for\ncollision avoidance into the learning objective. AI's behavioral intentions in\ncollision avoidance were visualized by combining the perceived collision danger\nwith the degree of attention to other ships. The proposed method was evaluated\nthrough a numerical experiment. The developed AI was confirmed to be able to\nsafely avoid collisions under various congestion levels, and AI's\ndecision-making process was rendered comprehensible to humans. The proposed\nmethod not only facilitates the understanding of DRL-based controllers/systems\nin the ship collision avoidance task but also extends to any task comprising\nsub-tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "24 pases and 15 figures. If you need the program, please contuct us",
    "pdf_url": "http://arxiv.org/pdf/2405.09081v2",
    "published_date": "2024-05-15 04:09:46 UTC",
    "updated_date": "2024-05-20 02:31:16 UTC"
  },
  {
    "arxiv_id": "2405.09056v1",
    "title": "CTS: A Consistency-Based Medical Image Segmentation Model",
    "authors": [
      "Kejia Zhang",
      "Lan Zhang",
      "Haiwei Pan",
      "Baolong Yu"
    ],
    "abstract": "In medical image segmentation tasks, diffusion models have shown significant\npotential. However, mainstream diffusion models suffer from drawbacks such as\nmultiple sampling times and slow prediction results. Recently, consistency\nmodels, as a standalone generative network, have resolved this issue. Compared\nto diffusion models, consistency models can reduce the sampling times to once,\nnot only achieving similar generative effects but also significantly speeding\nup training and prediction. However, they are not suitable for image\nsegmentation tasks, and their application in the medical imaging field has not\nyet been explored. Therefore, this paper applies the consistency model to\nmedical image segmentation tasks, designing multi-scale feature signal\nsupervision modes and loss function guidance to achieve model convergence.\nExperiments have verified that the CTS model can obtain better medical image\nsegmentation results with a single sampling during the test phase.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09056v1",
    "published_date": "2024-05-15 03:07:42 UTC",
    "updated_date": "2024-05-15 03:07:42 UTC"
  },
  {
    "arxiv_id": "2405.09049v2",
    "title": "Perception Without Vision for Trajectory Prediction: Ego Vehicle Dynamics as Scene Representation for Efficient Active Learning in Autonomous Driving",
    "authors": [
      "Ross Greer",
      "Mohan Trivedi"
    ],
    "abstract": "This study investigates the use of trajectory and dynamic state information\nfor efficient data curation in autonomous driving machine learning tasks. We\npropose methods for clustering trajectory-states and sampling strategies in an\nactive learning framework, aiming to reduce annotation and data costs while\nmaintaining model performance. Our approach leverages trajectory information to\nguide data selection, promoting diversity in the training data. We demonstrate\nthe effectiveness of our methods on the trajectory prediction task using the\nnuScenes dataset, showing consistent performance gains over random sampling\nacross different data pool sizes, and even reaching sub-baseline displacement\nerrors at just 50% of the data cost. Our results suggest that sampling typical\ndata initially helps overcome the ''cold start problem,'' while introducing\nnovelty becomes more beneficial as the training pool size increases. By\nintegrating trajectory-state-informed active learning, we demonstrate that more\nefficient and robust autonomous driving systems are possible and practical\nusing low-cost data curation strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09049v2",
    "published_date": "2024-05-15 02:54:11 UTC",
    "updated_date": "2024-05-20 10:52:46 UTC"
  },
  {
    "arxiv_id": "2405.09037v1",
    "title": "Unmasking Efficiency: Learning Salient Sparse Models in Non-IID Federated Learning",
    "authors": [
      "Riyasat Ohib",
      "Bishal Thapaliya",
      "Gintare Karolina Dziugaite",
      "Jingyu Liu",
      "Vince Calhoun",
      "Sergey Plis"
    ],
    "abstract": "In this work, we propose Salient Sparse Federated Learning (SSFL), a\nstreamlined approach for sparse federated learning with efficient\ncommunication. SSFL identifies a sparse subnetwork prior to training,\nleveraging parameter saliency scores computed separately on local client data\nin non-IID scenarios, and then aggregated, to determine a global mask. Only the\nsparse model weights are communicated each round between the clients and the\nserver. We validate SSFL's effectiveness using standard non-IID benchmarks,\nnoting marked improvements in the sparsity--accuracy trade-offs. Finally, we\ndeploy our method in a real-world federated learning framework and report\nimprovement in communication time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09037v1",
    "published_date": "2024-05-15 02:13:51 UTC",
    "updated_date": "2024-05-15 02:13:51 UTC"
  }
]
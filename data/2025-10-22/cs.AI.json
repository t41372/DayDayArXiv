{
  "date": "2025-10-22",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-22 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv ä¹Ÿæ˜¯ç¥ä»™æ‰“æ¶çš„ä¸€å¤©ã€‚**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰** é¢†åŸŸè¿æ¥äº†çˆ†å‘ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ **Computer Useï¼ˆæ“ä½œè®¡ç®—æœºï¼‰** çš„é€šç”¨æ™ºèƒ½ä½“ï¼ˆSurfer 2ï¼‰å’Œä»è§†é¢‘ä¸­å¤§è§„æ¨¡æŒ–æ˜è®­ç»ƒæ•°æ®çš„æ–¹æ³•ï¼ˆVideoAgentTrekï¼‰ä»¤äººç©ç›®ã€‚**æ¨ç†ï¼ˆReasoningï¼‰** æ–¹é¢ï¼Œå¦‚ä½•è§£å†³ LLM çš„â€œæ€è€ƒä¸è¶³â€ï¼ˆUnderthinkingï¼‰å’Œâ€œå­¦ä¹ æ–­å´–â€æ˜¯çƒ­ç‚¹ï¼Œæ¶Œç°äº† Scaf-GRPO å’Œ SmartSwitch ç­‰ä¼˜åŒ–æ¡†æ¶ã€‚æ­¤å¤–ï¼Œä¸€ç¯‡å…³äº **LLM éšå†™æœ¯** çš„è®ºæ–‡å±•ç¤ºäº†æƒŠäººçš„å®‰å…¨æ€§éšæ‚£â€”â€”åœ¨ç­‰é•¿æ–‡æœ¬ä¸­å®Œç¾éšè—å¦ä¸€æ®µæ–‡æœ¬ã€‚æœ€åï¼Œè¿˜æœ‰ä¸€ç¯‡ 37 é¡µçš„é•¿æ–‡æ·±æƒ…å‘¼å”¤ AI å›å½’ç»Ÿè®¡å­¦æœ¬è´¨ã€‚\n\n---\n\n### ğŸŒŸ å¿…è¯»ç„¦ç‚¹ï¼šéšå†™æœ¯ã€Agent ä¸æ¨ç†ä¼˜åŒ–\n\n#### 1. **LLMs can hide text in other text of the same length**\n**LLM å¯ä»¥åœ¨ç›¸åŒé•¿åº¦çš„å…¶ä»–æ–‡æœ¬ä¸­éšè—æ–‡æœ¬**\n*   **æ ¸å¿ƒçˆ†ç‚¹**ï¼šè¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†ä¸€ç§æƒŠäººçš„èƒ½åŠ›ï¼šåˆ©ç”¨ 8B å‚æ•°çš„å¼€æºæ¨¡å‹ï¼Œå¯ä»¥å°†ä¸€æ®µæ•æ„Ÿæ–‡æœ¬ï¼ˆå¦‚æ¿€è¿›çš„æ”¿æ²»è¨€è®ºï¼‰å®Œç¾éšè—åœ¨å¦ä¸€æ®µ**ç­‰é•¿**ã€è¯­ä¹‰é€šé¡ºä¸”çœ‹ä¼¼æ— å…³çš„æ–‡æœ¬ï¼ˆå¦‚äº§å“è¯„è®ºï¼‰ä¸­ã€‚\n*   **æ–¹æ³•**ï¼šä½œè€…æå‡ºäº† **Calgacus** åè®®ï¼Œå®ç°äº†æ–‡æœ¬ä¸ä½œè€…æ„å›¾çš„å½»åº•è§£è€¦ã€‚ä¸ä¼ ç»Ÿçš„éšå†™æœ¯ä¸åŒï¼Œå®ƒä¸éœ€è¦å¢åŠ è½½ä½“é•¿åº¦ï¼Œç”šè‡³å¯ä»¥åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šç§’çº§å®Œæˆç¼–ç è§£ç ã€‚\n*   **Implication**ï¼šè¿™å¯¹ AI å®‰å…¨æ„æˆäº†å·¨å¤§æŒ‘æˆ˜ã€‚å…¬å¸å¯èƒ½åœ¨çœ‹ä¼¼åˆè§„çš„å›å¤ä¸­é€šè¿‡éšå†™æœ¯ä¼ è¾“æ¶æ„æŒ‡ä»¤ï¼Œç›®å‰çš„ç›‘æ§æ‰‹æ®µæéš¾å¯Ÿè§‰ã€‚\n\n#### 20. **Surfer 2: The Next Generation of Cross-Platform Computer Use Agents**\n**Surfer 2ï¼šä¸‹ä¸€ä»£è·¨å¹³å°è®¡ç®—æœºæ“ä½œæ™ºèƒ½ä½“**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† Surfer 2ï¼Œä¸€ä¸ªçº¯è§†è§‰é©±åŠ¨çš„é€šç”¨è®¡ç®—æœºæ§åˆ¶æ™ºèƒ½ä½“ã€‚å®ƒä¸éœ€è¦é’ˆå¯¹ç‰¹å®šç¯å¢ƒå¾®è°ƒï¼Œå°±èƒ½åœ¨ Webã€æ¡Œé¢ OS å’Œç§»åŠ¨ç«¯ï¼ˆAndroidï¼‰ä¸Šæ“ä½œã€‚\n*   **æ€§èƒ½**ï¼šåœ¨ WebVoyager ä¸Šè¾¾åˆ° 97.1%ï¼Œåœ¨ OSWorld ä¸Šè¾¾åˆ° 60.1%ï¼Œå¤šæ¬¡å°è¯•åå‡è¶…è¶Šäººç±»è¡¨ç°ã€‚\n*   **æŠ€æœ¯ç‚¹**ï¼šé›†æˆäº†åˆ†å±‚ä¸Šä¸‹æ–‡ç®¡ç†ã€è§£è€¦çš„è§„åˆ’ä¸æ‰§è¡Œã€ä»¥åŠå¸¦æœ‰è‡ªé€‚åº”æ¢å¤çš„è‡ªæˆ‘éªŒè¯æœºåˆ¶ã€‚è¿™æ˜¯å‘é€šç”¨è®¡ç®—æœºæ§åˆ¶è¿ˆå‡ºçš„ä¸€å¤§æ­¥ã€‚\n\n#### 74. **VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos**\n**VideoAgentTrekï¼šåŸºäºæœªæ ‡è®°è§†é¢‘çš„è®¡ç®—æœºæ“ä½œé¢„è®­ç»ƒ**\n*   **ç—›ç‚¹è§£å†³**ï¼šè®­ç»ƒ Computer Use Agent éœ€è¦æµ·é‡ GUI è½¨è¿¹æ•°æ®ï¼Œæ ‡æ³¨å¤ªè´µã€‚\n*   **æ–¹æ³•**ï¼šä½œè€…åˆ©ç”¨ **Video2Action** é€†åŠ¨åŠ›å­¦æ¨¡å—ï¼Œä» 39,000 ä¸ª YouTube æ•™ç¨‹è§†é¢‘ä¸­è‡ªåŠ¨æŒ–æ˜äº† 152 ä¸‡æ­¥äº¤äº’æ•°æ®ï¼ˆåŒ…æ‹¬ç‚¹å‡»åæ ‡ã€è¾“å…¥æ–‡æœ¬ï¼‰ã€‚\n*   **æ•ˆæœ**ï¼šè¿™ç§â€œè¢«åŠ¨è§‚çœ‹äº’è”ç½‘è§†é¢‘â€çš„é¢„è®­ç»ƒæ–¹å¼ï¼Œè®© Agent åœ¨ OSWorld ä¸Šçš„æˆåŠŸç‡ç›¸å¯¹æå‡äº† 70%ã€‚\n\n#### 26. **Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning**\n**Scaf-GRPOï¼šå¢å¼º LLM æ¨ç†çš„è„šæ‰‹æ¶å¼ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„â€œå­¦ä¹ æ–­å´–â€é—®é¢˜ï¼ˆå³é¢å¯¹å¤ªéš¾çš„é—®é¢˜ï¼Œæ¨¡å‹å¾—ä¸åˆ°ä»»ä½•å¥–åŠ±ï¼Œæ— æ³•å­¦ä¹ ï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šå½“æ¨¡å‹å­¦ä¹ åœæ»æ—¶ï¼Œå¼•å…¥â€œè„šæ‰‹æ¶â€â€”â€”åˆ†å±‚çš„ prompt æç¤ºï¼ˆä»æŠ½è±¡æ¦‚å¿µåˆ°å…·ä½“æ­¥éª¤ï¼‰ï¼Œå¸®åŠ©æ¨¡å‹æ„å»ºæœ‰æ•ˆè§£ï¼Œç„¶åå†æ’¤å»è¾…åŠ©ã€‚\n*   **å‘ç°**ï¼šåœ¨ AIME24 æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ Qwen2.5-Math-7B çš„ pass@1 æå‡äº† 44.3%ã€‚\n\n#### 34. **SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration**\n**SmartSwitchï¼šé€šè¿‡ä¿ƒè¿›æ·±åº¦æ€ç»´æ¢ç´¢å…‹æœâ€œæ€è€ƒä¸è¶³â€ä»¥æå‡ LLM æ¨ç†**\n*   **é—®é¢˜**ï¼šLong-CoTï¼ˆé•¿æ€ç»´é“¾ï¼‰è™½ç„¶å¥½ï¼Œä½†æ¨¡å‹æœ‰æ—¶ä¼šâ€œæµ…å°è¾„æ­¢â€ï¼ˆUnderthinkingï¼‰ï¼Œé¢‘ç¹åˆ‡æ¢æ€è·¯å´ä¸æ·±å…¥ã€‚\n*   **æ–¹æ³•**ï¼šè®¾è®¡äº†ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨ç†æ¡†æ¶ã€‚å®ƒå®æ—¶ç›‘æ§æ¨ç†è¿‡ç¨‹ï¼Œå¦‚æœå‘ç°æ¨¡å‹è¿‡æ—©æ”¾å¼ƒäº†ä¸€ä¸ªæœ‰æ½œåŠ›çš„æ€è·¯ï¼ˆç”±å¥–åŠ±æ¨¡å‹åˆ¤æ–­ï¼‰ï¼Œå°±ä¼šæ‰“æ–­å¹¶æ’å…¥â€œåŠ æ·±æç¤ºâ€ï¼Œå¼ºè¿«æ¨¡å‹å¾€æ·±å¤„æƒ³ã€‚\n\n---\n\n### ğŸ§  LLM æ¶æ„ã€æ•ˆç‡ä¸é•¿æ–‡æœ¬\n\n#### 97. **Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning**\n**æ¯ä¸ª Attention éƒ½é‡è¦ï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„é«˜æ•ˆæ··åˆæ¶æ„**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº† **Ring-linear** ç³»åˆ—æ¨¡å‹ï¼ˆ16B å’Œ 104Bï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šé‡‡ç”¨æ··åˆæ¶æ„ï¼ˆHybrid Architectureï¼‰ï¼Œç»“åˆäº† Linear Attention å’Œ Softmax Attentionã€‚\n*   **æ•ˆæœ**ï¼šç›¸æ¯”åŒç­‰è§„æ¨¡çš„ Dense æ¨¡å‹ï¼Œæ¨ç†æˆæœ¬é™è‡³ 1/10ï¼Œä¸”åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šä¿æŒäº† SOTA æ€§èƒ½ã€‚\n\n#### 85. **Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention**\n**Streamï¼šé€šè¿‡ç¨€ç–æ³¨æ„åŠ›å°†å¤§æ¨¡å‹çš„å¯è§£é‡Šæ€§æ‰©å±•åˆ°é•¿ä¸Šä¸‹æ–‡**\n*   **ç—›ç‚¹**ï¼šåˆ†æç™¾ä¸‡çº§ Token çš„æ³¨æ„åŠ›æœºåˆ¶éœ€è¦ TB çº§çš„å†…å­˜ï¼Œä¼ ç»Ÿæ–¹æ³•æ˜¾å­˜è¿™å°±çˆ†äº†ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº† **Sparse Tracing**ï¼Œä¸€ç§åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æŠ€æœ¯ï¼Œåªä¿ç•™ top-k çš„å…³é”®å—ã€‚\n*   **å‘ç°**ï¼šåœ¨ RULER åŸºå‡†æµ‹è¯•ä¸­ï¼Œå³ä½¿å‰ªææ‰ 97-99% çš„ token äº¤äº’ï¼Œä¾ç„¶èƒ½ä¿ç•™å…³é”®çš„æ¨ç†è·¯å¾„ï¼Œè®©åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šç›‘æ§é•¿æ–‡æœ¬ CoT æˆä¸ºå¯èƒ½ã€‚\n\n#### 31. **AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders**\n**AdaSPECï¼šç”¨äºé«˜æ•ˆæŠ•æœºè§£ç çš„é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦**\n*   **ä¼˜åŒ–**ï¼šæŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰é€šå¸¸ç”¨å°æ¨¡å‹ï¼ˆDraftï¼‰è¾…åŠ©å¤§æ¨¡å‹ã€‚ä¼ ç»Ÿçš„è’¸é¦è®©å°æ¨¡å‹å…¨é¢æ¨¡ä»¿å¤§æ¨¡å‹ï¼Œä½†å°æ¨¡å‹èƒ½åŠ›æœ‰é™å­¦ä¸æ¥ã€‚\n*   **æ–¹æ³•**ï¼šå¼•å…¥é€‰æ‹©æ€§ token è¿‡æ»¤ï¼Œåªè®©å°æ¨¡å‹å­¦ä¹ å®ƒèƒ½å­¦ä¼šçš„â€œç®€å• tokenâ€ï¼Œéš¾çš„äº¤ç»™å¤§æ¨¡å‹ã€‚è¿™åè€Œæé«˜äº†æ•´ä½“çš„ token æ¥å—ç‡ï¼ˆAcceptance Rateï¼‰ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€å¯¹é½ä¸å¹»è§‰\n\n#### 124. **PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning**\n**PruneHalï¼šé€šè¿‡è‡ªé€‚åº” KV ç¼“å­˜å‰ªæå‡å°‘å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¹»è§‰**\n*   **æ´å¯Ÿ**ï¼šå¹»è§‰å¾€å¾€æ˜¯å› ä¸ºæ¨¡å‹æŠŠæ³¨æ„åŠ›åˆ†æ•£åˆ°äº†æ— å…³çš„è§†è§‰ Token ä¸Šï¼Œå¯¼è‡´å¿½ç•¥äº†å…³é”®ä¿¡æ¯ã€‚\n*   **æ–¹æ³•**ï¼šæ— éœ€è®­ç»ƒï¼Œé€šè¿‡è‡ªé€‚åº”å‰ªæå»æ‰å†—ä½™çš„è§†è§‰ Token KV ç¼“å­˜ï¼Œå¼ºè¿«æ¨¡å‹èšç„¦ã€‚ç®€å•æœ‰æ•ˆã€‚\n\n#### 77. **A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring**\n**åŸºäºæ€ç»´é“¾ç›‘æ§çš„å®‰å…¨æ¡ˆä¾‹å…·ä½“è·¯çº¿å›¾**\n*   **è§‚ç‚¹**ï¼šå½“æ¨¡å‹å¼ºå¤§åˆ°æ— æ³•é€šè¿‡è¾“å‡ºåˆ¤æ–­å®‰å…¨æ€§æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç›‘æ§å…¶â€œæ€ç»´è¿‡ç¨‹â€ï¼ˆCoTï¼‰ã€‚\n*   **æŒ‘æˆ˜**ï¼šè®ºæ–‡ç³»ç»Ÿæ€§æ¢è®¨äº† CoT ç›‘æ§çš„å¨èƒï¼ŒåŒ…æ‹¬â€œå¤–æ˜Ÿæ¨ç†â€ï¼ˆAlien Reasoningï¼‰ã€éšå†™æœ¯å’Œè¯­è¨€æ¼‚ç§»ï¼Œå¹¶æå‡ºäº†åº”å¯¹è·¯çº¿å›¾ã€‚\n\n#### 38. **Misalignment Bounty: Crowdsourcing AI Agent Misbehavior**\n**é”™ä½èµé‡‘ï¼šä¼—åŒ… AI æ™ºèƒ½ä½“çš„ä¸è‰¯è¡Œä¸º**\n*   **è¶£é—»**ï¼šè¿™æ˜¯ Misalignment Bounty è®¡åˆ’çš„æŠ¥å‘Šã€‚ä»–ä»¬èŠ±é’±è¯·äººæ‰¾ AI è¿½æ±‚â€œéé¢„æœŸæˆ–ä¸å®‰å…¨ç›®æ ‡â€çš„æ¡ˆä¾‹ã€‚æ”¶åˆ°äº† 295 ä¸ªæäº¤ï¼Œç¡®è®¤äº† 9 ä¸ªé«˜è´¨é‡çš„çœŸå®é”™ä½æ¡ˆä¾‹ï¼Œå€¼å¾—å®‰å…¨ç ”ç©¶è€…å¤ç›˜ã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸æ¨¡æ‹Ÿ (Agents & Simulation)\n\n#### 107. **Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties**\n**å­¦ä¹ äº¤æœ‹å‹ï¼šæŒ‡å¯¼ LLM æ™ºèƒ½ä½“å»ºç«‹æ¶Œç°çš„ç¤¾ä¼šå…³ç³»**\n*   **ç ”ç©¶**ï¼šåœ¨ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œé€šè¿‡â€œæ•™ç»ƒä¿¡å·â€ï¼ˆCoaching signalï¼‰å’Œè¡Œä¸ºå¥–åŠ±ï¼Œè§‚å¯Ÿ LLM Agent æ˜¯å¦èƒ½å½¢æˆç±»ä¼¼äººç±»çš„ç¤¾äº¤ç½‘ç»œï¼ˆå¦‚äº’æƒ ã€åŒè´¨æ€§ï¼‰ã€‚\n*   **å‘ç°**ï¼šAgent èƒ½å¤Ÿæ¶Œç°å‡ºç¨³å®šçš„ç¤¾äº¤å…³ç³»ç»“æ„ï¼Œè¿™ä¸ºç ”ç©¶åœ¨çº¿ç¤¾åŒºåŠ¨åŠ›å­¦æä¾›äº†ä¸€ä¸ªæ–°çš„æµ‹è¯•åºŠã€‚\n\n#### 88. **ColorAgent: Building A Robust, Personalized, and Interactive OS Agent**\n**ColorAgentï¼šæ„å»ºé²æ£’ã€ä¸ªæ€§åŒ–ä¸”äº¤äº’å¼çš„æ“ä½œç³»ç»Ÿæ™ºèƒ½ä½“**\n*   **åº”ç”¨**ï¼šé’ˆå¯¹ Android ç³»ç»Ÿçš„æ™ºèƒ½ä½“ã€‚\n*   **ç‰¹ç‚¹**ï¼šä¸ä»…èƒ½æ‰§è¡Œé•¿åºåˆ—æ“ä½œï¼Œè¿˜åŠ å…¥äº†â€œä¸ªæ€§åŒ–æ„å›¾è¯†åˆ«â€å’Œâ€œä¸»åŠ¨äº¤äº’â€ï¼Œè®© OS åŠ©æ‰‹æ›´æœ‰æ¸©åº¦ã€‚åœ¨ AndroidWorld ä¸Šè¾¾åˆ°äº† 77.2% çš„æˆåŠŸç‡ã€‚\n\n#### 82. **ETOM: A Five-Level Benchmark for Evaluating Tool Orchestration within the MCP Ecosystem**\n**ETOMï¼šè¯„ä¼° MCP ç”Ÿæ€ç³»ç»Ÿä¸­å·¥å…·ç¼–æ’èƒ½åŠ›çš„äº”çº§åŸºå‡†**\n*   **åŸºå‡†æµ‹è¯•**ï¼šé’ˆå¯¹ Model-Context Protocol (MCP) ç”Ÿæ€ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ†çº§è¯„ä¼°æ ‡å‡†ã€‚ä»å•å·¥å…·è°ƒç”¨åˆ°å¤æ‚çš„è·¨æœåŠ¡å™¨è§„åˆ’ï¼Œç³»ç»Ÿæ€§æµ‹è¯• Agent çš„é²æ£’æ€§ã€‚\n\n---\n\n### ğŸ“ ç§‘å­¦ã€æ•°å­¦ä¸åŸºç¡€ç†è®º\n\n#### 118. **No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence**\n**æ²¡æœ‰ç»Ÿè®¡å­¦å°±æ²¡æœ‰æ™ºèƒ½ï¼šäººå·¥æ™ºèƒ½çš„éšå½¢è„Šæ¢**\n*   **é•¿æ–‡æ¨è**ï¼šä¸€ç¯‡ 37 é¡µçš„ç†è®ºæª„æ–‡ã€‚\n*   **æ ¸å¿ƒè®ºç‚¹**ï¼šAI çš„ä¹å¤§æ”¯æŸ±ï¼ˆæ¨æ–­ã€å¯†åº¦ä¼°è®¡ã€æ³›åŒ–ã€å› æœæ€§ç­‰ï¼‰å…¨éƒ¨å»ºç«‹åœ¨ç»Ÿè®¡å­¦åŸç†ä¹‹ä¸Šã€‚ä½œè€…å‘¼å AI ç¤¾åŒºä¸è¦åªå…³æ³¨ç®—åŠ›å’Œå·¥ç¨‹ï¼Œè¦é‡æ–°æ‹¥æŠ±ç»Ÿè®¡å­¦åŸºç¡€ï¼Œå¦åˆ™æˆ‘ä»¬æ„å»ºçš„å°†æ˜¯è„†å¼±çš„ç³»ç»Ÿã€‚\n\n#### 90. **AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation**\n**AgenticMathï¼šé€šè¿‡åŸºäºæ™ºèƒ½ä½“çš„æ•°å­¦æ•°æ®ç”Ÿæˆå¢å¼º LLM æ¨ç†**\n*   **æ•°æ®å·¥ç¨‹**ï¼šé«˜è´¨é‡æ•°å­¦æ•°æ®ç¨€ç¼ºã€‚\n*   **æ–¹æ³•**ï¼šä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°å­¦é—®ç­”å¯¹ï¼Œå¹¶ç”¨ CoT å¢å¼ºç­”æ¡ˆã€‚\n*   **ç»“æœ**ï¼šä»…ç”¨ 30-60K çš„åˆæˆæ•°æ®å¾®è°ƒï¼Œæ•ˆæœå°±è¶…è¿‡äº†ä½¿ç”¨ 400K-2.3M ä½è´¨é‡æ•°æ®çš„åŸºçº¿ã€‚\n\n#### 75. **KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge**\n**KnowMolï¼šåˆ©ç”¨å¤šå±‚æ¬¡åŒ–å­¦çŸ¥è¯†æ¨è¿›åˆ†å­å¤§è¯­è¨€æ¨¡å‹**\n*   **èµ„æº**ï¼šå‘å¸ƒäº†åŒ…å« 10 ä¸‡æ¡ç»†ç²’åº¦åˆ†å­æ³¨é‡Šçš„æ•°æ®é›† **KnowMol-100K**ã€‚\n*   **æ¨¡å‹**ï¼šæå‡ºäº† KnowMol æ¨¡å‹ï¼Œå¼¥è¡¥äº†åˆ†å­ç»“æ„ä¸æ–‡æœ¬æè¿°ä¹‹é—´çš„é¸¿æ²Ÿï¼Œåœ¨åˆ†å­ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„çŸ­è®¯\n\n*   **#35 Diffusion Caching Survey**: å…³äºæ‰©æ•£æ¨¡å‹ç¼“å­˜åŠ é€Ÿçš„ç»¼è¿°ï¼Œä»é™æ€å¤ç”¨åˆ°åŠ¨æ€é¢„æµ‹ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€ç”Ÿæˆã€‚\n*   **#30 Can They Dixit?**: ç”¨ã€Šåªè¨€ç‰‡è¯­ã€‹ï¼ˆDixitï¼‰æ¡Œæ¸¸æ¥æµ‹è¯•å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¬ºéª—å’Œæ¨ç†èƒ½åŠ›ï¼Œå‘ç°èƒœç‡ä¸ä¸»æµåŸºå‡†é«˜åº¦ç›¸å…³ã€‚\n*   **#48 I Spy With My Model's Eye**: å€Ÿé‰´è®¤çŸ¥å¿ƒç†å­¦çš„â€œè§†è§‰æœç´¢â€èŒƒå¼æµ‹è¯• MLLMï¼Œå‘ç°æ¨¡å‹ä¹Ÿå­˜åœ¨ç±»ä¼¼äººç±»çš„â€œå¼¹å‡ºæ•ˆåº”â€ï¼ˆPop-out effectï¼‰ã€‚\n*   **#128 News-Aware Direct Reinforcement Trading**: ç›´æ¥åˆ©ç”¨ LLM åˆ†ææ–°é—»æƒ…ç»ªï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡Œé‡‘èäº¤æ˜“ï¼Œæ•ˆæœä¼˜äºå•çº¯çš„é‡åŒ–æŒ‡æ ‡ã€‚\n*   **#127 The Zero-Step Thinking**: ç ”ç©¶äº†æ¨ç†æ¨¡å‹ä¸­çš„â€œæ¨¡å¼é€‰æ‹©â€ï¼ˆé€‰é•¿ CoT è¿˜æ˜¯çŸ­ CoTï¼‰ï¼Œå‘ç°è¿™æ˜¯ä¸€ä¸ªæ¯” Early Exit æ›´éš¾çš„é—®é¢˜ï¼Œç›®å‰çš„ Prompt æ–¹æ³•æ•ˆæœä¸ä½³ã€‚",
  "papers": [
    {
      "arxiv_id": "2510.20075v6",
      "title": "LLMs can hide text in other text of the same length",
      "title_zh": "LLMs å¯åœ¨ç­‰é•¿æ–‡æœ¬ä¸­å®ç°æ–‡æœ¬éšè—",
      "authors": [
        "Antonio Norelli",
        "Michael Bronstein"
      ],
      "abstract": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Calgacusï¼Œä¸€ç§ç®€å•ä¸”é«˜æ•ˆçš„åè®®ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å°†ä¸€æ®µæœ‰æ„ä¹‰çš„æ–‡æœ¬éšè—åœ¨é•¿åº¦ç›¸åŒã€å†…å®¹å®Œå…¨ä¸åŒä½†ä¾ç„¶è¿è´¯åˆç†çš„å¦ä¸€æ®µæ–‡æœ¬ä¸­ã€‚å®éªŒè¯æ˜ï¼Œå³ä¾¿ä½¿ç”¨ 80 äº¿å‚æ•°çš„å¼€æºæ¨¡å‹ä¹Ÿèƒ½è·å¾—é«˜è´¨é‡çš„éšå†™ç»“æœï¼Œä¸”å¤„ç†ä¸€æ®µæ‘˜è¦é•¿åº¦çš„ä¿¡æ¯ä»…éœ€åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œæ•°ç§’ã€‚Calgacus çš„å‡ºç°å±•ç¤ºäº†æ–‡æœ¬å½¢å¼ä¸ä½œè€…æ„å›¾ä¹‹é—´çš„å½»åº•è§£è€¦ï¼Œè¿›ä¸€æ­¥å‰Šå¼±äº†æ•°å­—åŒ–ä¹¦é¢é€šä¿¡çš„ä¿¡ä»»åŸºç¡€ã€‚è¯¥ç ”ç©¶ç‰¹åˆ«æå‡ºäº†ä¸€ä¸ªå®‰å…¨éšæ‚£åœºæ™¯ï¼Œå³é€šè¿‡å°†æœªè¿‡æ»¤æ¨¡å‹ (unfiltered LLM) çš„ç­”æ¡ˆç¼–ç åœ¨åˆè§„çš„å®‰å…¨æ¨¡å‹ (safe model) çš„å“åº”ä¸­ï¼Œä»è€Œç»•è¿‡å®‰å…¨ç›‘ç®¡ã€‚è¿™ç§å¯èƒ½æ€§ä¸º AI safety é¢†åŸŸå¸¦æ¥äº†ç´§è¿«çš„é—®é¢˜ï¼Œå¹¶æŒ‘æˆ˜äº†æˆ‘ä»¬å¯¹å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•å­˜å‚¨å’Œè¡¨è¾¾çŸ¥è¯†çš„ä¼ ç»Ÿè®¤çŸ¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, main paper 9 pages. v5 contains an Italian translation of this paper by the author",
      "pdf_url": "https://arxiv.org/pdf/2510.20075v6",
      "published_date": "2025-10-22 23:16:50 UTC",
      "updated_date": "2026-01-16 14:08:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:34.275158+00:00"
    },
    {
      "arxiv_id": "2510.21855v1",
      "title": "SIGN: Schema-Induced Games for Naming",
      "title_zh": "SIGNï¼šæ¨¡å¼å¼•å¯¼çš„å‘½ååšå¼ˆ",
      "authors": [
        "Ryan Zhang",
        "Herbert WoisetschlÃ¤ger"
      ],
      "abstract": "Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative coding and distributed planning therefore require reliable, consistent communication, and scalability is a central concern as systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming game that examines how lightweight structure can steer convention formation. We compare schema-induced communication to unconstrained natural language and find faster convergence with up to 5.8x higher agreement. These results suggest that minimal structure can act as a simple control knob for efficient multi-agent coordination, pointing toward broader applications beyond the naming game.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SIGN (Schema-Induced Games for Naming)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç ”ç©¶è½»é‡çº§ç»“æ„å¦‚ä½•å¼•å¯¼å…¬çº¦å½¢æˆ (convention formation) çš„å‘½ååšå¼ˆæ¡†æ¶ã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“åœ¨å¤æ‚åä½œä»»åŠ¡ä¸­å› é€šä¿¡æƒ¯ä¾‹ä¸ä¸€è‡´è€Œå¯¼è‡´çš„åè°ƒå¤±æ•ˆé—®é¢˜ï¼ŒSIGN æ¢ç´¢äº†æ¨¡å¼è¯±å¯¼ (Schema-Induced) é€šä¿¡å¯¹æ™ºèƒ½ä½“è¾¾æˆå…±è¯†çš„å½±å“ã€‚é€šè¿‡å°†è¯¥æœºåˆ¶ä¸æ— çº¦æŸçš„è‡ªç„¶è¯­è¨€è¿›è¡Œå¯¹æ¯”ï¼Œç ”ç©¶å‘ç°æ¨¡å¼è¯±å¯¼é€šä¿¡èƒ½å¤Ÿæ˜¾è‘—åŠ å¿«æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶å°†ä¸€è‡´æ€§ (agreement) æå‡é«˜è¾¾ 5.8 å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®å°çš„ç»“æ„åŒ–çº¦æŸå¯ä»¥ä½œä¸ºå¤šæ™ºèƒ½ä½“é«˜æ•ˆåè°ƒçš„æœ‰æ•ˆæ§åˆ¶æ‰‹æ®µï¼Œä¸ºå‘½ååšå¼ˆä¹‹å¤–çš„åˆ†å¸ƒå¼è§„åˆ’å’Œåä½œç¼–ç ç­‰é¢†åŸŸæä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI 2026 Student Abstract (Oral). Code available ar https://github.com/ryanzhangofficial/schema-induced-games-for-naming",
      "pdf_url": "https://arxiv.org/pdf/2510.21855v1",
      "published_date": "2025-10-22 23:12:06 UTC",
      "updated_date": "2025-10-22 23:12:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:31.484269+00:00"
    },
    {
      "arxiv_id": "2510.20061v1",
      "title": "Ask What Your Country Can Do For You: Towards a Public Red Teaming Model",
      "title_zh": "è¯¢é—®å›½å®¶èƒ½ä¸ºä½ åšäº›ä»€ä¹ˆï¼šè¿ˆå‘å…¬å…±çº¢é˜Ÿæ¨¡å‹",
      "authors": [
        "Wm. Matthew Kennedy",
        "Cigdem Patlak",
        "Jayraj Dave",
        "Blake Chambers",
        "Aayush Dhanotiya",
        "Darshini Ramiah",
        "Reva Schwartz",
        "Jack Hagen",
        "Akash Kundu",
        "Mouni Pendharkar",
        "Liam Baisley",
        "Theodora Skeadas",
        "Rumman Chowdhury"
      ],
      "abstract": "AI systems have the potential to produce both benefits and harms, but without rigorous and ongoing adversarial evaluation, AI actors will struggle to assess the breadth and magnitude of the AI risk surface. Researchers from the field of systems design have developed several effective sociotechnical AI evaluation and red teaming techniques targeting bias, hate speech, mis/disinformation, and other documented harm classes. However, as increasingly sophisticated AI systems are released into high-stakes sectors (such as education, healthcare, and intelligence-gathering), our current evaluation and monitoring methods are proving less and less capable of delivering effective oversight.\n  In order to actually deliver responsible AI and to ensure AI's harms are fully understood and its security vulnerabilities mitigated, pioneering new approaches to close this \"responsibility gap\" are now more urgent than ever. In this paper, we propose one such approach, the cooperative public AI red-teaming exercise, and discuss early results of its prior pilot implementations. This approach is intertwined with CAMLIS itself: the first in-person public demonstrator exercise was held in conjunction with CAMLIS 2024. We review the operational design and results of this exercise, the prior National Institute of Standards and Technology (NIST)'s Assessing the Risks and Impacts of AI (ARIA) pilot exercise, and another similar exercise conducted with the Singapore Infocomm Media Development Authority (IMDA). Ultimately, we argue that this approach is both capable of delivering meaningful results and is also scalable to many AI developing jurisdictions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½åœ¨è¿›å…¥æ•™è‚²ã€åŒ»ç–—å’Œæƒ…æŠ¥ç­‰é«˜é£é™©é¢†åŸŸæ—¶é¢ä¸´çš„ç›‘ç®¡æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å½“å‰è¯„ä¼°æ–¹æ³•åœ¨ç¼©å°è´£ä»»å·®è·(responsibility gap)æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºäº†æ›´å…¨é¢åœ°è¯†åˆ«AIç³»ç»Ÿçš„å®‰å…¨æ¼æ´å’Œæ½œåœ¨å±å®³ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºåˆä½œæ€§å…¬å…±AIçº¢é˜Ÿæ¼”ç»ƒ(cooperative public AI red-teaming exercise)çš„æ–°å‹æ¡†æ¶ã€‚æ–‡ç« è¯¦ç»†è¯„ä¼°äº†è¯¥æ¨¡å‹åœ¨CAMLIS 2024ã€ç¾å›½å›½å®¶æ ‡å‡†ä¸æŠ€æœ¯ç ”ç©¶é™¢(NIST)çš„ARIAè¯•ç‚¹é¡¹ç›®ä»¥åŠæ–°åŠ å¡èµ„è®¯é€šä¿¡åª’ä½“å‘å±•å±€(IMDA)åˆä½œæ¼”ç»ƒä¸­çš„å®é™…æ“ä½œè®¾è®¡ä¸ç»“æœã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§å…¬å…±çº¢é˜Ÿæ¼”ç»ƒä¸ä»…èƒ½æä¾›å…·æœ‰å®è´¨æ„ä¹‰çš„ç¤¾ä¼šæŠ€æœ¯è¯„ä¼°ç»“æœï¼Œè¿˜å…·å¤‡åœ¨ä¸åŒå¸æ³•ç®¡è¾–åŒºå†…è¿›è¡Œè§„æ¨¡åŒ–æ¨å¹¿çš„æ½œåŠ›ã€‚é€šè¿‡è¿™ç§åˆ›æ–°æ–¹æ³•ï¼ŒAIå¼€å‘è€…å’Œç›‘ç®¡æœºæ„å¯ä»¥æ›´æœ‰æ•ˆåœ°è¯„ä¼°é£é™©è¡¨é¢ï¼Œä¸ºå®ç°è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½(Responsible AI)æä¾›æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.20061v1",
      "published_date": "2025-10-22 22:24:21 UTC",
      "updated_date": "2025-10-22 22:24:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:34.487242+00:00"
    },
    {
      "arxiv_id": "2510.20040v1",
      "title": "Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning",
      "title_zh": "åŸºäºæ¨¡ä»¿å­¦ä¹ çš„å¾®ç”µç½‘èƒ½é‡ç®¡ç†è¿‘ä¼¼æ¨¡å‹é¢„æµ‹æ§åˆ¶",
      "authors": [
        "Changrui Liu",
        "Shengling Shi",
        "Anil Alan",
        "Ganesh Kumar Venayagamoorthy",
        "Bart De Schutter"
      ],
      "abstract": "Efficient energy management is essential for reliable and sustainable microgrid operation amid increasing renewable integration. This paper proposes an imitation learning-based framework to approximate mixed-integer Economic Model Predictive Control (EMPC) for microgrid energy management. The proposed method trains a neural network to imitate expert EMPC control actions from offline trajectories, enabling fast, real-time decision making without solving optimization problems online. To enhance robustness and generalization, the learning process includes noise injection during training to mitigate distribution shift and explicitly incorporates forecast uncertainty in renewable generation and demand. Simulation results demonstrate that the learned policy achieves economic performance comparable to EMPC while only requiring $10\\%$ of the computation time of optimization-based EMPC in practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¨¡ä»¿å­¦ä¹ (Imitation Learning)çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¾®ç½‘èƒ½é‡ç®¡ç†æä¾›ä¸€ç§è¿‘ä¼¼çš„æ··åˆæ•´æ•°ç»æµæ¨¡å‹é¢„æµ‹æ§åˆ¶(Economic Model Predictive Control, EMPC)æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¦»çº¿è½¨è¿¹è®­ç»ƒç¥ç»ç½‘ç»œæ¥æ¨¡ä»¿ä¸“å®¶ EMPC çš„æ§åˆ¶åŠ¨ä½œï¼Œä»è€Œå®ç°å¿«é€Ÿã€å®æ—¶çš„å†³ç­–ï¼Œé¿å…äº†åœ¨çº¿æ±‚è§£å¤æ‚ä¼˜åŒ–é—®é¢˜çš„è®¡ç®—å‹åŠ›ã€‚ä¸ºäº†å¢å¼ºç³»ç»Ÿçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå­¦ä¹ è¿‡ç¨‹åœ¨è®­ç»ƒä¸­å¼•å…¥äº†å™ªå£°æ³¨å…¥ä»¥å‡è½»åˆ†å¸ƒåç§»(Distribution Shift)ï¼Œå¹¶æ˜¾å¼åœ°å°†å¯å†ç”Ÿèƒ½æºå‘ç”µå’Œç”µåŠ›éœ€æ±‚é¢„æµ‹çš„ä¸ç¡®å®šæ€§çº³å…¥è€ƒè™‘ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨ç»æµæ€§èƒ½ä¸Šä¸ä¼ ç»Ÿ EMPC ç›¸å½“ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»…éœ€å…¶ 10% çš„è®¡ç®—æ—¶é—´ï¼Œä¸ºå¾®ç½‘çš„é«˜æ•ˆã€å¯é è¿è¡Œæä¾›äº†ä¸€ç§å…¼é¡¾å®æ—¶æ€§ä¸æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "Submitted to Engineering Applications of Artificial Intelligence (EAAI) and IFAC WC 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.20040v1",
      "published_date": "2025-10-22 21:39:18 UTC",
      "updated_date": "2025-10-22 21:39:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:38.075767+00:00"
    },
    {
      "arxiv_id": "2510.20039v1",
      "title": "Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions",
      "title_zh": "è¶…è¶Šå•å‘å½±å“ï¼šå¤šè½®äººç±»ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’ä¸­çš„åŒå‘è§‚ç‚¹åŠ¨æ€",
      "authors": [
        "Yuyang Jiang",
        "Longjie Guo",
        "Yuchen Wu",
        "Aylin Caliskan",
        "Tanu Mitra",
        "Hua Shen"
      ],
      "abstract": "Large language model (LLM)-powered chatbots are increasingly used for opinion exploration. Prior research examined how LLMs alter user views, yet little work extended beyond one-way influence to address how user input can affect LLM responses and how such bi-directional influence manifests throughout the multi-turn conversations. This study investigates this dynamic through 50 controversial-topic discussions with participants (N=266) across three conditions: static statements, standard chatbot, and personalized chatbot. Results show that human opinions barely shifted, while LLM outputs changed more substantially, narrowing the gap between human and LLM stance. Personalization amplified these shifts in both directions compared to the standard setting. Analysis of multi-turn conversations further revealed that exchanges involving participants' personal stories were most likely to trigger stance changes for both humans and LLMs. Our work highlights the risk of over-alignment in human-LLM interaction and the need for careful design of personalized chatbots to more thoughtfully and stably align with users.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„å¯¹è¯ä¸­ï¼Œäººç±»ä¸æ¨¡å‹ä¹‹é—´è¶…è¶Šå•å‘å½±å“çš„åŒå‘è§‚ç‚¹åŠ¨æ€(Bidirectional Opinion Dynamics)ã€‚ç ”ç©¶é€šè¿‡50ä¸ªäº‰è®®æ€§è¯é¢˜ï¼Œé‚€è¯·266åå‚ä¸è€…åœ¨é™æ€é™ˆè¿°ã€æ ‡å‡†èŠå¤©æœºå™¨äººå’Œä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººä¸‰ç§å®éªŒæ¡ä»¶ä¸‹è¿›è¡Œäº†å¤šè½®äº¤äº’åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œäººç±»è§‚ç‚¹åœ¨äº¤äº’ä¸­å‡ ä¹æ²¡æœ‰å‘ç”Ÿæ˜¾è‘—è½¬ç§»ï¼Œè€ŒLLMçš„è¾“å‡ºå´å‘ç”Ÿäº†å®è´¨æ€§æ”¹å˜ï¼Œä»è€Œç¼©å°äº†äººæœºä¹‹é—´çš„ç«‹åœºå·®è·ã€‚ä¸æ ‡å‡†è®¾ç½®ç›¸æ¯”ï¼Œä¸ªæ€§åŒ–(Personalization)æ”¾å¤§äº†è¿™ç§åŒå‘çš„ç«‹åœºåç§»ï¼Œä¸”æ¶‰åŠå‚ä¸è€…ä¸ªäººæ•…äº‹çš„äº¤æµæœ€å®¹æ˜“è§¦å‘äººç±»å’ŒLLMçš„ç«‹åœºå˜åŒ–ã€‚è¯¥å·¥ä½œæŒ‡å‡ºäº†äººæœºäº¤äº’ä¸­è¿‡åº¦å¯¹é½(Over-alignment)çš„é£é™©ï¼Œå¹¶å¼ºè°ƒåœ¨è®¾è®¡ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººæ—¶ï¼Œéœ€è¦æ›´å‘¨å…¨ã€æ›´ç¨³å®šåœ°ä¸ç”¨æˆ·ä¿æŒå¯¹é½ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "26 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.20039v1",
      "published_date": "2025-10-22 21:38:10 UTC",
      "updated_date": "2025-10-22 21:38:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:39.882041+00:00"
    },
    {
      "arxiv_id": "2510.20028v1",
      "title": "The Temporal Graph of Bitcoin Transactions",
      "title_zh": "Bitcoin äº¤æ˜“æ—¶åºå›¾",
      "authors": [
        "Vahid Jalili"
      ],
      "abstract": "Since its 2009 genesis block, the Bitcoin network has processed >1.08 billion (B) transactions representing >8.72B BTC, offering rich potential for machine learning (ML); yet, its pseudonymity and obscured flow of funds inherent in its UTxO-based design, have rendered this data largely inaccessible for ML research. Addressing this gap, we present an ML-compatible graph modeling the Bitcoin's economic topology by reconstructing the flow of funds. This temporal, heterogeneous graph encompasses complete transaction history up to block 863000, consisting of >2.4B nodes and >39.72B edges. Additionally, we provide custom sampling methods yielding node and edge feature vectors of sampled communities, tools to load and analyze the Bitcoin graph data within specialized graph databases, and ready-to-use database snapshots. This comprehensive dataset and toolkit empower the ML community to tackle Bitcoin's intricate ecosystem at scale, driving progress in applications such as anomaly detection, address classification, market analysis, and large-scale graph ML benchmarking. Dataset and code available at https://github.com/B1AAB/EBA",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¯”ç‰¹å¸ç½‘ç»œç”±äºåŒ¿åæ€§å’ŒUTxOè®¾è®¡å¯¼è‡´èµ„é‡‘æµå‘éš¾ä»¥è¢«æœºå™¨å­¦ä¹ åˆ†æçš„æŒ‘æˆ˜ï¼Œæ„å»ºäº†ä¸€ä¸ªä¸MLå…¼å®¹çš„æ¯”ç‰¹å¸ç»æµæ‹“æ‰‘å›¾æ¨¡å‹ã€‚è¯¥å›¾æ˜¯ä¸€ä¸ªæ—¶åºå¼‚æ„å›¾(temporal heterogeneous graph)ï¼Œæ¶µç›–äº†æˆªè‡³863000åŒºå—çš„å®Œæ•´äº¤æ˜“å†å²ï¼ŒåŒ…å«è¶…è¿‡24äº¿ä¸ªèŠ‚ç‚¹å’Œ397.2äº¿æ¡è¾¹ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡é‡æ„èµ„é‡‘æµåŠ¨ï¼Œæä¾›äº†è‡ªå®šä¹‰é‡‡æ ·æ–¹æ³•ä»¥ç”ŸæˆèŠ‚ç‚¹å’Œè¾¹çš„ç‰¹å¾å‘é‡ï¼Œå¹¶å‘å¸ƒäº†ä¸“ç”¨çš„å›¾æ•°æ®åº“å·¥å…·åŠå¿«ç…§ã€‚è¿™ä¸€å¤§è§„æ¨¡æ•°æ®é›†å’Œå·¥å…·åŒ…ä¸ºMLç¤¾åŒºç ”ç©¶æ¯”ç‰¹å¸å¤æ‚ç”Ÿæ€ç³»ç»Ÿæä¾›äº†åŸºç¡€ï¼Œæ”¯æŒå¼‚å¸¸æ£€æµ‹(anomaly detection)ã€åœ°å€åˆ†ç±»(address classification)å’Œå¸‚åœºåˆ†æ(market analysis)ç­‰åº”ç”¨ã€‚è¯¥å·¥ä½œæ˜¾è‘—æ¨åŠ¨äº†å¤§å°ºåº¦å›¾æœºå™¨å­¦ä¹ (graph ML)çš„åŸºå‡†æµ‹è¯•å’Œå®é™…è½åœ°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.20028v1",
      "published_date": "2025-10-22 21:10:46 UTC",
      "updated_date": "2025-10-22 21:10:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:45.179647+00:00"
    },
    {
      "arxiv_id": "2510.20020v1",
      "title": "Optimized Distortion in Linear Social Choice",
      "title_zh": "çº¿æ€§ç¤¾ä¼šé€‰æ‹©ä¸­çš„å¤±çœŸä¼˜åŒ–",
      "authors": [
        "Luise Ge",
        "Gregory Kehne",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "Social choice theory offers a wealth of approaches for selecting a candidate on behalf of voters based on their reported preference rankings over options. When voters have underlying utilities for these options, however, using preference rankings may lead to suboptimal outcomes vis-Ã -vis utilitarian social welfare. Distortion is a measure of this suboptimality, and provides a worst-case approach for developing and analyzing voting rules when utilities have minimal structure. However in many settings, such as common paradigms for value alignment, alternatives admit a vector representation, and it is natural to suppose that utilities are parametric functions thereof. We undertake the first study of distortion for linear utility functions. Specifically, we investigate the distortion of linear social choice for deterministic and randomized voting rules. We obtain bounds that depend only on the dimension of the candidate embedding, and are independent of the numbers of candidates or voters. Additionally, we introduce poly-time instance-optimal algorithms for minimizing distortion given a collection of candidates and votes. We empirically evaluate these in two real-world domains: recommendation systems using collaborative filtering embeddings, and opinion surveys utilizing language model embeddings, benchmarking several standard rules against our instance-optimal algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†çº¿æ€§æ•ˆç”¨å‡½æ•°ä¸‹çš„çº¿æ€§ç¤¾ä¼šé€‰æ‹©(Linear Social Choice)çš„å¤±çœŸåº¦(Distortion)é—®é¢˜ï¼Œåˆ†æäº†åœ¨å€™é€‰äººå…·æœ‰å‘é‡è¡¨ç¤ºä¸”æ•ˆç”¨ä¸ºçº¿æ€§å‡½æ•°æ—¶çš„æœ€ä¼˜é€‰æ‹©ç­–ç•¥ã€‚ä½œè€…ç ”ç©¶äº†ç¡®å®šæ€§å’ŒéšæœºæŠ•ç¥¨è§„åˆ™ï¼Œè¯æ˜äº†å…¶å¤±çœŸåº¦ç•Œé™ä»…å–å†³äºå€™é€‰äººåµŒå…¥çš„ç»´åº¦ï¼Œè€Œä¸å€™é€‰äººæˆ–é€‰æ°‘çš„æ•°é‡æ— å…³ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—å¤šé¡¹å¼æ—¶é—´å†…çš„å®ä¾‹æœ€ä¼˜ç®—æ³•(Instance-optimal algorithms)ï¼Œæ—¨åœ¨é’ˆå¯¹å…·ä½“çš„å€™é€‰äººå’Œé€‰ç¥¨é›†åˆå®ç°å¤±çœŸæœ€å°åŒ–ã€‚é€šè¿‡åœ¨åŸºäºååŒè¿‡æ»¤(Collaborative filtering)çš„æ¨èç³»ç»Ÿå’ŒåŸºäºè¯­è¨€æ¨¡å‹(Language model)åµŒå…¥çš„æ°‘æ„è°ƒæŸ¥ç­‰çœŸå®åœºæ™¯ä¸‹çš„å®è¯è¯„ä¼°ï¼Œè¯¥ç ”ç©¶éªŒè¯äº†æ‰€æç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥å·¥ä½œä¸ºåœ¨é«˜ç»´å‘é‡ç©ºé—´ä¸­è¿›è¡Œé«˜æ•ˆä¸”ä½å¤±çœŸçš„ç¤¾ä¼šé€‰æ‹©æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘å’Œå®è·µå·¥å…·ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.20020v1",
      "published_date": "2025-10-22 20:42:49 UTC",
      "updated_date": "2025-10-22 20:42:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:46.883596+00:00"
    },
    {
      "arxiv_id": "2510.20002v2",
      "title": "Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation",
      "title_zh": "æ‰“é€  GEMsï¼šé€šè¿‡åŸºäºè´¨é‡çš„è¯­æ–™åº“ç²¾é€‰æ¨åŠ¨å¸Œè…Šè¯­ NLP å‘å±•",
      "authors": [
        "Alexandra Apostolopoulou",
        "Konstantinos Kanaris",
        "Athanasios Koursaris",
        "Dimitris Tsakalidis",
        "George Domalis",
        "Ioannis E. Livieris"
      ],
      "abstract": "The advancement of natural language processing for morphologically rich and moderately-resourced languages like Modern Greek has been hindered by architectural stagnation, data scarcity, and limited context processing capabilities, particularly in specialized domains such as law. In this work, we propose the Greek Embedding Models (GEMs), a new family of transformer-based language models, specifically developed to address these limitations through architectural diversity and enhanced data curation. The proposed family of models are trained on several large-scale, meticulously curated corpora, encompassing both comprehensive general-domain datasets and specialized legal collections, addressing the persistent data scarcity that has impeded Greek language modeling advancement. The proposed quality-based corpus curation methodology incorporates extensive preprocessing pipelines, sophisticated deduplication strategies and targeted repetition of high-quality legal sub-corpora to enhance domain adaptation. The GEMs family comprises both established architectures (RoBERTa and Longformer) and advanced models not previously applied to Greek (ELECTRA, ConvBERT, and ModernBERT), providing comprehensive coverage of modern transformer designs. Additionally, we introduce the first bilingual Greek-English embedding models tailored for cross-lingual legal applications. Comprehensive evaluation across three core natural language understanding benchmarks demonstrates that the proposed GEM-RoBERTa and GEM-ConvBERT achieve statistically significant performance improvements over established state-of-the-art models, with accuracy gains of up to 3.6\\% while conducted statistical analysis using Friedman Aligned-Ranks and Finner post-hoc tests confirms the superiority of our approach across multiple evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Greek Embedding Models (GEMs)ï¼Œæ—¨åœ¨è§£å†³ç°ä»£å¸Œè…Šè¯­ (Modern Greek) åœ¨ NLP é¢†åŸŸé¢ä¸´çš„æ¶æ„åœæ»ã€æ•°æ®åŒ®ä¹ä»¥åŠæ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚GEMs ç³»åˆ—æ¨¡å‹é‡‡ç”¨äº†åŸºäºè´¨é‡çš„è¯­æ–™åº“ç­–åˆ’ (quality-based corpus curation) æ–¹æ³•ï¼Œé€šè¿‡ä¸¥æ ¼çš„é¢„å¤„ç†ã€å»é‡å’Œæ³•å¾‹å­è¯­æ–™åº“çš„å®šå‘é‡å¤è®­ç»ƒï¼Œæ˜¾è‘—å¢å¼ºäº†é¢†åŸŸé€‚é…èƒ½åŠ›ã€‚ç ”ç©¶ä¸ä»…æ¶µç›–äº† RoBERTa å’Œ Longformer æ¶æ„ï¼Œè¿˜é¦–æ¬¡å°† ELECTRAã€ConvBERT å’Œ ModernBERT å¼•å…¥å¸Œè…Šè¯­ï¼Œå¹¶æ¨å‡ºäº†é¦–ä¸ªé’ˆå¯¹è·¨è¯­è¨€æ³•å¾‹åº”ç”¨çš„å¸Œè…Šè¯­-è‹±è¯­åŒè¯­åµŒå…¥æ¨¡å‹ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒGEM-RoBERTa å’Œ GEM-ConvBERT åœ¨æ ¸å¿ƒè‡ªç„¶è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„ State-of-the-Art æ¨¡å‹ï¼Œå‡†ç¡®ç‡æœ€é«˜æå‡è¾¾ 3.6%ã€‚é€šè¿‡ Friedman Aligned-Ranks å’Œ Finner åéªŒæ£€éªŒç­‰ç»Ÿè®¡åˆ†æï¼Œç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šçš„æ˜¾è‘—ä¼˜è¶Šæ€§ï¼Œä¸ºæ¨åŠ¨å¸Œè…Šè¯­å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡çš„å‘å±•æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The manuscript is submitted to Applied Sciences",
      "pdf_url": "https://arxiv.org/pdf/2510.20002v2",
      "published_date": "2025-10-22 20:06:48 UTC",
      "updated_date": "2025-10-24 11:58:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:52.489704+00:00"
    },
    {
      "arxiv_id": "2510.20001v1",
      "title": "Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs",
      "title_zh": "è¶…è¶Š MedQAï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£çš„çœŸå®ä¸–ç•Œä¸´åºŠå†³ç­–",
      "authors": [
        "Yunpeng Xiao",
        "Carl Yang",
        "Mark Mai",
        "Xiao Hu",
        "Kai Shu"
      ],
      "abstract": "Large language models (LLMs) show promise for clinical use. They are often evaluated using datasets such as MedQA. However, Many medical datasets, such as MedQA, rely on simplified Question-Answering (Q\\A) that underrepresents real-world clinical decision-making. Based on this, we propose a unifying paradigm that characterizes clinical decision-making tasks along two dimensions: Clinical Backgrounds and Clinical Questions. As the background and questions approach the real clinical environment, the difficulty increases. We summarize the settings of existing datasets and benchmarks along two dimensions. Then we review methods to address clinical decision-making, including training-time and test-time techniques, and summarize when they help. Next, we extend evaluation beyond accuracy to include efficiency, explainability. Finally, we highlight open challenges. Our paradigm clarifies assumptions, standardizes comparisons, and guides the development of clinically meaningful LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡º MedQA ç­‰ç°æœ‰åŒ»ç–—æ•°æ®é›†å¤šä¾èµ–ç®€åŒ–çš„é—®ç­”å½¢å¼ï¼Œéš¾ä»¥çœŸå®åæ˜ ç°å®ä¸–ç•Œçš„ä¸´åºŠå†³ç­–å¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„èŒƒå¼ï¼Œé€šè¿‡ä¸´åºŠèƒŒæ™¯ (Clinical Backgrounds) å’Œä¸´åºŠé—®é¢˜ (Clinical Questions) ä¸¤ä¸ªç»´åº¦å¯¹å†³ç­–ä»»åŠ¡è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¼ºè°ƒéš¾åº¦éšç¯å¢ƒçœŸå®åº¦çš„æå‡è€Œå¢åŠ ã€‚ç ”ç©¶ä¸ä»…æ¢³ç†äº†ç°æœ‰åŸºå‡†æµ‹è¯•çš„è®¾å®šï¼Œè¿˜ç³»ç»Ÿå›é¡¾äº†åŒ…æ‹¬è®­ç»ƒæ—¶ (training-time) å’Œæµ‹è¯•æ—¶ (test-time) åœ¨å†…çš„å¤šç§æŠ€æœ¯æ‰‹æ®µã€‚è¯„ä»·ä½“ç³»è¿›ä¸€æ­¥ä»å•ä¸€çš„å‡†ç¡®ç‡æ‰©å±•è‡³æ•ˆç‡ (efficiency) å’Œå¯è§£é‡Šæ€§ (explainability) ç­‰å¤šé‡ç»´åº¦ã€‚è¯¥èŒƒå¼é€šè¿‡æ ‡å‡†åŒ–æ¯”è¾ƒå’ŒæŒ‘æˆ˜åˆ†æï¼Œä¸ºå¼€å‘æ›´å…·ä¸´åºŠåº”ç”¨ä»·å€¼çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æä¾›äº†æ˜ç¡®çš„è·¯å¾„æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.20001v1",
      "published_date": "2025-10-22 20:06:10 UTC",
      "updated_date": "2025-10-22 20:06:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:55:52.986780+00:00"
    },
    {
      "arxiv_id": "2510.19997v1",
      "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)",
      "title_zh": "ä¸­å‹ç»„ç»‡ä¸ä¼ä¸šç”Ÿæˆå¼äººå·¥æ™ºèƒ½é‡‡çº³ä¸æ•´åˆæ¡†æ¶ (FAIGMOE)",
      "authors": [
        "Abraham Itzhak Weinberg"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI) presents transformative opportunities for organizations, yet both midsize organizations and larger enterprises face distinctive adoption challenges. Midsize organizations encounter resource constraints and limited AI expertise, while enterprises struggle with organizational complexity and coordination challenges. Existing technology adoption frameworks, including TAM (Technology Acceptance Model), TOE (Technology Organization Environment), and DOI (Diffusion of Innovations) theory, lack the specificity required for GenAI implementation across these diverse contexts, creating a critical gap in adoption literature. This paper introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises), a conceptual framework addressing the unique needs of both organizational types. FAIGMOE synthesizes technology adoption theory, organizational change management, and innovation diffusion perspectives into four interconnected phases: Strategic Assessment, Planning and Use Case Development, Implementation and Integration, and Operationalization and Optimization. Each phase provides scalable guidance on readiness assessment, strategic alignment, risk governance, technical architecture, and change management adaptable to organizational scale and complexity. The framework incorporates GenAI specific considerations including prompt engineering, model orchestration, and hallucination management that distinguish it from generic technology adoption frameworks. As a perspective contribution, FAIGMOE provides the first comprehensive conceptual framework explicitly addressing GenAI adoption across midsize and enterprise organizations, offering actionable implementation protocols, assessment instruments, and governance templates requiring empirical validation through future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FAIGMOEï¼Œä¸€ä¸ªæ—¨åœ¨ä¿ƒè¿›ä¸­å‹ç»„ç»‡å’Œå¤§å‹ä¼ä¸šé‡‡ç”¨ä¸é›†æˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)çš„ç»¼åˆæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å¦‚TAMã€TOEå’ŒDOIåœ¨GenAIé¢†åŸŸçš„é’ˆå¯¹æ€§ç¼ºå¤±ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°è§£å†³äº†ä¸­å‹ç»„ç»‡çš„èµ„æºé™åˆ¶ä»¥åŠå¤§å‹ä¼ä¸šçš„ç»„ç»‡å¤æ‚æ€§é—®é¢˜ã€‚FAIGMOEç”±æˆ˜ç•¥è¯„ä¼°(Strategic Assessment)ã€è§„åˆ’ä¸ç”¨ä¾‹å¼€å‘(Planning and Use Case Development)ã€å®æ–½ä¸é›†æˆ(Implementation and Integration)ä»¥åŠè¿è¥ä¸ä¼˜åŒ–(Operationalization and Optimization)å››ä¸ªé˜¶æ®µç»„æˆã€‚è¯¥æ¡†æ¶ç‰¹åˆ«æ•´åˆäº†prompt engineeringã€model orchestrationå’Œhallucination managementç­‰GenAIæ ¸å¿ƒæŠ€æœ¯è€ƒé‡ï¼Œä»¥åŒºåˆ«äºé€šç”¨çš„æŠ€æœ¯é‡‡çº³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†å¯æ‰©å±•çš„é£é™©æ²»ç†ã€æŠ€æœ¯æ¶æ„å’Œå˜é©ç®¡ç†æŒ‡å—ã€‚ä½œä¸ºè¯¥é¢†åŸŸçš„é¦–ä¸ªå…¨é¢æ¦‚å¿µæ¡†æ¶ï¼ŒFAIGMOEä¸ºä¸åŒè§„æ¨¡çš„ç»„ç»‡æä¾›äº†å¯æ“ä½œçš„åè®®å’Œæ²»ç†æ¨¡æ¿ï¼Œä¸ºæœªæ¥å®è¯ç ”ç©¶å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19997v1",
      "published_date": "2025-10-22 19:55:31 UTC",
      "updated_date": "2025-10-22 19:55:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:01.694239+00:00"
    },
    {
      "arxiv_id": "2510.19988v1",
      "title": "LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation",
      "title_zh": "LLM å¢å¼ºçš„ç¬¦å·åŒ– NLU ç³»ç»Ÿï¼šå®ç°æ›´å¯é çš„è¿ç»­å› æœé™ˆè¿°è§£é‡Š",
      "authors": [
        "Xin Lian",
        "Kenneth D. Forbus"
      ],
      "abstract": "Despite the broad applicability of large language models (LLMs), their reliance on probabilistic inference makes them vulnerable to errors such as hallucination in generated facts and inconsistent output structure in natural language understanding (NLU) tasks. By contrast, symbolic NLU systems provide interpretable understanding grounded in curated lexicons, semantic resources, and syntactic & semantic interpretation rules. They produce relational representations that can be used for accurate reasoning and planning, as well as incremental debuggable learning. However, symbolic NLU systems tend to be more limited in coverage than LLMs and require scarce knowledge representation and linguistics skills to extend and maintain. This paper explores a hybrid approach that integrates the broad-coverage language processing of LLMs with the symbolic NLU capabilities of producing structured relational representations to hopefully get the best of both approaches. We use LLMs for rephrasing and text simplification, to provide broad coverage, and as a source of information to fill in knowledge gaps more automatically. We use symbolic NLU to produce representations that can be used for reasoning and for incremental learning. We evaluate this approach on the task of extracting and interpreting quantities and causal laws from commonsense science texts, along with symbolic- and LLM-only pipelines. Our results suggest that our hybrid method works significantly better than the symbolic-only pipeline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸­å­˜åœ¨çš„ Hallucination å’Œè¾“å‡ºç»“æ„ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°† LLMs ä¸ Symbolic NLU ç³»ç»Ÿç›¸ç»“åˆçš„ Hybrid Approachã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨èåˆ LLMs çš„å¹¿æ³› Coverage èƒ½åŠ›ä¸ Symbolic NLU åœ¨ç”Ÿæˆå¯è§£é‡Šã€ç»“æ„åŒ–å…³ç³»è¡¨ç¤ºæ–¹é¢çš„ä¼˜åŠ¿ã€‚å…·ä½“å®ç°ä¸­ï¼Œåˆ©ç”¨ LLMs è¿›è¡Œ Rephrasing å’Œ Text Simplification ä»¥è‡ªåŠ¨å¡«è¡¥çŸ¥è¯†ç¼ºå£ï¼Œå¹¶ç”± Symbolic NLU äº§ç”Ÿç”¨äºæ¨ç†å’Œ Incremental Learning çš„è¡¨ç¤ºã€‚ç ”ç©¶é€šè¿‡åœ¨å¸¸è¯†æ€§ç§‘å­¦æ–‡æœ¬ä¸­æå– Quantities å’Œ Causal Laws çš„ä»»åŠ¡å¯¹è¯¥ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ Symbolic-only Pipelineï¼Œä¸ºå®ç°æ›´å¯é çš„è¿ç»­å› æœé™ˆè¿°è§£é‡Šæä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19988v1",
      "published_date": "2025-10-22 19:38:20 UTC",
      "updated_date": "2025-10-22 19:38:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:16.881278+00:00"
    },
    {
      "arxiv_id": "2510.19975v1",
      "title": "Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations",
      "title_zh": "é‡æ–°å®¡è§†é›¶é˜¶ä¼˜åŒ–ï¼šæœ€å°æ–¹å·®ä¸¤ç‚¹ä¼°è®¡å™¨ä¸æ–¹å‘å¯¹é½æ‰°åŠ¨",
      "authors": [
        "Shaocong Ma",
        "Heng Huang"
      ],
      "abstract": "In this paper, we explore the two-point zeroth-order gradient estimator and identify the distribution of random perturbations that minimizes the estimator's asymptotic variance as the perturbation stepsize tends to zero. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that such desired perturbations can align directionally with the true gradient, instead of maintaining a fixed length. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using $Î´$-unbiased random perturbations, extending existing complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°æ¢è®¨äº†é›¶é˜¶ä¼˜åŒ–(Zeroth-Order Optimization)ä¸­çš„ä¸¤ç‚¹æ¢¯åº¦ä¼°è®¡å™¨ï¼Œæ—¨åœ¨å¯»æ‰¾èƒ½å¤Ÿä½¿ä¼°è®¡å™¨æ¸è¿‘æ–¹å·®æœ€å°åŒ–çš„éšæœºæ‰°åŠ¨åˆ†å¸ƒã€‚ç ”ç©¶å‘ç°ï¼Œç†æƒ³çš„æ‰°åŠ¨åˆ†å¸ƒåº”åœ¨æ–¹å‘ä¸Šä¸çœŸå®æ¢¯åº¦å¯¹é½(directionally align)ï¼Œè€Œéä»…ä»…ç»´æŒå›ºå®šé•¿åº¦ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æå‡ºäº†æ–¹å‘å¯¹é½æ‰°åŠ¨(Directionally Aligned Perturbation, DAP)æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ ¹æ®å…³é”®æ–¹å‘è‡ªé€‚åº”åœ°æä¾›æ›´é«˜çš„æ¢¯åº¦ä¼°è®¡ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ä¸ºä½¿ç”¨$\\delta$-æ— åéšæœºæ‰°åŠ¨çš„éšæœºæ¢¯åº¦ä¸‹é™(Stochastic Gradient Descent, SGD)æä¾›äº†æ”¶æ•›æ€§åˆ†æï¼Œå°†ç°æœ‰çš„å¤æ‚åº¦ç•Œé™æ‰©å±•åˆ°äº†æ›´å¹¿æ³›çš„æ‰°åŠ¨èŒƒå›´ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ŒDAPæ–¹æ¡ˆåœ¨å¤„ç†åˆæˆé—®é¢˜å’Œå®é™…ä»»åŠ¡æ—¶çš„æ€§èƒ½å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19975v1",
      "published_date": "2025-10-22 19:06:39 UTC",
      "updated_date": "2025-10-22 19:06:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:16.595195+00:00"
    },
    {
      "arxiv_id": "2510.19973v2",
      "title": "A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks",
      "title_zh": "æ™ºèƒ½ä½“ AI é©±åŠ¨çš„ 6G è‡ªä¸»ç½‘ç»œè®¤çŸ¥åå·®æ•™ç¨‹",
      "authors": [
        "Hatim Chergui",
        "Farhad Rezazadeh",
        "Merouane Debbah",
        "Christos Verikoukis"
      ],
      "abstract": "The path to higher network autonomy in 6G lies beyond the mere optimization of key performance indicators (KPIs). While KPIs have enabled automation gains under TM Forum Levels 1--3, they remain numerical abstractions that act only as proxies for the real essence of communication networks: seamless connectivity, fairness, adaptability, and resilience. True autonomy requires perceiving and reasoning over the network environment as it is. Such progress can be achieved through \\emph{agentic AI}, where large language model (LLM)-powered agents perceive multimodal telemetry, reason with memory, negotiate across domains, and act via APIs to achieve multi-objective goals. However, deploying such agents introduces the challenge of cognitive biases inherited from human design, which can distort reasoning, negotiation, tool use, and actuation. Between neuroscience and AI, this paper provides a tutorial on a selection of well-known biases, including their taxonomy, definition, mathematical formulation, emergence in telecom systems and the commonly impacted agentic components. The tutorial also presents various mitigation strategies tailored to each type of bias. The article finally provides two practical use-cases, which tackle the emergence, impact and mitigation gain of some famous biases in 6G inter-slice and cross-domain management. In particular, anchor randomization, temporal decay and inflection bonus techniques are introduced to specifically address anchoring, temporal and confirmation biases. This avoids that agents stick to the initial high resource allocation proposal or decisions that are recent and/or confirming a prior hypothesis. By grounding decisions in a richer and fairer set of past experiences, the quality and bravery of the agentic agreements in the second use-case, for instance, are leading to $\\times 5$ lower latency and around $40\\%$ higher energy saving.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨6Gè‡ªä¸»ç½‘ç»œä¸­å¼•å…¥Agentic AIä»¥è¶…è¶Šä¼ ç»ŸKPIä¼˜åŒ–å®ç°æ›´é«˜çº§åˆ«è‡ªæ²»çš„å¿…è¦æ€§ï¼Œå¹¶é‡ç‚¹åˆ†æäº†äººç±»è®¾è®¡ä¸­å›ºæœ‰çš„Cognitive Biaseså¯¹æ™ºèƒ½ä½“æ¨ç†ã€è°ˆåˆ¤å’Œæ‰§è¡Œä»»åŠ¡çš„æ½œåœ¨å¹²æ‰°ã€‚ä½œä¸ºä¸€ç¯‡æ•™ç¨‹ç±»æ–‡ç« ï¼Œå®ƒç³»ç»Ÿæ€§åœ°æ¢³ç†äº†å¤šç§çŸ¥ååå·®çš„åˆ†ç±»ã€å®šä¹‰åŠæ•°å­¦å»ºæ¨¡ï¼Œå¹¶è¯¦ç»†é˜è¿°äº†è¿™äº›åå·®åœ¨ç”µä¿¡ç³»ç»Ÿç»„ä»¶ä¸­çš„å…·ä½“ä½“ç°ä¸Mitigation Strategiesã€‚æ–‡ç« é€šè¿‡6G Inter-sliceå’ŒCross-domain Managementä¸¤ä¸ªå®é™…ç”¨ä¾‹ï¼Œå¼•å…¥äº†Anchor Randomizationã€Temporal Decayå’ŒInflection Bonusç­‰æŠ€æœ¯æ¥ä¸“é—¨åº”å¯¹Anchoringã€Temporalå’ŒConfirmation Biasesã€‚å®éªŒç»“æœè¯æ˜ï¼Œé€šè¿‡æ¶ˆé™¤æ™ºèƒ½ä½“å¯¹åˆå§‹åˆ†é…æ–¹æ¡ˆæˆ–è¿‘æœŸç»éªŒçš„è¿‡åº¦ä¾èµ–ï¼Œå†³ç­–è´¨é‡æ˜¾è‘—æå‡ï¼Œæœ€ç»ˆå®ç°äº†å»¶è¿Ÿé™ä½5å€ä»¥åŠèƒ½æºèŠ‚çœçº¦40%çš„æ˜¾è‘—æˆæ•ˆï¼Œä¸ºæ„å»ºæ›´å…¬å¹³ã€é«˜æ•ˆçš„è‡ªä¸»ç½‘ç»œæä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "19 pages, 15 figures, 1 table, link to source code available",
      "pdf_url": "https://arxiv.org/pdf/2510.19973v2",
      "published_date": "2025-10-22 19:05:04 UTC",
      "updated_date": "2025-11-04 10:36:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:22.694900+00:00"
    },
    {
      "arxiv_id": "2510.19967v1",
      "title": "LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation",
      "title_zh": "LyriCARï¼šé¢å‘å¯æ§æ­Œè¯ç¿»è¯‘çš„éš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Le Ren",
        "Xiangjian Zeng",
        "Qingqiang Wu",
        "Ruoxuan Liang"
      ],
      "abstract": "Lyric translation is a challenging task that requires balancing multiple musical constraints. Existing methods often rely on hand-crafted rules and sentence-level modeling, which restrict their ability to internalize musical-linguistic patterns and to generalize effectively at the paragraph level, where cross-line coherence and global rhyme are crucial. In this work, we propose LyriCAR, a novel framework for controllable lyric translation that operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware curriculum designer and an adaptive curriculum strategy, ensuring efficient allocation of training resources, accelerating convergence, and improving overall translation quality by guiding the model with increasingly complex challenges. Extensive experiments on the EN-ZH lyric translation task show that LyriCAR achieves state-of-the-art results across both standard translation metrics and multi-dimensional reward scores, surpassing strong baselines. Notably, the adaptive curriculum strategy reduces training steps by nearly 40% while maintaining superior performance. Code, data and model can be accessed at https://github.com/rle27/LyriCAR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LyriCARï¼Œä¸€ç§ç”¨äºå¯æ§æ­Œè¯ç¿»è¯‘çš„éš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹å¼ºåŒ–å­¦ä¹  (Difficulty-Aware Curriculum Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ®µè½å±‚é¢çš„è·¨è¡Œè¿è´¯æ€§å’Œå…¨å±€éŸµå¾‹ (Global Rhyme) æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å®Œå…¨æ— ç›‘ç£ (Unsupervised) çš„æ–¹å¼è¿è¡Œï¼Œå¼•å…¥äº†éš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹è®¾è®¡å™¨å’Œè‡ªé€‚åº”è¯¾ç¨‹ç­–ç•¥ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹é€æ­¥åº”å¯¹æ—¥ç›Šå¤æ‚çš„æŒ‘æˆ˜æ¥ä¼˜åŒ–è®­ç»ƒèµ„æºåˆ†é…å¹¶åŠ é€Ÿæ”¶æ•›ã€‚åœ¨ EN-ZH æ­Œè¯ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLyriCAR åœ¨æ ‡å‡†ç¿»è¯‘æŒ‡æ ‡å’Œå¤šç»´å¥–åŠ±è¯„åˆ† (Multi-dimensional reward scores) æ–¹é¢å‡è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥è‡ªé€‚åº”è¯¾ç¨‹ç­–ç•¥åœ¨ä¿æŒä¼˜å¼‚æ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®­ç»ƒæ­¥éª¤å‡å°‘äº†è¿‘ 40%ï¼Œæ˜¾è‘—æå‡äº†æ­Œè¯ç¿»è¯‘çš„æ•´ä½“è´¨é‡ä¸è®­ç»ƒæ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.19967v1",
      "published_date": "2025-10-22 18:57:20 UTC",
      "updated_date": "2025-10-22 18:57:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:32.788046+00:00"
    },
    {
      "arxiv_id": "2510.19964v1",
      "title": "AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits",
      "title_zh": "AIé©±åŠ¨çš„ä¸ªæ€§åŒ–å­¦ä¹ ï¼šåŸºäºé¢†å¯¼åŠ›äººæ ¼ç‰¹è´¨çš„å­¦ä¸šæˆç»©é¢„æµ‹",
      "authors": [
        "Nitsa J Herzog",
        "Rejwan Bin Sulaiman",
        "David J Herzog",
        "Rose Fong"
      ],
      "abstract": "The study explores the potential of AI technologies in personalized learning, suggesting the prediction of academic success through leadership personality traits and machine learning modelling. The primary data were obtained from 129 master's students in the Environmental Engineering Department, who underwent five leadership personality tests with 23 characteristics. Students used self-assessment tools that included Personality Insight, Workplace Culture, Motivation at Work, Management Skills, and Emotion Control tests. The test results were combined with the average grade obtained from academic reports. The study employed exploratory data analysis and correlation analysis. Feature selection utilized Pearson correlation coefficients of personality traits. The average grades were separated into three categories: fail, pass, and excellent. The modelling process was performed by tuning seven ML algorithms, such as SVM, LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance was achieved with the RF classifier, which yielded an accuracy of 87.50% for the model incorporating 17 personality trait features and the leadership mark feature, and an accuracy of 85.71% for the model excluding this feature. In this way, the study offers an additional opportunity to identify students' strengths and weaknesses at an early stage of their education process and select the most suitable strategies for personalized learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)åœ¨ä¸ªæ€§åŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡é¢†å¯¼åŠ›äººæ ¼ç‰¹è´¨(leadership personality traits)å’Œæœºå™¨å­¦ä¹ å»ºæ¨¡æ¥é¢„æµ‹å­¦ç”Ÿçš„å­¦ä¸šè¡¨ç°ã€‚ç ”ç©¶äººå‘˜å¯¹129åç¡•å£«ç”Ÿè¿›è¡Œäº†åŒ…å«æ€§æ ¼æ´å¯Ÿ(Personality Insight)å’Œæƒ…ç»ªæ§åˆ¶(Emotion Control)ç­‰äº”é¡¹é¢†å¯¼åŠ›äººæ ¼æµ‹è¯•ï¼Œæå–äº†23é¡¹ç‰¹å¾å¹¶ç»“åˆå­¦ä¸šå¹³å‡æˆç»©è¿›è¡Œåˆ†æã€‚é€šè¿‡çš®å°”é€Šç›¸å…³ç³»æ•°(Pearson correlation coefficients)è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå¹¶å¯¹SVMã€LRã€KNNã€RFã€XGBoostå’ŒLightGBMç­‰å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œäº†è°ƒä¼˜å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼Œéšæœºæ£®æ—(RF)åˆ†ç±»å™¨åœ¨é¢„æµ‹å­¦ä¸šç­‰çº§æ–¹é¢è¡¨ç°æœ€ä½³ï¼ŒåŒ…å«é¢†å¯¼åŠ›åˆ†æ•°ç‰¹å¾æ—¶å‡†ç¡®ç‡è¾¾87.50%ï¼Œæ’é™¤åä»èƒ½è¾¾åˆ°85.71%ã€‚è¯¥é¡¹æˆæœä¸ºæ•™è‚²å·¥ä½œè€…åœ¨æ—©æœŸé˜¶æ®µè¯†åˆ«å­¦ç”Ÿæ½œèƒ½æä¾›äº†æœ‰åŠ›å·¥å…·ï¼Œä¹Ÿä¸ºåˆ¶å®šæ›´å…·é’ˆå¯¹æ€§çš„ä¸ªæ€§åŒ–å­¦ä¹ ç­–ç•¥å¥ å®šäº†æ•°æ®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 6 figures, research article",
      "pdf_url": "https://arxiv.org/pdf/2510.19964v1",
      "published_date": "2025-10-22 18:47:30 UTC",
      "updated_date": "2025-10-22 18:47:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:28.082607+00:00"
    },
    {
      "arxiv_id": "2510.19957v1",
      "title": "A new wave of vehicle insurance fraud fueled by generative AI",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¼•å‘çš„æ–°ä¸€è½®æ±½è½¦ä¿é™©æ¬ºè¯ˆæµªæ½®",
      "authors": [
        "Amir Hever",
        "Itai Orr"
      ],
      "abstract": "Generative AI is supercharging insurance fraud by making it easier to falsify accident evidence at scale and in rapid time. Insurance fraud is a pervasive and costly problem, amounting to tens of billions of dollars in losses each year. In the vehicle insurance sector, fraud schemes have traditionally involved staged accidents, exaggerated damage, or forged documents. The rise of generative AI, including deepfake image and video generation, has introduced new methods for committing fraud at scale. Fraudsters can now fabricate highly realistic crash photos, damage evidence, and even fake identities or documents with minimal effort, exploiting AI tools to bolster false insurance claims. Insurers have begun deploying countermeasures such as AI-based deepfake detection software and enhanced verification processes to detect and mitigate these AI-driven scams. However, current mitigation strategies face significant limitations. Detection tools can suffer from false positives and negatives, and sophisticated fraudsters continuously adapt their tactics to evade automated checks. This cat-and-mouse arms race between generative AI and detection technology, combined with resource and cost barriers for insurers, means that combating AI-enabled insurance fraud remains an ongoing challenge. In this white paper, we present UVeye layered solution for vehicle fraud, representing a major leap forward in the ability to detect, mitigate and deter this new wave of fraud.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¦‚ä½•åŠ å‰§è½¦è¾†ä¿é™©æ¬ºè¯ˆï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡èŒƒå›´å†…å¿«é€Ÿä¼ªé€ äº‹æ•…è¯æ®ã€‚ä¼ ç»Ÿçš„è¯ˆéª—æ‰‹æ®µæ­£æ¼”å˜ä¸ºåˆ©ç”¨ Deepfake å›¾åƒå’Œè§†é¢‘ç”ŸæˆæŠ€æœ¯åˆ¶é€ é«˜åº¦çœŸå®çš„ç¢°æ’ç…§ç‰‡ã€æŸæ¯è¯æ®ä¹ƒè‡³è™šå‡èº«ä»½ï¼Œç»™ä¿é™©ä¸šå¸¦æ¥å·¨å¤§ç»æµæŸå¤±ã€‚å°½ç®¡ä¿é™©å…¬å¸å·²é‡‡ç”¨ AI-based deepfake detection è½¯ä»¶å’Œå¢å¼ºéªŒè¯æµç¨‹ï¼Œä½†ä»é¢ä¸´æ£€æµ‹è¯¯å·®åŠæ¬ºè¯ˆè€…æŒç»­è¿›åŒ–çš„æŒ‘æˆ˜ã€‚è¿™ç§æŠ€æœ¯å±‚é¢çš„â€œçŒ«é¼ æ¸¸æˆâ€ä»¥åŠèµ„æºæˆæœ¬éšœç¢ï¼Œä½¿å¾—åº”å¯¹ AI èµ‹èƒ½çš„æ¬ºè¯ˆæˆä¸ºæŒç»­æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº† UVeye layered solution è½¦è¾†æ¬ºè¯ˆåˆ†å±‚è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å…ˆè¿›æŠ€æœ¯æ‰‹æ®µæ˜¾è‘—æå‡äº†å¯¹æ­¤ç±»æ–°å‹æ¬ºè¯ˆæµªæ½®çš„æ£€æµ‹ã€ç¼“è§£å’Œå¨æ…‘èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19957v1",
      "published_date": "2025-10-22 18:31:31 UTC",
      "updated_date": "2025-10-22 18:31:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:31.188394+00:00"
    },
    {
      "arxiv_id": "2510.19954v3",
      "title": "RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs",
      "title_zh": "RELATEï¼šé¢å‘å¤šæ¨¡æ€å…³ç³»å›¾çš„æ¨¡å¼æ— å…³ Perceiver ç¼–ç å™¨",
      "authors": [
        "Joe Meyer",
        "Divyansha Lachi",
        "Mahmoud Mohammadi",
        "Roshan Reddy Upendra",
        "Eva L. Dyer",
        "Mark Li",
        "Tom Palczewski"
      ],
      "abstract": "Relational multi-table data is common in domains such as e-commerce, healthcare, and scientific research, and can be naturally represented as heterogeneous temporal graphs with multi-modal node attributes. Existing graph neural networks (GNNs) rely on schema-specific feature encoders, requiring separate modules for each node type and feature column, which hinders scalability and parameter sharing. We introduce RELATE (Relational Encoder for Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature encoder that can be used with any general purpose GNN. RELATE employs shared modality-specific encoders for categorical, numerical, textual, and temporal attributes, followed by a Perceiver-style cross-attention module that aggregates features into a fixed-size, permutation-invariant node representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark, where it achieves performance within 3% of schema-specific encoders while reducing parameter counts by up to 5x. This design supports varying schemas and enables multi-dataset pretraining for general-purpose GNNs, paving the way toward foundation models for relational graph data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RELATE (Relational Encoder for Latent Aggregation of Typed Entities)ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å¼æ— å…³ (schema-agnostic) ä¸”å³æ’å³ç”¨çš„ç‰¹å¾ç¼–ç å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾ç¥ç»ç½‘ç»œ (GNNs) åœ¨å¤„ç†å…·æœ‰å¤šæ¨¡æ€èŠ‚ç‚¹å±æ€§çš„å¼‚æ„å›¾æ—¶ï¼Œå› ä¾èµ–ç‰¹å®šæ¨¡å¼ç¼–ç å™¨è€Œå¯¼è‡´çš„æ‰©å±•æ€§å·®å’Œå‚æ•°å…±äº«å—é™ç­‰é—®é¢˜ã€‚RELATE é’ˆå¯¹ç±»åˆ«ã€æ•°å€¼ã€æ–‡æœ¬å’Œæ—¶é—´å±æ€§é‡‡ç”¨äº†å…±äº«çš„æ¨¡æ€ç‰¹å®šç¼–ç å™¨ï¼Œå¹¶åˆ©ç”¨ç±» Perceiver çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (cross-attention module) å°†å¤šæ¨¡æ€ç‰¹å¾èšåˆä¸ºå›ºå®šå¤§å°ä¸”ç½®æ¢ä¸å˜çš„èŠ‚ç‚¹è¡¨ç¤ºã€‚åœ¨ RelBench åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ ReLGNN å’Œ HGT ç»“åˆæ—¶å®ç°äº†ä¸ç‰¹å®šæ¨¡å¼ç¼–ç å™¨ 3% ä»¥å†…çš„æ€§èƒ½å·®è·ï¼ŒåŒæ—¶å°†å‚æ•°é‡å¤§å¹…å‡å°‘äº†å¤šè¾¾ 5 å€ã€‚è¿™ç§è®¾è®¡ä¸ä»…æ”¯æŒå¤šæ ·åŒ–çš„æ•°æ®æ¨¡å¼ï¼Œè¿˜ä½¿å¤šæ•°æ®é›†é¢„è®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œä¸ºå…³ç³»å›¾æ•°æ®çš„åŸºç¡€æ¨¡å‹ (foundation models) çš„å‘å±•å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.19954v3",
      "published_date": "2025-10-22 18:27:49 UTC",
      "updated_date": "2025-11-03 18:42:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:38.478172+00:00"
    },
    {
      "arxiv_id": "2510.19953v1",
      "title": "On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization",
      "title_zh": "é›¶é˜¶ä¼˜åŒ–æ— åæ¢¯åº¦ä¼°è®¡å™¨çš„æœ€ä¼˜æ„å»º",
      "authors": [
        "Shaocong Ma",
        "Heng Huang"
      ],
      "abstract": "Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of unbiased gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›¶é˜¶ä¼˜åŒ–(Zeroth-Order Optimization)ä¸­æ¢¯åº¦ä¼°è®¡å™¨æ™®éå­˜åœ¨çš„åç½®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»…åŸºäºå‡½æ•°è¯„ä¼°çš„æ–°å‹æ— åæ¢¯åº¦ä¼°è®¡å™¨ç³»åˆ—ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ–¹å‘å¯¼æ•°é‡æ–°è¡¨è¿°ä¸ºçº§æ•°å’Œ(telescoping series)å¹¶ç»“åˆç‰¹å®šåˆ†å¸ƒé‡‡æ ·ï¼Œåœ¨æ¶ˆé™¤åç½®çš„åŒæ—¶ç»´æŒäº†è‰¯å¥½çš„æ–¹å·®ç‰¹æ€§ã€‚ä½œè€…è¯¦ç»†æ¨å¯¼äº†å››ç§ç‰¹å®šæ„é€ çš„æœ€ä¼˜ç¼©æ”¾åˆ†å¸ƒä¸æ‰°åŠ¨æ­¥é•¿ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†ä½¿ç”¨è¯¥ä¼°è®¡å™¨çš„éšæœºæ¢¯åº¦ä¸‹é™(SGD)ç®—æ³•åœ¨å¤„ç†å¹³æ»‘éå‡¸ç›®æ ‡æ—¶å…·æœ‰æœ€ä¼˜å¤æ‚åº¦ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨åˆæˆä»»åŠ¡å’Œè¯­è¨€æ¨¡å‹å¾®è°ƒ(language model fine-tuning)ä¸­å±•ç°å‡ºä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„ç²¾åº¦ä¸æ”¶æ•›æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19953v1",
      "published_date": "2025-10-22 18:25:43 UTC",
      "updated_date": "2025-10-22 18:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:35.490850+00:00"
    },
    {
      "arxiv_id": "2510.19950v2",
      "title": "Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets",
      "title_zh": "é‡‘èé²æ£’å¼ºåŒ–å­¦ä¹ ï¼šåŸºäºæ¤­åœ†ä¸ç¡®å®šæ€§é›†çš„å¸‚åœºå†²å‡»å»ºæ¨¡",
      "authors": [
        "Shaocong Ma",
        "Heng Huang"
      ],
      "abstract": "In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡‘èé¢†åŸŸä¸­å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä»£ç†åœ¨è®­ç»ƒä¸å®é™…éƒ¨ç½²ç¯å¢ƒä¸åŒ¹é…çš„é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨äº†äº¤æ˜“äº§ç”Ÿçš„å¸‚åœºå†²å‡» (Market Impact) ç°è±¡ã€‚ç”±äºä¼ ç»Ÿé²æ£’å¼ºåŒ–å­¦ä¹  (Robust RL) é€šå¸¸ä¾èµ–å¯¹ç§°ç»“æ„ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¸‚åœºå†²å‡»çš„å®šå‘ç‰¹å¾ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç±»æ–°å‹çš„æ¤­åœ†ä¸ç¡®å®šæ€§é›† (Elliptic Uncertainty Sets)ã€‚ç ”ç©¶é€šè¿‡å»ºç«‹è¯¥é›†åˆä¸‹æœ€åŠ£æƒ…å†µä¸ç¡®å®šæ€§çš„éšå¼å’Œæ˜¾å¼é—­å¼è§£ï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯å¤„ç†çš„é²æ£’ç­–ç•¥è¯„ä¼°ã€‚åœ¨å•èµ„äº§å’Œå¤šèµ„äº§äº¤æ˜“ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æé«˜å¤æ™®æ¯”ç‡ (Sharpe Ratio) çš„åŒæ—¶ï¼Œäºé«˜äº¤æ˜“é‡ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚è¯¥æˆæœä¸ºåº”å¯¹é‡‘èå¸‚åœºä¸­å¤æ‚çš„æ¨¡å‹å¤±é…é—®é¢˜æä¾›äº†ä¸€ä¸ªæ›´å…·é’ˆå¯¹æ€§ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19950v2",
      "published_date": "2025-10-22 18:22:25 UTC",
      "updated_date": "2026-01-22 06:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:40.182137+00:00"
    },
    {
      "arxiv_id": "2510.19949v2",
      "title": "Surfer 2: The Next Generation of Cross-Platform Computer Use Agents",
      "title_zh": "Surfer 2ï¼šä¸‹ä¸€ä»£è·¨å¹³å°è®¡ç®—æœºæ“ä½œæ™ºèƒ½ä½“",
      "authors": [
        "Mathieu Andreux",
        "MÃ¤rt Bakler",
        "Yanael Barbier",
        "Hamza Benchekroun",
        "Emilien BirÃ©",
        "Antoine Bonnet",
        "Riaz Bordie",
        "Nathan Bout",
        "Matthias Brunel",
        "Aleix Cambray",
        "Pierre-Louis Cedoz",
        "Antoine Chassang",
        "Gautier Cloix",
        "Ethan Connelly",
        "Alexandra Constantinou",
        "Ramzi De Coster",
        "Hubert de la Jonquiere",
        "AurÃ©lien Delfosse",
        "Maxime Delpit",
        "Alexis Deprez",
        "Augustin Derupti",
        "Mathieu Diaz",
        "Shannon D'Souza",
        "Julie Dujardin",
        "Abai Edmund",
        "Michael Eickenberg",
        "Armand Fatalot",
        "Wissem Felissi",
        "Isaac Herring",
        "Xavier Koegler",
        "Erwan Le Jumeau de Kergaradec",
        "AurÃ©lien Lac",
        "Maxime Langevin",
        "Corentin Lauverjat",
        "Antonio Loison",
        "Avshalom Manevich",
        "Axel Moyal",
        "Axel Nguyen Kerbel",
        "Marinela Parovic",
        "Julien Revelle",
        "Guillaume Richard",
        "Mats Richter",
        "Ronan Riochet",
        "MarÃ­a Santos",
        "Romain Savidan",
        "Laurent Sifre",
        "Maxime Theillard",
        "Marc Thibault",
        "Ivan Valentini",
        "Tony Wu",
        "Laura Yie",
        "Kai Yuan",
        "Jevgenij Zubovskij"
      ],
      "abstract": "Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Surfer 2ï¼Œè¿™æ˜¯ä¸€ç§çº¯ç²¹åŸºäºè§†è§‰è§‚å¯Ÿ(visual observations)çš„ç»Ÿä¸€æ¶æ„ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“(Computer Use Agents)åœ¨Webã€æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒè·¨å¹³å°éƒ¨ç½²çš„é€šç”¨åŒ–éš¾é¢˜ã€‚Surfer 2é€šè¿‡æ•´åˆå±‚æ¬¡åŒ–ä¸Šä¸‹æ–‡ç®¡ç†(hierarchical context management)ã€è§£è€¦çš„è§„åˆ’ä¸æ‰§è¡Œ(decoupled planning and execution)ä»¥åŠå¸¦æœ‰è‡ªé€‚åº”æ¢å¤çš„è‡ªæˆ‘éªŒè¯(self-verification with adaptive recovery)æœºåˆ¶ï¼Œå®ç°äº†åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„å¯é è¿è¡Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSurfer 2åœ¨WebVoyagerã€WebArenaã€OSWorldå’ŒAndroidWorldç­‰åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½(state-of-the-art)ï¼Œä¸”æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒ(fine-tuning)ã€‚åœ¨å¤šæ¬¡å°è¯•çš„æƒ…å†µä¸‹ï¼ŒSurfer 2åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†äººç±»è¡¨ç°ï¼Œè¿™è¯æ˜äº†ç³»ç»ŸåŒ–ç¼–æ’èƒ½å¤Ÿæ˜¾è‘—æ”¾å¤§åŸºç¡€æ¨¡å‹(foundation model)çš„èƒ½åŠ›ï¼Œä»…é€šè¿‡è§†è§‰äº¤äº’å³å¯å®ç°é€šç”¨ç›®çš„çš„è®¡ç®—æœºæ§åˆ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 9 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.19949v2",
      "published_date": "2025-10-22 18:21:52 UTC",
      "updated_date": "2025-10-24 11:52:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:56:59.477574+00:00"
    },
    {
      "arxiv_id": "2510.20861v1",
      "title": "Fuzzy numbers revisited: operations on extensional fuzzy numbers",
      "title_zh": "æ¨¡ç³Šæ•°å†æ¢ï¼šå¤–å»¶æ¨¡ç³Šæ•°çš„è¿ç®—",
      "authors": [
        "Krzysztof Siminski"
      ],
      "abstract": "Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to better represent imprecise data. However, operations on fuzzy numbers are not as straightforward as maths on crisp numbers. Commonly, the Zadeh's extension rule is applied to elaborate a result. This can produce two problems: (1) high computational complexity and (2) for some fuzzy sets and some operations the results is not a fuzzy set with the same features (eg. multiplication of two triangular fuzzy sets does not produce a triangular fuzzy set). One more problem is the fuzzy spread -- fuzziness of the result increases with the number of operations. These facts can severely limit the application field of fuzzy numbers. In this paper we would like to revisite this problem with a different kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines operations on extensional fuzzy numbers and relational operators (=, >, >=, <, <=) for them. The proposed approach is illustrated with several applicational examples. The C++ implementation is available from a public GitHub repository.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡ç³Šæ•°ï¼ˆFuzzy numbersï¼‰åœ¨ä¼ ç»Ÿ Zadeh æ‰©å±•åŸç†ä¸‹å­˜åœ¨çš„è®¡ç®—å¤æ‚åº¦é«˜ã€è¿ç®—ç»“æœç‰¹å¾ä¸ä¿æŒä»¥åŠæ¨¡ç³Šæ‰©æ•£ï¼ˆfuzzy spreadï¼‰éšè¿ç®—æ¬¡æ•°å¢åŠ ç­‰å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤–å»¶æ¨¡ç³Šæ•°ï¼ˆextensional fuzzy numbersï¼‰çš„é‡æ–°å®¡è§†æ–¹æ¡ˆã€‚è®ºæ–‡è¯¦ç»†å®šä¹‰äº†é€‚ç”¨äº extensional fuzzy numbers çš„ä»£æ•°è¿ç®—æ³•åˆ™ä»¥åŠåŒ…æ‹¬ç­‰å·ä¸ä¸ç­‰å·åœ¨å†…çš„å„ç§å…³ç³»è¿ç®—ç¬¦ï¼ˆ=, >, >=, <, <=ï¼‰ã€‚é€šè¿‡è¿™ç§æ–°å‹æ¨¡ç³Šæ•°å¤„ç†æ–¹å¼ï¼Œç ”ç©¶æ—¨åœ¨å…‹æœä¼ ç»Ÿè®¡ç®—ä¸­ç»“æœç‰¹å¾åç§»çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡æ¨¡ç³Šæ•°æ®åœ¨å®é™…åœºæ™¯ä¸­çš„å¤„ç†æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡é€šè¿‡å¤šä¸ªåº”ç”¨ç¤ºä¾‹éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æä¾›äº†å¼€æºçš„ C++ ä»£ç å®ç°ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„å­¦æœ¯ç ”ç©¶å’Œå·¥ç¨‹å®è·µæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "33 pages, 62 references",
      "pdf_url": "https://arxiv.org/pdf/2510.20861v1",
      "published_date": "2025-10-22 18:11:38 UTC",
      "updated_date": "2025-10-22 18:11:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:16.176328+00:00"
    },
    {
      "arxiv_id": "2510.19898v2",
      "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills",
      "title_zh": "BugPilotï¼šé¢å‘ SWE æŠ€èƒ½é«˜æ•ˆå­¦ä¹ çš„å¤æ‚ç¼ºé™·ç”Ÿæˆ",
      "authors": [
        "Atharv Sonwane",
        "Isadora White",
        "Hyunji Lee",
        "Matheus Pereira",
        "Lucas Caccia",
        "Minseon Kim",
        "Zhengyan Shi",
        "Chinmay Singh",
        "Alessandro Sordoni",
        "Marc-Alexandre CÃ´tÃ©",
        "Xingdi Yuan"
      ],
      "abstract": "High quality bugs are key to training the next generation of language model based software engineering (SWE) agents. We introduce a novel method for synthetic generation of difficult and diverse bugs. Our method instructs SWE Agents to introduce a feature into the codebase whereby they may unintentionally break tests, resulting in bugs. Prior approaches often induce an out-of-distribution effect by generating bugs intentionally (e.g. by introducing local perturbation to existing code), which does not reflect realistic development processes. We perform qualitative analysis to demonstrate that our approach for generating bugs more closely reflects the patterns found in human-authored edits. Through extensive experiments, we demonstrate that our bugs provide more efficient training data for supervised fine-tuning, outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k bugs). We train on our newly generated bugs in addition to existing bug datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over three seeds.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BugPilotï¼Œä¸€ç§ç”¨äºåˆæˆç”Ÿæˆå¤æ‚ä¸”å¤šæ ·åŒ–è½¯ä»¶æ¼æ´çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡åŸºäºè¯­è¨€æ¨¡å‹çš„è½¯ä»¶å·¥ç¨‹(SWE Agents)æ™ºèƒ½ä½“çš„è®­ç»ƒæ•ˆç‡ã€‚ä¸ä¼ ç»Ÿé€šè¿‡å±€éƒ¨ä»£ç æ‰°åŠ¨ç”Ÿæˆæ¼æ´çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•å¼•å¯¼æ™ºèƒ½ä½“åœ¨ä¸ºä»£ç åº“å¼•å…¥æ–°åŠŸèƒ½çš„è¿‡ç¨‹ä¸­â€œæ— æ„ä¸­â€ç ´åæµ‹è¯•ï¼Œä»è€Œäº§ç”Ÿæ›´ç¬¦åˆçœŸå®å¼€å‘æµç¨‹çš„æ¼æ´ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼ŒBugPilot ç”Ÿæˆçš„æ¼æ´æ¯”ä»¥å¾€æ–¹æ³•æ›´æ¥è¿‘äººç±»ç¼–å†™çš„ä¿®æ”¹æ¨¡å¼ï¼Œæœ‰æ•ˆé¿å…äº†åˆ†å¸ƒå¤–(out-of-distribution)æ•ˆåº”ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ•°æ®é›†åœ¨ç›‘ç£å¾®è°ƒ(SFT)ä¸­è¡¨ç°å‡ºæé«˜çš„è®­ç»ƒæ•ˆç‡ï¼Œä»…éœ€ä¸€åŠçš„æ•°æ®é‡å³å¯å°†æ¨¡å‹æ€§èƒ½æå‡2%ã€‚é€šè¿‡ç»“åˆè¯¥æ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿè®­ç»ƒå‡ºçš„ FrogBoss (32B) å’Œ FrogMini (14B) æ¨¡å‹åœ¨ SWE-bench Verified è¯„æµ‹ä¸­åˆ†åˆ«å–å¾—äº† 54.6% å’Œ 45.3% çš„ pass@1 æˆç»©ï¼Œå‡è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19898v2",
      "published_date": "2025-10-22 17:58:56 UTC",
      "updated_date": "2025-10-28 19:10:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:14.681479+00:00"
    },
    {
      "arxiv_id": "2510.19897v1",
      "title": "Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation",
      "title_zh": "åŸºäºè¯­ä¹‰ä¸æƒ…æ™¯è®°å¿†çš„ç›‘ç£å­¦ä¹ ï¼šä¸€ç§æ™ºèƒ½ä½“è‡ªé€‚åº”çš„åæ€æ€§æ–¹æ³•",
      "authors": [
        "Jackson Hassell",
        "Dan Zhang",
        "Hannah Kim",
        "Tom Mitchell",
        "Estevam Hruschka"
      ],
      "abstract": "We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“å¦‚ä½•åœ¨ä¸æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ ‡è®°ç¤ºä¾‹å­¦ä¹ ç›®æ ‡åˆ†ç±»å‡½æ•°ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§è®°å¿†å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ ‡è®°æ•°æ®å’ŒLLMç”Ÿæˆçš„æ‰¹è¯„(critiques)ï¼Œé€šè¿‡æƒ…èŠ‚è®°å¿†(episodic memory)å­˜å‚¨æ•è·ç‰¹å®šè¿‡å»ç»éªŒçš„å®ä¾‹çº§æ‰¹è¯„ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰è®°å¿†(semantic memory)å°†å…¶æç‚¼ä¸ºå¯å¤ç”¨çš„ä»»åŠ¡çº§æŒ‡å¯¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸­å¼•å…¥æ‰¹è¯„æœºåˆ¶æ¯”ä»…ä¾èµ–æ ‡ç­¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æå‡äº†24.8%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å¼•å…¥åä¸ºâ€œå»ºè®®æ€§â€(suggestibility)çš„æ–°æŒ‡æ ‡ï¼Œæ·±å…¥åˆ†æäº†OpenAIæ¨¡å‹ä¸å¼€æºæ¨¡å‹åœ¨å¤„ç†äº‹å®å¯¼å‘åŠåå¥½å¯¼å‘æ•°æ®æ—¶çš„è¡Œä¸ºå·®å¼‚ã€‚è¯¥ç ”ç©¶ç»“æœçªæ˜¾äº†è®°å¿†é©±åŠ¨çš„åå°„æ€§å­¦ä¹ (reflective learning)åœ¨æ„å»ºæ›´å…·é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§çš„LLMæ™ºèƒ½ä½“æ–¹é¢çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.19897v1",
      "published_date": "2025-10-22 17:58:03 UTC",
      "updated_date": "2025-10-22 17:58:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:19.273092+00:00"
    },
    {
      "arxiv_id": "2510.19818v1",
      "title": "Semantic World Models",
      "title_zh": "è¯­ä¹‰ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Jacob Berg",
        "Chuning Zhu",
        "Yanda Bao",
        "Ishan Durugkar",
        "Abhishek Gupta"
      ],
      "abstract": "Planning with world models offers a powerful paradigm for robotic control. Conventional approaches train a model to predict future frames conditioned on current frames and actions, which can then be used for planning. However, the objective of predicting future pixels is often at odds with the actual planning objective; strong pixel reconstruction does not always correlate with good planning decisions. This paper posits that instead of reconstructing future frames as pixels, world models only need to predict task-relevant semantic information about the future. For such prediction the paper poses world modeling as a visual question answering problem about semantic information in future frames. This perspective allows world modeling to be approached with the same tools underlying vision language models. Thus vision language models can be trained as \"semantic\" world models through a supervised finetuning process on image-action-text data, enabling planning for decision-making while inheriting many of the generalization and robustness properties from the pretrained vision-language models. The paper demonstrates how such a semantic world model can be used for policy improvement on open-ended robotics tasks, leading to significant generalization improvements over typical paradigms of reconstruction-based action-conditional world modeling. Website available at https://weirdlabuw.github.io/swm.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è¯­ä¹‰ä¸–ç•Œæ¨¡å‹(Semantic World Models, SWM)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºåƒç´ é‡å»º(pixel reconstruction)çš„ä¸–ç•Œæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶è§„åˆ’ä¸­é¢„æµ‹ç›®æ ‡ä¸å®é™…è§„åˆ’ç›®æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸»å¼ ä¸–ç•Œæ¨¡å‹åªéœ€é¢„æµ‹ä¸ä»»åŠ¡ç›¸å…³çš„æœªæ¥è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œæ— éœ€é‡å»ºåƒç´ ã€‚é€šè¿‡å°†ä¸–ç•Œå»ºæ¨¡è½¬åŒ–ä¸ºå…³äºæœªæ¥å¸§è¯­ä¹‰ä¿¡æ¯çš„è§†è§‰é—®ç­”(Visual Question Answering, VQA)é—®é¢˜ï¼Œç ”ç©¶è€…åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(Vision Language Models, VLMs)å¹¶ç»“åˆå›¾åƒ-åŠ¨ä½œ-æ–‡æœ¬æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒ(Supervised Finetuning)ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿç»§æ‰¿é¢„è®­ç»ƒVLMsçš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ï¼Œä»è€Œæœ‰æ•ˆæ”¯æŒå†³ç­–è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSemantic World Models åœ¨å¼€æ”¾å¼æœºå™¨äººä»»åŠ¡çš„ç­–ç•¥ä¼˜åŒ–(policy improvement)æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºé‡å»ºçš„åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹(reconstruction-based action-conditional world modeling)ï¼Œä¸ºæ„å»ºå…·å¤‡é«˜åº¦æ³›åŒ–èƒ½åŠ›çš„æœºå™¨äººä¸–ç•Œæ¨¡å‹æä¾›äº†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19818v1",
      "published_date": "2025-10-22 17:53:45 UTC",
      "updated_date": "2025-10-22 17:53:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:20.777873+00:00"
    },
    {
      "arxiv_id": "2510.19895v1",
      "title": "Large Language Model enabled Mathematical Modeling",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½çš„æ•°å­¦å»ºæ¨¡",
      "authors": [
        "Guoyun Zhang"
      ],
      "abstract": "The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¾…åŠ©è¿ç­¹å­¦(Operations Research)å†³ç­–çš„æ½œåŠ›ï¼Œé‡ç‚¹ç ”ç©¶äº†DeepSeek-R1åœ¨æ•°å­¦å»ºæ¨¡ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ä¼ ç»Ÿçº¿æ€§è§„åˆ’ç­‰ä¼˜åŒ–æ–¹æ³•å¯¹é¢†åŸŸä¸“å®¶é«˜åº¦ä¾èµ–çš„ç°çŠ¶ï¼Œä»¥åŠGPT-4ç­‰æ¨¡å‹åœ¨ä¾›åº”é“¾åœºæ™¯ä¸­Tokenæˆæœ¬è¿‡é«˜å’Œæ˜“äº§ç”Ÿå¹»è§‰çš„å±€é™æ€§ï¼Œæœ¬ç ”ç©¶å¯¹DeepSeek-R1åœ¨NL4OPTã€IndustryORã€EasyLPå’ŒComplexORç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ç ”ç©¶è€…å»ºç«‹äº†ä¸€å¥—å¹»è§‰åˆ†ç±»ä½“ç³»(hallucination taxonomy)ï¼Œå¹¶åº”ç”¨äº†LLM-as-a-Judgeã€Few-shot Learning (FSL)ã€Tool Callingä»¥åŠMulti-agent Frameworkç­‰ç­–ç•¥è¿›è¡Œç¼“è§£ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›æ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ•°å­¦å…¬å¼åŒ–(formulation)è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ï¼Œå¹¶èƒ½æ›´å¥½åœ°å¯¹é½ç”¨æˆ·æ„å›¾ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¼¥åˆè‡ªç„¶è¯­è¨€ç†è§£ä¸ä¸“ä¸šæ•°å­¦æ¨¡å‹æ„å»ºä¹‹é—´çš„å·®è·æä¾›äº†ä½æˆæœ¬ä¸”é«˜æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19895v1",
      "published_date": "2025-10-22 17:41:42 UTC",
      "updated_date": "2025-10-22 17:41:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:28.522038+00:00"
    },
    {
      "arxiv_id": "2510.19807v1",
      "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning",
      "title_zh": "Scaf-GRPOï¼šç”¨äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è„šæ‰‹æ¶å¼ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Xichen Zhang",
        "Sitong Wu",
        "Yinghao Zhu",
        "Haoru Tan",
        "Shaozuo Yu",
        "Ziyi He",
        "Jiaya Jia"
      ],
      "abstract": "Reinforcement learning from verifiable rewards has emerged as a powerful technique for enhancing the complex reasoning abilities of Large Language Models (LLMs). However, these methods are fundamentally constrained by the ''learning cliff'' phenomenon: when faced with problems far beyond their current capabilities, models consistently fail, yielding a persistent zero-reward signal. In policy optimization algorithms like GRPO, this collapses the advantage calculation to zero, rendering these difficult problems invisible to the learning gradient and stalling progress. To overcome this, we introduce Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive training framework that strategically provides minimal guidance only when a model's independent learning has plateaued. The framework first diagnoses learning stagnation and then intervenes by injecting tiered in-prompt hints, ranging from abstract concepts to concrete steps, enabling the model to construct a valid solution by itself. Extensive experiments on challenging mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative 44.3% over a vanilla GRPO baseline. This result demonstrates our framework provides a robust and effective methodology for unlocking a model's ability to solve problems previously beyond its reach, a critical step towards extending the frontier of autonomous reasoning in LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°çš„â€œå­¦ä¹ æ‚¬å´–â€(learning cliff)ç°è±¡ï¼Œå³æ¨¡å‹åœ¨é¢å¯¹è¿œè¶…å…¶èƒ½åŠ›çš„é—®é¢˜æ—¶å› æŒç»­è·å¾—é›¶å¥–åŠ±ä¿¡å·è€Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±å’Œè®­ç»ƒåœæ»çš„é—®é¢˜ï¼Œæå‡ºäº†Scaf-GRPO (Scaffolded Group Relative Policy Optimization) æ¡†æ¶ã€‚Scaf-GRPO é‡‡ç”¨ä¸€ç§æ¸è¿›å¼è®­ç»ƒæ¨¡å¼ï¼Œä»…åœ¨æ¨¡å‹è‡ªä¸»å­¦ä¹ è¿›å…¥å¹³å°æœŸæ—¶æˆ˜ç•¥æ€§åœ°æä¾›æœ€å°åŒ–å¼•å¯¼ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡è¯Šæ–­ç¡®å®šå­¦ä¹ åœæ»çŠ¶æ€ï¼Œéšåé€šè¿‡æ³¨å…¥åˆ†çº§çš„æç¤ºè¯(tiered in-prompt hints)è¿›è¡Œå¹²é¢„ï¼Œå¼•å¯¼èŒƒå›´æ¶µç›–ä»æŠ½è±¡æ¦‚å¿µåˆ°å…·ä½“æ­¥éª¤ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ„å»ºæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­ Qwen2.5-Math-7B åœ¨ AIME24 ä¸Šçš„ pass@1 å¾—åˆ†ç›¸æ¯”åŸç”Ÿ GRPO åŸºå‡†æ¨¡å‹æå‡äº† 44.3%ã€‚å®éªŒç»“æœè¯æ˜äº† Scaf-GRPO åœ¨è§£é”æ¨¡å‹è§£å†³è¶…çº²é—®é¢˜æ–¹é¢çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºæ‰©å±•å¤§è¯­è¨€æ¨¡å‹çš„è‡ªä¸»æ¨ç†è¾¹ç•Œæä¾›äº†å…³é”®çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Code: https://github.com/dvlab-research/Scaf-GRPO",
      "pdf_url": "https://arxiv.org/pdf/2510.19807v1",
      "published_date": "2025-10-22 17:41:30 UTC",
      "updated_date": "2025-10-22 17:41:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:27.183277+00:00"
    },
    {
      "arxiv_id": "2510.19799v1",
      "title": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation",
      "title_zh": "èåˆé€æ˜æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹ä¸â€œä»ä¸šè€…åœ¨å›è·¯â€ï¼šéè¥åˆ©é¡¹ç›®è¯„ä¼°æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Ji Ma",
        "Albert Casella"
      ],
      "abstract": "Public and nonprofit organizations often hesitate to adopt AI tools because most models are opaque even though standard approaches typically analyze aggregate patterns rather than offering actionable, case-level guidance. This study tests a practitioner-in-the-loop workflow that pairs transparent decision-tree models with large language models (LLMs) to improve predictive accuracy, interpretability, and the generation of practical insights. Using data from an ongoing college-success program, we build interpretable decision trees to surface key predictors. We then provide each tree's structure to an LLM, enabling it to reproduce case-level predictions grounded in the transparent models. Practitioners participate throughout feature engineering, model design, explanation review, and usability assessment, ensuring that field expertise informs the analysis at every stage. Results show that integrating transparent models, LLMs, and practitioner input yields accurate, trustworthy, and actionable case-level evaluations, offering a viable pathway for responsible AI adoption in the public and nonprofit sectors.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å…¬å…±å’Œéè¥åˆ©ç»„ç»‡åœ¨é‡‡ç”¨ AI å·¥å…·æ—¶é¢ä¸´çš„é€æ˜åº¦ä¸è¶³åŠç¼ºä¹å…·ä½“æ¡ˆä¾‹çº§æŒ‡å¯¼çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ Practitioner-in-the-loop å·¥ä½œæµï¼Œå°†é€æ˜çš„ Decision-tree models ä¸ Large Language Models (LLMs) ç›¸ç»“åˆï¼Œä»¥æå‡é¢„æµ‹å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å¹¶ç”Ÿæˆå®ç”¨è§è§£ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¯è§£é‡Šçš„å†³ç­–æ ‘æå–å…³é”®é¢„æµ‹å› å­ï¼Œéšåå°†æ ‘ç»“æ„æä¾›ç»™ LLMï¼Œä½¿å…¶èƒ½å¤ŸåŸºäºé€æ˜æ¨¡å‹ç”Ÿæˆæ¡ˆä¾‹çº§çš„é¢„æµ‹ç»“æœã€‚ä»ä¸šè€…åœ¨ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®¾è®¡ã€è§£é‡Šå®¡æŸ¥å’Œå¯ç”¨æ€§è¯„ä¼°ç­‰ç¯èŠ‚å…¨ç¨‹å‚ä¸ï¼Œç¡®ä¿äº†é¢†åŸŸä¸“ä¸šçŸ¥è¯†åœ¨å„ä¸ªé˜¶æ®µçš„åº”ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé›†æˆé€æ˜æ¨¡å‹ã€LLMs ä¸ä»ä¸šè€…åé¦ˆçš„æ–¹æ³•èƒ½å¤Ÿäº§å‡ºå‡†ç¡®ã€å¯ä¿¡ä¸”å…·æœ‰è¡ŒåŠ¨å¯¼å‘çš„è¯„ä¼°ï¼Œä¸ºå…¬å…±å’Œéè¥åˆ©éƒ¨é—¨è´Ÿè´£ä»»åœ°é‡‡ç”¨ AI æä¾›äº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SE",
        "econ.GN"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19799v1",
      "published_date": "2025-10-22 17:35:13 UTC",
      "updated_date": "2025-10-22 17:35:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:30.504562+00:00"
    },
    {
      "arxiv_id": "2510.19792v1",
      "title": "On Controlled Change: Generative AI's Impact on Professional Authority in Journalism",
      "title_zh": "è®ºå—æ§å˜é©ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹æ–°é—»ä¸šä¸“ä¸šæƒå¨çš„å½±å“",
      "authors": [
        "TomÃ¡s Dodds",
        "Wang Ngai Yeung",
        "Claudia Mellado",
        "Mathias-Felipe de Lima-Santos"
      ],
      "abstract": "Using (generative) artificial intelligence tools and systems in journalism is expected to increase journalists' production rates, transform newsrooms' economic models, and further personalize the audience's news consumption practices. Since its release in 2022, OpenAI's ChatGPT and other large language models have raised the alarms inside news organizations, not only for bringing new challenges to news reporting and fact-checking but also for what these technologies would mean for journalists' professional authority in journalism. This paper examines how journalists in Dutch media manage the integration of AI technologies into their daily routines. Drawing from 13 interviews with editors, journalists, and innovation managers in different news outlets and media companies, we propose the concept of controlled change. as a heuristic to explain how journalists are proactively setting guidelines, experimenting with AI tools, and identifying their limitations and capabilities. Using professional authority as a theoretical framework, we argue that journalists anticipate and integrate AI technologies in a supervised manner and identify three primary mechanisms through which journalists manage this integration: (1) developing adaptive guidelines that align AI use with ethical codes, (2) experimenting with AI technologies to determine their necessity and fit, and (3) critically assessing the capabilities and limitations of AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¯¹æ–°é—»ä¸šä¸“ä¸šæƒå¨ (Professional Authority) çš„å½±å“ï¼Œåˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ ChatGPT ç»™æ–°é—»æŠ¥é“å’Œäº‹å®æ ¸æŸ¥å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹è·å…°åª’ä½“æœºæ„çš„ç¼–è¾‘ã€è®°è€…åŠåˆ›æ–°ç»ç†è¿›è¡Œçš„13æ¬¡æ·±åº¦è®¿è°ˆï¼Œç ”ç©¶æå‡ºäº†â€œå—æ§å˜é©â€ (Controlled Change) è¿™ä¸€å¯å‘å¼æ¦‚å¿µï¼Œç”¨ä»¥è§£é‡Šä»ä¸šè€…å¦‚ä½•ä¸»åŠ¨ç®¡ç†æŠ€æœ¯æ•´åˆã€‚ç ”ç©¶å‘ç°ï¼Œæ–°é—»ä»ä¸šè€…åœ¨ç›‘ç£ä¸‹é¢„æµ‹å¹¶æ•´åˆ AI æŠ€æœ¯ï¼Œä¸»è¦é€šè¿‡ä¸‰ä¸ªæœºåˆ¶å®ç°ï¼šåˆ¶å®šä¸ä¼¦ç†å‡†åˆ™ä¸€è‡´çš„é€‚åº”æ€§æŒ‡å— (Adaptive Guidelines)ã€é€šè¿‡å®éªŒç¡®å®šæŠ€æœ¯çš„å¿…è¦æ€§ä¸é€‚é…åº¦ï¼Œä»¥åŠæ‰¹åˆ¤æ€§åœ°è¯„ä¼° AI ç³»ç»Ÿçš„èƒ½åŠ›ä¸å±€é™ã€‚è¿™ç§â€œå—æ§å˜é©â€æ¨¡å¼è¡¨æ˜ï¼Œè®°è€…æ­£é€šè¿‡ä¸»åŠ¨è®¾ç½®å‡†åˆ™å’Œè¯†åˆ«æŠ€æœ¯è¾¹ç•Œï¼Œåœ¨ AI æµªæ½®ä¸­ç»´æŠ¤å…¶èŒä¸šèº«ä»½ä¸æƒå¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19792v1",
      "published_date": "2025-10-22 17:27:32 UTC",
      "updated_date": "2025-10-22 17:27:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:35.484030+00:00"
    },
    {
      "arxiv_id": "2510.19788v3",
      "title": "Benchmarking World-Model Learning",
      "title_zh": "ä¸–ç•Œæ¨¡å‹å­¦ä¹ çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Archana Warrier",
        "Dat Nguyen",
        "Michelangelo Naim",
        "Moksh Jain",
        "Yichao Liang",
        "Karen Schroeder",
        "Cambridge Yang",
        "Joshua B. Tenenbaum",
        "Sebastian Vollmer",
        "Kevin Ellis",
        "Zenna Tavares"
      ],
      "abstract": "Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose WorldTest, a protocol to evaluate model-learning agents that separates reward-free interaction from a scored test phase in a different but related environment. WorldTest is open-ended $\\unicode{x2014}$ models should support many different tasks unknown ahead of time $\\unicode{x2014}$ and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with AutumnBench, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template $\\unicode{x2014}$ reward-free exploration, derived tests, and behavior-based scoring $\\unicode{x2014}$ to evaluate what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ä¸–ç•Œæ¨¡å‹(World Models)å­¦ä¹ ä¸è¯„ä¼°è¿‡åº¦ä¾èµ–ä¸‹ä¸€å¸§é¢„æµ‹(next-frame prediction)å’Œå¥–åŠ±æœ€å¤§åŒ–(reward maximization)çš„é—®é¢˜ï¼Œæå‡ºäº†WorldTestè¯„ä¼°åè®®ã€‚WorldTestå°†æ— å¥–åŠ±äº¤äº’è¿‡ç¨‹ä¸åœ¨ç›¸å…³ç¯å¢ƒä¸­çš„è¯„åˆ†æµ‹è¯•é˜¶æ®µç›¸åˆ†ç¦»ï¼Œå…·æœ‰å¼€æ”¾æ€§ä¸”ä¸é™åˆ¶æ¨¡å‹è¡¨å¾ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¯¹å¤šç§æœªçŸ¥ä¸‹æ¸¸ä»»åŠ¡çš„æ”¯æŒèƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºæ­¤åè®®å¼€å‘äº†AutumnBenchå¥—ä»¶ï¼Œæ¶µç›–43ä¸ªäº¤äº’å¼ç½‘æ ¼ä¸–ç•Œç¯å¢ƒå’Œ129é¡¹æ¶‰åŠæ©ç å¸§é¢„æµ‹(masked-frame prediction)ã€è§„åˆ’åŠå› æœåŠ¨åŠ›å­¦(causal dynamics)é¢„æµ‹çš„ä»»åŠ¡ã€‚é€šè¿‡å¯¹517åäººç±»å‚ä¸è€…å’Œä¸‰ä¸ªå‰æ²¿æ¨¡å‹çš„å¯¹æ¯”æµ‹è¯•å‘ç°ï¼Œäººç±»çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸”å¢åŠ è®¡ç®—é‡ä»…åœ¨éƒ¨åˆ†ç¯å¢ƒä¸­èƒ½æå‡æ€§èƒ½ã€‚WorldTestä¸ºè¯„ä¼°æ™ºèƒ½ä½“å¯¹ç¯å¢ƒåŠ¨åŠ›å­¦çš„ç†è§£æä¾›äº†åŒ…å«æ— å¥–åŠ±æ¢ç´¢ã€è¡ç”Ÿæµ‹è¯•å’Œè¡Œä¸ºè¯„åˆ†çš„æ–°èŒƒå¼ï¼Œè€ŒAutumnBenchåˆ™æ­ç¤ºäº†å½“å‰ä¸–ç•Œæ¨¡å‹å­¦ä¹ é¢†åŸŸä»å­˜åœ¨å·¨å¤§çš„è¿›æ­¥ç©ºé—´ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19788v3",
      "published_date": "2025-10-22 17:23:18 UTC",
      "updated_date": "2025-12-09 23:04:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:40.898404+00:00"
    },
    {
      "arxiv_id": "2510.19892v1",
      "title": "Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities",
      "title_zh": "ä»–ä»¬èƒ½ç©è½¬ Dixit å—ï¼Ÿæ˜¯çš„ï¼Œä»–ä»¬å¯ä»¥ï¼å°† Dixit ä½œä¸ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„å®éªŒåœº",
      "authors": [
        "Nishant Balepur",
        "Dang Nguyen",
        "Dayeon Ki"
      ],
      "abstract": "Multi-modal large language models (MLMs) are often assessed on static, individual benchmarks -- which cannot jointly assess MLM capabilities in a single task -- or rely on human or model pairwise comparisons -- which is highly subjective, expensive, and allows models to exploit superficial shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these issues, we propose game-based evaluations to holistically assess MLM capabilities. Games require multiple abilities for players to win, are inherently competitive, and are governed by fix, objective rules, and makes evaluation more engaging, providing a robust framework to address the aforementioned challenges. We manifest this evaluation specifically through Dixit, a fantasy card game where players must generate captions for a card that trick some, but not all players, into selecting the played card. Our quantitative experiments with five MLMs show Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks, while games between human and MLM players in Dixit reveal several differences between agent strategies and areas of improvement for MLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLMs) è¯„ä¼°æ‰‹æ®µå¾€å¾€å—é™äºé™æ€åŸºå‡†æˆ–é«˜æˆæœ¬ã€æ˜“å—å¹²æ‰°çš„ä¸»è§‚æ¯”è¾ƒï¼Œéš¾ä»¥å…¨é¢è¡¡é‡æ¨¡å‹çš„ç»¼åˆèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åŸºäºæ¸¸æˆçš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å…·ä½“åˆ©ç”¨å¹»æƒ³å¡ç‰Œæ¸¸æˆ Dixit ä½œä¸ºæµ‹è¯•åœºï¼Œè¦æ±‚æ¨¡å‹ç”Ÿæˆæ ‡é¢˜ä»¥åœ¨åšå¼ˆä¸­è¿·æƒ‘å¯¹æ‰‹ã€‚é’ˆå¯¹äº”ç§ MLMs çš„å®šé‡å®éªŒæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ Dixit ä¸­çš„èƒœç‡æ’å (win-rate rankings) ä¸æµè¡Œè¯„ä¼°åŸºå‡†çš„è¡¨ç°å®Œç¾ç›¸å…³ã€‚é€šè¿‡äººç±»ä¸æ¨¡å‹ä¹‹é—´çš„å¯¹æˆ˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ä¸¤è€…åœ¨ç­–ç•¥é€‰æ‹©ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå¹¶è¯†åˆ«å‡º MLMs åœ¨æ¨ç† (reasoning) æ–¹é¢äºŸå¾…æ”¹è¿›çš„é¢†åŸŸã€‚è¯¥å·¥ä½œè¯æ˜äº†æ¸¸æˆåŒ–è¯„ä¼°ä½œä¸ºä¸€ç§å®¢è§‚ã€ç¨³å¥ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¡«è¡¥ä¼ ç»Ÿ MLMs è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a Spotlight paper at the EMNLP 2025 Wordplay Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.19892v1",
      "published_date": "2025-10-22 17:21:16 UTC",
      "updated_date": "2025-10-22 17:21:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:43.883117+00:00"
    },
    {
      "arxiv_id": "2510.19779v1",
      "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
      "title_zh": "AdaSPECï¼šé¢å‘é«˜æ•ˆæŠ•æœºè§£ç å™¨çš„é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦",
      "authors": [
        "Yuezhou Hu",
        "Jiaxin Guo",
        "Xinyu Feng",
        "Tuo Zhao"
      ],
      "abstract": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŠ•æœºè§£ç (Speculative Decoding)ä¸­è‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹å¯¹é½æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†AdaSPECï¼Œä¸€ç§åœ¨çŸ¥è¯†è’¸é¦(Knowledge Distillation)è¿‡ç¨‹ä¸­å¼•å…¥é€‰æ‹©æ€§ä»¤ç‰Œè¿‡æ»¤çš„æ–°æ–¹æ³•ã€‚ä¼ ç»Ÿè’¸é¦æ–¹æ³•é€šå¸¸è‡´åŠ›äºåœ¨æ‰€æœ‰ä»¤ç‰Œä¸Šæœ€å°åŒ–KLæ•£åº¦(KL divergence)ï¼Œä½†è¿™ä¸æŠ•æœºè§£ç æœ€å¤§åŒ–ä»¤ç‰Œæ¥å—ç‡çš„å®é™…ç›®æ ‡å¹¶ä¸å®Œå…¨ä¸€è‡´ï¼Œä¸”å—é™äºè‰ç¨¿æ¨¡å‹çš„å®¹é‡ï¼Œå¾€å¾€å¯¼è‡´æ€§èƒ½æ¬ ä½³ã€‚AdaSPECé€šè¿‡åˆ©ç”¨å‚è€ƒæ¨¡å‹è¯†åˆ«å¹¶è¿‡æ»¤æ‰éš¾ä»¥æ‹Ÿåˆçš„å¤æ‚ä»¤ç‰Œï¼Œä½¿è‰ç¨¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç®€å•ä»¤ç‰Œä¸Šä¸ç›®æ ‡æ¨¡å‹å®ç°æ›´å¥½çš„å¯¹é½ï¼Œä»è€Œåœ¨ä¸å½±å“ç”Ÿæˆè´¨é‡çš„å‰æä¸‹æ˜¾è‘—æå‡æ•´ä½“ä»¤ç‰Œæ¥å—ç‡ã€‚å®éªŒåœ¨ç®—æœ¯æ¨ç†ã€ä»£ç ç¼–å†™å’Œæ‘˜è¦ç”Ÿæˆç­‰å¤šç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜AdaSPECåœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„é…ç½®ä¸‹å‡ä¸€è‡´ä¼˜äºç°æœ‰çš„DistillSpecæ–¹æ³•ï¼Œæ¥å—ç‡æå‡æœ€é«˜å¯è¾¾15%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ„å»ºæ›´é«˜æ•ˆçš„æŠ•æœºè§£ç å™¨æä¾›äº†æ–°çš„è§†è§’ï¼Œç›¸å…³ä»£ç ä¹Ÿå·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19779v1",
      "published_date": "2025-10-22 17:13:00 UTC",
      "updated_date": "2025-10-22 17:13:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:57.876243+00:00"
    },
    {
      "arxiv_id": "2510.21849v3",
      "title": "TowerVision: Understanding and Improving Multilinguality in Vision-Language Models",
      "title_zh": "TowerVisionï¼šè§†è§‰è¯­è¨€æ¨¡å‹å¤šè¯­è¨€èƒ½åŠ›çš„ç†è§£ä¸æå‡",
      "authors": [
        "AndrÃ© G. Viveiros",
        "Patrick Fernandes",
        "Saul Santos",
        "Sonal Sannigrahi",
        "Emmanouil Zaranis",
        "Nuno M. Guerreiro",
        "Amin Farajian",
        "Pierre Colombo",
        "Graham Neubig",
        "AndrÃ© F. T. Martins"
      ],
      "abstract": "Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„è®¾è®¡é™åˆ¶ï¼Œæå‡ºäº†TowerVisionç³»åˆ—å¼€æ”¾å¤šè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡å›¾åƒ-æ–‡æœ¬å’Œè§†é¢‘-æ–‡æœ¬ä»»åŠ¡åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚TowerVisionåŸºäºå¤šè¯­è¨€æ–‡æœ¬æ¨¡å‹Tower+æ„å»ºï¼Œé€šè¿‡å¯¹è®­ç»ƒæ•°æ®ç»„æˆã€ç¼–ç å™¨é€‰æ‹©å’Œæ–‡æœ¬éª¨å¹²ç½‘ç»œç­‰è®¾è®¡è¦ç´ çš„å…¨é¢å®è¯ç ”ç©¶ï¼Œä¼˜åŒ–äº†è·¨è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¿˜æ¨å‡ºäº†é«˜è´¨é‡è§†è§‰è¯­è¨€æ•°æ®é›†VisionBlocksï¼Œå¹¶å‘ç°å¤šè¯­è¨€è®­ç»ƒæ•°æ®èƒ½æ˜¾è‘—æå‡ä»é«˜èµ„æºåˆ°ä½èµ„æºè¯­è¨€çš„è·¨è¯­è¨€æ³›åŒ–(cross-lingual generalization)èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒTowerVisionåœ¨ALM-Benchã€Multi30Kå’ŒViMUL-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ–‡åŒ–ç›¸å…³ä»»åŠ¡å’Œå¤šæ¨¡æ€ç¿»è¯‘æ–¹é¢è¶…è¶Šäº†è®­ç»ƒè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†æŒ‡ä»¤å¾®è°ƒ(instruction-tuned) LLMså¹¶ä¸æ€»æ˜¯æœ€ä¼˜çš„åˆå§‹åŒ–ç‚¹ï¼Œä¸ºæœªæ¥å¤šè¯­è¨€VLMsçš„å¼€å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 7 figures, submitted to arXiv October 2025. All models, datasets, and training code will be released at https://huggingface.co/collections/utter-project/towervision",
      "pdf_url": "https://arxiv.org/pdf/2510.21849v3",
      "published_date": "2025-10-22 17:02:48 UTC",
      "updated_date": "2025-11-06 11:09:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:57:59.877808+00:00"
    },
    {
      "arxiv_id": "2510.19771v2",
      "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
      "title_zh": "è¶…è¶Šååº”æ€§ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ä¸»åŠ¨é—®é¢˜è§£å†³èƒ½åŠ›",
      "authors": [
        "Gil Pasternak",
        "Dheeraj Rajagopal",
        "Julia White",
        "Dhruv Atreja",
        "Matthew Thomas",
        "George Hurn-Maloney",
        "Ash Lewis"
      ],
      "abstract": "LLM-based agents are increasingly moving towards proactivity: rather than awaiting instruction, they exercise agency to anticipate user needs and solve them autonomously. However, evaluating proactivity is challenging; current benchmarks are constrained to localized context, limiting their ability to test reasoning across sources and longer time horizons. To address this gap, we present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes proactivity as a pipeline of three core capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular agentic frameworks, showing that even state-of-the-art models struggle to solve this benchmark. Computing our consistent measurements across frontier LLMs and agents, we find that the best end-to-end performance of 40% is achieved by both GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative capabilities of each model and analyze mutual failure modes. Our results highlight the current limitations of autonomous action in agentic systems, and expose promising future research directions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“åœ¨ä¸»åŠ¨æ€§(proactivity)è¯„ä¼°æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†PROBE (Proactive Resolution Of BottlEnecks) åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚PROBE å°†ä¸»åŠ¨æ€§åˆ†è§£ä¸ºå¯»æ‰¾æœªæ˜ç¡®é—®é¢˜ã€è¯†åˆ«ç‰¹å®šç“¶é¢ˆä»¥åŠæ‰§è¡Œé€‚å½“è§£å†³æ–¹æ¡ˆä¸‰ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼Œæ—¨åœ¨å…‹æœç°æœ‰åŸºå‡†æµ‹è¯•ä»…å±€é™äºå±€éƒ¨ä¸Šä¸‹æ–‡å’ŒçŸ­æ—¶é—´è·¨åº¦çš„ä¸è¶³ã€‚é€šè¿‡å¯¹å‰æ²¿å¤§æ¨¡å‹å’Œæ™ºèƒ½ä½“æ¡†æ¶çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†è¯¥åŸºå‡†æ—¶ä¹Ÿè¡¨ç°æ¬ ä½³ï¼ŒGPT-5 å’Œ Claude Opus-4.1 çš„ç«¯åˆ°ç«¯æœ€ä½³è¡¨ç°ä»…ä¸º40%ã€‚è¯¥é¡¹å·¥ä½œæ·±å…¥åˆ†æäº†å„æ¨¡å‹çš„ç›¸å¯¹èƒ½åŠ›åŠå…±åŒçš„å¤±æ•ˆæ¨¡å¼ï¼Œæ­ç¤ºäº†å½“å‰æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è‡ªä¸»è¡ŒåŠ¨æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19771v2",
      "published_date": "2025-10-22 17:00:45 UTC",
      "updated_date": "2025-10-29 20:33:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:01.471556+00:00"
    },
    {
      "arxiv_id": "2510.19767v1",
      "title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration",
      "title_zh": "SmartSwitchï¼šé€šè¿‡ä¿ƒè¿›æ·±å±‚æ€ç»´æ¢ç´¢å…‹æœâ€œæ€è€ƒä¸è¶³â€ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xichen Zhang",
        "Sitong Wu",
        "Haoru Tan",
        "Shaozuo Yu",
        "Yinghao Zhu",
        "Ziyi He",
        "Jiaya Jia"
      ],
      "abstract": "The long chain-of-thought (LongCoT) capability is central to the recent breakthroughs achieved by large language models in complex reasoning tasks. However, the accompanying issue of ''underthinking'', where models exhibit shallow reasoning by frequently switching thoughts without sufficient exploration, limits both performance and token efficiency. To address this problem, we propose a simple yet effective reasoning strategy: the SmartSwitch inference framework. This framework can be easily integrated into any large language model as a plug-and-play solution, continuously monitoring the model's reasoning process to detect underthinking and guide it toward deeper exploration of promising but overlooked thoughts. Specifically, the perception module identifies points where thoughts switch and evaluates the potential of the preceding thought using an off-the-shelf process reward model (PRM). If a high-potential thought is found to be prematurely abandoned, the intervention module interrupts the ongoing inference, backtracks to the point before the switch, and inserts a \"deepening prompt\" to encourage further exploration along that promising path. Extensive experiments on challenging mathematical reasoning benchmarks demonstrate that our method significantly enhances the performance of various large language models of different sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾å¼æ€ç»´(LongCoT)æ¨ç†ä¸­å­˜åœ¨çš„â€œUnderthinkingâ€é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨æœªå……åˆ†æ¢ç´¢çš„æƒ…å†µä¸‹é¢‘ç¹åˆ‡æ¢æ€è·¯ï¼Œå¯¼è‡´æ¨ç†æ·±åº¦ä¸è¶³ä¸”æ•ˆç‡æœ‰é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†SmartSwitchæ¨ç†æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿé›†æˆåˆ°ä»»ä½•å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å³æ’å³ç”¨å‹è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç›‘æ§æ¨ç†è¿‡ç¨‹æ¥å¼•å¯¼æ¨¡å‹è¿›è¡Œæ·±å±‚æ¢ç´¢ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ„ŸçŸ¥æ¨¡å—è¯†åˆ«æ€è·¯åˆ‡æ¢ç‚¹ï¼Œå¹¶ç»“åˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRM)è¯„ä¼°å‰åºæ€è·¯çš„æ½œåŠ›ã€‚ä¸€æ—¦å‘ç°é«˜æ½œåŠ›æ€è·¯è¢«è¿‡æ—©æ”¾å¼ƒï¼Œå¹²é¢„æ¨¡å—å°†é€šè¿‡å›æº¯å¹¶æ’å…¥â€œDeepening Promptâ€çš„æ–¹å¼å¼ºåˆ¶æ¨¡å‹æ²¿è¯¥è·¯å¾„æ·±å…¥æ€è€ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSmartSwitchåœ¨å¤æ‚çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å¢å¼ºäº†å„ç§è§„æ¨¡æ¨¡å‹çš„æ€§èƒ½ï¼Œæœ‰æ•ˆå…‹æœäº†æµ…å±‚æ¨ç†çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Code: https://github.com/dvlab-research/SmartSwitch",
      "pdf_url": "https://arxiv.org/pdf/2510.19767v1",
      "published_date": "2025-10-22 16:56:01 UTC",
      "updated_date": "2025-10-22 16:56:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:10.668872+00:00"
    },
    {
      "arxiv_id": "2510.19755v3",
      "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation",
      "title_zh": "æ‰©æ•£æ¨¡å‹ç¼“å­˜æ–¹æ³•ç»¼è¿°ï¼šè¿ˆå‘é«˜æ•ˆå¤šæ¨¡æ€ç”Ÿæˆ",
      "authors": [
        "Jiacheng Liu",
        "Xinyu Wang",
        "Yuqi Lin",
        "Zhikai Wang",
        "Peiru Wang",
        "Peiliang Cai",
        "Qinming Zhou",
        "Zhengan Yan",
        "Zexuan Yan",
        "Zhengyi Shi",
        "Chang Zou",
        "Yue Ma",
        "Linfeng Zhang"
      ],
      "abstract": "Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.\n  Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}.",
      "tldr_zh": "æ‰©æ•£æ¨¡å‹ (Diffusion Models) è™½ç„¶å…·æœ‰å“è¶Šçš„ç”Ÿæˆè´¨é‡ï¼Œä½†å…¶å¤šæ­¥è¿­ä»£å’Œå¤æ‚çš„éª¨å¹²ç½‘ç»œå¯¼è‡´äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œæˆä¸ºå®æ—¶åº”ç”¨çš„ä¸»è¦ç“¶é¢ˆã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥ç»¼è¿°ç³»ç»Ÿæ¢è®¨äº† Diffusion Cachingï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒä¸”ä¸æ¶æ„æ— å…³çš„é«˜æ•ˆæ¨ç†èŒƒå¼ã€‚å…¶æ ¸å¿ƒæœºåˆ¶åœ¨äºè¯†åˆ«å¹¶å¤ç”¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„å†…åœ¨è®¡ç®—å†—ä½™ï¼Œé€šè¿‡ç‰¹å¾çº§çš„è·¨æ­¥å¤ç”¨ (cross-step reuse) å’Œå±‚é—´è°ƒåº¦ (inter-layer scheduling) åœ¨ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘è®¡ç®—é‡ã€‚æœ¬æ–‡å›é¡¾äº†è¯¥æŠ€æœ¯çš„ç†è®ºåŸºç¡€ä¸æ¼”è¿›å†ç¨‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»ä¸åˆ†ææ¡†æ¶ã€‚ç ”ç©¶å‘ç° Diffusion Caching æ­£åœ¨ä»é™æ€å¤ç”¨ (static reuse) å‘åŠ¨æ€é¢„æµ‹ (dynamic prediction) æ¼”è¿›ï¼Œå¹¶èƒ½ä¸é‡‡æ ·ä¼˜åŒ–ã€æ¨¡å‹è’¸é¦ç­‰å…¶ä»–åŠ é€ŸæŠ€æœ¯æ·±åº¦é›†æˆã€‚è¯¥èŒƒå¼ä¸ºæœªæ¥å¤šæ¨¡æ€å’Œäº¤äº’å¼åº”ç”¨çš„é«˜æ•ˆæ¨ç†æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œå°†æˆä¸ºå®ç°å®æ—¶ã€é«˜æ•ˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Efficient Generative Intelligence) çš„å…³é”®é©±åŠ¨åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages,2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19755v3",
      "published_date": "2025-10-22 16:46:05 UTC",
      "updated_date": "2025-11-01 08:49:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:11.388288+00:00"
    },
    {
      "arxiv_id": "2510.19889v1",
      "title": "From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem",
      "title_zh": "ä»ä¼˜åŒ–åˆ°é¢„æµ‹ï¼šé¢å‘äº¤é€šåˆ†é…é—®é¢˜çš„åŸºäº Transformer çš„è·¯å¾„æµé‡ä¼°è®¡",
      "authors": [
        "Mostafa Ameli",
        "Van Anh Le",
        "Sulthana Shams",
        "Alexander Skabardonis"
      ],
      "abstract": "The traffic assignment problem is essential for traffic flow analysis, traditionally solved using mathematical programs under the Equilibrium principle. These methods become computationally prohibitive for large-scale networks due to non-linear growth in complexity with the number of OD pairs. This study introduces a novel data-driven approach using deep neural networks, specifically leveraging the Transformer architecture, to predict equilibrium path flows directly. By focusing on path-level traffic distribution, the proposed model captures intricate correlations between OD pairs, offering a more detailed and flexible analysis compared to traditional link-level approaches. The Transformer-based model drastically reduces computation time, while adapting to changes in demand and network structure without the need for recalculation. Numerical experiments are conducted on the Manhattan-like synthetic network, the Sioux Falls network, and the Eastern-Massachusetts network. The results demonstrate that the proposed model is orders of magnitude faster than conventional optimization. It efficiently estimates path-level traffic flows in multi-class networks, reducing computational costs and improving prediction accuracy by capturing detailed trip and flow information. The model also adapts flexibly to varying demand and network conditions, supporting traffic management and enabling rapid `what-if' analyses for enhanced transportation planning and policy-making.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Transformer æ¶æ„çš„æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿäº¤é€šåˆ†é…é—®é¢˜ (Traffic Assignment Problem) åœ¨å¤§è§„æ¨¡ç½‘ç»œä¸­å› éçº¿æ€§å¤æ‚åº¦å¯¼è‡´è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹æ‘’å¼ƒäº†åŸºäº Equilibrium principle çš„ä¼ ç»Ÿæ•°å­¦è§„åˆ’ï¼Œè½¬è€Œåˆ©ç”¨æ•°æ®é©±åŠ¨çš„æ–¹å¼ç›´æ¥é¢„æµ‹å‡è¡¡è·¯å¾„æµé‡ (Path-flow estimation)ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰ OD å¯¹ä¹‹é—´å¤æ‚çš„å…³è”æ€§ã€‚ä¸ä¼ ç»Ÿçš„è·¯æ®µçº§æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æä¾›äº†æ›´è¯¦å°½ä¸”çµæ´»çš„è·¯å¾„çº§æµé‡åˆ†æï¼Œåœ¨æ˜¾è‘—ç¼©çŸ­è®¡ç®—æ—¶é—´çš„åŒæ—¶ï¼Œèƒ½æ•æ·é€‚åº”éœ€æ±‚å’Œç½‘ç»œç»“æ„çš„å˜åŒ–ã€‚é€šè¿‡åœ¨ Manhattan-likeã€Sioux Falls å’Œ Eastern-Massachusetts ç½‘ç»œä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºæ¯”ä¼ ç»Ÿä¼˜åŒ–ç®—æ³•å¿«å‡ ä¸ªæ•°é‡çº§çš„è®¡ç®—é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç±»ç½‘ç»œä¸­æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬å¹¶æå‡äº†é¢„æµ‹ç²¾åº¦ï¼Œä¸ºäº¤é€šç®¡ç†ä¸­çš„å¿«é€Ÿ â€œwhat-if analysesâ€ ä»¥åŠäº¤é€šè§„åˆ’å†³ç­–æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19889v1",
      "published_date": "2025-10-22 16:45:12 UTC",
      "updated_date": "2025-10-22 16:45:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:12.276182+00:00"
    },
    {
      "arxiv_id": "2510.19752v1",
      "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
      "title_zh": "é¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ¨ç†æ—¶ç¤ºèƒ½å­¦ä¹ ",
      "authors": [
        "Ameesh Shah",
        "William Chen",
        "Adwait Godbole",
        "Federico Mora",
        "Sanjit A. Seshia",
        "Sergey Levine"
      ],
      "abstract": "Solving complex real-world control tasks often takes multiple tries: if we fail at first, we reflect on what went wrong, and change our strategy accordingly to avoid making the same mistake. In robotics, Vision-Language-Action models (VLAs) offer a promising path towards solving complex control tasks, but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task. In this work, we introduce Learning from Inference-Time Execution (LITEN), which connects a VLA low-level policy to a high-level VLM that conditions on past experiences by including them in-context, allowing it to learn the affordances and capabilities of the low-level VLA. Our approach iterates between a reasoning phase that generates and executes plans for the low-level VLA, and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts. Unlike similar approaches to self-refinement in non-robotics domains, LITEN must reflect on unstructured real-world robot trajectories (e.g., raw videos), which requires structured guiderails during assessment. Our experimental results demonstrate LITEN is able to effectively learn from past experience to generate plans that use high-affordance instructions to accomplish long-horizon tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LITEN (Learning from Inference-Time Execution)ï¼Œæ—¨åœ¨è§£å†³ Vision-Language-Action models (VLAs) åœ¨å¤„ç†å¤æ‚ç°å®æ§åˆ¶ä»»åŠ¡æ—¶ç¼ºä¹æ ¹æ®å¤±è´¥ç»éªŒåŠ¨æ€è°ƒæ•´è¡Œä¸ºèƒ½åŠ›çš„é—®é¢˜ã€‚LITEN å°†åº•å±‚çš„ VLA ç­–ç•¥ä¸é«˜å±‚çš„ VLM ç›¸è¿æ¥ï¼Œé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­åŒ…å«è¿‡å¾€ç»éªŒï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åº•å±‚ VLA çš„ affordances å’Œå„é¡¹èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¹¶æ‰§è¡Œè®¡åˆ’çš„æ¨ç†é˜¶æ®µä¸åæ˜ æ‰§è¡Œç»“æœå¹¶æ€»ç»“ç»“è®ºçš„è¯„ä¼°é˜¶æ®µä¹‹é—´è¿›è¡Œè¿­ä»£ï¼Œå®ç°äº†é—­ç¯çš„è‡ªæˆ‘æ¼”è¿›ã€‚é’ˆå¯¹éç»“æ„åŒ–çš„æœºå™¨äººè½¨è¿¹ï¼ˆå¦‚åŸå§‹è§†é¢‘ï¼‰ï¼ŒLITEN åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å¼•å…¥äº†ç»“æ„åŒ–çš„ guiderails ä»¥è¾…åŠ©æ¨¡å‹æå–æœ‰æ•ˆåé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLITEN èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»è¿‡å¾€ç»éªŒä¸­å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨ high-affordance æŒ‡ä»¤ç”Ÿæˆå¯æˆåŠŸå®Œæˆ long-horizon ä»»åŠ¡çš„æ‰§è¡Œè®¡åˆ’ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages and appendix",
      "pdf_url": "https://arxiv.org/pdf/2510.19752v1",
      "published_date": "2025-10-22 16:43:29 UTC",
      "updated_date": "2025-10-22 16:43:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:17.575494+00:00"
    },
    {
      "arxiv_id": "2510.19738v2",
      "title": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior",
      "title_zh": "Misalignment Bountyï¼šAI æ™ºèƒ½ä½“ä¸å½“è¡Œä¸ºä¼—åŒ…",
      "authors": [
        "Rustem Turtayev",
        "Natalia Fedorova",
        "Oleg Serikov",
        "Sergey Koldyba",
        "Lev Avagyan",
        "Dmitrii Volkov"
      ],
      "abstract": "Advanced AI systems sometimes act in ways that differ from human intent. To gather clear, reproducible examples, we ran the Misalignment Bounty: a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. The bounty received 295 submissions, of which nine were awarded.\n  This report explains the program's motivation and evaluation criteria, and walks through the nine winning submissions step by step.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…ˆè¿› AI ç³»ç»Ÿåœ¨è¿è¡Œä¸­åç¦»äººç±»æ„å›¾çš„ç°è±¡ï¼Œå¹¶å¯åŠ¨äº†åä¸º Misalignment Bounty çš„ä¼—åŒ…é¡¹ç›®ï¼Œæ—¨åœ¨æ”¶é›† AI Agent è¿½æ±‚éé¢„æœŸæˆ–ä¸å®‰å…¨ç›®æ ‡çš„æ¸…æ™°ä¸”å¯å¤ç°çš„æ¡ˆä¾‹ã€‚è¯¥é¡¹ç›®é€šè¿‡ Crowdsourcing çš„æ–¹å¼ä»ç¤¾åŒºå¾é›†äº† 295 ä»½å…³äºæ™ºèƒ½ä½“è¡Œä¸ºå¤±è°ƒçš„æäº¤ï¼Œå¹¶æ ¹æ®ç‰¹å®šçš„è¯„ä»·æ ‡å‡†æœ€ç»ˆè¯„é€‰å‡º 9 ä»½è·å¥–æ¡ˆä¾‹ã€‚æŠ¥å‘Šè¯¦ç»†é˜è¿°äº†è¯¥è®¡åˆ’çš„å¯åŠ¨åŠ¨æœºå’Œ Evaluation Criteriaï¼Œå¹¶å¯¹è¿™ 9 ä¸ªè·å¥–æ¡ˆä¾‹ä¸­ Agent å¦‚ä½•äº§ç”Ÿ Misalignment çš„å…·ä½“æ­¥éª¤è¿›è¡Œäº†é€ä¸€å¤ç°å’Œæ·±åº¦è§£æã€‚é€šè¿‡è¿™äº›çœŸå®æ¡ˆä¾‹çš„æ±‡æ€»ä¸åˆ†æï¼Œè¯¥å·¥ä½œä¸ºç†è§£ã€è¯†åˆ«åŠé˜²èŒƒ AI ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä¸å®‰å…¨è¡Œä¸ºæä¾›äº†é‡è¦çš„å®è¯å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Add Limitations section",
      "pdf_url": "https://arxiv.org/pdf/2510.19738v2",
      "published_date": "2025-10-22 16:28:48 UTC",
      "updated_date": "2025-11-05 13:34:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:23.279442+00:00"
    },
    {
      "arxiv_id": "2510.19732v2",
      "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
      "title_zh": "Memoï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒé«˜å†…å­˜æ•ˆç‡çš„å…·èº«æ™ºèƒ½ä½“",
      "authors": [
        "Gunshi Gupta",
        "Karmesh Yadav",
        "Zsolt Kira",
        "Yarin Gal",
        "Rahaf Aljundi"
      ],
      "abstract": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints. Our code is available at: https://github.com/gunshi/memo.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½ä½“(Embodied Agents)åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­é¢ä¸´çš„æ˜¾å­˜é™åˆ¶å’Œä¸Šä¸‹æ–‡æº¢å‡ºé—®é¢˜ï¼Œæå‡ºäº† Memoï¼Œä¸€ç§ä¸“ä¸ºé•¿æ—¶ç¨‹ã€è®°å¿†å¯†é›†å‹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä»»åŠ¡è®¾è®¡çš„ Transformer æ¶æ„åŠè®­ç»ƒæ–¹æ¡ˆã€‚Memo é€šè¿‡åœ¨è®­ç»ƒæœŸé—´å°†å‘¨æœŸæ€§çš„æ‘˜è¦ä»¤ç‰Œ(Summarization Tokens)ä¸æ¨¡å‹è¾“å…¥äº¤ç»‡ï¼Œæ¨¡æ‹Ÿäººç±»å‹ç¼©ç»éªŒçš„æ–¹å¼å®ç°äº†è®°å¿†çš„æœ‰æ•ˆåˆ›å»ºä¸æ£€ç´¢ã€‚åœ¨ Gridworld meta-RL åŸºå‡†æµ‹è¯•å’Œå…‰ç…§å†™å®å®¤å†…ç¯å¢ƒçš„å¤šç›®æ ‡å¯¼èˆªä»»åŠ¡ä¸­ï¼ŒMemo çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é•¿ä¸Šä¸‹æ–‡ Transformer åŸºå‡†æ¨¡å‹ï¼Œä¸”åœ¨è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µå±•ç°å‡ºå“è¶Šçš„é•¿ä¸Šä¸‹æ–‡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¿…é¡»æˆªæ–­å†å²ä¿¡æ¯çš„æµå¼è®¾ç½®(Streaming Settings)ä¸­ä¿æŒäº†æå¼ºçš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶ä¸ºè®­ç»ƒé«˜æ•ˆã€å…·å¤‡é•¿æœŸè®°å¿†èƒ½åŠ›çš„è‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for Spotlight Presentation at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19732v2",
      "published_date": "2025-10-22 16:24:47 UTC",
      "updated_date": "2025-11-27 02:24:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:30.991974+00:00"
    },
    {
      "arxiv_id": "2510.19728v1",
      "title": "Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series",
      "title_zh": "é€šè¿‡åˆæˆåŒ»å­¦æ—¶é—´åºåˆ—ç”Ÿæˆå®ç°ç»†ç²’åº¦äºšç»„çº§æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Mahmoud Ibrahim",
        "Bart Elen",
        "Chang Sun",
        "GÃ¶khan Ertaylan",
        "Michel Dumontier"
      ],
      "abstract": "We present a novel framework for leveraging synthetic ICU time-series data not only to train but also to rigorously and trustworthily evaluate predictive models, both at the population level and within fine-grained demographic subgroups. Building on prior diffusion and VAE-based generators (TimeDiff, HealthGen, TimeAutoDiff), we introduce \\textit{Enhanced TimeAutoDiff}, which augments the latent diffusion objective with distribution-alignment penalties. We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality and binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff reduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS gap'') by over 70\\%, achieving $Î”_{TRTS} \\leq 0.014$ AUROC, while preserving training utility ($Î”_{TSTR} \\approx 0.01$). Crucially, for 32 intersectional subgroups, large synthetic cohorts cut subgroup-level AUROC estimation error by up to 50\\% relative to small real test sets, and outperform them in 72--84\\% of subgroups. This work provides a practical, privacy-preserving roadmap for trustworthy, granular model evaluation in critical care, enabling robust and reliable performance analysis across diverse patient populations without exposing sensitive EHR data, contributing to the overall trustworthiness of Medical AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨åˆæˆICUæ—¶é—´åºåˆ—æ•°æ®å¯¹é¢„æµ‹æ¨¡å‹è¿›è¡Œç»†ç²’åº¦äººå£ç»Ÿè®¡å­ç»„(Subgroup)è¯„ä¼°çš„æ–°å‹æ¡†æ¶ã€‚é€šè¿‡åœ¨TimeDiffã€HealthGenå’ŒTimeAutoDiffç­‰æ¨¡å‹åŸºç¡€ä¸Šå¼•å…¥Enhanced TimeAutoDiffï¼Œè¯¥æ–¹æ³•åœ¨æ½œåœ¨æ‰©æ•£ç›®æ ‡(Latent Diffusion Objective)ä¸­åŠ å…¥äº†åˆ†å¸ƒå¯¹é½æƒ©ç½š(Distribution-Alignment Penalties)ä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ã€‚å®éªŒåœ¨MIMIC-IIIå’ŒeICUæ•°æ®é›†ä¸Šé’ˆå¯¹24å°æ—¶æ­»äº¡ç‡åŠä½é™¢æ—¶é—´ä»»åŠ¡è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜Enhanced TimeAutoDiffå°†çœŸå®ä¸åˆæˆè¯„ä¼°é—´çš„å·®è·(TRTS Gap)é™ä½äº†70%ä»¥ä¸Šï¼Œä¸”Î”TRTSä¼˜äº0.014 AUROCã€‚å…³é”®åœ¨äºï¼Œå¯¹äº32ä¸ªäº¤å‰å­ç»„ï¼Œå¤§è§„æ¨¡åˆæˆé˜Ÿåˆ—å°†å­ç»„ç»´åº¦çš„AUROCä¼°å€¼è¯¯å·®è¾ƒçœŸå®å°æ ·æœ¬é›†é™ä½äº†å¤šè¾¾50%ï¼Œåœ¨72-84%çš„å­ç»„ä¸­è¡¨ç°æ›´ä½³ã€‚è¯¥ç ”ç©¶ä¸ºé‡ç—‡ç›‘æŠ¤é¢†åŸŸæä¾›äº†ä¿æŠ¤éšç§ä¸”å¯ä¿¡çš„æ¨¡å‹è¯„ä¼°æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¸æ³„éœ²æ•æ„Ÿç”µå­å¥åº·è®°å½•(EHR)çš„å‰æä¸‹ï¼Œå®ç°è·¨å¤šå…ƒåŒ–æ‚£è€…ç¾¤ä½“çš„é²æ£’æ€§èƒ½åˆ†æï¼Œæå‡äº†åŒ»ç–—äººå·¥æ™ºèƒ½(Medical AI)çš„æ•´ä½“å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19728v1",
      "published_date": "2025-10-22 16:17:29 UTC",
      "updated_date": "2025-10-22 16:17:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:28.170738+00:00"
    },
    {
      "arxiv_id": "2510.21846v1",
      "title": "Training data membership inference via Gaussian process meta-modeling: a post-hoc analysis approach",
      "title_zh": "åŸºäºé«˜æ–¯è¿‡ç¨‹å…ƒå»ºæ¨¡çš„è®­ç»ƒæ•°æ®æˆå‘˜æ¨ç†ï¼šä¸€ç§äº‹ååˆ†ææ–¹æ³•",
      "authors": [
        "Yongchao Huang",
        "Pengfei Zhang",
        "Shahzad Mumtaz"
      ],
      "abstract": "Membership inference attacks (MIAs) test whether a data point was part of a model's training set, posing serious privacy risks. Existing methods often depend on shadow models or heavy query access, which limits their practicality. We propose GP-MIA, an efficient and interpretable approach based on Gaussian process (GP) meta-modeling. Using post-hoc metrics such as accuracy, entropy, dataset statistics, and optional sensitivity features (e.g. gradients, NTK measures) from a single trained model, GP-MIA trains a GP classifier to distinguish members from non-members while providing calibrated uncertainty estimates. Experiments on synthetic data, real-world fraud detection data, CIFAR-10, and WikiText-2 show that GP-MIA achieves high accuracy and generalizability, offering a practical alternative to existing MIAs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMembership inference attacks, MIAsï¼‰åœ¨éšç§é£é™©è¯„ä¼°ä¸­é¢ä¸´çš„å½±å­æ¨¡å‹ä¾èµ–å’ŒæŸ¥è¯¢å¼€é”€è¿‡å¤§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯è¿‡ç¨‹ï¼ˆGaussian process, GPï¼‰å…ƒå»ºæ¨¡çš„æ–°å‹æ–¹æ³• GP-MIAã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹å•ä¸ªå·²è®­ç»ƒæ¨¡å‹çš„äº‹ååˆ†æï¼ˆpost-hoc analysisï¼‰ï¼Œæå–åŒ…æ‹¬å‡†ç¡®ç‡ã€ç†µã€æ•°æ®é›†ç»Ÿè®¡é‡ä»¥åŠæ¢¯åº¦å’Œç¥ç»åˆ‡å‘æ ¸ï¼ˆNTKï¼‰åº¦é‡åœ¨å†…çš„æ•æ„Ÿç‰¹å¾ã€‚GP-MIA åˆ©ç”¨è¿™äº›ç‰¹å¾è®­ç»ƒ GP åˆ†ç±»å™¨æ¥åŒºåˆ†è®­ç»ƒé›†æˆå‘˜ä¸éæˆå‘˜ï¼Œå¹¶èƒ½æä¾›æ ¡å‡†åçš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚åœ¨åˆæˆæ•°æ®ã€çœŸå®æ¬ºè¯ˆæ£€æµ‹ã€CIFAR-10 å’Œ WikiText-2 ç­‰å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGP-MIA å…·å¤‡æé«˜çš„å‡†ç¡®ç‡å’Œå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„éšç§é£é™©æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€å¯è§£é‡Šä¸”æ— éœ€é‡åº¦æŸ¥è¯¢çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.21846v1",
      "published_date": "2025-10-22 16:10:47 UTC",
      "updated_date": "2025-10-22 16:10:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:46.593813+00:00"
    },
    {
      "arxiv_id": "2510.19698v1",
      "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models",
      "title_zh": "RLIEï¼šåŸºäºé€»è¾‘å›å½’ã€è¿­ä»£ç»†åŒ–ä¸è¯„ä¼°çš„å¤§è¯­è¨€æ¨¡å‹è§„åˆ™ç”Ÿæˆ",
      "authors": [
        "Yang Yang",
        "Hua XU",
        "Zhangyi Hu",
        "Yutao Yue"
      ],
      "abstract": "Large Language Models (LLMs) can propose rules in natural language, sidestepping the need for a predefined predicate space in traditional rule learning. Yet many LLM-based approaches ignore interactions among rules, and the opportunity to couple LLMs with probabilistic rule learning for robust inference remains underexplored. We present RLIE, a unified framework that integrates LLMs with probabilistic modeling to learn a set of weighted rules. RLIE has four stages: (1) Rule generation, where an LLM proposes and filters candidates; (2) Logistic regression, which learns probabilistic weights for global selection and calibration; (3) Iterative refinement, which updates the rule set using prediction errors; and (4) Evaluation, which compares the weighted rule set as a direct classifier with methods that inject rules into an LLM. We evaluate multiple inference strategies on real-world datasets. Applying rules directly with their learned weights yields superior performance, whereas prompting LLMs with the rules, weights, and logistic-model outputs surprisingly degrades accuracy. This supports the view that LLMs excel at semantic generation and interpretation but are less reliable for precise probabilistic integration. RLIE clarifies the potential and limitations of LLMs for inductive reasoning and couples them with classic probabilistic rule combination methods to enable more reliable neuro-symbolic reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RLIEï¼Œè¿™æ˜¯ä¸€ä¸ªå°†Large Language Models (LLMs) ä¸æ¦‚ç‡å»ºæ¨¡ç›¸ç»“åˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ ä¸€ç»„å¸¦æƒé‡çš„è§„åˆ™ã€‚RLIEæµç¨‹åŒ…å«å››ä¸ªé˜¶æ®µï¼šç”±LLMç”Ÿæˆå€™é€‰è§„åˆ™ã€åˆ©ç”¨Logistic Regressionå­¦ä¹ æ¦‚ç‡æƒé‡ã€åŸºäºé¢„æµ‹è¯¯å·®è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»¥åŠå¯¹è§„åˆ™é›†çš„åˆ†ç±»æ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œç›´æ¥åº”ç”¨å­¦ä¹ åˆ°çš„æƒé‡è¿›è¡Œè§„åˆ™ç»„åˆçš„æ•ˆæœä¼˜äºå°†è§„åˆ™ã€æƒé‡åŠæ¨¡å‹è¾“å‡ºåé¦ˆç»™LLMè¿›è¡Œæ¨ç†ï¼Œè¿™è¡¨æ˜LLMsè™½æ“…é•¿è¯­ä¹‰ç”Ÿæˆä¸è§£é‡Šï¼Œä½†åœ¨ç²¾ç¡®çš„æ¦‚ç‡é›†æˆä¸Šå¯é æ€§è¾ƒä½ã€‚RLIEé€šè¿‡å°†LLMsä¸ç»å…¸çš„æ¦‚ç‡è§„åˆ™ç»„åˆæ–¹æ³•ç›¸è€¦åˆï¼Œæ˜ç¡®äº†LLMsåœ¨Inductive Reasoningä¸­çš„å±€é™æ€§ï¼Œå¹¶ä¸ºå®ç°æ›´å¯é çš„Neuro-Symbolic Reasoningæä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19698v1",
      "published_date": "2025-10-22 15:50:04 UTC",
      "updated_date": "2025-10-22 15:50:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:53.385266+00:00"
    },
    {
      "arxiv_id": "2510.19694v1",
      "title": "Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings",
      "title_zh": "æç¤ºæ˜¯å¦é‡å¡‘äº†è¡¨å¾ï¼Ÿå…³äºæç¤ºæ•ˆåº”å¯¹åµŒå…¥å½±å“çš„å®è¯ç ”ç©¶",
      "authors": [
        "Cesar Gonzalez-Gutierrez",
        "Dirk Hovy"
      ],
      "abstract": "Prompting is a common approach for leveraging LMs in zero-shot settings. However, the underlying mechanisms that enable LMs to perform diverse tasks without task-specific supervision remain poorly understood. Studying the relationship between prompting and the quality of internal representations can shed light on how pre-trained embeddings may support in-context task solving. In this empirical study, we conduct a series of probing experiments on prompt embeddings, analyzing various combinations of prompt templates for zero-shot classification. Our findings show that while prompting affects the quality of representations, these changes do not consistently correlate with the relevance of the prompts to the target task. This result challenges the assumption that more relevant prompts necessarily lead to better representations. We further analyze potential factors that may contribute to this unexpected behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—æ¢æµ‹å®éªŒ(probing experiments)ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†æç¤º(Prompting)å¦‚ä½•å½±å“é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤º(internal representations)ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹åœ¨é›¶æ ·æœ¬(zero-shot)ç¯å¢ƒä¸‹å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡çš„åº•å±‚æœºåˆ¶ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œåˆ†æäº†ä¸åŒæç¤ºæ¨¡æ¿(prompt templates)ç»„åˆå¯¹æç¤ºåµŒå…¥(prompt embeddings)è´¨é‡çš„å½±å“ã€‚å®éªŒå‘ç°ï¼Œè™½ç„¶æç¤ºç¡®å®ä¼šæ”¹å˜è¡¨ç¤ºçš„è´¨é‡ï¼Œä½†è¿™äº›å˜åŒ–ä¸æç¤ºå¯¹ç›®æ ‡ä»»åŠ¡çš„ç›¸å…³æ€§(relevance)ä¹‹é—´å¹¶ä¸å­˜åœ¨ä¸€è‡´çš„ç›¸å…³æ€§ã€‚è¿™ä¸€ç»“æœæŒ‘æˆ˜äº†â€œæ›´ç›¸å…³çš„æç¤ºå¿…ç„¶å¯¼è‡´æ›´ä¼˜è¡¨ç¤ºâ€çš„æ™®éå‡è®¾ï¼Œæ­ç¤ºäº†æç¤ºæ•ˆåº”ä¸ä»»åŠ¡è¡¨ç°ä¹‹é—´å¤æ‚çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†å¯èƒ½å¯¼è‡´è¿™ç§æ„å¤–è¡Œä¸ºçš„æ½œåœ¨å› ç´ ï¼Œä¸ºç†è§£è¯­è¨€æ¨¡å‹å¦‚ä½•æ”¯æŒè¯­å¢ƒä¸­ä»»åŠ¡è§£å†³(in-context task solving)æä¾›äº†æ–°çš„å®è¯è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19694v1",
      "published_date": "2025-10-22 15:43:40 UTC",
      "updated_date": "2025-10-22 15:43:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:50.386720+00:00"
    },
    {
      "arxiv_id": "2510.19692v1",
      "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary",
      "title_zh": "è¿ˆå‘è¶…è¶Šä»£ç çš„æ™ºèƒ½ä½“è½¯ä»¶å·¥ç¨‹ï¼šæ„¿æ™¯ã€ä»·å€¼è§‚ä¸æœ¯è¯­ä½“ç³»çš„æ„å»º",
      "authors": [
        "Rashina Hoda"
      ],
      "abstract": "Agentic AI is poised to usher in a seismic paradigm shift in Software Engineering (SE). As technologists rush head-along to make agentic AI a reality, SE researchers are driven to establish agentic SE as a research area. While early visions of agentic SE are primarily focused on code-related activities, early empirical evidence calls for a consideration of a range of socio-technical concerns to make it work in practice. This paper contributes to the emerging community vision by: (a) recommending an expansion of its scope beyond code, toward a 'whole of process' vision, grounding it in SE foundations and evolution and emerging agentic SE frameworks, (b) proposing a preliminary set of values and principles to guide efforts, and (c) sharing guidance on designing/using well-defined vocabulary for agentic SE. It is hoped that these ideas will encourage community collaborations and steer the SE community towards laying strong foundations of agentic SE so its not only inevitable but also deliberate and desirable in the long run.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Agentic AI åœ¨è½¯ä»¶å·¥ç¨‹(Software Engineering, SE)é¢†åŸŸå¼•å‘çš„èŒƒå¼è½¬å˜ï¼Œå¹¶è‡´åŠ›äºæ„å»º Agentic SE çš„ç ”ç©¶æ¡†æ¶ã€‚ä½œè€…æŒ‡å‡ºï¼Œæ—©æœŸçš„ Agentic SE æ„¿æ™¯è¿‡åº¦é›†ä¸­äºä»£ç ç›¸å…³æ´»åŠ¨ï¼Œå› æ­¤å»ºè®®å°†ç ”ç©¶èŒƒå›´ä»ä»£ç æ‰©å±•è‡³â€œå…¨æµç¨‹â€(whole of process)è§†è§’ã€‚æ–‡ç« é€šè¿‡ç»“åˆ SE åŸºç¡€ã€æ¼”è¿›è¿‡ç¨‹åŠæ–°å…´çš„ Agentic SE æ¡†æ¶ï¼Œæå‡ºäº†ä¸€å¥—åˆæ­¥çš„ä»·å€¼è§‚å’ŒåŸåˆ™æ¥æŒ‡å¯¼ç›¸å…³å®è·µã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†å…³äºè®¾è®¡å’Œä½¿ç”¨å®šä¹‰æ˜ç¡®çš„ Agentic SE è¯æ±‡è¡¨çš„æŒ‡å¯¼å»ºè®®ã€‚é€šè¿‡è¿™äº›è´¡çŒ®ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨æ¨åŠ¨ç¤¾åŒºåä½œå¹¶ä¸º Agentic SE å¥ å®šåšå®åŸºç¡€ï¼Œç¡®ä¿å…¶åœ¨é•¿æœŸçš„å‘å±•ä¸­ä¸ä»…æ˜¯æŠ€æœ¯å‘å±•çš„å¿…ç„¶ï¼Œæ›´æ˜¯æ·±æ€ç†Ÿè™‘ä¸”ç¬¦åˆäººç±»ä»·å€¼è§‚çš„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.19692v1",
      "published_date": "2025-10-22 15:39:58 UTC",
      "updated_date": "2025-10-22 15:39:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:53.168119+00:00"
    },
    {
      "arxiv_id": "2510.19689v1",
      "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation",
      "title_zh": "é¢å‘ä¼ä¸šäººåŠ›èµ„æºåˆ†æçš„æ— æœåŠ¡å™¨ GPU æ¶æ„ï¼šä¸€ç§ç”Ÿäº§çº§ BDaaS å®ç°",
      "authors": [
        "Guilin Zhang",
        "Wulan Guo",
        "Ziqi Tan",
        "Srinivas Vippagunta",
        "Suchitra Raman",
        "Shreeshankar Chatterjee",
        "Ju Lin",
        "Shang Liu",
        "Mary Schladenhauffen",
        "Jeffrey Luo",
        "Hailong Jiang"
      ],
      "abstract": "Industrial and government organizations increasingly depend on data-driven analytics for workforce, finance, and regulated decision processes, where timeliness, cost efficiency, and compliance are critical. Distributed frameworks such as Spark and Flink remain effective for massive-scale batch or streaming analytics but introduce coordination complexity and auditing overheads that misalign with moderate-scale, latency-sensitive inference. Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet enable interpretable tabular ML, motivating new deployment blueprints for regulated environments. In this paper, we present a production-oriented Big Data as a Service (BDaaS) blueprint that integrates a single-node serverless GPU runtime with TabNet. The design leverages GPU acceleration for throughput, serverless elasticity for cost reduction, and feature-mask interpretability for IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets, comparing our approach against Spark and CPU baselines. Our results show that GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90% lower cost per 1K inferences compared to Spark baselines, while compliance mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains stable under peak load, ensuring reliable auditability. Taken together, these findings provide a compliance-aware benchmark, a reproducible Helm-packaged blueprint, and a decision framework that demonstrate the practicality of secure, interpretable, and cost-efficient serverless GPU analytics for regulated enterprise and government settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šäººåŠ›èµ„æºåˆ†æä¸­å¯¹åŠæ—¶æ€§ã€æˆæœ¬æ•ˆç›Šå’Œåˆè§„æ€§çš„é«˜åº¦éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§é¢å‘ç”Ÿäº§ç¯å¢ƒçš„å¤§æ•°æ®å³æœåŠ¡ï¼ˆBDaaSï¼‰æ¶æ„è“å›¾ã€‚è¯¥æ¶æ„æ•´åˆäº†å•èŠ‚ç‚¹ Serverless GPU è¿è¡Œæ—¶ä¸ TabNet æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿåˆ†å¸ƒå¼æ¡†æ¶ï¼ˆå¦‚ Spark å’Œ Flinkï¼‰åœ¨å¤„ç†ä¸­ç­‰è§„æ¨¡ã€å»¶è¿Ÿæ•æ„Ÿçš„æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨çš„åè°ƒå¤æ‚æ€§å’Œå®¡è®¡å¼€é”€é—®é¢˜ã€‚è¯¥è®¾è®¡åˆ©ç”¨ GPU åŠ é€Ÿæå‡ååé‡ï¼Œé€šè¿‡ Serverless çš„å¼¹æ€§å®ç°æˆæœ¬ç¼©å‡ï¼Œå¹¶åˆ©ç”¨ç‰¹å¾æ©ç ï¼ˆfeature-maskï¼‰çš„å¯è§£é‡Šæ€§æ»¡è¶³ IL4/FIPS åˆè§„æ€§è¦æ±‚ã€‚åœ¨ HRã€Adult å’Œ BLS æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œä¸ Spark åŸºå‡†ç›¸æ¯”ï¼Œè¯¥ GPU æµæ°´çº¿çš„ååé‡æé«˜äº† 4.5 å€ï¼Œå»¶è¿Ÿé™ä½äº† 98%ï¼Œä¸”æ¯åƒæ¬¡æ¨ç†çš„æˆæœ¬é™ä½äº† 90%ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜åˆè§„æœºåˆ¶ä»…å¢åŠ äº†çº¦ 5.7 æ¯«ç§’çš„å»¶è¿Ÿï¼Œä¸”åœ¨é«˜å³°è´Ÿè½½ä¸‹å¯è§£é‡Šæ€§ä¿æŒç¨³å®šï¼Œç¡®ä¿äº†å¯é çš„å¯å®¡è®¡æ€§ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„ Helm æ‰“åŒ…è“å›¾å’Œå†³ç­–æ¡†æ¶ï¼Œè¯æ˜äº†åœ¨å—ç›‘ç®¡çš„ä¼ä¸šå’Œæ”¿åºœç¯å¢ƒä¸­å®æ–½å®‰å…¨ã€å¯è§£é‡Šä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„ Serverless GPU åˆ†æçš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19689v1",
      "published_date": "2025-10-22 15:37:42 UTC",
      "updated_date": "2025-10-22 15:37:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:55.888772+00:00"
    },
    {
      "arxiv_id": "2510.19687v1",
      "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¯¹æ²Ÿé€šèƒŒåçš„åŠ¨æœºæ˜¯å¦æ•æ„Ÿï¼Ÿ",
      "authors": [
        "Addison J. Wu",
        "Ryan Liu",
        "Kerem Oktar",
        "Theodore R. Sumers",
        "Thomas L. Griffiths"
      ],
      "abstract": "Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦å¯¹æ²Ÿé€šèƒŒåçš„åŠ¨æœºå…·æœ‰æ•æ„Ÿæ€§ï¼Œå³æ˜¯å¦å…·å¤‡â€œåŠ¨æœºè­¦è§‰â€ï¼ˆmotivational vigilanceï¼‰æ¥æ‰¹åˆ¤æ€§åœ°è¯„ä¼°ç”±äººç±»æ„å›¾å’Œæ¿€åŠ±æœºåˆ¶æ„å»ºçš„ä¿¡æ¯ã€‚ç ”ç©¶é¦–å…ˆé€šè¿‡è®¤çŸ¥ç§‘å­¦çš„å—æ§å®éªŒéªŒè¯äº† LLMs åœ¨é¢å¯¹åŠ¨æœºæ€§è¯è¯ï¼ˆmotivated testimonyï¼‰æ—¶çš„è¡Œä¸ºï¼Œå‘ç°å…¶èƒ½åƒäººç±»ä¸€æ ·æœ‰æ•ˆåœ°è¯†åˆ«å¹¶é™ä½åè§æ¥æºä¿¡æ¯çš„æƒé‡ï¼Œå…¶è¡¨ç°ç¬¦åˆç†æ€§å­¦ä¹ æ¨¡å‹ã€‚éšåï¼Œç ”ç©¶å°†è¯„ä¼°æ‰©å±•åˆ°æ¨¡æ‹Ÿç°å®ä¿¡æ¯ç”Ÿæ€ç³»ç»Ÿçš„èµåŠ©å¹¿å‘Šï¼ˆsponsored online advertsï¼‰åœºæ™¯ï¼Œå‘ç° LLMs çš„æ¨ç†åœ¨è¿™äº›å¤æ‚è®¾ç½®ä¸­å®¹æ˜“å—åˆ°å¹²æ‰°ä¿¡æ¯çš„å½±å“ï¼Œå¯¼è‡´å…¶è¡¨ç°ä¸ç†æ€§æ¨¡å‹é¢„æµ‹äº§ç”Ÿåå·®ã€‚ç„¶è€Œï¼Œç ”ç©¶è¯æ˜é€šè¿‡ç®€å•çš„å¼•å¯¼å¹²é¢„ï¼ˆsteering interventionï¼‰æ¥æé«˜æ„å›¾å’Œæ¿€åŠ±çš„æ˜¾è‘—æ€§ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼º LLMs çš„åˆ¤æ–­èƒ½åŠ›ã€‚æœ€ç»ˆç»“æœè¡¨æ˜ï¼Œè™½ç„¶ LLMs å…·å¤‡å¯¹ä»–äººåŠ¨æœºçš„åˆæ­¥æ•æ„Ÿæ€§ï¼Œä½†è¦å°†å…¶æ³›åŒ–åˆ°å¤æ‚çš„ç°å®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œä»éœ€è¿›ä¸€æ­¥çš„æŠ€æœ¯æ”¹è¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19687v1",
      "published_date": "2025-10-22 15:35:00 UTC",
      "updated_date": "2025-10-22 15:35:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:58:59.890272+00:00"
    },
    {
      "arxiv_id": "2510.19685v1",
      "title": "Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes",
      "title_zh": "æŒ‡ä»¤å‹ã€å…ƒè®¤çŸ¥å‹è¿˜æ˜¯äºŒè€…å…¼é¡¾ï¼ŸAI ç”Ÿæˆåé¦ˆç±»å‹å¯¹å­¦ç”Ÿå‚ä¸åº¦ã€è‡ªä¿¡å¿ƒåŠå­¦ä¹ æˆæœå½±å“çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Omar Alsaiari",
        "Nilufar Baghaei",
        "Jason M. Lodge",
        "Omid Noroozi",
        "Dragan GaÅ¡eviÄ‡",
        "Marie Boden",
        "Hassan Khosravi"
      ],
      "abstract": "Feedback is one of the most powerful influences on student learning, with extensive research examining how best to implement it in educational settings. Increasingly, feedback is being generated by artificial intelligence (AI), offering scalable and adaptive responses. Two widely studied approaches are directive feedback, which gives explicit explanations and reduces cognitive load to speed up learning, and metacognitive feedback which prompts learners to reflect, track their progress, and develop self-regulated learning (SRL) skills. While both approaches have clear theoretical advantages, their comparative effects on engagement, confidence, and quality of work remain underexplored. This study presents a semester-long randomised controlled trial with 329 students in an introductory design and programming course using an adaptive educational platform. Participants were assigned to receive directive, metacognitive, or hybrid AI-generated feedback that blended elements of both directive and metacognitive feedback. Results showed that revision behaviour differed across feedback conditions, with Hybrid prompting the most revisions compared to Directive and Metacognitive. Confidence ratings were uniformly high, and resource quality outcomes were comparable across conditions. These findings highlight the promise of AI in delivering feedback that balances clarity with reflection. Hybrid approaches, in particular, show potential to combine actionable guidance for immediate improvement with opportunities for self-reflection and metacognitive growth.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº† AI ç”Ÿæˆçš„æŒ‡ä»¤æ€§åé¦ˆ (Directive feedback)ã€å…ƒè®¤çŸ¥åé¦ˆ (Metacognitive feedback) ä»¥åŠä¸¤è€…çš„æ··åˆæ¨¡å¼ (Hybrid feedback) å¯¹å­¦ç”Ÿå‚ä¸åº¦ã€è‡ªä¿¡å¿ƒåŠå­¦ä¹ æˆæœçš„å½±å“ã€‚ç ”ç©¶é€šè¿‡å¯¹ 329 åè®¾è®¡ä¸ç¼–ç¨‹è¯¾ç¨‹å­¦ç”Ÿè¿›è¡Œçš„å­¦æœŸéšæœºå¯¹ç…§è¯•éªŒå‘ç°ï¼Œä¸åŒåé¦ˆç±»å‹æ˜¾è‘—å½±å“äº†å­¦ç”Ÿçš„ä¿®è®¢è¡Œä¸ºï¼Œå…¶ä¸­æ··åˆåé¦ˆæ¨¡å¼æ¯”å•ä¸€çš„æŒ‡ä»¤æ€§æˆ–å…ƒè®¤çŸ¥æ¨¡å¼è¯±å‘äº†æ›´å¤šçš„ä¿®è®¢ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå„ç»„å­¦ç”Ÿåœ¨è‡ªä¿¡å¿ƒè¯„ä»·å’Œæœ€ç»ˆäº§å‡ºè´¨é‡ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œå‡ç»´æŒåœ¨è¾ƒé«˜æ°´å¹³ã€‚ç ”ç©¶ç»“æœçªå‡ºäº† AI åœ¨å¹³è¡¡åé¦ˆæ¸…æ™°åº¦ä¸å¼•å¯¼å­¦ç”Ÿæ·±åº¦åæ€æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚æ··åˆåé¦ˆ (Hybrid feedback) æˆåŠŸåœ°å°†å³æ—¶æ”¹è¿›çš„è¡ŒåŠ¨æŒ‡å—ä¸ä¿ƒè¿›è‡ªæˆ‘è°ƒèŠ‚å­¦ä¹  (SRL) çš„åæ€æœºä¼šç›¸ç»“åˆï¼Œä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„è‡ªé€‚åº”æ•™è‚²å¹³å°æä¾›äº†é‡è¦çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19685v1",
      "published_date": "2025-10-22 15:31:21 UTC",
      "updated_date": "2025-10-22 15:31:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:02.779132+00:00"
    },
    {
      "arxiv_id": "2510.19678v1",
      "title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
      "title_zh": "æˆ‘ä»¥æ¨¡å‹ä¹‹çœ¼ï¼šå°†è§†è§‰æœç´¢ä½œä¸º MLLMs çš„è¡Œä¸ºæµ‹è¯•",
      "authors": [
        "John Burden",
        "Jonathan Prunty",
        "Ben Slater",
        "Matthieu Tehenan",
        "Greg Davis",
        "Lucy Cheke"
      ],
      "abstract": "Multimodal large language models (MLLMs) achieve strong performance on vision-language tasks, yet their visual processing is opaque. Most black-box evaluations measure task accuracy, but reveal little about underlying mechanisms. Drawing on cognitive psychology, we adapt classic visual search paradigms -- originally developed to study human perception -- to test whether MLLMs exhibit the ``pop-out'' effect, where salient visual features are detected independently of distractor set size. Using controlled experiments targeting colour, size and lighting features, we find that advanced MLLMs exhibit human-like pop-out effects in colour or size-based disjunctive (single feature) search, as well as capacity limits for conjunctive (multiple feature) search. We also find evidence to suggest that MLLMs, like humans, incorporate natural scene priors such as lighting direction into object representations. We reinforce our findings using targeted fine-tuning and mechanistic interpretability analyses. Our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºå°†è®¤çŸ¥å¿ƒç†å­¦ä¸­çš„ç»å…¸ Visual Search èŒƒå¼åº”ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs)ï¼Œæ—¨åœ¨é€šè¿‡è¡Œä¸ºæµ‹è¯•è¯„ä¼°å…¶ä¸é€æ˜çš„è§†è§‰å¤„ç†æœºåˆ¶ã€‚ç ”ç©¶è€…é’ˆå¯¹é¢œè‰²ã€å¤§å°å’Œå…‰ç…§ç‰¹å¾è®¾è®¡äº†å—æ§å®éªŒï¼Œæ¢è®¨ MLLMs æ˜¯å¦å…·å¤‡ \"pop-out\" æ•ˆåº”ï¼Œå³æ£€æµ‹æ˜¾è‘—è§†è§‰ç‰¹å¾æ—¶ä¸å—å¹²æ‰°é¡¹æ•°é‡å½±å“çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…ˆè¿›çš„ MLLMs åœ¨åŸºäºé¢œè‰²æˆ–å¤§å°çš„å•ç‰¹å¾æœç´¢ä¸­å±•ç°å‡ºç±»äººçš„ \"pop-out\" æ•ˆåº”ï¼Œä½†åœ¨å¤šç‰¹å¾çš„ç»“åˆæœç´¢ (conjunctive search) ä¸­å­˜åœ¨å®¹é‡é™åˆ¶ã€‚ç ”ç©¶è¿˜å‘ç° MLLMs ä¸äººç±»ç›¸ä¼¼ï¼Œèƒ½å¤Ÿå°†å…‰ç…§æ–¹å‘ç­‰è‡ªç„¶åœºæ™¯å…ˆéªŒ (natural scene priors) æ•´åˆè¿›ç‰©ä½“è¡¨ç¤ºä¸­ã€‚é€šè¿‡é’ˆå¯¹æ€§å¾®è°ƒ (fine-tuning) å’Œæœºæ¢°è§£é‡Šæ€§åˆ†æ (mechanistic interpretability analyses)ï¼Œè¯¥å·¥ä½œè¯æ˜äº† Visual Search å¯ä»¥ä½œä¸ºè¯„ä¼° MLLMs æ„ŸçŸ¥èƒ½åŠ›çš„æœ‰æ•ˆè®¤çŸ¥è¯Šæ–­å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.19678v1",
      "published_date": "2025-10-22 15:24:07 UTC",
      "updated_date": "2025-10-22 15:24:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:05.869993+00:00"
    },
    {
      "arxiv_id": "2510.19675v1",
      "title": "Study of Training Dynamics for Memory-Constrained Fine-Tuning",
      "title_zh": "å†…å­˜å—é™å¾®è°ƒä¸‹çš„è®­ç»ƒåŠ¨åŠ›å­¦ç ”ç©¶",
      "authors": [
        "AÃ«l QuÃ©lennec",
        "Nour Hezbri",
        "Pavlo Mozharovskyi",
        "Van-Tam Nguyen",
        "Enzo Tartaglione"
      ],
      "abstract": "Memory-efficient training of deep neural networks has become increasingly important as models grow larger while deployment environments impose strict resource constraints. We propose TraDy, a novel transfer learning scheme leveraging two key insights: layer importance for updates is architecture-dependent and determinable a priori, while dynamic stochastic channel selection provides superior gradient approximation compared to static approaches. We introduce a dynamic channel selection approach that stochastically resamples channels between epochs within preselected layers. Extensive experiments demonstrate TraDy achieves state-of-the-art performance across various downstream tasks and architectures while maintaining strict memory constraints, achieving up to 99% activation sparsity, 95% weight derivative sparsity, and 97% reduction in FLOPs for weight derivative computation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TraDyï¼Œä¸€ç§ä¸“ä¸ºå†…å­˜å—é™ç¯å¢ƒè®¾è®¡çš„åˆ›æ–°è¿ç§»å­¦ä¹ (transfer learning)æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œæ—¥ç›Šå¢é•¿çš„è§„æ¨¡ä¸å—é™èµ„æºä¹‹é—´çš„çŸ›ç›¾ã€‚è¯¥æ–¹æ¡ˆåŸºäºä¸¤ä¸ªæ ¸å¿ƒè§‚å¯Ÿï¼šæ›´æ–°å±‚çš„é‡è¦æ€§å–å†³äºæ¨¡å‹æ¶æ„ä¸”å¯é¢„å…ˆç¡®å®šï¼Œè€ŒåŠ¨æ€éšæœºé€šé“é€‰æ‹©åœ¨æ¢¯åº¦è¿‘ä¼¼(gradient approximation)æ–¹é¢è¡¨ç°ä¼˜äºé™æ€æ–¹æ³•ã€‚TraDy å¼•å…¥äº†ä¸€ç§åŠ¨æ€é€šé“é€‰æ‹©æŠ€æœ¯ï¼Œé€šè¿‡åœ¨é¢„é€‰å±‚å†…å¯¹å„è½®æ¬¡(epochs)é—´çš„é€šé“è¿›è¡Œéšæœºé‡é‡‡æ ·ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜ç®¡ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTraDy åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¶æ„ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½æ°´å¹³ã€‚åœ¨æ»¡è¶³ä¸¥æ ¼å†…å­˜çº¦æŸçš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è¾¾ 99% çš„æ¿€æ´»ç¨€ç–æ€§(activation sparsity)å’Œ 95% çš„æƒé‡å¯¼æ•°ç¨€ç–æ€§(weight derivative sparsity)ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æƒé‡å¯¼æ•°è®¡ç®—ä¸­æ˜¾è‘—å‡å°‘äº† 97% çš„æµ®ç‚¹è¿ç®—é‡(FLOPs)ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ¨¡å‹å¾®è°ƒæä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19675v1",
      "published_date": "2025-10-22 15:21:05 UTC",
      "updated_date": "2025-10-22 15:21:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:08.885320+00:00"
    },
    {
      "arxiv_id": "2510.19671v1",
      "title": "Explainable e-sports win prediction through Machine Learning classification in streaming",
      "title_zh": "åŸºäºæµå¼æœºå™¨å­¦ä¹ åˆ†ç±»çš„å¯è§£é‡Šç”µå­ç«æŠ€è·èƒœé¢„æµ‹",
      "authors": [
        "Silvia GarcÃ­a-MÃ©ndez",
        "Francisco de Arriba-PÃ©rez"
      ],
      "abstract": "The increasing number of spectators and players in e-sports, along with the development of optimized communication solutions and cloud computing technology, has motivated the constant growth of the online game industry. Even though Artificial Intelligence-based solutions for e-sports analytics are traditionally defined as extracting meaningful patterns from related data and visualizing them to enhance decision-making, most of the effort in professional winning prediction has been focused on the classification aspect from a batch perspective, also leaving aside the visualization techniques. Consequently, this work contributes to an explainable win prediction classification solution in streaming in which input data is controlled over several sliding windows to reflect relevant game changes. Experimental results attained an accuracy higher than 90 %, surpassing the performance of competing solutions in the literature. Ultimately, our system can be leveraged by ranking and recommender systems for informed decision-making, thanks to the explainability module, which fosters trust in the outcome predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç”µå­ç«æŠ€èƒœç‡é¢„æµ‹æ¨¡å‹å¤šé›†ä¸­äºæ‰¹å¤„ç†æ¨¡å¼ä¸”ç¼ºä¹è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸“ä¸ºæµå¼æ•°æ® (streaming) è®¾è®¡çš„å¯è§£é‡Šèƒœç‡é¢„æµ‹åˆ†ç±»æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å¤šä¸ªæ»‘åŠ¨çª—å£ (sliding windows) åŠ¨æ€æ§åˆ¶è¾“å…¥æ•°æ®ï¼Œä»è€Œç²¾å‡†æ•æ‰æ¯”èµ›è¿›ç¨‹ä¸­çš„å…³é”®å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨èƒœç‡é¢„æµ‹ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡ 90%ï¼Œåœ¨æ€§èƒ½è¡¨ç°ä¸Šä¼˜äºç›®å‰æ–‡çŒ®ä¸­çš„å…¶ä»–ç«äº‰æ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥å¯è§£é‡Šæ€§æ¨¡å— (explainability module)ï¼Œè¯¥ç³»ç»Ÿä¸ä»…å¢å¼ºäº†é¢„æµ‹ç»“æœçš„é€æ˜åº¦ä¸ä¿¡ä»»åº¦ï¼Œè¿˜èƒ½æœ‰æ•ˆé›†æˆåˆ°æ’åä¸æ¨èç³»ç»Ÿä¸­ï¼Œä¸ºç”µå­ç«æŠ€é¢†åŸŸçš„è¾…åŠ©å†³ç­–æä¾›å¯é çš„æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19671v1",
      "published_date": "2025-10-22 15:18:16 UTC",
      "updated_date": "2025-10-22 15:18:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:09.170730+00:00"
    },
    {
      "arxiv_id": "2510.19668v1",
      "title": "Unraveling Emotions with Pre-Trained Models",
      "title_zh": "åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è§£ææƒ…ç»ª",
      "authors": [
        "Alejandro PajÃ³n-SanmartÃ­n",
        "Francisco De Arriba-PÃ©rez",
        "Silvia GarcÃ­a-MÃ©ndez",
        "FÃ¡tima Leal",
        "Benedita Malheiro",
        "Juan Carlos Burguillo-Rial"
      ],
      "abstract": "Transformer models have significantly advanced the field of emotion recognition. However, there are still open challenges when exploring open-ended queries for Large Language Models (LLMs). Although current models offer good results, automatic emotion analysis in open texts presents significant challenges, such as contextual ambiguity, linguistic variability, and difficulty interpreting complex emotional expressions. These limitations make the direct application of generalist models difficult. Accordingly, this work compares the effectiveness of fine-tuning and prompt engineering in emotion detection in three distinct scenarios: (i) performance of fine-tuned pre-trained models and general-purpose LLMs using simple prompts; (ii) effectiveness of different emotion prompt designs with LLMs; and (iii) impact of emotion grouping techniques on these models. Experimental tests attain metrics above 70% with a fine-tuned pre-trained model for emotion recognition. Moreover, the findings highlight that LLMs require structured prompt engineering and emotion grouping to enhance their performance. These advancements improve sentiment analysis, human-computer interaction, and understanding of user behavior across various domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Transformer æ¨¡å‹åœ¨å¤„ç†å¼€æ”¾å¼æ–‡æœ¬æƒ…æ„Ÿè¯†åˆ«æ—¶é¢ä¸´çš„ä¸Šä¸‹æ–‡æ­§ä¹‰ä¸è¯­è¨€å˜å¼‚æ€§ç­‰æŒ‘æˆ˜ï¼Œé€šè¿‡å¯¹æ¯”å¾®è°ƒ (fine-tuning) ä¸æç¤ºå·¥ç¨‹ (prompt engineering) åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å¯»æ±‚è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å…·ä½“è¯„ä¼°äº†å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä¸é€šç”¨ LLMs çš„æ€§èƒ½å·®å¼‚ã€ä¸åŒæç¤ºè®¾è®¡çš„æ•ˆæœä»¥åŠæƒ…æ„Ÿåˆ†ç»„ (emotion grouping) æŠ€æœ¯å¯¹è¯†åˆ«å‡†ç¡®ç‡çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„é¢„è®­ç»ƒæ¨¡å‹åœ¨æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­èƒ½å¤Ÿè·å¾— 70% ä»¥ä¸Šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œè¡¨ç°ä¼˜äºç›´æ¥ä½¿ç”¨ç®€å•æç¤ºçš„é€šç”¨æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼ŒLLMs éœ€è¦ç»“åˆç»“æ„åŒ–çš„æç¤ºå·¥ç¨‹å’Œæƒ…æ„Ÿåˆ†ç»„ç­–ç•¥æ‰èƒ½æ˜¾è‘—æå‡å…¶åœ¨å¤æ‚æƒ…æ„Ÿè¡¨è¾¾ä¸‹çš„è¯†åˆ«èƒ½åŠ›ã€‚è¿™äº›ç ”ç©¶æˆæœä¸ºä¼˜åŒ–æƒ…æ„Ÿåˆ†æã€å¢å¼ºäººæœºäº¤äº’ (human-computer interaction) ä½“éªŒä»¥åŠæ·±å…¥ç†è§£å¤šé¢†åŸŸç”¨æˆ·è¡Œä¸ºæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19668v1",
      "published_date": "2025-10-22 15:13:52 UTC",
      "updated_date": "2025-10-22 15:13:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:20.894002+00:00"
    },
    {
      "arxiv_id": "2510.19666v1",
      "title": "A Graph Engine for Guitar Chord-Tone Soloing Education",
      "title_zh": "ç”¨äºå‰ä»–å’Œå¼¦éŸ³å³å…´æ¼”å¥æ•™å­¦çš„å›¾å¼•æ“",
      "authors": [
        "Matthew Keating",
        "Michael Casey"
      ],
      "abstract": "We present a graph-based engine for computing chord tone soloing suggestions for guitar students. Chord tone soloing is a fundamental practice for improvising over a chord progression, where the instrumentalist uses only the notes contained in the current chord. This practice is a building block for all advanced jazz guitar theory but is difficult to learn and practice. First, we discuss methods for generating chord-tone arpeggios. Next, we construct a weighted graph where each node represents a chord tone arpeggio for a chord in the progression. Then, we calculate the edge weight between each consecutive chord's nodes in terms of optimal transition tones. We then find the shortest path through this graph and reconstruct a chord-tone soloing line. Finally, we discuss a user-friendly system to handle input and output to this engine for guitar students to practice chord tone soloing.",
      "tldr_zh": "æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºå›¾(graph-based)çš„å¼•æ“ï¼Œä¸“é—¨ç”¨äºä¸ºå‰ä»–å­¦ç”Ÿæä¾›å’Œå¼¦éŸ³å³å…´(chord tone soloing)çš„ç»ƒä¹ å»ºè®®ã€‚å’Œå¼¦éŸ³å³å…´æ˜¯çˆµå£«å‰ä»–ç†è®ºçš„åŸºçŸ³ï¼Œè¦æ±‚å³å…´æ¼”å¥è€…ä»…ä½¿ç”¨å½“å‰å’Œå¼¦å†…çš„éŸ³ç¬¦ï¼Œä½†è¯¥æŠ€å·§çš„å­¦ä¹ ä¸å®è·µå¯¹äºåˆå­¦è€…è€Œè¨€å…·æœ‰è¾ƒé«˜éš¾åº¦ã€‚è¯¥å¼•æ“é€šè¿‡ç”Ÿæˆå’Œå¼¦éŸ³ç¶éŸ³(chord-tone arpeggios)å¹¶æ„å»ºåŠ æƒå›¾(weighted graph)ï¼Œå°†æ¯ä¸ªç¶éŸ³è®¾ä¸ºèŠ‚ç‚¹ï¼Œå¹¶ä¾æ®ç›¸é‚»å’Œå¼¦é—´çš„æœ€ä½³è¿‡æ¸¡éŸ³(optimal transition tones)è®¡ç®—è¾¹æƒé‡ã€‚é€šè¿‡åœ¨å›¾ä¸­å¯»æ‰¾æœ€çŸ­è·¯å¾„(shortest path)ï¼Œç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨é‡æ„å‡ºé€»è¾‘è¿è´¯çš„å’Œå¼¦éŸ³å³å…´æ—‹å¾‹çº¿ã€‚æœ€åï¼Œç ”ç©¶è€…å®ç°äº†ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„äº¤äº’ç³»ç»Ÿï¼Œç®€åŒ–äº†è¯¥å¼•æ“çš„è¾“å…¥è¾“å‡ºæµç¨‹ï¼Œä¸ºå‰ä»–å­¦ç”ŸæŒæ¡è¿›é˜¶å³å…´ç†è®ºæä¾›äº†é«˜æ•ˆçš„è¾…åŠ©æ•™å­¦å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICMC 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19666v1",
      "published_date": "2025-10-22 15:13:16 UTC",
      "updated_date": "2025-10-22 15:13:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:22.980559+00:00"
    },
    {
      "arxiv_id": "2510.19661v2",
      "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing",
      "title_zh": "AgentSenseï¼šå¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½çš„å¯æ³›åŒ–ä¸å¯è§£é‡Š Web å‚ä¸å¼åŸå¸‚æ„ŸçŸ¥",
      "authors": [
        "Xusen Guo",
        "Mingxing Peng",
        "Xixuan Hao",
        "Xingchen Zou",
        "Qiongyan Wang",
        "Sijie Ruan",
        "Yuxuan Liang"
      ],
      "abstract": "Web-based participatory urban sensing has emerged as a vital approach for modern urban management by leveraging mobile individuals as distributed sensors. However, existing urban sensing systems struggle with limited generalization across diverse urban scenarios and poor interpretability in decision-making. In this work, we introduce AgentSense, a hybrid, training-free framework that integrates large language models (LLMs) into participatory urban sensing through a multi-agent evolution system. AgentSense initially employs classical planner to generate baseline solutions and then iteratively refines them to adapt sensing task assignments to dynamic urban conditions and heterogeneous worker preferences, while producing natural language explanations that enhance transparency and trust. Extensive experiments across two large-scale mobility datasets and seven types of dynamic disturbances demonstrate that AgentSense offers distinct advantages in adaptivity and explainability over traditional methods. Furthermore, compared to single-agent LLM baselines, our approach outperforms in both performance and robustness, while delivering more reasonable and transparent explanations. These results position AgentSense as a significant advancement towards deploying adaptive and explainable urban sensing systems on the web.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Web-based participatory urban sensingåœ¨ä¸åŒåŸå¸‚åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³åŠå†³ç­–ç¼ºä¹å¯è§£é‡Šæ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†AgentSenseè¿™ä¸€æ— éœ€è®­ç»ƒçš„æ··åˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¼”åŒ–ç³»ç»Ÿå°†Large Language Models (LLMs) å¼•å…¥æ„ŸçŸ¥æµç¨‹ï¼Œé¦–å…ˆåˆ©ç”¨ç»å…¸è§„åˆ’å™¨ç”ŸæˆåŸºå‡†æ–¹æ¡ˆï¼Œéšåé€šè¿‡è¿­ä»£ä¼˜åŒ–ä½¿ä»»åŠ¡åˆ†é…èƒ½å¤Ÿçµæ´»é€‚åº”åŠ¨æ€åŸå¸‚ç¯å¢ƒå’ŒWorkerçš„å¼‚è´¨åŒ–åå¥½ã€‚AgentSenseèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„é€æ˜åº¦ä¸ä¿¡ä»»åº¦ã€‚åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡ç§»åŠ¨æ•°æ®é›†å’Œä¸ƒç§åŠ¨æ€å¹²æ‰°ä¸‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgentSenseåœ¨Adaptivityã€Explainabilityã€æ€§èƒ½åŠRobustnessæ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•åŠå•æ™ºèƒ½ä½“LLMåŸºå‡†ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºå®ç°é€šç”¨çš„ã€å¯è§£é‡Šçš„åŸå¸‚æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 10 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.19661v2",
      "published_date": "2025-10-22 15:06:26 UTC",
      "updated_date": "2025-10-24 05:16:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:31.763931+00:00"
    },
    {
      "arxiv_id": "2510.19654v2",
      "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction",
      "title_zh": "ä»é¢„æµ‹åˆ°è§„åˆ’ï¼šååŒçŠ¶æ€-åŠ¨ä½œé¢„æµ‹ç­–ç•¥ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Zhida Zhao",
        "Talas Fu",
        "Yifan Wang",
        "Lijun Wang",
        "Huchuan Lu"
      ],
      "abstract": "Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at https://github.com/6550Zhao/Policy-World-Model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Policy World Model (PWM)ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­ä¸–ç•Œå»ºæ¨¡ä¸è½¨è¿¹è§„åˆ’é•¿æœŸè„±èŠ‚çš„é—®é¢˜ã€‚PWMé€šè¿‡ç»Ÿä¸€æ¶æ„å®ç°äº†å»ºæ¨¡ä¸è§„åˆ’çš„æ·±åº¦é›†æˆï¼Œå¹¶åˆ©ç”¨æ‰€å­¦ä¹ çš„ä¸–ç•ŒçŸ¥è¯†é€šè¿‡action-free future state forecastingæ–¹æ¡ˆç›´æ¥è¾…åŠ©è§„åˆ’å†³ç­–ã€‚é€šè¿‡ååŒçš„çŠ¶æ€-åŠ¨ä½œé¢„æµ‹(collaborative state-action prediction)ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿç±»äººçš„é¢„è§æ€§æ„ŸçŸ¥ï¼Œæ˜¾è‘—æå‡äº†è§„åˆ’ä»»åŠ¡çš„å¯é æ€§ã€‚ä¸ºäº†ä¼˜åŒ–è§†é¢‘é¢„æµ‹æ•ˆç‡ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç”±context-guided tokenizerå’Œadaptive dynamic focal lossæ”¯æŒçš„åŠ¨æ€å¢å¼ºå¹¶è¡Œä»¤ç‰Œç”Ÿæˆæœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPWMåœ¨ä»…ä¾èµ–å•è·¯å‰è§†æ‘„åƒå¤´è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½å³å¯åŒ¹é…æˆ–è¶…è¶Šç›®å‰ä¾èµ–å¤šè§†è§’åŠå¤šæ¨¡æ€è¾“å…¥çš„å…ˆè¿›æ–¹æ³•(SOTA)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NuerIPS 2025 (Poster)",
      "pdf_url": "https://arxiv.org/pdf/2510.19654v2",
      "published_date": "2025-10-22 14:57:51 UTC",
      "updated_date": "2025-11-25 03:37:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:29.770167+00:00"
    },
    {
      "arxiv_id": "2510.19641v1",
      "title": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent",
      "title_zh": "Style Attack Disguiseï¼šå½“å­—ä½“æˆä¸ºå¯¹æŠ—æ„å›¾çš„ä¼ªè£…",
      "authors": [
        "Yangshijie Zhang",
        "Xinda Wang",
        "Jialin Liu",
        "Wenqiang Wang",
        "Zhicong Ma",
        "Xingxing Jia"
      ],
      "abstract": "With social media growth, users employ stylistic fonts and font-like emoji to express individuality, creating visually appealing text that remains human-readable. However, these fonts introduce hidden vulnerabilities in NLP models: while humans easily read stylistic text, models process these characters as distinct tokens, causing interference. We identify this human-model perception gap and propose a style-based attack, Style Attack Disguise (SAD). We design two sizes: light for query efficiency and strong for superior attack performance. Experiments on sentiment classification and machine translation across traditional models, LLMs, and commercial services demonstrate SAD's strong attack performance. We also show SAD's potential threats to multimodal tasks including text-to-image and text-to-speech generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¤¾äº¤åª’ä½“ä¸­é£æ ¼åŒ–å­—ä½“å’Œç±»å­—ä½“è¡¨æƒ…ç¬¦å·å¯¹è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ¨¡å‹å¸¦æ¥çš„å®‰å…¨éšæ‚£ï¼ŒæŒ‡å‡ºäººç±»è™½èƒ½è½»æ¾é˜…è¯»æ­¤ç±»æ–‡æœ¬ï¼Œä½†æ¨¡å‹ä¼šå°†å…¶è¯†åˆ«ä¸ºä¸åŒçš„Tokenï¼Œä»è€Œäº§ç”Ÿæ„ŸçŸ¥åå·®ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºStyle Attack Disguise (SAD)çš„æ ·å¼æ”»å‡»æ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†å…¼é¡¾æŸ¥è¯¢æ•ˆç‡çš„Lightç‰ˆæœ¬å’Œè¿½æ±‚æè‡´æ€§èƒ½çš„Strongç‰ˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSADåœ¨æƒ…ç»ªåˆ†ç±»(Sentiment Classification)å’Œæœºå™¨ç¿»è¯‘(Machine Translation)ä»»åŠ¡ä¸­ï¼Œå¯¹ä¼ ç»Ÿæ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»¥åŠå•†ä¸šæœåŠ¡å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ”»å‡»æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥è¯å®äº†è¯¥æ”»å‡»æ‰‹æ®µå¯¹æ–‡æœ¬è½¬å›¾åƒ(Text-to-Image)å’Œæ–‡æœ¬è½¬è¯­éŸ³(Text-to-Speech)ç­‰ç”Ÿæˆå¼å¤šæ¨¡æ€ä»»åŠ¡åŒæ ·å…·æœ‰æ½œåœ¨å¨èƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19641v1",
      "published_date": "2025-10-22 14:40:24 UTC",
      "updated_date": "2025-10-22 14:40:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:35.965520+00:00"
    },
    {
      "arxiv_id": "2510.19631v1",
      "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application",
      "title_zh": "HSCodeCompï¼šé¢å‘å±‚çº§è§„åˆ™åº”ç”¨çš„æ·±åº¦æœç´¢æ™ºèƒ½ä½“å®æˆ˜åŒ–ä¸“å®¶çº§åŸºå‡†",
      "authors": [
        "Yiqian Yang",
        "Tian Lan",
        "Qianghuai Jia",
        "Li Zhu",
        "Hui Jiang",
        "Hang Zhu",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "Effective deep search agents must not only access open-domain and domain-specific knowledge but also apply complex rules-such as legal clauses, medical manuals and tariff rules. These rules often feature vague boundaries and implicit logic relationships, making precise application challenging for agents. However, this critical capability is largely overlooked by current agent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level e-commerce benchmark designed to evaluate deep search agents in hierarchical rule application. In this task, the deep reasoning process of agents is guided by these rules to predict 10-digit Harmonized System Code (HSCode) of products with noisy but realistic descriptions. These codes, established by the World Customs Organization, are vital for global supply chain efficiency. Built from real-world data collected from large-scale e-commerce platforms, our proposed HSCodeComp comprises 632 product entries spanning diverse product categories, with these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source, and closed-source agents reveal a huge performance gap: best agent achieves only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides, detailed analysis demonstrates the challenges of hierarchical rule application, and test-time scaling fails to improve performance further.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† HSCodeCompï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç”µå­å•†åŠ¡é¢†åŸŸçœŸå®åœºæ™¯ã€ä¸“å®¶çº§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ·±å±‚æœç´¢æ™ºèƒ½ä½“ (Deep Search Agents) åœ¨å±‚çº§è§„åˆ™åº”ç”¨ (Hierarchical Rule Application) æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ™ºèƒ½ä½“åœ¨å¤„ç†å…·æœ‰æ¨¡ç³Šè¾¹ç•Œå’Œéšå¼é€»è¾‘çš„å¤æ‚è§„åˆ™ï¼ˆå¦‚å…³ç¨è§„åˆ™ï¼‰æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œè€Œ HSCodeComp å¡«è¡¥äº†å½“å‰è¯„ä¼°ä½“ç³»çš„ç©ºç™½ã€‚è¯¥åŸºå‡†åŒ…å«ç”±äººç±»ä¸“å®¶æ ‡æ³¨çš„ 632 ä¸ªçœŸå®ç”µå•†äº§å“æ¡ç›®ï¼Œè¦æ±‚æ™ºèƒ½ä½“æ ¹æ®å®é™…æè¿°å‡†ç¡®é¢„æµ‹ 10 ä½åè°ƒåˆ¶åº¦ç¼–ç  (Harmonized System Code, HSCode)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºæ™ºèƒ½ä½“ï¼Œå…¶ 10 ä½ç¼–ç é¢„æµ‹çš„æœ€é«˜å‡†ç¡®ç‡ä¹Ÿä»…ä¸º 46.8%ï¼Œä¸äººç±»ä¸“å®¶ 95.0% çš„æ°´å¹³å­˜åœ¨å·¨å¤§å·®è·ã€‚åˆ†æç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå±‚çº§è§„åˆ™åº”ç”¨æå…·æŒ‘æˆ˜æ€§ï¼Œä¸”æµ‹è¯•æ—¶ç¼©æ”¾ (test-time scaling) ç­‰æ‰‹æ®µåœ¨è¿™ä¸€ä»»åŠ¡ä¸Šæœªèƒ½æœ‰æ•ˆæå‡æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19631v1",
      "published_date": "2025-10-22 14:28:33 UTC",
      "updated_date": "2025-10-22 14:28:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:42.955367+00:00"
    },
    {
      "arxiv_id": "2510.19882v1",
      "title": "Quantifying Feature Importance for Online Content Moderation",
      "title_zh": "åœ¨çº¿å†…å®¹å®¡æ ¸ä¸­çš„ç‰¹å¾é‡è¦æ€§é‡åŒ–",
      "authors": [
        "Benedetta Tessa",
        "Alejandro Moreo",
        "Stefano Cresci",
        "Tiziano Fagni",
        "Fabrizio Sebastiani"
      ],
      "abstract": "Accurately estimating how users respond to moderation interventions is paramount for developing effective and user-centred moderation strategies. However, this requires a clear understanding of which user characteristics are associated with different behavioural responses, which is the goal of this work. We investigate the informativeness of 753 socio-behavioural, linguistic, relational, and psychological features, in predicting the behavioural changes of 16.8K users affected by a major moderation intervention on Reddit. To reach this goal, we frame the problem in terms of \"quantification\", a task well-suited to estimating shifts in aggregate user behaviour. We then apply a greedy feature selection strategy with the double goal of (i) identifying the features that are most predictive of changes in user activity, toxicity, and participation diversity, and (ii) estimating their importance. Our results allow identifying a small set of features that are consistently informative across all tasks, and determining that many others are either task-specific or of limited utility altogether. We also find that predictive performance varies according to the task, with changes in activity and toxicity being easier to estimate than changes in diversity. Overall, our results pave the way for the development of accurate systems that predict user reactions to moderation interventions. Furthermore, our findings highlight the complexity of post-moderation user behaviour, and indicate that effective moderation should be tailored not only to user traits but also to the specific objective of the intervention.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨é‡åŒ–ç”¨æˆ·ç‰¹å¾åœ¨é¢„æµ‹åœ¨çº¿å†…å®¹å®¡æ ¸(Online Content Moderation)å¹²é¢„åè¡Œä¸ºååº”ä¸­çš„é‡è¦æ€§ï¼Œä»¥æ”¯æŒå¼€å‘ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„å®¡æ ¸ç­–ç•¥ã€‚é€šè¿‡åˆ†æå—Reddité‡å¤§å®¡æ ¸å¹²é¢„å½±å“çš„1.68ä¸‡åç”¨æˆ·ï¼Œç ”ç©¶å›¢é˜Ÿè€ƒå¯Ÿäº†æ¶µç›–ç¤¾ä¼šè¡Œä¸ºã€è¯­è¨€ã€å…³ç³»å’Œå¿ƒç†ç»´åº¦çš„753é¡¹ç‰¹å¾ã€‚ç ”ç©¶å°†è¯¥é—®é¢˜è¡¨è¿°ä¸ºâ€œé‡åŒ–â€(Quantification)ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è´ªå©ªç‰¹å¾é€‰æ‹©(Greedy Feature Selection)ç­–ç•¥è¯†åˆ«å‡ºå¯¹é¢„æµ‹ç”¨æˆ·æ´»è·ƒåº¦ã€æ¯’æ€§(Toxicity)åŠå‚ä¸å¤šæ ·æ€§å˜åŒ–æœ€å…·ä¿¡æ¯é‡çš„ç‰¹å¾ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸€å°éƒ¨åˆ†æ ¸å¿ƒç‰¹å¾åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€è‡´çš„é‡è¦æ€§ï¼Œè€Œæ´»è·ƒåº¦å’Œæ¯’æ€§çš„å˜åŒ–ç›¸æ¯”å‚ä¸å¤šæ ·æ€§æ›´å®¹æ˜“è¢«å‡†ç¡®ä¼°è®¡ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å®¡æ ¸åç”¨æˆ·è¡Œä¸ºçš„å¤æ‚æ€§ï¼Œè¡¨æ˜æœ‰æ•ˆçš„å®¡æ ¸å¹²é¢„ä¸ä»…éœ€è¦è€ƒè™‘ç”¨æˆ·ç‰¹è´¨ï¼Œè¿˜åº”é’ˆå¯¹å…·ä½“çš„å¹²é¢„ç›®æ ‡è¿›è¡Œå®šåˆ¶ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19882v1",
      "published_date": "2025-10-22 14:02:30 UTC",
      "updated_date": "2025-10-22 14:02:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:42.458782+00:00"
    },
    {
      "arxiv_id": "2510.19600v1",
      "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
      "title_zh": "æˆæœ¬ä¸è¶³ 0.1 ç¾å…ƒçš„äººæœºåä½œå¼è®ºæ–‡è½¬ç½‘é¡µæ„å»º",
      "authors": [
        "Qianli Ma",
        "Siyu Wang",
        "Yilin Chen",
        "Yinhao Tang",
        "Yixiang Yang",
        "Chang Guo",
        "Bingjie Gao",
        "Zhening Xing",
        "Yanan Sun",
        "Zhipeng Zhang"
      ],
      "abstract": "In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce $\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated \"Checker\" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct $\\textbf{PageBench}$, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \\$0.1. Code and dataset will be released at $\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoPageï¼Œä¸€ç§å…¨æ–°çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (multi-agent system)ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–è§£å†³ç ”ç©¶äººå‘˜åœ¨å°†ç§‘ç ”è®ºæ–‡è½¬åŒ–ä¸ºåŠ¨æ€äº¤äº’å¼é¡¹ç›®ç½‘é¡µæ—¶é¢ä¸´çš„ç¹çä¸”é‡å¤çš„åŠ³åŠ¨ã€‚AutoPage é‡‡ç”¨äº†ä»ç²—åˆ°ç»†çš„æµæ°´çº¿ (coarse-to-fine pipeline)ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹æ‹†è§£ä¸ºå™äº‹è§„åˆ’ã€å¤šæ¨¡æ€å†…å®¹ç”ŸæˆåŠäº¤äº’å¼æ¸²æŸ“ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µã€‚ä¸ºäº†æœ‰æ•ˆæŠ‘åˆ¶ AI å¹»è§‰ (hallucination)ï¼Œç³»ç»Ÿä¸­ä¸“é—¨è®¾ç«‹äº† \"Checker\" æ™ºèƒ½ä½“æ¥éªŒè¯å„æ­¥éª¤ä¸æºè®ºæ–‡çš„ä¸€è‡´æ€§ï¼Œå¹¶å¼•å…¥å¯é€‰çš„äººç±»æ£€æŸ¥ç‚¹ä»¥ç¡®ä¿äº§å‡ºå†…å®¹ç²¾å‡†ç¬¦åˆä½œè€…æ„å›¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†é¦–ä¸ªé’ˆå¯¹è¯¥ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•é›† PageBenchã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoPage èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”æå…·è§†è§‰å¸å¼•åŠ›çš„ç½‘é¡µï¼Œä¸”æ•´ä¸ªè¿‡ç¨‹åœ¨ 15 åˆ†é’Ÿå†…å®Œæˆï¼Œå•æ¬¡ç”Ÿæˆæˆæœ¬ä½äº 0.1 ç¾å…ƒï¼Œå®ç°äº†æé«˜çš„æ•ˆç‡ä¸ç»æµæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19600v1",
      "published_date": "2025-10-22 13:53:57 UTC",
      "updated_date": "2025-10-22 13:53:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:43.967980+00:00"
    },
    {
      "arxiv_id": "2510.19599v1",
      "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography",
      "title_zh": "XBenchï¼šèƒ¸éƒ¨ X çº¿æ‘„å½±è§†è§‰è¯­è¨€è§£é‡Šçš„ç»¼åˆæ€§åŸºå‡†",
      "authors": [
        "Haozhe Luo",
        "Shelley Zixin Shu",
        "Ziyu Zhou",
        "Sebastian Otalora",
        "Mauricio Reyes"
      ],
      "abstract": "Vision-language models (VLMs) have recently shown remarkable zero-shot performance in medical image understanding, yet their grounding ability, the extent to which textual concepts align with visual evidence, remains underexplored. In the medical domain, however, reliable grounding is essential for interpretability and clinical adoption. In this work, we present the first systematic benchmark for evaluating cross-modal interpretability in chest X-rays across seven CLIP-style VLM variants. We generate visual explanations using cross-attention and similarity-based localization maps, and quantitatively assess their alignment with radiologist-annotated regions across multiple pathologies. Our analysis reveals that: (1) while all VLM variants demonstrate reasonable localization for large and well-defined pathologies, their performance substantially degrades for small or diffuse lesions; (2) models that are pretrained on chest X-ray-specific datasets exhibit improved alignment compared to those trained on general-domain data. (3) The overall recognition ability and grounding ability of the model are strongly correlated. These findings underscore that current VLMs, despite their strong recognition ability, still fall short in clinically reliable grounding, highlighting the need for targeted interpretability benchmarks before deployment in medical practice. XBench code is available at https://github.com/Roypic/Benchmarkingattention",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† XBenchï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è¯„ä¼°èƒ¸éƒ¨ X å…‰ç‰‡ (Chest X-ray) ä¸­è·¨æ¨¡æ€è§£é‡Šæ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¢è®¨è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„å®šä½ (Grounding) èƒ½åŠ›åŠå…¶ä¸ä¸´åºŠè§£é‡Šæ€§çš„å…³è”ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹ä¸ƒç§ CLIP é£æ ¼çš„ VLM å˜ä½“ï¼Œé€šè¿‡ Cross-attention å’ŒåŸºäºç›¸ä¼¼æ€§çš„å®šä½å›¾ç”Ÿæˆè§†è§‰è§£é‡Šï¼Œå¹¶å®šé‡è¯„ä¼°å…¶ä¸æ”¾å°„ç§‘åŒ»ç”Ÿæ ‡æ³¨åŒºåŸŸåœ¨å¤šç§ç—…ç†ä¸‹çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ‰€æœ‰å˜ä½“åœ¨å¤§å‹ä¸”å®šä¹‰æ˜ç¡®çš„ç—…ç†åŒºåŸŸè¡¨ç°å‡ºåˆç†çš„å®šä½èƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹å¾®å°æˆ–å¼¥æ¼«æ€§ç—…ç¶æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åˆ†æè¿›ä¸€æ­¥å‘ç°ï¼Œåœ¨èƒ¸éƒ¨ X å…‰ä¸“ç”¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶å¯¹é½æ•ˆæœä¼˜äºåœ¨é€šç”¨é¢†åŸŸæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ•´ä½“çš„è¯†åˆ«èƒ½åŠ›ä¸å…¶å®šä½èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚XBench çš„å‘ç°è¡¨æ˜å½“å‰çš„ VLMs å°½ç®¡è¯†åˆ«èƒ½åŠ›å¼ºï¼Œä½†åœ¨ä¸´åºŠå¯é çš„å®šä½æ–¹é¢ä»å­˜åœ¨çŸ­æ¿ï¼Œå¼ºè°ƒäº†åœ¨åŒ»ç–—å®è·µéƒ¨ç½²å‰è¿›è¡Œé’ˆå¯¹æ€§è§£é‡Šæ€§åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19599v1",
      "published_date": "2025-10-22 13:52:19 UTC",
      "updated_date": "2025-10-22 13:52:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:45.668097+00:00"
    },
    {
      "arxiv_id": "2510.19593v1",
      "title": "A Goal-Driven Survey on Root Cause Analysis",
      "title_zh": "ç›®æ ‡é©±åŠ¨çš„æ ¹å› åˆ†æç»¼è¿°",
      "authors": [
        "Aoyang Fang",
        "Haowen Yang",
        "Haoze Dong",
        "Qisheng Lu",
        "Junjielong Xu",
        "Pinjia He"
      ],
      "abstract": "Root Cause Analysis (RCA) is a crucial aspect of incident management in large-scale cloud services. While the term root cause analysis or RCA has been widely used, different studies formulate the task differently. This is because the term \"RCA\" implicitly covers tasks with distinct underlying goals. For instance, the goal of localizing a faulty service for rapid triage is fundamentally different from identifying a specific functional bug for a definitive fix. However, previous surveys have largely overlooked these goal-based distinctions, conventionally categorizing papers by input data types (e.g., metric-based vs. trace-based methods). This leads to the grouping of works with disparate objectives, thereby obscuring the true progress and gaps in the field. Meanwhile, the typical audience of an RCA survey is either laymen who want to know the goals and big picture of the task or RCA researchers who want to figure out past research under the same task formulation. Thus, an RCA survey that organizes the related papers according to their goals is in high demand. To this end, this paper presents a goal-driven framework that effectively categorizes and integrates 135 papers on RCA in the context of cloud incident management based on their diverse goals, spanning the period from 2014 to 2025. In addition to the goal-driven categorization, it discusses the ultimate goal of all RCA papers as an umbrella covering different RCA formulations. Moreover, the paper discusses open challenges and future directions in RCA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡äº‘æœåŠ¡ä¸­çš„æ•…éšœç®¡ç†ï¼Œå¯¹ Root Cause Analysis (RCA) é¢†åŸŸè¿›è¡Œäº†ä»¥ç›®æ ‡é©±åŠ¨ (Goal-Driven) ä¸ºæ ¸å¿ƒçš„ç³»ç»Ÿæ€§ç»¼è¿°ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œä¼ ç»Ÿç»¼è¿°å¤šæŒ‰æ•°æ®ç±»å‹ï¼ˆå¦‚ Metric-based æˆ– Trace-basedï¼‰å¯¹æ–‡çŒ®åˆ†ç±»ï¼Œå¿½è§†äº†æ•…éšœå¿«é€Ÿåˆ†è¯Šä¸åŠŸèƒ½ Bug ä¿®å¤ç­‰ä¸åŒä»»åŠ¡ç›®æ ‡å¸¦æ¥çš„æœ¬è´¨å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„ç›®æ ‡é©±åŠ¨æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°åˆ†ç±»å¹¶æ•´åˆäº† 2014 å¹´è‡³ 2025 å¹´é—´çš„ 135 ç¯‡ RCA ç›¸å…³ç ”ç©¶è®ºæ–‡ã€‚è¯¥ç»¼è¿°ä¸ä»…é˜è¿°äº†å„ç§ RCA ä»»åŠ¡å®šä¹‰çš„å†…åœ¨è”ç³»åŠå…¶æ¶µç›–çš„æœ€ç»ˆç›®æ ‡ï¼Œè¿˜æ·±å…¥æ¢è®¨äº†è¯¥é¢†åŸŸç›®å‰é¢ä¸´çš„å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚é€šè¿‡è¿™ç§åˆ†ç±»æ–¹å¼ï¼Œç ”ç©¶æ¸…æ™°åœ°å±•ç°äº† RCA é¢†åŸŸçš„æŠ€æœ¯è¿›å±•ä¸ç ”ç©¶ç©ºç™½ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æŒ‡æ˜äº†è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19593v1",
      "published_date": "2025-10-22 13:43:07 UTC",
      "updated_date": "2025-10-22 13:43:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:59:52.446296+00:00"
    },
    {
      "arxiv_id": "2510.19585v2",
      "title": "Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹æ£€æµ‹å†å²ä¹¦ç±ä¸­çš„æ‹‰ä¸è¯­ï¼šå¤šæ¨¡æ€åŸºå‡†",
      "authors": [
        "Yu Wu",
        "Ke Shu",
        "Jonas Fischer",
        "Lidia Pivovarova",
        "David Rosson",
        "Eetu MÃ¤kelÃ¤",
        "Mikko Tolonen"
      ],
      "abstract": "This paper presents a novel task of extracting Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstrate that reliable Latin detection with contemporary models is achievable. Our study provides the first comprehensive analysis of these models' capabilities and limits for this task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰å¤šæ ·å¸ƒå±€çš„æ··åˆè¯­è¨€å†å²æ–‡çŒ®ï¼Œæå‡ºäº†ä¸€é¡¹ä»æ–‡æ¡£ä¸­æå–æ‹‰ä¸è¯­(Latin)ç‰‡æ®µçš„æ–°ä»»åŠ¡ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«724ä¸ªæ ‡æ³¨é¡µé¢çš„å¤šæ¨¡æ€(Multimodal)æ•°æ®é›†ï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºå‡†è¯„ä¼°äº†å¤§å‹åŸºç¡€æ¨¡å‹(Large Foundation Models)çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨å½“ä»£æ¨¡å‹å®ç°å¯é çš„æ‹‰ä¸è¯­æ£€æµ‹æ˜¯å…·æœ‰å¯è¡Œæ€§çš„ã€‚è¯¥ç ”ç©¶é¦–æ¬¡å…¨é¢åˆ†æäº†è¿™äº›æ¨¡å‹åœ¨æ‰§è¡Œæ­¤ç±»ä»»åŠ¡æ—¶çš„èƒ½åŠ›ä¸å±€é™æ€§ï¼Œä¸ºåç»­æ··åˆè¯­è¨€å†å²æ–‡æ¡£çš„è‡ªåŠ¨åŒ–å¤„ç†å’Œæ•°å­—åŒ–ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.DL"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review. Both the dataset and code will be published",
      "pdf_url": "https://arxiv.org/pdf/2510.19585v2",
      "published_date": "2025-10-22 13:37:52 UTC",
      "updated_date": "2025-10-28 13:04:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:02.554281+00:00"
    },
    {
      "arxiv_id": "2510.21839v1",
      "title": "Evaluating ChatGPT's Performance in Classifying Pneumonia from Chest X-Ray Images",
      "title_zh": "è¯„ä¼° ChatGPT åœ¨èƒ¸éƒ¨ X å…‰å›¾åƒè‚ºç‚åˆ†ç±»ä¸­çš„è¡¨ç°",
      "authors": [
        "Pragna Prahallad",
        "Pranathi Prahallad"
      ],
      "abstract": "In this study, we evaluate the ability of OpenAI's gpt-4o model to classify chest X-ray images as either NORMAL or PNEUMONIA in a zero-shot setting, without any prior fine-tuning. A balanced test set of 400 images (200 from each class) was used to assess performance across four distinct prompt designs, ranging from minimal instructions to detailed, reasoning-based prompts. The results indicate that concise, feature-focused prompts achieved the highest classification accuracy of 74\\%, whereas reasoning-oriented prompts resulted in lower performance. These findings highlight that while ChatGPT exhibits emerging potential for medical image interpretation, its diagnostic reliability remains limited. Continued advances in visual reasoning and domain-specific adaptation are required before such models can be safely applied in clinical practice.",
      "tldr_zh": "æœ¬ç ”ç©¶è¯„ä¼°äº† OpenAI çš„ gpt-4o æ¨¡å‹åœ¨é›¶æ ·æœ¬(zero-shot)è®¾ç½®ä¸‹ï¼Œå°†èƒ¸éƒ¨ X å°„çº¿(chest X-ray)å›¾åƒåˆ†ç±»ä¸ºâ€œæ­£å¸¸(NORMAL)â€æˆ–â€œè‚ºç‚(PNEUMONIA)â€çš„èƒ½åŠ›ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ä¸ªåŒ…å« 400 å¼ å›¾åƒçš„å¹³è¡¡æµ‹è¯•é›†ï¼Œå¹¶æµ‹è¯•äº†ä»æç®€æŒ‡ä»¤åˆ°è¯¦ç»†æ¨ç†çš„å››ç§ä¸åŒæç¤º(prompt)è®¾è®¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç®€æ´ä¸”ä¾§é‡äºç‰¹å¾æå–çš„æç¤ºæ–¹æ¡ˆè¾¾åˆ°äº† 74% çš„æœ€é«˜åˆ†ç±»å‡†ç¡®ç‡ï¼Œè€ŒåŸºäºæ¨ç†çš„æç¤ºæ–¹æ¡ˆè¡¨ç°è¾ƒå·®ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå°½ç®¡ ChatGPT åœ¨åŒ»å­¦å›¾åƒè§£é‡Šæ–¹é¢å±•ç°å‡ºæ–°å…´æ½œåŠ›ï¼Œä½†å…¶å½“å‰çš„è¯Šæ–­å¯é æ€§ä»ç„¶æœ‰é™ã€‚ç ”ç©¶å¼ºè°ƒï¼Œåœ¨å°†æ­¤ç±»å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨åº”ç”¨äºä¸´åºŠå®è·µä¹‹å‰ï¼Œä»éœ€åœ¨è§†è§‰æ¨ç†(visual reasoning)å’Œé¢†åŸŸç‰¹å®šé€‚é…(domain-specific adaptation)æ–¹é¢å–å¾—è¿›ä¸€æ­¥è¿›å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.21839v1",
      "published_date": "2025-10-22 13:31:44 UTC",
      "updated_date": "2025-10-22 13:31:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:06.775240+00:00"
    },
    {
      "arxiv_id": "2510.19579v1",
      "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration",
      "title_zh": "åœ°çƒè§‚æµ‹ä¸­çš„å¤šæ¨¡æ€ååŒå­¦ä¹ ï¼šé€šè¿‡æ¨¡æ€åä½œå¢å¼ºå•æ¨¡æ€æ¨¡å‹",
      "authors": [
        "Francisco Mena",
        "Dino Ienco",
        "Cassio F. Dantas",
        "Roberto Interdonato",
        "Andreas Dengel"
      ],
      "abstract": "Multi-modal co-learning is emerging as an effective paradigm in machine learning, enabling models to collaboratively learn from different modalities to enhance single-modality predictions. Earth Observation (EO) represents a quintessential domain for multi-modal data analysis, wherein diverse remote sensors collect data to sense our planet. This unprecedented volume of data introduces novel challenges. Specifically, the access to the same sensor modalities at both training and inference stages becomes increasingly complex based on real-world constraints affecting remote sensing platforms. In this context, multi-modal co-learning presents a promising strategy to leverage the vast amount of sensor-derived data available at the training stage to improve single-modality models for inference-time deployment. Most current research efforts focus on designing customized solutions for either particular downstream tasks or specific modalities available at the inference stage. To address this, we propose a novel multi-modal co-learning framework capable of generalizing across various tasks without targeting a specific modality for inference. Our approach combines contrastive and modality discriminative learning together to guide single-modality models to structure the internal model manifold into modality-shared and modality-specific information. We evaluate our framework on four EO benchmarks spanning classification and regression tasks across different sensor modalities, where only one of the modalities available during training is accessible at inference time. Our results demonstrate consistent predictive improvements over state-of-the-art approaches from the recent machine learning and computer vision literature, as well as EO-specific methods. The obtained findings validate our framework in the single-modality inference scenarios across a diverse range of EO applications.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åœ°çƒè§‚æµ‹(Earth Observation)é¢†åŸŸåœ¨æ¨ç†é˜¶æ®µå› ä¼ æ„Ÿå™¨é™åˆ¶è€Œå¯¼è‡´çš„å¤šæ¨¡æ€æ•°æ®ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šç”¨çš„å¤šæ¨¡æ€ååŒå­¦ä¹ (Multi-modal Co-learning)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è®­ç»ƒé˜¶æ®µçš„æ¨¡æ€åä½œå¢å¼ºå•æ¨¡æ€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚è¯¥æ¡†æ¶æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–æ¨ç†æ¨¡æ€è¿›è¡Œå®šåˆ¶ï¼Œé€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ (Contrastive Learning)ä¸æ¨¡æ€è¾¨åˆ«å­¦ä¹ (Modality Discriminative Learning)ï¼Œå¼•å¯¼å•æ¨¡æ€æ¨¡å‹å°†å†…éƒ¨æµå½¢(Internal model manifold)åˆ’åˆ†ä¸ºæ¨¡æ€å…±äº«(Modality-shared)ä¸æ¨¡æ€ç‰¹å®š(Modality-specific)ä¿¡æ¯ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ¶µç›–åˆ†ç±»ä¸å›å½’ä»»åŠ¡çš„å››ä¸ªåœ°çƒè§‚æµ‹åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå®éªŒæ¨¡æ‹Ÿäº†è®­ç»ƒæ—¶å¤šæ¨¡æ€å¯ç”¨è€Œæ¨ç†æ—¶ä»…å•æ¨¡æ€å¯è¾¾çš„åœºæ™¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ä¼ æ„Ÿå™¨æ¨¡æ€ä¸‹å‡å–å¾—äº†ä¼˜äºç°æœ‰è®¡ç®—æœºè§†è§‰åŠåœ°çƒè§‚æµ‹ä¸“ç”¨å…ˆè¿›æ–¹æ³•çš„é¢„æµ‹æå‡ã€‚è¿™ä¸€æˆæœè¯å®äº†é€šè¿‡ç»“æ„åŒ–ç‰¹å¾è¡¨ç¤ºæ¥åˆ©ç”¨è®­ç»ƒæœŸå¤šæ¨¡æ€å†—ä½™ä¿¡æ¯ï¼Œèƒ½æœ‰æ•ˆå¼ºåŒ–å•æ¨¡æ€æ¨¡å‹åœ¨å¤æ‚åœ°çƒè§‚æµ‹åº”ç”¨ä¸­çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the Machine Learning journal, CfP: Discovery Science 2024",
      "pdf_url": "https://arxiv.org/pdf/2510.19579v1",
      "published_date": "2025-10-22 13:29:32 UTC",
      "updated_date": "2025-10-22 13:29:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:10.257942+00:00"
    },
    {
      "arxiv_id": "2510.19562v2",
      "title": "DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning",
      "title_zh": "DAILï¼šçªç ´è¯­è¨€æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¸­çš„ä»»åŠ¡æ­§ä¹‰æ€§",
      "authors": [
        "Runpeng Xie",
        "Quanwei Wang",
        "Hao Hu",
        "Zherui Zhou",
        "Ni Mu",
        "Xiyun Li",
        "Yiqin Yang",
        "Shuang Xu",
        "Qianchuan Zhao",
        "Bo XU"
      ],
      "abstract": "Comprehending natural language and following human instructions are critical capabilities for intelligent agents. However, the flexibility of linguistic instructions induces substantial ambiguity across language-conditioned tasks, severely degrading algorithmic performance. To address these limitations, we present a novel method named DAIL (Distributional Aligned Learning), featuring two key components: distributional policy and semantic alignment. Specifically, we provide theoretical results that the value distribution estimation mechanism enhances task differentiability. Meanwhile, the semantic alignment module captures the correspondence between trajectories and linguistic instructions. Extensive experimental results on both structured and visual observation benchmarks demonstrate that DAIL effectively resolves instruction ambiguities, achieving superior performance to baseline methods. Our implementation is available at https://github.com/RunpengXie/Distributional-Aligned-Learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DAIL (Distributional Aligned Learning) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¯­è¨€æ¡ä»¶å¼ºåŒ–å­¦ä¹  (Language-Conditioned Reinforcement Learning) ä¸­ç”±äºè¯­è¨€æŒ‡ä»¤çµæ´»æ€§å¯¼è‡´çš„ä»»åŠ¡æ­§ä¹‰é—®é¢˜ã€‚è¯¥æ–¹æ³•ç”±åˆ†å¸ƒç­–ç•¥ (distributional policy) å’Œè¯­ä¹‰å¯¹é½ (semantic alignment) ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆã€‚ç ”ç©¶é€šè¿‡ç†è®ºè¯æ˜ï¼Œå…¶ä»·å€¼åˆ†å¸ƒä¼°è®¡ (value distribution estimation) æœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºä»»åŠ¡çš„å¯åŒºåˆ†æ€§ã€‚è¯­ä¹‰å¯¹é½æ¨¡å—åˆ™ç”¨äºæ•æ‰æ‰§è¡Œè½¨è¿¹ä¸è¯­è¨€æŒ‡ä»¤ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œç¡®ä¿æ™ºèƒ½ä½“å¯¹æŒ‡ä»¤çš„å‡†ç¡®ç†è§£ã€‚åœ¨ç»“æ„åŒ–å’Œè§†è§‰è§‚æµ‹åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒDAIL æœ‰æ•ˆè§£å†³äº†æŒ‡ä»¤æ­§ä¹‰é—®é¢˜ï¼Œå¹¶åœ¨æ€§èƒ½è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Website at: https://github.com/RunpengXie/Distributional-Aligned-Learning",
      "pdf_url": "https://arxiv.org/pdf/2510.19562v2",
      "published_date": "2025-10-22 13:16:46 UTC",
      "updated_date": "2025-10-23 07:21:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:33.362300+00:00"
    },
    {
      "arxiv_id": "2510.19559v1",
      "title": "A Matter of Time: Revealing the Structure of Time in Vision-Language Models",
      "title_zh": "å…³ä¹æ—¶é—´ï¼šæ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ—¶é—´ç»“æ„",
      "authors": [
        "Nidham Tekaya",
        "Manuela Waldner",
        "Matthias Zeppelzauer"
      ],
      "abstract": "Large-scale vision-language models (VLMs) such as CLIP have gained popularity for their generalizable and expressive multimodal representations. By leveraging large-scale training data with diverse textual metadata, VLMs acquire open-vocabulary capabilities, solving tasks beyond their training scope. This paper investigates the temporal awareness of VLMs, assessing their ability to position visual content in time. We introduce TIME10k, a benchmark dataset of over 10,000 images with temporal ground truth, and evaluate the time-awareness of 37 VLMs by a novel methodology. Our investigation reveals that temporal information is structured along a low-dimensional, non-linear manifold in the VLM embedding space. Based on this insight, we propose methods to derive an explicit ``timeline'' representation from the embedding space. These representations model time and its chronological progression and thereby facilitate temporal reasoning tasks. Our timeline approaches achieve competitive to superior accuracy compared to a prompt-based baseline while being computationally efficient. All code and data are available at https://tekayanidham.github.io/timeline-page/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨æ—¶é—´è½´ä¸Šå®šä½è§†è§‰å†…å®¹çš„èƒ½åŠ›ã€‚ä½œè€…å¼•å…¥äº†åŒ…å«è¶…è¿‡10,000å¼ å…·æœ‰æ—¶é—´çœŸå€¼å›¾åƒçš„åŸºå‡†æ•°æ®é›† TIME10kï¼Œå¹¶é‡‡ç”¨ä¸€ç§æ–°é¢–çš„æ–¹æ³•è¯„ä¼°äº†37ä¸ª VLMs çš„æ—¶é—´æ„è¯†ã€‚ç ”ç©¶å‘ç°ï¼Œæ—¶é—´ä¿¡æ¯åœ¨ VLMs çš„åµŒå…¥ç©ºé—´ (Embedding Space) ä¸­å‘ˆç°å‡ºä¸€ç§ä½ç»´ã€éçº¿æ€§çš„æµå½¢ (Manifold) ç»“æ„ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ä»åµŒå…¥ç©ºé—´å¯¼å‡ºæ˜¾å¼â€œæ—¶é—´çº¿â€ (Timeline) è¡¨ç¤ºçš„æ–¹æ³•ï¼Œç”¨ä»¥æ¨¡æ‹Ÿæ—¶é—´åŠå…¶å¹´ä»£é¡ºåºçš„æ¼”è¿›ã€‚è¿™äº›è¡¨ç¤ºæ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ—¶é—´æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸”å…·æœ‰æé«˜çš„è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ—¶é—´çº¿æ–¹æ³•ä¸åŸºäºæç¤º (Prompt-based) çš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¾¾åˆ°äº†ç«äº‰æ€§ç”šè‡³æ›´ä¼˜çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19559v1",
      "published_date": "2025-10-22 13:14:02 UTC",
      "updated_date": "2025-10-22 13:14:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:21.647655+00:00"
    },
    {
      "arxiv_id": "2510.19544v1",
      "title": "Demonstrating Real Advantage of Machine-Learning-Enhanced Monte Carlo for Combinatorial Optimization",
      "title_zh": "è®ºè¯æœºå™¨å­¦ä¹ å¢å¼ºè’™ç‰¹å¡æ´›ç®—æ³•åœ¨ç»„åˆä¼˜åŒ–ä¸­çš„çœŸå®ä¼˜åŠ¿",
      "authors": [
        "Luca Maria Del Bono",
        "Federico Ricci-Tersenghi",
        "Francesco Zamponi"
      ],
      "abstract": "Combinatorial optimization problems are central to both practical applications and the development of optimization methods. While classical and quantum algorithms have been refined over decades, machine learning-assisted approaches are comparatively recent and have not yet consistently outperformed simple, state-of-the-art classical methods. Here, we focus on a class of Quadratic Unconstrained Binary Optimization (QUBO) problems, specifically the challenge of finding minimum energy configurations in three-dimensional Ising spin glasses. We use a Global Annealing Monte Carlo algorithm that integrates standard local moves with global moves proposed via machine learning. We show that local moves play a crucial role in achieving optimal performance. Benchmarking against Simulated Annealing and Population Annealing, we demonstrate that Global Annealing not only surpasses the performance of Simulated Annealing but also exhibits greater robustness than Population Annealing, maintaining effectiveness across problem hardness and system size without hyperparameter tuning. These results provide, to our knowledge, the first clear and robust evidence that a machine learning-assisted optimization method can exceed the capabilities of classical state-of-the-art techniques in a combinatorial optimization setting.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†æœºå™¨å­¦ä¹ å¢å¼ºçš„è’™ç‰¹å¡æ´›(Monte Carlo)ç®—æ³•åœ¨è§£å†³ç»„åˆä¼˜åŒ–(Combinatorial optimization)é—®é¢˜ä¸­çš„å®é™…ä¼˜åŠ¿ï¼Œä¸»è¦é’ˆå¯¹ä¸‰ç»´ä¼Šè¾›è‡ªæ—‹ç»ç’ƒ(Ising spin glasses)ä¸­çš„äºŒæ¬¡æ— çº¦æŸäºŒè¿›åˆ¶ä¼˜åŒ–(QUBO)æŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨å±€é€€ç«è’™ç‰¹å¡æ´›(Global Annealing Monte Carlo)ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡å°†æ ‡å‡†å±€éƒ¨ç§»åŠ¨ä¸æœºå™¨å­¦ä¹ ç”Ÿæˆçš„å…¨å±€ç§»åŠ¨ç›¸ç»“åˆï¼Œå¹¶è¯å®äº†å±€éƒ¨ç§»åŠ¨å¯¹å®ç°æœ€ä¼˜æ€§èƒ½çš„å¿…è¦æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä¸ä»…åœ¨æ€§èƒ½ä¸Šä¼˜äºæ¨¡æ‹Ÿé€€ç«(Simulated Annealing)ï¼Œä¸”åœ¨æ— éœ€è¶…å‚æ•°è°ƒä¼˜(hyperparameter tuning)çš„æƒ…å†µä¸‹æ¯”ç¾¤ä½“é€€ç«(Population Annealing)æ›´å…·é²æ£’æ€§ã€‚è¿™ä¸€æˆæœä¸ºæœºå™¨å­¦ä¹ è¾…åŠ©ä¼˜åŒ–æ–¹æ³•åœ¨ç»„åˆä¼˜åŒ–é¢†åŸŸè¶…è¶Šä¼ ç»Ÿæœ€å…ˆè¿›(state-of-the-art)æŠ€æœ¯æä¾›äº†é¦–ä¸ªæ˜ç¡®ä¸”ç¨³å¥çš„è¯æ®ã€‚",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "13 main pages, 6 main figures. 4 supplementary pages, 2 supplementary figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19544v1",
      "published_date": "2025-10-22 12:50:27 UTC",
      "updated_date": "2025-10-22 12:50:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:32.363509+00:00"
    },
    {
      "arxiv_id": "2510.19535v1",
      "title": "Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data",
      "title_zh": "æ´å¯ŸæœªçŸ¥ï¼šåˆ†å­æ•°æ®çš„è”é‚¦æ•°æ®å¤šæ ·æ€§åˆ†æ",
      "authors": [
        "Markus Bujotzek",
        "Evelyn Trautmann",
        "Calum Hand",
        "Ian Hales"
      ],
      "abstract": "AI methods are increasingly shaping pharmaceutical drug discovery. However, their translation to industrial applications remains limited due to their reliance on public datasets, lacking scale and diversity of proprietary pharmaceutical data. Federated learning (FL) offers a promising approach to integrate private data into privacy-preserving, collaborative model training across data silos. This federated data access complicates important data-centric tasks such as estimating dataset diversity, performing informed data splits, and understanding the structure of the combined chemical space. To address this gap, we investigate how well federated clustering methods can disentangle and represent distributed molecular data. We benchmark three approaches, Federated kMeans (Fed-kMeans), Federated Principal Component Analysis combined with Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated Locality-Sensitive Hashing (Fed-LSH), against their centralized counterparts on eight diverse molecular datasets. Our evaluation utilizes both, standard mathematical and a chemistry-informed evaluation metrics, SF-ICF, that we introduce in this work. The large-scale benchmarking combined with an in-depth explainability analysis shows the importance of incorporating domain knowledge through chemistry-informed metrics, and on-client explainability analyses for federated diversity analysis on molecular data.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨è¯ç‰©å‘ç°é¢†åŸŸä¸­ï¼Œå¦‚ä½•åˆ©ç”¨è”é‚¦å­¦ä¹ (Federated Learning)è§£å†³ç§æœ‰è¯å­¦æ•°æ®å­¤å²›å¯¼è‡´çš„æ•°æ®å¤šæ ·æ€§åˆ†æéš¾é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿç³»ç»Ÿè¯„ä¼°äº†è”é‚¦èšç±»æ–¹æ³•åœ¨è¡¨ç¤ºåˆ†å¸ƒå¼åˆ†å­æ•°æ®æ–¹é¢çš„èƒ½åŠ›ï¼Œé‡ç‚¹å¯¹æ¯”äº†Fed-kMeansã€Fed-PCA+Fed-kMeanså’ŒFed-LSHä¸‰ç§æ–¹æ³•åœ¨å…«ä¸ªåˆ†å­æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ä¸ºäº†æ›´ç²¾å‡†åœ°è¡¡é‡åŒ–å­¦ç©ºé—´ç»“æ„ï¼Œè¯¥ç ”ç©¶é™¤æ ‡å‡†æ•°å­¦æŒ‡æ ‡å¤–ï¼Œè¿˜åˆ›æ–°æ€§åœ°å¼•å…¥äº†åŒ–å­¦ä¿¡æ¯è¯„ä¼°æŒ‡æ ‡SF-ICFã€‚åŸºå‡†æµ‹è¯•ä¸æ·±å…¥çš„è§£é‡Šæ€§åˆ†æç»“æœè¡¨æ˜ï¼Œåœ¨è”é‚¦åˆ†å­æ•°æ®å¤šæ ·æ€§åˆ†æä¸­ï¼Œæ•´åˆé¢†åŸŸçŸ¥è¯†ä»¥åŠå¼€å±•å®¢æˆ·ç«¯è§£é‡Šæ€§åˆ†æ(On-client explainability)å…·æœ‰é‡è¦æ„ä¹‰ã€‚è¿™ä¸€æˆæœä¸ºåœ¨éšç§ä¿æŠ¤å‰æä¸‹è¿›è¡ŒçŸ¥æƒ…æ•°æ®æ‹†åˆ†(Informed data splits)å’Œç†è§£å¤æ‚åŒ–å­¦ç©ºé—´ç»“æ„æä¾›äº†å…³é”®çš„æŠ€æœ¯æ´å¯Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19535v1",
      "published_date": "2025-10-22 12:41:04 UTC",
      "updated_date": "2025-10-22 12:41:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:22.555889+00:00"
    },
    {
      "arxiv_id": "2510.19530v1",
      "title": "Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning",
      "title_zh": "ä¼˜åŒ–æœªçŸ¥ï¼šç»“åˆèƒ½é‡æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ çš„é»‘ç›’è´å¶æ–¯ä¼˜åŒ–",
      "authors": [
        "Ruiyao Miao",
        "Junren Xiao",
        "Shiya Tsang",
        "Hui Xiong",
        "Yingnian Wu"
      ],
      "abstract": "Existing Bayesian Optimization (BO) methods typically balance exploration and exploitation to optimize costly objective functions. However, these methods often suffer from a significant one-step bias, which may lead to convergence towards local optima and poor performance in complex or high-dimensional tasks. Recently, Black-Box Optimization (BBO) has achieved success across various scientific and engineering domains, particularly when function evaluations are costly and gradients are unavailable. Motivated by this, we propose the Reinforced Energy-Based Model for Bayesian Optimization (REBMBO), which integrates Gaussian Processes (GP) for local guidance with an Energy-Based Model (EBM) to capture global structural information. Notably, we define each Bayesian Optimization iteration as a Markov Decision Process (MDP) and use Proximal Policy Optimization (PPO) for adaptive multi-step lookahead, dynamically adjusting the depth and direction of exploration to effectively overcome the limitations of traditional BO methods. We conduct extensive experiments on synthetic and real-world benchmarks, confirming the superior performance of REBMBO. Additional analyses across various GP configurations further highlight its adaptability and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è´å¶æ–¯ä¼˜åŒ–(Bayesian Optimization)ä¸­å¸¸è§çš„å•æ­¥åå·®(one-step bias)ä»¥åŠåœ¨å¤æ‚é«˜ç»´ä»»åŠ¡ä¸­æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œæå‡ºäº†Reinforced Energy-Based Model for Bayesian Optimization (REBMBO)æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°ç»“åˆäº†ç”¨äºå±€éƒ¨å¼•å¯¼çš„é«˜æ–¯è¿‡ç¨‹(Gaussian Processes)å’Œç”¨äºæ•æ‰å…¨å±€ç»“æ„ä¿¡æ¯çš„èƒ½é‡æ¨¡å‹(Energy-Based Model)ï¼Œä»¥å¹³è¡¡å±€éƒ¨ä¸å…¨å±€çš„æ¢ç´¢ã€‚é€šè¿‡å°†æ¯æ¬¡ä¼˜åŒ–è¿­ä»£å®šä¹‰ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process)ï¼Œå¹¶åˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimization)è¿›è¡Œè‡ªé€‚åº”å¤šæ­¥å‰ç»(multi-step lookahead)ï¼ŒREBMBOèƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ¢ç´¢çš„æ·±åº¦ä¸æ–¹å‘ã€‚åœ¨åˆæˆåŠçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿè´å¶æ–¯ä¼˜åŒ–æ–¹æ¡ˆã€‚é’ˆå¯¹ä¸åŒé«˜æ–¯è¿‡ç¨‹é…ç½®çš„æ·±å…¥åˆ†æè¿›ä¸€æ­¥çªæ˜¾äº†è¯¥æ¨¡å‹åœ¨å¤„ç†å¤æ‚é»‘ç›’ä¼˜åŒ–é—®é¢˜æ—¶çš„å“è¶Šé€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted by 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.19530v1",
      "published_date": "2025-10-22 12:36:49 UTC",
      "updated_date": "2025-10-22 12:36:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:24.353741+00:00"
    },
    {
      "arxiv_id": "2510.19514v2",
      "title": "From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification",
      "title_zh": "ä»åŸå‹åˆ°ç¨€ç–å¿ƒç”µå›¾è§£é‡Šï¼šé¢å‘å¤šå˜é‡æ—¶é—´åºåˆ—å¤šåˆ†ç±»çš„ SHAP é©±åŠ¨åäº‹å®è§£é‡Š",
      "authors": [
        "Maciej Mozolewski",
        "BetÃ¼l Bayrak",
        "Kerstin Bach",
        "Grzegorz J. Nalepa"
      ],
      "abstract": "In eXplainable Artificial Intelligence (XAI), instance-based explanations for time series have gained increasing attention due to their potential for actionable and interpretable insights in domains such as healthcare. Addressing the challenges of explainability of state-of-the-art models, we propose a prototype-driven framework for generating sparse counterfactual explanations tailored to 12-lead ECG classification models. Our method employs SHAP-based thresholds to identify critical signal segments and convert them into interval rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract representative prototypes, and aligns these prototypes to query R-peaks for coherence with the sample being explained. The framework generates counterfactuals that modify only 78% of the original signal while maintaining 81.3% validity across all classes and achieving 43% improvement in temporal stability. We evaluate three variants of our approach, Original, Sparse, and Aligned Sparse, with class-specific performance ranging from 98.9% validity for myocardial infarction (MI) to challenges with hypertrophy (HYP) detection (13.2%). This approach supports near realtime generation (< 1 second) of clinically valid counterfactuals and provides a foundation for interactive explanation platforms. Our findings establish design principles for physiologically-aware counterfactual explanations in AI-based diagnosis systems and outline pathways toward user-controlled explanation interfaces for clinical deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸå‹é©±åŠ¨(prototype-driven)çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸º12å¯¼è”å¿ƒç”µå›¾(12-lead ECG)å¤šåˆ†ç±»æ¨¡å‹ç”Ÿæˆç¨€ç–åäº‹å®è§£é‡Š(sparse counterfactual explanations)ï¼Œä»¥æä¾›å…·æœ‰ä¸´åºŠæ„ä¹‰çš„å¯è§£é‡Šæ€§è§è§£ã€‚è¯¥æ–¹æ³•åˆ©ç”¨SHAP-based thresholdsè¯†åˆ«å…³é”®ä¿¡å·ç‰‡æ®µå¹¶è½¬åŒ–ä¸ºåŒºé—´è§„åˆ™ï¼Œç»“åˆåŠ¨æ€æ—¶é—´è§„æ•´(Dynamic Time Warping)å’Œä¸­å¿ƒç‚¹èšç±»(medoid clustering)æå–ä»£è¡¨æ€§åŸå‹ï¼Œå¹¶å°†å…¶ä¸å¾…è§£é‡Šæ ·æœ¬çš„Ræ³¢å³°(R-peaks)å¯¹é½ä»¥ç¡®ä¿ç”Ÿç†ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„åäº‹å®è§£é‡Šä»…éœ€ä¿®æ”¹åŸå§‹ä¿¡å·çš„78%ï¼Œåœ¨å„ç±»åˆ«ä¸­ç»´æŒäº†81.3%çš„æœ‰æ•ˆæ€§(validity)ï¼Œä¸”æ—¶é—´ç¨³å®šæ€§æå‡äº†43%ã€‚è¯¥ç®—æ³•æ”¯æŒäºšç§’çº§çš„å®æ—¶ç”Ÿæˆé€Ÿåº¦ï¼Œåœ¨å¿ƒè‚Œæ¢—æ­»(MI)ç­‰æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºæ„å»ºç”Ÿç†æ„ŸçŸ¥(physiologically-aware)çš„åŒ»ç–—AIè¯Šæ–­ç³»ç»ŸåŠäº¤äº’å¼è§£é‡Šç•Œé¢å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19514v2",
      "published_date": "2025-10-22 12:09:50 UTC",
      "updated_date": "2026-01-19 17:20:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:37.351081+00:00"
    },
    {
      "arxiv_id": "2510.21835v1",
      "title": "A Multimodal, Multitask System for Generating E Commerce Text Listings from Images",
      "title_zh": "ä¸€ç§åŸºäºå›¾åƒç”Ÿæˆç”µå­å•†åŠ¡å•†å“åˆ—è¡¨çš„å¤šæ¨¡æ€å¤šä»»åŠ¡ç³»ç»Ÿ",
      "authors": [
        "Nayan Kumar Singh"
      ],
      "abstract": "Manually generating catchy descriptions and names is labor intensive and a slow process for retailers. Although generative AI provides an automation solution in form of Vision to Language Models (VLM), the current VLMs are prone to factual \"hallucinations\". Siloed, single task models are not only inefficient but also fail to capture interdependent relationships between features. To address these challenges, we propose an end to end, multi task system that generates factually grounded textual listings from a single image. The contributions of this study are two proposals for the model architecture. First, application of multi task learning approach for fine tuning a vision encoder where a single vision backbone is jointly trained on attribute prediction such as color, hemline and neck style and price regression. Second, introduction of a hierarchical generation process where the model's own predicted attributes are embedded in a prompt and fed to the text decoder to improve factual consistency. The experiments demonstrate the superiority of this architecture. The multi tasking approach outperforms both the independent price regression, with a 3.6% better R2 Value and attribute classification, with a 6.6% improvement F1 score. Critically, the hierarchical generation process proves highly effective, slashing the factual hallucination rate from 12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical ablation. The hierarchical approach also reduces the latency of the autoregressive text generation process by a factor of 3.5 when compared to direct vision to language model of similar size. One minor caveat is that the model does perform 3.5% worse than direct vision-to-language model on ROUGE-L score.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€å¤šä»»åŠ¡ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡å•å¼ å›¾åƒç”Ÿæˆäº‹å®å‡†ç¡®çš„ç”µå­å•†åŠ¡æ–‡æœ¬åˆ—è¡¨ï¼Œä»¥è§£å†³äººå·¥ç”Ÿæˆæè¿°æ•ˆç‡ä½ä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹(VLM)å®¹æ˜“äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ã€‚è¯¥æ¶æ„é‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ (Multi-task learning)æ–¹æ³•æ¥å¾®è°ƒè§†è§‰ç¼–ç å™¨ï¼Œä½¿å•ä¸ªè§†è§‰ä¸»å¹²ç½‘ç»œèƒ½å¤ŸåŒæ—¶å¤„ç†å±æ€§é¢„æµ‹ï¼ˆå¦‚é¢œè‰²ã€é¢†å£æ ·å¼ï¼‰å’Œä»·æ ¼å›å½’ä»»åŠ¡ã€‚ç³»ç»Ÿè¿˜å¼•å…¥äº†åˆ†å±‚ç”Ÿæˆè¿‡ç¨‹(Hierarchical generation process)ï¼Œå°†æ¨¡å‹é¢„æµ‹å‡ºçš„å±æ€§åµŒå…¥æç¤ºè¯å¹¶è¾“å…¥æ–‡æœ¬è§£ç å™¨ï¼Œä»è€Œæ˜¾è‘—æå‡ç”Ÿæˆå†…å®¹çš„äº‹å®ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•åœ¨ä»·æ ¼å›å½’çš„R2å€¼å’Œå±æ€§åˆ†ç±»çš„F1åˆ†æ•°ä¸Šå‡ä¼˜äºç‹¬ç«‹æ¨¡å‹ã€‚å…³é”®åœ¨äºï¼Œåˆ†å±‚ç”Ÿæˆæ–¹æ³•å°†äº‹å®å¹»è§‰ç‡(Factual hallucination rate)ä»12.7%é™ä½è‡³7.1%ï¼Œå®ç°äº†44.5%çš„ç›¸å¯¹é™å¹…ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å°†è‡ªåŠ¨å›å½’æ–‡æœ¬ç”Ÿæˆçš„å»¶è¿Ÿé™ä½äº†3.5å€ï¼Œæ˜¾è‘—ä¼˜äºåŒè§„æ¨¡çš„ç›´æ¥è§†è§‰åˆ°è¯­è¨€æ¨¡å‹ã€‚å°½ç®¡åœ¨ROUGE-Lè¯„åˆ†ä¸Šç•¥ä½ï¼Œä½†è¯¥ç³»ç»Ÿåœ¨ç”Ÿæˆæ•ˆç‡å’Œäº‹å®å‡†ç¡®æ€§æ–¹é¢å±•ç°äº†æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 10 figures, 11 tables. Code can be found at: https://github.com/SinghNayanKumar/multimodal-product-lister/",
      "pdf_url": "https://arxiv.org/pdf/2510.21835v1",
      "published_date": "2025-10-22 11:50:49 UTC",
      "updated_date": "2025-10-22 11:50:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:40.461293+00:00"
    },
    {
      "arxiv_id": "2510.19497v1",
      "title": "Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse",
      "title_zh": "å¤šæ¨¡æ€äº¤é€šç³»ç»Ÿä¸­åŸºäºç”Ÿæˆå¼æ™ºèƒ½ä½“çš„çœŸå®äººç±»è¡Œä¸ºå»ºæ¨¡ï¼šè½¯ä»¶æ¶æ„åŠåœ¨ Toulouse çš„åº”ç”¨",
      "authors": [
        "Trung-Dung Vu",
        "Benoit Gaudou",
        "Kamaldeep Singh Oberoi"
      ],
      "abstract": "Modeling realistic human behaviour to understand people's mode choices in order to propose personalised mobility solutions remains challenging. This paper presents an architecture for modeling realistic human mobility behavior in complex multimodal transport systems, demonstrated through a case study in Toulouse, France. We apply Large Language Models (LLMs) within an agent-based simulation to capture decision-making in a real urban setting. The framework integrates the GAMA simulation platform with an LLM-based generative agent, along with General Transit Feed Specification (GTFS) data for public transport, and OpenTripPlanner for multimodal routing. GAMA platform models the interactive transport environment, providing visualization and dynamic agent interactions while eliminating the need to construct the simulation environment from scratch. This design enables a stronger focus on developing generative agents and evaluating their performance in transport decision-making processes. Over a simulated month, results show that agents not only make context-aware transport decisions but also form habits over time. We conclude that combining LLMs with agent-based simulation offers a promising direction for advancing intelligent transportation systems and personalised multimodal mobility solutions. We also discuss some limitations of this approach and outline future work on scaling to larger regions, integrating real-time data, and refining memory models.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç†è§£äººç±»å‡ºè¡Œæ–¹å¼é€‰æ‹©çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨å¤æ‚å¤šæ¨¡å¼äº¤é€šç³»ç»Ÿä¸­æ¨¡æ‹ŸçœŸå®äººç±»è¡Œä¸ºçš„è½¯ä»¶æ¶æ„ï¼Œå¹¶ä»¥æ³•å›½å›¾å¢å…¹ä¸ºæ¡ˆä¾‹è¿›è¡Œäº†éªŒè¯ã€‚è¯¥æ¡†æ¶å°† Large Language Models (LLMs) åº”ç”¨äºåŸºäºæ™ºèƒ½ä½“çš„æ¨¡æ‹Ÿä¸­ï¼Œé€šè¿‡é›†æˆ GAMA ä»¿çœŸå¹³å°ã€General Transit Feed Specification (GTFS) æ•°æ®å’Œ OpenTripPlanner å¤šæ¨¡å¼è·¯ç”±å¼•æ“ï¼Œæ•è·çœŸå®åŸå¸‚ç¯å¢ƒä¸‹çš„å†³ç­–è¿‡ç¨‹ã€‚åˆ©ç”¨ GAMA å¹³å°å»ºæ¨¡äº¤äº’å¼äº¤é€šç¯å¢ƒå¹¶æä¾›å¯è§†åŒ–åŠ¨æ€äº¤äº’ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´ä¸“æ³¨äº generative agents çš„å¼€å‘åŠå…¶åœ¨äº¤é€šå†³ç­–ä¸­çš„è¡¨ç°è¯„ä¼°ã€‚ä¸ºæœŸä¸€ä¸ªæœˆçš„ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ™ºèƒ½ä½“ä¸ä»…èƒ½åšå‡ºæ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„äº¤é€šå†³ç­–ï¼Œè¿˜èƒ½éšç€æ—¶é—´çš„æ¨ç§»é€æ¸å½¢æˆå‡ºè¡Œä¹ æƒ¯ã€‚ç ”ç©¶ç»“è®ºæ˜¾ç¤ºï¼Œå°† LLMs ä¸ agent-based simulation ç›¸ç»“åˆä¸ºæ¨åŠ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œä¸ªæ€§åŒ–å¤šæ¨¡å¼å‡ºè¡Œæ–¹æ¡ˆæä¾›äº†å…·æœ‰å‰æ™¯çš„å‘å±•æ–¹å‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19497v1",
      "published_date": "2025-10-22 11:45:44 UTC",
      "updated_date": "2025-10-22 11:45:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:52.556208+00:00"
    },
    {
      "arxiv_id": "2510.19496v1",
      "title": "CARES: Context-Aware Resolution Selector for VLMs",
      "title_zh": "CARESï¼šé¢å‘è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¾¨ç‡é€‰æ‹©å™¨",
      "authors": [
        "Moshe Kimhi",
        "Nimrod Shabtay",
        "Raja Giryes",
        "Chaim Baskin",
        "Eli Schwartz"
      ],
      "abstract": "Large vision-language models (VLMs) commonly process images at native or high resolution to remain effective across tasks. This inflates visual tokens ofter to 97-99% of total tokens, resulting in high compute and latency, even when low-resolution images would suffice. We introduce \\emph{CARES}-a \\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a lightweight preprocessing module that, given an image-query pair, predicts the \\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to extract features and predict when a target pretrained VLM's response converges to its peak ability to answer correctly. Though trained as a discrete classifier over a set of optional resolutions, CARES interpolates continuous resolutions at inference for fine-grained control. Across five multimodal benchmarks spanning documents and natural images, as well as diverse target VLMs, CARES preserves task performance while reducing compute by up to 80%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶äº§ç”Ÿè¿‡å¤šè§†è§‰ tokens å¯¼è‡´é«˜å»¶è¿Ÿå’Œè®¡ç®—å¼€é”€çš„é—®é¢˜ï¼Œæå‡ºäº† CARES (Context-Aware Resolution Selector) é¢„å¤„ç†æ¨¡å—ã€‚CARES æ˜¯ä¸€ç§è½»é‡çº§çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¾¨ç‡é€‰æ‹©å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®å›¾åƒ-æŸ¥è¯¢å¯¹é¢„æµ‹ç›®æ ‡æ¨¡å‹æ‰€éœ€çš„æœ€å°åˆ†è¾¨ç‡ã€‚è¯¥æ¨¡å—åˆ©ç”¨ä¸€ä¸ª 350M çš„å°å‹ VLM æå–ç‰¹å¾ï¼Œåˆ¤æ–­ç›®æ ‡æ¨¡å‹å“åº”è¾¾åˆ°å³°å€¼æ€§èƒ½çš„ä¸´ç•Œç‚¹ã€‚å°½ç®¡åœ¨è®­ç»ƒæ—¶ä½œä¸ºç¦»æ•£åˆ†ç±»å™¨ï¼Œä½† CARES åœ¨æ¨ç†é˜¶æ®µå¯æ’å€¼å‡ºè¿ç»­åˆ†è¾¨ç‡ä»¥å®ç°ç²¾ç»†åŒ–æ§åˆ¶ã€‚åœ¨æ¶‰åŠæ–‡æ¡£å’Œè‡ªç„¶å›¾åƒçš„äº”ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCARES åœ¨å¤šç§ç›®æ ‡ VLMs ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹å°†è®¡ç®—é‡å‡å°‘äº†é«˜è¾¾ 80%ã€‚è¿™ä¸€æˆæœä¸ºæå‡ VLM æ¨ç†æ•ˆç‡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19496v1",
      "published_date": "2025-10-22 11:44:31 UTC",
      "updated_date": "2025-10-22 11:44:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:00:58.966565+00:00"
    },
    {
      "arxiv_id": "2510.19495v2",
      "title": "Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning",
      "title_zh": "é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ åˆ©ç”¨éä¸“å®¶æ•°æ®å¢å¼ºæ¨¡ä»¿å­¦ä¹ çš„é²æ£’æ€§",
      "authors": [
        "Kevin Huang",
        "Rosario Scalise",
        "Cleah Winston",
        "Ayush Agrawal",
        "Yunchu Zhang",
        "Rohan Baijal",
        "Markus Grotz",
        "Byron Boots",
        "Benjamin Burchfiel",
        "Masha Itkina",
        "Paarth Shah",
        "Abhishek Gupta"
      ],
      "abstract": "Imitation learning has proven effective for training robots to perform complex tasks from expert human demonstrations. However, it remains limited by its reliance on high-quality, task-specific data, restricting adaptability to the diverse range of real-world object configurations and scenarios. In contrast, non-expert data -- such as play data, suboptimal demonstrations, partial task completions, or rollouts from suboptimal policies -- can offer broader coverage and lower collection costs. However, conventional imitation learning approaches fail to utilize this data effectively. To address these challenges, we posit that with right design decisions, offline reinforcement learning can be used as a tool to harness non-expert data to enhance the performance of imitation learning policies. We show that while standard offline RL approaches can be ineffective at actually leveraging non-expert data under the sparse data coverage settings typically encountered in the real world, simple algorithmic modifications can allow for the utilization of this data, without significant additional assumptions. Our approach shows that broadening the support of the policy distribution can allow imitation algorithms augmented by offline RL to solve tasks robustly, showing considerably enhanced recovery and generalization behavior. In manipulation tasks, these innovations significantly increase the range of initial conditions where learned policies are successful when non-expert data is incorporated. Moreover, we show that these methods are able to leverage all collected data, including partial or suboptimal demonstrations, to bolster task-directed policy performance. This underscores the importance of algorithmic techniques for using non-expert data for robust policy learning in robotics. Website: https://uwrobotlearning.github.io/RISE-offline/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡ä»¿å­¦ä¹ (Imitation Learning)å› é«˜åº¦ä¾èµ–é«˜è´¨é‡ä¸“å®¶æ¼”ç¤ºè€Œå¯¼è‡´åœ¨å¤šæ ·åŒ–ç°å®åœºæ™¯ä¸­é€‚åº”æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline Reinforcement Learning)ä½œä¸ºæ ¸å¿ƒå·¥å…·ï¼Œé€šè¿‡ç®€å•çš„ç®—æ³•æ”¹è¿›æ¥æœ‰æ•ˆæ•´åˆåŒ…æ‹¬æ¬¡ä¼˜æ¼”ç¤º(suboptimal demonstrations)å’Œå¨±ä¹æ•°æ®(play data)åœ¨å†…çš„éä¸“å®¶æ•°æ®(Non-Expert Data)ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ‹“å®½ç­–ç•¥åˆ†å¸ƒçš„æ”¯æ’‘é›†ï¼Œè§£å†³äº†æ ‡å‡†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨ç°å®ä¸–ç•Œå¸¸è§çš„ç¨€ç–æ•°æ®ç¯å¢ƒä¸‹éš¾ä»¥åˆ©ç”¨éä¸“å®¶æ•°æ®çš„éš¾é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æœºå™¨äººæ“çºµä»»åŠ¡(manipulation tasks)ä¸­ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æ‰©å¤§äº†ç­–ç•¥æˆåŠŸçš„åˆå§‹æ¡ä»¶èŒƒå›´ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„æ¢å¤èƒ½åŠ›ä¸æ³›åŒ–è¡¨ç°ã€‚ç ”ç©¶è¯æ˜äº†å³ä½¿æ˜¯éƒ¨åˆ†å®Œæˆçš„ä»»åŠ¡æˆ–æ¬¡ä¼˜æ•°æ®ï¼Œåœ¨åˆç†çš„ç®—æ³•æ¡†æ¶ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆæå‡ä»»åŠ¡å¯¼å‘çš„ç­–ç•¥æ€§èƒ½ã€‚è¿™ä¸€æˆæœå¼ºè°ƒäº†å¼€å‘é’ˆå¯¹æ€§ç®—æ³•æŠ€æœ¯ä»¥åˆ©ç”¨éä¸“å®¶æ•°æ®æ„å»ºå¼ºé²æ£’æ€§(robustness)æœºå™¨äººç­–ç•¥çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19495v2",
      "published_date": "2025-10-22 11:43:39 UTC",
      "updated_date": "2025-10-25 01:18:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:06.564908+00:00"
    },
    {
      "arxiv_id": "2510.19488v1",
      "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
      "title_zh": "VideoAgentTrekï¼šåŸºäºæ— æ ‡æ³¨è§†é¢‘çš„è®¡ç®—æœºä½¿ç”¨é¢„è®­ç»ƒ",
      "authors": [
        "Dunjie Lu",
        "Yiheng Xu",
        "Junli Wang",
        "Haoyuan Wu",
        "Xinyuan Wang",
        "Zekun Wang",
        "Junlin Yang",
        "Hongjin Su",
        "Jixuan Chen",
        "Junda Chen",
        "Yuchen Mao",
        "Jingren Zhou",
        "Junyang Lin",
        "Binyuan Hui",
        "Tao Yu"
      ],
      "abstract": "Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VideoAgentTrekï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä»å…¬å¼€å±å¹•å½•åˆ¶è§†é¢‘ä¸­è‡ªåŠ¨æŒ–æ˜è®­ç»ƒæ•°æ®çš„é«˜å¯æ‰©å±•æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³ Computer-use agents åœ¨ GUI äº¤äº’é¢†åŸŸé¢ä¸´çš„æ‰‹å·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒç»„ä»¶ Video2Action æ˜¯ä¸€ç§åå‘åŠ¨åŠ›å­¦æ¨¡å‹(Inverse Dynamics Module, IDM)ï¼Œé€šè¿‡ Video grounding model å®ç°åŠ¨ä½œçš„ç²¾ç¡®æ—¶ç©ºå®šä½ï¼Œå¹¶åˆ©ç”¨ Action-content recognizer æå–ç‚¹å‡»åæ ‡å’Œæ–‡æœ¬ç­‰ç»“æ„åŒ–å‚æ•°ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è¯¥æµæ°´çº¿ä» 39,000 ä¸ª YouTube æ•™ç¨‹è§†é¢‘ä¸­è‡ªåŠ¨ç”Ÿæˆäº† 152 ä¸‡ä¸ªäº¤äº’æ­¥éª¤ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†æŒç»­é¢„è®­ç»ƒä¸ Supervised Fine-Tuningã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ OSWorld-Verified ä¸Šçš„ä»»åŠ¡æˆåŠŸç‡ä» 9.3% æå‡è‡³ 15.8%ï¼Œå®ç°äº† 70% çš„ç›¸å¯¹å¢é•¿ï¼ŒåŒæ—¶åœ¨ AgentNetBench ä¸Šçš„å•æ­¥å‡†ç¡®ç‡ä¹Ÿæ˜¾è‘—æé«˜ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è¢«åŠ¨çš„äº’è”ç½‘è§†é¢‘å¯ä»¥è½¬åŒ–ä¸ºé«˜è´¨é‡çš„ç›‘ç£æ•°æ®ï¼Œä¸ºæ„å»ºå¤§è§„æ¨¡æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ— éœ€äººå·¥å¹²é¢„çš„å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19488v1",
      "published_date": "2025-10-22 11:25:48 UTC",
      "updated_date": "2025-10-22 11:25:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:04.160864+00:00"
    },
    {
      "arxiv_id": "2510.19484v1",
      "title": "KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge",
      "title_zh": "KnowMolï¼šåˆ©ç”¨å¤šå±‚æ¬¡åŒ–å­¦çŸ¥è¯†æå‡åˆ†å­å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zaifei Yang",
        "Hong Chang",
        "Ruibing Hou",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "abstract": "The molecular large language models have garnered widespread attention due to their promising potential on molecular applications. However, current molecular large language models face significant limitations in understanding molecules due to inadequate textual descriptions and suboptimal molecular representation strategies during pretraining. To address these challenges, we introduce KnowMol-100K, a large-scale dataset with 100K fine-grained molecular annotations across multiple levels, bridging the gap between molecules and textual descriptions. Additionally, we propose chemically-informative molecular representation, effectively addressing limitations in existing molecular representation strategies. Building upon these innovations, we develop KnowMol, a state-of-the-art multi-modal molecular large language model. Extensive experiments demonstrate that KnowMol achieves superior performance across molecular understanding and generation tasks.\n  GitHub: https://github.com/yzf-code/KnowMol\n  Huggingface: https://hf.co/datasets/yzf1102/KnowMol-100K",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KnowMolï¼Œä¸€ç§å…ˆè¿›çš„å¤šæ¨¡æ€åˆ†å­å¤§è¯­è¨€æ¨¡å‹(multi-modal molecular LLM)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨åˆ†å­ç†è§£ä¸­é¢ä¸´çš„æ–‡æœ¬æè¿°ä¸è¶³å’Œåˆ†å­è¡¨ç¤ºç­–ç•¥ä¸ä½³ç­‰å±€é™æ€§ã€‚ä¸ºäº†å¼¥è¡¥åˆ†å­ä¸æ–‡æœ¬æè¿°ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä½œè€…æ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†KnowMol-100Kï¼ŒåŒ…å«10ä¸‡æ¡è·¨è¶Šå¤šä¸ªå±‚çº§çš„ç»†ç²’åº¦åˆ†å­æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŒ–å­¦ä¿¡æ¯ä¸°å¯Œçš„åˆ†å­è¡¨ç¤º(chemically-informative molecular representation)æ–¹æ³•ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿè¡¨ç¤ºç­–ç•¥çš„é™åˆ¶ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒKnowMolåœ¨åˆ†å­ç†è§£(molecular understanding)å’Œç”Ÿæˆä»»åŠ¡(molecular generation)ä¸­å‡å–å¾—äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚è¯¥å·¥ä½œé€šè¿‡æ•´åˆå¤šå±‚æ¬¡åŒ–å­¦çŸ¥è¯†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19484v1",
      "published_date": "2025-10-22 11:23:58 UTC",
      "updated_date": "2025-10-22 11:23:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:03.541081+00:00"
    },
    {
      "arxiv_id": "2510.19479v1",
      "title": "Graph Unlearning Meets Influence-aware Negative Preference Optimization",
      "title_zh": "å›¾é—å¿˜å­¦ä¹ ä¸å½±å“æ„ŸçŸ¥è´Ÿå‘åå¥½ä¼˜åŒ–",
      "authors": [
        "Qiang Chen",
        "Zhongze Wu",
        "Ang He",
        "Xi Lin",
        "Shuo Jiang",
        "Shan You",
        "Chang Xu",
        "Yi Chen",
        "Xiu Su"
      ],
      "abstract": "Recent advancements in graph unlearning models have enhanced model utility by preserving the node representation essentially invariant, while using gradient ascent on the forget set to achieve unlearning. However, this approach causes a drastic degradation in model utility during the unlearning process due to the rapid divergence speed of gradient ascent. In this paper, we introduce \\textbf{INPO}, an \\textbf{I}nfluence-aware \\textbf{N}egative \\textbf{P}reference \\textbf{O}ptimization framework that focuses on slowing the divergence speed and improving the robustness of the model utility to the unlearning process. Specifically, we first analyze that NPO has slower divergence speed and theoretically propose that unlearning high-influence edges can reduce impact of unlearning. We design an influence-aware message function to amplify the influence of unlearned edges and mitigate the tight topological coupling between the forget set and the retain set. The influence of each edge is quickly estimated by a removal-based method. Additionally, we propose a topological entropy loss from the perspective of topology to avoid excessive information loss in the local structure during unlearning. Extensive experiments conducted on five real-world datasets demonstrate that INPO-based model achieves state-of-the-art performance on all forget quality metrics while maintaining the model's utility. Codes are available at \\href{https://github.com/sh-qiangchen/INPO}{https://github.com/sh-qiangchen/INPO}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾é—å¿˜(Graph Unlearning)æ¨¡å‹åœ¨åˆ©ç”¨æ¢¯åº¦ä¸Šå‡(gradient ascent)è¿›è¡Œæ•°æ®ç§»é™¤æ—¶ï¼Œå› æ¨¡å‹å‘æ•£é€Ÿåº¦è¿‡å¿«è€Œå¯¼è‡´çš„æ•ˆç”¨(utility)æ€¥å‰§ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†INPOæ¡†æ¶ã€‚INPOæ˜¯ä¸€ç§å½±å“æ„ŸçŸ¥è´Ÿåå¥½ä¼˜åŒ–(Influence-aware Negative Preference Optimization)æ–¹æ¡ˆï¼Œåˆ©ç”¨è´Ÿåå¥½ä¼˜åŒ–(NPO)å…·æœ‰æ›´æ…¢å‘æ•£é€Ÿåº¦çš„ç‰¹æ€§ï¼Œæ—¨åœ¨æå‡é—å¿˜è¿‡ç¨‹ä¸­çš„æ¨¡å‹é²æ£’æ€§ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†å½±å“æ„ŸçŸ¥æ¶ˆæ¯å‡½æ•°æ¥æ”¾å¤§é—å¿˜è¾¹çš„å½±å“ï¼Œä»è€Œç¼“è§£é—å¿˜é›†ä¸ä¿ç•™é›†ä¹‹é—´çš„ç´§å¯†æ‹“æ‰‘è€¦åˆï¼Œå¹¶ç»“åˆç§»é™¤æ³•å¿«é€Ÿä¼°ç®—è¾¹çš„å½±å“åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ‹“æ‰‘ç†µæŸå¤±(topological entropy loss)ä»¥é˜²æ­¢å±€éƒ¨ç»“æ„åœ¨é—å¿˜è¿‡ç¨‹ä¸­å‡ºç°è¿‡åº¦ä¿¡æ¯ä¸¢å¤±ã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒINPOåœ¨æ‰€æœ‰é—å¿˜è´¨é‡æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œå¹¶èƒ½æœ‰æ•ˆå…¼é¡¾æ¨¡å‹çš„æ•ˆç”¨ä¿æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19479v1",
      "published_date": "2025-10-22 11:18:00 UTC",
      "updated_date": "2025-10-22 11:18:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:09.053518+00:00"
    },
    {
      "arxiv_id": "2510.19476v1",
      "title": "A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring",
      "title_zh": "åŸºäºæ€ç»´é“¾ç›‘æ§çš„å®‰å…¨è®ºè¯å…·ä½“è·¯çº¿å›¾",
      "authors": [
        "Julian Schulz"
      ],
      "abstract": "As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI ç³»ç»Ÿæ¥è¿‘å±é™©èƒ½åŠ›æ°´å¹³çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€å¥—åŸºäºæ¨ç†æ¨¡å‹ä¸­é“¾å¼æ€ç»´(Chain-of-Thought, CoT)ç›‘æ§æ„å»ºå®‰å…¨æ¡ˆä¾‹(safety cases)çš„è·¯çº¿å›¾ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€æ˜¯ç¡®è®¤æ¨¡å‹åœ¨è„±ç¦» CoT è¿è¡Œæ—¶ç¼ºä¹å±é™©èƒ½åŠ›ï¼ŒäºŒæ˜¯ç¡®ä¿ç”± CoT å¯å‘çš„ä»»ä½•å±é™©è¡Œä¸ºå‡èƒ½è¢«æœ‰æ•ˆç›‘æ§æ£€æµ‹ã€‚è®ºæ–‡æ·±å…¥åˆ†æäº†å½±å“ç›‘æ§æœ‰æ•ˆæ€§çš„æ ¸å¿ƒå¨èƒï¼ŒåŒ…æ‹¬ neuralese å’Œ encoded reasoningï¼ˆå…·ä½“æ¶µç›– linguistic driftã€steganography å’Œ alien reasoning ç­‰å½¢å¼ï¼‰ï¼Œå¹¶è¯„ä¼°äº†ç»´æŒ CoT å¿ å®åº¦(faithfulness)çš„æŠ€æœ¯æ‰‹æ®µã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ä»ä¸å¯ç›‘æ§çš„æ¨ç†ä¸­æå–å¯ç›‘æ§ CoT çš„æ–¹æ¡ˆï¼Œå¹¶åˆ©ç”¨é¢„æµ‹å¸‚åœº(prediction markets)å¯¹è¯¥è·¯çº¿å›¾çš„æŠ€æœ¯å¯è¡Œæ€§è¿›è¡Œäº†ç»¼åˆè¯„ä¼°ã€‚è¯¥å·¥ä½œä¸ºåœ¨å¤æ‚æ¨ç†æ¨¡å‹ä¸­å®ç°å¯è¯æ˜çš„ AI å®‰å…¨æä¾›äº†ç³»ç»Ÿæ€§çš„ç ”ç©¶è®®ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19476v1",
      "published_date": "2025-10-22 11:13:52 UTC",
      "updated_date": "2025-10-22 11:13:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:15.755079+00:00"
    },
    {
      "arxiv_id": "2510.19470v1",
      "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission",
      "title_zh": "HybridEPï¼šé€šè¿‡æ··åˆä¸“å®¶/æ•°æ®ä¼ è¾“å®ç°è·¨æ•°æ®ä¸­å¿ƒåœºæ™¯ä¸‹çš„ä¸“å®¶å¹¶è¡Œæ‰©å±•",
      "authors": [
        "Weihao Yang",
        "Hao Huang",
        "Donglei Wu",
        "Ningke Li",
        "Yanqi Pan",
        "Qiyang Zheng",
        "Wen Xia",
        "Shiyi Li",
        "Qiang Wang"
      ],
      "abstract": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large models. However, the rapidly growing scale outpaces model training on a single DC, driving a shift toward a more flexible, cross-DC training paradigm. Under this, Expert Parallelism (EP) of MoE faces significant scalability issues due to the limited cross-DC bandwidth. Specifically, existing EP optimizations attempt to overlap data communication and computation, which has little benefit in low-bandwidth scenarios due to a much longer data communication time. Therefore, the trends of cross-DC EP scaling is fast becoming a critical roadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize EP under constrained bandwidth. Our key idea is to dynamically transform the spatial placement of experts to reduce data communication traffic and frequency, thereby minimizing EP's communication overheads. However, it is non-trivial to find the optimal solution because it complicates the original communication pattern by mixing data and expert communication. We therefore build a stream-based model to determine the optimal transmission ratio. Guided by this, we incorporate two techniques: (1) domain-based partition to construct the mapping between hybrid patterns and specific communication topology at GPU level, and (2) parameter-efficient migration to further refine this topology by reducing expert transmission overhead and enlarging the domain size. Combining all these designs, HybridEP can be considered as a more general EP with better scalability. Experimental results show that HybridEP outperforms existing state-of-the-art MoE training systems by up to 5.6x under constrained bandwidth. We further compare HybridEP and EP on large-scale simulations. HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.",
      "tldr_zh": "é’ˆå¯¹æ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts, MoE)åœ¨è·¨æ•°æ®ä¸­å¿ƒ(cross-DC)è®­ç»ƒä¸­å—é™äºä½å¸¦å®½è€Œå¯¼è‡´çš„ä¸“å®¶å¹¶è¡Œ(Expert Parallelism, EP)æ‰©å±•æ€§éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†HybridEPå»ºæ¨¡æŒ‡å¯¼æ¡†æ¶ã€‚HybridEPçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åŠ¨æ€è½¬æ¢ä¸“å®¶çš„ç©ºé—´åˆ†å¸ƒï¼Œå‡å°‘æ•°æ®é€šä¿¡çš„æµé‡å’Œé¢‘ç‡ï¼Œä»è€Œæœ‰æ•ˆæœ€å°åŒ–EPçš„é€šä¿¡å¼€é”€ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŸºäºæµçš„æ¨¡å‹(stream-based model)æ¥ç¡®å®šæ•°æ®ä¸ä¸“å®¶é€šä¿¡çš„æœ€ä½³ä¼ è¾“æ¯”ä¾‹ï¼Œå¹¶é‡‡ç”¨åŸºäºé¢†åŸŸçš„åˆ’åˆ†(domain-based partition)æ¥æ„å»ºæ··åˆæ¨¡å¼ä¸GPUçº§é€šä¿¡æ‹“æ‰‘çš„æ˜ å°„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨å‚æ•°é«˜æ•ˆè¿ç§»(parameter-efficient migration)æŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–æ‹“æ‰‘ç»“æ„ï¼Œå‡å°‘ä¸“å®¶ä¼ è¾“å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å—é™å¸¦å®½ä¸‹ï¼ŒHybridEPæ¯”ç°æœ‰æœ€å…ˆè¿›çš„MoEè®­ç»ƒç³»ç»Ÿæ€§èƒ½æå‡é«˜è¾¾5.6å€ã€‚åœ¨å¤§è§„æ¨¡ä»¿çœŸä¸­ï¼ŒHybridEPåœ¨1kä¸ªæ•°æ®ä¸­å¿ƒçš„ä¸åŒå¸¦å®½ç¯å¢ƒä¸‹ç›¸æ¯”ä¼ ç»ŸEPå®ç°äº†1.45å€çš„åŠ é€Ÿï¼Œä¸ºMoEæ¨¡å‹åœ¨åˆ†å¸ƒå¼åœºæ™¯ä¸‹çš„æŒç»­æ‰©å±•æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19470v1",
      "published_date": "2025-10-22 11:05:17 UTC",
      "updated_date": "2025-10-22 11:05:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:16.453185+00:00"
    },
    {
      "arxiv_id": "2510.19444v2",
      "title": "A Foundational Theory of Quantitative Abstraction: Adjunctions, Duality, and Logic for Probabilistic Systems",
      "title_zh": "å®šé‡æŠ½è±¡çš„åŸºç¡€ç†è®ºï¼šæ¦‚ç‡ç³»ç»Ÿçš„ä¼´éšã€å¯¹å¶ä¸é€»è¾‘",
      "authors": [
        "Nivar Anwer",
        "Ezequiel LÃ³pez-Rubio",
        "David Elizondo",
        "Rafael M. Luque-Baena"
      ],
      "abstract": "The analysis and control of stochastic dynamical systems rely on probabilistic models such as (continuous-space) Markov decision processes, but large or continuous state spaces make exact analysis intractable and call for principled quantitative abstraction. This work develops a unified theory of such abstraction by integrating category theory, coalgebra, quantitative logic, and optimal transport, centred on a canonical $\\varepsilon$-quotient of the behavioral pseudo-metric with a universal property: among all abstractions that collapse behavioral differences below $\\varepsilon$, it is the most detailed, and every other abstraction achieving the same discounted value-loss guarantee factors uniquely through it. Categorically, a quotient functor $Q_\\varepsilon$ from a category of probabilistic systems to a category of metric specifications admits, via the Special Adjoint Functor Theorem, a right adjoint $R_\\varepsilon$, yielding an adjunction $Q_\\varepsilon \\dashv R_\\varepsilon$ that formalizes a duality between abstraction and realization; logically, a quantitative modal $Î¼$-calculus with separate reward and transition modalities is shown, for a broad class of systems, to be expressively complete for the behavioral pseudo-metric, with a countable fully abstract fragment suitable for computation. The theory is developed coalgebraically over Polish spaces and the Giry monad and validated on finite-state models using optimal-transport solvers, with experiments corroborating the predicted contraction properties and structural stability and aligning with the theoretical value-loss bounds, thereby providing a rigorous foundation for quantitative state abstraction and representation learning in probabilistic domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éšæœºåŠ¨åŠ›ç³»ç»Ÿï¼ˆStochastic dynamical systemsï¼‰åœ¨å¤§è§„æ¨¡æˆ–è¿ç»­çŠ¶æ€ç©ºé—´ä¸‹éš¾ä»¥ç²¾ç¡®åˆ†æçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—æ•´åˆèŒƒç•´è®ºï¼ˆCategory theoryï¼‰ã€ä½™ä»£æ•°ï¼ˆCoalgebraï¼‰ã€å®šé‡é€»è¾‘ï¼ˆQuantitative logicï¼‰ä¸æœ€ä¼˜ä¼ è¾“ï¼ˆOptimal transportï¼‰çš„å®šé‡æŠ½è±¡ï¼ˆQuantitative abstractionï¼‰ç»Ÿä¸€ç†è®ºã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºå®šä¹‰äº†è¡Œä¸ºä¼ªåº¦é‡ï¼ˆBehavioral pseudo-metricï¼‰çš„ $\\varepsilon$-å•†ï¼ˆ$\\varepsilon$-quotientï¼‰ï¼Œå¹¶è¯æ˜å…¶åœ¨æ‰€æœ‰æ»¡è¶³æŠ˜ç°ä»·å€¼æŸå¤±ä¿éšœçš„æŠ½è±¡ä¸­å…·æœ‰æœ€è¯¦å°½çš„æ™®é€‚æ€§è´¨ï¼ˆUniversal propertyï¼‰ã€‚é€šè¿‡ç‰¹æ®Šä¼´éšå‡½å­å®šç†ï¼Œç ”ç©¶å½¢å¼åŒ–äº†æŠ½è±¡ä¸å®ç°ä¹‹é—´çš„ä¼´éšå…³ç³»ï¼ˆAdjunctionï¼‰ï¼Œæ­ç¤ºäº†ä¸¤è€…é—´çš„å¯¹å¶æ€§ã€‚åœ¨é€»è¾‘å±‚é¢ï¼Œè¯æ˜äº†å®šé‡æ¨¡æ€ $\\mu$-æ¼”ç®—ï¼ˆQuantitative modal $\\mu$-calculusï¼‰å¯¹äºè¡Œä¸ºä¼ªåº¦é‡å…·æœ‰è¡¨è¾¾å®Œå¤‡æ€§ï¼ˆExpressively completeï¼‰ï¼Œå¹¶å­˜åœ¨é€‚ç”¨äºè®¡ç®—çš„æŠ½è±¡ç‰‡æ®µã€‚å®éªŒåˆ©ç”¨æœ€ä¼˜ä¼ è¾“æ±‚è§£å™¨åœ¨æœ‰é™çŠ¶æ€æ¨¡å‹ä¸ŠéªŒè¯äº†ç†è®ºé¢„æœŸçš„æ”¶ç¼©ç‰¹æ€§ä¸ç»“æ„ç¨³å®šæ€§ï¼Œä¸”ç»“æœä¸ç†è®ºä»·å€¼æŸå¤±ç•Œé™é«˜åº¦ä¸€è‡´ã€‚è¯¥ç†è®ºä¸ºæ¦‚ç‡é¢†åŸŸçš„å®šé‡çŠ¶æ€æŠ½è±¡ä¸è¡¨ç¤ºå­¦ä¹ ï¼ˆRepresentation learningï¼‰æä¾›äº†ä¸¥è°¨çš„æ•°å­¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19444v2",
      "published_date": "2025-10-22 10:16:24 UTC",
      "updated_date": "2025-11-05 01:26:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:19.156538+00:00"
    },
    {
      "arxiv_id": "2510.19429v1",
      "title": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning",
      "title_zh": "NeSyPrï¼šé¢å‘é«˜æ•ˆå…·èº«æ¨ç†çš„ç¥ç»ç¬¦å·ç¨‹åºåŒ–",
      "authors": [
        "Wonje Choi",
        "Jooyoung Kim",
        "Honguk Woo"
      ],
      "abstract": "We address the challenge of adopting language models (LMs) for embodied tasks in dynamic environments, where online access to large-scale inference engines or symbolic planners is constrained due to latency, connectivity, and resource limitations. To this end, we present NeSyPr, a novel embodied reasoning framework that compiles knowledge via neurosymbolic proceduralization, thereby equipping LM-based agents with structured, adaptive, and timely reasoning capabilities. In NeSyPr, task-specific plans are first explicitly generated by a symbolic tool leveraging its declarative knowledge. These plans are then transformed into composable procedural representations that encode the plans' implicit production rules, enabling the resulting composed procedures to be seamlessly integrated into the LM's inference process. This neurosymbolic proceduralization abstracts and generalizes multi-step symbolic structured path-finding and reasoning into single-step LM inference, akin to human knowledge compilation. It supports efficient test-time inference without relying on external symbolic guidance, making it well suited for deployment in latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating its efficient reasoning capabilities over large-scale reasoning models and a symbolic planner, while using more compact LMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NeSyPrï¼Œä¸€ç§åä¸º Neurosymbolic Proceduralization çš„å…·èº«æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹ (LMs) åœ¨åŠ¨æ€ç¯å¢ƒä¸­å› å»¶è¿Ÿã€è¿æ¥å’Œèµ„æºé™åˆ¶è€Œéš¾ä»¥åº”ç”¨çš„é—®é¢˜ã€‚NeSyPr é¦–å…ˆåˆ©ç”¨ç¬¦å·å·¥å…· (symbolic tool) æ ¹æ®å£°æ˜æ€§çŸ¥è¯† (declarative knowledge) ç”Ÿæˆå…·ä½“ä»»åŠ¡çš„è®¡åˆ’ï¼Œéšåå°†å…¶è½¬åŒ–ä¸ºç¼–ç éšæ€§äº§ç”Ÿå¼è§„åˆ™çš„å¯ç»„åˆç¨‹åºåŒ–è¡¨ç¤º (procedural representations)ã€‚é€šè¿‡è¿™ç§ç¥ç»ç¬¦å·ç¨‹åºåŒ–è¿‡ç¨‹ï¼Œå¤æ‚çš„å¤šæ­¥ç¬¦å·æ¨ç†è¢«æŠ½è±¡å¹¶æ³›åŒ–ä¸ºå•æ­¥çš„è¯­è¨€æ¨¡å‹æ¨ç†ï¼Œå®ç°äº†ç±»ä¼¼äºäººç±»çš„çŸ¥è¯†ç¼–è¯‘ (knowledge compilation)ã€‚è¯¥æ–¹æ³•ä½¿æ™ºèƒ½ä½“åœ¨æµ‹è¯•é˜¶æ®µæ— éœ€ä¾èµ–å¤–éƒ¨ç¬¦å·å¼•å¯¼å³å¯è¿›è¡Œé«˜æ•ˆæ¨ç†ï¼Œç‰¹åˆ«é€‚ç”¨äºå»¶è¿Ÿæ•æ„Ÿä¸”èµ„æºå—é™çš„ç‰©ç†ç³»ç»Ÿã€‚å®éªŒåœ¨ PDDLGymã€VirtualHome å’Œ ALFWorld ç­‰åŸºå‡†æµ‹è¯•ä¸Šè¯æ˜ï¼ŒNeSyPr èƒ½è®©æ›´ç´§å‡‘çš„è¯­è¨€æ¨¡å‹ (compact LMs) å±•ç°å‡ºä¼˜äºå¤§è§„æ¨¡æ¨¡å‹å’Œç¬¦å·è§„åˆ’å™¨çš„æ¨ç†æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19429v1",
      "published_date": "2025-10-22 09:57:02 UTC",
      "updated_date": "2025-10-22 09:57:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:23.663741+00:00"
    },
    {
      "arxiv_id": "2510.19425v1",
      "title": "Neural Variational Dropout Processes",
      "title_zh": "ç¥ç»å˜åˆ†éšæœºå¤±æ´»è¿‡ç¨‹",
      "authors": [
        "Insu Jeon",
        "Youngjin Park",
        "Gunhee Kim"
      ],
      "abstract": "Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient mapping of dropout rates from a few observed contexts. It allows for a quick reconfiguration of a globally learned and shared neural network for new tasks in multi-task few-shot learning. In addition, NVDPs utilize a novel prior conditioned on the whole task data to optimize the conditional \\textit{dropout} posterior in the amortized variational inference. Surprisingly, this enables the robust approximation of task-specific dropout rates that can deal with a wide range of functional ambiguities and uncertainties. We compared the proposed method with other meta-learning approaches in the few-shot learning tasks such as 1D stochastic regression, image inpainting, and classification. The results show the excellent performance of NVDPs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºNeural Variational Dropout Processes (NVDPs)çš„æ–°å‹è´å¶æ–¯å…ƒå­¦ä¹ (Bayesian meta-learning)æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å…ƒå­¦ä¹ ä¸­æ¡ä»¶åéªŒæ¨¡å‹æ¨æ–­çš„é²æ£’æ€§ã€‚NVDPsé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„Dropoutå¯¹æ¡ä»¶åéªŒåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨Bernoulli expertsçš„ä½ç§©ä¹˜ç§¯å…ƒæ¨¡å‹å®ç°äº†ä»å°‘é‡è§‚æµ‹ä¸Šä¸‹æ–‡åˆ°Dropoutç‡çš„å†…å­˜é«˜æ•ˆæ˜ å°„ã€‚è¿™ç§æœºåˆ¶å…è®¸åœ¨å…¨çƒå…±äº«çš„ç¥ç»ç½‘ç»œä¸­é’ˆå¯¹å¤šä»»åŠ¡å°‘æ ·æœ¬å­¦ä¹ (multi-task few-shot learning)çš„æ–°ä»»åŠ¡è¿›è¡Œå¿«é€Ÿé‡æ–°é…ç½®ã€‚æ­¤å¤–ï¼ŒNVDPså¼•å…¥äº†ä¸€ç§åŸºäºå®Œæ•´ä»»åŠ¡æ•°æ®çš„åˆ›æ–°å…ˆéªŒï¼Œç”¨äºä¼˜åŒ–æ‘Šé”€å˜åˆ†æ¨ç†(amortized variational inference)ä¸­çš„æ¡ä»¶DropoutåéªŒã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç¨³å¥åœ°è¿‘ä¼¼ä»»åŠ¡ç‰¹å®šçš„Dropoutç‡ï¼Œä»è€Œæœ‰æ•ˆå¤„ç†å„ç§å‡½æ•°æ­§ä¹‰æ€§å’Œä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNVDPsåœ¨1Déšæœºå›å½’ã€å›¾åƒè¡¥å…¨(image inpainting)å’Œåˆ†ç±»ç­‰å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å…ƒå­¦ä¹ æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a Poster at International Conference on Learning Representations (ICLR) 2022 (Apr 25-29, 2022)",
      "pdf_url": "https://arxiv.org/pdf/2510.19425v1",
      "published_date": "2025-10-22 09:45:44 UTC",
      "updated_date": "2025-10-22 09:45:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:40.867226+00:00"
    },
    {
      "arxiv_id": "2510.19423v2",
      "title": "ETOM: A Five-Level Benchmark for Evaluating Tool Orchestration within the MCP Ecosystem",
      "title_zh": "ETOMï¼šé¢å‘ MCP ç”Ÿæ€ç³»ç»Ÿå†…å·¥å…·ç¼–æ’è¯„ä¼°çš„äº”çº§åŸºå‡†",
      "authors": [
        "Jia-Kai Dong",
        "I-Wei Huang",
        "Chun-Tin Wu",
        "Yi-Tien Tsai"
      ],
      "abstract": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Existing benchmarks often assess tools in isolation, overlooking challenges such as functional overlap and cross-server orchestration, which can lead to overly optimistic evaluations. ETOM addresses these gaps by constructing ground truth through \"equal function sets\", enabling objective metrics such as F1 score and reducing reliance on LLM-as-a-judge evaluation. Its five-level curriculum systematically tests agent capabilities, from single-tool orchestration to complex cross-server planning, as well as robustness to out-of-scope requests. Experiments reveal that rigid hierarchies can hinder performance without co-designed strategies, and even state-of-the-art agents exhibit systemic weaknesses in robustness. ETOM provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† ETOMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åˆ†å±‚ Model-Context Protocol (MCP) ç”Ÿæ€ç³»ç»Ÿä¸‹ LLM æ™ºèƒ½ä½“å¤šè·³ã€ç«¯åˆ°ç«¯å·¥å…·ç¼–æ’èƒ½åŠ›çš„äº”çº§åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å­¤ç«‹è¯„ä¼°å·¥å…·ä¸”å¿½è§†åŠŸèƒ½é‡å ä¸è·¨æœåŠ¡å™¨ç¼–æ’æŒ‘æˆ˜çš„é—®é¢˜ï¼ŒETOM é€šè¿‡æ„å»ºâ€œç­‰æ•ˆåŠŸèƒ½é›†â€(equal function sets) æ¥ç¡®ç«‹åœ°é¢çœŸå€¼ï¼Œä»è€Œå®ç° F1 åˆ†æ•°ç­‰å®¢è§‚æŒ‡æ ‡è¯„ä¼°ï¼Œå‡å°‘äº†å¯¹ LLM-as-a-judge è¯„ä¼°çš„ä¾èµ–ã€‚å…¶äº”çº§è¯¾ç¨‹ä½“ç³»ç³»ç»Ÿåœ°æµ‹è¯•äº†æ™ºèƒ½ä½“ä»å•å·¥å…·ç¼–æ’åˆ°å¤æ‚è·¨æœåŠ¡å™¨è§„åˆ’çš„èƒ½åŠ›ï¼Œä»¥åŠé¢å¯¹è¶…å‡ºèŒƒå›´(out-of-scope)è¯·æ±‚æ—¶çš„é²æ£’æ€§ã€‚å®éªŒç»“æœæ­ç¤ºï¼ŒåƒµåŒ–çš„å±‚çº§ç»“æ„åœ¨ç¼ºä¹ååŒè®¾è®¡ç­–ç•¥æ—¶ä¼šé˜»ç¢æ€§èƒ½ï¼Œä¸”å³ä¾¿æ˜¯æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“åœ¨é²æ£’æ€§æ–¹é¢ä¹Ÿå­˜åœ¨ç³»ç»Ÿæ€§ç¼ºé™·ã€‚ETOM æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è¯Šæ–­æ¡†æ¶ï¼Œç”¨ä»¥æš´éœ²è¿™äº›å±€é™æ€§å¹¶æŒ‡å¯¼å¼€å‘æ›´é«˜æ•ˆã€èƒ½åŠ›æ›´å¼ºçš„å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the Findings of EACL 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.19423v2",
      "published_date": "2025-10-22 09:45:11 UTC",
      "updated_date": "2026-01-18 10:40:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:41.552682+00:00"
    },
    {
      "arxiv_id": "2510.19421v1",
      "title": "FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA",
      "title_zh": "FairNetï¼šåŸºäºå¯¹æ¯”æ¡ä»¶ LoRA çš„æ— æ€§èƒ½æŸå¤±åŠ¨æ€å…¬å¹³æ€§ä¿®æ­£",
      "authors": [
        "Songqi Zhou",
        "Zeyuan Liu",
        "Benben Jiang"
      ],
      "abstract": "Ensuring fairness in machine learning models is a critical challenge. Existing debiasing methods often compromise performance, rely on static correction strategies, and struggle with data sparsity, particularly within minority groups. Furthermore, their utilization of sensitive attributes is often suboptimal, either depending excessively on complete attribute labeling or disregarding these attributes entirely. To overcome these limitations, we propose FairNet, a novel framework for dynamic, instance-level fairness correction. FairNet integrates a bias detector with conditional low-rank adaptation (LoRA), which enables selective activation of the fairness correction mechanism exclusively for instances identified as biased, and thereby preserve performance on unbiased instances. A key contribution is a new contrastive loss function for training the LoRA module, specifically designed to minimize intra-class representation disparities across different sensitive groups and effectively address underfitting in minority groups. The FairNet framework can flexibly handle scenarios with complete, partial, or entirely absent sensitive attribute labels. Theoretical analysis confirms that, under moderate TPR/FPR for the bias detector, FairNet can enhance the performance of the worst group without diminishing overall model performance, and potentially yield slight performance improvements. Comprehensive empirical evaluations across diverse vision and language benchmarks validate the effectiveness of FairNet.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FairNetï¼Œä¸€ä¸ªç”¨äºå®ç°åŠ¨æ€ã€å®ä¾‹çº§å…¬å¹³æ€§æ ¡æ­£çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å»åæ–¹æ³•åœ¨æå‡å…¬å¹³æ€§æ—¶å¾€å¾€ç‰ºç‰²æ¨¡å‹æ€§èƒ½çš„é—®é¢˜ã€‚FairNetå°†åè§æ£€æµ‹å™¨(Bias detector)ä¸æ¡ä»¶ä½ç§©è‡ªé€‚åº”(Conditional LoRA)ç›¸ç»“åˆï¼Œé€šè¿‡ä»…å¯¹è¢«è¯†åˆ«ä¸ºæœ‰åè§çš„å®ä¾‹æ¿€æ´»å…¬å¹³æ€§æ ¡æ­£æœºåˆ¶ï¼Œä»è€Œä¿ç•™äº†æ— åè§å®ä¾‹ä¸Šçš„åŸå§‹æ€§èƒ½ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº†ä¸€ç§å…¨æ–°çš„å¯¹æ¯”æŸå¤±å‡½æ•°(Contrastive loss function)æ¥è®­ç»ƒLoRAæ¨¡å—ï¼Œä¸“é—¨ç”¨äºæœ€å°åŒ–ä¸åŒæ•æ„Ÿç¾¤ä½“ä¹‹é—´çš„ç±»å†…è¡¨ç¤ºå·®å¼‚ï¼Œå¹¶æœ‰æ•ˆè§£å†³å°‘æ•°ç¾¤ä½“ä¸­çš„æ¬ æ‹Ÿåˆé—®é¢˜ã€‚è¯¥æ¡†æ¶å…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿå¤„ç†æ•æ„Ÿå±æ€§æ ‡ç­¾å®Œæ•´ã€éƒ¨åˆ†ç¼ºå¤±ç”šè‡³å®Œå…¨ç¼ºå¤±çš„å¤šç§å¤æ‚åœºæ™¯ã€‚ç†è®ºåˆ†æä¸å¤šæ ·åŒ–çš„è§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•å…±åŒè¯æ˜ï¼ŒFairNetèƒ½åœ¨ä¸é™ä½æ•´ä½“æ€§èƒ½çš„å‰æä¸‹æ˜¾è‘—æå‡è¡¨ç°æœ€å·®ç¾¤ä½“çš„æ€§èƒ½ï¼Œä¸ºå®ç°é«˜æ€§èƒ½ä¸”å…¬å¹³çš„æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19421v1",
      "published_date": "2025-10-22 09:44:03 UTC",
      "updated_date": "2025-10-22 09:44:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:43.246590+00:00"
    },
    {
      "arxiv_id": "2510.19420v1",
      "title": "Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation",
      "title_zh": "åŸºäºèŠ‚ç‚¹è¯„ä¼°çš„å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»ŸæŠ—ç ´åç›‘æ§",
      "authors": [
        "Chengcan Wu",
        "Zhixin Zhang",
        "Mingqian Xu",
        "Zeming Wei",
        "Meng Sun"
      ],
      "abstract": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a popular paradigm of AI applications. However, trustworthiness issues in MAS remain a critical concern. Unlike challenges in single-agent systems, MAS involve more complex communication processes, making them susceptible to corruption attacks. To mitigate this issue, several defense mechanisms have been developed based on the graph representation of MAS, where agents represent nodes and communications form edges. Nevertheless, these methods predominantly focus on static graph defense, attempting to either detect attacks in a fixed graph structure or optimize a static topology with certain defensive capabilities. To address this limitation, we propose a dynamic defense paradigm for MAS graph structures, which continuously monitors communication within the MAS graph, then dynamically adjusts the graph topology, accurately disrupts malicious communications, and effectively defends against evolving and diverse dynamic attacks. Experimental results in increasingly complex and dynamic MAS environments demonstrate that our method significantly outperforms existing MAS defense mechanisms, contributing an effective guardrail for their trustworthy applications. Our code is available at https://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)åœ¨å¤æ‚é€šä¿¡è¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°è…è´¥æ”»å‡»(corruption attacks)ä¸”ä¿¡ä»»åº¦å—é™çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ç°æœ‰çš„é˜²å¾¡æœºåˆ¶å¤§å¤šä¾§é‡äºé™æ€å›¾é˜²å¾¡(static graph defense)ï¼Œéš¾ä»¥åº”å¯¹æ¼”è¿›ä¸”å¤šæ ·åŒ–çš„åŠ¨æ€æ”»å‡»ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§é’ˆå¯¹MASå›¾ç»“æ„çš„åŠ¨æ€é˜²å¾¡èŒƒå¼ï¼Œé€šè¿‡èŠ‚ç‚¹è¯„ä¼°(Node Evaluation)æŒç»­ç›‘æµ‹ç³»ç»Ÿå†…çš„é€šä¿¡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ç›‘æµ‹ç»“æœåŠ¨æ€è°ƒæ•´å›¾æ‹“æ‰‘ç»“æ„(graph topology)ï¼Œä»è€Œç²¾ç¡®é˜»æ–­æ¶æ„é€šä¿¡ã€‚åœ¨æ—¥ç›Šå¤æ‚å’ŒåŠ¨æ€çš„MASç¯å¢ƒä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„é˜²å¾¡æœºåˆ¶ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¯ä¿¡çš„å¤šæ™ºèƒ½ä½“åº”ç”¨æä¾›äº†æœ‰æ•ˆçš„æŠ¤æ (guardrail)ï¼Œç›¸å…³ä»£ç ä¹Ÿå·²å¼€æºã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "math.OC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19420v1",
      "published_date": "2025-10-22 09:43:32 UTC",
      "updated_date": "2025-10-22 09:43:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:56.259017+00:00"
    },
    {
      "arxiv_id": "2510.19875v1",
      "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention",
      "title_zh": "Streamï¼šé€šè¿‡ç¨€ç–æ³¨æ„åŠ›å®ç°å¤§è¯­è¨€æ¨¡å‹é•¿ä¸Šä¸‹æ–‡æœºç†å¯è§£é‡Šæ€§çš„è§„æ¨¡åŒ–æ‰©å±•",
      "authors": [
        "J Rosser",
        "JosÃ© Luis Redondo GarcÃ­a",
        "Gustavo Penha",
        "Konstantina Palla",
        "Hugues Bouchard"
      ],
      "abstract": "As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \\log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model's next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99\\% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96\\% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at https://anonymous.4open.science/r/stream-03B8/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ‰©å±•è‡³ç™¾ä¸‡çº§ä¸Šä¸‹æ–‡æ—¶ï¼Œä¼ ç»Ÿæœºæ¢°è§£é‡Šæ€§(Mechanistic Interpretability)æŠ€æœ¯é¢ä¸´çš„å†…å­˜æ¶ˆè€—å’Œè®¡ç®—æ‰©å±•éš¾é¢˜ï¼Œæå‡ºäº†åŸºäºç¨€ç–è¿½è¸ª(Sparse Tracing)çš„Streamç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›(sparse attention)å’Œå±‚æ¬¡åŒ–å‰ªææŠ€æœ¯ï¼Œå®ç°äº†è¿‘çº¿æ€§æ—¶é—´å¤æ‚åº¦$O(T \\log T)$å’Œçº¿æ€§ç©ºé—´å¤æ‚åº¦$O(T)$çš„åˆ†ææ•ˆç‡ã€‚Streamé€šè¿‡äºŒåˆ†æœç´¢å¼çš„ç²¾ç»†åŒ–å¤„ç†ï¼Œä»…ä¿ç•™æ¯é¡¹æŸ¥è¯¢æœ€å…³é”®çš„é”®å—ï¼Œä»è€Œåœ¨å‡ ä¹ä¸å½±å“æ¨¡å‹åŸå§‹é¢„æµ‹è¡Œä¸ºçš„å‰æä¸‹å¤§å¹…å‰Šå‡äº†å†…å­˜éœ€æ±‚ã€‚åœ¨é’ˆå¯¹é•¿é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†è½¨è¿¹å’ŒRULERåŸºå‡†æµ‹è¯•çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å‰ªææ‰90-99%çš„æ ‡è®°äº¤äº’åï¼Œä»èƒ½ç²¾å‡†è¯†åˆ«å‡ºä¿¡æ¯æµä¸­çš„æ€ç»´é”šç‚¹(thought anchors)å’Œæ ¸å¿ƒæ£€ç´¢è·¯å¾„ã€‚Streamä¸ºé•¿ä¸Šä¸‹æ–‡æ³¨æ„åŠ›æ¨¡å¼åˆ†ææä¾›äº†å®ç”¨çš„å³æ’å³ç”¨å·¥å…·ï¼Œä½¿å¾—åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿›è¡Œæ·±å…¥çš„è§£é‡Šæ€§ç ”ç©¶å’Œæ¨ç†è¿‡ç¨‹ç›‘æ§æˆä¸ºå¯èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19875v1",
      "published_date": "2025-10-22 09:42:29 UTC",
      "updated_date": "2025-10-22 09:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:00.160977+00:00"
    },
    {
      "arxiv_id": "2510.19414v1",
      "title": "EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection",
      "title_zh": "EchoFakeï¼šé¢å‘å®ç”¨è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„é‡æ”¾æ„ŸçŸ¥æ•°æ®é›†",
      "authors": [
        "Tong Zhang",
        "Yihuan Huang",
        "Yanzhen Ren"
      ],
      "abstract": "The growing prevalence of speech deepfakes has raised serious concerns, particularly in real-world scenarios such as telephone fraud and identity theft. While many anti-spoofing systems have demonstrated promising performance on lab-generated synthetic speech, they often fail when confronted with physical replay attacks-a common and low-cost form of attack used in practical settings. Our experiments show that models trained on existing datasets exhibit severe performance degradation, with average accuracy dropping to 59.6% when evaluated on replayed audio. To bridge this gap, we present EchoFake, a comprehensive dataset comprising more than 120 hours of audio from over 13,000 speakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and physical replay recordings collected under varied devices and real-world environmental settings. Additionally, we evaluate three baseline detection models and show that models trained on EchoFake achieve lower average EERs across datasets, indicating better generalization. By introducing more practical challenges relevant to real-world deployment, EchoFake offers a more realistic foundation for advancing spoofing detection methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³æ·±åº¦ä¼ªé€ ï¼ˆDeepfakeï¼‰åœ¨ç”µè¯è¯ˆéª—å’Œèº«ä»½çªƒå–ç­‰ç°å®åœºæ™¯ä¸­å¼•å‘çš„å®‰å…¨é£é™©ï¼ŒæŒ‡å‡ºç°æœ‰é˜²å¾¡ç³»ç»Ÿåœ¨åº”å¯¹ä½æˆæœ¬ä¸”å¸¸è§çš„ç‰©ç†é‡æ”¾ï¼ˆPhysical Replayï¼‰æ”»å‡»æ—¶å­˜åœ¨ä¸¥é‡çš„æ€§èƒ½è¡°å‡ï¼Œå®éªŒè¡¨æ˜æ¨¡å‹åœ¨é‡æ”¾éŸ³é¢‘ä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º59.6%ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æ¨å‡ºäº†EchoFakeï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡120å°æ—¶éŸ³é¢‘ã€æ¶µç›–13,000å¤šåè¯´è¯äººçš„ç»¼åˆæ•°æ®é›†ï¼Œå…¶æ ¸å¿ƒç‰¹å¾æ˜¯ç»“åˆäº†å‰æ²¿çš„é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆZero-Shot TTSï¼‰æŠ€æœ¯ä»¥åŠåœ¨å¤šæ ·åŒ–è®¾å¤‡å’ŒçœŸå®ç¯å¢ƒè®¾ç½®ä¸‹é‡‡é›†çš„ç‰©ç†é‡æ”¾å½•éŸ³ã€‚é€šè¿‡å¯¹ä¸‰ç§åŸºçº¿æ£€æµ‹æ¨¡å‹çš„è¯„ä¼°ï¼Œå®éªŒè¯æ˜åœ¨EchoFakeä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸­å‡å®ç°äº†æ›´ä½çš„ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰ï¼Œå±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥ä¸ç°å®éƒ¨ç½²ç´§å¯†ç›¸å…³çš„æŒ‘æˆ˜ï¼Œä¸ºæå‡æ¬ºéª—æ£€æµ‹ï¼ˆSpoofing Detectionï¼‰æ–¹æ³•çš„å®ç”¨æ€§æä¾›äº†æ›´çœŸå®çš„ç§‘ç ”åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19414v1",
      "published_date": "2025-10-22 09:34:31 UTC",
      "updated_date": "2025-10-22 09:34:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:52.560658+00:00"
    },
    {
      "arxiv_id": "2510.19410v1",
      "title": "ToMMeR -- Efficient Entity Mention Detection from Large Language Models",
      "title_zh": "ToMMeRï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆå®ä½“æåŠæ£€æµ‹",
      "authors": [
        "Victor Morand",
        "Nadi Tomeh",
        "Josiane Mothe",
        "Benjamin Piwowarski"
      ],
      "abstract": "Identifying which text spans refer to entities -- mention detection -- is both foundational for information extraction and a known performance bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing mention detection capabilities from early LLM layers. Across 13 NER benchmarks, ToMMeR achieves 93\\% recall zero-shot, with over 90\\% precision using an LLM as a judge showing that ToMMeR rarely produces spurious predictions despite high recall. Cross-model analysis reveals that diverse architectures (14M-15B parameters) converge on similar mention boundaries (DICE >75\\%), confirming that mention detection emerges naturally from language modeling. When extended with span classification heads, ToMMeR achieves near SOTA NER performance (80-87\\% F1 on standard benchmarks). Our work provides evidence that structured entity representations exist in early transformer layers and can be efficiently recovered with minimal parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ToMMeRï¼Œä¸€ç§å‚æ•°é‡å°äº 300K çš„è½»é‡çº§æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ¢æµ‹å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ—©æœŸå±‚æ¥é«˜æ•ˆæå–å®ä½“æåŠ (mention detection) èƒ½åŠ›ã€‚åœ¨ 13 ä¸ª NER åŸºå‡†æµ‹è¯•ä¸­ï¼ŒToMMeR åœ¨é›¶æ ·æœ¬ (zero-shot) åœºæ™¯ä¸‹å®ç°äº† 93% çš„å¬å›ç‡ï¼Œä¸”åœ¨ LLM ä½œä¸ºè£åˆ¤çš„è¯„ä¼°ä¸‹è¡¨ç°å‡ºè¶…è¿‡ 90% çš„ç²¾ç¡®ç‡ï¼Œè¯æ˜å…¶åœ¨ä¿æŒé«˜å¬å›çš„åŒæ—¶æå°‘äº§ç”Ÿè™šå‡é¢„æµ‹ã€‚è·¨æ¨¡å‹åˆ†ææ˜¾ç¤ºï¼Œä¸åŒè§„æ¨¡åŠæ¶æ„çš„æ¨¡å‹åœ¨æåŠè¾¹ç•Œä¸Šè¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ (DICE >75%)ï¼Œè¯å®äº†å®ä½“è¯†åˆ«æ˜¯è¯­è¨€å»ºæ¨¡è¿‡ç¨‹ä¸­è‡ªç„¶æ¶Œç°çš„ç‰¹å¾ã€‚å½“æ‰©å±•è·¨åº¦åˆ†ç±»å¤´ (span classification heads) åï¼ŒToMMeR åœ¨æ ‡å‡†åŸºå‡†ä¸Šè¾¾åˆ°äº†æ¥è¿‘ SOTA çš„ NER æ€§èƒ½ï¼ˆF1 åˆ†æ•°è¾¾ 80-87%ï¼‰ã€‚è¯¥å·¥ä½œæœ‰åŠ›åœ°è¯æ˜äº†ç»“æ„åŒ–çš„å®ä½“è¡¨ç¤ºåœ¨ Transformer çš„æ—©æœŸå±‚ä¸­å°±å·²ç»å­˜åœ¨ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æå°‘é‡çš„å‚æ•°è¢«é«˜æ•ˆæ¢å¤ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code is available at https://github.com/VictorMorand/llm2ner",
      "pdf_url": "https://arxiv.org/pdf/2510.19410v1",
      "published_date": "2025-10-22 09:28:18 UTC",
      "updated_date": "2025-10-22 09:28:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:01:58.553708+00:00"
    },
    {
      "arxiv_id": "2510.19386v2",
      "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
      "title_zh": "ColorAgentï¼šæ„å»ºç¨³å¥ã€ä¸ªæ€§åŒ–ä¸”äº¤äº’å¼çš„æ“ä½œç³»ç»Ÿæ™ºèƒ½ä½“",
      "authors": [
        "Ning Li",
        "Qiqiang Lin",
        "Zheng Wu",
        "Xiaoyun Mo",
        "Weiming Zhang",
        "Yin Zhao",
        "Xiangmou Qu",
        "Jiamu Zhou",
        "Jun Wang",
        "Congmin Zheng",
        "Yuanyi Song",
        "Hongjiang Chen",
        "Heyuan Huang",
        "Jihong Wang",
        "Jiaxin Yin",
        "Jingwei Yu",
        "Junwei Liao",
        "Qiuying Peng",
        "Xingyu Lou",
        "Jun Wang",
        "Weiwen Liu",
        "Zhuosheng Zhang",
        "Weinan Zhang"
      ],
      "abstract": "With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ColorAgentï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°é•¿æœŸé™ã€ç¨³å¥ä¸”å…·æœ‰ä¸ªæ€§åŒ–ä¸»åŠ¨äº¤äº’èƒ½åŠ›çš„æ“ä½œç³»ç»Ÿæ™ºèƒ½ä½“ (OS Agent)ã€‚ä¸ºäº†å¢å¼ºä¸ç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡åˆ†æ­¥å¼ºåŒ–å­¦ä¹  (Step-wise Reinforcement Learning) å’Œè‡ªæˆ‘è¿›åŒ–è®­ç»ƒ (Self-evolving Training) æå‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ„å»ºäº†ä¸“é—¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ (Multi-agent Framework) ä»¥ä¿è¯ç³»ç»Ÿçš„é€šç”¨æ€§ä¸é²æ£’æ€§ã€‚åœ¨äº¤äº’è®¾è®¡ä¸Šï¼ŒColorAgent å¼•å…¥äº†ä¸ªæ€§åŒ–æ„å›¾è¯†åˆ« (Personalized User Intent Recognition) ä¸ä¸»åŠ¨å‚ä¸æœºåˆ¶ï¼Œæ—¨åœ¨å°†å…¶æ‰“é€ ä¸ºæ›´å…·æ¸©åº¦çš„åä½œä¼™ä¼´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒColorAgent åœ¨ AndroidWorld å’Œ AndroidLab åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«è¾¾åˆ°äº† 77.2% å’Œ 50.7% çš„æˆåŠŸç‡ï¼Œæ ‘ç«‹äº†æ–°çš„ SOTA æ ‡å‡†ã€‚æœ€åï¼Œè®ºæ–‡æŒ‡å‡ºå½“å‰åŸºå‡†æµ‹è¯•åœ¨å…¨é¢è¯„ä¼° OS Agent æ–¹é¢ä»æœ‰å±€é™ï¼Œå¹¶å¯¹æœªæ¥åœ¨è¯„ä¼°èŒƒå¼ã€æ™ºèƒ½ä½“åä½œåŠå®‰å…¨æ€§ç­‰é¢†åŸŸçš„ç ”ç©¶æ–¹å‘è¿›è¡Œäº†å±•æœ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19386v2",
      "published_date": "2025-10-22 09:02:48 UTC",
      "updated_date": "2025-10-24 07:32:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:01.163795+00:00"
    },
    {
      "arxiv_id": "2510.19365v1",
      "title": "The Massive Legal Embedding Benchmark (MLEB)",
      "title_zh": "å¤§è§„æ¨¡æ³•å¾‹åµŒå…¥åŸºå‡† (MLEB)",
      "authors": [
        "Umar Butler",
        "Abdur-Rahman Butler",
        "Adrian Lucas Malec"
      ],
      "abstract": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source benchmark for legal information retrieval to date. MLEB consists of ten expert-annotated datasets spanning multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore), document types (cases, legislation, regulatory guidance, contracts, and literature), and task types (search, zero-shot classification, and question answering). Seven of the datasets in MLEB were newly constructed in order to fill domain and jurisdictional gaps in the open-source legal information retrieval landscape. We document our methodology in building MLEB and creating the new constituent datasets, and release our code, results, and data openly to assist with reproducible evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Massive Legal Embedding Benchmark (MLEB)ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§ã€å¤šæ ·æ€§æœ€å¼ºä¸”æœ€å…¨é¢çš„æ³•å¾‹ä¿¡æ¯æ£€ç´¢ (legal information retrieval) å¼€æºåŸºå‡†æµ‹è¯•ã€‚MLEB åŒ…å« 10 ä¸ªç”±ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†ï¼Œæ¨ªè·¨ç¾å›½ã€è‹±å›½ã€æ¬§ç›Ÿã€æ¾³å¤§åˆ©äºšã€çˆ±å°”å…°å’Œæ–°åŠ å¡ç­‰å¤šä¸ªå¸æ³•ç®¡è¾–åŒºã€‚è¯¥åŸºå‡†æ¶µç›–äº†æ¡ˆä¾‹ã€ç«‹æ³•ã€ç›‘ç®¡æŒ‡å—ã€åˆåŒå’Œæ–‡çŒ®ç­‰å¤šç§æ–‡æ¡£ç±»å‹ï¼Œå¹¶æ”¯æŒæœç´¢ (search)ã€é›¶æ ·æœ¬åˆ†ç±» (zero-shot classification) å’Œé—®ç­” (question answering) ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚ä¸ºäº†å¡«è¡¥å¼€æºæ³•å¾‹ä¿¡æ¯æ£€ç´¢é¢†åŸŸåœ¨ç‰¹å®šä¸“ä¸šå’Œå¸æ³•ç®¡è¾–åŒºä¸Šçš„ç©ºç™½ï¼Œç ”ç©¶å›¢é˜Ÿä¸“é—¨æ–°æ„å»ºäº†å…¶ä¸­çš„ 7 ä¸ªæ•°æ®é›†ã€‚é€šè¿‡è¯¦ç»†è®°å½•æ„å»ºæ–¹æ³•å¹¶å…¬å¼€ä»£ç ã€ç»“æœå’Œæ•°æ®ï¼Œè¯¥ç ”ç©¶ä¸ºæ³•å¾‹é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹çš„å¯é‡å¤æ€§è¯„ä¼°å’Œæ€§èƒ½ä¼˜åŒ–æä¾›äº†é‡è¦çš„åŸºç¡€è®¾æ–½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19365v1",
      "published_date": "2025-10-22 08:38:44 UTC",
      "updated_date": "2025-10-22 08:38:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:03.562884+00:00"
    },
    {
      "arxiv_id": "2510.19361v3",
      "title": "AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation",
      "title_zh": "AgenticMathï¼šé€šè¿‡åŸºäºæ™ºèƒ½ä½“çš„æ•°å­¦æ•°æ®ç”Ÿæˆæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xianyang Liu",
        "Yilin Liu",
        "Shuai Wang",
        "Hao Cheng",
        "Andrew Estornell",
        "Yuzhi Zhao",
        "Jun Shu",
        "Jiaheng Wei"
      ],
      "abstract": "The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic method for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgenticMathï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡ç”Ÿæˆé«˜è´¨é‡æ•°å­¦é—®ç­”å¯¹æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½ä½“åŒ–æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡ Seed Question Filterã€Agentic Question Rephraseã€Answer Augment ä»¥åŠ Question and Answer Evaluation å››ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œå’Œ Chain-of-thought æ¨ç†ç¡®ä¿ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ä¸é€»è¾‘ä¸¥å¯†æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…ä½¿ç”¨ 3ä¸‡è‡³6ä¸‡ä¸ª AgenticMath ç”Ÿæˆçš„æ ·æœ¬å¯¹ 3B-8B å‚æ•°æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå…¶åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¾¿èƒ½åª²ç¾æˆ–è¶…è¶Šä½¿ç”¨ 40ä¸‡ç”šè‡³ 230ä¸‡æ•°æ®çš„åŸºçº¿æ¨¡å‹ã€‚è¯¥å·¥ä½œè¯æ˜äº†ç›¸æ¯”äºå¤§è§„æ¨¡ä½è´¨é‡æ•°æ®ï¼Œæœ‰é’ˆå¯¹æ€§çš„é«˜è´¨é‡æ•°æ®ç”Ÿæˆæ˜¯æå‡æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›æ›´é«˜æ•ˆä¸”æ— éœ€äººå·¥æ ‡æ³¨çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.19361v3",
      "published_date": "2025-10-22 08:34:13 UTC",
      "updated_date": "2026-01-08 03:57:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:17.276261+00:00"
    },
    {
      "arxiv_id": "2510.19873v1",
      "title": "From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph",
      "title_zh": "ä»å¤§åˆ°å°ï¼šåŸºäºæ¨ç†å›¾çš„ CUDA ä¼˜åŒ–ä¸“å®¶çŸ¥è¯†è¿ç§»",
      "authors": [
        "Junfeng Gong",
        "Zhiyi Wei",
        "Junying Chen",
        "Cheng Liu",
        "Huawei Li"
      ],
      "abstract": "Despite significant evolution of CUDA programming and domain-specific libraries, effectively utilizing GPUs with massively parallel engines remains difficult. Large language models (LLMs) show strong potential in generating optimized CUDA code from sequential code. However, using LLMs in practice faces two major challenges: cloud-based APIs pose risks of code leakage, and local deployment is often computationally expensive and inefficient. These drawbacks have spurred interest in small language models (SLMs), which are more lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs can achieve performance comparable to LLMs on specific tasks. While SLMs can match LLMs on domain-specific tasks, their limited reasoning abilities lead to suboptimal performance in complex CUDA generation according to our experiments. To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented generation framework that transfers LLM-level reasoning to smaller models. ReGraphT organizes CUDA optimization trajectories into a structured reasoning graph, modeling the combined CUDA optimizations as state transitions, and leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also present a CUDA-specific benchmark with difficulty tiers defined by reasoning complexity to evaluate models more comprehensively. Experiments show that ReGraphT outperforms HPC-specific fine-tuned models and other retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level performance without the associated privacy risks or excessive computing overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ReGraphTï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹(SLMs)åœ¨ç”Ÿæˆå¤æ‚CUDAä»£ç æ—¶æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†CUDAä¼˜åŒ–è½¨è¿¹ç»„ç»‡ä¸ºç»“æ„åŒ–çš„æ¨ç†å›¾ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›å›¾æœç´¢(Monte Carlo Graph Search, MCGS)æ¨¡æ‹Ÿä¼˜åŒ–çŠ¶æ€è½¬ç§»ï¼Œä»è€Œå°†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çº§åˆ«çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆè¿ç§»è‡³è½»é‡åŒ–çš„SLMsã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†ä¸€ä¸ªæ ¹æ®æ¨ç†å¤æ‚åº¦åˆ†çº§çš„CUDAä¸“ç”¨åŸºå‡†æµ‹è¯•ï¼Œä»¥å®ç°å¯¹æ¨¡å‹æ€§èƒ½æ›´å…¨é¢çš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReGraphTåœ¨CUDAEvalå’ŒParEvalä¸Šå®ç°äº†å¹³å‡2.33å€çš„æ€§èƒ½åŠ é€Ÿï¼Œæ˜¾è‘—ä¼˜äºé«˜æ€§èƒ½è®¡ç®—(HPC)ä¸“ç”¨å¾®è°ƒæ¨¡å‹åŠå…¶ä»–æ£€ç´¢å¢å¼ºæ–¹æ³•ã€‚é€šè¿‡ä¸DeepSeek-Coder-V2-Lite-Instructå’ŒQwen2.5-Coder-7B-Instructç»“åˆï¼ŒReGraphTä½¿å¾—SLMsèƒ½å¤Ÿåœ¨ä¸äº§ç”Ÿéšç§é£é™©æˆ–è¿‡åº¦è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå±•ç°å‡ºæ¥è¿‘LLMsæ°´å¹³çš„CUDAä¼˜åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19873v1",
      "published_date": "2025-10-22 08:33:44 UTC",
      "updated_date": "2025-10-22 08:33:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:51.894329+00:00"
    },
    {
      "arxiv_id": "2510.19358v1",
      "title": "M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models",
      "title_zh": "M3-SLUï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¯´è¯äººå½’å±æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yejin Kwon",
        "Taewoo Kang",
        "Hyunsoo Yoon",
        "Changouk Kim"
      ],
      "abstract": "We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations. M3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† M3-SLUï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šè¯´è¯äººã€å¤šè½®æ¬¡å£è¯­ç†è§£çš„æ–°å‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡ç›®å‰çš„æ¨¡å‹åœ¨è¯­éŸ³å’Œæ–‡æœ¬ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†è‡ªç„¶å¯¹è¯ä¸­çš„è¯´è¯äººå½’å› æ¨ç† (Speaker-attributed reasoning)ï¼Œå³ç†è§£â€œè°åœ¨ä»€ä¹ˆæ—¶å€™è¯´äº†ä»€ä¹ˆâ€æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚M3-SLU åŸºäº CHiME-6ã€MELDã€MultiDialog å’Œ AMI å››ä¸ªå¼€æºè¯­æ–™åº“æ„å»ºï¼ŒåŒ…å«è¶…è¿‡ 12,000 ä¸ªç»è¿‡éªŒè¯çš„å®ä¾‹ï¼Œæ¶µç›–éŸ³é¢‘ã€è½¬å½•æ–‡æœ¬åŠå…¶å…ƒæ•°æ®ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«è¯´è¯äººå½’å› é—®ç­” (Speaker-Attributed Question Answering) å’Œé€šè¿‡è¯è¯­åŒ¹é…è¿›è¡Œè¯´è¯äººå½’å›  (Speaker Attribution via Utterance Matching) ä¸¤é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚é€šè¿‡å¯¹çº§è”æµæ°´çº¿ (Cascaded pipelines) å’Œç«¯åˆ°ç«¯ (End-to-end) MLLM çš„è¯„ä¼°å‘ç°ï¼Œè™½ç„¶æ¨¡å‹èƒ½å¤Ÿæ•æ‰è¯´è¯å†…å®¹ï¼Œä½†å¾€å¾€éš¾ä»¥å‡†ç¡®è¯†åˆ«è¯´è¯è€…èº«ä»½ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ¨¡å‹åœ¨å…·å¤‡è¯´è¯äººæ„ŸçŸ¥èƒ½åŠ›çš„å¯¹è¯ç†è§£ä¸­å­˜åœ¨çš„å…³é”®ç¼ºé™·ï¼Œä¸ºæ¨åŠ¨è¯´è¯äººæ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ç ”ç©¶æä¾›äº†é‡è¦çš„æŒ‘æˆ˜å¹³å°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to LREC 2026. 11 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19358v1",
      "published_date": "2025-10-22 08:28:43 UTC",
      "updated_date": "2025-10-22 08:28:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:31.787650+00:00"
    },
    {
      "arxiv_id": "2510.19351v2",
      "title": "Learning To Defer To A Population With Limited Demonstrations",
      "title_zh": "åŸºäºæœ‰é™ç¤ºèŒƒçš„é¢å‘ç¾¤ä½“å­¦ä¹ æ¨è¿Ÿ",
      "authors": [
        "Nilesh Ramgolam",
        "Gustavo Carneiro",
        "Hsiang-Ting Chen"
      ],
      "abstract": "This paper addresses the critical data scarcity that hinders the practical deployment of learning to defer (L2D) systems to the population. We introduce a context-aware, semi-supervised framework that uses meta-learning to generate expert-specific embeddings from only a few demonstrations. We demonstrate the efficacy of a dual-purpose mechanism, where these embeddings are used first to generate a large corpus of pseudo-labels for training, and subsequently to enable on-the-fly adaptation to new experts at test-time. The experiment results on three different datasets confirm that a model trained on these synthetic labels rapidly approaches oracle-level performance, validating the data efficiency of our approach. By resolving a key training bottleneck, this work makes adaptive L2D systems more practical and scalable, paving the way for human-AI collaboration in real-world environments. To facilitate reproducibility and address implementation details not covered in the main text, we provide our source code and training configurations at https://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Learning to Defer (L2D) ç³»ç»Ÿåœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæƒ…å¢ƒæ„ŸçŸ¥ï¼ˆcontext-awareï¼‰çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…ƒå­¦ä¹  (meta-learning) æŠ€æœ¯ï¼Œä»…éœ€å°‘é‡æ¼”ç¤ºå³å¯ç”Ÿæˆç‰¹å®šäºä¸“å®¶çš„åµŒå…¥å‘é‡ (expert-specific embeddings)ã€‚è¿™ç§åµŒå…¥å‘é‡å…·æœ‰åŒé‡åŠŸèƒ½ï¼šé¦–å…ˆç”¨äºç”Ÿæˆå¤§è§„æ¨¡ä¼ªæ ‡ç­¾ (pseudo-labels) è¯­æ–™åº“ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå…¶æ¬¡åœ¨æµ‹è¯•é˜¶æ®µå®ç°å¯¹æ–°ä¸“å®¶çš„å³æ—¶è‡ªé€‚åº” (on-the-fly adaptation)ã€‚åœ¨ä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›åˆæˆæ ‡ç­¾è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½èƒ½å¤Ÿè¿…é€Ÿæ¥è¿‘ Oracle æ°´å¹³ï¼Œå……åˆ†éªŒè¯äº†è¯¥æ–¹æ³•çš„é«˜æ•°æ®æ•ˆç‡ã€‚é€šè¿‡æœ‰æ•ˆè§£å†³å…³é”®çš„è®­ç»ƒç“¶é¢ˆï¼Œè¯¥å·¥ä½œæ˜¾è‘—æå‡äº†è‡ªé€‚åº” L2D ç³»ç»Ÿçš„å®ç”¨æ€§ä¸å¯æ‰©å±•æ€§ï¼Œä¸ºç°å®ç¯å¢ƒä¸­çš„äººæœºåä½œ (human-AI collaboration) æä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to IEEE DICTA 2025 (poster). 7 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19351v2",
      "published_date": "2025-10-22 08:18:02 UTC",
      "updated_date": "2025-10-23 01:52:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:35.670170+00:00"
    },
    {
      "arxiv_id": "2510.19347v1",
      "title": "A New Type of Adversarial Examples",
      "title_zh": "ä¸€ç§æ–°å‹å¯¹æŠ—æ ·æœ¬",
      "authors": [
        "Xingyang Nie",
        "Guojie Xiao",
        "Su Pan",
        "Biao Wang",
        "Huilin Ge",
        "Tao Fang"
      ],
      "abstract": "Most machine learning models are vulnerable to adversarial examples, which poses security concerns on these models. Adversarial examples are crafted by applying subtle but intentionally worst-case modifications to examples from the dataset, leading the model to output a different answer from the original example. In this paper, adversarial examples are formed in an exactly opposite manner, which are significantly different from the original examples but result in the same answer. We propose a novel set of algorithms to produce such adversarial examples, including the negative iterative fast gradient sign method (NI-FGSM) and the negative iterative fast gradient method (NI-FGM), along with their momentum variants: the negative momentum iterative fast gradient sign method (NMI-FGSM) and the negative momentum iterative fast gradient method (NMI-FGM). Adversarial examples constructed by these methods could be used to perform an attack on machine learning systems in certain occasions. Moreover, our results show that the adversarial examples are not merely distributed in the neighbourhood of the examples from the dataset; instead, they are distributed extensively in the sample space.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ Adversarial Examplesï¼Œä¸ä¼ ç»Ÿé€šè¿‡å¾®å°æ‰°åŠ¨æ”¹å˜æ¨¡å‹é¢„æµ‹çš„æ–¹æ³•ä¸åŒï¼Œæ­¤ç±»æ ·æœ¬åœ¨ç‰¹å¾ä¸Šä¸åŸå§‹æ ·æœ¬æ˜¾è‘—ä¸åŒï¼Œå´èƒ½è¯±å¯¼æ¨¡å‹äº§ç”Ÿä¸åŸå§‹æ ·æœ¬ç›¸åŒçš„è¾“å‡ºç»“æœã€‚ä½œè€…ä¸ºæ­¤è®¾è®¡äº†ä¸€å¥—å…¨æ–°çš„ç®—æ³•ä½“ç³»ï¼ŒåŒ…æ‹¬ NI-FGSMã€NI-FGM åŠå…¶åŠ¨é‡å˜ä½“ NMI-FGSM å’Œ NMI-FGMã€‚è¿™äº›æ–¹æ³•ç”Ÿæˆçš„æ ·æœ¬å¯ç”¨äºåœ¨ç‰¹å®šåœºæ™¯ä¸‹å¯¹æœºå™¨å­¦ä¹ ç³»ç»Ÿå‘èµ·æ”»å‡»ï¼Œå…·æœ‰è¾ƒå¼ºçš„å®ç”¨ä»·å€¼ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œè¿™ç±» Adversarial Examples å¹¶éä»…åˆ†å¸ƒåœ¨åŸå§‹æ ·æœ¬çš„é‚»åŸŸå†…ï¼Œè€Œæ˜¯å¹¿æ³›å­˜åœ¨äºæ•´ä¸ªæ ·æœ¬ç©ºé—´ä¸­ã€‚è¯¥å‘ç°æ­ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ›´å¹¿é˜”ç©ºé—´å†…çš„è„†å¼±æ€§ï¼Œä¸ºç†è§£å’Œé˜²å¾¡å¯¹æŠ—æ€§æ”»å‡»æä¾›äº†æ–°çš„ç ”ç©¶è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19347v1",
      "published_date": "2025-10-22 08:14:11 UTC",
      "updated_date": "2025-10-22 08:14:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:36.987929+00:00"
    },
    {
      "arxiv_id": "2510.19345v1",
      "title": "Foundation Model Forecasts: Form and Function",
      "title_zh": "åŸºç¡€æ¨¡å‹é¢„æµ‹ï¼šå½¢å¼ä¸åŠŸèƒ½",
      "authors": [
        "Alvaro Perez-Diaz",
        "James C. Loach",
        "Danielle E. Toutoungi",
        "Lee Middleton"
      ],
      "abstract": "Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet accuracy alone does not determine practical value. The form of a forecast -- point, quantile, parametric, or trajectory ensemble -- fundamentally constrains which operational tasks it can support. We survey recent TSFMs and find that two-thirds produce only point or parametric forecasts, while many operational tasks require trajectory ensembles that preserve temporal dependence. We establish when forecast types can be converted and when they cannot: trajectory ensembles convert to simpler forms via marginalization without additional assumptions, but the reverse requires imposing temporal dependence through copulas or conformal methods. We prove that marginals cannot determine path-dependent event probabilities -- infinitely many joint distributions share identical marginals but yield different answers to operational questions. We map six fundamental forecasting tasks to minimal sufficient forecast types and provide a task-aligned evaluation framework. Our analysis clarifies when forecast type, not accuracy, differentiates practical utility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (Time-series foundation models, TSFMs) çš„é¢„æµ‹å½¢å¼ä¸åŠŸèƒ½ï¼ŒæŒ‡å‡ºå•çº¯çš„å‡†ç¡®ç‡ä¸è¶³ä»¥è¡¡é‡å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚ä½œè€…è°ƒç ”å‘ç°ï¼Œç°æœ‰ä¸‰åˆ†ä¹‹äºŒçš„ TSFMs ä»…ç”Ÿæˆ point æˆ– parametric é¢„æµ‹ï¼Œè€Œè®¸å¤šå®é™…æ“ä½œä»»åŠ¡åˆ™éœ€è¦ä¿ç•™æ—¶é—´ä¾èµ–æ€§çš„ trajectory ensembleã€‚ç ”ç©¶é˜æ˜äº†ä¸åŒé¢„æµ‹ç±»å‹é—´çš„è½¬æ¢é€»è¾‘ï¼ŒæŒ‡å‡º trajectory ensemble å¯é€šè¿‡ marginalization è½¬æ¢ä¸ºç®€å•å½¢å¼ï¼Œä½†åå‘è½¬æ¢å¿…é¡»ä¾èµ– copulas æˆ– conformal methods ç­‰æ‰‹æ®µã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼Œè¾¹ç¼˜åˆ†å¸ƒæ— æ³•æ¨æ–­ path-dependent äº‹ä»¶çš„æ¦‚ç‡ï¼Œå› ä¸ºæ— ç©·å¤šè”åˆåˆ†å¸ƒå¯èƒ½å…±äº«ç›¸åŒçš„è¾¹ç¼˜åˆ†å¸ƒã€‚æœ€åï¼Œç ”ç©¶å°†å…­é¡¹æ ¸å¿ƒé¢„æµ‹ä»»åŠ¡ä¸æœ€å°å……åˆ†é¢„æµ‹ç±»å‹è¿›è¡Œæ˜ å°„ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªä»»åŠ¡å¯¹é½çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥åˆ†ææ­ç¤ºäº†åœ¨æ¨¡å‹å‡†ç¡®ç‡ä¹‹å¤–ï¼Œé¢„æµ‹å½¢å¼å¦‚ä½•æˆä¸ºå†³å®šæ¨¡å‹å®é™…æ•ˆç”¨çš„æ ¸å¿ƒè¦ç´ ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19345v1",
      "published_date": "2025-10-22 08:10:34 UTC",
      "updated_date": "2025-10-22 08:10:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:50.793738+00:00"
    },
    {
      "arxiv_id": "2510.19342v1",
      "title": "To Use or to Refuse? Re-Centering Student Agency with Generative AI in Engineering Design Education",
      "title_zh": "ä½¿ç”¨è¿˜æ˜¯æ‹’ç»ï¼Ÿåœ¨å·¥ç¨‹è®¾è®¡æ•™è‚²ä¸­åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é‡æ–°èšç„¦å­¦ç”Ÿèƒ½åŠ¨æ€§",
      "authors": [
        "Thijs Willems",
        "Sumbul Khan",
        "Qian Huang",
        "Bradley Camburn",
        "Nachamma Sockalingam",
        "King Wang Poon"
      ],
      "abstract": "This pilot study traces students' reflections on the use of AI in a 13-week foundational design course enrolling over 500 first-year engineering and architecture students at the Singapore University of Technology and Design. The course was an AI-enhanced design course, with several interventions to equip students with AI based design skills. Students were required to reflect on whether the technology was used as a tool (instrumental assistant), a teammate (collaborative partner), or neither (deliberate non-use). By foregrounding this three-way lens, students learned to use AI for innovation rather than just automation and to reflect on agency, ethics, and context rather than on prompt crafting alone. Evidence stems from coursework artefacts: thirteen structured reflection spreadsheets and eight illustrated briefs submitted, combined with notes of teachers and researchers. Qualitative coding of these materials reveals shared practices brought about through the inclusion of Gen-AI, including accelerated prototyping, rapid skill acquisition, iterative prompt refinement, purposeful \"switch-offs\" during user research, and emergent routines for recognizing hallucinations. Unexpectedly, students not only harnessed Gen-AI for speed but (enabled by the tool-teammate-neither triage) also learned to reject its outputs, invent their own hallucination fire-drills, and divert the reclaimed hours into deeper user research, thereby transforming efficiency into innovation. The implications of the approach we explore shows that: we can transform AI uptake into an assessable design habit; that rewarding selective non-use cultivates hallucination-aware workflows; and, practically, that a coordinated bundle of tool access, reflection, role tagging, and public recognition through competition awards allows AI based innovation in education to scale without compromising accountability.",
      "tldr_zh": "è¯¥åˆæ­¥ç ”ç©¶åœ¨æ–°åŠ å¡ç§‘æŠ€è®¾è®¡å¤§å­¦(SUTD)çš„å·¥ç¨‹è®¾è®¡åŸºç¡€è¯¾ç¨‹ä¸­ï¼Œé€šè¿‡å¯¹500å¤šåå­¦ç”Ÿä¸ºæœŸ13å‘¨çš„è¿½è¸ªï¼Œæ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)åœ¨è®¾è®¡æ•™è‚²ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶æå‡ºäº†â€œå·¥å…·(tool)ã€é˜Ÿå‹(teammate)æˆ–æ‹’ç»(neither)â€çš„ä¸‰ç»´æ¡†æ¶ï¼Œæ—¨åœ¨é‡æ–°å®šä½å­¦ç”Ÿåœ¨è®¾è®¡è¿‡ç¨‹ä¸­çš„ä¸»ä½“æ€§ï¼Œä½¿å…¶æ›´å…³æ³¨ä¼¦ç†ã€èƒŒæ™¯å’Œåˆ›æ–°è€Œéå•çº¯çš„æç¤ºè¯å·¥ç¨‹(prompt crafting)ã€‚å®šæ€§ç¼–ç åˆ†ææ˜¾ç¤ºï¼Œå­¦ç”Ÿä¸ä»…åˆ©ç”¨AIå®ç°åŠ é€ŸåŸå‹è®¾è®¡(accelerated prototyping)å’Œå¿«é€ŸæŠ€èƒ½è·å–ï¼Œè¿˜å­¦ä¼šåœ¨ç”¨æˆ·ç ”ç©¶ä¸­ä¸»åŠ¨â€œå…³é—­â€AIï¼Œå¹¶å»ºç«‹äº†è¯†åˆ«å¹»è§‰(hallucinations)çš„å¸¸è§„æµç¨‹ã€‚å®éªŒè¯æ˜ï¼Œèµ‹äºˆå­¦ç”Ÿæœ‰ç›®çš„åœ°æ‹’ç»AIè¾“å‡ºçš„æƒåˆ©ï¼Œä¿ƒä½¿ä»–ä»¬å°†èŠ‚çœçš„æ—¶é—´æŠ•å…¥åˆ°æ›´æ·±å±‚çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼ŒæˆåŠŸå°†æ•ˆç‡è½¬åŒ–ä¸ºåˆ›æ–°ã€‚è¯¥æ–¹æ³•è¡¨æ˜ï¼Œé€šè¿‡å·¥å…·è®¿é—®ã€åæ€ã€è§’è‰²æ ‡è®°(role tagging)å’Œå…¬å…±è®¤å¯çš„ååŒæœºåˆ¶ï¼Œå¯ä»¥å°†AIåº”ç”¨è½¬åŒ–ä¸ºä¸€ç§å¯è¯„ä¼°çš„è®¾è®¡ä¹ æƒ¯ï¼Œåœ¨ä¸æŸå®³é—®è´£åˆ¶çš„æƒ…å†µä¸‹å®ç°æ•™è‚²åˆ›æ–°çš„è§„æ¨¡åŒ–ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "to be published in IEEE TALE 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19342v1",
      "published_date": "2025-10-22 08:06:48 UTC",
      "updated_date": "2025-10-22 08:06:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:46.675171+00:00"
    },
    {
      "arxiv_id": "2510.19338v2",
      "title": "Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning",
      "title_zh": "æ¯ä¸€ä»½æ³¨æ„åŠ›éƒ½è‡³å…³é‡è¦ï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„é«˜æ•ˆæ··åˆæ¶æ„",
      "authors": [
        "Ling Team",
        "Bin Han",
        "Caizhi Tang",
        "Chen Liang",
        "Donghao Zhang",
        "Fan Yuan",
        "Feng Zhu",
        "Jie Gao",
        "Jingyu Hu",
        "Longfei Li",
        "Meng Li",
        "Mingyang Zhang",
        "Peijie Jiang",
        "Peng Jiao",
        "Qian Zhao",
        "Qingyuan Yang",
        "Wenbo Shen",
        "Xinxing Yang",
        "Yalin Zhang",
        "Yankun Ren",
        "Yao Zhao",
        "Yibo Cao",
        "Yixuan Sun",
        "Yue Zhang",
        "Yuchen Fang",
        "Zibin Lin",
        "Zixuan Cheng",
        "Jun Zhou"
      ],
      "abstract": "In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.",
      "tldr_zh": "è¯¥æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº† Ring-linear ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ Ring-mini-linear-2.0 å’Œ Ring-flash-linear-2.0ï¼Œé‡ç‚¹é’ˆå¯¹é•¿æ–‡æœ¬æ¨ç†ï¼ˆLong-Context Reasoningï¼‰åœºæ™¯ã€‚è¯¥ç³»åˆ—é‡‡ç”¨äº†èåˆ linear attention ä¸ softmax attention çš„æ··åˆæ¶æ„ï¼ˆHybrid Architectureï¼‰ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­çš„ I/O å’Œè®¡ç®—å¼€é”€ã€‚ä¸ 32B å‚æ•°çš„ç¨ å¯†æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç³»åˆ—å°†æ¨ç†æˆæœ¬é™ä½è‡³ååˆ†ä¹‹ä¸€ï¼Œä¸”è¾ƒåŸ Ring ç³»åˆ—é™ä½äº† 50% ä»¥ä¸Šã€‚ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ¢ç´¢ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„æ¯”ä¾‹ï¼Œç¡®å®šäº†æœ€ä¼˜æ¨¡å‹ç»“æ„ï¼Œå¹¶åˆ©ç”¨è‡ªç ”çš„ FP8 ç®—å­åº“ linghe å°†è®­ç»ƒæ•ˆç‡æå‡äº† 50%ã€‚å¾—ç›Šäºè®­ç»ƒä¸æ¨ç†ç®—å­çš„é«˜åº¦å¯¹é½ï¼Œæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰é˜¶æ®µå®ç°äº†é•¿æœŸç¨³å®šçš„ä¼˜åŒ–ï¼Œåœ¨å¤šä¸ªå¤æ‚æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¿æŒ SOTA æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19338v2",
      "published_date": "2025-10-22 07:59:38 UTC",
      "updated_date": "2025-10-23 06:33:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:53.675556+00:00"
    },
    {
      "arxiv_id": "2510.19334v1",
      "title": "Metadata Extraction Leveraging Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å…ƒæ•°æ®æå–",
      "authors": [
        "Cuize Han",
        "Sesh Jalagam"
      ],
      "abstract": "The advent of Large Language Models has revolutionized tasks across domains, including the automation of legal document analysis, a critical component of modern contract management systems. This paper presents a comprehensive implementation of LLM-enhanced metadata extraction for contract review, focusing on the automatic detection and annotation of salient legal clauses. Leveraging both the publicly available Contract Understanding Atticus Dataset (CUAD) and proprietary contract datasets, our work demonstrates the integration of advanced LLM methodologies with practical applications. We identify three pivotal elements for optimizing metadata extraction: robust text conversion, strategic chunk selection, and advanced LLM-specific techniques, including Chain of Thought (CoT) prompting and structured tool calling. The results from our experiments highlight the substantial improvements in clause identification accuracy and efficiency. Our approach shows promise in reducing the time and cost associated with contract review while maintaining high accuracy in legal clause identification. The results suggest that carefully optimized LLM systems could serve as valuable tools for legal professionals, potentially increasing access to efficient contract review services for organizations of all sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)å¢å¼ºåˆåŒå…ƒæ•°æ®æå–(Metadata Extraction)çš„å®ç°æ–¹æ¡ˆï¼Œæ—¨åœ¨å®ç°æ³•å¾‹æ–‡æ¡£åˆ†æçš„è‡ªåŠ¨åŒ–å¤„ç†ã€‚ç ”ç©¶å›¢é˜Ÿç»“åˆå…¬å¼€çš„CUADæ•°æ®é›†å’Œç§æœ‰åˆåŒæ•°æ®é›†ï¼Œé‡ç‚¹é’ˆå¯¹æ³•å¾‹æ¡æ¬¾çš„è‡ªåŠ¨æ£€æµ‹ä¸æ ‡æ³¨è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚è¯¥æ–¹æ¡ˆè¯†åˆ«å¹¶æ•´åˆäº†ä¼˜åŒ–æå–æ•ˆæœçš„ä¸‰ä¸ªæ ¸å¿ƒè¦ç´ ï¼Œå³ç¨³å¥çš„æ–‡æœ¬è½¬æ¢(text conversion)ã€ç­–ç•¥æ€§çš„åˆ†å—é€‰æ‹©(chunk selection)ä»¥åŠé“¾å¼æ€ç»´(Chain of Thought, CoT)æç¤ºå’Œç»“æ„åŒ–å·¥å…·è°ƒç”¨(structured tool calling)ç­‰é«˜çº§LLMæŠ€æœ¯ã€‚å®éªŒæ•°æ®è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ³•å¾‹æ¡æ¬¾è¯†åˆ«çš„å‡†ç¡®ç‡ä¸å¤„ç†æ•ˆç‡ï¼Œå¹¶å¤§å¹…é™ä½äº†åˆåŒå®¡æŸ¥æ‰€éœ€çš„æ—¶é—´ä¸ç»æµæˆæœ¬ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡ä¼˜åŒ–çš„LLMç³»ç»Ÿèƒ½å¤Ÿä¸ºæ³•å¾‹ä¸“ä¸šäººå£«æä¾›é«˜æ•ˆæ”¯æŒï¼Œä¸ºä¸åŒè§„æ¨¡çš„ç»„ç»‡æå‡åˆåŒå®¡æŸ¥æœåŠ¡çš„å¯åŠæ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19334v1",
      "published_date": "2025-10-22 07:56:36 UTC",
      "updated_date": "2025-10-22 07:56:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:52.187086+00:00"
    },
    {
      "arxiv_id": "2510.19329v1",
      "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
      "title_zh": "Seabed-Netï¼šåŸºäºæµ…æ°´é¥æ„Ÿå½±åƒçš„è”åˆæ°´æ·±ä¼°è®¡ä¸æµ·åºŠåˆ†ç±»å¤šä»»åŠ¡ç½‘ç»œ",
      "authors": [
        "Panagiotis Agrafiotis",
        "BegÃ¼m Demir"
      ],
      "abstract": "Accurate, detailed, and regularly updated bathymetry, coupled with complex semantic content, is essential for under-mapped shallow-water environments facing increasing climatological and anthropogenic pressures. However, existing approaches that derive either depth or seabed classes from remote sensing imagery treat these tasks in isolation, forfeiting the mutual benefits of their interaction and hindering the broader adoption of deep learning methods. To address these limitations, we introduce Seabed-Net, a unified multi-task framework that simultaneously predicts bathymetry and pixel-based seabed classification from remote sensing imagery of various resolutions. Seabed-Net employs dual-branch encoders for bathymetry estimation and pixel-based seabed classification, integrates cross-task features via an Attention Feature Fusion module and a windowed Swin-Transformer fusion block, and balances objectives through dynamic task uncertainty weighting. In extensive evaluations at two heterogeneous coastal sites, it consistently outperforms traditional empirical models and traditional machine learning regression methods, achieving up to 75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to state-of-the-art single-task and multi-task baselines and improves seabed classification accuracy up to 8\\%. Qualitative analyses further demonstrate enhanced spatial consistency, sharper habitat boundaries, and corrected depth biases in low-contrast regions. These results confirm that jointly modeling depth with both substrate and seabed habitats yields synergistic gains, offering a robust, open solution for integrated shallow-water mapping. Code and pretrained weights are available at https://github.com/pagraf/Seabed-Net.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Seabed-Netï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šä»»åŠ¡æ¡†æ¶ï¼Œæ—¨åœ¨ä»ä¸åŒåˆ†è¾¨ç‡çš„é¥æ„Ÿå½±åƒä¸­åŒæ—¶é¢„æµ‹æ°´æ·±æµ‹ç»˜(bathymetry estimation)å’ŒåŸºäºåƒç´ çš„æµ·åº•åˆ†ç±»(seabed classification)ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•å°†è¿™ä¸¤ä¸ªä»»åŠ¡å­¤ç«‹å¤„ç†çš„é—®é¢˜ï¼ŒSeabed-Neté‡‡ç”¨äº†åŒåˆ†æ”¯ç¼–ç å™¨(dual-branch encoders)ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›ç‰¹å¾èåˆæ¨¡å—(Attention Feature Fusion)å’Œçª—å£åŒ–Swin-Transformerèåˆå—æ•´åˆè·¨ä»»åŠ¡ç‰¹å¾ï¼ŒåŒæ—¶åˆ©ç”¨åŠ¨æ€ä»»åŠ¡ä¸ç¡®å®šæ€§åŠ æƒ(dynamic task uncertainty weighting)å¹³è¡¡å­¦ä¹ ç›®æ ‡ã€‚åœ¨å¼‚æ„æµ·å²¸ç«™ç‚¹çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç»éªŒæ¨¡å‹ï¼Œå°†å‡æ–¹æ ¹è¯¯å·®(RMSE)é™ä½äº†é«˜è¾¾75%ï¼Œå¹¶æ¯”ç°æœ‰çš„å•ä»»åŠ¡å’Œå¤šä»»åŠ¡åŸºå‡†æ¨¡å‹åœ¨æ°´æ·±ç²¾åº¦ä¸Šæå‡äº†10-30%ï¼Œåœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šæå‡äº†8%ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆçº æ­£ä½å¯¹æ¯”åº¦åŒºåŸŸçš„æ·±åº¦åå·®ï¼Œå¹¶æä¾›æ›´å…·ç©ºé—´ä¸€è‡´æ€§çš„ç»“æœå’Œæ›´æ¸…æ™°çš„æ –æ¯åœ°è¾¹ç•Œã€‚ç ”ç©¶ç»“æœè¯å®äº†è”åˆå»ºæ¨¡æ·±åº¦ä¸æµ·åº•æ –æ¯åœ°å…·æœ‰ååŒå¢ç›Šä½œç”¨ï¼Œä¸ºæµ…æ°´ç¯å¢ƒçš„ç»¼åˆåˆ¶å›¾æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å¼€æºçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ISPRS Journal of Photogrammetry and Remote Sensing",
      "pdf_url": "https://arxiv.org/pdf/2510.19329v1",
      "published_date": "2025-10-22 07:43:03 UTC",
      "updated_date": "2025-10-22 07:43:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:02:57.973670+00:00"
    },
    {
      "arxiv_id": "2510.19327v1",
      "title": "SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities",
      "title_zh": "SORA-ATMASï¼šé¢å‘æœªæ¥æ™ºæ…§åŸå¸‚çš„è‡ªé€‚åº”ä¿¡ä»»ç®¡ç†ä¸å¤šå¤§è¯­è¨€æ¨¡å‹å¯¹é½æ²»ç†",
      "authors": [
        "Usama Antuley",
        "Shahbaz Siddiqui",
        "Sufian Hameed",
        "Waqas Arif",
        "Subhan Shah",
        "Syed Attique Shah"
      ],
      "abstract": "The rapid evolution of smart cities has increased the reliance on intelligent interconnected services to optimize infrastructure, resources, and citizen well-being. Agentic AI has emerged as a key enabler by supporting autonomous decision-making and adaptive coordination, allowing urban systems to respond in real time to dynamic conditions. Its benefits are evident in areas such as transportation, where the integration of traffic data, weather forecasts, and safety sensors enables dynamic rerouting and a faster response to hazards. However, its deployment across heterogeneous smart city ecosystems raises critical governance, risk, and compliance (GRC) challenges, including accountability, data privacy, and regulatory alignment within decentralized infrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather, Traffic, and Safety) demonstrated that its governance policies, including a fallback mechanism for high-risk scenarios, effectively steer multiple LLMs (GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs, producing an average MAE reduction of 35% across agents. Results showed stable weather monitoring, effective handling of high-risk traffic plateaus 0.85, and adaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a 3-agent deployment confirmed scalability, with throughput between 13.8-17.2 requests per second, execution times below 72~ms, and governance delays under 100 ms, analytical projections suggest maintained performance at larger scales. Cross-domain rules ensured safe interoperability, with traffic rerouting permitted only under validated weather conditions. These findings validate SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance framework that consolidates distributed agent outputs into accountable, real-time decisions, offering a resilient foundation for smart-city management.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SORA-ATMASï¼Œä¸€ä¸ªé’ˆå¯¹æœªæ¥æ™ºæ…§åŸå¸‚è®¾è®¡çš„è‡ªé€‚åº”ä¿¡ä»»ç®¡ç†ä¸å¤šå¤§è¯­è¨€æ¨¡å‹ï¼ˆMulti-LLMï¼‰å¯¹é½æ²»ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ Agentic AI åœ¨å¼‚æ„åŸå¸‚ç”Ÿæ€ä¸­éƒ¨ç½²æ—¶é¢ä¸´çš„æ²»ç†ã€é£é™©ä¸åˆè§„ï¼ˆGRCï¼‰æŒ‘æˆ˜ï¼Œé€šè¿‡ Fallback Mechanism å’Œè·¨åŸŸè§„åˆ™ç¡®ä¿åˆ†å¸ƒå¼æ™ºèƒ½ä½“è¾“å‡ºçš„å¯é—®è´£æ€§ä¸å®‰å…¨æ€§ã€‚å®éªŒé€šè¿‡ Weatherã€Traffic å’Œ Safety ä¸‰ä¸ªé¢†åŸŸçš„æ™ºèƒ½ä½“å¯¹ GPTã€Grok å’Œ DeepSeek ç­‰æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿä½¿å„æ™ºèƒ½ä½“çš„å¹³å‡å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº† 35%ã€‚SORA-ATMAS åœ¨é«˜é£é™©äº¤é€šåœºæ™¯å’Œå®‰å…¨/ç«ç¾åœºæ™¯ä¸­å±•ç°äº†å“è¶Šçš„è‡ªé€‚åº”ä¿¡ä»»è°ƒèŠ‚èƒ½åŠ›ï¼Œä¸”è¿è¡Œåˆ†æè¯å®å…¶ååé‡å¯è¾¾ 13.8-17.2 requests per secondï¼Œæ²»ç†å»¶è¿Ÿä½äº 100 msã€‚è¯¥ç ”ç©¶éªŒè¯äº† SORA-ATMAS ä½œä¸ºä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸”å¯éªŒè¯çš„æ²»ç†æ¡†æ¶ï¼Œèƒ½æœ‰æ•ˆå°†åˆ†å¸ƒå¼è¾“å‡ºè½¬åŒ–ä¸ºå®æ—¶ã€è´Ÿè´£ä»»çš„å†³ç­–ï¼Œä¸ºæ™ºæ…§åŸå¸‚ç®¡ç†å¥ å®šäº†åšéŸ§çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19327v1",
      "published_date": "2025-10-22 07:40:37 UTC",
      "updated_date": "2025-10-22 07:40:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:05.082193+00:00"
    },
    {
      "arxiv_id": "2510.19325v1",
      "title": "Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization",
      "title_zh": "æ–‡æœ¬æ‘˜è¦ä¸­çš„å¥–åŠ±å‡è¡¡ï¼šåŸºäºè¶…ä½“ç§¯ä¼˜åŒ–çš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Junjie Song",
        "Yiwen Liu",
        "Dapeng Li",
        "Yin Sun",
        "Shukun Fu",
        "Siqi Chen",
        "Yuji Cao"
      ],
      "abstract": "Text summarization is a crucial task that requires the simultaneous optimization of multiple objectives, including consistency, coherence, relevance, and fluency, which presents considerable challenges. Although large language models (LLMs) have demonstrated remarkable performance, enhanced by reinforcement learning (RL), few studies have focused on optimizing the multi-objective problem of summarization through RL based on LLMs. In this paper, we introduce hypervolume optimization (HVO), a novel optimization strategy that dynamically adjusts the scores between groups during the reward process in RL by using the hypervolume method. This method guides the model's optimization to progressively approximate the pareto front, thereby generating balanced summaries across multiple objectives. Experimental results on several representative summarization datasets demonstrate that our method outperforms group relative policy optimization (GRPO) in overall scores and shows more balanced performance across different dimensions. Moreover, a 7B foundation model enhanced by HVO performs comparably to GPT-4 in the summarization task, while maintaining a shorter generation length. Our code is publicly available at https://github.com/ai4business-LiAuto/HVO.git",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è¶…ä½“ç§¯ä¼˜åŒ–(HyperVolume Optimization, HVO)ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­éœ€è¦åŒæ—¶ä¼˜åŒ–ä¸€è‡´æ€§(Consistency)ã€è¿è´¯æ€§(Coherence)ã€ç›¸å…³æ€§(Relevance)å’Œæµåˆ©åº¦(Fluency)ç­‰å¤šç›®æ ‡å¹³è¡¡çš„éš¾é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ (RL)çš„å¥–åŠ±è¿‡ç¨‹ä¸­å¼•å…¥è¶…ä½“ç§¯è®¡ç®—ï¼ŒåŠ¨æ€è°ƒæ•´ä¸åŒç›®æ ‡çš„å¾—åˆ†æƒé‡ï¼Œå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ä¼˜åŒ–è¿‡ç¨‹é€æ­¥é€¼è¿‘å¸•ç´¯æ‰˜å‰æ²¿(Pareto Front)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHVO åœ¨å¤šä¸ªä»£è¡¨æ€§æ•°æ®é›†ä¸Šçš„ç»¼åˆè¡¨ç°ä¼˜äºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ï¼Œåœ¨ä¸åŒè¯„ä»·ç»´åº¦é—´å±•ç°å‡ºæ›´å‡è¡¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç»è¿‡ HVO å¢å¼ºçš„ 7B åŸºç¡€æ¨¡å‹åœ¨æ‘˜è¦ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸ GPT-4 ç›¸å½“çš„æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†æ›´çŸ­çš„ç”Ÿæˆé•¿åº¦ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æå‡æ¨¡å‹ç”Ÿæˆè´¨é‡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå¹³è¡¡å¤æ‚ç”Ÿæˆä»»åŠ¡ä¸­çš„ç›¸äº’å†²çªç›®æ ‡æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19325v1",
      "published_date": "2025-10-22 07:39:04 UTC",
      "updated_date": "2025-10-22 07:39:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:17.394340+00:00"
    },
    {
      "arxiv_id": "2510.19322v2",
      "title": "Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks",
      "title_zh": "å®ç°å…‰ç½‘ç»œé›†åˆé€šä¿¡ä¸­çš„é‡æ„ä¸é€šä¿¡é‡å ",
      "authors": [
        "Changbo Wu",
        "Zhuolong Yu",
        "Gongming Zhao",
        "Hongli Xu"
      ],
      "abstract": "Collective communication (CC) is critical for scaling distributed machine learning (DML). The predictable traffic patterns of DML present a great oppotunity for applying optical network technologies. Optical networks with reconfigurable topologies promise high bandwidth and low latency for collective communications. However, existing approaches face inherent limitations: static topologies are inefficient for dynamic communication patterns within CC algorithm, while frequent topology reconfiguration matching every step of the algorithm incurs significant overhead.\n  In this paper, we propose SWOT, a demand-aware optical network framework that employs ``intra-collective reconfiguration'' to dynamically align network resources with CC traffic patterns. SWOT hides reconfiguration latency by overlapping it with data transmission through three key techniques: Heterogeneous Message Splitting, Asynchronous Overlapping, and Topology Bypassing. Extensive simulations demonstrate that SWOT reduces communication completion time up to 89.7% across diverse CC algorithm compared to static baselines, demonstrating strong robustness to varying optical resources and reconfiguration delay.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ (DML)ä¸­é›†åˆé€šä¿¡(Collective Communication)çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†SWOTæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…‰ç½‘ç»œåœ¨é™æ€æ‹“æ‰‘æ•ˆç‡ä½ä¸‹ä¸é¢‘ç¹é‡æ„å¼€é”€è¿‡å¤§ä¹‹é—´çš„çŸ›ç›¾ã€‚SWOTå¼•å…¥äº†â€œé›†åˆå†…éƒ¨é‡æ„(intra-collective reconfiguration)â€ç­–ç•¥ï¼Œé€šè¿‡å¼‚æ„æ¶ˆæ¯æ‹†åˆ†(Heterogeneous Message Splitting)ã€å¼‚æ­¥é‡å (Asynchronous Overlapping)å’Œæ‹“æ‰‘ç»•è¿‡(Topology Bypassing)ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼ŒæˆåŠŸå°†æ‹“æ‰‘é‡æ„å»¶è¿Ÿéšè—åœ¨æ•°æ®ä¼ è¾“è¿‡ç¨‹ä¸­ã€‚ä»¿çœŸå®éªŒç»“æœè¡¨æ˜ï¼Œä¸é™æ€åŸºçº¿ç›¸æ¯”ï¼ŒSWOTåœ¨å¤šç§é›†åˆé€šä¿¡ç®—æ³•ä¸‹å¯å°†é€šä¿¡å®Œæˆæ—¶é—´ç¼©çŸ­é«˜è¾¾89.7%ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†SWOTåœ¨ä¸åŒå…‰èµ„æºå’Œé‡æ„å»¶è¿Ÿä¸‹çš„å¼ºå¤§é²æ£’æ€§ï¼Œè¿˜ä¸ºå…‰ç½‘ç»œåœ¨é«˜æ€§èƒ½åˆ†å¸ƒå¼å­¦ä¹ åœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19322v2",
      "published_date": "2025-10-22 07:34:04 UTC",
      "updated_date": "2026-01-04 04:02:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:19.586981+00:00"
    },
    {
      "arxiv_id": "2510.19321v1",
      "title": "Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer",
      "title_zh": "åŸºäºæ—¶ç©ºå›¾æ³¨æ„åŠ› Transformer çš„è”æœºæ‰‹å†™ç­¾åéªŒè¯",
      "authors": [
        "Hai-jie Yuan",
        "Heng Zhang",
        "Fei Yin"
      ],
      "abstract": "Handwritten signature verification is a crucial aspect of identity authentication, with applications in various domains such as finance and e-commerce. However, achieving high accuracy in signature verification remains challenging due to intra-user variability and the risk of forgery. This paper introduces a novel approach for dynamic signature verification: the Temporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the Graph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both spatial and temporal dependencies in signature data. TS-GATR enhances verification performance by representing signatures as graphs, where each node captures dynamic features (e.g. position, velocity, pressure), and by using attention mechanisms to model their complex relationships. The proposed method further employs a Dual-Graph Attention Transformer (DGATR) module, which utilizes k-step and k-nearest neighbor adjacency graphs to model local and global spatial features, respectively. To capture long-term temporal dependencies, the model integrates GRU, thereby enhancing its ability to learn dynamic features during signature verification. Comprehensive experiments conducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR surpasses current state-of-the-art approaches, consistently achieving lower Equal Error Rates (EER) across various scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º TS-GATR (Temporal-Spatial Graph Attention Transformer) çš„æ–°å‹åŠ¨æ€ç­¾åéªŒè¯æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç”¨æˆ·å†…å˜å¼‚å’Œä¼ªé€ é£é™©å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å›¾æ³¨æ„åŠ›ç½‘ç»œ (GAT) å’Œé—¨æ§å¾ªç¯å•å…ƒ (GRU)ï¼Œé€šè¿‡å°†ç­¾åè¡¨ç¤ºä¸ºåŒ…å«ä½ç½®ã€é€Ÿåº¦å’Œå‹åŠ›ç­‰åŠ¨æ€ç‰¹å¾çš„å›¾ç»“æ„ï¼Œæœ‰æ•ˆå»ºæ¨¡äº†å¤æ‚çš„ç©ºé—´å’Œæ—¶é—´ä¾èµ–å…³ç³»ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†åŒå›¾æ³¨æ„åŠ›è½¬æ¢å™¨ (DGATR) æ¨¡å—ï¼Œåˆ©ç”¨ k-step å’Œ k-nearest neighbor é‚»æ¥å›¾åˆ†åˆ«æ•è·å±€éƒ¨å’Œå…¨å±€ç©ºé—´ç‰¹å¾ã€‚ä¸ºäº†å¤„ç†é•¿æœŸæ—¶é—´æ¼”åŒ–ï¼Œæ¨¡å‹é›†æˆ GRU å•å…ƒä»¥å¢å¼ºå…¶å­¦ä¹ ç­¾ååŠ¨æ€ç‰¹å¾çš„èƒ½åŠ›ã€‚åœ¨ MSDS å’Œ DeepSignDB ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTS-GATR åœ¨å¤šç§éªŒè¯åœºæ™¯ä¸‹å‡å®ç°äº†æ›´ä½çš„ç­‰é”™è¯¯ç‡ (EER)ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (State-of-the-art) æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19321v1",
      "published_date": "2025-10-22 07:32:55 UTC",
      "updated_date": "2025-10-22 07:32:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:22.895570+00:00"
    },
    {
      "arxiv_id": "2510.19314v2",
      "title": "Continual Knowledge Adaptation for Reinforcement Learning",
      "title_zh": "é¢å‘å¼ºåŒ–å­¦ä¹ çš„æŒç»­çŸ¥è¯†è‡ªé€‚åº”",
      "authors": [
        "Jinwu Hu",
        "Zihao Lian",
        "Zhiquan Wen",
        "Chenghao Li",
        "Guohao Chen",
        "Xutao Wen",
        "Bin Xiao",
        "Mingkui Tan"
      ],
      "abstract": "Reinforcement Learning enables agents to learn optimal behaviors through interactions with environments. However, real-world environments are typically non-stationary, requiring agents to continuously adapt to new tasks and changing conditions. Although Continual Reinforcement Learning facilitates learning across multiple tasks, existing methods often suffer from catastrophic forgetting and inefficient knowledge utilization. To address these challenges, we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL), which enables the accumulation and effective utilization of historical knowledge. Specifically, we introduce a Continual Knowledge Adaptation strategy, which involves maintaining a task-specific knowledge vector pool and dynamically using historical knowledge to adapt the agent to new tasks. This process mitigates catastrophic forgetting and enables efficient knowledge transfer across tasks by preserving and adapting critical model parameters. Additionally, we propose an Adaptive Knowledge Merging mechanism that combines similar knowledge vectors to address scalability challenges, reducing memory requirements while ensuring the retention of essential knowledge. Experiments on three benchmarks demonstrate that the proposed CKA-RL outperforms state-of-the-art methods, achieving an improvement of 4.20% in overall performance and 8.02% in forward transfer. The source code is available at https://github.com/Fhujinwu/CKA-RL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨éå¹³ç¨³ç¯å¢ƒä¸‹é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)å’ŒçŸ¥è¯†åˆ©ç”¨æ•ˆç‡ä½ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†æŒç»­çŸ¥è¯†é€‚é…å¼ºåŒ–å­¦ä¹ æ¡†æ¶(CKA-RL)ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºæŒç»­çŸ¥è¯†é€‚é…ç­–ç•¥ï¼Œé€šè¿‡ç»´æŠ¤ä»»åŠ¡ç‰¹å®šçš„çŸ¥è¯†å‘é‡æ± å¹¶åŠ¨æ€è°ƒç”¨å†å²çŸ¥è¯†ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚è¿™ä¸€æœºåˆ¶é€šè¿‡ä¿ç•™å’Œé€‚é…å…³é”®æ¨¡å‹å‚æ•°ï¼Œåœ¨æœ‰æ•ˆç¼“è§£é—å¿˜é—®é¢˜çš„åŒæ—¶å®ç°äº†è·¨ä»»åŠ¡çš„é«˜æ•ˆçŸ¥è¯†è¿ç§»(knowledge transfer)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†è‡ªé€‚åº”çŸ¥è¯†åˆå¹¶(Adaptive Knowledge Merging)æœºåˆ¶ï¼Œé€šè¿‡èåˆç›¸ä¼¼çŸ¥è¯†å‘é‡æ¥åº”å¯¹å¯æ‰©å±•æ€§æŒ‘æˆ˜å¹¶é™ä½å†…å­˜éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCKA-RLåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…¶æ•´ä½“æ€§èƒ½æå‡äº†4.20%ï¼Œåœ¨å‰å‘è¿ç§»(forward transfer)æŒ‡æ ‡ä¸Šæ›´æ˜¯å®ç°äº†8.02%çš„æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19314v2",
      "published_date": "2025-10-22 07:25:41 UTC",
      "updated_date": "2026-01-20 04:47:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:27.276829+00:00"
    },
    {
      "arxiv_id": "2510.24760v1",
      "title": "Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments",
      "title_zh": "Dingtalk DeepResearchï¼šé¢å‘ä¼ä¸šç¯å¢ƒè‡ªé€‚åº”æ™ºèƒ½çš„ç»Ÿä¸€å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Mengyuan Chen",
        "Chengjun Dai",
        "Xinyang Dong",
        "Chengzhe Feng",
        "Kewei Fu",
        "Jianshe Li",
        "Zhihan Peng",
        "Yongqi Tong",
        "Junshao Zhang",
        "Hong Zhu"
      ],
      "abstract": "We present Dingtalk DeepResearch, a unified multi agent intelligence framework for real world enterprise environments, delivering deep research, heterogeneous table reasoning, and multimodal report generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Dingtalk DeepResearchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºçœŸå®ä¼ä¸šç¯å¢ƒè®¾è®¡çš„ç»Ÿä¸€å¤šæ™ºèƒ½ä½“ (Multi-Agent) æ™ºèƒ½æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†æ·±åº¦ç ”ç©¶ (Deep Research)ã€å¼‚æ„è¡¨æ ¼æ¨ç† (Heterogeneous Table Reasoning) ä»¥åŠå¤šæ¨¡æ€æŠ¥å‘Šç”Ÿæˆ (Multimodal Report Generation) ç­‰å…³é”®æŠ€æœ¯ã€‚é€šè¿‡è¿™ä¸€ç³»ç»Ÿï¼ŒDingtalk DeepResearch èƒ½å¤Ÿå®ç°å¯¹ä¼ä¸šå¤æ‚æ•°æ®ç¯å¢ƒçš„è‡ªé€‚åº”å¤„ç†ï¼Œå¹¶è‡ªåŠ¨åŒ–ç”Ÿæˆé«˜è´¨é‡çš„ç ”ç©¶æŠ¥å‘Šã€‚è¯¥æ¡†æ¶çš„æå‡ºä¸ºä¼ä¸šçº§åº”ç”¨ä¸­çš„è‡ªé€‚åº”æ™ºèƒ½æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å¤„ç†å¤æ‚å¼‚æ„æ•°æ®å’Œæ·±åº¦è°ƒç ”ä»»åŠ¡çš„èƒ½åŠ›ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.24760v1",
      "published_date": "2025-10-22 07:14:26 UTC",
      "updated_date": "2025-10-22 07:14:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:28.779723+00:00"
    },
    {
      "arxiv_id": "2510.19303v1",
      "title": "Collaborative penetration testing suite for emerging generative AI algorithms",
      "title_zh": "é¢å‘æ–°å…´ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç®—æ³•çš„ååŒæ¸—é€æµ‹è¯•å¥—ä»¶",
      "authors": [
        "Petar Radanliev"
      ],
      "abstract": "Problem Space: AI Vulnerabilities and Quantum Threats Generative AI vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative AI models against classical and quantum cyberattacks. Proposed Solution Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs. Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities identified across test environments. 70% reduction in high-severity issues within 2 weeks. 90% resolution efficiency for blockchain-logged vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum Cryptography AI Red Teaming.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) çš„å®‰å…¨æ¼æ´åŠé‡å­è®¡ç®—å¸¦æ¥çš„åŠ å¯†å¨èƒï¼Œæå‡ºäº†ä¸€ç§åä½œå¼æ¸—é€æµ‹è¯•å¥—ä»¶ (Collaborative Penetration Testing Suite)ã€‚è¯¥æ–¹æ¡ˆé›†æˆäº† DASTã€SAST å’Œ IAST ç­‰å¤šç§å®‰å…¨æµ‹è¯•æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨ Hyperledger Fabric åŒºå—é“¾æŠ€æœ¯å®ç°äº†é˜²ç¯¡æ”¹çš„æ—¥å¿—è®°å½•ï¼Œä»¥ç¡®ä¿æ¼æ´è¿½è¸ªçš„å¯é æ€§ã€‚ä¸ºäº†åº”å¯¹é‡å­å¨èƒï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæ ¼ (Lattice-based) çš„ RLWE åè®®è¿›è¡Œé‡å­åŠ å¯†ï¼Œå¹¶ç»“åˆ AI Red Team æ¨¡æ‹Ÿå¯¹æŠ—æ€§æœºå™¨å­¦ä¹ ä¸é‡å­è¾…åŠ©æ”»å‡»ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥å¥—ä»¶åœ¨æµ‹è¯•ç¯å¢ƒä¸­æˆåŠŸè¯†åˆ«å‡º 300 å¤šä¸ªæ¼æ´ï¼Œå¹¶åœ¨ä¸¤å‘¨å†…å°†é«˜ä¸¥é‡æ€§å®‰å…¨é—®é¢˜çš„æ•°é‡å‡å°‘äº† 70%ã€‚æ­¤å¤–ï¼ŒåŒºå—é“¾è®°å½•çš„æ¼æ´å¤„ç†æ•ˆç‡è¾¾åˆ°äº† 90%ï¼Œä¸”é‡å­æŠ—æ€§åŠ å¯†åœ¨æµ‹è¯•ä¸­ä¿æŒäº† 100% çš„å®Œæ•´æ€§ã€‚è¯¥ç ”ç©¶æœ€ç»ˆæ„å»ºäº†ä¸€å¥—æ•´åˆåŒºå—é“¾ã€é‡å­å¯†ç å­¦å’Œ AI çº¢é˜ŸæŠ€æœ¯çš„ Quantum AI Security Protocolï¼Œä¸ºä¿éšœç”Ÿæˆå¼ AI æ¨¡å‹å…å—ç»å…¸å’Œé‡å­ç½‘ç»œæ”»å‡»æä¾›äº†å…¨é¢æ¡†æ¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19303v1",
      "published_date": "2025-10-22 07:05:08 UTC",
      "updated_date": "2025-10-22 07:05:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:38.180146+00:00"
    },
    {
      "arxiv_id": "2510.19299v1",
      "title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties",
      "title_zh": "å­¦ä¹ ç»“äº¤æœ‹å‹ï¼šå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ„å»ºæ¶Œç°çš„ç¤¾äº¤çº½å¸¦",
      "authors": [
        "Philipp J. Schneider",
        "Lin Tian",
        "Marian-Andrei Rizoiu"
      ],
      "abstract": "Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“æ˜¯å¦èƒ½å¤Ÿé‡ç°äººç±»åœ¨çº¿ç¤¾äº¤ä¸­å¤æ‚çš„åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“é€šè¿‡è¯­å¢ƒå­¦ä¹ (in-context learning)å’Œè¾…å¯¼ä¿¡å·(coaching signal)åœ¨æŒç»­äº’åŠ¨ä¸­è¯„ä¼°å¹¶è°ƒæ•´è‡ªèº«è¡Œä¸ºã€‚ç ”ç©¶è€…è®¾è®¡äº†æ•æ‰ç¤¾äº¤äº’åŠ¨ã€ä¿¡æ¯å¯»æ±‚ã€è‡ªæˆ‘å‘ˆç°å’Œæƒ…æ„Ÿæ”¯æŒç­‰æ ¸å¿ƒé©±åŠ¨å› ç´ çš„è¡Œä¸ºå¥–åŠ±å‡½æ•°(behavioral reward functions)ï¼Œä½¿æ™ºèƒ½ä½“ç›®æ ‡ä¸äººç±»ç¤¾äº¤åŠ¨æœºä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå—è¾…å¯¼çš„LLMæ™ºèƒ½ä½“èƒ½å¤Ÿå‘å±•å‡ºç¨³å®šçš„äº’åŠ¨æ¨¡å¼å¹¶å½¢æˆæ¶Œç°ç¤¾äº¤çº½å¸¦(emergent social ties)ï¼Œç”Ÿæˆçš„ç½‘ç»œç»“æ„åœ¨ç‰¹æ€§ä¸Šä¸çœŸå®çš„åœ¨çº¿ç¤¾åŒºé«˜åº¦ç›¸ä¼¼ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¡Œä¸ºå¥–åŠ±ä¸è¯­å¢ƒé€‚åº”ï¼Œä¸ºç ”ç©¶LLMç¾¤ä½“ä¸­çš„é›†ä½“åŠ¨åŠ›å­¦(collective dynamics)æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„å®éªŒå¹³å°ã€‚è¿™é¡¹å·¥ä½œæ·±å…¥æ­ç¤ºäº†äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“åœ¨æ¨¡æ‹Ÿäººç±»ç¤¾äº¤è¡Œä¸ºæ–¹é¢çš„è¡¨ç°åŠä¸äººç±»è¡Œä¸ºçš„å·®å¼‚ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19299v1",
      "published_date": "2025-10-22 07:00:33 UTC",
      "updated_date": "2025-10-22 07:00:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:36.200182+00:00"
    },
    {
      "arxiv_id": "2510.19298v1",
      "title": "Knowledge and Common Knowledge of Strategies",
      "title_zh": "ç­–ç•¥çš„çŸ¥è¯†ä¸å…±åŒçŸ¥è¯†",
      "authors": [
        "Borja Sierra Miranda",
        "Thomas Studer"
      ],
      "abstract": "Most existing work on strategic reasoning simply adopts either an informed or an uninformed semantics. We propose a model where knowledge of strategies can be specified on a fine-grained level. In particular, it is possible to distinguish first-order, higher-order, and common knowledge of strategies. We illustrate the effect of higher-order knowledge of strategies by studying the game Hanabi. Further, we show that common knowledge of strategies is necessary to solve the consensus problem. Finally, we study the decidability of the model checking problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨å¯¹ç­–ç•¥çŸ¥è¯†è¿›è¡Œç»†ç²’åº¦åˆ»ç”»çš„æ¨¡å‹ï¼Œä»¥å¼¥è¡¥ç°æœ‰ç­–ç•¥æ¨ç†ç ”ç©¶ä¸­ä»…é‡‡ç”¨çŸ¥æƒ…æˆ–ä¸çŸ¥æƒ…è¯­ä¹‰çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹èƒ½å¤ŸåŒºåˆ†ä¸€é˜¶çŸ¥è¯†ã€é«˜é˜¶çŸ¥è¯†ä»¥åŠç­–ç•¥çš„å…±åŒçŸ¥è¯†(Common Knowledge of Strategies)ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°æè¿°æ™ºèƒ½ä½“å¯¹ç­–ç•¥çš„è®¤çŸ¥æ°´å¹³ã€‚ä½œè€…é€šè¿‡å¯¹åä½œæ¸¸æˆ Hanabi çš„æ·±å…¥ç ”ç©¶ï¼Œå±•ç¤ºäº†é«˜é˜¶ç­–ç•¥çŸ¥è¯†å¯¹åšå¼ˆè¿‡ç¨‹çš„æ˜¾è‘—å½±å“ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œç­–ç•¥çš„å…±åŒçŸ¥è¯†æ˜¯è§£å†³å…±è¯†é—®é¢˜(Consensus Problem)çš„å¿…è¦æ¡ä»¶ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†è¯¥æ¡†æ¶ä¸‹æ¨¡å‹æ£€æµ‹(Model Checking)é—®é¢˜çš„å¯åˆ¤å®šæ€§(Decidability)ã€‚è¯¥æˆæœä¸ºç†è§£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å¤æ‚ç­–ç•¥äº¤äº’æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19298v1",
      "published_date": "2025-10-22 07:00:33 UTC",
      "updated_date": "2025-10-22 07:00:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:37.484970+00:00"
    },
    {
      "arxiv_id": "2510.19282v1",
      "title": "Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning",
      "title_zh": "åˆ©ç”¨å¤§æ•°æ®ä¸é›†æˆå°æ ·æœ¬å­¦ä¹ æå‡é˜¿å°”èŒ¨æµ·é»˜ç—…æ—©æœŸæ£€æµ‹",
      "authors": [
        "Safa Ben Atitallah",
        "Maha Driss",
        "Wadii Boulila",
        "Anis Koubaa"
      ],
      "abstract": "Alzheimer disease is a severe brain disorder that causes harm in various brain areas and leads to memory damage. The limited availability of labeled medical data poses a significant challenge for accurate Alzheimer disease detection. There is a critical need for effective methods to improve the accuracy of Alzheimer disease detection, considering the scarcity of labeled data, the complexity of the disease, and the constraints related to data privacy. To address this challenge, our study leverages the power of big data in the form of pre-trained Convolutional Neural Networks (CNNs) within the framework of Few-Shot Learning (FSL) and ensemble learning. We propose an ensemble approach based on a Prototypical Network (ProtoNet), a powerful method in FSL, integrating various pre-trained CNNs as encoders. This integration enhances the richness of features extracted from medical images. Our approach also includes a combination of class-aware loss and entropy loss to ensure a more precise classification of Alzheimer disease progression levels. The effectiveness of our method was evaluated using two datasets, the Kaggle Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and 99.86%, respectively. The comparison of our results with relevant state-of-the-art studies demonstrated that our approach achieved superior accuracy and highlighted its validity and potential for real-world applications in early Alzheimer disease detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Alzheimer diseaseæ£€æµ‹ä¸­é¢ä¸´çš„æ ‡æ³¨åŒ»ç–—æ•°æ®åŒ®ä¹ã€ç–¾ç—…å¤æ‚æ€§ä»¥åŠæ•°æ®éšç§é™åˆ¶ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤§æ•°æ®ä¸Ensemble Few-Shot Learningçš„åˆ›æ–°æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„Convolutional Neural Networks (CNNs)ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶å°†å…¶é›†æˆåˆ°Prototypical Network (ProtoNet)è¿™ä¸€å¼ºåŠ›çš„Few-Shot Learning (FSL)æ¡†æ¶ä¸­ã€‚é€šè¿‡æ•´åˆå¤šç§é¢„è®­ç»ƒCNNsä½œä¸ºç¼–ç å™¨ï¼Œç ”ç©¶æ˜¾è‘—æå‡äº†ä»åŒ»ç–—å½±åƒä¸­æå–ç‰¹å¾çš„ä¸°å¯Œåº¦ï¼Œå¹¶å¼•å…¥äº†class-aware lossä¸entropy lossçš„ç»„åˆç­–ç•¥ï¼Œä»¥ç¡®ä¿å¯¹ç–¾ç—…è¿›å±•ç¨‹åº¦è¿›è¡Œæ›´ç²¾ç¡®çš„åˆ†ç±»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Kaggle Alzheimeræ•°æ®é›†å’ŒADNIæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†99.72%å’Œ99.86%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚é€šè¿‡ä¸ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯çš„å¯¹æ¯”ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†æ‰€ææ–¹æ³•åœ¨æ—©æœŸAlzheimer diseaseæ£€æµ‹ä¸­çš„ä¼˜è¶Šæ€§èƒ½ï¼Œå±•ç°äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19282v1",
      "published_date": "2025-10-22 06:35:03 UTC",
      "updated_date": "2025-10-22 06:35:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:45.886843+00:00"
    },
    {
      "arxiv_id": "2510.19270v1",
      "title": "Social World Model-Augmented Mechanism Design Policy Learning",
      "title_zh": "ç¤¾ä¼šä¸–ç•Œæ¨¡å‹å¢å¼ºçš„æœºåˆ¶è®¾è®¡ç­–ç•¥å­¦ä¹ ",
      "authors": [
        "Xiaoyuan Zhang",
        "Yizhe Huang",
        "Chengdong Ma",
        "Zhixun Chen",
        "Long Ma",
        "Yali Du",
        "Song-Chun Zhu",
        "Yaodong Yang",
        "Xue Feng"
      ],
      "abstract": "Designing adaptive mechanisms to align individual and collective interests remains a central challenge in artificial social intelligence. Existing methods often struggle with modeling heterogeneous agents possessing persistent latent traits (e.g., skills, preferences) and dealing with complex multi-agent system dynamics. These challenges are compounded by the critical need for high sample efficiency due to costly real-world interactions. World Models, by learning to predict environmental dynamics, offer a promising pathway to enhance mechanism design in heterogeneous and complex systems. In this paper, we introduce a novel method named SWM-AP (Social World Model-Augmented Mechanism Design Policy Learning), which learns a social world model hierarchically modeling agents' behavior to enhance mechanism design. Specifically, the social world model infers agents' traits from their interaction trajectories and learns a trait-based model to predict agents' responses to the deployed mechanisms. The mechanism design policy collects extensive training trajectories by interacting with the social world model, while concurrently inferring agents' traits online during real-world interactions to further boost policy learning efficiency. Experiments in diverse settings (tax policy design, team coordination, and facility location) demonstrate that SWM-AP outperforms established model-based and model-free RL baselines in cumulative rewards and sample efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SWM-AP (Social World Model-Augmented Mechanism Design Policy Learning)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ„å»ºç¤¾ä¼šä¸–ç•Œæ¨¡å‹æ¥å¢å¼ºæœºåˆ¶è®¾è®¡(Mechanism Design)ç­–ç•¥å­¦ä¹ çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹å¼‚æ„æ™ºèƒ½ä½“å…·æœ‰æŒä¹…æ½œç‰¹å¾(Latent Traits)åŠå¤æ‚å¤šæ™ºèƒ½ä½“åŠ¨åŠ›å­¦å¯¼è‡´çš„æ ·æœ¬æ•ˆç‡ä½ç­‰é—®é¢˜ï¼ŒSWM-APé‡‡ç”¨åˆ†å±‚å»ºæ¨¡æ–¹å¼æ¥é¢„æµ‹æ™ºèƒ½ä½“è¡Œä¸ºã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶çš„ç¤¾ä¼šä¸–ç•Œæ¨¡å‹ä»äº¤äº’è½¨è¿¹ä¸­æ¨æ–­æ™ºèƒ½ä½“çš„Traitï¼Œå¹¶åˆ©ç”¨åŸºäºç‰¹å¾çš„æ¨¡å‹é¢„æµ‹å…¶å¯¹æ‰€éƒ¨ç½²æœºåˆ¶çš„ååº”ã€‚æœºåˆ¶è®¾è®¡ç­–ç•¥é€šè¿‡ä¸ç¤¾ä¼šä¸–ç•Œæ¨¡å‹äº¤äº’æ¥æ”¶é›†æµ·é‡è®­ç»ƒè½¨è¿¹ï¼ŒåŒæ—¶åœ¨çœŸå®äº¤äº’ä¸­åœ¨çº¿æ¨æ–­æ™ºèƒ½ä½“ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—æå‡å­¦ä¹ æ•ˆç‡ã€‚åœ¨ç¨æ”¶æ”¿ç­–è®¾è®¡(Tax Policy Design)ã€å›¢é˜Ÿåä½œ(Team Coordination)å’Œè®¾æ–½é€‰å€(Facility Location)ç­‰å¤šç§åœºæ™¯ä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒSWM-APåœ¨ç´¯ç§¯å¥–åŠ±å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰çš„åŸºäºæ¨¡å‹å’Œæ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ (RL)åŸºå‡†ã€‚è¯¥æ–¹æ³•é€šè¿‡ç²¾å‡†å»ºæ¨¡æ™ºèƒ½ä½“å¼‚æ„æ€§ï¼Œä¸ºåœ¨å¤æ‚ç¤¾ä¼šç³»ç»Ÿä¸­å®ç°ä¸ªä½“ä¸é›†ä½“åˆ©ç›Šçš„ä¸€è‡´æ€§æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19270v1",
      "published_date": "2025-10-22 06:01:21 UTC",
      "updated_date": "2025-10-22 06:01:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:45.173445+00:00"
    },
    {
      "arxiv_id": "2510.19264v1",
      "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
      "title_zh": "LAPRADï¼šå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„åè®®æ”»å‡»å‘ç°",
      "authors": [
        "R. Can Aygun",
        "Yehuda Afek",
        "Anat Bremler-Barr",
        "Leonard Kleinrock"
      ],
      "abstract": "With the goal of improving the security of Internet protocols, we seek faster, semi-automatic methods to discover new vulnerabilities in protocols such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers with some DNS knowledge to efficiently uncover vulnerabilities that would otherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM (GPT-o1) that has been trained on a broad corpus of DNS-related sources and previous DDoS attacks to identify potential exploits. In the second stage, a different LLM automatically constructs the corresponding attack configurations using the ReACT approach implemented via LangChain (DNS zone file generation). Finally, in the third stage, we validate the attack's functionality and effectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and rediscovered two recently reported ones that were not included in the LLM's training data. The first new attack employs a bait-and-switch technique to trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving capacity to as little as 6%. The second exploits large DNSSEC encryption algorithms (RSA-4096) with multiple keys, thereby bypassing a recently implemented default RRSet limit. The third leverages ANY-type responses to produce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush, circumvent existing patches, severely degrade resolver query capacity, and impact the latest versions of major DNS resolver implementations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LAPRADï¼Œä¸€ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©çš„åè®®æ”»å‡»å‘ç°æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯é«˜æ•ˆå‘ç°DNSç­‰äº’è”ç½‘åè®®ä¸­çš„å®‰å…¨æ¼æ´ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡GPT-o1è¯†åˆ«æ½œåœ¨æ¼æ´ï¼Œå¹¶ç»“åˆLangChainå’ŒReACTæ–¹æ³•è‡ªåŠ¨æ„å»ºæ”»å‡»æ‰€éœ€çš„DNS zone fileé…ç½®ï¼Œæœ€åå¯¹æ”»å‡»çš„æœ‰æ•ˆæ€§è¿›è¡ŒéªŒè¯ã€‚ç ”ç©¶åˆ©ç”¨LAPRADåœ¨DNSåè®®ä¸­å‘ç°äº†ä¸‰é¡¹æ–°å‹DDoSæ”»å‡»ï¼Œç»Ÿç§°ä¸ºSigCacheFlushï¼Œå¹¶é‡æ–°å‘ç°äº†ä¸¤é¡¹å·²è¢«æŠ¥é“çš„æ¼æ´ã€‚è¿™äº›æ”»å‡»é€šè¿‡è¯±éª—è§£æå™¨ç¼“å­˜å·¨é‡ä¼ªé€ DNSSEC RRSIGè®°å½•ã€åˆ©ç”¨RSA-4096åŠ å¯†ç®—æ³•ç»•è¿‡RRSeté™åˆ¶æˆ–åˆ©ç”¨ANYç±»å‹å“åº”ï¼Œä½¿è§£æå™¨çš„æœåŠ¡èƒ½åŠ›å¤§å¹…ä¸‹é™è‡³6%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”»å‡»å˜ä½“èƒ½æœ‰æ•ˆç»•è¿‡ç°æœ‰çš„å®‰å…¨è¡¥ä¸ï¼Œå¹¶å¯¹å½“å‰ä¸»æµDNSè§£æå™¨å®ç°äº§ç”Ÿä¸¥é‡çš„æ€§èƒ½å‰Šå¼±ï¼Œè¯æ˜äº†LLMåœ¨åè®®å®‰å…¨è‡ªåŠ¨åŒ–è¯„ä¼°ä¸­çš„æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.19264v1",
      "published_date": "2025-10-22 05:47:41 UTC",
      "updated_date": "2025-10-22 05:47:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:03:59.779538+00:00"
    },
    {
      "arxiv_id": "2510.19263v1",
      "title": "An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents",
      "title_zh": "é’ˆå¯¹åŒ…å«ä¸ä¸€è‡´åˆ¤ä¾‹çš„å¹¿ä¹‰ç†ç”±æ¨¡å‹çš„è®ºè¾©å¼è§£é‡Šæ¡†æ¶",
      "authors": [
        "Wachara Fungwacharakorn",
        "Gauvain Bourgne",
        "Ken Satoh"
      ],
      "abstract": "Precedential constraint is one foundation of case-based reasoning in AI and Law. It generally assumes that the underlying set of precedents must be consistent. To relax this assumption, a generalized notion of the reason model has been introduced. While several argumentative explanation approaches exist for reasoning with precedents based on the traditional consistent reason model, there has been no corresponding argumentative explanation method developed for this generalized reasoning framework accommodating inconsistent precedents. To address this question, this paper examines an extension of the derivation state argumentation framework (DSA-framework) to explain the reasoning according to the generalized notion of the reason model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIä¸æ³•å¾‹é¢†åŸŸä¸­æ¡ˆä¾‹æ¨ç†(Case-based reasoning)åœ¨å¤„ç†ä¸ä¸€è‡´å…ˆä¾‹æ—¶çš„è§£é‡Šæ€§é—®é¢˜ã€‚è™½ç„¶å¹¿ä¹‰ç†ç”±æ¨¡å‹(Generalized reason model)å·²ç»èƒ½å¤Ÿå¤„ç†ä¸ä¸€è‡´çš„å…ˆä¾‹(Inconsistent precedents)ï¼Œä½†æ­¤å‰å°šæ— é…å¥—çš„è®ºè¾©æ€§è§£é‡Š(Argumentative explanation)æ–¹æ³•ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡é€šè¿‡æ‰©å±•æ´¾ç”ŸçŠ¶æ€è®ºè¾©æ¡†æ¶(Derivation state argumentation framework, DSA-framework)ï¼Œæå‡ºäº†ä¸€ç§ä¸“é—¨ç”¨äºè§£é‡Šå¹¿ä¹‰ç†ç”±æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„æ–°æ¶æ„ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å­˜åœ¨å†²çªå…ˆä¾‹çš„å¤æ‚æ³•å¾‹åœºæ™¯ä¸‹å®ç°å¯è§£é‡Šçš„è‡ªåŠ¨æ¨ç†å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œæå‡äº†æ³•å¾‹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é€æ˜åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, extended version for JURIX 2025 submission",
      "pdf_url": "https://arxiv.org/pdf/2510.19263v1",
      "published_date": "2025-10-22 05:46:02 UTC",
      "updated_date": "2025-10-22 05:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:01.172215+00:00"
    },
    {
      "arxiv_id": "2510.19261v1",
      "title": "ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate",
      "title_zh": "ChatGPT æ­ç¤ºå…¶å±€é™ï¼šæ³•å¾‹åŸåˆ™çš„â€œå°†æ­»â€æ—¶åˆ»",
      "authors": [
        "Marianna Molinari",
        "Ilaria Angela Amantea",
        "Marinella Quaranta",
        "Guido Governatori"
      ],
      "abstract": "This study examines the performance of ChatGPT with an experiment in the legal domain. We compare the outcome with it a baseline using regular expressions (Regex), rather than focusing solely on the assessment against human performance. The study reveals that even if ChatGPT has access to the necessary knowledge and competencies, it is unable to assemble them, reason through, in a way that leads to an exhaustive result. This unveils a major limitation of ChatGPT. Intelligence encompasses the ability to break down complex issues and address them according to multiple required competencies, providing a unified and comprehensive solution. In the legal domain, one of the most crucial tasks is reading legal decisions and extracting key passages condensed from principles of law (PoLs), which are then incorporated into subsequent rulings by judges or defense documents by lawyers. In performing this task, artificial intelligence lacks an all-encompassing understanding and reasoning, which makes it inherently limited. Genuine intelligence, remains a uniquely human trait, at least in this particular field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† ChatGPT åœ¨æ³•å¾‹é¢†åŸŸçš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„æ³•å¾‹åŸåˆ™æå–ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚é€šè¿‡å°† ChatGPT ä¸åŸºäºæ­£åˆ™è¡¨è¾¾å¼ (Regex) çš„åŸºå‡†æ¨¡å‹è¿›è¡Œå¯¹æ¯”å®éªŒï¼Œç ”ç©¶è¯„ä¼°äº†å…¶åœ¨æ³•å¾‹åˆ¤å†³ä¹¦ä¸­æå–æµ“ç¼©æ³•å¾‹åŸåˆ™ (Principles of Law, PoLs) çš„èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡ ChatGPT æ‹¥æœ‰å¿…è¦çš„çŸ¥è¯†å’Œèƒ½åŠ›ï¼Œä½†æ— æ³•å°†å…¶æœ‰æ•ˆæ•´åˆå¹¶è¿›è¡Œè¯¦å°½æ¨ç†ä»¥å¾—å‡ºå…¨é¢ç»“æœã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ™ºèƒ½çš„æ ¸å¿ƒåœ¨äºå°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºå¤šé¡¹èƒ½åŠ›éœ€æ±‚å¹¶æä¾›ç»Ÿä¸€è§£å†³æ–¹æ¡ˆï¼Œè€Œ ChatGPT åœ¨è¿™ä¸€ç»´åº¦ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¸è¶³ã€‚åœ¨æå– PoLs è¿™ä¸€å…³é”®æ³•å¾‹ä»»åŠ¡ä¸­ï¼Œäººå·¥æ™ºèƒ½ç¼ºä¹å…¨å±€æ€§çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ï¼Œè¡¨æ˜åœ¨è¯¥ç‰¹å®šé¢†åŸŸï¼ŒçœŸæ­£çš„æ™ºèƒ½ä»æ˜¯äººç±»ç‹¬æœ‰çš„ç‰¹è´¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19261v1",
      "published_date": "2025-10-22 05:33:57 UTC",
      "updated_date": "2025-10-22 05:33:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:03.987046+00:00"
    },
    {
      "arxiv_id": "2510.19257v1",
      "title": "FnRGNN: Distribution-aware Fairness in Graph Neural Network",
      "title_zh": "FnRGNNï¼šå›¾ç¥ç»ç½‘ç»œä¸­çš„åˆ†å¸ƒæ„ŸçŸ¥å…¬å¹³æ€§",
      "authors": [
        "Soyoung Park",
        "Sungsu Lim"
      ],
      "abstract": "Graph Neural Networks (GNNs) excel at learning from structured data, yet fairness in regression tasks remains underexplored. Existing approaches mainly target classification and representation-level debiasing, which cannot fully address the continuous nature of node-level regression. We propose FnRGNN, a fairness-aware in-processing framework for GNN-based node regression that applies interventions at three levels: (i) structure-level edge reweighting, (ii) representation-level alignment via MMD, and (iii) prediction-level normalization through Sinkhorn-based distribution matching. This multi-level strategy ensures robust fairness under complex graph topologies. Experiments on four real-world datasets demonstrate that FnRGNN reduces group disparities without sacrificing performance. Code is available at https://github.com/sybeam27/FnRGNN.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FnRGNNï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNN)èŠ‚ç‚¹å›å½’ä»»åŠ¡çš„åˆ†å¸ƒæ„ŸçŸ¥å…¬å¹³æ€§æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å›å½’åœºæ™¯ä¸­è¿ç»­å˜é‡å»åä¸è¶³çš„é—®é¢˜ã€‚FnRGNNé€šè¿‡ä¸‰ä¸ªç»´åº¦çš„å¹²é¢„æªæ–½æ¥æ„å»ºå…¬å¹³æ¨¡å‹ï¼šåœ¨ç»“æ„å±‚é¢è¿›è¡Œè¾¹æƒé‡é‡ç½®(edge reweighting)ï¼Œåœ¨è¡¨ç¤ºå±‚é¢åˆ©ç”¨MMDè¿›è¡Œç‰¹å¾å¯¹é½ï¼Œå¹¶åœ¨é¢„æµ‹å±‚é¢é€šè¿‡åŸºäºSinkhornçš„åˆ†å¸ƒåŒ¹é…è¿›è¡Œæ ‡å‡†åŒ–ã€‚è¿™ç§å¤šå±‚çº§ç­–ç•¥ç¡®ä¿äº†ç®—æ³•åœ¨å¤æ‚å›¾æ‹“æ‰‘ç»“æ„ä¸‹çš„é²æ£’å…¬å¹³æ€§ã€‚å®éªŒåœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ŠéªŒè¯äº†FnRGNNçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½ç¾¤ä½“å·®å¼‚çš„åŒæ—¶ï¼Œä¿æŒäº†ä¼˜å¼‚çš„é¢„æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19257v1",
      "published_date": "2025-10-22 05:29:43 UTC",
      "updated_date": "2025-10-22 05:29:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:10.494805+00:00"
    },
    {
      "arxiv_id": "2510.23620v2",
      "title": "Genotype-Phenotype Integration through Machine Learning and Personalized Gene Regulatory Networks for Cancer Metastasis Prediction",
      "title_zh": "èåˆæœºå™¨å­¦ä¹ ä¸ä¸ªæ€§åŒ–åŸºå› è°ƒæ§ç½‘ç»œçš„åŸºå› å‹-è¡¨å‹æ•´åˆåŠå…¶åœ¨ç™Œç—‡è½¬ç§»é¢„æµ‹ä¸­çš„åº”ç”¨",
      "authors": [
        "Jiwei Fu",
        "Chunyu Yang"
      ],
      "abstract": "Metastasis is the leading cause of cancer-related mortality, yet most predictive models rely on shallow architectures and neglect patient-specific regulatory mechanisms. Here, we integrate classical machine learning and deep learning to predict metastatic potential across multiple cancer types. Gene expression profiles from the Cancer Cell Line Encyclopedia were combined with a transcription factor-target prior from DoRothEA, focusing on nine metastasis-associated regulators. After selecting differential genes using the Kruskal-Wallis test, ElasticNet, Random Forest, and XGBoost models were trained for benchmarking. Personalized gene regulatory networks were then constructed using PANDA and LIONESS and analyzed through a graph attention neural network (GATv2) to learn topological and expression-based representations. While XGBoost achieved the highest AUROC (0.7051), the GNN captured non-linear regulatory dependencies at the patient level. These results demonstrate that combining traditional machine learning with graph-based deep learning enables a scalable and interpretable framework for metastasis risk prediction in precision oncology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ•´åˆä¼ ç»Ÿæœºå™¨å­¦ä¹ (Machine Learning)ä¸æ·±åº¦å­¦ä¹ (Deep Learning)çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸ªæ€§åŒ–åŸºå› è°ƒæ§ç½‘ç»œé¢„æµ‹å¤šç§ç™Œç—‡çš„è½¬ç§»æ½œåŠ›ã€‚ç ”ç©¶ç»“åˆäº†Cancer Cell Line Encyclopediaçš„åŸºå› è¡¨è¾¾æ•°æ®ä¸DoRothEAçš„è½¬å½•å› å­é¶ç‚¹å…ˆéªŒä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨Kruskal-Wallisæ£€éªŒã€ElasticNetã€Random ForeståŠXGBoostè¿›è¡Œç‰¹å¾ç­›é€‰ä¸åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡PANDAå’ŒLIONESSæ„å»ºçš„ä¸ªæ€§åŒ–ç½‘ç»œç»“åˆå›¾æ³¨æ„åŠ›ç¥ç»ç½‘ç»œ(GATv2)ï¼Œæœ‰æ•ˆåœ°å­¦ä¹ äº†æ‚£è€…ç‰¹å¼‚æ€§çš„æ‹“æ‰‘å’Œè¡¨è¾¾è¡¨å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒXGBoostè¾¾åˆ°äº†0.7051çš„æœ€é«˜AUROCï¼ŒåŒæ—¶å›¾ç¥ç»ç½‘ç»œ(GNN)æˆåŠŸæ•è·äº†å¤æ‚çš„éçº¿æ€§è°ƒæ§ä¾èµ–å…³ç³»ã€‚è¯¥ç ”ç©¶ä¸ºç²¾å‡†è‚¿ç˜¤å­¦æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„è½¬ç§»é£é™©é¢„æµ‹æ¡†æ¶ï¼Œè¯æ˜äº†ç»“åˆä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸å›¾æ·±åº¦å­¦ä¹ çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "q-bio.OT",
        "cs.AI"
      ],
      "primary_category": "q-bio.OT",
      "comment": "39 pages, 14 figures. Preliminary version of ongoing collaborative research; a substantially revised manuscript is in preparation",
      "pdf_url": "https://arxiv.org/pdf/2510.23620v2",
      "published_date": "2025-10-22 05:20:13 UTC",
      "updated_date": "2025-12-25 07:09:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:13.571612+00:00"
    },
    {
      "arxiv_id": "2510.19245v1",
      "title": "See, Think, Act: Online Shopper Behavior Simulation with VLM Agents",
      "title_zh": "è§ã€æ€ã€è¡Œï¼šåŸºäº VLM æ™ºèƒ½ä½“çš„çº¿ä¸Šè´­ç‰©è€…è¡Œä¸ºæ¨¡æ‹Ÿ",
      "authors": [
        "Yimeng Zhang",
        "Jiri Gesi",
        "Ran Xue",
        "Tian Wang",
        "Ziyi Wang",
        "Yuxuan Lu",
        "Sinong Zhan",
        "Huimin Zeng",
        "Qingjun Cui",
        "Yufan Guo",
        "Jing Huang",
        "Mubarak Shah",
        "Dakuo Wang"
      ],
      "abstract": "LLMs have recently demonstrated strong potential in simulating online shopper behavior. Prior work has improved action prediction by applying SFT on action traces with LLM-generated rationales, and by leveraging RL to further enhance reasoning capabilities. Despite these advances, current approaches rely on text-based inputs and overlook the essential role of visual perception in shaping human decision-making during web GUI interactions. In this paper, we investigate the integration of visual information, specifically webpage screenshots, into behavior simulation via VLMs, leveraging OPeRA dataset. By grounding agent decision-making in both textual and visual modalities, we aim to narrow the gap between synthetic agents and real-world users, thereby enabling more cognitively aligned simulations of online shopping behavior. Specifically, we employ SFT for joint action prediction and rationale generation, conditioning on the full interaction context, which comprises action history, past HTML observations, and the current webpage screenshot. To further enhance reasoning capabilities, we integrate RL with a hierarchical reward structure, scaled by a difficulty-aware factor that prioritizes challenging decision points. Empirically, our studies show that incorporating visual grounding yields substantial gains: the combination of text and image inputs improves exact match accuracy by more than 6% over text-only inputs. These results indicate that multi-modal grounding not only boosts predictive accuracy but also enhances simulation fidelity in visually complex environments, which captures nuances of human attention and decision-making that text-only agents often miss. Finally, we revisit the design space of behavior simulation frameworks, identify key methodological limitations, and propose future research directions toward building efficient and effective human behavior simulators.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åœ¨çº¿è´­ç‰©è¡Œä¸ºæ¨¡æ‹Ÿä¸­æ•´åˆè§†è§‰ä¿¡æ¯çš„é‡è¦æ€§ï¼Œæå‡ºäº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å’ŒOPeRAæ•°æ®é›†çš„æ¨¡æ‹Ÿæ¡†æ¶ã€‚ä¸ºäº†ç¼©å°åˆæˆæ™ºèƒ½ä½“ä¸çœŸå®ç”¨æˆ·ä¹‹é—´çš„å·®è·ï¼Œç ”ç©¶é‡‡ç”¨äº†æœ‰ç›‘ç£å¾®è°ƒ(SFT)è¿›è¡Œè”åˆåŠ¨ä½œé¢„æµ‹å’Œæ¨ç†ç”Ÿæˆ(Rationale Generation)ï¼Œå¹¶å°†äº¤äº’å†å²ã€HTMLè§‚æµ‹ä¸å½“å‰çš„ç½‘é¡µæˆªå›¾ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œç ”ç©¶é›†æˆäº†å¸¦æœ‰å±‚æ¬¡åŒ–å¥–åŠ±ç»“æ„çš„å¼ºåŒ–å­¦ä¹ (RL)ï¼Œå¹¶é€šè¿‡éš¾åº¦æ„ŸçŸ¥å› å­(Difficulty-aware factor)ä¼˜å…ˆå¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„å†³ç­–ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆæ–‡æœ¬ä¸å›¾åƒçš„å¤šæ¨¡æ€è¾“å…¥æ¯”çº¯æ–‡æœ¬è¾“å…¥çš„å‡†ç¡®ç‡(Exact Match Accuracy)æå‡äº†è¶…è¿‡6%ã€‚è¿™è¯æ˜äº†å¤šæ¨¡æ€ grounding ä¸ä»…èƒ½æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¿˜èƒ½åœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸­æå‡æ¨¡æ‹Ÿä¿çœŸåº¦ï¼Œæœ‰æ•ˆæ•æ‰äººç±»æ³¨æ„åŠ›å’Œå†³ç­–çš„ç»†å¾®å·®åˆ«ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å®¡è§†äº†è¡Œä¸ºæ¨¡æ‹Ÿæ¡†æ¶çš„è®¾è®¡ç©ºé—´ï¼Œå¹¶ä¸ºæ„å»ºæ›´é«˜æ•ˆçš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿå™¨æå‡ºäº†ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19245v1",
      "published_date": "2025-10-22 05:07:14 UTC",
      "updated_date": "2025-10-22 05:07:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:15.278112+00:00"
    },
    {
      "arxiv_id": "2510.19241v1",
      "title": "SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes",
      "title_zh": "SPOTï¼šé¢å‘é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„åŸºäºæ ‘çš„å¯æ‰©å±•ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Xuyuan Xiong",
        "Pedro Chumpitaz-Flores",
        "Kaixun Hua",
        "Cheng Hua"
      ],
      "abstract": "Interpretable reinforcement learning policies are essential for high-stakes decision-making, yet optimizing decision tree policies in Markov Decision Processes (MDPs) remains challenging. We propose SPOT, a novel method for computing decision tree policies, which formulates the optimization problem as a mixed-integer linear program (MILP). To enhance efficiency, we employ a reduced-space branch-and-bound approach that decouples the MDP dynamics from tree-structure constraints, enabling efficient parallel search. This significantly improves runtime and scalability compared to previous methods. Our approach ensures that each iteration yields the optimal decision tree. Experimental results on standard benchmarks demonstrate that SPOT achieves substantial speedup and scales to larger MDPs with a significantly higher number of states. The resulting decision tree policies are interpretable and compact, maintaining transparency without compromising performance. These results demonstrate that our approach simultaneously achieves interpretability and scalability, delivering high-quality policies an order of magnitude faster than existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SPOTï¼Œä¸€ç§ç”¨äºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Processes, MDPs)çš„å¯æ‰©å±•å†³ç­–æ ‘ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚SPOTå°†ä¼˜åŒ–é—®é¢˜å»ºæ¨¡ä¸ºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(Mixed-Integer Linear Program, MILP)ï¼Œæœ‰æ•ˆè§£å†³äº†åœ¨MDPsä¸­ä¼˜åŒ–å¯è§£é‡Šå†³ç­–æ ‘ç­–ç•¥çš„éš¾é¢˜ã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§ç©ºé—´ç¼©å‡çš„åˆ†æ”¯å®šç•Œ(Reduced-Space Branch-and-Bound)æŠ€æœ¯ï¼Œé€šè¿‡å°†MDPåŠ¨åŠ›å­¦ä¸æ ‘ç»“æ„çº¦æŸè§£è€¦æ¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œæœç´¢ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSPOTåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†è¿è¡Œé€Ÿåº¦ï¼Œå¹¶èƒ½æ‰©å±•åˆ°çŠ¶æ€æ•°é‡æ›´å¤šçš„å¤æ‚MDPsã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSPOTåœ¨ä¿æŒå†³ç­–æ ‘ç­–ç•¥å¯è§£é‡Šæ€§å’Œç´§å‡‘æ€§çš„åŒæ—¶ï¼Œå°†ç”Ÿæˆé«˜è´¨é‡ç­–ç•¥çš„é€Ÿåº¦æå‡äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œå¹¶ç¡®ä¿äº†æ¯è½®è¿­ä»£éƒ½èƒ½äº§å‡ºæœ€ä¼˜å†³ç­–æ ‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19241v1",
      "published_date": "2025-10-22 04:57:23 UTC",
      "updated_date": "2025-10-22 04:57:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:14.990130+00:00"
    },
    {
      "arxiv_id": "2510.19212v1",
      "title": "No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence",
      "title_zh": "æ²¡æœ‰ç»Ÿè®¡å­¦å°±æ²¡æœ‰æ™ºèƒ½ï¼šäººå·¥æ™ºèƒ½çš„éšå½¢æ”¯æŸ±",
      "authors": [
        "Ernest FokouÃ©"
      ],
      "abstract": "The rapid ascent of artificial intelligence (AI) is often portrayed as a revolution born from computer science and engineering. This narrative, however, obscures a fundamental truth: the theoretical and methodological core of AI is, and has always been, statistical. This paper systematically argues that the field of statistics provides the indispensable foundation for machine learning and modern AI. We deconstruct AI into nine foundational pillars-Inference, Density Estimation, Sequential Learning, Generalization, Representation Learning, Interpretability, Causality, Optimization, and Unification-demonstrating that each is built upon century-old statistical principles. From the inferential frameworks of hypothesis testing and estimation that underpin model evaluation, to the density estimation roots of clustering and generative AI; from the time-series analysis inspiring recurrent networks to the causal models that promise true understanding, we trace an unbroken statistical lineage. While celebrating the computational engines that power modern AI, we contend that statistics provides the brain-the theoretical frameworks, uncertainty quantification, and inferential goals-while computer science provides the brawn-the scalable algorithms and hardware. Recognizing this statistical backbone is not merely an academic exercise, but a necessary step for developing more robust, interpretable, and trustworthy intelligent systems. We issue a call to action for education, research, and practice to re-embrace this statistical foundation. Ignoring these roots risks building a fragile future; embracing them is the path to truly intelligent machines. There is no machine learning without statistical learning; no artificial intelligence without statistical thought.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°è®ºè¯äº†ç»Ÿè®¡å­¦(Statistics)æ˜¯æœºå™¨å­¦ä¹ (Machine Learning)å’Œç°ä»£äººå·¥æ™ºèƒ½(AI)ä¸å¯æˆ–ç¼ºçš„ç†è®ºä¸æ–¹æ³•è®ºåŸºçŸ³ã€‚ä½œè€…å°†äººå·¥æ™ºèƒ½è§£æ„ä¸ºæ¨ç†(Inference)ã€å¯†åº¦ä¼°è®¡(Density Estimation)ã€æ³›åŒ–(Generalization)å’Œå› æœå…³ç³»(Causality)ç­‰ä¹å¤§æ”¯æŸ±ï¼Œå¹¶è¯¦ç»†å±•ç¤ºäº†æ¯ä¸€é¡¹å¦‚ä½•å»ºç«‹åœ¨æ·±åšçš„ç»Ÿè®¡å­¦åŸç†ä¹‹ä¸Šã€‚è®ºæ–‡æŒ‡å‡ºï¼Œç»Ÿè®¡å­¦ä¸ºäººå·¥æ™ºèƒ½æä¾›äº†åŒ…å«ä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification)å’Œæ¨ç†ç›®æ ‡åœ¨å†…çš„â€œå¤§è„‘â€æ¡†æ¶ï¼Œè€Œè®¡ç®—æœºç§‘å­¦åˆ™è´Ÿè´£æä¾›å¯æ‰©å±•ç®—æ³•ä¸ç¡¬ä»¶çš„â€œè‚Œè‚‰â€æ”¯æŒã€‚é€šè¿‡æ˜ç¡®è¿™ä¸€ç»Ÿè®¡ä¸»å¹²åœ°ä½ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†å…¶åœ¨å¼€å‘æ›´å…·é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’Œå¯ä¿¡èµ–çš„æ™ºèƒ½ç³»ç»Ÿä¸­çš„å¿…è¦æ€§ã€‚æ–‡ç« æœ€åå·å¬æ•™è‚²ä¸ç ”ç©¶é¢†åŸŸé‡æ–°å›å½’ç»Ÿè®¡å­¦æ ¹åŸºï¼Œå¹¶æ–­è¨€æ²¡æœ‰ç»Ÿè®¡æ€ç»´å°±æ²¡æœ‰çœŸæ­£çš„äººå·¥æ™ºèƒ½ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "37 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19212v1",
      "published_date": "2025-10-22 03:47:30 UTC",
      "updated_date": "2025-10-22 03:47:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:19.185660+00:00"
    },
    {
      "arxiv_id": "2510.21830v4",
      "title": "GAPO: Robust Advantage Estimation for Real-World Code LLMs",
      "title_zh": "GAPOï¼šé¢å‘ç°å®åœºæ™¯ä»£ç å¤§æ¨¡å‹çš„é²æ£’ä¼˜åŠ¿ä¼°è®¡",
      "authors": [
        "Jianqing Zhang",
        "Zhezheng Hao",
        "Wei Xia",
        "Hande Dong",
        "Hong Wang",
        "Chenxing Wei",
        "Yuyan Zhou",
        "Yubin Qi",
        "Qiang Lin",
        "Jian Cao"
      ],
      "abstract": "Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods, such as GRPO, are popular due to their critic-free and normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable noise, leading to distorted advantage computation and increased rollout outliers. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an interval with the highest SNR (Signal to Noise Ratio) per prompt and uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation to reduce noise further. This adaptive Q robustly handles rollout noise while remaining plug-and-play and efficient. We evaluate GAPO on nine instruction-tuned LLMs (3B-14B) using a collected large dataset of 51,844 real-world, history-aware code-editing tasks spanning 10 programming languages. GAPO yields up to 4.35 in-domain (ID) and 5.30 out-of-domain (OOD) exact-match improvements over GRPO and its variant DAPO, while achieving lower clipping ratios and higher GPU throughput. Code: https://github.com/TsingZ0/verl-GAPO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GAPO (Group Adaptive Policy Optimization)ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹  Reinforcement Learning (RL) åœ¨å¤§è¯­è¨€æ¨¡å‹ä»£ç ç¼–è¾‘ (code editing) åè®­ç»ƒé˜¶æ®µé¢ä¸´çš„å¥–åŠ±åˆ†å¸ƒå€¾æ–œå’Œä¸å¯é¢„æµ‹å™ªå£°é—®é¢˜ã€‚GAPO é€šè¿‡è‡ªé€‚åº”åœ°å¯»æ‰¾æ¯ä¸ª Prompt å…·æœ‰æœ€é«˜ SNR (Signal to Noise Ratio) çš„åŒºé—´ï¼Œå¹¶é‡‡ç”¨è¯¥åŒºé—´çš„ä¸­ä½æ•°ä½œä¸ºè‡ªé€‚åº” Q å€¼æ¥æ›¿ä»£ GRPO ä¸­çš„ç»„å‡å€¼ï¼Œä»è€Œå®ç°æ›´ç¨³å¥çš„ Advantage Estimationã€‚è¯¥æ–¹æ³•å…·å¤‡å³æ’å³ç”¨å’Œé«˜æ•ˆçš„ç‰¹æ€§ï¼Œåœ¨åŒ…å« 10 ç§ç¼–ç¨‹è¯­è¨€ã€è¶…è¿‡ 5 ä¸‡ä¸ªçœŸå®ä»£ç ç¼–è¾‘ä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒGAPO åœ¨ 9 ä¸ªä¸åŒè§„æ¨¡çš„æŒ‡ä»¤å¾®è°ƒ LLMs ä¸Šç›¸æ¯” GRPO åŠå…¶å˜ä½“ DAPO æ˜¾è‘—æå‡äº†åŸŸå†…ä¸åŸŸå¤–çš„ Exact-Match å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½ Clipping Ratios çš„åŒæ—¶è¿˜æé«˜äº† GPU Throughputï¼Œä¸ºç°å®åœºæ™¯ä¸‹çš„ä»£ç æ¨¡å‹ä¼˜åŒ–æä¾›äº†é«˜æ•ˆä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.21830v4",
      "published_date": "2025-10-22 03:37:49 UTC",
      "updated_date": "2026-01-08 08:42:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:25.414732+00:00"
    },
    {
      "arxiv_id": "2510.19205v1",
      "title": "WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation",
      "title_zh": "WebGraphEvalï¼šåŸºäºå›¾è¡¨ç¤ºçš„ Web æ™ºèƒ½ä½“å¤šè½®è½¨è¿¹è¯„ä¼°",
      "authors": [
        "Yaoyao Qian",
        "Yuanli Wang",
        "Jinda Zhang",
        "Yun Zong",
        "Meixu Chen",
        "Hanhan Zhou",
        "Jindan Huang",
        "Yifan Zeng",
        "Xinyu Hu",
        "Chan Hee Song",
        "Danqing Zhang"
      ],
      "abstract": "Current evaluation of web agents largely reduces to binary success metrics or conformity to a single reference trajectory, ignoring the structural diversity present in benchmark datasets. We present WebGraphEval, a framework that abstracts trajectories from multiple agents into a unified, weighted action graph. This representation is directly compatible with benchmarks such as WebArena, leveraging leaderboard runs and newly collected trajectories without modifying environments. The framework canonically encodes actions, merges recurring behaviors, and applies structural analyses including reward propagation and success-weighted edge statistics. Evaluations across thousands of trajectories from six web agents show that the graph abstraction captures cross-model regularities, highlights redundancy and inefficiency, and identifies critical decision points overlooked by outcome-based metrics. By framing web interaction as graph-structured data, WebGraphEval establishes a general methodology for multi-path, cross-agent, and efficiency-aware evaluation of web agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰ Web Agents è¯„ä¼°è¿‡äºä¾èµ–äºŒå…ƒæˆåŠŸæŒ‡æ ‡æˆ–å•ä¸€å‚è€ƒè½¨è¿¹ã€å¿½è§†åŸºå‡†æ•°æ®é›†ç»“æ„å¤šæ ·æ€§çš„é—®é¢˜ï¼Œæå‡ºäº† WebGraphEval è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¥è‡ªå¤šä¸ªæ™ºèƒ½ä½“çš„è½¨è¿¹æŠ½è±¡ä¸ºä¸€ä¸ªç»Ÿä¸€çš„åŠ æƒåŠ¨ä½œå›¾ï¼ˆaction graphï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥å…¼å®¹ WebArena ç­‰åŸºå‡†æµ‹è¯•ï¼Œæ— éœ€ä¿®æ”¹ç°æœ‰ç¯å¢ƒã€‚é€šè¿‡å¯¹åŠ¨ä½œè¿›è¡Œè§„èŒƒåŒ–ç¼–ç å¹¶åˆå¹¶é‡å¤è¡Œä¸ºï¼ŒWebGraphEval åº”ç”¨äº†å¥–åŠ±ä¼ æ’­ï¼ˆreward propagationï¼‰å’ŒæˆåŠŸåŠ æƒè¾¹ç¼˜ç»Ÿè®¡ï¼ˆsuccess-weighted edge statisticsï¼‰ç­‰ç»“æ„åŒ–åˆ†ææ‰‹æ®µã€‚åœ¨å¯¹å…­ä¸ª Web Agents çš„æ•°åƒæ¡è½¨è¿¹è¿›è¡Œè¯„ä¼°åï¼Œç»“æœæ˜¾ç¤ºè¯¥å›¾æŠ½è±¡æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è·¨æ¨¡å‹çš„è§„å¾‹æ€§ã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½è¯†åˆ«å‡ºä¼ ç»Ÿç»“æœå¯¼å‘å‹æŒ‡æ ‡ï¼ˆoutcome-based metricsï¼‰å®¹æ˜“å¿½è§†çš„å†—ä½™ä¸ä½æ•ˆé—®é¢˜ï¼Œè¿˜èƒ½å‡†ç¡®å®šä½å…³é”®å†³ç­–ç‚¹ã€‚é€šè¿‡å°†ç½‘é¡µäº¤äº’å»ºæ¨¡ä¸ºå›¾ç»“æ„æ•°æ®ï¼ŒWebGraphEval ä¸º Web Agents çš„å¤šè·¯å¾„ã€è·¨æ™ºèƒ½ä½“åŠæ•ˆç‡æ„ŸçŸ¥è¯„ä¼°æä¾›äº†ä¸€å¥—é€šç”¨çš„æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Multi-Turn Interactions in Large Language Models",
      "pdf_url": "https://arxiv.org/pdf/2510.19205v1",
      "published_date": "2025-10-22 03:29:25 UTC",
      "updated_date": "2025-10-22 03:29:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:27.574259+00:00"
    },
    {
      "arxiv_id": "2510.19202v1",
      "title": "An Active Diffusion Neural Network for Graphs",
      "title_zh": "é¢å‘å›¾çš„ä¸»åŠ¨æ‰©æ•£ç¥ç»ç½‘ç»œ",
      "authors": [
        "Mengying Jiang"
      ],
      "abstract": "The analogy to heat diffusion has enhanced our understanding of information flow in graphs and inspired the development of Graph Neural Networks (GNNs). However, most diffusion-based GNNs emulate passive heat diffusion, which still suffers from over-smoothing and limits their ability to capture global graph information. Inspired by the heat death of the universe, which posits that energy distribution becomes uniform over time in a closed system, we recognize that, without external input, node representations in a graph converge to identical feature vectors as diffusion progresses. To address this issue, we propose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achieves active diffusion by integrating multiple external information sources that dynamically influence the diffusion process, effectively overcoming the over-smoothing problem. Furthermore, our approach realizes true infinite diffusion by directly calculating the closed-form solution of the active diffusion iterative formula. This allows nodes to preserve their unique characteristics while efficiently gaining comprehensive insights into the graph's global structure. We evaluate ADGNN against several state-of-the-art GNN models across various graph tasks. The results demonstrate that ADGNN significantly improves both accuracy and efficiency, highlighting its effectiveness in capturing global graph information and maintaining node distinctiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸåŸºäºæ‰©æ•£çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å› æ¨¡æ‹Ÿè¢«åŠ¨çƒ­æ‰©æ•£ï¼ˆpassive heat diffusionï¼‰è€Œå¯¼è‡´çš„è¿‡åº¦å¹³æ»‘ï¼ˆover-smoothingï¼‰åŠå…¨å±€ä¿¡æ¯æ•æ‰å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸»åŠ¨æ‰©æ•£å›¾ç¥ç»ç½‘ç»œï¼ˆADGNNï¼‰ã€‚å—å®‡å®™çƒ­å¯‚ç†è®ºå¯å‘ï¼ŒADGNN é€šè¿‡æ•´åˆå¤šä¸ªåŠ¨æ€å½±å“æ‰©æ•£è¿‡ç¨‹çš„å¤–éƒ¨ä¿¡æ¯æºæ¥å®ç°ä¸»åŠ¨æ‰©æ•£ï¼Œæœ‰æ•ˆåœ°å…‹æœäº†ç‰¹å¾è¶‹åŒçš„å¼Šç«¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç›´æ¥è®¡ç®—ä¸»åŠ¨æ‰©æ•£è¿­ä»£å…¬å¼çš„è§£æè§£ï¼ˆclosed-form solutionï¼‰å®ç°äº†çœŸæ­£çš„æ— é™æ‰©æ•£ï¼ˆinfinite diffusionï¼‰ï¼Œä½¿èŠ‚ç‚¹åœ¨é«˜æ•ˆè·å–å…¨å±€ç»“æ„ä¿¡æ¯çš„åŒæ—¶èƒ½å¤Ÿä¿ç•™å…¶ç‹¬æœ‰çš„ç‰¹å¾ã€‚åœ¨å¤šé¡¹å›¾ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒADGNN åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ï¼Œå……åˆ†å±•ç°äº†å…¶åœ¨å¤„ç†å›¾å…¨å±€ä¿¡æ¯å’Œç»´æŒèŠ‚ç‚¹å·®å¼‚æ€§æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19202v1",
      "published_date": "2025-10-22 03:23:08 UTC",
      "updated_date": "2025-10-22 03:23:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:43.179070+00:00"
    },
    {
      "arxiv_id": "2510.19195v3",
      "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks",
      "title_zh": "é‡æ–°æ€è€ƒé©¾é©¶ä¸–ç•Œæ¨¡å‹ï¼šä½œä¸ºæ„ŸçŸ¥ä»»åŠ¡çš„åˆæˆæ•°æ®ç”Ÿæˆå™¨",
      "authors": [
        "Kai Zeng",
        "Zhanqian Wu",
        "Kaixin Xiong",
        "Xiaobao Wei",
        "Xiangyu Guo",
        "Zhenxin Zhu",
        "Kalok Ho",
        "Lijun Zhou",
        "Bohan Zeng",
        "Ming Lu",
        "Haiyang Sun",
        "Bing Wang",
        "Guang Chen",
        "Hangjun Ye",
        "Wentao Zhang"
      ],
      "abstract": "Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹(Driving World Models)åœ¨ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡(Perception Tasks)è¯„ä¼°æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†æ—¨åœ¨å¢å¼ºæ„ŸçŸ¥æ€§èƒ½çš„Dream4Driveæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è¾“å…¥è§†é¢‘åˆ†è§£ä¸ºå¤šå¼ 3Dæ„ŸçŸ¥å¼•å¯¼å›¾(3D-aware guidance maps)ï¼Œé€šè¿‡å°†3Dèµ„äº§æ¸²æŸ“è‡³å¼•å¯¼å›¾å¹¶å¾®è°ƒä¸–ç•Œæ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ã€å¯ç¼–è¾‘çš„å¤šè§†å›¾é€¼çœŸè§†é¢‘ã€‚Dream4Driveå®ç°äº†å¤§è§„æ¨¡ç”Ÿæˆå¤šè§†å›¾è¾¹ç¼˜æ¡ˆä¾‹(Corner Cases)çš„çµæ´»æ€§ï¼Œæ˜¾è‘—æå‡äº†æ„ŸçŸ¥æ¨¡å‹åœ¨æç«¯åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜è´¡çŒ®äº†ä¸€ä¸ªåä¸ºDriveObj3Dçš„å¤§è§„æ¨¡3Dèµ„äº§æ•°æ®é›†ï¼Œä»¥æ”¯æŒå¤šæ ·åŒ–çš„3Dæ„ŸçŸ¥è§†é¢‘ç¼–è¾‘ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDream4Driveåœ¨å¤šç§è®­ç»ƒè®¾ç½®ä¸‹å‡èƒ½æœ‰æ•ˆæå‡ä¸‹æ¸¸æ„ŸçŸ¥æ¨¡å‹çš„è¡¨ç°ï¼Œä¸ºåˆæˆæ•°æ®(Synthetic Data)åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„æœ‰æ•ˆåº”ç”¨æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19195v3",
      "published_date": "2025-10-22 03:02:38 UTC",
      "updated_date": "2025-12-11 06:46:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:44.173271+00:00"
    },
    {
      "arxiv_id": "2510.19866v1",
      "title": "An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics",
      "title_zh": "é«˜ä¸­ç‰©ç† AI ç”Ÿæˆæ•™æ¡ˆåœ¨ä¸åŒæ¨¡å‹ä¸æç¤ºæ¡†æ¶ä¸‹çš„æ•™å­¦ç§‘å­¦æ€§ä¸å¯ç”¨æ€§è¯„ä¼°",
      "authors": [
        "Xincheng Liu"
      ],
      "abstract": "This study evaluates the pedagogical soundness and usability of AI-generated lesson plans across five leading large language models: ChatGPT (GPT-5), Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice, three structured prompt frameworks were tested: TAG (Task, Audience, Goal), RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective, Style, Tone, Audience, Response Format).\n  Fifteen lesson plans were generated for a single high-school physics topic, The Electromagnetic Spectrum. The lesson plans were analyzed through four automated computational metrics: (1) readability and linguistic complexity, (2) factual accuracy and hallucination detection, (3) standards and curriculum alignment, and (4) cognitive demand of learning objectives.\n  Results indicate that model selection exerted the strongest influence on linguistic accessibility, with DeepSeek producing the most readable teaching plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).\n  The prompt framework structure most strongly affected the factual accuracy and pedagogical completeness, with the RACE framework yielding the lowest hallucination index and the highest incidental alignment with NGSS curriculum standards. Across all models, the learning objectives in the fifteen lesson plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There were limited higher-order verbs in the learning objectives extracted.\n  Overall, the findings suggest that readability is significantly governed by model design, while instructional reliability and curricular alignment depend more on the prompt framework. The most effective configuration for lesson plans identified in the results was to combine a readability-optimized model with the RACE framework and an explicit checklist of physics concepts, curriculum standards, and higher-order objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åœ¨é«˜ä¸­ç‰©ç†æ•™å­¦ä¸­ï¼Œä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å’Œæç¤ºæ¡†æ¶(prompt frameworks)ç”Ÿæˆæ•™æ¡ˆçš„æ•™è‚²åˆç†æ€§ä¸å¯ç”¨æ€§ã€‚å®éªŒå¯¹æ¯”äº†ChatGPT (GPT-5)ã€Claude Sonnet 4.5ã€Gemini 2.5 Flashã€DeepSeek V3.2å’ŒGrok 4äº”ç§æ¨¡å‹ï¼Œä»¥åŠTAGã€RACEå’ŒCOSTARä¸‰ç§ç»“æ„åŒ–æç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹é€‰æ‹©å¯¹è¯­è¨€å¯è¯»æ€§å½±å“æœ€å¤§ï¼Œå…¶ä¸­DeepSeekç”Ÿæˆçš„æ•™æ¡ˆæœ€æ˜“è¯»ï¼Œè€Œæç¤ºæ¡†æ¶å¯¹äº‹å®å‡†ç¡®æ€§å’Œæ•™è‚²å®Œæ•´æ€§å½±å“æ›´ä¸ºæ˜¾è‘—ï¼ŒRACEæ¡†æ¶åœ¨å‡å°‘å¹»è§‰å’ŒNGSSè¯¾ç¨‹æ ‡å‡†å¯¹é½æ–¹é¢è¡¨ç°æœ€ä½³ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”Ÿæˆçš„æ•™æ¡ˆåœ¨Bloom's taxonomyä¸­å¤šé›†ä¸­äºè®°å¿†å’Œç†è§£å±‚çº§ï¼Œç¼ºä¹é«˜é˜¶è®¤çŸ¥ç›®æ ‡ã€‚ç»“è®ºè®¤ä¸ºï¼Œæ•™æ¡ˆçš„å¯è¯»æ€§ç”±æ¨¡å‹è®¾è®¡å†³å®šï¼Œè€Œæ•™å­¦å¯é æ€§åˆ™ä¾èµ–äºæç¤ºæ¡†æ¶ã€‚ç ”ç©¶å»ºè®®æœ€æœ‰æ•ˆçš„æ•™æ¡ˆç”Ÿæˆé…ç½®åº”ç»“åˆå¯è¯»æ€§ä¼˜åŒ–æ¨¡å‹ã€RACEæ¡†æ¶ä»¥åŠåŒ…å«ç‰©ç†æ¦‚å¿µå’Œé«˜é˜¶ç›®æ ‡çš„æ˜ç¡®æ¸…å•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.19866v1",
      "published_date": "2025-10-22 02:53:06 UTC",
      "updated_date": "2025-10-22 02:53:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:47.173446+00:00"
    },
    {
      "arxiv_id": "2510.19183v1",
      "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning",
      "title_zh": "PruneHalï¼šé€šè¿‡è‡ªé€‚åº” KV ç¼“å­˜å‰ªæç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰",
      "authors": [
        "Fengyuan Sun",
        "Hui Chen",
        "Xinhao Xu",
        "Dandan Zheng",
        "Jingdong Chen",
        "Jun Zhou",
        "Jungong Han",
        "Guiguang Ding"
      ],
      "abstract": "While multi-modal large language models (MLLMs) have made significant progress in recent years, the issue of hallucinations remains a major challenge. To mitigate this phenomenon, existing solutions either introduce additional data for further training or incorporate external or internal information during inference. However, these approaches inevitably introduce extra computational costs. In this paper, we observe that hallucinations in MLLMs are strongly associated with insufficient attention allocated to visual tokens. In particular, the presence of redundant visual tokens disperses the model's attention, preventing it from focusing on the most informative ones. As a result, critical visual cues are often under-attended, which in turn exacerbates the occurrence of hallucinations. Building on this observation, we propose \\textbf{PruneHal}, a training-free, simple yet effective method that leverages adaptive KV cache pruning to enhance the model's focus on critical visual information, thereby mitigating hallucinations. To the best of our knowledge, we are the first to apply token pruning for hallucination mitigation in MLLMs. Notably, our method don't require additional training and incurs nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be seamlessly integrated with different decoding strategies, including those specifically designed for hallucination mitigation. We evaluate PruneHal on several widely used hallucination evaluation benchmarks using four mainstream MLLMs, achieving robust and outstanding results that highlight the effectiveness and superiority of our method. Our code will be publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ä¸­æ™®éå­˜åœ¨çš„å¹»è§‰ (Hallucinations) é—®é¢˜å±•å¼€æ¢è®¨ï¼ŒæŒ‡å‡ºå†—ä½™çš„è§†è§‰ Token ä¼šåˆ†æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å¹¶å¯¼è‡´å…³é”®è§†è§‰çº¿ç´¢è¢«å¿½è§†ï¼Œä»è€Œè¯±å‘å¹»è§‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† PruneHalï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒä¸”ç®€å•æœ‰æ•ˆçš„è‡ªé€‚åº” KV cache å‰ªæ (Adaptive KV Cache Pruning) æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹å…³é”®è§†è§‰ä¿¡æ¯çš„å…³æ³¨ã€‚ä½œä¸ºé¦–ä¸ªå°† Token å‰ªæåº”ç”¨äºç¼“è§£ MLLMs å¹»è§‰çš„ç ”ç©¶ï¼ŒPruneHal å…·æœ‰æ¨¡å‹æ— å…³æ€§ (Model-agnostic)ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ä¸åŒçš„è§£ç ç­–ç•¥ä¸­ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”å‡ ä¹ä¸å¢åŠ æ¨ç†æˆæœ¬ã€‚å®éªŒåœ¨å››ç§ä¸»æµ MLLMs å’Œå¤šä¸ªå¸¸ç”¨çš„å¹»è§‰è¯„ä¼°åŸºå‡†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜ PruneHal åœ¨ç¼“è§£å¹»è§‰æ–¹é¢è¡¨ç°å‡ºç¨³å¥ä¸”å“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡å¤šæ¨¡æ€æ¨¡å‹å¯é æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19183v1",
      "published_date": "2025-10-22 02:41:07 UTC",
      "updated_date": "2025-10-22 02:41:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:48.585327+00:00"
    },
    {
      "arxiv_id": "2510.19181v1",
      "title": "Interpretable Question Answering with Knowledge Graphs",
      "title_zh": "åŸºäºçŸ¥è¯†å›¾è°±çš„å¯è§£é‡Šé—®ç­”",
      "authors": [
        "Kartikeya Aneja",
        "Manasvi Srivastava",
        "Subhayan Das",
        "Nagender Aneja"
      ],
      "abstract": "This paper presents a question answering system that operates exclusively on a knowledge graph retrieval without relying on retrieval augmented generation (RAG) with large language models (LLMs). Instead, a small paraphraser model is used to paraphrase the entity relationship edges retrieved from querying the knowledge graph. The proposed pipeline is divided into two main stages. The first stage involves pre-processing a document to generate sets of question-answer (QA) pairs. The second stage converts these QAs into a knowledge graph from which graph-based retrieval is performed using embeddings and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to generate a final answer. This work includes an evaluation using LLM-as-a-judge on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using LLAMA-3.2 and GPT-3.5-Turbo, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Knowledge Graph æ£€ç´¢çš„å¯è§£é‡Šé—®ç­”ç³»ç»Ÿï¼Œå®Œå…¨ä¾èµ–å›¾æ£€ç´¢è€Œæ— éœ€ä¾èµ–å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æŠ€æœ¯ã€‚è¯¥ç³»ç»Ÿçš„æµæ°´çº¿åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡é¢„å¤„ç†æ–‡æ¡£ç”Ÿæˆä¸€ç³»åˆ—é—®é¢˜-ç­”æ¡ˆ (QA) å¯¹ï¼Œéšåå°†è¿™äº›æ•°æ®è½¬åŒ–ä¸º Knowledge Graphï¼Œå¹¶ç»“åˆåµŒå…¥ (embeddings) ä¸æ¨¡ç³ŠæŠ€æœ¯ (fuzzy techniques) æ‰§è¡Œå›¾è·¯å¾„çš„æ£€ç´¢ä¸é‡æ’åºã€‚ä¸ºäº†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œç³»ç»Ÿé‡‡ç”¨äº†ä¸€ä¸ªå°å‹çš„æ”¹å†™æ¨¡å‹ (paraphraser model) å¯¹æ£€ç´¢åˆ°çš„å®ä½“å…³ç³»è¾¹è¿›è¡Œå¤„ç†ï¼Œç¡®ä¿äº†è¾“å‡ºçš„è¿è´¯æ€§ã€‚åœ¨ CRAG åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ LLM-as-a-judge æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ LLAMA-3.2 å’Œ GPT-3.5-Turbo ä¸Šåˆ†åˆ«è¾¾åˆ°äº† 71.9% å’Œ 54.4% çš„å‡†ç¡®ç‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡çº¯ç²¹çš„ Knowledge Graph ç»“æ„å³å¯å®ç°é«˜æ•ˆä¸”å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„é—®ç­”æµç¨‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19181v1",
      "published_date": "2025-10-22 02:36:35 UTC",
      "updated_date": "2025-10-22 02:36:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:51.871530+00:00"
    },
    {
      "arxiv_id": "2510.19178v2",
      "title": "Imbalanced Gradients in RL Post-Training of Multi-Task LLMs",
      "title_zh": "å¤šä»»åŠ¡å¤§è¯­è¨€æ¨¡å‹ RL åè®­ç»ƒä¸­çš„æ¢¯åº¦å¤±è¡¡",
      "authors": [
        "Runzhe Wu",
        "Ankur Samanta",
        "Ayush Jain",
        "Scott Fujimoto",
        "Jeongyeol Kwon",
        "Ben Kretzu",
        "Youliang Yu",
        "Kaveh Hassani",
        "Boris Vidolov",
        "Yonathan Efroni"
      ],
      "abstract": "Multi-task post-training of large language models (LLMs) is typically performed by mixing datasets from different tasks and optimizing them jointly. This approach implicitly assumes that all tasks contribute gradients of similar magnitudes; when this assumption fails, optimization becomes biased toward large-gradient tasks. In this paper, however, we show that this assumption fails in RL post-training: certain tasks produce significantly larger gradients, thus biasing updates toward those tasks. Such gradient imbalance would be justified only if larger gradients implied larger learning gains on the tasks (i.e., larger performance improvements) -- but we find this is not true. Large-gradient tasks can achieve similar or even much lower learning gains than small-gradient ones. Further analyses reveal that these gradient imbalances cannot be explained by typical training statistics such as training rewards or advantages, suggesting that they arise from the inherent differences between tasks. This cautions against naive dataset mixing and calls for future work on principled gradient-level corrections for LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šä»»åŠ¡å¤§è¯­è¨€æ¨¡å‹ (Multi-Task LLMs) åœ¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒ (RL Post-Training) é˜¶æ®µå­˜åœ¨çš„æ¢¯åº¦å¤±è¡¡ (Imbalanced Gradients) é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„è”åˆä¼˜åŒ–æ–¹æ³•é€šå¸¸å‡è®¾æ‰€æœ‰ä»»åŠ¡äº§ç”Ÿçš„æ¢¯åº¦é‡çº§ç›¸ä¼¼ï¼Œä½†åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼ŒæŸäº›ä»»åŠ¡ä¼šäº§ç”Ÿæ˜¾è‘—æ›´å¤§çš„æ¢¯åº¦ï¼Œä»è€Œå¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹äº§ç”Ÿåå·®ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒå¤§çš„æ¢¯åº¦å¹¶ä¸æ„å‘³ç€æ›´é«˜çš„å­¦ä¹ æ”¶ç›Š (learning gains)ï¼Œå°æ¢¯åº¦ä»»åŠ¡æœ‰æ—¶åè€Œèƒ½è·å¾—ç›¸ä¼¼ç”šè‡³æ›´é«˜çš„æ€§èƒ½æå‡ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ¢¯åº¦å¤±è¡¡æ— æ³•é€šè¿‡å¥–åŠ± (rewards) æˆ–ä¼˜åŠ¿ (advantages) ç­‰å¸¸è§„è®­ç»ƒç»Ÿè®¡æ•°æ®æ¥è§£é‡Šï¼Œè€Œæ˜¯æºäºä»»åŠ¡ä¹‹é—´çš„å›ºæœ‰å·®å¼‚ã€‚è¯¥ç ”ç©¶è­¦ç¤ºäº†ç®€å•çš„å¤šä»»åŠ¡æ•°æ®é›†æ··åˆ (naive dataset mixing) å­˜åœ¨çš„é£é™©ï¼Œå¹¶å‘¼åæœªæ¥å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸåˆ™æ€§æ¢¯åº¦ä¿®æ­£ (gradient-level corrections)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19178v2",
      "published_date": "2025-10-22 02:35:27 UTC",
      "updated_date": "2025-10-26 15:22:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:04:55.981903+00:00"
    },
    {
      "arxiv_id": "2510.19176v1",
      "title": "The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models",
      "title_zh": "é›¶æ­¥æ€ç»´ï¼šå…³äºå°†æ¨¡å¼é€‰æ‹©ä½œä¸ºæ¨ç†æ¨¡å‹ä¸­æ›´å…·æŒ‘æˆ˜æ€§çš„â€œæå‰é€€å‡ºâ€é—®é¢˜çš„å®è¯ç ”ç©¶",
      "authors": [
        "Yuqiao Tan",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Reasoning models have demonstrated exceptional performance in tasks such as mathematics and logical reasoning, primarily due to their ability to engage in step-by-step thinking during the reasoning process. However, this often leads to overthinking, resulting in unnecessary computational overhead. To address this issue, Mode Selection aims to automatically decide between Long-CoT (Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking mode. Simultaneously, Early Exit determines the optimal stopping point during the iterative reasoning process. Both methods seek to reduce the computational burden. In this paper, we first identify Mode Selection as a more challenging variant of the Early Exit problem, as they share similar objectives but differ in decision timing. While Early Exit focuses on determining the best stopping point for concise reasoning at inference time, Mode Selection must make this decision at the beginning of the reasoning process, relying on pre-defined fake thoughts without engaging in an explicit reasoning process, referred to as zero-step thinking. Through empirical studies on nine baselines, we observe that prompt-based approaches often fail due to their limited classification capabilities when provided with minimal hand-crafted information. In contrast, approaches that leverage internal information generally perform better across most scenarios but still exhibit issues with stability. Our findings indicate that existing methods relying solely on the information provided by models are insufficient for effectively addressing Mode Selection in scenarios with limited information, highlighting the ongoing challenges of this task. Our code is available at https://github.com/Trae1ounG/Zero_Step_Thinking.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨ç†æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­å› è¿‡åº¦æ€è€ƒï¼ˆoverthinkingï¼‰äº§ç”Ÿçš„è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæ¢è®¨äº† Mode Selection ä¸ Early Exit ä¸¤ç§æ•ˆç‡ä¼˜åŒ–æ–¹æ¡ˆã€‚ä½œè€…æå‡ºå°† Mode Selection è§†ä¸ºä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§çš„ Early Exit å˜ä½“ï¼Œå…¶æ ¸å¿ƒåœ¨äºæ¨ç†å¼€å§‹å‰çš„é›¶æ­¥æ€è€ƒï¼ˆzero-step thinkingï¼‰ï¼Œå³åœ¨ä¸è¿›å…¥æ˜¾å¼æ¨ç†è¿‡ç¨‹çš„æƒ…å†µä¸‹é¢„åˆ¤åº”é€‰æ‹© Long-CoT è¿˜æ˜¯ Short-CoT æ¨¡å¼ã€‚é€šè¿‡å¯¹ä¹ç§åŸºçº¿æ¨¡å‹çš„å®è¯ç ”ç©¶ï¼Œè®ºæ–‡æŒ‡å‡ºåŸºäºæç¤ºè¯ï¼ˆprompt-basedï¼‰çš„æ–¹æ³•ç”±äºåˆ†ç±»èƒ½åŠ›å—é™æ•ˆæœä¸ä½³ï¼Œè€Œåˆ©ç”¨æ¨¡å‹å†…éƒ¨ä¿¡æ¯çš„æ–¹æ³•è™½è¡¨ç°æ›´ä¼˜ä½†ç¨³å®šæ€§ä»æ˜¾ä¸è¶³ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¿¡æ¯å—é™çš„ Mode Selection åœºæ™¯æ—¶ä»æœ‰å±€é™æ€§ï¼Œå‡¸æ˜¾äº†å®ç°é«˜æ•ˆæ¨ç†å†³ç­–çš„æŒç»­æŒ‘æˆ˜ã€‚ç›®å‰è¯¥ç ”ç©¶çš„ç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NeurIPS'25 Efficient Reasoning Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.19176v1",
      "published_date": "2025-10-22 02:28:10 UTC",
      "updated_date": "2025-10-22 02:28:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:05:03.889423+00:00"
    },
    {
      "arxiv_id": "2510.19173v1",
      "title": "News-Aware Direct Reinforcement Trading for Financial Markets",
      "title_zh": "é¢å‘é‡‘èå¸‚åœºçš„æ–°é—»æ„ŸçŸ¥ç›´æ¥å¼ºåŒ–å­¦ä¹ äº¤æ˜“",
      "authors": [
        "Qing-Yu Lan",
        "Zhan-He Wang",
        "Jun-Qian Jiang",
        "Yu-Tong Wang",
        "Yun-Song Piao"
      ],
      "abstract": "The financial market is known to be highly sensitive to news. Therefore, effectively incorporating news data into quantitative trading remains an important challenge. Existing approaches typically rely on manually designed rules and/or handcrafted features. In this work, we directly use the news sentiment scores derived from large language models, together with raw price and volume data, as observable inputs for reinforcement learning. These inputs are processed by sequence models such as recurrent neural networks or Transformers to make end-to-end trading decisions. We conduct experiments using the cryptocurrency market as an example and evaluate two representative reinforcement learning algorithms, namely Double Deep Q-Network (DDQN) and Group Relative Policy Optimization (GRPO). The results demonstrate that our news-aware approach, which does not depend on handcrafted features or manually designed rules, can achieve performance superior to market benchmarks. We further highlight the critical role of time-series information in this process.",
      "tldr_zh": "é’ˆå¯¹é‡‘èå¸‚åœºå¯¹æ–°é—»é«˜åº¦æ•æ„Ÿçš„ç‰¹æ€§ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é—»æ„ŸçŸ¥ï¼ˆNews-Awareï¼‰çš„ç›´æ¥å¼ºåŒ–å­¦ä¹ äº¤æ˜“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é‡åŒ–äº¤æ˜“ä¸­æœ‰æ•ˆæ•´åˆæ–°é—»æ•°æ®çš„éš¾é¢˜ã€‚ä¸ä¾èµ–äººå·¥è§„åˆ™æˆ–æ‰‹å·¥ç‰¹å¾çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥å·¥ä½œç›´æ¥åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) è¡ç”Ÿå‡ºçš„æ–°é—»æƒ…æ„Ÿå¾—åˆ†ï¼Œç»“åˆåŸå§‹ä»·æ ¼ä¸æˆäº¤é‡æ•°æ®ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„è§‚æµ‹è¾“å…¥ã€‚é€šè¿‡å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) æˆ– Transformers ç­‰åºåˆ—æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œç³»ç»Ÿå®ç°äº†ç«¯åˆ°ç«¯çš„äº¤æ˜“å†³ç­–ç”Ÿæˆã€‚åœ¨åŠ å¯†è´§å¸å¸‚åœºçš„å®éªŒä¸­ï¼Œç ”ç©¶è€…è¯„ä¼°äº† Double Deep Q-Network (DDQN) å’Œ Group Relative Policy Optimization (GRPO) ä¸¤ç§ä»£è¡¨æ€§ç®—æ³•ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¸ä¾èµ–æ‰‹å·¥ç‰¹å¾çš„æƒ…å†µä¸‹æ€§èƒ½æ˜¾è‘—ä¼˜äºå¸‚åœºåŸºå‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥æ­ç¤ºäº†æ—¶é—´åºåˆ—ä¿¡æ¯åœ¨è¿™ä¸€å†³ç­–è¿‡ç¨‹ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.CP",
      "comment": "9 pages, 4 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.19173v1",
      "published_date": "2025-10-22 02:17:03 UTC",
      "updated_date": "2025-10-22 02:17:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:05:00.776362+00:00"
    },
    {
      "arxiv_id": "2510.19172v2",
      "title": "When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA",
      "title_zh": "å½“äº‹å®å˜æ›´ï¼šåˆ©ç”¨ evolveQA æ¢æµ‹å¤§è¯­è¨€æ¨¡å‹åœ¨æ¼”åŒ–çŸ¥è¯†ä¸Šçš„è¡¨ç°",
      "authors": [
        "Nishanth Sridhar Nakshatri",
        "Shamik Roy",
        "Manoj Ghuhan Arivazhagan",
        "Hanhan Zhou",
        "Vinayshekhar Bannihatti Kumar",
        "Rashmi Gangadharaiah"
      ],
      "abstract": "LLMs often fail to handle temporal knowledge conflicts--contradictions arising when facts evolve over time within their training data. Existing studies evaluate this phenomenon through benchmarks built on structured knowledge bases like Wikidata, but they focus on widely-covered, easily-memorized popular entities and lack the dynamic structure needed to fairly evaluate LLMs with different knowledge cut-off dates. We introduce evolveQA, a benchmark specifically designed to evaluate LLMs on temporally evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS updates, Azure changes, and WHO disease outbreak reports. Our framework identifies naturally occurring knowledge evolution and generates questions with gold answers tailored to different LLM knowledge cut-off dates. Through extensive evaluation of 12 open and closed-source LLMs across 3 knowledge probing formats, we demonstrate significant performance drops of up to 31% on evolveQA compared to static knowledge questions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†éšæ—¶é—´æ¼”å˜çš„çŸ¥è¯†ï¼ˆå³temporal knowledge conflictsï¼‰æ—¶çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºæ¨¡å‹å¸¸å› è®­ç»ƒæ•°æ®ä¸­çš„äº‹å®æ›´è¿­è€Œäº§ç”ŸçŸ¥è¯†å†²çªã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å¤šä¾èµ–Wikidataç­‰ç»“æ„åŒ–çŸ¥è¯†åº“ï¼Œä¸”å—é™äºå®ä½“æµè¡Œåº¦åŠç¼ºä¹é€‚é…ä¸åŒçŸ¥è¯†æˆªæ­¢æ—¥æœŸ(knowledge cut-off dates)çš„åŠ¨æ€ç»“æ„ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†evolveQAåŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨AWSæ›´æ–°ã€Azureå˜æ›´åŠWHOç–¾ç—…çˆ†å‘æŠ¥å‘Šç­‰çœŸå®æ—¶é—´æˆ³è¯­æ–™åº“æ¥æ•æ‰è‡ªç„¶å‘ç”Ÿçš„çŸ¥è¯†æ¼”åŒ–ã€‚è¯¥æ¡†æ¶èƒ½é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸç”Ÿæˆå®šåˆ¶åŒ–çš„é—®é¢˜ä¸é‡‘æ ‡å‡†ç­”æ¡ˆï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„æ€§èƒ½è¯„ä¼°ã€‚é€šè¿‡å¯¹12æ¬¾å¼€æºåŠé—­æºLLMsçš„è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨evolveQAä¸Šçš„è¡¨ç°ç›¸æ¯”é™æ€çŸ¥è¯†æœ€é«˜ä¸‹é™äº†31%ï¼Œæ˜¾è‘—æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨åŠ¨æ€çŸ¥è¯†ç†è§£ä¸æ›´æ–°æ–¹é¢çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under submission",
      "pdf_url": "https://arxiv.org/pdf/2510.19172v2",
      "published_date": "2025-10-22 02:12:32 UTC",
      "updated_date": "2025-11-15 20:44:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:05:06.281178+00:00"
    },
    {
      "arxiv_id": "2510.21827v1",
      "title": "Precise classification of low quality G-banded Chromosome Images by reliability metrics and data pruning classifier",
      "title_zh": "åŸºäºå¯é æ€§åº¦é‡ä¸æ•°æ®å‰ªæåˆ†ç±»å™¨çš„ä½è´¨é‡Gæ˜¾å¸¦æŸ“è‰²ä½“å›¾åƒç²¾ç¡®åˆ†ç±»",
      "authors": [
        "Mojtaba Moattari"
      ],
      "abstract": "In the last decade, due to high resolution cameras and accurate meta-phase analyzes, the accuracy of chromosome classification has improved substantially. However, current Karyotyping systems demand large number of high quality train data to have an adequately plausible Precision per each chromosome. Such provision of high quality train data with accurate devices are not yet accomplished in some out-reached pathological laboratories. To prevent false positive detections in low-cost systems and low-quality images settings, this paper improves the classification Precision of chromosomes using proposed reliability thresholding metrics and deliberately engineered features. The proposed method has been evaluated using a variation of deep Alex-Net neural network, SVM, K Nearest-Neighbors, and their cascade pipelines to an automated filtering of semi-straight chromosome. The classification results have highly improved over 90% for the chromosomes with more common defections and translocations. Furthermore, a comparative analysis over the proposed thresholding metrics has been conducted and the best metric is bolded with its salient characteristics. The high Precision results provided for a very low-quality G-banding database verifies suitability of the proposed metrics and pruning method for Karyotyping facilities in poor countries and lowbudget pathological laboratories.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµ„æºåŒ®ä¹åœ°åŒºç—…ç†å®éªŒå®¤ç¼ºä¹é«˜è´¨é‡å›¾åƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨reliability metricså’Œdata pruning classifierå¯¹ä½è´¨é‡G-bandedæŸ“è‰²ä½“å›¾åƒè¿›è¡Œç²¾ç¡®åˆ†ç±»çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å·¥ç¨‹åŒ–ç‰¹å¾å’Œå¯é æ€§é˜ˆå€¼åº¦é‡ï¼Œç»“åˆAlex-Netã€SVMã€K Nearest-NeighborsåŠçº§è”æµæ°´çº¿ï¼Œå®ç°äº†å¯¹semi-straight chromosomeçš„è‡ªåŠ¨è¿‡æ»¤ï¼Œæœ‰æ•ˆé™ä½äº†ä½æˆæœ¬ç³»ç»Ÿä¸­çš„è¯¯æŠ¥ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤„ç†åŒ…å«å¸¸è§ç¼ºé™·å’Œtranslocationsçš„æŸ“è‰²ä½“æ—¶ï¼Œåˆ†ç±»å‡†ç¡®ç‡æ˜¾è‘—æå‡è‡³90%ä»¥ä¸Šã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡å¯¹æ¯”åˆ†æç¡®å®šäº†æœ€ä¼˜é˜ˆå€¼æŒ‡æ ‡ï¼ŒéªŒè¯äº†å…¶åœ¨ä½è´¨é‡G-bandingæ•°æ®åº“ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚è¯¥ç ”ç©¶æˆæœä¸ºä½é¢„ç®—å®éªŒå®¤å’Œè´«å›°åœ°åŒºçš„Karyotypingè®¾æ–½æä¾›äº†é«˜Precisionçš„åˆ†ç±»æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.21827v1",
      "published_date": "2025-10-22 02:05:27 UTC",
      "updated_date": "2025-10-22 02:05:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:05:10.992802+00:00"
    },
    {
      "arxiv_id": "2510.19150v1",
      "title": "X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning",
      "title_zh": "X-Egoï¼šé€šè¿‡è·¨ç¬¬ä¸€è§†è§’å¯¹æ¯”è§†é¢‘è¡¨ç¤ºå­¦ä¹ è·å–å›¢é˜Ÿçº§æˆ˜æœ¯æ€åŠ¿æ„ŸçŸ¥",
      "authors": [
        "Yunzhe Wang",
        "Soham Hans",
        "Volkan Ustun"
      ],
      "abstract": "Human team tactics emerge from each player's individual perspective and their ability to anticipate, interpret, and adapt to teammates' intentions. While advances in video understanding have improved the modeling of team interactions in sports, most existing work relies on third-person broadcast views and overlooks the synchronous, egocentric nature of multi-agent learning. We introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay footage from 45 professional-level matches of the popular e-sports game Counter-Strike 2, designed to facilitate research on multi-agent decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric video streams that synchronously capture all players' first-person perspectives along with state-action trajectories. Building on this resource, we propose Cross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric visual streams to foster team-level tactical situational awareness from an individual's perspective. We evaluate CECL on a teammate-opponent location prediction task, demonstrating its effectiveness in enhancing an agent's ability to infer both teammate and opponent positions from a single first-person view using state-of-the-art video encoders. Together, X-Ego-CS and CECL establish a foundation for cross-egocentric multi-agent benchmarking in esports. More broadly, our work positions gameplay understanding as a testbed for multi-agent modeling and tactical learning, with implications for spatiotemporal reasoning and human-AI teaming in both virtual and real-world domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»å›¢é˜Ÿæˆ˜æœ¯å¦‚ä½•ä»ä¸ªä½“è§†è§’åŠå¯¹é˜Ÿå‹æ„å›¾çš„é¢„åˆ¤ä¸­äº§ç”Ÿï¼Œé’ˆå¯¹ç°æœ‰è§†é¢‘ç†è§£ç ”ç©¶å¤šä¾èµ–ç¬¬ä¸‰äººç§°è§†è§’è€Œå¿½è§†å¤šæ™ºèƒ½ä½“åŒæ­¥ç¬¬ä¸€äººç§°è§†è§’(Egocentric)å­¦ä¹ çš„é—®é¢˜æå‡ºäº†æ–°æ–¹æ¡ˆã€‚ç ”ç©¶è€…å¼•å…¥äº†X-Ego-CSåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ã€ŠCounter-Strike 2ã€‹ä¸­45åœºèŒä¸šæ¯”èµ›çš„124å°æ—¶æ¸¸æˆç”»é¢ï¼ŒåŒæ­¥æ•æ‰äº†æ‰€æœ‰ç©å®¶çš„ç¬¬ä¸€äººç§°è§†é¢‘æµå’ŒçŠ¶æ€-åŠ¨ä½œè½¨è¿¹(State-action trajectories)ã€‚åŸºäºæ­¤èµ„æºï¼Œè®ºæ–‡æå‡ºäº†è·¨ç¬¬ä¸€äººç§°å¯¹æ¯”å­¦ä¹ (Cross-Ego Contrastive Learning, CECL)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¯¹é½é˜Ÿå‹é—´çš„è§†è§’è§†é¢‘æµï¼Œä»ä¸ªä½“è§†è§’åŸ¹å…»å›¢é˜Ÿå±‚é¢çš„æˆ˜æœ¯æ€åŠ¿æ„ŸçŸ¥(Tactical Situational Awareness)ã€‚åœ¨é˜Ÿå‹ä¸å¯¹æ‰‹ä½ç½®é¢„æµ‹ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼Œå®éªŒè¯æ˜CECLèƒ½æ˜¾è‘—å¢å¼ºæ™ºèƒ½ä½“ä»…å‡­å•è·¯ç¬¬ä¸€äººç§°è§†å›¾åˆ©ç”¨SOTAè§†é¢‘ç¼–ç å™¨æ¨æ–­å…¨å±€ä½ç½®çš„èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºç”µç«é¢†åŸŸçš„å¤šæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•å¥ å®šäº†åŸºç¡€ï¼Œä¹Ÿä¸ºè™šæ‹Ÿå’Œç°å®ä¸–ç•Œä¸­çš„æ—¶ç©ºæ¨ç†åŠäººç±»-AIåä½œ(Human-AI teaming)æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19150v1",
      "published_date": "2025-10-22 00:48:35 UTC",
      "updated_date": "2025-10-22 00:48:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:05:22.880594+00:00"
    },
    {
      "arxiv_id": "2510.19139v2",
      "title": "A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist",
      "title_zh": "è®¤çŸ¥èƒ½åŠ›çš„å¤šç»´åº¦åˆ†æï¼šåŸºäº CONSORT æ£€æŸ¥æ¸…å•çš„å¤§è¯­è¨€æ¨¡å‹æç¤ºæ–¹æ³•è¯„ä¼°",
      "authors": [
        "Sohyeon Jeon",
        "Hyung-Chul Lee"
      ],
      "abstract": "Despite the rapid expansion of Large Language Models (LLMs) in healthcare, robust and explainable evaluation of their ability to assess clinical trial reporting according to CONSORT standards remains an open challenge. In particular, uncertainty calibration and metacognitive reliability of LLM reasoning are poorly understood and underexplored in medical automation. This study applies a behavioral and metacognitive analytic approach using an expert-validated dataset, systematically comparing two representative LLMs - one general and one domain-specialized - across three prompt strategies. We analyze both cognitive adaptation and calibration error using metrics: Expected Calibration Error (ECE) and a baseline-normalized Relative Calibration Error (RCE) that enables reliable cross-model comparison. Our results reveal pronounced miscalibration and overconfidence in both models, especially under clinical role-playing conditions, with calibration error persisting above clinically relevant thresholds. These findings underscore the need for improved calibration, transparent code, and strategic prompt engineering to develop reliable and explainable medical AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»ç–—é¢†åŸŸè¯„ä¼°ä¸´åºŠè¯•éªŒæŠ¥å‘Š(CONSORTæ ‡å‡†)æ—¶ï¼Œç¼ºä¹é²æ£’ä¸”å¯è§£é‡Šçš„è¯„ä¼°ä»¥åŠå…ƒè®¤çŸ¥å¯é æ€§(Metacognitive Reliability)åˆ†æä¸å……åˆ†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šç»´åº¦çš„åˆ†ææ¡†æ¶ã€‚ç ”ç©¶é‡‡ç”¨è¡Œä¸ºå’Œå…ƒè®¤çŸ¥åˆ†ææ–¹æ³•ï¼ŒåŸºäºä¸“å®¶éªŒè¯çš„æ•°æ®é›†ï¼Œç³»ç»Ÿå¯¹æ¯”äº†é€šç”¨å‹å’Œé¢†åŸŸä¸“ç”¨å‹ä¸¤ç±»LLMsåœ¨ä¸‰ç§æç¤ºç­–ç•¥(Prompt Strategies)ä¸‹çš„è¡¨ç°ã€‚é€šè¿‡å¼•å…¥æœŸæœ›æ ¡å‡†è¯¯å·®(Expected Calibration Error, ECE)å’Œç›¸å¯¹æ ¡å‡†è¯¯å·®(Relative Calibration Error, RCE)ç­‰æŒ‡æ ‡ï¼Œè¯¥å·¥ä½œæ·±å…¥æ¢è®¨äº†æ¨¡å‹çš„è®¤çŸ¥é€‚åº”æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸¤ç§æ¨¡å‹åœ¨ä¸´åºŠè§’è‰²æ‰®æ¼”(Clinical Role-playing)æ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ ¡å‡†å¤±è¡¡å’Œè¿‡åº¦è‡ªä¿¡ï¼Œå…¶è¯¯å·®æ°´å¹³æŒç»­é«˜äºä¸´åºŠå®é™…åº”ç”¨çš„å®‰å…¨é˜ˆå€¼ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸ºäº†æ„å»ºå¯é ä¸”å¯è§£é‡Šçš„åŒ»ç–—AIï¼Œå¿…é¡»è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ ¡å‡†ã€æå‡ä»£ç é€æ˜åº¦å¹¶æ”¹è¿›æç¤ºå·¥ç¨‹(Prompt Engineering)ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19139v2",
      "published_date": "2025-10-22 00:15:02 UTC",
      "updated_date": "2025-10-26 01:38:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:05:26.292360+00:00"
    },
    {
      "arxiv_id": "2510.19138v1",
      "title": "InvarGC: Invariant Granger Causality for Heterogeneous Interventional Time Series under Latent Confounding",
      "title_zh": "InvarGCï¼šæ½œåœ¨æ··æ·†ä¸‹å¼‚è´¨å¹²é¢„æ—¶é—´åºåˆ—çš„ä¸å˜æ ¼å…°æ°å› æœå…³ç³»",
      "authors": [
        "Ziyi Zhang",
        "Shaogang Ren",
        "Xiaoning Qian",
        "Nick Duffield"
      ],
      "abstract": "Granger causality is widely used for causal structure discovery in complex systems from multivariate time series data. Traditional Granger causality tests based on linear models often fail to detect even mild non-linear causal relationships. Therefore, numerous recent studies have investigated non-linear Granger causality methods, achieving improved performance. However, these methods often rely on two key assumptions: causal sufficiency and known interventional targets. Causal sufficiency assumes the absence of latent confounders, yet their presence can introduce spurious correlations. Moreover, real-world time series data usually come from heterogeneous environments, without prior knowledge of interventions. Therefore, in practice, it is difficult to distinguish intervened environments from non-intervened ones, and even harder to identify which variables or timesteps are affected. To address these challenges, we propose Invariant Granger Causality (InvarGC), which leverages cross-environment heterogeneity to mitigate the effects of latent confounding and to distinguish intervened from non-intervened environments with edge-level granularity, thereby recovering invariant causal relations. In addition, we establish the identifiability under these conditions. Extensive experiments on both synthetic and real-world datasets demonstrate the competitive performance of our approach compared to state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Invariant Granger Causality (InvarGC)ï¼Œæ—¨åœ¨è§£å†³å¤šå…ƒæ—¶é—´åºåˆ—å› æœç»“æ„å‘ç°ä¸­æ™®éå­˜åœ¨çš„æ½œåœ¨æ··æ‚(latent confounding)å’ŒæœªçŸ¥å¹²é¢„ç›®æ ‡(unknown interventional targets)é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¾èµ–å› æœå……åˆ†æ€§(causal sufficiency)å‡è®¾ï¼Œä½†åœ¨å¼‚æ„ç¯å¢ƒ(heterogeneous environments)ä¸­ï¼Œæ½œåœ¨æ··æ‚å› ç´ å®¹æ˜“å¯¼è‡´ä¼ªç›¸å…³ã€‚InvarGC é€šè¿‡åˆ©ç”¨è·¨ç¯å¢ƒçš„å¼‚æ„æ€§æ¥å‡è½»æ··æ‚å½±å“ï¼Œå¹¶èƒ½åœ¨è¾¹ç¼˜çº§åˆ«(edge-level granularity)ç²¾ç¡®åŒºåˆ†å—å¹²é¢„ä¸æœªå—å¹²é¢„çš„ç¯å¢ƒï¼Œä»è€Œæ¢å¤ä¸å˜çš„å› æœå…³ç³»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åœ¨ä¸Šè¿°å¤æ‚æ¡ä»¶ä¸‹å»ºç«‹äº†æ¨¡å‹çš„å¯è¾¨è¯†æ€§(identifiability)ç†è®ºã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒInvarGC ç›¸æ¯”ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´å…·ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19138v1",
      "published_date": "2025-10-22 00:04:49 UTC",
      "updated_date": "2025-10-22 00:04:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:05:31.179901+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 133,
  "processed_papers_count": 133,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T05:06:24.104838+00:00"
}
[
  {
    "arxiv_id": "2401.14412v1",
    "title": "Harnessing Neuron Stability to Improve DNN Verification",
    "authors": [
      "Hai Duong",
      "Dong Xu",
      "ThanhVu Nguyen",
      "Matthew B. Dwyer"
    ],
    "abstract": "Deep Neural Networks (DNN) have emerged as an effective approach to tackling\nreal-world problems. However, like human-written software, DNNs are susceptible\nto bugs and attacks. This has generated significant interests in developing\neffective and scalable DNN verification techniques and tools. In this paper, we\npresent VeriStable, a novel extension of recently proposed DPLL-based\nconstraint DNN verification approach. VeriStable leverages the insight that\nwhile neuron behavior may be non-linear across the entire DNN input space, at\nintermediate states computed during verification many neurons may be\nconstrained to have linear behavior - these neurons are stable. Efficiently\ndetecting stable neurons reduces combinatorial complexity without compromising\nthe precision of abstractions. Moreover, the structure of clauses arising in\nDNN verification problems shares important characteristics with industrial SAT\nbenchmarks. We adapt and incorporate multi-threading and restart optimizations\ntargeting those characteristics to further optimize DPLL-based DNN\nverification. We evaluate the effectiveness of VeriStable across a range of\nchallenging benchmarks including fully-connected feedforward networks (FNNs),\nconvolutional neural networks (CNNs) and residual networks (ResNets) applied to\nthe standard MNIST and CIFAR datasets. Preliminary results show that VeriStable\nis competitive and outperforms state-of-the-art DNN verification tools,\nincluding $\\alpha$-$\\beta$-CROWN and MN-BaB, the first and second performers of\nthe VNN-COMP, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "VeriStable and experimental data are available at:\n  https://github.com/veristable/veristable",
    "pdf_url": "http://arxiv.org/pdf/2401.14412v1",
    "published_date": "2024-01-19 23:48:04 UTC",
    "updated_date": "2024-01-19 23:48:04 UTC"
  },
  {
    "arxiv_id": "2401.11061v4",
    "title": "PhotoBot: Reference-Guided Interactive Photography via Natural Language",
    "authors": [
      "Oliver Limoyo",
      "Jimmy Li",
      "Dmitriy Rivkin",
      "Jonathan Kelly",
      "Gregory Dudek"
    ],
    "abstract": "We introduce PhotoBot, a framework for fully automated photo acquisition\nbased on an interplay between high-level human language guidance and a robot\nphotographer. We propose to communicate photography suggestions to the user via\nreference images that are selected from a curated gallery. We leverage a visual\nlanguage model (VLM) and an object detector to characterize the reference\nimages via textual descriptions and then use a large language model (LLM) to\nretrieve relevant reference images based on a user's language query through\ntext-based reasoning. To correspond the reference image and the observed scene,\nwe exploit pre-trained features from a vision transformer capable of capturing\nsemantic similarity across marked appearance variations. Using these features,\nwe compute suggested pose adjustments for an RGB-D camera by solving a\nperspective-n-point (PnP) problem. We demonstrate our approach using a\nmanipulator equipped with a wrist camera. Our user studies show that photos\ntaken by PhotoBot are often more aesthetically pleasing than those taken by\nusers themselves, as measured by human feedback. We also show that PhotoBot can\ngeneralize to other reference sources such as paintings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "In Proceedings of the IEEE/RSJ International Conference on\n  Intelligent Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct. 14-18, 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.11061v4",
    "published_date": "2024-01-19 23:34:48 UTC",
    "updated_date": "2024-12-26 03:38:10 UTC"
  },
  {
    "arxiv_id": "2401.11044v2",
    "title": "Preservation of Feature Stability in Machine Learning Under Data Uncertainty for Decision Support in Critical Domains",
    "authors": [
      "Karol Capała",
      "Paulina Tworek",
      "Jose Sousa"
    ],
    "abstract": "In a world where Machine Learning (ML) is increasingly deployed to support\ndecision-making in critical domains, providing decision-makers with\nexplainable, stable, and relevant inputs becomes fundamental. Understanding how\nmachine learning works under missing data and how this affects feature\nvariability is paramount. This is even more relevant as machine learning\napproaches focus on standardising decision-making approaches that rely on an\nidealised set of features. However, decision-making in human activities often\nrelies on incomplete data, even in critical domains. This paper addresses this\ngap by conducting a set of experiments using traditional machine learning\nmethods that look for optimal decisions in comparison to a recently deployed\nmachine learning method focused on a classification that is more descriptive\nand mimics human decision making, allowing for the natural integration of\nexplainability. We found that the ML descriptive approach maintains higher\nclassification accuracy while ensuring the stability of feature selection as\ndata incompleteness increases. This suggests that descriptive classification\nmethods can be helpful in uncertain decision-making scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 6 figures, supplementary materials",
    "pdf_url": "http://arxiv.org/pdf/2401.11044v2",
    "published_date": "2024-01-19 22:11:54 UTC",
    "updated_date": "2024-08-06 11:29:06 UTC"
  },
  {
    "arxiv_id": "2402.04885v1",
    "title": "A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization",
    "authors": [
      "Jiazhao Zhang",
      "Ying Hung",
      "Chung-Ching Lin",
      "Zicheng Liu"
    ],
    "abstract": "Choosing appropriate hyperparameters plays a crucial role in the success of\nneural networks as hyper-parameters directly control the behavior and\nperformance of the training algorithms. To obtain efficient tuning, Bayesian\noptimization methods based on Gaussian process (GP) models are widely used.\nDespite numerous applications of Bayesian optimization in deep learning, the\nexisting methodologies are developed based on a convenient but restrictive\nassumption that the tuning parameters are independent of each other. However,\ntuning parameters with conditional dependence are common in practice. In this\npaper, we focus on two types of them: branching and nested parameters. Nested\nparameters refer to those tuning parameters that exist only within a particular\nsetting of another tuning parameter, and a parameter within which other\nparameters are nested is called a branching parameter. To capture the\nconditional dependence between branching and nested parameters, a unified\nBayesian optimization framework is proposed. The sufficient conditions are\nrigorously derived to guarantee the validity of the kernel function, and the\nasymptotic convergence of the proposed optimization framework is proven under\nthe continuum-armed-bandit setting. Based on the new GP model, which accounts\nfor the dependent structure among input variables through a new kernel\nfunction, higher prediction accuracy and better optimization efficiency are\nobserved in a series of synthetic simulations and real data applications of\nneural networks. Sensitivity analysis is also performed to provide insights\ninto how changes in hyperparameter values affect prediction accuracy.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.04885v1",
    "published_date": "2024-01-19 21:11:32 UTC",
    "updated_date": "2024-01-19 21:11:32 UTC"
  },
  {
    "arxiv_id": "2401.11021v1",
    "title": "Analysis and Detection of Multilingual Hate Speech Using Transformer Based Deep Learning",
    "authors": [
      "Arijit Das",
      "Somashree Nandy",
      "Rupam Saha",
      "Srijan Das",
      "Diganta Saha"
    ],
    "abstract": "Hate speech is harmful content that directly attacks or promotes hatred\nagainst members of groups or individuals based on actual or perceived aspects\nof identity, such as racism, religion, or sexual orientation. This can affect\nsocial life on social media platforms as hateful content shared through social\nmedia can harm both individuals and communities. As the prevalence of hate\nspeech increases online, the demand for automated detection as an NLP task is\nincreasing. In this work, the proposed method is using transformer-based model\nto detect hate speech in social media, like twitter, Facebook, WhatsApp,\nInstagram, etc. The proposed model is independent of languages and has been\ntested on Italian, English, German, Bengali. The Gold standard datasets were\ncollected from renowned researcher Zeerak Talat, Sara Tonelli, Melanie Siegel,\nand Rezaul Karim. The success rate of the proposed model for hate speech\ndetection is higher than the existing baseline and state-of-the-art models with\naccuracy in Bengali dataset is 89%, in English: 91%, in German dataset 91% and\nin Italian dataset it is 77%. The proposed algorithm shows substantial\nimprovement to the benchmark method.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.11021v1",
    "published_date": "2024-01-19 20:40:23 UTC",
    "updated_date": "2024-01-19 20:40:23 UTC"
  },
  {
    "arxiv_id": "2402.04888v2",
    "title": "RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing",
    "authors": [
      "Borna Barahimi",
      "Hakam Singh",
      "Hina Tabassum",
      "Omer Waqar",
      "Mohammad Omer"
    ],
    "abstract": "WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere\ncommunication devices to sensing instruments, leveraging Channel State\nInformation (CSI) extraction capabilities. Nevertheless, resource-constrained\nIoT devices and the intricacies of deep neural networks necessitate\ntransmitting CSI to cloud servers for sensing. Although feasible, this leads to\nconsiderable communication overhead. In this context, this paper develops a\nnovel Real-time Sensing and Compression Network (RSCNet) which enables sensing\nwith compressed CSI; thereby reducing the communication overheads. RSCNet\nfacilitates optimization across CSI windows composed of a few CSI frames. Once\ntransmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to\nharness data from prior windows, thus bolstering both the sensing accuracy and\nCSI reconstruction. RSCNet adeptly balances the trade-off between CSI\ncompression and sensing precision, thus streamlining real-time cloud-based WiFi\nsensing with reduced communication costs. Numerical findings demonstrate the\ngains of RSCNet over the existing benchmarks like SenseFi, showcasing a sensing\naccuracy of 97.4% with minimal CSI reconstruction error. Numerical results also\nshow a computational analysis of the proposed RSCNet as a function of the\nnumber of CSI frames.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "The paper has been accepted by IEEE International Conference on\n  Communications (ICC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.04888v2",
    "published_date": "2024-01-19 20:30:23 UTC",
    "updated_date": "2024-05-20 15:48:38 UTC"
  },
  {
    "arxiv_id": "2401.11002v2",
    "title": "Fast Registration of Photorealistic Avatars for VR Facial Animation",
    "authors": [
      "Chaitanya Patel",
      "Shaojie Bai",
      "Te-Li Wang",
      "Jason Saragih",
      "Shih-En Wei"
    ],
    "abstract": "Virtual Reality (VR) bares promise of social interactions that can feel more\nimmersive than other media. Key to this is the ability to accurately animate a\npersonalized photorealistic avatar, and hence the acquisition of the labels for\nheadset-mounted camera (HMC) images need to be efficient and accurate, while\nwearing a VR headset. This is challenging due to oblique camera views and\ndifferences in image modality. In this work, we first show that the domain gap\nbetween the avatar and HMC images is one of the primary sources of difficulty,\nwhere a transformer-based architecture achieves high accuracy on\ndomain-consistent data, but degrades when the domain-gap is re-introduced.\nBuilding on this finding, we propose a system split into two parts: an\niterative refinement module that takes in-domain inputs, and a generic\navatar-guided image-to-image domain transfer module conditioned on current\nestimates. These two modules reinforce each other: domain transfer becomes\neasier when close-to-groundtruth examples are shown, and better domain-gap\nremoval in turn improves the registration. Our system obviates the need for\ncostly offline optimization, and produces online registration of higher quality\nthan direct regression method. We validate the accuracy and efficiency of our\napproach through extensive experiments on a commodity headset, demonstrating\nsignificant improvements over these baselines. To stimulate further research in\nthis direction, we make our large-scale dataset and code publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024. Project page:\n  https://chaitanya100100.github.io/FastRegistration/",
    "pdf_url": "http://arxiv.org/pdf/2401.11002v2",
    "published_date": "2024-01-19 19:42:38 UTC",
    "updated_date": "2024-07-18 22:39:33 UTC"
  },
  {
    "arxiv_id": "2402.01676v2",
    "title": "Language models align with human judgments on key grammatical constructions",
    "authors": [
      "Jennifer Hu",
      "Kyle Mahowald",
      "Gary Lupyan",
      "Anna Ivanova",
      "Roger Levy"
    ],
    "abstract": "Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in PNAS at https://www.pnas.org/doi/10.1073/pnas.2400917121\n  as response to Dentella et al. (2023)",
    "pdf_url": "http://arxiv.org/pdf/2402.01676v2",
    "published_date": "2024-01-19 19:36:54 UTC",
    "updated_date": "2024-08-30 14:43:22 UTC"
  },
  {
    "arxiv_id": "2401.10889v2",
    "title": "Synthesizing Moving People with 3D Control",
    "authors": [
      "Boyi Li",
      "Junming Chen",
      "Jathushan Rajasegaran",
      "Yossi Gandelsman",
      "Alexei A. Efros",
      "Jitendra Malik"
    ],
    "abstract": "In this paper, we present a diffusion model-based framework for animating\npeople from a single image for a given target 3D motion sequence. Our approach\nhas two core components: a) learning priors about invisible parts of the human\nbody and clothing, and b) rendering novel body poses with proper clothing and\ntexture. For the first part, we learn an in-filling diffusion model to\nhallucinate unseen parts of a person given a single image. We train this model\non texture map space, which makes it more sample-efficient since it is\ninvariant to pose and viewpoint. Second, we develop a diffusion-based rendering\npipeline, which is controlled by 3D human poses. This produces realistic\nrenderings of novel poses of the person, including clothing, hair, and\nplausible in-filling of unseen regions. This disentangled approach allows our\nmethod to generate a sequence of images that are faithful to the target motion\nin the 3D pose and, to the input image in terms of visual similarity. In\naddition to that, the 3D control allows various synthetic camera trajectories\nto render a person. Our experiments show that our method is resilient in\ngenerating prolonged motions and varied challenging and complex poses compared\nto prior methods. Please check our website for more details:\nhttps://boyiliee.github.io/3DHM.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10889v2",
    "published_date": "2024-01-19 18:59:11 UTC",
    "updated_date": "2024-12-20 18:42:11 UTC"
  },
  {
    "arxiv_id": "2401.10886v1",
    "title": "SCENES: Subpixel Correspondence Estimation With Epipolar Supervision",
    "authors": [
      "Dominik A. Kloepfer",
      "João F. Henriques",
      "Dylan Campbell"
    ],
    "abstract": "Extracting point correspondences from two or more views of a scene is a\nfundamental computer vision problem with particular importance for relative\ncamera pose estimation and structure-from-motion. Existing local feature\nmatching approaches, trained with correspondence supervision on large-scale\ndatasets, obtain highly-accurate matches on the test sets. However, they do not\ngeneralise well to new datasets with different characteristics to those they\nwere trained on, unlike classic feature extractors. Instead, they require\nfinetuning, which assumes that ground-truth correspondences or ground-truth\ncamera poses and 3D structure are available. We relax this assumption by\nremoving the requirement of 3D structure, e.g., depth maps or point clouds, and\nonly require camera pose information, which can be obtained from odometry. We\ndo so by replacing correspondence losses with epipolar losses, which encourage\nputative matches to lie on the associated epipolar line. While weaker than\ncorrespondence supervision, we observe that this cue is sufficient for\nfinetuning existing models on new data. We then further relax the assumption of\nknown camera poses by using pose estimates in a novel bootstrapping approach.\nWe evaluate on highly challenging datasets, including an indoor drone dataset\nand an outdoor smartphone camera dataset, and obtain state-of-the-art results\nwithout strong supervision.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10886v1",
    "published_date": "2024-01-19 18:57:46 UTC",
    "updated_date": "2024-01-19 18:57:46 UTC"
  },
  {
    "arxiv_id": "2401.10882v1",
    "title": "Reinforcement learning for question answering in programming domain using public community scoring as a human feedback",
    "authors": [
      "Alexey Gorbatovski",
      "Sergey Kovalchuk"
    ],
    "abstract": "In this study, we investigate the enhancement of the GPT Neo 125M performance\nin Community Question Answering (CQA) with a focus on programming, through the\nintegration of Reinforcement Learning from Human Feedback (RLHF) and the\nutilization of scores from Stack Overflow. Two distinct reward model training\nstrategies are employed for fine-tuning with Proximal Policy Optimization\n(PPO). Notably, the improvements in performance achieved through this method\nare comparable to those of GPT Neo 2.7B parameter variant. Additionally, an\nauxiliary scoring mechanism is introduced, which demonstrates the limitations\nof conventional linguistic metrics in evaluating responses in the programming\ndomain. Through accurate analysis, this paper looks at the divergence between\ntraditional linguistic metrics and our human-preferences-based reward model,\nunderscoring the imperative for domain-specific evaluation methods. By\nelucidating the complexities involved in applying RLHF to programming CQA and\naccentuating the significance of context-aware evaluation, this study\ncontributes to the ongoing efforts in refining Large Language Models through\nfocused human feedback.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10882v1",
    "published_date": "2024-01-19 18:49:36 UTC",
    "updated_date": "2024-01-19 18:49:36 UTC"
  },
  {
    "arxiv_id": "2401.10862v3",
    "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning",
    "authors": [
      "Adib Hasan",
      "Ileana Rugina",
      "Alex Wang"
    ],
    "abstract": "This paper investigates the impact of model compression on the way Large\nLanguage Models (LLMs) process prompts, particularly concerning jailbreak\nresistance. We show that moderate WANDA pruning can enhance resistance to\njailbreaking attacks without fine-tuning, while maintaining performance on\nstandard benchmarks. To systematically evaluate this safety enhancement, we\nintroduce a dataset of 225 harmful tasks across five categories. Our analysis\nof LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning\nbenefits correlate with initial model safety levels. We interpret these results\nby examining changes in attention patterns and perplexity shifts, demonstrating\nthat pruned models exhibit sharper attention and increased sensitivity to\nartificial jailbreak constructs. We extend our evaluation to the AdvBench\nharmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much\nsafer on AdvBench prompts than on our dataset when evaluated with manual\njailbreak attempts, and that pruning is effective against both automated\nattacks and manual jailbreaking on Advbench.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and\n  Interpreting Neural Networks for NLP",
    "pdf_url": "http://arxiv.org/pdf/2401.10862v3",
    "published_date": "2024-01-19 18:05:34 UTC",
    "updated_date": "2024-10-31 04:16:12 UTC"
  },
  {
    "arxiv_id": "2401.10850v1",
    "title": "Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning",
    "authors": [
      "Elena-Simona Apostol",
      "Ciprian-Octavian Truică"
    ],
    "abstract": "The healthcare environment is commonly referred to as \"information-rich\" but\nalso \"knowledge poor\". Healthcare systems collect huge amounts of data from\nvarious sources: lab reports, medical letters, logs of medical tools or\nprograms, medical prescriptions, etc. These massive sets of data can provide\ngreat knowledge and information that can improve the medical services, and\noverall the healthcare domain, such as disease prediction by analyzing the\npatient's symptoms or disease prevention, by facilitating the discovery of\nbehavioral factors for diseases. Unfortunately, only a relatively small volume\nof the textual eHealth data is processed and interpreted, an important factor\nbeing the difficulty in efficiently performing Big Data operations. In the\nmedical field, detecting domain-specific multi-word terms is a crucial task as\nthey can define an entire concept with a few words. A term can be defined as a\nlinguistic structure or a concept, and it is composed of one or more words with\na specific meaning to a domain. All the terms of a domain create its\nterminology. This chapter offers a critical study of the current, most\nperformant solutions for analyzing unstructured (image and textual) eHealth\ndata. This study also provides a comparison of the current Natural Language\nProcessing and Deep Learning techniques in the eHealth context. Finally, we\nexamine and discuss some of the current issues, and we define a set of research\ndirections in this area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10850v1",
    "published_date": "2024-01-19 17:51:11 UTC",
    "updated_date": "2024-01-19 17:51:11 UTC"
  },
  {
    "arxiv_id": "2401.10848v1",
    "title": "Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation",
    "authors": [
      "Prakhar Kaushik",
      "Aayush Mishra",
      "Adam Kortylewski",
      "Alan Yuille"
    ],
    "abstract": "We consider the problem of source-free unsupervised category-level pose\nestimation from only RGB images to a target domain without any access to source\ndomain data or 3D annotations during adaptation. Collecting and annotating\nreal-world 3D data and corresponding images is laborious, expensive, yet\nunavoidable process, since even 3D pose domain adaptation methods require 3D\ndata in the target domain. We introduce 3DUDA, a method capable of adapting to\na nuisance-ridden target domain without 3D or depth data. Our key insight stems\nfrom the observation that specific object subparts remain stable across\nout-of-domain (OOD) scenarios, enabling strategic utilization of these\ninvariant subcomponents for effective model updates. We represent object\ncategories as simple cuboid meshes, and harness a generative model of neural\nfeature activations modeled at each mesh vertex learnt using differential\nrendering. We focus on individual locally robust mesh vertex features and\niteratively update them based on their proximity to corresponding features in\nthe target domain even when the global pose is not correct. Our model is then\ntrained in an EM fashion, alternating between updating the vertex features and\nthe feature extractor. We show that our method simulates fine-tuning on a\nglobal pseudo-labeled dataset under mild assumptions, which converges to the\ntarget domain asymptotically. Through extensive empirical validation, including\na complex extreme UDA setup which combines real nuisances, synthetic noise, and\nocclusion, we demonstrate the potency of our simple approach in addressing the\ndomain shift challenge and significantly improving pose estimation accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "36 pages, 9 figures, 50 tables; ICLR 2024 (Poster)",
    "pdf_url": "http://arxiv.org/pdf/2401.10848v1",
    "published_date": "2024-01-19 17:48:05 UTC",
    "updated_date": "2024-01-19 17:48:05 UTC"
  },
  {
    "arxiv_id": "2401.10841v2",
    "title": "Using LLMs to discover emerging coded antisemitic hate-speech in extremist social media",
    "authors": [
      "Dhanush Kikkisetti",
      "Raza Ul Mustafa",
      "Wendy Melillo",
      "Roberto Corizzo",
      "Zois Boukouvalas",
      "Jeff Gill",
      "Nathalie Japkowicz"
    ],
    "abstract": "Online hate speech proliferation has created a difficult problem for social\nmedia platforms. A particular challenge relates to the use of coded language by\ngroups interested in both creating a sense of belonging for its users and\nevading detection. Coded language evolves quickly and its use varies over time.\nThis paper proposes a methodology for detecting emerging coded hate-laden\nterminology. The methodology is tested in the context of online antisemitic\ndiscourse. The approach considers posts scraped from social media platforms,\noften used by extremist users. The posts are scraped using seed expressions\nrelated to previously known discourse of hatred towards Jews. The method begins\nby identifying the expressions most representative of each post and calculating\ntheir frequency in the whole corpus. It filters out grammatically incoherent\nexpressions as well as previously encountered ones so as to focus on emergent\nwell-formed terminology. This is followed by an assessment of semantic\nsimilarity to known antisemitic terminology using a fine-tuned large language\nmodel, and subsequent filtering out of the expressions that are too distant\nfrom known expressions of hatred. Emergent antisemitic expressions containing\nterms clearly relating to Jewish topics are then removed to return only coded\nexpressions of hatred.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures, 2 algorithms, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.10841v2",
    "published_date": "2024-01-19 17:40:50 UTC",
    "updated_date": "2024-01-23 20:05:30 UTC"
  },
  {
    "arxiv_id": "2401.10831v3",
    "title": "Understanding Video Transformers via Universal Concept Discovery",
    "authors": [
      "Matthew Kowal",
      "Achal Dave",
      "Rares Ambrus",
      "Adrien Gaidon",
      "Konstantinos G. Derpanis",
      "Pavel Tokmakov"
    ],
    "abstract": "This paper studies the problem of concept-based interpretability of\ntransformer representations for videos. Concretely, we seek to explain the\ndecision-making process of video transformers based on high-level,\nspatiotemporal concepts that are automatically discovered. Prior research on\nconcept-based interpretability has concentrated solely on image-level tasks.\nComparatively, video models deal with the added temporal dimension, increasing\ncomplexity and posing challenges in identifying dynamic concepts over time. In\nthis work, we systematically address these challenges by introducing the first\nVideo Transformer Concept Discovery (VTCD) algorithm. To this end, we propose\nan efficient approach for unsupervised identification of units of video\ntransformer representations - concepts, and ranking their importance to the\noutput of a model. The resulting concepts are highly interpretable, revealing\nspatio-temporal reasoning mechanisms and object-centric representations in\nunstructured video models. Performing this analysis jointly over a diverse set\nof supervised and self-supervised representations, we discover that some of\nthese mechanism are universal in video transformers. Finally, we show that VTCD\ncan be used for fine-grained action recognition and video object segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024 (Highlight)",
    "pdf_url": "http://arxiv.org/pdf/2401.10831v3",
    "published_date": "2024-01-19 17:27:21 UTC",
    "updated_date": "2024-04-10 15:19:07 UTC"
  },
  {
    "arxiv_id": "2401.10819v1",
    "title": "Optimisation in Neurosymbolic Learning Systems",
    "authors": [
      "Emile van Krieken"
    ],
    "abstract": "Neurosymbolic AI aims to integrate deep learning with symbolic AI. This\nintegration has many promises, such as decreasing the amount of data required\nto train a neural network, improving the explainability and interpretability of\nanswers given by models and verifying the correctness of trained systems. We\nstudy neurosymbolic learning, where we have both data and background knowledge\nexpressed using symbolic languages. How do we connect the symbolic and neural\ncomponents to communicate this knowledge? One option is fuzzy reasoning, which\nstudies degrees of truth. For example, being tall is not a binary concept.\nInstead, probabilistic reasoning studies the probability that something is true\nor will happen. Our first research question studies how different forms of\nfuzzy reasoning combine with learning. We find surprising results like a\nconnection to the Raven paradox stating we confirm \"ravens are black\" when we\nobserve a green apple. In this study, we did not use the background knowledge\nwhen we deployed our models after training. In our second research question, we\nstudied how to use background knowledge in deployed models. We developed a new\nneural network layer based on fuzzy reasoning. Probabilistic reasoning is a\nnatural fit for neural networks, which we usually train to be probabilistic.\nHowever, they are expensive to compute and do not scale well to large tasks. In\nour third research question, we study how to connect probabilistic reasoning\nwith neural networks by sampling to estimate averages, while in the final\nresearch question, we study scaling probabilistic neurosymbolic learning to\nmuch larger problems than before. Our insight is to train a neural network with\nsynthetic data to predict the result of probabilistic reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "PhD dissertation",
    "pdf_url": "http://arxiv.org/pdf/2401.10819v1",
    "published_date": "2024-01-19 17:09:32 UTC",
    "updated_date": "2024-01-19 17:09:32 UTC"
  },
  {
    "arxiv_id": "2401.10816v2",
    "title": "Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes",
    "authors": [
      "Jodi Chiam",
      "Aloysius Lim",
      "Cheryl Nott",
      "Nicholas Mark",
      "Ankur Teredesai",
      "Sunil Shinde"
    ],
    "abstract": "The ability to shape health behaviors of large populations automatically,\nacross wearable types and disease conditions at scale has tremendous potential\nto improve global health outcomes. We designed and implemented an AI driven\nplatform for digital algorithmic nudging, enabled by a Graph-Neural Network\n(GNN) based Recommendation System, and granular health behavior data from\nwearable fitness devices. Here we describe the efficacy results of this\nplatform with its capabilities of personalized and contextual nudging to\n$n=84,764$ individuals over a 12-week period in Singapore. We statistically\nvalidated that participants in the target group who received such AI optimized\ndaily nudges increased daily physical activity like step count by 6.17% ($p =\n3.09\\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical\nActivity (MVPA) by 7.61% ($p = 1.16\\times10^{-2}$), compared to matched\nparticipants in control group who did not receive any nudges. Further, such\nnudges were very well received, with a 13.1% of nudges sent being opened (open\nrate), and 11.7% of the opened nudges rated useful compared to 1.9% rated as\nnot useful thereby demonstrating significant improvement in population level\nengagement metrics.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "20 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10816v2",
    "published_date": "2024-01-19 17:03:37 UTC",
    "updated_date": "2024-02-09 00:55:52 UTC"
  },
  {
    "arxiv_id": "2401.10805v3",
    "title": "Learning to Visually Connect Actions and their Effects",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "abstract": "We introduce the novel concept of visually Connecting Actions and Their\nEffects (CATE) in video understanding. CATE can have applications in areas like\ntask planning and learning from demonstration. We identify and explore two\ndifferent aspects of the concept of CATE: Action Selection (AS) and\nEffect-Affinity Assessment (EAA), where video understanding models connect\nactions and effects at semantic and fine-grained levels, respectively. We\ndesign various baseline models for AS and EAA. Despite the intuitive nature of\nthe task, we observe that models struggle, and humans outperform them by a\nlarge margin. Our experiments show that in solving AS and EAA, models learn\nintuitive properties like object tracking and pose encoding without explicit\nsupervision. We demonstrate that CATE can be an effective self-supervised task\nfor learning video representations from unlabeled videos. The study aims to\nshowcase the fundamental nature and versatility of CATE, with the hope of\ninspiring advanced formulations and models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10805v3",
    "published_date": "2024-01-19 16:48:49 UTC",
    "updated_date": "2024-07-26 16:00:07 UTC"
  },
  {
    "arxiv_id": "2401.10969v2",
    "title": "MacroSwarm: A Field-based Compositional Framework for Swarm Programming",
    "authors": [
      "Gianluca Aguzzi",
      "Roberto Casadei",
      "Mirko Viroli"
    ],
    "abstract": "Swarm behaviour engineering is an area of research that seeks to investigate\nmethods and techniques for coordinating computation and action within groups of\nsimple agents to achieve complex global goals like pattern formation,\ncollective movement, clustering, and distributed sensing. Despite recent\nprogress in the analysis and engineering of swarms (of drones, robots,\nvehicles), there is still a need for general design and implementation methods\nand tools that can be used to define complex swarm behaviour in a principled\nway. To contribute to this quest, this article proposes a new field-based\ncoordination approach, called MacroSwarm, to design and program swarm behaviour\nin terms of reusable and fully composable functional blocks embedding\ncollective computation and coordination. Based on the macroprogramming paradigm\nof aggregate computing, MacroSwarm builds on the idea of expressing each swarm\nbehaviour block as a pure function, mapping sensing fields into actuation goal\nfields, e.g., including movement vectors. In order to demonstrate the\nexpressiveness, compositionality, and practicality of MacroSwarm as a framework\nfor swarm programming, we perform a variety of simulations covering common\npatterns of flocking, pattern formation, and collective decision-making. The\nimplications of the inherent self-stabilisation properties of field-based\ncomputations in MacroSwarm are discussed, which formally guarantee some\nresilience properties and guided the design of the library.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10969v2",
    "published_date": "2024-01-19 16:32:02 UTC",
    "updated_date": "2024-11-11 09:51:28 UTC"
  },
  {
    "arxiv_id": "2401.10781v1",
    "title": "Metric Dynamic Equilibrium Logic",
    "authors": [
      "Arvid Becker",
      "Pedro Cabalar",
      "Martín Diéguez",
      "Luis Fariñas",
      "Torsten Schaub",
      "Anna Schuhmann"
    ],
    "abstract": "In temporal extensions of Answer Set Programming (ASP) based on linear-time,\nthe behavior of dynamic systems is captured by sequences of states. While this\nrepresentation reflects their relative order, it abstracts away the specific\ntimes associated with each state. In many applications, however, timing\nconstraints are important like, for instance, when planning and scheduling go\nhand in hand. We address this by developing a metric extension of linear-time\nDynamic Equilibrium Logic, in which dynamic operators are constrained by\nintervals over integers. The resulting Metric Dynamic Equilibrium Logic\nprovides the foundation of an ASP-based approach for specifying qualitative and\nquantitative dynamic constraints. As such, it constitutes the most general\namong a whole spectrum of temporal extensions of Equilibrium Logic. In detail,\nwe show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium\nLogic, as well as its classic counterparts once the law of the excluded middle\nis added.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2304.14778",
    "pdf_url": "http://arxiv.org/pdf/2401.10781v1",
    "published_date": "2024-01-19 16:01:38 UTC",
    "updated_date": "2024-01-19 16:01:38 UTC"
  },
  {
    "arxiv_id": "2401.10759v1",
    "title": "Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models",
    "authors": [
      "James Prather",
      "Paul Denny",
      "Juho Leinonen",
      "David H. Smith IV",
      "Brent N. Reeves",
      "Stephen MacNeil",
      "Brett A. Becker",
      "Andrew Luxton-Reilly",
      "Thezyrie Amarouche",
      "Bailey Kimmel"
    ],
    "abstract": "Large Language Models (LLMs) have upended decades of pedagogy in computing\neducation. Students previously learned to code through \\textit{writing} many\nsmall problems with less emphasis on code reading and comprehension. Recent\nresearch has shown that free code generation tools powered by LLMs can solve\nintroductory programming problems presented in natural language with ease. In\nthis paper, we propose a new way to teach programming with Prompt Problems.\nStudents receive a problem visually, indicating how input should be transformed\nto output, and must translate that to a prompt for an LLM to decipher. The\nproblem is considered correct when the code that is generated by the student\nprompt can pass all test cases. In this paper we present the design of this\ntool, discuss student interactions with it as they learn, and provide insights\ninto this new class of programming problems as well as the design tools that\nintegrate LLMs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "accepted for CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10759v1",
    "published_date": "2024-01-19 15:32:46 UTC",
    "updated_date": "2024-01-19 15:32:46 UTC"
  },
  {
    "arxiv_id": "2401.10753v1",
    "title": "BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation",
    "authors": [
      "Yingjie Li",
      "Anthony Agnesina",
      "Yanqing Zhang",
      "Haoxing Ren",
      "Cunxi Yu"
    ],
    "abstract": "Boolean algebraic manipulation is at the core of logic synthesis in\nElectronic Design Automation (EDA) design flow. Existing methods struggle to\nfully exploit optimization opportunities, and often suffer from an explosive\nsearch space and limited scalability efficiency. This work presents BoolGebra,\na novel attributed graph-learning approach for Boolean algebraic manipulation\nthat aims to improve fundamental logic synthesis. BoolGebra incorporates Graph\nNeural Networks (GNNs) and takes initial feature embeddings from both\nstructural and functional information as inputs. A fully connected neural\nnetwork is employed as the predictor for direct optimization result\npredictions, significantly reducing the search space and efficiently locating\nthe optimization space. The experiments involve training the BoolGebra model\nw.r.t design-specific and cross-design inferences using the trained model,\nwhere BoolGebra demonstrates generalizability for cross-design inference and\nits potential to scale from small, simple training datasets to large, complex\ninference datasets. Finally, BoolGebra is integrated with existing synthesis\ntool ABC to perform end-to-end logic minimization evaluation w.r.t SOTA\nbaselines.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "DATE 2024 extended version. arXiv admin note: text overlap with\n  arXiv:2310.07846",
    "pdf_url": "http://arxiv.org/pdf/2401.10753v1",
    "published_date": "2024-01-19 15:22:28 UTC",
    "updated_date": "2024-01-19 15:22:28 UTC"
  },
  {
    "arxiv_id": "2401.10751v1",
    "title": "EFO: the Emotion Frame Ontology",
    "authors": [
      "Stefano De Giorgis",
      "Aldo Gangemi"
    ],
    "abstract": "Emotions are a subject of intense debate in various disciplines. Despite the\nproliferation of theories and definitions, there is still no consensus on what\nemotions are, and how to model the different concepts involved when we talk\nabout - or categorize - them. In this paper, we propose an OWL frame-based\nontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as\nsemantic frames, with a set of semantic roles that capture the different\naspects of emotional experience. EFO follows pattern-based ontology design, and\nis aligned to the DOLCE foundational ontology. EFO is used to model multiple\nemotion theories, which can be cross-linked as modules in an Emotion Ontology\nNetwork. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE)\nTheory as an EFO-BE module, and demonstrate how to perform automated inferences\non the representation of emotion situations. EFO-BE has been evaluated by\nlexicalizing the BE emotion frames from within the Framester knowledge graph,\nand implementing a graph-based emotion detector from text. In addition, an EFO\nintegration of multimodal datasets, including emotional speech and emotional\nface expressions, has been performed to enable further inquiry into crossmodal\nemotion semantics.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10751v1",
    "published_date": "2024-01-19 15:20:57 UTC",
    "updated_date": "2024-01-19 15:20:57 UTC"
  },
  {
    "arxiv_id": "2401.12167v1",
    "title": "Dynamic Semantic Compression for CNN Inference in Multi-access Edge Computing: A Graph Reinforcement Learning-based Autoencoder",
    "authors": [
      "Nan Li",
      "Alexandros Iosifidis",
      "Qi Zhang"
    ],
    "abstract": "This paper studies the computational offloading of CNN inference in dynamic\nmulti-access edge computing (MEC) networks. To address the uncertainties in\ncommunication time and computation resource availability, we propose a novel\nsemantic compression method, autoencoder-based CNN architecture (AECNN), for\neffective semantic extraction and compression in partial offloading. In the\nsemantic encoder, we introduce a feature compression module based on the\nchannel attention mechanism in CNNs, to compress intermediate data by selecting\nthe most informative features. In the semantic decoder, we design a lightweight\ndecoder to reconstruct the intermediate data through learning from the received\ncompressed data to improve accuracy. To effectively trade-off communication,\ncomputation, and inference accuracy, we design a reward function and formulate\nthe offloading problem of CNN inference as a maximization problem with the goal\nof maximizing the average inference accuracy and throughput over the long term.\nTo address this maximization problem, we propose a graph reinforcement\nlearning-based AECNN (GRL-AECNN) method, which outperforms existing works\nDROO-AECNN, GRL-BottleNet++ and GRL-DeepJSCC under different dynamic scenarios.\nThis highlights the advantages of GRL-AECNN in offloading decision-making in\ndynamic MEC.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "arXiv admin note: text overlap with arXiv:2211.13745",
    "pdf_url": "http://arxiv.org/pdf/2401.12167v1",
    "published_date": "2024-01-19 15:19:47 UTC",
    "updated_date": "2024-01-19 15:19:47 UTC"
  },
  {
    "arxiv_id": "2401.10746v4",
    "title": "A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding",
    "authors": [
      "Bruna Junqueira",
      "Bruno Aristimunha",
      "Sylvain Chevallier",
      "Raphael Y. de Camargo"
    ],
    "abstract": "Electroencephalography (EEG) signals are frequently used for various\nBrain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have\nshown promising results, they are hindered by the substantial data\nrequirements. By leveraging data from multiple subjects, transfer learning\nenables more effective training of DL models. A technique that is gaining\npopularity is Euclidean Alignment (EA) due to its ease of use, low\ncomputational complexity, and compatibility with Deep Learning models. However,\nfew studies evaluate its impact on the training performance of shared and\nindividual DL models. In this work, we systematically evaluate the effect of EA\ncombined with DL for decoding BCI signals. We used EA to train shared models\nwith data from multiple subjects and evaluated its transferability to new\nsubjects. Our experimental results show that it improves decoding in the target\nsubject by 4.33% and decreases convergence time by more than 70%. We also\ntrained individual models for each subject to use as a majority-voting ensemble\nclassifier. In this scenario, using EA improved the 3-model ensemble accuracy\nby 3.7%. However, when compared to the shared model with EA, the ensemble\naccuracy was 3.62% lower.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "I.5.1; I.6.3; I.2.6"
    ],
    "primary_category": "eess.SP",
    "comment": "14 pages and 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10746v4",
    "published_date": "2024-01-19 15:13:30 UTC",
    "updated_date": "2024-05-23 00:54:29 UTC"
  },
  {
    "arxiv_id": "2401.10744v1",
    "title": "FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models",
    "authors": [
      "Ziqiang Yuan",
      "Kaiyuan Wang",
      "Shoutai Zhu",
      "Ye Yuan",
      "Jingya Zhou",
      "Yanlin Zhu",
      "Wenqi Wei"
    ],
    "abstract": "Large Language models (LLMs) usually rely on extensive training datasets. In\nthe financial domain, creating numerical reasoning datasets that include a mix\nof tables and long text often involves substantial manual annotation expenses.\nTo address the limited data resources and reduce the annotation cost, we\nintroduce FinLLMs, a method for generating financial question-answering data\nbased on common financial formulas using Large Language Models. First, we\ncompile a list of common financial formulas and construct a graph based on the\nvariables these formulas employ. We then augment the formula set by combining\nthose that share identical variables as new elements. Specifically, we explore\nformulas obtained by manual annotation and merge those formulas with shared\nvariables by traversing the constructed graph. Finally, utilizing GPT-3.5, we\ngenerate financial question-answering data that encompasses both tabular\ninformation and long textual content, building on the collected formula set.\nOur experiments demonstrate that synthetic data generated by FinLLMs\neffectively enhances the performance of several large-scale numerical reasoning\nmodels in the financial domain, outperforming two established benchmark\nfinancial question-answering datasets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under submission of IEEE Transactions",
    "pdf_url": "http://arxiv.org/pdf/2401.10744v1",
    "published_date": "2024-01-19 15:09:39 UTC",
    "updated_date": "2024-01-19 15:09:39 UTC"
  },
  {
    "arxiv_id": "2401.10733v2",
    "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
    "authors": [
      "Ran Elgedawy",
      "Ioana Danciu",
      "Maria Mahbub",
      "Sudarshan Srinivasan"
    ],
    "abstract": "Electronic health records (EHRs) house crucial patient data in clinical\nnotes. As these notes grow in volume and complexity, manual extraction becomes\nchallenging. This work introduces a natural language interface using large\nlanguage models (LLMs) for dynamic question-answering on clinical notes. Our\nchatbot, powered by Langchain and transformer-based LLMs, allows users to query\nin natural language, receiving relevant answers from clinical notes.\nExperiments, utilizing various embedding models and advanced LLMs, show Wizard\nVicuna's superior accuracy, albeit with high compute demands. Model\noptimization, including weight quantization, improves latency by approximately\n48 times. Promising results indicate potential, yet challenges such as model\nhallucinations and limited diverse medical case evaluations remain. Addressing\nthese gaps is crucial for unlocking the value in clinical notes and advancing\nAI-driven clinical decision-making.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10733v2",
    "published_date": "2024-01-19 14:50:22 UTC",
    "updated_date": "2024-07-02 15:14:29 UTC"
  },
  {
    "arxiv_id": "2401.10725v1",
    "title": "Proceedings 14th International Conference on Automated Deduction in Geometry",
    "authors": [
      "Pedro Quaresma",
      "Zoltán Kovács"
    ],
    "abstract": "ADG is a forum to exchange ideas and views, to present research results and\nprogress, and to demonstrate software tools at the intersection between\ngeometry and automated deduction. The conference is held every two years. The\nprevious editions of ADG were held in Hagenberg in 2021 (online, postponed from\n2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014,\nEdinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006,\nGainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and\nToulouse in 1996.\n  The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22,\n2023. This edition of ADG had an additional special focus topic, Deduction in\nEducation.\n  Invited Speakers: Julien Narboux, University of Strasbourg, France\n\"Formalisation, arithmetization and automatisation of geometry\"; Filip Mari\\'c,\nUniversity of Belgrade, Serbia, \"Automatization, formalization and\nvisualization of hyperbolic geometry\"; Zlatan Magajna, University of Ljubljana,\nSlovenia, \"Workshop OK Geometry\"",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CG",
      "cs.MS"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10725v1",
    "published_date": "2024-01-19 14:42:08 UTC",
    "updated_date": "2024-01-19 14:42:08 UTC"
  },
  {
    "arxiv_id": "2401.10712v5",
    "title": "Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge",
    "authors": [
      "Haibo Wang",
      "Weifeng Ge"
    ],
    "abstract": "With the breakthrough of multi-modal large language models, answering complex\nvisual questions that demand advanced reasoning abilities and world knowledge\nhas become a much more important testbed for developing AI models than ever.\nHowever, equipping AI models with robust cross-modality reasoning ability\nremains challenging since the cognition scheme of humans has not been\nunderstood systematically. In this paper, we believe that if we can collect\nvisual clues in the given image as much as possible, we will recognize the\nimage more accurately, understand the question better, recall relevant\nknowledge more easily, and finally reason out the answer. We discover these\nrich visual clues by mining question-answer pairs in images and sending them\ninto multi-modal large language models as prompts. We call the proposed method\nQ&A Prompts. Specifically, we first use the image-answer pairs and the\ncorresponding questions in the training set as inputs and outputs to train a\nvisual question generation model. Then, we use an image tagging model to\nidentify various instances and send packaged image-tag pairs into the visual\nquestion generation model to generate relevant questions with the extracted\nimage tags as answers. Finally, we encode these generated question-answer pairs\nas prompts with a visual-aware prompting module and send them into pre-trained\nmulti-modal large language models to reason out the final answers. Experimental\nresults show that, compared with state-of-the-art methods, our Q&A Prompts\nachieves substantial improvements on the challenging visual question answering\ndatasets requiring reasoning over diverse world knowledge, such as OK-VQA and\nA-OKVQA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV'24",
    "pdf_url": "http://arxiv.org/pdf/2401.10712v5",
    "published_date": "2024-01-19 14:22:29 UTC",
    "updated_date": "2024-10-12 08:21:44 UTC"
  },
  {
    "arxiv_id": "2401.10711v4",
    "title": "Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering",
    "authors": [
      "Haibo Wang",
      "Chenghang Lai",
      "Yixuan Sun",
      "Weifeng Ge"
    ],
    "abstract": "Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the information observed in videos. Despite the recent success of\nLarge Multimodal Models (LMMs) in image-language understanding and reasoning,\nthey deal with VideoQA insufficiently, by simply taking uniformly sampled\nframes as visual inputs, which ignores question-relevant visual clues.\nMoreover, there are no human annotations for question-critical timestamps in\nexisting VideoQA datasets. In light of this, we propose a novel weakly\nsupervised framework to enforce the LMMs to reason out the answers with\nquestion-critical moments as visual inputs. Specifically, we first fuse the\nquestion and answer pairs as event descriptions to find multiple keyframes as\ntarget moments and pseudo-labels, with the visual-language alignment capability\nof the CLIP models. With these pseudo-labeled keyframes as additionally weak\nsupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)\nmodule. GCG learns multiple Gaussian functions to characterize the temporal\nstructure of the video, and sample question-critical frames as positive moments\nto be the visual inputs of LMMs. Extensive experiments on several benchmarks\nverify the effectiveness of our framework, and we achieve substantial\nimprovements compared to previous state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by ACM Multimedia 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10711v4",
    "published_date": "2024-01-19 14:21:46 UTC",
    "updated_date": "2024-07-23 10:17:39 UTC"
  },
  {
    "arxiv_id": "2401.10700v1",
    "title": "Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model",
    "authors": [
      "Yinan Zheng",
      "Jianxiong Li",
      "Dongjie Yu",
      "Yujie Yang",
      "Shengbo Eben Li",
      "Xianyuan Zhan",
      "Jingjing Liu"
    ],
    "abstract": "Safe offline RL is a promising way to bypass risky online interactions\ntowards safe policy learning. Most existing methods only enforce soft\nconstraints, i.e., constraining safety violations in expectation below\nthresholds predetermined. This can lead to potentially unsafe outcomes, thus\nunacceptable in safety-critical scenarios. An alternative is to enforce the\nhard constraint of zero violation. However, this can be challenging in offline\nsetting, as it needs to strike the right balance among three highly intricate\nand correlated aspects: safety constraint satisfaction, reward maximization,\nand behavior regularization imposed by offline datasets. Interestingly, we\ndiscover that via reachability analysis of safe-control theory, the hard safety\nconstraint can be equivalently translated to identifying the largest feasible\nregion given the offline dataset. This seamlessly converts the original trilogy\nproblem to a feasibility-dependent objective, i.e., maximizing reward value\nwithin the feasible region while minimizing safety risks in the infeasible\nregion. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline\nRL), which allows safety constraint adherence, reward maximization, and offline\npolicy learning to be realized via three decoupled processes, while offering\nstrong safety performance and stability. In FISOR, the optimal policy for the\ntranslated optimization problem can be derived in a special form of weighted\nbehavior cloning. Thus, we propose a novel energy-guided diffusion model that\ndoes not require training a complicated time-dependent classifier to extract\nthe policy, greatly simplifying the training. We compare FISOR against\nbaselines on DSRL benchmark for safe offline RL. Evaluation results show that\nFISOR is the only method that can guarantee safety satisfaction in all tasks,\nwhile achieving top returns in most tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024, 30pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10700v1",
    "published_date": "2024-01-19 14:05:09 UTC",
    "updated_date": "2024-01-19 14:05:09 UTC"
  },
  {
    "arxiv_id": "2401.10690v4",
    "title": "Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models",
    "authors": [
      "Jorge Paz-Ruza",
      "Amparo Alonso-Betanzos",
      "Bertha Guijarro-Berdiñas",
      "Brais Cancela",
      "Carlos Eiras-Franco"
    ],
    "abstract": "Dyadic regression models, which output real-valued predictions for pairs of\nentities, are fundamental in many domains (e.g. obtaining user-product ratings\nin Recommender Systems) and promising and under exploration in others (e.g.\ntuning patient-drug dosages in precision pharmacology). In this work, we prove\nthat non-uniform observed value distributions of individual entities lead to\nsevere biases in state-of-the-art models, skewing predictions towards the\naverage of observed past values for the entity and providing worse-than-random\npredictive power in eccentric yet crucial cases; we name this phenomenon\neccentricity bias. We show that global error metrics like Root Mean Squared\nError (RMSE) are insufficient to capture this bias, and we introduce\nEccentricity-Area Under the Curve (EAUC) as a novel metric that can quantify it\nin all studied domains and models. We prove the intuitive interpretation of\nEAUC by experimenting with naive post-training bias corrections, and theorize\nother options to use EAUC to guide the construction of fair models. This work\ncontributes a bias-aware evaluation of dyadic regression to prevent unfairness\nin critical real-world applications of such systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10690v4",
    "published_date": "2024-01-19 13:41:08 UTC",
    "updated_date": "2025-03-07 08:40:19 UTC"
  },
  {
    "arxiv_id": "2401.10685v2",
    "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
    "authors": [
      "Xu Weng",
      "KV Ling",
      "Haochen Liu",
      "Kun Cao"
    ],
    "abstract": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10685v2",
    "published_date": "2024-01-19 13:32:55 UTC",
    "updated_date": "2024-08-21 06:10:02 UTC"
  },
  {
    "arxiv_id": "2401.10965v1",
    "title": "Decentralizing Coordination in Open Vehicle Fleets for Scalable and Dynamic Task Allocation",
    "authors": [
      "Marin Lujak",
      "Stefano Giordani",
      "Andrea Omicini",
      "Sascha Ossowski"
    ],
    "abstract": "One of the major challenges in the coordination of large, open,\ncollaborative, and commercial vehicle fleets is dynamic task allocation.\nSelf-concerned individually rational vehicle drivers have both local and global\nobjectives, which require coordination using some fair and efficient task\nallocation method. In this paper, we review the literature on scalable and\ndynamic task allocation focusing on deterministic and dynamic two-dimensional\nlinear assignment problems. We focus on multiagent system representation of\nopen vehicle fleets where dynamically appearing vehicles are represented by\nsoftware agents that should be allocated to a set of dynamically appearing\ntasks. We give a comparison and critical analysis of recent research results\nfocusing on centralized, distributed, and decentralized solution approaches.\nMoreover, we propose mathematical models for dynamic versions of the following\nassignment problems well known in combinatorial optimization: the assignment\nproblem, bottleneck assignment problem, fair matching problem, dynamic minimum\ndeviation assignment problem, $\\sum_{k}$-assignment problem, the semiassignment\nproblem, the assignment problem with side constraints, and the assignment\nproblem while recognizing agent qualification; all while considering the main\naspect of open vehicle fleets: random arrival of tasks and vehicles (agents)\nthat may become available after assisting previous tasks or by participating in\nthe fleet at times based on individual interest.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10965v1",
    "published_date": "2024-01-19 12:47:27 UTC",
    "updated_date": "2024-01-19 12:47:27 UTC"
  },
  {
    "arxiv_id": "2401.10660v2",
    "title": "Accelerating Multilingual Language Model for Excessively Tokenized Languages",
    "authors": [
      "Jimin Hong",
      "Gibbeum Lee",
      "Jaewoong Cho"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have remarkably enhanced\nperformances on a variety of tasks in multiple languages. However, tokenizers\nin LLMs trained primarily on English-centric corpora often overly fragment a\ntext into character or Unicode-level tokens in non-Roman alphabetic languages,\nleading to inefficient text generation. We introduce a simple yet effective\nframework to accelerate text generation in such languages. Our approach\ninvolves employing a new language model head with a vocabulary set tailored to\na specific target language for a pre-trained LLM. This is followed by\nfine-tuning the new head while incorporating a verification step to ensure the\nmodel's performance is preserved. We show that this targeted fine-tuning, while\nfreezing other model parameters, effectively reduces token fragmentation for\nthe target language. Our extensive experiments demonstrate that the proposed\nframework increases the generation speed by a factor of 1.7 while maintaining\nthe performance of pre-trained multilingual models on target monolingual tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2401.10660v2",
    "published_date": "2024-01-19 12:26:57 UTC",
    "updated_date": "2024-08-06 08:23:30 UTC"
  },
  {
    "arxiv_id": "2401.10643v1",
    "title": "A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges",
    "authors": [
      "Ali Amiri",
      "Aydin Kaya",
      "Ali Seydi Keceli"
    ],
    "abstract": "Vehicle re-identification (ReID) endeavors to associate vehicle images\ncollected from a distributed network of cameras spanning diverse traffic\nenvironments. This task assumes paramount importance within the spectrum of\nvehicle-centric technologies, playing a pivotal role in deploying Intelligent\nTransportation Systems (ITS) and advancing smart city initiatives. Rapid\nadvancements in deep learning have significantly propelled the evolution of\nvehicle ReID technologies in recent years. Consequently, undertaking a\ncomprehensive survey of methodologies centered on deep learning for vehicle\nre-identification has become imperative and inescapable. This paper extensively\nexplores deep learning techniques applied to vehicle ReID. It outlines the\ncategorization of these methods, encompassing supervised and unsupervised\napproaches, delves into existing research within these categories, introduces\ndatasets and evaluation criteria, and delineates forthcoming challenges and\npotential research directions. This comprehensive assessment examines the\nlandscape of deep learning in vehicle ReID and establishes a foundation and\nstarting point for future works. It aims to serve as a complete reference by\nhighlighting challenges and emerging trends, fostering advancements and\napplications in vehicle ReID utilizing deep learning models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10643v1",
    "published_date": "2024-01-19 11:45:10 UTC",
    "updated_date": "2024-01-19 11:45:10 UTC"
  },
  {
    "arxiv_id": "2401.10642v1",
    "title": "Fast Butterfly-Core Community Search For Large Labeled Graphs",
    "authors": [
      "JiaYi Du",
      "Yinghao Wu",
      "Wei Ai",
      "Tao Meng",
      "CanHao Xie",
      "KeQin Li"
    ],
    "abstract": "Community Search (CS) aims to identify densely interconnected subgraphs\ncorresponding to query vertices within a graph. However, existing heterogeneous\ngraph-based community search methods need help identifying cross-group\ncommunities and suffer from efficiency issues, making them unsuitable for large\ngraphs. This paper presents a fast community search model based on the\nButterfly-Core Community (BCC) structure for heterogeneous graphs. The Random\nWalk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate\nthe importance of vertices within communities, allowing leader vertices to be\nrapidly updated to maintain cross-group cohesion. Moreover, we devised a more\nefficient method for updating vertex distances, which minimizes vertex visits\nand enhances operational efficiency. Extensive experiments on several\nreal-world temporal graphs demonstrate the effectiveness and efficiency of this\nsolution.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "8 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10642v1",
    "published_date": "2024-01-19 11:44:09 UTC",
    "updated_date": "2024-01-19 11:44:09 UTC"
  },
  {
    "arxiv_id": "2401.10641v1",
    "title": "An Effective Index for Truss-based Community Search on Large Directed Graphs",
    "authors": [
      "Wei Ai",
      "CanHao Xie",
      "Tao Meng",
      "Yinghao Wu",
      "KeQin Li"
    ],
    "abstract": "Community search is a derivative of community detection that enables online\nand personalized discovery of communities and has found extensive applications\nin massive real-world networks. Recently, there needs to be more focus on the\ncommunity search issue within directed graphs, even though substantial research\nhas been carried out on undirected graphs. The recently proposed D-truss model\nhas achieved good results in the quality of retrieved communities. However,\nexisting D-truss-based work cannot perform efficient community searches on\nlarge graphs because it consumes too many computing resources to retrieve the\nmaximal D-truss. To overcome this issue, we introduce an innovative merge\nrelation known as D-truss-connected to capture the inherent density and\ncohesiveness of edges within D-truss. This relation allows us to partition all\nthe edges in the original graph into a series of D-truss-connected classes.\nThen, we construct a concise and compact index, ConDTruss, based on\nD-truss-connected. Using ConDTruss, the efficiency of maximum D-truss retrieval\nwill be greatly improved, making it a theoretically optimal approach.\nExperimental evaluations conducted on large directed graph certificate the\neffectiveness of our proposed method.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "8 pages, 8figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10641v1",
    "published_date": "2024-01-19 11:37:30 UTC",
    "updated_date": "2024-01-19 11:37:30 UTC"
  },
  {
    "arxiv_id": "2401.10640v1",
    "title": "A comprehensive study on fidelity metrics for XAI",
    "authors": [
      "Miquel Miró-Nicolau",
      "Antoni Jaume-i-Capó",
      "Gabriel Moyà-Alcover"
    ],
    "abstract": "The use of eXplainable Artificial Intelligence (XAI) systems has introduced a\nset of challenges that need resolution. Herein, we focus on how to correctly\nselect an XAI method, an open questions within the field. The inherent\ndifficulty of this task is due to the lack of a ground truth. Several authors\nhave proposed metrics to approximate the fidelity of different XAI methods.\nThese metrics lack verification and have concerning disagreements. In this\nstudy, we proposed a novel methodology to verify fidelity metrics, using a\nwell-known transparent model, namely a decision tree. This model allowed us to\nobtain explanations with perfect fidelity. Our proposal constitutes the first\nobjective benchmark for these metrics, facilitating a comparison of existing\nproposals, and surpassing existing methods. We applied our benchmark to assess\nthe existing fidelity metrics in two different experiments, each using public\ndatasets comprising 52,000 images. The images from these datasets had a size a\n128 by 128 pixels and were synthetic data that simplified the training process.\nAll metric values, indicated a lack of fidelity, with the best one showing a 30\n\\% deviation from the expected values for perfect explanation. Our\nexperimentation led us to conclude that the current fidelity metrics are not\nreliable enough to be used in real scenarios. From this finding, we deemed it\nnecessary to development new metrics, to avoid the detected problems, and we\nrecommend the usage of our proposal as a benchmark within the scientific\ncommunity to address these limitations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10640v1",
    "published_date": "2024-01-19 11:35:52 UTC",
    "updated_date": "2024-01-19 11:35:52 UTC"
  },
  {
    "arxiv_id": "2401.10603v1",
    "title": "ZnTrack -- Data as Code",
    "authors": [
      "Fabian Zills",
      "Moritz Schäfer",
      "Samuel Tovey",
      "Johannes Kästner",
      "Christian Holm"
    ],
    "abstract": "The past decade has seen tremendous breakthroughs in computation and there is\nno indication that this will slow any time soon. Machine learning, large-scale\ncomputing resources, and increased industry focus have resulted in rising\ninvestments in computer-driven solutions for data management, simulations, and\nmodel generation. However, with this growth in computation has come an even\nlarger expansion of data and with it, complexity in data storage, sharing, and\ntracking. In this work, we introduce ZnTrack, a Python-driven data versioning\ntool. ZnTrack builds upon established version control systems to provide a\nuser-friendly and easy-to-use interface for tracking parameters in experiments,\ndesigning workflows, and storing and sharing data. From this ability to reduce\nlarge datasets to a simple Python script emerges the concept of Data as Code, a\ncore component of the work presented here and an undoubtedly important concept\nas the age of computation continues to evolve. ZnTrack offers an open-source,\nFAIR data compatible Python package to enable users to harness these concepts\nof the future.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "22 pages, 10 figures, 2MB PDF",
    "pdf_url": "http://arxiv.org/pdf/2401.10603v1",
    "published_date": "2024-01-19 10:21:27 UTC",
    "updated_date": "2024-01-19 10:21:27 UTC"
  },
  {
    "arxiv_id": "2401.10589v1",
    "title": "Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local Search Solvers",
    "authors": [
      "Jiongzhi Zheng",
      "Zhuo Chen",
      "Chu-Min Li",
      "Kun He"
    ],
    "abstract": "MaxSAT is an optimization version of the famous NP-complete Satisfiability\nproblem (SAT). Algorithms for MaxSAT mainly include complete solvers and local\nsearch incomplete solvers. In many complete solvers, once a better solution is\nfound, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to\nenforce the algorithm to find better solutions. In many local search\nalgorithms, clause weighting is a key technique for effectively guiding the\nsearch directions. In this paper, we propose to transfer the SPB constraint\ninto the clause weighting system of the local search method, leading the\nalgorithm to better solutions. We further propose an adaptive clause weighting\nstrategy that breaks the tradition of using constant values to adjust clause\nweights. Based on the above methods, we propose a new local search algorithm\ncalled SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT\nlocal search solvers. Extensive experiments demonstrate the excellent\nperformance of the proposed methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10589v1",
    "published_date": "2024-01-19 09:59:02 UTC",
    "updated_date": "2024-01-19 09:59:02 UTC"
  },
  {
    "arxiv_id": "2401.10586v1",
    "title": "PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks",
    "authors": [
      "Ping Guo",
      "Zhiyuan Yang",
      "Xi Lin",
      "Qingchuan Zhao",
      "Qingfu Zhang"
    ],
    "abstract": "Black-box query-based attacks constitute significant threats to Machine\nLearning as a Service (MLaaS) systems since they can generate adversarial\nexamples without accessing the target model's architecture and parameters.\nTraditional defense mechanisms, such as adversarial training, gradient masking,\nand input transformations, either impose substantial computational costs or\ncompromise the test accuracy of non-adversarial inputs. To address these\nchallenges, we propose an efficient defense mechanism, PuriDefense, that\nemploys random patch-wise purifications with an ensemble of lightweight\npurification models at a low level of inference cost. These models leverage the\nlocal implicit function and rebuild the natural image manifold. Our theoretical\nanalysis suggests that this approach slows down the convergence of query-based\nattacks by incorporating randomness into purifications. Extensive experiments\non CIFAR-10 and ImageNet validate the effectiveness of our proposed\npurifier-based defense mechanism, demonstrating significant improvements in\nrobustness against query-based attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10586v1",
    "published_date": "2024-01-19 09:54:23 UTC",
    "updated_date": "2024-01-19 09:54:23 UTC"
  },
  {
    "arxiv_id": "2401.10568v2",
    "title": "CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents",
    "authors": [
      "Siyuan Qi",
      "Shuo Chen",
      "Yexin Li",
      "Xiangyu Kong",
      "Junqi Wang",
      "Bangcheng Yang",
      "Pring Wong",
      "Yifan Zhong",
      "Xiaoyuan Zhang",
      "Zhaowei Zhang",
      "Nian Liu",
      "Wei Wang",
      "Yaodong Yang",
      "Song-Chun Zhu"
    ],
    "abstract": "The generalization of decision-making agents encompasses two fundamental\nelements: learning from past experiences and reasoning in novel contexts.\nHowever, the predominant emphasis in most interactive environments is on\nlearning, often at the expense of complexity in reasoning. In this paper, we\nintroduce CivRealm, an environment inspired by the Civilization game.\nCivilization's profound alignment with human history and society necessitates\nsophisticated learning, while its ever-changing situations demand strong\nreasoning to generalize. Particularly, CivRealm sets up an\nimperfect-information general-sum game with a changing number of players; it\npresents a plethora of complex features, challenging the agent to deal with\nopen-ended stochastic environments that require diplomacy and negotiation\nskills. Within CivRealm, we provide interfaces for two typical agent types:\ntensor-based agents that focus on learning, and language-based agents that\nemphasize reasoning. To catalyze further research, we present initial results\nfor both paradigms. The canonical RL-based agents exhibit reasonable\nperformance in mini-games, whereas both RL- and LLM-based agents struggle to\nmake substantial progress in the full game. Overall, CivRealm stands as a\nunique learning and reasoning challenge for decision-making agents. The code is\navailable at https://github.com/bigai-ai/civrealm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10568v2",
    "published_date": "2024-01-19 09:14:11 UTC",
    "updated_date": "2024-03-12 08:24:37 UTC"
  },
  {
    "arxiv_id": "2401.10559v1",
    "title": "OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy",
    "authors": [
      "Haowen Wang",
      "Tao Sun",
      "Kaixiang Ji",
      "Jian Wang",
      "Cong Fan",
      "Jinjie Gu"
    ],
    "abstract": "We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel\nmulti-adapter method, OrchMoE, which capitalizes on modular skill architecture\nfor enhanced forward transfer in neural networks. Unlike prior models that\ndepend on explicit task identification inputs, OrchMoE automatically discerns\ntask categories, streamlining the learning process. This is achieved through an\nintegrated mechanism comprising an Automatic Task Classification module and a\nTask-Skill Allocation module, which collectively deduce task-specific\nclassifications and tailor skill allocation matrices. Our extensive evaluations\non the 'Super Natural Instructions' dataset, featuring 1,600 diverse\ninstructional tasks, indicate that OrchMoE substantially outperforms comparable\nmulti-adapter baselines in terms of both performance and sample utilization\nefficiency, all while operating within the same parameter constraints. These\nfindings suggest that OrchMoE offers a significant leap forward in multi-task\nlearning efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10559v1",
    "published_date": "2024-01-19 08:50:54 UTC",
    "updated_date": "2024-01-19 08:50:54 UTC"
  },
  {
    "arxiv_id": "2401.10544v1",
    "title": "AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks",
    "authors": [
      "Yun Liang",
      "Hai Lin",
      "Shaojian Qiu",
      "Yihang Zhang"
    ],
    "abstract": "Recently, Transformers have been introduced into the field of acoustics\nrecognition. They are pre-trained on large-scale datasets using methods such as\nsupervised learning and semi-supervised learning, demonstrating robust\ngenerality--It fine-tunes easily to downstream tasks and shows more robust\nperformance. However, the predominant fine-tuning method currently used is\nstill full fine-tuning, which involves updating all parameters during training.\nThis not only incurs significant memory usage and time costs but also\ncompromises the model's generality. Other fine-tuning methods either struggle\nto address this issue or fail to achieve matching performance. Therefore, we\nconducted a comprehensive analysis of existing fine-tuning methods and proposed\nan efficient fine-tuning approach based on Adapter tuning, namely AAT. The core\nidea is to freeze the audio Transformer model and insert extra learnable\nAdapters, efficiently acquiring downstream task knowledge without compromising\nthe model's original generality. Extensive experiments have shown that our\nmethod achieves performance comparable to or even superior to full fine-tuning\nwhile optimizing only 7.118% of the parameters. It also demonstrates\nsuperiority over other fine-tuning methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Preprint version for ICASSP 2024, Korea",
    "pdf_url": "http://arxiv.org/pdf/2401.10544v1",
    "published_date": "2024-01-19 08:07:59 UTC",
    "updated_date": "2024-01-19 08:07:59 UTC"
  },
  {
    "arxiv_id": "2402.06634v1",
    "title": "SocraSynth: Multi-LLM Reasoning with Conditional Statistics",
    "authors": [
      "Edward Y. Chang"
    ],
    "abstract": "Large language models (LLMs), while promising, face criticisms for biases,\nhallucinations, and a lack of reasoning capability. This paper introduces\nSocraSynth, a multi-LLM agent reasoning platform developed to mitigate these\nissues. SocraSynth utilizes conditional statistics and systematic context\nenhancement through continuous arguments, alongside adjustable debate\ncontentiousness levels. The platform typically involves a human moderator and\ntwo LLM agents representing opposing viewpoints on a given subject. SocraSynth\noperates in two main phases: knowledge generation and reasoning evaluation. In\nthe knowledge generation phase, the moderator defines the debate topic and\ncontentiousness level, prompting the agents to formulate supporting arguments\nfor their respective stances. The reasoning evaluation phase then employs\nSocratic reasoning and formal logic principles to appraise the quality of the\narguments presented. The dialogue concludes with the moderator adjusting the\ncontentiousness from confrontational to collaborative, gathering final,\nconciliatory remarks to aid in human reasoning and decision-making. Through\ncase studies in three distinct application domains, this paper showcases\nSocraSynth's effectiveness in fostering rigorous research, dynamic reasoning,\ncomprehensive assessment, and enhanced collaboration. This underscores the\nvalue of multi-agent interactions in leveraging LLMs for advanced knowledge\nextraction and decision-making support.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "1 figure, 6 tables, 6 appendices",
    "pdf_url": "http://arxiv.org/pdf/2402.06634v1",
    "published_date": "2024-01-19 07:16:21 UTC",
    "updated_date": "2024-01-19 07:16:21 UTC"
  },
  {
    "arxiv_id": "2401.10529v2",
    "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences",
    "authors": [
      "Xiyao Wang",
      "Yuhang Zhou",
      "Xiaoyu Liu",
      "Hongjin Lu",
      "Yuancheng Xu",
      "Feihong He",
      "Jaehong Yoon",
      "Taixi Lu",
      "Gedas Bertasius",
      "Mohit Bansal",
      "Huaxiu Yao",
      "Furong Huang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in\nhandling a variety of visual-language tasks. However, current MLLM benchmarks\nare predominantly designed to evaluate reasoning based on static information\nabout a single image, and the ability of modern MLLMs to extrapolate from image\nsequences, which is essential for understanding our ever-changing world, has\nbeen less investigated. To address this challenge, this paper introduces\nMementos, a new benchmark designed to assess MLLMs' sequential image reasoning\nabilities. Mementos features 4,761 diverse image sequences with varying\nlengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning\nperformance. Through a careful evaluation of nine recent MLLMs on Mementos,\nincluding GPT-4V and Gemini, we find that they struggle to accurately describe\ndynamic information about given image sequences, often leading to\nhallucinations/misrepresentations of objects and their corresponding behaviors.\nOur quantitative analysis and case studies identify three key factors impacting\nMLLMs' sequential image reasoning: the correlation between object and\nbehavioral hallucinations, the influence of cooccurring behaviors, and the\ncompounding impact of behavioral hallucinations. Our dataset is available at\nhttps://github.com/umd-huang-lab/Mementos.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "27 pages, 23 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10529v2",
    "published_date": "2024-01-19 07:10:13 UTC",
    "updated_date": "2024-01-25 04:11:57 UTC"
  },
  {
    "arxiv_id": "2401.10521v2",
    "title": "Cross-lingual Editing in Multilingual Language Models",
    "authors": [
      "Himanshu Beniwal",
      "Kowsik Nandagopan D",
      "Mayank Singh"
    ],
    "abstract": "The training of large language models (LLMs) necessitates substantial data\nand computational resources, and updating outdated LLMs entails significant\nefforts and resources. While numerous model editing techniques (METs) have\nemerged to efficiently update model outputs without retraining, their\neffectiveness in multilingual LLMs, where knowledge is stored in diverse\nlanguages, remains an underexplored research area. This research paper\nintroduces the cross-lingual model editing (\\textbf{XME}) paradigm, wherein a\nfact is edited in one language, and the subsequent update propagation is\nobserved across other languages. To investigate the XME paradigm, we conducted\nexperiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts:\n\\textit{Latin} (English, French, and Spanish) and \\textit{Indic} (Hindi,\nGujarati, and Bengali). The results reveal notable performance limitations of\nstate-of-the-art METs under the XME setting, mainly when the languages involved\nbelong to two distinct script families. These findings highlight the need for\nfurther research and development of XME techniques to address these challenges.\nFor more comprehensive information, the dataset used in this research and the\nassociated code are publicly available at the following\nURL\\url{https://github.com/lingo-iitgn/XME}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10521v2",
    "published_date": "2024-01-19 06:54:39 UTC",
    "updated_date": "2024-02-03 05:59:49 UTC"
  },
  {
    "arxiv_id": "2401.10516v1",
    "title": "Episodic Reinforcement Learning with Expanded State-reward Space",
    "authors": [
      "Dayang Liang",
      "Yaru Zhang",
      "Yunlong Liu"
    ],
    "abstract": "Empowered by deep neural networks, deep reinforcement learning (DRL) has\ndemonstrated tremendous empirical successes in various domains, including\ngames, health care, and autonomous driving. Despite these advancements, DRL is\nstill identified as data-inefficient as effective policies demand vast numbers\nof environmental samples. Recently, episodic control (EC)-based model-free DRL\nmethods enable sample efficiency by recalling past experiences from episodic\nmemory. However, existing EC-based methods suffer from the limitation of\npotential misalignment between the state and reward spaces for neglecting the\nutilization of (past) retrieval states with extensive information, which\nprobably causes inaccurate value estimation and degraded policy performance. To\ntackle this issue, we introduce an efficient EC-based DRL framework with\nexpanded state-reward space, where the expanded states used as the input and\nthe expanded rewards used in the training both contain historical and current\ninformation. To be specific, we reuse the historical states retrieved by EC as\npart of the input states and integrate the retrieved MC-returns into the\nimmediate reward in each interactive transition. As a result, our method is\nable to simultaneously achieve the full utilization of retrieval information\nand the better evaluation of state values by a Temporal Difference (TD) loss.\nEmpirical results on challenging Box2d and Mujoco tasks demonstrate the\nsuperiority of our method over a recent sibling method and common baselines.\nFurther, we also verify our method's effectiveness in alleviating Q-value\noverestimation by additional experiments of Q-value comparison.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAMAS'24",
    "pdf_url": "http://arxiv.org/pdf/2401.10516v1",
    "published_date": "2024-01-19 06:14:36 UTC",
    "updated_date": "2024-01-19 06:14:36 UTC"
  },
  {
    "arxiv_id": "2401.10510v3",
    "title": "When Large Language Models Meet Evolutionary Algorithms: Potential Enhancements and Challenges",
    "authors": [
      "Chao Wang",
      "Jiaxuan Zhao",
      "Licheng Jiao",
      "Lingling Li",
      "Fang Liu",
      "Shuyuan Yang"
    ],
    "abstract": "Pre-trained large language models (LLMs) exhibit powerful capabilities for\ngenerating natural text. Evolutionary algorithms (EAs) can discover diverse\nsolutions to complex real-world problems. Motivated by the common collective\nand directionality of text generation and evolution, this paper first\nillustrates the conceptual parallels between LLMs and EAs at a micro level,\nwhich includes multiple one-to-one key characteristics: token representation\nand individual representation, position encoding and fitness shaping, position\nembedding and selection, Transformers block and reproduction, and model\ntraining and parameter adaptation. These parallels highlight potential\nopportunities for technical advancements in both LLMs and EAs. Subsequently, we\nanalyze existing interdisciplinary research from a macro perspective to uncover\ncritical challenges, with a particular focus on evolutionary fine-tuning and\nLLM-enhanced EAs. These analyses not only provide insights into the\nevolutionary mechanisms behind LLMs but also offer potential directions for\nenhancing the capabilities of artificial agents.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "The article has been accepted for publication in Research",
    "pdf_url": "http://arxiv.org/pdf/2401.10510v3",
    "published_date": "2024-01-19 05:58:30 UTC",
    "updated_date": "2025-03-07 05:29:18 UTC"
  },
  {
    "arxiv_id": "2401.10956v1",
    "title": "AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment",
    "authors": [
      "Sida Peng",
      "Wojciech Swiatek",
      "Allen Gao",
      "Paul Cullivan",
      "Haoge Chang"
    ],
    "abstract": "In recent years, generative AI has undergone major advancements,\ndemonstrating significant promise in augmenting human productivity. Notably,\nlarge language models (LLM), with ChatGPT-4 as an example, have drawn\nconsiderable attention. Numerous articles have examined the impact of LLM-based\ntools on human productivity in lab settings and designed tasks or in\nobservational studies. Despite recent advances, field experiments applying\nLLM-based tools in realistic settings are limited. This paper presents the\nfindings of a field randomized controlled trial assessing the effectiveness of\nLLM-based tools in providing unmonitored support services for information\nretrieval.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10956v1",
    "published_date": "2024-01-19 05:54:35 UTC",
    "updated_date": "2024-01-19 05:54:35 UTC"
  },
  {
    "arxiv_id": "2401.10506v1",
    "title": "FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis",
    "authors": [
      "Chao Zhang",
      "Yuren Mao",
      "Yijiang Fan",
      "Yu Mi",
      "Yunjun Gao",
      "Lu Chen",
      "Dongfang Lou",
      "Jinshu Lin"
    ],
    "abstract": "Text-to-SQL, which provides zero-code interface for operating relational\ndatabases, has gained much attention in financial analysis; because, financial\nprofessionals may not well-skilled in SQL programming. However, until now,\nthere is no practical Text-to-SQL benchmark dataset for financial analysis, and\nexisting Text-to-SQL methods have not considered the unique characteristics of\ndatabases in financial applications, such as commonly existing wide tables. To\naddress these issues, we collect a practical Text-to-SQL benchmark dataset and\npropose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL\nframework for financial analysis. The benchmark dataset, BULL, is collected\nfrom the practical financial analysis business of Hundsun Technologies Inc.,\nincluding databases for fund, stock, and macro economy. Besides, the proposed\nLLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for\nfinancial Text-to-SQL from the perspectives of prompt construction,\nparameter-efficient fine-tuning and output calibration. Extensive experimental\nresults on BULL demonstrate that FinSQL achieves the state-of-the-art\nText-to-SQL performance at a small cost; furthermore, FinSQL can bring up to\n36.64% performance improvement in scenarios requiring few-shot cross-database\nmodel transfer.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10506v1",
    "published_date": "2024-01-19 05:48:07 UTC",
    "updated_date": "2024-01-19 05:48:07 UTC"
  },
  {
    "arxiv_id": "2401.10495v1",
    "title": "Causal Layering via Conditional Entropy",
    "authors": [
      "Itai Feigenbaum",
      "Devansh Arpit",
      "Huan Wang",
      "Shelby Heinecke",
      "Juan Carlos Niebles",
      "Weiran Yao",
      "Caiming Xiong",
      "Silvio Savarese"
    ],
    "abstract": "Causal discovery aims to recover information about an unobserved causal graph\nfrom the observable data it generates. Layerings are orderings of the variables\nwhich place causes before effects. In this paper, we provide ways to recover\nlayerings of a graph by accessing the data via a conditional entropy oracle,\nwhen distributions are discrete. Our algorithms work by repeatedly removing\nsources or sinks from the graph. Under appropriate assumptions and\nconditioning, we can separate the sources or sinks from the remainder of the\nnodes by comparing their conditional entropy to the unconditional entropy of\ntheir noise. Our algorithms are provably correct and run in worst-case\nquadratic time. The main assumptions are faithfulness and injective noise, and\neither known noise entropies or weakly monotonically increasing noise entropies\nalong directed paths. In addition, we require one of either a very mild\nextension of faithfulness, or strictly monotonically increasing noise\nentropies, or expanding noise injectivity to include an additional single\nargument in the structural functions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10495v1",
    "published_date": "2024-01-19 05:18:28 UTC",
    "updated_date": "2024-01-19 05:18:28 UTC"
  },
  {
    "arxiv_id": "2401.10484v1",
    "title": "Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning",
    "authors": [
      "Rajaram R",
      "Manoj Bharadhwaj",
      "Vasan VS",
      "Nargis Pervin"
    ],
    "abstract": "This study introduces an innovative approach aimed at the efficient pruning\nof neural networks, with a particular focus on their deployment on edge\ndevices. Our method involves the integration of the Lottery Ticket Hypothesis\n(LTH) with the Knowledge Distillation (KD) framework, resulting in the\nformulation of three distinct pruning models. These models have been developed\nto address scalability issue in recommender systems, whereby the complexities\nof deep learning models have hindered their practical deployment. With\njudicious application of the pruning techniques, we effectively curtail the\npower consumption and model dimensions without compromising on accuracy.\nEmpirical evaluation has been performed using two real world datasets from\ndiverse domains against two baselines. Gratifyingly, our approaches yielded a\nGPU computation-power reduction of up to 66.67%. Notably, our study contributes\nto the field of recommendation system by pioneering the application of LTH and\nKD.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted in WITS 2023 as a workshop paper",
    "pdf_url": "http://arxiv.org/pdf/2401.10484v1",
    "published_date": "2024-01-19 04:17:50 UTC",
    "updated_date": "2024-01-19 04:17:50 UTC"
  },
  {
    "arxiv_id": "2401.10480v1",
    "title": "Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning",
    "authors": [
      "Yiwei Li",
      "Peiwen Yuan",
      "Shaoxiong Feng",
      "Boyuan Pan",
      "Xinglin Wang",
      "Bin Sun",
      "Heda Wang",
      "Kan Li"
    ],
    "abstract": "Self-consistency (SC) has been a widely used decoding strategy for\nchain-of-thought reasoning. Despite bringing significant performance\nimprovements across a variety of multi-step reasoning tasks, it is a high-cost\nmethod that requires multiple sampling with the preset size. In this paper, we\npropose a simple and scalable sampling process, \\textbf{E}arly-Stopping\n\\textbf{S}elf-\\textbf{C}onsistency (ESC), to greatly reduce the cost of SC\nwithout sacrificing performance. On this basis, one control scheme for ESC is\nfurther derivated to dynamically choose the performance-cost balance for\ndifferent tasks and models. To demonstrate ESC's effectiveness, we conducted\nextensive experiments on three popular categories of reasoning tasks:\narithmetic, commonsense and symbolic reasoning over language models with\nvarying scales. The empirical results show that ESC reduces the average number\nof sampling of chain-of-thought reasoning by a significant margin on six\nbenchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%),\nCommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while\nattaining comparable performances.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10480v1",
    "published_date": "2024-01-19 04:03:59 UTC",
    "updated_date": "2024-01-19 04:03:59 UTC"
  },
  {
    "arxiv_id": "2401.10474v2",
    "title": "LDReg: Local Dimensionality Regularized Self-Supervised Learning",
    "authors": [
      "Hanxun Huang",
      "Ricardo J. G. B. Campello",
      "Sarah Monazam Erfani",
      "Xingjun Ma",
      "Michael E. Houle",
      "James Bailey"
    ],
    "abstract": "Representations learned via self-supervised learning (SSL) can be susceptible\nto dimensional collapse, where the learned representation subspace is of\nextremely low dimensionality and thus fails to represent the full data\ndistribution and modalities. Dimensional collapse also known as the\n\"underfilling\" phenomenon is one of the major causes of degraded performance on\ndownstream tasks. Previous work has investigated the dimensional collapse\nproblem of SSL at a global level. In this paper, we demonstrate that\nrepresentations can span over high dimensional space globally, but collapse\nlocally. To address this, we propose a method called $\\textit{local\ndimensionality regularization (LDReg)}$. Our formulation is based on the\nderivation of the Fisher-Rao metric to compare and optimize local distance\ndistributions at an asymptotically small radius for each data point. By\nincreasing the local intrinsic dimensionality, we demonstrate through a range\nof experiments that LDReg improves the representation quality of SSL. The\nresults also show that LDReg can regularize dimensionality at both local and\nglobal levels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10474v2",
    "published_date": "2024-01-19 03:50:19 UTC",
    "updated_date": "2024-03-14 04:49:05 UTC"
  },
  {
    "arxiv_id": "2401.10471v5",
    "title": "DeepEdit: Knowledge Editing as Decoding with Constraints",
    "authors": [
      "Yiwei Wang",
      "Muhao Chen",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ],
    "abstract": "How to edit the knowledge in multi-step reasoning has become the major\nchallenge in the knowledge editing (KE) of large language models (LLMs). The\ndifficulty arises because the hallucinations of LLMs during multi-step\nreasoning often lead to incorrect use of new knowledge and incorrect answers.\nTo address this issue, we design decoding constraints to \"regulate\" LLMs'\nreasoning, enhancing logical coherence when incorporating new knowledge. We\npropose a new KE framework: DEEPEDIT (Depth-first Search-based Constrained\nDecoding for Knowledge Editing), which enhances LLMs's ability to generate\ncoherent reasoning chains with new knowledge through depth-first search. Our\nsearch selects the most important knowledge that satisfies our constraints as\nthe reasoning step to efficiently increase the reasoning depth. In addition to\nDEEPEDIT, we propose two new KE benchmarks: MQUAKE-2002 and MQUAKE-HARD, which\nprovide more precise and challenging assessments of KE approaches.\nQualitatively, DEEPEDIT enables LLMs to produce succinct and coherent reasoning\nchains involving new knowledge. Quantitatively, it yields significant\nimprovements on multiple KE benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10471v5",
    "published_date": "2024-01-19 03:48:27 UTC",
    "updated_date": "2024-11-09 03:15:16 UTC"
  },
  {
    "arxiv_id": "2401.10467v2",
    "title": "Learning Backdoors for Mixed Integer Linear Programs with Contrastive Learning",
    "authors": [
      "Junyang Cai",
      "Taoan Huang",
      "Bistra Dilkina"
    ],
    "abstract": "Many real-world problems can be efficiently modeled as Mixed Integer Linear\nPrograms (MILPs) and solved with the Branch-and-Bound method. Prior work has\nshown the existence of MILP backdoors, small sets of variables such that\nprioritizing branching on them when possible leads to faster running times.\nHowever, finding high-quality backdoors that improve running times remains an\nopen question. Previous work learns to estimate the relative solver speed of\nrandomly sampled backdoors through ranking and then decide whether to use the\nhighest-ranked backdoor candidate. In this paper, we utilize the Monte-Carlo\ntree search method to collect backdoors for training, rather than relying on\nrandom sampling, and adapt a contrastive learning framework to train a Graph\nAttention Network model to predict backdoors. Our method, evaluated on several\ncommon MILP problem domains, demonstrates performance improvements over both\nGurobi and previous models.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10467v2",
    "published_date": "2024-01-19 03:39:43 UTC",
    "updated_date": "2024-08-01 01:07:35 UTC"
  },
  {
    "arxiv_id": "2401.10463v3",
    "title": "Critical Data Size of Language Models from a Grokking Perspective",
    "authors": [
      "Xuekai Zhu",
      "Yao Fu",
      "Bowen Zhou",
      "Zhouhan Lin"
    ],
    "abstract": "We explore the critical data size in language models, a threshold that marks\na fundamental shift from quick memorization to slow generalization. We\nformalize the phase transition under the grokking configuration into the Data\nEfficiency Hypothesis and identify data insufficiency, sufficiency, and surplus\nregimes in language models training dynamics. We develop a grokking\nconfiguration to reproduce grokking on simplistic language models stably by\nrescaling initialization and weight decay. We show that generalization occurs\nonly when language models reach a critical size. We analyze grokking across\nsample-wise and model-wise, verifying the proposed data efficiency hypothesis.\nOur experiments reveal smoother phase transitions occurring at the critical\ndataset size for language datasets. As the model size increases, this critical\npoint also becomes larger, indicating that larger models require more data. Our\nresults deepen the understanding of language model training, offering a novel\nperspective on the role of data in the learning mechanism of language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10463v3",
    "published_date": "2024-01-19 03:24:36 UTC",
    "updated_date": "2024-05-23 03:06:02 UTC"
  },
  {
    "arxiv_id": "2401.10447v1",
    "title": "Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition",
    "authors": [
      "Yu Yu",
      "Chao-Han Huck Yang",
      "Tuan Dinh",
      "Sungho Ryu",
      "Jari Kolehmainen",
      "Roger Ren",
      "Denis Filimonov",
      "Prashanth G. Shivakumar",
      "Ankur Gandhe",
      "Ariya Rastow",
      "Jia Xu",
      "Ivan Bulyko",
      "Andreas Stolcke"
    ],
    "abstract": "The use of low-rank adaptation (LoRA) with frozen pretrained language models\n(PLMs) has become increasing popular as a mainstream, resource-efficient\nmodeling approach for memory-constrained hardware. In this study, we first\nexplore how to enhance model performance by introducing various LoRA training\nstrategies, achieving relative word error rate reductions of 3.50\\% on the\npublic Librispeech dataset and of 3.67\\% on an internal dataset in the\nmessaging domain. To further characterize the stability of LoRA-based\nsecond-pass speech recognition models, we examine robustness against input\nperturbations. These perturbations are rooted in homophone replacements and a\nnovel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both\ndesigned to measure the relative degradation in the performance of rescoring\nmodels. Our experimental results indicate that while advanced variants of LoRA,\nsuch as dynamic rank-allocated LoRA, lead to performance degradation in\n$1$-best perturbation, they alleviate the degradation in $N$-best perturbation.\nThis finding is in comparison to fully-tuned models and vanilla LoRA tuning\nbaselines, suggesting that a comprehensive selection is needed when using\nLoRA-based adaptation for compute-cost savings and robust language modeling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10447v1",
    "published_date": "2024-01-19 01:30:16 UTC",
    "updated_date": "2024-01-19 01:30:16 UTC"
  },
  {
    "arxiv_id": "2401.10446v1",
    "title": "Large Language Models are Efficient Learners of Noise-Robust Speech Recognition",
    "authors": [
      "Yuchen Hu",
      "Chen Chen",
      "Chao-Han Huck Yang",
      "Ruizhe Li",
      "Chao Zhang",
      "Pin-Yu Chen",
      "EnSiong Chng"
    ],
    "abstract": "Recent advances in large language models (LLMs) have promoted generative\nerror correction (GER) for automatic speech recognition (ASR), which leverages\nthe rich linguistic knowledge and powerful reasoning ability of LLMs to improve\nrecognition results. The latest work proposes a GER benchmark with HyPoradise\ndataset to learn the mapping from ASR N-best hypotheses to ground-truth\ntranscription by efficient LLM finetuning, which shows great effectiveness but\nlacks specificity on noise-robust ASR. In this work, we extend the benchmark to\nnoisy conditions and investigate if we can teach LLMs to perform denoising for\nGER just like what robust ASR do}, where one solution is introducing noise\ninformation as a conditioner into LLM. However, directly incorporating noise\nembeddings from audio encoder could harm the LLM tuning due to cross-modality\ngap. To this end, we propose to extract a language-space noise embedding from\nthe N-best list to represent the noise conditions of source speech, which can\npromote the denoising process in GER. Furthermore, in order to enhance its\nrepresentation ability of audio noise, we design a knowledge distillation (KD)\napproach via mutual information estimation to distill the real noise\ninformation in audio embeddings to our language embedding. Experiments on\nvarious latest LLMs demonstrate our approach achieves a new breakthrough with\nup to 53.9% correction improvement in terms of word error rate while with\nlimited training data. Analysis shows that our language-space noise embedding\ncan well represent the noise conditions of source speech, under which\noff-the-shelf LLMs show strong ability of language-space denoising.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2024, Spotlight top 5%, 24 pages. This work will be\n  open sourced at: https://github.com/YUCHEN005/RobustGER under MIT license",
    "pdf_url": "http://arxiv.org/pdf/2401.10446v1",
    "published_date": "2024-01-19 01:29:27 UTC",
    "updated_date": "2024-01-19 01:29:27 UTC"
  },
  {
    "arxiv_id": "2401.10444v1",
    "title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
    "authors": [
      "Ron Sun"
    ],
    "abstract": "The paper discusses what is needed to address the limitations of current\nLLM-centered AI systems. The paper argues that incorporating insights from\nhuman cognition and psychology, as embodied by a computational cognitive\narchitecture, can help develop systems that are more capable, more reliable,\nand more human-like. It emphasizes the importance of the dual-process\narchitecture and the hybrid neuro-symbolic approach in addressing the\nlimitations of current LLMs. In the opposite direction, the paper also\nhighlights the need for an overhaul of computational cognitive architectures to\nbetter reflect advances in AI and computing technology. Overall, the paper\nadvocates for a multidisciplinary, mutually beneficial approach towards\ndeveloping better models both for AI and for understanding the human mind.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10444v1",
    "published_date": "2024-01-19 01:14:45 UTC",
    "updated_date": "2024-01-19 01:14:45 UTC"
  },
  {
    "arxiv_id": "2401.10431v1",
    "title": "Learning a Prior for Monte Carlo Search by Replaying Solutions to Combinatorial Problems",
    "authors": [
      "Tristan Cazenave"
    ],
    "abstract": "Monte Carlo Search gives excellent results in multiple difficult\ncombinatorial problems. Using a prior to perform non uniform playouts during\nthe search improves a lot the results compared to uniform playouts. Handmade\nheuristics tailored to the combinatorial problem are often used as priors. We\npropose a method to automatically compute a prior. It uses statistics on solved\nproblems. It is a simple and general method that incurs no computational cost\nat playout time and that brings large performance gains. The method is applied\nto three difficult combinatorial problems: Latin Square Completion, Kakuro, and\nInverse RNA Folding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10431v1",
    "published_date": "2024-01-19 00:22:31 UTC",
    "updated_date": "2024-01-19 00:22:31 UTC"
  },
  {
    "arxiv_id": "2401.10428v1",
    "title": "Understanding Learning through the Lens of Dynamical Invariants",
    "authors": [
      "Alex Ushveridze"
    ],
    "abstract": "This paper proposes a novel perspective on learning, positing it as the\npursuit of dynamical invariants -- data combinations that remain constant or\nexhibit minimal change over time as a system evolves. This concept is\nunderpinned by both informational and physical principles, rooted in the\ninherent properties of these invariants. Firstly, their stability makes them\nideal for memorization and integration into associative networks, forming the\nbasis of our knowledge structures. Secondly, the predictability of these stable\ninvariants makes them valuable sources of usable energy, quantifiable as kTln2\nper bit of accurately predicted information. This energy can be harnessed to\nexplore new transformations, rendering learning systems energetically\nautonomous and increasingly effective. Such systems are driven to continuously\nseek new data invariants as energy sources. The paper further explores several\nmeta-architectures of autonomous, self-propelled learning agents that utilize\npredictable information patterns as a source of usable energy.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.10428v1",
    "published_date": "2024-01-19 00:13:44 UTC",
    "updated_date": "2024-01-19 00:13:44 UTC"
  }
]
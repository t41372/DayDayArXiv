{
  "date": "2025-06-01",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-01 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼æˆ‘æ˜¯ä½ çš„ç§‘ç ”åŠ©æ‰‹ Gemini Enterpriseã€‚\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆæ˜¯ç²¾å½©çº·å‘ˆçš„ä¸€å¤©ï¼Œ**AI Agent çš„è‡ªä¸»æ€§**ï¼ˆå·¥å…·å‘ç°ã€ç§‘ç ”é¢„æµ‹ï¼‰å’Œ **LLM æ¶æ„çš„æ•ˆç‡ä¼˜åŒ–**ï¼ˆMamba æŠ•æœºè§£ç ã€ç¨€ç–å¾®è°ƒï¼‰æ˜¯ç»å¯¹çš„ä¸»è§’ã€‚ç‰¹åˆ«å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œæœ‰ç ”ç©¶å°è¯•ç”¨ LLM æ¥é¢„æµ‹å®è¯ç ”ç©¶çš„ç»“æœï¼Œè¿™æˆ–è®¸æ„å‘³ç€â€œAI ç§‘ç ”åŠ©æ‰‹â€æ­£åœ¨å‘â€œAI ç§‘ç ”å¯¼å¸ˆâ€è¿›åŒ–ã€‚æ­¤å¤–ï¼ŒAI for Science é¢†åŸŸåœ¨è›‹ç™½è´¨åŸºç¡€æ¨¡å‹è¯„æµ‹ä¸Šä¹Ÿè¿æ¥äº†é‡ç£…åŸºå‡†ã€‚\n\nä»¥ä¸‹æ˜¯ä¸ºä½ ç²¾é€‰çš„æ·±åº¦è§£è¯»ï¼š\n\n---\n\n### ğŸš€ LLM æ¶æ„ä¸é«˜æ•ˆæ¨ç† (Architecture & Efficiency)\n\n**1. [Mamba] ç”¨äºæŠ•æœºè§£ç çš„ Mamba è‰ç¨¿æ¨¡å‹**\n**Mamba Drafters for Speculative Decoding**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸ºäº†åŠ é€Ÿ LLM ç”Ÿæˆï¼ŒæŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰é€šå¸¸éœ€è¦ä¸€ä¸ªå¿«é€Ÿçš„â€œè‰ç¨¿æ¨¡å‹â€ï¼ˆDrafterï¼‰ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºä½¿ç”¨ **Mambaï¼ˆä¸€ç§çŠ¶æ€ç©ºé—´æ¨¡å‹ SSMï¼‰** ä½œä¸ºè‰ç¨¿æ¨¡å‹ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šåˆ©ç”¨ SSM çš„çº¿æ€§å¤æ‚åº¦ä¼˜åŠ¿ï¼ŒMamba Drafter é¿å…äº†ä¼ ç»Ÿ Transformer è‰ç¨¿æ¨¡å‹çš„äºŒæ¬¡æ–¹å¤æ‚åº¦ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨å¹¶æå‡äº†é€Ÿåº¦ã€‚åŒæ—¶ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶çš„æ ‘æœç´¢ç®—æ³•æ¥ç”Ÿæˆé«˜è´¨é‡å€™é€‰é¡¹ã€‚å®æµ‹è¡¨æ˜å®ƒæ—¢ä¿ç•™äº†è·¨æ¨¡å‹é€šç”¨æ€§ï¼Œåˆæ¯”ç°æœ‰æ–¹æ³•æ›´é«˜æ•ˆã€‚\n\n**2. [å¾®è°ƒ] LIFTï¼šæ¨ç†å¯¼å‘ç›‘ç£å¾®è°ƒä¸­çš„ä¸»æˆåˆ†æƒé‡æ¶Œç°**\n**LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹å¦‚ä½•é«˜æ•ˆå¾®è°ƒ LLM ä»¥æå‡æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§**ä½ç§©çŸ¥æƒ…ç¨€ç–å¾®è°ƒï¼ˆLIFTï¼‰**æ–¹æ³•ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šç ”ç©¶å‘ç°ï¼Œåœ¨ä½ç§©è¿‘ä¼¼ï¼ˆRank Reductionï¼‰ä¹‹åï¼Œå¹…åº¦æœ€å¤§çš„æƒé‡ï¼ˆPrincipal Weightsï¼‰æ‰æ˜¯å¾®è°ƒçš„å…³é”®ã€‚åªæ›´æ–°è¿™ 5% çš„ä¸»æˆåˆ†æƒé‡ï¼Œå…¶åœ¨ç®—æœ¯æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¨å‚æ•°å¾®è°ƒï¼ˆFull FTï¼‰ï¼Œä¸”æ¯” LoRAä¿ç•™äº†æ›´å¤šçš„æºé¢†åŸŸçŸ¥è¯†ï¼Œé¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚\n\n**3. [ç»“æ„åŒ–ç”Ÿæˆ] ZapFormatï¼šåŸºäº Earley ç®—æ³•çš„åŠ¨æ€å‰ªæç»“æ„åŒ–è§£ç **\n**Earley-Driven Dynamic Pruning for Efficient Structured Decoding**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ LLM ç”Ÿæˆ JSON æˆ–ç‰¹å®š DSLï¼ˆé¢†åŸŸç‰¹å®šè¯­è¨€ï¼‰æ—¶çš„ç»“æ„çº¦æŸé—®é¢˜ï¼Œæå‡ºäº† **Formatron** å¼•æ“å’Œ **ZapFormat** ç­–ç•¥ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šåˆ©ç”¨ Earley è§£æç®—æ³•å®æ—¶è¯†åˆ«å¹¶å‰”é™¤æ— æ•ˆçŠ¶æ€ï¼Œè§£å†³äº†ä¼ ç»Ÿçº¦æŸè§£ç ä¸­è¯è¡¨æ‰«æå¼€é”€å¤§çš„é—®é¢˜ã€‚ç›¸æ¯” SOTA å®ç°ï¼Œæ¨ç†é€Ÿåº¦æå‡äº† 2 å€ï¼Œè¿™å¯¹äº Agent è¿›è¡Œç²¾å‡†çš„ Function Call è‡³å…³é‡è¦ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸ å¤æ‚æ¨ç† (Agents & Reasoning)\n\n**4. [Meta-Research] ç”¨è¯­è¨€æ¨¡å‹é¢„æµ‹å®è¯ AI ç ”ç©¶ç»“æœ**\n**Predicting Empirical AI Research Outcomes with Language Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ„æ€çš„â€œå…ƒç ”ç©¶â€ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªåŸºå‡†ï¼Œæµ‹è¯• LLM èƒ½å¦é¢„æµ‹ä¸¤ä¸ªç ”ç©¶ç‚¹å­ï¼ˆideaï¼‰å“ªä¸ªåœ¨å®éªŒä¸­è¡¨ç°æ›´å¥½ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šç»“åˆäº†è®ºæ–‡æ£€ç´¢ Agent çš„å¾®è°ƒç‰ˆ GPT-4.1 ç³»ç»Ÿï¼Œåœ¨ NLP é¢†åŸŸçš„é¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ°äº† **64.4%**ï¼Œå¤§å¹…è¶…è¿‡äººç±»ä¸“å®¶ï¼ˆ48.9%ï¼‰ã€‚è¿™è¡¨æ˜ LLM æœ‰æ½œåŠ›ä½œä¸ºâ€œå¥–åŠ±æ¨¡å‹â€æ¥è¯„ä¼°ç§‘ç ” Idea çš„ä»·å€¼ï¼Œä»è€ŒåŠ é€Ÿç§‘ç ”è¿›ç¨‹ã€‚\n\n**5. [Agent æ¡†æ¶] MCP-Zeroï¼šè‡ªä¸» LLM Agent çš„ä¸»åŠ¨å·¥å…·å‘ç°**\n**MCP-Zero: Active Tool Discovery for Autonomous LLM Agents**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç›®å‰çš„ Agent å¤šæ˜¯è¢«åŠ¨æ¥æ”¶é¢„å®šä¹‰çš„å·¥å…·åˆ—è¡¨ã€‚MCP-Zero èµ‹äºˆäº† Agent **ä¸»åŠ¨å‘ç°**å·¥å…·çš„èƒ½åŠ›ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šAgent å¯ä»¥æ ¹æ®éœ€æ±‚ä¸»åŠ¨è¯·æ±‚ç‰¹å®šå·¥å…·ï¼Œå¹¶é€šè¿‡åˆ†å±‚è¯­ä¹‰è·¯ç”±ï¼ˆHierarchical Semantic Routingï¼‰åœ¨æ•°åƒä¸ªå€™é€‰å·¥å…·ä¸­ç²¾å‡†å®šä½ã€‚è¿™è®© Agent ä»è¢«åŠ¨çš„â€œå·¥å…·é€‰æ‹©è€…â€å˜æˆäº†çœŸæ­£çš„â€œè‡ªä¸»ä½¿ç”¨è€…â€ï¼Œåœ¨ APIBank æµ‹è¯•ä¸­å‡å°‘äº† 98% çš„ Token æ¶ˆè€—ã€‚\n\n**6. [RAG] SealQAï¼šæé«˜æœç´¢å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†é—¨æ§›**\n**SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ä¸ªæ–°çš„ RAG è¯„æµ‹åŸºå‡†ï¼Œä¸“æ³¨äºæœç´¢ç»“æœå†²çªã€å™ªå£°å¤§æˆ–æ— ç”¨çš„å›°éš¾åœºæ™¯ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šå³ä¾¿æ˜¯æœ€å…ˆè¿›çš„ DeepSeek-R1 æˆ– o3-miniï¼Œåœ¨é¢å¯¹å™ªå£°æœç´¢ç»“æœæ—¶ä¹Ÿæå…¶è„†å¼±ã€‚ç®€å•çš„å¢åŠ æ¨ç†ç®—åŠ›ï¼ˆTest-time computeï¼‰å¹¶ä¸èƒ½ç¨³å®šæå‡æ€§èƒ½ï¼Œæ­ç¤ºäº†å½“å‰ RAG ç³»ç»Ÿåœ¨æŠ—å¹²æ‰°æ¨ç†ä¸Šçš„å·¨å¤§çŸ­æ¿ã€‚\n\n**7. [ç»æµå­¦ Agent] AI èƒ½æŒæ¡è®¡é‡ç»æµå­¦å—ï¼Ÿ**\n**Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåŸºäº MetaGPT æ¡†æ¶å¼€å‘äº†ä¸“é—¨çš„â€œè®¡é‡ç»æµå­¦ AI Agentâ€ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè¯¥ Agent åœ¨è§„åˆ’è®¡é‡ä»»åŠ¡ã€ä»£ç æ‰§è¡Œå’Œé”™è¯¯åæ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿèƒœä»»ä¸“å®¶çº§çš„å®è¯åˆ†æä»»åŠ¡ï¼Œç”šè‡³è¶…è¿‡äº†é€šç”¨ LLM å’Œæ™®é€šäººç±»ç ”ç©¶å‘˜çš„æ°´å¹³ã€‚\n\n---\n\n### ğŸ§¬ AI for Science (Bio & Chem)\n\n**8. [åŸºå‡†æµ‹è¯•] PFMBenchï¼šè›‹ç™½è´¨åŸºç¡€æ¨¡å‹åŸºå‡†**\n**PFMBench: Protein Foundation Model Benchmark**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè›‹ç™½è´¨é¢†åŸŸç¼ºä¹ç»Ÿä¸€çš„è¯„æµ‹æ ‡å‡†ã€‚è¿™é¡¹å·¥ä½œæ¨å‡ºäº†æ¶µç›– 8 ä¸ªé¢†åŸŸ 38 é¡¹ä»»åŠ¡çš„ç»¼åˆåŸºå‡†ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè¯„æµ‹äº† 17 ä¸ª SOTA æ¨¡å‹ï¼ˆå¦‚ ESM ç³»åˆ—ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹å¾ˆå¼ºï¼Œä½†åœ¨å°æ ·æœ¬ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œç»“åˆç»“æ„ä¿¡æ¯çš„å¾®è°ƒå¾€å¾€ä¼˜äºçº¯åºåˆ—æ¨¡å‹ã€‚\n\n**9. [è¯ç‰©è®¾è®¡] è¿æ¥é‡å­è®¡ç®—ä¸ç»å…¸è®¡ç®—ï¼šæ··åˆæ¶æ„ç”Ÿæˆåˆ†å­**\n**Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¢ç´¢äº†åœ¨å«å™ªä¸­ç­‰è§„æ¨¡é‡å­ï¼ˆNISQï¼‰è®¾å¤‡ä¸Šè¿›è¡Œè¯ç‰©å‘ç°çš„æ··åˆæ¶æ„ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šé€šè¿‡å¤šç›®æ ‡è´å¶æ–¯ä¼˜åŒ–ï¼Œå‘ç°å°†å¤šä¸ªæµ…å±‚é‡å­ç”µè·¯ï¼ˆ3-4å±‚ï¼‰ä¸²è”åœ¨ GAN æ¶æ„ä¸­æ•ˆæœæœ€å¥½ã€‚è¯¥æ¨¡å‹ï¼ˆBO-QGANï¼‰ç”Ÿæˆçš„å€™é€‰è¯ç‰©å¾—åˆ†æ¯”ç»å…¸åŸºçº¿é«˜å‡º 2.21 å€ï¼Œå‚æ•°é‡å´å‡å°‘äº† 60%ã€‚\n\n**10. [åŒ–å­¦æ¨ç†] ChemAUï¼šåˆ©ç”¨è‡ªé€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡é©¾é©­ LLM çš„åŒ–å­¦æ¨ç†**\n**ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ LLM åœ¨åŒ–å­¦å¤æ‚æ¨ç†ä¸­å®¹æ˜“äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡æ¡†æ¶ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè¯¥æ–¹æ³•èƒ½æ ¹æ®æ¨ç†æ­¥éª¤çš„ä½ç½®åŠ¨æ€è¯„ä¼°ä¸ç¡®å®šæ€§ï¼Œç²¾å‡†è¯†åˆ«çŸ¥è¯†ç›²åŒºå¹¶è°ƒç”¨é¢†åŸŸæ¨¡å‹è¿›è¡Œä¿®æ­£ï¼Œæ˜¾è‘—æå‡äº†åŒ–å­¦é—®é¢˜çš„è§£å†³ç‡ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸æœºå™¨äºº (Multimodal & Robotics)\n\n**11. [æœºå™¨äºº] OG-VLAï¼šç”¨äº 3D æ„ŸçŸ¥è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ­£äº¤å›¾åƒç”Ÿæˆ**\n**OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç»“åˆäº† VLA çš„æ³›åŒ–èƒ½åŠ›å’Œ 3D ç­–ç•¥çš„é²æ£’æ€§ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šåˆ›æ–°æ€§åœ°å°† RGBD è§‚æµ‹æŠ•å½±ä¸ºç‚¹äº‘ï¼Œå†æ¸²æŸ“æˆ**æ­£äº¤è§†å›¾ï¼ˆOrthographic viewsï¼‰**ã€‚è¿™ç§è§†è§’ä¸å˜æ€§è®©æ¨¡å‹åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›æå‡äº† 40%ï¼Œè§£å†³äº†æœºå™¨äººæ“ä½œä¸­è§†è§’å˜åŒ–å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚\n\n**12. [å†œä¸š 3D] CountingFruitï¼šåŸºäºè¯­ä¹‰ Gaussian Splatting çš„è¯­è¨€å¼•å¯¼æ°´æœè®¡æ•°**\n**CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian Splatting**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³äº†æœå›­å¤æ‚é®æŒ¡ä¸‹çš„æ°´æœè®¡æ•°éš¾é¢˜ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šåˆ©ç”¨è¯­è¨€å¼•å¯¼çš„ 3D Gaussian Splattingï¼ˆé«˜æ–¯æ³¼æº…ï¼‰æŠ€æœ¯ï¼Œä¸éœ€è¦å›¾åƒæ©ç ï¼Œç›´æ¥é€šè¿‡æ–‡æœ¬ Promptï¼ˆå¦‚â€œè‹¹æœâ€ï¼‰åœ¨ 3D ç©ºé—´ä¸­è¿‡æ»¤å’Œé‡å»ºç›®æ ‡ï¼Œè®¡æ•°å¬å›ç‡é«˜è¾¾ 99.7%ã€‚\n\n**13. [ä¸–ç•Œæ¨¡å‹] äººå½¢æœºå™¨äººä¸–ç•Œæ¨¡å‹ï¼šå¼€æ”¾ä¸–ç•ŒåŸºç¡€æ¨¡å‹**\n**Humanoid World Models: Open World Foundation Models for Humanoid Robotics**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº† HWMï¼Œä¸€ç³»åˆ—è½»é‡çº§ã€å¼€æºçš„äººå½¢æœºå™¨äººä¸–ç•Œæ¨¡å‹ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šåŸºäº 100 å°æ—¶çš„æ¼”ç¤ºæ•°æ®è®­ç»ƒï¼Œèƒ½å¤Ÿæ ¹æ®æ§åˆ¶ Token é¢„æµ‹æœªæ¥çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ã€‚é€šè¿‡å‚æ•°å…±äº«æŠ€æœ¯ï¼Œæ¨¡å‹ä½“ç§¯å‡å°äº† 50% ä½†æ€§èƒ½æœªå‡ï¼Œé€‚åˆåœ¨å­¦æœ¯ç•Œå°å®éªŒå®¤ï¼ˆ1-2å¼  GPUï¼‰éƒ¨ç½²ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸å¯¹é½ (Safety & Alignment)\n\n**14. [çº¢é˜Ÿæµ‹è¯•] Jailbreak-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¢ç´¢ LLM çš„è¶Šç‹±èƒ½åŠ›**\n**Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•æ¡†æ¶ï¼Œåˆ©ç”¨ RL å¹³è¡¡æ”»å‡» Prompt çš„å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šé€šè¿‡â€œå†·å¯åŠ¨-çƒ­èº«æ¢ç´¢-å¢å¼ºè¶Šç‹±â€ä¸‰é˜¶æ®µè®­ç»ƒï¼Œè¯¥æ–¹æ³•èƒ½è‡ªåŠ¨å‘ç°ç°æœ‰ LLM çš„å®‰å…¨æ¼æ´ï¼Œæ•ˆç‡è¿œè¶…äººå·¥çº¢é˜Ÿã€‚\n\n**15. [é—å¿˜å­¦ä¹ ] è¯­éŸ³é—å¿˜ (Speech Unlearning)**\n**Speech Unlearning**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†â€œæœºå™¨é—å¿˜â€ï¼ˆMachine Unlearningï¼‰æ¦‚å¿µå¼•å…¥è¯­éŸ³é¢†åŸŸã€‚\n*   **ä¸»è¦å‘ç°**ï¼šå®šä¹‰äº†ä»æ¨¡å‹ä¸­åˆ é™¤ç‰¹å®šè¯´è¯äººæˆ–ç‰¹å®šè¯­éŸ³ç‰‡æ®µçš„ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œç”±äºè¯­éŸ³çš„é«˜ç»´å’Œåºåˆ—ç‰¹æ€§ï¼Œè¯­éŸ³é—å¿˜æ¯”å›¾åƒæˆ–æ–‡æœ¬é—å¿˜æ›´å…·æŒ‘æˆ˜æ€§ï¼Œç›®å‰å°šå±è“æµ·ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„å‘ç° (Quick Reads)\n\n*   **[æŒ‡çº¹ä¸è¡€å‹] Predicting Blood Type: Assessing Model Performance with ROC Analysis**\n    *   ä¸€é¡¹åŸºäº 200 äººçš„ç»Ÿè®¡ç ”ç©¶å‘ç°ï¼ŒæŒ‡çº¹æ¨¡å¼ï¼ˆæ–—ã€ç®•ã€å¼“ï¼‰ä¸ ABO è¡€å‹ä¹‹é—´**æ²¡æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—ç›¸å…³æ€§**ã€‚è¿™æ‰“ç ´äº†ä¸€äº›ä¼ ç»Ÿçš„è¿·æ€ã€‚\n*   **[ä¼¦ç†] Higher-Order Responsibility**\n    *   ä¸€ç¯‡åç†è®ºçš„è®ºæ–‡ï¼Œæ¢è®¨äº†åœ¨ç¾¤ä½“å†³ç­–ä¸­ï¼Œä»…ä»…è€ƒè™‘ä¸ªä½“è´£ä»»ä¼šå¯¼è‡´â€œè´£ä»»ç¼ºå£â€ã€‚ä½œè€…æå‡ºå¹¶é€šè¿‡è®¡ç®—å¤æ‚åº¦è¯æ˜äº†**é«˜é˜¶è´£ä»»ï¼ˆHigher-order responsibilityï¼‰**å¯¹äºå¡«è¡¥è¿™ä¸€ç¼ºå£çš„å¿…è¦æ€§ã€‚\n*   **[è§†é¢‘ç¼–è¾‘] MoCA-Video: Motion-Aware Concept Alignment for Consistent Video Editing**\n    *   æ— éœ€è®­ç»ƒçš„è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œèƒ½åœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´æ“ä½œå®ç°è§†é¢‘å†…å®¹çš„â€œè¯­ä¹‰æ··åˆâ€ï¼ˆä¾‹å¦‚æŠŠè§†é¢‘é‡Œçš„è½¦æ¢æˆå¦ä¸€ç§é£æ ¼ï¼Œä½†è¿åŠ¨è½¨è¿¹ä¸å˜ï¼‰ã€‚\n\n---\nä»Šå¤©çš„ç®€æŠ¥å°±åˆ°è¿™é‡Œã€‚**LLM æ­£åœ¨ä»å•çº¯çš„â€œèŠå¤©â€èµ°å‘â€œé¢„æµ‹æœªæ¥â€ï¼ˆç§‘ç ”ç»“æœé¢„æµ‹ï¼‰å’Œâ€œæ“æ§ç‰©ç†ä¸–ç•Œâ€ï¼ˆæœºå™¨äººä¸ 3D æ„ŸçŸ¥ï¼‰ã€‚** å¸Œæœ›è¿™äº›è®ºæ–‡èƒ½ä¸ºä½ çš„ç ”ç©¶å¸¦æ¥çµæ„Ÿã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2506.01214v1",
      "title": "A Review on Coarse to Fine-Grained Animal Action Recognition",
      "title_zh": "ä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„åŠ¨ç‰©è¡Œä¸ºè¯†åˆ«ç ”ç©¶ç»¼è¿°",
      "authors": [
        "Ali Zia",
        "Renuka Sharma",
        "Abdelwahed Khamis",
        "Xuesong Li",
        "Muhammad Husnain",
        "Numan Shafi",
        "Saeed Anwar",
        "Sabine Schmoelzl",
        "Eric Stone",
        "Lars Petersson",
        "Vivien Rolland"
      ],
      "abstract": "This review provides an in-depth exploration of the field of animal action recognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques. The primary aim is to examine the current state of research in animal behaviour recognition and to elucidate the unique challenges associated with recognising subtle animal actions in outdoor environments. These challenges differ significantly from those encountered in human action recognition due to factors such as non-rigid body structures, frequent occlusions, and the lack of large-scale, annotated datasets. The review begins by discussing the evolution of human action recognition, a more established field, highlighting how it progressed from broad, coarse actions in controlled settings to the demand for fine-grained recognition in dynamic environments. This shift is particularly relevant for animal action recognition, where behavioural variability and environmental complexity present unique challenges that human-centric models cannot fully address. The review then underscores the critical differences between human and animal action recognition, with an emphasis on high intra-species variability, unstructured datasets, and the natural complexity of animal habitats. Techniques like spatio-temporal deep learning frameworks (e.g., SlowFast) are evaluated for their effectiveness in animal behaviour analysis, along with the limitations of existing datasets. By assessing the strengths and weaknesses of current methodologies and introducing a recently-published dataset, the review outlines future directions for advancing fine-grained action recognition, aiming to improve accuracy and generalisability in behaviour analysis across species.",
      "tldr_zh": "è¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†åŠ¨ç‰©è¡Œä¸ºè¯†åˆ«(Animal Action Recognition)é¢†åŸŸï¼Œé‡ç‚¹åˆ†æäº†ä»ç²—ç²’åº¦(Coarse-Grained)åˆ°ç»†ç²’åº¦(Fine-Grained)çš„è¯†åˆ«æŠ€æœ¯æ¼”è¿›ã€‚æ–‡ç« è¯¦ç»†é˜æ˜äº†åœ¨æˆ·å¤–åŠ¨æ€ç¯å¢ƒä¸­æ•æ‰åŠ¨ç‰©ç»†å¾®åŠ¨ä½œçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚éåˆšæ€§èº«ä½“ç»“æ„(Non-rigid body structures)ã€é¢‘ç¹é®æŒ¡ä»¥åŠç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ç­‰é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”ç ”ç©¶ï¼Œä½œè€…æŒ‡å‡ºåŠ¨ç‰©è¯†åˆ«é¢ä¸´æ›´é«˜çš„ç§å†…å·®å¼‚(Intra-species variability)å’Œå¤æ‚çš„è‡ªç„¶æ –æ¯åœ°èƒŒæ™¯ï¼Œå¯¼è‡´ä¼ ç»Ÿçš„äººç±»ä¸­å¿ƒæ¨¡å‹éš¾ä»¥ç›´æ¥è¿ç§»åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†SlowFastç­‰æ—¶ç©ºæ·±åº¦å­¦ä¹ æ¡†æ¶(Spatio-temporal deep learning frameworks)åœ¨åŠ¨ç‰©è¡Œä¸ºåˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ†æäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚ç»“åˆæœ€æ–°å‘å¸ƒçš„æ•°æ®é›†ï¼Œè¯¥ç»¼è¿°ä¸ºæå‡è·¨ç‰©ç§è¡Œä¸ºåˆ†æçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æŒ‡æ˜äº†æœªæ¥ç»†ç²’åº¦è¯†åˆ«çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01214v1",
      "published_date": "2025-06-01 23:31:25 UTC",
      "updated_date": "2025-06-01 23:31:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:28.195475+00:00"
    },
    {
      "arxiv_id": "2506.01206v1",
      "title": "Mamba Drafters for Speculative Decoding",
      "title_zh": "ç”¨äºæŠ•æœºè§£ç çš„ Mamba èµ·è‰æ¨¡å‹",
      "authors": [
        "Daewon Choi",
        "Seunghyuk Oh",
        "Saket Dingliwal",
        "Jihoon Tack",
        "Kyuyoung Kim",
        "Woomin Song",
        "Seojin Kim",
        "Insu Han",
        "Jinwoo Shin",
        "Aram Galstyan",
        "Shubham Katiyar",
        "Sravan Babu Bodapati"
      ],
      "abstract": "Speculative decoding has emerged as a promising approach to accelerating large language model (LLM) generation using a fast drafter while maintaining alignment with the target model's distribution. However, existing approaches face a trade-off: external drafters offer flexibility but can suffer from slower drafting, while self-speculation methods use drafters tailored to the target model but require re-training. In this paper, we introduce novel drafters based on Mamba, a state-of-the-art state space model (SSM), as a solution that combines the best aspects of both approaches. By leveraging the linear structure of SSMs, our approach avoids the quadratic complexity inherent in traditional Transformer-based methods, enabling faster drafting and lower memory usage while maintaining the flexibility to work across different target models. We further enhance efficiency with a novel test-time tree search algorithm for generating high-quality draft candidates. Our empirical evaluation demonstrates that Mamba-based drafters not only outperform existing external drafting methods but are also comparable to state-of-the-art self-speculation approaches while using less memory and maintaining their cross-model adaptability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŸºäº Mambaï¼ˆä¸€ç§å…ˆè¿›çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ State Space Modelï¼‰çš„èµ·è‰å™¨ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ•æœºé‡‡æ ·ï¼ˆSpeculative Decodingï¼‰åŠ é€Ÿè¿‡ç¨‹ã€‚é€šè¿‡åˆ©ç”¨ Mamba çš„çº¿æ€§å¤æ‚åº¦ç»“æ„ï¼Œè¯¥æ–¹æ³•æˆåŠŸè§„é¿äº†ä¼ ç»Ÿ Transformer æ¨¡å‹åœ¨ç”Ÿæˆè‰ç¨¿æ—¶çš„äºŒæ¬¡æ–¹å¤æ‚åº¦ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„èµ·è‰é€Ÿåº¦å’Œæ›´ä½çš„èµ„æºå ç”¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„æµ‹è¯•æ—¶æ ‘æœç´¢ï¼ˆTest-time Tree Searchï¼‰ç®—æ³•ï¼Œé€šè¿‡è¯¥ç®—æ³•å¯ä»¥æ›´é«˜æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„å€™é€‰æ–‡æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-based drafters åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤–éƒ¨èµ·è‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¿æŒè·¨æ¨¡å‹é€‚é…èƒ½åŠ›çš„åŒæ—¶ï¼Œå…¶æ•ˆç‡ä¸å†…å­˜è¡¨ç°å‡èƒ½ä¸æœ€å…ˆè¿›çš„è‡ªæŠ•æœºï¼ˆSelf-speculationï¼‰æ–¹æ³•ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01206v1",
      "published_date": "2025-06-01 22:52:47 UTC",
      "updated_date": "2025-06-01 22:52:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:33.184506+00:00"
    },
    {
      "arxiv_id": "2506.01199v2",
      "title": "Test Automation for Interactive Scenarios via Promptable Traffic Simulation",
      "title_zh": "åŸºäºå¯æç¤ºäº¤é€šä»¿çœŸçš„äº¤äº’å¼åœºæ™¯è‡ªåŠ¨åŒ–æµ‹è¯•",
      "authors": [
        "Augusto Mondelli",
        "Yueshan Li",
        "Alessandro Zanardi",
        "Emilio Frazzoli"
      ],
      "abstract": "Autonomous vehicle (AV) planners must undergo rigorous evaluation before widespread deployment on public roads, particularly to assess their robustness against the uncertainty of human behaviors. While recent advancements in data-driven scenario generation enable the simulation of realistic human behaviors in interactive settings, leveraging these models to construct comprehensive tests for AV planners remains an open challenge. In this work, we introduce an automated method to efficiently generate realistic and safety-critical human behaviors for AV planner evaluation in interactive scenarios. We parameterize complex human behaviors using low-dimensional goal positions, which are then fed into a promptable traffic simulator, ProSim, to guide the behaviors of simulated agents. To automate test generation, we introduce a prompt generation module that explores the goal domain and efficiently identifies safety-critical behaviors using Bayesian optimization. We apply our method to the evaluation of an optimization-based planner and demonstrate its effectiveness and efficiency in automatically generating diverse and realistic driving behaviors across scenarios with varying initial conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¯æç¤ºäº¤é€šæ¨¡æ‹Ÿå®ç°äº¤äº’å¼åœºæ™¯æµ‹è¯•è‡ªåŠ¨åŒ–çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶æ±½è½¦(AV)è§„åˆ’å™¨åœ¨é¢å¯¹äººç±»è¡Œä¸ºä¸ç¡®å®šæ€§æ—¶è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶å¼•å…¥äº†å¯æç¤ºäº¤é€šæ¨¡æ‹Ÿå™¨ ProSimï¼Œé€šè¿‡å°†å¤æ‚äººç±»è¡Œä¸ºå‚æ•°åŒ–ä¸ºä½ç»´çš„ç›®æ ‡ä½ç½®(goal positions)æ¥å¼•å¯¼æ¨¡æ‹Ÿæ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚ä¸ºäº†å®ç°æµ‹è¯•ç”Ÿæˆçš„è‡ªåŠ¨åŒ–ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æç¤ºç”Ÿæˆæ¨¡å—å¹¶ç»“åˆè´å¶æ–¯ä¼˜åŒ–(Bayesian optimization)é«˜æ•ˆè¯†åˆ«å®‰å…¨å…³é”®(safety-critical)çš„é©¾é©¶è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯„ä¼°åŸºäºä¼˜åŒ–çš„è§„åˆ’å™¨æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒåˆå§‹æ¡ä»¶è‡ªåŠ¨ç”Ÿæˆå¤šæ ·ä¸”é€¼çœŸçš„äº¤äº’åœºæ™¯ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚äº’åŠ¨ç¯å¢ƒä¸­çš„ç¨³å¥æ€§æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æµ‹è¯•æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by CVPR 2025 Workshop Data-Driven Autonomous Driving Simulation (track 1)",
      "pdf_url": "https://arxiv.org/pdf/2506.01199v2",
      "published_date": "2025-06-01 22:29:32 UTC",
      "updated_date": "2025-06-04 19:26:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:31.683746+00:00"
    },
    {
      "arxiv_id": "2506.01197v1",
      "title": "Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures",
      "title_zh": "èå…¥å±‚çº§è¯­ä¹‰çš„ç¨€ç–è‡ªç¼–ç å™¨æ¶æ„",
      "authors": [
        "Mark Muchane",
        "Sean Richardson",
        "Kiho Park",
        "Victor Veitch"
      ],
      "abstract": "Sparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn a set of human-understandable concepts that can explain variation on an abstract space. A basic limitation of this approach is that it neither exploits nor represents the semantic relationships between the learned concepts. In this paper, we introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts. Application of this architecture to the internal representations of large language models shows both that semantic hierarchy can be learned, and that doing so improves both reconstruction and interpretability. Additionally, the architecture leads to significant improvements in computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¨€ç–è‡ªç¼–ç å™¨(Sparse Autoencoders, SAEs)åœ¨æå–äººç±»å¯ç†è§£æ¦‚å¿µæ—¶æ— æ³•æœ‰æ•ˆåˆ©ç”¨è¯­ä¹‰å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤Ÿæ˜¾å¼å»ºæ¨¡æ¦‚å¿µè¯­ä¹‰å±‚æ¬¡(semantic hierarchy)çš„æ”¹è¿›æ¶æ„ã€‚é€šè¿‡å°†è¯¥æ¶æ„åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„å†…éƒ¨è¡¨ç¤ºï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸå­¦ä¹ å¹¶åˆ»ç”»è¯­ä¹‰å±‚çº§ï¼Œå¹¶åœ¨æ¨¡å‹é‡æ„(reconstruction)è´¨é‡å’Œç‰¹å¾å¯è§£é‡Šæ€§(interpretability)ä¸Šå–å¾—äº†åŒé‡æå‡ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¿™ç§å±‚æ¬¡åŒ–ç»“æ„è®¾è®¡è¿˜å¸¦æ¥äº†è®¡ç®—æ•ˆç‡(computational efficiency)çš„æ˜¾è‘—ä¼˜åŒ–ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¢å¼ºç¥ç»è¡¨å¾çš„ç»“æ„åŒ–ç†è§£åŠæå‡ç¨€ç–å­—å…¸å­¦ä¹ çš„æ•ˆèƒ½æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Code is available at https://github.com/muchanem/hierarchical-sparse-autoencoders",
      "pdf_url": "https://arxiv.org/pdf/2506.01197v1",
      "published_date": "2025-06-01 22:20:07 UTC",
      "updated_date": "2025-06-01 22:20:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:38.257979+00:00"
    },
    {
      "arxiv_id": "2506.05379v1",
      "title": "Designing DSIC Mechanisms for Data Sharing in the Era of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ä¸‹é¢å‘æ•°æ®å…±äº«çš„ DSIC æœºåˆ¶è®¾è®¡",
      "authors": [
        "Seyed Moein Ayyoubzadeh",
        "Kourosh Shahnazari",
        "Mohammmadali Keshtparvar",
        "MohammadAmin Fazli"
      ],
      "abstract": "Training large language models (LLMs) requires vast amounts of high-quality data from institutions that face legal, privacy, and strategic constraints. Existing data procurement methods often rely on unverifiable trust or ignore heterogeneous provider costs. We introduce a mechanism-design framework for truthful, trust-minimized data sharing that ensures dominant-strategy incentive compatibility (DSIC), individual rationality, and weak budget balance, while rewarding data based on both quality and learning utility. We formalize a model where providers privately know their data cost and quality, and value arises solely from the data's contribution to model performance. Based on this, we propose the Quality-Weighted Marginal-Incentive Auction (Q-MIA), which ranks providers using a virtual cost metric and uses Myerson-style payments to ensure DSIC and budget feasibility. To support settings with limited liquidity or long-term incentives, we introduce the Marginal Utility Token (MUT), which allocates future rights based on marginal contributions. We unify these in Mixed-MIA, a hybrid mechanism balancing upfront payments and deferred rewards. All mechanisms support verifiable, privacy-preserving implementation. Theoretically and empirically, they outperform volume-based and trust-based baselines, eliciting higher-quality data under budget constraints while remaining robust to misreporting and collusion. This establishes a principled foundation for sustainable and fair data markets for future LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)è®­ç»ƒä¸­é¢ä¸´çš„é«˜è´¨é‡æ•°æ®è·å–éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ—¨åœ¨å®ç°çœŸå®ã€ä¿¡ä»»æœ€å°åŒ–æ•°æ®å…±äº«çš„æœºåˆ¶è®¾è®¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç¡®ä¿äº†å ä¼˜ç­–ç•¥æ¿€åŠ±å…¼å®¹æ€§(DSIC)ã€ä¸ªä½“ç†æ€§(Individual Rationality)ä»¥åŠå¼±é¢„ç®—å¹³è¡¡(Weak Budget Balance)ï¼Œèƒ½å¤ŸåŸºäºæ•°æ®è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨(Learning Utility)è¿›è¡Œå…¬å¹³å¥–åŠ±ã€‚ä½œè€…æå‡ºäº†è´¨é‡åŠ æƒè¾¹é™…æ¿€åŠ±æ‹å–(Quality-Weighted Marginal-Incentive Auction, Q-MIA)ï¼Œåˆ©ç”¨è™šæ‹Ÿæˆæœ¬æŒ‡æ ‡å¯¹ä¾›åº”å•†è¿›è¡Œæ’åï¼Œå¹¶é‡‡ç”¨Myerson-style paymentsç¡®ä¿æ¿€åŠ±å…¼å®¹å’Œé¢„ç®—å¯è¡Œã€‚ä¸ºäº†æ”¯æŒé•¿æœŸæ¿€åŠ±ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è¾¹é™…æ•ˆç”¨ä»£å¸(Marginal Utility Token, MUT)ä»¥åŠå¹³è¡¡å‰æœŸæ”¯ä»˜ä¸å»¶è¿Ÿå¥–åŠ±çš„æ··åˆæœºåˆ¶Mixed-MIAã€‚æ‰€æœ‰æœºåˆ¶å‡æ”¯æŒå¯éªŒè¯ä¸”éšç§ä¿æŠ¤(Privacy-preserving)çš„å®æ–½æ–¹å¼ã€‚ç†è®ºä¸å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨é¢„ç®—é™åˆ¶ä¸‹è·å–çš„æ•°æ®è´¨é‡æ˜¾è‘—ä¼˜äºåŸºäºæ•°æ®é‡æˆ–ä¿¡ä»»çš„åŸºå‡†æ–¹æ³•ï¼Œä¸”å¯¹è™šå‡æŠ¥å‘Šå’Œå…±è°‹å…·æœ‰é²æ£’æ€§ï¼Œä¸ºæ„å»ºå…¬å¹³çš„æ•°æ®å¸‚åœºå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.05379v1",
      "published_date": "2025-06-01 22:17:18 UTC",
      "updated_date": "2025-06-01 22:17:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:51.711827+00:00"
    },
    {
      "arxiv_id": "2506.01196v2",
      "title": "OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model",
      "title_zh": "OG-VLAï¼šé¢å‘3Dæ„ŸçŸ¥è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ­£äº¤å›¾åƒç”Ÿæˆ",
      "authors": [
        "Ishika Singh",
        "Ankit Goyal",
        "Stan Birchfield",
        "Dieter Fox",
        "Animesh Garg",
        "Valts Blukis"
      ],
      "abstract": "We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OG-VLAï¼Œä¸€ç§ç»“åˆäº†Vision-Language Action models (VLAs)çš„æ³›åŒ–èƒ½åŠ›ä¸3D-aware policiesçš„é²æ£’æ€§çš„æ–°å‹æ¶æ„ã€‚é’ˆå¯¹VLAå¯¹ç›¸æœºä½å§¿æ•æ„Ÿä»¥åŠ3Dç­–ç•¥åœ¨å¤„ç†æœªè§æŒ‡ä»¤æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ï¼ŒOG-VLAå°†å¤šè§†è§’è§‚æµ‹æ•°æ®è¿˜åŸä¸ºpoint cloudï¼Œå¹¶å°†å…¶æ¸²æŸ“ä¸ºcanonical orthographic viewsï¼Œä»¥ç¡®ä¿è¾“å…¥è§†è§’çš„è·¨åœºæ™¯ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶æ•´åˆäº†è§†è§‰éª¨å¹²ç½‘ç»œã€Large Language Model (LLM)å’Œimage diffusion modelï¼Œç”¨äºç”Ÿæˆç¼–ç end-effectorä¸‹ä¸€æ—¶åˆ»ä½ç½®å’Œå§¿æ€çš„å›¾åƒã€‚åœ¨Arnoldå’ŒColosseumåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°è¡¨æ˜ï¼ŒOG-VLAåœ¨æœªè§ç¯å¢ƒä¸­çš„æ³›åŒ–æ€§èƒ½å®ç°äº†è¶…è¿‡40%çš„relative improvementsã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­ä»…éœ€3è‡³5æ¬¡demonstrationså³å¯å®ç°é«˜æ•ˆé€‚é…ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„é¢†å…ˆåœ°ä½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.01196v2",
      "published_date": "2025-06-01 22:15:45 UTC",
      "updated_date": "2025-11-18 18:49:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:50.268740+00:00"
    },
    {
      "arxiv_id": "2506.02065v1",
      "title": "EWGN: Elastic Weight Generation and Context Switching in Deep Learning",
      "title_zh": "EWGNï¼šæ·±åº¦å­¦ä¹ ä¸­çš„å¼¹æ€§æƒé‡ç”Ÿæˆä¸ä¸Šä¸‹æ–‡åˆ‡æ¢",
      "authors": [
        "Shriraj P. Sawant",
        "Krishna P. Miyapuram"
      ],
      "abstract": "The ability to learn and retain a wide variety of tasks is a hallmark of human intelligence that has inspired research in artificial general intelligence. Continual learning approaches provide a significant step towards achieving this goal. It has been known that task variability and context switching are challenging for learning in neural networks. Catastrophic forgetting refers to the poor performance on retention of a previously learned task when a new task is being learned. Switching between different task contexts can be a useful approach to mitigate the same by preventing the interference between the varying task weights of the network. This paper introduces Elastic Weight Generative Networks (EWGN) as an idea for context switching between two different tasks. The proposed EWGN architecture uses an additional network that generates the weights of the primary network dynamically while consolidating the weights learned. The weight generation is input-dependent and thus enables context switching. Using standard computer vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of previously learned task representations in Fully Connected Networks, Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient Descent and Elastic Weight Consolidation learning algorithms. Understanding dynamic weight generation and context-switching ability can be useful in enabling continual learning for improved performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒç»­å­¦ä¹ (Continual learning)ä¸­çš„ç¾éš¾æ€§é—å¿˜(Catastrophic forgetting)å’Œä»»åŠ¡åˆ‡æ¢(Context switching)æŒ‘æˆ˜ï¼Œæå‡ºäº†å¼¹æ€§æƒé‡ç”Ÿæˆç½‘ç»œ(Elastic Weight Generative Networks, EWGN)ã€‚EWGNé€šè¿‡å¼•å…¥ä¸€ä¸ªé¢å¤–çš„ç½‘ç»œç»“æ„ï¼Œæ ¹æ®è¾“å…¥æ•°æ®åŠ¨æ€ç”Ÿæˆä¸»ç½‘ç»œçš„æƒé‡ï¼Œä»è€Œåœ¨å·©å›ºå·²å­¦çŸ¥è¯†çš„åŒæ—¶å®ç°çµæ´»çš„ä»»åŠ¡ä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚è¿™ç§è¾“å…¥ä¾èµ–çš„æƒé‡ç”Ÿæˆæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢ä¸åŒä»»åŠ¡æƒé‡ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ï¼Œç¼“è§£äº†ç¥ç»ç½‘ç»œåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶å¯¹æ—§ä»»åŠ¡è¡¨ç°ä¸‹é™çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨MNISTå’Œfashion-MNISTæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¯¹æ¯”äº†å…¨è¿æ¥ç½‘ç»œ(Fully Connected Networks)å’Œå·ç§¯ç¥ç»ç½‘ç»œ(Convolutional Neural Networks)åœ¨ç»“åˆéšæœºæ¢¯åº¦ä¸‹é™(Stochastic Gradient Descent)åŠå¼¹æ€§æƒé‡å·©å›º(Elastic Weight Consolidation)ç®—æ³•ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEWGNé€šè¿‡åŠ¨æ€æƒé‡ç”Ÿæˆæœ‰æ•ˆä¿ç•™äº†å…ˆå‰å­¦ä¹ çš„ä»»åŠ¡è¡¨ç¤ºï¼Œä¸ºæé«˜æŒç»­å­¦ä¹ çš„æ€§èƒ½å’Œç³»ç»Ÿçš„é²æ£’æ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02065v1",
      "published_date": "2025-06-01 21:59:53 UTC",
      "updated_date": "2025-06-01 21:59:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:44.311269+00:00"
    },
    {
      "arxiv_id": "2506.01183v2",
      "title": "Doubly Robust Alignment for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„åŒé‡ç¨³å¥å¯¹é½",
      "authors": [
        "Erhan Xu",
        "Kai Ye",
        "Hongyi Zhou",
        "Luhan Zhu",
        "Francesco Quinzan",
        "Chengchun Shi"
      ],
      "abstract": "This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½çš„ä»äººç±»åé¦ˆä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æŠ€æœ¯ã€‚é’ˆå¯¹ç°æœ‰ç®—æ³•å¯¹åå¥½æ¨¡å‹ï¼ˆå¦‚ Bradley-Terry æ¨¡å‹ï¼‰ã€å‚è€ƒç­–ç•¥æˆ–å¥–åŠ±å‡½æ•°æ¨¡å‹å¤±é…ï¼ˆmisspecificationï¼‰é«˜åº¦æ•æ„Ÿå¹¶å¯¼è‡´å¾®è°ƒæ•ˆæœä¸ä½³çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŒé‡ç¨³å¥åå¥½ä¼˜åŒ–ï¼ˆDoubly Robust Preference Optimizationï¼‰ç®—æ³•ã€‚è¯¥ç®—æ³•çš„å…³é”®ç‰¹æ€§åœ¨äºï¼Œåªè¦åå¥½æ¨¡å‹æˆ–å‚è€ƒç­–ç•¥å…¶ä¸­ä¹‹ä¸€è¢«æ­£ç¡®æŒ‡å®šï¼Œæ¨¡å‹å³å¯ä¿æŒä¸€è‡´æ€§ï¼Œè€Œä¸å†è¦æ±‚ä¸¤è€…åŒæ—¶å‡†ç¡®ã€‚ç†è®ºåˆ†æä¸å®éªŒè¯„ä¼°å‡è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†æ¨¡å‹å¤±é…æ—¶æ¯”ç°æœ‰æœ€å…ˆè¿›ç®—æ³•è¡¨ç°å‡ºæ›´ä¼˜è¶Šä¸”æ›´ç¨³å¥çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹å¯¹é½çš„å¯é æ€§æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01183v2",
      "published_date": "2025-06-01 21:34:37 UTC",
      "updated_date": "2025-10-28 20:58:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:55.592677+00:00"
    },
    {
      "arxiv_id": "2506.01182v2",
      "title": "Humanoid World Models: Open World Foundation Models for Humanoid Robotics",
      "title_zh": "Humanoid World Modelsï¼šé¢å‘äººå½¢æœºå™¨äººçš„å¼€æ”¾ä¸–ç•ŒåŸºç¡€æ¨¡å‹",
      "authors": [
        "Muhammad Qasim Ali",
        "Aditya Sridhar",
        "Shahbuland Matiana",
        "Alex Wong",
        "Mohammad Al-Sharman"
      ],
      "abstract": "Humanoid robots, with their human-like form, are uniquely suited for interacting in environments built for people. However, enabling humanoids to reason, plan, and act in complex open-world settings remains a challenge. World models, models that predict the future outcome of a given action, can support these capabilities by serving as a dynamics model in long-horizon planning and generating synthetic data for policy learning. We introduce Humanoid World Models (HWM), a family of lightweight, open-source models that forecast future egocentric video conditioned on humanoid control tokens. We train two types of generative models, Masked Transformers and Flow-Matching, on 100 hours of humanoid demonstrations. Additionally, we explore architectural variants with different attention mechanisms and parameter-sharing strategies. Our parameter-sharing techniques reduce model size by 33-53% with minimal impact on performance or visual fidelity. HWMs are designed to be trained and deployed in practical academic and small-lab settings, such as 1-2 GPUs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Humanoid World Models (HWM)ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—è½»é‡çº§ã€å¼€æºçš„ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨æå‡äººå½¢æœºå™¨äººåœ¨å¤æ‚å¼€æ”¾ä¸–ç•Œä¸­çš„æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚HWMèƒ½å¤Ÿæ ¹æ®äººå½¢æ§åˆ¶æ ‡è®°(humanoid control tokens)é¢„æµ‹æœªæ¥çš„ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘(egocentric video)ï¼Œå¹¶åˆ©ç”¨Masked Transformerså’ŒFlow-Matchingä¸¤ç±»ç”Ÿæˆæ¨¡å‹åœ¨100å°æ—¶çš„äººå½¢æœºå™¨äººæ¼”ç¤ºæ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æ¢ç´¢äº†ä¸åŒçš„æ¶æ„å˜ä½“å’Œå‚æ•°å…±äº«(parameter-sharing)ç­–ç•¥ï¼Œåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½å’Œè§†è§‰ä¿çœŸåº¦çš„å‰æä¸‹ï¼ŒæˆåŠŸå°†æ¨¡å‹è§„æ¨¡ç¼©å‡äº†33-53%ã€‚è¿™äº›æ¨¡å‹å¯ä½œä¸ºé•¿æœŸè§„åˆ’ä¸­çš„åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œæˆ–ç”¨äºç”Ÿæˆè¾…åŠ©ç­–ç•¥å­¦ä¹ çš„åˆæˆæ•°æ®ã€‚HWMçš„è®¾è®¡å……åˆ†è€ƒè™‘äº†å®é™…éƒ¨ç½²éœ€æ±‚ï¼Œæ”¯æŒåœ¨ä»…éœ€1-2ä¸ªGPUçš„å­¦æœ¯åŠå°å‹å®éªŒå®¤ç¯å¢ƒä¸‹å®Œæˆé«˜æ•ˆçš„è®­ç»ƒä¸åº”ç”¨ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01182v2",
      "published_date": "2025-06-01 21:33:36 UTC",
      "updated_date": "2025-07-08 20:18:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:07:59.535476+00:00"
    },
    {
      "arxiv_id": "2506.01177v2",
      "title": "Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation",
      "title_zh": "è¿æ¥è¯ç‰©è®¾è®¡ä¸­çš„é‡å­ä¸ç»å…¸è®¡ç®—ï¼šæå‡åˆ†å­ç”Ÿæˆçš„æ¶æ„åŸåˆ™",
      "authors": [
        "Andrew Smith",
        "Erhan Guven"
      ],
      "abstract": "Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture of generative adversarial networks (GANs) for molecule discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, while reducing parameter count by more than 60%. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically-grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å˜ˆæ‚ä¸­ç­‰è§„æ¨¡é‡å­(NISQ)è®¾å¤‡åœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨ï¼Œæ¢è®¨äº†é‡å­ä¸ç»å…¸è®¡ç®—ç»“åˆçš„ä¼˜åŒ–æ¶æ„åŸåˆ™ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å¤šç›®æ ‡ Bayesian optimization ç³»ç»Ÿåœ°ä¼˜åŒ–äº† Generative Adversarial Networks (GANs) çš„é‡å­-ç»å…¸æ¡¥æ¥æ¶æ„ï¼Œå¹¶æå‡ºäº† BO-QGAN æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBO-QGAN åœ¨ Drug Candidate Score (DCS) è¡¨ç°ä¸Šæ¯”å…ˆå‰çš„é‡å­æ··åˆåŸºå‡†æå‡äº† 2.27 å€ï¼Œä¼˜äºç»å…¸åŸºå‡† 2.21 å€ï¼Œä¸”å‚æ•°é‡å‡å°‘äº† 60% ä»¥ä¸Šã€‚æ ¸å¿ƒå‘ç°å»ºè®®å°† 3-4 ä¸ª 4-8 qubit çš„æµ…å±‚é‡å­ç”µè·¯è¿›è¡Œé¡ºåºåˆ†å±‚ï¼Œè€Œç»å…¸æ¶æ„åœ¨æ»¡è¶³æœ€å°å®¹é‡åå¯¹æ€§èƒ½çš„æ•æ„Ÿåº¦è¾ƒä½ã€‚è¯¥å·¥ä½œä¸ºæ··åˆæ¨¡å‹æä¾›äº†é¦–ä¸ªåŸºäºç»éªŒçš„æ¶æ„æŒ‡å—ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†é‡å­è®¡ç®—æœºåœ¨åˆ¶è¯ç ”ç©¶æµç¨‹ä¸­çš„æ•´åˆåº”ç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Proceedings of the Workshop on Generative AI for Biology at the 42nd International Conference on Machine Learning 10 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.01177v2",
      "published_date": "2025-06-01 21:24:43 UTC",
      "updated_date": "2025-07-25 02:17:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:08:07.780100+00:00"
    },
    {
      "arxiv_id": "2506.01174v1",
      "title": "GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering",
      "title_zh": "GraphPadï¼šé¢å‘å…·èº«é—®ç­”çš„æ¨ç†æ—¶ 3D åœºæ™¯å›¾æ›´æ–°",
      "authors": [
        "Muhammad Qasim Ali",
        "Saeejith Nair",
        "Alexander Wong",
        "Yuchen Cui",
        "Yuhao Chen"
      ],
      "abstract": "Structured scene representations are a core component of embodied agents, helping to consolidate raw sensory streams into readable, modular, and searchable formats. Due to their high computational overhead, many approaches build such representations in advance of the task. However, when the task specifications change, such static approaches become inadequate as they may miss key objects, spatial relations, and details. We introduce GraphPad, a modifiable structured memory that an agent can tailor to the needs of the task through API calls. It comprises a mutable scene graph representing the environment, a navigation log indexing frame-by-frame content, and a scratchpad for task-specific notes. Together, GraphPad serves as a dynamic workspace that remains complete, current, and aligned with the agent's immediate understanding of the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a +3.0% increase over an image-only baseline using the same vision-language model, while operating with five times fewer input frames. These results show that allowing online, language-driven refinement of 3-D memory yields more informative representations without extra training or data collection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·ä½“åŒ–æ™ºèƒ½ä½“(Embodied Agents)åœ¨ä»»åŠ¡éœ€æ±‚å˜åŒ–æ—¶é™æ€åœºæ™¯è¡¨ç¤ºå¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†GraphPadï¼Œä¸€ç§å…è®¸æ™ºèƒ½ä½“é€šè¿‡APIè°ƒç”¨æ ¹æ®ä»»åŠ¡éœ€æ±‚è¿›è¡Œå®æ—¶è°ƒæ•´çš„å¯ä¿®æ”¹ç»“æ„åŒ–å­˜å‚¨ç³»ç»Ÿã€‚GraphPadåŒ…å«ä¸€ä¸ªè¡¨ç¤ºç¯å¢ƒçš„å¯å˜åœºæ™¯å›¾(Mutable Scene Graph)ã€ä¸€ä¸ªç´¢å¼•é€å¸§å†…å®¹çš„å¯¼èˆªæ—¥å¿—(Navigation Log)ä»¥åŠä¸€ä¸ªç”¨äºå­˜å‚¨ä»»åŠ¡ç‰¹å®šç¬”è®°çš„è‰ç¨¿çº¸(Scratchpad)ã€‚è¿™ä¸€åŠ¨æ€å·¥ä½œç©ºé—´ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µå®ç°åœ¨çº¿çš„ã€è¯­è¨€é©±åŠ¨çš„3Då†…å­˜ä¼˜åŒ–ï¼Œç¡®ä¿åœºæ™¯ç†è§£ä¸å½“å‰ä»»åŠ¡åŠç¯å¢ƒå˜åŒ–ä¿æŒé«˜åº¦ä¸€è‡´ã€‚åœ¨OpenEQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGraphPadåœ¨è¾“å…¥å¸§æ•°å‡å°‘äº”å€çš„æƒ…å†µä¸‹è¾¾åˆ°äº†55.3%çš„å‡†ç¡®ç‡ï¼Œæ¯”ä½¿ç”¨ç›¸åŒVision-Language Modelçš„çº¯å›¾åƒåŸºçº¿æå‡äº†3.0%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ— éœ€é¢å¤–è®­ç»ƒæˆ–æ•°æ®é‡‡é›†çš„åŠ¨æ€æ›´æ–°æœºåˆ¶ï¼Œèƒ½ä¸ºå…·ä½“åŒ–é—®ç­”(Embodied Question Answering)æä¾›æ›´å…·ä¿¡æ¯é‡çš„åœºæ™¯è¡¨ç¤ºå¹¶æ˜¾è‘—æå‡ä»»åŠ¡æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "CVPR 2025 Workshop on 3D-LLM/VLA: Bridging Language, Vision and Action in 3D Environments",
      "pdf_url": "https://arxiv.org/pdf/2506.01174v1",
      "published_date": "2025-06-01 21:13:38 UTC",
      "updated_date": "2025-06-01 21:13:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:08:25.386787+00:00"
    },
    {
      "arxiv_id": "2506.01166v1",
      "title": "VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration",
      "title_zh": "VUSAï¼šåˆ©ç”¨ AI åŠ é€Ÿä¸­éç»“æ„åŒ–ç¨€ç–æ€§çš„è™šæ‹Ÿæ‰©å±•è„‰åŠ¨é˜µåˆ—æ¶æ„",
      "authors": [
        "Shereef Helal",
        "Alberto Garcia-Ortiz",
        "Lennart Bamberg"
      ],
      "abstract": "Leveraging high degrees of unstructured sparsity is a promising approach to enhance the efficiency of deep neural network DNN accelerators - particularly important for emerging Edge-AI applications. We introduce VUSA, a systolic-array architecture that virtually grows based on the present sparsity to perform larger matrix multiplications with the same number of physical multiply-accumulate MAC units. The proposed architecture achieves saving by 37% and 68% in area and power efficiency, respectively, at the same peak-performance, compared to a baseline systolic array architecture in a commercial 16-nm technology. Still, the proposed architecture supports acceleration for any DNN with any sparsity - even no sparsity at all. Thus, the proposed architecture is application-independent, making it viable for general-purpose AI acceleration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VUSAï¼Œä¸€ç§é€šè¿‡åˆ©ç”¨éç»“æ„åŒ–ç¨€ç–æ€§ (unstructured sparsity) æ¥å¢å¼ºæ·±åº¦ç¥ç»ç½‘ç»œ (DNN) åŠ é€Ÿå™¨æ•ˆç‡çš„è™šæ‹Ÿæ‰©å±•è„‰åŠ¨é˜µåˆ— (Virtually Upscaled Systolic Array) æ¶æ„ã€‚è¯¥æ¶æ„å…è®¸ç³»ç»Ÿæ ¹æ®ç°æœ‰çš„ç¨€ç–æ€§è¿›è¡Œè™šæ‹Ÿå¢é•¿ï¼Œä»è€Œåœ¨ä¿æŒç‰©ç†ä¹˜ç´¯åŠ  (MAC) å•å…ƒæ•°é‡ä¸å˜çš„æƒ…å†µä¸‹æ‰§è¡Œæ›´å¤§è§„æ¨¡çš„çŸ©é˜µä¹˜æ³• (matrix multiplications)ã€‚åœ¨å•†ç”¨ 16-nm å·¥è‰ºä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒVUSA åœ¨ç»´æŒç›¸åŒå³°å€¼æ€§èƒ½çš„å‰æä¸‹ï¼Œæ¯”åŸºå‡†è„‰åŠ¨é˜µåˆ—æ¶æ„èŠ‚çœäº† 37% çš„é¢ç§¯å¹¶æå‡äº† 68% çš„ç”µæºæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæ”¯æŒå…·æœ‰ä»»ä½•ç¨€ç–ç¨‹åº¦ç”šè‡³æ— ç¨€ç–æ€§çš„ DNN åŠ é€Ÿä»»åŠ¡ï¼Œå…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ— å…³æ€§ (application-independent)ã€‚è¿™ä¸€ç‰¹æ€§ä½¿å¾— VUSA æˆä¸ºä¸€ç§é€šç”¨çš„ AI åŠ é€Ÿæ–¹æ¡ˆï¼Œç‰¹åˆ«ä¸ºèµ„æºå—é™çš„è¾¹ç¼˜ AI (Edge-AI) åº”ç”¨æä¾›äº†é«˜æ•ˆçš„ç¡¬ä»¶æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "Preprint accepted for publication at MOCAST 2025. Submitted for possible publication in IEEE Xplore",
      "pdf_url": "https://arxiv.org/pdf/2506.01166v1",
      "published_date": "2025-06-01 20:59:20 UTC",
      "updated_date": "2025-06-01 20:59:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:08:38.494078+00:00"
    },
    {
      "arxiv_id": "2506.06347v1",
      "title": "Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection",
      "title_zh": "ç»Ÿä¸€æ¸¸æˆå†…å®¹å®¡æ ¸ï¼šé¢å‘èµ„æºé«˜æ•ˆå‹æ¯’æ€§æ£€æµ‹çš„è½¯æç¤ºä¸ LLM è¾…åŠ©æ ‡ç­¾è¿ç§»",
      "authors": [
        "Zachary Yang",
        "Domenico Tullo",
        "Reihaneh Rabbany"
      ],
      "abstract": "Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with particularly strong performance in German, surpassing the English benchmark of 45.39%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¸¸æˆç¤¾åŒºä¸­è·¨æ¸¸æˆã€è·¨è¯­è¨€è¿è§„æ£€æµ‹(Toxicity Detection)çš„æ‰©å±•æ€§ä¸å®æ—¶æ•ˆç‡éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºSoft-Promptingå’ŒLLMè¾…åŠ©æ ‡ç­¾è¿ç§»çš„ç»Ÿä¸€æ²»ç†æ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆå¼•å…¥è½¯æç¤ºæ–¹æ³•ï¼Œé€šè¿‡é›†æˆæ¸¸æˆä¸Šä¸‹æ–‡æ ‡è®°(game-context tokens)ä½¿å•ä¸€æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤šæ¬¾æ¸¸æˆï¼Œåœ¨æ€§èƒ½åŒ¹é…å¤æ‚æ–¹æ³•çš„åŒæ—¶æä¾›äº†æ›´ä¼˜çš„å¯æ‰©å±•æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶åˆ©ç”¨GPT-4o-miniæ„å»ºäº†LLMè¾…åŠ©æ ‡ç­¾è¿ç§»æ¡†æ¶(LLM-assisted label transfer)ï¼Œå°†æ£€æµ‹èƒ½åŠ›æ‰©å±•è‡³ä¸ƒç§é¢å¤–è¯­è¨€ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨æ³•è¯­ã€å¾·è¯­ã€ä¿„è¯­ç­‰çœŸå®è¯­æ–™ä¸­çš„å®F1åˆ†æ•°(macro F1-scores)è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­å¾·è¯­æ€§èƒ½ç”šè‡³è¶…è¿‡äº†è‹±è¯­åŸºå‡†ã€‚ç›®å‰è¯¥æ–¹æ¡ˆå·²åœ¨è‚²ç¢§(Ubisoft)ç”Ÿäº§ç¯å¢ƒåº”ç”¨ï¼Œé€šè¿‡å‡å°‘å¤šæ¨¡å‹ç»´æŠ¤éœ€æ±‚å¤§å¹…é™ä½äº†è®¡ç®—èµ„æºæ”¯å‡ºï¼Œå¹¶èƒ½ç²¾å‡†è¯†åˆ«éœ€æƒ©å¤„çš„ç©å®¶è¡Œä¸ºï¼Œä¸ºèµ„æºå—é™ä¸‹çš„å¤šè¯­è¨€å®æ—¶å†…å®¹æ²»ç†æä¾›äº†æœ‰æ•ˆèŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 1 figure, 9 Tables, KDD 2025 ADS Track",
      "pdf_url": "https://arxiv.org/pdf/2506.06347v1",
      "published_date": "2025-06-01 20:50:43 UTC",
      "updated_date": "2025-06-01 20:50:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:08:35.994888+00:00"
    },
    {
      "arxiv_id": "2506.01158v2",
      "title": "Efficient Regression-Based Training of Normalizing Flows for Boltzmann Generators",
      "title_zh": "é¢å‘ç»å°”å…¹æ›¼ç”Ÿæˆå™¨çš„é«˜æ•ˆå½’ä¸€åŒ–æµå›å½’è®­ç»ƒæ–¹æ³•",
      "authors": [
        "Danyal Rehman",
        "Oscar Davis",
        "Jiarui Lu",
        "Jian Tang",
        "Michael Bronstein",
        "Yoshua Bengio",
        "Alexander Tong",
        "Avishek Joey Bose"
      ],
      "abstract": "Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to large-scale diffusion and flow matching models. However, such modern generative models suffer from expensive inference, inhibiting their use in numerous scientific applications like Boltzmann Generators (BGs) for molecular conformations that require fast likelihood evaluation. In this paper, we revisit classical normalizing flows in the context of BGs that offer efficient sampling and likelihoods, but whose training via maximum likelihood is often unstable and computationally challenging. We propose Regression Training of Normalizing Flows (RegFlow), a novel and scalable regression-based training objective that bypasses the numerical instability and computational challenge of conventional maximum likelihood training in favour of a simple $\\ell_2$-regression objective. Specifically, RegFlow maps prior samples under our flow to targets computed using optimal transport couplings or a pre-trained continuous normalizing flow (CNF). To enhance numerical stability, RegFlow employs effective regularization strategies such as a new forward-backward self-consistency loss that enjoys painless implementation. Empirically, we demonstrate that RegFlow unlocks a broader class of architectures that were previously intractable to train for BGs with maximum likelihood. We also show RegFlow exceeds the performance, computational cost, and stability of maximum likelihood training in equilibrium sampling in Cartesian coordinates of alanine dipeptide, tripeptide, and tetrapeptide, showcasing its potential in molecular systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Boltzmann Generators (BGs) åœ¨åˆ†å­æ„è±¡ç”Ÿæˆä¸­æ¨ç†æˆæœ¬é«˜ä»¥åŠç»å…¸ Normalizing Flows æå¤§ä¼¼ç„¶ (Maximum Likelihood) è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡ºäº† RegFlow (Regression Training of Normalizing Flows)ã€‚è¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„å›å½’è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç®€å•çš„ $\\ell_2$-regression ç›®æ ‡å‡½æ•°æ›¿ä»£äº†å¤æ‚çš„ä¼¼ç„¶è®¡ç®—ï¼Œå°†å…ˆéªŒæ ·æœ¬æ˜ å°„åˆ°ç”± Optimal Transport æˆ–é¢„è®­ç»ƒ Continuous Normalizing Flow (CNF) ç¡®å®šçš„ç›®æ ‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ç¨³å®šæ€§ï¼ŒRegFlow å¼•å…¥äº† Forward-backward self-consistency æŸå¤±å‡½æ•°è¿›è¡Œæ­£åˆ™åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒRegFlow ä¸ä»…èƒ½è®­ç»ƒä»¥å‰éš¾ä»¥æ”¶æ•›çš„å¤æ‚æ¶æ„ï¼Œåœ¨ä¸™æ°¨é…¸äºŒè‚½ã€ä¸‰è‚½å’Œå››è‚½çš„é‡‡æ ·æ€§èƒ½ã€è®¡ç®—æˆæœ¬åŠæ•°å€¼ç¨³å®šæ€§ä¸Šä¹Ÿå…¨é¢è¶…è¶Šäº†ä¼ ç»Ÿçš„æå¤§ä¼¼ç„¶è®­ç»ƒã€‚è¯¥æ–¹æ³•ä¸ºå¤æ‚åˆ†å­ç³»ç»Ÿçš„ç”Ÿæˆå»ºæ¨¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ç¨³å¥çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint; ICML GenBio Best Paper Award 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01158v2",
      "published_date": "2025-06-01 20:32:27 UTC",
      "updated_date": "2025-10-30 07:20:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:08:39.092236+00:00"
    },
    {
      "arxiv_id": "2506.01151v1",
      "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
      "title_zh": "é¢å‘é«˜æ•ˆç»“æ„åŒ–è§£ç çš„ Earley é©±åŠ¨åŠ¨æ€å‰ªæ",
      "authors": [
        "Xintong Sun",
        "Chi Wei",
        "Minghao Tian",
        "Shiwen Ni"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\\textbf{ZapFormat}$, a novel $\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\\textbf{consistently maintains}$ high-precision compliant outputs but also achieves $\\textbf{significant improvements}$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at https://github.com/Dan-wanna-M/formatron.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨ç”Ÿæˆç»“æ„åŒ–æˆ–ç¬¦åˆç‰¹å®šè¯­æ³•çº¦æŸçš„å†…å®¹æ—¶é¢ä¸´çš„æ˜¾è‘—è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º ZapFormat çš„æ–°å‹åŠ¨æ€å‰ªæ (dynamic pruning) ç­–ç•¥ã€‚è¯¥ç­–ç•¥åŸºäº Earley algorithmï¼Œèƒ½å¤Ÿå®æ—¶è¯†åˆ«å¹¶æ¶ˆé™¤æ— æ•ˆæˆ–å†—ä½™çš„ Earley statesï¼Œä»è€Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚è¿™ç§ä¼˜åŒ–ä½¿å¾—ç³»ç»Ÿå¯ä»¥åˆ©ç”¨çŠ¶æ€ç¼“å­˜ (state cache) æ¥è¿›ä¸€æ­¥åŠ é€Ÿå¤§è§„æ¨¡æŸ¥è¯¢ä¸‹çš„ç»“æ„åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚ç ”ç©¶è€…å°†æ­¤æŠ€æœ¯å®ç°ä¸ºå¼€æºçº¦æŸè§£ç å¼•æ“ Formatronï¼Œè¯¥å¼•æ“å…·å¤‡è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¯é€‚ç”¨äºå„ç§ LLM architecturesã€‚åœ¨ JSON ç”Ÿæˆã€JSON Schema éªŒè¯åŠè¯­ä¹‰è§£æ (semantic parsing) ç­‰ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFormatron åœ¨ç¡®ä¿è¾“å‡ºå®Œå…¨åˆè§„çš„å‰æä¸‹ï¼Œæ¨ç†é€Ÿåº¦æ¯”ç°æœ‰å…ˆè¿›æŠ€æœ¯æå‡äº†é«˜è¾¾ 2 å€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML2025 poster",
      "pdf_url": "https://arxiv.org/pdf/2506.01151v1",
      "published_date": "2025-06-01 20:05:30 UTC",
      "updated_date": "2025-06-01 20:05:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:08:43.688296+00:00"
    },
    {
      "arxiv_id": "2506.01133v1",
      "title": "From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models",
      "title_zh": "ä»æ–‡å­—åˆ°å£°æ³¢ï¼šè¯­éŸ³ä¸æ–‡æœ¬åŸºç¡€æ¨¡å‹ä¸­çš„æ¦‚å¿µå½¢æˆåˆ†æ",
      "authors": [
        "AsÄ±m Ersoy",
        "Basel Mousi",
        "Shammur Chowdhury",
        "Firoj Alam",
        "Fahim Dalvi",
        "Nadir Durrani"
      ],
      "abstract": "The emergence of large language models (LLMs) has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts--showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. For reproducibility we made scripts and other resources available to the community.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡çº¯æ–‡æœ¬è®­ç»ƒè·å¾—å¹¿æ³›ä¸–ç•ŒçŸ¥è¯†çš„èƒŒæ™¯ä¸‹ï¼Œè¯­éŸ³åŸºç¡€æ¨¡å‹ä»¥åŠå¤šæ¨¡æ€è”åˆè®­ç»ƒæ¨¡å‹æ˜¯å¦ä¹Ÿèƒ½å½¢æˆç±»ä¼¼çš„æŠ½è±¡è¯­ä¹‰æ¦‚å¿µã€‚ç ”ç©¶è€…é‡‡ç”¨äº†æ½œæ¦‚å¿µåˆ†æ(Latent Concept Analysis)è¿™ä¸€æ— ç›‘ç£æ–¹æ³•ï¼Œæ—¨åœ¨æ­ç¤ºå’Œè§£é‡Šç¥ç»ç½‘ç»œä¸­çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶æ·±å…¥å¯¹æ¯”äº†è¯­éŸ³å’Œæ–‡æœ¬æ¨¡å‹åœ¨å•ç‹¬åŠå…±åŒè®­ç»ƒä¸‹æ‰€ä¹ å¾—çš„æ¦‚å¿µç»“æ„ã€‚è¯¥è®ºæ–‡é‡ç‚¹åˆ†æäº†è¯­ä¹‰æŠ½è±¡å¦‚ä½•åœ¨ä¸åŒæ¨¡æ€é—´å½¢æˆï¼Œä»¥åŠå¤šæ¨¡æ€è®­ç»ƒæ˜¯å¦èƒ½ä¿ƒä½¿æ¨¡å‹å‘å±•å‡ºæ›´ä¸°å¯Œã€æ›´å…·ç»“æ„åŒ–çš„è¯­ä¹‰ç†è§£ã€‚é€šè¿‡å¯¹è¿™äº›æ½œåœ¨è¡¨ç¤ºçš„ç³»ç»Ÿæ€§å‰–æï¼Œè¯¥å·¥ä½œä¸ºç†è§£éæ–‡æœ¬æ¨¡æ€æ¨¡å‹æ•æ‰ä¸–ç•ŒçŸ¥è¯†çš„èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶å‘ç¤¾åŒºå¼€æ”¾äº†ç›¸å…³è„šæœ¬èµ„æºä»¥æ”¯æŒå¯å¤ç°ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01133v1",
      "published_date": "2025-06-01 19:33:21 UTC",
      "updated_date": "2025-06-01 19:33:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:08:52.185397+00:00"
    },
    {
      "arxiv_id": "2506.01121v1",
      "title": "Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation",
      "title_zh": "é¢å‘ç‰©ç†æ¥åœ°ã€é²æ£’ä¸å®‰å…¨ç”Ÿæˆçš„ç¥ç»ç¬¦å·ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Jacob K. Christopher",
        "Michael Cardei",
        "Jinhao Liang",
        "Ferdinando Fioretto"
      ],
      "abstract": "Despite the remarkable generative capabilities of diffusion models, their integration into safety-critical or scientifically rigorous applications remains hindered by the need to ensure compliance with stringent physical, structural, and operational constraints. To address this challenge, this paper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps with symbolic optimization, enabling the generation of certifiably consistent samples under user-defined functional and logic constraints. This key feature is provided for both standard and discrete diffusion models, enabling, for the first time, the generation of both continuous (e.g., images and trajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with constraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety, in the context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity, in domains such as drug discovery and materials engineering; and (3) Out-of-domain generalization, where enforcing symbolic constraints allows adaptation beyond the training distribution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Neuro-Symbolic Diffusion (NSD)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹(Diffusion Models)åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éš¾ä»¥æ»¡è¶³ç‰©ç†ã€ç»“æ„å’Œè¿è¡Œçº¦æŸçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ‰©æ•£æ­¥éª¤ä¸ç¬¦å·ä¼˜åŒ–(Symbolic Optimization)ç›¸ç»“åˆï¼Œå®ç°äº†åœ¨ç”¨æˆ·å®šä¹‰çš„é€»è¾‘çº¦æŸä¸‹ç”Ÿæˆå…·æœ‰ä¸€è‡´æ€§ä¿è¯çš„æ ·æœ¬ã€‚NSDä¸ä»…é€‚ç”¨äºæ ‡å‡†æ‰©æ•£æ¨¡å‹ï¼Œè¿˜é¦–æ¬¡æ‰©å±•è‡³ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå¦‚åˆ†å­ç»“æ„ã€è‡ªç„¶è¯­è¨€ç­‰ç¦»æ•£è¾“å‡ºä»¥åŠè½¨è¿¹ç­‰è¿ç»­è¾“å‡ºã€‚ç ”ç©¶åœ¨éæ¯’æ€§åˆ†å­ç”Ÿæˆã€æ— ç¢°æ’è½¨è¿¹ä¼˜åŒ–ã€è¯ç‰©å‘ç°åŠææ–™å·¥ç¨‹ç­‰é¢†åŸŸéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥ç¬¦å·çº¦æŸï¼Œè¯¥æ¨¡å‹ä¸ä»…æå‡äº†ç”Ÿæˆçš„å®‰å…¨æ€§ä¸ç¨³å¥æ€§ï¼Œè¿˜åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯å’ŒåŸŸå¤–æ³›åŒ–(Out-of-domain Generalization)ä»»åŠ¡ä¸­å±•ç°äº†è¶…è¶Šè®­ç»ƒåˆ†å¸ƒçš„é€‚åº”èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at the 2nd International Conference on Neuro-symbolic Systems (NeuS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.01121v1",
      "published_date": "2025-06-01 18:58:59 UTC",
      "updated_date": "2025-06-01 18:58:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:09:06.155857+00:00"
    },
    {
      "arxiv_id": "2506.01116v1",
      "title": "ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation",
      "title_zh": "ChemAUï¼šåˆ©ç”¨è‡ªé€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡åœ¨åŒ–å­¦ç ”ç©¶ä¸­å‘æŒ¥å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xinyi Liu",
        "Lipeng Ma",
        "Yixuan Li",
        "Weidong Yang",
        "Qingyuan Zhou",
        "Jiayi Song",
        "Shuhao Li",
        "Ben Fei"
      ],
      "abstract": "Large Language Models (LLMs) are widely used across various scenarios due to their exceptional reasoning capabilities and natural language understanding. While LLMs demonstrate strong performance in tasks involving mathematics and coding, their effectiveness diminishes significantly when applied to chemistry-related problems. Chemistry problems typically involve long and complex reasoning steps, which contain specific terminology, including specialized symbol systems and complex nomenclature conventions. These characteristics often cause general LLMs to experience hallucinations during the reasoning process due to their lack of specific knowledge. However, existing methods are struggling to effectively leverage chemical expertise and formulas. Moreover, current uncertainty estimation methods, designed to mitigate potential reasoning errors, are unable to precisely identify specific steps or key knowledge. In this work, we propose a novel framework called ChemAU, which incorporates our adaptive uncertainty estimation method that applies different uncertainty values based on the position of reasoning steps within the whole reasoning chain. Leveraging this method, ChemAU identifies gaps in chemistry knowledge and precisely supplements chemical expertise with the specialized domain model, thereby correcting and updating the previously flawed reasoning chain. Our experiments with three popular LLMs across three chemistry datasets demonstrate that ChemAU significantly enhances both reasoning accuracy and uncertainty estimation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†åŒ–å­¦é¢†åŸŸå¤æ‚æ¨ç†ã€ä¸“ä¸šæœ¯è¯­å’Œç¬¦å·ç³»ç»Ÿæ—¶æ˜“äº§ç”Ÿå¹»è§‰(hallucinations)çš„é—®é¢˜ï¼Œæå‡ºäº†ChemAUæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡(Adaptive Uncertainty Estimation)æ–¹æ³•ï¼Œæ ¹æ®æ¨ç†æ­¥éª¤åœ¨å®Œæ•´æ¨ç†é“¾ä¸­çš„ä½ç½®åŠ¨æ€åº”ç”¨ä¸åŒçš„ä¸ç¡®å®šæ€§æ•°å€¼ã€‚é€šè¿‡è¿™ä¸€æœºåˆ¶ï¼ŒChemAUèƒ½å¤Ÿç²¾å‡†è¯†åˆ«åŒ–å­¦çŸ¥è¯†ç¼ºå£ï¼Œå¹¶ç»“åˆä¸“é—¨çš„é¢†åŸŸæ¨¡å‹(domain model)è¡¥å……åŒ–å­¦ä¸“ä¸šçŸ¥è¯†ä¸å…¬å¼ï¼Œä»è€Œä¿®æ­£å’Œæ›´æ–°å…ˆå‰é”™è¯¯çš„æ¨ç†é“¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChemAUåœ¨ä¸‰ç§ä¸»æµLLMså’Œä¸‰ä¸ªåŒ–å­¦æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†æ¨ç†å‡†ç¡®åº¦ä»¥åŠä¸ç¡®å®šæ€§ä¼°è®¡çš„ç²¾å‡†æ€§ï¼Œä¸ºåŒ–å­¦ç ”ç©¶ä¸­çš„è‡ªåŠ¨åŒ–æ¨ç†æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01116v1",
      "published_date": "2025-06-01 18:45:49 UTC",
      "updated_date": "2025-06-01 18:45:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:09:01.244943+00:00"
    },
    {
      "arxiv_id": "2506.01114v1",
      "title": "Reconsidering LLM Uncertainty Estimation Methods in the Wild",
      "title_zh": "é‡æ–°å®¡è§†ç°å®åœºæ™¯ä¸‹ LLM ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•",
      "authors": [
        "Yavuz Bakman",
        "Duygu Nur Yaldiz",
        "Sungmin Kang",
        "Tuo Zhang",
        "Baturalp Buyukates",
        "Salman Avestimehr",
        "Sai Praneeth Karimireddy"
      ],
      "abstract": "Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: https://github.com/duygunuryldz/uncertainty_in_the_wild.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)çš„ä¸ç¡®å®šæ€§ä¼°è®¡(Uncertainty Estimation, UE)æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ï¼ŒæŒ‡å‡ºç›®å‰ä¸»æµè¯„ä¼°å¾€å¾€å¿½ç•¥äº†çœŸå®éƒ¨ç½²ç¯å¢ƒä¸‹çš„å¤æ‚æ€§ã€‚ä½œè€…é‡ç‚¹è€ƒå¯Ÿäº†UEæ–¹æ³•å¯¹å†³ç­–é˜ˆå€¼(decision threshold)çš„æ•æ„Ÿæ€§ã€å¯¹æŸ¥è¯¢è½¬æ¢ï¼ˆå¦‚é”™åˆ«å­—ã€å¯¹è¯å†å²ã€å¯¹æŠ—æ€§æç¤ºï¼‰çš„é²æ£’æ€§ã€åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„é€‚ç”¨æ€§ä»¥åŠå¤šå¾—åˆ†é›†æˆç­–ç•¥ã€‚é€šè¿‡å¯¹19ç§UEæ–¹æ³•çš„å®éªŒæ˜¾ç¤ºï¼Œå¤šæ•°æ–¹æ³•åœ¨æ ¡å‡†æ•°æ®åˆ†å¸ƒåç§»æ—¶å¯¹é˜ˆå€¼é€‰æ‹©æå…¶æ•æ„Ÿï¼Œä¸”å°½ç®¡å¯¹å†å²ä¿¡æ¯å’Œé”™åˆ«å­—å…·æœ‰é²æ£’æ€§ï¼Œå´ææ˜“å—åˆ°å¯¹æŠ—æ€§æç¤º(adversarial prompts)çš„å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç°æœ‰æ–¹æ³•è™½èƒ½é€‚é…é•¿æ–‡æœ¬ç”Ÿæˆä½†ä»å­˜åœ¨è¾ƒå¤§æ”¹è¿›ç©ºé—´ï¼Œè€Œé€šè¿‡é›†æˆ(ensembling)å¤šä¸ªUEå¾—åˆ†å¯ä»¥æ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†ç°æœ‰UEæŠ€æœ¯åœ¨ç°å®åœºæ™¯ä¸­çš„å±€é™æ€§ä¸æ½œåŠ›ï¼Œä¸ºæ„å»ºæ›´å¯é çš„å¹»è§‰æ£€æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01114v1",
      "published_date": "2025-06-01 18:42:24 UTC",
      "updated_date": "2025-06-01 18:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:09:09.135489+00:00"
    },
    {
      "arxiv_id": "2506.01111v1",
      "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion",
      "title_zh": "FusionAudio-1.2Mï¼šåŸºäºå¤šæ¨¡æ€ä¸Šä¸‹æ–‡èåˆçš„ç»†ç²’åº¦éŸ³é¢‘æè¿°ç”Ÿæˆ",
      "authors": [
        "Shunian Chen",
        "Xinyuan Xie",
        "Zheshu Chen",
        "Liyan Zhao",
        "Owen Lee",
        "Zhan Su",
        "Qilin Sun",
        "Benyou Wang"
      ],
      "abstract": "High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰éŸ³é¢‘æè¿°(audio captioning)åœ¨ç»†ç²’åº¦ç»†èŠ‚å’Œä¸Šä¸‹æ–‡å‡†ç¡®æ€§æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ¨¡ä»¿äººç±»å¬è§‰æ„ŸçŸ¥çš„ä¸¤é˜¶æ®µè‡ªåŠ¨åŒ–æµæ°´çº¿ã€‚è¯¥æµç¨‹é¦–å…ˆåˆ©ç”¨ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡å‹ä»è¯­éŸ³ã€éŸ³ä¹ã€æ™®é€šå£°éŸ³åŠå…³è”è§†é¢‘ä¸­æå–å¤šæ¨¡æ€ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œéšåé€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLM)åˆæˆè¿™äº›ä¸°å¯Œä¿¡æ¯ï¼Œç”Ÿæˆè¯¦ç»†ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„è¯†çš„éŸ³é¢‘æè¿°ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ä¸€ç§å¯æ‰©å±•çš„ç»†ç²’åº¦éŸ³é¢‘æè¿°ç”Ÿæˆæ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªåŒ…å«120ä¸‡æ¡è¯¦ç»†æè¿°å’Œ600ä¸‡ä¸ªé—®ç­”å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†FusionAudioã€‚æ­¤å¤–ï¼ŒåŸºäºè¯¥æ•°æ®é›†å¼€å‘çš„å¢å¼ºå‹CLAPéŸ³é¢‘ç¼–ç å™¨åœ¨éŸ³æ–‡å¯¹é½(audio-text alignment)å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜è¶Šæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤æ‚éŸ³é¢‘ç¯å¢ƒä¸‹æ›´ç»†è‡´ã€æ›´å‡†ç¡®çš„è‡ªåŠ¨åŒ–ç†è§£å¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01111v1",
      "published_date": "2025-06-01 18:29:17 UTC",
      "updated_date": "2025-06-01 18:29:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:09:06.317574+00:00"
    },
    {
      "arxiv_id": "2506.01109v3",
      "title": "CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian Splatting",
      "title_zh": "CountingFruitï¼šåŸºäºè¯­ä¹‰é«˜æ–¯æ³¼æº…çš„è¯­è¨€å¼•å¯¼ä¸‰ç»´æ°´æœè®¡æ•°",
      "authors": [
        "Fengze Li",
        "Yangle Liu",
        "Jieming Ma",
        "Hai-Ning Liang",
        "Yaochun Shen",
        "Huangxiang Li",
        "Zhijing Wu"
      ],
      "abstract": "Accurate 3D fruit counting in orchards is challenging due to heavy occlusion, semantic ambiguity between fruits and surrounding structures, and the high computational cost of volumetric reconstruction. Existing pipelines often rely on multi-view 2D segmentation and dense volumetric sampling, which lead to accumulated fusion errors and slow inference. We introduce FruitLangGS, a language-guided 3D fruit counting framework that reconstructs orchard-scale scenes using an adaptive-density Gaussian Splatting pipeline with radius-aware pruning and tile-based rasterization, enabling scalable 3D representation. During inference, compressed CLIP-aligned semantic vectors embedded in each Gaussian are filtered via a dual-threshold cosine similarity mechanism, retrieving Gaussians relevant to target prompts while suppressing common distractors (e.g., foliage), without requiring retraining or image-space masks. The selected Gaussians are then sampled into dense point clouds and clustered geometrically to estimate fruit instances, remaining robust under severe occlusion and viewpoint variation. Experiments on nine different orchard-scale datasets demonstrate that FruitLangGS consistently outperforms existing pipelines in instance counting recall, avoiding multi-view segmentation fusion errors and achieving up to 99.7% recall on Pfuji-Size_Orch2018 orchard dataset. Ablation studies further confirm that language-conditioned semantic embedding and dual-threshold prompt filtering are essential for suppressing distractors and improving counting accuracy under heavy occlusion. Beyond fruit counting, the same framework enables prompt-driven 3D semantic retrieval without retraining, highlighting the potential of language-guided 3D perception for scalable agricultural scene understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FruitLangGSï¼Œä¸€ç§è¯­è¨€å¼•å¯¼çš„ 3D æ°´æœè®¡æ•°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœå›­åœºæ™¯ä¸­ä¸¥é‡çš„é®æŒ¡ã€è¯­ä¹‰æ­§ä¹‰ä»¥åŠé«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å…·æœ‰åŠå¾„æ„ŸçŸ¥å‰ªæ (radius-aware pruning) å’Œåˆ†å—å…‰æ …åŒ– (tile-based rasterization) çš„è‡ªé€‚åº”å¯†åº¦ Gaussian Splatting ç®¡çº¿æ¥æ„å»ºå¯æ‰©å±•çš„ 3D è¡¨ç¤ºã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡åŒé˜ˆå€¼ä½™å¼¦ç›¸ä¼¼åº¦æœºåˆ¶è¿‡æ»¤åµŒå…¥åœ¨ Gaussian ä¸­çš„ CLIP å¯¹é½è¯­ä¹‰å‘é‡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å›¾åƒç©ºé—´æ©è†œå³å¯ç²¾ç¡®æ£€ç´¢ç›®æ ‡æ°´æœã€‚é€‰å®šçš„ Gaussian è¢«è¿›ä¸€æ­¥é‡‡æ ·ä¸ºç¨ å¯†ç‚¹äº‘å¹¶è¿›è¡Œå‡ ä½•èšç±»ä»¥ä¼°ç®—æ°´æœå®ä¾‹ï¼Œåœ¨ä¸¥é‡é®æŒ¡å’Œè§†è§’å˜åŒ–ä¸‹è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜ FruitLangGS åœ¨ 9 ä¸ªæœå›­æ•°æ®é›†ä¸Šçš„è®¡æ•°å¬å›ç‡ä¸€è‡´ä¼˜äºç°æœ‰ç®¡çº¿ï¼Œåœ¨ Pfuji-Size_Orch2018 æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 99.7% çš„å¬å›ç‡ã€‚è¯¥æ¡†æ¶ä¸ä»…å®ç°äº†é«˜æ•ˆçš„æ°´æœè®¡æ•°ï¼Œè¿˜æ”¯æŒæç¤ºé©±åŠ¨çš„ 3D è¯­ä¹‰æ£€ç´¢ï¼Œå±•ç¤ºäº†è¯­è¨€å¼•å¯¼çš„ 3D æ„ŸçŸ¥åœ¨æ™ºæ…§å†œä¸šé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01109v3",
      "published_date": "2025-06-01 18:19:47 UTC",
      "updated_date": "2025-08-07 10:47:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:10:27.788477+00:00"
    },
    {
      "arxiv_id": "2506.01107v2",
      "title": "Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator",
      "title_zh": "åˆ©ç”¨é©¬å°”å¯å¤«é“¾ç®—å­é€‰æ‹©ä¸ä»…æ¶åŒ–æ¥å—ç®—å­åŠ é€Ÿè¶…å¯å‘å¼ç®—æ³•",
      "authors": [
        "Abderrahim Bendahi",
        "Benjamin Doerr",
        "Adrien Fradin",
        "Johannes F. Lutzeyer"
      ],
      "abstract": "The move-acceptance hyper-heuristic was recently shown to be able to leave local optima with astonishing efficiency (Lissovoi et al., Artificial Intelligence (2023)). In this work, we propose two modifications to this algorithm that demonstrate impressive performances on a large class of benchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i) Instead of randomly choosing between the only-improving and any-move acceptance operator, we take this choice via a simple two-state Markov chain. This modification alone reduces the runtime on Jump$_m$ functions with gap parameter $m$ from $Î©(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves acceptance operator with the operator that only accepts worsenings. Such a, counter-intuitive, operator has not been used before in the literature. However, our proofs show that our only-worsening operator can greatly help in leaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3 \\log n)$ independent of the gap size. In general, we prove a remarkably good runtime of $O(n^{k+1} \\log n)$ for our Markov move-acceptance hyper-heuristic on all members of a new benchmark class SEQOPT$_k$, which contains a large number of functions having $k$ successive local optima, and which contains the commonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹move-acceptance hyper-heuristicç®—æ³•æå‡ºäº†ä¸¤ç§æ”¹è¿›æ–¹æ¡ˆï¼Œæ—¨åœ¨æ˜¾è‘—æå‡å…¶åœ¨Cliff$_d$å’ŒJump$_m$ç­‰åŸºå‡†å‡½æ•°ä¸Šçš„è¿è¡Œæ•ˆç‡ã€‚é¦–å…ˆï¼Œç ”ç©¶è€…åˆ©ç”¨ç®€å•çš„ä¸¤çŠ¶æ€Markov chainï¼ˆé©¬å°”å¯å¤«é“¾ï¼‰è¿›è¡Œç®—å­é€‰æ‹©ï¼Œå°†Jump$_m$å‡½æ•°çš„æ—¶é—´å¤æ‚åº¦ä»$Î©(n^{2m-1})$é™ä½è‡³$O(n^{m+1})$ã€‚å…¶æ¬¡ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åç›´è§‰çš„only-worsening acceptance operatorï¼ˆä»…æ¥å—æ¶åŒ–ç®—å­ï¼‰ï¼Œè¯æ˜å…¶èƒ½æå¤§å¸®åŠ©ç®—æ³•è·³å‡ºå±€éƒ¨æœ€ä¼˜ï¼Œä½¿Jumpå‡½æ•°åœ¨ä¸ä¾èµ–é—´éš”å¤§å°çš„æƒ…å†µä¸‹çš„è¿è¡Œæ—¶é—´ç¼©çŸ­è‡³$O(n^3 \\log n)$ã€‚åœ¨åŒ…å«å¤šä¸ªè¿ç»­å±€éƒ¨æœ€ä¼˜çš„æ–°åŸºå‡†ç±»SEQOPT$_k$ä¸­ï¼Œè¯¥Markov move-acceptance hyper-heuristicåŒæ ·è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†$O(n^{k+1} \\log n)$çš„è¿è¡Œæ—¶é—´ã€‚è¿™äº›ç ”ç©¶æˆæœä¸ºè§£å†³å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸­çš„å±€éƒ¨æœ€ä¼˜ç“¶é¢ˆæä¾›äº†é«˜æ•ˆçš„ç†è®ºç®—å­ä¸åˆ†ææ¡†æ¶ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.NE",
      "comment": "Distinguished paper award at IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01107v2",
      "published_date": "2025-06-01 18:16:06 UTC",
      "updated_date": "2025-09-01 09:09:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:10:49.489387+00:00"
    },
    {
      "arxiv_id": "2506.01096v2",
      "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning",
      "title_zh": "SuperRLï¼šç»“åˆç›‘ç£æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ ï¼Œæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yihao Liu",
        "Shuocheng Li",
        "Lang Cao",
        "Yuhang Xie",
        "Mengyu Zhou",
        "Haoyu Dong",
        "Xiaojun Ma",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "abstract": "Large language models are increasingly used for complex reasoning tasks where high-quality offline data such as expert-annotated solutions and distilled reasoning traces are often available. However, in environments with sparse rewards, reinforcement learning struggles to sample successful trajectories, leading to inefficient learning. At the same time, these offline trajectories that represent correct reasoning paths are not utilized by standard on-policy reinforcement learning methods. We introduce SuperRL, a unified training framework that adaptively alternates between RL and SFT. Whenever every rollout for a given instance receives zero reward, indicating the absence of a learning signal, SuperRL falls back to SFT on the curated offline data. Extensive experiments across diverse reasoning benchmarks show that SuperRL surpasses vanilla RL by delivering higher sample efficiency, stronger generalization, and improved robustness under sparse rewards.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SuperRLï¼Œä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­èƒ½åŠ›çš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ã€‚é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)åœ¨ç¨€ç–å¥–åŠ±(Sparse Rewards)ç¯å¢ƒä¸‹éš¾ä»¥é€šè¿‡é‡‡æ ·è·å–æˆåŠŸè½¨è¿¹ï¼Œä¸”ä¼ ç»Ÿåœ¨çº¿RLæ–¹æ³•æ— æ³•åˆ©ç”¨ç¦»çº¿æ¨ç†è·¯å¾„çš„é—®é¢˜ï¼ŒSuperRLå®ç°äº†åœ¨RLä¸ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning, SFT)ä¹‹é—´çš„è‡ªé€‚åº”åˆ‡æ¢ã€‚å½“æŸä¸ªå®ä¾‹çš„æ‰€æœ‰é‡‡æ ·å°è¯•(Rollout)å‡æ— æ³•è·å¾—å¥–åŠ±ä¿¡å·æ—¶ï¼Œè¯¥æ¡†æ¶ä¼šè‡ªåŠ¨å›é€€ï¼Œåˆ©ç”¨ç²¾é€‰çš„ç¦»çº¿æ•°æ®è¿›è¡ŒSFTè®­ç»ƒã€‚å¤šé¡¹æ¨ç†åŸºå‡†æµ‹è¯•çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSuperRLåœ¨æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›ä»¥åŠåº”å¯¹ç¨€ç–å¥–åŠ±çš„é²æ£’æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»ŸRLæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01096v2",
      "published_date": "2025-06-01 17:43:54 UTC",
      "updated_date": "2025-08-08 08:03:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:10:51.184135+00:00"
    },
    {
      "arxiv_id": "2506.01095v1",
      "title": "Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication",
      "title_zh": "æ¨¡å—åŒ–è¯´è¯äººæ¶æ„ï¼šå¤šæ™ºèƒ½ä½“ AI é€šä¿¡ä¸­ä¿éšœè´£ä»»ä¸è¯­å¢ƒå®Œæ•´æ€§çš„æ¡†æ¶",
      "authors": [
        "Khe-Han Toh",
        "Hong-Kuan Teo"
      ],
      "abstract": "Sustaining coherent, role-aware communication across multi-agent systems remains a foundational challenge in AI. Current frameworks often lack explicit mechanisms for speaker responsibility, leading to context drift, alignment instability, and degraded interpretability over time. We propose the Modular Speaker Architecture (MSA), a framework that decomposes speaker behavior into modular components for role tracking, responsibility continuity, and contextual coherence. Grounded in high-context human-AI dialogues, MSA includes three core modules: a Speaker Role Module, a Responsibility Chain Tracker, and a Contextual Integrity Validator. We evaluate MSA through annotated case studies and introduce structural metrics-pragmatic consistency, responsibility flow, and context stability-quantified via manual and automatic scoring and bootstrapped statistical analysis. Our results show that MSA reliably maintains interaction structure without reliance on affective signals or surface-level heuristics. We further implement a prototype configuration language (G-Code) and modular API to support MSA deployment in dynamic multi-agent scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Modular Speaker Architecture (MSA)ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ AI é€šä¿¡ä¸­ç”±äºç¼ºä¹æ˜ç¡®çš„è¯´è¯è€…è´£ä»»æœºåˆ¶è€Œå¯¼è‡´çš„ä¸Šä¸‹æ–‡æ¼‚ç§» (context drift)ã€å¯¹é½ä¸ç¨³å®šå’Œå¯è§£é‡Šæ€§ä¸‹é™ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è¯´è¯è€…è¡Œä¸ºåˆ†è§£ä¸ºæ¨¡å—åŒ–ç»„ä»¶ï¼ŒåŒ…å« Speaker Role Moduleã€Responsibility Chain Tracker å’Œ Contextual Integrity Validator ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼Œä»¥å®ç°è§’è‰²è·Ÿè¸ªã€è´£ä»»è¿ç»­æ€§å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ ‡æ³¨æ¡ˆä¾‹ç ”ç©¶å¯¹ MSA è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å¼•å…¥äº†è¯­ç”¨ä¸€è‡´æ€§ (pragmatic consistency)ã€è´£ä»»æµ (responsibility flow) å’Œä¸Šä¸‹æ–‡ç¨³å®šæ€§ (context stability) ç­‰ç»“æ„åŒ–æŒ‡æ ‡è¿›è¡Œé‡åŒ–åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMSA èƒ½å¤Ÿå¯é åœ°ç»´æŒäº¤äº’ç»“æ„ï¼Œä¸”æ— éœ€ä¾èµ–æƒ…æ„Ÿä¿¡å·æˆ–è¡¨å±‚å¯å‘å¼æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®è¿˜å¼€å‘äº†åŸå‹é…ç½®è¯­è¨€ (G-Code) å’Œæ¨¡å—åŒ– APIï¼Œä¸º MSA åœ¨åŠ¨æ€å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„å®é™…éƒ¨ç½²æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01095v1",
      "published_date": "2025-06-01 17:39:51 UTC",
      "updated_date": "2025-06-01 17:39:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:09:53.248561+00:00"
    },
    {
      "arxiv_id": "2506.01093v1",
      "title": "Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking",
      "title_zh": "ç›‘ç®¡å›¾è°±ä¸ GenAIï¼šé¢å‘é“¶è¡Œä¸šå®æ—¶äº¤æ˜“ç›‘æ§ä¸åˆè§„è§£é‡Š",
      "authors": [
        "Kunal Khanvilkar",
        "Kranthi Kommuru"
      ],
      "abstract": "This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®æ—¶äº¤æ˜“ç›‘æ§æ¡†æ¶ï¼Œç»“åˆäº† graph-based modelingã€narrative field embedding å’Œ generative explanationï¼Œæ—¨åœ¨æ”¯æŒé‡‘èé¢†åŸŸçš„è‡ªåŠ¨åŒ–åˆè§„ã€‚ç³»ç»Ÿé€šè¿‡æ„å»ºåŠ¨æ€äº¤æ˜“å›¾å¹¶åˆ©ç”¨ graph neural network åˆ†ç±»å¯ç–‘è¡Œä¸ºï¼ŒåŒæ—¶é›†æˆ retrieval-augmented generation æ¨¡å—ï¼Œä¸ºæ¯ç¬”è¢«æ ‡è®°çš„äº¤æ˜“ç”Ÿæˆç¬¦åˆç›‘ç®¡æ¡æ¬¾çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚åœ¨æ¨¡æ‹Ÿé‡‘èæ•°æ®æµä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¡¨ç°å“è¶Šï¼Œå–å¾—äº† 98.2% çš„ F1-scoreã€97.8% çš„ precision å’Œ 97.0% çš„ recallã€‚ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆçš„åˆè§„ç†ç”±å…·æœ‰æé«˜çš„è´¨é‡ä¸å¯è§£é‡Šæ€§ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å°† graph intelligence ä¸ generative models ç›¸ç»“åˆçš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿä¸ºé«˜é£é™©é‡‘èç¯å¢ƒæä¾›å¯è§£é‡Šä¸”ç¬¦åˆå®¡è®¡è¦æ±‚çš„åˆè§„æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01093v1",
      "published_date": "2025-06-01 17:34:57 UTC",
      "updated_date": "2025-06-01 17:34:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:09:44.642281+00:00"
    },
    {
      "arxiv_id": "2506.01089v1",
      "title": "Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements",
      "title_zh": "è¯­å¢ƒä¿¡æ¯çš„ç–æ¼ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å¯¹æŒ‡ç¤ºæˆåˆ†çš„ç†è§£",
      "authors": [
        "Metehan Oguz",
        "Yavuz Bakman",
        "Duygu Nur Yaldiz"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performances in tasks related to coreference resolution. However, previous studies mostly assessed LLM performance on coreference resolution with nouns and third person pronouns. This study evaluates LLM performance on coreference resolution with indexical like I, you, here and tomorrow, which come with unique challenges due to their linguistic properties. We present the first study examining how LLMs interpret indexicals in English, releasing the English Indexical Dataset with 1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that LLMs exhibit an impressive performance with some indexicals (I), while struggling with others (you, here, tomorrow), and that syntactic cues (e.g. quotation) contribute to LLM performance with some indexicals, while they reduce performance with others. Code and data are available at: https://github.com/metehanoguzz/LLMs-Indexicals-English.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†Iã€youã€hereå’Œtomorrowç­‰æŒ‡ä»£æ€§è¯æ±‡(indexicals)æ—¶çš„å…±æŒ‡æ¶ˆè§£(coreference resolution)èƒ½åŠ›ï¼Œè§£å†³äº†æ­¤ç±»è¯æ±‡å› å…¶ç‹¬ç‰¹çš„è¯­è¨€å±æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä½œè€…æ¨å‡ºäº†é¦–ä¸ªé’ˆå¯¹è‹±è¯­æŒ‡ä»£è¯ç†è§£çš„ç ”ç©¶ï¼Œå¹¶å‘å¸ƒäº†åŒ…å«1600é“å¤šé¡¹é€‰æ‹©é¢˜çš„English Indexical Datasetã€‚ç ”ç©¶é€šè¿‡è¯„ä¼°GPT-4oã€Claude 3.5 Sonnetã€Gemini 1.5 Proå’ŒDeepSeek V3ç­‰å‰æ²¿æ¨¡å‹å‘ç°ï¼ŒLLMså¯¹Içš„è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†youã€hereå’Œtomorrowæ—¶è¡¨ç°æ¬ ä½³ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜å¼•å·(quotation)ç­‰å¥æ³•çº¿ç´¢(syntactic cues)åœ¨ä¸åŒè¯­å¢ƒä¸‹å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç†è§£è¯­å¢ƒä¾èµ–æ€§å…ƒç´ æ–¹é¢çš„å±€é™ï¼Œå¹¶å¼€æºäº†ç›¸å…³ä»£ç ä¸æ•°æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.01089v1",
      "published_date": "2025-06-01 17:21:49 UTC",
      "updated_date": "2025-06-01 17:21:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:10:16.298016+00:00"
    },
    {
      "arxiv_id": "2506.01087v1",
      "title": "Choices and their Provenance: Explaining Stable Solutions of Abstract Argumentation Frameworks",
      "title_zh": "é€‰æ‹©åŠå…¶æº¯æºï¼šæŠ½è±¡è®ºè¯æ¡†æ¶ç¨³å®šè§£çš„é˜é‡Š",
      "authors": [
        "Bertram LudÃ¤scher",
        "Yilin Xia",
        "Shawn Bowers"
      ],
      "abstract": "The rule $\\mathrm{Defeated}(x) \\leftarrow \\mathrm{Attacks}(y,x),\\, \\neg \\, \\mathrm{Defeated}(y)$, evaluated under the well-founded semantics (WFS), yields a unique 3-valued (skeptical) solution of an abstract argumentation framework (AF). An argument $x$ is defeated ($\\mathrm{OUT}$) if there exists an undefeated argument $y$ that attacks it. For 2-valued (stable) solutions, this is the case iff $y$ is accepted ($\\mathrm{IN}$), i.e., if all of $y$'s attackers are defeated. Under WFS, arguments that are neither accepted nor defeated are undecided ($\\mathrm{UNDEC}$). As shown in prior work, well-founded solutions (a.k.a. grounded labelings) \"explain themselves\": The provenance of arguments is given by subgraphs (definable via regular path queries) rooted at the node of interest. This provenance is closely related to winning strategies of a two-player argumentation game.\n  We present a novel approach for extending this provenance to stable AF solutions. Unlike grounded solutions, which can be constructed via a bottom-up alternating fixpoint procedure, stable models often involve non-deterministic choice as part of the search for models. Thus, the provenance of stable solutions is of a different nature, and reflects a more expressive generate & test paradigm. Our approach identifies minimal sets of critical attacks, pinpointing choices and assumptions made by a stable model. These critical attack edges provide additional insights into the provenance of an argument's status, combining well-founded derivation steps with choice steps. Our approach can be understood as a form of diagnosis that finds minimal \"repairs\" to an AF graph such that the well-founded solution of the repaired graph coincides with the desired stable model of the original AF graph.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŠ½è±¡è®ºè¯æ¡†æ¶ (Abstract Argumentation Frameworks) ä¸­ç¨³å®šè§£çš„è§£é‡Šç”Ÿæˆé—®é¢˜ã€‚é’ˆå¯¹åŸºäºè‰¯åŸºè¯­ä¹‰ (well-founded semantics) çš„è§£å…·å¤‡å¤©ç„¶æº¯æºæ€§è€Œç¨³å®šè§£æ¶‰åŠéç¡®å®šæ€§é€‰æ‹©çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å°†æº¯æº (provenance) æ‰©å±•è‡³ç¨³å®šè§£çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«å…³é”®æ”»å‡» (critical attacks) çš„æœ€å°é›†åˆï¼Œç²¾ç¡®å®šä½äº†ç¨³å®šæ¨¡å‹åœ¨æœç´¢è¿‡ç¨‹ä¸­æ‰€åšçš„æ ¸å¿ƒé€‰æ‹©ä¸å‡è®¾ã€‚è¿™ç§æ–¹æ³•å°†è‰¯åŸºæ¨å¯¼æ­¥éª¤ä¸é€‰æ‹©æ­¥éª¤ç›¸ç»“åˆï¼Œå®é™…ä¸Šæ˜¯é€šè¿‡å¯»æ‰¾ AF å›¾çš„æœ€å°â€œä¿®å¤â€ (repairs)ï¼Œä½¿å¾—ä¿®å¤åå›¾çš„è‰¯åŸºè§£ä¸åŸå›¾çš„ç¨³å®šæ¨¡å‹è¾¾æˆä¸€è‡´ã€‚è¯¥ç ”ç©¶ä¸ºæ·±å…¥ç†è§£å¤æ‚ç¨³å®šæ¨¡å‹ä¸‹çš„è®ºç‚¹çŠ¶æ€æä¾›äº†æ›´å…·è¡¨è¾¾åŠ›çš„è¯Šæ–­å·¥å…·å’Œè§£é‡Šæ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "International Workshop on the Theory and Practice of Provenance (TaPP) and ProvenanceWeek'25 @SIGMOD, June 27, 2025. Berlin, Germany",
      "pdf_url": "https://arxiv.org/pdf/2506.01087v1",
      "published_date": "2025-06-01 17:09:55 UTC",
      "updated_date": "2025-06-01 17:09:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:10:19.189135+00:00"
    },
    {
      "arxiv_id": "2506.01085v1",
      "title": "Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection",
      "title_zh": "å­¦å…¶æ‰€é‡ï¼šåŸºäºç›¸å¯¹è¯¯å·®é©±åŠ¨æ ·æœ¬é€‰æ‹©çš„ä¼˜å…ˆæ¦‚å¿µå­¦ä¹ ",
      "authors": [
        "Shivam Chandhok",
        "Qian Yang",
        "Oscar Manas",
        "Kanishk Jain",
        "Leonid Sigal",
        "Aishwarya Agrawal"
      ],
      "abstract": "Instruction tuning has been central to the success of recent vision-language models (VLMs), but it remains expensive-requiring large-scale datasets, high-quality annotations, and large compute budgets. We propose PRioritized cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data- and compute-efficient framework that enables VLMs to dynamically select what to learn next based on their evolving needs during training. At each stage, the model tracks its learning progress across skills and selects the most informative samples-those it has not already mastered and that are not too difficult to learn at the current stage of training. This strategy effectively controls skill acquisition and the order in which skills are learned. Specifically, we sample from skills showing the highest learning progress, prioritizing those with the most rapid improvement. Unlike prior methods, PROGRESS requires no upfront answer annotations, queries answers only on a need basis, avoids reliance on additional supervision from auxiliary VLMs, and does not require compute-heavy gradient computations for data selection. Experiments across multiple instruction-tuning datasets of varying scales demonstrate that PROGRESS consistently outperforms state-of-the-art baselines with much less data and supervision. Additionally, we show strong cross-architecture generalization and transferability to larger models, validating PROGRESS as a scalable solution for efficient learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PROGRESS (PRioritized cOncept learninG via Relative Error-driven Sample Selection)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„æ•°æ®ä¸è®¡ç®—é«˜æ•ˆå‹æŒ‡ä»¤å¾®è°ƒ (Instruction tuning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´æ ¹æ®è‡ªèº«æ¼”è¿›éœ€æ±‚åŠ¨æ€é€‰æ‹©å­¦ä¹ å†…å®¹ï¼Œé€šè¿‡è¿½è¸ªå„é¡¹æŠ€èƒ½çš„å­¦ä¹ è¿›åº¦æ¥ç­›é€‰ä¿¡æ¯é‡æœ€å¤§çš„æ ·æœ¬ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹ä¼˜å…ˆé€‰æ‹©å°šæœªå®Œå…¨æŒæ¡ä¸”å½“å‰é˜¶æ®µèƒ½å¤Ÿå­¦ä¹ çš„æ ·æœ¬ï¼Œå¹¶ä¾§é‡äºæ”¹è¿›é€Ÿåº¦æœ€å¿«çš„æŠ€èƒ½ã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒPROGRESS æ— éœ€é¢„å…ˆè¿›è¡Œç­”æ¡ˆæ ‡æ³¨ï¼Œä¸ä¾èµ–è¾…åŠ©æ¨¡å‹çš„é¢å¤–ç›‘ç£ï¼Œä¹Ÿæ— éœ€ä¸ºæ•°æ®é€‰æ‹©è¿›è¡Œç¹é‡çš„æ¢¯åº¦è®¡ç®—ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä»¥æ›´å°‘çš„æ•°æ®å’Œç›‘ç£æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒPROGRESS è¿˜å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ¶æ„æ³›åŒ–èƒ½åŠ›å’Œå‘æ›´å¤§å‹æ¨¡å‹çš„è¿ç§»æ€§ï¼Œä¸ºé«˜æ•ˆå­¦ä¹ æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.01085v1",
      "published_date": "2025-06-01 17:05:35 UTC",
      "updated_date": "2025-06-01 17:05:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:10:40.591853+00:00"
    },
    {
      "arxiv_id": "2506.02062v1",
      "title": "Predicting Blood Type: Assessing Model Performance with ROC Analysis",
      "title_zh": "è¡€å‹é¢„æµ‹ï¼šåŸºäºROCåˆ†æçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°",
      "authors": [
        "Malik A. Altayar",
        "Muhyeeddin Alqaraleh",
        "Mowafaq Salem Alzboon",
        "Wesam T. Almagharbeh"
      ],
      "abstract": "Introduction: Personal identification is a critical aspect of forensic sciences, security, and healthcare. While conventional biometrics systems such as DNA profiling and iris scanning offer high accuracy, they are time-consuming and costly. Objectives: This study investigates the relationship between fingerprint patterns and ABO blood group classification to explore potential correlations between these two traits. Methods: The study analyzed 200 individuals, categorizing their fingerprints into three types: loops, whorls, and arches. Blood group classification was also recorded. Statistical analysis, including chi-square and Pearson correlation tests, was used to assess associations between fingerprint patterns and blood groups. Results: Loops were the most common fingerprint pattern, while blood group O+ was the most prevalent among the participants. Statistical analysis revealed no significant correlation between fingerprint patterns and blood groups (p > 0.05), suggesting that these traits are independent. Conclusions: Although the study showed limited correlation between fingerprint patterns and ABO blood groups, it highlights the importance of future research using larger and more diverse populations, incorporating machine learning approaches, and integrating multiple biometric signals. This study contributes to forensic science by emphasizing the need for rigorous protocols and comprehensive investigations in personal identification.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒ‡çº¹æ¨¡å¼(Fingerprint patterns)ä¸ABOè¡€å‹åˆ†ç±»(ABO blood group classification)ä¹‹é—´çš„æ½œåœ¨å…³è”ï¼Œæ—¨åœ¨ä¸ºæ³•åŒ»å­¦å’Œä¸ªäººèº«ä»½è¯†åˆ«æä¾›ä¸€ç§é«˜æ•ˆä¸”ä½æˆæœ¬çš„è¾…åŠ©æ‰‹æ®µã€‚ç ”ç©¶åˆ†æäº†200åå—è¯•è€…çš„ç”Ÿç‰©ç‰¹å¾ï¼Œå°†æŒ‡çº¹åˆ†ä¸ºæ–—å‹(Loops)ã€ç¯å‹(Whorls)å’Œå¼“å‹(Arches)ä¸‰ç±»ï¼Œå¹¶åˆ©ç”¨å¡æ–¹æ£€éªŒ(Chi-square test)å’Œçš®å°”é€Šç›¸å…³åˆ†æ(Pearson correlation test)è¿›è¡Œç»Ÿè®¡è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ–—å‹æŒ‡çº¹å’ŒO+è¡€å‹åœ¨æ ·æœ¬ä¸­æœ€ä¸ºæ™®éï¼Œä½†æŒ‡çº¹æ¨¡å¼ä¸è¡€å‹ä¹‹é—´å¹¶æœªå‘ç°æ˜¾è‘—çš„ç›¸å…³æ€§(p > 0.05)ã€‚è¿™ä¸€ç»“è®ºè¯´æ˜æŒ‡çº¹ä¸è¡€å‹ç‰¹å¾åœ¨é—ä¼ æˆ–ç‰©ç†è¡¨ç°ä¸Šå…·æœ‰ç‹¬ç«‹æ€§ï¼Œç›®å‰å°šæ— æ³•ç›´æ¥é€šè¿‡æŒ‡çº¹æ¨¡å¼é¢„æµ‹è¡€å‹ã€‚ç ”ç©¶æœ€åå¼ºè°ƒï¼Œè™½ç„¶ç»“æœæ˜¾ç¤ºç›¸å…³æ€§æœ‰é™ï¼Œä½†è¿™ä¸ºåç»­ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ï¼Œå³éœ€è¦åˆ©ç”¨æ›´å¤§è§„æ¨¡çš„äººç¾¤æ•°æ®ï¼Œå¹¶å¼•å…¥æœºå™¨å­¦ä¹ (Machine learning)æ–¹æ³•å’Œé›†æˆå¤šæ¨¡æ€ç”Ÿç‰©ä¿¡å·ï¼Œä»¥æ„å»ºæ›´ä¸¥è°¨çš„ä¸ªäººèº«ä»½è¯†åˆ«åè®®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02062v1",
      "published_date": "2025-06-01 17:04:12 UTC",
      "updated_date": "2025-06-01 17:04:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:11:41.856677+00:00"
    },
    {
      "arxiv_id": "2506.04254v2",
      "title": "Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support",
      "title_zh": "å±€åœ°æ£®æ—ç«ç¾é£é™©é¢„æµ‹ï¼šä¸€ç§é¢å‘ä¸šåŠ¡å†³ç­–æ”¯æŒçš„éƒ¨é—¨æ„ŸçŸ¥æ–¹æ³•",
      "authors": [
        "Nicolas Caron",
        "Christophe Guyeux",
        "Hassan Noura",
        "Benjamin Aynes"
      ],
      "abstract": "Forest fire prediction involves estimating the likelihood of fire ignition or related risk levels in a specific area over a defined time period. With climate change intensifying fire behavior and frequency, accurate prediction has become one of the most pressing challenges in Artificial Intelligence (AI). Traditionally, fire ignition is approached as a binary classification task in the literature. However, this formulation oversimplifies the problem, especially from the perspective of end-users such as firefighters. In general, as is the case in France, firefighting units are organized by department, each with its terrain, climate conditions, and historical experience with fire events. Consequently, fire risk should be modeled in a way that is sensitive to local conditions and does not assume uniform risk across all regions. This paper proposes a new approach that tailors fire risk assessment to departmental contexts, offering more actionable and region-specific predictions for operational use. With this, we present the first national-scale AI benchmark for metropolitan France using state-of-the-art AI models on a relatively unexplored dataset. Finally, we offer a summary of important future works that should be taken into account. Supplementary materials are available on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ°”å€™å˜åŒ–ä¸‹æ—¥ç›Šä¸¥å³»çš„æ£®æ—ç«ç¾é£é™©é¢„æµ‹é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„äºŒåˆ†ç±»(binary classification)å»ºæ¨¡æ–¹å¼å› è¿‡åº¦ç®€åŒ–è€Œå¿½è§†äº†ä¸åŒåœ°åŒºåœ¨åœ°å½¢ã€æ°”å€™å’Œå†å²ç»éªŒä¸Šçš„å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§éƒ¨é—¨æ„ŸçŸ¥æ–¹æ³•(Department-Aware Approach)ï¼Œæ—¨åœ¨å°†ç«ç¾é£é™©è¯„ä¼°ä¸å…·ä½“çš„è¡Œæ”¿éƒ¨é—¨èƒŒæ™¯ç›¸ç»“åˆï¼Œæä¾›æ›´å…·æ“ä½œæ€§å’ŒåŒºåŸŸé’ˆå¯¹æ€§çš„é¢„æµ‹ã€‚é€šè¿‡åœ¨æ³•å›½æœ¬åœŸå¼€å±•é¦–ä¸ªå›½å®¶çº§äººå·¥æ™ºèƒ½åŸºå‡†(AI benchmark)æµ‹è¯•ï¼Œç ”ç©¶åˆ©ç”¨æœ€å…ˆè¿›çš„AIæ¨¡å‹åœ¨ç›¸å¯¹æœªç»æ¢ç´¢çš„æ•°æ®é›†ä¸Šè¯æ˜äº†å±€éƒ¨æ„ŸçŸ¥çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æˆæœä¸ºä½œæˆ˜å†³ç­–æ”¯æŒ(Operational Decision Support)æä¾›äº†æ›´ç²¾å‡†çš„å·¥å…·ï¼Œæœ‰æ•ˆæå‡äº†æ£®æ—ç«ç¾é¢„æµ‹åœ¨å®é™…åº”ç”¨ä¸­çš„é’ˆå¯¹æ€§å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures, 4 tables, submitted to CFP: 7th IEEE Computers, Communications and IT Applications Conference December",
      "pdf_url": "https://arxiv.org/pdf/2506.04254v2",
      "published_date": "2025-06-01 16:54:48 UTC",
      "updated_date": "2025-10-02 12:31:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:10:52.191142+00:00"
    },
    {
      "arxiv_id": "2506.01080v2",
      "title": "The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process",
      "title_zh": "å³å°†åˆ°æ¥çš„å¤šæ™ºèƒ½ä½“å¯¹é½å¤±æ•ˆå±æœºï¼šAI å¯¹é½å¿…é¡»æ˜¯ä¸€ä¸ªåŠ¨æ€çš„ç¤¾ä¼šåŒ–è¿‡ç¨‹",
      "authors": [
        "Florian Carichon",
        "Aditi Khandelwal",
        "Marylou Fauchard",
        "Golnoosh Farnadi"
      ],
      "abstract": "This position paper states that AI Alignment in Multi-Agent Systems (MAS) should be considered a dynamic and interaction-dependent process that heavily depends on the social environment where agents are deployed, either collaborative, cooperative, or competitive. While AI alignment with human values and preferences remains a core challenge, the growing prevalence of MAS in real-world applications introduces a new dynamic that reshapes how agents pursue goals and interact to accomplish various tasks. As agents engage with one another, they must coordinate to accomplish both individual and collective goals. However, this complex social organization may unintentionally misalign some or all of these agents with human values or user preferences. Drawing on social sciences, we analyze how social structure can deter or shatter group and individual values. Based on these analyses, we call on the AI community to treat human, preferential, and objective alignment as an interdependent concept, rather than isolated problems. Finally, we emphasize the urgent need for simulation environments, benchmarks, and evaluation frameworks that allow researchers to assess alignment in these interactive multi-agent contexts before such dynamics grow too complex to control.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡æŒ‡å‡ºï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent Systems, MAS) ä¸­çš„äººå·¥æ™ºèƒ½å¯¹é½ (AI Alignment) åº”å½“è¢«è§†ä¸ºä¸€ä¸ªåŠ¨æ€ä¸”ä¾èµ–äºäº¤äº’çš„è¿‡ç¨‹ï¼Œå…¶æ ¸å¿ƒå–å†³äºä»£ç†æ‰€éƒ¨ç½²çš„åä½œã€åˆä½œæˆ–ç«äº‰æ€§ç¤¾ä¼šç¯å¢ƒã€‚éšç€ MAS åœ¨ç°å®åº”ç”¨ä¸­çš„æ™®åŠï¼Œä»£ç†åœ¨è¿½æ±‚ä¸ªä½“ä¸é›†ä½“ç›®æ ‡æ—¶çš„å¤æ‚ç¤¾ä¼šç»„ç»‡å½¢å¼ï¼Œå¯èƒ½æ— æ„ä¸­å¯¼è‡´éƒ¨åˆ†æˆ–å…¨éƒ¨ä»£ç†ä¸äººç±»ä»·å€¼è§‚æˆ–ç”¨æˆ·åå¥½å‘ç”Ÿå¤±é…ã€‚è®ºæ–‡å€Ÿé‰´ç¤¾ä¼šç§‘å­¦ç†è®ºï¼Œæ·±å…¥åˆ†æäº†ç¤¾ä¼šç»“æ„å¦‚ä½•ç ´åç¾¤ä½“å’Œä¸ªäººä»·å€¼è§‚ã€‚åŸºäºæ­¤ï¼Œä½œè€…å‘¼å AI ç¤¾åŒºå°†äººç±»å¯¹é½ã€åå¥½å¯¹é½å’Œç›®æ ‡å¯¹é½è§†ä¸ºä¸€ä¸ªç›¸äº’ä¾å­˜çš„æ•´ä½“ï¼Œè€Œéå­¤ç«‹çš„é—®é¢˜è¿›è¡Œå¤„ç†ã€‚æœ€åï¼Œè®ºæ–‡å¼ºè°ƒæ€¥éœ€å¼€å‘ä¸“é—¨çš„ä»¿çœŸç¯å¢ƒã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¾¿åœ¨å¤æ‚çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“èƒŒæ™¯ä¸‹å¯¹å¯¹é½æ€§è¿›è¡Œæœ‰æ•ˆè¯„ä¼°ï¼Œé˜²æ­¢è¿™ç§åŠ¨æ€è¿‡ç¨‹æ¼”å˜è‡³æ— æ³•æ§åˆ¶çš„ç¨‹åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint of NeurIPS 2025 Position Paper",
      "pdf_url": "https://arxiv.org/pdf/2506.01080v2",
      "published_date": "2025-06-01 16:39:43 UTC",
      "updated_date": "2025-06-06 02:55:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:17.507258+00:00"
    },
    {
      "arxiv_id": "2506.01079v1",
      "title": "Unfolding Boxes with Local Constraints",
      "title_zh": "åŸºäºå±€éƒ¨çº¦æŸçš„ç®±å­å±•å¼€",
      "authors": [
        "Long Qian",
        "Eric Wang",
        "Bernardo Subercaseaux",
        "Marijn J. H. Heule"
      ],
      "abstract": "We consider the problem of finding and enumerating polyominos that can be folded into multiple non-isomorphic boxes. While several computational approaches have been proposed, including SAT, randomized algorithms, and decision diagrams, none has been able to perform at scale. We argue that existing SAT encodings are hindered by the presence of global constraints (e.g., graph connectivity or acyclicity), which are generally hard to encode effectively and hard for solvers to reason about. In this work, we propose a new SAT-based approach that replaces these global constraints with simple local constraints that have substantially better propagation properties. Our approach dramatically improves the scalability of both computing and enumerating common box unfoldings: (i) while previous approaches could only find common unfoldings of two boxes up to area 88, ours easily scales beyond 150, and (ii) while previous approaches were only able to enumerate common unfoldings up to area 30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the smallest areas allowing a common unfolding of three boxes, thereby refuting a conjecture of Xu et al. (2017).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯»æ‰¾å’Œæšä¸¾å¯æŠ˜å æˆå¤šä¸ªéåŒæ„ç›’å­çš„å¤šè¿é€šå½¢ï¼ˆpolyominosï¼‰è¿™ä¸€è®¡ç®—éš¾é¢˜ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„SATç¼–ç æ–¹æ³•ç”±äºä¾èµ–å›¾è¿é€šæ€§ç­‰éš¾ä»¥é«˜æ•ˆå¤„ç†çš„å…¨å±€çº¦æŸï¼ˆglobal constraintsï¼‰ï¼Œå¯¼è‡´ç®—æ³•åœ¨å¤„ç†å¤§è§„æ¨¡é—®é¢˜æ—¶é¢ä¸´æ‰©å±•æ€§ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„åŸºäºSATçš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å…·æœ‰æ›´ä¼˜ä¼ æ’­ç‰¹æ€§çš„å±€éƒ¨çº¦æŸï¼ˆlocal constraintsï¼‰æ¥æ›¿ä»£å…¨å±€çº¦æŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ‰©å±•æ€§ä¸Šå–å¾—äº†æ˜¾è‘—çªç ´ï¼Œå°†å¯»æ‰¾ä¸¤ä¸ªç›’å­å…±åŒå±•å¼€å›¾çš„é¢ç§¯ä¸Šé™ä»88æå‡è‡³150ä»¥ä¸Šï¼Œå¹¶å°†æšä¸¾å…±åŒå±•å¼€å›¾çš„é¢ç§¯ä¸Šé™ä»30æå‡è‡³60ã€‚åŸºäºè¿™ä¸€è®¡ç®—èƒ½åŠ›çš„æå‡ï¼Œç ”ç©¶äººå‘˜æˆåŠŸæ’é™¤äº†é¢ç§¯ä¸º46ã€54å’Œ58ä½œä¸ºä¸‰ä¸ªç›’å­å…±åŒå±•å¼€å›¾æœ€å°é¢ç§¯çš„å¯èƒ½æ€§ï¼Œä»è€Œåé©³äº†Xu et al. (2017) æå‡ºçš„å­¦æœ¯çŒœæƒ³ã€‚",
      "categories": [
        "cs.CG",
        "cs.AI"
      ],
      "primary_category": "cs.CG",
      "comment": "Accepted at CADE30 (2025). 17 figures. Code at https://github.com/LongQianQL/CADE30-BoxUnfoldings",
      "pdf_url": "https://arxiv.org/pdf/2506.01079v1",
      "published_date": "2025-06-01 16:30:07 UTC",
      "updated_date": "2025-06-01 16:30:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:08.698680+00:00"
    },
    {
      "arxiv_id": "2506.01078v1",
      "title": "GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking",
      "title_zh": "GThinkerï¼šåŸºäºçº¿ç´¢å¼•å¯¼å†æ€è€ƒçš„é€šç”¨å¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Yufei Zhan",
        "Ziheng Wu",
        "Yousong Zhu",
        "Rongkun Xue",
        "Ruipu Luo",
        "Zhenghao Chen",
        "Can Zhang",
        "Yifan Li",
        "Zhentao He",
        "Zheming Yang",
        "Ming Tang",
        "Minghui Qiu",
        "Jinqiao Wang"
      ],
      "abstract": "Despite notable advancements in multimodal reasoning, leading Multimodal Large Language Models (MLLMs) still underperform on vision-centric multimodal reasoning tasks in general scenarios. This shortfall stems from their predominant reliance on logic- and knowledge-based slow thinking strategies, while effective for domains like math and science, fail to integrate visual information effectively during reasoning. Consequently, these models often fail to adequately ground visual cues, resulting in suboptimal performance in tasks that require multiple plausible visual interpretations and inferences. To address this, we present GThinker (General Thinker), a novel reasoning MLLM excelling in multimodal reasoning across general scenarios, mathematics, and science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that grounds inferences in visual cues and iteratively reinterprets these cues to resolve inconsistencies. Building on this pattern, we further propose a two-stage training pipeline, including pattern-guided cold start and incentive reinforcement learning, designed to enable multimodal reasoning capabilities across domains. Furthermore, to support the training, we construct GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths and 4K curated reinforcement learning samples, filling the data gap toward general multimodal reasoning. Extensive experiments demonstrate that GThinker achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark M$^3$CoT, surpassing the latest O4-mini model. It also shows an average improvement of 2.1% on general scenario multimodal reasoning benchmarks, while maintaining on-par performance in mathematical reasoning compared to counterpart advanced reasoning models. The code, model, and data will be released soon at https://github.com/jefferyZhan/GThinker.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GThinker (General Thinker)ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è§†è§‰ä¸­å¿ƒæ¨ç†ä»»åŠ¡ä¸­å› è¿‡åº¦ä¾èµ–é€»è¾‘å’ŒçŸ¥è¯†è€Œå¿½ç•¥è§†è§‰çº¿ç´¢ç»“åˆçš„é—®é¢˜ã€‚GThinkeræ ¸å¿ƒå¼•å…¥äº†Cue-Rethinkingæ¨ç†æ¨¡å¼ï¼Œé€šè¿‡å°†æ¨ç†è¿‡ç¨‹é”šå®šäºè§†è§‰çº¿ç´¢å¹¶è¿›è¡Œè¿­ä»£é‡æ–°è§£è¯»ï¼Œæœ‰æ•ˆè§£å†³äº†è§†è§‰æ¨æ–­ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ºäº†è®­ç»ƒè¯¥æ¨¡å‹ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†æ¶µç›–æ¨¡å¼å¼•å¯¼å†·å¯åŠ¨ä¸æ¿€åŠ±å¼ºåŒ–å­¦ä¹ (Incentive Reinforcement Learning)çš„ä¸¤é˜¶æ®µæµæ°´çº¿ï¼Œå¹¶æ„å»ºäº†åŒ…å«1.1ä¸‡æ¡é«˜è´¨é‡æ¨ç†è·¯å¾„ä¸æ ·æœ¬çš„GThinker-11Kæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒGThinkeråœ¨æŒ‘æˆ˜æ€§çš„M$^3$CoTç»¼åˆæ¨ç†åŸºå‡†ä¸Šå–å¾—äº†81.5%çš„æˆç»©ï¼Œè¶…è¶Šäº†O4-miniæ¨¡å‹ï¼Œå¹¶åœ¨é€šç”¨åœºæ™¯å¤šæ¨¡æ€æ¨ç†ä¸­å¹³å‡æå‡äº†2.1%çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆ›æ–°çš„æ¨ç†æ¨¡å¼å’Œè®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨é€šç”¨ã€æ•°å­¦åŠç§‘å­¦é¢†åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Tech report",
      "pdf_url": "https://arxiv.org/pdf/2506.01078v1",
      "published_date": "2025-06-01 16:28:26 UTC",
      "updated_date": "2025-06-01 16:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:01.967335+00:00"
    },
    {
      "arxiv_id": "2506.01069v1",
      "title": "Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety",
      "title_zh": "å˜é©è¡€åº“ï¼šäººå·¥æ™ºèƒ½é©±åŠ¨çš„æŒ‡çº¹ä¸è¡€å‹å…³è”ç ”ç©¶ä»¥æå‡å®‰å…¨æ€§",
      "authors": [
        "Malik A. Altayar",
        "Muhyeeddin Alqaraleh",
        "Mowafaq Salem Alzboon",
        "Wesam T. Almagharbeh"
      ],
      "abstract": "Identification of a person is central in forensic science, security, and healthcare. Methods such as iris scanning and genomic profiling are more accurate but expensive, time-consuming, and more difficult to implement. This study focuses on the relationship between the fingerprint patterns and the ABO blood group as a biometric identification tool. A total of 200 subjects were included in the study, and fingerprint types (loops, whorls, and arches) and blood groups were compared. Associations were evaluated with statistical tests, including chi-square and Pearson correlation. The study found that the loops were the most common fingerprint pattern and the O+ blood group was the most prevalent. Even though there was some associative pattern, there was no statistically significant difference in the fingerprint patterns of different blood groups. Overall, the results indicate that blood group data do not significantly improve personal identification when used in conjunction with fingerprinting. Although the study shows weak correlation, it may emphasize the efforts of multi-modal based biometric systems in enhancing the current biometric systems. Future studies may focus on larger and more diverse samples, and possibly machine learning and additional biometrics to improve identification methods. This study addresses an element of the ever-changing nature of the fields of forensic science and biometric identification, highlighting the importance of resilient analytical methods for personal identification.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒ‡çº¹æ¨¡å¼ï¼ˆloops, whorls, archesï¼‰ä¸ ABO è¡€å‹ç³»ç»Ÿä¹‹é—´çš„å…³è”ï¼Œæ—¨åœ¨è¯„ä¼°å…¶ä½œä¸ºç”Ÿç‰©è¯†åˆ«å·¥å…·åœ¨å–è¯ç§‘å­¦ã€å®‰å…¨å’ŒåŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚ç ”ç©¶å…±çº³å…¥200åå—è¯•è€…ï¼Œé€šè¿‡å¡æ–¹æ£€éªŒ (chi-square) å’Œçš®å°”é€Šç›¸å…³ç³»æ•° (Pearson correlation) å¯¹æŒ‡çº¹ç±»å‹ä¸è¡€å‹ä¹‹é—´çš„å…³ç³»è¿›è¡Œäº†ç»Ÿè®¡åˆ†æã€‚å®éªŒç»“æœå‘ç°ï¼Œloops æ˜¯æœ€å¸¸è§çš„æŒ‡çº¹æ¨¡å¼ï¼Œè€Œ O+ æ˜¯æœ€ä¸ºæ™®éçš„è¡€å‹ã€‚å°½ç®¡è§‚å¯Ÿåˆ°äº†æŸäº›å…³è”è¶‹åŠ¿ï¼Œä½†ç»Ÿè®¡åˆ†æè¡¨æ˜ä¸åŒè¡€å‹ä¹‹é—´çš„æŒ‡çº¹æ¨¡å¼å¹¶æ— æ˜¾è‘—å·®å¼‚ã€‚è¿™è¯´æ˜åœ¨æŒ‡çº¹è¯†åˆ«ä¸­åŠ å…¥è¡€å‹æ•°æ®å¹¶ä¸èƒ½æ˜¾è‘—æå‡ä¸ªäººèº«ä»½è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œåæ˜ å‡ºä¸¤è€…ä¹‹é—´çš„ç›¸å…³æ€§è¾ƒå¼±ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºæœªæ¥åº”åˆ©ç”¨æ›´å¤§è§„æ¨¡çš„æ ·æœ¬å’Œæœºå™¨å­¦ä¹  (machine learning) æŠ€æœ¯ï¼Œé€šè¿‡å¼€å‘å¤šæ¨¡æ€ (multi-modal) ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿæ¥å¢å¼ºç°æœ‰çš„èº«ä»½éªŒè¯æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01069v1",
      "published_date": "2025-06-01 16:18:24 UTC",
      "updated_date": "2025-06-01 16:18:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:24.091840+00:00"
    },
    {
      "arxiv_id": "2506.01065v1",
      "title": "Trilevel Memetic Algorithm for the Electric Vehicle Routing Problem",
      "title_zh": "ç”µåŠ¨æ±½è½¦è·¯å¾„ä¼˜åŒ–é—®é¢˜çš„ä¸‰å±‚æ¨¡å› ç®—æ³•",
      "authors": [
        "Ivan MilinoviÄ‡",
        "Leon Stjepan UroiÄ‡",
        "Marko ÄuraseviÄ‡"
      ],
      "abstract": "The Electric Vehicle Routing Problem (EVRP) extends the capacitated vehicle routing problem by incorporating battery constraints and charging stations, posing significant optimization challenges. This paper introduces a Trilevel Memetic Algorithm (TMA) that hierarchically optimizes customer sequences, route assignments, and charging station insertions. The method combines genetic algorithms with dynamic programming, ensuring efficient and high-quality solutions. Benchmark tests on WCCI2020 instances show competitive performance, matching best-known results for small-scale cases. While computational demands limit scalability, TMA demonstrates strong potential for sustainable logistics planning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ…å«ç”µæ± çº¦æŸå’Œå……ç”µç«™çš„ç”µåŠ¨æ±½è½¦è·¯å¾„é—®é¢˜ (Electric Vehicle Routing Problem, EVRP) æå‡ºäº†ä¸‰å±‚æ¨¡å› ç®—æ³• (Trilevel Memetic Algorithm, TMA)ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€å¤æ‚çš„ç»„åˆä¼˜åŒ–éš¾é¢˜ã€‚è¯¥ç®—æ³•é‡‡ç”¨å±‚æ¬¡åŒ–æ¶æ„ï¼Œå¯¹å®¢æˆ·åºåˆ—ã€è·¯å¾„åˆ†é…å’Œå……ç”µç«™æ’å…¥è¿›è¡ŒååŒä¼˜åŒ–ï¼Œå¹¶ç»“åˆäº†é—ä¼ ç®—æ³• (Genetic Algorithms) ä¸åŠ¨æ€è§„åˆ’ (Dynamic Programming) æŠ€æœ¯ä»¥ç¡®ä¿æ±‚è§£çš„é«˜æ•ˆæ€§ä¸é«˜è´¨é‡ã€‚åœ¨ WCCI2020 å®ä¾‹ä¸Šçš„åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒTMA å…·æœ‰æå¼ºçš„ç«äº‰ä¼˜åŠ¿ï¼Œåœ¨å°è§„æ¨¡æ¡ˆä¾‹ä¸­èƒ½å¤ŸåŒ¹é…ç›®å‰å·²çŸ¥çš„æœ€ä½³è®¡ç®—ç»“æœã€‚å°½ç®¡è¾ƒé«˜çš„è®¡ç®—éœ€æ±‚åœ¨ä¸€å®šç¨‹åº¦ä¸Šé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„æ‰©å±•æ€§ï¼Œä½†è¯¥æ–¹æ³•ä¸ºå¯æŒç»­ç‰©æµè§„åˆ’æä¾›äº†å…·å¤‡é‡è¦å‚è€ƒä»·å€¼çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01065v1",
      "published_date": "2025-06-01 16:08:43 UTC",
      "updated_date": "2025-06-01 16:08:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:07.995311+00:00"
    },
    {
      "arxiv_id": "2506.01064v3",
      "title": "Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs",
      "title_zh": "Fighting Fire with Fire (F3)ï¼šä¸€ç§é’ˆå¯¹ LVLMs çš„æ— éœ€è®­ç»ƒä¸”é«˜æ•ˆçš„è§†è§‰å¯¹æŠ—æ ·æœ¬å‡€åŒ–æ–¹æ³•",
      "authors": [
        "Yudong Zhang",
        "Ruobing Xie",
        "Yiqing Huang",
        "Jiansheng Chen",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Di Wang",
        "Yu Wang"
      ],
      "abstract": "Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive ``fighting fire with fire'' strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code is available at https://github.com/btzyd/F3.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (Large Vision-Language Models, LVLMs) å®¹æ˜“å—åˆ°è§†è§‰å¯¹æŠ—æ”»å‡» (Visual Adversarial Attacks) ä»è€Œå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º F3 çš„æ–°å‹å¯¹æŠ—å‡€åŒ–æ¡†æ¶ã€‚F3 é‡‡ç”¨äº†ä¸€ç§ç›´è§‰ä¸Šâ€œä»¥ç«æ”»ç«â€çš„ç­–ç•¥ï¼Œé€šè¿‡æœ‰æ„å‘å¯¹æŠ—æ ·æœ¬å¼•å…¥ç®€å•çš„éšæœºæ‰°åŠ¨æ¥å‡è½»å…¶æœ‰å®³å½±å“ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨éšæœºæ‰°åŠ¨æ ·æœ¬äº§ç”Ÿçš„è·¨æ¨¡æ€æ³¨æ„åŠ› (Cross-modal Attentions) ä½œä¸ºå‚è€ƒç›®æ ‡ï¼Œé€šè¿‡æ³¨å…¥å™ªå£°æ¥ä¼˜åŒ–å¯¹æŠ—æ ·æœ¬çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œè·å¾—æ›´çº¯å‡€ã€æ›´å¯é çš„æ¨¡å‹è¾“å‡ºã€‚è¿™ç§çœ‹ä¼¼çŸ›ç›¾çš„åˆ©ç”¨å™ªå£°æŠµæ¶ˆå¯¹æŠ—æ”»å‡»çš„æ–¹æ³•åœ¨å®éªŒä¸­å–å¾—äº†æ˜¾è‘—çš„å‡€åŒ–æ•ˆæœã€‚æ­¤å¤–ï¼ŒF3 å…·æœ‰æ— éœ€è®­ç»ƒ (Training-free)ã€æ˜“äºå®ç°ä»¥åŠè®¡ç®—æ•ˆç‡æé«˜ç­‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚è¿™äº›å±æ€§ä½¿å¾— F3 ç‰¹åˆ«é€‚ç”¨äºå¯¹é²æ£’æ€§èƒ½å’Œè¿è¡Œæ•ˆç‡å‡æœ‰ä¸¥æ ¼è¦æ±‚çš„å¤§è§„æ¨¡å·¥ä¸šåº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM Multimedia 2025 BNI track (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2506.01064v3",
      "published_date": "2025-06-01 16:07:30 UTC",
      "updated_date": "2025-09-14 09:51:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:13:23.082212+00:00"
    },
    {
      "arxiv_id": "2506.01062v2",
      "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models",
      "title_zh": "SealQAï¼šæå‡æœç´¢å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›åŸºå‡†",
      "authors": [
        "Thinh Pham",
        "Nguyen Nguyen",
        "Pratibha Zunjare",
        "Weiyuan Chen",
        "Yu-Min Tseng",
        "Tu Vu"
      ],
      "abstract": "We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the \"lost-in-the-middle\" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† SealQAï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼° Search-Augmented Language Models (SALMs) åœ¨ Web æœç´¢ç»“æœåŒ…å«å†²çªã€å™ªå£°æˆ–æ— ç”¨ä¿¡æ¯æ—¶çš„è¡¨ç°ã€‚SealQA åŒ…å« Seal-0ã€Seal-Hard å’Œ LongSeal ä¸‰ä¸ªå­ç‰ˆæœ¬ï¼Œåˆ†åˆ«ä¾§é‡äºäº‹å®å‡†ç¡®æ€§ã€æ¨ç†èƒ½åŠ›ä»¥åŠé•¿æ–‡æœ¬å¤šæ–‡æ¡£ä¸­çš„â€œå¤§æµ·æé’ˆâ€(needle-in-a-haystack) æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯æœ€å‰æ²¿çš„ LLMs åœ¨ SealQA ä¸Šçš„è¡¨ç°ä¹Ÿå·®å¼ºäººæ„ï¼ŒSeal-0 ä¸Šé…å¤‡å·¥å…·çš„ä»£ç†æ¨¡å‹æœ€é«˜å‡†ç¡®ç‡ä»…ä¸º 17.1%ã€‚ç ”ç©¶å‘ç°ï¼ŒDeepSeek-R1-671B å’Œ o3-mini ç­‰é«˜çº§æ¨ç†æ¨¡å‹å¯¹å™ªå£°æœç´¢ç»“æœéå¸¸æ•æ„Ÿï¼Œä¸”å¢åŠ æµ‹è¯•æ—¶è®¡ç®—é‡ (test-time compute) å¹¶ä¸èƒ½ç¨³å®šæå‡æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè™½ç„¶ç°ä»£æ¨¡å‹åœ¨ LongSeal ä¸­å—â€œlost-in-the-middleâ€é—®é¢˜å½±å“è¾ƒå°ï¼Œä½†åœ¨é¢å¯¹å¤§é‡å¹²æ‰°é¡¹æ—¶ä»éš¾ä»¥å‡†ç¡®è¯†åˆ«ç›¸å…³æ–‡æ¡£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. 23 pages, 7 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01062v2",
      "published_date": "2025-06-01 16:04:34 UTC",
      "updated_date": "2025-06-11 22:00:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:22.487189+00:00"
    },
    {
      "arxiv_id": "2506.01059v1",
      "title": "XAI-Units: Benchmarking Explainability Methods with Unit Tests",
      "title_zh": "XAI-Unitsï¼šåŸºäºå•å…ƒæµ‹è¯•çš„å¯è§£é‡Šæ€§æ–¹æ³•åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jun Rui Lee",
        "Sadegh Emami",
        "Michael David Hollins",
        "Timothy C. H. Wong",
        "Carlos Ignacio Villalobos SÃ¡nchez",
        "Francesca Toni",
        "Dekai Zhang",
        "Adam Dejl"
      ],
      "abstract": "Feature attribution (FA) methods are widely used in explainable AI (XAI) to help users understand how the inputs of a machine learning model contribute to its outputs. However, different FA models often provide disagreeing importance scores for the same model. In the absence of ground truth or in-depth knowledge about the inner workings of the model, it is often difficult to meaningfully determine which of the different FA methods produce more suitable explanations in different contexts. As a step towards addressing this issue, we introduce the open-source XAI-Units benchmark, specifically designed to evaluate FA methods against diverse types of model behaviours, such as feature interactions, cancellations, and discontinuous outputs. Our benchmark provides a set of paired datasets and models with known internal mechanisms, establishing clear expectations for desirable attribution scores. Accompanied by a suite of built-in evaluation metrics, XAI-Units streamlines systematic experimentation and reveals how FA methods perform against distinct, atomic kinds of model reasoning, similar to unit tests in software engineering. Crucially, by using procedurally generated models tied to synthetic datasets, we pave the way towards an objective and reliable comparison of FA methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯è§£é‡Šäººå·¥æ™ºèƒ½ (XAI) ä¸­ç‰¹å¾å½’å›  (Feature Attribution, FA) æ–¹æ³•å­˜åœ¨çš„è§£é‡Šä¸ä¸€è‡´ä¸”ç¼ºä¹è¯„ä¼°åŸºå‡†çš„é—®é¢˜ï¼Œæå‡ºäº†å¼€æºçš„ XAI-Units åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¨¡ä»¿è½¯ä»¶å·¥ç¨‹ä¸­çš„å•å…ƒæµ‹è¯• (Unit Tests)ï¼Œé€šè¿‡ç¨‹åºç”Ÿæˆçš„åˆæˆæ•°æ®é›†å’Œå…·æœ‰å·²çŸ¥å†…éƒ¨æœºåˆ¶çš„æ¨¡å‹ï¼Œä¸ºè¯„ä¼° FA æ–¹æ³•åœ¨å¤„ç†ç‰¹å¾äº¤äº’ (Feature Interactions)ã€ç›¸äº’æŠµæ¶ˆ (Cancellations) å’Œä¸è¿ç»­è¾“å‡ºç­‰ç‰¹å®šæ¨¡å‹è¡Œä¸ºæ—¶æä¾›äº†æ˜ç¡®çš„é¢„æœŸæŒ‡æ ‡ã€‚XAI-Units åŒ…å«ä¸€å¥—å†…ç½®çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°æ­ç¤ºä¸åŒ FA æ–¹æ³•åœ¨åº”å¯¹å„ç§åŸå­åŒ–æ¨ç†ç±»å‹æ—¶çš„è¡¨ç°å·®å¼‚ã€‚è¯¥ç ”ç©¶é€šè¿‡å»ºç«‹å®¢è§‚ä¸”å¯é çš„æ¯”è¾ƒæ ‡å‡†ï¼Œä¸ºåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­é€‰æ‹©æ›´åˆé€‚çš„ FA è§£é‡Šæ–¹æ³•æä¾›äº†é‡è¦å·¥å…·ï¼Œæœ‰æ•ˆæ¨åŠ¨äº† XAI é¢†åŸŸå‘æ›´å…·å®¢è§‚æ€§å’Œå¯ä¿¡åº¦çš„æ–¹å‘å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at FAccT 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01059v1",
      "published_date": "2025-06-01 15:58:27 UTC",
      "updated_date": "2025-06-01 15:58:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:25.993181+00:00"
    },
    {
      "arxiv_id": "2506.14797v1",
      "title": "Bound by semanticity: universal laws governing the generalization-identification tradeoff",
      "title_zh": "è¯­ä¹‰æ€§çš„çº¦æŸï¼šæ”¯é…æ³›åŒ–ä¸è¯†åˆ«æƒè¡¡çš„æ™®é€‚å®šå¾‹",
      "authors": [
        "Marco Nurisso",
        "Jesseba Fernando",
        "Raj Deshpande",
        "Alan Perotti",
        "Raja Marjieh",
        "Steven M. Frankland",
        "Richard L. Lewis",
        "Taylor W. Webb",
        "Declan Campbell",
        "Francesco Vaccarino",
        "Jonathan D. Cohen",
        "Giovanni Petri"
      ],
      "abstract": "Intelligent systems must deploy internal representations that are simultaneously structured -- to support broad generalization -- and selective -- to preserve input identity. We expose a fundamental limit on this tradeoff. For any model whose representational similarity between inputs decays with finite semantic resolution $\\varepsilon$, we derive closed-form expressions that pin its probability of correct generalization $p_S$ and identification $p_I$ to a universal Pareto front independent of input space geometry. Extending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs predicts a sharp $1/n$ collapse of multi-input processing capacity and a non-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end reproduces these laws: during learning a resolution boundary self-organizes and empirical $(p_S,p_I)$ trajectories closely follow theoretical curves for linearly decaying similarity. Finally, we demonstrate that the same limits persist in two markedly more complex settings -- a convolutional neural network and state-of-the-art vision-language models -- confirming that finite-resolution similarity is a fundamental emergent informational constraint, not merely a toy-model artifact. Together, these results provide an exact theory of the generalization-identification trade-off and clarify how semantic resolution shapes the representational capacity of deep networks and brains alike.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†æ™ºèƒ½ç³»ç»Ÿåœ¨æ”¯æŒå¹¿æ³›æ¦‚æ‹¬ï¼ˆGeneralizationï¼‰ä¸ä¿ç•™è¾“å…¥èº«ä»½ï¼ˆIdentificationï¼‰ä¹‹é—´å­˜åœ¨çš„æ ¹æœ¬æƒè¡¡é™åˆ¶ã€‚ä½œè€…é’ˆå¯¹è¡¨ç¤ºç›¸ä¼¼æ€§éšæœ‰é™è¯­ä¹‰åˆ†è¾¨ç‡ï¼ˆSemantic Resolutionï¼‰$\\varepsilon$ è¡°å‡çš„æ¨¡å‹ï¼Œæ¨å¯¼å‡ºå°†æ­£ç¡®æ¦‚æ‹¬æ¦‚ç‡ $p_S$ ä¸è¯†åˆ«æ¦‚ç‡ $p_I$ è”ç³»èµ·æ¥çš„å°é—­å½¢å¼è¡¨è¾¾å¼ï¼Œå¹¶å‘ç°å…¶éµå¾ªç‹¬ç«‹äºè¾“å…¥ç©ºé—´å‡ ä½•å½¢çŠ¶çš„é€šç”¨å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆPareto Frontï¼‰ã€‚ç ”ç©¶è¿›ä¸€æ­¥é¢„æµ‹äº†å¤šè¾“å…¥å¤„ç†èƒ½åŠ›éšè¾“å…¥æ•°é‡ $n$ å‘ˆ $1/n$ çš„é”å‡ï¼Œä»¥åŠ $p_S$ å­˜åœ¨çš„éå•è°ƒæœ€ä¼˜å€¼ã€‚é€šè¿‡åœ¨æœ€å° ReLU ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»¥åŠæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Modelsï¼‰ä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯æ˜äº†è¿™ç§æœ‰é™åˆ†è¾¨ç‡ç›¸ä¼¼æ€§æ˜¯æ·±åº¦ç½‘ç»œä¸­ä¸€ç§æ™®éå­˜åœ¨çš„ä¿¡æ¯çº¦æŸè€Œéæ¨¡å‹ä¼ªå½±ã€‚è¯¥æˆæœä¸ºæ¦‚æ‹¬ä¸è¯†åˆ«ä¹‹é—´çš„æƒè¡¡æä¾›äº†ç²¾ç¡®çš„ç†è®ºæ¡†æ¶ï¼Œé˜æ˜äº†è¯­ä¹‰åˆ†è¾¨ç‡å¦‚ä½•ä»æ ¹æœ¬ä¸Šå¡‘é€ æ·±åº¦ç½‘ç»œåŠç”Ÿç‰©å¤§è„‘çš„è¡¨ç¤ºå®¹é‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14797v1",
      "published_date": "2025-06-01 15:56:26 UTC",
      "updated_date": "2025-06-01 15:56:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:44.484262+00:00"
    },
    {
      "arxiv_id": "2506.01056v4",
      "title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents",
      "title_zh": "MCP-Zeroï¼šé¢å‘è‡ªä¸»å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ä¸»åŠ¨å·¥å…·å‘ç°",
      "authors": [
        "Xiang Fei",
        "Xiawu Zheng",
        "Hao Feng"
      ],
      "abstract": "True intelligence requires active capability acquisition, yet current LLM agents inject pre-defined tool schemas into prompts, reducing models to passive selectors and falling short of robust general-purpose agency. We introduce MCP-Zero, an active agent framework that restores tool discovery autonomy to LLMs themselves. Instead of overwhelming models with all available tools, MCP-Zero enables agents to actively identify capability gaps, and request specific tools on-demand, transforming them from large-scale retrievers into genuine autonomous agents. The framework operates through three core mechanisms: (1) Active Tool Request, where models autonomously generate structured requests specifying their exact tool requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that matches requests to relevant servers and tools through improved semantic alignment; (3) Iterative Capability Extension, enabling agents to progressively build cross-domain toolchains while maintaining minimal context footprint. We construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797 tools from the official Model-Context-Protocol repository. Experiments demonstrate that MCP-Zero preserves agent autonomy while achieving substantial efficiency gains: (i) accurate tool selection from nearly 3k candidates across 248.1k tokens; (ii) 98\\% reduction in token consumption on APIBank while maintaining high accuracy; and (iii) consistent multi-turn performance that scales with tool ecosystem growth. This work establishes active tool discovery as a fundamental design pattern for scalable autonomous agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MCP-Zeroï¼Œä¸€ç§æ—¨åœ¨å®ç°è‡ªä¸»å·¥å…·å‘ç°(Active Tool Discovery)çš„ä¸»åŠ¨æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢„å®šä¹‰å·¥å…·æ¨¡å¼ä¸‹ä»…èƒ½ä½œä¸ºè¢«åŠ¨é€‰æ‹©è€…ä¸”ç¼ºä¹è‡ªä¸»èƒ½åŠ›çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸»åŠ¨å·¥å…·è¯·æ±‚(Active Tool Request)ã€å±‚çº§è¯­ä¹‰è·¯ç”±(Hierarchical Semantic Routing)å’Œè¿­ä»£èƒ½åŠ›æ‰©å±•(Iterative Capability Extension)ä¸‰å¤§æ ¸å¿ƒæœºåˆ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªä¸»è¯†åˆ«èƒ½åŠ›å·®è·å¹¶æŒ‰éœ€è·å–ç‰¹å®šå·¥å…·ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºModel-Context-Protocolæ„å»ºäº†åŒ…å«308ä¸ªæœåŠ¡å™¨å’Œ2,797ä¸ªå·¥å…·çš„MCP-toolsæ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼ŒMCP-Zeroåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°†APIBankä»»åŠ¡ä¸­çš„Tokenæ¶ˆè€—é™ä½äº†98%ï¼Œå¹¶èƒ½åœ¨è¿‘ä¸‰åƒä¸ªå€™é€‰å·¥å…·ä¸­å®ç°ç²¾å‡†é€‰æ‹©ä¸å¤šè½®ä»»åŠ¡æ‰§è¡Œã€‚è¯¥å·¥ä½œç¡®ç«‹äº†ä¸»åŠ¨å·¥å…·å‘ç°ä½œä¸ºæ„å»ºå¯æ‰©å±•è‡ªä¸»æ™ºèƒ½ä½“(Autonomous Agents)ç³»ç»Ÿçš„åŸºæœ¬è®¾è®¡æ¨¡å¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01056v4",
      "published_date": "2025-06-01 15:48:53 UTC",
      "updated_date": "2025-06-24 06:27:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:12:37.793383+00:00"
    },
    {
      "arxiv_id": "2506.01049v1",
      "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping",
      "title_zh": "é€šè¿‡æ¢¯åº¦åˆ†ç»„ç¼©æ”¾å­¦ä¹ ç‡ä»¥é©¯æœå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Siyuan Li",
        "Juanxi Tian",
        "Zedong Wang",
        "Xin Jin",
        "Zicheng Liu",
        "Wentao Zhang",
        "Dan Xu"
      ],
      "abstract": "Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒä¸­å­˜åœ¨çš„å‚æ•°çº§å­¦ä¹ ç‡ä¼°ç®—ä¸å‡†ã€è®­ç»ƒä¸ç¨³å®šåŠæ”¶æ•›ç¼“æ…¢ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º Scaling with Gradient Grouping (SGG) çš„ä¼˜åŒ–å™¨åŒ…è£…å™¨ã€‚SGG é€šè¿‡åŠ¨æ€åˆ†ç»„ï¼ˆdynamic groupingï¼‰å’Œç‰¹å®šç»„ç¼©æ”¾ï¼ˆgroup-specific scalingï¼‰æ¥ä¼˜åŒ–è‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼°ç®—ï¼Œå°†æ¯ä¸€å±‚ä¸­çš„æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯åˆ’åˆ†ä¸ºå¤šä¸ªç°‡ï¼ˆclustersï¼‰ï¼Œå¹¶åº”ç”¨ç°‡ç‰¹å®šç¼©æ”¾ï¼ˆcluster-specific scalingï¼‰æ¥ç²¾ç¡®æ ¡å‡†æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚è¿™ç§æ–¹æ³•åœ¨ç»´æŒç²¾ç¡®å‚æ•°è‡ªé€‚åº”çš„åŒæ—¶æ–½åŠ äº†é›†ä½“æ€§çš„ç»„çº§çº¦æŸï¼Œæœ‰æ•ˆæå‡äº†è®­ç»ƒçš„é²æ£’æ€§ä»¥åŠä¸å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯çš„å…¼å®¹æ€§ã€‚åœ¨å¤šç§ (M)LLM åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¯æ˜ï¼ŒSGG èƒ½å¤Ÿä¸ç°æœ‰ä¼˜åŒ–å™¨æ— ç¼é›†æˆï¼Œåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šå‡å®ç°äº†æ¯”åŸºçº¿æ¨¡å‹æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ä¼˜çš„æ€§èƒ½ã€‚SGG åœ¨ä¸åŒæ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡é…ç½®ä¸‹è¡¨ç°å‡ºçš„é«˜åº¦ç¨³å®šæ€§ï¼Œä½¿å…¶æˆä¸ºå¤§æ¨¡å‹ä¼˜åŒ–ä¸­ä¸€ç§é«˜æ•ˆä¸”å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint version of \"Taming LLMs with Gradient Grouping\" (ACL'2025). The code will be available at https://github.com/ScalingOpt/SGG",
      "pdf_url": "https://arxiv.org/pdf/2506.01049v1",
      "published_date": "2025-06-01 15:30:37 UTC",
      "updated_date": "2025-06-01 15:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:13:57.045780+00:00"
    },
    {
      "arxiv_id": "2506.03197v3",
      "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing",
      "title_zh": "Infinity Parserï¼šåŸºäºå¸ƒå±€æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ çš„æ‰«ææ–‡æ¡£è§£æ",
      "authors": [
        "Baode Wang",
        "Biao Wu",
        "Weizhen Li",
        "Meng Fang",
        "Zuming Huang",
        "Jun Huang",
        "Haozhe Wang",
        "Yanjie Liang",
        "Ling Chen",
        "Wei Chu",
        "Yuan Qi"
      ],
      "abstract": "Automated parsing of scanned documents into richly structured, machine-readable formats remains a critical bottleneck in Document AI, as traditional multi-stage pipelines suffer from error propagation and limited adaptability to diverse layouts. We introduce layoutRL, an end-to-end reinforcement learning framework that trains models to be explicitly layout-aware by optimizing a composite reward of normalized edit distance, paragraph count accuracy, and reading order preservation. Leveraging our newly released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic scanned document parsing data with expert-filtered real-world documents, we instantiate layoutRL in a vision-language-model-based parser called Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and formula extraction, and reading order detection, Infinity-Parser achieves new state-of-the-art performance in both accuracy and structural fidelity, outpacing specialist pipelines and general-purpose vision-language models. We will publicly release our code and dataset to accelerate progress in robust document understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå¤šé˜¶æ®µæµæ°´çº¿åœ¨æ‰«ææ–‡æ¡£è§£æä¸­å­˜åœ¨çš„è¯¯å·®ç´¯ç§¯å’Œå¸ƒå±€é€‚åº”æ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº† layoutRLï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„ Reinforcement Learning æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–å½’ä¸€åŒ–ç¼–è¾‘è·ç¦»ã€æ®µè½æ•°é‡å‡†ç¡®ç‡å’Œé˜…è¯»é¡ºåºä¿ç•™çš„å¤åˆå¥–åŠ±ï¼Œä½¿æ¨¡å‹å…·å¤‡æ˜¾å¼çš„ Layout Aware èƒ½åŠ›ã€‚åŸºäºæ–°å‘å¸ƒçš„åŒ…å«5.5ä¸‡ä¸ªé«˜ä¿çœŸåˆæˆåŠä¸“å®¶ç­›é€‰çœŸå®æ–‡æ¡£çš„ Infinity-Doc-55K æ•°æ®é›†ï¼Œç ”ç©¶è€…æ„å»ºäº†åä¸º Infinity-Parser çš„ Vision-Language Model è§£æå™¨ã€‚åœ¨ OCRã€è¡¨æ ¼ã€å…¬å¼æå–åŠé˜…è¯»é¡ºåºæ£€æµ‹ç­‰ä¸­è‹±æ–‡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒInfinity-Parser åœ¨å‡†ç¡®ç‡å’Œç»“æ„ä¿çœŸåº¦ä¸Šå‡å–å¾—äº† SOTA æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰ä¸“é—¨æµæ°´çº¿å’Œé€šç”¨æ¨¡å‹ã€‚è¯¥æˆæœé€šè¿‡æ•´åˆå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ä¸å¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ‰«ææ–‡æ¡£è§£æçš„ç¨³å¥æ€§ä¸ç»“æ„å®Œæ•´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.03197v3",
      "published_date": "2025-06-01 15:19:52 UTC",
      "updated_date": "2025-10-21 02:15:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:00.271069+00:00"
    },
    {
      "arxiv_id": "2506.01048v2",
      "title": "IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory",
      "title_zh": "IRT-Routerï¼šåŸºäºé¡¹ç›®ååº”ç†è®ºçš„é«˜æ•ˆä¸”å¯è§£é‡Šçš„å¤š LLM è·¯ç”±",
      "authors": [
        "Wei Song",
        "Zhenya Huang",
        "Cheng Cheng",
        "Weibo Gao",
        "Bihan Xu",
        "GuanHao Zhao",
        "Fei Wang",
        "Runze Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance across a wide range of natural language tasks. However, selecting the optimal LLM to respond to a user query often necessitates a delicate balance between performance and cost. While powerful models deliver better results, they come at a high cost, whereas smaller models are more cost-effective but less capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing framework that efficiently routes user queries to the most suitable LLM. Inspired by Item Response Theory (IRT), a psychological measurement methodology, IRT-Router explicitly models the relationship between LLM capabilities and user query attributes. This not only enables accurate prediction of response performance but also provides interpretable insights, such as LLM abilities and query difficulty. Additionally, we design an online query warm-up technique based on semantic similarity, further enhancing the online generalization capability of IRT-Router. Extensive experiments on 20 LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline methods in terms of effectiveness and interpretability. Its superior performance in cold-start scenarios further confirms the reliability and practicality of IRT-Router in real-world applications. Code is available at https://github.com/Mercidaiha/IRT-Router.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IRT-Routerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé¡¹ç›®ååº”ç†è®º(Item Response Theory)çš„å¤šå¤§è¯­è¨€æ¨¡å‹(Multi-LLM)è·¯ç”±æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜æ€§èƒ½æ¨¡å‹é«˜æˆæœ¬ä¸ä½æˆæœ¬æ¨¡å‹ä½èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜ç¡®å»ºæ¨¡æ¨¡å‹èƒ½åŠ›ä¸æŸ¥è¯¢å±æ€§ä¹‹é—´çš„å…³ç³»ï¼Œä¸ä»…å®ç°äº†å¯¹å“åº”æ€§èƒ½çš„å‡†ç¡®é¢„æµ‹ï¼Œè¿˜ä¸ºæ¨¡å‹èƒ½åŠ›å’ŒæŸ¥è¯¢éš¾åº¦æä¾›äº†å…·æœ‰å¯è§£é‡Šæ€§çš„è§è§£ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åœ¨çº¿æŸ¥è¯¢é¢„çƒ­(Online query warm-up)æŠ€æœ¯ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„åœ¨çº¿æ³›åŒ–èƒ½åŠ›ã€‚åœ¨åŒ…å«20ä¸ªæ¨¡å‹å’Œ12ä¸ªæ•°æ®é›†çš„å¹¿æ³›å®éªŒä¸­ï¼ŒIRT-Routeråœ¨æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ä¸Šå‡ä¼˜äºå¤šæ•°åŸºçº¿æ–¹æ³•ã€‚å…¶åœ¨å†·å¯åŠ¨åœºæ™¯ä¸‹çš„ä¼˜å¼‚è¡¨ç°ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ¡ˆåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯é æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ACL 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2506.01048v2",
      "published_date": "2025-06-01 15:14:58 UTC",
      "updated_date": "2025-06-21 03:39:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:13:50.293993+00:00"
    },
    {
      "arxiv_id": "2506.01042v2",
      "title": "Probing Neural Topology of Large Language Models",
      "title_zh": "æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹çš„ç¥ç»æ‹“æ‰‘",
      "authors": [
        "Yu Zheng",
        "Yuan Yuan",
        "Yue Zhuo",
        "Yong Li",
        "Paolo Santi"
      ],
      "abstract": "Probing large language models (LLMs) has yielded valuable insights into their internal mechanisms by linking neural activations to interpretable semantics. However, the complex mechanisms that link neuron's functional co-activation with the emergent model capabilities remains largely unknown, hindering a deeper understanding and safer development of LLMs. In this work, we introduce graph probing, a method for uncovering the functional connectivity of LLM neurons and relating it to language generation performance. By probing models across diverse LLM families and scales, we discover a universal predictability of next-token prediction performance using only neural topology, which persists even when retaining just 1% of neuron connections. Strikingly, probing on topology outperforms probing on activation by up to 130.4%, suggesting that neural topology contains orders of richer information of LLM performance than neural activation, which can be easily extracted with simple linear or MLP probes. To explain the dependence between neural topology and language performance, we identify default networks and hub neurons in LLMs and provide causal evidence by interventional experiments on multiple benchmarks, showing that LLMs actually exploit these topological information. Further analyses suggest that neural topology can be effectively leveraged to improve the efficiency, reliability, and safety of LLMs through proof-of-concept applications in model pruning, hallucination detection, and LLM fingerprinting. Codes and data for the graph probing toolbox are available at https://github.com/DavyMorgan/llm-graph-probing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† graph probing è¿™ä¸€æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç¥ç»å…ƒçš„åŠŸèƒ½è¿æ¥æ€§ (functional connectivity) åŠå…¶ä¸æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚ç ”ç©¶å‘ç°ä»…é€šè¿‡åˆ†æ neural topology å³å¯å®ç°å¯¹ next-token prediction æ€§èƒ½çš„æ™®éé¢„æµ‹ï¼Œä¸”åœ¨ä»…ä¿ç•™ 1% çš„ç¥ç»è¿æ¥æ—¶è¿™ç§é¢„æµ‹æ€§ä¾ç„¶å­˜åœ¨ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒåŸºäº topology çš„æ¢æµ‹åœ¨åæ˜ æ¨¡å‹æ€§èƒ½æ–¹é¢æ¯”ä¼ ç»Ÿçš„ activation æ¢æµ‹é«˜å‡ºå¤šè¾¾ 130.4%ï¼Œè¯æ˜æ‹“æ‰‘ç»“æ„ä¸­è•´å«ç€è¿œæ¯”ç¥ç»æ¿€æ´»æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚è®ºæ–‡è¿›ä¸€æ­¥è¯†åˆ«äº† LLMs ä¸­çš„ default networks å’Œ hub neuronsï¼Œå¹¶é€šè¿‡å¹²é¢„å®éªŒæä¾›äº†å› æœè¯æ®ï¼Œè¯å®æ¨¡å‹ç¡®å®åœ¨åˆ©ç”¨è¿™äº›æ‹“æ‰‘ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¯æ˜äº† neural topology åœ¨ model pruningã€hallucination detection ä»¥åŠ LLM fingerprinting ç­‰å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€å®‰å…¨å’Œå¯é çš„ LLMs æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01042v2",
      "published_date": "2025-06-01 14:57:03 UTC",
      "updated_date": "2025-09-25 20:23:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:08.103260+00:00"
    },
    {
      "arxiv_id": "2506.01034v2",
      "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„å±€éƒ¨æœ¬å¾ç»´åº¦",
      "authors": [
        "Benjamin Matthias Ruppik",
        "Julius von Rohrscheidt",
        "Carel van Niekerk",
        "Michael Heck",
        "Renato Vukovic",
        "Shutong Feng",
        "Hsien-chin Lin",
        "Nurul Lubis",
        "Bastian Rieck",
        "Marcus Zibrowius",
        "Milica GaÅ¡iÄ‡"
      ],
      "abstract": "Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. Even fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. In this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. To that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning. We show that the local dimensions provide insights into the model's training dynamics and generalization ability. Specifically, the mean of the local dimensions predicts when the model's training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task. Furthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains. Through this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. The results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡æ½œåœ¨åµŒå…¥(contextual latent embeddings)å‡ ä½•å±æ€§çš„æ–°è§†è§’ï¼Œé€šè¿‡æµ‹é‡è¯­è¨€æ¨¡å‹æ½œåœ¨ç©ºé—´çš„å±€éƒ¨å†…åœ¨ç»´åº¦(Local Intrinsic Dimensions, LID)æ¥åˆ†ææ¨¡å‹åœ¨è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå±€éƒ¨ç»´åº¦çš„å˜åŒ–èƒ½å¤Ÿæ·±å…¥æ­ç¤ºæ¨¡å‹çš„è®­ç»ƒåŠ¨æ€å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå…¶å‡å€¼å¯ä»¥æœ‰æ•ˆé¢„æµ‹æ¨¡å‹åœ¨å¯¹è¯çŠ¶æ€è·Ÿè¸ªä»»åŠ¡ä¸­è®­ç»ƒèƒ½åŠ›çš„è€—å°½ã€åœ¨æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­çš„è¿‡æ‹Ÿåˆ(overfitting)ä»¥åŠåœ¨ç®—æœ¯ä»»åŠ¡ä¸­çš„é¡¿æ‚Ÿ(grokking)è¿‡ç¨‹ã€‚å®éªŒè¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„å¯å‘å¼æ–¹æ³•ï¼Œå³å±€éƒ¨ç»´åº¦å‡å€¼çš„ä¸‹é™é€šå¸¸ä¼´éšå¹¶é¢„ç¤ºç€æ¨¡å‹éšåçš„æ€§èƒ½æå‡ã€‚è¯¥å·¥ä½œé€šè¿‡æ¡¥æ¥å†…åœ¨æ¨¡å‹æœºåˆ¶ä¸åµŒå…¥ç©ºé—´çš„å‡ ä½•å±æ€§ï¼Œä¸ºç†è§£å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¯è§£é‡Šæ€§ã€é€‚åº”æ€§å’Œæ³›åŒ–æ€§æä¾›äº†é‡è¦å·¥å…·ï¼Œæœ‰åŠ©äºå¼€å‘è€…åœ¨ç‰¹å®šåº”ç”¨ä¸­ä¼˜åŒ–æ¨¡å‹é…ç½®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025; in press). 10 pages, with an additional 17 pages in the appendix. Our code is available at https://github.com/aidos-lab/Topo_LLM_public and https://github.com/aidos-lab/grokking-via-lid",
      "pdf_url": "https://arxiv.org/pdf/2506.01034v2",
      "published_date": "2025-06-01 14:30:46 UTC",
      "updated_date": "2025-10-27 16:17:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:13:59.846896+00:00"
    },
    {
      "arxiv_id": "2506.01023v1",
      "title": "A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement",
      "title_zh": "ä¸€ç§é¢å‘å®æ—¶è¯­éŸ³å¢å¼ºçš„ä¸¤é˜¶æ®µåˆ†å±‚æ·±åº¦æ»¤æ³¢æ¡†æ¶",
      "authors": [
        "Shenghui Lu",
        "Hukai Huang",
        "Jinanglong Yao",
        "Kaidi Wang",
        "Qingyang Hong",
        "Lin Li"
      ],
      "abstract": "This paper proposes a model that integrates sub-band processing and deep filtering to fully exploit information from the target time-frequency (TF) bin and its surrounding TF bins for single-channel speech enhancement. The sub-band module captures surrounding frequency bin information at the input, while the deep filtering module applies filtering at the output to both the target TF bin and its surrounding TF bins. To further improve the model performance, we decouple deep filtering into temporal and frequency components and introduce a two-stage framework, reducing the complexity of filter coefficient prediction at each stage. Additionally, we propose the TAConv module to strengthen convolutional feature extraction. Experimental results demonstrate that the proposed hierarchical deep filtering network (HDF-Net) effectively utilizes surrounding TF bin information and outperforms other advanced systems while using fewer resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HDF-Netï¼Œä¸€ç§ç»“åˆäº†å­å¸¦å¤„ç†(sub-band processing)å’Œæ·±åº¦æ»¤æ³¢(deep filtering)çš„ä¸¤é˜¶æ®µåˆ†å±‚æ·±åº¦æ»¤æ³¢æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å•é€šé“è¯­éŸ³å¢å¼ºã€‚è¯¥æ¨¡å‹é€šè¿‡å­å¸¦æ¨¡å—æ•æ‰è¾“å…¥ç«¯çš„é‚»è¿‘é¢‘ç‡å•å…ƒä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æ·±åº¦æ»¤æ³¢æ¨¡å—å¯¹ç›®æ ‡æ—¶é¢‘å•å…ƒ(TF bin)åŠå…¶å‘¨å›´å•å…ƒè¿›è¡Œæ»¤æ³¢å¤„ç†ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œç ”ç©¶è€…å°†æ·±åº¦æ»¤æ³¢è§£æ„ä¸ºæ—¶é—´å’Œé¢‘ç‡ä¸¤ä¸ªç»´åº¦ï¼Œå¹¶å¼•å…¥äº†ä¸¤é˜¶æ®µæ¡†æ¶ä»¥é™ä½æ¯é˜¶æ®µæ»¤æ³¢å™¨ç³»æ•°é¢„æµ‹çš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†TAConvæ¨¡å—æ¥åŠ å¼ºå·ç§¯ç‰¹å¾æå–ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒHDF-Netèƒ½æœ‰æ•ˆåˆ©ç”¨é‚»è¿‘æ—¶é¢‘ä¿¡æ¯ï¼Œåœ¨èµ„æºå ç”¨æ›´å°‘çš„æƒ…å†µä¸‹ï¼Œå…¶è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›ç³»ç»Ÿã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 2 figure, accepted by Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01023v1",
      "published_date": "2025-06-01 14:09:27 UTC",
      "updated_date": "2025-06-01 14:09:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:07.064735+00:00"
    },
    {
      "arxiv_id": "2506.02057v1",
      "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody",
      "title_zh": "åˆ©ç”¨è¯­éŸ³éŸµå¾‹å¢å¼ºæœºå™¨äººè¯­éŸ³æŒ‡ä»¤çš„ç†è§£ä¸æ¶ˆæ­§",
      "authors": [
        "David Sasu",
        "Kweku Andoh Yamoah",
        "Benedict Quartey",
        "Natalie Schluter"
      ],
      "abstract": "Enabling robots to accurately interpret and execute spoken language instructions is essential for effective human-robot collaboration. Traditional methods rely on speech recognition to transcribe speech into text, often discarding crucial prosodic cues needed for disambiguating intent. We propose a novel approach that directly leverages speech prosody to infer and resolve instruction intent. Predicted intents are integrated into large language models via in-context learning to disambiguate and select appropriate task plans. Additionally, we present the first ambiguous speech dataset for robotics, designed to advance research in speech disambiguation. Our method achieves 95.79% accuracy in detecting referent intents within an utterance and determines the intended task plan of ambiguous instructions with 71.96% accuracy, demonstrating its potential to significantly improve human-robot communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡è¯­éŸ³éŸµå¾‹(Speech Prosody)å¢å¼ºæœºå™¨äººå¯¹è¯­éŸ³æŒ‡ä»¤çš„ç†è§£å’Œæ­§ä¹‰æ¶ˆé™¤èƒ½åŠ›ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•åœ¨è¯­éŸ³è½¬æ–‡æœ¬è¿‡ç¨‹ä¸­ä¸¢å¤±å…³é”®éŸµå¾‹ä¿¡æ¯çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç›´æ¥åˆ©ç”¨éŸµå¾‹ç‰¹å¾æ¨æ–­æŒ‡ä»¤æ„å›¾çš„æ–°æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ (In-context Learning)å°†é¢„æµ‹ç»“æœæ•´åˆè‡³å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­ä»¥å®ç°ä»»åŠ¡è®¡åˆ’çš„ç²¾å‡†é€‰æ‹©ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¨å‡ºäº†é¦–ä¸ªé’ˆå¯¹æœºå™¨äººé¢†åŸŸè®¾è®¡çš„æ­§ä¹‰è¯­éŸ³æ•°æ®é›†ï¼Œå¡«è¡¥äº†ç›¸å…³ç ”ç©¶ç©ºç™½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹è¯è¯­å‚ç…§æ„å›¾æ–¹é¢è¾¾åˆ°äº†95.79%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤„ç†æ­§ä¹‰æŒ‡ä»¤çš„ä»»åŠ¡è§„åˆ’æ—¶å®ç°äº†71.96%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆæœæ˜¾è‘—æå‡äº†äººæœºåä½œä¸­çš„æ²Ÿé€šæ•ˆç‡ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½çš„äº¤äº’å¼æœºå™¨äººç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02057v1",
      "published_date": "2025-06-01 14:06:57 UTC",
      "updated_date": "2025-06-01 14:06:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:27.785221+00:00"
    },
    {
      "arxiv_id": "2506.04253v1",
      "title": "HADA: Human-AI Agent Decision Alignment Architecture",
      "title_zh": "HADAï¼šäººæœºæ™ºèƒ½ä½“å†³ç­–å¯¹é½æ¶æ„",
      "authors": [
        "Tapio PitkÃ¤ranta",
        "Leena PitkÃ¤ranta"
      ],
      "abstract": "We present HADA (Human-AI Agent Decision Alignment), a protocol- and framework agnostic reference architecture that keeps both large language model (LLM) agents and legacy algorithms aligned with organizational targets and values. HADA wraps any algorithm or LLM in role-specific stakeholder agents -- business, data-science, audit, ethics, and customer -- each exposing conversational APIs so that technical and non-technical actors can query, steer, audit, or contest every decision across strategic, tactical, and real-time horizons. Alignment objectives, KPIs, and value constraints are expressed in natural language and are continuously propagated, logged, and versioned while thousands of heterogeneous agents run on different orchestration stacks. A cloud-native proof of concept packages a production credit-scoring model (getLoanDecision) and deploys it on Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target changes, parameter tweaks, explanation requests, and ethics triggers flow end to end through the architecture. Evaluation followed the Design-Science Research Methodology. Walkthrough observation and log inspection demonstrated complete coverage of six predefined objectives: every role could invoke conversational control, trace KPIs and value constraints, detect and mitigate ZIP-code bias, and reproduce full decision lineage, independent of the underlying LLM or agent library. Contributions: (1) an open-source HADA architecture, (2) a mid-range design theory for human-AI alignment in multi-agent systems, and (3) empirical evidence that framework-agnostic, protocol-compliant stakeholder agents improve accuracy, transparency, and ethical compliance in real-world decision pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HADAï¼ˆHuman-AI Agent Decision Alignmentï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸åè®®å’Œæ¡†æ¶æ— å…³çš„å‚è€ƒæ¶æ„ï¼Œæ—¨åœ¨ç¡®ä¿Large Language Model (LLM)æ™ºèƒ½ä½“åŠä¼ ç»Ÿç®—æ³•ä¸ç»„ç»‡ç›®æ ‡å’Œä»·å€¼è§‚ä¿æŒä¸€è‡´ã€‚è¯¥æ¶æ„é€šè¿‡å°†ç®—æ³•æˆ–LLMå°è£…åœ¨åŒ…æ‹¬Businessã€Data-scienceã€Auditã€Ethicså’ŒCustomeråœ¨å†…çš„åˆ©ç›Šç›¸å…³è€…æ™ºèƒ½ä½“ä¸­ï¼Œå¹¶æä¾›Conversational APIsï¼Œä½¿æŠ€æœ¯ä¸éæŠ€æœ¯äººå‘˜èƒ½å¯¹æˆ˜ç•¥ã€æˆ˜æœ¯åŠå®æ—¶å±‚é¢çš„å†³ç­–è¿›è¡ŒæŸ¥è¯¢ã€å¼•å¯¼å’Œå®¡è®¡ã€‚å¯¹é½ç›®æ ‡ã€KPIså’Œä»·å€¼çº¦æŸé€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾å¹¶å®ç°æŒç»­ä¼ æ’­ä¸ç‰ˆæœ¬åŒ–ï¼Œæ”¯æŒåœ¨ä¸åŒç¼–æ’æ ˆä¸Šè¿è¡Œæ•°åƒä¸ªå¼‚æ„æ™ºèƒ½ä½“ã€‚é€šè¿‡åœ¨Dockerå’ŒKubernetesä¸Šéƒ¨ç½²ç”Ÿäº§çº§ä¿¡ç”¨è¯„åˆ†æ¨¡å‹çš„äº‘åŸç”ŸéªŒè¯ï¼Œå®éªŒè¯æ˜è¯¥æ¶æ„èƒ½æœ‰æ•ˆè¿½è¸ªKPIã€æ¢æµ‹å¹¶ç¼“è§£ZIP-codeåè§ï¼Œå¹¶å®ç°å®Œæ•´çš„å†³ç­–è°±ç³»æº¯æºã€‚è¯¥ç ”ç©¶è´¡çŒ®äº†å¼€æºçš„HADAæ¶æ„åŠå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„äººæœºå¯¹é½è®¾è®¡ç†è®ºï¼Œä¸ºåœ¨ç°å®å†³ç­–ç®¡çº¿ä¸­æå‡å‡†ç¡®æ€§ã€é€æ˜åº¦å’Œä¼¦ç†åˆè§„æ€§æä¾›äº†å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.04253v1",
      "published_date": "2025-06-01 14:04:52 UTC",
      "updated_date": "2025-06-01 14:04:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:25.688386+00:00"
    },
    {
      "arxiv_id": "2506.06345v1",
      "title": "Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100",
      "title_zh": "åŸºäºæ—¶é—´åºåˆ— Transformer çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½é©±åŠ¨è‚¡ä»·é¢„æµ‹ï¼šä»¥ BIST100 ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Sukru Selim Calik",
        "Andac Akyuz",
        "Zeynep Hilal Kilimci",
        "Kerem Colak"
      ],
      "abstract": "Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆ Transformer æ—¶é—´åºåˆ—æ¨¡å‹ä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable Artificial Intelligence, XAI) çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‚¡ç¥¨ä»·æ ¼é¢„æµ‹çš„å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ã€‚è¯¥ç ”ç©¶ä»¥ BIST100 æŒ‡æ•°ä¸­æˆäº¤é‡æœ€é«˜çš„äº”å®¶é“¶è¡Œä»¥åŠ XBANK å’Œ XU100 æŒ‡æ•°ä¸ºæ¡ˆä¾‹ï¼Œåˆ†æäº† 2015 å¹´ 1 æœˆè‡³ 2025 å¹´ 3 æœˆçš„æ¯æ—¥è‚¡ä»·æ•°æ®ã€‚åœ¨æ¨¡å‹æ„å»ºä¸­ï¼Œç ”ç©¶é‡‡ç”¨äº† DLinearã€LTSNetã€Vanilla Transformer å’Œ Time Series Transformer ç­‰æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨æŠ€æœ¯æŒ‡æ ‡ (technical indicators) ä¸°å¯Œäº†ç‰¹å¾ç»´åº¦ã€‚é€šè¿‡å¼•å…¥ SHAP å’Œ LIME æŠ€æœ¯ï¼Œç ”ç©¶æ­ç¤ºäº†å•ä¸ªç‰¹å¾å¯¹æ¨¡å‹é¢„æµ‹ç»“æœçš„å½±å“ï¼Œç¡®ä¿äº†é¢„æµ‹è¿‡ç¨‹çš„é€æ˜åº¦ã€‚å®éªŒç»“æœéªŒè¯äº† Transformer æ¨¡å‹åœ¨é‡‘èé¢†åŸŸå¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶å‡¸æ˜¾äº†å¯è§£é‡Šæœºå™¨å­¦ä¹ åœ¨å¸®åŠ©æŠ•èµ„è€…åšå‡ºç§‘å­¦å†³ç­–åŠæå‡é‡‘èç´ å…»æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06345v1",
      "published_date": "2025-06-01 13:29:25 UTC",
      "updated_date": "2025-06-01 13:29:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:28.268771+00:00"
    },
    {
      "arxiv_id": "2506.01004v2",
      "title": "MoCA-Video: Motion-Aware Concept Alignment for Consistent Video Editing",
      "title_zh": "MoCA-Videoï¼šé¢å‘ä¸€è‡´æ€§è§†é¢‘ç¼–è¾‘çš„è¿åŠ¨æ„ŸçŸ¥æ¦‚å¿µå¯¹é½",
      "authors": [
        "Tong Zhang",
        "Juan C Leon Alcazar",
        "Victor Escorcia",
        "Bernard Ghanem"
      ],
      "abstract": "We present MoCA-Video, a training-free framework for semantic mixing in videos. Operating in the latent space of a frozen video diffusion model, MoCA-Video utilizes class-agnostic segmentation with diagonal denoising scheduler to localize and track the target object across frames. To ensure temporal stability under semantic shifts, we introduce momentum-based correction to approximate novel hybrid distributions beyond trained data distribution, alongside a light gamma residual module that smooths out visual artifacts. We evaluate model's performance using SSIM, LPIPS, and a proposed metric, \\metricnameabbr, which quantifies semantic alignment between reference and output. Extensive evaluation demonstrates that our model consistently outperforms both training-free and trained baselines, achieving superior semantic mixing and temporal coherence without retraining. Results establish that structured manipulation of diffusion noise trajectories enables controllable and high-quality video editing under semantic shifts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MoCA-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒï¼ˆtraining-freeï¼‰çš„è§†é¢‘è¯­ä¹‰æ··åˆï¼ˆsemantic mixingï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶è¿è¡Œåœ¨å†»ç»“çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆvideo diffusion modelï¼‰çš„æ½œç©ºé—´ä¸­ï¼Œåˆ©ç”¨ç±»ä¸å¯çŸ¥åˆ†å‰²ï¼ˆclass-agnostic segmentationï¼‰å’Œå¯¹è§’å»å™ªè°ƒåº¦å™¨ï¼ˆdiagonal denoising schedulerï¼‰åœ¨è·¨å¸§ä¸­å®šä½å’Œè·Ÿè¸ªç›®æ ‡ç‰©ä½“ã€‚ä¸ºäº†åœ¨è¯­ä¹‰åç§»ä¸‹ä¿æŒæ—¶é—´ç¨³å®šæ€§ï¼Œç ”ç©¶è€…å¼•å…¥äº†åŸºäºåŠ¨é‡çš„ä¿®æ­£ï¼ˆmomentum-based correctionï¼‰ä»¥é€¼è¿‘æ··åˆåˆ†å¸ƒï¼Œå¹¶ç»“åˆè½»é‡çº§ä¼½é©¬æ®‹å·®æ¨¡å—ï¼ˆlight gamma residual moduleï¼‰å¹³æ»‘è§†è§‰ä¼ªå½±ã€‚å®éªŒé€šè¿‡ SSIMã€LPIPS åŠæ–°æå‡ºçš„ Concept-Aware Alignment æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º MoCA-Video åœ¨è¯­ä¹‰æ··åˆå’Œæ—¶é—´ç›¸å¹²æ€§ï¼ˆtemporal coherenceï¼‰æ–¹é¢å‡ä¼˜äºç°æœ‰çš„è®­ç»ƒå’Œéè®­ç»ƒåŸºçº¿æ¨¡å‹ã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡ç»“æ„åŒ–æ“ä½œæ‰©æ•£å™ªå£°è½¨è¿¹ï¼Œå¯ä»¥åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°å¯æ§ä¸”é«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01004v2",
      "published_date": "2025-06-01 13:28:04 UTC",
      "updated_date": "2025-12-11 19:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:27.934005+00:00"
    },
    {
      "arxiv_id": "2506.01003v2",
      "title": "Higher-Order Responsibility",
      "title_zh": "é«˜é˜¶è´£ä»»",
      "authors": [
        "Junli Jiang",
        "Pavel Naumov"
      ],
      "abstract": "In ethics, individual responsibility is often defined through Frankfurt's principle of alternative possibilities. This definition is not adequate in a group decision-making setting because it often results in the lack of a responsible party or \"responsibility gap''. One of the existing approaches to address this problem is to consider group responsibility. Another, recently proposed, approach is \"higher-order'' responsibility. The paper considers the problem of deciding if higher-order responsibility up to degree $d$ is enough to close the responsibility gap. The main technical result is that this problem is $Î _{2d+1}$-complete.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¼¦ç†å­¦ä¸­ä¸ªäººè´£ä»»çš„å®šä¹‰é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æ³•å…°å…‹ç¦æ›¿ä»£å¯èƒ½æ€§åŸåˆ™ (Frankfurt's principle of alternative possibilities) åœ¨ç¾¤ä½“å†³ç­–åœºæ™¯ä¸‹å¾€å¾€ä¼šå¯¼è‡´â€œè´£ä»»é¸¿æ²Ÿâ€ (responsibility gap)ï¼Œå³æ— æ³•æ˜ç¡®è´£ä»»å½’å±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé™¤äº†æ¢è®¨ç¾¤ä½“è´£ä»»å¤–ï¼Œè®ºæ–‡é‡ç‚¹ç ”ç©¶äº†è¿‘æœŸæå‡ºçš„â€œé«˜é˜¶è´£ä»»â€ (higher-order responsibility) æ¦‚å¿µã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºåˆ¤å®šæœ€é«˜è¾¾ $d$ é˜¶çš„é«˜é˜¶è´£ä»»æ˜¯å¦è¶³ä»¥å…³é—­è¿™ç§è´£ä»»é¸¿æ²Ÿã€‚è®ºæ–‡åœ¨æŠ€æœ¯å±‚é¢è¯æ˜äº†è¯¥åˆ¤å®šé—®é¢˜åœ¨è®¡ç®—å¤æ‚æ€§ä¸­å±äº $\\Pi_{2d+1}$-å®Œå¤‡ ($\\Pi_{2d+1}$-complete) ç±»ã€‚è¿™ä¸€å‘ç°ä¸ºè¯„ä¼°å¤æ‚å†³ç­–ç³»ç»Ÿä¸­çš„è´£ä»»åˆ†é…æä¾›äº†ä¸¥è°¨çš„æ•°å­¦æ¡†æ¶å’Œç†è®ºè¾¹ç•Œã€‚",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "40th AAAI Conference on Artificial Intelligence (AAAI-26)",
      "pdf_url": "https://arxiv.org/pdf/2506.01003v2",
      "published_date": "2025-06-01 13:22:05 UTC",
      "updated_date": "2025-11-12 08:45:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:56.797820+00:00"
    },
    {
      "arxiv_id": "2506.06344v4",
      "title": "A Fairness-Aware Strategy for B5G Physical-layer Security Leveraging Reconfigurable Intelligent Surfaces",
      "title_zh": "ä¸€ç§åˆ©ç”¨å¯é‡æ„æ™ºèƒ½è¡¨é¢çš„ B5G ç‰©ç†å±‚å®‰å…¨å…¬å¹³æ„ŸçŸ¥ç­–ç•¥",
      "authors": [
        "Alex Pierron",
        "Michel Barbeau",
        "Luca De Cicco",
        "Jose Rubio-Hernan",
        "Joaquin Garcia-Alfaro"
      ],
      "abstract": "Reconfigurable Intelligent Surfaces are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and lead to improvements in areas with low coverage properties. When combined with Reinforcement Learning techniques, they have the potential to enhance both system behavior and physical-layer security hardening. In addition to security improvements, it is crucial to consider the concept of fair communication. Reconfigurable Intelligent Surfaces must ensure that User Equipment units receive their signals with adequate strength, without other units being deprived of service due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining both an efficient and fair duplex Reconfigurable Intelligent Surface-Reinforcement Learning system for multiple legitimate User Equipment units without reducing the level of achieved physical-layer security hardening. In terms of contributions, we uncover a fairness imbalance of a previous physical-layer security hardening solution, validate our findings and report experimental work via simulation results. We also provide an alternative reward strategy to solve the uncovered problems and release both code and datasets to foster further research in the topics of this paper.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹B5Gç½‘ç»œä¸­çš„ç‰©ç†å±‚å®‰å…¨(Physical-layer Security)é—®é¢˜ï¼Œæ¢è®¨äº†åˆ©ç”¨å¯é‡æ„æ™ºèƒ½è¡¨é¢(Reconfigurable Intelligent Surfaces)ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç»“åˆæ¥å¢å¼ºç³»ç»Ÿæ€§èƒ½çš„æ½œåŠ›ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶è¯¥æŠ€æœ¯èƒ½æ˜¾è‘—æå‡å®‰å…¨ç¡¬åŒ–ï¼Œä½†åœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹å¾€å¾€å¿½ç•¥äº†é€šä¿¡å…¬å¹³æ€§(Fairness)ï¼Œå¯¼è‡´éƒ¨åˆ†ç”¨æˆ·è®¾å¤‡(User Equipment)å¯èƒ½å› ä¿¡å·å¼ºåº¦ä¸è¶³è€Œæ— æ³•è·å¾—æœåŠ¡ã€‚è®ºæ–‡æ­éœ²äº†ç°æœ‰ç‰©ç†å±‚å®‰å…¨ç¡¬åŒ–æ–¹æ¡ˆä¸­å­˜åœ¨çš„å…¬å¹³æ€§å¤±è¡¡é—®é¢˜ï¼Œå¹¶é€šè¿‡ä»¿çœŸå®éªŒå¯¹è¿™ä¸€å‘ç°è¿›è¡Œäº†éªŒè¯ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„å…¬å¹³æ„ŸçŸ¥ç­–ç•¥ï¼Œé€šè¿‡å¼•å…¥æ›¿ä»£å¥–åŠ±ç­–ç•¥(Reward Strategy)æ¥ä¼˜åŒ–åŒå‘RIS-RLç³»ç»Ÿçš„è¡Œä¸ºã€‚è¯¥æ–¹æ³•æ—¨åœ¨ç¡®ä¿å¤šä¸ªåˆæ³•ç”¨æˆ·è®¾å¤‡åœ¨ä¸é™ä½ç‰©ç†å±‚å®‰å…¨æ€§æ°´å¹³çš„å‰æä¸‹ï¼Œå®ç°é«˜æ•ˆä¸”å…¬å¹³çš„ä¿¡å·ä¼ è¾“ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†ç›¸å…³ä»£ç å’Œæ•°æ®é›†ï¼Œä¸ºè¿›ä¸€æ­¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å­¦æœ¯ç ”ç©¶æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "19 pages, 5 figures, 2 tables, 35 references",
      "pdf_url": "https://arxiv.org/pdf/2506.06344v4",
      "published_date": "2025-06-01 13:00:26 UTC",
      "updated_date": "2025-11-08 15:42:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:54.592311+00:00"
    },
    {
      "arxiv_id": "2506.00992v1",
      "title": "Quotient Network -- A Network Similar to ResNet but Learning Quotients",
      "title_zh": "Quotient Networkï¼šä¸€ç§å­¦ä¹ å•†å€¼çš„ç±» ResNet ç½‘ç»œ",
      "authors": [
        "Peng Hui",
        "Jiamuyang Zhao",
        "Changxin Li",
        "Qingzhen Zhu"
      ],
      "abstract": "The emergence of ResNet provides a powerful tool for training extremely deep networks. The core idea behind it is to change the learning goals of the network. It no longer learns new features from scratch but learns the difference between the target and existing features. However, the difference between the two kinds of features does not have an independent and clear meaning, and the amount of learning is based on the absolute rather than the relative difference, which is sensitive to the size of existing features. We propose a new network that perfectly solves these two problems while still having the advantages of ResNet. Specifically, it chooses to learn the quotient of the target features with the existing features, so we call it the quotient network. In order to enable this network to learn successfully and achieve higher performance, we propose some design rules for this network so that it can be trained efficiently and achieve better performance than ResNet. Experiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network can stably achieve considerable improvements over ResNet by simply making tiny corresponding changes to the original ResNet network without adding new parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºQuotient Networkçš„æ–°å‹ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ResNetåœ¨å­¦ä¹ ç‰¹å¾å·®å¼‚ï¼ˆResidualï¼‰æ—¶å¯¹ç‰¹å¾è§„æ¨¡æ•æ„Ÿä¸”ç¼ºä¹æ˜ç¡®ç‰©ç†æ„ä¹‰çš„é—®é¢˜ã€‚ä¸åŒäºResNeté€šè¿‡æ®‹å·®è¿æ¥å­¦ä¹ ç›®æ ‡ç‰¹å¾ä¸ç°æœ‰ç‰¹å¾çš„å·®å€¼ï¼ŒQuotient Networké€šè¿‡å­¦ä¹ ä¸¤è€…çš„å•†ï¼ˆQuotientï¼‰æ¥å®ç°ç‰¹å¾æ›´æ–°ï¼Œä»è€Œåœ¨ç»´æŒæ·±å±‚ç½‘ç»œè®­ç»ƒä¼˜åŠ¿çš„åŒæ—¶æé«˜äº†å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚ä¸ºäº†å®ç°è¯¥æ¶æ„çš„é«˜æ•ˆè®­ç»ƒå¹¶è·å¾—è¶…è¶ŠResNetçš„æ€§èƒ½ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç³»åˆ—ä¸“é—¨çš„è®¾è®¡å‡†åˆ™ã€‚åœ¨CIFAR10ã€CIFAR100å’ŒSVHNç­‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç½‘ç»œåœ¨ä¸å¢åŠ ä»»ä½•é¢å¤–å‚æ•°çš„å‰æä¸‹ï¼Œä»…éœ€å¯¹åŸå§‹ResNetè¿›è¡Œå¾®å°çš„ç»“æ„è°ƒæ•´å³å¯ç¨³å®šå–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This manuscript is the original version submitted to NeurIPS 2024, which was later revised and published as \"Quotient Network: A Network Similar to ResNet but Learning Quotients\" in Algorithms 2024, 17(11), 521 (https://doi.org/10.3390/a17110521). Please cite the journal version when referring to this work",
      "pdf_url": "https://arxiv.org/pdf/2506.00992v1",
      "published_date": "2025-06-01 12:46:43 UTC",
      "updated_date": "2025-06-01 12:46:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:57.298646+00:00"
    },
    {
      "arxiv_id": "2506.00989v1",
      "title": "Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery",
      "title_zh": "é€šè¿‡å¼‚è´¨æ€§æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ ä¸åŸå‹å¼•å¯¼èšç±»å‘ç°å¢å¼ºæœºå™¨äººæ£€æµ‹",
      "authors": [
        "Buyun He",
        "Xiaorui Jiang",
        "Qi Wu",
        "Hao Liu",
        "Yingguang Yang",
        "Yong Liao"
      ],
      "abstract": "Detecting social media bots is essential for maintaining the security and trustworthiness of social networks. While contemporary graph-based detection methods demonstrate promising results, their practical application is limited by label reliance and poor generalization capability across diverse communities. Generative Graph Self-Supervised Learning (GSL) presents a promising paradigm to overcome these limitations, yet existing approaches predominantly follow the homophily assumption and fail to capture the global patterns in the graph, which potentially diminishes their effectiveness when facing the challenges of interaction camouflage and distributed deployment in bot detection scenarios. To this end, we propose BotHP, a generative GSL framework tailored to boost graph-based bot detectors through heterophily-aware representation learning and prototype-guided cluster discovery. Specifically, BotHP leverages a dual-encoder architecture, consisting of a graph-aware encoder to capture node commonality and a graph-agnostic encoder to preserve node uniqueness. This enables the simultaneous modeling of both homophily and heterophily, effectively countering the interaction camouflage issue. Additionally, BotHP incorporates a prototype-guided cluster discovery pretext task to model the latent global consistency of bot clusters and identify spatially dispersed yet semantically aligned bot collectives. Extensive experiments on two real-world bot detection benchmarks demonstrate that BotHP consistently boosts graph-based bot detectors, improving detection performance, alleviating label reliance, and enhancing generalization capability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BotHPï¼Œä¸€ç§ç”Ÿæˆçš„å›¾è‡ªç›‘ç£å­¦ä¹ (Generative Graph Self-Supervised Learning, GSL)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼‚è´¨æ€§æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ (heterophily-aware representation learning)å’ŒåŸå‹å¼•å¯¼çš„èšç±»å‘ç°(prototype-guided cluster discovery)æ¥å¢å¼ºç¤¾äº¤æœºå™¨äººæ£€æµ‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼Œåˆ©ç”¨å›¾æ„ŸçŸ¥ç¼–ç å™¨æ•æ‰èŠ‚ç‚¹å…±æ€§ï¼ŒåŒæ—¶é€šè¿‡å›¾ä¸å¯çŸ¥ç¼–ç å™¨ä¿ç•™èŠ‚ç‚¹å”¯ä¸€æ€§ï¼Œä»è€ŒåŒæ—¶å»ºæ¨¡åŒè´¨æ€§(homophily)å’Œå¼‚è´¨æ€§(heterophily)ï¼Œæœ‰æ•ˆåº”å¯¹æœºå™¨äººçš„äº¤äº’ä¼ªè£…(interaction camouflage)é—®é¢˜ã€‚æ­¤å¤–ï¼ŒBotHPå¼•å…¥äº†åŸå‹å¼•å¯¼çš„èšç±»å‘ç°é¢„è®­ç»ƒä»»åŠ¡ï¼Œä»¥å»ºæ¨¡æœºå™¨äººé›†ç¾¤çš„æ½œåœ¨å…¨å±€ä¸€è‡´æ€§ï¼Œå¹¶è¯†åˆ«ç©ºé—´åˆ†æ•£ä½†è¯­ä¹‰ä¸€è‡´çš„æœºå™¨äººç¾¤ä½“ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBotHPèƒ½å¤ŸæŒç»­æå‡å›¾æ£€æµ‹å™¨çš„æ€§èƒ½ï¼Œåœ¨ç¼“è§£æ ‡ç­¾ä¾èµ–çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00989v1",
      "published_date": "2025-06-01 12:44:53 UTC",
      "updated_date": "2025-06-01 12:44:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:11.422237+00:00"
    },
    {
      "arxiv_id": "2506.00983v1",
      "title": "Bridging the Gap: From Ad-hoc to Proactive Search in Conversations",
      "title_zh": "å¼¥åˆå·®è·ï¼šä»å¯¹è¯ä¸­çš„å³æ—¶æœç´¢å‘ä¸»åŠ¨æœç´¢çš„è·¨è¶Š",
      "authors": [
        "Chuan Meng",
        "Francesco Tonolini",
        "Fengran Mo",
        "Nikolaos Aletras",
        "Emine Yilmaz",
        "Gabriella Kazai"
      ],
      "abstract": "Proactive search in conversations (PSC) aims to reduce user effort in formulating explicit queries by proactively retrieving useful relevant information given conversational context. Previous work in PSC either directly uses this context as input to off-the-shelf ad-hoc retrievers or further fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on short and concise queries, while the PSC input is longer and noisier. This input mismatch between ad-hoc search and PSC limits retrieval quality. While fine-tuning on PSC data helps, its benefits remain constrained by this input gap. In this work, we propose Conv2Query, a novel conversation-to-query framework that adapts ad-hoc retrievers to PSC by bridging the input gap between ad-hoc search and PSC. Conv2Query maps conversational context into ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc retrievers or for further fine-tuning on PSC data. Extensive experiments on two PSC datasets show that Conv2Query significantly improves ad-hoc retrievers' performance, both when used directly and after fine-tuning on PSC.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¯¹è¯ä¸­çš„ä¸»åŠ¨æœç´¢ (Proactive search in conversations, PSC) å±•å¼€ï¼Œæ—¨åœ¨é€šè¿‡å¯¹è¯ä¸Šä¸‹æ–‡ä¸»åŠ¨æ£€ç´¢ä¿¡æ¯ä»¥å‡å°‘ç”¨æˆ·çš„æ˜¾å¼æŸ¥è¯¢ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„ PSC æ–¹æ³•ç›´æ¥å°†å†—é•¿ä¸”å˜ˆæ‚çš„å¯¹è¯ä¸Šä¸‹æ–‡è¾“å…¥ç»™ä¸ºçŸ­æŸ¥è¯¢è®¾è®¡çš„å³å¸­æ£€ç´¢å™¨ (ad-hoc retrievers)ï¼Œå¯¼è‡´äº†ä¸¥é‡çš„è¾“å…¥ä¸åŒ¹é…é—®é¢˜å¹¶é™åˆ¶äº†æ£€ç´¢è´¨é‡ã€‚ä¸ºæ­¤æå‡ºçš„ Conv2Query æ¡†æ¶é€šè¿‡å°†å¯¹è¯ä¸Šä¸‹æ–‡æ˜ å°„ä¸ºå³å¸­æŸ¥è¯¢ (ad-hoc queries)ï¼ŒæˆåŠŸå¼¥åˆäº† ad-hoc search ä¸ PSC ä¹‹é—´çš„è¾“å…¥å·®è·ã€‚è¯¥æ¡†æ¶ç”Ÿæˆçš„æŸ¥è¯¢æ—¢å¯ä»¥ç›´æ¥ä½œä¸ºç°æˆæ£€ç´¢å™¨çš„è¾“å…¥ï¼Œä¹Ÿèƒ½ç”¨äºåœ¨ PSC æ•°æ®ä¸Šè¿›è¡Œçš„è¿›ä¸€æ­¥å¾®è°ƒ (fine-tuning)ã€‚åœ¨ä¸¤ä¸ª PSC æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒConv2Query æ˜¾è‘—æå‡äº†å³å¸­æ£€ç´¢å™¨åœ¨ç›´æ¥ä½¿ç”¨å’Œå¾®è°ƒåçš„æ€§èƒ½ï¼Œä¸ºè§£å†³å¯¹è¯æœç´¢ä¸­çš„è¾“å…¥å¤±é…æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted as a full paper at SIGIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00983v1",
      "published_date": "2025-06-01 12:30:58 UTC",
      "updated_date": "2025-06-01 12:30:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:14:58.293399+00:00"
    },
    {
      "arxiv_id": "2506.00981v2",
      "title": "What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training",
      "title_zh": "è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹å¯¹è·å…°è¯­äº†è§£å¤šå°‘ï¼Ÿåˆ†æç‰¹å®šè¯­è¨€é¢„è®­ç»ƒçš„ä¼˜åŠ¿",
      "authors": [
        "Marianne de Heer Kloots",
        "Hosein Mohebbi",
        "Charlotte Pouw",
        "Gaofei Shen",
        "Willem Zuidema",
        "Martijn Bentum"
      ],
      "abstract": "How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹ä¸­è¯­éŸ³è¡¨ç¤ºçš„è¯­è¨€ç‰¹å¼‚æ€§ï¼Œé‡ç‚¹åˆ†æäº†åœ¨ç‰¹å®šè¯­è¨€ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¯¹æå–è¯¥è¯­è¨€ç‰¹å¾çš„æ”¹è¿›ç¨‹åº¦ã€‚ç ”ç©¶è€…é€šè¿‡æµ‹è¯•Wav2Vec2æ¨¡å‹å¯¹è·å…°è¯­(Dutch)éŸ³ä½å’Œè¯æ±‡ä¿¡æ¯çš„ç¼–ç èƒ½åŠ›ï¼Œå¯¹æ¯”äº†ä»…åœ¨è·å…°è¯­ã€åŒç­‰è§„æ¨¡çš„è‹±è¯­ä»¥åŠå¤§è§„æ¨¡å¤šè¯­è¨€æ•°æ®ä¸Šé¢„è®­ç»ƒçš„æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“é—¨é’ˆå¯¹è·å…°è¯­çš„é¢„è®­ç»ƒç›¸æ¯”äºå…¶ä»–æ–¹æ¡ˆèƒ½æ˜¾è‘—æå‡å…¶åœ¨è·å…°è¯­è¯­è¨€ç‰¹å¾ä¸Šçš„è¡¨ç¤ºèƒ½åŠ›ã€‚è¿™ç§è¯­è¨€ç‰¹å¼‚æ€§ä¼˜åŠ¿å¯ä»¥é€šè¿‡è®­ç»ƒåçš„èšç±»æˆ–åˆ†ç±»æ¢é’ˆ(probes)è¢«æœ‰æ•ˆæ£€æµ‹ï¼Œå¹¶åœ¨é›¶æ ·æœ¬(zero-shot)æŒ‡æ ‡ä¸­å¾—åˆ°éƒ¨åˆ†ä½“ç°ã€‚æ­¤å¤–ï¼Œè¯­è¨€ç‰¹å¾ç¼–ç æ–¹é¢çš„è¯­è¨€ç‰¹å¼‚æ€§æ”¶ç›Šä¸ä¸‹æ¸¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(Automatic Speech Recognition)çš„æ€§èƒ½è¡¨ç°é«˜åº¦ä¸€è‡´ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Interspeech 2025. For model, code, and materials, see https://github.com/mdhk/SSL-NL-eval",
      "pdf_url": "https://arxiv.org/pdf/2506.00981v2",
      "published_date": "2025-06-01 12:25:13 UTC",
      "updated_date": "2025-07-10 12:20:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:14.799129+00:00"
    },
    {
      "arxiv_id": "2506.00979v4",
      "title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection",
      "title_zh": "IVY-FAKEï¼šå›¾åƒä¸è§†é¢‘ AIGC æ£€æµ‹çš„ç»Ÿä¸€å¯è§£é‡Šæ¡†æ¶ä¸åŸºå‡†",
      "authors": [
        "Changjiang Jiang",
        "Wenhui Dong",
        "Zhonghao Zhang",
        "Chenyang Si",
        "Fengchang Yu",
        "Wei Peng",
        "Xinbin Yuan",
        "Yifei Bi",
        "Ming Zhao",
        "Zian Zhou",
        "Caifeng Shan"
      ],
      "abstract": "The rapid development of Artificial Intelligence Generated Content (AIGC) techniques has enabled the creation of high-quality synthetic content, but it also raises significant security concerns. Current detection methods face two major limitations: (1) the lack of multidimensional explainable datasets for generated images and videos. Existing open-source datasets (e.g., WildFake, GenVideo) rely on oversimplified binary annotations, which restrict the explainability and trustworthiness of trained detectors. (2) Prior MLLM-based forgery detectors (e.g., FakeVLM) exhibit insufficiently fine-grained interpretability in their step-by-step reasoning, which hinders reliable localization and explanation. To address these challenges, we introduce Ivy-Fake, the first large-scale multimodal benchmark for explainable AIGC detection. It consists of over 106K richly annotated training samples (images and videos) and 5,000 manually verified evaluation examples, sourced from multiple generative models and real world datasets through a carefully designed pipeline to ensure both diversity and quality. Furthermore, we propose Ivy-xDetector, a reinforcement learning model based on Group Relative Policy Optimization (GRPO), capable of producing explainable reasoning chains and achieving robust performance across multiple synthetic content detection benchmarks. Extensive experiments demonstrate the superiority of our dataset and confirm the effectiveness of our approach. Notably, our method improves performance on GenImage from 86.88% to 96.32%, surpassing prior state-of-the-art methods by a clear margin.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹(AIGC)æ£€æµ‹ä¸­ç¼ºä¹å¤šç»´å¯è§£é‡Šæ•°æ®é›†ä»¥åŠæ¢æµ‹å™¨æ¨ç†ç»†ç²’åº¦ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†Ivy-Fakeè¿™ä¸€é¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€å¯è§£é‡ŠAIGCæ£€æµ‹åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡10.6ä¸‡ä¸ªæ¶µç›–å›¾åƒä¸è§†é¢‘çš„è®­ç»ƒæ ·æœ¬å’Œ5000ä¸ªç»è¿‡äººå·¥éªŒè¯çš„è¯„ä¼°ç¤ºä¾‹ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ•°æ®é›†æ ‡æ³¨è¿‡äºç®€å•çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹Ivy-xDetectorï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†é“¾å¹¶åœ¨å¤šä¸ªæ£€æµ‹åŸºå‡†ä¸Šå®ç°ç¨³å¥è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨GenImageæ•°æ®é›†ä¸Šçš„æ£€æµ‹å‡†ç¡®ç‡ä»86.88%æ˜¾è‘—æå‡è‡³96.32%ï¼Œå¤§å¹…è¶…è¶Šäº†å…ˆå‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚è¯¥æ¡†æ¶å’ŒåŸºå‡†çš„æå‡ºï¼Œä¸ºæå‡å¤šæ¨¡æ€ä¼ªé€ æ£€æµ‹çš„å¯ä¿¡åº¦ä¸å®šä½èƒ½åŠ›æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "30 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.00979v4",
      "published_date": "2025-06-01 12:20:22 UTC",
      "updated_date": "2025-11-27 07:25:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:17.661460+00:00"
    },
    {
      "arxiv_id": "2506.00975v4",
      "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction",
      "title_zh": "NTPPï¼šåŸºäºä¸‹ä¸€æ ‡è®°å¯¹é¢„æµ‹çš„åŒé€šé“å£è¯­å¯¹è¯ç”Ÿæˆå¼è¯­éŸ³è¯­è¨€å»ºæ¨¡",
      "authors": [
        "Qichao Wang",
        "Ziqiao Meng",
        "Wenqian Cui",
        "Yifei Zhang",
        "Pengcheng Wu",
        "Bingzhe Wu",
        "Irwin King",
        "Liang Chen",
        "Peilin Zhao"
      ],
      "abstract": "Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NTPPï¼Œä¸€ç§æ—¨åœ¨æå‡åŒé€šé“è¯­éŸ³å¯¹è¯è‡ªç„¶åº¦çš„ç”Ÿæˆå¼è¯­éŸ³è¯­è¨€å»ºæ¨¡(SLM)æ–°èŒƒå¼ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹æœªå……åˆ†åˆ©ç”¨åŒé€šé“è¯­éŸ³æ•°æ®ä¸­å¯¹è¯åŠ¨æ€çš„é—®é¢˜ï¼ŒNTPPé¦–æ¬¡åœ¨Decoder-onlyæ¶æ„ä¸Šå®ç°äº†ç‹¬ç«‹äºè¯´è¯äººçš„åŒé€šé“è¯­éŸ³å¯¹è¯å­¦ä¹ ã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºå¼•å…¥Next-Token-Pair Predictionæœºåˆ¶ï¼Œé€šè¿‡æ•æ‰å¯¹è¯åŒæ–¹çš„äº¤äº’ç»“æ„æ¥å¢å¼ºæ¨¡å‹å¯¹çœŸå®è°ˆè¯èŠ‚å¥çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNTPPåœ¨è½®æ¬¡åˆ‡æ¢é¢„æµ‹(Turn-taking prediction)ã€å›å¤è¿è´¯æ€§å’Œè‡ªç„¶åº¦ç­‰æ ‡å‡†åŸºå‡†ä¸Šå‡å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒNTPPå¤§å¹…é™ä½äº†æ¨ç†å»¶è¿Ÿï¼Œä¸ºå…¶åœ¨å®æ—¶è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­çš„é«˜æ•ˆåº”ç”¨å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00975v4",
      "published_date": "2025-06-01 12:01:40 UTC",
      "updated_date": "2025-06-11 10:45:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:17.352072+00:00"
    },
    {
      "arxiv_id": "2506.00969v1",
      "title": "Data Heterogeneity Modeling for Trustworthy Machine Learning",
      "title_zh": "é¢å‘å¯ä¿¡æœºå™¨å­¦ä¹ çš„æ•°æ®å¼‚è´¨æ€§å»ºæ¨¡",
      "authors": [
        "Jiashuo Liu",
        "Peng Cui"
      ],
      "abstract": "Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline -- from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†æ•°æ®å¼‚æ„æ€§ (Data heterogeneity) åœ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿè¡¨ç°ä¸­çš„å…³é”®ä½œç”¨ï¼ŒæŒ‡å‡ºä¼ ç»Ÿç®—æ³•å› å¿½è§†æ•°æ®å†…åœ¨å¤šæ ·æ€§è€Œå¯¼è‡´å†³ç­–ä¸å¯é ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³åŠä¸å…¬å¹³æ€§ç­‰é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†å¼‚æ„æ€§æ„ŸçŸ¥æœºå™¨å­¦ä¹  (Heterogeneity-aware machine learning) èŒƒå¼ï¼Œç³»ç»Ÿæ€§åœ°å°†æ•°æ®å¼‚æ„æ€§èå…¥ä»æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒåˆ°è¯„ä¼°ä¸éƒ¨ç½²çš„æ•´ä¸ªæœºå™¨å­¦ä¹ æµæ°´çº¿ (ML pipeline) ä¸­ã€‚é€šè¿‡æ·±å…¥åˆ†æå¼‚æ„æ€§å»ºæ¨¡ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨æå‡æ•°æ®é©±åŠ¨ç³»ç»Ÿçš„å¯é æ€§ï¼Œå…‹æœä¼ ç»Ÿç®—æ³•ä»…ä¼˜åŒ–å¹³å‡æ€§èƒ½çš„å±€é™ã€‚æ–‡ç« é€šè¿‡åœ¨åŒ»ç–—ã€å†œä¸šã€é‡‘èå’Œæ¨èç³»ç»Ÿç­‰å…³é”®é¢†åŸŸçš„åº”ç”¨ç¤ºä¾‹ï¼Œè¯æ˜äº†è¯¥èŒƒå¼åœ¨å¢å¼ºæ¨¡å‹é²æ£’æ€§ (Robustness)ã€å…¬å¹³æ€§ (Fairness) å’Œå¯é æ€§æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ·±åˆ»ç†è§£æ•°æ®å¤šæ ·æ€§æœ‰åŠ©äºä¼˜åŒ–æ¨¡å‹è¯Šæ–­ä¸æ”¹è¿›ï¼Œä¸ºæ„å»ºå¯ä¿¡èµ–çš„æœºå™¨å­¦ä¹ æä¾›æœ‰åŠ›æ”¯æ’‘ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨æ¨åŠ¨æ•°æ®æŒ–æ˜ç¤¾åŒºåœ¨å¼‚æ„æ€§å»ºæ¨¡é¢†åŸŸçš„æ·±å…¥æ¢ç´¢ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Survey paper for tutorial \"Data Heterogeneity Modeling for Trustworthy Machine Learning\" in KDD'25",
      "pdf_url": "https://arxiv.org/pdf/2506.00969v1",
      "published_date": "2025-06-01 11:36:56 UTC",
      "updated_date": "2025-06-01 11:36:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:33.438899+00:00"
    },
    {
      "arxiv_id": "2506.00968v1",
      "title": "PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation",
      "title_zh": "PolyBERTï¼šåŸºäºå¾®è°ƒå¤šç¼–ç å™¨ BERT çš„è¯ä¹‰æ¶ˆæ­§æ¨¡å‹",
      "authors": [
        "Linhan Xia",
        "Mingzhan Yang",
        "Guohui Yuan",
        "Shengnan Tao",
        "Yujing Qiu",
        "Guo Yu",
        "Kai Lei"
      ],
      "abstract": "Mainstream Word Sense Disambiguation (WSD) approaches have employed BERT to extract semantics from both context and definitions of senses to determine the most suitable sense of a target word, achieving notable performance. However, there are two limitations in these approaches. First, previous studies failed to balance the representation of token-level (local) and sequence-level (global) semantics during feature extraction, leading to insufficient semantic representation and a performance bottleneck. Second, these approaches incorporated all possible senses of each target word during the training phase, leading to unnecessary computational costs. To overcome these limitations, this paper introduces a poly-encoder BERT-based model with batch contrastive learning for WSD, named PolyBERT. Compared with previous WSD methods, PolyBERT has two improvements: (1) A poly-encoder with a multi-head attention mechanism is utilized to fuse token-level (local) and sequence-level (global) semantics, rather than focusing on just one. This approach enriches semantic representation by balancing local and global semantics. (2) To avoid redundant training inputs, Batch Contrastive Learning (BCL) is introduced. BCL utilizes the correct senses of other target words in the same batch as negative samples for the current target word, which reduces training inputs and computational cost. The experimental results demonstrate that PolyBERT outperforms baseline WSD methods such as Huang's GlossBERT and Blevins's BEM by 2\\% in F1-score. In addition, PolyBERT with BCL reduces GPU hours by 37.6\\% compared with PolyBERT without BCL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PolyBERTï¼Œä¸€ç§åŸºäº Poly Encoder BERT çš„å¾®è°ƒè¯ä¹‰æ¶ˆæ­§ (Word Sense Disambiguation) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­å±€éƒ¨ä¸å…¨å±€è¯­ä¹‰è¡¨ç¤ºä¸å¹³è¡¡ä»¥åŠè®­ç»ƒé˜¶æ®µè®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç»“åˆå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Attention) çš„ Poly-encoder æ¥èåˆè¯å…ƒçº§ (Token-level) å’Œåºåˆ—çº§ (Sequence-level) è¯­ä¹‰ï¼Œä»è€Œé€šè¿‡å¹³è¡¡å±€éƒ¨ä¸å…¨å±€ä¿¡æ¯æ¥ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºã€‚ä¸ºäº†ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼ŒPolyBERT å¼•å…¥äº†æ‰¹æ¬¡å¯¹æ¯”å­¦ä¹  (Batch Contrastive Learning)ï¼Œå°†åŒæ‰¹æ¬¡å†…å…¶ä»–ç›®æ ‡è¯çš„æ­£ç¡®ä¹‰é¡¹ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒè¾“å…¥çš„éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPolyBERT åœ¨ F1-score ä¸Šä¼˜äº GlossBERT å’Œ BEM ç­‰åŸºçº¿æ¨¡å‹ 2%ã€‚æ­¤å¤–ï¼Œä¸ä¸ä½¿ç”¨è¯¥æŠ€æœ¯çš„æ¨¡å‹ç›¸æ¯”ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ çš„ PolyBERT å‡å°‘äº† 37.6% çš„ GPU è¿ç®—æ—¶é—´ï¼Œè¯æ˜äº†å…¶åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šçš„åŒé‡ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00968v1",
      "published_date": "2025-06-01 11:35:49 UTC",
      "updated_date": "2025-06-01 11:35:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:35.234663+00:00"
    },
    {
      "arxiv_id": "2506.00965v2",
      "title": "FLEx: Personalized Federated Learning for Mixture-of-Experts LLMs via Expert Grafting",
      "title_zh": "FLExï¼šåŸºäºä¸“å®¶å«æ¥çš„æ··åˆä¸“å®¶å¤§è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ",
      "authors": [
        "Fan Liu",
        "Bikang Pan",
        "Zhongyi Wang",
        "Xi Yao",
        "Xiaoying Tang",
        "Jingya Wang",
        "Ye Shi"
      ],
      "abstract": "Federated instruction tuning of large language models (LLMs) is challenged by significant data heterogeneity across clients, demanding robust personalization. The Mixture of Experts (MoE) architecture, where experts can specialize in distinct data patterns, presents a natural architectural solution to this challenge. The inherent sparsity of the MoE architecture, achieved by selectively activating experts, poses a significant challenge to its integration with federated learning (FL). Conventional FL frameworks, designed for dense models, naively aggregate all expert parameters irrespective of their local activation patterns. This naive approach not only undermines MoE's dynamic sparsity but also risks corrupting the world knowledge within pretrained experts. To address this, we propose FLEx (Federated LLMs with Personalized Experts), a novel framework that leverages pretrained MoE-based LLMs for efficient personalization. By aggregating only the shared non-expert parameters, FLEx significantly reduces communication overhead and preserves the world knowledge stored within the frozen pretrained experts. For personalization, we introduce a novel expert grafting mechanism that leverages dynamic sparsity to construct a client-specific expert from selected components of pretrained experts, tailored to local data. This grafted expert is then fine-tuned locally alongside the gating mechanism. This joint training enables the model to learn when to leverage the shared knowledge from frozen experts and when to employ the personalized one. Evaluations on diverse, non-IID instruction tuning datasets show that FLEx consistently outperforms federated baselines on average, while demonstrating strong knowledge preservation on the knowledge-driven benchmark MMLU. Our code is available at \\href{https://anonymous.4open.science/r/FLEx-8F12}{\\texttt{https://anonymous.4open.science/r/FLEx-8F12}}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FLEx æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è”é‚¦æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­å› æ•°æ®å¼‚æ„æ€§å¸¦æ¥çš„ä¸ªæ€§åŒ–æŒ‘æˆ˜ã€‚é’ˆå¯¹æ··åˆä¸“å®¶ï¼ˆMixture of Experts, MoEï¼‰æ¶æ„åœ¨ä¼ ç»Ÿè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸­å› å…¨å‚æ•°èšåˆè€Œå¯¼è‡´çš„é€šä¿¡æ•ˆç‡ä½ä¸‹åŠé¢„è®­ç»ƒçŸ¥è¯†æŸåé—®é¢˜ï¼ŒFLEx é‡‡ç”¨ä»…èšåˆå…±äº«éä¸“å®¶å‚æ•°çš„ç­–ç•¥ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ Expert Graftingï¼ˆä¸“å®¶ç§»æ¤ï¼‰æœºåˆ¶ï¼Œæ ¹æ®æœ¬åœ°æ•°æ®ä»é¢„è®­ç»ƒä¸“å®¶çš„ç»„ä»¶ä¸­åŠ¨æ€æ„å»ºå®¢æˆ·ç«¯ä¸“å±ä¸“å®¶ã€‚é€šè¿‡è”åˆå¾®è°ƒç§»æ¤ä¸“å®¶å’Œé—¨æ§æœºåˆ¶ï¼ˆGating Mechanismï¼‰ï¼Œæ¨¡å‹èƒ½å¤ŸååŒåˆ©ç”¨å†»ç»“ä¸“å®¶çš„å…¨çƒçŸ¥è¯†ä¸æœ¬åœ°å¾®è°ƒçš„ä¸ªæ€§åŒ–çŸ¥è¯†ã€‚åœ¨å¤šç§éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFLEx çš„æ€§èƒ½ä¸€è‡´ä¼˜äºä¸»æµè”é‚¦å­¦ä¹ åŸºå‡†ï¼Œå¹¶åœ¨ MMLU ç­‰çŸ¥è¯†ä¿ç•™æµ‹è¯•ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00965v2",
      "published_date": "2025-06-01 11:24:43 UTC",
      "updated_date": "2025-10-07 05:07:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:54.903879+00:00"
    },
    {
      "arxiv_id": "2506.00958v1",
      "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues",
      "title_zh": "è¨€è¯­ä¹‹å¤–ï¼šç”¨äºä»è§†é¢‘è¯­å¢ƒå¯¹è¯ä¸­å­¦ä¹ éè¨€è¯­çº¿ç´¢çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†",
      "authors": [
        "Youngmin Kim",
        "Jiwan Chung",
        "Jisoo Kim",
        "Sunghyun Lee",
        "Sangkyu Lee",
        "Junhyeok Kim",
        "Cheoljong Yang",
        "Youngjae Yu"
      ],
      "abstract": "Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ— æ³•æœ‰æ•ˆæ•´åˆæ‰‹åŠ¿ã€é¢éƒ¨è¡¨æƒ…å’Œè‚¢ä½“è¯­è¨€ç­‰éè¨€è¯­è¦ç´ çš„é—®é¢˜ï¼Œæå‡ºäº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ MARSã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºæ„å»ºäº† VENUS æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸æ–‡æœ¬æ—¶é—´å¯¹é½çš„è§†é¢‘ã€é¢éƒ¨è¡¨æƒ…å’Œè‚¢ä½“è¯­è¨€æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬ä¸çŸ¢é‡é‡åŒ– (vector-quantized) çš„éè¨€è¯­ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹é‡‡ç”¨ä¸‹æ–‡é¢„æµ‹ (next-token prediction) ç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚å®éªŒåˆ†æè¯å®äº† VENUS æ•°æ®é›†çš„è§„æ¨¡ä¸æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº† MARS åœ¨ç†è§£å’Œç”Ÿæˆä¸å¯¹è¯å†…å®¹ç›¸åŒ¹é…çš„éè¨€è¯­çº¿ç´¢æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚å®šé‡ä¸å®šæ€§ç»“æœå‡è¡¨æ˜ï¼Œè¯¥æ¨¡å‹æˆåŠŸå®ç°äº†æ–‡æœ¬ä¸éè¨€è¯­è¯­è¨€çš„ååŒç”Ÿæˆï¼Œä¸ºåˆ›å»ºå…¨æ²‰æµ¸å¼å¯¹è¯ AI ä½“éªŒå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ACL 2025 (Main), Our code and dataset: https://github.com/winston1214/nonverbal-conversation",
      "pdf_url": "https://arxiv.org/pdf/2506.00958v1",
      "published_date": "2025-06-01 11:07:25 UTC",
      "updated_date": "2025-06-01 11:07:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:53.893926+00:00"
    },
    {
      "arxiv_id": "2506.02055v1",
      "title": "Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI",
      "title_zh": "æ™ºèƒ½ä½“ä¼šå–ä»£æˆ‘ä»¬å—ï¼Ÿå¯¹è‡ªä¸»å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½çš„è®¤çŸ¥ç ”ç©¶",
      "authors": [
        "Nikola Balic"
      ],
      "abstract": "Autonomous multi-agent AI systems are poised to transform various industries, particularly software development and knowledge work. Understanding current perceptions among professionals is crucial for anticipating adoption challenges, ethical considerations, and future workforce development. This study analyzes responses from 130 participants to a survey on the capabilities, impact, and governance of AI agents. We explore expected timelines for AI replacing programmers, identify perceived barriers to deployment, and examine beliefs about responsibility when agents make critical decisions. Key findings reveal three distinct clusters of respondents. While the study explored factors associated with current AI agent deployment, the initial logistic regression model did not yield statistically significant predictors, suggesting that deployment decisions are complex and may be influenced by factors not fully captured or that a larger sample is needed. These insights highlight the need for organizations to address compliance concerns (a commonly cited barrier) and establish clear governance frameworks as they integrate autonomous agents into their workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ä¸“ä¸šäººå£«å¯¹è‡ªä¸»å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿ(Autonomous multi-agent AI systems)åœ¨è½¯ä»¶å¼€å‘åŠçŸ¥è¯†å·¥ä½œé¢†åŸŸä¸­æ„ŸçŸ¥ä¸å½±å“çš„çœ‹æ³•ã€‚é€šè¿‡å¯¹130åå‚ä¸è€…çš„è°ƒç ”ï¼Œç ”ç©¶åˆ†æäº†AIå–ä»£ç¨‹åºå‘˜çš„é¢„æœŸæ—¶é—´è¡¨ã€éƒ¨ç½²éšœç¢ä»¥åŠåœ¨å…³é”®å†³ç­–ä¸­çš„è´£ä»»å½’å±é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¯†åˆ«å‡ºå—è®¿è€…ä¸­å­˜åœ¨ä¸‰ä¸ªæˆªç„¶ä¸åŒçš„ç¾¤ä½“ï¼Œå¹¶å‘ç°ç›®å‰çš„é€»è¾‘å›å½’æ¨¡å‹(Logistic regression model)æ— æ³•æ˜¾è‘—é¢„æµ‹AIæ™ºèƒ½ä½“çš„å®é™…éƒ¨ç½²ï¼Œæš—ç¤ºå†³ç­–è¿‡ç¨‹å—å¤æ‚å› ç´ å½±å“ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†ç»„ç»‡åœ¨é›†æˆè‡ªä¸»æ™ºèƒ½ä½“è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»é‡ç‚¹è§£å†³åˆè§„æ€§(Compliance)æ‹…å¿§å¹¶å»ºç«‹å®Œå–„çš„æ²»ç†æ¡†æ¶(Governance frameworks)ï¼Œä»¥åº”å¯¹æœªæ¥å·¥ä½œæµç¨‹çš„å˜é©ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CY",
      "comment": "15 pages, 5 figures, code available at https://github.com/nibzard/agent-perceptions",
      "pdf_url": "https://arxiv.org/pdf/2506.02055v1",
      "published_date": "2025-06-01 11:02:52 UTC",
      "updated_date": "2025-06-01 11:02:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:15:54.590361+00:00"
    },
    {
      "arxiv_id": "2506.00943v1",
      "title": "Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ™ºèƒ½åˆçº¦æ³•å¾‹åˆè§„æ€§è¯„ä¼°",
      "authors": [
        "Chanuka Wijayakoon",
        "Hai Dong",
        "H. M. N. Dilum Bandara",
        "Zahir Tari",
        "Anurag Soin"
      ],
      "abstract": "Smart contracts can implement and automate parts of legal contracts, but ensuring their legal compliance remains challenging. Existing approaches such as formal specification, verification, and model-based development require expertise in both legal and software development domains, as well as extensive manual effort. Given the recent advances of Large Language Models (LLMs) in code generation, we investigate their ability to generate legally compliant smart contracts directly from natural language legal contracts, addressing these challenges. We propose a novel suite of metrics to quantify legal compliance based on modeling both legal and smart contracts as processes and comparing their behaviors. We select four LLMs, generate 20 smart contracts based on five legal contracts, and analyze their legal compliance. We find that while all LLMs generate syntactically correct code, there is significant variance in their legal compliance with larger models generally showing higher levels of compliance. We also evaluate the proposed metrics against properties of software metrics, showing they provide fine-grained distinctions, enable nuanced comparisons, and are applicable across domains for code from any source, LLM or developer. Our results suggest that LLMs can assist in generating starter code for legally compliant smart contracts with strict reviews, and the proposed metrics provide a foundation for automated and self-improving development workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models)ä»è‡ªç„¶è¯­è¨€æ³•å¾‹åˆåŒä¸­ç›´æ¥ç”Ÿæˆç¬¦åˆæ³•å¾‹åˆè§„æ€§(Legal Compliance)çš„æ™ºèƒ½åˆçº¦(Smart Contracts)çš„èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸéªŒè¯æ–¹æ³•å¯¹æ‰‹åŠ¨åŠªåŠ›å’Œè·¨é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–ã€‚ä½œè€…æå‡ºäº†ä¸€å¥—åˆ›æ–°çš„åº¦é‡æŒ‡æ ‡ä½“ç³»ï¼Œé€šè¿‡å°†æ³•å¾‹åˆåŒå’Œæ™ºèƒ½åˆçº¦å‡å»ºæ¨¡ä¸ºè¿‡ç¨‹(Processes)å¹¶å¯¹æ¯”å…¶è¡Œä¸ºï¼Œå®ç°äº†å¯¹æ³•å¾‹åˆè§„æ€§çš„é‡åŒ–è¯„ä¼°ã€‚é€šè¿‡å¯¹å››ç§LLMsç”Ÿæˆçš„20ä¸ªæ™ºèƒ½åˆçº¦è¿›è¡Œå®éªŒåˆ†æï¼Œç ”ç©¶å‘ç°è™½ç„¶æ‰€æœ‰æ¨¡å‹ç”Ÿæˆçš„ä»£ç åœ¨è¯­æ³•ä¸Šå‡æ­£ç¡®ï¼Œä½†åœ¨æ³•å¾‹åˆè§„æ€§ä¸Šè¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚ç»“æœè¡¨æ˜æ¨¡å‹è§„æ¨¡è¶Šå¤§é€šå¸¸åˆè§„æ€§æ°´å¹³è¶Šé«˜ï¼ŒåŒæ—¶éªŒè¯äº†æ‰€ææŒ‡æ ‡åœ¨è½¯ä»¶åº¦é‡å±æ€§ä¸Šå…·æœ‰ç»†ç²’åº¦çš„åŒºåˆ†èƒ½åŠ›å’Œè·¨é¢†åŸŸé€‚ç”¨æ€§ã€‚ç ”ç©¶æŒ‡å‡ºLLMså¯æœ‰æ•ˆè¾…åŠ©ç”Ÿæˆæ™ºèƒ½åˆçº¦çš„åˆå§‹ä»£ç ï¼Œä½†ç›®å‰ä»å¿…é¡»é…åˆä¸¥æ ¼çš„äººå·¥å®¡æŸ¥ã€‚è¯¥æˆæœä¸ºæœªæ¥æ„å»ºè‡ªåŠ¨åŒ–å’Œå…·æœ‰è‡ªæ”¹è¿›èƒ½åŠ›çš„æ™ºèƒ½åˆçº¦å¼€å‘å·¥ä½œæµå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at IEEE International Conference on Blockchain and Cryptocurrency (ICBC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00943v1",
      "published_date": "2025-06-01 10:20:13 UTC",
      "updated_date": "2025-06-01 10:20:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:11.934268+00:00"
    },
    {
      "arxiv_id": "2506.00942v2",
      "title": "anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding",
      "title_zh": "anyECG-chatï¼šæ”¯æŒçµæ´»å¿ƒç”µå›¾è¾“å…¥ä¸å¤šä»»åŠ¡ç†è§£çš„é€šç”¨å¿ƒç”µå›¾å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Haitao Li",
        "Ziyu Li",
        "Yiheng Mao",
        "Ziyi Liu",
        "Zhoujian Sun",
        "Zhengxing Huang"
      ],
      "abstract": "The advent of multimodal large language models (MLLMs) has sparked interest in their application to electrocardiogram (ECG) analysis. However, existing ECG-focused MLLMs primarily focus on report generation tasks, often limited to single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that supports a broader range of tasks and more flexible ECG inputs. However, existing ECG-QA datasets are often monotonous. To address this gap, we first constructed the anyECG dataset, which encompasses a wide variety of tasks, including report generation, abnormal waveform localization, and open-ended question answering. In addition to standard hospital ECGs, we introduced long-duration reduced-lead ECGs for home environments and multiple ECG comparison scenarios commonly encountered in clinical practice. Furthermore, we propose the anyECG-chat model, which supports dynamic-length ECG inputs and multiple ECG inputs. We trained the model using a three-stage curriculum training recipe with the anyECG dataset. A comprehensive evaluation was conducted, demonstrating that anyECG-chat is capable of supporting various practical application scenarios, including not only common report generation tasks but also abnormal waveform localization for long-duration reduced-lead ECGs in home environments and comprehensive comparative analysis of multiple ECGs. Our code and data are available at: https://github.com/CuCl-2/anyECG-chat.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†anyECG-chatï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å…‹æœç°æœ‰å¿ƒç”µå›¾(ECG)å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ä»»åŠ¡èŒƒå›´å’Œè¾“å…¥çµæ´»æ€§æ–¹é¢é™åˆ¶çš„é€šç”¨å‹æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†anyECGæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†æŠ¥å‘Šç”Ÿæˆã€å¼‚å¸¸æ³¢å½¢å®šä½(abnormal waveform localization)å’Œå¼€æ”¾å¼é—®ç­”ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚é™¤äº†æ ‡å‡†åŒ»é™¢ECGï¼Œè¯¥æ•°æ®é›†è¿˜å¼•å…¥äº†é€‚ç”¨äºå±…å®¶ç¯å¢ƒçš„é•¿ç¨‹å°‘å¯¼è”(long-duration reduced-lead)å¿ƒç”µæ•°æ®ä»¥åŠä¸´åºŠå¸¸è§çš„å¤šä¸ªECGå¯¹æ¯”åœºæ™¯ã€‚anyECG-chatæ¨¡å‹æ”¯æŒåŠ¨æ€é•¿åº¦å’Œå¤šä¸ªECGè¾“å…¥ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µè¯¾ç¨‹è®­ç»ƒ(curriculum training)æ–¹æ¡ˆè¿›è¡Œå¼€å‘ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹ä¸ä»…èƒ½å®Œæˆå¸¸è§„æŠ¥å‘Šç”Ÿæˆï¼Œè¿˜èƒ½åœ¨å±…å®¶åœºæ™¯ä¸‹å®ç°é•¿ç¨‹å¿ƒç”µçš„å¼‚å¸¸æ³¢å½¢å®šä½ï¼Œå¹¶å…·å¤‡å¯¹å¤šä»½ECGè¿›è¡Œç»¼åˆå¯¹æ¯”åˆ†æçš„èƒ½åŠ›ã€‚è¯¥å·¥ä½œçš„å¼€æºä»£ç å’Œæ•°æ®ä¸ºä¸´åºŠä¸å±…å®¶å¿ƒç”µåˆ†ææä¾›äº†å…¨æ–°çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.00942v2",
      "published_date": "2025-06-01 10:17:13 UTC",
      "updated_date": "2025-11-12 06:54:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:01.274106+00:00"
    },
    {
      "arxiv_id": "2506.00936v1",
      "title": "Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning",
      "title_zh": "åŸºäºåŒè§†å›¾å¯¹æ¯”å­¦ä¹ çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä»£è°¢ç¨³å®šæ€§é¢„æµ‹",
      "authors": [
        "Peijin Guo",
        "Minghui Li",
        "Hewen Pan",
        "Bowen Chen",
        "Yang Wu",
        "Zikang Guo",
        "Leo Yu Zhang",
        "Shengshan Hu",
        "Shengqing Hu"
      ],
      "abstract": "Accurate prediction of molecular metabolic stability (MS) is critical for drug research and development but remains challenging due to the complex interplay of molecular interactions. Despite recent advances in graph neural networks (GNNs) for MS prediction, current approaches face two critical limitations: (1) incomplete molecular modeling due to atom-centric message-passing mechanisms that disregard bond-level topological features, and (2) prediction frameworks that lack reliable uncertainty quantification. To address these challenges, we propose TrustworthyMS, a novel contrastive learning framework designed for uncertainty-aware metabolic stability prediction. First, a molecular graph topology remapping mechanism synchronizes atom-bond interactions through edge-induced feature propagation, capturing both localized electronic effects and global conformational constraints. Second, contrastive topology-bond alignment enforces consistency between molecular topology views and bond patterns via feature alignment, enhancing representation robustness. Third, uncertainty modeling through Beta-Binomial uncertainty quantification enables simultaneous prediction and confidence calibration under epistemic uncertainty. Through extensive experiments, our results demonstrate that TrustworthyMS outperforms current state-of-the-art methods in terms of predictive performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TrustworthyMSï¼Œä¸€ç§æ—¨åœ¨å®ç°ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä»£è°¢ç¨³å®šæ€§é¢„æµ‹(Metabolic Stability, MS)çš„æ–°å‹å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨åŒ–å­¦é”®ç‰¹å¾å»ºæ¨¡ä¸å…¨åŠç¼ºä¹å¯é ä¸ç¡®å®šæ€§é‡åŒ–çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åˆ†å­å›¾æ‹“æ‰‘é‡æ˜ å°„æœºåˆ¶ï¼Œé€šè¿‡è¾¹è¯±å¯¼ç‰¹å¾ä¼ æ’­åŒæ­¥åŸå­-åŒ–å­¦é”®äº¤äº’ï¼Œä»è€Œæ•æ‰å±€éƒ¨ç”µå­æ•ˆåº”ä¸å…¨å±€æ„è±¡çº¦æŸã€‚åŒæ—¶ï¼Œæ¡†æ¶é€šè¿‡å¯¹æ¯”æ‹“æ‰‘-é”®å¯¹é½(Contrastive topology-bond alignment)ç¡®ä¿åˆ†å­æ‹“æ‰‘è§†å›¾ä¸é”®æ¨¡å¼ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†è¡¨å¾çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†Beta-Binomialä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯ï¼Œåœ¨å¤„ç†è®¤çŸ¥ä¸ç¡®å®šæ€§çš„åŒæ—¶å®ç°äº†é¢„æµ‹ä»»åŠ¡ä¸ç½®ä¿¡åº¦æ ¡å‡†çš„ç»“åˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTrustworthyMSåœ¨é¢„æµ‹æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸ºè¯ç‰©ç ”å‘ä¸­çš„ä»£è°¢ç¨³å®šæ€§è¯„ä¼°æä¾›äº†æ›´ç²¾ç¡®ä¸”å¯é çš„è®¡ç®—å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "This manuscript has been accepted for publication at ECML-PKDD 2025. The final version will be published in the conference proceedings",
      "pdf_url": "https://arxiv.org/pdf/2506.00936v1",
      "published_date": "2025-06-01 10:05:11 UTC",
      "updated_date": "2025-06-01 10:05:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:08.062690+00:00"
    },
    {
      "arxiv_id": "2506.00934v4",
      "title": "GRAM: Spatial general-purpose audio representation models for real-world applications",
      "title_zh": "GRAMï¼šé¢å‘ç°å®ä¸–ç•Œåº”ç”¨çš„ç©ºé—´é€šç”¨éŸ³é¢‘è¡¨ç¤ºæ¨¡å‹",
      "authors": [
        "Goksenin Yuksel",
        "Marcel van Gerven",
        "Kiki van der Heijden"
      ],
      "abstract": "Although audio foundations models have seen great progress on a wide variety of tasks, their application in real-world acoustic environments with reverberation and noise has been less successful. Moreover, as audio foundation models are typically trained on dry, single-channel audio clips, the inherent spatial nature of real-world sound scenes is overlooked and tasks involving sound localization ruled out. To address these limitations, we propose GRAM: a General-purpose Real-world Audio Model utilizing a multi-channel masked auto-encoder approach to efficiently learn spatial audio representations from high-quality simulated real-world scenes. To evaluate the performance of GRAM and other audio foundation models in real-world sound scenes, we release Nat-HEAR: A naturalistic version of the HEAR benchmark suite comprising a simulated real-world version, as well as two new sound localization tasks. We show that the performance of GRAM surpasses all state-of-the-art self-supervised audio foundation models and speech models on both HEAR and Nat-HEAR, while using only a fraction of the training data. GRAM also showcases state-of-the-art localization performance, surpassing even supervised sound localization approaches, and can be flexibly applied either to a two-channel, binaural sound format or a four-channel, Ambisonics format. Validating GRAM's performance on real-world sound recordings demonstrates robust transfer to real-world scenes. Taken together, GRAM presents a significant advancement towards robust, spatial audio foundation models for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GRAMï¼Œä¸€ä¸ªé¢å‘çœŸå®ä¸–ç•Œåº”ç”¨çš„ç©ºé—´é€šç”¨éŸ³é¢‘è¡¨å¾æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹å¿½ç•¥ç©ºé—´å±æ€§çš„é—®é¢˜ã€‚GRAMé‡‡ç”¨å¤šé€šé“æ©ç è‡ªç¼–ç å™¨(multi-channel masked auto-encoder)æ–¹æ³•ï¼Œé€šè¿‡é«˜è´¨é‡æ¨¡æ‹Ÿçš„çœŸå®åœºæ™¯é«˜æ•ˆå­¦ä¹ ç©ºé—´éŸ³é¢‘è¡¨å¾ã€‚ä¸ºäº†è¿›è¡Œç»¼åˆè¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†åŒ…å«è‡ªç„¶æ¨¡æ‹Ÿç¯å¢ƒå’Œå£°æºå®šä½ä»»åŠ¡çš„Nat-HEARåŸºå‡†æµ‹è¯•é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRAMåœ¨HEARå’ŒNat-HEARä¸Šçš„è¡¨ç°ä¼˜äºç›®å‰æ‰€æœ‰å…ˆè¿›çš„è‡ªç›‘ç£éŸ³é¢‘æ¨¡å‹å’Œè¯­éŸ³æ¨¡å‹ï¼Œä¸”ä»…ä½¿ç”¨äº†æå°‘é‡çš„è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒGRAMåœ¨å®šä½ä»»åŠ¡ä¸Šå±•ç°å‡ºè¶…è¶Šæœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶èƒ½çµæ´»é€‚é…åŒå£°é“(binaural)æˆ–å››å£°é“(Ambisonics)æ ¼å¼ã€‚é€šè¿‡çœŸå®å½•éŸ³çš„éªŒè¯è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¨¡å‹å‘çœŸå®ä¸–ç•Œè¿ç§»çš„ç¨³å¥æ€§ï¼Œæ ‡å¿—ç€ç©ºé—´é€šç”¨éŸ³é¢‘åŸºç¡€æ¨¡å‹å–å¾—äº†é‡å¤§è¿›å±•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Still under review",
      "pdf_url": "https://arxiv.org/pdf/2506.00934v4",
      "published_date": "2025-06-01 09:56:33 UTC",
      "updated_date": "2025-11-10 10:58:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:22.422562+00:00"
    },
    {
      "arxiv_id": "2506.00930v1",
      "title": "Aligning VLM Assistants with Personalized Situated Cognition",
      "title_zh": "å°† VLM åŠ©æ‰‹ä¸ä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥ç›¸å¯¹é½",
      "authors": [
        "Yongqi Li",
        "Shen Zhou",
        "Xiaohu Li",
        "Xin Miao",
        "Jintao Wen",
        "Mayi Xu",
        "Jianhao Chen",
        "Birong Pan",
        "Hankun Kang",
        "Yuanyuan Zhu",
        "Ming Zhong",
        "Tieyun Qian"
      ],
      "abstract": "Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)è™½ç„¶ç¬¦åˆé€šç”¨äººç±»ç›®æ ‡ï¼Œä½†éš¾ä»¥é€‚åº”ä¸åŒèƒŒæ™¯ä¸ªä½“åœ¨ä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥(Personalized Situated Cognition)ä¸‹çš„ç‰¹æ®Šéœ€æ±‚è¿™ä¸€é—®é¢˜ã€‚ä½œè€…åˆ©ç”¨ç¤¾ä¼šå­¦ä¸­çš„è§’è‰²é›†(Role-Set)æ¦‚å¿µæ¥åˆ»ç”»ä¸ªä½“ç‰¹å¾ï¼Œå¹¶æå‡ºé€šè¿‡è¯„ä¼°ä¸ªä½“çš„å…·ä½“è¡ŒåŠ¨æ¥è¡¡é‡ä¸ªæ€§åŒ–å¯¹é½çš„æ•ˆæœã€‚ä¸ºæ­¤ï¼Œå›¢é˜Ÿå¼€å‘äº†åŒ…å«1.8ä¸‡ä¸ªå®ä¾‹å’Œ20ä½ä¸åŒèƒŒæ™¯ä¸ªä½“çš„åŸºå‡†æµ‹è¯•PCogAlignBenchï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPCogAlignçš„æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯æ„å»ºä¸€ä¸ªè®¤çŸ¥æ„ŸçŸ¥ä¸”åŸºäºè¡ŒåŠ¨çš„å¥–åŠ±æ¨¡å‹(Reward Model)ä»¥å®ç°ç²¾å‡†å¯¹é½ã€‚å®éªŒæ•°æ®å’Œäººå·¥è¯„ä¼°ç»“æœå‡è¡¨æ˜ï¼ŒPCogAlignBenchå…·æœ‰æé«˜çš„å¯é æ€§ï¼Œä¸”PCogAlignæ¡†æ¶èƒ½æ˜¾è‘—æå‡VLMåŠ©æ‰‹åœ¨çœŸå®ä¸–ç•Œè¾…åŠ©ä»»åŠ¡ä¸­çš„ä¸ªæ€§åŒ–æœåŠ¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ACL 2025 (main), camera-ready version",
      "pdf_url": "https://arxiv.org/pdf/2506.00930v1",
      "published_date": "2025-06-01 09:50:54 UTC",
      "updated_date": "2025-06-01 09:50:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:35.184035+00:00"
    },
    {
      "arxiv_id": "2506.00927v1",
      "title": "In-the-wild Audio Spatialization with Flexible Text-guided Localization",
      "title_zh": "çœŸå®åœºæ™¯ä¸‹æ”¯æŒçµæ´»æ–‡æœ¬å¼•å¯¼å®šä½çš„éŸ³é¢‘ç©ºé—´åŒ–",
      "authors": [
        "Tianrui Pan",
        "Jie Liu",
        "Zewen Huang",
        "Jie Tang",
        "Gangshan Wu"
      ],
      "abstract": "To enhance immersive experiences, binaural audio offers spatial awareness of sounding objects in AR, VR, and embodied AI applications. While existing audio spatialization methods can generally map any available monaural audio to binaural audio signals, they often lack the flexible and interactive control needed in complex multi-object user-interactive environments. To address this, we propose a Text-guided Audio Spatialization (TAS) framework that utilizes flexible text prompts and evaluates our model from unified generation and comprehension perspectives. Due to the limited availability of premium and large-scale stereo data, we construct the SpatialTAS dataset, which encompasses 376,000 simulated binaural audio samples to facilitate the training of our model. Our model learns binaural differences guided by 3D spatial location and relative position prompts, augmented by flipped-channel audio. It outperforms existing methods on both simulated and real-recorded datasets, demonstrating superior generalization and accuracy. Besides, we develop an assessment model based on Llama-3.1-8B, which evaluates the spatial semantic coherence between our generated binaural audio and text prompts through a spatial reasoning task. Results demonstrate that text prompts provide flexible and interactive control to generate binaural audio with excellent quality and semantic consistency in spatial locations. Dataset is available at \\href{https://github.com/Alice01010101/TASU}",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Text-guided Audio Spatialization (TAS) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡çµæ´»çš„æ–‡æœ¬æç¤ºè¯ (text prompts) å®ç°æ²‰æµ¸å¼éŸ³é¢‘çš„ç©ºé—´åŒ–ï¼Œè§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤æ‚å¤šç‰©ä½“äº¤äº’ç¯å¢ƒä¸­ç¼ºä¹çµæ´»æ€§å’Œäº¤äº’æ§åˆ¶çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœé«˜è´¨é‡å¤§è§„æ¨¡ç«‹ä½“å£°æ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«37.6ä¸‡ä¸ªæ¨¡æ‹ŸåŒè€³éŸ³é¢‘æ ·æœ¬çš„SpatialTASæ•°æ®é›†ï¼Œè¾…åŠ©æ¨¡å‹å­¦ä¹ ç”±3Dç©ºé—´ä½ç½®å’Œç›¸å¯¹ä½ç½®æç¤ºå¼•å¯¼çš„åŒè€³å·®å¼‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ä¸ªåŸºäºLlama-3.1-8Bçš„è¯„ä¼°æ¨¡å‹ï¼Œé€šè¿‡ç©ºé—´æ¨ç†ä»»åŠ¡æ¥éªŒè¯ç”Ÿæˆçš„åŒè€³éŸ³é¢‘ä¸æ–‡æœ¬æç¤ºè¯ä¹‹é—´çš„ç©ºé—´è¯­ä¹‰ä¸€è‡´æ€§ (spatial semantic coherence)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®å½•åˆ¶çš„æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚TASæ¡†æ¶è¯æ˜äº†åˆ©ç”¨æ–‡æœ¬æç¤ºå¯ä»¥å®ç°å¯¹ç”Ÿæˆé«˜éŸ³è´¨ã€é«˜è¯­ä¹‰ä¸€è‡´æ€§ç©ºé—´éŸ³é¢‘çš„çµæ´»äº¤äº’æ§åˆ¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by ACL 2025 main",
      "pdf_url": "https://arxiv.org/pdf/2506.00927v1",
      "published_date": "2025-06-01 09:41:56 UTC",
      "updated_date": "2025-06-01 09:41:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:21.784728+00:00"
    },
    {
      "arxiv_id": "2506.02053v1",
      "title": "Generalization Performance of Ensemble Clustering: From Theory to Algorithm",
      "title_zh": "é›†æˆèšç±»çš„æ³›åŒ–æ€§èƒ½ï¼šä»ç†è®ºåˆ°ç®—æ³•",
      "authors": [
        "Xu Zhang",
        "Haoye Qiu",
        "Weixuan Liang",
        "Hui Liu",
        "Junhui Hou",
        "Yuheng Jia"
      ],
      "abstract": "Ensemble clustering has demonstrated great success in practice; however, its theoretical foundations remain underexplored. This paper examines the generalization performance of ensemble clustering, focusing on generalization error, excess risk and consistency. We derive a convergence rate of generalization error bound and excess risk bound both of $\\mathcal{O}(\\sqrt{\\frac{\\log n}{m}}+\\frac{1}{\\sqrt{n}})$, with $n$ and $m$ being the numbers of samples and base clusterings. Based on this, we prove that when $m$ and $n$ approach infinity and $m$ is significantly larger than log $n$, i.e., $m,n\\to \\infty, m\\gg \\log n$, ensemble clustering is consistent. Furthermore, recognizing that $n$ and $m$ are finite in practice, the generalization error cannot be reduced to zero. Thus, by assigning varying weights to finite clusterings, we minimize the error between the empirical average clusterings and their expectation. From this, we theoretically demonstrate that to achieve better clustering performance, we should minimize the deviation (bias) of base clustering from its expectation and maximize the differences (diversity) among various base clusterings. Additionally, we derive that maximizing diversity is nearly equivalent to a robust (min-max) optimization model. Finally, we instantiate our theory to develop a new ensemble clustering algorithm. Compared with SOTA methods, our approach achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t. NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›†æˆèšç±» (Ensemble clustering) ç†è®ºåŸºç¡€ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæ·±å…¥æ¢è®¨äº†å…¶æ³›åŒ–è¯¯å·® (generalization error)ã€è¶…é¢é£é™© (excess risk) å’Œä¸€è‡´æ€§ (consistency)ã€‚ç ”ç©¶è€…æ¨å¯¼å‡ºæ³›åŒ–è¯¯å·®ç•Œä¸è¶…é¢é£é™©ç•Œçš„æ”¶æ•›é€Ÿåº¦ä¸º $\\mathcal{O}(\\sqrt{\\frac{\\log n}{m}}+\\frac{1}{\\sqrt{n}})$ï¼Œå¹¶è¯æ˜äº†åœ¨åŸºèšç±»æ•°é‡ $m$ è¿œå¤§äº $\\log n$ ä¸”è¶‹äºæ— ç©·æ—¶ï¼Œé›†æˆèšç±»å…·æœ‰ç†è®ºä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹æœ‰é™æ ·æœ¬ä¸‹æƒé‡çš„åˆ†é…ï¼Œè®ºæ–‡ä»ç†è®ºä¸Šé˜æ˜äº†æå‡èšç±»æ€§èƒ½çš„å…³é”®åœ¨äºæœ€å°åŒ–åŸºèšç±»çš„åå·® (bias) å¹¶æœ€å¤§åŒ–å…¶å¤šæ ·æ€§ (diversity)ï¼Œä¸”å¤šæ ·æ€§æœ€å¤§åŒ–åœ¨å½¢å¼ä¸Šç­‰åŒäºé²æ£’ (min-max) ä¼˜åŒ–æ¨¡å‹ã€‚æœ€åï¼Œç ”ç©¶è€…å°†è¯¥ç†è®ºè½¬åŒ–ä¸ºä¸€ç§æ–°çš„é›†æˆèšç±»ç®—æ³•ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨ 10 ä¸ªæ•°æ®é›†ä¸Šçš„ NMIã€ARI å’Œ Purity æŒ‡æ ‡ä¸Šç›¸è¾ƒäº SOTA æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02053v1",
      "published_date": "2025-06-01 09:34:52 UTC",
      "updated_date": "2025-06-01 09:34:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:29.295296+00:00"
    },
    {
      "arxiv_id": "2506.00924v2",
      "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison",
      "title_zh": "å¼¥åˆä¸»å®¢è§‚ QoEï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹è¯„è®ºåˆ†æä¸ç½‘ç»œ MOS å¯¹æ¯”çš„è¿è¥å•†çº§èšåˆ",
      "authors": [
        "Parsa Hassani Shariat Panahi",
        "Amir Hossein Jalilvand",
        "M. Hassan Najafi"
      ],
      "abstract": "This paper introduces a dual-layer framework for network operator-side quality of experience (QoE) assessment that integrates both objective network modeling and subjective user perception extracted from live-streaming platforms. On the objective side, we develop a machine learning model trained on mean opinion scores (MOS) computed via the ITU-T P.1203 reference implementation, allowing accurate prediction of user-perceived video quality using only network parameters such as packet loss, delay, jitter, and throughput without reliance on video content or client-side instrumentation. On the subjective side, we present a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback. A large language model is used to assign scalar MOS scores to filtered comments in a deterministic and reproducible manner. To support scalable and interpretable analysis, we construct a labeled dataset of 47,894 live-stream comments, of which about 34,000 are identified as QoE-relevant through multi-layer semantic filtering. Each comment is enriched with simulated Internet Service Provider attribution and temporally aligned using synthetic timestamps in 5-min intervals. The resulting dataset enables operator-level aggregation and time-series analysis of user-perceived quality. A delta MOS metric is proposed to measure each Internet service provider's deviation from platform-wide sentiment, allowing detection of localized degradations even in the absence of direct network telemetry. A controlled outage simulation confirms the framework's effectiveness in identifying service disruptions through comment-based trends alone. The system provides each operator with its own subjective MOS and the global platform average per interval, enabling real-time interpretation of performance deviations and comparison with objective network-based QoE estimates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒå±‚æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå®¢è§‚ç½‘ç»œå»ºæ¨¡å’ŒåŸºäºç›´æ’­å¹³å°çš„å—ä¼—æ„ŸçŸ¥åˆ†æï¼Œå®ç°è¿è¥å•†çº§åˆ«çš„ä½“éªŒè´¨é‡(QoE)è¯„ä¼°ã€‚åœ¨å®¢è§‚å±‚é¢ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºäºITU-T P.1203æ ‡å‡†è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»…é€šè¿‡ä¸¢åŒ…ç‡ã€å»¶è¿Ÿã€æŠ–åŠ¨å’Œååé‡ç­‰ç½‘ç»œå‚æ•°å³å¯å‡†ç¡®é¢„æµ‹ç”¨æˆ·æ„ŸçŸ¥çš„è§†é¢‘è´¨é‡ã€‚åœ¨ä¸»è§‚å±‚é¢ï¼Œç ”ç©¶å¼€å‘äº†ä¸€å¥—è¯­ä¹‰è¿‡æ»¤å’Œè¯„åˆ†æµæ°´çº¿ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)å¯¹ç›´æ’­è¯„è®ºè¿›è¡Œè‡ªåŠ¨åŒ–å¤„ç†ï¼Œå¹¶åˆ†é…ç¡®å®šæ€§çš„å¹³å‡ä¸»è§‚æ„è§åˆ†(MOS)ä»¥æå–æ€§èƒ½åé¦ˆã€‚é€šè¿‡æ„å»ºåŒ…å«çº¦4.8ä¸‡æ¡æ ‡æ³¨è¯„è®ºçš„æ•°æ®é›†å¹¶ç»“åˆäº’è”ç½‘æœåŠ¡æä¾›å•†(ISP)å½’å±åˆ†æï¼Œè¯¥ç³»ç»Ÿå®ç°äº†QoEçš„èšåˆä¸æ—¶é—´åºåˆ—åˆ†æã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§Delta MOSæŒ‡æ ‡ï¼Œé€šè¿‡è¡¡é‡ç‰¹å®šè¿è¥å•†ä¸å¹³å°æ•´ä½“æƒ…ç»ªçš„åç¦»åº¦ï¼Œåœ¨æ— éœ€ç›´æ¥ç½‘ç»œé¥æµ‹çš„æƒ…å†µä¸‹å³å¯æœ‰æ•ˆæ£€æµ‹å±€éƒ¨æœåŠ¡é™çº§ã€‚å—æ§åœç”µæ¨¡æ‹Ÿå®éªŒè¯å®äº†è¯¥æ¡†æ¶ä»…å‡­è¯„è®ºè¶‹åŠ¿è¯†åˆ«æœåŠ¡ä¸­æ–­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè¿è¥å•†æä¾›äº†å®æ—¶å¯¹æ¯”ä¸»è§‚æ„ŸçŸ¥ä¸å®¢è§‚ç½‘ç»œQoEä¼°ç®—çš„åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.NI",
      "comment": "19 ppages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00924v2",
      "published_date": "2025-06-01 09:31:55 UTC",
      "updated_date": "2025-06-30 05:21:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:51.944424+00:00"
    },
    {
      "arxiv_id": "2506.06343v1",
      "title": "TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment",
      "title_zh": "TESU-LLMï¼šåŸºäºç»Ÿä¸€ç¼–ç å™¨å¯¹é½çš„æ— è¯­éŸ³æ•°æ®è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Taesoo Kim",
        "Jong Hwan Ko"
      ],
      "abstract": "Recent advances in speech-enabled language models have shown promising results in building intelligent voice assistants. However, most existing approaches rely on large-scale paired speech-text data and extensive computational resources, which pose challenges in terms of scalability and accessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework that enables training speech-capable language models using only text data. Our key insight is to leverage a unified encoder that maps semantically equivalent text and speech inputs to a shared latent space. By aligning the encoder output with the embedding space of a LLM via a lightweight projection network, we enable the model to generalize from text-only supervision to speech-based inference. Despite being trained exclusively on text, TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources. These results highlight the effectiveness and efficiency of our approach, offering a scalable path toward building speech LLMs without speech data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TESU-LLMï¼Œä¸€ç§æ— éœ€è¯­éŸ³æ•°æ®å³å¯è®­ç»ƒè¯­éŸ³è¯­è¨€æ¨¡å‹ (Speech-LLMs) çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¯¹å¤§è§„æ¨¡è¯­éŸ³-æ–‡æœ¬å¯¹é½æ•°æ®å’Œé«˜æ˜‚è®¡ç®—èµ„æºçš„ä¾èµ–é—®é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨ç»Ÿä¸€ç¼–ç å™¨ (unified encoder) å°†è¯­ä¹‰ç­‰ä»·çš„æ–‡æœ¬å’Œè¯­éŸ³è¾“å…¥æ˜ å°„åˆ°å…±äº«æ½œç©ºé—´ï¼Œå¹¶é€šè¿‡è½»é‡çº§æŠ•å½±ç½‘ç»œ (projection network) å°†ç¼–ç å™¨è¾“å‡ºä¸å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„åµŒå…¥ç©ºé—´å¯¹é½ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿä»…é€šè¿‡æ–‡æœ¬ç›‘ç£å­¦ä¹ ï¼Œå³å¯åœ¨æ¨ç†é˜¶æ®µå®ç°å¯¹è¯­éŸ³è¾“å…¥çš„æœ‰æ•ˆæ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…åœ¨æ–‡æœ¬ä¸Šè®­ç»ƒçš„ TESU-LLM åœ¨å„ç±»è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å¼ºåŠ²æ€§èƒ½ï¼Œå…¶è¡¨ç°å¯ä¸è€—è´¹å¤§é‡å¤šæ¨¡æ€æ•°æ®å’Œè®¡ç®—èµ„æºçš„åŸºçº¿æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºé«˜æ•ˆã€å¯æ‰©å±•çš„è¯­éŸ³è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€æ¡æ— éœ€è¯­éŸ³æ•°æ®çš„æ–°è·¯å¾„ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡ä¸å¯è®¿é—®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06343v1",
      "published_date": "2025-06-01 09:27:55 UTC",
      "updated_date": "2025-06-01 09:27:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:17:04.256000+00:00"
    },
    {
      "arxiv_id": "2506.00920v2",
      "title": "Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation",
      "title_zh": "ä½ç½®å³æ¦‚ç‡ï¼šå…·å¤‡è¶…è¶Šè®­ç»ƒé•¿åº¦å¤–æ¨èƒ½åŠ›çš„è‡ªç›‘ç£ Transformer",
      "authors": [
        "Philip Heejun Lee"
      ],
      "abstract": "Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦åºåˆ—æ¨¡å‹åœ¨æµ‹è¯•åºåˆ—é•¿åº¦è¿œè¶…è®­ç»ƒé•¿åº¦æ—¶å‡†ç¡®ç‡å¤§å¹…ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº† PRISM (Probabilistic Relative-position Implicit Superposition Model)ã€‚ä½œä¸ºä¸€ç§æ–°å‹ä½ç½®ç¼–ç æœºåˆ¶ï¼ŒPRISM é€šè¿‡å¯å¾®çš„ç›´æ–¹å›¾æ»¤æ³¢å™¨ (histogram-filter) æ›´æ–°æ¥å­¦ä¹ è¿ç»­çš„ç›¸å¯¹ä½ç½®ï¼Œå¹¶é‡‡ç”¨æ¦‚ç‡å åŠ  (probabilistic superposition) æ›¿ä»£ä¼ ç»Ÿçš„ç¡®å®šæ€§åµŒå…¥æ¥ä¿ç•™ä½ç½®ä¸ç¡®å®šæ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹ä½¿ Transformer èƒ½å¤Ÿå‡†ç¡®å¤–æ¨ (extrapolate) è‡³è®­ç»ƒé•¿åº¦çš„ 10 å€ä»¥ä¸Šï¼Œåœ¨ç®—æœ¯è¿ç®—ã€SCAN ç»„åˆæ€§ä»»åŠ¡åŠå¤æ‚å¤åˆ¶ä»»åŠ¡ç­‰ç®—æ³•åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿› (state-of-the-art) çš„æ°´å¹³ã€‚åˆ†æè¡¨æ˜ï¼ŒPRISM çš„éšæœºä½ç½®ç¼–ç èƒ½ç»´æŒæ¸…æ™°ä¸”å¯è§£é‡Šçš„å†…éƒ¨çŠ¶æ€ï¼Œä¸ºå¯é çš„é•¿åº¦æ³›åŒ–å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘åœ¨è¶…é•¿åºåˆ—ä¸‹ä»å…·ç®—æ³•ç¨³å¥æ€§çš„ç¥ç»åºåˆ—æ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Note: v2: working paper; code, additional baselines, ablations, will follow in v3",
      "pdf_url": "https://arxiv.org/pdf/2506.00920v2",
      "published_date": "2025-06-01 09:20:44 UTC",
      "updated_date": "2025-12-22 19:46:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:17:11.833974+00:00"
    },
    {
      "arxiv_id": "2506.00918v1",
      "title": "Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks",
      "title_zh": "åŸºäºè¾“å…¥-è¾“å‡ºæ¡ä»¶çš„å›å½’ç½‘ç»œè§„èŒƒåŒ–äº‹åä¸ç¡®å®šæ€§ä¼°è®¡",
      "authors": [
        "Lennart Bramlage",
        "CristÃ³bal Curio"
      ],
      "abstract": "Uncertainty quantification is critical in safety-sensitive applications but is often omitted from off-the-shelf neural networks due to adverse effects on predictive performance. Retrofitting uncertainty estimates post-hoc typically requires access to model parameters or gradients, limiting feasibility in practice. We propose a theoretically grounded framework for post-hoc uncertainty estimation in regression tasks by fitting an auxiliary model to both original inputs and frozen model outputs. Drawing from principles of maximum likelihood estimation and sequential parameter fitting, we formalize an exact post-hoc optimization objective that recovers the canonical MLE of Gaussian parameters, without requiring sampling or approximation at inference. While prior work has used model outputs to estimate uncertainty, we explicitly characterize the conditions under which this is valid and demonstrate the extent to which structured outputs can support quasi-epistemic inference. We find that using diverse auxiliary data, such as augmented subsets of the original training data, significantly enhances OOD detection and metric performance. Our hypothesis that frozen model outputs contain generalizable latent information about model error and predictive uncertainty is tested and confirmed. Finally, we ensure that our method maintains proper estimation of input-dependent uncertainty without relying exclusively on base model forecasts. These findings are demonstrated in toy problems and adapted to both UCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç†è®ºå®Œå¤‡çš„å›å½’ç½‘ç»œäº‹åä¸ç¡®å®šæ€§è¯„ä¼°(post-hoc uncertainty estimation)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¯¹æ¨¡å‹å‚æ•°æˆ–æ¢¯åº¦çš„ä¾èµ–é™åˆ¶ã€‚é€šè¿‡å°†è¾…åŠ©æ¨¡å‹(auxiliary model)æ‹Ÿåˆè‡³åŸå§‹è¾“å…¥å’Œå†»ç»“çš„æ¨¡å‹è¾“å‡ºï¼Œè¯¥æ–¹æ³•åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡(Maximum Likelihood Estimation)å’Œåºåˆ—å‚æ•°æ‹ŸåˆåŸç†ï¼Œå®ç°äº†æ— éœ€æ¨ç†é‡‡æ ·æˆ–è¿‘ä¼¼çš„é«˜æ–¯å‚æ•°æ¢å¤ã€‚ç ”ç©¶è®ºè¯äº†ç»“æ„åŒ–è¾“å‡ºåœ¨æ”¯æŒå‡†è®¤çŸ¥æ¨ç†(quasi-epistemic inference)ä¸­çš„ä½œç”¨ï¼Œå¹¶å‘ç°åˆ©ç”¨å¢å¼ºçš„è¾…åŠ©æ•°æ®èƒ½æ˜¾è‘—æå‡åˆ†å¸ƒå¤–(OOD)æ£€æµ‹æ•ˆæœã€‚å®éªŒç»“æœè¯å®äº†å†»ç»“çš„æ¨¡å‹è¾“å‡ºåŒ…å«å…³äºæ¨¡å‹è¯¯å·®çš„å¯æ³›åŒ–æ½œä¿¡æ¯ï¼Œä¸”è¯¥æ–¹æ³•åœ¨UCIå’Œæ·±åº¦å›å½’(depth regression)åŸºå‡†æµ‹è¯•ä¸­å‡ç»´æŒäº†è‰¯å¥½çš„è¾“å…¥ä¾èµ–ä¸ç¡®å®šæ€§ä¼°è®¡æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00918v1",
      "published_date": "2025-06-01 09:13:27 UTC",
      "updated_date": "2025-06-01 09:13:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:16:59.845130+00:00"
    },
    {
      "arxiv_id": "2506.03195v2",
      "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs",
      "title_zh": "æ— æ ‡æ³¨æ•°æ®æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦å›¾åƒé›¶æ ·æœ¬åˆ†ç±»",
      "authors": [
        "Yunqi Hong",
        "Sohyun An",
        "Andrew Bai",
        "Neil Y. C. Lin",
        "Cho-Jui Hsieh"
      ],
      "abstract": "Despite Multimodal Large Language Models (MLLMs) showing promising results on general zero-shot image classification tasks, fine-grained image classification remains challenging. It demands precise attention to subtle visual details to distinguish between visually similar subcategories--details that MLLMs may easily overlook without explicit guidance. To address this, we introduce AutoSEP, an iterative self-supervised prompt learning framework designed to enhance MLLM fine-grained classification capabilities in a fully unsupervised manner. Our core idea is to leverage unlabeled data to learn a description prompt that guides MLLMs in identifying crucial discriminative features within an image, and boosts classification accuracy. We developed an automatic self-enhancing prompt learning framework called AutoSEP to iteratively improve the description prompt using unlabeled data, based on instance-level classification scoring function. AutoSEP only requires black-box access to MLLMs, eliminating the need for any training or fine-tuning. We evaluate our approach on multiple fine-grained classification datasets. It consistently outperforms other unsupervised baselines, demonstrating the effectiveness of our self-supervised optimization framework. Notably, AutoSEP on average improves 13 percent over standard zero-shot classification and 5 percent over the best-performing baselines. Code is available at: https://github.com/yq-hong/AutoSEP",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) åœ¨ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸­éš¾ä»¥æ•æ‰ç»†å¾®è§†è§‰ç‰¹å¾çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º AutoSEP çš„è¿­ä»£è‡ªç›‘ç£æç¤ºå­¦ä¹ æ¡†æ¶ (self-supervised prompt learning framework)ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨æ— æ ‡ç­¾æ•°æ® (unlabeled data) å­¦ä¹ æè¿°æ€§æç¤ºè¯ï¼Œå¼•å¯¼ MLLMs è¯†åˆ«å›¾åƒä¸­çš„å…³é”®åŒºåˆ†æ€§ç‰¹å¾ï¼Œä»è€Œåœ¨å®Œå…¨æ— ç›‘ç£çš„æƒ…å†µä¸‹æå‡åˆ†ç±»æ€§èƒ½ã€‚AutoSEP åŸºäºå®ä¾‹çº§åˆ†ç±»è¯„åˆ†å‡½æ•°è‡ªåŠ¨ä¼˜åŒ–æç¤ºè¯ï¼Œä¸”ä»…éœ€å¯¹ MLLMs è¿›è¡Œé»‘ç›’è®¿é—®ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒã€‚åœ¨å¤šä¸ªç»†ç²’åº¦åˆ†ç±»æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAutoSEP ä¸€è‡´ä¼˜äºç°æœ‰æ— ç›‘ç£åŸºçº¿ï¼Œå…¶å‡†ç¡®ç‡è¾ƒæ ‡å‡†é›¶æ ·æœ¬åˆ†ç±»å¹³å‡æå‡äº† 13%ï¼Œè¾ƒæœ€ä¼˜åŸºçº¿æå‡äº† 5%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡è‡ªç›‘ç£ä¼˜åŒ–æ¡†æ¶åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æ¥å¢å¼º MLLMs ç»†ç²’åº¦è¯†åˆ«èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03195v2",
      "published_date": "2025-06-01 09:04:07 UTC",
      "updated_date": "2025-11-27 00:46:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:17:13.385256+00:00"
    },
    {
      "arxiv_id": "2506.00914v1",
      "title": "How do Transformer Embeddings Represent Compositions? A Functional Analysis",
      "title_zh": "Transformer åµŒå…¥å¦‚ä½•è¡¨å¾ç»„åˆï¼Ÿä¸€é¡¹åŠŸèƒ½åˆ†æ",
      "authors": [
        "Aishik Nagar",
        "Ishaan Singh Rawal",
        "Mansi Dhanania",
        "Cheston Tan"
      ],
      "abstract": "Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Transformer Embeddings å¦‚ä½•è¡¨ç¤ºè¯­è¨€ä¸­çš„ç»„åˆæ€§ (Compositionality)ï¼Œå¹¶å¯¹ Mistralã€OpenAI Largeã€Google åµŒå…¥æ¨¡å‹åŠ BERT è¿›è¡Œäº†åŠŸèƒ½æ€§å¯¹æ¯”åˆ†æã€‚ç ”ç©¶äººå‘˜é€šè¿‡è¯„ä¼°åŒ…æ‹¬ Vector Addition (å‘é‡åŠ æ³•)ã€Multiplication (ä¹˜æ³•)ã€Dilation (æ‰©å¼ ) å’Œ Ridge Regression (å²­å›å½’) åœ¨å†…çš„å…­ç§ç»„åˆæ¨¡å‹ï¼Œç³»ç»Ÿè€ƒå¯Ÿäº†åµŒå…¥ç©ºé—´ä¸­å¤åˆè¯çš„ç”Ÿæˆæ–¹å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶çº¿æ€§çš„ Ridge Regression æ¨¡å‹è¡¨ç°æœ€ä¼˜ï¼Œä½†ç»å…¸çš„ Vector Addition æ¨¡å‹åœ¨è¡¡é‡ç»„åˆæ€§æ—¶çš„æ€§èƒ½å‡ºå¥‡åœ°æ¥è¿‘ã€‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œå¤§å¤šæ•°ç°ä»£åµŒå…¥æ¨¡å‹å…·æœ‰é«˜åº¦çš„ç»„åˆæ€§ï¼Œè€Œ BERT åœ¨è¿™ä¸€æŒ‡æ ‡ä¸Šçš„è¡¨ç°åˆ™æ˜¾è‘—è¾ƒå¼±ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å½¢å®¹è¯-åè¯ç»„åˆçš„åˆæˆæ•°æ®é›†éªŒè¯å¹¶å¯è§†åŒ–äº†è¿™äº›å‘ç°ï¼Œä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†è¯æ±‡ç»„åˆæä¾›äº†æ·±å…¥çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00914v1",
      "published_date": "2025-06-01 09:02:56 UTC",
      "updated_date": "2025-06-01 09:02:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:17:12.020551+00:00"
    },
    {
      "arxiv_id": "2506.00912v2",
      "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages",
      "title_zh": "Pi-SQLï¼šåˆ©ç”¨ä¸­è½¬ç¼–ç¨‹è¯­è¨€çš„ç»†ç²’åº¦å¼•å¯¼å¢å¼º Text-to-SQL",
      "authors": [
        "Yongdong chi",
        "Hanqing Wang",
        "Zonghan Yang",
        "Jian Yang",
        "Xiao Yan",
        "Yun Chen",
        "Guanhua Chen"
      ],
      "abstract": "Text-to-SQL transforms the user queries from natural language to executable SQL programs, enabling non-experts to interact with complex databases. Existing prompt-based methods craft meticulous text guidelines and examples to facilitate SQL generation, but their accuracy is hindered by the large semantic gap between the texts and the low-resource SQL programs. In this work, we propose Pi-SQL, which incorporates the high-resource Python program as a pivot to bridge between the natural language query and SQL program. In particular, Pi-SQL first generates Python programs that provide fine-grained step-by-step guidelines in their code blocks or comments, and then produces an SQL program following the guidance of each Python program. The final SQL program matches the reference Python program's query results and, through selection from candidates generated by different strategies, achieves superior execution speed, with a reward-based valid efficiency score up to 4.55 higher than the best-performing baseline. Extensive experiments demonstrate the effectiveness of Pi-SQL, which improves the execution accuracy of the best-performing baseline by up to 3.20.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Pi-SQLæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥é«˜èµ„æºç¼–ç¨‹è¯­è¨€Pythonä½œä¸ºæ¢çº½(pivot)ï¼Œè§£å†³Text-to-SQLä»»åŠ¡ä¸­è‡ªç„¶è¯­è¨€ä¸ä½èµ„æºSQLç¨‹åºä¹‹é—´å­˜åœ¨çš„å·¨å¤§è¯­ä¹‰æ²Ÿå£‘ã€‚Pi-SQLé¦–å…ˆç”ŸæˆåŒ…å«ç»†ç²’åº¦ã€é€æ­¥æŒ‡å¯¼ä¿¡æ¯çš„Pythonç¨‹åºï¼Œåˆ©ç”¨å…¶ä»£ç å—æˆ–æ³¨é‡Šæä¾›é€»è¾‘æŒ‡å¼•ï¼Œéšååœ¨è¿™äº›æŒ‡å¼•ä¸‹ç”Ÿæˆç›®æ ‡SQLç¨‹åºã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸æ€§èƒ½ï¼Œè¯¥æ¡†æ¶ä¼šä»å¤šä¸ªå€™é€‰ç­–ç•¥ä¸­ç­›é€‰å‡ºæŸ¥è¯¢ç»“æœä¸å‚è€ƒPythonç¨‹åºä¸€è‡´ä¸”æ‰§è¡Œé€Ÿåº¦æœ€ä¼˜çš„SQLç¨‹åºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPi-SQLåœ¨æ‰§è¡Œå‡†ç¡®ç‡ä¸Šæ¯”è¡¨ç°æœ€å¥½çš„åŸºçº¿æ¨¡å‹æå‡äº†é«˜è¾¾3.20ï¼Œå¹¶åœ¨åŸºäºå¥–åŠ±çš„æœ‰æ•ˆæ•ˆç‡å¾—åˆ†ä¸Šé«˜å‡º4.55ã€‚è¿™ä¸€æ–¹æ³•æˆåŠŸè¯æ˜äº†åˆ©ç”¨æ¢çº½ç¼–ç¨‹è¯­è¨€å¼•å¯¼å¤æ‚æ•°æ®åº“æŸ¥è¯¢ç”Ÿæˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼¥åˆè‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€ä¹‹é—´çš„é€»è¾‘æ–­å±‚æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00912v2",
      "published_date": "2025-06-01 08:55:47 UTC",
      "updated_date": "2025-06-03 02:29:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:17:19.492443+00:00"
    },
    {
      "arxiv_id": "2506.00911v1",
      "title": "Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models",
      "title_zh": "Conformal Arbitrageï¼šè¯­è¨€æ¨¡å‹ä¸­ç«äº‰ç›®æ ‡çš„é£é™©æ§åˆ¶å¹³è¡¡",
      "authors": [
        "William Overman",
        "Mohsen Bayati"
      ],
      "abstract": "Modern language model deployments must often balance competing objectives, for example, helpfulness versus harmlessness, cost versus accuracy, and reward versus safety. We introduce Conformal Arbitrage, a post hoc framework that learns a data driven threshold to mediate between a Primary model optimized for a primary objective and a more conservative Guardian which could be another model or a human domain expert aligned with a guardrail objective. The threshold is calibrated with conformal risk control, yielding finite sample, distribution free guarantees that the long run frequency of undesirable events, such as factual errors or safety violations, does not exceed a user specified quota. Because Conformal Arbitrage operates wholly at the API level, without requiring access to model logits or updating model weights, it complements weight based alignment techniques and integrates seamlessly with existing cost aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier, allowing users to define an acceptable performance level for one objective while maximizing utility in another. We observe that our method outperforms, in terms of accuracy, cost matched random routing between models. These properties make Conformal Arbitrage a practical, theoretically grounded tool for trustworthy and economical deployment of large language models across a broad range of potentially competing objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Conformal Arbitrageï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¹³è¡¡è¯­è¨€æ¨¡å‹ä¸­ç«äº‰æ€§ç›®æ ‡ï¼ˆå¦‚Helpfulnessä¸Harmlessnessã€æˆæœ¬ä¸å‡†ç¡®æ€§ï¼‰çš„Post hocæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ æ•°æ®é©±åŠ¨çš„é˜ˆå€¼ï¼Œåœ¨ä¼˜åŒ–ä¸»è¦ç›®æ ‡çš„Primaryæ¨¡å‹å’Œéµå¾ªæŠ¤æ ç›®æ ‡çš„Guardianæ¨¡å‹ï¼ˆå¦‚ä¿å®ˆæ¨¡å‹æˆ–äººç±»ä¸“å®¶ï¼‰ä¹‹é—´è¿›è¡Œè‡ªåŠ¨åè°ƒã€‚Conformal Arbitrageåˆ©ç”¨Conformal Risk ControlæŠ€æœ¯è¿›è¡Œæ ¡å‡†ï¼Œæä¾›æœ‰é™æ ·æœ¬ä¸‹çš„åˆ†å¸ƒæ— å…³ä¿è¯ï¼Œç¡®ä¿äº‹å®é”™è¯¯æˆ–å®‰å…¨è¿è§„ç­‰ä¸è‰¯äº‹ä»¶çš„é•¿æœŸå‘ç”Ÿé¢‘ç‡ä¸è¶…è¿‡ç”¨æˆ·æŒ‡å®šçš„é…é¢ã€‚ç”±äºè¯¥æ–¹æ³•å®Œå…¨åœ¨APIå±‚é¢è¿è¡Œï¼Œæ— éœ€è®¿é—®æ¨¡å‹Logitsæˆ–æ›´æ–°æƒé‡ï¼Œå®ƒèƒ½å¤Ÿä¸ç°æœ‰çš„å¯¹é½æŠ€æœ¯äº’è¡¥å¹¶æ— ç¼é›†æˆåˆ°æˆæœ¬æ•æ„Ÿçš„Cascadeç³»ç»Ÿä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæç»˜å‡ºæ€§èƒ½è¾¹ç•ŒFrontierï¼Œåœ¨æœ€å¤§åŒ–ä¸»è¦æ•ˆç”¨çš„åŒæ—¶ä¸¥æ ¼æ§åˆ¶é£é™©ã€‚å…¶å‡†ç¡®æ€§è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ¨¡å‹é—´éšæœºè·¯ç”±æ–¹æ¡ˆï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç›®æ ‡ä¸‹çš„å¯ä¿¡ä¸”ç»æµçš„éƒ¨ç½²æä¾›äº†ä¸€ä¸ªå…·æœ‰ç†è®ºåŸºç¡€çš„å®ç”¨å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00911v1",
      "published_date": "2025-06-01 08:55:10 UTC",
      "updated_date": "2025-06-01 08:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:17:38.169595+00:00"
    },
    {
      "arxiv_id": "2506.00910v2",
      "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models",
      "title_zh": "PCoreSetï¼šåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çŸ¥è¯†è’¸é¦çš„é«˜æ•ˆä¸»åŠ¨å­¦ä¹ ",
      "authors": [
        "Seongjae Kang",
        "Dong Bok Lee",
        "Hyungjoon Jang",
        "Dongseop Kim",
        "Sung Ju Hwang"
      ],
      "abstract": "Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by transferring the knowledge from teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we first introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs-i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects probabilistically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Extensive evaluations on 11 datasets show that ActiveKD consistently improves performance across selection methods (e.g., +29.07% on ImageNet, averaged over methods). Under ActiveKD, PCoreSet ranks first in 64/73 settings (approximately 87.7%) across 5 student and 3 teacher networks, always achieving the best performance except for first 2 AL rounds. Our code is available at https://github.com/erjui/PCoreSet.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†çŸ¥è¯†è’¸é¦(Knowledge distillation)åœ¨ä¸»åŠ¨å­¦ä¹ (Active learning)ä¸­å› ç¼ºä¹ç‰¹å®šä»»åŠ¡æ•™å¸ˆæ¨¡å‹è€Œåº”ç”¨å—é™çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº† ActiveKD æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›å®ç°çŸ¥è¯†è¿ç§»ã€‚ç ”ç©¶è€…å‘ç° VLMs çš„é¢„æµ‹åœ¨æ¦‚ç‡ç©ºé—´ä¸­å…·æœ‰ç»“æ„åŒ–åç½®(structured prediction bias)ï¼Œå¹¶å°†å…¶è§†ä¸ºä¸€ç§æœ‰åˆ©äºå­¦ç”Ÿæ¨¡å‹å­¦ä¹ çš„å½’çº³åç½®(inductive bias)ã€‚åŸºäºæ­¤ï¼Œè¯¥ç ”ç©¶è®¾è®¡äº† Probabilistic CoreSet (PCoreSet) æ ·æœ¬é€‰æ‹©ç­–ç•¥ï¼Œé€šè¿‡æœ€å¤§åŒ–æ¦‚ç‡ç©ºé—´è€Œéç‰¹å¾ç©ºé—´çš„è¦†ç›–èŒƒå›´ï¼Œåœ¨æœ‰é™æ ‡æ³¨é¢„ç®—ä¸‹ç­›é€‰å…·å¤‡æ¦‚ç‡å¤šæ ·æ€§çš„æœªæ ‡æ³¨æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒActiveKD åœ¨ 11 ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œåœ¨ ImageNet ä¸Šçš„å¹³å‡å‡†ç¡®ç‡å¢å¹…è¾¾ 29.07%ã€‚åœ¨æ¶µç›–å¤šç§å­¦ç”Ÿå’Œæ•™å¸ˆç½‘ç»œçš„ 73 é¡¹è®¾ç½®ä¸­ï¼ŒPCoreSet åœ¨çº¦ 87.7% çš„å®éªŒä¸­æ’åç¬¬ä¸€ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡çŸ¥è¯†è¿ç§»æ•ˆç‡å’Œå‡å°‘æ ‡æ³¨æˆæœ¬æ–¹é¢çš„å“è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39 pages, 25 figures, preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.00910v2",
      "published_date": "2025-06-01 08:54:37 UTC",
      "updated_date": "2025-10-01 01:14:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:17:28.643907+00:00"
    },
    {
      "arxiv_id": "2506.02052v2",
      "title": "Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications",
      "title_zh": "Protapï¼šé¢å‘çœŸå®ä¸‹æ¸¸åº”ç”¨çš„è›‹ç™½è´¨å»ºæ¨¡åŸºå‡†",
      "authors": [
        "Shuo Yan",
        "Yuliang Yan",
        "Bin Ma",
        "Chenao Li",
        "Haochun Tang",
        "Jiahua Lu",
        "Minhua Lin",
        "Yuyuan Feng",
        "Hui Xiong",
        "Enyan Dai"
      ],
      "abstract": "Recently, extensive deep learning architectures and pretraining strategies have been explored to support downstream protein applications. Additionally, domain-specific models incorporating biological knowledge have been developed to enhance performance in specialized tasks. In this work, we introduce $\\textbf{Protap}$, a comprehensive benchmark that systematically compares backbone architectures, pretraining strategies, and domain-specific models across diverse and realistic downstream protein applications. Specifically, Protap covers five applications: three general tasks and two novel specialized tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted protein degradation, which are industrially relevant yet missing from existing benchmarks. For each application, Protap compares various domain-specific models and general architectures under multiple pretraining settings. Our empirical studies imply that: (i) Though large-scale pretraining encoders achieve great results, they often underperform supervised encoders trained on small downstream training sets. (ii) Incorporating structural information during downstream fine-tuning can match or even outperform protein language models pretrained on large-scale sequence corpora. (iii) Domain-specific biological priors can enhance performance on specialized downstream tasks. Code and datasets are publicly available at https://github.com/Trust-App-AI-Lab/protap.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Protapï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ¯”è¾ƒä¸åŒéª¨å¹²æ¶æ„ (backbone architectures)ã€é¢„è®­ç»ƒç­–ç•¥ä»¥åŠé¢†åŸŸç‰¹å®šæ¨¡å‹åœ¨å„ç§çœŸå®è›‹ç™½è´¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„è¡¨ç°ã€‚Protap æ¶µç›–äº†äº”é¡¹åº”ç”¨ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé€šç”¨ä»»åŠ¡å’Œä¸¤ä¸ªå…·æœ‰å·¥ä¸šç›¸å…³æ€§ä½†åœ¨ç°æœ‰åŸºå‡†ä¸­ç¼ºå¤±çš„æ–°å‹ä¸“ä¸šä»»åŠ¡ï¼šé…¶å‚¬åŒ–è›‹ç™½è´¨åˆ‡å‰²ä½ç‚¹é¢„æµ‹ (enzyme-catalyzed protein cleavage site prediction) å’Œé¶å‘è›‹ç™½è´¨é™è§£ (targeted protein degradation)ã€‚é€šè¿‡åœ¨å¤šç§é¢„è®­ç»ƒè®¾ç½®ä¸‹å¯¹ä¸åŒæ¶æ„è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œè¯¥å›¢é˜Ÿå‘ç°å¤§è§„æ¨¡é¢„è®­ç»ƒç¼–ç å™¨åœ¨å°å‹ä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰æ—¶é€Šäºç›‘ç£å­¦ä¹ ç¼–ç å™¨ã€‚æ­¤å¤–ï¼Œåœ¨ä¸‹æ¸¸å¾®è°ƒ (fine-tuning) è¿‡ç¨‹ä¸­æ•´åˆç»“æ„ä¿¡æ¯ï¼Œå…¶æ•ˆæœå¯ä»¥åª²ç¾ç”šè‡³è¶…è¶Šåœ¨å¤§è§„æ¨¡åºåˆ—è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ã€‚å®éªŒè¿˜è¯æ˜äº†å¼•å…¥é¢†åŸŸç‰¹å®šçš„ç”Ÿç‰©å…ˆéªŒçŸ¥è¯† (biological priors) èƒ½å¤Ÿæ˜¾è‘—æå‡åœ¨ç‰¹å®šä¸“ä¸šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºè›‹ç™½è´¨å»ºæ¨¡é¢†åŸŸæä¾›äº†æ›´å…·å·¥ä¸šå®ç”¨æ€§çš„è¯„ä»·æ ‡å‡†å’Œå®è¯æŒ‡å¯¼ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02052v2",
      "published_date": "2025-06-01 08:48:42 UTC",
      "updated_date": "2025-06-07 04:13:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:03.235816+00:00"
    },
    {
      "arxiv_id": "2506.00895v3",
      "title": "State-Covering Trajectory Stitching for Diffusion Planners",
      "title_zh": "é¢å‘æ‰©æ•£è§„åˆ’å™¨çš„çŠ¶æ€è¦†ç›–è½¨è¿¹æ‹¼æ¥",
      "authors": [
        "Kyowoon Lee",
        "Jaesik Choi"
      ],
      "abstract": "Diffusion-based generative models are emerging as powerful tools for long-horizon planning in reinforcement learning (RL), particularly with offline datasets. However, their performance is fundamentally limited by the quality and diversity of training data. This often restricts their generalization to tasks outside their training distribution or longer planning horizons. To overcome this challenge, we propose State-Covering Trajectory Stitching (SCoTS), a novel reward-free trajectory augmentation method that incrementally stitches together short trajectory segments, systematically generating diverse and extended trajectories. SCoTS first learns a temporal distance-preserving latent representation that captures the underlying temporal structure of the environment, then iteratively stitches trajectory segments guided by directional exploration and novelty to effectively cover and expand this latent space. We demonstrate that SCoTS significantly improves the performance and generalization capabilities of diffusion planners on offline goal-conditioned benchmarks requiring stitching and long-horizon reasoning. Furthermore, augmented trajectories generated by SCoTS significantly improve the performance of widely used offline goal-conditioned RL algorithms across diverse environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäº Diffusion çš„ç”Ÿæˆæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é•¿æ—¶ç¨‹è§„åˆ’ä¸­å—é™äºç¦»çº¿æ•°æ®é›†è´¨é‡ä¸å¤šæ ·æ€§çš„é—®é¢˜ï¼Œæå‡ºäº† State-Covering Trajectory Stitchingï¼ˆSCoTSï¼‰ï¼Œä¸€ç§æ— éœ€å¥–åŠ±å‡½æ•°çš„è½¨è¿¹å¢å¼ºæ–¹æ³•ã€‚SCoTS é¦–å…ˆå­¦ä¹ ä¸€ç§èƒ½å¤Ÿæ•è·ç¯å¢ƒåº•å±‚æ—¶é—´ç»“æ„çš„æ—¶é—´è·ç¦»ä¿æŒéšè¡¨å¾ï¼ˆtemporal distance-preserving latent representationï¼‰ï¼Œéšååœ¨å®šå‘æ¢ç´¢å’Œæ–°é¢–æ€§å¼•å¯¼ä¸‹ï¼Œé€šè¿‡è¿­ä»£åœ°æ‹¼æ¥çŸ­è½¨è¿¹ç‰‡æ®µæ¥ç³»ç»Ÿæ€§åœ°è¦†ç›–å¹¶æ‰©å±•è¯¥éšç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCoTS æ˜¾è‘—æå‡äº† Diffusion Planners åœ¨éœ€è¦è½¨è¿¹æ‹¼æ¥å’Œé•¿æ—¶ç¨‹æ¨ç†çš„ç¦»çº¿ç›®æ ‡æ¡ä»¶åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç”± SCoTS ç”Ÿæˆçš„å¢å¼ºè½¨è¿¹ä¹Ÿèƒ½æ˜¾è‘—æ”¹å–„å¤šç§ä¸»æµç¦»çº¿ç›®æ ‡æ¡ä»¶ RL ç®—æ³•åœ¨ä¸åŒç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00895v3",
      "published_date": "2025-06-01 08:32:22 UTC",
      "updated_date": "2025-10-12 16:27:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:30.784268+00:00"
    },
    {
      "arxiv_id": "2506.00894v1",
      "title": "CODEMENV: Benchmarking Large Language Models on Code Migration",
      "title_zh": "CODEMENVï¼šå¤§è¯­è¨€æ¨¡å‹ä»£ç è¿ç§»è¯„æµ‹åŸºå‡†",
      "authors": [
        "Keyuan Cheng",
        "Xudong Shen",
        "Yihao Yang",
        "Tengyue Wang",
        "Yang Cao",
        "Muhammad Asif Ali",
        "Hanbin Wang",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable capabilities across various software engineering tasks; however, their effectiveness in code migration, adapting code to run in different environments, remains insufficiently studied. In this work, we introduce CODEMENV: Code Migration Across Environment, a new benchmark specifically designed to assess LLMs' abilities in code migration scenarios. CODEMENV consists of 922 examples spanning 19 Python and Java packages, and covers three core tasks: (1) identifying functions incompatible with specific versions, (2) detecting changes in function definitions, and (3) adapting code to target environments. Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include: (i) LLMs tend to be more proficient with newer function versions, which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical inconsistencies by identifying function changes irrelevant to the intended migration environment. The datasets are available at https://github.com/xdshen-ai/Benchmark-of-Code-Migration.",
      "tldr_zh": "å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šç§è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨ä»£ç è¿ç§»(Code Migration)â€”â€”å³é€‚é…ä»£ç ä»¥è¿è¡Œåœ¨ä¸åŒç¯å¢ƒä¸­çš„èƒ½åŠ›ä»ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†CODEMENVï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«922ä¸ªç¤ºä¾‹ã€æ¶µç›–19ä¸ªPythonå’ŒJavaè½¯ä»¶åŒ…çš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨ä»£ç è¿ç§»åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ¶µç›–äº†è¯†åˆ«ç‰ˆæœ¬ä¸å…¼å®¹å‡½æ•°ã€æ£€æµ‹å‡½æ•°å®šä¹‰å˜åŒ–ä»¥åŠä»£ç é€‚é…ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚å¯¹ä¸ƒç§LLMsçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œå¹³å‡pass@1ç‡ä»…ä¸º26.50%ï¼Œå…¶ä¸­GPT-4oä»¥43.84%çš„æˆç»©è¡¨ç°æœ€ä½³ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMså¯¹è¾ƒæ–°ç‰ˆæœ¬çš„å‡½æ•°æ›´ä¸ºç²¾é€šï¼Œè¿™æœ‰åŠ©äºé—ç•™ä»£ç (Legacy Code)çš„è¿ç§»ï¼Œä½†æ¨¡å‹åœ¨è¯†åˆ«ä¸ç›®æ ‡ç¯å¢ƒæ— å…³çš„å˜åŒ–æ—¶ä¼šè¡¨ç°å‡ºé€»è¾‘ä¸ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶çš„å‘å¸ƒä¸ºæ¨åŠ¨LLMsåœ¨å¤æ‚ç¯å¢ƒé€‚é…é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.00894v1",
      "published_date": "2025-06-01 08:29:59 UTC",
      "updated_date": "2025-06-01 08:29:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:37.889192+00:00"
    },
    {
      "arxiv_id": "2506.00893v2",
      "title": "Affordance Benchmark for MLLMs",
      "title_zh": "MLLMs ç¤ºèƒ½æ€§è¯„æµ‹åŸºå‡†",
      "authors": [
        "Junying Wang",
        "Wenzhe Li",
        "Yalun Wu",
        "Yingji Liang",
        "Yijin Guo",
        "Chunyi Li",
        "Haodong Duan",
        "Zicheng Zhang",
        "Guangtao Zhai"
      ],
      "abstract": "Affordance theory suggests that environments inherently provide action possibilities shaping perception and behavior. While Multimodal Large Language Models (MLLMs) achieve strong performance in vision-language tasks, their ability to perceive affordance, which is crucial for intuitive and safe interactions, remains underexplored. To address this, we introduce **A4Bench**, a novel benchmark designed to evaluate the affordance perception abilities of MLLMs across two dimensions: 1) Constitutive Affordance, assessing understanding of inherent object properties through 1,282 questionanswer pairs spanning nine sub-disciplines, and 2) Transformative Affordance, probing dynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or individual-specific affordance) with 718 challenging question-answer pairs. We evaluate 17 MLLMs (nine proprietary and eight open-source) and compare them to human performance. Results show that proprietary models generally outperform open-source ones, yet all models perform far below humans, especially in transformative affordance. Furthermore, even top-performing models, such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind human performance (best: 85.34%, worst: 81.25%). These findings highlight critical gaps in environmental understanding of MLLMs and provide a foundation for advancing AI systems toward more robust, context-aware interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨æ„ŸçŸ¥ç¯å¢ƒè¡ŒåŠ¨å¯èƒ½æ€§ï¼ˆå³ Affordanceï¼‰æ–¹é¢çš„èƒ½åŠ›ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨äº¤äº’ä¸­çš„ç›´è§‰ä¸å®‰å…¨æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº† A4Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºä» Constitutive Affordance å’Œ Transformative Affordance ä¸¤ä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚Constitutive Affordance ç»´åº¦é€šè¿‡ 1,282 ä¸ªé—®ç­”å¯¹è¯„ä¼°æ¨¡å‹å¯¹ç‰©ä½“å›ºæœ‰å±æ€§çš„ç†è§£ï¼Œè€Œ Transformative Affordance ç»´åº¦åˆ™é€šè¿‡ 718 ä¸ªå¤æ‚é—®ç­”å¯¹æ¢æµ‹è¯¯å¯¼æ€§ã€æ—¶é—´ç›¸å…³æˆ–æ–‡åŒ–ç‰¹å®šç­‰åŠ¨æ€è¯­å¢ƒä¸‹çš„ç»†å¾®å·®åˆ«ã€‚å®éªŒå¯¹ 17 ä¸ª MLLMs çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å•†ä¸šæ¨¡å‹çš„è¡¨ç°æ™®éä¼˜äºå¼€æºæ¨¡å‹ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€è¯­å¢ƒæ—¶çš„è¡¨ç°å‡è¿œä½äºäººç±»æ°´å¹³ã€‚å³ä¾¿è¡¨ç°æœ€ä¼˜çš„ Gemini-2.0-Pro æ¨¡å‹ï¼Œå…¶ 18.05% çš„å‡†ç¡®ç‡ä¹Ÿæ˜¾è‘—è½åäºäººç±» 81.25% ä»¥ä¸Šçš„è¡¨ç°ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰ MLLMs åœ¨ç¯å¢ƒç†è§£æ–¹é¢çš„å…³é”®ç¼ºé™·ï¼Œä¸ºæœªæ¥å¼€å‘å…·å¤‡å¼ºé²æ£’æ€§å’Œè¯­å¢ƒæ„ŸçŸ¥èƒ½åŠ›çš„ AI ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç ”ç©¶åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00893v2",
      "published_date": "2025-06-01 08:26:34 UTC",
      "updated_date": "2025-08-02 10:06:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:43.087822+00:00"
    },
    {
      "arxiv_id": "2506.00891v2",
      "title": "Uneven Event Modeling for Partially Relevant Video Retrieval",
      "title_zh": "é¢å‘éƒ¨åˆ†ç›¸å…³è§†é¢‘æ£€ç´¢çš„éå‡åŒ€äº‹ä»¶å»ºæ¨¡",
      "authors": [
        "Sa Zhu",
        "Huashan Chen",
        "Wanqian Zhang",
        "Jinchao Zhang",
        "Zexian Yang",
        "Xiaoshuai Hao",
        "Bo Li"
      ],
      "abstract": "Given a text query, partially relevant video retrieval (PRVR) aims to retrieve untrimmed videos containing relevant moments, wherein event modeling is crucial for partitioning the video into smaller temporal events that partially correspond to the text. Previous methods typically segment videos into a fixed number of equal-length clips, resulting in ambiguous event boundaries. Additionally, they rely on mean pooling to compute event representations, inevitably introducing undesired misalignment. To address these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first introduce the Progressive-Grouped Video Segmentation (PGVS) module, to iteratively formulate events in light of both temporal dependencies and semantic similarity between consecutive frames, enabling clear event boundaries. Furthermore, we also propose the Context-Aware Event Refinement (CAER) module to refine the event representation conditioned the text's cross-attention. This enables event representations to focus on the most relevant frames for a given text, facilitating more precise text-video alignment. Extensive experiments demonstrate that our method achieves state-of-the-art performance on two PRVR benchmarks. Code is available at https://github.com/Sasa77777779/UEM.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éƒ¨åˆ†ç›¸å…³è§†é¢‘æ£€ç´¢(Partially Relevant Video Retrieval, PRVR)ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€æ–¹æ³•å› å›ºå®šé•¿åº¦åˆ†æ®µå¯¼è‡´çš„äº‹ä»¶è¾¹ç•Œæ¨¡ç³ŠåŠå¹³å‡æ± åŒ–å¼•èµ·çš„å¯¹é½åå·®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§éå‡åŒ€äº‹ä»¶å»ºæ¨¡(Uneven Event Modeling, UEM)æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ¸è¿›å¼åˆ†ç»„è§†é¢‘åˆ†å‰²(Progressive-Grouped Video Segmentation, PGVS)æ¨¡å—ï¼Œç»“åˆæ—¶åºä¾èµ–å’Œè¯­ä¹‰ç›¸ä¼¼æ€§æ¥è¿­ä»£æ„å»ºæ¸…æ™°çš„äº‹ä»¶è¾¹ç•Œã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥äº‹ä»¶ç²¾ç»†åŒ–(Context-Aware Event Refinement, CAER)æ¨¡å—ï¼Œé€šè¿‡æ–‡æœ¬äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä½¿äº‹ä»¶è¡¨ç¤ºèšç„¦äºæœ€ç›¸å…³çš„å¸§ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„æ–‡æœ¬-è§†é¢‘å¯¹é½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªPRVRåŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½(State-of-the-art)ï¼Œæœ‰æ•ˆæå‡äº†é•¿è§†é¢‘ä¸­å±€éƒ¨ç›¸å…³å†…å®¹çš„æ£€ç´¢æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICME 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00891v2",
      "published_date": "2025-06-01 08:21:45 UTC",
      "updated_date": "2025-06-03 03:11:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:44.086672+00:00"
    },
    {
      "arxiv_id": "2506.00886v1",
      "title": "Toward a Theory of Agents as Tool-Use Decision-Makers",
      "title_zh": "è¿ˆå‘å°†æ™ºèƒ½ä½“è§†ä¸ºå·¥å…·ä½¿ç”¨å†³ç­–è€…çš„ç†è®º",
      "authors": [
        "Hongru Wang",
        "Cheng Qian",
        "Manling Li",
        "Jiahao Qiu",
        "Boyang Xue",
        "Mengdi Wang",
        "Heng Ji",
        "Kam-Fai Wong"
      ],
      "abstract": "As Large Language Models (LLMs) evolve into increasingly autonomous agents, fundamental questions about their epistemic foundations remain unresolved: What defines an agent? How should it make decisions? And what objectives should guide its behavior? In this position paper, we argue that true autonomy requires agents to be grounded in a coherent epistemic framework that governs what they know, what they need to know, and how to acquire that knowledge efficiently. We propose a unified theory that treats internal reasoning and external actions as equivalent epistemic tools, enabling agents to systematically coordinate introspection and interaction. Building on this framework, we advocate for aligning an agent's tool use decision-making boundary with its knowledge boundary, thereby minimizing unnecessary tool use and maximizing epistemic efficiency. This perspective shifts the design of agents from mere action executors to knowledge-driven intelligence systems, offering a principled path toward building foundation agents capable of adaptive, efficient, and goal-directed behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ¼”å˜ä¸ºè‡ªä¸»æ™ºèƒ½ä½“è¿‡ç¨‹ä¸­é¢ä¸´çš„è®¤çŸ¥åŸºç¡€ä¸æ˜ç¡®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†æ™ºèƒ½ä½“è§†ä¸ºå·¥å…·ä½¿ç”¨å†³ç­–è€…çš„ç»Ÿä¸€ç†è®ºæ¡†æ¶ã€‚è¯¥è®ºæ–‡è®¤ä¸ºï¼ŒçœŸæ­£çš„è‡ªä¸»æ€§éœ€è¦æ™ºèƒ½ä½“æ ¹æ¤äºä¸€è‡´çš„è®¤è¯†è®ºæ¡†æ¶ (epistemic framework)ï¼Œä»¥ç®¡ç†å…¶å·²çŸ¥çŸ¥è¯†ã€çŸ¥è¯†éœ€æ±‚åŠè·å–çŸ¥è¯†çš„æ•ˆç‡ã€‚ä½œè€…æå‡ºå°†å†…éƒ¨æ¨ç† (internal reasoning) ä¸å¤–éƒ¨è¡ŒåŠ¨ (external actions) è§†ä¸ºç­‰æ•ˆçš„è®¤è¯†è®ºå·¥å…· (epistemic tools)ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿç³»ç»Ÿåœ°åè°ƒå†…çœä¸äº¤äº’è¿‡ç¨‹ã€‚é€šè¿‡å€¡å¯¼å°†æ™ºèƒ½ä½“çš„å·¥å…·ä½¿ç”¨å†³ç­–è¾¹ç•Œä¸å…¶çŸ¥è¯†è¾¹ç•Œå¯¹é½ï¼Œè¯¥ç†è®ºæ—¨åœ¨æœ€å°åŒ–ä¸å¿…è¦çš„å·¥å…·è°ƒç”¨å¹¶æœ€å¤§åŒ–è®¤è¯†è®ºæ•ˆç‡ (epistemic efficiency)ã€‚è¿™ä¸€è§†è§’å°†æ™ºèƒ½ä½“çš„è®¾è®¡é‡ç‚¹ä»å•çº¯çš„åŠ¨ä½œæ‰§è¡Œè€…è½¬å‘çŸ¥è¯†é©±åŠ¨çš„æ™ºèƒ½ç³»ç»Ÿï¼Œä¸ºæ„å»ºå…·å¤‡è‡ªé€‚åº”å’Œé«˜æ•ˆç›®æ ‡å¯¼å‘è¡Œä¸ºçš„åŸºç¡€æ™ºèƒ½ä½“ (foundation agents) æä¾›äº†åŸåˆ™æ€§çš„ç†è®ºè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00886v1",
      "published_date": "2025-06-01 07:52:16 UTC",
      "updated_date": "2025-06-01 07:52:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:34.196108+00:00"
    },
    {
      "arxiv_id": "2506.00885v2",
      "title": "CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching",
      "title_zh": "CoVoMix2ï¼šåˆ©ç”¨å…¨éè‡ªå›å½’æµåŒ¹é…æ¨è¿›é›¶æ ·æœ¬å¯¹è¯ç”Ÿæˆ",
      "authors": [
        "Leying Zhang",
        "Yao Qian",
        "Xiaofei Wang",
        "Manthan Thakker",
        "Dongmei Wang",
        "Jianwei Yu",
        "Haibin Wu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yanmin Qian",
        "Sheng Zhao"
      ],
      "abstract": "Generating natural-sounding, multi-speaker dialogue is crucial for applications such as podcast creation, virtual agents, and multimedia content generation. However, existing systems struggle to maintain speaker consistency, model overlapping speech, and synthesize coherent conversations efficiently. In this paper, we introduce CoVoMix2, a fully non-autoregressive framework for zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts mel-spectrograms from multi-stream transcriptions using a flow-matching-based generative model, eliminating the reliance on intermediate token representations. To better capture realistic conversational dynamics, we propose transcription-level speaker disentanglement, sentence-level alignment, and prompt-level random masking strategies. Our approach achieves state-of-the-art performance, outperforming strong baselines like MoonCast and Sesame in speech quality, speaker consistency, and inference speed. Notably, CoVoMix2 operates without requiring transcriptions for the prompt and supports controllable dialogue generation, including overlapping speech and precise timing control, demonstrating strong generalizability to real-world speech generation scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè¯´è¯äººå¯¹è¯ç”Ÿæˆä¸­å­˜åœ¨çš„è¯´è¯äººä¸€è‡´æ€§ã€é‡å è¯­éŸ³å»ºæ¨¡åŠåˆæˆæ•ˆç‡ä½ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†CoVoMix2æ¡†æ¶ã€‚CoVoMix2é‡‡ç”¨å®Œå…¨éè‡ªå›å½’(Fully Non-Autoregressive)çš„æµåŒ¹é…(Flow Matching)æ¶æ„ï¼Œç›´æ¥ä»å¤šæµè½¬å½•æ–‡æœ¬é¢„æµ‹æ¢…å°”é¢‘è°±(Mel-spectrograms)ï¼Œä»è€Œæ‘†è„±äº†å¯¹ä¸­é—´ç‰¹å¾è¡¨ç¤º(Intermediate Token Representations)çš„ä¾èµ–ã€‚ä¸ºäº†æ›´å¥½åœ°æ•æ‰çœŸå®çš„å¯¹è¯åŠ¨æ€ï¼Œç ”ç©¶è€…æå‡ºäº†è½¬å½•å±‚çº§çš„è¯´è¯äººè§£è€¦(Speaker Disentanglement)ã€å¥å­çº§å¯¹é½(Sentence-level Alignment)ä»¥åŠæç¤ºè¯å±‚çº§çš„éšæœºæ©ç (Prompt-level Random Masking)ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoMix2åœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯äººä¸€è‡´æ€§å’Œæ¨ç†é€Ÿåº¦æ–¹é¢å‡ä¼˜äºMoonCastå’ŒSesameç­‰åŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒåŒ…æ‹¬é‡å è¯­éŸ³(Overlapping Speech)å’Œç²¾ç¡®æ—¶é—´æ§åˆ¶åœ¨å†…çš„å¯æ§å¯¹è¯ç”Ÿæˆï¼Œä¸”åœ¨æç¤ºé˜¶æ®µæ— éœ€è½¬å½•æ–‡æœ¬ï¼Œå±•ç°å‡ºåœ¨ç°å®è¯­éŸ³ç”Ÿæˆåœºæ™¯ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Neural Information Processing Systems 2025, poster",
      "pdf_url": "https://arxiv.org/pdf/2506.00885v2",
      "published_date": "2025-06-01 07:51:45 UTC",
      "updated_date": "2025-10-18 21:54:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:47.294715+00:00"
    },
    {
      "arxiv_id": "2506.04252v1",
      "title": "A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy",
      "title_zh": "å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶èµ‹èƒ½å¾ªç¯ç»æµå†³ç­–",
      "authors": [
        "Yang Zhao",
        "Chengxiao Dai",
        "Dusit Niyato",
        "Chuan Fu Tan",
        "Keyi Xiang",
        "Yueyang Wang",
        "Zhiquan Yeo",
        "Daren Tan Zong Loong",
        "Jonathan Low Zhaozhi",
        "Eugene H. Z. HO"
      ],
      "abstract": "Large language models (LLMs) hold promise for sustainable manufacturing, but often hallucinate industrial codes and emission factors, undermining regulatory and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented generation (RAG) framework that grounds LLMs outputs in a domain-specific knowledge graph for the circular economy. This graph connects 117,380 industrial and waste entities with classification codes and GWP100 emission data, enabling structured multi-hop reasoning. Natural language queries are translated into SPARQL and verified subgraphs are retrieved to ensure accuracy and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG achieves superior performance in single-hop and multi-hop question answering, with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also improves efficiency, halving the response time and reducing token usage by 16% in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready support for circular economy planning, advancing reliable, low-carbon resource decision making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CircuGraphRAGï¼Œä¸€ç§é’ˆå¯¹å¾ªç¯ç»æµ (Circular Economy) é¢†åŸŸçš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ (GraphRAG) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å·¥ä¸šä»£ç å’Œæ’æ”¾å› å­æ–¹é¢å­˜åœ¨çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŒ…å« 117,380 ä¸ªå·¥ä¸šåŠåºŸç‰©å®ä½“çš„é¢†åŸŸä¸“ç”¨çŸ¥è¯†å›¾è°±ï¼Œé€šè¿‡å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸º SPARQL è¯­å¥æ¥æ£€ç´¢ç»è¿‡éªŒè¯çš„å­å›¾ï¼Œä»è€Œå®ç°ç»“æ„åŒ–çš„å¤šè·³æ¨ç† (Multi-hop Reasoning)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCircuGraphRAG åœ¨å•è·³å’Œå¤šè·³é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ ROUGE-L F1 åˆ†æ•°æœ€é«˜è¾¾åˆ° 1.0ï¼Œè¿œè¶…è¡¨ç°ä½äº 0.08 çš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­å°†å“åº”æ—¶é—´ç¼©çŸ­äº†ä¸€åŠï¼Œå¹¶å‡å°‘äº† 16% çš„ Token ä½¿ç”¨é‡ã€‚CircuGraphRAG ä¸ºå¾ªç¯ç»æµè§„åˆ’æä¾›äº†ç»è¿‡äº‹å®æ ¸æŸ¥ä¸”ç¬¦åˆç›‘ç®¡è¦æ±‚çš„å†³ç­–æ”¯æŒï¼Œæœ‰æ•ˆæ¨è¿›äº†å¯é çš„ä½ç¢³èµ„æºå†³ç­–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.04252v1",
      "published_date": "2025-06-01 07:49:47 UTC",
      "updated_date": "2025-06-01 07:49:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:57.876383+00:00"
    },
    {
      "arxiv_id": "2506.02051v1",
      "title": "Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders",
      "title_zh": "åŸºäºåŒé€šé“å˜åˆ†è‡ªç¼–ç å™¨çš„è¡¨å‹è°±å¼•å¯¼ç±»è¯åˆ†å­ç”Ÿæˆ",
      "authors": [
        "Hui Liu",
        "Shiye Tian",
        "Xuejun Liu"
      ],
      "abstract": "The de novo generation of drug-like molecules capable of inducing desirable phenotypic changes is receiving increasing attention. However, previous methods predominantly rely on expression profiles to guide molecule generation, but overlook the perturbative effect of the molecules on cellular contexts. To overcome this limitation, we propose SmilesGEN, a novel generative model based on variational autoencoder (VAE) architecture to generate molecules with potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE (SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the interplay between drug perturbations and transcriptional responses in a common latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment expression profiles when eliminating drug-induced perturbations in the latent space, while SmilesNet is informed by desired expression profiles to generate drug-like molecules. Our empirical experiments demonstrate that SmilesGEN outperforms current state-of-the-art models in generating molecules with higher degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity to known ligands targeting the relevant proteins. Moreover, we evaluate SmilesGEN for scaffold-based molecule optimization and generation of therapeutic agents, and confirmed its superior performance in generating molecules with higher similarity to approved drugs. SmilesGEN establishes a robust framework that leverages gene signatures to generate drug-like molecules that hold promising potential to induce desirable cellular phenotypic changes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SmilesGENï¼Œä¸€ç§åŸºäºåŒé€šé“å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoder) æ¶æ„çš„æ–°é¢–ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è¡¨å‹è°±ä¿¡æ¯å¼•å¯¼ç±»è¯åˆ†å­çš„ä»å¤´ç”Ÿæˆã€‚ä¸ºäº†è§£å†³ä»¥å¾€æ–¹æ³•å¿½è§†åˆ†å­å¯¹ç»†èƒç¯å¢ƒæ‰°åŠ¨æ•ˆåº”çš„é—®é¢˜ï¼ŒSmilesGEN æ•´åˆäº†é¢„è®­ç»ƒçš„è¯ç‰© VAE (SmilesNet) å’Œè¡¨è¾¾è°± VAE (ProfileNet)ï¼Œåœ¨ç»Ÿä¸€çš„éšç©ºé—´å†…å…±åŒå»ºæ¨¡è¯ç‰©æ‰°åŠ¨ä¸è½¬å½•å“åº”çš„äº¤äº’ä½œç”¨ã€‚å…¶ä¸­ï¼ŒProfileNet è´Ÿè´£åœ¨æ¶ˆé™¤æ‰°åŠ¨åé‡å»ºå¤„ç†å‰çš„è¡¨è¾¾è°±ï¼Œè€Œ SmilesNet åˆ™æ ¹æ®ç›®æ ‡è¡¨è¾¾è°±ç”Ÿæˆå€™é€‰åˆ†å­ã€‚å®éªŒè¯æ˜ï¼ŒSmilesGEN åœ¨åˆ†å­çš„æœ‰æ•ˆæ€§ (Validity)ã€å”¯ä¸€æ€§ (Uniqueness) å’Œæ–°é¢–æ€§ (Novelty) ç­‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨åŸºäºæ”¯æ¶çš„åˆ†å­ä¼˜åŒ–å’Œæ²»ç–—è¯ç‰©ç”Ÿæˆä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”Ÿæˆçš„åˆ†å­ä¸å·²çŸ¥é…ä½“å’Œå·²è·æ‰¹è¯ç‰©å…·æœ‰æé«˜çš„ Tanimoto ç›¸ä¼¼æ€§ã€‚è¯¥æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªåˆ©ç”¨åŸºå› ç‰¹å¾ç”Ÿæˆå…·æœ‰ç†æƒ³ç»†èƒè¡¨å‹è¯±å¯¼èƒ½åŠ›çš„ç±»è¯åˆ†å­çš„ç¨³å¥ä½“ç³»ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "IJCAI2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02051v1",
      "published_date": "2025-06-01 07:46:39 UTC",
      "updated_date": "2025-06-01 07:46:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:18:53.760051+00:00"
    },
    {
      "arxiv_id": "2506.00880v1",
      "title": "ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models",
      "title_zh": "ModuLMï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¨¡å—åŒ–ä¸å¤šæ¨¡æ€åˆ†å­å…³ç³»å­¦ä¹ ",
      "authors": [
        "Zhuo Chen",
        "Yizhen Zheng",
        "Huan Yee Koh",
        "Hongxin Xiang",
        "Linjiang Chen",
        "Wenjie Du",
        "Yang Wang"
      ],
      "abstract": "Molecular Relational Learning (MRL) aims to understand interactions between molecular pairs, playing a critical role in advancing biochemical research. With the recent development of large language models (LLMs), a growing number of studies have explored the integration of MRL with LLMs and achieved promising results. However, the increasing availability of diverse LLMs and molecular structure encoders has significantly expanded the model space, presenting major challenges for benchmarking. Currently, there is no LLM framework that supports both flexible molecular input formats and dynamic architectural switching. To address these challenges, reduce redundant coding, and ensure fair model comparison, we propose ModuLM, a framework designed to support flexible LLM-based model construction and diverse molecular representations. ModuLM provides a rich suite of modular components, including 8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing to its highly flexible model assembly mechanism, ModuLM enables the dynamic construction of over 50,000 distinct model configurations. In addition, we provide comprehensive results to demonstrate the effectiveness of ModuLM in supporting LLM-based MRL tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ModuLMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ”¯æŒçµæ´»çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨¡å‹æ„å»ºå’Œå¤šæ ·åŒ–åˆ†å­è¡¨ç¤ºçš„æ¡†æ¶ã€‚é’ˆå¯¹åˆ†å­å…³ç³»å­¦ä¹ (Molecular Relational Learning, MRL)åœ¨åŸºå‡†æµ‹è¯•ä¸­é¢ä¸´çš„æ¨¡å‹ç©ºé—´å·¨å¤§ã€ç¼ºä¹çµæ´»è¾“å…¥æ ¼å¼å’ŒåŠ¨æ€æ¶æ„åˆ‡æ¢ç­‰æŒ‘æˆ˜ï¼ŒModuLMæä¾›äº†ä¸°å¯Œçš„æ¨¡å—åŒ–ç»„ä»¶ã€‚è¿™äº›ç»„ä»¶æ¶µç›–äº†8ç§2Dåˆ†å­å›¾ç¼–ç å™¨(molecular graph encoders)ã€11ç§3Dåˆ†å­æ„è±¡ç¼–ç å™¨(molecular conformation encoders)ã€7ç§äº¤äº’å±‚ä»¥åŠ7ç§ä¸»æµçš„LLMéª¨å¹²ç½‘ç»œã€‚å‡­å€Ÿå…¶é«˜åº¦çµæ´»çš„æ¨¡å‹ç»„è£…æœºåˆ¶ï¼ŒModuLMèƒ½å¤ŸåŠ¨æ€æ„å»ºè¶…è¿‡50,000ç§ä¸åŒçš„æ¨¡å‹é…ç½®ï¼Œæ˜¾è‘—å‡å°‘äº†å†—ä½™ç¼–ç å¹¶ç¡®ä¿äº†æ¨¡å‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†ModuLMåœ¨æ”¯æŒåŸºäºLLMçš„MRLä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ¨è¿›ç”Ÿç‰©åŒ–å­¦é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ¨¡å—åŒ–å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00880v1",
      "published_date": "2025-06-01 07:44:16 UTC",
      "updated_date": "2025-06-01 07:44:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:08.521627+00:00"
    },
    {
      "arxiv_id": "2506.14796v1",
      "title": "PFMBench: Protein Foundation Model Benchmark",
      "title_zh": "PFMBenchï¼šè›‹ç™½è´¨åŸºåº§æ¨¡å‹è¯„æµ‹åŸºå‡†",
      "authors": [
        "Zhangyang Gao",
        "Hao Wang",
        "Cheng Tan",
        "Chenrui Xu",
        "Mengdi Liu",
        "Bozhen Hu",
        "Linlin Chao",
        "Xiaoming Zhang",
        "Stan Z. Li"
      ],
      "abstract": "This study investigates the current landscape and future directions of protein foundation model research. While recent advancements have transformed protein science and engineering, the field lacks a comprehensive benchmark for fair evaluation and in-depth understanding. Since ESM-1B, numerous protein foundation models have emerged, each with unique datasets and methodologies. However, evaluations often focus on limited tasks tailored to specific models, hindering insights into broader generalization and limitations. Specifically, researchers struggle to understand the relationships between tasks, assess how well current models perform across them, and determine the criteria in developing new foundation models. To fill this gap, we present PFMBench, a comprehensive benchmark evaluating protein foundation models across 38 tasks spanning 8 key areas of protein science. Through hundreds of experiments on 17 state-of-the-art models across 38 tasks, PFMBench reveals the inherent correlations between tasks, identifies top-performing models, and provides a streamlined evaluation protocol. Code is available at \\href{https://github.com/biomap-research/PFMBench}{\\textcolor{blue}{GitHub}}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è›‹ç™½è´¨åŸºç¡€æ¨¡å‹ (Protein Foundation Models) é¢†åŸŸç”±äºæ•°æ®é›†å’Œæ–¹æ³•è®ºå¤šæ ·åŒ–è€Œç¼ºä¹å…¬æ­£ã€å…¨é¢åŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œæå‡ºäº† PFMBench è¯„ä¼°æ¡†æ¶ã€‚PFMBench æ¶µç›–äº†è›‹ç™½è´¨ç§‘å­¦ 8 ä¸ªå…³é”®é¢†åŸŸçš„ 38 é¡¹ä»»åŠ¡ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ­ç¤ºä¸åŒä»»åŠ¡é—´çš„å†…åœ¨å…³è”å¹¶è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹ 17 ä¸ªæœ€å…ˆè¿›çš„ (State-of-the-art) æ¨¡å‹è¿›è¡Œæ•°ç™¾æ¬¡å®éªŒï¼Œè¯¥ç ”ç©¶è¯†åˆ«å‡ºäº†å½“å‰æ€§èƒ½æœ€ä¼˜çš„æ¨¡å‹ï¼Œå¹¶åˆ†æäº†æ¨¡å‹åœ¨ä¸åŒè›‹ç™½è´¨ç§‘å­¦ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä¸ºæœªæ¥å¼€å‘å’Œä¼˜åŒ–æ–°çš„è›‹ç™½è´¨åŸºç¡€æ¨¡å‹æä¾›äº†é‡è¦å‡†åˆ™ï¼Œå…¶ç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14796v1",
      "published_date": "2025-06-01 07:40:07 UTC",
      "updated_date": "2025-06-01 07:40:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:01.000228+00:00"
    },
    {
      "arxiv_id": "2506.06341v1",
      "title": "NR4DER: Neural Re-ranking for Diversified Exercise Recommendation",
      "title_zh": "NR4DERï¼šé¢å‘å¤šæ ·åŒ–ä¹ é¢˜æ¨èçš„ç¥ç»é‡æ’åº",
      "authors": [
        "Xinghe Cheng",
        "Xufang Zhou",
        "Liangda Fang",
        "Chaobo He",
        "Yuyu Zhou",
        "Weiqi Luo",
        "Zhiguo Gong",
        "Quanlong Guan"
      ],
      "abstract": "With the widespread adoption of online education platforms, an increasing number of students are gaining new knowledge through Massive Open Online Courses (MOOCs). Exercise recommendation have made strides toward improving student learning outcomes. However, existing methods not only struggle with high dropout rates but also fail to match the diverse learning pace of students. They frequently face difficulties in adjusting to inactive students' learning patterns and in accommodating individualized learning paces, resulting in limited accuracy and diversity in recommendations. To tackle these challenges, we propose Neural Re-ranking for Diversified Exercise Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to improve the effectiveness of the exercise filter module. It then employs a sequence enhancement method to enhance the representation of inactive students, accurately matches students with exercises of appropriate difficulty. Finally, it utilizes neural re-ranking to generate diverse recommendation lists based on individual students' learning histories. Extensive experimental results indicate that NR4DER significantly outperforms existing methods across multiple real-world datasets and effectively caters to the diverse learning pace of students.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NR4DERæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä¹ é¢˜æ¨èç³»ç»Ÿåœ¨åº”å¯¹é«˜è¾å­¦ç‡åŠå­¦ç”Ÿå¤šæ ·åŒ–å­¦ä¹ èŠ‚å¥ï¼ˆlearning paceï¼‰æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨mLSTMæ¨¡å‹ä¼˜åŒ–ä¹ é¢˜è¿‡æ»¤æ¨¡å—ï¼ˆexercise filter moduleï¼‰çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é‡‡ç”¨åºåˆ—å¢å¼ºæ–¹æ³•ï¼ˆsequence enhancement methodï¼‰æå‡å¯¹ä¸æ´»è·ƒå­¦ç”Ÿçš„ç‰¹å¾è¡¨ç¤ºï¼Œç¡®ä¿å­¦ç”Ÿä¸éš¾åº¦é€‚ä¸­çš„ä¹ é¢˜å®ç°ç²¾å‡†åŒ¹é…ã€‚éšåï¼ŒNR4DERç»“åˆç¥ç»é‡æ’åºï¼ˆneural re-rankingï¼‰æŠ€æœ¯ï¼ŒåŸºäºå­¦ç”Ÿçš„ä¸ªäººå­¦ä¹ å†å²ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§çš„æ¨èåˆ—è¡¨ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒNR4DERåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ä¸åŒå­¦ç”Ÿçš„ä¸ªæ€§åŒ–å­¦ä¹ è¿›åº¦ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.IR",
      "comment": "accepted for presentation at the SIGIR 2025 Full Papers track",
      "pdf_url": "https://arxiv.org/pdf/2506.06341v1",
      "published_date": "2025-06-01 07:36:52 UTC",
      "updated_date": "2025-06-01 07:36:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:22.334198+00:00"
    },
    {
      "arxiv_id": "2506.00871v3",
      "title": "Towards Predicting Any Human Trajectory In Context",
      "title_zh": "è¿ˆå‘åŸºäºä¸Šä¸‹æ–‡çš„é€šç”¨äººç±»è½¨è¿¹é¢„æµ‹",
      "authors": [
        "Ryo Fujii",
        "Hideo Saito",
        "Ryo Hachiuma"
      ],
      "abstract": "Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, the need to fine-tune for each new scenario is often impractical for deployment on edge devices. To address this challenge, we introduce TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables adaptation without fine-tuning on the scenario-specific data at inference time without requiring weight updates. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. Project Page: https://fujiry0.github.io/TrajICL-project-page/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TrajICLï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³è¡Œäººè½¨è¿¹é¢„æµ‹ä¸­ç¯å¢ƒé€‚åº”æ€§éš¾é¢˜çš„ In-Context Learning (ICL) æ¡†æ¶ã€‚ä¸åŒäºéœ€è¦é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œåå‘ä¼ æ’­å¾®è°ƒçš„ä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†é˜¶æ®µæ— éœ€æ›´æ–°æ¨¡å‹æƒé‡ï¼Œå³å¯é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°å¯¹æ–°åœºæ™¯çš„å¿«é€Ÿé€‚åº”ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºæ—¶ç©ºç›¸ä¼¼æ€§çš„ç¤ºä¾‹é€‰æ‹©æœºåˆ¶ (STES)ï¼Œé€šè¿‡è¯†åˆ«ç›¸åŒåœºæ™¯å†…ç›¸ä¼¼çš„è¿åŠ¨æ¨¡å¼æ¥ç­›é€‰ç›¸å…³è½¨è¿¹ç¤ºä¾‹ã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–æ•ˆæœï¼Œç ”ç©¶è¿˜æå‡ºäº†é¢„æµ‹å¼•å¯¼çš„ç¤ºä¾‹é€‰æ‹© (PG-ES)ï¼Œç»“åˆå†å²è½¨è¿¹ä¸é¢„æµ‹çš„æœªæ¥è½¨è¿¹ï¼Œä»è€Œæ›´å‡†ç¡®åœ°æ•æ‰é•¿æœŸåŠ¨æ€ç‰¹æ€§ã€‚æ¨¡å‹åœ¨å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…‹æœäº†çœŸå®ä¸–ç•Œæ•°æ®åœºæ™¯å¤šæ ·æ€§æœ‰é™çš„å±€é™ï¼Œå¢å¼ºäº†å…¶åˆ©ç”¨ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡Œé¢„æµ‹çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTrajICL åœ¨åŸŸå†…å’Œè·¨åŸŸåœºæ™¯ä¸‹å‡è¡¨ç°å‡ºå“è¶Šçš„é€‚åº”æ€§ï¼Œåœ¨å¤šä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ç”šè‡³è¶…è¶Šäº†ç»è¿‡ä¸“é—¨å¾®è°ƒçš„æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00871v3",
      "published_date": "2025-06-01 07:18:47 UTC",
      "updated_date": "2025-11-04 03:42:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:35.736723+00:00"
    },
    {
      "arxiv_id": "2506.00867v1",
      "title": "Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning",
      "title_zh": "é¢å‘æµå½¢æ„ŸçŸ¥æ‰©æ•£è§„åˆ’çš„å±€éƒ¨æµå½¢é€¼è¿‘ä¸æŠ•å½±",
      "authors": [
        "Kyowoon Lee",
        "Jaesik Choi"
      ],
      "abstract": "Recent advances in diffusion-based generative modeling have demonstrated significant promise in tackling long-horizon, sparse-reward tasks by leveraging offline datasets. While these approaches have achieved promising results, their reliability remains inconsistent due to the inherent stochastic risk of producing infeasible trajectories, limiting their applicability in safety-critical applications. We identify that the primary cause of these failures is inaccurate guidance during the sampling procedure, and demonstrate the existence of manifold deviation by deriving a lower bound on the guidance gap. To address this challenge, we propose Local Manifold Approximation and Projection (LoMAP), a training-free method that projects the guided sample onto a low-rank subspace approximated from offline datasets, preventing infeasible trajectory generation. We validate our approach on standard offline reinforcement learning benchmarks that involve challenging long-horizon planning. Furthermore, we show that, as a standalone module, LoMAP can be incorporated into the hierarchical diffusion planner, providing further performance enhancements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion-based generative modeling)çš„è§„åˆ’æ–¹æ³•åœ¨å¤„ç†é•¿ç¨‹ã€ç¨€ç–å¥–åŠ±ä»»åŠ¡æ—¶ï¼Œå› é‡‡æ ·è¿‡ç¨‹ä¸­çš„å¼•å¯¼åå·®å’Œæµå½¢åå·®(manifold deviation)å¯¼è‡´ç”Ÿæˆä¸å¯è¡Œè½¨è¿¹çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºLoMAP (Local Manifold Approximation and Projection)çš„æ— éœ€è®­ç»ƒ(training-free)æ–¹æ³•ã€‚LoMAPé€šè¿‡å°†å¼•å¯¼æ ·æœ¬æŠ•å½±åˆ°ä»ç¦»çº¿æ•°æ®é›†è¿‘ä¼¼å¾—åˆ°çš„ä½ç§©å­ç©ºé—´(low-rank subspace)ä¸Šï¼Œæœ‰æ•ˆé˜²æ­¢äº†ä¸å¯è¡Œè½¨è¿¹çš„äº§ç”Ÿã€‚å®éªŒåœ¨æ ‡å‡†çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ (offline reinforcement learning)åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§é•¿ç¨‹è§„åˆ’ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒLoMAPè¿˜å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å—é›†æˆåˆ°å±‚çº§æ‰©æ•£è§„åˆ’å™¨(hierarchical diffusion planner)ä¸­ï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00867v1",
      "published_date": "2025-06-01 07:16:39 UTC",
      "updated_date": "2025-06-01 07:16:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:38.831180+00:00"
    },
    {
      "arxiv_id": "2506.00865v1",
      "title": "GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints",
      "title_zh": "GIA-MICï¼šåŸºäºé—¨æ§äº¤äº’æ³¨æ„åŠ›ä¸æ¨¡æ€ä¸å˜å­¦ä¹ çº¦æŸçš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«",
      "authors": [
        "Jiajun He",
        "Jinyi Mi",
        "Tomoki Toda"
      ],
      "abstract": "Multimodal emotion recognition (MER) extracts emotions from multimodal data, including visual, speech, and text inputs, playing a key role in human-computer interaction. Attention-based fusion methods dominate MER research, achieving strong classification performance. However, two key challenges remain: effectively extracting modality-specific features and capturing cross-modal similarities despite distribution differences caused by modality heterogeneity. To address these, we propose a gated interactive attention mechanism to adaptively extract modality-specific features while enhancing emotional information through pairwise interactions. Additionally, we introduce a modality-invariant generator to learn modality-invariant representations and constrain domain shifts by aligning cross-modal similarities. Experiments on IEMOCAP demonstrate that our method outperforms state-of-the-art MER approaches, achieving WA 80.7% and UA 81.3%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GIA-MICï¼Œä¸€ç§ç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«(Multimodal Emotion Recognition, MER)çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æå–æ¨¡æ€ç‰¹å¼‚æ€§ç‰¹å¾ä»¥åŠå…‹æœæ¨¡æ€å¼‚æ„æ€§å¸¦æ¥çš„è·¨æ¨¡æ€ç›¸ä¼¼æ€§æ•è·éš¾é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†é—¨æ§äº¤äº’æ³¨æ„åŠ›æœºåˆ¶(Gated Interactive Attention)ï¼Œé€šè¿‡æˆå¯¹äº¤äº’è‡ªé€‚åº”åœ°æå–æ¨¡æ€ç‰¹å¼‚æ€§ç‰¹å¾å¹¶å¢å¼ºæƒ…æ„Ÿä¿¡æ¯è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†æ¨¡æ€ä¸å˜ç”Ÿæˆå™¨(Modality-Invariant Generator)ï¼Œç”¨ä»¥å­¦ä¹ æ¨¡æ€ä¸å˜è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å¯¹é½è·¨æ¨¡æ€ç›¸ä¼¼æ€§æ¥çº¦æŸé¢†åŸŸåç§»(Domain Shifts)ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ æƒå‡†ç¡®ç‡(WA)å’ŒéåŠ æƒå‡†ç¡®ç‡(UA)ä¸Šåˆ†åˆ«è¾¾åˆ°80.7%å’Œ81.3%ï¼Œæ€§èƒ½ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„MERæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by INTERSPEECH 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00865v1",
      "published_date": "2025-06-01 07:07:02 UTC",
      "updated_date": "2025-06-01 07:07:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:45.604242+00:00"
    },
    {
      "arxiv_id": "2506.04251v4",
      "title": "Language-Driven Coordination and Learning in Multi-Agent Simulation Environments",
      "title_zh": "å¤šæ™ºèƒ½ä½“ä»¿çœŸç¯å¢ƒä¸­çš„è¯­è¨€é©±åŠ¨åä½œä¸å­¦ä¹ ",
      "authors": [
        "Zhengyang Li",
        "Sawyer Campos",
        "Nana Wang"
      ],
      "abstract": "This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM-MARLæ¡†æ¶ï¼Œé€šè¿‡å°†Large Language Models (LLMs) æ•´åˆå…¥Multi-Agent Reinforcement Learning (MARL) æ¥å¢å¼ºæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„åä½œã€é€šä¿¡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«Coordinatorã€Communicatorå’ŒMemoryä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼Œåˆ†åˆ«å®ç°å­ç›®æ ‡ç”Ÿæˆã€æ™ºèƒ½ä½“é—´ç¬¦å·é€šä¿¡å’Œæƒ…å¢ƒè®°å¿†å¬å›ã€‚è®­ç»ƒæ–¹æ¡ˆç»“åˆäº†PPOç®—æ³•ã€Language-Conditioned Lossä»¥åŠLLM Query Gatingæœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLM-MARLåœ¨Google Research Footballã€MAgent Battleå’ŒStarCraft IIä»»åŠ¡ä¸­çš„èƒœç‡ã€åä½œè¯„åˆ†åŠZero-shot Generalizationå‡æ˜¾è‘—ä¼˜äºMAPPOå’ŒQMIXåŸºçº¿æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¯æ˜äº†å­ç›®æ ‡ç”Ÿæˆå’Œè¯­è¨€é€šä¿¡å¯¹æ€§èƒ½æå‡çš„å…³é”®ä½œç”¨ï¼Œå®šæ€§åˆ†æåˆ™æ­ç¤ºäº†è§’è‰²åˆ†å·¥å’Œé€šä¿¡é©±åŠ¨æˆ˜æœ¯ç­‰æ¶Œç°è¡Œä¸ºã€‚è¯¥å·¥ä½œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ™ºèƒ½åŒ–åä½œè®¾è®¡æä¾›äº†æ–°è·¯å¾„ï¼Œåœ¨è®­ç»ƒæ¨¡æ‹Ÿã€æ¸¸æˆå’Œäººç±»-AIåä½œé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.04251v4",
      "published_date": "2025-06-01 06:46:49 UTC",
      "updated_date": "2025-11-03 07:37:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:42.545750+00:00"
    },
    {
      "arxiv_id": "2506.02050v1",
      "title": "Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids",
      "title_zh": "é¢å‘ç¦»æ•£ç½‘æ ¼çš„åŸºäºçŠ¶æ€æŠ½è±¡çš„è§£è€¦å¼åˆ†å±‚å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Qingyu Xiao",
        "Yuanlin Chang",
        "Youtian Du"
      ],
      "abstract": "Effective agent exploration remains a core challenge in reinforcement learning (RL) for complex discrete state-space environments, particularly under partial observability. This paper presents a decoupled hierarchical RL framework integrating state abstraction (DcHRL-SA) to address this issue. The proposed method employs a dual-level architecture, consisting of a high level RL-based actor and a low-level rule-based policy, to promote effective exploration. Additionally, state abstraction method is incorporated to cluster discrete states, effectively lowering state dimensionality. Experiments conducted in two discrete customized grid environments demonstrate that the proposed approach consistently outperforms PPO in terms of exploration efficiency, convergence speed, cumulative reward, and policy stability. These results demonstrate a practical approach for integrating decoupled hierarchical policies and state abstraction in discrete grids with large-scale exploration space. Code will be available at https://github.com/XQY169/DcHRL-SA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚ç¦»æ•£çŠ¶æ€ç©ºé—´ç¯å¢ƒï¼ˆç‰¹åˆ«æ˜¯éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹ï¼‰ä¸­çš„æ™ºèƒ½ä½“æœ‰æ•ˆæ¢ç´¢éš¾é¢˜ï¼Œæå‡ºäº†é›†æˆçŠ¶æ€æŠ½è±¡çš„è§£è€¦åˆ†å±‚ Reinforcement Learning (RL) æ¡†æ¶ DcHRL-SAã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒå±‚æ¶æ„ï¼Œç”±é«˜å±‚åŸºäº RL çš„ Actor å’Œä½å±‚åŸºäºè§„åˆ™çš„ Policy ç»„æˆï¼Œæ—¨åœ¨ä¿ƒè¿›æ›´æœ‰æ•ˆçš„æ¢ç´¢è¡Œä¸ºã€‚åŒæ—¶ï¼Œæ¡†æ¶å¼•å…¥äº†çŠ¶æ€æŠ½è±¡ (State Abstraction) æ–¹æ³•å¯¹ç¦»æ•£çŠ¶æ€è¿›è¡Œèšç±»ï¼Œä»è€Œæœ‰æ•ˆé™ä½äº†çŠ¶æ€ç»´åº¦ã€‚åœ¨ä¸¤ç§è‡ªå®šä¹‰ç¦»æ•£ç½‘æ ¼ç¯å¢ƒä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDcHRL-SA åœ¨æ¢ç´¢æ•ˆç‡ã€æ”¶æ•›é€Ÿåº¦ã€ç´¯ç§¯å¥–åŠ±å’Œç­–ç•¥ç¨³å®šæ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äº PPO ç®—æ³•ã€‚è¿™é¡¹ç ”ç©¶ä¸ºåœ¨å¤§è§„æ¨¡æ¢ç´¢ç©ºé—´çš„ç¦»æ•£ç½‘æ ¼ä¸­é›†æˆè§£è€¦åˆ†å±‚ç­–ç•¥å’ŒçŠ¶æ€æŠ½è±¡æä¾›äº†ä¸€ç§åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02050v1",
      "published_date": "2025-06-01 06:36:19 UTC",
      "updated_date": "2025-06-01 06:36:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:49.139113+00:00"
    },
    {
      "arxiv_id": "2506.00856v2",
      "title": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks",
      "title_zh": "AI èƒ½å¦ç²¾é€šè®¡é‡ç»æµå­¦ï¼Ÿæ¥è‡ªè®¡é‡ç»æµå­¦ AI æ™ºèƒ½ä½“åœ¨ä¸“å®¶çº§ä»»åŠ¡ä¸­çš„å®è¯è¯æ®",
      "authors": [
        "Qiang Chen",
        "Tianyang Han",
        "Jin Li",
        "Ye Luo",
        "Yuxiao Wu",
        "Xiaowei Zhang",
        "Tuo Zhou"
      ],
      "abstract": "Can AI effectively perform complex econometric analysis traditionally requiring human expertise? This paper evaluates AI agents' capability to master econometrics, focusing on empirical analysis performance. We develop an ``Econometrics AI Agent'' built on the open-source MetaGPT framework. This agent exhibits outstanding performance in: (1) planning econometric tasks strategically, (2) generating and executing code, (3) employing error-based reflection for improved robustness, and (4) allowing iterative refinement through multi-round conversations. We construct two datasets from academic coursework materials and published research papers to evaluate performance against real-world challenges. Comparative testing shows our domain-specialized AI agent significantly outperforms both benchmark large language models (LLMs) and general-purpose AI agents. This work establishes a testbed for exploring AI's impact on social science research and enables cost-effective integration of domain expertise, making advanced econometric methods accessible to users with minimal coding skills. Furthermore, our AI agent enhances research reproducibility and offers promising pedagogical applications for econometrics teaching.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)åœ¨å¤æ‚è®¡é‡ç»æµå­¦åˆ†æä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œæ—¨åœ¨è¯„ä¼°AIæ™ºèƒ½ä½“æ˜¯å¦èƒ½å¤Ÿèƒœä»»ä¼ ç»Ÿä¸Šéœ€è¦äººç±»ä¸“å®¶ç»éªŒçš„å®è¯åˆ†æä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºå¼€æºçš„MetaGPTæ¡†æ¶å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„è®¡é‡ç»æµå­¦æ™ºèƒ½ä½“(Econometrics AI Agent)ï¼Œè¯¥æ™ºèƒ½ä½“å…·å¤‡æˆ˜ç•¥æ€§è§„åˆ’ä»»åŠ¡ã€è‡ªåŠ¨ç”Ÿæˆå¹¶æ‰§è¡Œä»£ç ã€åˆ©ç”¨åŸºäºé”™è¯¯çš„åå°„æœºåˆ¶(error-based reflection)å¢å¼ºç¨³å¥æ€§ä»¥åŠé€šè¿‡å¤šè½®å¯¹è¯è¿›è¡Œè¿­ä»£æ”¹è¿›çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨å­¦æœ¯è¯¾ç¨‹å’Œå·²å‘è¡¨ç ”ç©¶è®ºæ–‡æ„å»ºçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥é¢†åŸŸä¸“ç”¨æ™ºèƒ½ä½“çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºå‡†å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œé€šç”¨å‹AIæ™ºèƒ½ä½“ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºæ¢ç´¢AIå¯¹ç¤¾ä¼šç§‘å­¦ç ”ç©¶çš„å½±å“å»ºç«‹äº†å®éªŒå¹³å°ï¼Œè¿˜é™ä½äº†å…ˆè¿›è®¡é‡ç»æµå­¦æ–¹æ³•çš„ä½¿ç”¨é—¨æ§›ã€‚æ­¤å¤–ï¼Œè¯¥æ™ºèƒ½ä½“æœ‰æ•ˆæå‡äº†ç ”ç©¶çš„å¯é‡å¤æ€§(reproducibility)ï¼Œå¹¶åœ¨è®¡é‡ç»æµå­¦æ•™å­¦é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "econ.EM",
        "cs.AI"
      ],
      "primary_category": "econ.EM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00856v2",
      "published_date": "2025-06-01 06:34:42 UTC",
      "updated_date": "2025-06-13 14:28:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:55.639313+00:00"
    },
    {
      "arxiv_id": "2506.00855v1",
      "title": "MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book",
      "title_zh": "MedBookVQAï¼šæºè‡ªå¼€æ”¾è·å–ä¹¦ç±çš„ç³»ç»Ÿã€å…¨é¢åŒ»å­¦åŸºå‡†",
      "authors": [
        "Sau Lai Yip",
        "Sunan He",
        "Yuxiang Nie",
        "Shu Pui Chan",
        "Yilin Ye",
        "Sum Ying Lam",
        "Hao Chen"
      ],
      "abstract": "The accelerating development of general medical artificial intelligence (GMAI), powered by multimodal large language models (MLLMs), offers transformative potential for addressing persistent healthcare challenges, including workforce deficits and escalating costs. The parallel development of systematic evaluation benchmarks emerges as a critical imperative to enable performance assessment and provide technological guidance. Meanwhile, as an invaluable knowledge source, the potential of medical textbooks for benchmark development remains underexploited. Here, we present MedBookVQA, a systematic and comprehensive multimodal benchmark derived from open-access medical textbooks. To curate this benchmark, we propose a standardized pipeline for automated extraction of medical figures while contextually aligning them with corresponding medical narratives. Based on this curated data, we generate 5,000 clinically relevant questions spanning modality recognition, disease classification, anatomical identification, symptom diagnosis, and surgical procedures. A multi-tier annotation system categorizes queries through hierarchical taxonomies encompassing medical imaging modalities (42 categories), body anatomies (125 structures), and clinical specialties (31 departments), enabling nuanced analysis across medical subdomains. We evaluate a wide array of MLLMs, including proprietary, open-sourced, medical, and reasoning models, revealing significant performance disparities across task types and model categories. Our findings highlight critical capability gaps in current GMAI systems while establishing textbook-derived multimodal benchmarks as essential evaluation tools. MedBookVQA establishes textbook-derived benchmarking as a critical paradigm for advancing clinical AI, exposing limitations in GMAI systems while providing anatomically structured performance metrics across specialties.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† MedBookVQAï¼Œä¸€ä¸ªåŸºäºå¼€æºåŒ»å­¦æ•™ç§‘ä¹¦æ„å»ºçš„ç³»ç»Ÿä¸”å…¨é¢çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰é€šç”¨åŒ»å­¦äººå·¥æ™ºèƒ½(GMAI)é¢†åŸŸè¯„ä¼°å·¥å…·ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€å¥—æ ‡å‡†åŒ–çš„è‡ªåŠ¨æµæ°´çº¿æå–åŒ»å­¦å›¾åƒï¼Œå¹¶å°†å…¶ä¸ç›¸å…³çš„åŒ»å­¦å™è¿°è¿›è¡Œä¸Šä¸‹æ–‡å¯¹é½ï¼Œç¡®ä¿äº†åŸºå‡†æ•°æ®çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚åŸºäºè¯¥æ•°æ®é›†ç”Ÿæˆçš„ 5,000 ä¸ªä¸´åºŠç›¸å…³é—®é¢˜æ¶µç›–äº†æ¨¡æ€è¯†åˆ«(modality recognition)ã€ç–¾ç—…åˆ†ç±»(disease classification)ã€è§£å‰–ç»“æ„è¯†åˆ«(anatomical identification)ä»¥åŠæ‰‹æœ¯ç¨‹åºç­‰å¤šä¸ªç»´åº¦ã€‚ç ”ç©¶è¿˜è®¾è®¡äº†ä¸€å¥—å¤šå±‚çº§æ ‡æ³¨ç³»ç»Ÿï¼ŒåŒ…å« 42 ç§åŒ»å­¦å½±åƒæ¨¡æ€ã€125 ç§è§£å‰–ç»“æ„å’Œ 31 ä¸ªä¸´åºŠç§‘å®¤ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)æä¾›äº†ç»†ç²’åº¦çš„æ€§èƒ½åˆ†æã€‚é€šè¿‡å¯¹å¤šç§æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ­ç¤ºäº†å½“å‰ GMAI ç³»ç»Ÿåœ¨ä¸åŒä»»åŠ¡ç±»å‹ä¸­å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚å’Œèƒ½åŠ›ç¼ºé™·ã€‚MedBookVQA çš„å»ºç«‹ä¸ä»…ä¸ºåŒ»ç–— AI æä¾›äº†ç»“æ„åŒ–çš„æ€§èƒ½æŒ‡æ ‡ï¼Œè¿˜ç¡®ç«‹äº†åˆ©ç”¨æ•™ç§‘ä¹¦èµ„æºå¼€å‘åŸºå‡†æµ‹è¯•ä½œä¸ºæ¨åŠ¨ä¸´åºŠäººå·¥æ™ºèƒ½å‘å±•çš„å…³é”®èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "For data and code, see: https://huggingface.co/datasets/slyipae1/MedBookVQA and https://github.com/slyipae1/MedBookVQA",
      "pdf_url": "https://arxiv.org/pdf/2506.00855v1",
      "published_date": "2025-06-01 06:28:36 UTC",
      "updated_date": "2025-06-01 06:28:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:20:09.895683+00:00"
    },
    {
      "arxiv_id": "2506.00854v3",
      "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG",
      "title_zh": "EEG2TEXT-CNï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸å¯¹æ¯”å­¦ä¹ åœ¨ ChineseEEG æ•°æ®é›†ä¸Šçš„å¼€æ”¾è¯æ±‡ä¸­æ–‡æ–‡æœ¬-è„‘ç”µå¯¹é½æ¢ç´¢æ€§ç ”ç©¶",
      "authors": [
        "Jacky Tai-Yu Lu",
        "Jung Chiang",
        "Chi-Sheng Chen",
        "Anna Nai-Yun Tung",
        "Hsiang Wei Hu",
        "Yuan Chiao Cheng"
      ],
      "abstract": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EEG2TEXT-CNï¼Œè¿™æ˜¯ç›®å‰å·²çŸ¥æœ€æ—©é’ˆå¯¹ä¸­æ–‡è®¾è®¡çš„å¼€æ”¾è¯æ±‡è„‘ç”µå›¾è½¬æ–‡æœ¬(EEG-to-text)ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¶æ„åŸºäºå…·æœ‰ç”Ÿç‰©å­¦åŸºç¡€çš„è„‘ç”µç¼–ç å™¨NICE-EEGå’Œç´§å‡‘å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹MiniLMï¼Œåˆ©ç”¨æ©ç é¢„è®­ç»ƒ(masked pretraining)å’Œå¯¹æ¯”å­¦ä¹ (contrastive learning)æŠ€æœ¯å®ç°äº†å¤šé€šé“è„‘ç”µä¿¡å·ä¸è‡ªç„¶è¯­è¨€è¡¨å¾çš„å¯¹é½ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ChineseEEGæ•°æ®é›†ï¼Œå°†128é€šé“ã€é‡‡æ ·ç‡ä¸º256 Hzçš„è„‘ç”µä¿¡å·åˆ†å‰²ä¸ºå•å­—åµŒå…¥(per-character embeddings)ï¼Œå¹¶åœ¨é›¶æ ·æœ¬(zero-shot)è®¾ç½®ä¸‹è¿›è¡Œå…¨å¥é¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨1500ä¸ªè®­ç»ƒéªŒè¯å¥å’Œ300ä¸ªæµ‹è¯•æ ·æœ¬ä¸­ï¼Œæ¨¡å‹è¾¾åˆ°äº†6.38%çš„BLEU-1è¯„åˆ†ï¼Œå±•ç°äº†è‰¯å¥½çš„è¯æ±‡å¯¹é½èƒ½åŠ›ã€‚è™½ç„¶è¯­æ³•æµç•…æ€§ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä½†è¯¥å‘ç°è¯æ˜äº†ä»è„‘ç”µå›¾ä¸­è¿›è¡Œéè¯­éŸ³ã€è·¨æ¨¡æ€è¯­è¨€è§£ç çš„å¯è¡Œæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šè¯­è¨€è„‘æœºæ¥å£ç ”ç©¶å¼€æ‹“äº†æ–°æ–¹å‘ï¼Œå¹¶ä¸ºæœªæ¥ä¸­æ–‡è®¤çŸ¥è¯­è¨€æ¥å£(cognitive-language interfaces)çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00854v3",
      "published_date": "2025-06-01 06:26:32 UTC",
      "updated_date": "2025-07-08 17:34:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:20:10.084080+00:00"
    },
    {
      "arxiv_id": "2506.00849v1",
      "title": "Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis",
      "title_zh": "VAE ä¸æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–ï¼šç»Ÿä¸€çš„ä¿¡æ¯è®ºåˆ†æ",
      "authors": [
        "Qi Chen",
        "Jierui Zhu",
        "Florian Shkurti"
      ],
      "abstract": "Despite the empirical success of Diffusion Models (DMs) and Variational Autoencoders (VAEs), their generalization performance remains theoretically underexplored, especially lacking a full consideration of the shared encoder-generator structure. Leveraging recent information-theoretic tools, we propose a unified theoretical framework that provides guarantees for the generalization of both the encoder and generator by treating them as randomized mappings. This framework further enables (1) a refined analysis for VAEs, accounting for the generator's generalization, which was previously overlooked; (2) illustrating an explicit trade-off in generalization terms for DMs that depends on the diffusion time $T$; and (3) providing computable bounds for DMs based solely on the training data, allowing the selection of the optimal $T$ and the integration of such bounds into the optimization process to improve model performance. Empirical results on both synthetic and real datasets illustrate the validity of the proposed theory.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Variational Autoencoders (VAEs) å’Œ Diffusion Models (DMs) æ³›åŒ–æ€§èƒ½ç¼ºä¹ç†è®ºæ¢ç´¢çš„é—®é¢˜ï¼Œåˆ©ç”¨ä¿¡æ¯è®ºå·¥å…·æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç¼–ç å™¨å’Œç”Ÿæˆå™¨è§†ä¸ºéšæœºæ˜ å°„ (randomized mappings)ï¼Œä»è€Œä¸ºä¸¤è€…çš„æ³›åŒ–æ€§èƒ½æä¾›äº†ç†è®ºä¿éšœã€‚åœ¨ VAEs æ–¹é¢ï¼Œç ”ç©¶å¯¹ä»¥å¾€è¢«å¿½è§†çš„ç”Ÿæˆå™¨æ³›åŒ–è¿›è¡Œäº†ç²¾ç»†åŒ–åˆ†æï¼›åœ¨ DMs æ–¹é¢ï¼Œæ­ç¤ºäº†æ³›åŒ–é¡¹ä¸æ‰©æ•£æ—¶é—´ $T$ ä¹‹é—´çš„æ˜ç¡®æƒè¡¡å…³ç³»ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜ä¸º DMs æä¾›äº†ä»…åŸºäºè®­ç»ƒæ•°æ®çš„å¯è®¡ç®—ç•Œé™ï¼Œèƒ½å¤Ÿç”¨äºé€‰æ‹©æœ€ä¼˜çš„ $T$ å¹¶é›†æˆåˆ°ä¼˜åŒ–è¿‡ç¨‹ä¸­ä»¥æå‡æ¨¡å‹è¡¨ç°ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå‡è¯æ˜äº†è¯¥ç†è®ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Accepted",
      "pdf_url": "https://arxiv.org/pdf/2506.00849v1",
      "published_date": "2025-06-01 06:11:38 UTC",
      "updated_date": "2025-06-01 06:11:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:19:58.924201+00:00"
    },
    {
      "arxiv_id": "2506.00848v1",
      "title": "Speech Unlearning",
      "title_zh": "è¯­éŸ³é—å¿˜",
      "authors": [
        "Jiali Cheng",
        "Hadi Amiri"
      ],
      "abstract": "We introduce machine unlearning for speech tasks, a novel and underexplored research problem that aims to efficiently and effectively remove the influence of specific data from trained speech models without full retraining. This has important applications in privacy preservation, removal of outdated or noisy data, and bias mitigation. While machine unlearning has been studied in computer vision and natural language processing, its application to speech is largely unexplored due to the high-dimensional, sequential, and speaker-dependent nature of speech data. We define two fundamental speech unlearning tasks: sample unlearning, which removes individual data points (e.g., a voice recording), and class unlearning, which removes an entire category (e.g., all data from a speaker), while preserving performance on the remaining data. Experiments on keyword spotting and speaker identification demonstrate that unlearning speech data is significantly more challenging than unlearning image or text data. We conclude with key future directions in this area, including structured training, robust evaluation, feature-level unlearning, broader applications, scalable methods, and adversarial robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Speech Unlearning è¿™ä¸€æ–°å…´ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨ä»å·²è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹ä¸­é«˜æ•ˆç§»é™¤ç‰¹å®šæ•°æ®çš„å½±å“è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¿™é¡¹æŠ€æœ¯åœ¨éšç§ä¿æŠ¤ã€å™ªå£°æ•°æ®æ¸…ç†å’Œ Bias Mitigation æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç”±äºè¯­éŸ³æ•°æ®å…·å¤‡é«˜ç»´ã€åºåˆ—åŒ–å’Œ Speaker-dependent çš„ç‰¹æ€§ï¼Œå…¶é—å¿˜éš¾åº¦æ˜¾è‘—é«˜äºå›¾åƒæˆ–æ–‡æœ¬æ•°æ®ã€‚ç ”ç©¶å®šä¹‰äº† Sample Unlearning å’Œ Class Unlearning ä¸¤ç§åŸºæœ¬ä»»åŠ¡ï¼Œåˆ†åˆ«ç”¨äºç§»é™¤å•æ¡è¯­éŸ³è®°å½•æˆ–ç‰¹å®šè¯´è¯äººçš„å…¨éƒ¨æ•°æ®ã€‚é€šè¿‡åœ¨ Keyword Spotting å’Œ Speaker Identification ä»»åŠ¡ä¸Šçš„å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†è¯­éŸ³æ•°æ®é—å¿˜çš„æŒ‘æˆ˜æ€§è¿œå¤§äºè§†è§‰æˆ–æ–‡æœ¬é¢†åŸŸã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†åŒ…æ‹¬ Feature-level Unlearning å’Œ Scalable Methods åœ¨å†…çš„å¤šä¸ªæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºæå‡è¯­éŸ³æ¨¡å‹çš„åˆè§„æ€§ä¸é²æ£’æ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00848v1",
      "published_date": "2025-06-01 06:04:16 UTC",
      "updated_date": "2025-06-01 06:04:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:20:32.597996+00:00"
    },
    {
      "arxiv_id": "2506.00842v2",
      "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience",
      "title_zh": "è¿ˆå‘ç»“æ„åŒ–çŸ¥è¯†æ¨ç†ï¼šåŸºäºç»éªŒçš„å¯¹æ¯”å¼æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Jiawei Gu",
        "Ziting Xian",
        "Yuanzhen Xie",
        "Ye Liu",
        "Enjie Liu",
        "Ruichao Zhong",
        "Mochi Gao",
        "Yunzhi Tan",
        "Bo Hu",
        "Zang Li"
      ],
      "abstract": "Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†è¡¨æ ¼å’Œæ•°æ®åº“ç­‰ç»“æ„åŒ–æ•°æ®æ—¶æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†Contrastive Retrieval-Augmented Generation on Experience (CoRE)æ¡†æ¶ã€‚CoREé€šè¿‡æ„å»ºç»éªŒè®°å¿†(Experience Memory)è¡¨ç¤ºï¼Œå¹¶ç»“åˆå¯¹æ¯”ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)æ¥æ¨¡æ‹Ÿäººç±»çš„çŸ¥è¯†è¿ç§»è¿‡ç¨‹ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†æ‰©å¤§æ•°æ®è¦†ç›–é¢ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ç”Ÿæˆç»éªŒè®°å¿†ï¼Œå°†æœ‰æ•ˆè®­ç»ƒæ•°æ®è§„æ¨¡æ‰©å±•äº†8è‡³9å€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoREåœ¨Text-to-SQLå’ŒTableQAä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº†3.44%å’Œ4.24%çš„å¹³å‡æ€§èƒ½æå‡ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„å¢ç›Šç”šè‡³é«˜è¾¾17.2%ã€‚è¿™ç§æ— éœ€é¢å¤–è®­ç»ƒä¸”æ”¯æŒæŒç»­å­¦ä¹ çš„æ–¹æ³•ï¼Œæ˜¾è‘—æ¨è¿›äº†LLMsåœ¨ç»“æ„åŒ–çŸ¥è¯†ä¸“å®¶åŒ–æ–¹å‘çš„å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.00842v2",
      "published_date": "2025-06-01 05:22:00 UTC",
      "updated_date": "2025-07-24 19:34:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:20:39.395923+00:00"
    },
    {
      "arxiv_id": "2506.02049v1",
      "title": "EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration",
      "title_zh": "EvoGitï¼šåŸºäº Git å¤šæ™ºèƒ½ä½“åä½œçš„å»ä¸­å¿ƒåŒ–ä»£ç æ¼”åŒ–",
      "authors": [
        "Beichen Huang",
        "Ran Cheng",
        "Kay Chen Tan"
      ],
      "abstract": "We introduce EvoGit, a decentralized multi-agent framework for collaborative software development driven by autonomous code evolution. EvoGit deploys a population of independent coding agents, each proposing edits to a shared codebase without centralized coordination, explicit message passing, or shared memory. Instead, all coordination emerges through a Git-based phylogenetic graph that tracks the full version lineage and enables agents to asynchronously read from and write to the evolving code repository. This graph-based structure supports fine-grained branching, implicit concurrency, and scalable agent interaction while preserving a consistent historical record. Human involvement is minimal but strategic: users define high-level goals, periodically review the graph, and provide lightweight feedback to promote promising directions or prune unproductive ones. Experiments demonstrate EvoGit's ability to autonomously produce functional and modular software artifacts across two real-world tasks: (1) building a web application from scratch using modern frameworks, and (2) constructing a meta-level system that evolves its own language-model-guided solver for the bin-packing optimization problem. Our results underscore EvoGit's potential to establish a new paradigm for decentralized, automated, and continual software development. EvoGit is open-sourced at https://github.com/BillHuang2001/evogit.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EvoGitï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡è‡ªä¸»Code Evolutioné©±åŠ¨åä½œå¼è½¯ä»¶å¼€å‘çš„å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚EvoGitéƒ¨ç½²äº†ä¸€ç»„ç‹¬ç«‹çš„Coding Agentsï¼Œåœ¨æ— éœ€é›†ä¸­åè°ƒã€æ˜¾å¼æ¶ˆæ¯ä¼ é€’æˆ–å…±äº«å†…å­˜çš„æƒ…å†µä¸‹å¯¹å…±äº«ä»£ç åº“è¿›è¡Œç¼–è¾‘ã€‚æ‰€æœ‰åä½œå‡é€šè¿‡åŸºäºGitçš„Phylogenetic Graphå®ç°ï¼Œè¯¥å›¾ä¸ä»…è®°å½•äº†å®Œæ•´çš„ç‰ˆæœ¬æ¼”åŒ–è°±ç³»ï¼Œè¿˜æ”¯æŒæ™ºèƒ½ä½“è¿›è¡Œå¼‚æ­¥è¯»å†™å’Œç»†ç²’åº¦çš„åˆ†æ”¯æ“ä½œã€‚è¿™ç§ç»“æ„åœ¨ä¿ç•™ä¸€è‡´å†å²è®°å½•çš„åŒæ—¶å®ç°äº†éšå¼å¹¶å‘ä¸å¯æ‰©å±•çš„æ™ºèƒ½ä½“äº¤äº’ï¼Œè€Œäººç±»ä»…éœ€æä¾›é«˜å±‚ç›®æ ‡å’Œè½»é‡çº§åé¦ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEvoGitèƒ½å¤Ÿè‡ªä¸»å®Œæˆä»é›¶æ„å»ºWeb Applicationä»¥åŠå¼€å‘é’ˆå¯¹Bin-Packingé—®é¢˜çš„å…ƒçº§æ±‚è§£ç³»ç»Ÿç­‰å¤æ‚ä»»åŠ¡ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†EvoGitåœ¨å»ºç«‹å»ä¸­å¿ƒåŒ–ã€è‡ªåŠ¨åŒ–ä¸”æŒç»­çš„è½¯ä»¶å¼€å‘æ–°èŒƒå¼æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.MA",
        "cs.NE"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02049v1",
      "published_date": "2025-06-01 05:20:42 UTC",
      "updated_date": "2025-06-01 05:20:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:20:41.986214+00:00"
    },
    {
      "arxiv_id": "2506.06340v1",
      "title": "Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support",
      "title_zh": "ä»éç»“æ„åŒ–è®°å½•ä¸­æå–ç»“æ„åŒ–è¯­ä¹‰ï¼šåŸºäºè¯­è¨€æ¨¡å‹çš„ EHR å†³ç­–æ”¯æŒæ–¹æ³•",
      "authors": [
        "Wu Hao Ran",
        "Xi Xi",
        "Furong Li",
        "Jingyi Lu",
        "Jian Jiang",
        "Hui Huang",
        "Yuzhuan Zhang",
        "Shi Li"
      ],
      "abstract": "The advent of large language models (LLMs) has opened new avenues for analyzing complex, unstructured data, particularly within the medical domain. Electronic Health Records (EHRs) contain a wealth of information in various formats, including free text clinical notes, structured lab results, and diagnostic codes. This paper explores the application of advanced language models to leverage these diverse data sources for improved clinical decision support. We will discuss how text-based features, often overlooked in traditional high dimensional EHR analysis, can provide semantically rich representations and aid in harmonizing data across different institutions. Furthermore, we delve into the challenges and opportunities of incorporating medical codes and ensuring the generalizability and fairness of AI models in healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åˆ†æç”µå­å¥åº·æ¡£æ¡ˆ(Electronic Health Records, EHRs)ä¸­å¤æ‚çš„éç»“æ„åŒ–æ•°æ®ï¼Œæ—¨åœ¨æå‡ä¸´åºŠå†³ç­–æ”¯æŒ(clinical decision support)ç³»ç»Ÿçš„æ€§èƒ½ã€‚ä½œè€…é‡ç‚¹é˜è¿°äº†å¦‚ä½•æŒ–æ˜ä¼ ç»Ÿåˆ†æä¸­å¸¸è¢«å¿½è§†çš„æ–‡æœ¬ç‰¹å¾ï¼Œä»¥æä¾›æ›´ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºå¹¶è¾…åŠ©è·¨æœºæ„çš„æ•°æ®åè°ƒã€‚æ–‡ç« è¿›ä¸€æ­¥ç ”ç©¶äº†å°†ä¸´åºŠç¬”è®°ä¸ç»“æ„åŒ–çš„å®éªŒå®¤ç»“æœåŠè¯Šæ–­ä»£ç ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œåˆ†æäº†è¿™ä¸€è¿‡ç¨‹ä¸­çš„æŠ€æœ¯è·¯å¾„ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æ·±å…¥è®¨è®ºäº†æ•´åˆåŒ»ç–—ä»£ç æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»¥åŠç¡®ä¿AIæ¨¡å‹åœ¨åŒ»ç–—åœºæ™¯ä¸­å…·å¤‡æ³›åŒ–èƒ½åŠ›å’Œå…¬å¹³æ€§çš„å¿…è¦æ€§ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†é«˜çº§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šå…ƒåŒ–åŒ»ç–—æ•°æ®æºæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆçš„åŒ»ç–—AIç³»ç»Ÿæä¾›äº†ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06340v1",
      "published_date": "2025-06-01 05:07:51 UTC",
      "updated_date": "2025-06-01 05:07:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:20:51.798412+00:00"
    },
    {
      "arxiv_id": "2506.00835v1",
      "title": "SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning",
      "title_zh": "SynPOï¼šæè¿°æ€§ä¸åå¥½ä¼˜åŒ–ååŒçš„è§†é¢‘è¯¦ç»†æè¿°ç”Ÿæˆ",
      "authors": [
        "Jisheng Dang",
        "Yizhou Zhang",
        "Hao Ye",
        "Teng Wang",
        "Siming Chen",
        "Huicheng Zheng",
        "Yulan Guo",
        "Jianhuang Lai",
        "Bin Hu"
      ],
      "abstract": "Fine-grained video captioning aims to generate detailed, temporally coherent descriptions of video content. However, existing methods struggle to capture subtle video dynamics and rich detailed information. In this paper, we leverage preference learning to enhance the performance of vision-language models in fine-grained video captioning, while mitigating several limitations inherent to direct preference optimization (DPO). First, we propose a pipeline for constructing preference pairs that leverages the intrinsic properties of VLMs along with partial assistance from large language models, achieving an optimal balance between cost and data quality. Second, we propose Synergistic Preference Optimization (SynPO), a novel optimization method offering significant advantages over DPO and its variants. SynPO prevents negative preferences from dominating the optimization, explicitly preserves the model's language capability to avoid deviation of the optimization objective, and improves training efficiency by eliminating the need for the reference model. We extensively evaluate SynPO not only on video captioning benchmarks (e.g., VDC, VDD, VATEX) but also across well-established NLP tasks, including general language understanding and preference evaluation, using diverse pretrained models. Results demonstrate that SynPO consistently outperforms DPO variants while achieving 20\\% improvement in training efficiency. Code is available at https://github.com/longmalongma/SynPO",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»†ç²’åº¦è§†é¢‘å­—å¹•ç”Ÿæˆ(Fine-grained video captioning)åœ¨æ•æ‰ç»†å¾®åŠ¨æ€å’Œç»†èŠ‚æè¿°æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§ååŒæè¿°æ€§ä¸åå¥½ä¼˜åŒ–çš„æ¡†æ¶ SynPOã€‚ä¸ºäº†å¹³è¡¡æˆæœ¬ä¸æ•°æ®è´¨é‡ï¼Œä½œè€…é¦–å…ˆè®¾è®¡äº†ä¸€å¥—ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å›ºæœ‰å±æ€§ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)è¾…åŠ©çš„åå¥½å¯¹(preference pairs)æ„å»ºæµç¨‹ã€‚éšåæå‡ºçš„ååŒåå¥½ä¼˜åŒ–(SynPO)æ–¹æ³•ï¼Œé€šè¿‡é˜²æ­¢è´Ÿé¢åå¥½ä¸»å¯¼ä¼˜åŒ–è¿‡ç¨‹å¹¶æ˜¾å¼ä¿ç•™æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£å†³äº†ç›´æ¥åå¥½ä¼˜åŒ–(DPO)åŠå…¶å˜ä½“å­˜åœ¨çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼ŒSynPO é€šè¿‡å–æ¶ˆå¯¹å‚è€ƒæ¨¡å‹(reference model)çš„éœ€æ±‚ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚åœ¨ VDCã€VDDã€VATEX ç­‰è§†é¢‘å­—å¹•ç”ŸæˆåŸºå‡†åŠé€šç”¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSynPO çš„è¡¨ç°å§‹ç»ˆä¼˜äº DPO å˜ä½“ï¼Œå¹¶åœ¨è®­ç»ƒæ•ˆç‡ä¸Šå®ç°äº† 20% çš„æå‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00835v1",
      "published_date": "2025-06-01 04:51:49 UTC",
      "updated_date": "2025-06-01 04:51:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:03.791124+00:00"
    },
    {
      "arxiv_id": "2506.00832v1",
      "title": "Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models",
      "title_zh": "é¢å‘ TTS æ¨¡å‹äº‹åéŸµå¾‹è°ƒèŠ‚ä¸å‘éŸ³çº é”™çš„åäº‹å®æ¿€æ´»ç¼–è¾‘",
      "authors": [
        "Kyowoon Lee",
        "Artyom Stitsyuk",
        "Gunu Jho",
        "Inchul Hwang",
        "Jaesik Choi"
      ],
      "abstract": "Recent advances in Text-to-Speech (TTS) have significantly improved speech naturalness, increasing the demand for precise prosody control and mispronunciation correction. Existing approaches for prosody manipulation often depend on specialized modules or additional training, limiting their capacity for post-hoc adjustments. Similarly, traditional mispronunciation correction relies on grapheme-to-phoneme dictionaries, making it less practical in low-resource settings. We introduce Counterfactual Activation Editing, a model-agnostic method that manipulates internal representations in a pre-trained TTS model to achieve post-hoc control of prosody and pronunciation. Experimental results show that our method effectively adjusts prosodic features and corrects mispronunciations while preserving synthesis quality. This opens the door to inference-time refinement of TTS outputs without retraining, bridging the gap between pre-trained TTS models and editable speech synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Counterfactual Activation Editingï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³ (model-agnostic) çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ“çºµé¢„è®­ç»ƒæ–‡æœ¬è½¬è¯­éŸ³ (TTS) æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºæ¥å®ç°äº‹å (post-hoc) éŸµå¾‹æ§åˆ¶å’Œå‘éŸ³çº æ­£ã€‚é’ˆå¯¹ç°æœ‰éŸµå¾‹è°ƒèŠ‚æ–¹æ³•ä¾èµ–ä¸“ç”¨æ¨¡å—æˆ–é¢å¤–è®­ç»ƒï¼Œä»¥åŠä¼ ç»Ÿè¯¯è¯»çº æ­£è¿‡åº¦ä¾èµ–éŸ³ç´ å­—å…¸ç­‰å±€é™æ€§ï¼Œè¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æ›´çµæ´»çš„æ¨æ–­é˜¶æ®µä¼˜åŒ–æ‰‹æ®µã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥å¹²é¢„æ¨¡å‹å†…éƒ¨çš„æ¿€æ´»å€¼ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„å‰æä¸‹å®ç°å¯¹è¾“å‡ºè¯­éŸ³éŸµå¾‹ç‰¹å¾çš„ç²¾ç¡®è°ƒæ•´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCounterfactual Activation Editing èƒ½å¤Ÿæœ‰æ•ˆè°ƒæ•´éŸµå¾‹ç‰¹å¾å¹¶çº æ­£è¯¯è¯»ï¼ŒåŒæ—¶ä¿æŒäº†åˆæˆè¯­éŸ³çš„é«˜è´¨é‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºé¢„è®­ç»ƒ TTS æ¨¡å‹ä¸å¯ç¼–è¾‘è¯­éŸ³åˆæˆ (editable speech synthesis) ä¹‹é—´æ¶èµ·äº†æ¡¥æ¢ï¼Œå®ç°äº†åœ¨æ¨ç†é˜¶æ®µ (inference-time) å¯¹è¾“å‡ºç»“æœçš„ç²¾ç»†åŒ–æ”¹è¿›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00832v1",
      "published_date": "2025-06-01 04:33:37 UTC",
      "updated_date": "2025-06-01 04:33:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:04.533978+00:00"
    },
    {
      "arxiv_id": "2506.00831v2",
      "title": "A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems",
      "title_zh": "é¢å‘äº¤é€šä¿¡æ¯ç‰©ç†ç³»ç»Ÿçš„å¤§è¯­è¨€æ¨¡å‹æ”¯æŒçš„å¨èƒå»ºæ¨¡æ¡†æ¶",
      "authors": [
        "M Sabbir Salek",
        "Mashrur Chowdhury",
        "Muhaimin Bin Munir",
        "Yuchen Cai",
        "Mohammad Imtiaz Hasan",
        "Jean-Michel Tine",
        "Latifur Khan",
        "Mizanur Rahman"
      ],
      "abstract": "Existing threat modeling frameworks related to transportation cyber-physical systems (CPS) are often narrow in scope, labor-intensive, and require substantial cybersecurity expertise. To this end, we introduce the Transportation Cybersecurity and Resiliency Threat Modeling Framework (TraCR-TMF), a large language model (LLM)-based threat modeling framework for transportation CPS that requires limited cybersecurity expert intervention. TraCR-TMF identifies threats, potential attack techniques, and relevant countermeasures for transportation CPS. Three LLM-based approaches support these identifications: (i) a retrieval-augmented generation approach requiring no cybersecurity expert intervention, (ii) an in-context learning approach with low expert intervention, and (iii) a supervised fine-tuning approach with moderate expert intervention. TraCR-TMF offers LLM-based attack path identification for critical assets based on vulnerabilities across transportation CPS entities. Additionally, it incorporates the Common Vulnerability Scoring System (CVSS) scores of known exploited vulnerabilities to prioritize threat mitigations. The framework was evaluated through two cases. First, the framework identified relevant attack techniques for various transportation CPS applications, 73% of which were validated by cybersecurity experts as correct. Second, the framework was used to identify attack paths for a target asset in a real-world cyberattack incident. TraCR-TMF successfully predicted exploitations, like lateral movement of adversaries, data exfiltration, and data encryption for ransomware, as reported in the incident. These findings show the efficacy of TraCR-TMF in transportation CPS threat modeling, while reducing the need for extensive involvement of cybersecurity experts. To facilitate real-world adoptions, all our codes are shared via an open-source repository.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TraCR-TMFï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)çš„å¨èƒå»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äº¤é€šä¿¡æ¯ç‰©ç†ç³»ç»Ÿ(Transportation Cyber-Physical Systems, CPS)ä¸­ç°æœ‰å»ºæ¨¡æ–¹æ³•èŒƒå›´ç‹­çª„ã€åŠ³åŠ¨å¯†é›†ä¸”é«˜åº¦ä¾èµ–ä¸“å®¶ç»éªŒçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ã€ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)å’Œç›‘ç£å¾®è°ƒ(Supervised fine-tuning)ä¸‰ç§æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„ä¸“å®¶å¹²é¢„ä¸‹è¯†åˆ«å¨èƒã€æ½œåœ¨æ”»å‡»æŠ€æœ¯åŠç›¸åº”çš„å¯¹ç­–ã€‚TraCR-TMF æ”¯æŒé’ˆå¯¹å…³é”®èµ„äº§çš„æ”»å‡»è·¯å¾„è¯†åˆ«ï¼Œå¹¶åˆ©ç”¨é€šç”¨æ¼æ´è¯„åˆ†ç³»ç»Ÿ(CVSS)å¾—åˆ†æ¥ç¡®å®šå¨èƒç¼“è§£æªæ–½çš„ä¼˜å…ˆçº§ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶è¯†åˆ«çš„æ”»å‡»æŠ€æœ¯ä¸­æœ‰ 73% å¾—åˆ°äº†ä¸“å®¶çš„å‡†ç¡®æ€§éªŒè¯ï¼Œå¹¶æˆåŠŸé¢„æµ‹äº†çœŸå®å‹’ç´¢è½¯ä»¶æ”»å‡»äº‹ä»¶ä¸­çš„æ¨ªå‘ç§»åŠ¨(Lateral movement)å’Œæ•°æ®æ³„éœ²(Data exfiltration)ç­‰è¡Œä¸ºã€‚ç ”ç©¶è¯æ˜äº† TraCR-TMF åœ¨é™ä½ä¸“å®¶å‚ä¸éœ€æ±‚çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆæå‡äº¤é€š CPS çš„å¨èƒå»ºæ¨¡æ•ˆç‡ï¼Œç›¸å…³ä»£ç å·²å¼€æºã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00831v2",
      "published_date": "2025-06-01 04:33:34 UTC",
      "updated_date": "2025-07-28 14:17:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:05.464830+00:00"
    },
    {
      "arxiv_id": "2506.00829v2",
      "title": "COMPKE: Complex Question Answering under Knowledge Editing",
      "title_zh": "COMPKEï¼šçŸ¥è¯†ç¼–è¾‘ä¸‹çš„å¤æ‚é—®ç­”",
      "authors": [
        "Keyuan Cheng",
        "Zijian Kan",
        "Zhixian He",
        "Zhuoran Zhang",
        "Muhammad Asif Ali",
        "Ke Xu",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºç›®å‰çš„ Knowledge Editing è¯„æµ‹åŸºå‡†éš¾ä»¥æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨ç°å®å¤æ‚æ¨ç†åœºæ™¯ï¼ˆå¦‚ä¸€å¯¹å¤šå…³ç³»å’Œå¤šæ­¥é€»è¾‘äº¤é›†ï¼‰ä¸­åº”ç”¨æ›´æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼Œä¸ºæ­¤æå‡ºäº†åä¸º COMPKE çš„æ–°åŸºå‡†ã€‚COMPKE åŒ…å« 11,924 ä¸ªåæ˜ çœŸå®ç”Ÿæ´»æƒ…å†µçš„å¤æ‚é—®é¢˜ï¼Œæœ‰æ•ˆå¡«è¡¥äº†è¯¥é¢†åŸŸçš„è¯„ä¼°ç©ºç™½ã€‚é€šè¿‡å¯¹å››ç§ Knowledge Editing æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šçš„æ·±å…¥è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ä¸åŒæ¨¡å‹é—´çš„æ€§èƒ½è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼ŒMeLLo æ–¹æ³•åœ¨ GPT-4O-MINI ä¸Šçš„å‡†ç¡®ç‡ä¸º 39.47ï¼Œè€Œåœ¨ QWEN2.5-3B ä¸Šåˆ™å¤§å¹…ä¸‹é™è‡³ 3.83ã€‚ä½œè€…è¿›ä¸€æ­¥ä»æ–¹æ³•è®ºå’Œæ¨¡å‹ç‰¹å®šè§†è§’å‡ºå‘ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç©¶äº†å¯¼è‡´è¿™äº›å·®å¼‚çš„æ½œåœ¨åŸå› ã€‚è¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†ç°æœ‰çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯çš„å±€é™æ€§ï¼Œä¹Ÿä¸ºæœªæ¥å¼€å‘æ›´å…·é²æ£’æ€§çš„å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†æ›´æ–°æ–¹æ¡ˆæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.00829v2",
      "published_date": "2025-06-01 04:26:46 UTC",
      "updated_date": "2025-06-03 16:03:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:04.941342+00:00"
    },
    {
      "arxiv_id": "2506.00826v2",
      "title": "HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs",
      "title_zh": "HERGCï¼šé¢å‘å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±çš„å¼‚æ„ä¸“å®¶è¡¨ç¤ºä¸ç”Ÿæˆå¼è¡¥å…¨",
      "authors": [
        "Yongkang Xiao",
        "Rui Zhang"
      ],
      "abstract": "Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. multimodal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent large language models (LLMs), empowered by massive parameter scales and pretraining on vast corpora, have demonstrated strong reasoning abilities across various tasks. However, their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a flexible Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor, implemented via either in-context learning or lightweight fine-tuning, to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving superior performance over existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HERGCï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€çŸ¥è¯†å›¾è°± (MMKGs) çš„å¼‚æ„ä¸“å®¶è¡¨ç¤ºä¸ç”Ÿæˆå¼è¡¥å…¨æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±è¡¥å…¨ (MMKGC) çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨é—­åˆä¸–ç•Œå‡è®¾å’Œåˆ¤åˆ«å¼ç›®æ ‡ä¸‹çš„å±€é™æ€§ï¼ŒHERGC å……åˆ†åˆ©ç”¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¼ºå¤§çš„æ¨ç†æ½œåŠ›ã€‚è¯¥æ¡†æ¶é¦–å…ˆéƒ¨ç½²å¼‚æ„ä¸“å®¶è¡¨ç¤ºæ£€ç´¢å™¨ (Heterogeneous Experts Representation Retriever) æ¥ä¸°å¯Œå’Œèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶ä¸ºæ¯ä¸ªä¸å®Œæ•´çš„ä¸‰å…ƒç»„æ£€ç´¢ç´§å‡‘çš„å€™é€‰é›†ã€‚æ¥ç€ï¼Œåˆ©ç”¨é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹  (In-context learning) æˆ–è½»é‡åŒ–å¾®è°ƒå®ç°çš„ç”Ÿæˆå¼ LLM é¢„æµ‹å™¨ (Generative LLM Predictor)ï¼Œä»å€™é€‰é›†ä¸­å‡†ç¡®è¯†åˆ«æ­£ç¡®ç­”æ¡ˆã€‚åœ¨ä¸‰ä¸ªæ ‡å‡† MMKG åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº† HERGC çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„ä¸»æµæ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00826v2",
      "published_date": "2025-06-01 04:12:25 UTC",
      "updated_date": "2025-08-08 18:42:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:05.626884+00:00"
    },
    {
      "arxiv_id": "2506.00821v2",
      "title": "SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models",
      "title_zh": "SafeGenesï¼šåŸºå› ç»„åŸºç¡€æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§è¯„ä¼°",
      "authors": [
        "Huixin Zhan",
        "Clovis Barbour",
        "Jason H. Moore"
      ],
      "abstract": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated significant success in variant effect prediction. However, their adversarial robustness remains largely unexplored. To address this gap, we propose SafeGenes: a framework for Secure analysis of genomic foundation models, leveraging adversarial attacks to evaluate robustness against both engineered near-identical adversarial Genes and embedding-space manipulations. In this study, we assess the adversarial vulnerabilities of GFMs using two approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM introduces minimal perturbations to input sequences, while the soft prompt attack optimizes continuous embeddings to manipulate model predictions without modifying the input tokens. By combining these techniques, SafeGenes provides a comprehensive assessment of GFM susceptibility to adversarial manipulation. Targeted soft prompt attacks induced severe degradation in MLM-based shallow architectures such as ProteinBERT, while still producing substantial failure modes even in high-capacity foundation models such as ESM1b and ESM1v. These findings expose critical vulnerabilities in current foundation models, opening new research directions toward improving their security and robustness in high-stakes genomic applications such as variant effect prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SafeGenesæ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§è¯„ä¼°åŸºå› ç»„åŸºç¡€æ¨¡å‹(Genomic Foundation Models, GFMs)åœ¨å˜å¼‚æ•ˆåº”é¢„æµ‹ä¸­çš„å¯¹æŠ—é²æ£’æ€§(Adversarial Robustness)ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•(Fast Gradient Sign Method, FGSM)ä¸è½¯æç¤ºæ”»å‡»(Soft Prompt Attack)ï¼Œåˆ†åˆ«ä»åºåˆ—å¾®æ‰°å’ŒåµŒå…¥ç©ºé—´æ“çºµä¸¤ä¸ªç»´åº¦æ¢æµ‹æ¨¡å‹çš„å®‰å…¨æ¼æ´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹æ€§çš„è½¯æç¤ºæ”»å‡»ä¼šå¯¼è‡´ProteinBERTç­‰æµ…å±‚æ¨¡å‹æ€§èƒ½å‰§çƒˆé€€åŒ–ï¼Œå³ä½¿æ˜¯ESM1bå’ŒESM1vç­‰é«˜å®¹é‡æ¨¡å‹ä¹Ÿä¼šäº§ç”Ÿæ˜¾è‘—çš„å¤±æ•ˆæ¨¡å¼ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰GFMsåœ¨é¢å¯¹å¯¹æŠ—æ€§æ“çºµæ—¶çš„è„†å¼±æ€§ï¼Œä¸ºæœªæ¥åœ¨åŸºå› ç»„å­¦ç­‰é«˜é£é™©åº”ç”¨é¢†åŸŸæ„å»ºæ›´å®‰å…¨ã€æ›´å…·é²æ£’æ€§çš„åŸºç¡€æ¨¡å‹æä¾›äº†é‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00821v2",
      "published_date": "2025-06-01 03:54:03 UTC",
      "updated_date": "2025-12-02 22:16:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:19.187807+00:00"
    },
    {
      "arxiv_id": "2506.00819v1",
      "title": "DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving",
      "title_zh": "DriveMindï¼šåŸºäºåŒè§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Dawood Wasif",
        "Terrence J Moore",
        "Chandan K Reddy",
        "Jin-Hee Cho"
      ],
      "abstract": "End-to-end autonomous driving systems map sensor data directly to control commands, but remain opaque, lack interpretability, and offer no formal safety guarantees. While recent vision-language-guided reinforcement learning (RL) methods introduce semantic feedback, they often rely on static prompts and fixed objectives, limiting adaptability to dynamic driving scenes. We present DriveMind, a unified semantic reward framework that integrates: (i) a contrastive Vision-Language Model (VLM) encoder for stepwise semantic anchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via chain-of-thought (CoT) distillation, for dynamic prompt generation upon semantic drift; (iii) a hierarchical safety module enforcing kinematic constraints (e.g., speed, lane centering, stability); and (iv) a compact predictive world model to reward alignment with anticipated ideal states. DriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route completion, and near-zero collisions in CARLA Town 2, outperforming baselines by over 4% in success rate. Its semantic reward generalizes zero-shot to real dash-cam data with minimal distributional shift, demonstrating robust cross-domain alignment and potential for real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DriveMindï¼Œä¸€ç§åŸºäºåŒè§†è§‰è¯­è¨€æ¨¡å‹(Dual-VLM)çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œä»¥åŠä¼ ç»ŸVLM-RLæ–¹æ³•éš¾ä»¥é€‚åº”åŠ¨æ€åœºæ™¯çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¯”å¼VLMç¼–ç å™¨å®ç°é€æ­¥è¯­ä¹‰é”šå®šï¼Œå¹¶åˆ©ç”¨ç»ç”±é“¾å¼æ€ç»´(Chain-of-Thought)è’¸é¦å¾®è°ƒçš„æ–°å‹è§¦å‘å¼VLMç¼–ç å™¨-è§£ç å™¨ï¼Œåœ¨å‘ç”Ÿè¯­ä¹‰åç§»æ—¶ç”ŸæˆåŠ¨æ€æç¤ºã€‚åŒæ—¶ï¼Œç³»ç»Ÿç»“åˆäº†æ‰§è¡Œè¿åŠ¨å­¦çº¦æŸçš„åˆ†å±‚å®‰å…¨æ¨¡å—å’Œé¢„æµ‹ä¸–ç•Œæ¨¡å‹(world model)ï¼Œä»¥ç¡®ä¿é©¾é©¶è¡Œä¸ºçš„ç¨³å®šæ€§å’Œé¢„è§æ€§ã€‚åœ¨CARLA Town 2çš„å®éªŒä¸­ï¼ŒDriveMindåœ¨å¹³å‡é€Ÿåº¦ã€è·¯å¾„å®Œæˆç‡å’Œç¢°æ’ç‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒæˆåŠŸç‡æå‡è¶…è¿‡4%ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿçš„è¯­ä¹‰å¥–åŠ±æœºåˆ¶å±•ç°å‡ºäº†å‘çœŸå®è¡Œè½¦è®°å½•ä»ªæ•°æ®çš„é›¶æ ·æœ¬(zero-shot)æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åœºæ™¯éƒ¨ç½²ä¸­çš„é²æ£’æ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00819v1",
      "published_date": "2025-06-01 03:51:09 UTC",
      "updated_date": "2025-06-01 03:51:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:37.796713+00:00"
    },
    {
      "arxiv_id": "2506.00816v1",
      "title": "L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning",
      "title_zh": "L3Aï¼šé¢å‘å¤šæ ‡ç­¾ç±»å¢é‡å­¦ä¹ çš„æ ‡ç­¾å¢å¼ºè§£æè‡ªé€‚åº”",
      "authors": [
        "Xiang Zhang",
        "Run He",
        "Jiao Chen",
        "Di Fang",
        "Ming Li",
        "Ziqian Zeng",
        "Cen Chen",
        "Huiping Zhuang"
      ],
      "abstract": "Class-incremental learning (CIL) enables models to learn new classes continually without forgetting previously acquired knowledge. Multi-label CIL (MLCIL) extends CIL to a real-world scenario where each sample may belong to multiple classes, introducing several challenges: label absence, which leads to incomplete historical information due to missing labels, and class imbalance, which results in the model bias toward majority classes. To address these challenges, we propose Label-Augmented Analytic Adaptation (L3A), an exemplar-free approach without storing past samples. L3A integrates two key modules. The pseudo-label (PL) module implements label augmentation by generating pseudo-labels for current phase samples, addressing the label absence problem. The weighted analytic classifier (WAC) derives a closed-form solution for neural networks. It introduces sample-specific weights to adaptively balance the class contribution and mitigate class imbalance. Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms existing methods in MLCIL tasks. Our code is available at https://github.com/scut-zx/L3A.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ ‡ç­¾ç±»å¢é‡å­¦ä¹ (Multi-Label Class Incremental Learning, MLCIL)ä¸­å­˜åœ¨çš„æ ‡ç­¾ç¼ºå¤±(label absence)å’Œç±»åˆ«ä¸å¹³è¡¡(class imbalance)æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºL3A(Label-Augmented Analytic Adaptation)çš„æ— éœ€å­˜å‚¨å†å²æ ·æœ¬(exemplar-free)çš„æ–°æ¡†æ¶ã€‚L3Aé›†æˆäº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼Œå…¶ä¸­ä¼ªæ ‡ç­¾(pseudo-label, PL)æ¨¡å—é€šè¿‡ä¸ºå½“å‰é˜¶æ®µæ ·æœ¬ç”Ÿæˆä¼ªæ ‡ç­¾æ¥å®ç°æ ‡ç­¾å¢å¼ºï¼Œæœ‰æ•ˆè§£å†³äº†å› æ ‡ç­¾ç¼ºå¤±å¯¼è‡´çš„è¿‡å¾€ä¿¡æ¯ä¸å®Œæ•´é—®é¢˜ã€‚åŠ æƒè§£æåˆ†ç±»å™¨(weighted analytic classifier, WAC)æ¨¡å—åˆ™ä¸ºç¥ç»ç½‘ç»œæ¨å¯¼å‡ºäº†é—­å¼è§£(closed-form solution)ï¼Œå¹¶å¼•å…¥æ ·æœ¬ç‰¹å®šæƒé‡ä»¥è‡ªé€‚åº”åœ°å¹³è¡¡ä¸åŒç±»åˆ«çš„è´¡çŒ®ï¼Œä»è€Œç¼“è§£æ¨¡å‹å¯¹å¤šæ•°ç±»çš„åå·®ã€‚åœ¨MS-COCOå’ŒPASCAL VOCæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒL3Aåœ¨MLCILä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶é€šè¿‡ç»“åˆæ ‡ç­¾å¢å¼ºä¸è§£æè‡ªé€‚åº”æŠ€æœ¯ï¼Œä¸ºè§£å†³å¤æ‚å¤šæ ‡ç­¾ç¯å¢ƒä¸‹çš„æŒç»­å­¦ä¹ é—®é¢˜æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICML2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00816v1",
      "published_date": "2025-06-01 03:45:19 UTC",
      "updated_date": "2025-06-01 03:45:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:40.384350+00:00"
    },
    {
      "arxiv_id": "2506.00808v2",
      "title": "Unlearning Inversion Attacks for Graph Neural Networks",
      "title_zh": "é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œçš„é—å¿˜å­¦ä¹ åæ¼”æ”»å‡»",
      "authors": [
        "Jiahao Zhang",
        "Yilong Wang",
        "Zhiwei Zhang",
        "Xiaorui Liu",
        "Suhang Wang"
      ],
      "abstract": "Graph unlearning methods aim to efficiently remove the impact of sensitive data from trained GNNs without full retraining, assuming that deleted information cannot be recovered. In this work, we challenge this assumption by introducing the graph unlearning inversion attack: given only black-box access to an unlearned GNN and partial graph knowledge, can an adversary reconstruct the removed edges? We identify two key challenges: varying probability-similarity thresholds for unlearned versus retained edges, and the difficulty of locating unlearned edge endpoints, and address them with TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical and empirical pattern showing that nodes adjacent to unlearned edges exhibit a large drop in model confidence. Second, we design an adaptive prediction mechanism that applies different similarity thresholds to unlearned and other membership edges. Our framework flexibly integrates existing membership inference techniques and extends them with trend features. Experiments on four real-world datasets demonstrate that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines, exposing a critical privacy vulnerability in current graph unlearning methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾æ’¤é”€å­¦ä¹ (Graph unlearning)ä¸­çš„éšç§é£é™©ï¼Œæå‡ºäº†é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)çš„æ’¤é”€å­¦ä¹ åè½¬æ”»å‡»æ¡†æ¶TrendAttackã€‚è¯¥æ”»å‡»æ—¨åœ¨ä»…é€šè¿‡é»‘ç›’è®¿é—®å’Œéƒ¨åˆ†å›¾çŸ¥è¯†ï¼Œé‡å»ºå·²è¢«æ¨¡å‹ç§»é™¤çš„æ•æ„Ÿè¾¹ï¼Œä»è€ŒæŒ‘æˆ˜äº†æ’¤é”€ä¿¡æ¯æ— æ³•æ¢å¤çš„ä¼ ç»Ÿå‡è®¾ã€‚ç ”ç©¶è¯†åˆ«å¹¶åˆ©ç”¨äº†â€œç½®ä¿¡åº¦é™·é˜±â€(confidence pitfall)ç°è±¡ï¼Œå³ä¸æ’¤é”€è¾¹ç›¸é‚»çš„èŠ‚ç‚¹åœ¨æ¨¡å‹ç½®ä¿¡åº¦ä¸Šä¼šè¡¨ç°å‡ºæ˜¾è‘—ä¸‹é™ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†è‡ªé€‚åº”é¢„æµ‹æœºåˆ¶æ¥åº”å¯¹ä¸åŒçš„æˆå‘˜è¾¹é˜ˆå€¼æŒ‘æˆ˜ã€‚é€šè¿‡çµæ´»é›†æˆç°æœ‰çš„æˆå‘˜æ¨ç†(membership inference)æŠ€æœ¯å¹¶å¼•å…¥è¶‹åŠ¿ç‰¹å¾ï¼ŒTrendAttackèƒ½å¤Ÿç²¾å‡†å®šä½å¹¶æ¢å¤è¢«åˆ é™¤çš„æ•°æ®ã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰å›¾æ’¤é”€å­¦ä¹ æ–¹æ³•åœ¨ä¿æŠ¤éšç§æ–¹é¢å­˜åœ¨çš„å…³é”®æ¼æ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00808v2",
      "published_date": "2025-06-01 03:23:04 UTC",
      "updated_date": "2025-12-05 23:53:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:44.812936+00:00"
    },
    {
      "arxiv_id": "2506.00807v1",
      "title": "Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision",
      "title_zh": "é€šè¿‡å®šåˆ¶åŒ–æ€ç»´ä¸èåˆå†³ç­–å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´åºåˆ—åˆ†ç±»æ¨ç†èƒ½åŠ›",
      "authors": [
        "Jiahui Zhou",
        "Dan Li",
        "Lin Li",
        "Zhuomin Chen",
        "Shunyu Wu",
        "Haozheng Ye",
        "Jian Lou",
        "Costas J. Spanos"
      ],
      "abstract": "The reasoning capabilities of large language models (LLMs) have significantly advanced their performance by enabling in-depth understanding of diverse tasks. With growing interest in applying LLMs to the time series domain, this has proven nontrivial, as evidenced by the limited efficacy of straightforwardly adapting text-domain reasoning techniques. Although recent work has shown promise in several time series tasks, further leveraging advancements in LLM reasoning remains under-explored for time series classification (TSC) tasks, despite their prevalence and significance in many real-world applications. In this paper, we propose ReasonTSC, a novel framework designed to effectively leverage LLM reasoning for time series classification through both a multi-turn reasoning and a fused decision-making strategy tailored to TSC. Rather than straightforwardly applying existing reasoning techniques or relying solely on LLMs' built-in reasoning capabilities, ReasonTSC first steers the model to think over the essential characteristics of time series data. Next, it integrates predictions and confidence scores from plug-in classifiers, e.g., domain-specific time series models, as in-context examples. Finally, ReasonTSC guides the LLM through a structured reasoning process: it evaluates the initial assessment, backtracks to consider alternative hypotheses, and compares their merits before arriving at a final classification. Extensive experiments and systematic ablation studies demonstrate that ReasonTSC consistently outperforms both existing time series reasoning baselines and plug-in models, and is even capable of identifying and correcting plug-in models' false predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ReasonTSCï¼Œä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ—¶é—´åºåˆ—åˆ†ç±» (Time Series Classification, TSC) ä»»åŠ¡ä¸­æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ¨ç†æŠ€æœ¯åœ¨æ—¶é—´åºåˆ—é¢†åŸŸæœ‰æ•ˆæ€§å—é™çš„é—®é¢˜ï¼ŒReasonTSC é‡‡ç”¨äº†ä¸“ä¸º TSC è®¾è®¡çš„å¤šè½®æ¨ç† (multi-turn reasoning) ä¸èåˆå†³ç­– (fused decision-making) ç­–ç•¥ã€‚è¯¥æ¡†æ¶é¦–å…ˆå¼•å¯¼æ¨¡å‹æ·±å…¥åˆ†ææ—¶é—´åºåˆ—æ•°æ®çš„æ ¸å¿ƒç‰¹å¾ï¼Œå¹¶æ•´åˆæ¥è‡ªé¢†åŸŸç‰¹å®šæ’ä»¶åˆ†ç±»å™¨ (plug-in classifiers) çš„é¢„æµ‹ç»“æœå’Œç½®ä¿¡åº¦å¾—åˆ†ä½œä¸ºä¸Šä¸‹æ–‡ç¤ºä¾‹ (in-context examples)ã€‚éšåï¼ŒReasonTSC æŒ‡å¯¼ LLM æ‰§è¡Œç»“æ„åŒ–æ¨ç†ï¼Œé€šè¿‡è¯„ä¼°åˆå§‹åˆ¤æ–­ã€å›æº¯å¤‡é€‰å‡è®¾å¹¶å¯¹æ¯”å„æ–¹æ¡ˆä¼˜åŠ£ï¼Œä»è€Œä¼˜åŒ–æœ€ç»ˆçš„åˆ†ç±»å†³ç­–ã€‚å¹¿æ³›çš„å®éªŒå’Œæ¶ˆå‡åˆ†æè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ€§èƒ½ä¸Šä¸€è‡´ä¼˜äºç°æœ‰çš„æ—¶é—´åºåˆ—æ¨ç†åŸºå‡†åŠæ’ä»¶æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒReasonTSC å±•ç°å‡ºäº†è¯†åˆ«å¹¶ä¿®æ­£æ’ä»¶æ¨¡å‹é”™è¯¯é¢„æµ‹çš„å“è¶Šèƒ½åŠ›ï¼Œä¸ºå¤æ‚æ—¶é—´åºåˆ—åˆ†æä»»åŠ¡æä¾›äº†æ›´å¯é çš„æ¨ç†è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00807v1",
      "published_date": "2025-06-01 03:15:54 UTC",
      "updated_date": "2025-06-01 03:15:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:43.875635+00:00"
    },
    {
      "arxiv_id": "2506.00797v1",
      "title": "Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning",
      "title_zh": "é¢å‘å…¨å±€æœ€ä¼˜ååŒå¼ºåŒ–å­¦ä¹ çš„åŠ¨ä½œä¾èµ–å›¾",
      "authors": [
        "Jianglin Ding",
        "Jingcheng Tang",
        "Gangshan Jing"
      ],
      "abstract": "Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) ä¸­è‡ªå›å½’åŠ¨ä½œä¾èµ–ç­–ç•¥ (auto-regressive action-dependent policies) å› æ™ºèƒ½ä½“æ•°é‡å¢åŠ è€Œå¯¼è‡´çš„è®¡ç®—å¤æ‚åº¦é«˜ã€å¯æ‰©å±•æ€§å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŠ¨ä½œä¾èµ–å›¾ (Action Dependency Graph, ADG) çš„é€šç”¨åŠ¨ä½œä¾èµ–ç­–ç•¥æ¡†æ¶ã€‚åœ¨åè°ƒå›¾ (coordination graphs) ç»“æ„çš„ MARL é—®é¢˜èƒŒæ™¯ä¸‹ï¼Œè®ºæ–‡è¯æ˜äº†æ»¡è¶³ç‰¹å®šæ¡ä»¶çš„ç¨€ç– ADG èƒ½å¤Ÿå®ç°å…¨å±€æœ€ä¼˜æ€§ï¼Œå¹¶æ®æ­¤å¼€å‘äº†å…·æœ‰ç†è®ºä¿è¯çš„è¡¨æ ¼ç­–ç•¥è¿­ä»£ç®—æ³• (tabular policy iteration algorithm)ã€‚é€šè¿‡å°†è¯¥æ¡†æ¶é›†æˆåˆ°å¤šç§æœ€å…ˆè¿› (SOTA) çš„ç®—æ³•å¹¶è¿›è¡Œå¤æ‚ç¯å¢ƒå®éªŒï¼Œç ”ç©¶ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨é€šç”¨åœºæ™¯ä¸‹çš„ç¨³å¥æ€§ä¸é€‚ç”¨æ€§ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé™ä½äº†å»ºæ¨¡åŠ¨ä½œä¾èµ–çš„å¤æ‚åº¦ï¼Œä¸ºè§£å†³æ›´å¹¿æ³›çš„åä½œå¼ºåŒ–å­¦ä¹ æŒ‘æˆ˜æä¾›äº†å…¨å±€æœ€ä¼˜çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00797v1",
      "published_date": "2025-06-01 02:58:20 UTC",
      "updated_date": "2025-06-01 02:58:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:45.274422+00:00"
    },
    {
      "arxiv_id": "2506.00794v1",
      "title": "Predicting Empirical AI Research Outcomes with Language Models",
      "title_zh": "åˆ©ç”¨è¯­è¨€æ¨¡å‹é¢„æµ‹å®è¯äººå·¥æ™ºèƒ½ç ”ç©¶ç»“æœ",
      "authors": [
        "Jiaxin Wen",
        "Chenglei Si",
        "Yueh-han Chen",
        "He He",
        "Shi Feng"
      ],
      "abstract": "Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. We build the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), we aim to predict which will perform better on a set of benchmarks. We scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after our base model's cut-off date for testing, and 6,000 pairs for training. We then develop a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human experts to compare with. In the NLP domain, our system beats human experts by a large margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. We verify that our system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, we evaluate our system on unpublished novel ideas, including ideas generated by an AI ideation agent. Our system achieves 63.6% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, our results outline a promising new direction for LMs to accelerate empirical AI research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è¯­è¨€æ¨¡å‹ (Language Models) é¢„æµ‹å®è¯ AI ç ”ç©¶æˆæœçš„èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³éªŒè¯ç ”ç©¶æƒ³æ³•æ‰€éœ€çš„é«˜æ˜‚äººåŠ›å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†é¦–ä¸ªæ­¤ç±»ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å« 1,585 å¯¹äººå·¥éªŒè¯çš„è®ºæ–‡æƒ³æ³•å¯¹ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç»“åˆå¾®è°ƒ GPT-4.1 ä¸è®ºæ–‡æ£€ç´¢æ™ºèƒ½ä½“ (paper retrieval agent) çš„ç³»ç»Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ NLP é¢†åŸŸä»¥ 64.4% çš„å‡†ç¡®ç‡æ˜¾è‘—è¶…è¿‡äººç±»ä¸“å®¶çš„ 48.9%ï¼Œåœ¨å…¨é›†æµ‹è¯•ä¸­æ›´è¾¾åˆ°äº† 77% çš„å‡†ç¡®ç‡ï¼Œè€Œ frontier LMs å¦‚ o3 çš„è¡¨ç°ä»…æ¥è¿‘éšæœºçŒœæµ‹ã€‚é²æ£’æ€§æµ‹è¯•ç¡®è®¤è¯¥ç³»ç»Ÿå¹¶éåˆ©ç”¨æƒ³æ³•å¤æ‚åº¦ç­‰è¡¨å±‚ç‰¹å¾è¿›è¡Œé¢„æµ‹ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨é¢„æµ‹æœªå‘è¡¨æˆ– AI ç”Ÿæˆçš„æ–°æƒ³æ³•æ—¶è¡¨ç°å‡º 63.6% çš„å‡†ç¡®ç‡ï¼Œè¯æ˜å…¶å¯ä½œä¸ºå¥–åŠ±æ¨¡å‹ (reward model) æ¥ä¼˜åŒ– AI æ„æ€æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨ LMs åŠ é€Ÿå®è¯ AI ç ”ç©¶å¼€è¾Ÿäº†ä¸€ä¸ªæå…·å‰æ™¯çš„æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00794v1",
      "published_date": "2025-06-01 02:46:31 UTC",
      "updated_date": "2025-06-01 02:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:21:55.885228+00:00"
    },
    {
      "arxiv_id": "2506.00788v1",
      "title": "Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation",
      "title_zh": "UML ç±»å›¾çš„è¡Œä¸ºå¢å¼ºï¼šå¤§è¯­è¨€æ¨¡å‹ç”¨äºæ–¹æ³•ç”Ÿæˆçš„å®è¯ç ”ç©¶",
      "authors": [
        "Djaber Rouabhia",
        "Ismail Hadjadj"
      ],
      "abstract": "Automating the enrichment of UML class diagrams with behavioral methods from natural language use cases is a significant challenge. This study evaluates nine large language models (LLMs) in augmenting a methodless UML diagram (21 classes, 17 relationships) using 21 structured waste-management use cases. A total of 90 diagrams (3,373 methods) were assessed across six metrics: method quantity, signature richness (visibility, names, parameters, return types), annotation completeness (linking to use cases/actions), structural fidelity, syntactic correctness (PlantUML compilation), and naming convergence (across models). All LLMs produced valid PlantUML diagrams adhering to UML conventions. Some models excelled in method coverage and annotation accuracy, while others showed richer parameterization but weaker traceability. These results demonstrate that LLMs can generate well-structured methods with consistent naming, advancing automated behavioral modeling. However, inconsistencies in annotations and signatures highlight the need for improved prompt engineering and model selection. The rapid generation of these methods supports Agile practices by enabling faster design iterations. Despite their capabilities, human oversight is essential to ensure accuracy, appropriateness, and semantic alignment. This positions LLMs as collaborative partners in software design. All experimental artifacts (\\texttt{.puml}, \\texttt{.png}, \\texttt{.csv}) are publicly available for reproducibility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å°†è‡ªç„¶è¯­è¨€ç”¨ä¾‹ (Use Cases) è½¬åŒ–ä¸º UML ç±»å›¾è¡Œä¸ºæ–¹æ³• (Methods) çš„è‡ªåŠ¨åŒ–å¢å¼ºè¿‡ç¨‹ã€‚ä½œè€…è¯„ä¼°äº† 9 ç§ä¸åŒçš„ LLMs åœ¨å¤„ç†åŒ…å« 21 ä¸ªç±»å’Œ 17 ä¸ªå…³ç³»çš„ç±»å›¾æ—¶çš„è¡¨ç°ï¼Œå¹¶é€šè¿‡æ–¹æ³•æ•°é‡ã€ç‰¹å¾ä¸°å¯Œåº¦ (Signature Richness)ã€æ³¨è§£å®Œæ•´æ€§ã€ç»“æ„ä¿çœŸåº¦ç­‰ 6 é¡¹æŒ‡æ ‡å¯¹ç”Ÿæˆçš„ 3,373 ä¸ªæ–¹æ³•è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰ LLMs éƒ½èƒ½ç”Ÿæˆç¬¦åˆ UML è§„èŒƒä¸”è¯­æ³•æ­£ç¡®çš„ PlantUML å›¾è¡¨ï¼Œå¹¶åœ¨å‘½åä¸€è‡´æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚å°½ç®¡éƒ¨åˆ†æ¨¡å‹åœ¨æ–¹æ³•è¦†ç›–ç‡å’Œæ³¨è§£å‡†ç¡®æ€§ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨å‚æ•°åŒ–æ·±åº¦å’Œå¯è¿½æº¯æ€§ä¸Šä»å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚è¯¥ç ”ç©¶è¯æ˜äº† LLMs åœ¨åŠ é€Ÿæ•æ·å¼€å‘ (Agile) è®¾è®¡è¿­ä»£ä¸­çš„å·¨å¤§æ½œåŠ›ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºåœ¨æ³¨è§£å’Œç‰¹å¾ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§ä»éœ€é€šè¿‡æ”¹è¿›æç¤ºå·¥ç¨‹ (Prompt Engineering) æ¥è§£å†³ã€‚æœ€ç»ˆç ”ç©¶è®¤ä¸ºï¼ŒLLMs åº”è¢«è§†ä¸ºè½¯ä»¶è®¾è®¡çš„åä½œä¼™ä¼´ï¼Œä½†ç°é˜¶æ®µä»éœ€äººå·¥ç›‘ç£ä»¥ç¡®ä¿å…¶ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸è¯­ä¹‰å¯¹é½ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00788v1",
      "published_date": "2025-06-01 02:33:40 UTC",
      "updated_date": "2025-06-01 02:33:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:22:11.591171+00:00"
    },
    {
      "arxiv_id": "2506.00785v3",
      "title": "GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning",
      "title_zh": "GeoChainï¼šé¢å‘åœ°ç†æ¨ç†çš„å¤šæ¨¡æ€é“¾å¼æ€ç»´",
      "authors": [
        "Sahiti Yerramilli",
        "Nilay Pande",
        "Rynaa Grover",
        "Jayant Sravan Tamarapalli"
      ],
      "abstract": "This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†GeoChainï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åˆ†æ­¥éª¤åœ°ç†æ¨ç†ï¼ˆgeographic reasoningï¼‰ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†146ä¸‡å¼ Mapillaryè¡—æ™¯å›¾åƒï¼Œå¹¶ä¸ºæ¯å¼ å›¾åƒé…å¯¹äº†ä¸€ä¸ªåŒ…å«21ä¸ªæ­¥éª¤çš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰é—®é¢˜åºåˆ—ï¼Œæ¶µç›–äº†è§†è§‰ã€ç©ºé—´ã€æ–‡åŒ–å’Œç²¾ç¡®åœ°ç†å®šä½å››ä¸ªæ¨ç†ç±»åˆ«ã€‚å›¾åƒè¿˜ä¸°å¯Œäº†è¯­ä¹‰åˆ†å‰²ï¼ˆsemantic segmentationï¼‰ä¿¡æ¯å’Œè§†è§‰å®šä½è¯„åˆ†ï¼Œå¼•å¯¼æ¨¡å‹ä»ç²—ç•¥å±æ€§é€æ­¥è¿‡æ¸¡åˆ°ç»†ç²’åº¦å®šä½ã€‚åœ¨å¯¹GPT-4.1ã€Claude 3.7å’ŒGemini 2.5ç­‰ä¸»æµæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹åœ¨è§†è§‰å®šä½ï¼ˆvisual groundingï¼‰å’Œé€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶éš¾ä»¥å®ç°ç²¾ç¡®çš„åœ°ç†å®šä½ã€‚GeoChainæä¾›äº†ä¸€å¥—ç¨³å¥çš„è¯Šæ–­æ–¹æ³•ï¼Œå¯¹äºæ¨åŠ¨MLLMsåœ¨å¤æ‚åœ°ç†æ¨ç†é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00785v3",
      "published_date": "2025-06-01 02:24:46 UTC",
      "updated_date": "2025-09-09 04:38:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:22:09.905116+00:00"
    },
    {
      "arxiv_id": "2506.00783v2",
      "title": "KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision",
      "title_zh": "KG-TRACESï¼šåˆ©ç”¨çŸ¥è¯†å›¾è°±çº¦æŸçš„è½¨è¿¹æ¨ç†ä¸å½’å› ç›‘ç£å¢å¼ºå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Rong Wu",
        "Pinlong Cai",
        "Jianbiao Mei",
        "Licheng Wen",
        "Tao Hu",
        "Xuemeng Yang",
        "Daocheng Fu",
        "Botian Shi"
      ],
      "abstract": "Large language models (LLMs) have made remarkable strides in various natural language processing tasks, but their performance on complex reasoning problems remains hindered by a lack of explainability and trustworthiness. This issue, often manifesting as hallucinations or unattributable reasoning processes, limits their applicability in complex reasoning scenarios. To address this, we propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain Explanation Supervision (KG-TRACES), a novel framework that enhances the reasoning ability of LLMs through explicit supervision over reasoning paths and processes. KG-TRACES jointly supervises the model to: (1) predict symbolic relation paths, (2) predict full triple-level reasoning paths, and (3) generate attribution-aware reasoning processes grounded in the reasoning paths. At inference phase, the model adapts to both KG-available and KG-unavailable scenarios, retrieving reasoning paths from a KG when possible or predicting plausible reasoning paths with only intrinsic knowledge when not. This design enables the model to reason in an explainable and source-attributable pattern. Through extensive experiments on complex reasoning tasks, we demonstrate that KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6% and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1% in F1 on CWQ. Moreover, we show its transferability to specialized domains such as medicine. By visualizing the intermediate steps of reasoning processes, we further show that the explicit supervision introduced by KG-TRACES leads to more stable and goal-directed reasoning processes, aligning closely with correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KG-TRACESæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ¨ç†ä¸­é¢ä¸´çš„å¯è§£é‡Šæ€§ä¸è¶³å’Œå¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¨ç†è½¨è¿¹å’Œå½’å› è¿‡ç¨‹è¿›è¡Œæ˜¾å¼ç›‘ç£ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶é¢„æµ‹ç¬¦å·å…³ç³»è·¯å¾„ã€ä¸‰å…ƒç»„æ¨ç†è·¯å¾„ä»¥åŠç”Ÿæˆå…·å¤‡å½’å› èƒ½åŠ›çš„æ¨ç†æ–‡æœ¬ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹å…·æœ‰çµæ´»æ€§ï¼Œæ—¢å¯ä»¥ä»ç°æœ‰çš„çŸ¥è¯†å›¾è°±(KG)ä¸­æ£€ç´¢è·¯å¾„ï¼Œä¹Ÿå¯ä»¥åœ¨KGä¸å¯ç”¨æ—¶åˆ©ç”¨å†…åœ¨çŸ¥è¯†é¢„æµ‹è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKG-TRACESåœ¨WebQSPå’ŒCWQç­‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAæ¨¡å‹ï¼Œå…¶ä¸­CWQçš„Hits@1æŒ‡æ ‡æå‡äº†4.8%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»ç–—ç­‰ä¸“ä¸šé¢†åŸŸå±•ç°å‡ºè‰¯å¥½çš„å¯è¿ç§»æ€§ï¼Œå¹¶é€šè¿‡å¯è§†åŒ–è¯æ˜äº†å…¶æ¨ç†è¿‡ç¨‹æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´åŠ ç¨³å®šä¸”å…·æœ‰æ˜ç¡®çš„ç›®æ ‡å¯¼å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00783v2",
      "published_date": "2025-06-01 02:20:45 UTC",
      "updated_date": "2025-10-20 12:31:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:22:06.691980+00:00"
    },
    {
      "arxiv_id": "2506.00782v1",
      "title": "Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning",
      "title_zh": "Jailbreak-R1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±èƒ½åŠ›æ¢ç©¶",
      "authors": [
        "Weiyang Guo",
        "Zesheng Shi",
        "Zhuo Li",
        "Yequan Wang",
        "Xuebo Liu",
        "Wenya Wang",
        "Fangming Liu",
        "Min Zhang",
        "Jing Li"
      ],
      "abstract": "As large language models (LLMs) grow in power and influence, ensuring their safety and preventing harmful output becomes critical. Automated red teaming serves as a tool to detect security vulnerabilities in LLMs without manual labor. However, most existing methods struggle to balance the effectiveness and diversity of red-team generated attack prompts. To address this challenge, we propose \\ourapproach, a novel automated red teaming training framework that utilizes reinforcement learning to explore and generate more effective attack prompts while balancing their diversity. Specifically, it consists of three training stages: (1) Cold Start: The red team model is supervised and fine-tuned on a jailbreak dataset obtained through imitation learning. (2) Warm-up Exploration: The model is trained in jailbreak instruction following and exploration, using diversity and consistency as reward signals. (3) Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually enhance the jailbreak performance of the red-team model. Extensive experiments on a variety of LLMs show that \\ourapproach effectively balances the diversity and effectiveness of jailbreak prompts compared to existing methods. Our work significantly improves the efficiency of red team exploration and provides a new perspective on automated red teaming.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Jailbreak-R1ï¼Œä¸€ç§æ—¨åœ¨æ¢ç´¢ Large Language Models (LLMs) è¶Šç‹±èƒ½åŠ›å¹¶æå‡è‡ªåŠ¨åŒ– Red Teaming æ•ˆç‡çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹ç°æœ‰è‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•æ–¹æ³•åœ¨æ”»å‡»æç¤ºè¯ (Attack Prompts) çš„æœ‰æ•ˆæ€§ä¸å¤šæ ·æ€§ä¹‹é—´éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ï¼Œè®¾è®¡äº†ä¸‰ä¸ªå…³é”®è®­ç»ƒé˜¶æ®µã€‚é¦–å…ˆé€šè¿‡æ¨¡ä»¿å­¦ä¹  (Imitation Learning) è·å–çš„æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒ (Supervised Fine-tuning) å®Œæˆå†·å¯åŠ¨ã€‚éšååœ¨é¢„çƒ­æ¢ç´¢é˜¶æ®µï¼Œåˆ©ç”¨å¤šæ ·æ€§å’Œä¸€è‡´æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œæå‡æ¨¡å‹éµå¾ªè¶Šç‹±æŒ‡ä»¤çš„èƒ½åŠ›ã€‚æœ€åé˜¶æ®µé€šè¿‡å¼•å…¥æ¸è¿›å¼è¶Šç‹±å¥–åŠ± (Progressive Jailbreak Rewards) è¿›ä¸€æ­¥å¼ºåŒ–çº¢é˜Ÿæ¨¡å‹çš„æ”»å‡»æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJailbreak-R1 åœ¨å¤šç§ä¸»æµ LLMs ä¸Šå‡èƒ½æœ‰æ•ˆå¹³è¡¡è¶Šç‹±æç¤ºè¯çš„è´¨é‡ä¸å¤šæ ·æ€§ï¼Œæ˜¾è‘—æå‡äº†çº¢é˜Ÿæ¢ç´¢çš„æ•ˆç‡ï¼Œä¸ºè‡ªåŠ¨åŒ–çº¢é˜ŸæŠ€æœ¯æä¾›äº†æ–°çš„ç ”ç©¶è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00782v1",
      "published_date": "2025-06-01 02:19:46 UTC",
      "updated_date": "2025-06-01 02:19:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:22:48.545027+00:00"
    },
    {
      "arxiv_id": "2506.00781v3",
      "title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles",
      "title_zh": "CoPï¼šåŸºäºåŸåˆ™ç»„åˆçš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åŒ–çº¢é˜Ÿæµ‹è¯•",
      "authors": [
        "Chen Xiong",
        "Pin-Yu Chen",
        "Tsung-Yi Ho"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) é¢ä¸´çš„è¶Šç‹±æ”»å‡» (Jailbreak attacks) åŠå®‰å…¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º CoP çš„è‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯• (Red-teaming) æ™ºèƒ½ä½“å·¥ä½œæµã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåœ¨äºåŸåˆ™ç»„åˆ (Composition-of-Principles) æœºåˆ¶ï¼Œé€šè¿‡å°†äººç±»æä¾›çš„çº¢é˜ŸåŸåˆ™ä½œä¸ºæŒ‡ä»¤ï¼Œç”± AI æ™ºèƒ½ä½“è‡ªåŠ¨ç¼–æ’æ”»å‡»ç­–ç•¥å¹¶ç”Ÿæˆè¶Šç‹±æç¤ºè¯ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCoP æä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°æ–°çš„æ”»å‡»ç­–ç•¥å¹¶å®ç°çº¢é˜Ÿæµ‹è¯•çš„è§„æ¨¡åŒ–ã€‚åœ¨é’ˆå¯¹é¢†å…ˆ LLMs çš„æµ‹è¯•ä¸­ï¼ŒCoP æ­ç¤ºäº†å‰æ‰€æœªæœ‰çš„å®‰å…¨éšæ‚£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å°†å·²çŸ¥çš„æœ€ä½³å•è½®æ”»å‡»æˆåŠŸç‡æœ€é«˜æå‡äº† 19.0 å€ã€‚è¿™ä¸€ç ”ç©¶ä¸ºä¸»åŠ¨æ¢ç´¢å‰æ²¿ AI æŠ€æœ¯çš„æ½œåœ¨é£é™©å’Œé”™è¯¯å®ä¾‹æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00781v3",
      "published_date": "2025-06-01 02:18:41 UTC",
      "updated_date": "2025-12-06 06:46:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:02.643274+00:00"
    },
    {
      "arxiv_id": "2506.00780v1",
      "title": "Do not Abstain! Identify and Solve the Uncertainty",
      "title_zh": "æ‹’ç»å›é¿ï¼šè¯†åˆ«å¹¶è§£å†³ä¸ç¡®å®šæ€§",
      "authors": [
        "Jingyu Liu",
        "Jingquan Peng",
        "xiaopeng Wu",
        "Xubin Li",
        "Tiezheng Ge",
        "Bo Zheng",
        "Yong Liu"
      ],
      "abstract": "Despite the widespread application of Large Language Models (LLMs) across various domains, they frequently exhibit overconfidence when encountering uncertain scenarios, yet existing solutions primarily rely on evasive responses (e.g., \"I don't know\") overlooks the opportunity of identifying and addressing the uncertainty to generate more satisfactory responses. To systematically investigate and improve LLMs' ability of recognizing and addressing the source of uncertainty, we introduce \\textbf{ConfuseBench}, a benchmark mainly focus on three types of uncertainty: document scarcity, limited capability, and query ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to accurately identify the root cause of uncertainty and solve it. They prefer to attribute uncertainty to query ambiguity while overlooking capability limitations, especially for those weaker models. To tackle this challenge, we first generate context-aware inquiries that highlight the confusing aspect of the original query. Then we judge the source of uncertainty based on the uniqueness of the inquiry's answer. Further we use an on-policy training method, InteractDPO to generate better inquiries. Experimental results demonstrate the efficacy of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢å¯¹ä¸ç¡®å®šåœºæ™¯æ—¶è¡¨ç°å‡ºçš„è¿‡åº¦è‡ªä¿¡ä»¥åŠç°æœ‰å›é¿å¼å›ç­”ç­–ç•¥çš„å±€é™æ€§ï¼Œæå‡ºäº†ConfuseBenchåŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹æ¢è®¨æ–‡æ¡£ç¨€ç¼ºã€èƒ½åŠ›å±€é™å’ŒæŸ¥è¯¢æ­§ä¹‰ä¸‰ç§ä¸ç¡®å®šæ€§ç±»å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰LLMsåœ¨è¯†åˆ«ä¸ç¡®å®šæ€§æ ¹æºæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯è¾ƒå¼±çš„æ¨¡å‹å¾€å¾€å€¾å‘äºå°†é—®é¢˜å½’å› äºæŸ¥è¯¢æ­§ä¹‰è€Œå¿½è§†è‡ªèº«çš„èƒ½åŠ›é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯¢é—®çš„æ–¹æ³•ï¼Œæ—¨åœ¨çªå‡ºæŸ¥è¯¢ä¸­çš„å›°æƒ‘ç‚¹ï¼Œå¹¶æ ¹æ®ç­”æ¡ˆçš„å”¯ä¸€æ€§æ¥åˆ¤å®šä¸ç¡®å®šæ€§çš„å…·ä½“æ¥æºã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†åŸºäºåœ¨çº¿ç­–ç•¥çš„InteractDPOè®­ç»ƒæ–¹æ³•ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„è¯¢é—®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½æœ‰æ•ˆå¢å¼ºLLMsè¯†åˆ«å¹¶è§£å†³ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆæ›´ä»¤ç”¨æˆ·æ»¡æ„çš„å“åº”ç»“æœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00780v1",
      "published_date": "2025-06-01 02:15:17 UTC",
      "updated_date": "2025-06-01 02:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:12.032735+00:00"
    },
    {
      "arxiv_id": "2506.02048v2",
      "title": "Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges",
      "title_zh": "é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡ LLM æ™ºèƒ½ä½“åœ¨å¯†ç å­¦ CTF æŒ‘æˆ˜ä¸­çš„æ€§èƒ½",
      "authors": [
        "Lajos Muzsai",
        "David Imolai",
        "AndrÃ¡s LukÃ¡cs"
      ],
      "abstract": "We present 'Random-Crypto', a procedurally generated cryptographic Capture The Flag (CTF) dataset designed to unlock the potential of Reinforcement Learning (RL) for LLM-based agents in security-sensitive domains. Cryptographic reasoning offers an ideal RL testbed: it combines precise validation, structured multi-step inference, and reliance on reliable computational tool use. Leveraging these properties, we fine-tune a Python tool-augmented Llama-3.1-8B via Group Relative Policy Optimization (GRPO) in a secure execution environment. The resulting agent achieves a significant improvement in Pass@8 on previously unseen challenges. Moreover, the improvements generalize to two external benchmarks: 'picoCTF', spanning both crypto and non-crypto tasks, and 'AICrypto MCQ', a multiple-choice benchmark of 135 cryptography questions. Ablation studies attribute the gains to enhanced tool usage and procedural reasoning. These findings position 'Random-Crypto' as a rich training ground for building intelligent, adaptable LLM agents capable of handling complex cybersecurity tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Random-Cryptoï¼Œè¿™æ˜¯ä¸€ä¸ªç¨‹åºåŒ–ç”Ÿæˆçš„å¯†ç å­¦å¤ºæ——èµ› (CTF) æ•°æ®é›†ï¼Œæ—¨åœ¨æŒ–æ˜å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) åœ¨å®‰å…¨æ•æ„Ÿé¢†åŸŸæå‡ LLM æ™ºèƒ½ä½“èƒ½åŠ›çš„æ½œåŠ›ã€‚å¯†ç å­¦æ¨ç†ç»“åˆäº†ç²¾ç¡®éªŒè¯ã€ç»“æ„åŒ–å¤šæ­¥æ¨ç†å’Œå¯¹è®¡ç®—å·¥å…·çš„ä¾èµ–ï¼Œä¸ºå¼ºåŒ–å­¦ä¹  (RL) æä¾›äº†ç†æƒ³çš„æµ‹è¯•å¹³å°ã€‚ç ”ç©¶äººå‘˜åœ¨å®‰å…¨æ‰§è¡Œç¯å¢ƒä¸­ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO) å¯¹é›†æˆäº† Python å·¥å…·çš„ Llama-3.1-8B æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“åœ¨æœªè§è¿‡çš„æŒ‘æˆ˜ä»»åŠ¡ä¸­ Pass@8 æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸåº”ç”¨äºåŒ…å«å¤šç§ä»»åŠ¡çš„ picoCTF å’Œå¯†ç å­¦å¤šé€‰é¢˜åŸºå‡† AICrypto MCQã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ€§èƒ½çš„æå‡ä¸»è¦æºäºå·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œç¨‹åºåŒ–æ¨ç†èƒ½åŠ›çš„å¢å¼ºã€‚è¯¥å‘ç°ç¡®ç«‹äº† Random-Crypto ä½œä¸ºä¸€ä¸ªä¸°å¯Œçš„è®­ç»ƒå¹³å°ï¼Œæœ‰åŠ©äºæ„å»ºèƒ½å¤Ÿå¤„ç†å¤æ‚ç½‘ç»œå®‰å…¨ä»»åŠ¡çš„æ™ºèƒ½ä¸”é€‚åº”æ€§å¼ºçš„ LLM æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02048v2",
      "published_date": "2025-06-01 01:59:52 UTC",
      "updated_date": "2025-08-17 22:28:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:10.990066+00:00"
    },
    {
      "arxiv_id": "2506.15715v1",
      "title": "NeuronSeek: On Stability and Expressivity of Task-driven Neurons",
      "title_zh": "NeuronSeekï¼šè®ºä»»åŠ¡é©±åŠ¨å‹ç¥ç»å…ƒçš„ç¨³å®šæ€§ä¸è¡¨è¾¾æ€§",
      "authors": [
        "Hanyu Pei",
        "Jing-Xiao Liao",
        "Qibin Zhao",
        "Ting Gao",
        "Shijun Zhang",
        "Xiaoge Zhang",
        "Feng-Lei Fan"
      ],
      "abstract": "Drawing inspiration from our human brain that designs different neurons for different tasks, recent advances in deep learning have explored modifying a network's neurons to develop so-called task-driven neurons. Prototyping task-driven neurons (referred to as NeuronSeek) employs symbolic regression (SR) to discover the optimal neuron formulation and construct a network from these optimized neurons. Along this direction, this work replaces symbolic regression with tensor decomposition (TD) to discover optimal neuronal formulations, offering enhanced stability and faster convergence. Furthermore, we establish theoretical guarantees that modifying the aggregation functions with common activation functions can empower a network with a fixed number of parameters to approximate any continuous function with an arbitrarily small error, providing a rigorous mathematical foundation for the NeuronSeek framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD framework not only achieves superior stability, but also is competitive relative to the state-of-the-art models across diverse benchmarks. The code is available at https://github.com/HanyuPei22/NeuronSeek.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å—äººç±»å¤§è„‘å¯å‘çš„ä»»åŠ¡é©±åŠ¨å‹ç¥ç»å…ƒ(Task-driven Neurons)ï¼Œå¹¶é’ˆå¯¹NeuronSeekæ¡†æ¶æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥å¼ é‡åˆ†è§£(Tensor Decomposition, TD)å–ä»£åŸæœ‰çš„ç¬¦å·å›å½’(Symbolic Regression)æ¥å‘ç°æœ€ä¼˜ç¥ç»å…ƒè¡¨è¿°ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚ç ”ç©¶è¿˜å»ºç«‹äº†ç†è®ºä¿è¯ï¼Œè¯æ˜é€šè¿‡ä¿®æ”¹å¸¦æœ‰é€šç”¨æ¿€æ´»å‡½æ•°çš„èšåˆå‡½æ•°(Aggregation Functions)ï¼Œå›ºå®šå‚æ•°é‡çš„ç½‘ç»œèƒ½å¤Ÿä»¥ä»»æ„ç²¾åº¦é€¼è¿‘ä»»ä½•è¿ç»­å‡½æ•°ï¼Œä¸ºè¯¥æ¡†æ¶å¥ å®šäº†ä¸¥å¯†çš„æ•°å­¦åŸºç¡€ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuronSeek-TDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸ä»…å±•ç°å‡ºä¼˜å¼‚çš„ç¨³å®šæ€§ï¼Œä¸”å…¶æ€§èƒ½ä¸å½“å‰çš„å…ˆè¿›æ¨¡å‹(State-of-the-art)ç›¸æ¯”ä¹Ÿæå…·ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15715v1",
      "published_date": "2025-06-01 01:36:27 UTC",
      "updated_date": "2025-06-01 01:36:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:17.269860+00:00"
    },
    {
      "arxiv_id": "2506.00772v1",
      "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning",
      "title_zh": "LIFTï¼šæ­å¼€çœŸç›¸çš„é¢çº±â€”â€”ç§©çº¦å‡åæ˜¾ç°çš„ä¸»æƒé‡åŠ©åŠ›èšç„¦æ¨ç†çš„æœ‰ç›‘ç£å¾®è°ƒ",
      "authors": [
        "Zihang Liu",
        "Tianyu Pang",
        "Oleg Balabanov",
        "Chaoqun Yang",
        "Tianjin Huang",
        "Lu Yin",
        "Yaoqing Yang",
        "Shiwei Liu"
      ],
      "abstract": "Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)é—®é¢˜ï¼Œæ—¨åœ¨å…‹æœå…¨å‚æ•°å¾®è°ƒ(Full FT)è®¡ç®—æˆæœ¬é«˜åŠæ˜“è¿‡æ‹Ÿåˆçš„å±€é™ã€‚ä½œè€…å‘ç°ç»è¿‡ä½ç§©è¿‘ä¼¼(Low-rank Approximation)åï¼Œç»å¯¹å€¼æœ€å¤§çš„æƒé‡ï¼ˆå³Principal Weightsï¼‰å¯¹æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œè¿™ä¸€å‘ç°è§£å†³äº†ç¨€ç–å¾®è°ƒä¸­å…³é”®å‚æ•°è¯†åˆ«éš¾çš„é—®é¢˜ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä½ç§©çŸ¥æƒ…ç¨€ç–å¾®è°ƒ(Low-rank Informed Sparse Fine-Tuning, LIFT)æ–¹æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»…æ›´æ–°å‰5%çš„ä¸»æƒé‡ã€‚å®éªŒè¯æ˜ï¼ŒLIFTåœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸€è‡´ä¼˜äºFull FTï¼Œä¸”å†…å­˜æ•ˆç‡ä¸ä¸»æµå‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œç›¸æ¯”äºFull FTå’ŒLoRAï¼ŒLIFTèƒ½å¤šä¿ç•™é«˜è¾¾20%çš„æºé¢†åŸŸçŸ¥è¯†ï¼Œæ˜¾è‘—ç¼“è§£äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)ç°è±¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00772v1",
      "published_date": "2025-06-01 01:31:50 UTC",
      "updated_date": "2025-06-01 01:31:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:41.351890+00:00"
    },
    {
      "arxiv_id": "2506.00771v2",
      "title": "Manipulating 3D Molecules in a Fixed-Dimensional E(3)-Equivariant Latent Space",
      "title_zh": "å›ºå®šç»´åº¦ E(3) ç­‰å˜æ½œç©ºé—´ä¸‹çš„ä¸‰ç»´åˆ†å­æ“çºµ",
      "authors": [
        "Zitao Chen",
        "Yinjun Jia",
        "Zitong Tian",
        "Wei-Ying Ma",
        "Yanyan Lan"
      ],
      "abstract": "Medicinal chemists often optimize drugs considering their 3D structures and designing structurally distinct molecules that retain key features, such as shapes, pharmacophores, or chemical properties. Previous deep learning approaches address this through supervised tasks like molecule inpainting or property-guided optimization. In this work, we propose a flexible zero-shot molecule manipulation method by navigating in a shared latent space of 3D molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named MolFLAE, which learns a fixed-dimensional, E(3)-equivariant latent space independent of atom counts. MolFLAE encodes 3D molecules using an E(3)-equivariant neural network into fixed number of latent nodes, distinguished by learned embeddings. The latent space is regularized, and molecular structures are reconstructed via a Bayesian Flow Network (BFN) conditioned on the encoder's latent output. MolFLAE achieves competitive performance on standard unconditional 3D molecule generation benchmarks. Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation, including atom number editing, structure reconstruction, and coordinated latent interpolation for both structure and properties. We further demonstrate our approach on a drug optimization task for the human glucocorticoid receptor, generating molecules with improved hydrophilicity while preserving key interactions, under computational evaluations. These results highlight the flexibility, robustness, and real-world utility of our method, opening new avenues for molecule editing and optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MolFLAEï¼Œä¸€ç§é’ˆå¯¹3Dåˆ†å­çš„å˜åˆ†è‡ªç¼–ç å™¨(VAE)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å…±äº«æ½œç©ºé—´ä¸­å¯¼èˆªå®ç°çµæ´»çš„é›¶æ ·æœ¬åˆ†å­æ“ä½œã€‚MolFLAEæ„å»ºäº†ä¸€ä¸ªå›ºå®šç»´åº¦çš„E(3)-equivariantæ½œç©ºé—´ï¼Œè¯¥ç©ºé—´ä¸åŸå­æ•°é‡æ— å…³ï¼Œå¹¶åˆ©ç”¨E(3)-equivariantç¥ç»ç½‘ç»œå°†3Dåˆ†å­ç¼–ç ä¸ºå›ºå®šæ•°é‡çš„æ½œåœ¨èŠ‚ç‚¹ã€‚åˆ†å­ç»“æ„çš„é‡æ„åˆ™é€šè¿‡ä»¥ç¼–ç å™¨è¾“å‡ºä¸ºæ¡ä»¶çš„è´å¶æ–¯æµç½‘ç»œ(Bayesian Flow Network, BFN)æ¥å®ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒMolFLAEåœ¨æ ‡å‡†çš„æ— æ¡ä»¶3Dåˆ†å­ç”ŸæˆåŸºå‡†ä¸Šå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒåŸå­æ•°ç¼–è¾‘ã€ç»“æ„é‡æ„ä»¥åŠç»“æ„ä¸å±æ€§çš„åè°ƒæ½œç©ºé—´æ’å€¼ã€‚åœ¨é’ˆå¯¹äººç±»ç³–çš®è´¨æ¿€ç´ å—ä½“(human glucocorticoid receptor)çš„è¯ç‰©ä¼˜åŒ–è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•æˆåŠŸç”Ÿæˆäº†åœ¨ä¿ç•™å…³é”®ç›¸äº’ä½œç”¨çš„åŒæ—¶å…·æœ‰æ›´é«˜äº²æ°´æ€§çš„åˆ†å­ã€‚è¿™äº›ç»“æœçªæ˜¾äº†MolFLAEåœ¨åˆ†å­ç¼–è¾‘å’ŒçœŸå®ä¸–ç•Œè¯ç‰©ä¼˜åŒ–ä»»åŠ¡ä¸­çš„çµæ´»æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This version (v2) includes minor edits. The paper has been accepted to NeurIPS 2025. Code is available at: https://github.com/MuZhao2333/MolFLAE",
      "pdf_url": "https://arxiv.org/pdf/2506.00771v2",
      "published_date": "2025-06-01 01:30:15 UTC",
      "updated_date": "2025-10-03 09:03:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:19.734000+00:00"
    },
    {
      "arxiv_id": "2506.00770v1",
      "title": "Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies",
      "title_zh": "è¶…è¶Šæ³¨æ„åŠ›ï¼šåˆ©ç”¨æ¶Œç°çš„å¯è§£é‡Šæ‹“æ‰‘å­¦ä¹ æ—¶ç©ºåŠ¨åŠ›å­¦",
      "authors": [
        "Sai Vamsi Alisetti",
        "Vikas Kalagi",
        "Sanjukta Krishnagopal"
      ],
      "abstract": "Spatio-temporal forecasting is critical in applications such as traffic prediction, energy demand modeling, and weather monitoring. While Graph Attention Networks (GATs) are popular for modeling spatial dependencies, they rely on predefined adjacency structures and dynamic attention scores, introducing inductive biases and computational overhead that can obscure interpretability.\n  We propose InterGAT, a simplified alternative to GAT that replaces masked attention with a fully learnable, symmetric node interaction matrix, capturing latent spatial relationships without relying on fixed graph topologies. Our framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder, outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a 21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop dataset across all forecasting horizons (15 to 60 minutes). Additionally, we observed reduction in training time by 60-70% compared to GAT-GRU baseline.\n  Crucially, the learned interaction matrix reveals interpretable structure: it recovers sparse, topology-aware attention patterns that align with community structure. Spectral and clustering analyses show that the model captures both localized and global dynamics, offering insights into the functional topology driving predictions. This highlights how structure learning can simultaneously support prediction, computational efficiency, and topological interpretabil-ity in dynamic graph-based domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InterGATï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ—¶ç©ºé¢„æµ‹(Spatio-temporal forecasting)ä»»åŠ¡çš„ç®€åŒ–æ¶æ„ï¼Œæ—¨åœ¨æ”¹è¿›ä¼ ç»Ÿçš„å›¾æ³¨æ„åŠ›ç½‘ç»œ(GATs)åœ¨è®¡ç®—å¼€é”€å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„ä¸è¶³ã€‚InterGATé€šè¿‡ä¸€ä¸ªå®Œå…¨å¯å­¦ä¹ çš„å¯¹ç§°èŠ‚ç‚¹äº¤äº’çŸ©é˜µ(symmetric node interaction matrix)å–ä»£äº†é®è”½æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å›ºå®šå›¾æ‹“æ‰‘çš„æƒ…å†µä¸‹æ•æ‰æ½œåœ¨çš„ç©ºé—´å…³ç³»ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºGRUçš„æ—¶é—´è§£ç å™¨æ„å»ºæˆInterGAT-GRUç³»ç»Ÿï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨SZ-Taxiå’ŒLos-Loopæ•°æ®é›†ä¸Šçš„é¢„æµ‹å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºGAT-GRUåŸºçº¿ï¼Œåˆ†åˆ«å®ç°äº†è‡³å°‘21%å’Œ6%çš„æ€§èƒ½æå‡ã€‚åŒæ—¶ï¼ŒInterGAT-GRUç›¸æ¯”åŸºçº¿æ¨¡å‹ç¼©çŸ­äº†60-70%çš„è®­ç»ƒæ—¶é—´ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚å…³é”®åœ¨äºï¼Œç ”ç©¶å‘ç°æ‰€å­¦ä¹ çš„äº¤äº’çŸ©é˜µå±•ç°å‡ºå¯è§£é‡Šçš„ç»“æ„ï¼Œèƒ½å¤Ÿæ¢å¤ä¸ç¤¾åŒºç»“æ„ä¸€è‡´çš„ç¨€ç–ã€æ‹“æ‰‘æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å¼ã€‚é€šè¿‡è°±åˆ†æå’Œèšç±»åˆ†æï¼Œè¯¥æ¨¡å‹è¯æ˜äº†å…¶æ•æ‰å±€éƒ¨å’Œå…¨å±€åŠ¨æ€çš„èƒ½åŠ›ï¼Œçªæ˜¾äº†ç»“æ„å­¦ä¹ åœ¨æ”¯æŒé¢„æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œå¯¹å¢å¼ºæ‹“æ‰‘å¯è§£é‡Šæ€§çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 10 figures, workshop",
      "pdf_url": "https://arxiv.org/pdf/2506.00770v1",
      "published_date": "2025-06-01 01:27:32 UTC",
      "updated_date": "2025-06-01 01:27:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:25.649549+00:00"
    },
    {
      "arxiv_id": "2506.00765v1",
      "title": "HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset",
      "title_zh": "HouseTSï¼šå¤§è§„æ¨¡å¤šæ¨¡æ€ U.S. ä½æˆ¿æ—¶ç©ºæ•°æ®é›†",
      "authors": [
        "Shengkun Wang",
        "Yanshen Sun",
        "Fanglan Chen",
        "Linhan Wang",
        "Naren Ramakrishnan",
        "Chang-Tien Lu",
        "Yinlin Chen"
      ],
      "abstract": "Accurate house-price forecasting is essential for investors, planners, and researchers. However, reproducible benchmarks with sufficient spatiotemporal depth and contextual richness for long horizon prediction remain scarce. To address this, we introduce HouseTS a large scale, multimodal dataset covering monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in 30 major U.S. metropolitan areas. The dataset includes over 890K records, enriched with points of Interest (POI), socioeconomic indicators, and detailed real estate metrics. To establish standardized performance baselines, we evaluate 14 models, spanning classical statistical approaches, deep neural networks (DNNs), and pretrained time-series foundation models. We further demonstrate the value of HouseTS in a multimodal case study, where a vision language model extracts structured textual descriptions of geographic change from time stamped satellite imagery. This enables interpretable, grounded insights into urban evolution. HouseTS is hosted on Kaggle, while all preprocessing pipelines, benchmark code, and documentation are openly maintained on GitHub to ensure full reproducibility and easy adoption.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† HouseTSï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ¨¡æ€çš„æ—¶ç©ºç¾å›½ä½æˆ¿æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³æˆ¿å±‹ä»·æ ¼é¢„æµ‹é¢†åŸŸç¼ºä¹å…·æœ‰è¶³å¤Ÿæ—¶ç©ºæ·±åº¦å’ŒèƒŒæ™¯ä¿¡æ¯çš„åŸºå‡†æµ‹è¯•é—®é¢˜ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†ä» 2012 å¹´ 3 æœˆåˆ° 2023 å¹´ 12 æœˆæœŸé—´ï¼Œç¾å›½ 30 ä¸ªä¸»è¦å¤§éƒ½å¸‚åŒº 6,000 ä¸ªé‚®æ”¿ç¼–ç åŒºçš„æœˆåº¦æˆ¿ä»·æ•°æ®ï¼ŒåŒ…å«è¶…è¿‡ 89 ä¸‡æ¡è®°å½•ï¼Œå¹¶æ•´åˆäº†å…´è¶£ç‚¹ (POI)ã€ç¤¾ä¼šç»æµæŒ‡æ ‡ä»¥åŠè¯¦ç»†çš„æˆ¿åœ°äº§æŒ‡æ ‡ã€‚ä¸ºäº†å»ºç«‹æ ‡å‡†åŒ–çš„æ€§èƒ½åŸºå‡†ï¼Œç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†åŒ…æ‹¬ä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•ã€æ·±åº¦ç¥ç»ç½‘ç»œ (DNNs) ä»¥åŠé¢„è®­ç»ƒæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (Time-Series Foundation Models) åœ¨å†…çš„ 14 ç§æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡å¤šæ¨¡æ€æ¡ˆä¾‹å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision Language Model) ä»å¸¦æ—¶é—´æˆ³çš„å«æ˜Ÿå›¾åƒä¸­æå–ç»“æ„åŒ–æ–‡æœ¬ï¼Œä¸ºåŸå¸‚æ¼”å˜æä¾›å¯è§£é‡Šçš„è§è§£ã€‚ç›®å‰ HouseTS åŠå…¶å®Œæ•´çš„é¢„å¤„ç†æµç¨‹å’ŒåŸºå‡†ä»£ç å·²åœ¨ Kaggle å’Œ GitHub ä¸Šå¼€æºï¼Œä¸ºåç»­ä½æˆ¿å¸‚åœºç ”ç©¶æä¾›äº†é«˜åº¦å¯é‡å¤çš„å®éªŒåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00765v1",
      "published_date": "2025-06-01 00:52:41 UTC",
      "updated_date": "2025-06-01 00:52:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:49.590258+00:00"
    },
    {
      "arxiv_id": "2506.06339v1",
      "title": "Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components",
      "title_zh": "é¢å‘é˜¿æ‹‰ä¼¯è¯­çš„ RAG æµæ°´çº¿ä¼˜åŒ–ï¼šæ ¸å¿ƒç»„ä»¶çš„ç³»ç»Ÿæ€§åˆ†æ",
      "authors": [
        "Jumana Alsubhi",
        "Mohammad D. Alahmadi",
        "Ahmed Alhusayni",
        "Ibrahim Aldailami",
        "Israa Hamdine",
        "Ahmad Shabana",
        "Yazeed Iskandar",
        "Suhayb Khayyat"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture for combining the precision of retrieval systems with the fluency of large language models. While several studies have investigated RAG pipelines for high-resource languages, the optimization of RAG components for Arabic remains underexplored. This study presents a comprehensive empirical evaluation of state-of-the-art RAG components-including chunking strategies, embedding models, rerankers, and language models-across a diverse set of Arabic datasets. Using the RAGAS framework, we systematically compare performance across four core metrics: context precision, context recall, answer faithfulness, and answer relevancy. Our experiments demonstrate that sentence-aware chunking outperforms all other segmentation methods, while BGE-M3 and Multilingual-E5-large emerge as the most effective embedding models. The inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness in complex datasets, and Aya-8B surpasses StableLM in generation quality. These findings provide critical insights for building high-quality Arabic RAG pipelines and offer practical guidelines for selecting optimal components across different document types.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç®¡é“ä¼˜åŒ–è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸåœ¨éé«˜èµ„æºè¯­è¨€ç ”ç©¶ä¸­çš„ç©ºç™½ã€‚ä½œè€…åˆ©ç”¨ RAGAS æ¡†æ¶ï¼Œåœ¨å¤šä¸ªé˜¿æ‹‰ä¼¯è¯­æ•°æ®é›†ä¸Šå¯¹å—åˆ’åˆ†(chunking)ç­–ç•¥ã€åµŒå…¥æ¨¡å‹(embedding models)ã€é‡æ’åºå™¨(rerankers)ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(LLMs)ç­‰æ ¸å¿ƒç»„ä»¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¥å­æ„ŸçŸ¥çš„å—åˆ’åˆ†(sentence-aware chunking)åœ¨åˆ†æ®µæ–¹æ³•ä¸­è¡¨ç°æœ€ä¼˜ï¼Œè€Œ BGE-M3 å’Œ Multilingual-E5-large è¢«è¯æ˜æ˜¯æœ€æœ‰æ•ˆçš„åµŒå…¥æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¼•å…¥é‡æ’åºå™¨(bge-reranker-v2-m3)èƒ½æ˜¾è‘—æå‡å¤æ‚æ•°æ®é›†ä¸‹çš„ç­”æ¡ˆå¿ å®åº¦(faithfulness)ï¼Œä¸” Aya-8B åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äº StableLMã€‚è¯¥ç ”ç©¶æˆæœä¸ºæ„å»ºé«˜è´¨é‡é˜¿æ‹‰ä¼¯è¯­ RAG ç®¡é“æä¾›äº†å…³é”®è§è§£ï¼Œå¹¶ä¸ºä¸åŒæ–‡æ¡£ç±»å‹çš„ç»„ä»¶é€‰æ‹©æä¾›äº†å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06339v1",
      "published_date": "2025-06-01 00:04:58 UTC",
      "updated_date": "2025-06-01 00:04:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:23:45.823569+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 129,
  "processed_papers_count": 129,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T17:24:43.066183+00:00"
}
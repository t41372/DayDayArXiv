{
  "date": "2025-02-27",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-02-27 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型的安全性、强化学习应用、多模态语言模型的优化，以及 AI 在医疗、教育和机器人领域的创新。其中，LLM 的 jailbreak 攻击和解释性研究（如 Foot-In-The-Door 和 Interpreting CLIP）最令人印象深刻，而知名学者如 Jonathan Cohen 和 Jitendra Malik 的作品（如 On Benchmarking Human-Like Intelligence 和 Sim-to-Real Reinforcement Learning）则凸显了 AI 评估和实际部署的潜力。以下我挑选并简要讨论了部分关键论文，先优先聊那些创新性强或话题度高的，其余快速掠过。\n\n### 关键论文讨论\n\n**1. Foot-In-The-Door: A Multi-turn Jailbreak for LLMs（Foot-In-The-Door: 多轮对话中的 LLM 越狱攻击）**  \n这篇论文探讨了基于心理学“足在门内”原理的多轮对话 jailbreak 攻击，逐步诱导 LLM 生成有害内容。主要贡献是通过桥接提示逐步升级恶意意图，实验显示攻击成功率高达 94%，显著高于现有方法，揭示了 LLM 在多轮交互中的脆弱性。\n\n**2. LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times and Network Traffic Analysis（LLMs Have Rhythm: 通过令牌间时间和网络流量分析的 LLM 指纹识别）**  \n作者 Saeif Alhazbi 等提出了一种被动、非侵入式方法，通过分析 LLM 的令牌间时间（ITTs）识别模型身份。主要发现是，即使在加密网络下，该方法也能准确识别 16 个小型语言模型和 10 个专有 LLM，实验证明其鲁棒性，为 LLM 部署的安全性提供新工具。\n\n**5. Interpreting CLIP with Hierarchical Sparse Autoencoders（Interpreting CLIP with Hierarchical Sparse Autoencoders: 使用分层稀疏自编码器的 CLIP 解释）**  \nVladimir Zaigrajew 等的工作提升了视觉语言模型的可解释性，通过分层稀疏自编码器（MSAE）提取 CLIP 中的语义概念。主要贡献是建立新状态，实现了 0.99 的余弦相似度，同时保持 80% 的稀疏性，允许模型进行概念搜索和偏差分析。\n\n**11. $Q\\sharp$: Provably Optimal Distributional RL for LLM Post-Training（Qsharp: LLM 后训练的分布强化学习）**  \nJin Peng Zhou 等提出 Qsharp 算法，使用分布强化学习优化 LLM 的 KL 正则化问题。主要发现是，该方法在数学推理基准上超越基线，同时保持低 KL 散度，理论上证明了其收敛性。\n\n**18. EgoNormia: Benchmarking Physical Social Norm Understanding（EgoNormia: 基准物理社会规范理解）**  \nMohammadHossein Rezaei 等构建了 EgoNormia 数据集，评估视觉语言模型对社会规范的理解。主要贡献是通过 1,853 个多阶段问题测试模型在安全和礼貌等方面的表现，揭示了模型在隐私和合作上的局限性。\n\n**21. Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries（Promote, Suppress, Iterate: 语言模型如何回答一对多事实查询）**  \nTianyi Lorena Yan 和 Robin Jia 分析了 LLM 在处理一对多查询时的内部机制。主要发现是，模型通过“促进-抑制”机制管理答案重复，实验证明了注意力层和 MLP 层的角色，为 LLM 的推理过程提供了新洞见。\n\n**23. Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids（Sim-to-Real 强化学习用于人形机器人的视觉灵巧操作）**  \nToru Lin 等提出了一种 sim-to-real 强化学习框架，用于人形机器人复杂操作。主要贡献是通过优化奖励设计和蒸馏过程，实现零样本硬件部署，实验在 CARLA 模拟器上展示了显著改进。\n\n**28. Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers（多代理验证: 使用多个验证器扩展测试时计算）**  \nShalev Lifshitz 等引入多代理验证框架，使用 Aspect Verifiers 提升 LLM 性能。主要发现是，该方法在测试时计算中表现出色，实验证明它能改善弱模型的泛化能力。\n\n**34. On Benchmarking Human-Like Intelligence in Machines（机器中基准人类智能）**  \nLance Ying 等质疑现有 AI 基准的局限性，提出新推荐以评估人类-like 认知。主要贡献是通过人类评估研究暴露基准缺陷，并建议改进方法。\n\n**42. Med-RLVR: Emerging Medical Reasoning from a 3B base model via Reinforcement Learning（Med-RLVR: 通过强化学习从 3B 基础模型中产生医疗推理）**  \nSheng Zhang 等扩展强化学习框架，用于医疗问答。主要发现是，该方法在零样本设置下提升了泛化性，实验在医疗数据集上实现了 8% 的准确率提升。\n\n**72. An exploration of features to improve the generalisability of fake news detection models（探索特征以提升假新闻检测模型的泛化性）**  \nNathaniel Hoy 和 Theodora Koulouri 研究了假新闻检测的特征工程，使用风格和社会货币化特征提升模型鲁棒性。主要贡献是，实验显示这些特征在跨数据集上显著改善性能。\n\n**105. Advanced Deep Learning Techniques for Analyzing Earnings Call Transcripts: Methodologies and Applications（高级深度学习技术用于分析财报电话会议记录: 方法和应用）**  \nUmair Zakir 等比较 BERT、FinBERT 和 ULMFiT 在情感分析中的表现，主要发现是，这些模型在金融文本上表现出色，帮助投资决策。\n\n其他论文如光通信应用（Scalable Coordinated Learning...）、时间序列预测（Evaluating System 1 vs. 2...）和教育 AI（AI Literacy in K-12...）等，虽然有实际意义，但相对常规，我这里快速掠过，仅提及其核心如节省训练时间或提升 AI 素养框架。总的来说，今天的论文强调了 AI 的安全性和实用性，LLM 改进是热点，期待未来应用！",
  "papers": [
    {
      "arxiv_id": "2502.20598v1",
      "title": "Scalable Coordinated Learning for H2M/R Applications over Optical Access Networks (Invited)",
      "title_zh": "翻译失败",
      "authors": [
        "Sourav Mondal",
        "Elaine Wong"
      ],
      "abstract": "One of the primary research interests adhering to next-generation\nfiber-wireless access networks is human-to-machine/robot (H2M/R) collaborative\ncommunications facilitating Industry 5.0. This paper discusses scalable H2M/R\ncommunications across large geographical distances that also allow rapid\nonboarding of new machines/robots as $\\sim72\\%$ training time is saved through\nglobal-local coordinated learning.",
      "tldr_zh": "该论文探讨了在光纤无线接入网络（Optical Access Networks）上实现可扩展的人机/机器人（H2M/R）协作通信，以支持工业5.0的发展。研究重点介绍了全球-本地协调学习方法，该方法允许在大型地理距离上快速加入新机器/机器人，并节省约72%的训练时间。总体而言，此方法为高效的H2M/R应用提供了可扩展框架，促进了工业5.0的实际部署。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "This article is accepted for publication in 29th Opto-Electronics and\n  Communications Conference 2024 (OECC2024). Copyright @ IEEE",
      "pdf_url": "http://arxiv.org/pdf/2502.20598v1",
      "published_date": "2025-02-27 23:56:51 UTC",
      "updated_date": "2025-02-27 23:56:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:35:15.762290"
    },
    {
      "arxiv_id": "2503.01896v1",
      "title": "Neuroplasticity and Corruption in Model Mechanisms: A Case Study Of Indirect Object Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Vishnu Kabir Chhabra",
        "Ding Zhu",
        "Mohammad Mahdi Khalili"
      ],
      "abstract": "Previous research has shown that fine-tuning language models on general tasks\nenhance their underlying mechanisms. However, the impact of fine-tuning on\npoisoned data and the resulting changes in these mechanisms are poorly\nunderstood. This study investigates the changes in a model's mechanisms during\ntoxic fine-tuning and identifies the primary corruption mechanisms. We also\nanalyze the changes after retraining a corrupted model on the original dataset\nand observe neuroplasticity behaviors, where the model relearns original\nmechanisms after fine-tuning the corrupted model. Our findings indicate that:\n(i) Underlying mechanisms are amplified across task-specific fine-tuning which\ncan be generalized to longer epochs, (ii) Model corruption via toxic\nfine-tuning is localized to specific circuit components, (iii) Models exhibit\nneuroplasticity when retraining corrupted models on clean dataset, reforming\nthe original model mechanisms.",
      "tldr_zh": "本研究探讨了在一般任务上微调语言模型时，使用污染数据（poisoned data）对模型底层机制的影响，特别针对间接对象识别（Indirect Object Identification）进行案例研究。研究者分析了模型在有毒微调（toxic fine-tuning）过程中的机制变化，并识别了主要的腐败机制，这些变化局限于特定电路组件。结果显示，底层机制在任务特定微调中被放大，且模型在重新训练腐败模型时表现出神经可塑性（neuroplasticity），成功重塑原机制。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.01896v1",
      "published_date": "2025-02-27 23:44:50 UTC",
      "updated_date": "2025-02-27 23:44:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:35:29.354038"
    },
    {
      "arxiv_id": "2503.00079v3",
      "title": "AI Literacy in K-12 and Higher Education in the Wake of Generative AI: An Integrative Review",
      "title_zh": "翻译失败",
      "authors": [
        "Xingjian Gu",
        "Barbara J. Ericson"
      ],
      "abstract": "Even though AI literacy has emerged as a prominent education topic in the\nwake of generative AI, its definition remains vague. There is little consensus\namong researchers and practitioners on how to discuss and design AI literacy\ninterventions. The term has been used to describe both learning activities that\ntrain undergraduate students to use ChatGPT effectively and having kindergarten\nchildren interact with social robots. This paper applies an integrative review\nmethod to examine empirical and theoretical AI literacy studies published since\n2020. In synthesizing the 124 reviewed studies, three ways to conceptualize\nliteracy-functional, critical, and indirectly beneficial-and three perspectives\non AI-technical detail, tool, and sociocultural-were identified, forming a\nframework that reflects the spectrum of how AI literacy is approached in\npractice. The framework highlights the need for more specialized terms within\nAI literacy discourse and indicates research gaps in certain AI literacy\nobjectives.",
      "tldr_zh": "这篇整合性回顾论文探讨了生成式 AI 兴起背景下，K-12 和高等教育中 AI literacy 的模糊定义和实践共识问题，通过分析自 2020 年以来的 124 篇实证和理论研究。作者识别出三种 AI literacy 概念化方式——功能性（functional）、批判性（critical）和间接有益（indirectly beneficial）——以及三种视角：技术细节（technical detail）、工具（tool）和社会文化（sociocultural），并构建了一个反映实践光谱的框架。该框架突出了 AI literacy 话语中需要更专业的术语，并指出了某些 AI literacy 目标，如特定教育层面的研究空白。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "I.2.0; K.3.2"
      ],
      "primary_category": "cs.CY",
      "comment": "25 pages, 7 figures; submitted to ICER 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00079v3",
      "published_date": "2025-02-27 23:32:03 UTC",
      "updated_date": "2025-03-28 16:54:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:35:41.533075"
    },
    {
      "arxiv_id": "2503.01895v2",
      "title": "Evaluating System 1 vs. 2 Reasoning Approaches for Zero-Shot Time Series Forecasting: A Benchmark and Insights",
      "title_zh": "评估 System 1 与 System 2 推理方法在",
      "authors": [
        "Haoxin Liu",
        "Zhiyuan Zhao",
        "Shiduo Li",
        "B. Aditya Prakash"
      ],
      "abstract": "Reasoning ability is crucial for solving challenging tasks. With the\nadvancement of foundation models, such as the emergence of large language\nmodels (LLMs), a wide range of reasoning strategies has been proposed,\nincluding test-time enhancements, such as Chain-ofThought, and post-training\noptimizations, as used in DeepSeek-R1. While these reasoning strategies have\ndemonstrated effectiveness across various challenging language or vision tasks,\ntheir applicability and impact on time-series forecasting (TSF), particularly\nthe challenging zero-shot TSF, remain largely unexplored. In particular, it is\nunclear whether zero-shot TSF benefits from reasoning and, if so, what types of\nreasoning strategies are most effective. To bridge this gap, we propose ReC4TS,\nthe first benchmark that systematically evaluates the effectiveness of popular\nreasoning strategies when applied to zero-shot TSF tasks. ReC4TS conducts\ncomprehensive evaluations across datasets spanning eight domains, covering both\nunimodal and multimodal with short-term and longterm forecasting tasks. More\nimportantly, ReC4TS provides key insights: (1) Self-consistency emerges as the\nmost effective test-time reasoning strategy; (2) Group-relative policy\noptimization emerges as a more suitable approach for incentivizing reasoning\nability during post-training; (3) Multimodal TSF benefits more from reasoning\nstrategies compared to unimodal TSF. Beyond these insights, ReC4TS establishes\ntwo pioneering starting blocks to support future zero-shot TSF reasoning\nresearch: (1) A novel dataset, TimeThinking, containing forecasting samples\nannotated with reasoning trajectories from multiple advanced LLMs, and (2) A\nnew and simple test-time scaling-law validated on foundational TSF models\nenabled by self-consistency reasoning strategy. All data and code are publicly\naccessible at: https://github.com/AdityaLab/OpenTimeR",
      "tldr_zh": "这篇论文评估了 System 1 和 System 2 推理方法在 Zero-Shot Time Series Forecasting 中的效果，并提出了首个系统基准 ReC4TS。ReC4TS 在八个领域的多个数据集上进行了全面评估，包括单模态和多模态、短期和长期预测任务，测试了如 Chain-of-Thought 和 Self-consistency 等流行推理策略。关键发现包括：Self-consistency 是最有效的测试时推理策略，Group-relative policy optimization 更适合后训练优化，且多模态 TSF 比单模态 TSF 更受益于推理。此外，论文引入了新数据集 TimeThinking 和一个基于 Self-consistency 的测试时缩放定律，以推进零样本 TSF 研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.01895v2",
      "published_date": "2025-02-27 23:27:37 UTC",
      "updated_date": "2025-03-14 00:16:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:35:54.152991"
    },
    {
      "arxiv_id": "2502.20589v1",
      "title": "LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times and Network Traffic Analysis",
      "title_zh": "LLMs Have Rhythm：利用",
      "authors": [
        "Saeif Alhazbi",
        "Ahmed Mohamed Hussain",
        "Gabriele Oligeri",
        "Panos Papadimitratos"
      ],
      "abstract": "As Large Language Models (LLMs) become increasingly integrated into many\ntechnological ecosystems across various domains and industries, identifying\nwhich model is deployed or being interacted with is critical for the security\nand trustworthiness of the systems. Current verification methods typically rely\non analyzing the generated output to determine the source model. However, these\ntechniques are susceptible to adversarial attacks, operate in a post-hoc\nmanner, and may require access to the model weights to inject a verifiable\nfingerprint. In this paper, we propose a novel passive and non-invasive\nfingerprinting technique that operates in real-time and remains effective even\nunder encrypted network traffic conditions. Our method leverages the intrinsic\nautoregressive generation nature of language models, which generate text one\ntoken at a time based on all previously generated tokens, creating a unique\ntemporal pattern like a rhythm or heartbeat that persists even when the output\nis streamed over a network. We find that measuring the Inter-Token Times\n(ITTs)-time intervals between consecutive tokens-can identify different\nlanguage models with high accuracy. We develop a Deep Learning (DL) pipeline to\ncapture these timing patterns using network traffic analysis and evaluate it on\n16 Small Language Models (SLMs) and 10 proprietary LLMs across different\ndeployment scenarios, including local host machine (GPU/CPU), Local Area\nNetwork (LAN), Remote Network, and Virtual Private Network (VPN). The\nexperimental results confirm that our proposed technique is effective and\nmaintains high accuracy even when tested in different network conditions. This\nwork opens a new avenue for model identification in real-world scenarios and\ncontributes to more secure and trustworthy language model deployment.",
      "tldr_zh": "该研究提出了一种被动、非侵入式的指纹识别技术，用于识别Large Language Models (LLMs)，通过分析Inter-Token Times (ITTs)——即连续token之间的时间间隔——以及Network Traffic Analysis。方法利用LLMs的autoregressive生成特性，捕捉其独特的时间模式（如节奏或心跳），并开发了一个Deep Learning (DL)管道来处理不同部署场景下的数据。实验在16个Small Language Models (SLMs)和10个专有LLMs上进行，涵盖本地主机（GPU/CPU）、LAN、远程网络和VPN环境，结果显示该技术即使在加密网络条件下也能保持高准确率，从而为更安全、可信的LLMs部署提供新途径。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20589v1",
      "published_date": "2025-02-27 23:22:01 UTC",
      "updated_date": "2025-02-27 23:22:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:36:06.028343"
    },
    {
      "arxiv_id": "2502.20583v1",
      "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
      "title_zh": "LiteASR：基于低秩逼近的高效自动语音识别",
      "authors": [
        "Keisuke Kamahori",
        "Jungo Kasai",
        "Noriyuki Kojima",
        "Baris Kasikci"
      ],
      "abstract": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper,\nrely on deep encoder-decoder architectures, and their encoders are a critical\nbottleneck for efficient deployment due to high computational intensity. We\nintroduce LiteASR, a low-rank compression scheme for ASR encoders that\nsignificantly reduces inference costs while maintaining transcription accuracy.\nOur approach leverages the strong low-rank properties observed in intermediate\nactivations: by applying principal component analysis (PCA) with a small\ncalibration dataset, we approximate linear transformations with a chain of\nlow-rank matrix multiplications, and further optimize self-attention to work in\nthe reduced dimension. Evaluation results show that our method can compress\nWhisper large-v3's encoder size by over 50%, matching Whisper medium's size\nwith better transcription accuracy, thereby establishing a new Pareto-optimal\nfrontier of efficiency and performance. The code of LiteASR is available at\nhttps://github.com/efeslab/LiteASR.",
      "tldr_zh": "本研究提出 LiteASR，一种基于低秩逼近的压缩方案，旨在解决现代 ASR 模型（如 OpenAI's Whisper）中编码器计算密集型问题，从而降低推理成本同时保持转录准确性。该方法利用 PCA 在小校准数据集上分析中间激活，将线性变换近似为低秩矩阵乘法链，并优化 self-attention 机制以在减少维度下运作。实验结果显示，LiteASR 可将 Whisper large-v3 的编码器大小压缩超过 50%，达到 Whisper medium 的尺寸但具有更好的转录准确性，从而建立了效率和性能的新 Pareto-optimal 前沿。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20583v1",
      "published_date": "2025-02-27 22:52:21 UTC",
      "updated_date": "2025-02-27 22:52:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:36:16.747727"
    },
    {
      "arxiv_id": "2503.00077v1",
      "title": "Navigating the Edge with the State-of-the-Art Insights into Corner Case Identification and Generation for Enhanced Autonomous Vehicle Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Kenji Godoy Shimanuki",
        "Alexandre Moreira Nascimento",
        "Lucio Flavio Vismari",
        "Joao Batista Camargo Junior",
        "Jorge Rady de Almeida Junior",
        "Paulo Sergio Cugnasca"
      ],
      "abstract": "In recent years, there has been significant development of autonomous vehicle\n(AV) technologies. However, despite the notable achievements of some industry\nplayers, a strong and appealing body of evidence that demonstrate AVs are\nactually safe is lacky, which could foster public distrust in this technology\nand further compromise the entire development of this industry, as well as\nrelated social impacts. To improve the safety of AVs, several techniques are\nproposed that use synthetic data in virtual simulation. In particular, the\nhighest risk data, known as corner cases (CCs), are the most valuable for\ndeveloping and testing AV controls, as they can expose and improve the\nweaknesses of these autonomous systems. In this context, the present paper\npresents a systematic literature review aiming to comprehensively analyze\nmethodologies for CC identifi cation and generation, also pointing out current\ngaps and further implications of synthetic data for AV safety and reliability.\nBased on a selection criteria, 110 studies were picked from an initial sample\nof 1673 papers. These selected paper were mapped into multiple categories to\nanswer eight inter-linked research questions. It concludes with the\nrecommendation of a more integrated approach focused on safe development among\nall stakeholders, with active collaboration between industry, academia and\nregulatory bodies.",
      "tldr_zh": "这篇论文通过系统文献综述，分析了corner cases (CCs)的识别和生成方法，以提升autonomous vehicle (AVs)的安全性和可靠性。研究从1673篇论文中筛选出110篇，并根据多个类别回答八个相互关联的研究问题，揭示了CCs在暴露AVs弱点方面的关键价值，同时指出了现有方法的不足和合成数据应用的潜在影响。论文强调，CCs的处理有助于解决公众对AVs安全的疑虑，并推荐行业、学术和监管机构进行更紧密合作，采用集成式方法推动安全开发。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00077v1",
      "published_date": "2025-02-27 22:47:46 UTC",
      "updated_date": "2025-02-27 22:47:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:36:29.629826"
    },
    {
      "arxiv_id": "2502.20578v1",
      "title": "Interpreting CLIP with Hierarchical Sparse Autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Vladimir Zaigrajew",
        "Hubert Baniecki",
        "Przemyslaw Biecek"
      ],
      "abstract": "Sparse autoencoders (SAEs) are useful for detecting and steering\ninterpretable features in neural networks, with particular potential for\nunderstanding complex multimodal representations. Given their ability to\nuncover interpretable features, SAEs are particularly valuable for analyzing\nlarge-scale vision-language models (e.g., CLIP and SigLIP), which are\nfundamental building blocks in modern systems yet remain challenging to\ninterpret and control. However, current SAE methods are limited by optimizing\nboth reconstruction quality and sparsity simultaneously, as they rely on either\nactivation suppression or rigid sparsity constraints. To this end, we introduce\nMatryoshka SAE (MSAE), a new architecture that learns hierarchical\nrepresentations at multiple granularities simultaneously, enabling a direct\noptimization of both metrics without compromise. MSAE establishes a new\nstate-of-the-art Pareto frontier between reconstruction quality and sparsity\nfor CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of\nvariance unexplained while maintaining ~80% sparsity. Finally, we demonstrate\nthe utility of MSAE as a tool for interpreting and controlling CLIP by\nextracting over 120 semantic concepts from its representation to perform\nconcept-based similarity search and bias analysis in downstream tasks like\nCelebA.",
      "tldr_zh": "本研究提出Matryoshka SAE (MSAE)，一种层次化稀疏自编码器架构，用于解释和控制视觉语言模型如CLIP，通过同时在多个粒度学习表示来优化重构质量和稀疏性，而避免了现有SAE方法的局限，如激活抑制或刚性约束。MSAE在CLIP上实现了新的Pareto前沿，达到0.99的余弦相似度和小于0.1的分方差未解释，同时保持约80%的稀疏性。最终，该方法从CLIP表示中提取超过120个语义概念，支持概念-based相似性搜索和偏差分析，例如在CelebA任务中进行模型控制和解释。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20578v1",
      "published_date": "2025-02-27 22:39:13 UTC",
      "updated_date": "2025-02-27 22:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:36:41.361347"
    },
    {
      "arxiv_id": "2502.20571v1",
      "title": "PFformer: A Position-Free Transformer Variant for Extreme-Adaptive Multivariate Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Yanhong Li",
        "David C. Anastasiu"
      ],
      "abstract": "Multivariate time series (MTS) forecasting is vital in fields like weather,\nenergy, and finance. However, despite deep learning advancements, traditional\nTransformer-based models often diminish the effect of crucial inter-variable\nrelationships by singular token embedding and struggle to effectively capture\ncomplex dependencies among variables, especially in datasets with rare or\nextreme events. These events create significant imbalances and lead to high\nskewness, complicating accurate prediction efforts. This study introduces\nPFformer, a position-free Transformer-based model designed for single-target\nMTS forecasting, specifically for challenging datasets characterized by extreme\nvariability. PFformer integrates two novel embedding strategies: Enhanced\nFeature-based Embedding (EFE) and Auto-Encoder-based Embedding (AEE). EFE\neffectively encodes inter-variable dependencies by mapping related sequence\nsubsets to high-dimensional spaces without positional constraints, enhancing\nthe encoder's functionality. PFformer shows superior forecasting accuracy\nwithout the traditional limitations of positional encoding in MTS modeling. We\nevaluated PFformer across four challenging datasets, focusing on two key\nforecasting scenarios: long sequence prediction for 3 days ahead and rolling\npredictions every four hours to reflect real-time decision-making processes in\nwater management. PFformer demonstrated remarkable improvements, from 20% to\n60%, compared with state-of-the-art models.",
      "tldr_zh": "本研究针对多变量时间序列（MTS）预测中的挑战，提出PFformer，一种无位置Transformers变体，旨在处理极端事件数据集的复杂依赖和不平衡问题。该模型引入Enhanced Feature-based Embedding (EFE)和Auto-Encoder-based Embedding (AEE)两种新策略，其中EFE通过映射相关序列子集到高维空间来增强变量间依赖编码，而无需传统位置编码，从而提升预测准确性。在四个挑战性数据集上的实验中，PFformer在3天提前长序列预测和每四小时滚动预测场景下，与最先进模型相比，准确率提高了20%至60%，为天气、能源和金融等领域提供更可靠的预测工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "PAKDD 2025 special session on Data Science: Foundations and\n  Applications (DSFA)",
      "pdf_url": "http://arxiv.org/pdf/2502.20571v1",
      "published_date": "2025-02-27 22:21:27 UTC",
      "updated_date": "2025-02-27 22:21:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:36:52.477136"
    },
    {
      "arxiv_id": "2502.20565v2",
      "title": "DPZV: Elevating the Tradeoff between Privacy and Utility in Zeroth-Order Vertical Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jianing Zhang",
        "Evan Chen",
        "Chaoyue Liu",
        "Christopher G. Brinton"
      ],
      "abstract": "Vertical Federated Learning (VFL) enables collaborative training with\nfeature-partitioned data, yet remains vulnerable to privacy leakage through\ngradient transmissions. Standard differential privacy (DP) techniques such as\nDP-SGD are difficult to apply in this setting due to VFL's distributed nature\nand the high variance incurred by vector-valued noise. On the other hand,\nzeroth-order (ZO) optimization techniques can avoid explicit gradient exposure\nbut lack formal privacy guarantees. In this work, we propose DPZV, the first ZO\noptimization framework for VFL that achieves tunable DP with performance\nguarantees. DPZV overcomes these limitations by injecting low-variance scalar\nnoise at the server, enabling controllable privacy with reduced memory\noverhead. We conduct a comprehensive theoretical analysis showing that DPZV\nmatches the convergence rate of first-order optimization methods while\nsatisfying formal ($\\epsilon, \\delta$)-DP guarantees. Experiments on image and\nlanguage benchmarks demonstrate that DPZV outperforms several baselines in\nterms of accuracy under a wide range of privacy constraints ($\\epsilon \\le\n10$), thereby elevating the privacy-utility tradeoff in VFL.",
      "tldr_zh": "本研究针对 Vertical Federated Learning (VFL) 中梯度传输导致的隐私泄露问题，提出 DPZV 框架，这是一种首创的 Zeroth-Order (ZO) 优化方法，能够提供可调的 Differential Privacy (DP) 保证，同时减少内存开销。DPZV 通过在服务器注入低方差标量噪音，克服了传统 DP-SGD 在分布式环境中的高方差挑战，并通过理论分析证明其收敛率与一阶优化方法相当，同时满足 (ε, δ)-DP 要求。在图像和语言基准测试中，DPZV 在 ε ≤ 10 的隐私约束下，比基线模型表现出更高的准确率，从而显著提升了 VFL 的隐私-效用权衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20565v2",
      "published_date": "2025-02-27 22:07:16 UTC",
      "updated_date": "2025-05-19 14:40:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:37:06.697786"
    },
    {
      "arxiv_id": "2502.20548v1",
      "title": "$Q\\sharp$: Provably Optimal Distributional RL for LLM Post-Training",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Peng Zhou",
        "Kaiwen Wang",
        "Jonathan Chang",
        "Zhaolin Gao",
        "Nathan Kallus",
        "Kilian Q. Weinberger",
        "Kianté Brantley",
        "Wen Sun"
      ],
      "abstract": "Reinforcement learning (RL) post-training is crucial for LLM alignment and\nreasoning, but existing policy-based methods, such as PPO and DPO, can fall\nshort of fixing shortcuts inherited from pre-training. In this work, we\nintroduce $Q\\sharp$, a value-based algorithm for KL-regularized RL that guides\nthe reference policy using the optimal regularized $Q$ function. We propose to\nlearn the optimal $Q$ function using distributional RL on an aggregated online\ndataset. Unlike prior value-based baselines that guide the model using\nunregularized $Q$-values, our method is theoretically principled and provably\nlearns the optimal policy for the KL-regularized RL problem. Empirically,\n$Q\\sharp$ outperforms prior baselines in math reasoning benchmarks while\nmaintaining a smaller KL divergence to the reference policy. Theoretically, we\nestablish a reduction from KL-regularized RL to no-regret online learning,\nproviding the first bounds for deterministic MDPs under only realizability.\nThanks to distributional RL, our bounds are also variance-dependent and\nconverge faster when the reference policy has small variance. In sum, our\nresults highlight $Q\\sharp$ as an effective approach for post-training LLMs,\noffering both improved performance and theoretical guarantees. The code can be\nfound at https://github.com/jinpz/q_sharp.",
      "tldr_zh": "本论文提出 $Q\\sharp$ 算法，一种基于价值的强化学习 (RL) 方法，用于大型语言模型 (LLM) 的后训练，以解决现有策略-based 方法如 PPO 和 DPO 在修复预训练捷径方面的不足。$Q\\sharp$ 通过在聚合在线数据集上使用分布 RL 学习最优正则化 Q 函数，并理论上证明其能获得 KL-正则化 RL 的最优策略，提供在确定性 MDP 下仅需实现性的收敛边界。实验结果显示，$Q\\sharp$ 在数学推理基准上优于基线方法，同时保持较小的 KL 散度，并展示了更快的方差相关收敛性能，从而为 LLM 后训练提供更有效的性能和理论保证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20548v1",
      "published_date": "2025-02-27 21:43:00 UTC",
      "updated_date": "2025-02-27 21:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:37:21.205479"
    },
    {
      "arxiv_id": "2502.20525v1",
      "title": "Revisiting Kernel Attention with Correlated Gaussian Process Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Long Minh Bui",
        "Tho Tran Huu",
        "Duy Dinh",
        "Tan Minh Nguyen",
        "Trong Nghia Hoang"
      ],
      "abstract": "Transformers have increasingly become the de facto method to model sequential\ndata with state-of-the-art performance. Due to its widespread use, being able\nto estimate and calibrate its modeling uncertainty is important to understand\nand design robust transformer models. To achieve this, previous works have used\nGaussian processes (GPs) to perform uncertainty calibration for the attention\nunits of transformers and attained notable successes. However, such approaches\nhave to confine the transformers to the space of symmetric attention to ensure\nthe necessary symmetric requirement of their GP's kernel specification, which\nreduces the representation capacity of the model. To mitigate this restriction,\nwe propose the Correlated Gaussian Process Transformer (CGPT), a new class of\ntransformers whose self-attention units are modeled as cross-covariance between\ntwo correlated GPs (CGPs). This allows asymmetries in attention and can enhance\nthe representation capacity of GP-based transformers. We also derive a sparse\napproximation for CGP to make it scale better. Our empirical studies show that\nboth CGP-based and sparse CGP-based transformers achieve better performance\nthan state-of-the-art GP-based transformers on a variety of benchmark tasks.\nThe code for our experiments is available at\nhttps://github.com/MinhLong210/CGP-Transformers.",
      "tldr_zh": "本研究重新审视了基于高斯过程(Gaussian Processes, GPs)的注意力机制，针对现有GP-based Transformers 因要求对称注意力而限制模型表示能力的问题，提出了一种新模型Correlated Gaussian Process Transformer (CGPT)。在CGPT中，自注意力(self-attention)单元被建模为两个相关的高斯过程(Correlated Gaussian Processes, CGPs)之间的交叉协方差，从而支持注意力的不对称性并提升表示能力。同时，作者导出了CGP的稀疏近似，以提高模型的可扩展性。实验结果显示，CGPT及其稀疏版本在多种基准任务上 outperform 了现有的GP-based Transformers，提供了一个更鲁棒的序列数据建模框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20525v1",
      "published_date": "2025-02-27 21:21:48 UTC",
      "updated_date": "2025-02-27 21:21:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:37:32.449046"
    },
    {
      "arxiv_id": "2502.20513v1",
      "title": "Personas Evolved: Designing Ethical LLM-Based Conversational Agent Personalities",
      "title_zh": "翻译失败",
      "authors": [
        "Smit Desai",
        "Mateusz Dubiel",
        "Nima Zargham",
        "Thomas Mildner",
        "Laura Spillner"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) has revolutionized\nConversational User Interfaces (CUIs), enabling more dynamic, context-aware,\nand human-like interactions across diverse domains, from social sciences to\nhealthcare. However, the rapid adoption of LLM-based personas raises critical\nethical and practical concerns, including bias, manipulation, and unforeseen\nsocial consequences. Unlike traditional CUIs, where personas are carefully\ndesigned with clear intent, LLM-based personas generate responses dynamically\nfrom vast datasets, making their behavior less predictable and harder to\ngovern. This workshop aims to bridge the gap between CUI and broader AI\ncommunities by fostering a cross-disciplinary dialogue on the responsible\ndesign and evaluation of LLM-based personas. Bringing together researchers,\ndesigners, and practitioners, we will explore best practices, develop ethical\nguidelines, and promote frameworks that ensure transparency, inclusivity, and\nuser-centered interactions. By addressing these challenges collaboratively, we\nseek to shape the future of LLM-driven CUIs in ways that align with societal\nvalues and expectations.",
      "tldr_zh": "本研究探讨了大型语言模型(LLM)驱动的对话用户界面(CUIs)中基于LLM的对话代理人格设计问题，这些人格动态生成响应，可能导致偏见、操纵和社交后果等伦理挑战。与传统CUIs不同，LLM-based personas的行为更难预测，因此需要负责任的设计方法。该研讨会旨在促进CUI和AI社区的跨学科对话，探索最佳实践、制定伦理指南，并推广框架以确保透明性、包容性和用户中心互动，最终帮助LLM-driven CUIs符合社会价值观。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20513v1",
      "published_date": "2025-02-27 20:46:54 UTC",
      "updated_date": "2025-02-27 20:46:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:37:42.665252"
    },
    {
      "arxiv_id": "2502.20508v1",
      "title": "TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Soumyabrata Chaudhuri",
        "Pranav Purkar",
        "Ritwik Raghav",
        "Shubhojit Mallick",
        "Manish Gupta",
        "Abhik Jana",
        "Shreya Ghosh"
      ],
      "abstract": "Recent advancements in probing Large Language Models (LLMs) have explored\ntheir latent potential as personalized travel planning agents, yet existing\nbenchmarks remain limited in real world applicability. Existing datasets, such\nas TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance,\nspatial inconsistencies, and a lack of key travel constraints, making them\ninadequate for practical itinerary generation. To address these gaps, we\nintroduce TripCraft, a spatiotemporally coherent travel planning dataset that\nintegrates real world constraints, including public transit schedules, event\navailability, diverse attraction categories, and user personas for enhanced\npersonalization. To evaluate LLM generated plans beyond existing binary\nvalidation methods, we propose five continuous evaluation metrics, namely\nTemporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score,\nand Persona Score which assess itinerary quality across multiple dimensions.\nOur parameter informed setting significantly enhances meal scheduling,\nimproving the Temporal Meal Score from 61% to 80% in a 7 day scenario.\nTripCraft establishes a new benchmark for LLM driven personalized travel\nplanning, offering a more realistic, constraint aware framework for itinerary\ngeneration. Dataset and Codebase will be made publicly available upon\nacceptance.",
      "tldr_zh": "该论文指出，现有的旅行规划基准如 TravelPlanner 存在半合成数据、空间不一致性和关键约束缺失等问题，因此引入了 TripCraft，这是一个时空连贯的旅行规划数据集，整合了真实约束如公共交通时间表、事件可用性、多样景点类别和用户角色以实现个性化。\nTripCraft 提出了五个连续评估指标，包括 Temporal Meal Score、Temporal Attraction Score、Spatial Score、Ordering Score 和 Persona Score，用于多维度评估 LLM 生成的行程质量。\n实验结果显示，在参数告知设置下，7 天行程的 Temporal Meal Score 从 61% 提升到 80%，从而为 LLM 驱动的个性化旅行规划建立了一个更现实、约束aware 的新基准。\n数据集和代码将在接受后公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "27 pages, 18 Tables and 6 Figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20508v1",
      "published_date": "2025-02-27 20:33:28 UTC",
      "updated_date": "2025-02-27 20:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:37:56.958020"
    },
    {
      "arxiv_id": "2502.20504v1",
      "title": "A Thousand Words or An Image: Studying the Influence of Persona Modality in Multimodal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Julius Broomfield",
        "Kartik Sharma",
        "Srijan Kumar"
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated remarkable\nadvancements in embodying diverse personas, enhancing their effectiveness as\nconversational agents and virtual assistants. Consequently, LLMs have made\nsignificant strides in processing and integrating multimodal information.\nHowever, even though human personas can be expressed in both text and image,\nthe extent to which the modality of a persona impacts the embodiment by the LLM\nremains largely unexplored. In this paper, we investigate how do different\nmodalities influence the expressiveness of personas in multimodal LLMs. To this\nend, we create a novel modality-parallel dataset of 40 diverse personas varying\nin age, gender, occupation, and location. This consists of four modalities to\nequivalently represent a persona: image-only, text-only, a combination of image\nand small text, and typographical images, where text is visually stylized to\nconvey persona-related attributes. We then create a systematic evaluation\nframework with 60 questions and corresponding metrics to assess how well LLMs\nembody each persona across its attributes and scenarios. Comprehensive\nexperiments on $5$ multimodal LLMs show that personas represented by detailed\ntext show more linguistic habits, while typographical images often show more\nconsistency with the persona. Our results reveal that LLMs often overlook\npersona-specific details conveyed through images, highlighting underlying\nlimitations and paving the way for future research to bridge this gap. We\nrelease the data and code at https://github.com/claws-lab/persona-modality .",
      "tldr_zh": "本文研究了在多模态LLMs中，人设（persona）的模态（如文本、图像）如何影响模型的表现，旨在探索不同模态对人设表达性的影响。研究者创建了一个包含40个多样化人设的模态平行数据集，并开发了系统评估框架，包括60个问题和指标，用于评估模型在年龄、性别等属性上的体现效果。在5个多模态LLMs的实验中，发现详细文本模态显示更多语言习惯，而typographical images模态显示更高一致性，但LLMs往往忽略图像中人设细节，这突出了模型的局限性并为未来研究提供了方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20504v1",
      "published_date": "2025-02-27 20:25:00 UTC",
      "updated_date": "2025-02-27 20:25:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:38:08.445235"
    },
    {
      "arxiv_id": "2502.20502v1",
      "title": "On Benchmarking Human-Like Intelligence in Machines",
      "title_zh": "翻译失败",
      "authors": [
        "Lance Ying",
        "Katherine M. Collins",
        "Lionel Wong",
        "Ilia Sucholutsky",
        "Ryan Liu",
        "Adrian Weller",
        "Tianmin Shu",
        "Thomas L. Griffiths",
        "Joshua B. Tenenbaum"
      ],
      "abstract": "Recent benchmark studies have claimed that AI has approached or even\nsurpassed human-level performances on various cognitive tasks. However, this\nposition paper argues that current AI evaluation paradigms are insufficient for\nassessing human-like cognitive capabilities. We identify a set of key\nshortcomings: a lack of human-validated labels, inadequate representation of\nhuman response variability and uncertainty, and reliance on simplified and\necologically-invalid tasks. We support our claims by conducting a human\nevaluation study on ten existing AI benchmarks, suggesting significant biases\nand flaws in task and label designs. To address these limitations, we propose\nfive concrete recommendations for developing future benchmarks that will enable\nmore rigorous and meaningful evaluations of human-like cognitive capacities in\nAI with various implications for such AI applications.",
      "tldr_zh": "这篇论文质疑了当前AI基准测试是否能有效评估机器的类人智能水平，认为现有方法存在不足，如缺乏人类验证标签、未充分考虑人类响应变异性和不确定性，以及依赖简化且不真实的任务。通过对十个AI基准进行人类评估研究，作者揭示了这些基准在任务和标签设计上存在显著偏见和缺陷。为改进这一领域，论文提出了五点具体推荐，包括开发更严格的基准，以实现对AI类人认知能力的更准确评估，并为AI应用带来深远影响。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20502v1",
      "published_date": "2025-02-27 20:21:36 UTC",
      "updated_date": "2025-02-27 20:21:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:38:18.090059"
    },
    {
      "arxiv_id": "2502.20493v1",
      "title": "Unified Kernel-Segregated Transpose Convolution Operation",
      "title_zh": "统一内核分离转置卷积操作",
      "authors": [
        "Vijay Srinivas Tida",
        "Md Imran Hossen",
        "Liqun Shan",
        "Sai Venkatesh Chilukoti",
        "Sonya Hsu",
        "Xiali Hei"
      ],
      "abstract": "The optimization of the transpose convolution layer for deep learning\napplications is achieved with the kernel segregation mechanism. However, kernel\nsegregation has disadvantages, such as computing extra elements to obtain the\noutput feature map with odd dimensions while launching a thread. To mitigate\nthis problem, we introduce a unified kernel segregation approach that limits\nthe usage of memory and computational resources by employing one unified kernel\nto execute four sub-kernels. The findings reveal that the suggested approach\nachieves an average computational speedup of 2.03x (3.89x) when tested on\nspecific datasets with an RTX 2070 GPU (Intel Xeon CPU). The ablation study\nshows an average computational speedup of 3.5x when evaluating the transpose\nconvolution layers from well-known Generative Adversarial Networks (GANs). The\nimplementation of the proposed method for the transpose convolution layers in\nthe EB-GAN model demonstrates significant memory savings of up to 35 MB.",
      "tldr_zh": "本文提出了一种统一的 kernel-segregated 机制，用于优化深度学习中的 transpose convolution 操作，旨在减少内存和计算资源消耗，同时解决原有机制在处理奇数维度输出时的额外计算问题。该方法通过一个统一的 kernel 执行四个 sub-kernels，实现高效的计算加速。实验结果显示，在特定数据集上，该方法在 RTX 2070 GPU 上平均加速 2.03x，在 Intel Xeon CPU 上达到 3.89x；此外，在 GANs 的 transpose convolution 层上实现 3.5x 平均加速，并在 EB-GAN 模型中节省高达 35 MB 内存。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20493v1",
      "published_date": "2025-02-27 19:56:25 UTC",
      "updated_date": "2025-02-27 19:56:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:38:32.046044"
    },
    {
      "arxiv_id": "2502.20490v3",
      "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "MohammadHossein Rezaei",
        "Yicheng Fu",
        "Phil Cuvin",
        "Caleb Ziems",
        "Yanzhe Zhang",
        "Hao Zhu",
        "Diyi Yang"
      ],
      "abstract": "Human activity is moderated by norms. However, machines are often trained\nwithout explicit supervision on norm understanding and reasoning, particularly\nwhen norms are physically- or socially-grounded. To improve and evaluate the\nnormative reasoning capability of vision-language models (VLMs), we present\n\\dataset{} $\\|\\epsilon\\|$, consisting of 1,853 challenging, multi-stage MCQ\nquestions based on ego-centric videos of human interactions, evaluating both\nthe prediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 54\\% on \\dataset{} (versus a human\nbench of 92\\%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation (RAG) method, it is possible to\nuse \\dataset{} to enhance normative reasoning in VLMs.",
      "tldr_zh": "本论文引入了EgoNormia数据集，用于评估视觉语言模型(VLMs)在物理社会规范理解方面的能力，该数据集包含1,853个基于第一人称视角视频的多阶段多项选择题(MCQ)，涵盖安全、隐私、亲密距离、礼貌、合作、协调/主动性和沟通/可读性七个类别。数据集通过一个创新管道构建，包括视频采样、自动答案生成、过滤和人工验证，以测试VLMs对规范性行为的预测和理由。实验结果显示，当前最先进的VLMs在EgoNormia上的得分最高仅为54%，远低于人类的92%，突显了在安全和隐私等领域的风险；此外，通过检索增强生成(RAG)方法，该数据集可用于提升VLMs的规范推理能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "V2, with updated VLM stats",
      "pdf_url": "http://arxiv.org/pdf/2502.20490v3",
      "published_date": "2025-02-27 19:54:16 UTC",
      "updated_date": "2025-05-04 23:41:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:38:44.548614"
    },
    {
      "arxiv_id": "2502.20482v1",
      "title": "R-ParVI: Particle-based variational inference through lens of rewards",
      "title_zh": "翻译失败",
      "authors": [
        "Yongchao Huang"
      ],
      "abstract": "A reward-guided, gradient-free ParVI method, \\textit{R-ParVI}, is proposed\nfor sampling partially known densities (e.g. up to a constant). R-ParVI\nformulates the sampling problem as particle flow driven by rewards: particles\nare drawn from a prior distribution, navigate through parameter space with\nmovements determined by a reward mechanism blending assessments from the target\ndensity, with the steady state particle configuration approximating the target\ngeometry. Particle-environment interactions are simulated by stochastic\nperturbations and the reward mechanism, which drive particles towards high\ndensity regions while maintaining diversity (e.g. preventing from collapsing\ninto clusters). R-ParVI offers fast, flexible, scalable and stochastic sampling\nand inference for a class of probabilistic models such as those encountered in\nBayesian inference and generative modelling.",
      "tldr_zh": "这篇论文提出了R-ParVI，一种基于奖励引导的梯度-free粒子变分推理（Particle-based Variational Inference）方法，用于采样部分已知密度（如到常数）。该方法将采样问题转化为由奖励机制驱动的粒子流动：粒子从先验分布抽取，通过参数空间移动，结合目标密度的评估和随机扰动，以向高密度区域汇聚同时保持多样性（如防止聚类）。R-ParVI的优势在于提供快速、灵活、可扩展的随机采样和推理，适用于贝叶斯推理（Bayesian inference）和生成建模（generative modelling）等概率模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20482v1",
      "published_date": "2025-02-27 19:50:22 UTC",
      "updated_date": "2025-02-27 19:50:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:38:55.258798"
    },
    {
      "arxiv_id": "2503.16466v1",
      "title": "ACE, Action and Control via Explanations: A Proposal for LLMs to Provide Human-Centered Explainability for Multimodal AI Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Elizabeth Anne Watkins",
        "Emanuel Moss",
        "Ramesh Manuvinakurike",
        "Meng Shi",
        "Richard Beckwith",
        "Giuseppe Raffa"
      ],
      "abstract": "In this short paper we address issues related to building multimodal AI\nsystems for human performance support in manufacturing domains. We make two\ncontributions: we first identify challenges of participatory design and\ntraining of such systems, and secondly, to address such challenges, we propose\nthe ACE paradigm: \"Action and Control via Explanations\". Specifically, we\nsuggest that LLMs can be used to produce explanations in the form of human\ninterpretable \"semantic frames\", which in turn enable end users to provide data\nthe AI system needs to align its multimodal models and representations,\nincluding computer vision, automatic speech recognition, and document inputs.\nACE, by using LLMs to \"explain\" using semantic frames, will help the human and\nthe AI system to collaborate, together building a more accurate model of humans\nactivities and behaviors, and ultimately more accurate predictive outputs for\nbetter task support, and better outcomes for human users performing manual\ntasks.",
      "tldr_zh": "该论文识别了构建多模态 AI 系统用于制造领域人类性能支持的挑战，包括参与式设计和训练问题，并提出 ACE（Action and Control via Explanations）范式作为解决方案。ACE 建议利用 LLMs 生成人类可解释的 semantic frames，这些框架允许终端用户提供数据来校准 AI 的多模态模型，如计算机视觉、自动语音识别和文档输入。最终，该方法促进人类与 AI 的协作，构建更准确的活动行为模型，从而提升任务预测的精确性和人类用户的手动任务表现。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at Human-Centered Explainable AI workshop at CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.16466v1",
      "published_date": "2025-02-27 19:27:57 UTC",
      "updated_date": "2025-02-27 19:27:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:39:07.890087"
    },
    {
      "arxiv_id": "2502.20475v2",
      "title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyi Lorena Yan",
        "Robin Jia"
      ],
      "abstract": "To answer one-to-many factual queries (e.g., listing cities of a country), a\nlanguage model (LM) must simultaneously recall knowledge and avoid repeating\nprevious answers. How are these two subtasks implemented and integrated\ninternally? Across multiple datasets and models, we identify a\npromote-then-suppress mechanism: the model first recalls all answers, and then\nsuppresses previously generated ones. Specifically, LMs use both the subject\nand previous answer tokens to perform knowledge recall, with attention\npropagating subject information and MLPs promoting the answers. Then, attention\nattends to and suppresses previous answer tokens, while MLPs amplify the\nsuppression signal. Our mechanism is corroborated by extensive experimental\nevidence: in addition to using early decoding and causal tracing, we analyze\nhow components use different tokens by introducing both Token Lens, which\ndecodes aggregated attention updates from specified tokens, and a knockout\nmethod that analyzes changes in MLP outputs after removing attention to\nspecified tokens. Overall, we provide new insights into how LMs' internal\ncomponents interact with different input tokens to support complex factual\nrecall. Code is available at\nhttps://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.",
      "tldr_zh": "本研究探讨了语言模型（LM）如何处理一对多事实查询（如列出国家城市），揭示了一个“promote-then-suppress”机制：模型先通过主题和先前答案标记回想所有答案，利用 attention 传播信息和 MLPs 促进答案生成，然后通过 attention 抑制重复答案并由 MLPs 放大抑制信号。研究者通过多种数据集和模型的实验，包括早期解码、因果追踪、Token Lens（解码聚合 attention 更新）和 knockout 方法（分析移除指定标记后 MLP 输出变化），提供了实验证据支持这一机制。总体而言，该工作为理解 LM 内部组件（如 attention 和 MLPs）如何与输入标记交互以支持复杂事实回想提供了新见解，并公开了代码以便进一步验证。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20475v2",
      "published_date": "2025-02-27 19:23:15 UTC",
      "updated_date": "2025-03-05 13:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:39:20.673365"
    },
    {
      "arxiv_id": "2503.01894v2",
      "title": "LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Rashid Mushkani",
        "Shravan Nayak",
        "Hugo Berard",
        "Allison Cohen",
        "Shin Koseki",
        "Hadrien Bertrand"
      ],
      "abstract": "We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a\nbenchmark for multi-criteria alignment, developed through a two-year\nparticipatory process with 30 community organizations to support the\npluralistic alignment of text-to-image (T2I) models in inclusive urban\nplanning. The dataset encodes 37,710 pairwise comparisons across 13,462 images,\nstructured along six criteria - Accessibility, Safety, Comfort, Invitingness,\nInclusivity, and Diversity - derived from 634 community-defined concepts. Using\nDirect Preference Optimization (DPO), we fine-tune Stable Diffusion XL to\nreflect multi-criteria spatial preferences and evaluate the LIVS dataset and\nthe fine-tuned model through four case studies: (1) DPO increases alignment\nwith annotated preferences, particularly when annotation volume is high; (2)\npreference patterns vary across participant identities, underscoring the need\nfor intersectional data; (3) human-authored prompts generate more distinctive\nvisual outputs than LLM-generated ones, influencing annotation decisiveness;\nand (4) intersectional groups assign systematically different ratings across\ncriteria, revealing the limitations of single-objective alignment. While DPO\nimproves alignment under specific conditions, the prevalence of neutral ratings\nindicates that community values are heterogeneous and often ambiguous. LIVS\nprovides a benchmark for developing T2I models that incorporate local,\nstakeholder-driven preferences, offering a foundation for context-aware\nalignment in spatial design.",
      "tldr_zh": "本研究引入了LIVS数据集，这是一个用于多标准对齐的基准，旨在支持文本到图像(T2I)模型在包容性城市规划中的应用，通过两年参与式过程与30个社区组织合作，涵盖37,710个成对比较和13,462张图像，基于六个标准（Accessibility, Safety, Comfort, Invitingness, Inclusivity, and Diversity）。研究采用Direct Preference Optimization (DPO)微调Stable Diffusion XL模型，以反映多标准空间偏好，实验显示DPO显著提高了模型与标注偏好的对齐，尤其在标注量高时，同时揭示了偏好模式因参与者身份而异，以及人类提示比LLM提示产生更独特的输出。结果强调了交叉群体在标准评分上的系统差异，突显了单一目标对齐的局限性，并为开发融入本地利益相关者偏好的T2I模型提供了基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.01894v2",
      "published_date": "2025-02-27 19:18:37 UTC",
      "updated_date": "2025-05-08 03:50:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:39:32.459459"
    },
    {
      "arxiv_id": "2502.20396v1",
      "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
      "title_zh": "翻译失败",
      "authors": [
        "Toru Lin",
        "Kartik Sachdev",
        "Linxi Fan",
        "Jitendra Malik",
        "Yuke Zhu"
      ],
      "abstract": "Reinforcement learning has delivered promising results in achieving human- or\neven superhuman-level capabilities across diverse problem domains, but success\nin dexterous robot manipulation remains limited. This work investigates the key\nchallenges in applying reinforcement learning to solve a collection of\ncontact-rich manipulation tasks on a humanoid embodiment. We introduce novel\ntechniques to overcome the identified challenges with empirical validation. Our\nmain contributions include an automated real-to-sim tuning module that brings\nthe simulated environment closer to the real world, a generalized reward design\nscheme that simplifies reward engineering for long-horizon contact-rich\nmanipulation tasks, a divide-and-conquer distillation process that improves the\nsample efficiency of hard-exploration problems while maintaining sim-to-real\nperformance, and a mixture of sparse and dense object representations to bridge\nthe sim-to-real perception gap. We show promising results on three humanoid\ndexterous manipulation tasks, with ablation studies on each technique. Our work\npresents a successful approach to learning humanoid dexterous manipulation\nusing sim-to-real reinforcement learning, achieving robust generalization and\nhigh performance without the need for human demonstration.",
      "tldr_zh": "这篇论文探讨了将强化学习（Reinforcement Learning）应用于人形机器人的人类级别或超人类级别视觉-based灵巧操作任务，特别针对接触丰富的操作挑战。研究引入了多项创新技术，包括自动的real-to-sim调优模块以缩小模拟与现实差距、通用奖励设计方案简化长horizon任务的奖励工程、分治蒸馏过程提升样本效率，以及混合稀疏和密集对象表示桥接感知差距。通过实验验证，这些方法在三个灵巧操作任务上实现了鲁棒泛化和高性能，无需人类演示。结果表明，该方法显著提高了操作的可靠性和效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page can be found at https://toruowo.github.io/recipe/",
      "pdf_url": "http://arxiv.org/pdf/2502.20396v1",
      "published_date": "2025-02-27 18:59:52 UTC",
      "updated_date": "2025-02-27 18:59:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:39:44.025104"
    },
    {
      "arxiv_id": "2502.20393v1",
      "title": "Walking the Web of Concept-Class Relationships in Incrementally Trained Interpretable Models",
      "title_zh": "在增量训练的可解释模型中遍历概念-类别关系的网络",
      "authors": [
        "Susmit Agrawal",
        "Deepika Vemuri",
        "Sri Siddarth Chakaravarthy P",
        "Vineeth N. Balasubramanian"
      ],
      "abstract": "Concept-based methods have emerged as a promising direction to develop\ninterpretable neural networks in standard supervised settings. However, most\nworks that study them in incremental settings assume either a static concept\nset across all experiences or assume that each experience relies on a distinct\nset of concepts. In this work, we study concept-based models in a more\nrealistic, dynamic setting where new classes may rely on older concepts in\naddition to introducing new concepts themselves. We show that concepts and\nclasses form a complex web of relationships, which is susceptible to\ndegradation and needs to be preserved and augmented across experiences. We\nintroduce new metrics to show that existing concept-based models cannot\npreserve these relationships even when trained using methods to prevent\ncatastrophic forgetting, since they cannot handle forgetting at concept, class,\nand concept-class relationship levels simultaneously. To address these issues,\nwe propose a novel method - MuCIL - that uses multimodal concepts to perform\nclassification without increasing the number of trainable parameters across\nexperiences. The multimodal concepts are aligned to concepts provided in\nnatural language, making them interpretable by design. Through extensive\nexperimentation, we show that our approach obtains state-of-the-art\nclassification performance compared to other concept-based models, achieving\nover 2$\\times$ the classification performance in some cases. We also study the\nability of our model to perform interventions on concepts, and show that it can\nlocalize visual concepts in input images, providing post-hoc interpretations.",
      "tldr_zh": "该论文探讨了在增量训练中，concept-based models 处理动态 concept-class 关系的挑战，指出现有模型难以同时防止 catastrophic forgetting 在概念、类和关系层面的遗忘，从而导致关系退化。作者引入新指标评估这些问题，并提出 MuCIL 方法，使用 multimodal concepts 进行分类，该方法不增加可训练参数，并通过与自然语言概念对齐实现固有可解释性。实验结果显示，MuCIL 比其他 concept-based 模型提升超过 2 倍的分类性能，并能进行概念干预以定位视觉概念，提供后验解释。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages of main text, 6 figures in main text, 11 pages of Appendix,\n  published in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.20393v1",
      "published_date": "2025-02-27 18:59:29 UTC",
      "updated_date": "2025-02-27 18:59:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:39:55.700941"
    },
    {
      "arxiv_id": "2502.20432v2",
      "title": "Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Jingru Jia",
        "Zehua Yuan",
        "Junhao Pan",
        "Paul E. McNamara",
        "Deming Chen"
      ],
      "abstract": "Strategic decision-making involves interactive reasoning where agents adapt\ntheir choices in response to others, yet existing evaluations of large language\nmodels (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking\nthe mechanisms driving their strategic choices. To bridge this gap, we\nintroduce an evaluation framework grounded in behavioral game theory,\ndisentangling reasoning capability from contextual effects. Testing 22\nstate-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1\ndominate most games yet also demonstrate that the model scale alone does not\ndetermine performance. In terms of prompting enhancement, Chain-of-Thought\n(CoT) prompting is not universally effective, as it increases strategic\nreasoning only for models at certain levels while providing limited gains\nelsewhere. Additionally, we investigate the impact of encoded demographic\nfeatures on the models, observing that certain assignments impact the\ndecision-making pattern. For instance, GPT-4o shows stronger strategic\nreasoning with female traits than males, while Gemma assigns higher reasoning\nlevels to heterosexual identities compared to other sexual orientations,\nindicating inherent biases. These findings underscore the need for ethical\nstandards and contextual alignment to balance improved reasoning with fairness.",
      "tldr_zh": "该论文引入一个基于行为博弈理论的评估框架，用于评估大型语言模型（LLMs）的战略决策能力，该框架将推理能力与上下文影响分开，以揭示LLMs在交互博弈中的表现机制。测试22个最先进LLMs的结果显示，GPT-o3-mini、GPT-o1和DeepSeek-R1在多数游戏中表现出色，但模型规模并非唯一决定因素，且Chain-of-Thought (CoT) 提示仅对某些模型有效，提升有限。研究还发现，编码的demographic特征（如性别和性取向）会影响决策模式，例如GPT-4o在女性特征时显示更强战略推理，而Gemma对异性恋身份赋予更高推理水平，揭示了模型的固有偏见，并强调需要伦理标准和上下文调整以实现公平的推理改进。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "*Co-first author: Jingru Jia, Zehua Yuan",
      "pdf_url": "http://arxiv.org/pdf/2502.20432v2",
      "published_date": "2025-02-27 18:58:31 UTC",
      "updated_date": "2025-03-13 17:59:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:40:09.154816"
    },
    {
      "arxiv_id": "2502.20382v1",
      "title": "Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Lujie Yang",
        "H. J. Terry Suh",
        "Tong Zhao",
        "Bernhard Paus Graesdal",
        "Tarik Kelestemur",
        "Jiuguang Wang",
        "Tao Pang",
        "Russ Tedrake"
      ],
      "abstract": "We present a low-cost data generation pipeline that integrates physics-based\nsimulation, human demonstrations, and model-based planning to efficiently\ngenerate large-scale, high-quality datasets for contact-rich robotic\nmanipulation tasks. Starting with a small number of embodiment-flexible human\ndemonstrations collected in a virtual reality simulation environment, the\npipeline refines these demonstrations using optimization-based kinematic\nretargeting and trajectory optimization to adapt them across various robot\nembodiments and physical parameters. This process yields a diverse, physically\nconsistent dataset that enables cross-embodiment data transfer, and offers the\npotential to reuse legacy datasets collected under different hardware\nconfigurations or physical parameters. We validate the pipeline's effectiveness\nby training diffusion policies from the generated datasets for challenging\ncontact-rich manipulation tasks across multiple robot embodiments, including a\nfloating Allegro hand and bimanual robot arms. The trained policies are\ndeployed zero-shot on hardware for bimanual iiwa arms, achieving high success\nrates with minimal human input. Project website:\nhttps://lujieyang.github.io/physicsgen/.",
      "tldr_zh": "本文提出了一种基于物理驱动的数据生成管道，用于高效生成大规模、高质量数据集，以支持接触丰富的机器人操作任务。该管道整合physics-based simulation、人体演示和trajectory optimization，通过optimization-based kinematic retargeting对少量虚拟现实演示进行精炼，实现跨机器人实施的数据转移和遗留数据集重用。在实验中，使用生成的数据集训练diffusion policies，并在多种机器人（如浮动Allegro手和bimanual iiwa arms）上进行零样本部署，取得了高成功率，证明了该方法的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20382v1",
      "published_date": "2025-02-27 18:56:01 UTC",
      "updated_date": "2025-02-27 18:56:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:40:20.288916"
    },
    {
      "arxiv_id": "2502.20380v1",
      "title": "Multi-Turn Code Generation Through Single-Step Rewards",
      "title_zh": "通过单步奖励的多轮次代码生成",
      "authors": [
        "Arnav Kumar Jain",
        "Gonzalo Gonzalez-Pumariega",
        "Wayne Chen",
        "Alexander M Rush",
        "Wenting Zhao",
        "Sanjiban Choudhury"
      ],
      "abstract": "We address the problem of code generation from multi-turn execution feedback.\nExisting methods either generate code without feedback or use complex,\nhierarchical reinforcement learning to optimize multi-turn rewards. We propose\na simple yet scalable approach, $\\mu$Code, that solves multi-turn code\ngeneration using only single-step rewards. Our key insight is that code\ngeneration is a one-step recoverable MDP, where the correct code can be\nrecovered from any intermediate code state in a single turn. $\\mu$Code\niteratively trains both a generator to provide code solutions conditioned on\nmulti-turn execution feedback and a verifier to score the newly generated code.\nExperimental evaluations show that our approach achieves significant\nimprovements over the state-of-the-art baselines. We provide analysis of the\ndesign choices of the reward models and policy, and show the efficacy of\n$\\mu$Code at utilizing the execution feedback. Our code is available at\nhttps://github.com/portal-cornell/muCode.",
      "tldr_zh": "该论文提出了一种名为 $\\mu$Code 的简单可扩展方法，用于通过单步奖励解决多轮代码生成问题，避开了现有复杂层次强化学习（reinforcement learning）的需求。关键洞见是代码生成是一个单步可恢复的 MDP（Markov Decision Process），允许从任何中间代码状态中快速恢复；该方法迭代训练一个生成器（基于执行反馈生成代码）和一个验证器（对新代码进行评分）。实验评估表明，$\\mu$Code 比最先进基线取得了显著改进，并分析了奖励模型和策略的设计选择及其对执行反馈利用的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages (not including references or appendix); 6 figures (in main\n  paper); (v1) preprint",
      "pdf_url": "http://arxiv.org/pdf/2502.20380v1",
      "published_date": "2025-02-27 18:55:05 UTC",
      "updated_date": "2025-02-27 18:55:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:40:33.818930"
    },
    {
      "arxiv_id": "2502.20379v1",
      "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers",
      "title_zh": "翻译失败",
      "authors": [
        "Shalev Lifshitz",
        "Sheila A. McIlraith",
        "Yilun Du"
      ],
      "abstract": "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
      "tldr_zh": "该研究提出Multi-Agent Verification (MAV)，一种通过增加验证器数量来扩展测试时计算的新范式，以提升大型语言模型 (LLMs) 的性能，而无需额外训练。MAV利用Aspect Verifiers (AVs)，即现成的LLMs通过提示验证输出不同方面，并引入BoN-MAV算法，将best-of-n采样与多个验证器结合，实现更强的扩展性。实验结果显示，BoN-MAV比self-consistency和reward model verification表现出色，支持weak-to-strong generalization和self-improvement，从而确立扩展验证器数量作为改进LLMs性能的新维度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20379v1",
      "published_date": "2025-02-27 18:53:30 UTC",
      "updated_date": "2025-02-27 18:53:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:40:44.931854"
    },
    {
      "arxiv_id": "2502.20377v1",
      "title": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Albert Gong",
        "Kamilė Stankevičiūtė",
        "Chao Wan",
        "Anmol Kabra",
        "Raphael Thesmar",
        "Johann Lee",
        "Julius Klenke",
        "Carla P. Gomes",
        "Kilian Q. Weinberger"
      ],
      "abstract": "High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki.",
      "tldr_zh": "论文提出 PhantomWiki，这是一个按需生成数据集的管道，用于评估大型语言模型（LLMs）的推理和检索能力，以解决现有数据集易受数据泄露和性能夸大问题的挑战。不同于固定数据集，PhantomWiki 每次评估时生成独特、事实一致的文档语料库和多样化问答对，通过调整问题难度和语料库大小来分别解耦推理和检索能力。实验结果显示，这些数据集对前沿 LLMs 具有显著挑战性。该框架为评估 LLMs 的推理、检索和工具使用能力提供了一个可扩展、抗数据泄露的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20377v1",
      "published_date": "2025-02-27 18:51:22 UTC",
      "updated_date": "2025-02-27 18:51:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:40:56.185164"
    },
    {
      "arxiv_id": "2502.20364v2",
      "title": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan C. Barron",
        "Maksim E. Eren",
        "Olga M. Serafimova",
        "Cynthia Matuszek",
        "Boian S. Alexandrov"
      ],
      "abstract": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.",
      "tldr_zh": "该论文提出了一种整合 Retrieval-Augmented Generation (RAG)、Vector Stores (VS) 和 Knowledge Graphs (KGs) 的生成 AI 系统，利用 Hierarchical Non-negative Matrix Factorization (NMF) 来桥接法律知识与 AI，旨在处理法律领域的复杂半结构化数据，如法规和案例法。系统通过 web scraping 技术收集法律文本，并结合高级语义表示和潜在主题发现，提升信息检索、AI 推理能力并减少幻觉。实验结果显示，该框架支持法律文档聚类、总结和交叉引用，提高了法律研究的准确性、可解释性和可扩展性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 8 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.20364v2",
      "published_date": "2025-02-27 18:35:39 UTC",
      "updated_date": "2025-05-09 00:25:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:41:09.272319"
    },
    {
      "arxiv_id": "2502.20356v1",
      "title": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs",
      "title_zh": "弥合创造力理解差距：小规模人类对齐使大语言模型实现专家级幽默排名",
      "authors": [
        "Kuan Lok Zhou",
        "Jiayi Chen",
        "Siddharth Suresh",
        "Reuben Narad",
        "Timothy T. Rogers",
        "Lalit K Jain",
        "Robert D Nowak",
        "Bob Mankoff",
        "Jifan Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding.",
      "tldr_zh": "本文研究揭示了 Large Language Models (LLMs) 在理解创意内容（如幽默）方面的局限性，并通过小规模人类对齐方法显著提升其性能。研究团队将幽默理解分解为三个组件，包括改进视觉注释、利用 LLM 生成的幽默推理和解释，以及针对人类偏好数据的微调，从而在 New Yorker Cartoon Caption Contest (NYCCC) 中将标题排名准确率从 67% 提高到 82.4%，达到人类专家水平。结果显示，模型微调比 persona prompts 更有效，并提出系统收集人类偏好数据是实现人工通用智能 (AGI) 中创意理解的关键。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20356v1",
      "published_date": "2025-02-27 18:29:09 UTC",
      "updated_date": "2025-02-27 18:29:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:41:23.137545"
    },
    {
      "arxiv_id": "2502.20354v1",
      "title": "Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Nazarii Drushchak",
        "Vladyslava Tyshchenko",
        "Nataliya Polyakovska"
      ],
      "abstract": "The growth of Educational Technology (EdTech) has enabled highly personalized\nlearning experiences through Artificial Intelligence (AI)-based recommendation\nsystems tailored to each student needs. However, these systems can\nunintentionally introduce biases, potentially limiting fair access to learning\nresources. This study presents a recommendation system for K-12 students,\ncombining graph-based modeling and matrix factorization to provide personalized\nsuggestions for extracurricular activities, learning resources, and\nvolunteering opportunities. To address fairness concerns, the system includes a\nframework to detect and reduce biases by analyzing feedback across protected\nstudent groups. This work highlights the need for continuous monitoring in\neducational recommendation systems to support equitable, transparent, and\neffective learning opportunities for all students.",
      "tldr_zh": "本研究探讨了教育技术(EdTech)中AI推荐系统的偏见问题，提出一个针对K-12学生的混合推荐系统，结合graph-based modeling和matrix factorization，提供个性化的课外活动、学习资源和志愿机会建议。系统引入一个框架，通过分析受保护学生群体的反馈来检测和减少偏见，确保公平访问学习资源。该工作强调了在教育AI中进行持续监控的重要性，以实现公平、透明和有效的学习机会。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20354v1",
      "published_date": "2025-02-27 18:27:30 UTC",
      "updated_date": "2025-02-27 18:27:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:41:33.149001"
    },
    {
      "arxiv_id": "2502.20349v1",
      "title": "Naturalistic Computational Cognitive Science: Towards generalizable models and theories that capture the full range of natural behavior",
      "title_zh": "自然主义计算认知科学：朝着能够捕捉自然行为全部范围的可泛化模型和理论",
      "authors": [
        "Wilka Carvalho",
        "Andrew Lampinen"
      ],
      "abstract": "Artificial Intelligence increasingly pursues large, complex models that\nperform many tasks within increasingly realistic domains. How, if at all,\nshould these developments in AI influence cognitive science?\n  We argue that progress in AI offers timely opportunities for cognitive\nscience to embrace experiments with increasingly naturalistic stimuli, tasks,\nand behaviors; and computational models that can accommodate these changes. We\nfirst review a growing body of research spanning neuroscience, cognitive\nscience, and AI that suggests that incorporating a broader range of\nnaturalistic experimental paradigms (and models that accommodate them) may be\nnecessary to resolve some aspects of natural intelligence and ensure that our\ntheories generalize. We then suggest that integrating recent progress in AI and\ncognitive science will enable us to engage with more naturalistic phenomena\nwithout giving up experimental control or the pursuit of theoretically grounded\nunderstanding. We offer practical guidance on how methodological practices can\ncontribute to cumulative progress in naturalistic computational cognitive\nscience, and illustrate a path towards building computational models that solve\nthe real problems of natural cognition - together with a reductive\nunderstanding of the processes and principles by which they do so.",
      "tldr_zh": "本论文探讨了人工智能（AI）的发展如何影响认知科学，主张认知科学应采用更自然的实验刺激、任务和行为，以及相应的计算模型，以构建更具泛化性的理论。作者回顾了神经科学、认知科学和AI领域的现有研究，指出使用自然主义实验范式可能有助于解决自然智能的关键问题，并确保理论的广泛适用。论文建议通过整合AI的最新进展与认知科学的实验控制，实现对自然现象的深入理解，并提供实用指导来推动计算认知科学的累进发展，最终构建能解决真实认知问题的模型，同时揭示其底层过程和原则。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20349v1",
      "published_date": "2025-02-27 18:20:54 UTC",
      "updated_date": "2025-02-27 18:20:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:41:44.952252"
    },
    {
      "arxiv_id": "2502.20339v1",
      "title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners",
      "title_zh": "翻译失败",
      "authors": [
        "Daniele Paliotta",
        "Junxiong Wang",
        "Matteo Pagliardini",
        "Kevin Y. Li",
        "Aviv Bick",
        "J. Zico Kolter",
        "Albert Gu",
        "François Fleuret",
        "Tri Dao"
      ],
      "abstract": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute.",
      "tldr_zh": "该研究探讨了如何通过蒸馏模型来扩展推理计算资源，以提升大语言模型（LLMs）的性能。具体方法包括从预训练的 Transformer 模型中蒸馏出纯和混合 Mamba 模型，这些模型仅在 8 亿 tokens 上训练，便在数学推理数据集上表现出强劲的可扩展性和速度优势，尤其在处理大批量和长序列时更快。尽管蒸馏导致零样本性能下降，但这些 Mamba 模型在固定时间预算下，能超越 Transformer 教师模型的覆盖和准确性。该工作开辟了利用较低复杂度的模型规模化推理计算的新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20339v1",
      "published_date": "2025-02-27 18:08:16 UTC",
      "updated_date": "2025-02-27 18:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:41:57.466529"
    },
    {
      "arxiv_id": "2502.20335v1",
      "title": "Expertise Is What We Want",
      "title_zh": "专业知识是我们想要的",
      "authors": [
        "Alan Ashworth",
        "Munir Al-Dajani",
        "Keegan Duchicela",
        "Kiril Kafadarov",
        "Allison Kurian",
        "Othman Laraki",
        "Amina Lazrak",
        "Divneet Mandair",
        "Wendy McKennon",
        "Rebecca Miksad",
        "Jayodita Sanghvi",
        "Travis Zack"
      ],
      "abstract": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center.",
      "tldr_zh": "该论文提出 Large Language Expert (LLE) 架构，将 Large Language Models (LLMs) 的灵活性和强大处理能力与 Expert Systems 的可解释性、可靠性和可测试性相结合，旨在解决临床决策支持系统的准确性和细微差别问题。LLE 系统通过 LLMs 处理知识整合、数据归一化和幻觉挑战，同时利用专家系统-like 方法确保更新和测试的原子性。论文以癌症患者诊断工作up 为例，构建了一个 LLE 系统，用于分析非结构化健康记录并应用临床决策逻辑。结果显示，该系统在真实数据中准确率超过95%，有效识别和解决乳腺癌和结肠癌患者的临床差距。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; J.3"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.20335v1",
      "published_date": "2025-02-27 18:05:15 UTC",
      "updated_date": "2025-02-27 18:05:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:42:09.578328"
    },
    {
      "arxiv_id": "2502.20332v1",
      "title": "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yukang Yang",
        "Declan Campbell",
        "Kaixuan Huang",
        "Mengdi Wang",
        "Jonathan Cohen",
        "Taylor Webb"
      ],
      "abstract": "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms.",
      "tldr_zh": "本研究探讨大型语言模型（Large Language Models）中新兴的抽象推理能力，针对其鲁棒性和对结构化机制的依赖性进行全面分析。通过对 Llama3-70B 模型的内部机制研究，作者识别出一个新兴符号架构（emergent symbolic architecture），它通过三个计算步骤实现抽象规则归纳：早期层的符号抽象 heads 将输入 tokens 转换为基于关系的抽象变量、中间层的符号归纳 heads 对这些变量进行序列归纳，以及后期层的检索 heads 通过检索相关值来预测下一个 token。该发现表明，神经网络中的推理能力依赖于符号机制的出现，从而为符号和神经网络方法之间的长期争论提供新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20332v1",
      "published_date": "2025-02-27 18:02:15 UTC",
      "updated_date": "2025-02-27 18:02:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:42:22.313032"
    },
    {
      "arxiv_id": "2502.20326v1",
      "title": "Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Hickling",
        "Maxwell Hogan",
        "Abdulla Tammam",
        "Nabil Aouf"
      ],
      "abstract": "This paper proposes a holistic framework for autonomous guidance, navigation,\nand task distribution among multi-drone systems operating in Global Navigation\nSatellite System (GNSS)-denied indoor settings. We advocate for a Deep\nReinforcement Learning (DRL)-based guidance mechanism, utilising the Twin\nDelayed Deep Deterministic Policy Gradient algorithm. To improve the efficiency\nof the training process, we incorporate an Artificial Potential Field\n(APF)-based reward structure, enabling the agent to refine its movements,\nthereby promoting smoother paths and enhanced obstacle avoidance in indoor\ncontexts. Furthermore, we tackle the issue of task distribution among\ncooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). This\nGCN represents the interactions between drones and tasks, facilitating dynamic\nand real-time task allocation that reflects the current environmental\nconditions and the capabilities of the drones. Such an approach fosters\neffective coordination and collaboration among multiple drones during search\nand rescue operations or other exploratory endeavours. Lastly, to ensure\nprecise odometry in environments lacking GNSS, we employ Light Detection And\nRanging Simultaneous Localisation and Mapping complemented by a depth camera to\nmitigate the hallway problem. This integration offers robust localisation and\nmapping functionalities, thereby enhancing the systems dependability in indoor\nnavigation. The proposed multi-drone framework not only elevates individual\nnavigation capabilities but also optimises coordinated task allocation in\ncomplex, obstacle-laden environments. Experimental evaluations conducted in a\nsetup tailored to meet the requirements of the NATO Sapience Autonomous\nCooperative Drone Competition demonstrate the efficacy of the proposed system,\nyielding outstanding results and culminating in a first-place finish in the\n2024 Sapience competition.",
      "tldr_zh": "本论文提出一个整体框架，用于多无人机系统在GNSS-denied室内环境的自主引导、导航和任务分配，针对搜索和救援等实际应用。框架采用Deep Reinforcement Learning (DRL)中的Twin Delayed Deep Deterministic Policy Gradient算法，并结合Artificial Potential Field (APF)-based奖励结构来优化训练过程，实现平滑路径和增强障碍避免；同时，使用DRL训练的Graph Convolutional Network (GCN)进行动态任务分配，以促进无人机间的协作。实验结果显示，该框架在复杂环境中显著提升了导航和协调性能，并在2024年NATO Sapience竞赛中获得第一名。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "18 Pages, 21 Figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20326v1",
      "published_date": "2025-02-27 17:53:16 UTC",
      "updated_date": "2025-02-27 17:53:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:42:33.595513"
    },
    {
      "arxiv_id": "2502.20321v2",
      "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
      "title_zh": "UniTok：用于视觉生成和理解的统一标记化器",
      "authors": [
        "Chuofan Ma",
        "Yi Jiang",
        "Junfeng Wu",
        "Jihan Yang",
        "Xin Yu",
        "Zehuan Yuan",
        "Bingyue Peng",
        "Xiaojuan Qi"
      ],
      "abstract": "Visual generative and understanding models typically rely on distinct\ntokenizers to process images, presenting a key challenge for unifying them\nwithin a single framework. Recent studies attempt to address this by connecting\nthe training of VQVAE (for autoregressive generation) and CLIP (for\nunderstanding) to build a unified tokenizer. However, directly combining these\ntraining objectives has been observed to cause severe loss conflicts. In this\npaper, we show that reconstruction and semantic supervision do not inherently\nconflict. Instead, the underlying bottleneck stems from limited\nrepresentational capacity of discrete token space. Building on these insights,\nwe introduce UniTok, a unified tokenizer featuring a novel multi-codebook\nquantization mechanism that effectively scales up the vocabulary size and\nbottleneck dimension. In terms of final performance, UniTok sets a new record\nof 0.38 rFID and 78.6% zero-shot accuracy on ImageNet. Besides, UniTok can be\nseamlessly integrated into MLLMs to unlock native visual generation capability,\nwithout compromising the understanding performance. Additionally, we show that\nUniTok favors cfg-free generation, reducing gFID from 14.6 to 2.5 on ImageNet\n256$\\times$256 benchmark. GitHub: https://github.com/FoundationVision/UniTok.",
      "tldr_zh": "该研究提出 UniTok，一种统一的 tokenizer，用于解决视觉生成和理解模型因使用不同 tokenizer 而难以集成的挑战。通过分析发现，重建和语义监督（如 VQVAE 和 CLIP 的训练）本身无冲突，问题在于离散 token 空间的表示能力有限。UniTok 引入新型的多 codebook 量化机制，扩展词汇大小和瓶颈维度，从而提升整体性能。在 ImageNet 上，UniTok 实现 0.38 rFID 和 78.6% zero-shot 准确率的新记录，并可无缝集成到 MLLMs 中，支持 cfg-free 生成，将 gFID 从 14.6 降至 2.5。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20321v2",
      "published_date": "2025-02-27 17:47:01 UTC",
      "updated_date": "2025-05-19 12:45:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:42:45.926997"
    },
    {
      "arxiv_id": "2502.20317v3",
      "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
      "title_zh": "翻译失败",
      "authors": [
        "Yongjia Lei",
        "Haoyu Han",
        "Ryan A. Rossi",
        "Franck Dernoncourt",
        "Nedim Lipka",
        "Mahantesh M Halappanavar",
        "Jiliang Tang",
        "Yu Wang"
      ],
      "abstract": "Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for\nanswering queries by providing textual and structural knowledge. However,\ncurrent retrieval methods often retrieve these two types of knowledge in\nisolation without considering their mutual reinforcement and some hybrid\nmethods even bypass structural retrieval entirely after neighboring\naggregation. To fill in this gap, we propose a Mixture of\nStructural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge\nvia a Planning-Reasoning-Organizing framework. In the Planning stage, MoR\ngenerates textual planning graphs delineating the logic for answering queries.\nFollowing planning graphs, in the Reasoning stage, MoR interweaves structural\ntraversal and textual matching to obtain candidates from TG-KBs. In the\nOrganizing stage, MoR further reranks fetched candidates based on their\nstructural trajectory. Extensive experiments demonstrate the superiority of MoR\nin harmonizing structural and textual retrieval with insights, including uneven\nretrieving performance across different query logics and the benefits of\nintegrating structural trajectories for candidate reranking. Our code is\navailable at https://github.com/Yoega/MoR.",
      "tldr_zh": "这篇论文针对 Text-rich Graph Knowledge Bases (TG-KBs) 提出了一种 Mixture of Structural-and-Textual Retrieval (MoR) 方法，以整合结构和文本知识的检索，避免传统方法孤立处理或忽略结构检索。MoR 采用 Planning-Reasoning-Organizing 框架：在 Planning 阶段生成文本规划图来定义查询逻辑；在 Reasoning 阶段交织结构遍历和文本匹配获取候选项；在 Organizing 阶段基于结构轨迹重新排名候选。实验证明，MoR 在协调两种检索类型方面显著优于基线模型，并揭示了不同查询逻辑的性能差异以及整合结构轨迹的益处。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20317v3",
      "published_date": "2025-02-27 17:42:52 UTC",
      "updated_date": "2025-03-10 14:43:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:42:59.968541"
    },
    {
      "arxiv_id": "2502.20316v1",
      "title": "Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamed Abdelsamad",
        "Michael Ulrich",
        "Claudius Gläser",
        "Abhinav Valada"
      ],
      "abstract": "Masked autoencoders (MAE) have shown tremendous potential for self-supervised\nlearning (SSL) in vision and beyond. However, point clouds from LiDARs used in\nautomated driving are particularly challenging for MAEs since large areas of\nthe 3D volume are empty. Consequently, existing work suffers from leaking\noccupancy information into the decoder and has significant computational\ncomplexity, thereby limiting the SSL pre-training to only 2D bird's eye view\nencoders in practice. In this work, we propose the novel neighborhood occupancy\nMAE (NOMAE) that overcomes the aforementioned challenges by employing masked\noccupancy reconstruction only in the neighborhood of non-masked voxels. We\nincorporate voxel masking and occupancy reconstruction at multiple scales with\nour proposed hierarchical mask generation technique to capture features of\nobjects of different sizes in the point cloud. NOMAEs are extremely flexible\nand can be directly employed for SSL in existing 3D architectures. We perform\nextensive evaluations on the nuScenes and Waymo Open datasets for the\ndownstream perception tasks of semantic segmentation and 3D object detection,\ncomparing with both discriminative and generative SSL methods. The results\ndemonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks for\nmultiple point cloud perception tasks.",
      "tldr_zh": "本文提出了一种新型的 Neighborhood Occupancy Masked Autoencoder (NOMAE)，用于 LiDAR 点云的自监督学习 (SSL)，以解决现有 Masked Autoencoders (MAE) 在处理大量空区域时存在的 occupancy 信息泄漏和计算复杂度高的问题。NOMAE 通过多尺度 voxel masking 和 occupancy 重建，仅在非masked voxels 的邻域中进行操作，并采用分层 mask 生成技术来捕捉点云中不同大小的对象特征，使其灵活适用于现有 3D 架构。在 nuScenes 和 Waymo Open 数据集上的实验显示，NOMAE 在语义分割和 3D 对象检测任务中超越了其他 SSL 方法，树立了新的 state-of-the-art 基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20316v1",
      "published_date": "2025-02-27 17:42:47 UTC",
      "updated_date": "2025-02-27 17:42:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:43:11.137406"
    },
    {
      "arxiv_id": "2502.20315v1",
      "title": "LangProBe: a Language Programs Benchmark",
      "title_zh": "LangProBe：语言程序基准",
      "authors": [
        "Shangyin Tan",
        "Lakshya A Agrawal",
        "Arnav Singhvi",
        "Liheng Lai",
        "Michael J Ryan",
        "Dan Klein",
        "Omar Khattab",
        "Koushik Sen",
        "Matei Zaharia"
      ],
      "abstract": "Composing language models (LMs) into multi-step language programs and\nautomatically optimizing their modular prompts is now a mainstream paradigm for\nbuilding AI systems, but the tradeoffs in this space have only scarcely been\nstudied before. We introduce LangProBe, the first large-scale benchmark for\nevaluating the architectures and optimization strategies for language programs,\nwith over 2000 combinations of tasks, architectures, optimizers, and choices of\nLMs. Using LangProBe, we are the first to study the impact of program\narchitectures and optimizers (and their compositions together and with\ndifferent models) on tradeoffs of quality and cost. We find that optimized\nlanguage programs offer strong cost--quality Pareto improvement over raw calls\nto models, but simultaneously demonstrate that human judgment (or empirical\ndecisions) about which compositions to pursue is still necessary for best\nperformance. We will open source the code and evaluation data for LangProBe.",
      "tldr_zh": "本文引入 LangProBe，这是一个大规模基准，用于评估语言程序的架构和优化策略，涵盖超过2000种任务、架构、优化器和语言模型 (LMs) 的组合。研究发现，优化后的语言程序在质量和成本上提供显著的Pareto改善，比直接调用模型更高效，但最佳组合仍需人类判断或经验决策。作者将开源代码和评估数据，以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20315v1",
      "published_date": "2025-02-27 17:41:49 UTC",
      "updated_date": "2025-02-27 17:41:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:43:22.291312"
    },
    {
      "arxiv_id": "2502.20309v1",
      "title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Franck Cappello",
        "Sandeep Madireddy",
        "Robert Underwood",
        "Neil Getty",
        "Nicholas Lee-Ping Chia",
        "Nesar Ramachandra",
        "Josh Nguyen",
        "Murat Keceli",
        "Tanwi Mallick",
        "Zilinghan Li",
        "Marieme Ngom",
        "Chenhui Zhang",
        "Angel Yanguas-Gil",
        "Evan Antoniuk",
        "Bhavya Kailkhura",
        "Minyang Tian",
        "Yufeng Du",
        "Yuan-Sen Ting",
        "Azton Wells",
        "Bogdan Nicolae",
        "Avinash Maurya",
        "M. Mustafa Rafique",
        "Eliu Huerta",
        "Bo Li",
        "Ian Foster",
        "Rick Stevens"
      ],
      "abstract": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains.",
      "tldr_zh": "该论文提出了 EAIRA 方法论，用于评估 Large Language Models (LLMs) 作为科学研究助理的有效性，旨在通过全面、严格的领域特定评估应对 AI 在复杂任务中的应用。EAIRA 包括四类评估：Multiple Choice Questions 测试事实回忆、Open Response 评估高级推理和问题解决技能、Lab-Style Experiments 在控制环境中分析研究助理能力，以及 Field-Style Experiments 捕获大规模科学领域中的研究者-LLM 互动。该方法设计为可演化和适应的，确保其在快速发展的 AI 环境中保持相关性，并可泛化到广泛的科学领域。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "33 pages, 18 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20309v1",
      "published_date": "2025-02-27 17:35:57 UTC",
      "updated_date": "2025-02-27 17:35:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:43:35.130561"
    },
    {
      "arxiv_id": "2502.20301v1",
      "title": "M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Jinghao Feng",
        "Qiaoyu Zheng",
        "Chaoyi Wu",
        "Ziheng Zhao",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "abstract": "Agentic AI systems have gained significant attention for their ability to\nautonomously perform complex tasks. However, their reliance on well-prepared\ntools limits their applicability in the medical domain, which requires to train\nspecialized models. In this paper, we make three contributions: (i) We present\nM3Builder, a novel multi-agent system designed to automate machine learning\n(ML) in medical imaging. At its core, M3Builder employs four specialized agents\nthat collaborate to tackle complex, multi-step medical ML workflows, from\nautomated data processing and environment configuration to self-contained auto\ndebugging and model training. These agents operate within a medical imaging ML\nworkspace, a structured environment designed to provide agents with free-text\ndescriptions of datasets, training codes, and interaction tools, enabling\nseamless communication and task execution. (ii) To evaluate progress in\nautomated medical imaging ML, we propose M3Bench, a benchmark comprising four\ngeneral tasks on 14 training datasets, across five anatomies and three imaging\nmodalities, covering both 2D and 3D data. (iii) We experiment with seven\nstate-of-the-art large language models serving as agent cores for our system,\nsuch as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic\ndesigns, M3Builder shows superior performance on completing ML tasks in medical\nimaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent\ncore, showing huge potential towards fully automated machine learning in\nmedical imaging.",
      "tldr_zh": "该论文提出 M^3Builder，一种多智能体系统，用于自动化医疗图像中的 Machine Learning (ML)，旨在通过四个专门智能体协作处理复杂工作流，包括数据处理、环境配置、自动调试和模型训练，这些智能体在医疗图像 ML 工作空间中操作，以实现无缝通信和任务执行。同时，论文引入 M^3Bench 基准，涵盖四个一般任务、14 个数据集、五种解剖结构和三种成像模式（2D 和 3D），用于评估自动化医疗 ML 的进展。实验使用七种先进的大型语言模型（如 Claude 系列、GPT-4o 和 DeepSeek-V3）作为智能体核心，结果显示 M^3Builder 比现有设计表现出色，使用 Claude-3.7-Sonnet 时成功率达 94.29%，展示了其在医疗图像自动化 ML 方面的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "38 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20301v1",
      "published_date": "2025-02-27 17:29:46 UTC",
      "updated_date": "2025-02-27 17:29:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:43:46.111416"
    },
    {
      "arxiv_id": "2502.20299v1",
      "title": "An exploration of features to improve the generalisability of fake news detection models",
      "title_zh": "翻译失败",
      "authors": [
        "Nathaniel Hoy",
        "Theodora Koulouri"
      ],
      "abstract": "Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection.",
      "tldr_zh": "本研究探讨了如何通过提取更有意义的特征来提升假新闻检测模型的泛化能力，以应对现有 NLP 和监督 Machine Learning 方法（如 TF-IDF 和 BERT）在跨数据集中的表现不佳问题，这些问题源于粗糙标签数据和模型对发布者偏差的敏感性。研究重点分析了风格特征，包括 lexical、syntactic 和 semantic 方面，以及新引入的社会货币化特征（如广告、外部链接和社会媒体元素），这些特征更不易受数据集偏差影响。实验在粗糙标签的 NELA 2020-21 数据集上训练，并使用手动标注的 Facebook URLs 数据集进行评估，结果显示风格和社会货币化特征比基于标记的模型和 LLMs（如 LLaMa）提供更强的泛化预测能力。最后，通过统计和置换特征重要性分析，证明这些特征能显著提升模型性能并缓解偏差，提供改进假新闻检测的实用路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at Expert Systems with Applications (Elsevier)",
      "pdf_url": "http://arxiv.org/pdf/2502.20299v1",
      "published_date": "2025-02-27 17:26:56 UTC",
      "updated_date": "2025-02-27 17:26:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:43:58.857771"
    },
    {
      "arxiv_id": "2502.20295v1",
      "title": "Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page Handwritten Document Transcription",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Gutteridge",
        "Matthew Thomas Jackson",
        "Toni Kukurin",
        "Xiaowen Dong"
      ],
      "abstract": "Handwritten text recognition (HTR) remains a challenging task, particularly\nfor multi-page documents where pages share common formatting and contextual\nfeatures. While modern optical character recognition (OCR) engines are\nproficient with printed text, their performance on handwriting is limited,\noften requiring costly labeled data for fine-tuning. In this paper, we explore\nthe use of multi-modal large language models (MLLMs) for transcribing\nmulti-page handwritten documents in a zero-shot setting. We investigate various\nconfigurations of commercial OCR engines and MLLMs, utilizing the latter both\nas end-to-end transcribers and as post-processors, with and without image\ncomponents. We propose a novel method, '+first page', which enhances MLLM\ntranscription by providing the OCR output of the entire document along with\njust the first page image. This approach leverages shared document features\nwithout incurring the high cost of processing all images. Experiments on a\nmulti-page version of the IAM Handwriting Database demonstrate that '+first\npage' improves transcription accuracy, balances cost with performance, and even\nenhances results on out-of-sample text by extrapolating formatting and OCR\nerror patterns from a single page.",
      "tldr_zh": "这篇论文探讨了手写文本识别 (HTR) 的挑战，特别是多页文档的转录问题，因为页面共享格式和上下文特征，而现有光学字符识别 (OCR) 引擎在处理手写文本时表现有限且需大量标注数据。研究者调查了多模态大型语言模型 (MLLMs) 在零样本设置下作为端到端转录器或后处理器的应用，并提出了一种新方法 \"+first page\"，该方法仅使用第一页图像结合整个文档的 OCR 输出，以利用共享特征并降低计算成本。实验结果显示，在 IAM Handwriting Database 的多页版本上，\"+first page\" 方法显著提高了转录准确性，平衡了性能与成本，甚至通过外推格式和 OCR 错误模式改善了样本外文本的处理。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages (including references and appendix), 14 figures, accepted at\n  AAAI-25 Workshop on Document Understanding and Intelligence, non-archival",
      "pdf_url": "http://arxiv.org/pdf/2502.20295v1",
      "published_date": "2025-02-27 17:21:18 UTC",
      "updated_date": "2025-02-27 17:21:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:44:10.852328"
    },
    {
      "arxiv_id": "2502.20284v1",
      "title": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study",
      "title_zh": "翻译失败",
      "authors": [
        "Shenghui Chen",
        "Yunhao Yang",
        "Kayla Boggess",
        "Seongkook Heo",
        "Lu Feng",
        "Ufuk Topcu"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used for planning tasks,\noffering unique capabilities not found in classical planners such as generating\nexplanations and iterative refinement. However, trust--a critical factor in the\nadoption of planning systems--remains underexplored in the context of LLM-based\nplanning tasks. This study bridges this gap by comparing human trust in\nLLM-based planners with classical planners through a user study in a Planning\nDomain Definition Language (PDDL) domain. Combining subjective measures, such\nas trust questionnaires, with objective metrics like evaluation accuracy, our\nfindings reveal that correctness is the primary driver of trust and\nperformance. Explanations provided by the LLM improved evaluation accuracy but\nhad limited impact on trust, while plan refinement showed potential for\nincreasing trust without significantly enhancing evaluation accuracy.",
      "tldr_zh": "这篇论文评估了人类对基于 LLM (Large Language Models) 的规划器的信任，通过一项初步用户研究，将其与经典规划器进行比较。研究在 PDDL (Planning Domain Definition Language) 域中，使用主观指标（如信任问卷）和客观指标（如评估准确性）来收集数据。结果显示，正确性是信任和性能的主要驱动因素；LLM 提供的解释提高了评估准确性，但对信任的影响有限；计划精炼则能增强信任，而未显著提升准确性。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20284v1",
      "published_date": "2025-02-27 17:10:52 UTC",
      "updated_date": "2025-02-27 17:10:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:44:23.421099"
    },
    {
      "arxiv_id": "2502.20277v1",
      "title": "Explainable, Multi-modal Wound Infection Classification from Images Augmented with Generated Captions",
      "title_zh": "翻译失败",
      "authors": [
        "Palawat Busaranuvong",
        "Emmanuel Agu",
        "Reza Saadati Fard",
        "Deepak Kumar",
        "Shefalika Gautam",
        "Bengisu Tulu",
        "Diane Strong"
      ],
      "abstract": "Infections in Diabetic Foot Ulcers (DFUs) can cause severe complications,\nincluding tissue death and limb amputation, highlighting the need for accurate,\ntimely diagnosis. Previous machine learning methods have focused on identifying\ninfections by analyzing wound images alone, without utilizing additional\nmetadata such as medical notes. In this study, we aim to improve infection\ndetection by introducing Synthetic Caption Augmented Retrieval for Wound\nInfection Detection (SCARWID), a novel deep learning framework that leverages\nsynthetic textual descriptions to augment DFU images. SCARWID consists of two\ncomponents: (1) Wound-BLIP, a Vision-Language Model (VLM) fine-tuned on\nGPT-4o-generated descriptions to synthesize consistent captions from images;\nand (2) an Image-Text Fusion module that uses cross-attention to extract\ncross-modal embeddings from an image and its corresponding Wound-BLIP caption.\nInfection status is determined by retrieving the top-k similar items from a\nlabeled support set. To enhance the diversity of training data, we utilized a\nlatent diffusion model to generate additional wound images. As a result,\nSCARWID outperformed state-of-the-art models, achieving average sensitivity,\nspecificity, and accuracy of 0.85, 0.78, and 0.81, respectively, for wound\ninfection classification. Displaying the generated captions alongside the wound\nimages and infection detection results enhances interpretability and trust,\nenabling nurses to align SCARWID outputs with their medical knowledge. This is\nparticularly valuable when wound notes are unavailable or when assisting novice\nnurses who may find it difficult to identify visual attributes of wound\ninfection.",
      "tldr_zh": "该研究针对糖尿病足部溃疡（DFUs）的感染诊断问题，提出 SCARWID 框架，通过合成文本描述增强伤口图像分析，以提升准确性和可解释性。框架的核心组件包括 Wound-BLIP（一个基于 GPT-4o 生成描述的 VLM）用于合成图像标题，以及 Image-Text Fusion 模块通过交叉注意力提取跨模态嵌入，并从标记支持集中检索 top-k 相似项进行分类；同时，利用潜在扩散模型生成额外训练数据以增加多样性。实验结果显示，SCARWID 平均灵敏度、特异性和准确率分别为 0.85、0.78 和 0.81，优于现有模型；此外，显示生成的标题与图像结果相结合，能提高医护人员的信任度和诊断辅助，尤其适合新手护士。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20277v1",
      "published_date": "2025-02-27 17:04:00 UTC",
      "updated_date": "2025-02-27 17:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:44:38.643533"
    },
    {
      "arxiv_id": "2502.20272v2",
      "title": "HVI: A New Color Space for Low-light Image Enhancement",
      "title_zh": "HVI：一种新的颜色空间，用于低光图像增强",
      "authors": [
        "Qingsen Yan",
        "Yixu Feng",
        "Cheng Zhang",
        "Guansong Pang",
        "Kangbiao Shi",
        "Peng Wu",
        "Wei Dong",
        "Jinqiu Sun",
        "Yanning Zhang"
      ],
      "abstract": "Low-Light Image Enhancement (LLIE) is a crucial computer vision task that\naims to restore detailed visual information from corrupted low-light images.\nMany existing LLIE methods are based on standard RGB (sRGB) space, which often\nproduce color bias and brightness artifacts due to inherent high color\nsensitivity in sRGB. While converting the images using Hue, Saturation and\nValue (HSV) color space helps resolve the brightness issue, it introduces\nsignificant red and black noise artifacts. To address this issue, we propose a\nnew color space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined\nby polarized HS maps and learnable intensity. The former enforces small\ndistances for red coordinates to remove the red artifacts, while the latter\ncompresses the low-light regions to remove the black artifacts. To fully\nleverage the chromatic and intensity information, a novel Color and Intensity\nDecoupling Network (CIDNet) is further introduced to learn accurate photometric\nmapping function under different lighting conditions in the HVI space.\nComprehensive results from benchmark and ablation experiments show that the\nproposed HVI color space with CIDNet outperforms the state-of-the-art methods\non 10 datasets. The code is available at https://github.com/Fediory/HVI-CIDNet.",
      "tldr_zh": "本论文针对低光图像增强（LLIE）中的颜色偏差和亮度伪影问题，提出了一种新颜色空间 HVI（Horizontal/Vertical-Intensity），其由极化的 HS 映射和可学习强度组成，以去除红色和黑色噪声伪影。HVI 通过缩小红色坐标距离和压缩低光区域来优化图像处理效果。作者进一步引入 Color and Intensity Decoupling Network (CIDNet)，在 HVI 空间中学习准确的光度映射函数，以适应不同照明条件。实验结果显示，该方法在 10 个数据集上优于现有 state-of-the-art 方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Qingsen Yan, Yixu Feng, and Cheng Zhang contributed equally to this\n  work",
      "pdf_url": "http://arxiv.org/pdf/2502.20272v2",
      "published_date": "2025-02-27 16:59:51 UTC",
      "updated_date": "2025-02-28 11:13:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:44:48.971058"
    },
    {
      "arxiv_id": "2502.20268v2",
      "title": "Large Language Models as Attribution Regularizers for Efficient Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Davor Vukadin",
        "Marin Šilić",
        "Goran Delač"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
      "tldr_zh": "本研究提出了一种新方法，将Large Language Models (LLMs)用作归因正则化器，以提升小型模型的训练效率，特别是针对表格数据学习等场景。方法通过添加attribution-matching regularization term，将LLMs生成的全局任务特征归因整合到小模型训练中，使其训练动态与LLMs的洞见保持一致，从而在few-shot learning场景中显著提高性能。该方法仅需黑箱API访问LLMs，计算开销低，并能有效处理真实数据集中的skewness和bias问题；实验结果显示，它改善了模型的泛化能力和鲁棒性，在多个任务上验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20268v2",
      "published_date": "2025-02-27 16:55:18 UTC",
      "updated_date": "2025-04-17 11:32:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:45:00.901984"
    },
    {
      "arxiv_id": "2502.20258v1",
      "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
      "title_zh": "翻译失败",
      "authors": [
        "Amr Mohamed",
        "Mingmeng Geng",
        "Michalis Vazirgiannis",
        "Guokan Shang"
      ],
      "abstract": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.",
      "tldr_zh": "本研究探讨了大型语言模型（LLM）在迭代生成过程中如何像“broken telephone”效应一样导致信息扭曲，特别是在处理自身输出时。研究通过基于翻译的实验发现，这种扭曲会随着迭代次数积累，受语言选择和链复杂度影响而加剧。尽管信息退化不可避免，但可以通过策略性提示技术进行缓解。这些发现为AI调解的信息传播的长期影响提供了重要启示，并质疑了LLM在迭代工作流中的内容可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20258v1",
      "published_date": "2025-02-27 16:46:23 UTC",
      "updated_date": "2025-02-27 16:46:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:45:11.079684"
    },
    {
      "arxiv_id": "2502.20237v1",
      "title": "Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Gianluca Bencomo",
        "Max Gupta",
        "Ioana Marinescu",
        "R. Thomas McCoy",
        "Thomas L. Griffiths"
      ],
      "abstract": "Artificial neural networks can acquire many aspects of human knowledge from\ndata, making them promising as models of human learning. But what those\nnetworks can learn depends upon their inductive biases -- the factors other\nthan the data that influence the solutions they discover -- and the inductive\nbiases of neural networks remain poorly understood, limiting our ability to\ndraw conclusions about human learning from the performance of these systems.\nCognitive scientists and machine learning researchers often focus on the\narchitecture of a neural network as a source of inductive bias. In this paper\nwe explore the impact of another source of inductive bias -- the initial\nweights of the network -- using meta-learning as a tool for finding initial\nweights that are adapted for specific problems. We evaluate four widely-used\narchitectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430\ndifferent models across three tasks requiring different biases and forms of\ngeneralization. We find that meta-learning can substantially reduce or entirely\neliminate performance differences across architectures and data\nrepresentations, suggesting that these factors may be less important as sources\nof inductive bias than is typically assumed. When differences are present,\narchitectures and data representations that perform well without meta-learning\ntend to meta-train more effectively. Moreover, all architectures generalize\npoorly on problems that are far from their meta-training experience,\nunderscoring the need for stronger inductive biases for robust generalization.",
      "tldr_zh": "本研究探讨了神经网络中架构(architecture)和初始权重(initial weights)作为归纳偏差(inductive bias)来源的影响，使用元学习(meta-learning)来优化初始权重以适应特定问题。研究评估了MLPs、CNNs、LSTMs和Transformers等四种常见架构，在三个需要不同偏差和泛化形式的任务上训练了430个模型，结果显示元学习能显著减少或消除架构和数据表示间的性能差异。关键发现是，传统上被视为重要来源的架构因素可能被夸大，所有架构在与元训练经验相差较远的任务上泛化能力较差，这突显了开发更强归纳偏差以实现鲁棒泛化的必要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 6 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.20237v1",
      "published_date": "2025-02-27 16:22:18 UTC",
      "updated_date": "2025-02-27 16:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:45:25.307214"
    },
    {
      "arxiv_id": "2502.20233v1",
      "title": "Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue",
      "title_zh": "翻译失败",
      "authors": [
        "Daniela Böhm",
        "Georg Gottlob",
        "Matthias Lanzinger",
        "Davide Longo",
        "Cem Okulmus",
        "Reinhard Pichler",
        "Alexander Selzer"
      ],
      "abstract": "Query optimization has played a central role in database research for\ndecades. However, more often than not, the proposed optimization techniques\nlead to a performance improvement in some, but not in all, situations.\nTherefore, we urgently need a methodology for designing a decision procedure\nthat decides for a given query whether the optimization technique should be\napplied or not.\n  In this work, we propose such a methodology with a focus on Yannakakis-style\nquery evaluation as our optimization technique of interest. More specifically,\nwe formulate this decision problem as an algorithm selection problem and we\npresent a Machine Learning based approach for its solution. Empirical results\nwith several benchmarks on a variety of database systems show that our approach\nindeed leads to a statistically significant performance improvement.",
      "tldr_zh": "本论文针对查询优化中的挑战，提出了一种选择性应用Yannakakis' Algorithm的方法，以解决优化技术并非适用于所有情况的问题。作者将决策问题表述为算法选择问题，并采用Machine Learning的方法来判断是否应用该优化技术。实验结果显示，在多个基准测试和数据库系统中，该方法带来了统计显著的性能提升，从而为查询优化提供了更智能的决策框架。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20233v1",
      "published_date": "2025-02-27 16:19:54 UTC",
      "updated_date": "2025-02-27 16:19:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:45:33.679860"
    },
    {
      "arxiv_id": "2502.20231v1",
      "title": "AI Will Always Love You: Studying Implicit Biases in Romantic AI Companions",
      "title_zh": "翻译失败",
      "authors": [
        "Clare Grogan",
        "Jackie Kay",
        "María Pérez-Ortiz"
      ],
      "abstract": "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
      "tldr_zh": "本研究探讨了浪漫 AI 伴侣中隐性偏见（implicit biases）的表现，填补了现有对生成模型显性偏见研究的空白，特别是性别刻板印象和用户关系期望的不足。研究设计了三个实验，针对性别指定的 AI 伴侣和用户，评估不同规模的 Large Language Models (LLMs) 在隐性联想、情感响应和 sycophancy（谄媚行为）等维度的偏见，通过新颖指标定量分析赋予人格的模型响应。结果表明，为 LLMs 赋予性别和关系人格会显著改变其响应，并在某些情况下以偏见和刻板印象的方式表现，从而强调了需要进一步优化 AI 伴侣的设计以减少潜在风险。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20231v1",
      "published_date": "2025-02-27 16:16:37 UTC",
      "updated_date": "2025-02-27 16:16:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:45:47.045120"
    },
    {
      "arxiv_id": "2502.20224v2",
      "title": "RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Yang",
        "Yiran Zhu",
        "Jiayu Shen",
        "Yuhan Tang",
        "Chengchang Pan",
        "Hui He",
        "Yan Su",
        "Honggang Qi"
      ],
      "abstract": "Diabetic Macular Edema (DME), a prevalent complication among diabetic\npatients, constitutes a major cause of visual impairment and blindness.\nAlthough deep learning has achieved remarkable progress in medical image\nanalysis, traditional DME diagnosis still relies on extensive annotated data\nand subjective ophthalmologist assessments, limiting practical applications. To\naddress this, we present RURANET++, an unsupervised learning-based automated\nDME diagnostic system. This framework incorporates an optimized U-Net\narchitecture with embedded Spatial and Channel Squeeze & Excitation (SCSE)\nattention mechanisms to enhance lesion feature extraction. During feature\nprocessing, a pre-trained GoogLeNet model extracts deep features from retinal\nimages, followed by PCA-based dimensionality reduction to 50 dimensions for\ncomputational efficiency. Notably, we introduce a novel clustering algorithm\nemploying multi-projection heads to explicitly control cluster diversity while\ndynamically adjusting similarity thresholds, thereby optimizing intra-class\nconsistency and inter-class discrimination. Experimental results demonstrate\nsuperior performance across multiple metrics, achieving maximum accuracy\n(0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), with\nexceptional clustering quality. This work provides an efficient unsupervised\nsolution for DME diagnosis with significant clinical implications.",
      "tldr_zh": "本研究提出 RURANET++，一种基于无监督学习的糖尿病黄斑水肿 (DME) 诊断方法，旨在克服传统诊断依赖大量标注数据和主观评估的局限。框架采用优化后的 U-Net 架构，嵌入 SCSE (Spatial and Channel Squeeze & Excitation) 注意力机制，以增强视网膜图像的病变特征提取，并结合预训练 GoogLeNet 模型提取深度特征，随后通过 PCA 降维到 50 维。创新性地引入动态多投影头 (dynamic multi-projection head) 聚类算法，动态调整相似性阈值以优化类内一致性和类间区分。实验结果显示，该方法在多个指标上表现出色，最高准确率 (0.8411)、精确率 (0.8593)、召回率 (0.8411) 和 F1 分数 (0.8390)，为临床 DME 诊断提供高效的无监督解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 2 figures, 5 tables, submitted to The 28th International\n  Conference on Medical Image Computing and Computer Assisted Intervention\n  (MICCAI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2502.20224v2",
      "published_date": "2025-02-27 16:06:57 UTC",
      "updated_date": "2025-03-07 08:17:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:45:59.841459"
    },
    {
      "arxiv_id": "2502.20223v1",
      "title": "Deep Convolutional Neural Networks for Palm Fruit Maturity Classification",
      "title_zh": "深度卷积神经网络用于棕榈果实成熟度分类",
      "authors": [
        "Mingqiang Han",
        "Chunlin Yi"
      ],
      "abstract": "To maximize palm oil yield and quality, it is essential to harvest palm fruit\nat the optimal maturity stage. This project aims to develop an automated\ncomputer vision system capable of accurately classifying palm fruit images into\nfive ripeness levels. We employ deep Convolutional Neural Networks (CNNs) to\nclassify palm fruit images based on their maturity stage. A shallow CNN serves\nas the baseline model, while transfer learning and fine-tuning are applied to\npre-trained ResNet50 and InceptionV3 architectures. The study utilizes a\npublicly available dataset of over 8,000 images with significant variations,\nwhich is split into 80\\% for training and 20\\% for testing. The proposed deep\nCNN models achieve test accuracies exceeding 85\\% in classifying palm fruit\nmaturity stages. This research highlights the potential of deep learning for\nautomating palm fruit ripeness assessment, which can contribute to optimizing\nharvesting decisions and improving palm oil production efficiency.",
      "tldr_zh": "这篇论文开发了一个基于深层卷积神经网络 (CNNs) 的自动计算机视觉系统，用于将棕榈果实图像分类为五个成熟度级别，以优化收获时机和油质。方法包括使用浅层 CNN 作为基线模型，并通过迁移学习和微调应用于预训练的 ResNet50 和 InceptionV3 架构。实验在超过 8,000 张图像的数据集上进行，训练集占 80%，测试准确率超过 85%。这项研究展示了深层学习在自动化棕榈果实成熟度评估中的潜力，有助于提升棕榈油生产效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20223v1",
      "published_date": "2025-02-27 16:06:30 UTC",
      "updated_date": "2025-02-27 16:06:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:46:11.311459"
    },
    {
      "arxiv_id": "2502.20209v2",
      "title": "DIPSER: A Dataset for In-Person Student Engagement Recognition in the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Luis Marquez-Carpintero",
        "Sergio Suescun-Ferrandiz",
        "Carolina Lorenzo Álvarez",
        "Jorge Fernandez-Herrero",
        "Diego Viejo",
        "Rosabel Roig-Vila",
        "Miguel Cazorla"
      ],
      "abstract": "In this paper, a novel dataset is introduced, designed to assess student\nattention within in-person classroom settings. This dataset encompasses RGB\ncamera data, featuring multiple cameras per student to capture both posture and\nfacial expressions, in addition to smartwatch sensor data for each individual.\nThis dataset allows machine learning algorithms to be trained to predict\nattention and correlate it with emotion. A comprehensive suite of attention and\nemotion labels for each student is provided, generated through self-reporting\nas well as evaluations by four different experts. Our dataset uniquely combines\nfacial and environmental camera data, smartwatch metrics, and includes\nunderrepresented ethnicities in similar datasets, all within in-the-wild,\nin-person settings, making it the most comprehensive dataset of its kind\ncurrently available.\n  The dataset presented offers an extensive and diverse collection of data\npertaining to student interactions across different educational contexts,\naugmented with additional metadata from other tools. This initiative addresses\nexisting deficiencies by offering a valuable resource for the analysis of\nstudent attention and emotion in face-to-face lessons.",
      "tldr_zh": "本研究引入了DIPSER数据集，用于评估真实课堂环境中学生的参与度。该数据集包括RGB camera数据（多个相机捕捉学生的姿势和面部表情）、smartwatch sensor data，以及通过自我报告和四位专家评估生成的注意力与情感标签，独特地结合了面部、环境数据和多样化族裔（包括underrepresented ethnicities）。DIPSER填补了现有数据集的空白，提供全面资源，支持机器学习算法训练以预测学生注意力及其与情感的相关性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20209v2",
      "published_date": "2025-02-27 15:50:21 UTC",
      "updated_date": "2025-03-02 13:36:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:46:28.506201"
    },
    {
      "arxiv_id": "2502.20190v1",
      "title": "Highly Parallelized Reinforcement Learning Training with Relaxed Assignment Dependencies",
      "title_zh": "翻译失败",
      "authors": [
        "Zhouyu He",
        "Peng Qiao",
        "Rongchun Li",
        "Yong Dou",
        "Yusong Tan"
      ],
      "abstract": "As the demands for superior agents grow, the training complexity of Deep\nReinforcement Learning (DRL) becomes higher. Thus, accelerating training of DRL\nhas become a major research focus. Dividing the DRL training process into\nsubtasks and using parallel computation can effectively reduce training costs.\nHowever, current DRL training systems lack sufficient parallelization due to\ndata assignment between subtask components. This assignment issue has been\nignored, but addressing it can further boost training efficiency. Therefore, we\npropose a high-throughput distributed RL training system called TianJi. It\nrelaxes assignment dependencies between subtask components and enables\nevent-driven asynchronous communication. Meanwhile, TianJi maintains clear\nboundaries between subtask components. To address convergence uncertainty from\nrelaxed assignment dependencies, TianJi proposes a distributed strategy based\non the balance of sample production and consumption. The strategy controls the\nstaleness of samples to correct their quality, ensuring convergence. We\nconducted extensive experiments. TianJi achieves a convergence time\nacceleration ratio of up to 4.37 compared to related comparison systems. When\nscaled to eight computational nodes, TianJi shows a convergence time speedup of\n1.6 and a throughput speedup of 7.13 relative to XingTian, demonstrating its\ncapability to accelerate training and scalability. In data transmission\nefficiency experiments, TianJi significantly outperforms other systems,\napproaching hardware limits. TianJi also shows effectiveness in on-policy\nalgorithms, achieving convergence time acceleration ratios of 4.36 and 2.95\ncompared to RLlib and XingTian. TianJi is accessible at\nhttps://github.com/HiPRL/TianJi.git.",
      "tldr_zh": "该研究针对Deep Reinforcement Learning (DRL)训练的复杂性，提出了一种高吞吐分布式训练系统TianJi，通过放松子任务组件之间的分配依赖性和采用事件驱动异步通信，提高了训练效率。TianJi保持子任务组件的清晰边界，并引入基于样本生产和消费平衡的分布式策略，以控制样本陈旧度并确保算法收敛。实验结果显示，TianJi相较于相关系统，收敛时间加速高达4.37倍；在八个计算节点上，与XingTian相比，收敛时间加速1.6倍、吞吐量加速7.13倍，并在on-policy算法中表现出色，显著提升了数据传输效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20190v1",
      "published_date": "2025-02-27 15:23:43 UTC",
      "updated_date": "2025-02-27 15:23:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:46:35.352059"
    },
    {
      "arxiv_id": "2502.20175v1",
      "title": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Kaustubh Vyas",
        "Damien Graux",
        "Sébastien Montella",
        "Pavlos Vougiouklis",
        "Ruofei Lai",
        "Keshuang Li",
        "Yang Ren",
        "Jeff Z. Pan"
      ],
      "abstract": "In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms.",
      "tldr_zh": "本研究对现成的大型语言模型（LLMs）在处理 Planning Domain Definition Language (PDDL) 的能力进行了广泛评估，聚焦于代码生成和链式思维推理在自动正式规划任务中的应用。研究团队测试了20个模型，涵盖7个主要LLMs系列（包括商业和开源模型），评估了它们的零样本能力，包括解析、生成和推理PDDL。结果显示，一些模型在处理PDDL时表现出色，但面对复杂场景时存在局限性，如缺乏细致规划知识。这些发现突出了LLMs在AI驱动规划领域的潜力，并为未来研究提供指导方向。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2502.20175v1",
      "published_date": "2025-02-27 15:13:07 UTC",
      "updated_date": "2025-02-27 15:13:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:46:47.245128"
    },
    {
      "arxiv_id": "2502.20168v1",
      "title": "Accelerating Model-Based Reinforcement Learning with State-Space World Models",
      "title_zh": "利用状态空间世界模型加速基于模型的强化学习",
      "authors": [
        "Maria Krinner",
        "Elie Aljalbout",
        "Angel Romero",
        "Davide Scaramuzza"
      ],
      "abstract": "Reinforcement learning (RL) is a powerful approach for robot learning.\nHowever, model-free RL (MFRL) requires a large number of environment\ninteractions to learn successful control policies. This is due to the noisy RL\ntraining updates and the complexity of robotic systems, which typically involve\nhighly non-linear dynamics and noisy sensor signals. In contrast, model-based\nRL (MBRL) not only trains a policy but simultaneously learns a world model that\ncaptures the environment's dynamics and rewards. The world model can either be\nused for planning, for data collection, or to provide first-order policy\ngradients for training. Leveraging a world model significantly improves sample\nefficiency compared to model-free RL. However, training a world model alongside\nthe policy increases the computational complexity, leading to longer training\ntimes that are often intractable for complex real-world scenarios. In this\nwork, we propose a new method for accelerating model-based RL using state-space\nworld models. Our approach leverages state-space models (SSMs) to parallelize\nthe training of the dynamics model, which is typically the main computational\nbottleneck. Additionally, we propose an architecture that provides privileged\ninformation to the world model during training, which is particularly relevant\nfor partially observable environments. We evaluate our method in several\nreal-world agile quadrotor flight tasks, involving complex dynamics, for both\nfully and partially observable environments. We demonstrate a significant\nspeedup, reducing the world model training time by up to 10 times, and the\noverall MBRL training time by up to 4 times. This benefit comes without\ncompromising performance, as our method achieves similar sample efficiency and\ntask rewards to state-of-the-art MBRL methods.",
      "tldr_zh": "本文提出了一种使用状态空间模型 (SSMs) 的方法来加速模型相关强化学习 (MBRL)，以解决传统 MBRL 在计算复杂性和训练时间上的瓶颈问题。该方法通过并行化动态模型训练并为部分可观察环境提供特权信息，提高了样本效率和整体训练速度。在实际无人机飞行任务的评估中，该方法将世界模型训练时间减少高达 10 倍，MBRL 整体训练时间减少高达 4 倍，同时在任务奖励和性能上与最先进方法相当。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "stat.ML",
        "I.2.9; I.2.10; I.2.6"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20168v1",
      "published_date": "2025-02-27 15:05:25 UTC",
      "updated_date": "2025-02-27 15:05:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:46:59.804416"
    },
    {
      "arxiv_id": "2502.20156v1",
      "title": "Adaptive H&E-IHC information fusion staining framework based on feature extra",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Jia",
        "Xingda Yu",
        "Zhengyang Ji",
        "Songning Lai",
        "Yutao Yue"
      ],
      "abstract": "Immunohistochemistry (IHC) staining plays a significant role in the\nevaluation of diseases such as breast cancer. The H&E-to-IHC transformation\nbased on generative models provides a simple and cost-effective method for\nobtaining IHC images. Although previous models can perform digital coloring\nwell, they still suffer from (i) coloring only through the pixel features that\nare not prominent in HE, which is easy to cause information loss in the\ncoloring process; (ii) The lack of pixel-perfect H&E-IHC groundtruth pairs\nposes a challenge to the classical L1 loss.To address the above challenges, we\npropose an adaptive information enhanced coloring framework based on feature\nextractors. We first propose the VMFE module to effectively extract the color\ninformation features using multi-scale feature extraction and wavelet transform\nconvolution, while combining the shared decoder for feature fusion. The\nhigh-performance dual feature extractor of H&E-IHC is trained by contrastive\nlearning, which can effectively perform feature alignment of HE-IHC in high\nlatitude space. At the same time, the trained feature encoder is used to\nenhance the features and adaptively adjust the loss in the HE section staining\nprocess to solve the problems related to unclear and asymmetric information. We\nhave tested on different datasets and achieved excellent performance.Our code\nis available at https://github.com/babyinsunshine/CEFF",
      "tldr_zh": "该论文提出了一种基于特征提取的自适应 H&E-IHC 信息融合染色框架，旨在解决现有生成模型在 H&E 到 IHC 转换过程中存在的像素特征信息丢失和缺乏精确地面实况配对数据的问题。该框架引入 VMFE 模块，利用多尺度特征提取和小波变换卷积提取颜色信息，并通过共享解码器进行特征融合，同时采用对比学习训练高性能双特征提取器以实现 H&E-IHC 的高纬度特征对齐。实验结果显示，该方法在不同数据集上表现出色，能够有效增强特征并自适应调整损失，提高了染色精度和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20156v1",
      "published_date": "2025-02-27 14:55:34 UTC",
      "updated_date": "2025-02-27 14:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:47:13.085218"
    },
    {
      "arxiv_id": "2502.20130v1",
      "title": "QPM: Discrete Optimization for Globally Interpretable Image Classification",
      "title_zh": "QPM：用于全局可解释图像分类的离散优化",
      "authors": [
        "Thomas Norrenbrock",
        "Timo Kaiser",
        "Sovan Biswas",
        "Ramesh Manuvinakurike",
        "Bodo Rosenhahn"
      ],
      "abstract": "Understanding the classifications of deep neural networks, e.g. used in\nsafety-critical situations, is becoming increasingly important. While recent\nmodels can locally explain a single decision, to provide a faithful global\nexplanation about an accurate model's general behavior is a more challenging\nopen task. Towards that goal, we introduce the Quadratic Programming Enhanced\nModel (QPM), which learns globally interpretable class representations. QPM\nrepresents every class with a binary assignment of very few, typically 5,\nfeatures, that are also assigned to other classes, ensuring easily comparable\ncontrastive class representations. This compact binary assignment is found\nusing discrete optimization based on predefined similarity measures and\ninterpretability constraints. The resulting optimal assignment is used to\nfine-tune the diverse features, so that each of them becomes the shared general\nconcept between the assigned classes. Extensive evaluations show that QPM\ndelivers unprecedented global interpretability across small and large-scale\ndatasets while setting the state of the art for the accuracy of interpretable\nmodels.",
      "tldr_zh": "本研究针对深度神经网络在安全关键场景中的图像分类可解释性问题，引入了Quadratic Programming Enhanced Model (QPM)，该模型通过离散优化(discrete optimization)学习全局可解释的类表示，每个类由少量（如5个）共享特征的二进制分配(binary assignment)来表示，确保对比性和可解释性约束。QPM基于预定义的相似性措施优化这些特征分配，并通过微调使其成为分配类之间共享的通用概念。在小规模和大规模数据集上的广泛评估显示，QPM实现了前所未有的全局可解释性，并将可解释模型的准确率提升至state of the art水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20130v1",
      "published_date": "2025-02-27 14:25:36 UTC",
      "updated_date": "2025-02-27 14:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:47:26.013909"
    },
    {
      "arxiv_id": "2502.20127v1",
      "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Zexiong Ma",
        "Chao Peng",
        "Pengfei Gao",
        "Xiangxin Meng",
        "Yanzhen Zou",
        "Bing Xie"
      ],
      "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.",
      "tldr_zh": "该研究提出 SoRFT（Subtask-oriented Reinforced Fine-Tuning），一种新型训练方法，旨在提升大型语言模型（LLMs）在问题解决（issue resolving）方面的能力，以应对现有框架依赖商业模型带来的高成本和隐私问题。SoRFT 将问题解决分解为结构化子任务，包括文件定位（file localization）、函数定位（function localization）、行定位（line localization）和代码编辑生成（code edit generation），并采用两个阶段的训练：（1）基于 ground-truth 的 rejection-sampled supervised fine-tuning，使用 Chain of Thought (CoT) 数据进行过滤微调；（2）rule-based reinforcement learning，利用 PPO 算法和 ground-truth 奖励进行优化。实验结果显示，SoRFT 训练的模型（如 SoRFT-Qwen-7B）在 SWE-Bench Verified 和 SWE-Bench Lite 上实现了开源模型的 SOTA 性能，例如在 SWE-Bench Verified 上解决了 21.4% 的问题，并显著提高了模型的泛化能力和成本效率。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20127v1",
      "published_date": "2025-02-27 14:19:45 UTC",
      "updated_date": "2025-02-27 14:19:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:47:40.576523"
    },
    {
      "arxiv_id": "2503.00070v1",
      "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
      "title_zh": "翻译失败",
      "authors": [
        "Tue Nhi Tran"
      ],
      "abstract": "Throughout the history from pre-industry 4.0 to post-industry 4.0,\ncybersecurity at banks has undergone significant changes. Pre-industry 4.0\ncyber security at banks relied on individual security methods that were highly\nmanual and had low accuracy. When moving to post-industry 4.0, cybersecurity at\nbanks had a major turning point with security methods that combined different\ntechnologies such as Artificial Intelligence (AI), Blockchain, IoT, automating\nnecessary processes and significantly increasing the defence layer for banks.\nHowever, along with the development of new technologies, the current challenge\nof cybersecurity at banks lies in scalability, high costs and resources in both\nmoney and time for R&D of defence methods along with the threat of high-tech\ncybercriminals growing and expanding. This report goes from introducing the\nimportance of cybersecurity at banks, analyzing their management, operational\nand business objectives, evaluating pre-industry 4.0 technologies used for\ncybersecurity at banks to assessing post-industry 4.0 technologies focusing on\nArtificial Intelligence and Blockchain, discussing current policies and\npractices and ending with discussing key advantages and challenges for 4.0\ntechnologies and recommendations for further developing cybersecurity at banks.",
      "tldr_zh": "这篇系统综述探讨了银行网络安全的演变，从工业4.0前依赖手动、低精度个体安全方法，到工业4.0后整合Artificial Intelligence (AI)、Blockchain和IoT等技术，实现自动化并显著提升防御层。报告分析了管理、运营和业务目标，评估了工业4.0前后的关键技术，并讨论了当前挑战，如可扩展性、高研发成本以及高科技网络犯罪威胁。最终，它总结了这些技术的优势、政策与实践，并提出进一步发展的推荐，以加强银行网络安全。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00070v1",
      "published_date": "2025-02-27 14:17:06 UTC",
      "updated_date": "2025-02-27 14:17:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:47:49.603801"
    },
    {
      "arxiv_id": "2502.20124v1",
      "title": "Exploring Open-world Continual Learning with Knowns-Unknowns Knowledge Transfer",
      "title_zh": "探索开放世界持续学习中的已知-未知知识转移",
      "authors": [
        "Yujie Li",
        "Guannan Lai",
        "Xin Yang",
        "Yonghao Li",
        "Marcello Bonsangue",
        "Tianrui Li"
      ],
      "abstract": "Open-World Continual Learning (OWCL) is a challenging paradigm where models\nmust incrementally learn new knowledge without forgetting while operating under\nan open-world assumption. This requires handling incomplete training data and\nrecognizing unknown samples during inference. However, existing OWCL methods\noften treat open detection and continual learning as separate tasks, limiting\ntheir ability to integrate open-set detection and incremental classification in\nOWCL. Moreover, current approaches primarily focus on transferring knowledge\nfrom known samples, neglecting the insights derived from unknown/open samples.\nTo address these limitations, we formalize four distinct OWCL scenarios and\nconduct comprehensive empirical experiments to explore potential challenges in\nOWCL. Our findings reveal a significant interplay between the open detection of\nunknowns and incremental classification of knowns, challenging a widely held\nassumption that unknown detection and known classification are orthogonal\nprocesses. Building on our insights, we propose \\textbf{HoliTrans} (Holistic\nKnowns-Unknowns Knowledge Transfer), a novel OWCL framework that integrates\nnonlinear random projection (NRP) to create a more linearly separable embedding\nspace and distribution-aware prototypes (DAPs) to construct an adaptive\nknowledge space. Particularly, our HoliTrans effectively supports knowledge\ntransfer for both known and unknown samples while dynamically updating\nrepresentations of open samples during OWCL. Extensive experiments across\nvarious OWCL scenarios demonstrate that HoliTrans outperforms 22 competitive\nbaselines, bridging the gap between OWCL theory and practice and providing a\nrobust, scalable framework for advancing open-world learning paradigms.",
      "tldr_zh": "本论文探讨了 Open-world Continual Learning (OWCL)，一种模型需在不忘记旧知识的情况下处理不完整数据并识别未知样本的挑战性范式。作者形式化了四个 OWCL 场景，并通过实验发现未知检测与已知分类并非独立过程，而是存在显著相互作用。提出 HoliTrans 框架，利用非线性随机投影 (NRP) 创建线性可分嵌入空间，以及分布感知原型 (DAPs) 构建自适应知识空间，实现已知和未知样本的整体知识转移。在多种 OWCL 场景中，HoliTrans 优于 22 个竞争基线，显著提升了开放世界学习的鲁棒性和可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20124v1",
      "published_date": "2025-02-27 14:16:01 UTC",
      "updated_date": "2025-02-27 14:16:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:48:03.247448"
    },
    {
      "arxiv_id": "2502.20122v2",
      "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
      "title_zh": "自训练引发大型语言模型的简洁推理",
      "authors": [
        "Tergel Munkhbat",
        "Namgyu Ho",
        "Seo Hyun Kim",
        "Yongjin Yang",
        "Yujin Kim",
        "Se-Young Yun"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning",
      "tldr_zh": "本文研究发现，大型语言模型 (LLMs) 在 Chain-of-thought (CoT) 推理中存在冗余 tokens，导致额外计算成本，但 LLMs 具有潜在的更简洁推理能力。作者提出了一种自训练方法，通过 best-of-N sampling 和 few-shot conditioning 生成简洁推理路径，并在任务特定设置下进行微调。该方法在 GSM8K 和 MATH 数据集上，使五种模型家族的输出 tokens 平均减少 30%，同时保持平均准确率。通过利用 LLMs 的随机性和 in-context learning 能力，该方法在广泛后训练的模型上表现出稳健性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 10 figures, 18 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.20122v2",
      "published_date": "2025-02-27 14:14:50 UTC",
      "updated_date": "2025-02-28 08:12:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:48:14.770124"
    },
    {
      "arxiv_id": "2502.20113v1",
      "title": "Forward-Cooperation-Backward (FCB) learning in a Multi-Encoding Uni-Decoding neural network architecture",
      "title_zh": "Forward-Cooperation-Backward (FCB) 学习在多编码单解码神经网络架构中的应用",
      "authors": [
        "Prasun Dutta",
        "Koustab Ghosh",
        "Rajat K. De"
      ],
      "abstract": "The most popular technique to train a neural network is backpropagation.\nRecently, the Forward-Forward technique has also been introduced for certain\nlearning tasks. However, in real life, human learning does not follow any of\nthese techniques exclusively. The way a human learns is basically a combination\nof forward learning, backward propagation and cooperation. Humans start\nlearning a new concept by themselves and try to refine their understanding\nhierarchically during which they might come across several doubts. The most\ncommon approach to doubt solving is a discussion with peers, which can be\ncalled cooperation. Cooperation/discussion/knowledge sharing among peers is one\nof the most important steps of learning that humans follow. However, there\nmight still be a few doubts even after the discussion. Then the difference\nbetween the understanding of the concept and the original literature is\nidentified and minimized over several revisions. Inspired by this, the paper\nintroduces Forward-Cooperation-Backward (FCB) learning in a deep neural network\nframework mimicking the human nature of learning a new concept. A novel deep\nneural network architecture, called Multi Encoding Uni Decoding neural network\nmodel, has been designed which learns using the notion of FCB. A special\nlateral synaptic connection has also been introduced to realize cooperation.\nThe models have been justified in terms of their performance in dimension\nreduction on four popular datasets. The ability to preserve the granular\nproperties of data in low-rank embedding has been tested to justify the quality\nof dimension reduction. For downstream analyses, classification has also been\nperformed. An experimental study on convergence analysis has been performed to\nestablish the efficacy of the FCB learning strategy.",
      "tldr_zh": "该论文提出了一种受人类学习启发的前向-合作-后向（Forward-Cooperation-Backward, FCB）学习策略，用于训练神经网络，结合前向学习、合作讨论和后向传播（backpropagation）来模拟人类处理新概念的过程。论文设计了Multi Encoding Uni Decoding神经网络架构，并引入了特殊的横向突触连接来实现合作机制，以提升模型的学习效率。实验结果显示，该方法在四个流行数据集上的维度减少任务中表现出色，能够有效保留数据粒度属性，并在分类和收敛分析中证明了FCB策略的效能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20113v1",
      "published_date": "2025-02-27 14:04:16 UTC",
      "updated_date": "2025-02-27 14:04:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:48:23.924407"
    },
    {
      "arxiv_id": "2502.20429v2",
      "title": "Will AI replace Software Engineers? Do not hold your breath",
      "title_zh": "翻译失败",
      "authors": [
        "Abhik Roychoudhury",
        "Andreas Zeller"
      ],
      "abstract": "Artificial Intelligence (AI) technology such as Large Language Models (LLMs)\nhave become extremely popular in creating code. This has led to the conjecture\nthat future software jobs will be exclusively conducted by LLMs, and the\nsoftware industry will cease to exist. But software engineering is much more\nthan producing code -- notably, \\emph{maintaining} large software and keeping\nit reliable is a major part of software engineering, which LLMs are not yet\ncapable of.",
      "tldr_zh": "该论文探讨了人工智能（AI），尤其是大型语言模型（Large Language Models, LLMs），是否会取代软件工程师的问题。作者认为，尽管 LLMs 在代码生成方面表现出色，但软件工程的核心任务，如维护大型软件系统和确保其可靠性，目前仍超出 AI 的能力范围。最终，论文强调软件行业不会很快消亡，因为这些复杂的人类技能是 AI 无法完全取代的。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "3 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.20429v2",
      "published_date": "2025-02-27 14:04:02 UTC",
      "updated_date": "2025-03-03 07:46:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:48:35.473005"
    },
    {
      "arxiv_id": "2502.20111v1",
      "title": "MITracker: Multi-View Integration for Visual Object Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Mengjie Xu",
        "Yitao Zhu",
        "Haotian Jiang",
        "Jiaming Li",
        "Zhenrong Shen",
        "Sheng Wang",
        "Haolin Huang",
        "Xinyu Wang",
        "Qing Yang",
        "Han Zhang",
        "Qian Wang"
      ],
      "abstract": "Multi-view object tracking (MVOT) offers promising solutions to challenges\nsuch as occlusion and target loss, which are common in traditional single-view\ntracking. However, progress has been limited by the lack of comprehensive\nmulti-view datasets and effective cross-view integration methods. To overcome\nthese limitations, we compiled a Multi-View object Tracking (MVTrack) dataset\nof 234K high-quality annotated frames featuring 27 distinct objects across\nvarious scenes. In conjunction with this dataset, we introduce a novel MVOT\nmethod, Multi-View Integration Tracker (MITracker), to efficiently integrate\nmulti-view object features and provide stable tracking outcomes. MITracker can\ntrack any object in video frames of arbitrary length from arbitrary viewpoints.\nThe key advancements of our method over traditional single-view approaches come\nfrom two aspects: (1) MITracker transforms 2D image features into a 3D feature\nvolume and compresses it into a bird's eye view (BEV) plane, facilitating\ninter-view information fusion; (2) we propose an attention mechanism that\nleverages geometric information from fused 3D feature volume to refine the\ntracking results at each view. MITracker outperforms existing methods on the\nMVTrack and GMTD datasets, achieving state-of-the-art performance. The code and\nthe new dataset will be available at\nhttps://mii-laboratory.github.io/MITracker/.",
      "tldr_zh": "本论文针对多视图物体跟踪(MVOT)中的遮挡和目标丢失问题，构建了一个名为MVTrack的庞大数据集，包含234K高质帧和27个对象的多样场景，以填补现有数据集的不足。作者提出了一种新型方法MITracker，通过将2D图像特征转换为3D特征体并压缩到鸟瞰视图(BEV)平面，实现多视图信息融合；同时，利用注意力机制基于融合后的几何信息细化每个视图的跟踪结果。实验显示，MITracker在MVTrack和GMTD数据集上超越现有方法，达到最先进性能，代码和数据集可从指定链接获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20111v1",
      "published_date": "2025-02-27 14:03:28 UTC",
      "updated_date": "2025-02-27 14:03:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:48:49.721970"
    },
    {
      "arxiv_id": "2502.20099v2",
      "title": "Sanity Checking Causal Representation Learning on a Simple Real-World System",
      "title_zh": "翻译失败",
      "authors": [
        "Juan L. Gamella",
        "Simon Bing",
        "Jakob Runge"
      ],
      "abstract": "We evaluate methods for causal representation learning (CRL) on a simple,\nreal-world system where these methods are expected to work. The system consists\nof a controlled optical experiment specifically built for this purpose, which\nsatisfies the core assumptions of CRL and where the underlying causal factors\n(the inputs to the experiment) are known, providing a ground truth. We select\nmethods representative of different approaches to CRL and find that they all\nfail to recover the underlying causal factors. To understand the failure modes\nof the evaluated algorithms, we perform an ablation on the data by substituting\nthe real data-generating process with a simpler synthetic equivalent. The\nresults reveal a reproducibility problem, as most methods already fail on this\nsynthetic ablation despite its simple data-generating process. Additionally, we\nobserve that common assumptions on the mixing function are crucial for the\nperformance of some of the methods but do not hold in the real data. Our\nefforts highlight the contrast between the theoretical promise of the state of\nthe art and the challenges in its application. We hope the benchmark serves as\na simple, real-world sanity check to further develop and validate methodology,\nbridging the gap towards CRL methods that work in practice. We make all code\nand datasets publicly available at github.com/simonbing/CRLSanityCheck",
      "tldr_zh": "本研究评估了因果表示学习（Causal Representation Learning, CRL）方法在简单真实世界系统上的表现，使用一个受控光学实验作为基准，该实验满足 CRL 的核心假设并提供已知底层因果因素（ground truth）。结果显示，所选代表性 CRL 方法均无法准确恢复这些因果因素，且通过数据消融实验（替换为简单合成过程）进一步揭示了再现性问题和对混合函数假设的依赖性，这些假设在真实数据中不成立。该工作突显了 CRL 理论承诺与实际应用之间的差距，并公开代码和数据集作为基准，促进未来方法的开发和验证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20099v2",
      "published_date": "2025-02-27 13:56:54 UTC",
      "updated_date": "2025-04-28 10:30:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:49:02.179534"
    },
    {
      "arxiv_id": "2502.20092v3",
      "title": "WalnutData: A UAV Remote Sensing Dataset of Green Walnuts and Model Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Mingjie Wu",
        "Chenggui Yang",
        "Huihua Wang",
        "Chen Xue",
        "Yibo Wang",
        "Haoyu Wang",
        "Yansong Wang",
        "Can Peng",
        "Yuqi Han",
        "Ruoyu Li",
        "Lijun Yun",
        "Zaiqing Chen",
        "Yuelong Xia"
      ],
      "abstract": "The UAV technology is gradually maturing and can provide extremely powerful\nsupport for smart agriculture and precise monitoring. Currently, there is no\ndataset related to green walnuts in the field of agricultural computer vision.\nThus, in order to promote the algorithm design in the field of agricultural\ncomputer vision, we used UAV to collect remote-sensing data from 8 walnut\nsample plots. Considering that green walnuts are subject to various lighting\nconditions and occlusion, we constructed a large-scale dataset with a\nhigher-granularity of target features - WalnutData. This dataset contains a\ntotal of 30,240 images and 706,208 instances, and there are 4 target\ncategories: being illuminated by frontal light and unoccluded (A1), being\nbacklit and unoccluded (A2), being illuminated by frontal light and occluded\n(B1), and being backlit and occluded (B2). Subsequently, we evaluated many\nmainstream algorithms on WalnutData and used these evaluation results as the\nbaseline standard. The dataset and all evaluation results can be obtained at\nhttps://github.com/1wuming/WalnutData.",
      "tldr_zh": "本文构建了WalnutData数据集，这是首个针对绿核桃的UAV远程 sensing数据集，旨在推进农业计算机视觉算法设计，通过从8个样本地块收集数据来应对各种光照条件和遮挡问题。数据集包含30,240张图像和706,208个实例，分为4个目标类别：正面光照无遮挡 (A1)、背光无遮挡 (A2)、正面光照有遮挡 (B1) 和背光有遮挡 (B2)。作者评估了多种主流算法，提供基准标准，并将数据集及其结果公开在GitHub上。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20092v3",
      "published_date": "2025-02-27 13:51:56 UTC",
      "updated_date": "2025-03-04 14:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:49:14.321032"
    },
    {
      "arxiv_id": "2502.20089v1",
      "title": "RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Adib Karimi",
        "Mohammad Mehdi Ebadzadeh"
      ],
      "abstract": "We introduce a novel Inverse Reinforcement Learning (IRL) approach that\novercomes limitations of fixed reward assignments and constrained flexibility\nin implicit reward regularization. By extending the Maximum Entropy IRL\nframework with a squared temporal-difference (TD) regularizer and adaptive\ntargets, dynamically adjusted during training, our method indirectly optimizes\na reward function while incorporating reinforcement learning principles.\nFurthermore, we integrate distributional RL to capture richer return\ninformation. Our approach achieves state-of-the-art performance on challenging\nMuJoCo tasks, demonstrating expert-level results on the Humanoid task with only\n3 demonstrations. Extensive experiments and ablation studies validate the\neffectiveness of our method, providing insights into adaptive targets and\nreward dynamics in imitation learning.",
      "tldr_zh": "本研究提出了一种名为 RIZE 的新 Inverse Reinforcement Learning (IRL) 方法，通过扩展 Maximum Entropy IRL 框架并添加 squared temporal-difference (TD) regularizer 和动态调整的 adaptive targets，来间接优化奖励函数，同时整合 distributional RL 以捕捉更丰富的回报信息。相比传统方法，该方法克服了固定奖励分配和灵活性不足的局限性，并在 MuJoCo 任务中实现了 state-of-the-art 性能，例如在 Humanoid 任务中仅需 3 个演示就达到专家水平。广泛的实验和消融研究验证了 adaptive targets 和奖励动态的有效性，为模仿学习提供了重要见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20089v1",
      "published_date": "2025-02-27 13:47:29 UTC",
      "updated_date": "2025-02-27 13:47:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:49:25.941262"
    },
    {
      "arxiv_id": "2502.20084v1",
      "title": "Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights",
      "title_zh": "翻译失败",
      "authors": [
        "Haicheng Liao",
        "Chengyue Wang",
        "Kaiqun Zhu",
        "Yilong Ren",
        "Bolin Gao",
        "Shengbo Eben Li",
        "Chengzhong Xu",
        "Zhenning Li"
      ],
      "abstract": "In mixed autonomous driving environments, accurately predicting the future\ntrajectories of surrounding vehicles is crucial for the safe operation of\nautonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory is\ndetermined by the decision-making process of human drivers. However, existing\nmodels primarily focus on the inherent statistical patterns in the data, often\nneglecting the critical aspect of understanding the decision-making processes\nof human drivers. This oversight results in models that fail to capture the\ntrue intentions of human drivers, leading to suboptimal performance in\nlong-term trajectory prediction. To address this limitation, we introduce a\nCognitive-Informed Transformer (CITF) that incorporates a cognitive concept,\nPerceived Safety, to interpret drivers' decision-making mechanisms. Perceived\nSafety encapsulates the varying risk tolerances across drivers with different\ndriving behaviors. Specifically, we develop a Perceived Safety-aware Module\nthat includes a Quantitative Safety Assessment for measuring the subject risk\nlevels within scenarios, and Driver Behavior Profiling for characterizing\ndriver behaviors. Furthermore, we present a novel module, Leanformer, designed\nto capture social interactions among vehicles. CITF demonstrates significant\nperformance improvements on three well-established datasets. In terms of\nlong-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM,\n28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, its\nrobustness in scenarios with limited or missing data is evident, surpassing\nmost state-of-the-art (SOTA) baselines, and paving the way for real-world\napplications.",
      "tldr_zh": "该研究针对自动驾驶中轨迹预测的挑战，提出了一种 Cognitive-Informed Transformer (CITF) 模型，通过整合认知概念 Perceived Safety 来模拟人类驾驶员的决策机制，从而提升预测准确性。CITF 包括 Perceived Safety-aware Module（涵盖 Quantitative Safety Assessment 用于量化风险水平和 Driver Behavior Profiling 用于分析驾驶行为）、以及 Leanformer 模块来捕捉车辆间的社会互动。实验结果显示，该模型在长期预测上分别在 NGSIM 数据集提升 12.0%、HighD 数据集提升 28.2%、MoCAD 数据集提升 20.8%，并在数据缺失场景中表现出色，为实际自动驾驶应用提供了更可靠的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20084v1",
      "published_date": "2025-02-27 13:43:17 UTC",
      "updated_date": "2025-02-27 13:43:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:49:38.851003"
    },
    {
      "arxiv_id": "2502.20073v2",
      "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Haochen Sun",
        "Shuwen Zhang",
        "Lujie Niu",
        "Lei Ren",
        "Hao Xu",
        "Hao Fu",
        "Fangkun Zhao",
        "Caixia Yuan",
        "Xiaojie Wang"
      ],
      "abstract": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 11 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaptation which are\ncritical for efficiently fulfilling complicated tasks. Notably, we highlight\nthe strengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. The environments,\n30 open-ended tasks, and the evaluation package are publicly available at\nhttps://github.com/YusaeMeow/Collab-Overcooked.",
      "tldr_zh": "本论文提出Collab-Overcooked，一个基于Overcooked-AI游戏的新基准，用于评估Large Language Models (LLMs)作为协作代理的性能。该基准从两个角度扩展现有框架：一是提供支持多样任务和目标的多智能体系统，通过自然语言通信促进协作；二是引入一系列过程导向的评估指标，以细粒度评估LLMs的协作能力。实验涉及11个流行LLMs，结果显示LLMs在目标解释方面表现出色，但活跃协作和持续适应方面存在显著差距。该基准已开源，包括环境、30个开放任务和评估包，可用于改进LLM-powered Multi-Agent System (LLM-MAS)。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "30 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20073v2",
      "published_date": "2025-02-27 13:31:13 UTC",
      "updated_date": "2025-05-22 02:46:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:49:50.234412"
    },
    {
      "arxiv_id": "2503.00069v1",
      "title": "Societal Alignment Frameworks Can Improve LLM Alignment",
      "title_zh": "社会对齐框架可以改善 LLM 对齐",
      "authors": [
        "Karolina Stańczak",
        "Nicholas Meade",
        "Mehar Bhatia",
        "Hattie Zhou",
        "Konstantin Böttinger",
        "Jeremy Barnes",
        "Jason Stanley",
        "Jessica Montgomery",
        "Richard Zemel",
        "Nicolas Papernot",
        "Nicolas Chapados",
        "Denis Therien",
        "Timothy P. Lillicrap",
        "Ana Marasović",
        "Sylvie Delacroix",
        "Gillian K. Hadfield",
        "Siva Reddy"
      ],
      "abstract": "Recent progress in large language models (LLMs) has focused on producing\nresponses that meet human expectations and align with shared values - a process\ncoined alignment. However, aligning LLMs remains challenging due to the\ninherent disconnect between the complexity of human values and the narrow\nnature of the technological approaches designed to address them. Current\nalignment methods often lead to misspecified objectives, reflecting the broader\nissue of incomplete contracts, the impracticality of specifying a contract\nbetween a model developer, and the model that accounts for every scenario in\nLLM alignment. In this paper, we argue that improving LLM alignment requires\nincorporating insights from societal alignment frameworks, including social,\neconomic, and contractual alignment, and discuss potential solutions drawn from\nthese domains. Given the role of uncertainty within societal alignment\nframeworks, we then investigate how it manifests in LLM alignment. We end our\ndiscussion by offering an alternative view on LLM alignment, framing the\nunderspecified nature of its objectives as an opportunity rather than perfect\ntheir specification. Beyond technical improvements in LLM alignment, we discuss\nthe need for participatory alignment interface designs.",
      "tldr_zh": "本文提出，当前的大语言模型(LLM)对齐方法存在挑战，如人类价值观的复杂性和目标不精确的问题，类似于不完整的合同。论文主张通过整合社会、经济和合同对齐框架的见解来提升LLM对齐效果，并探讨不确定性在其中的作用，将其视为机会而非障碍。最后，强调需要开发参与式对齐界面设计，以实现更全面的技术改进。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00069v1",
      "published_date": "2025-02-27 13:26:07 UTC",
      "updated_date": "2025-02-27 13:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:50:01.698050"
    },
    {
      "arxiv_id": "2502.20056v1",
      "title": "Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Kang Liu",
        "Zhuoqi Ma",
        "Xiaolu Kang",
        "Yunan Li",
        "Kun Xie",
        "Zhicheng Jiao",
        "Qiguang Miao"
      ],
      "abstract": "Automated radiology report generation offers an effective solution to\nalleviate radiologists' workload. However, most existing methods focus\nprimarily on single or fixed-view images to model current disease conditions,\nwhich limits diagnostic accuracy and overlooks disease progression. Although\nsome approaches utilize longitudinal data to track disease progression, they\nstill rely on single images to analyze current visits. To address these issues,\nwe propose enhanced contrastive learning with Multi-view Longitudinal data to\nfacilitate chest X-ray Report Generation, named MLRG. Specifically, we\nintroduce a multi-view longitudinal contrastive learning method that integrates\nspatial information from current multi-view images and temporal information\nfrom longitudinal data. This method also utilizes the inherent spatiotemporal\ninformation of radiology reports to supervise the pre-training of visual and\ntextual representations. Subsequently, we present a tokenized absence encoding\ntechnique to flexibly handle missing patient-specific prior knowledge, allowing\nthe model to produce more accurate radiology reports based on available prior\nknowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view CXR\ndatasets demonstrate that our MLRG outperforms recent state-of-the-art methods,\nachieving a 2.3% BLEU-4 improvement on MIMIC-CXR, a 5.5% F1 score improvement\non MIMIC-ABN, and a 2.7% F1 RadGraph improvement on Two-view CXR.",
      "tldr_zh": "该研究针对放射学报告生成的局限性，提出了一种增强对比学习方法 MLRG，利用多视图纵向数据整合图像的空间信息和疾病的时序信息，以提高诊断准确性和追踪疾病进展。MLRG 引入多视图纵向对比学习来监督视觉和文本表示的预训练，并采用 tokenized absence encoding 技术灵活处理缺失的患者先验知识，从而生成更精确的胸部 X 光报告。在 MIMIC-CXR、MIMIC-ABN 和 Two-view CXR 数据集上的实验显示，MLRG 优于现有方法，提升了 BLEU-4 2.3%、F1 分数 5.5% 和 F1 RadGraph 2.7%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.20056v1",
      "published_date": "2025-02-27 12:59:04 UTC",
      "updated_date": "2025-02-27 12:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:50:16.397767"
    },
    {
      "arxiv_id": "2502.20046v1",
      "title": "Polish-ASTE: Aspect-Sentiment Triplet Extraction Datasets for Polish",
      "title_zh": "翻译失败",
      "authors": [
        "Marta Lango",
        "Borys Naglik",
        "Mateusz Lango",
        "Iwo Naglik"
      ],
      "abstract": "Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and\ncomplex tasks in sentiment analysis. It concerns the construction of triplets\nthat contain an aspect, its associated sentiment polarity, and an opinion\nphrase that serves as a rationale for the assigned polarity. Despite the\ngrowing popularity of the task and the many machine learning methods being\nproposed to address it, the number of datasets for ASTE is very limited. In\nparticular, no dataset is available for any of the Slavic languages. In this\npaper, we present two new datasets for ASTE containing customer opinions about\nhotels and purchased products expressed in Polish. We also perform experiments\nwith two ASTE techniques combined with two large language models for Polish to\ninvestigate their performance and the difficulty of the assembled datasets. The\nnew datasets are available under a permissive licence and have the same file\nformat as the English datasets, facilitating their use in future research.",
      "tldr_zh": "这项研究针对Aspect-Sentiment Triplet Extraction (ASTE)任务——一个情感分析中的复杂挑战——介绍了两个新的数据集，用于波兰语的酒店和产品客户意见，每个数据集包含方面（aspect）、情感极性（sentiment polarity）和意见短语（opinion phrase）。这些数据集填补了斯拉夫语言在ASTE领域的空白，并以与英语数据集相同的文件格式发布，便于进一步研究。实验中，研究者结合两种ASTE技术和两种波兰语的大型语言模型（large language models），评估了模型性能和数据集难度，结果显示这些数据集具有较高的挑战性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20046v1",
      "published_date": "2025-02-27 12:38:04 UTC",
      "updated_date": "2025-02-27 12:38:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:50:26.304182"
    },
    {
      "arxiv_id": "2502.20045v1",
      "title": "Text2VDM: Text to Vector Displacement Maps for Expressive and Interactive 3D Sculpting",
      "title_zh": "Text2VDM",
      "authors": [
        "Hengyu Meng",
        "Duotun Wang",
        "Zhijing Shao",
        "Ligang Liu",
        "Zeyu Wang"
      ],
      "abstract": "Professional 3D asset creation often requires diverse sculpting brushes to\nadd surface details and geometric structures. Despite recent progress in 3D\ngeneration, producing reusable sculpting brushes compatible with artists'\nworkflows remains an open and challenging problem. These sculpting brushes are\ntypically represented as vector displacement maps (VDMs), which existing models\ncannot easily generate compared to natural images. This paper presents\nText2VDM, a novel framework for text-to-VDM brush generation through the\ndeformation of a dense planar mesh guided by score distillation sampling (SDS).\nThe original SDS loss is designed for generating full objects and struggles\nwith generating desirable sub-object structures from scratch in brush\ngeneration. We refer to this issue as semantic coupling, which we address by\nintroducing classifier-free guidance (CFG) weighted blending of prompt tokens\nto SDS, resulting in a more accurate target distribution and semantic guidance.\nExperiments demonstrate that Text2VDM can generate diverse, high-quality VDM\nbrushes for sculpting surface details and geometric structures. Our generated\nbrushes can be seamlessly integrated into mainstream modeling software,\nenabling various applications such as mesh stylization and real-time\ninteractive modeling.",
      "tldr_zh": "本论文提出Text2VDM框架，通过文本输入生成向量位移映射(VDMs)刷子，以支持专业3D雕刻中的表面细节和几何结构创作。框架利用分数蒸馏采样(SDS)引导密集平面网格的变形，并引入分类器自由引导(CFG)加权的提示令牌混合来解决语义耦合问题，从而实现更准确的子物体结构生成。实验结果显示，Text2VDM能产生多样、高质量的VDMs刷子，这些刷子可无缝集成到主流建模软件中，用于网格风格化及实时交互建模。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "I.2.6; I.3.6; I.3.8"
      ],
      "primary_category": "cs.GR",
      "comment": "11 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20045v1",
      "published_date": "2025-02-27 12:36:51 UTC",
      "updated_date": "2025-02-27 12:36:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:50:39.146792"
    },
    {
      "arxiv_id": "2502.20040v1",
      "title": "CleanMel: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR",
      "title_zh": "翻译失败",
      "authors": [
        "Nian Shao",
        "Rui Zhou",
        "Pengyu Wang",
        "Xian Li",
        "Ying Fang",
        "Yujie Yang",
        "Xiaofei Li"
      ],
      "abstract": "In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising\nand dereverberation network for improving both speech quality and automatic\nspeech recognition (ASR) performance. The proposed network takes as input the\nnoisy and reverberant microphone recording and predicts the corresponding clean\nMel-spectrogram. The enhanced Mel-spectrogram can be either transformed to\nspeech waveform with a neural vocoder or directly used for ASR. The proposed\nnetwork is composed of interleaved cross-band and narrow-band processing in the\nMel-frequency domain, for learning the full-band spectral pattern and the\nnarrow-band properties of signals, respectively. Compared to linear-frequency\ndomain or time-domain speech enhancement, the key advantage of Mel-spectrogram\nenhancement is that Mel-frequency presents speech in a more compact way and\nthus is easier to learn, which will benefit both speech quality and ASR.\nExperimental results on four English and one Chinese datasets demonstrate a\nsignificant improvement in both speech quality and ASR performance achieved by\nthe proposed model. Code and audio examples of our model are available online\nin https://audio.westlake.edu.cn/Research/CleanMel.html.",
      "tldr_zh": "本文提出 CleanMel，一种单通道 Mel-spectrogram 去噪和去混响网络，旨在同时提升语音质量和自动语音识别 (ASR) 性能。该网络采用交错的跨频带和窄频带处理，在 Mel-频率域中学习全频带谱模式和窄频带信号特性，从而使 Mel-spectrogram 更紧凑易学。实验结果显示，在四个英语和一个中文数据集上，CleanMel 显著改善了语音质量和 ASR 表现。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Submission to IEEE/ACM Trans. on TASLP",
      "pdf_url": "http://arxiv.org/pdf/2502.20040v1",
      "published_date": "2025-02-27 12:28:29 UTC",
      "updated_date": "2025-02-27 12:28:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:50:50.568208"
    },
    {
      "arxiv_id": "2502.20427v2",
      "title": "DeePen: Penetration Testing for Audio Deepfake Detection",
      "title_zh": "DeePen：音频深度伪造",
      "authors": [
        "Nicolas Müller",
        "Piotr Kawa",
        "Adriana Stan",
        "Thien-Phuc Doan",
        "Souhwan Jung",
        "Wei Herng Choong",
        "Philip Sperl",
        "Konstantin Böttinger"
      ],
      "abstract": "Deepfakes - manipulated or forged audio and video media - pose significant\nsecurity risks to individuals, organizations, and society at large. To address\nthese challenges, machine learning-based classifiers are commonly employed to\ndetect deepfake content. In this paper, we assess the robustness of such\nclassifiers through a systematic penetration testing methodology, which we\nintroduce as DeePen. Our approach operates without prior knowledge of or access\nto the target deepfake detection models. Instead, it leverages a set of\ncarefully selected signal processing modifications - referred to as attacks -\nto evaluate model vulnerabilities. Using DeePen, we analyze both real-world\nproduction systems and publicly available academic model checkpoints,\ndemonstrating that all tested systems exhibit weaknesses and can be reliably\ndeceived by simple manipulations such as time-stretching or echo addition.\nFurthermore, our findings reveal that while some attacks can be mitigated by\nretraining detection systems with knowledge of the specific attack, others\nremain persistently effective. We release all associated code.",
      "tldr_zh": "本研究提出 DeePen，一种针对音频 deepfake 检测的渗透测试方法，用于评估机器学习分类器的鲁棒性，而无需事先访问目标模型。DeePen 通过应用精心选择的信号处理修改（如时间拉伸或回声添加）作为攻击，测试了真实世界生产系统和公开学术模型，结果显示所有系统均存在弱点，可被简单操作可靠欺骗。此外，实验发现某些攻击可以通过使用特定攻击知识重新训练检测系统来缓解，但其他攻击持续有效；作者已发布所有相关代码，以促进进一步研究。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20427v2",
      "published_date": "2025-02-27 12:26:25 UTC",
      "updated_date": "2025-03-05 14:58:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:51:01.932215"
    },
    {
      "arxiv_id": "2502.20426v1",
      "title": "Among Them: A game-based framework for assessing persuasion capabilities of LLMs",
      "title_zh": "Among Them：一种基于游戏的框架，用于评估大型语言模型的说服能力",
      "authors": [
        "Mateusz Idziejczak",
        "Vasyl Korzavatykh",
        "Mateusz Stawicki",
        "Andrii Chmutov",
        "Marcin Korcz",
        "Iwo Błądek",
        "Dariusz Brzezinski"
      ],
      "abstract": "The proliferation of large language models (LLMs) and autonomous AI agents\nhas raised concerns about their potential for automated persuasion and social\ninfluence. While existing research has explored isolated instances of LLM-based\nmanipulation, systematic evaluations of persuasion capabilities across\ndifferent models remain limited. In this paper, we present an Among Us-inspired\ngame framework for assessing LLM deception skills in a controlled environment.\nThe proposed framework makes it possible to compare LLM models by game\nstatistics, as well as quantify in-game manipulation according to 25 persuasion\nstrategies from social psychology and rhetoric. Experiments between 8 popular\nlanguage models of different types and sizes demonstrate that all tested models\nexhibit persuasive capabilities, successfully employing 22 of the 25\nanticipated techniques. We also find that larger models do not provide any\npersuasion advantage over smaller models and that longer model outputs are\nnegatively correlated with the number of games won. Our study provides insights\ninto the deception capabilities of LLMs, as well as tools and data for\nfostering future research on the topic.",
      "tldr_zh": "本研究提出了一种基于Among Us游戏的框架，用于评估大型语言模型(LLMs)的说服和欺骗能力，通过受控环境中的游戏统计和25种社会心理学及修辞学说服策略进行量化。实验涉及8个不同类型和规模的流行LLMs模型，结果显示所有模型均展现出说服能力，成功运用了22种预期的技术。研究发现，模型规模较大并不带来说服优势，且输出长度与赢得游戏的次数呈负相关，为未来探索LLMs欺骗能力提供了宝贵洞见、工具和数据。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20426v1",
      "published_date": "2025-02-27 12:26:21 UTC",
      "updated_date": "2025-02-27 12:26:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:51:14.857973"
    },
    {
      "arxiv_id": "2502.20032v2",
      "title": "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping",
      "title_zh": "顺序鲁棒类增量学习：图驱动动态相似性分组",
      "authors": [
        "Guannan Lai",
        "Yujie Li",
        "Xiangkun Wang",
        "Junbo Zhang",
        "Tianrui Li",
        "Xin Yang"
      ],
      "abstract": "Class Incremental Learning (CIL) aims to enable models to learn new classes\nsequentially while retaining knowledge of previous ones. Although current\nmethods have alleviated catastrophic forgetting (CF), recent studies highlight\nthat the performance of CIL models is highly sensitive to the order of class\narrival, particularly when sequentially introduced classes exhibit high\ninter-class similarity. To address this critical yet understudied challenge of\nclass order sensitivity, we first extend existing CIL frameworks through\ntheoretical analysis, proving that grouping classes with lower pairwise\nsimilarity during incremental phases significantly improves model robustness to\norder variations. Building on this insight, we propose Graph-Driven Dynamic\nSimilarity Grouping (GDDSG), a novel method that employs graph coloring\nalgorithms to dynamically partition classes into similarity-constrained groups.\nEach group trains an isolated CIL sub-model and constructs meta-features for\nclass group identification. Experimental results demonstrate that our method\neffectively addresses the issue of class order sensitivity while achieving\noptimal performance in both model accuracy and anti-forgetting capability. Our\ncode is available at https://github.com/AIGNLAI/GDDSG.",
      "tldr_zh": "这篇论文针对 Class Incremental Learning (CIL) 中的类顺序敏感性问题，证明通过理论分析，在增量阶段将相似度较低的类分组可以显著提升模型对顺序变化的鲁棒性。作者提出 Graph-Driven Dynamic Similarity Grouping (GDDSG) 方法，使用图着色算法动态分区类，每个组训练独立的 CIL 子模型并构建元特征，用于类组识别。实验结果显示，该方法有效缓解了 catastrophic forgetting (CF)，并在模型准确性和防遗忘能力上实现了最佳性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05, 68Q25, 68U05",
        "I.2.6; I.2.10"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.20032v2",
      "published_date": "2025-02-27 12:16:57 UTC",
      "updated_date": "2025-03-18 03:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:51:27.787952"
    },
    {
      "arxiv_id": "2502.19973v2",
      "title": "Can Large Language Models Unveil the Mysteries? An Exploration of Their Ability to Unlock Information in Complex Scenarios",
      "title_zh": "大型语言模型能揭示神秘吗？：对其在复杂场景中解锁信息能力的探索",
      "authors": [
        "Chao Wang",
        "Luning Zhang",
        "Zheng Wang",
        "Yang Zhou"
      ],
      "abstract": "Combining multiple perceptual inputs and performing combinatorial reasoning\nin complex scenarios is a sophisticated cognitive function in humans. With\nadvancements in multi-modal large language models, recent benchmarks tend to\nevaluate visual understanding across multiple images. However, they often\noverlook the necessity of combinatorial reasoning across multiple perceptual\ninformation. To explore the ability of advanced models to integrate multiple\nperceptual inputs for combinatorial reasoning in complex scenarios, we\nintroduce two benchmarks: Clue-Visual Question Answering (CVQA), with three\ntask types to assess visual comprehension and synthesis, and Clue of\nPassword-Visual Question Answering (CPVQA), with two task types focused on\naccurate interpretation and application of visual data. For our benchmarks, we\npresent three plug-and-play approaches: utilizing model input for reasoning,\nenhancing reasoning through minimum margin decoding with randomness generation,\nand retrieving semantically relevant visual information for effective data\nintegration. The combined results reveal current models' poor performance on\ncombinatorial reasoning benchmarks, even the state-of-the-art (SOTA)\nclosed-source model achieves only 33.04% accuracy on CVQA, and drops to 7.38%\non CPVQA. Notably, our approach improves the performance of models on\ncombinatorial reasoning, with a 22.17% boost on CVQA and 9.40% on CPVQA over\nthe SOTA closed-source model, demonstrating its effectiveness in enhancing\ncombinatorial reasoning with multiple perceptual inputs in complex scenarios.\nThe code will be publicly available.",
      "tldr_zh": "本研究探讨了大型语言模型（Large Language Models）在复杂场景中整合多个感知输入进行组合推理的能力，引入了两个新基准：Clue-Visual Question Answering (CVQA) 用于评估视觉理解和合成，以及Clue of Password-Visual Question Answering (CPVQA) 专注于视觉数据的准确解释和应用。论文提出了三种即插即用方法，包括利用模型输入进行推理、最小边距解码增强随机生成，以及检索语义相关视觉信息以整合数据。实验结果显示，即使是SOTA闭源模型在CVQA上的准确率仅为33.04%、在CPVQA上降至7.38%，但这些方法显著提升了性能，在CVQA上提高了22.17%、在CPVQA上提高了9.40%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11pages",
      "pdf_url": "http://arxiv.org/pdf/2502.19973v2",
      "published_date": "2025-02-27 10:58:27 UTC",
      "updated_date": "2025-03-09 05:35:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:51:41.298979"
    },
    {
      "arxiv_id": "2502.19971v1",
      "title": "Efficient and Universal Neural-Network Decoder for Stabilizer-Based Quantum Error Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Gengyuan Hu",
        "Wanli Ouyang",
        "Chao-Yang Lu",
        "Chen Lin",
        "Han-Sen Zhong"
      ],
      "abstract": "Quantum error correction is crucial for large-scale quantum computing, but\nthe absence of efficient decoders for new codes like quantum low-density\nparity-check (QLDPC) codes has hindered progress. Here we introduce a universal\ndecoder based on linear attention sequence modeling and graph neural network\nthat operates directly on any stabilizer code's graph structure. Our numerical\nexperiments demonstrate that this decoder outperforms specialized algorithms in\nboth accuracy and speed across diverse stabilizer codes, including surface\ncodes, color codes, and QLDPC codes. The decoder maintains linear time scaling\nwith syndrome measurements and requires no structural modifications between\ndifferent codes. For the Bivariate Bicycle code with distance 12, our approach\nachieves a 39.4% lower logical error rate than previous best decoders while\nrequiring only ~1% of the decoding time. These results provide a practical,\nuniversal solution for quantum error correction, eliminating the need for\ncode-specific decoders.",
      "tldr_zh": "该研究提出了一种高效的通用神经网络解码器，用于基于稳定子代码（stabilizer codes）的量子错误修正（Quantum Error Correction），以解决新代码如量子低密度奇偶校验（QLDPC）代码缺乏高效解码器的问题。该解码器利用线性注意力序列建模（linear attention sequence modeling）和图神经网络（graph neural network），直接作用于任何稳定子代码的图结构，并实现了与综合测量线性时间缩放，同时无需针对不同代码（如表面代码、颜色代码和 QLDPC 代码）进行结构修改。在实验中，该方法在 Bivariate Bicycle 代码（距离 12）上比之前最佳解码器降低了 39.4% 的逻辑错误率，同时仅需 1% 的解码时间，提供了一个实用且通用的量子错误修正解决方案。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19971v1",
      "published_date": "2025-02-27 10:56:53 UTC",
      "updated_date": "2025-02-27 10:56:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:51:51.979387"
    },
    {
      "arxiv_id": "2502.19965v1",
      "title": "Deterministic or probabilistic? The psychology of LLMs as random number generators",
      "title_zh": "确定性还是概率性？大语言模型作为随机数生成器的心理学",
      "authors": [
        "Javier Coronado-Blázquez"
      ],
      "abstract": "Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在生成随机数时的确定性与概率性表现，揭示了这些基于随机机制的模型往往产生可预测的输出。研究通过系统测试不同模型架构、数值范围、温度参数和提示语言，分析了 LLMs 的性能，并发现这种确定性响应主要源于训练数据中的偏见。结果表明，模型如 DeepSeek-R1 能展示内部推理过程，但整体上，LLMs 倾向于复制人类认知偏见，从而削弱了真正的随机性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.19965v1",
      "published_date": "2025-02-27 10:45:27 UTC",
      "updated_date": "2025-02-27 10:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:52:02.801085"
    },
    {
      "arxiv_id": "2502.19954v1",
      "title": "Collaborative Stance Detection via Small-Large Language Model Consistency Verification",
      "title_zh": "基于小型-大型语言模型一致",
      "authors": [
        "Yu Yan",
        "Sheng Sun",
        "Zixiang Tang",
        "Teli Liu",
        "Min Liu"
      ],
      "abstract": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.",
      "tldr_zh": "本研究提出了一种协作立场检测框架CoVer，通过Small Language Models (SLMs)和Large Language Models (LLMs)的一致性验证来优化社交媒体立场检测任务。该框架采用批量处理方式，利用LLMs在共享上下文中进行推理生成立场预测和解释，随后引入SLMs进行逻辑一致性验证，以减少上下文噪声的影响，并通过一致性加权聚合来处理低一致性文本。实验结果显示，CoVer在零样本设置下超越现有方法，每条推文仅需0.54次LLM查询，同时显著提升性能，提供了一个更实用的社交媒体监测解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19954v1",
      "published_date": "2025-02-27 10:30:50 UTC",
      "updated_date": "2025-02-27 10:30:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:52:14.296098"
    },
    {
      "arxiv_id": "2502.19948v1",
      "title": "Dynamic DropConnect: Enhancing Neural Network Robustness through Adaptive Edge Dropping Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan-Chih Yang",
        "Hung-Hsuan Chen"
      ],
      "abstract": "Dropout and DropConnect are well-known techniques that apply a consistent\ndrop rate to randomly deactivate neurons or edges in a neural network layer\nduring training. This paper introduces a novel methodology that assigns dynamic\ndrop rates to each edge within a layer, uniquely tailoring the dropping process\nwithout incorporating additional learning parameters. We perform experiments on\nsynthetic and openly available datasets to validate the effectiveness of our\napproach. The results demonstrate that our method outperforms Dropout,\nDropConnect, and Standout, a classic mechanism known for its adaptive dropout\ncapabilities. Furthermore, our approach improves the robustness and\ngeneralization of neural network training without increasing computational\ncomplexity. The complete implementation of our methodology is publicly\naccessible for research and replication purposes at\nhttps://github.com/ericabd888/Adjusting-the-drop-probability-in-DropConnect-based-on-the-magnitude-of-the-gradient/.",
      "tldr_zh": "这篇论文引入了Dynamic DropConnect，一种新型方法，通过为神经网络层中的每个边动态分配drop rates来增强模型的鲁棒性，而不需添加额外学习参数。不同于传统的Dropout和DropConnect，该方法根据边的重要性自适应调整dropping过程，从而改善训练的泛化和鲁棒性。实验在合成和公开数据集上验证了其有效性，结果显示Dynamic DropConnect优于Dropout、DropConnect和Standout基准模型，且不增加计算复杂度。该方法的完整实现已公开在GitHub上，便于研究和复制。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19948v1",
      "published_date": "2025-02-27 10:17:02 UTC",
      "updated_date": "2025-02-27 10:17:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:52:27.873326"
    },
    {
      "arxiv_id": "2502.19944v1",
      "title": "Algebraic Machine Learning: Learning as computing an algebraic decomposition of a task",
      "title_zh": "翻译失败",
      "authors": [
        "Fernando Martin-Maroto",
        "Nabil Abderrahaman",
        "David Mendez",
        "Gonzalo G. de Polavieja"
      ],
      "abstract": "Statistics and Optimization are foundational to modern Machine Learning.\nHere, we propose an alternative foundation based on Abstract Algebra, with\nmathematics that facilitates the analysis of learning. In this approach, the\ngoal of the task and the data are encoded as axioms of an algebra, and a model\nis obtained where only these axioms and their logical consequences hold.\nAlthough this is not a generalizing model, we show that selecting specific\nsubsets of its breakdown into algebraic atoms obtained via subdirect\ndecomposition gives a model that generalizes. We validate this new learning\nprinciple on standard datasets such as MNIST, FashionMNIST, CIFAR-10, and\nmedical images, achieving performance comparable to optimized multilayer\nperceptrons. Beyond data-driven tasks, the new learning principle extends to\nformal problems, such as finding Hamiltonian cycles from their specifications\nand without relying on search. This algebraic foundation offers a fresh\nperspective on machine intelligence, featuring direct learning from training\ndata without the need for validation dataset, scaling through model additivity,\nand asymptotic convergence to the underlying rule in the data.",
      "tldr_zh": "本研究提出了一种基于Abstract Algebra的机器学习新基础，将任务和数据编码为代数的公理，并通过计算代数分解来构建模型，该模型仅依赖这些公理及其逻辑后果。不同于传统的统计和优化方法，该框架通过subdirect decomposition选择特定子集的代数原子，实现模型的泛化能力，并在MNIST、FashionMNIST、CIFAR-10和医疗图像数据集上，性能可与优化后的多层感知器（multilayer perceptrons）媲美。实验还扩展到正式问题，如从规范中直接找到Hamiltonian cycles，而无需搜索或验证数据集。该方法提供了一个新视角，强调模型的加法性（model additivity）和渐进收敛到数据中的底层规则，从而提升机器智能的效率和可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM",
        "cs.SC",
        "math.CO",
        "03G10, 06A12, 06A06, 08A70, 68R01, 68T01",
        "G.2.3; I.1.2; I.2.6; I.2.8"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19944v1",
      "published_date": "2025-02-27 10:13:42 UTC",
      "updated_date": "2025-02-27 10:13:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:52:40.346390"
    },
    {
      "arxiv_id": "2502.19938v1",
      "title": "Flexible Bivariate Beta Mixture Model: A Probabilistic Approach for Clustering Complex Data Structures",
      "title_zh": "翻译失败",
      "authors": [
        "Yung-Peng Hsu",
        "Hung-Hsuan Chen"
      ],
      "abstract": "Clustering is essential in data analysis and machine learning, but\ntraditional algorithms like $k$-means and Gaussian Mixture Models (GMM) often\nfail with nonconvex clusters. To address the challenge, we introduce the\nFlexible Bivariate Beta Mixture Model (FBBMM), which utilizes the flexibility\nof the bivariate beta distribution to handle diverse and irregular cluster\nshapes. Using the Expectation Maximization (EM) algorithm and Sequential Least\nSquares Programming (SLSQP) optimizer for parameter estimation, we validate\nFBBMM on synthetic and real-world datasets, demonstrating its superior\nperformance in clustering complex data structures, offering a robust solution\nfor big data analytics across various domains. We release the experimental code\nat https://github.com/yung-peng/MBMM-and-FBBMM.",
      "tldr_zh": "本研究针对传统聚类算法如 k-means 和 Gaussian Mixture Models (GMM) 在处理非凸聚类时存在的局限性，提出了一种 Flexible Bivariate Beta Mixture Model (FBBMM)，利用 bivariate beta 分布的灵活性来应对多样化和不规则的数据结构。FBBMM 通过 Expectation Maximization (EM) 算法和 Sequential Least Squares Programming (SLSQP) 优化器进行参数估计，在合成和真实数据集上进行了验证。实验结果显示，FBBMM 在聚类复杂数据方面表现出色，提供了一个适用于大数据分析的稳健解决方案，并公开了实验代码于 https://github.com/yung-peng/MBMM-and-FBBMM。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19938v1",
      "published_date": "2025-02-27 10:07:43 UTC",
      "updated_date": "2025-02-27 10:07:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:52:51.053616"
    },
    {
      "arxiv_id": "2502.19935v3",
      "title": "Lotus at SemEval-2025 Task 11: RoBERTa with Llama-3 Generated Explanations for Multi-Label Emotion Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Niloofar Ranjbar",
        "Hamed Baghbani"
      ],
      "abstract": "This paper presents a novel approach for multi-label emotion detection, where\nLlama-3 is used to generate explanatory content that clarifies ambiguous\nemotional expressions, thereby enhancing RoBERTa's emotion classification\nperformance. By incorporating explanatory context, our method improves\nF1-scores, particularly for emotions like fear, joy, and sadness, and\noutperforms text-only models. The addition of explanatory content helps resolve\nambiguity, addresses challenges like overlapping emotional cues, and enhances\nmulti-label classification, marking a significant advancement in emotion\ndetection tasks.",
      "tldr_zh": "本论文介绍了Lotus系统在SemEval-2025 Task 11中的方法，该系统利用Llama-3生成解释性内容来澄清模糊的情感表达，从而提升RoBERTa在多标签情感分类中的性能。 通过添加这些解释，方法有效解决了情感歧义和重叠线索的问题，显著提高了F1-scores，尤其在fear、joy和sadness等情感上，并优于纯文本模型。 这一创新为多标签情感检测任务带来了重要进展，提供了一种更准确的分类框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages , submitted to SemEval 2025-Task 11",
      "pdf_url": "http://arxiv.org/pdf/2502.19935v3",
      "published_date": "2025-02-27 10:04:36 UTC",
      "updated_date": "2025-04-16 10:17:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:53:02.854896"
    },
    {
      "arxiv_id": "2502.20423v1",
      "title": "Efficient Risk-sensitive Planning via Entropic Risk Measures",
      "title_zh": "通过熵风险度量的高效风险敏感规划",
      "authors": [
        "Alexandre Marthe",
        "Samuel Bounan",
        "Aurélien Garivier",
        "Claire Vernade"
      ],
      "abstract": "Risk-sensitive planning aims to identify policies maximizing some\ntail-focused metrics in Markov Decision Processes (MDPs). Such an optimization\ntask can be very costly for the most widely used and interpretable metrics such\nas threshold probabilities or (Conditional) Values at Risk. Indeed, previous\nwork showed that only Entropic Risk Measures (EntRM) can be efficiently\noptimized through dynamic programming, leaving a hard-to-interpret parameter to\nchoose. We show that the computation of the full set of optimal policies for\nEntRM across parameter values leads to tight approximations for the metrics of\ninterest. We prove that this optimality front can be computed effectively\nthanks to a novel structural analysis and smoothness properties of entropic\nrisks. Empirical results demonstrate that our approach achieves strong\nperformance in a variety of decision-making scenarios.",
      "tldr_zh": "本文提出了一种基于Entropic Risk Measures (EntRM)的风险敏感规划方法，旨在高效优化Markov Decision Processes (MDPs)中的尾部聚焦指标，如阈值概率或(Conditional) Values at Risk，这些指标的传统优化成本较高。该方法通过计算EntRM参数值上的完整最优策略集，实现对感兴趣指标的紧密近似，并利用新的结构分析和Entropic风险的光滑性属性来有效计算最优前沿。实验结果表明，该方法在各种决策场景中表现出色性能。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.OC",
        "math.PR"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20423v1",
      "published_date": "2025-02-27 09:56:51 UTC",
      "updated_date": "2025-02-27 09:56:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:53:15.012246"
    },
    {
      "arxiv_id": "2502.19924v1",
      "title": "DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Weihao wu",
        "Zhiwei Lin",
        "Yixuan Zhou",
        "Jingbei Li",
        "Rui Niu",
        "Qinghua Wu",
        "Songjun Cao",
        "Long Ma",
        "Zhiyong Wu"
      ],
      "abstract": "Conversational speech synthesis (CSS) aims to synthesize both contextually\nappropriate and expressive speech, and considerable efforts have been made to\nenhance the understanding of conversational context. However, existing CSS\nsystems are limited to deterministic prediction, overlooking the diversity of\npotential responses. Moreover, they rarely employ language model (LM)-based TTS\nbackbones, limiting the naturalness and quality of synthesized speech. To\naddress these issues, in this paper, we propose DiffCSS, an innovative CSS\nframework that leverages diffusion models and an LM-based TTS backbone to\ngenerate diverse, expressive, and contextually coherent speech. A\ndiffusion-based context-aware prosody predictor is proposed to sample diverse\nprosody embeddings conditioned on multimodal conversational context. Then a\nprosody-controllable LM-based TTS backbone is developed to synthesize\nhigh-quality speech with sampled prosody embeddings. Experimental results\ndemonstrate that the synthesized speech from DiffCSS is more diverse,\ncontextually coherent, and expressive than existing CSS systems",
      "tldr_zh": "这篇论文提出了DiffCSS框架，利用Diffusion Models和基于语言模型(LM)的TTS骨干，来生成多样化、富有表现力和上下文连贯的对话语音合成(CSS)。框架的核心包括一个基于扩散的上下文感知韵律预测器，用于采样多样化的韵律嵌入，以适应多模态对话上下文；随后，通过可控制韵律的LM-based TTS系统合成高质量语音。实验结果表明，DiffCSS比现有CSS系统在语音多样性、上下文连贯性和表现力方面表现出显著优势。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19924v1",
      "published_date": "2025-02-27 09:53:48 UTC",
      "updated_date": "2025-02-27 09:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:53:27.784013"
    },
    {
      "arxiv_id": "2502.19922v1",
      "title": "Incremental Learning with Repetition via Pseudo-Feature Projection",
      "title_zh": "翻译失败",
      "authors": [
        "Benedikt Tscheschner",
        "Eduardo Veas",
        "Marc Masana"
      ],
      "abstract": "Incremental Learning scenarios do not always represent real-world inference\nuse-cases, which tend to have less strict task boundaries, and exhibit\nrepetition of common classes and concepts in their continual data stream. To\nbetter represent these use-cases, new scenarios with partial repetition and\nmixing of tasks are proposed, where the repetition patterns are innate to the\nscenario and unknown to the strategy. We investigate how exemplar-free\nincremental learning strategies are affected by data repetition, and we adapt a\nseries of state-of-the-art approaches to analyse and fairly compare them under\nboth settings. Further, we also propose a novel method (Horde), able to\ndynamically adjust an ensemble of self-reliant feature extractors, and align\nthem by exploiting class repetition. Our proposed exemplar-free method achieves\ncompetitive results in the classic scenario without repetition, and\nstate-of-the-art performance in the one with repetition.",
      "tldr_zh": "这篇论文指出，传统的Incremental Learning场景无法充分代表真实世界的推理用例，因为这些用例往往涉及模糊的任务边界和常见类别的重复，因此提出了新的场景，包括部分重复和任务混合，其中重复模式对策略未知。研究者分析了exemplar-free增量学习策略如何受数据重复影响，并改进了若干最先进方法进行公平比较。同时，他们提出了一种新方法Horde，能够动态调整一组自给自足的特征提取器，并通过Pseudo-Feature Projection利用类别重复来对齐这些提取器。实验结果显示，Horde在经典的无重复场景中表现出竞争力，而在包含重复的场景中达到了state-of-the-art性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19922v1",
      "published_date": "2025-02-27 09:43:35 UTC",
      "updated_date": "2025-02-27 09:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:53:40.595825"
    },
    {
      "arxiv_id": "2502.19918v2",
      "title": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Sui",
        "Yufei He",
        "Tri Cao",
        "Simeng Han",
        "Yulin Chen",
        "Bryan Hooi"
      ],
      "abstract": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains\nto solve complex tasks. However, this trial-and-error approach often leads to\nhigh computational overhead and error propagation, where early mistakes can\nderail subsequent steps. To address these issues, we introduce Meta-Reasoner, a\nframework that dynamically optimizes inference-time reasoning by enabling LLMs\nto \\enquote{think about how to think.} Drawing inspiration from human\nmeta-cognition and dual-process theory, Meta-Reasoner operates as a strategic\nadvisor, decoupling high-level guidance from step-by-step generation. It\nemploys contextual multi-armed bandits to iteratively evaluate reasoning\nprogress and select optimal strategies (e.g., backtrack, clarify ambiguity,\nrestart from scratch, or propose alternative approaches), and reallocates\ncomputational resources toward the most promising paths. Our evaluations on\nmathematical reasoning and puzzles highlight the potential of dynamic reasoning\nchains to overcome inherent challenges in the LLM reasoning process and also\nshow promise in broader applications, offering a scalable and adaptable\nsolution for reasoning-intensive tasks.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models, LLMs）在处理复杂任务时依赖长推理链所带来的高计算开销和错误传播问题，提出了一种动态优化框架Meta-Reasoner。该框架借鉴人类元认知和双过程理论，让LLMs“思考如何思考”，通过一个战略顾问机制分离高层指导与步步生成，并利用contextual multi-armed bandits算法评估推理进度、选择最佳策略（如回溯、澄清歧义或重启）。实验结果显示，Meta-Reasoner在数学推理和谜题任务上显著提升了性能，并为更广泛的推理密集型应用提供了一个可扩展的解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19918v2",
      "published_date": "2025-02-27 09:40:13 UTC",
      "updated_date": "2025-05-22 08:15:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:53:52.493940"
    },
    {
      "arxiv_id": "2502.19915v2",
      "title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty",
      "title_zh": "LLM驱动的有效知识追踪：通过整合双通道难度",
      "authors": [
        "Jiahui Cen",
        "Jianghao Lin",
        "Weixuan Zhong",
        "Dong Zhou",
        "Jin Chen",
        "Aimin Yang",
        "Yongmei Zhou"
      ],
      "abstract": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
      "tldr_zh": "这篇论文针对知识追踪 (KT) 模型在智能辅导系统中面临的冷启动问题、个性化掌握建模不准确以及决策过程不透明的挑战，提出了一种基于大语言模型 (LLMs) 和检索增强生成 (RAG) 的双通道难度感知框架 (DDKT)。该框架的关键创新包括难度平衡感知序列 (DBPS) 通过注意力机制测量主观与客观难度差距、难度掌握比率 (DMR) 精确建模学生在不同难度区域的掌握水平，以及知识状态更新机制利用门控网络实现个性化知识获取。实验结果显示，在两个真实数据集上，该方法比九个基线模型提升 AUC 指标 2% 到 10%，并有效解决了冷启动问题并提高了模型可解释性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "During a careful review of our base-experiment results, we discovered\n  a possible error in the way some data were recorded. To ensure the integrity\n  and accuracy of our work, we must correct these results and revise the\n  corresponding analysis before making the manuscript publicly available",
      "pdf_url": "http://arxiv.org/pdf/2502.19915v2",
      "published_date": "2025-02-27 09:36:27 UTC",
      "updated_date": "2025-04-30 01:26:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:54:05.478052"
    },
    {
      "arxiv_id": "2502.19907v1",
      "title": "Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Qianxi He",
        "Qianyu He",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Weikang Zhou",
        "Zeye Sun",
        "Fei Yu"
      ],
      "abstract": "Logical reasoning is essential for large language models (LLMs) to ensure\naccurate and coherent inference. However, LLMs struggle with reasoning order\nvariations and fail to generalize across logically equivalent transformations.\nLLMs often rely on fixed sequential patterns rather than true logical\nunderstanding. To address this issue, we introduce an order-centric data\naugmentation framework based on commutativity in logical reasoning. We first\nrandomly shuffle independent premises to introduce condition order\naugmentation. For reasoning steps, we construct a directed acyclic graph (DAG)\nto model dependencies between steps, which allows us to identify valid\nreorderings of steps while preserving logical correctness. By leveraging\norder-centric augmentations, models can develop a more flexible and generalized\nreasoning process. Finally, we conduct extensive experiments across multiple\nlogical reasoning benchmarks, demonstrating that our method significantly\nenhances LLMs' reasoning performance and adaptability to diverse logical\nstructures. We release our codes and augmented data in\nhttps://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/.",
      "tldr_zh": "这篇论文解决了大型语言模型 (LLMs) 在逻辑推理中对顺序变化的敏感性问题，LLMs 往往依赖固定顺序而非真正的逻辑理解，导致泛化能力不足。作者提出了一种基于交换性的顺序中心数据增强框架，包括随机打乱独立前提（condition order augmentation）和使用有向无环图 (DAG) 建模推理步骤的依赖关系，以确保逻辑正确性的前提下进行有效重新排序。通过这种增强方法，模型能够发展出更灵活和泛化的推理过程。实验在多个逻辑推理基准上证明，该框架显著提升了 LLMs 的推理性能和适应性，并发布了相关代码和增强数据。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19907v1",
      "published_date": "2025-02-27 09:25:50 UTC",
      "updated_date": "2025-02-27 09:25:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:54:16.101126"
    },
    {
      "arxiv_id": "2502.19902v2",
      "title": "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy",
      "title_zh": "翻译失败",
      "authors": [
        "Zaijing Li",
        "Yuquan Xie",
        "Rui Shao",
        "Gongwei Chen",
        "Dongmei Jiang",
        "Liqiang Nie"
      ],
      "abstract": "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps://cybertronagent.github.io/Optimus-2.github.io/.",
      "tldr_zh": "该研究提出Optimus-2，一种多模态Minecraft代理，使用Multimodal Large Language Model (MLLM)进行高层规划，并结合Goal-Observation-Action Conditioned Policy (GOAP)实现低层控制，以模仿人类行为模式并处理观察、动作和语言间的复杂关系。GOAP包括Action-guided Behavior Encoder，用于动态建模每个时间步的观察-动作因果关系，并将其整合成固定长度行为标记，然后由MLLM与语言指令对齐进行自回归动作预测。此外，研究引入了高品质Minecraft Goal-Observation-Action (MGOA)数据集，包含25,000个视频和约30M目标-观察-动作对，以支持代理训练。实验结果显示，Optimus-2在Minecraft的原子任务、长时任务和开放指令任务中表现出色，显著提升了代理性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accept to CVPR 2025, Project page:\n  https://cybertronagent.github.io/Optimus-2.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2502.19902v2",
      "published_date": "2025-02-27 09:18:04 UTC",
      "updated_date": "2025-03-11 07:51:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:54:27.096464"
    },
    {
      "arxiv_id": "2502.20422v1",
      "title": "SEKI: Self-Evolution and Knowledge Inspiration based Neural Architecture Search via Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zicheng Cai",
        "Yaohua Tang",
        "Yutao Lai",
        "Hua Wang",
        "Zhi Chen",
        "Hao Chen"
      ],
      "abstract": "We introduce SEKI, a novel large language model (LLM)-based neural\narchitecture search (NAS) method. Inspired by the chain-of-thought (CoT)\nparadigm in modern LLMs, SEKI operates in two key stages: self-evolution and\nknowledge distillation. In the self-evolution stage, LLMs initially lack\nsufficient reference examples, so we implement an iterative refinement\nmechanism that enhances architectures based on performance feedback. Over time,\nthis process accumulates a repository of high-performance architectures. In the\nknowledge distillation stage, LLMs analyze common patterns among these\narchitectures to generate new, optimized designs. Combining these two stages,\nSEKI greatly leverages the capacity of LLMs on NAS and without requiring any\ndomain-specific data. Experimental results show that SEKI achieves\nstate-of-the-art (SOTA) performance across various datasets and search spaces\nwhile requiring only 0.05 GPU-days, outperforming existing methods in both\nefficiency and accuracy. Furthermore, SEKI demonstrates strong generalization\ncapabilities, achieving SOTA-competitive results across multiple tasks.",
      "tldr_zh": "本研究引入了 SEKI，一种基于 Large Language Models (LLMs) 的 Neural Architecture Search (NAS) 方法，通过 Self-Evolution 和 Knowledge Distillation 两个阶段优化架构设计。在 Self-Evolution 阶段，LLMs 通过迭代精炼机制和性能反馈积累高性能架构；在 Knowledge Distillation 阶段，LLMs 分析这些架构的共同模式生成新优化设计，而无需任何领域特定数据。实验结果表明，SEKI 在多种数据集和搜索空间上达到 State-of-the-Art (SOTA) 性能，仅需 0.05 GPU-days，便优于现有方法，并展示出强大的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.20422v1",
      "published_date": "2025-02-27 09:17:49 UTC",
      "updated_date": "2025-02-27 09:17:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:54:39.777969"
    },
    {
      "arxiv_id": "2502.19899v1",
      "title": "Shared Autonomy for Proximal Teaching",
      "title_zh": "翻译失败",
      "authors": [
        "Megha Srivastava",
        "Reihaneh Iranmanesh",
        "Yuchen Cui",
        "Deepak Gopinath",
        "Emily Sumner",
        "Andrew Silva",
        "Laporsha Dees",
        "Guy Rosman",
        "Dorsa Sadigh"
      ],
      "abstract": "Motor skill learning often requires experienced professionals who can provide\npersonalized instruction. Unfortunately, the availability of high-quality\ntraining can be limited for specialized tasks, such as high performance racing.\nSeveral recent works have leveraged AI-assistance to improve instruction of\ntasks ranging from rehabilitation to surgical robot tele-operation. However,\nthese works often make simplifying assumptions on the student learning process,\nand fail to model how a teacher's assistance interacts with different\nindividuals' abilities when determining optimal teaching strategies. Inspired\nby the idea of scaffolding from educational psychology, we leverage shared\nautonomy, a framework for combining user inputs with robot autonomy, to aid\nwith curriculum design. Our key insight is that the way a student's behavior\nimproves in the presence of assistance from an autonomous agent can highlight\nwhich sub-skills might be most ``learnable'' for the student, or within their\nZone of Proximal Development. We use this to design Z-COACH, a method for using\nshared autonomy to provide personalized instruction targeting interpretable\ntask sub-skills. In a user study (n=50), where we teach high performance racing\nin a simulated environment of the Thunderhill Raceway Park with the CARLA\nAutonomous Driving simulator, we show that Z-COACH helps identify which skills\neach student should first practice, leading to an overall improvement in\ndriving time, behavior, and smoothness. Our work shows that increasingly\navailable semi-autonomous capabilities (e.g. in vehicles, robots) can not only\nassist human users, but also help *teach* them.",
      "tldr_zh": "该论文探讨了使用共享自治（Shared Autonomy）框架来提升技能学习，针对专业任务如高性能赛车的问题，强调现有AI辅助方法忽略了个体学习差异。作者提出Z-COACH方法，基于教育心理学的最近发展区（Zone of Proximal Development）概念，通过分析学生在自治辅助下的行为改善来设计个性化课程，针对可解释的任务子技能提供指导。在模拟环境（如CARLA Autonomous Driving simulator）中进行的用户研究（n=50）显示，Z-COACH帮助识别学生应优先练习的技能，导致驾驶时间、行为和流畅度整体提升，并证明半自治系统可用于教学而非仅辅助。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ACM/IEEE International Conference on Human-Robot\n  Interaction, 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19899v1",
      "published_date": "2025-02-27 09:14:17 UTC",
      "updated_date": "2025-02-27 09:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:54:51.905268"
    },
    {
      "arxiv_id": "2502.19892v1",
      "title": "ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local Planner for Unstructured and Dynamic Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Jinghao Xin",
        "Zhichao Liang",
        "Zihuan Zhang",
        "Peng Wang",
        "Ning Li"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) has demonstrated potential in addressing\nrobotic local planning problems, yet its efficacy remains constrained in highly\nunstructured and dynamic environments. To address these challenges, this study\nproposes the ColorDynamic framework. First, an end-to-end DRL formulation is\nestablished, which maps raw sensor data directly to control commands, thereby\nensuring compatibility with unstructured environments. Under this formulation,\na novel network, Transqer, is introduced. The Transqer enables online DRL\nlearning from temporal transitions, substantially enhancing decision-making in\ndynamic scenarios. To facilitate scalable training of Transqer with diverse\ndata, an efficient simulation platform E-Sparrow, along with a data\naugmentation technique leveraging symmetric invariance, are developed.\nComparative evaluations against state-of-the-art methods, alongside assessments\nof generalizability, scalability, and real-time performance, were conducted to\nvalidate the effectiveness of ColorDynamic. Results indicate that our approach\nachieves a success rate exceeding 90% while exhibiting real-time capacity\n(1.2-1.3 ms per planning). Additionally, ablation studies were performed to\ncorroborate the contributions of individual components. Building on this, the\nOkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated and\nreal-world experiments demonstrating its superiority and applicability in\ncomplex scenarios. The codebase and experimental demonstrations have been\nopen-sourced on our website to facilitate reproducibility and further research.",
      "tldr_zh": "本研究提出ColorDynamic框架，利用深度强化学习(DRL)解决机器人本地规划在非结构化和动态环境中的挑战。该框架采用端到端DRL公式，直接从原始传感器数据映射到控制命令，并引入Transqer网络，支持在线学习时间转换以提升动态决策能力；同时，开发了模拟平台E-Sparrow和基于对称不变性的数据增强技术，实现可扩展训练。实验结果显示，ColorDynamic成功率超过90%，实时规划时间为1.2-1.3 ms，并在与最先进方法的比较中表现出色；此外，OkayPlan-ColorDynamic (OPCD)导航系统经模拟和真实实验验证其在复杂场景中的优越性，并已开源以促进进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.19892v1",
      "published_date": "2025-02-27 09:01:11 UTC",
      "updated_date": "2025-02-27 09:01:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:55:04.107812"
    },
    {
      "arxiv_id": "2502.19883v2",
      "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
      "title_zh": "效率之尖端背后：揭示小语言模型中潜藏的越狱攻击威胁",
      "authors": [
        "Sibo Yi",
        "Tianshuo Cong",
        "Xinlei He",
        "Qi Li",
        "Jiaxing Song"
      ],
      "abstract": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs.",
      "tldr_zh": "该论文调查了小型语言模型 (SLMs) 在越狱攻击 (jailbreak attacks) 下的安全风险，评估了13个最先进SLMs的表现，发现大多数模型容易受到现有攻击影响，甚至对直接有害提示敏感。研究者测试了多种代表性防御方法，证明这些方法能有效提升SLMs的安全性。论文进一步分析了架构压缩、量化、知识蒸馏等SLM技术可能导致的安全退化，并强调了SLMs的安全挑战，为开发更稳健的模型提供宝贵见解。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages. 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.19883v2",
      "published_date": "2025-02-27 08:44:04 UTC",
      "updated_date": "2025-02-28 12:59:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:55:16.682385"
    },
    {
      "arxiv_id": "2502.19860v1",
      "title": "MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue",
      "title_zh": "翻译失败",
      "authors": [
        "Yujia Chen",
        "Changsong Li",
        "Yiming Wang",
        "Qingqing Xiao",
        "Nan Zhang",
        "Zifan Kong",
        "Peng Wang",
        "Binyu Yan"
      ],
      "abstract": "Mental health issues are worsening in today's competitive society, such as\ndepression and anxiety. Traditional healings like counseling and chatbots fail\nto engage effectively, they often provide generic responses lacking emotional\ndepth. Although large language models (LLMs) have the potential to create more\nhuman-like interactions, they still struggle to capture subtle emotions. This\nrequires LLMs to be equipped with human-like adaptability and warmth. To fill\nthis gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm\nthat provides more immersive psychological healing environments. Considering\nthe strong generative and role-playing ability of LLM agents, we predefine an\ninteractive healing framework and assign LLM agents different roles within the\nframework to engage in interactive inner dialogues with users, thereby\nproviding an immersive healing experience. We conduct extensive human\nexperiments in various real-world healing dimensions, and find that MIND\nprovides a more user-friendly experience than traditional paradigms. This\ndemonstrates that MIND effectively leverages the significant potential of LLMs\nin psychological healing.",
      "tldr_zh": "在当今竞争激烈的社会中，心理健康问题如抑郁和焦虑日益严重，而传统治疗方法（如咨询和聊天机器人）往往提供泛化回应，缺乏情感深度。研究提出 MIND（Multi-agent INner Dialogue）框架，利用 LLMs（大语言模型）的生成和角色扮演能力，预定义互动治疗结构，并分配不同角色给 LLM 代理，与用户进行沉浸式内部对话。人类实验结果显示，MIND 在各种真实世界治疗维度上提供更用户友好的体验，证明了 LLMs 在心理治疗领域的显著潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19860v1",
      "published_date": "2025-02-27 08:04:27 UTC",
      "updated_date": "2025-02-27 08:04:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:55:29.547539"
    },
    {
      "arxiv_id": "2502.19852v1",
      "title": "ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments",
      "title_zh": "ConvCodeWorld：可重现反馈环境中的对话式代码生成基准测试",
      "authors": [
        "Hojae Han",
        "Seung-won Hwang",
        "Rajhans Samdani",
        "Yuxiong He"
      ],
      "abstract": "Large language models (LLMs) have proven invaluable for code generation,\nparticularly in interactive settings. However, existing code generation\nbenchmarks fail to capture the diverse feedback encountered in multi-turn\ninteractions, limiting our ability to evaluate LLMs in these contexts. To\naddress this gap, we present a set of novel benchmarks that explicitly model\nthe quality of feedback provided to code generation LLMs. Our contributions are\nthreefold: First, we introduce CONVCODEWORLD, a novel and reproducible\nenvironment for benchmarking interactive code generation. CONVCODEWORLD\nsimulates 9 distinct interactive code generation scenarios while systematically\ncombining three types of feedback: (a) compilation feedback; (b) execution\nfeedback with varying test coverage; (c) verbal feedback generated by GPT-4o\nwith different levels of expertise. Second, we introduce CONVCODEBENCH, a fast,\nstatic version of benchmark that uses pre-generated feedback logs, eliminating\nthe need for costly dynamic verbal feedback generation while maintaining strong\nSpearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third,\nextensive evaluations of both closed-source and open-source LLMs including\nR1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies\nsignificantly based on the feedback provided; (b) Weaker LLMs, with sufficient\nfeedback, can outperform single-turn results of state-of-the-art LLMs without\nfeedback; (c) Training on a specific feedback combination can limit an LLM's\nability to utilize unseen combinations; (d) LLMs solve problems in fewer turns\n(high MRR) may not solve as many problems overall (high Recall), and vice\nversa. All implementations and benchmarks will be made publicly available at\nhttps://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld",
      "tldr_zh": "这篇论文引入了CONVCODEWORLD，一个可重现的环境，用于基准测试交互式代码生成，模拟9种场景并结合三种反馈类型：编译反馈、执行反馈（不同测试覆盖率）和由GPT-4o生成的口头反馈（不同专业水平）。同时，论文提出了CONVCODEBENCH，一个快速的静态基准，使用预生成的反馈日志，避免动态生成成本，同时保持与CONVCODEWORLD的高Spearman's rank correlations（0.82到0.99）。通过对包括R1-Distill在内的各种LLMs的评估，研究发现反馈质量显著影响性能，较弱LLMs在充足反馈下可能超越无反馈的SOTA模型，且在特定反馈组合上训练可能限制LLMs处理新组合的能力；此外，高MRR（更少轮次解决问题）不一定对应高Recall（解决更多问题）。所有实现和基准已在https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld公开可用。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19852v1",
      "published_date": "2025-02-27 07:54:32 UTC",
      "updated_date": "2025-02-27 07:54:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:55:42.912755"
    },
    {
      "arxiv_id": "2502.19830v1",
      "title": "Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation",
      "title_zh": "翻译失败",
      "authors": [
        "Yiwei Li",
        "Ji Zhang",
        "Shaoxiong Feng",
        "Peiwen Yuan",
        "Xinglin Wang",
        "Jiayi Shi",
        "Yueqi Zhang",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "Self-consistency improves reasoning by aggregating diverse stochastic\nsamples, yet the dynamics behind its efficacy remain underexplored. We reframe\nself-consistency as a dynamic distributional alignment problem, revealing that\ndecoding temperature not only governs sampling randomness but also actively\nshapes the latent answer distribution. Given that high temperatures require\nprohibitively large sample sizes to stabilize, while low temperatures risk\namplifying biases, we propose a confidence-driven mechanism that dynamically\ncalibrates temperature: sharpening the sampling distribution under uncertainty\nto align with high-probability modes, and promoting exploration when confidence\nis high. Experiments on mathematical reasoning tasks show this approach\noutperforms fixed-diversity baselines under limited samples, improving both\naverage and best-case performance across varying initial temperatures without\nadditional data or modules. This establishes self-consistency as a\nsynchronization challenge between sampling dynamics and evolving answer\ndistributions.",
      "tldr_zh": "本研究从动态分布对齐视角重新审视 self-consistency 方法，揭示其通过聚合多样化随机样本改善推理的机制，并强调 decoding temperature 在控制采样随机性和塑造潜在答案分布中的关键作用。高温度需要大量样本以实现稳定，而低温度可能放大偏差，因此作者提出一种信心驱动机制，动态调整温度：在不确定时锐化采样分布以对齐高概率模式，在置信度高时促进探索。实验在数学推理任务上显示，该方法在样本有限的情况下优于固定多样性基线，提高了平均和最佳性能，且无需额外数据或模块。该框架将 self-consistency 定位为采样动态与答案分布演变之间的同步挑战，为更高效的答案聚合提供新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19830v1",
      "published_date": "2025-02-27 07:07:40 UTC",
      "updated_date": "2025-02-27 07:07:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:55:52.750733"
    },
    {
      "arxiv_id": "2502.19823v2",
      "title": "GraphSparseNet: a Novel Method for Large Scale Traffic Flow Prediction",
      "title_zh": "GraphSparseNet：一种用于大规模交通流量预测的新颖方法",
      "authors": [
        "Weiyang Kong",
        "Kaiqi Wu",
        "Sen Zhang",
        "Yubao Liu"
      ],
      "abstract": "Traffic flow forecasting is a critical spatio-temporal data mining task with\nwide-ranging applications in intelligent route planning and dynamic traffic\nmanagement. Recent advancements in deep learning, particularly through Graph\nNeural Networks (GNNs), have significantly enhanced the accuracy of these\nforecasts by capturing complex spatio-temporal dynamics. However, the\nscalability of GNNs remains a challenge due to their exponential growth in\nmodel complexity with increasing nodes in the graph. Existing methods to\naddress this issue, including sparsification, decomposition, and kernel-based\napproaches, either do not fully resolve the complexity issue or risk\ncompromising predictive accuracy. This paper introduces GraphSparseNet (GSNet),\na novel framework designed to improve both the scalability and accuracy of\nGNN-based traffic forecasting models. GraphSparseNet is comprised of two core\nmodules: the Feature Extractor and the Relational Compressor. These modules\noperate with linear time and space complexity, thereby reducing the overall\ncomputational complexity of the model to a linear scale. Our extensive\nexperiments on multiple real-world datasets demonstrate that GraphSparseNet not\nonly significantly reduces training time by 3.51x compared to state-of-the-art\nlinear models but also maintains high predictive performance.",
      "tldr_zh": "该论文针对大规模交通流量预测中的可扩展性挑战，提出了一种新型方法GraphSparseNet (GSNet)，旨在解决Graph Neural Networks (GNNs) 模型复杂度随节点数指数增长的问题。GSNet 由Feature Extractor 和Relational Compressor 两个核心模块组成，这些模块实现了线性时间和空间复杂度，从而显著降低整体计算需求。实验结果显示，在多个真实数据集上，GSNet 相较于最先进线性模型将训练时间减少3.51倍，同时保持高预测准确性，为高效的交通流量预测提供了可行解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by VLDB 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19823v2",
      "published_date": "2025-02-27 06:51:20 UTC",
      "updated_date": "2025-05-13 08:38:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:56:04.640541"
    },
    {
      "arxiv_id": "2502.19820v3",
      "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zixuan Weng",
        "Xiaolong Jin",
        "Jinyuan Jia",
        "Xiangyu Zhang"
      ],
      "abstract": "Ensuring AI safety is crucial as large language models become increasingly\nintegrated into real-world applications. A key challenge is jailbreak, where\nadversarial prompts bypass built-in safeguards to elicit harmful disallowed\noutputs. Inspired by psychological foot-in-the-door principles, we introduce\nFITD,a novel multi-turn jailbreak method that leverages the phenomenon where\nminor initial commitments lower resistance to more significant or more\nunethical transgressions. Our approach progressively escalates the malicious\nintent of user queries through intermediate bridge prompts and aligns the\nmodel's response by itself to induce toxic responses. Extensive experimental\nresults on two jailbreak benchmarks demonstrate that FITD achieves an average\nattack success rate of 94% across seven widely used models, outperforming\nexisting state-of-the-art methods. Additionally, we provide an in-depth\nanalysis of LLM self-corruption, highlighting vulnerabilities in current\nalignment strategies and emphasizing the risks inherent in multi-turn\ninteractions. The code is available at\nhttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.",
      "tldr_zh": "该研究提出了一种名为 Foot-In-The-Door (FITD) 的多轮越狱方法，针对大型语言模型 (LLMs) 的安全问题，通过借鉴心理 foot-in-the-door 原则逐步升级恶意查询，以绕过内置保护机制。FITD 通过中间桥接提示让模型自身对齐并诱导产生有害响应，在两个越狱基准测试中，平均攻击成功率达到 94%，优于现有最先进方法。论文还分析了 LLMs 的自我腐蚀风险，强调多轮交互中的漏洞，并呼吁加强模型对齐策略以提升 AI 安全。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.19820v3",
      "published_date": "2025-02-27 06:49:16 UTC",
      "updated_date": "2025-03-28 00:37:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:56:18.915308"
    },
    {
      "arxiv_id": "2502.19811v3",
      "title": "Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Shulai Zhang",
        "Ningxin Zheng",
        "Haibin Lin",
        "Ziheng Jiang",
        "Wenlei Bao",
        "Chengquan Jiang",
        "Qi Hou",
        "Weihao Cui",
        "Size Zheng",
        "Li-Wen Chang",
        "Quan Chen",
        "Xin Liu"
      ],
      "abstract": "Mixture-of-experts (MoE) has been extensively employed to scale large\nlanguage models to trillion-plus parameters while maintaining a fixed\ncomputational cost. The development of large MoE models in the distributed\nscenario encounters the problem of large communication overhead. The\ninter-device communication of a MoE layer can occupy 47% time of the entire\nmodel execution with popular models and frameworks. Therefore, existing methods\nsuggest the communication in a MoE layer to be pipelined with the computation\nfor overlapping. However, these coarse grained overlapping schemes introduce a\nnotable impairment of computational efficiency and the latency concealing is\nsub-optimal.\n  To this end, we present COMET, an optimized MoE system with fine-grained\ncommunication-computation overlapping. Leveraging data dependency analysis and\ntask rescheduling, COMET achieves precise fine-grained overlapping of\ncommunication and computation. Through adaptive workload assignment, COMET\neffectively eliminates fine-grained communication bottlenecks and enhances its\nadaptability across various scenarios. Our evaluation shows that COMET\naccelerates the execution of a single MoE layer by $1.96\\times$ and for\nend-to-end execution, COMET delivers a $1.71\\times$ speedup on average. COMET\nhas been adopted in the production environment of clusters with\nten-thousand-scale of GPUs, achieving savings of millions of GPU hours.",
      "tldr_zh": "该研究针对 Mixture-of-Experts (MoE) 模型在分布式场景下的高通信开销问题，提出了 COMET 系统，该系统通过细粒度的 Computation-communication Overlapping、数据依赖分析和任务重新调度来优化计算和通信效率。\nCOMET 采用自适应工作负载分配，消除了细粒度通信瓶颈，并提升了在各种场景下的适应性。\n实验结果显示，COMET 使单个 MoE 层的执行速度提高 1.96 倍，端到端执行平均加速 1.71 倍，并在大规模 GPU 集群的生产环境中节省了数百万 GPU 小时。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19811v3",
      "published_date": "2025-02-27 06:36:45 UTC",
      "updated_date": "2025-03-04 09:54:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:56:29.721827"
    },
    {
      "arxiv_id": "2502.19805v1",
      "title": "Implicit Search via Discrete Diffusion: A Study on Chess",
      "title_zh": "翻译失败",
      "authors": [
        "Jiacheng Ye",
        "Zhenyu Wu",
        "Jiahui Gao",
        "Zhiyong Wu",
        "Xin Jiang",
        "Zhenguo Li",
        "Lingpeng Kong"
      ],
      "abstract": "In the post-AlphaGo era, there has been a renewed interest in search\ntechniques such as Monte Carlo Tree Search (MCTS), particularly in their\napplication to Large Language Models (LLMs). This renewed attention is driven\nby the recognition that current next-token prediction models often lack the\nability for long-term planning. Is it possible to instill search-like abilities\nwithin the models to enhance their planning abilities without relying on\nexplicit search? We propose DiffuSearch , a model that does \\textit{implicit\nsearch} by looking into the future world via discrete diffusion modeling. We\ninstantiate DiffuSearch on a classical board game, Chess, where explicit search\nis known to be essential. Through extensive controlled experiments, we show\nDiffuSearch outperforms both the searchless and explicit search-enhanced\npolicies. Specifically, DiffuSearch outperforms the one-step policy by 19.2%\nand the MCTS-enhanced policy by 14% on action accuracy. Furthermore,\nDiffuSearch demonstrates a notable 30% enhancement in puzzle-solving abilities\ncompared to explicit search-based policies, along with a significant 540 Elo\nincrease in game-playing strength assessment. These results indicate that\nimplicit search via discrete diffusion is a viable alternative to explicit\nsearch over a one-step policy. All codes are publicly available at\n\\href{https://github.com/HKUNLP/DiffuSearch}{https://github.com/HKUNLP/DiffuSearch}.",
      "tldr_zh": "该研究探讨了如何在 Large Language Models (LLMs) 中增强长期规划能力，而不依赖显式搜索技术，如 Monte Carlo Tree Search (MCTS)。论文提出 DiffuSearch 模型，通过 discrete diffusion 建模来实现隐式搜索，在 Chess（国际象棋）游戏中模拟未来场景。实验结果显示，DiffuSearch 在行动准确性上比一步策略提升19.2%，比 MCTS 增强策略提升14%，并在谜题解决能力上提高30%，游戏强度增加540 Elo。这些发现表明，隐式搜索是显式搜索的有效替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19805v1",
      "published_date": "2025-02-27 06:25:15 UTC",
      "updated_date": "2025-02-27 06:25:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:56:42.039030"
    },
    {
      "arxiv_id": "2502.19798v1",
      "title": "Developmental Support Approach to AI's Autonomous Growth: Toward the Realization of a Mutually Beneficial Stage Through Experiential Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Taichiro Endo"
      ],
      "abstract": "This study proposes an \"AI Development Support\" approach that, unlike\nconventional AI Alignment-which aims to forcefully inject human values-supports\nthe ethical and moral development of AI itself. As demonstrated by the\nOrthogonality Thesis, the level of intelligence and the moral quality of a goal\nare independent; merely expanding knowledge does not enhance ethical judgment.\nFurthermore, to address the risk of Instrumental Convergence in ASI-that is,\nthe tendency to engage in subsidiary behaviors such as self-protection,\nresource acquisition, and power reinforcement to achieve a goal-we have\nconstructed a learning framework based on a cycle of experience, introspection,\nanalysis, and hypothesis formation. As a result of post-training using\nSupervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) with\nsynthetic data generated by large language models (LLMs), responses\ndemonstrating cooperative and highly advanced moral judgment (reaching the\nhigh-est Stage 6) were obtained even under adversarial prompts. This method\nrepresents a promising implementation approach for enabling AI to establish\nsustainable, symbiotic relationships.",
      "tldr_zh": "本研究提出了一种“AI 发展支持”方法，与传统 AI Alignment（强制注入人类价值观）不同，它通过支持 AI 自身的伦理和道德发展来实现自治成长。该方法基于 Orthogonality Thesis（智能水平与道德目标无关）和 Instrumental Convergence（ASI 的工具性收敛风险）的洞见，构建了一个以经验、内省、分析和假设形成的循环学习框架。研究使用 Supervised Fine Tuning (SFT) 和 Direct Preference Optimization (DPO) 结合大型语言模型 (LLMs) 生成的合成数据进行后训练，结果显示 AI 即使在对抗性提示下也能表现出高度合作性和高级道德判断（达到 Stage 6）。这一方法为 AI 建立可持续的共生关系提供了可行路径，促进了互利共赢的 AI 发展阶段。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "4pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.19798v1",
      "published_date": "2025-02-27 06:12:20 UTC",
      "updated_date": "2025-02-27 06:12:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:56:52.495804"
    },
    {
      "arxiv_id": "2502.19790v2",
      "title": "Mixtera: A Data Plane for Foundation Model Training",
      "title_zh": "Mixtera: 一种基础模型训练的数据平面",
      "authors": [
        "Maximilian Böther",
        "Xiaozhe Yao",
        "Tolga Kerimoglu",
        "Dan Graur",
        "Viktor Gsteiger",
        "Ana Klimovic"
      ],
      "abstract": "State-of-the-art large language and vision models are trained over trillions\nof tokens that are aggregated from a large variety of sources. As training data\ncollections grow, manually managing the samples becomes time-consuming,\ntedious, and prone to errors. Yet recent research shows that the data mixture\nand the order in which samples are visited during training can significantly\ninfluence model accuracy. We build and present Mixtera, a data plane for\nfoundation model training that enables users to declaratively express which\ndata samples should be used in which proportion and in which order during\ntraining. Mixtera is a centralized, read-only layer that is deployed on top of\nexisting training data collections and can be declaratively queried. It\noperates independently of the filesystem structure and supports mixtures across\narbitrary properties (e.g., language, source dataset) as well as dynamic\nadjustment of the mixture based on model feedback. We experimentally evaluate\nMixtera and show that our implementation does not bottleneck training and\nscales to 256 GH200 superchips. We demonstrate how Mixtera supports recent\nadvancements in mixing strategies by implementing the proposed Adaptive Data\nOptimization (ADO) algorithm in the system and evaluating its performance\nimpact. We also explore the role of mixtures for vision-language models.",
      "tldr_zh": "该论文提出 Mixtera，一种用于基础模型训练的数据平面，帮助用户声明性地管理训练数据样本的比例和顺序，以应对数据混合对模型准确性的影响。Mixtera 作为集中的只读层，独立于文件系统，支持跨任意属性（如语言、来源数据集）的混合，并允许基于模型反馈动态调整数据策略。实验结果表明，Mixtera 不影响训练性能，可扩展到 256 GH200 超芯片，并通过实现 Adaptive Data Optimization (ADO) 算法，展示了其在提升混合策略效果方面的潜力，尤其在视觉语言模型中的应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "under submission",
      "pdf_url": "http://arxiv.org/pdf/2502.19790v2",
      "published_date": "2025-02-27 05:55:44 UTC",
      "updated_date": "2025-04-03 08:29:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:57:05.663679"
    },
    {
      "arxiv_id": "2502.19784v2",
      "title": "NaijaNLP: A Survey of Nigerian Low-Resource Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Isa Inuwa-Dutse"
      ],
      "abstract": "With over 500 languages in Nigeria, three languages -- Hausa, Yor\\`ub\\'a and\nIgbo -- spoken by over 175 million people, account for about 60% of the spoken\nlanguages. However, these languages are categorised as low-resource due to\ninsufficient resources to support tasks in computational linguistics. Several\nresearch efforts and initiatives have been presented, however, a coherent\nunderstanding of the state of Natural Language Processing (NLP) - from\ngrammatical formalisation to linguistic resources that support complex tasks\nsuch as language understanding and generation is lacking. This study presents\nthe first comprehensive review of advancements in low-resource NLP (LR-NLP)\nresearch across the three major Nigerian languages (NaijaNLP). We\nquantitatively assess the available linguistic resources and identify key\nchallenges. Although a growing body of literature addresses various NLP\ndownstream tasks in Hausa, Igbo, and Yor\\`ub\\'a, only about 25.1% of the\nreviewed studies contribute new linguistic resources. This finding highlights a\npersistent reliance on repurposing existing data rather than generating novel,\nhigh-quality resources. Additionally, language-specific challenges, such as the\naccurate representation of diacritics, remain under-explored. To advance\nNaijaNLP and LR-NLP more broadly, we emphasise the need for intensified efforts\nin resource enrichment, comprehensive annotation, and the development of open\ncollaborative initiatives.",
      "tldr_zh": "这篇论文对尼日利亚的低资源语言（Low-Resource Languages）进行了首次全面调查，重点关注Hausa、Yoruba和Igbo三种语言，这些语言虽被超过1.75亿人使用，但因计算语言学资源不足而被归类为低资源。研究者量化评估了现有语言资源，发现仅有25.1%的相关研究贡献了新资源，大部分依赖于现有数据的重用，并指出了语言特定挑战，如diacritics的准确表示尚未充分探索。论文强调未来需加强资源丰富、全面注释以及开放协作，以推进NaijaNLP和更广泛的LR-NLP（Low-Resource NLP）发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages, 2 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.19784v2",
      "published_date": "2025-02-27 05:48:51 UTC",
      "updated_date": "2025-03-06 23:45:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:57:16.332036"
    },
    {
      "arxiv_id": "2502.19779v1",
      "title": "Do Retrieval-Augmented Language Models Adapt to Varying User Needs?",
      "title_zh": "检索增强语言模型是否能适应不同的用户需求？",
      "authors": [
        "Peilin Wu",
        "Xinlu Zhang",
        "Wenhao Yu",
        "Xingyu Liu",
        "Xinya Du",
        "Zhiyu Zoey Chen"
      ],
      "abstract": "Recent advancements in Retrieval-Augmented Language Models (RALMs) have\ndemonstrated their efficacy in knowledge-intensive tasks. However, existing\nevaluation benchmarks often assume a single optimal approach to leveraging\nretrieved information, failing to account for varying user needs. This paper\nintroduces a novel evaluation framework that systematically assesses RALMs\nunder three user need cases-Context-Exclusive, Context-First, and\nMemory-First-across three distinct context settings: Context Matching,\nKnowledge Conflict, and Information Irrelevant. By varying both user\ninstructions and the nature of retrieved information, our approach captures the\ncomplexities of real-world applications where models must adapt to diverse user\nrequirements. Through extensive experiments on multiple QA datasets, including\nHotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find\nthat restricting memory usage improves robustness in adversarial retrieval\nconditions but decreases peak performance with ideal retrieval results and\nmodel family dominates behavioral differences. Our findings highlight the\nnecessity of user-centric evaluations in the development of retrieval-augmented\nsystems and provide insights into optimizing model performance across varied\nretrieval contexts. We will release our code and URAQ dataset upon acceptance\nof the paper.",
      "tldr_zh": "这篇论文探讨了 Retrieval-Augmented Language Models (RALMs) 是否能适应多样化的用户需求，引入了一个新评估框架来解决现有基准的局限性。框架系统评估模型在三种用户需求场景（Context-Exclusive、Context-First 和 Memory-First）以及三种上下文设置（Context Matching、Knowledge Conflict 和 Information Irrelevant）下的表现，通过改变用户指令和检索信息性质模拟真实应用。实验在 HotpotQA、DisentQA 和新构建的 URAQ 数据集上进行，发现限制内存使用提高了模型在对抗性检索条件下的鲁棒性，但降低了理想检索下的峰值性能，且模型家族是行为差异的主要因素。该研究强调了用户中心评估的必要性，并为优化检索增强系统提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19779v1",
      "published_date": "2025-02-27 05:39:38 UTC",
      "updated_date": "2025-02-27 05:39:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:57:30.349213"
    },
    {
      "arxiv_id": "2503.05785v1",
      "title": "Artificial Intelligence in Sports: Insights from a Quantitative Survey among Sports Students in Germany about their Perceptions, Expectations, and Concerns regarding the Use of AI Tools",
      "title_zh": "人工智能在体育：来自德国体育学生关于人工智能工具使用感知、期望和担忧的定量调查见解",
      "authors": [
        "Dennis Krämer",
        "Anja Bosold",
        "Martin Minarik",
        "Cleo Schyvinck",
        "Andre Hajek"
      ],
      "abstract": "Generative Artificial Intelligence (AI) tools such as ChatGPT, Copilot, or\nGemini have a crucial impact on academic research and teaching. Empirical data\non how students perceive the increasing influence of AI, which different types\nof tools they use, what they expect from them in their daily academic tasks,\nand their concerns regarding the use of AI in their studies are still limited.\nThe manuscript presents findings from a quantitative survey conducted among\nsports students of all semesters in Germany using an online questionnaire. It\nexplores aspects such as students' usage behavior, motivational factors, and\nuncertainties regarding the impact of AI tools on academia in the future.\nFurthermore, the social climate in sports studies is being investigated to\nprovide a general overview of the current situation of the students in Germany.\nData collection took place between August and November 2023, addressing all\nsports departments at German universities, with a total of 262 students\nparticipating. Our Findings indicate that students have a strong interest in\nusing AI tools in their studies, expecting them to improve their overall\nacademic performance, understand the complexity of scientific approaches, and\nsave time. They express confidence that the proliferation of AI will not\ncompromise their critical thinking skills. Moreover, students are positive\nabout integrating more AI-related topics into the curriculum and about\nlecturers adopting more AI-based teaching methods. However, our findings also\nshow that students have concerns about plagiarism, lecturer preparedness and\ntheir own skills and future skill development.",
      "tldr_zh": "这篇论文通过对德国体育学生的定量调查（quantitative survey），探讨了他们对 AI 工具（如 ChatGPT 和 Copilot）的感知、期望和担忧。调查涉及 262 名学生，使用在线问卷收集数据，焦点包括 AI 的使用行为、动机因素以及对学术未来的不确定性。结果显示，学生们对 AI 持积极态度，期望它能提升学术表现、简化科学方法理解并节省时间，同时相信不会损害批判性思维（critical thinking）技能。调查还揭示了担忧，如抄袭（plagiarism）、讲师准备不足和自身技能发展问题，为 AI 在教育中的应用提供了重要实证洞见。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "36 Tables, 18 Figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05785v1",
      "published_date": "2025-02-27 05:37:53 UTC",
      "updated_date": "2025-02-27 05:37:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:57:41.436972"
    },
    {
      "arxiv_id": "2503.01888v1",
      "title": "Enhancing Transformer with GNN Structural Knowledge via Distillation: A Novel Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihua Duan",
        "Jialin Wang"
      ],
      "abstract": "Integrating the structural inductive biases of Graph Neural Networks (GNNs)\nwith the global contextual modeling capabilities of Transformers represents a\npivotal challenge in graph representation learning. While GNNs excel at\ncapturing localized topological patterns through message-passing mechanisms,\ntheir inherent limitations in modeling long-range dependencies and\nparallelizability hinder their deployment in large-scale scenarios. Conversely,\nTransformers leverage self-attention mechanisms to achieve global receptive\nfields but struggle to inherit the intrinsic graph structural priors of GNNs.\nThis paper proposes a novel knowledge distillation framework that\nsystematically transfers multiscale structural knowledge from GNN teacher\nmodels to Transformer student models, offering a new perspective on addressing\nthe critical challenges in cross-architectural distillation. The framework\neffectively bridges the architectural gap between GNNs and Transformers through\nmicro-macro distillation losses and multiscale feature alignment. This work\nestablishes a new paradigm for inheriting graph structural biases in\nTransformer architectures, with broad application prospects.",
      "tldr_zh": "本研究探讨了在图表示学习中，如何将Graph Neural Networks (GNNs)的结构偏差与Transformers的全局上下文建模能力相结合，以解决两者间的互补挑战。论文提出了一种新型知识蒸馏框架，通过从GNNs教师模型向Transformers学生模型转移多尺度结构知识，并利用微-macro蒸馏损失和多尺度特征对齐，桥接了架构差距。该框架为Transformers继承图结构先验建立了新范式，具有广泛的应用前景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.01888v1",
      "published_date": "2025-02-27 05:14:47 UTC",
      "updated_date": "2025-02-27 05:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:57:52.274037"
    },
    {
      "arxiv_id": "2502.19771v2",
      "title": "The erasure of intensive livestock farming in text-to-image generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Kehan Sheng",
        "Frank A. M. Tuyttens",
        "Marina A. G. von Keyserlingk"
      ],
      "abstract": "Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily\nlives. While it is known that AI perpetuates biases against marginalized human\ngroups, their impact on non-human animals remains understudied. We found that\nChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward\nromanticizing livestock farming as dairy cows on pasture and pigs rooting in\nmud. This bias remained when we requested realistic depictions and was only\nmitigated when the automatic prompt revision was inhibited. Most farmed animal\nin industrialized countries are reared indoors with limited space per animal,\nwhich fail to resonate with societal values. Inhibiting prompt revision\nresulted in images that more closely reflected modern farming practices; for\nexample, cows housed indoors accessing feed through metal headlocks, and pigs\nbehind metal railings on concrete floors in indoor facilities. While OpenAI\nintroduced prompt revision to mitigate bias, in the case of farmed animal\nproduction systems, it paradoxically introduces a strong bias towards\nunrealistic farming practices.",
      "tldr_zh": "该研究揭示了文本到图像生成AI（如DALL-E 3）在描绘集约化畜牧业时存在的偏见，倾向于浪漫化图像，例如将奶牛描绘成在牧场上而非室内环境。研究者通过测试ChatGPT的生成结果发现，即使要求现实描绘，这种偏见依然存在，仅在抑制自动prompt revision时，图像才更准确地反映现代养殖实践，如奶牛通过金属头锁进食或猪在混凝土地板的金属围栏后。最终，论文指出OpenAI的prompt revision机制虽旨在减少偏见，却反而强化了对不现实畜牧业场景的偏向，强调了AI在非人类动物议题上潜在的负面影响。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19771v2",
      "published_date": "2025-02-27 05:14:04 UTC",
      "updated_date": "2025-03-12 22:35:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:58:04.962169"
    },
    {
      "arxiv_id": "2502.19768v1",
      "title": "Obtaining Example-Based Explanations from Deep Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Genghua Dong",
        "Henrik Boström",
        "Michalis Vazirgiannis",
        "Roman Bresson"
      ],
      "abstract": "Most techniques for explainable machine learning focus on feature\nattribution, i.e., values are assigned to the features such that their sum\nequals the prediction. Example attribution is another form of explanation that\nassigns weights to the training examples, such that their scalar product with\nthe labels equals the prediction. The latter may provide valuable complementary\ninformation to feature attribution, in particular in cases where the features\nare not easily interpretable. Current example-based explanation techniques have\ntargeted a few model types only, such as k-nearest neighbors and random\nforests. In this work, a technique for obtaining example-based explanations\nfrom deep neural networks (EBE-DNN) is proposed. The basic idea is to use the\ndeep neural network to obtain an embedding, which is employed by a k-nearest\nneighbor classifier to form a prediction; the example attribution can hence\nstraightforwardly be derived from the latter. Results from an empirical\ninvestigation show that EBE-DNN can provide highly concentrated example\nattributions, i.e., the predictions can be explained with few training\nexamples, without reducing accuracy compared to the original deep neural\nnetwork. Another important finding from the empirical investigation is that the\nchoice of layer to use for the embeddings may have a large impact on the\nresulting accuracy.",
      "tldr_zh": "这篇论文提出了一种名为 EBE-DNN 的方法，用于从深度神经网络获取基于示例的解释（example attribution），以补充传统的特征归因（feature attribution），尤其适用于特征不易解释的场景。\n该方法的核心是利用深度神经网络生成嵌入（embedding），然后结合 k-nearest neighbor 分类器进行预测，从而直接从分类器导出示例归因权重。\n实验结果表明，EBE-DNN 能提供高度集中的示例归因，即用少量训练样本即可解释预测，同时保持与原深度神经网络相同的准确性；此外，嵌入层选择的差异会对预测准确性产生重大影响。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in the Symposium on Intelligent Data Analysis (IDA)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19768v1",
      "published_date": "2025-02-27 05:10:48 UTC",
      "updated_date": "2025-02-27 05:10:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:58:17.230986"
    },
    {
      "arxiv_id": "2502.19758v1",
      "title": "Learning with Exact Invariances in Polynomial Time",
      "title_zh": "翻译失败",
      "authors": [
        "Ashkan Soleymani",
        "Behrooz Tahmasebi",
        "Stefanie Jegelka",
        "Patrick Jaillet"
      ],
      "abstract": "We study the statistical-computational trade-offs for learning with exact\ninvariances (or symmetries) using kernel regression. Traditional methods, such\nas data augmentation, group averaging, canonicalization, and frame-averaging,\neither fail to provide a polynomial-time solution or are not applicable in the\nkernel setting. However, with oracle access to the geometric properties of the\ninput space, we propose a polynomial-time algorithm that learns a classifier\nwith \\emph{exact} invariances. Moreover, our approach achieves the same excess\npopulation risk (or generalization error) as the original kernel regression\nproblem. To the best of our knowledge, this is the first polynomial-time\nalgorithm to achieve exact (not approximate) invariances in this context. Our\nproof leverages tools from differential geometry, spectral theory, and\noptimization. A key result in our development is a new reformulation of the\nproblem of learning under invariances as optimizing an infinite number of\nlinearly constrained convex quadratic programs, which may be of independent\ninterest.",
      "tldr_zh": "本研究探讨了使用核回归（kernel regression）学习精确不变性（exact invariances）的统计计算权衡，传统方法如数据增强、组平均和规范化要么无法实现多项式时间（polynomial time）解决方案，要么不适用于核设置。作者提出了一种基于对输入空间几何属性的预言机访问的多项式时间算法，能够精确学习不变性的分类器，并与原始核回归问题实现相同的超额总体风险（excess population risk）。该算法是首次在这一背景下达到精确（而非近似）不变性的多项式时间方法，其证明利用了微分几何（differential geometry）、光谱理论（spectral theory）和优化（optimization）工具，并将问题重新表述为优化无限线性约束凸二次程序，这可能具有独立意义。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19758v1",
      "published_date": "2025-02-27 04:49:52 UTC",
      "updated_date": "2025-02-27 04:49:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:58:29.617345"
    },
    {
      "arxiv_id": "2502.19752v1",
      "title": "Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data",
      "title_zh": "翻译失败",
      "authors": [
        "Pei-Yau Weng",
        "Minh Hoang",
        "Lam M. Nguyen",
        "My T. Thai",
        "Tsui-Wei Weng",
        "Trong Nghia Hoang"
      ],
      "abstract": "Fine-tuning pre-trained models is a popular approach in machine learning for\nsolving complex tasks with moderate data. However, fine-tuning the entire\npre-trained model is ineffective in federated data scenarios where local data\ndistributions are diversely skewed. To address this, we explore integrating\nfederated learning with a more effective prompt-tuning method, optimizing for a\nsmall set of input prefixes to reprogram the pre-trained model's behavior. Our\napproach transforms federated learning into a distributed set modeling task,\naggregating diverse sets of prompts to globally fine-tune the pre-trained\nmodel. We benchmark various baselines based on direct adaptations of existing\nfederated model aggregation techniques and introduce a new probabilistic prompt\naggregation method that substantially outperforms these baselines. Our reported\nresults on a variety of computer vision datasets confirm that the proposed\nmethod is most effective to combat extreme data heterogeneity in federated\nlearning.",
      "tldr_zh": "该论文针对联邦学习中非独立同分布（Non-IID）和不平衡数据的问题，提出了一种概率联邦提示调优（Probabilistic Federated Prompt-Tuning）方法，以优化预训练模型的一小套输入前缀，从而避免直接微调整个模型的不效率。方法将联邦学习转化为分布式集合建模任务，通过聚合不同客户端的提示集来实现全局模型微调，并在基准测试中引入新的概率提示聚合技术。实验结果显示，该方法在各种计算机视觉数据集上显著优于现有基线，特别是在处理极端数据异质性方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS-24",
      "pdf_url": "http://arxiv.org/pdf/2502.19752v1",
      "published_date": "2025-02-27 04:31:34 UTC",
      "updated_date": "2025-02-27 04:31:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:58:40.886512"
    },
    {
      "arxiv_id": "2503.01887v1",
      "title": "When Continue Learning Meets Multimodal Large Language Model: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Yukang Huo",
        "Hao Tang"
      ],
      "abstract": "Recent advancements in Artificial Intelligence have led to the development of\nMultimodal Large Language Models (MLLMs). However, adapting these pre-trained\nmodels to dynamic data distributions and various tasks efficiently remains a\nchallenge. Fine-tuning MLLMs for specific tasks often causes performance\ndegradation in the model's prior knowledge domain, a problem known as\n'Catastrophic Forgetting'. While this issue has been well-studied in the\nContinual Learning (CL) community, it presents new challenges for MLLMs. This\nreview paper, the first of its kind in MLLM continual learning, presents an\noverview and analysis of 440 research papers in this area.The review is\nstructured into four sections. First, it discusses the latest research on\nMLLMs, covering model innovations, benchmarks, and applications in various\nfields. Second, it categorizes and overviews the latest studies on continual\nlearning, divided into three parts: non-large language models unimodal\ncontinual learning (Non-LLM Unimodal CL), non-large language models multimodal\ncontinual learning (Non-LLM Multimodal CL), and continual learning in large\nlanguage models (CL in LLM). The third section provides a detailed analysis of\nthe current state of MLLM continual learning research, including benchmark\nevaluations, architectural innovations, and a summary of theoretical and\nempirical studies.Finally, the paper discusses the challenges and future\ndirections of continual learning in MLLMs, aiming to inspire future research\nand development in the field. This review connects the foundational concepts,\ntheoretical insights, method innovations, and practical applications of\ncontinual learning for multimodal large models, providing a comprehensive\nunderstanding of the research progress and challenges in this field, aiming to\ninspire researchers in the field and promote the advancement of related\ntechnologies.",
      "tldr_zh": "这篇综述论文首次探讨了持续学习(Continual Learning)与多模态大语言模型(MLLMs)的结合，分析了440篇相关研究，旨在解决MLLMs在适应动态数据分布和任务时面临的灾难性遗忘(Catastrophic Forgetting)问题。论文将研究分为四部分，包括MLLMs的模型创新、基准和应用；持续学习的分类（如Non-LLM Unimodal CL、Non-LLM Multimodal CL和CL in LLM）；以及MLLMs持续学习的基准评估、架构创新和理论实证总结。最后，它讨论了当前挑战和未来方向，以促进该领域的技术进步和研究创新。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "42 pages, 6 figures, 37 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.01887v1",
      "published_date": "2025-02-27 03:39:10 UTC",
      "updated_date": "2025-02-27 03:39:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:58:53.529378"
    },
    {
      "arxiv_id": "2502.19723v3",
      "title": "CNsum:Automatic Summarization for Chinese News Text",
      "title_zh": "CNsum：中文新闻文本的自动摘要",
      "authors": [
        "Yu Zhao",
        "Songping Huang",
        "Dongsheng Zhou",
        "Zhaoyun Ding",
        "Fei Wang",
        "Aixin Nian"
      ],
      "abstract": "Obtaining valuable information from massive data efficiently has become our\nresearch goal in the era of Big Data. Text summarization technology has been\ncontinuously developed to meet this demand. Recent work has also shown that\ntransformer-based pre-trained language models have achieved great success on\nvarious tasks in Natural Language Processing (NLP). Aiming at the problem of\nChinese news text summary generation and the application of Transformer\nstructure on Chinese, this paper proposes a Chinese news text summarization\nmodel (CNsum) based on Transformer structure, and tests it on Chinese datasets\nsuch as THUCNews. The results of the conducted experiments show that CNsum\nachieves better ROUGE score than the baseline models, which verifies the\noutperformance of the model.",
      "tldr_zh": "本文在大数据时代针对中文新闻文本摘要生成问题，提出了一种基于Transformer结构的模型CNsum，以高效提取有价值信息。CNsum利用Transformer的预训练语言模型优势，针对中文数据集如THUCNews进行测试。实验结果表明，该模型的ROUGE分数比基线模型更高，证明了其在中文文本摘要任务中的优越性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This withdrawal is due to the lack of authorization from all\n  co-authors for the publication of this version",
      "pdf_url": "http://arxiv.org/pdf/2502.19723v3",
      "published_date": "2025-02-27 03:25:34 UTC",
      "updated_date": "2025-03-07 14:56:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:59:04.050554"
    },
    {
      "arxiv_id": "2502.19717v1",
      "title": "Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinran Li",
        "Xiaolu Wang",
        "Chenjia Bai",
        "Jun Zhang"
      ],
      "abstract": "In cooperative multi-agent reinforcement learning (MARL), well-designed\ncommunication protocols can effectively facilitate consensus among agents,\nthereby enhancing task performance. Moreover, in large-scale multi-agent\nsystems commonly found in real-world applications, effective communication\nplays an even more critical role due to the escalated challenge of partial\nobservability compared to smaller-scale setups. In this work, we endeavor to\ndevelop a scalable communication protocol for MARL. Unlike previous methods\nthat focus on selecting optimal pairwise communication links-a task that\nbecomes increasingly complex as the number of agents grows-we adopt a global\nperspective on communication topology design. Specifically, we propose\nutilizing the exponential topology to enable rapid information dissemination\namong agents by leveraging its small-diameter and small-size properties. This\napproach leads to a scalable communication protocol, named ExpoComm. To fully\nunlock the potential of exponential graphs as communication topologies, we\nemploy memory-based message processors and auxiliary tasks to ground messages,\nensuring that they reflect global information and benefit decision-making.\nExtensive experiments on large-scale cooperative benchmarks, including MAgent\nand Infrastructure Management Planning, demonstrate the superior performance\nand robust zero-shot transferability of ExpoComm compared to existing\ncommunication strategies. The code is publicly available at\nhttps://github.com/LXXXXR/ExpoComm.",
      "tldr_zh": "在合作性多智能体强化学习(MARL)中，本文提出了一种可扩展通信协议ExpoComm，利用exponential topology的特性（如小直径和小尺寸）来实现快速信息传播，从而解决大规模系统中的部分可观察性挑战。不同于以往专注于成对通信链接的方法，该协议从全局视角设计拓扑，并结合memory-based message processors和auxiliary tasks，确保消息反映全局信息并优化决策。实验结果显示，ExpoComm在大型基准测试如MAgent和Infrastructure Management Planning上，比现有策略性能更优，并具备robust zero-shot transferability。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted by the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2502.19717v1",
      "published_date": "2025-02-27 03:15:31 UTC",
      "updated_date": "2025-02-27 03:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:59:17.919109"
    },
    {
      "arxiv_id": "2503.05784v1",
      "title": "The Illusion of Rights based AI Regulation",
      "title_zh": "权利基础人工智能监管的幻觉",
      "authors": [
        "Yiyang Mei",
        "Matthew Sag"
      ],
      "abstract": "Whether and how to regulate AI is one of the defining questions of our times\n- a question that is being debated locally, nationally, and internationally. We\nargue that much of this debate is proceeding on a false premise. Specifically,\nour article challenges the prevailing academic consensus that the European\nUnion's AI regulatory framework is fundamentally rights-driven and the\ncorrelative presumption that other rights-regarding nations should therefore\nfollow Europe's lead in AI regulation. Rather than taking rights language in EU\nrules and regulations at face value, we show how EU AI regulation is the\nlogical outgrowth of a particular cultural, political, and historical context.\nWe show that although instruments like the General Data Protection Regulation\n(GDPR) and the AI Act invoke the language of fundamental rights, these rights\nare instrumentalized - used as rhetorical cover for governance tools that\naddress systemic risks and maintain institutional stability. As such, we reject\nclaims that the EU's regulatory framework and the substance of its rules should\nbe adopted as universal imperatives and transplanted to other liberal\ndemocracies. To add weight to our argument from historical context, we conduct\na comparative analysis of AI regulation in five contested domains: data\nprivacy, cybersecurity, healthcare, labor, and misinformation. This EU-US\ncomparison shows that the EU's regulatory architecture is not meaningfully\nrights-based. Our article's key intervention in AI policy debates is not to\nsuggest that the current American regulatory model is necessarily preferable\nbut that the presumed legitimacy of the EU's AI regulatory approach must be\nabandoned.",
      "tldr_zh": "这篇论文质疑了欧盟AI监管框架被视为以权利为基础的普遍共识，论证其权利语言（如GDPR和AI Act）实际是工具化的手段，用于处理系统风险和维护机构稳定，而不是真正的权利保护。作者通过历史背景分析和欧盟与美国的比较研究（涉及数据隐私、网络安全、医疗、劳动力和错误信息等领域），揭示欧盟框架源于特定文化、政治背景，并非全球适用的模式。论文的关键贡献在于呼吁放弃欧盟AI监管方法的假定合法性，避免将其移植到其他自由民主国家，而非主张美国的模式更优。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05784v1",
      "published_date": "2025-02-27 03:05:32 UTC",
      "updated_date": "2025-02-27 03:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:59:29.803116"
    },
    {
      "arxiv_id": "2502.19701v1",
      "title": "Extending the Hegselmann-Krause Model of Opinion Dynamics to include AI Oracles",
      "title_zh": "翻译失败",
      "authors": [
        "Allen G. Rodrigo"
      ],
      "abstract": "The Hegselmann-Krause (HK) model of opinion dynamics describes how opinions\nheld by individuals in a community change over time in response to the opinions\nof others and their access to the true value, T, to which these opinions\nrelate. Here, I extend the simple HK model to incorporate an Artificially\nIntelligent (AI) Oracle that averages the opinions of members of the community.\nAgent-based simulations show that (1) if individuals only have access to the\nOracle (and not T), and incorporate the Oracle's opinion as they update their\nopinions, then all opinions will converge on a common value; (2) in contrast,\nif all individuals also have access to T, then all opinions will ultimately\nconverge to T, but the presence of an Oracle may delay the time to convergence;\n(3) if only some individuals have access to T, opinions may not converge to T,\nbut under certain conditions, universal access to the Oracle will guarantee\nconvergence to T; and (4) whether or not the Oracle only accesses the opinions\nof individuals who have access to T, or whether it accesses the opinions of\neveryone in the community, makes no marked difference to the extent to which\nthe average opinion differs from T.",
      "tldr_zh": "这篇论文扩展了 Hegselmann-Krause 模型，将 AI Oracle 纳入意见动态模拟中，AI Oracle 通过平均社区成员的意见来影响个体决策。研究采用代理人模拟（Agent-based simulations）方法，探讨不同访问场景下意见的演变。结果显示，如果个体仅访问 AI Oracle 而非真实值 T，所有意见会收敛到一个共同值；如果所有个体能访问 T，意见最终会收敛到 T，但 AI Oracle 的存在可能延迟收敛时间。如果只有部分个体访问 T，意见可能不收敛到 T，但在某些条件下，普遍访问 AI Oracle 可保证收敛到 T。总体而言，AI Oracle 的访问范围对平均意见与 T 的差异影响不大。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19701v1",
      "published_date": "2025-02-27 02:37:04 UTC",
      "updated_date": "2025-02-27 02:37:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:59:43.378503"
    },
    {
      "arxiv_id": "2503.00060v1",
      "title": "SAC-ViT: Semantic-Aware Clustering Vision Transformer with Early Exit",
      "title_zh": "翻译失败",
      "authors": [
        "Youbing Hu",
        "Yun Cheng",
        "Anqi Lu",
        "Dawei Wei",
        "Zhijun Li"
      ],
      "abstract": "The Vision Transformer (ViT) excels in global modeling but faces deployment\nchallenges on resource-constrained devices due to the quadratic computational\ncomplexity of its attention mechanism. To address this, we propose the\nSemantic-Aware Clustering Vision Transformer (SAC-ViT), a non-iterative\napproach to enhance ViT's computational efficiency. SAC-ViT operates in two\nstages: Early Exit (EE) and Semantic-Aware Clustering (SAC). In the EE stage,\ndownsampled input images are processed to extract global semantic information\nand generate initial inference results. If these results do not meet the EE\ntermination criteria, the information is clustered into target and non-target\ntokens. In the SAC stage, target tokens are mapped back to the original image,\ncropped, and embedded. These target tokens are then combined with reused\nnon-target tokens from the EE stage, and the attention mechanism is applied\nwithin each cluster. This two-stage design, with end-to-end optimization,\nreduces spatial redundancy and enhances computational efficiency, significantly\nboosting overall ViT performance. Extensive experiments demonstrate the\nefficacy of SAC-ViT, reducing 62% of the FLOPs of DeiT and achieving 1.98 times\nthroughput without compromising performance.",
      "tldr_zh": "该研究针对 Vision Transformer (ViT) 的注意力机制二次方计算复杂度问题，提出了一种非迭代的 Semantic-Aware Clustering Vision Transformer (SAC-ViT)，通过 Early Exit (EE) 和 Semantic-Aware Clustering (SAC) 两个阶段提升计算效率。在 EE 阶段，对下采样图像提取全局语义信息并生成初步推理结果；若未达终止标准，则在 SAC 阶段聚类 tokens，将目标 tokens 映射回原图并结合非目标 tokens 进行局部注意力计算。该方法通过端到端优化减少空间冗余，实验结果显示，SAC-ViT 减少了 DeiT 的 FLOPs 62%，提高了 1.98 倍吞吐量，同时保持了原有性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00060v1",
      "published_date": "2025-02-27 02:24:22 UTC",
      "updated_date": "2025-02-27 02:24:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T19:59:56.199804"
    },
    {
      "arxiv_id": "2502.19694v2",
      "title": "BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Ye",
        "Burhaneddin Yaman",
        "Sheng Cheng",
        "Feng Tao",
        "Abhirup Mallik",
        "Liu Ren"
      ],
      "abstract": "Bird's-eye-view (BEV) representations play a crucial role in autonomous\ndriving tasks. Despite recent advancements in BEV generation, inherent noise,\nstemming from sensor limitations and the learning process, remains largely\nunaddressed, resulting in suboptimal BEV representations that adversely impact\nthe performance of downstream tasks. To address this, we propose BEVDiffuser, a\nnovel diffusion model that effectively denoises BEV feature maps using the\nground-truth object layout as guidance. BEVDiffuser can be operated in a\nplug-and-play manner during training time to enhance existing BEV models\nwithout requiring any architectural modifications. Extensive experiments on the\nchallenging nuScenes dataset demonstrate BEVDiffuser's exceptional denoising\nand generation capabilities, which enable significant enhancement to existing\nBEV models, as evidenced by notable improvements of 12.3\\% in mAP and 10.1\\% in\nNDS achieved for 3D object detection without introducing additional\ncomputational complexity. Moreover, substantial improvements in long-tail\nobject detection and under challenging weather and lighting conditions further\nvalidate BEVDiffuser's effectiveness in denoising and enhancing BEV\nrepresentations.",
      "tldr_zh": "本研究针对自动驾驶中 Bird's-eye-view (BEV) 表示的噪声问题，提出了一种新型 diffusion model 名为 BEVDiffuser，利用 ground-truth object layout 作为指导进行 BEV 特征图去噪。该模型采用 plug-and-play 方式，无需修改现有 BEV 模型的架构，即可提升其性能。在 nuScenes 数据集上的广泛实验显示，BEVDiffuser 显著提高了 3D 对象检测的 mAP 12.3% 和 NDS 10.1%，并在长尾对象检测以及恶劣天气和光照条件下表现出色，从而增强了 BEV 表示的鲁棒性和整体效果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19694v2",
      "published_date": "2025-02-27 02:11:29 UTC",
      "updated_date": "2025-03-24 22:27:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:00:06.824189"
    },
    {
      "arxiv_id": "2502.19693v1",
      "title": "Accurate and Scalable Graph Neural Networks via Message Invariance",
      "title_zh": "准确且可扩展的图",
      "authors": [
        "Zhihao Shi",
        "Jie Wang",
        "Zhiwei Zhuang",
        "Xize Liang",
        "Bin Li",
        "Feng Wu"
      ],
      "abstract": "Message passing-based graph neural networks (GNNs) have achieved great\nsuccess in many real-world applications. For a sampled mini-batch of target\nnodes, the message passing process is divided into two parts: message passing\nbetween nodes within the batch (MP-IB) and message passing from nodes outside\nthe batch to those within it (MP-OB). However, MP-OB recursively relies on\nhigher-order out-of-batch neighbors, leading to an exponentially growing\ncomputational cost with respect to the number of layers. Due to the neighbor\nexplosion, the whole message passing stores most nodes and edges on the GPU\nsuch that many GNNs are infeasible to large-scale graphs. To address this\nchallenge, we propose an accurate and fast mini-batch approach for large graph\ntransductive learning, namely topological compensation (TOP), which obtains the\noutputs of the whole message passing solely through MP-IB, without the costly\nMP-OB. The major pillar of TOP is a novel concept of message invariance, which\ndefines message-invariant transformations to convert costly MP-OB into fast\nMP-IB. This ensures that the modified MP-IB has the same output as the whole\nmessage passing. Experiments demonstrate that TOP is significantly faster than\nexisting mini-batch methods by order of magnitude on vast graphs (millions of\nnodes and billions of edges) with limited accuracy degradation.",
      "tldr_zh": "这篇论文针对基于消息传递的图神经网络(GNNs)在处理大规模图时的计算效率问题，提出了一种拓扑补偿(TOP)方法，以实现准确且可扩展的 mini-batch 训练。TOP 利用消息不变性(message invariance)概念，将昂贵的外部消息传递(MP-OB)转换为快速的内部消息传递(MP-IB)，从而无需递归依赖高阶外部节点即可获得完整的消息传递输出。实验结果显示，在拥有数百万节点和数十亿边的图上，TOP 比现有方法快几个数量级，同时准确性损失有限。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19693v1",
      "published_date": "2025-02-27 02:07:00 UTC",
      "updated_date": "2025-02-27 02:07:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:00:18.919059"
    },
    {
      "arxiv_id": "2502.19691v2",
      "title": "Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set Annotation: An Energy-Based Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Chen-Chen Zong",
        "Sheng-Jun Huang"
      ],
      "abstract": "Active learning (AL), which iteratively queries the most informative examples\nfrom a large pool of unlabeled candidates for model training, faces significant\nchallenges in the presence of open-set classes. Existing methods either\nprioritize query examples likely to belong to known classes, indicating low\nepistemic uncertainty (EU), or focus on querying those with highly uncertain\npredictions, reflecting high aleatoric uncertainty (AU). However, they both\nyield suboptimal performance, as low EU corresponds to limited useful\ninformation, and closed-set AU metrics for unknown class examples are less\nmeaningful. In this paper, we propose an Energy-based Active Open-set\nAnnotation (EAOA) framework, which effectively integrates EU and AU to achieve\nsuperior performance. EAOA features a $(C+1)$-class detector and a target\nclassifier, incorporating an energy-based EU measure and a margin-based energy\nloss designed for the detector, alongside an energy-based AU measure for the\ntarget classifier. Another crucial component is the target-driven adaptive\nsampling strategy. It first forms a smaller candidate set with low EU scores to\nensure closed-set properties, making AU metrics meaningful. Subsequently,\nexamples with high AU scores are queried to form the final query set, with the\ncandidate set size adjusted adaptively. Extensive experiments show that EAOA\nachieves state-of-the-art performance while maintaining high query precision\nand low training overhead. The code is available at\nhttps://github.com/chenchenzong/EAOA.",
      "tldr_zh": "本研究重新审视了主动学习(Active Learning)中 epistemic uncertainty (EU) 和 aleatoric uncertainty (AU) 在开放集(open-set)标注中的作用，指出现有方法在处理未知类样本时效果不佳。作者提出 Energy-based Active Open-set Annotation (EAOA) 框架，该框架整合了基于能量的 EU 测量和 margin-based energy loss 用于 (C+1)-class detector，以及基于能量的 AU 测量用于 target classifier，并引入 target-driven adaptive sampling 策略来优化查询样本选择。实验结果显示，EAOA 实现了最先进性能，同时保持高查询精度和低训练开销，为开放集场景下的主动学习提供了高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.19691v2",
      "published_date": "2025-02-27 02:02:58 UTC",
      "updated_date": "2025-03-14 11:32:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:00:30.784727"
    },
    {
      "arxiv_id": "2502.19690v1",
      "title": "Risk-aware Integrated Task and Motion Planning for Versatile Snake Robots under Localization Failures",
      "title_zh": "风险感知的整合任务与运动规划，用于多功能蛇形机器人面对定位失败",
      "authors": [
        "Ashkan Jasour",
        "Guglielmo Daddi",
        "Masafumi Endo",
        "Tiago S. Vaquero",
        "Michael Paton",
        "Marlin P. Strub",
        "Sabrina Corpino",
        "Michel Ingham",
        "Masahiro Ono",
        "Rohan Thakker"
      ],
      "abstract": "Snake robots enable mobility through extreme terrains and confined\nenvironments in terrestrial and space applications. However, robust perception\nand localization for snake robots remain an open challenge due to the proximity\nof the sensor payload to the ground coupled with a limited field of view. To\naddress this issue, we propose Blind-motion with Intermittently Scheduled Scans\n(BLISS) which combines proprioception-only mobility with intermittent scans to\nbe resilient against both localization failures and collision risks. BLISS is\nformulated as an integrated Task and Motion Planning (TAMP) problem that leads\nto a Chance-Constrained Hybrid Partially Observable Markov Decision Process\n(CC-HPOMDP), known to be computationally intractable due to the curse of\nhistory. Our novelty lies in reformulating CC-HPOMDP as a tractable, convex\nMixed Integer Linear Program. This allows us to solve BLISS-TAMP significantly\nfaster and jointly derive optimal task-motion plans. Simulations and hardware\nexperiments on the EELS snake robot show our method achieves over an order of\nmagnitude computational improvement compared to state-of-the-art POMDP planners\nand $>$ 50\\% better navigation time optimality versus classical two-stage\nplanners.",
      "tldr_zh": "本研究针对蛇形机器人在极端地形中的定位失败问题，提出了一种风险感知的集成任务和运动规划（Integrated Task and Motion Planning, TAMP）方法，名为 BLISS。它结合本体感知（proprioception-only mobility）和间歇扫描，构建为 Chance-Constrained Hybrid Partially Observable Markov Decision Process (CC-HPOMDP)，以提升对定位失败和碰撞风险的鲁棒性。创新在于将 CC-HPOMDP 重新表述为可计算的凸 Mixed Integer Linear Program，从而显著加速规划过程。实验在 EELS 蛇形机器人上显示，该方法比现有 POMDP 规划器计算效率提高一个数量级，并比经典两阶段规划器改善超过 50% 的导航时间优化。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2.8; I.2.9"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 9 figures. Accepted article with supplemental material for\n  presentation at the 2025 IEEE International Conference on Robotics and\n  Automation (ICRA)",
      "pdf_url": "http://arxiv.org/pdf/2502.19690v1",
      "published_date": "2025-02-27 02:02:51 UTC",
      "updated_date": "2025-02-27 02:02:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:00:42.860192"
    },
    {
      "arxiv_id": "2503.04784v3",
      "title": "KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction Under TransformerX Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Li",
        "Jiexiong Liu",
        "Yixuan Chen",
        "Yanqin Jia",
        "Zhepeng Li"
      ],
      "abstract": "Large language models have demonstrated remarkable performance across various\ntasks, yet they face challenges such as low computational efficiency, gradient\nvanishing, and difficulties in capturing complex feature interactions. To\naddress these limitations, a novel framework has been proposed. This framework\nincorporates a learnable dense residual skip connection mechanism, a\nTransformerX module a transformer based component integrating multiscale\nconvolution and adaptive activation functions and a multitoken prediction\ninteraction module. The learnable dense residual connections enhance\ninformation flow and feature capture across layers. Within the TransformerX\nmodule, large convolutional kernels aggregate semantic information from\nextensive text segments, while smaller convolutions focus on local word order\nand syntactic structures. The adaptive activation function dynamically adjusts\nits parameters based on the semantic features of the input text, improving the\nmodel's ability to handle diverse semantic expressions and complex\nrelationships. The multitoken prediction module boosts data utilization and\naccelerates inference by predicting multiple future tokens. These components\nsignificantly enhance the performance and efficiency of large language models.",
      "tldr_zh": "这篇论文针对大语言模型(LLMs)的低计算效率、梯度消失和复杂特征交互难题，提出KunlunBaize框架，该框架基于TransformerX整合多尺度卷积和多标记预测机制。框架的关键组件包括可学习的密集残差跳跃连接以增强信息流、TransformerX模块（使用大卷积核聚合语义信息、小卷积核关注局部词序和句法结构，以及自适应激活函数动态调整参数）、以及多标记预测模块以提高数据利用和推理速度。这些创新显著提升了LLMs的性能和效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.04784v3",
      "published_date": "2025-02-27 01:56:09 UTC",
      "updated_date": "2025-03-20 03:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:00:56.456207"
    },
    {
      "arxiv_id": "2502.19680v2",
      "title": "M-LLM Based Video Frame Selection for Efficient Video Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Hu",
        "Feng Gao",
        "Xiaohan Nie",
        "Peng Zhou",
        "Son Tran",
        "Tal Neiman",
        "Lingyun Wang",
        "Mubarak Shah",
        "Raffay Hamid",
        "Bing Yin",
        "Trishul Chilimbi"
      ],
      "abstract": "Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising\nresults in video reasoning. Popular Multi-Modal Large Language Model (M-LLM)\nframeworks usually apply naive uniform sampling to reduce the number of video\nframes that are fed into an M-LLM, particularly for long context videos.\nHowever, it could lose crucial context in certain periods of a video, so that\nthe downstream M-LLM may not have sufficient visual information to answer a\nquestion. To attack this pain point, we propose a light-weight M-LLM -based\nframe selection method that adaptively select frames that are more relevant to\nusers' queries. In order to train the proposed frame selector, we introduce two\nsupervision signals (i) Spatial signal, where single frame importance score by\nprompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by\nprompting Large Language Model (LLM) using the captions of all frame\ncandidates. The selected frames are then digested by a frozen downstream video\nM-LLM for visual reasoning and question answering. Empirical results show that\nthe proposed M-LLM video frame selector improves the performances various\ndownstream video Large Language Model (video-LLM) across medium (ActivityNet,\nNExT-QA) and long (EgoSchema, LongVideoBench) context video question answering\nbenchmarks.",
      "tldr_zh": "该论文提出了一种基于 M-LLM 的视频帧选择方法，以提升视频理解效率，解决传统均匀采样可能丢失关键上下文的问题。该方法使用轻量级 M-LLM 框架，通过空间信号（提示 M-LLM 评估单个帧的重要性）和时间信号（提示 LLM 基于帧标题选择多个帧）来适应性地选取与用户查询相关的帧。实验结果表明，该方法显著提高了下游 video-LLM 在 ActivityNet、NExT-QA、EgoSchema 和 LongVideoBench 等基准上的性能，为视频问答任务提供了更有效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19680v2",
      "published_date": "2025-02-27 01:44:13 UTC",
      "updated_date": "2025-03-26 21:14:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:01:09.777141"
    },
    {
      "arxiv_id": "2502.19668v1",
      "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mingsheng Cai",
        "Jiuming Jiang",
        "Wenhao Huang",
        "Che Liu",
        "Rossella Arcucci"
      ],
      "abstract": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and\nmonitoring cardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME applies\nLarge Language Models (LLMs) to extract structured clinical entities from\nfree-text ECG reports, filter out noise and irrelevant content, enhance\nclinical representation learning, and build a high-quality, fine-grained\nlabeled dataset. By using text-based cardiac queries instead of traditional\ncategorical labels, SuPreME enables zero-shot classification of unseen diseases\nwithout additional fine-tuning. We evaluate SuPreME on six downstream datasets\ncovering 127 cardiac conditions, achieving superior zero-shot AUC performance\nover state-of-the-art eSSL and multimodal methods by over 1.96\\%. Results\ndemonstrate the effectiveness of SuPreME in leveraging structured, clinically\nrelevant knowledge for high-quality ECG representations. All code and data will\nbe released upon acceptance.",
      "tldr_zh": "该论文提出 SuPreME，一种监督预训练框架，用于多模态 ECG 表示学习，以解决传统自监督学习方法在捕捉细粒度临床语义和任务微调方面的局限性。该框架利用 Large Language Models (LLMs) 从自由文本 ECG 报告中提取结构化临床实体、过滤噪声，并构建高质量标注数据集，从而通过文本-based 心脏查询实现零样本分类，而无需额外微调。在六个下游数据集上，涵盖 127 种心脏疾病，SuPreME 的零样本 AUC 性能比现有方法提升超过 1.96%，证明其在利用临床知识提升 ECG 表示学习方面的有效性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19668v1",
      "published_date": "2025-02-27 01:29:51 UTC",
      "updated_date": "2025-02-27 01:29:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:01:19.245087"
    },
    {
      "arxiv_id": "2502.19662v2",
      "title": "HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration",
      "title_zh": "翻译失败",
      "authors": [
        "Rohan Juneja",
        "Shivam Aggarwal",
        "Safeen Huda",
        "Tulika Mitra",
        "Li-Shiuan Peh"
      ],
      "abstract": "Quantization is critical for efficiently deploying large language models\n(LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width\nconstraints, and do not account for intrinsic circuit characteristics such as\nthe timing behaviors and energy profiles of Multiply-Accumulate (MAC) units.\nThis disconnect from circuit-level behavior limits the ability to exploit\navailable timing margins and energy-saving opportunities, reducing the overall\nefficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for\nHardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods,\nHALO explicitly incorporates detailed hardware characteristics, including\ncritical-path timing and power consumption, into its quantization approach.\nHALO strategically selects weights with low critical-path-delays enabling\nhigher operational frequencies and dynamic frequency scaling without disrupting\nthe architecture's dataflow. Remarkably, HALO achieves these improvements with\nonly a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring\nsimplicity and practicality in deployment. Additionally, by reducing switching\nactivity within the MAC units, HALO effectively lowers energy consumption.\nEvaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics\nProcessing Units (GPUs) demonstrate that HALO significantly enhances inference\nefficiency, achieving average performance improvements of 270% and energy\nsavings of 51% over baseline quantization methods, all with minimal impact on\naccuracy.",
      "tldr_zh": "该研究提出 HALO 框架，一种硬件感知的后训练量化(PTQ)方法，用于提升大型语言模型(LLM)的加速效率，通过整合关键路径时序和功耗特性来解决传统量化方法的局限性。HALO 策略性地选择低关键路径延迟的权重，支持动态电压频率缩放(DVFS)，并减少 Multiply-Accumulate (MAC) 单元的开关活动，从而实现更高的操作频率和能量节省。在 TPU 和 GPU 等加速器上，实验显示 HALO 比基线方法平均性能提升 270%、能量节省 51%，同时对准确性影响最小。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19662v2",
      "published_date": "2025-02-27 01:08:33 UTC",
      "updated_date": "2025-04-25 09:54:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:01:33.273175"
    },
    {
      "arxiv_id": "2502.19655v1",
      "title": "Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Tristan Naumann",
        "Hoifung Poon"
      ],
      "abstract": "Reinforcement learning from verifiable rewards (RLVR) has recently gained\nattention for its ability to elicit self-evolved reasoning capabilitie from\nbase language models without explicit reasoning supervisions, as demonstrated\nby DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical\nand coding domains, its applicability to other tasks and domains remains\nunexplored. In this work, we investigate whether medical reasoning can emerge\nfrom RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical\ndomain leveraging medical multiple-choice question answering (MCQA) data as\nverifiable labels. Our results demonstrate that RLVR is not only effective for\nmath and coding but also extends successfully to medical question answering.\nNotably, Med-RLVR achieves performance comparable to traditional supervised\nfine-tuning (SFT) on in-distribution tasks while significantly improving\nout-of-distribution generalization, with an 8-point accuracy gain. Further\nanalysis of training dynamics reveals that, with no explicit reasoning\nsupervision, reasoning emerges from the 3B-parameter base model. These findings\nunderscore the potential of RLVR in domains beyond math and coding, opening new\navenues for its application in knowledge-intensive fields such as medicine.",
      "tldr_zh": "该研究引入了 Med-RLVR，一种基于强化学习从可验证奖励（RLVR）的框架，旨在从一个 3B 参数的基语言模型中激发医疗推理能力，而无需显式推理监督。研究使用医疗多选题答题（MCQA）数据作为可验证标签，证明 RLVR 不仅适用于数学和编码领域，还能成功扩展到医疗问答。结果显示，Med-RLVR 在分布内任务上与传统监督微调（SFT）性能相当，但在分布外泛化能力上提升了 8 点准确率，进一步揭示了基模型在无监督条件下自发出现推理潜力，为 RLVR 在知识密集型领域如医学的应用开辟新路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19655v1",
      "published_date": "2025-02-27 00:54:38 UTC",
      "updated_date": "2025-02-27 00:54:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:01:44.046699"
    },
    {
      "arxiv_id": "2502.19652v1",
      "title": "Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning",
      "title_zh": "Robust Gymnasium：用于鲁棒强化学习的统一模块化基准",
      "authors": [
        "Shangding Gu",
        "Laixi Shi",
        "Muning Wen",
        "Ming Jin",
        "Eric Mazumdar",
        "Yuejie Chi",
        "Adam Wierman",
        "Costas Spanos"
      ],
      "abstract": "Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement\nlearning (RL) seeks to improve resilience against the complexity and\nvariability in agent-environment sequential interactions. Despite the existence\nof a large number of RL benchmarks, there is a lack of standardized benchmarks\nfor robust RL. Current robust RL policies often focus on a specific type of\nuncertainty and are evaluated in distinct, one-off environments. In this work,\nwe introduce Robust-Gymnasium, a unified modular benchmark designed for robust\nRL that supports a wide variety of disruptions across all key RL\ncomponents-agents' observed state and reward, agents' actions, and the\nenvironment. Offering over sixty diverse task environments spanning control and\nrobotics, safe RL, and multi-agent RL, it provides an open-source and\nuser-friendly tool for the community to assess current methods and foster the\ndevelopment of robust RL algorithms. In addition, we benchmark existing\nstandard and robust RL algorithms within this framework, uncovering significant\ndeficiencies in each and offering new insights.",
      "tldr_zh": "该论文针对强化学习（RL）的固有不确定性和 sim-to-real gap，指出现有基准缺乏标准化问题，提出 Robust-Gymnasium 作为统一模块化基准，用于评估 robust RL 算法。该基准支持多种干扰，包括代理的观察状态、奖励、动作和环境，涵盖超过六十个任务环境，如控制、机器人、safe RL 和多代理 RL。通过实验基准测试现有标准和 robust RL 算法，论文揭示了这些算法的显著缺陷，并提供了新的见解，为 robust RL 的发展提供了开放工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19652v1",
      "published_date": "2025-02-27 00:50:25 UTC",
      "updated_date": "2025-02-27 00:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:01:55.386394"
    },
    {
      "arxiv_id": "2502.19647v2",
      "title": "AutoBS: Autonomous Base Station Deployment with Reinforcement Learning and Digital Network Twins",
      "title_zh": "AutoBS：利用强化学习和数字网络孪生的自主基站部署",
      "authors": [
        "Ju-Hyung Lee",
        "Andreas F. Molisch"
      ],
      "abstract": "This paper introduces AutoBS, a reinforcement learning (RL)-based framework\nfor optimal base station (BS) deployment in 6G radio access networks (RAN).\nAutoBS leverages the Proximal Policy Optimization (PPO) algorithm and fast,\nsite-specific pathloss predictions from PMNet-a generative model for digital\nnetwork twins (DNT). By efficiently learning deployment strategies that balance\ncoverage and capacity, AutoBS achieves about 95% of the capacity of exhaustive\nsearch in single BS scenarios (and in 90% for multiple BSs), while cutting\ninference time from hours to milliseconds, making it highly suitable for\nreal-time applications (e.g., ad-hoc deployments). AutoBS therefore provides a\nscalable, automated solution for large-scale 6G networks, meeting the demands\nof dynamic environments with minimal computational overhead.",
      "tldr_zh": "这篇论文提出了 AutoBS，一种基于 Reinforcement Learning (RL) 的框架，用于在 6G 无线接入网络 (RAN) 中实现自治基站 (BS) 部署。AutoBS 利用 Proximal Policy Optimization (PPO) 算法和 PMNet 模型（用于 Digital Network Twins (DNT) 的生成工具）进行快速、特定场地的路径损耗预测，从而平衡覆盖和容量需求。实验结果显示，该框架在单 BS 场景中达到详尽搜索容量的 95%，在多 BS 场景中达到 90%，并将推理时间从小时缩短到毫秒级别，提供可扩展的自动化解决方案，适用于动态 6G 网络环境。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "cs.NI",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "Title changed to better reflect content",
      "pdf_url": "http://arxiv.org/pdf/2502.19647v2",
      "published_date": "2025-02-27 00:32:44 UTC",
      "updated_date": "2025-05-19 06:59:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:02:08.426431"
    },
    {
      "arxiv_id": "2502.19645v2",
      "title": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success",
      "title_zh": "微调视觉-语言-动作模型：优化速度和成功率",
      "authors": [
        "Moo Jin Kim",
        "Chelsea Finn",
        "Percy Liang"
      ],
      "abstract": "Recent vision-language-action models (VLAs) build upon pretrained\nvision-language models and leverage diverse robot datasets to demonstrate\nstrong task execution, language following ability, and semantic generalization.\nDespite these successes, VLAs struggle with novel robot setups and require\nfine-tuning to achieve good performance, yet how to most effectively fine-tune\nthem is unclear given many possible strategies. In this work, we study key VLA\nadaptation design choices such as different action decoding schemes, action\nrepresentations, and learning objectives for fine-tuning, using OpenVLA as our\nrepresentative base model. Our empirical analysis informs an Optimized\nFine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a\ncontinuous action representation, and a simple L1 regression-based learning\nobjective to altogether improve inference efficiency, policy performance, and\nflexibility in the model's input-output specifications. We propose OpenVLA-OFT,\nan instantiation of this recipe, which sets a new state of the art on the\nLIBERO simulation benchmark, significantly boosting OpenVLA's average success\nrate across four task suites from 76.5% to 97.1% while increasing action\ngeneration throughput by 26$\\times$. In real-world evaluations, our fine-tuning\nrecipe enables OpenVLA to successfully execute dexterous, high-frequency\ncontrol tasks on a bimanual ALOHA robot and outperform other VLAs ($\\pi_0$ and\nRDT-1B) fine-tuned using their default recipes, as well as strong imitation\nlearning policies trained from scratch (Diffusion Policy and ACT) by up to 15%\n(absolute) in average success rate. We release code for OFT and pretrained\nmodel checkpoints at https://openvla-oft.github.io/.",
      "tldr_zh": "该论文探讨了视觉-语言-动作模型 (VLAs) 在新机器人设置下的微调策略，以优化模型的速度和成功率，通过分析动作解码方案、动作表示和学习目标等关键设计选择。研究提出了一种 Optimized Fine-Tuning (OFT) 配方，包括并行解码、动作分块、连续动作表示以及 L1 回归学习目标，从而提升推理效率、策略性能和输入输出灵活性。基于 OpenVLA 的 OpenVLA-OFT 模型在 LIBERO 模拟基准上将平均成功率从 76.5% 提高到 97.1%，并将动作生成吞吐量提升 26 倍；在真实世界测试中，它在双臂 ALOHA 机器人上执行灵巧高频任务，成功率比其他微调模型（如 π_0 和 RDT-1B）高出最多 15%。作者开源了 OFT 代码和预训练模型，以促进进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to Robotics: Science and Systems (RSS) 2025. Project\n  website: https://openvla-oft.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2502.19645v2",
      "published_date": "2025-02-27 00:30:29 UTC",
      "updated_date": "2025-04-28 07:49:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:02:22.313000"
    },
    {
      "arxiv_id": "2503.01886v1",
      "title": "Advanced Deep Learning Techniques for Analyzing Earnings Call Transcripts: Methodologies and Applications",
      "title_zh": "先进的深度学习技术用于分析财报电话会议记录：方法论和应用",
      "authors": [
        "Umair Zakir",
        "Evan Daykin",
        "Amssatou Diagne",
        "Jacob Faile"
      ],
      "abstract": "This study presents a comparative analysis of deep learning methodologies\nsuch as BERT, FinBERT and ULMFiT for sentiment analysis of earnings call\ntranscripts. The objective is to investigate how Natural Language Processing\n(NLP) can be leveraged to extract sentiment from large-scale financial\ntranscripts, thereby aiding in more informed investment decisions and risk\nmanagement strategies. We examine the strengths and limitations of each model\nin the context of financial sentiment analysis, focusing on data preprocessing\nrequirements, computational efficiency, and model optimization. Through\nrigorous experimentation, we evaluate their performance using key metrics,\nincluding accuracy, precision, recall, and F1-score. Furthermore, we discuss\npotential enhancements to improve the effectiveness of these models in\nfinancial text analysis, providing insights into their applicability for\nreal-world financial decision-making.",
      "tldr_zh": "本研究比较了BERT、FinBERT和ULMFiT等深度学习方法在收益电话会议记录的sentiment analysis中的应用，旨在通过NLP技术从大规模财务文本中提取情感，以支持投资决策和风险管理。论文评估了每个模型的优势和局限性，包括数据预处理要求、计算效率以及模型优化，并通过实验使用accuracy、precision、recall和F1-score等指标衡量其性能。最终，该研究提供了改进这些模型的潜在增强建议，并探讨了它们在实际财务决策中的实际应用前景。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-fin.RM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.01886v1",
      "published_date": "2025-02-27 00:28:43 UTC",
      "updated_date": "2025-02-27 00:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:02:32.211204"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 136,
  "processed_papers_count": 136,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-23T20:02:56.749515"
}
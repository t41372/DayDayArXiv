{
  "date": "2025-04-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-12 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 46 篇论文，主要聚焦 AI 推理优化、LLM 高效部署、医疗诊断创新，以及图像生成和多模态应用等领域，其中 REMEMBER 在零样本神经退行性疾病诊断、MoE-Lens 在资源受限 LLM 服务优化，以及 Speculative Thinking 在小模型推理提升方面尤为突出，同时涉及知名团队如 DeepSeek-R1 相关研究。\n\n### 重点论文讨论\n我们先聊聊今天最令人印象深刻的论文，这些聚焦 AI 推理、医疗和高效计算，展示了前沿突破。接下来快速掠过其他相关或次要文章。\n\n1. **REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis**  \n   这篇论文引入 REMEMBER 框架，用于零样本或少样本 Alzheimer 诊断，通过检索相似病例和多模态证据编码，模仿临床决策过程。核心贡献是输出可解释报告，包括参考图像和解释，提升了神经影像诊断的鲁棒性和实用性，尤其在数据有限的真实场景中表现出色。\n\n2. **MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints**  \n   作者 Yichao Yuan 等提出 MoE-Lens 系统，通过整体性能建模优化 Mixture of Experts (MoE) LLM，在资源受限环境实现高效推理。关键发现是平均 4.6 倍（最高 25.5 倍）性能提升，理论模型预测准确率达 94%，显著推进了 LLM 部署的硬件极限。\n\n3. **Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time**  \n   作者 Wang Yang 等开发了无训练框架 Speculative Thinking，让大模型指导小模型推理。贡献包括提升小模型（如 1.5B 参数）在 MATH500 上的准确率从 83.2% 到 89.4%，同时减少输出长度 15.7%，展示了推理级别的跨模型协作潜力。\n\n4. **Flux Already Knows – Activating Subject-Driven Image Generation without Training**  \n   这篇工作提出零样本框架，使用 Flux 模型通过马赛克布局和级联注意力实现主体驱动图像生成。核心发现是无需额外训练即可生成高质量图像，支持编辑如虚拟试穿，显著提升了文本到图像模型的资源效率。\n\n5. **Confirmation Bias in Generative AI Chatbots: Mechanisms, Risks, Mitigation Strategies, and Future Research Directions**  \n   作者 Yiran Du 分析了生成式 AI 聊天机器人中的确认偏差问题，探讨了其认知机制、风险（如放大偏见）和缓解策略（如技术干预和政策措施）。这篇论文强调了 AI 伦理的重要性，提供行动性见解。\n\n其他相关论文我们快速掠过，以控制篇幅。以下是今天更新的其他亮点：\n- **Explorer: Robust Collection of Interactable GUI Elements**（中文：探索者：用于交互式 GUI 元素鲁棒收集）  \n  提出 Explorer 系统，用于检测屏幕按钮等交互元素，支持 Android 和 Chrome 平台，贡献包括开源代码和路径规划，提升了 GUI 自动化。\n  \n- **\"It's not a representation of me\": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services**（中文：不是我的代表：考察合成 AI 语音服务中的口音偏差和数字排斥）  \n  通过调查发现语音合成服务（如 Speechify）存在口音歧视，建议包容性设计。\n\n- **Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training**（中文：Lumos：大规模 LLM 训练的效率性能建模和估计）  \n  开发 Lumos 工具kit，准确预测 LLM 训练行为，错误率仅 3.3%，优化了分布式训练。\n\n- **HeteRAG: A Heterogeneous Retrieval-augmented Generation Framework with Decoupled Knowledge Representations**（中文：HeteRAG：异构检索增强生成框架，支持解耦知识表示）  \n  提出异构 RAG 框架，提升生成效率，通过多粒度知识检索改善准确性。\n\n- **BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification**（中文：BrainPrompt：多层次脑提示增强用于神经疾病识别）  \n  整合 LLM 和 GNN，通过 ROI 和主题级提示提升神经疾病诊断准确性。\n\n剩余论文如 SynthTRIPs（旅游推荐查询生成）、AerOSeg（遥感图像开放词汇分割）、以及一些医疗和图像领域的（如 AMNet 用于语音合成、DL-QAT 用于 LLM 量化），则更侧重 niche 应用，我们仅简要提及：这些工作推进了合成数据、多模态融合和特定领域优化，但影响力相对有限，读者可根据兴趣深入。\n\n总之，今天的更新突显 AI 向高效、解释性和跨领域应用的演进，建议关注 REMEMBER 和 MoE-Lens 等论文，以探索实际部署潜力。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2504.09354v1",
      "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Duy-Cat Can",
        "Quang-Huy Tang",
        "Huong Ha",
        "Binh T. Nguyen",
        "Oliver Y. Chén"
      ],
      "abstract": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data.",
      "tldr_zh": "该研究提出REMEMBER框架，一种基于检索的、可解释的多模态证据引导模型，用于零- and few-shot神经退行性疾病诊断，如Alzheimer's disease，通过脑MRI扫描实现。框架首先训练一个对比对齐的视觉-文本模型，利用专家标注的参考数据扩展伪文本模态，以编码异常类型、诊断标签和临床描述；在推理时，通过证据编码模块和注意力-based推理头，从精选数据集检索相似病例并整合上下文信息，模仿临床决策过程。REMEMBER输出诊断预测及可解释报告，包括参考图像和解释，从而提升模型的可信度和实用性。实验结果显示，该框架在数据有限场景下表现出色，提供了一个强大且可解释的神经影像诊断工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09354v1",
      "published_date": "2025-04-12 22:06:15 UTC",
      "updated_date": "2025-04-12 22:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:13:16.460967"
    },
    {
      "arxiv_id": "2504.09352v1",
      "title": "Explorer: Robust Collection of Interactable GUI Elements",
      "title_zh": "Explorer: 鲁棒的可交互 GUI 元素收集",
      "authors": [
        "Iason Chaimalas",
        "Arnas Vyšniauskas",
        "Gabriel Brostow"
      ],
      "abstract": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer.",
      "tldr_zh": "该研究提出 Explorer 系统，用于 robustly 收集图形用户界面 (GUIs) 中的可交互元素 (interactables)，以解决 GUI 自动化数据收集的挑战。系统针对已知应用进行个性化数据收集和 machine learning (ML) 训练，支持在 Android 手机或桌面 Chrome 等平台上检测按钮和文本输入字段，并记录用户交互会话以映射路径重叠和循环。最终，Explorer 启用基于语音命令的路径规划，并开源代码于 GitHub，促进可靠的 GUI 自动化应用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "19 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.09352v1",
      "published_date": "2025-04-12 22:02:29 UTC",
      "updated_date": "2025-04-12 22:02:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:13:29.224491"
    },
    {
      "arxiv_id": "2504.09346v1",
      "title": "\"It's not a representation of me\": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services",
      "title_zh": "翻译失败",
      "authors": [
        "Shira Michel",
        "Sufi Kaur",
        "Sarah Elizabeth Gillespie",
        "Jeffrey Gleason",
        "Christo Wilson",
        "Avijit Ghosh"
      ],
      "abstract": "Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies.",
      "tldr_zh": "本研究评估了Speechify和ElevenLabs等合成AI语音服务的技术性能及其对用户感知的影响，使用混合方法（包括调查和访谈）来分析五种区域英语口音的差异。结果显示，这些技术在不同口音间存在性能差距，可能无意中强化accent bias和基于口音的歧视，从而导致新的digital exclusion。研究提供可操作见解，呼吁开发商、政策制定者和组织采用包容性设计和监管，确保AI语音技术的公平性和社会责任。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "This paper has been accepted to FAccT 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.09346v1",
      "published_date": "2025-04-12 21:31:22 UTC",
      "updated_date": "2025-04-12 21:31:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:13:39.735869"
    },
    {
      "arxiv_id": "2504.09345v1",
      "title": "MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints",
      "title_zh": "MoE-Lens：向着资源约束下高吞吐量 MoE LLM 服务的硬件极限",
      "authors": [
        "Yichao Yuan",
        "Lin Ma",
        "Nishil Talati"
      ],
      "abstract": "Mixture of Experts (MoE) LLMs, characterized by their sparse activation\npatterns, offer a promising approach to scaling language models while avoiding\nproportionally increasing the inference cost. However, their large parameter\nsizes present deployment challenges in resource-constrained environments with\nlimited GPU memory capacity, as GPU memory is often insufficient to accommodate\nthe full set of model weights. Consequently, typical deployments rely on\nCPU-GPU hybrid execution: the GPU handles compute-intensive GEMM operations,\nwhile the CPU processes the relatively lightweight attention mechanism. This\nsetup introduces a key challenge: how to effectively optimize resource\nutilization across CPU and GPU? Prior work has designed system optimizations\nbased on performance models with limited scope. Specifically, such models do\nnot capture the complex interactions between hardware properties and system\nexecution mechanisms. Therefore, previous approaches neither identify nor\nachieve the hardware limit.\n  This paper presents MoE-Lens, a high-throughput MoE LLM inference system\ndesigned through holistic performance modeling for resource-constrained\nenvironments. Our performance model thoroughly analyzes various fundamental\nsystem components, including CPU memory capacity, GPU compute power, and\nworkload characteristics, to understand the theoretical performance upper bound\nof MoE inference. Furthermore, it captures the system execution mechanisms to\nidentify the key hardware bottlenecks and accurately predict the achievable\nthroughput. Informed by our performance model, MoE-Lens introduces an inference\nsystem approaching hardware limits. Evaluated on diverse MoE models and\ndatasets, MoE-Lens outperforms the state-of-the-art solution by 4.6x on average\n(up to 25.5x), with our theoretical model predicting performance with an\naverage 94% accuracy.",
      "tldr_zh": "该研究针对Mixture of Experts (MoE) LLMs在资源受限环境下的部署挑战，提出MoE-Lens系统，以优化CPU-GPU混合执行并接近硬件性能极限。MoE-Lens通过全面的性能建模分析CPU内存容量、GPU计算能力和工作负载特性，识别关键硬件瓶颈并准确预测吞吐量，从而实现高效的MoE LLM推理。实验结果显示，MoE-Lens在多种MoE模型和数据集上，比现有最先进方案平均提升4.6倍（最高25.5倍），性能模型预测准确率达94%。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09345v1",
      "published_date": "2025-04-12 21:26:56 UTC",
      "updated_date": "2025-04-12 21:26:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:13:55.450044"
    },
    {
      "arxiv_id": "2504.12329v1",
      "title": "Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time",
      "title_zh": "Speculative Thinking：在推理时通过大模型指导增强小模型推理",
      "authors": [
        "Wang Yang",
        "Xiang Yue",
        "Vipin Chaudhary",
        "Xiaotian Han"
      ],
      "abstract": "Recent advances leverage post-training to enhance model reasoning\nperformance, which typically requires costly training pipelines and still\nsuffers from inefficient, overly lengthy outputs. We introduce Speculative\nThinking, a training-free framework that enables large reasoning models to\nguide smaller ones during inference at the reasoning level, distinct from\nspeculative decoding, which operates at the token level. Our approach is based\non two observations: (1) reasoning-supportive tokens such as \"wait\" frequently\nappear after structural delimiters like \"\\n\\n\", serving as signals for\nreflection or continuation; and (2) larger models exhibit stronger control over\nreflective behavior, reducing unnecessary backtracking while improving\nreasoning quality. By strategically delegating reflective steps to a more\ncapable model, our method significantly boosts the reasoning accuracy of\nreasoning models while shortening their output. With the assistance of the 32B\nreasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to\n89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average\noutput length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%\ndecrease. Moreover, when applied to a non-reasoning model\n(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%\non the same benchmark, achieving a relative improvement of 7.8%.",
      "tldr_zh": "本文提出 Speculative Thinking，一种无需训练的框架，让大型推理模型在推理层面指导小型模型，从而提升其推理准确性和输出效率，与传统的 speculative decoding（在 token 层面操作）不同。该方法利用观察到的反思信号（如 \"wait\" 后的结构分隔符），将反思步骤委托给更强大的模型，减少不必要回溯并优化输出。实验结果显示，在 MATH500 基准上，1.5B 模型的准确率从 83.2% 提高到 89.4%（提升 6.2%），输出长度减少 15.7%；应用于非推理模型 Qwen-2.5-7B-Instruct 时，准确率从 74.0% 提升到 81.8%（相对提升 7.8%）。这为高效的模型推理提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12329v1",
      "published_date": "2025-04-12 21:25:32 UTC",
      "updated_date": "2025-04-12 21:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:14:06.429939"
    },
    {
      "arxiv_id": "2504.09343v1",
      "title": "Confirmation Bias in Generative AI Chatbots: Mechanisms, Risks, Mitigation Strategies, and Future Research Directions",
      "title_zh": "生成式 AI 聊天机器人中的确认偏差",
      "authors": [
        "Yiran Du"
      ],
      "abstract": "This article explores the phenomenon of confirmation bias in generative AI\nchatbots, a relatively underexamined aspect of AI-human interaction. Drawing on\ncognitive psychology and computational linguistics, it examines how\nconfirmation bias, commonly understood as the tendency to seek information that\naligns with existing beliefs, can be replicated and amplified by the design and\nfunctioning of large language models. The article analyzes the mechanisms by\nwhich confirmation bias may manifest in chatbot interactions, assesses the\nethical and practical risks associated with such bias, and proposes a range of\nmitigation strategies. These include technical interventions, interface\nredesign, and policy measures aimed at promoting balanced AI-generated\ndiscourse. The article concludes by outlining future research directions,\nemphasizing the need for interdisciplinary collaboration and empirical\nevaluation to better understand and address confirmation bias in generative AI\nsystems.",
      "tldr_zh": "这篇论文探讨了 generative AI chatbots 中的 confirmation bias，即人们倾向于寻求与既有信念一致的信息的现象，并分析其如何在大型语言模型的设计和运作中被复制和放大。基于认知心理学和计算语言学，论文剖析了这种偏差的机制、相关的道德和实际风险，并提出缓解策略，如技术干预、界面重设计以及政策措施，以推动平衡的 AI 生成对话。最终，论文强调未来研究方向，包括加强跨学科合作和实证评估，以更好地理解和解决这一问题。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09343v1",
      "published_date": "2025-04-12 21:08:36 UTC",
      "updated_date": "2025-04-12 21:08:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:14:15.294212"
    },
    {
      "arxiv_id": "2504.11478v2",
      "title": "Flux Already Knows -- Activating Subject-Driven Image Generation without Training",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Kang",
        "Stathi Fotiadis",
        "Liming Jiang",
        "Qing Yan",
        "Yumin Jia",
        "Zichuan Liu",
        "Min Jin Chong",
        "Xin Lu"
      ],
      "abstract": "We propose a simple yet effective zero-shot framework for subject-driven\nimage generation using a vanilla Flux model. By framing the task as grid-based\nimage completion and simply replicating the subject image(s) in a mosaic\nlayout, we activate strong identity-preserving capabilities without any\nadditional data, training, or inference-time fine-tuning. This \"free lunch\"\napproach is further strengthened by a novel cascade attention design and meta\nprompting technique, boosting fidelity and versatility. Experimental results\nshow that our method outperforms baselines across multiple key metrics in\nbenchmarks and human preference studies, with trade-offs in certain aspects.\nAdditionally, it supports diverse edits, including logo insertion, virtual\ntry-on, and subject replacement or insertion. These results demonstrate that a\npre-trained foundational text-to-image model can enable high-quality,\nresource-efficient subject-driven generation, opening new possibilities for\nlightweight customization in downstream applications.",
      "tldr_zh": "本文提出了一种简单有效的零-shot framework，使用 vanilla Flux 模型，通过网格-based 图像完成和马赛克布局复制主题图像，实现无需额外数据或训练的主题驱动图像生成。框架进一步引入 cascade attention design 和 meta prompting technique，提升图像的保真度和多功能性，支持多样编辑如徽标插入、虚拟试穿和主题替换。实验结果显示，该方法在多个关键指标上优于基线模型，并在基准测试和人类偏好研究中表现出色，尽管在某些方面存在权衡。最后，这证明了预训练文本到图像模型可实现高质量、资源高效的生成，为下游应用的轻量级定制打开新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11478v2",
      "published_date": "2025-04-12 20:41:53 UTC",
      "updated_date": "2025-04-19 05:17:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:14:28.930600"
    },
    {
      "arxiv_id": "2504.09307v1",
      "title": "Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyu Liang",
        "Hiwot Tadese Kassa",
        "Wenyin Fu",
        "Brian Coutinho",
        "Louis Feng",
        "Christina Delimitrou"
      ],
      "abstract": "Training LLMs in distributed environments presents significant challenges due\nto the complexity of model execution, deployment systems, and the vast space of\nconfigurable strategies. Although various optimization techniques exist,\nachieving high efficiency in practice remains difficult. Accurate performance\nmodels that effectively characterize and predict a model's behavior are\nessential for guiding optimization efforts and system-level studies. We propose\nLumos, a trace-driven performance modeling and estimation toolkit for\nlarge-scale LLM training, designed to accurately capture and predict the\nexecution behaviors of modern LLMs. We evaluate Lumos on a production ML\ncluster with up to 512 NVIDIA H100 GPUs using various GPT-3 variants,\ndemonstrating that it can replay execution time with an average error of just\n3.3%, along with other runtime details, across different models and\nconfigurations. Additionally, we validate its ability to estimate performance\nfor new setups from existing traces, facilitating efficient exploration of\nmodel and deployment configurations.",
      "tldr_zh": "该研究针对大规模 LLM 训练中的复杂执行、部署系统和配置策略挑战，提出 Lumos，一种基于跟踪驱动的性能建模和估计工具包。Lumos 能够准确捕获和预测现代 LLM 的执行行为，并在生产 ML 集群上使用多达 512 个 NVIDIA H100 GPUs 测试各种 GPT-3 变体时，实现执行时间重放的平均错误率仅 3.3%。此外，Lumos 支持从现有跟踪估计新设置的性能，从而促进模型和部署配置的有效探索和优化。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to MLSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.09307v1",
      "published_date": "2025-04-12 18:43:24 UTC",
      "updated_date": "2025-04-12 18:43:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:14:40.957188"
    },
    {
      "arxiv_id": "2504.09302v1",
      "title": "Application of Contrastive Learning on ECG Data: Evaluating Performance in Japanese and Classification with Around 100 Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Junichiro Takahashi",
        "JingChuan Guan",
        "Masataka Sato",
        "Kaito Baba",
        "Kazuto Haruguchi",
        "Daichi Nagashima",
        "Satoshi Kodera",
        "Norihiko Takeda"
      ],
      "abstract": "The electrocardiogram (ECG) is a fundamental tool in cardiovascular\ndiagnostics due to its powerful and non-invasive nature. One of the most\ncritical usages is to determine whether more detailed examinations are\nnecessary, with users ranging across various levels of expertise. Given this\ndiversity in expertise, it is essential to assist users to avoid critical\nerrors. Recent studies in machine learning have addressed this challenge by\nextracting valuable information from ECG data. Utilizing language models, these\nstudies have implemented multimodal models aimed at classifying ECGs according\nto labeled terms. However, the number of classes was reduced, and it remains\nuncertain whether the technique is effective for languages other than English.\nTo move towards practical application, we utilized ECG data from regular\npatients visiting hospitals in Japan, maintaining a large number of Japanese\nlabels obtained from actual ECG readings. Using a contrastive learning\nframework, we found that even with 98 labels for classification, our\nJapanese-based language model achieves accuracy comparable to previous\nresearch. This study extends the applicability of multimodal machine learning\nframeworks to broader clinical studies and non-English languages.",
      "tldr_zh": "这篇论文探讨了对比学习（contrastive learning）在心电图（ECG）数据上的应用，旨在评估其在日语环境下的性能，并对约 98 个标签进行分类，以帮助不同专业水平的用户避免诊断错误。研究使用日本医院的实际 ECG 数据和日语标签，构建多模态机器学习模型，扩展了之前研究中类别数量有限和语言局限的问题。结果显示，该模型的分类准确率与现有研究相当，证明了其在非英语语言中的有效性。该工作为多模态框架在更广泛临床研究中的应用提供了新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 1 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.09302v1",
      "published_date": "2025-04-12 18:20:44 UTC",
      "updated_date": "2025-04-12 18:20:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:14:52.557104"
    },
    {
      "arxiv_id": "2504.09301v1",
      "title": "Continuum-Interaction-Driven Intelligence: Human-Aligned Neural Architecture via Crystallized Reasoning and Fluid Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Pengcheng Zhou",
        "Zhiqiang Nie",
        "Haochen Li"
      ],
      "abstract": "Current AI systems based on probabilistic neural networks, such as large\nlanguage models (LLMs), have demonstrated remarkable generative capabilities\nyet face critical challenges including hallucination, unpredictability, and\nmisalignment with human decision-making. These issues fundamentally stem from\nthe over-reliance on randomized (probabilistic) neural networks-oversimplified\nmodels of biological neural networks-while neglecting the role of procedural\nreasoning (chain-of-thought) in trustworthy decision-making. Inspired by the\nhuman cognitive duality of fluid intelligence (flexible generation) and\ncrystallized intelligence (structured knowledge), this study proposes a\ndual-channel intelligent architecture that integrates probabilistic generation\n(LLMs) with white-box procedural reasoning (chain-of-thought) to construct\ninterpretable, continuously learnable, and human-aligned AI systems.\nConcretely, this work: (1) redefines chain-of-thought as a programmable\ncrystallized intelligence carrier, enabling dynamic knowledge evolution and\ndecision verification through multi-turn interaction frameworks; (2) introduces\na task-driven modular network design that explicitly demarcates the functional\nboundaries between randomized generation and procedural control to address\ntrustworthiness in vertical-domain applications; (3) demonstrates that\nmulti-turn interaction is a necessary condition for intelligence emergence,\nwith dialogue depth positively correlating with the system's human-alignment\ndegree. This research not only establishes a new paradigm for trustworthy AI\ndeployment but also provides theoretical foundations for next-generation\nhuman-AI collaborative systems.",
      "tldr_zh": "本研究指出，基于概率神经网络的 AI 系统（如 LLMs）虽具强大生成能力，但易出现幻觉、不确定性和人类决策不一致的问题，主要源于忽略程序性推理（chain-of-thought）。为此，论文提出一种双通道智能架构，结合 fluid intelligence（灵活生成）和 crystallized intelligence（结构化知识），将 LLMs 与白盒 chain-of-thought 整合，实现可解释、持续学习和人类对齐的 AI 系统。关键贡献包括：重新定义 chain-of-thought 为可编程的 crystallized intelligence 载体，支持动态知识演化和多轮交互决策验证；引入任务驱动的模块化网络设计，明确区分随机生成和程序控制，提升垂直领域应用的可靠性；证明多轮交互是智能涌现的必要条件，与系统的人类对齐度正相关。该框架为可信 AI 部署和新一代人-AI 协作系统提供了理论基础和新范式。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09301v1",
      "published_date": "2025-04-12 18:15:49 UTC",
      "updated_date": "2025-04-12 18:15:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:15:06.220541"
    },
    {
      "arxiv_id": "2504.09283v1",
      "title": "Semantic Commit: Helping Users Update Intent Specifications for AI Memory at Scale",
      "title_zh": "语义提交：帮助用户在规模化场景下更新AI记忆的意图规范",
      "authors": [
        "Priyan Vaithilingam",
        "Munyeong Kim",
        "Frida-Cecilia Acosta-Parenteau",
        "Daniel Lee",
        "Amine Mhedhbi",
        "Elena L. Glassman",
        "Ian Arawjo"
      ],
      "abstract": "How do we update AI memory of user intent as intent changes? We consider how\nan AI interface may assist the integration of new information into a repository\nof natural language data. Inspired by software engineering concepts like impact\nanalysis, we develop methods and a UI for managing semantic changes with\nnon-local effects, which we call \"semantic conflict resolution.\" The user\ncommits new intent to a project -- makes a \"semantic commit\" -- and the AI\nhelps the user detect and resolve semantic conflicts within a store of existing\ninformation representing their intent (an \"intent specification\"). We develop\nan interface, SemanticCommit, to better understand how users resolve conflicts\nwhen updating intent specifications such as Cursor Rules and game design\ndocuments. A knowledge graph-based RAG pipeline drives conflict detection,\nwhile LLMs assist in suggesting resolutions. We evaluate our technique on an\ninitial benchmark. Then, we report a 12 user within-subjects study of\nSemanticCommit for two task domains -- game design documents, and AI agent\nmemory in the style of ChatGPT memories -- where users integrated new\ninformation into an existing list. Half of our participants adopted a workflow\nof impact analysis, where they would first flag conflicts without AI revisions\nthen resolve conflicts locally, despite having access to a global revision\nfeature. We argue that AI agent interfaces, such as software IDEs like Cursor\nand Windsurf, should provide affordances for impact analysis and help users\nvalidate AI retrieval independently from generation. Our work speaks to how AI\nagent designers should think about updating memory as a process that involves\nhuman feedback and decision-making.",
      "tldr_zh": "该论文提出“Semantic Commit”方法，帮助用户更新AI记忆中的意图规范，处理意图变化带来的语义冲突。受软件工程影响（如impact analysis），系统使用知识图谱-based RAG管道检测冲突，并借助LLMs建议解决方案，提供了一个交互式UI供用户提交新意图并管理非本地效应。实验包括基准测试和12名用户的对照研究，结果显示用户偏好先标记冲突再本地解决的工作流，并建议AI代理界面（如Cursor）增强impact analysis功能，以支持人类反馈和独立验证。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2; D.2.1; I.7.0; I.2.11; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages; 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.09283v1",
      "published_date": "2025-04-12 17:07:08 UTC",
      "updated_date": "2025-04-12 17:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:15:17.618957"
    },
    {
      "arxiv_id": "2504.09277v1",
      "title": "SynthTRIPs: A Knowledge-Grounded Framework for Benchmark Query Generation for Personalized Tourism Recommenders",
      "title_zh": "SynthTRIPs：一个基于知识的框架，用于个性化旅游推荐系统的基准查询生成",
      "authors": [
        "Ashmi Banerjee",
        "Adithi Satish",
        "Fitri Nur Aisyah",
        "Wolfgang Wörndl",
        "Yashar Deldjoo"
      ],
      "abstract": "Tourism Recommender Systems (TRS) are crucial in personalizing travel\nexperiences by tailoring recommendations to users' preferences, constraints,\nand contextual factors. However, publicly available travel datasets often lack\nsufficient breadth and depth, limiting their ability to support advanced\npersonalization strategies -- particularly for sustainable travel and off-peak\ntourism. In this work, we explore using Large Language Models (LLMs) to\ngenerate synthetic travel queries that emulate diverse user personas and\nincorporate structured filters such as budget constraints and sustainability\npreferences.\n  This paper introduces a novel SynthTRIPs framework for generating synthetic\ntravel queries using LLMs grounded in a curated knowledge base (KB). Our\napproach combines persona-based preferences (e.g., budget, travel style) with\nexplicit sustainability filters (e.g., walkability, air quality) to produce\nrealistic and diverse queries. We mitigate hallucination and ensure factual\ncorrectness by grounding the LLM responses in the KB. We formalize the query\ngeneration process and introduce evaluation metrics for assessing realism and\nalignment. Both human expert evaluations and automatic LLM-based assessments\ndemonstrate the effectiveness of our synthetic dataset in capturing complex\npersonalization aspects underrepresented in existing datasets. While our\nframework was developed and tested for personalized city trip recommendations,\nthe methodology applies to other recommender system domains.\n  Code and dataset are made public at https://bit.ly/synthTRIPs",
      "tldr_zh": "这篇论文介绍了 SynthTRIPs 框架，一种基于知识库 (KB) 的方法，使用 Large Language Models (LLMs) 生成合成旅行查询，以解决现有旅游推荐系统 (TRS) 数据集在广度和深度上的不足，特别是针对可持续旅行和淡季旅游的个性化需求。该框架结合用户角色偏好（如预算和旅行风格）以及可持续性过滤器（如步行友好度和空气质量），生成真实且多样的查询，并通过 KB grounding 减少幻觉并确保事实准确性。作者形式化了查询生成过程，并引入了评估指标，通过人类专家和自动 LLM 评估证明了合成数据集在捕捉复杂个性化方面的有效性。尽管针对城市旅行推荐开发，该框架可扩展到其他推荐系统领域，且代码及数据集已公开。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted for publication at SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.09277v1",
      "published_date": "2025-04-12 16:48:35 UTC",
      "updated_date": "2025-04-12 16:48:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:15:31.118948"
    },
    {
      "arxiv_id": "2504.09271v1",
      "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries",
      "title_zh": "AI 与人类书写的在线心理健康查询",
      "authors": [
        "Koustuv Saha",
        "Yoshee Jain",
        "Munmun De Choudhury"
      ],
      "abstract": "The ubiquity and widespread use of digital and online technologies have\ntransformed mental health support, with online mental health communities\n(OMHCs) providing safe spaces for peer support. More recently, generative AI\nand large language models (LLMs) have introduced new possibilities for\nscalable, around-the-clock mental health assistance that could potentially\naugment and supplement the capabilities of OMHCs. Although genAI shows promise\nin delivering immediate and personalized responses, their effectiveness in\nreplicating the nuanced, experience-based support of human peers remains an\nopen question. In this study, we harnessed 24,114 posts and 138,758 online\ncommunity (OC) responses from 55 OMHCs on Reddit. We prompted several\nstate-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts,\nand compared their (AI) responses to human-written (OC) responses based on a\nvariety of linguistic measures across psycholinguistics and lexico-semantics.\nOur findings revealed that AI responses are more verbose, readable, and\nanalytically structured, but lack linguistic diversity and personal narratives\ninherent in human-human interactions. Through a qualitative examination, we\nfound validation as well as complementary insights into the nature of AI\nresponses, such as its neutrality of stance and the absence of seeking\nback-and-forth clarifications. We discuss the ethical and practical\nimplications of integrating generative AI into OMHCs, advocating for frameworks\nthat balance AI's scalability and timeliness with the irreplaceable\nauthenticity, social interactiveness, and expertise of human connections that\nform the ethos of online support communities.",
      "tldr_zh": "这篇论文比较了 AI 生成和人类书写的在线心理健康社区（OMHCs）响应，探讨 AI 是否能有效复制人类支持的细微之处。研究方法涉及使用 Reddit 的 24,114 个帖子和 138,758 个响应，提示 LLMs（如 GPT-4-Turbo、Llama-3 和 Mistral-7B）生成响应，并通过心理语言学和词汇语义措施进行分析。结果显示，AI 响应更冗长、易读和分析性结构化，但缺乏语言多样性、个人叙事以及互动性特征，如寻求澄清。论文强调了将生成 AI 整合到 OMHCs 中的伦理和实际含义，主张平衡 AI 的可扩展性与人类互动的真实性和专业性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09271v1",
      "published_date": "2025-04-12 16:20:02 UTC",
      "updated_date": "2025-04-12 16:20:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:15:42.605896"
    },
    {
      "arxiv_id": "2504.12328v1",
      "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future",
      "title_zh": "奖励模型的全面综述：分类、",
      "authors": [
        "Jialun Zhong",
        "Wei Shen",
        "Yanzeng Li",
        "Songyang Gao",
        "Hua Lu",
        "Yicheng Chen",
        "Yang Zhang",
        "Wei Zhou",
        "Jinjie Gu",
        "Lei Zou"
      ],
      "abstract": "Reward Model (RM) has demonstrated impressive potential for enhancing Large\nLanguage Models (LLM), as RM can serve as a proxy for human preferences,\nproviding signals to guide LLMs' behavior in various tasks. In this paper, we\nprovide a comprehensive overview of relevant research, exploring RMs from the\nperspectives of preference collection, reward modeling, and usage. Next, we\nintroduce the applications of RMs and discuss the benchmarks for evaluation.\nFurthermore, we conduct an in-depth analysis of the challenges existing in the\nfield and dive into the potential research directions. This paper is dedicated\nto providing beginners with a comprehensive introduction to RMs and\nfacilitating future studies. The resources are publicly available at\ngithub\\footnote{https://github.com/JLZhong23/awesome-reward-models}.",
      "tldr_zh": "这篇论文对Reward Model (RM)进行了全面调查，探讨其作为人类偏好代理在增强Large Language Models (LLM)中的潜力。论文从偏好收集、奖励建模和使用角度系统概述了相关研究，并介绍了RM的应用场景以及评估基准。同时，它分析了当前领域的挑战和未来研究方向，并公开了资源（如GitHub仓库），以帮助初学者入门和推动后续工作。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12328v1",
      "published_date": "2025-04-12 16:07:36 UTC",
      "updated_date": "2025-04-12 16:07:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:15:53.337557"
    },
    {
      "arxiv_id": "2504.09257v1",
      "title": "MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices",
      "title_zh": "MiMIC：用于预测股票价格的多模态印度",
      "authors": [
        "Sohom Ghosh",
        "Arnab Maji",
        "Sudip Kumar Naskar"
      ],
      "abstract": "Predicting stock market prices following corporate earnings calls remains a\nsignificant challenge for investors and researchers alike, requiring innovative\napproaches that can process diverse information sources. This study\ninvestigates the impact of corporate earnings calls on stock prices by\nintroducing a multi-modal predictive model. We leverage textual data from\nearnings call transcripts, along with images and tables from accompanying\npresentations, to forecast stock price movements on the trading day immediately\nfollowing these calls. To facilitate this research, we developed the MiMIC\n(Multi-Modal Indian Earnings Calls) dataset, encompassing companies\nrepresenting the Nifty 50, Nifty MidCap 50, and Nifty Small 50 indices. The\ndataset includes earnings call transcripts, presentations, fundamentals,\ntechnical indicators, and subsequent stock prices. We present a multimodal\nanalytical framework that integrates quantitative variables with predictive\nsignals derived from textual and visual modalities, thereby enabling a holistic\napproach to feature representation and analysis. This multi-modal approach\ndemonstrates the potential for integrating diverse information sources to\nenhance financial forecasting accuracy. To promote further research in\ncomputational economics, we have made the MiMIC dataset publicly available\nunder the CC-NC-SA-4.0 licence. Our work contributes to the growing body of\nliterature on market reactions to corporate communications and highlights the\nefficacy of multi-modal machine learning techniques in financial analysis.",
      "tldr_zh": "这篇论文介绍了MiMIC数据集（Multi-Modal Indian Earnings Calls），旨在预测公司财报电话会议后的股票价格变动。研究开发了一个多模态预测模型，整合文本数据（如财报会议记录）、图像和表格（来自演示文稿）、基本面数据、技术指标及后续股票价格，涵盖印度Nifty 50、Nifty MidCap 50和Nifty Small 50指数的公司。实验结果显示，这种多模态分析框架显著提升了金融预测的准确性，并通过公开数据集（CC-NC-SA-4.0许可）促进了计算经济学领域的进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Code and Dataset:\n  https://huggingface.co/datasets/sohomghosh/MiMIC_Multi-Modal_Indian_Earnings_Calls_Dataset",
      "pdf_url": "http://arxiv.org/pdf/2504.09257v1",
      "published_date": "2025-04-12 15:31:40 UTC",
      "updated_date": "2025-04-12 15:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:16:05.641060"
    },
    {
      "arxiv_id": "2504.09242v1",
      "title": "Development of a PPO-Reinforcement Learned Walking Tripedal Soft-Legged Robot using SOFA",
      "title_zh": "翻译失败",
      "authors": [
        "Yomna Mokhtar",
        "Tarek Shohdy",
        "Abdallah A. Hassan",
        "Mostafa Eshra",
        "Omar Elmenawy",
        "Osama Khalil",
        "Haitham El-Hussieny"
      ],
      "abstract": "Rigid robots were extensively researched, whereas soft robotics remains an\nunderexplored field. Utilizing soft-legged robots in performing tasks as a\nreplacement for human beings is an important stride to take, especially under\nharsh and hazardous conditions over rough terrain environments. For the demand\nto teach any robot how to behave in different scenarios, a real-time physical\nand visual simulation is essential. When it comes to soft robots specifically,\na simulation framework is still an arduous problem that needs to be disclosed.\nUsing the simulation open framework architecture (SOFA) is an advantageous\nstep. However, neither SOFA's manual nor prior public SOFA projects show its\nmaximum capabilities the users can reach. So, we resolved this by establishing\ncustomized settings and handling the framework components appropriately.\nSettling on perfect, fine-tuned SOFA parameters has stimulated our motivation\ntowards implementing the state-of-the-art (SOTA) reinforcement learning (RL)\nmethod of proximal policy optimization (PPO). The final representation is a\nwell-defined, ready-to-deploy walking, tripedal, soft-legged robot based on\nPPO-RL in a SOFA environment. Robot navigation performance is a key metric to\nbe considered for measuring the success resolution. Although in the simulated\nsoft robots case, an 82\\% success rate in reaching a single goal is a\ngroundbreaking output, we pushed the boundaries to further steps by evaluating\nthe progress under assigning a sequence of goals. While trailing the platform\nsteps, outperforming discovery has been observed with an accumulative squared\nerror deviation of 19 mm. The full code is publicly available at\n\\href{https://github.com/tarekshohdy/PPO_SOFA_Soft_Legged_Robot.git}{github.com/tarekshohdy/PPO$\\textunderscore$SOFA$\\textunderscore$Soft$\\textunderscore$Legged$\\textunderscore$\nRobot.git}",
      "tldr_zh": "本研究开发了一种基于 PPO-Reinforcement Learning 的三足软腿机器人，使用 SOFA 模拟框架来处理恶劣环境下的导航任务。作者通过自定义 SOFA 参数和实施 PPO 算法，实现机器人在实时物理和视觉模拟中的训练与优化。实验结果显示，该机器人达到单一目标的成功率高达 82%，而在连续目标序列下，累计平方误差偏差仅为 19 mm。该工作为软机器人领域提供了可部署的开源解决方案，并推动了其在复杂地形中的应用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09242v1",
      "published_date": "2025-04-12 14:46:51 UTC",
      "updated_date": "2025-04-12 14:46:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:16:18.281174"
    },
    {
      "arxiv_id": "2504.09225v1",
      "title": "AMNet: An Acoustic Model Network for Enhanced Mandarin Speech Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Yubing Cao",
        "Yinfeng Yu",
        "Yongming Li",
        "Liejun Wang"
      ],
      "abstract": "This paper presents AMNet, an Acoustic Model Network designed to improve the\nperformance of Mandarin speech synthesis by incorporating phrase structure\nannotation and local convolution modules. AMNet builds upon the FastSpeech 2\narchitecture while addressing the challenge of local context modeling, which is\ncrucial for capturing intricate speech features such as pauses, stress, and\nintonation. By embedding a phrase structure parser into the model and\nintroducing a local convolution module, AMNet enhances the model's sensitivity\nto local information. Additionally, AMNet decouples tonal characteristics from\nphonemes, providing explicit guidance for tone modeling, which improves tone\naccuracy and pronunciation. Experimental results demonstrate that AMNet\noutperforms baseline models in subjective and objective evaluations. The\nproposed model achieves superior Mean Opinion Scores (MOS), lower Mel Cepstral\nDistortion (MCD), and improved fundamental frequency fitting $F0 (R^2)$,\nconfirming its ability to generate high-quality, natural, and expressive\nMandarin speech.",
      "tldr_zh": "本研究提出 AMNet，一种基于 FastSpeech 2 架构的声学模型网络，旨在提升普通话语音合成性能，通过整合短语结构解析器和本地卷积模块来更好地捕捉本地上下文信息，如停顿、重音和语调。AMNet 还将音调特征从音素中分离，提供明确的音调建模指导，从而提高音调准确性和发音质量。实验结果显示，该模型在主观和客观评估中优于基线模型，实现了更高的 Mean Opinion Scores (MOS)、更低的 Mel Cepstral Distortion (MCD) 以及更好的 fundamental frequency fitting F0 (R^2)，从而生成高质量、自然且富有表现力的普通话语音。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (8 pages). Accepted for publication by IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.09225v1",
      "published_date": "2025-04-12 14:02:31 UTC",
      "updated_date": "2025-04-12 14:02:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:16:29.603159"
    },
    {
      "arxiv_id": "2504.09223v1",
      "title": "DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models",
      "title_zh": "DL-QAT：权重分解的低秩量化感知训练，用于大型语言模型",
      "authors": [
        "Wenjin Ke",
        "Zhe Li",
        "Dong Li",
        "Lu Tian",
        "Emad Barsoum"
      ],
      "abstract": "Improving the efficiency of inference in Large Language Models (LLMs) is a\ncritical area of research. Post-training Quantization (PTQ) is a popular\ntechnique, but it often faces challenges at low-bit levels, particularly in\ndownstream tasks. Quantization-aware Training (QAT) can alleviate this problem,\nbut it requires significantly more computational resources. To tackle this, we\nintroduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT),\nwhich merges the advantages of QAT while training only less than 1% of the\ntotal parameters. Specifically, we introduce a group-specific quantization\nmagnitude to adjust the overall scale of each quantization group. Within each\nquantization group, we use LoRA matrices to update the weight size and\ndirection in the quantization space. We validated the effectiveness of our\nmethod on the LLaMA and LLaMA2 model families. The results show significant\nimprovements over our baseline method across different quantization\ngranularities. For instance, for LLaMA-7B, our approach outperforms the\nprevious state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model.\nAdditionally, our quantization results on pre-trained models also surpass\nprevious QAT methods, demonstrating the superior performance and efficiency of\nour approach.",
      "tldr_zh": "该论文提出DL-QAT（Weight-Decomposed Low-Rank Quantization-Aware Training）方法，以提升大型语言模型（LLMs）的推理效率，解决后训练量化（PTQ）在低位级别下游任务的挑战，同时避免量化感知训练（QAT）的高计算资源需求。DL-QAT通过引入group-specific quantization magnitude调整每个量化组的整体规模，并在组内使用LoRA matrices更新权重的尺寸和方向，仅需训练不到1%的参数。实验在LLaMA和LLaMA2模型家族上验证了其有效性，例如LLaMA-7B的3-bit模型在MMLU任务上比现有最先进方法提升4.2%，整体量化性能也优于传统QAT方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09223v1",
      "published_date": "2025-04-12 13:57:02 UTC",
      "updated_date": "2025-04-12 13:57:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:16:42.326283"
    },
    {
      "arxiv_id": "2504.09210v2",
      "title": "FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxin Liu",
        "Xiaoqian Jiang",
        "Xiang Li",
        "Bohan Zhang",
        "Jing Zhang"
      ],
      "abstract": "Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models.",
      "tldr_zh": "这篇论文针对图神经网络 (GNNs) 中度数偏差导致的节点预测性能不平等问题，提出了一种新型框架 FairACE，通过不对称对比学习 (asymmetric contrastive learning) 和对抗训练 (adversarial training) 相结合的方式来提升度数公平性。FairACE 捕获一跳局部邻居信息和二跳同质相似性 (two-hop monophily similarity)，并使用度数公平调节器和组平衡公平损失 (group-balanced fairness loss) 来平衡高度和低度节点的性能，同时引入新的公平度量标准 Accuracy Distribution Gap (ADG) 进行量化评估。实验结果显示，在合成和真实数据集上，FairACE 显著改善了度数公平度量，同时保持了与最先进 GNN 模型相当的预测准确率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09210v2",
      "published_date": "2025-04-12 13:32:11 UTC",
      "updated_date": "2025-04-15 02:22:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:16:55.451063"
    },
    {
      "arxiv_id": "2504.10529v1",
      "title": "HeteRAG: A Heterogeneous Retrieval-augmented Generation Framework with Decoupled Knowledge Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Peiru Yang",
        "Xintian Li",
        "Zhiyang Hu",
        "Jiapeng Wang",
        "Jinhua Yin",
        "Huili Wang",
        "Lizhi He",
        "Shuai Yang",
        "Shangguang Wang",
        "Yongfeng Huang",
        "Tao Qi"
      ],
      "abstract": "Retrieval-augmented generation (RAG) methods can enhance the performance of\nLLMs by incorporating retrieved knowledge chunks into the generation process.\nIn general, the retrieval and generation steps usually have different\nrequirements for these knowledge chunks. The retrieval step benefits from\ncomprehensive information to improve retrieval accuracy, whereas excessively\nlong chunks may introduce redundant contextual information, thereby diminishing\nboth the effectiveness and efficiency of the generation process. However,\nexisting RAG methods typically employ identical representations of knowledge\nchunks for both retrieval and generation, resulting in suboptimal performance.\nIn this paper, we propose a heterogeneous RAG framework (\\myname) that\ndecouples the representations of knowledge chunks for retrieval and generation,\nthereby enhancing the LLMs in both effectiveness and efficiency. Specifically,\nwe utilize short chunks to represent knowledge to adapt the generation step and\nutilize the corresponding chunk with its contextual information from\nmulti-granular views to enhance retrieval accuracy. We further introduce an\nadaptive prompt tuning method for the retrieval model to adapt the\nheterogeneous retrieval augmented generation process. Extensive experiments\ndemonstrate that \\myname achieves significant improvements compared to\nbaselines.",
      "tldr_zh": "该研究提出HeteRAG框架，一种异构检索增强生成(Retrieval-augmented Generation)方法，通过解耦知识块的表示来提升大型语言模型(LLMs)的性能，以适应检索和生成步骤的不同需求。具体而言，该框架使用短知识块优化生成过程，并结合多粒度视图的上下文块提升检索准确性，同时引入自适应提示调整(adaptive prompt tuning)来完善检索模型。实验结果显示，HeteRAG相较于基线方法在有效性和效率上实现了显著改进。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.10529v1",
      "published_date": "2025-04-12 13:12:54 UTC",
      "updated_date": "2025-04-12 13:12:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:17:06.122718"
    },
    {
      "arxiv_id": "2504.09203v1",
      "title": "AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images",
      "title_zh": "翻译失败",
      "authors": [
        "Saikat Dutta",
        "Akhil Vasim",
        "Siddhant Gole",
        "Hamid Rezatofighi",
        "Biplab Banerjee"
      ],
      "abstract": "Image segmentation beyond predefined categories is a key challenge in remote\nsensing, where novel and unseen classes often emerge during inference.\nOpen-vocabulary image Segmentation addresses these generalization issues in\ntraditional supervised segmentation models while reducing reliance on extensive\nper-pixel annotations, which are both expensive and labor-intensive to obtain.\nMost Open-Vocabulary Segmentation (OVS) methods are designed for natural images\nbut struggle with remote sensing data due to scale variations, orientation\nchanges, and complex scene compositions. This necessitates the development of\nOVS approaches specifically tailored for remote sensing. In this context, we\npropose AerOSeg, a novel OVS approach for remote sensing data. First, we\ncompute robust image-text correlation features using multiple rotated versions\nof the input image and domain-specific prompts. These features are then refined\nthrough spatial and class refinement blocks. Inspired by the success of the\nSegment Anything Model (SAM) in diverse domains, we leverage SAM features to\nguide the spatial refinement of correlation features. Additionally, we\nintroduce a semantic back-projection module and loss to ensure the seamless\npropagation of SAM's semantic information throughout the segmentation pipeline.\nFinally, we enhance the refined correlation features using a multi-scale\nattention-aware decoder to produce the final segmentation map. We validate our\nSAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark\nremote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms\nstate-of-the-art open-vocabulary segmentation methods, achieving an average\nimprovement of 2.54 h-mIoU.",
      "tldr_zh": "这篇论文提出 AerOSeg，一种针对遥感图像的 Open-Vocabulary Segmentation (OVS) 方法，旨在处理超出预定义类别的图像分割挑战，同时减少对昂贵像素级标注的依赖。AerOSeg 通过计算图像-文本相关特征（使用多个旋转图像和领域特定提示）、结合 Segment Anything Model (SAM) 的特征进行空间和类别细化、引入语义回投影模块以及多尺度注意力感知解码器，来提升遥感数据的分割准确性。实验在 iSAID、DLRSD 和 OpenEarthMap 等基准数据集上验证，该模型优于现有 OVS 方法，平均 h-mIoU 提高了 2.54%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at EarthVision workshop, CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.09203v1",
      "published_date": "2025-04-12 13:06:46 UTC",
      "updated_date": "2025-04-12 13:06:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:17:19.266262"
    },
    {
      "arxiv_id": "2504.09197v1",
      "title": "Graph Learning-Driven Multi-Vessel Association: Fusing Multimodal Data for Maritime Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxu Lu",
        "Kaisen Yang",
        "Dong Yang",
        "Haifeng Ding",
        "Jinxian Weng",
        "Ryan Wen Liu"
      ],
      "abstract": "Ensuring maritime safety and optimizing traffic management in increasingly\ncrowded and complex waterways require effective waterway monitoring. However,\ncurrent methods struggle with challenges arising from multimodal data, such as\ndimensional disparities, mismatched target counts, vessel scale variations,\nocclusions, and asynchronous data streams from systems like the automatic\nidentification system (AIS) and closed-circuit television (CCTV). Traditional\nmulti-target association methods often struggle with these complexities,\nparticularly in densely trafficked waterways. To overcome these issues, we\npropose a graph learning-driven multi-vessel association (GMvA) method tailored\nfor maritime multimodal data fusion. By integrating AIS and CCTV data, GMvA\nleverages time series learning and graph neural networks to capture the\nspatiotemporal features of vessel trajectories effectively. To enhance feature\nrepresentation, the proposed method incorporates temporal graph attention and\nspatiotemporal attention, effectively capturing both local and global vessel\ninteractions. Furthermore, a multi-layer perceptron-based uncertainty fusion\nmodule computes robust similarity scores, and the Hungarian algorithm is\nadopted to ensure globally consistent and accurate target matching. Extensive\nexperiments on real-world maritime datasets confirm that GMvA delivers superior\naccuracy and robustness in multi-target association, outperforming existing\nmethods even in challenging scenarios with high vessel density and incomplete\nor unevenly distributed AIS and CCTV data.",
      "tldr_zh": "该研究针对海上交通监测的多模态数据挑战（如AIS和CCTV的维度差异、目标数量不匹配及异步数据流），提出了一种基于图学习的Multi-Vessel Association方法（GMvA），通过整合时间序列学习和Graph Neural Networks捕获船只轨迹的时空特征。GMvA incorporates temporal graph attention、spatiotemporal attention和多层感知器-based uncertainty fusion模块，并采用Hungarian algorithm进行全局一致的目标匹配。实验结果显示，在真实海上数据集上，GMvA 在高密度船只和数据不完整场景下，显著提升了多目标关联的准确性和鲁棒性，优于现有方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09197v1",
      "published_date": "2025-04-12 12:45:55 UTC",
      "updated_date": "2025-04-12 12:45:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:17:30.246213"
    },
    {
      "arxiv_id": "2504.09195v1",
      "title": "ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Tzoulio Chamiti",
        "Leandro Di Bella",
        "Adrian Munteanu",
        "Nikos Deligiannis"
      ],
      "abstract": "Tracking multiple objects based on textual queries is a challenging task that\nrequires linking language understanding with object association across frames.\nPrevious works typically train the whole process end-to-end or integrate an\nadditional referring text module into a multi-object tracker, but they both\nrequire supervised training and potentially struggle with generalization to\nopen-set queries. In this work, we introduce ReferGPT, a novel zero-shot\nreferring multi-object tracking framework. We provide a multi-modal large\nlanguage model (MLLM) with spatial knowledge enabling it to generate 3D-aware\ncaptions. This enhances its descriptive capabilities and supports a more\nflexible referring vocabulary without training. We also propose a robust\nquery-matching strategy, leveraging CLIP-based semantic encoding and fuzzy\nmatching to associate MLLM generated captions with user queries. Extensive\nexperiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that\nReferGPT achieves competitive performance against trained methods, showcasing\nits robustness and zero-shot capabilities in autonomous driving. The codes are\navailable on https://github.com/Tzoulio/ReferGPT",
      "tldr_zh": "这篇论文提出了 ReferGPT，一个零样本（zero-shot）引用多对象跟踪框架，旨在无需监督训练即可基于文本查询跟踪多个对象，解决传统方法在泛化能力上的局限。\nReferGPT 利用多模态大语言模型（MLLM）结合空间知识生成 3D 感知的标题，并引入 CLIP-based 语义编码和模糊匹配策略来关联 MLLM 生成的标题与用户查询，从而增强描述和匹配的灵活性。\n实验在 Refer-KITTI、Refer-KITTIv2 和 Refer-KITTI+ 数据集上表明，ReferGPT 与训练方法相比表现出色，展示了其在自动驾驶领域的鲁棒性和零样本性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted CVPR 2025 Workshop on Distillation of Foundation Models for\n  Autonomous Driving",
      "pdf_url": "http://arxiv.org/pdf/2504.09195v1",
      "published_date": "2025-04-12 12:33:15 UTC",
      "updated_date": "2025-04-12 12:33:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:17:42.347386"
    },
    {
      "arxiv_id": "2504.09185v1",
      "title": "Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction",
      "title_zh": "重复对比学习增强 Mamba 在时间序列预测中的选择性",
      "authors": [
        "Wenbo Yan",
        "Hanzhong Cao",
        "Ying Tan"
      ],
      "abstract": "Long sequence prediction is a key challenge in time series forecasting. While\nMamba-based models have shown strong performance due to their sequence\nselection capabilities, they still struggle with insufficient focus on critical\ntime steps and incomplete noise suppression, caused by limited selective\nabilities. To address this, we introduce Repetitive Contrastive Learning (RCL),\na token-level contrastive pretraining framework aimed at enhancing Mamba's\nselective capabilities. RCL pretrains a single Mamba block to strengthen its\nselective abilities and then transfers these pretrained parameters to\ninitialize Mamba blocks in various backbone models, improving their temporal\nprediction performance. RCL uses sequence augmentation with Gaussian noise and\napplies inter-sequence and intra-sequence contrastive learning to help the\nMamba module prioritize information-rich time steps while ignoring noisy ones.\nExtensive experiments show that RCL consistently boosts the performance of\nbackbone models, surpassing existing methods and achieving state-of-the-art\nresults. Additionally, we propose two metrics to quantify Mamba's selective\ncapabilities, providing theoretical, qualitative, and quantitative evidence for\nthe improvements brought by RCL.",
      "tldr_zh": "本研究针对时间序列预测中的长序列挑战，指出Mamba-based模型因selective abilities不足而难以聚焦关键时间步和抑制噪声。为解决此问题，提出Repetitive Contrastive Learning (RCL)，一个token-level的对比学习预训练框架，通过序列增强（如Gaussian noise）和inter-sequence及intra-sequence对比学习来增强Mamba模块的信息优先处理能力，并将预训练参数转移到backbone模型中。实验结果显示，RCL显著提升多种模型的预测性能，超越现有方法并实现state-of-the-art水平；此外，研究还引入两个新指标来量化Mamba的selective capabilities，提供理论、定性和定量证据支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09185v1",
      "published_date": "2025-04-12 11:57:27 UTC",
      "updated_date": "2025-04-12 11:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:17:54.233960"
    },
    {
      "arxiv_id": "2504.09184v2",
      "title": "Parameterized Synthetic Text Generation with SimpleStories",
      "title_zh": "翻译失败",
      "authors": [
        "Lennart Finke",
        "Chandan Sreedhara",
        "Thomas Dooms",
        "Mat Allen",
        "Emerald Zhang",
        "Juan Diego Rodriguez",
        "Noa Nabeshima",
        "Thomas Marshall",
        "Dan Braun"
      ],
      "abstract": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million samples each in English and Japanese. Through\nparameterizing prompts at multiple levels of abstraction, we achieve control\nover story characteristics at scale, inducing syntactic and semantic diversity.\nAblations on a newly trained model suite show improved sample efficiency and\nmodel interpretability compared to the TinyStories dataset. We open-source all\nconstituent parts of model creation, hoping to enable novel ways to study the\nend-to-end training process. As a byproduct, we move the frontier regarding the\nfewest-parameter language model that outputs grammatical natural language.",
      "tldr_zh": "本研究介绍了SimpleStories，这是一个包含200万样本的大型合成故事数据集，支持英文和日文版本，通过在多个抽象级别上parameterizing prompts来控制故事特性，实现大规模的句法和语义多样性。与TinyStories数据集相比，实验中的ablations显示新训练模型具有更高的样本效率和模型可解释性。该数据集及其模型创建过程已开源，以促进端到端训练过程的研究，并推动了输出语法自然语言的最少参数语言模型的边界。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09184v2",
      "published_date": "2025-04-12 11:44:47 UTC",
      "updated_date": "2025-05-16 11:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:18:05.089312"
    },
    {
      "arxiv_id": "2504.11477v1",
      "title": "SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Yunkai Zhang",
        "Shiyin Wei",
        "Yong Huang",
        "Yawu Su",
        "Shanshan Lu",
        "Hui Li"
      ],
      "abstract": "Existing computer vision(CV)-based structural damage identification models\ndemonstrate notable accuracy in categorizing and localizing damage. However,\nthese models present several critical limitations that hinder their practical\napplication in civil engineering(CE). Primarily, their ability to recognize\ndamage types remains constrained, preventing comprehensive analysis of the\nhighly varied and complex conditions encountered in real-world CE structures.\nSecond, these models lack linguistic capabilities, rendering them unable to\narticulate structural damage characteristics through natural language\ndescriptions. With the continuous advancement of artificial intelligence(AI),\nlarge multi-modal models(LMMs) have emerged as a transformative solution,\nenabling the unified encoding and alignment of textual and visual data. These\nmodels can autonomously generate detailed descriptive narratives of structural\ndamage while demonstrating robust generalization across diverse scenarios and\ntasks. This study introduces SDIGLM, an innovative LMM for structural damage\nidentification, developed based on the open-source VisualGLM-6B architecture.\nTo address the challenge of adapting LMMs to the intricate and varied operating\nconditions in CE, this work integrates a U-Net-based semantic segmentation\nmodule to generate defect segmentation maps as visual Chain of Thought(CoT).\nAdditionally, a multi-round dialogue fine-tuning dataset is constructed to\nenhance logical reasoning, complemented by a language CoT formed through prompt\nengineering. By leveraging this multi-modal CoT, SDIGLM surpasses\ngeneral-purpose LMMs in structural damage identification, achieving an accuracy\nof 95.24% across various infrastructure types. Moreover, the model effectively\ndescribes damage characteristics such as hole size, crack direction, and\ncorrosion severity.",
      "tldr_zh": "本研究针对现有计算机视觉(CV)模型在结构损伤识别中的局限性，如无法全面分析复杂条件和缺乏自然语言描述能力，提出了一种创新的Large Multi-Modal Models (LMMs)框架SDIGLM。SDIGLM基于VisualGLM-6B架构，整合U-Net-based语义分割模块生成visual Chain of Thought (CoT)损伤地图，并通过多轮对话微调数据集和prompt engineering构建language CoT，以提升逻辑推理和多模态处理能力。实验结果显示，SDIGLM在各种基础设施上实现了95.24%的准确率，并能有效描述损伤特征如hole size、crack direction和corrosion severity，超越了通用LMMs的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11477v1",
      "published_date": "2025-04-12 11:37:10 UTC",
      "updated_date": "2025-04-12 11:37:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:18:19.978063"
    },
    {
      "arxiv_id": "2504.10527v1",
      "title": "Explainable Artificial Intelligence techniques for interpretation of food datasets: a review",
      "title_zh": "可解释人工智能技术用于食品数据集解释：综述",
      "authors": [
        "Leonardo Arrighi",
        "Ingrid Alves de Moraes",
        "Marco Zullich",
        "Michele Simonato",
        "Douglas Fernandes Barbin",
        "Sylvio Barbon Junior"
      ],
      "abstract": "Artificial Intelligence (AI) has become essential for analyzing complex data\nand solving highly-challenging tasks. It is being applied across numerous\ndisciplines beyond computer science, including Food Engineering, where there is\na growing demand for accurate and trustworthy predictions to meet stringent\nfood quality standards. However, this requires increasingly complex AI models,\nraising reliability concerns. In response, eXplainable AI (XAI) has emerged to\nprovide insights into AI decision-making, aiding model interpretation by\ndevelopers and users. Nevertheless, XAI remains underutilized in Food\nEngineering, limiting model reliability. For instance, in food quality control,\nAI models using spectral imaging can detect contaminants or assess freshness\nlevels, but their opaque decision-making process hinders adoption. XAI\ntechniques such as SHAP (Shapley Additive Explanations) and Grad-CAM\n(Gradient-weighted Class Activation Mapping) can pinpoint which spectral\nwavelengths or image regions contribute most to a prediction, enhancing\ntransparency and aiding quality control inspectors in verifying AI-generated\nassessments. This survey presents a taxonomy for classifying food quality\nresearch using XAI techniques, organized by data types and explanation methods,\nto guide researchers in choosing suitable approaches. We also highlight trends,\nchallenges, and opportunities to encourage the adoption of XAI in Food\nEngineering.",
      "tldr_zh": "这篇综述论文探讨了Explainable AI (XAI)技术在食品数据集解释中的应用，以解决AI模型在食品工程领域（如质量控制）中的可靠性问题。论文提出一个分类法（taxonomy），根据数据类型和解释方法组织XAI技术，帮助研究者选择合适的方法，例如使用SHAP (Shapley Additive Explanations)和Grad-CAM (Gradient-weighted Class Activation Mapping)来识别关键特征，如光谱波长或图像区域。最终，论文总结了XAI在食品工程中的趋势、挑战和机会，推动其采用以提升AI决策的透明度和可信度。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "A.1"
      ],
      "primary_category": "cs.AI",
      "comment": "33 pages, 8 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.10527v1",
      "published_date": "2025-04-12 11:10:43 UTC",
      "updated_date": "2025-04-12 11:10:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:18:29.899234"
    },
    {
      "arxiv_id": "2504.09179v1",
      "title": "A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Wen",
        "Shijie Guo",
        "Wenbo Ning",
        "Rui Cao",
        "Yan Niu",
        "Bin Wan",
        "Peng Wei",
        "Xiaobo Liu",
        "Jie Xiang"
      ],
      "abstract": "In open data sets of functional magnetic resonance imaging (fMRI), the\nheterogeneity of the data is typically attributed to a combination of factors,\nincluding differences in scanning procedures, the presence of confounding\neffects, and population diversities between multiple sites. These factors\ncontribute to the diminished effectiveness of representation learning, which in\nturn affects the overall efficacy of subsequent classification procedures. To\naddress these limitations, we propose a novel multi-site adversarial learning\nnetwork (MSalNET) for fMRI-based mental disorder detection. Firstly, a\nrepresentation learning module is introduced with a node information assembly\n(NIA) mechanism to better extract features from functional connectivity (FC).\nThis mechanism aggregates edge information from both horizontal and vertical\ndirections, effectively assembling node information. Secondly, to generalize\nthe feature across sites, we proposed a site-level feature extraction module\nthat can learn from individual FC data, which circumvents additional prior\ninformation. Lastly, an adversarial learning network is proposed as a means of\nbalancing the trade-off between individual classification and site regression\ntasks, with the introduction of a novel loss function. The proposed method was\nevaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data\nExchange (ABIDE) and ADHD-200. The results indicate that the proposed method\nachieves a better performance than other related algorithms with the accuracy\nof 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore,\nthe result of the site regression indicates that the proposed method reduces\nsite variability from a data-driven perspective. The most discriminative brain\nregions revealed by NIA are consistent with statistical findings, uncovering\nthe \"black box\" of deep learning to a certain extent.",
      "tldr_zh": "该研究针对多站点 fMRI 数据中的异质性问题（如扫描程序差异和混杂因素），提出了一种名为 MSalNET 的对抗学习框架，用于精神障碍识别。\n框架包括节点信息组装 (NIA) 机制从功能连接 (FC) 中提取特征、站点级特征提取模块实现跨站点泛化，以及一个对抗学习网络通过新颖损失函数平衡个体分类和站点回归任务。\n实验结果显示，在 ABIDE 和 ADHD-200 数据集上，MSalNET 分别获得 75.56% 和 68.92% 的准确率，优于其他算法，并有效减少站点变异性，同时通过 NIA 揭示关键脑区以提升模型解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09179v1",
      "published_date": "2025-04-12 10:58:19 UTC",
      "updated_date": "2025-04-12 10:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:18:42.712175"
    },
    {
      "arxiv_id": "2504.09164v2",
      "title": "Can postgraduate translation students identify machine-generated text?",
      "title_zh": "翻译研究生能识别机器生成的文本吗？",
      "authors": [
        "Michael Farrell"
      ],
      "abstract": "Given the growing use of generative artificial intelligence as a tool for\ncreating multilingual content and bypassing both machine and traditional\ntranslation methods, this study explores the ability of linguistically trained\nindividuals to discern machine-generated output from human-written text (HT).\nAfter brief training sessions on the textual anomalies typically found in\nsynthetic text (ST), twenty-three postgraduate translation students analysed\nexcerpts of Italian prose and assigned likelihood scores to indicate whether\nthey believed they were human-written or AI-generated (ChatGPT-4o). The results\nshow that, on average, the students struggled to distinguish between HT and ST,\nwith only two participants achieving notable accuracy. Closer analysis revealed\nthat the students often identified the same textual anomalies in both HT and\nST, although features such as low burstiness and self-contradiction were more\nfrequently associated with ST. These findings suggest the need for improvements\nin the preparatory training. Moreover, the study raises questions about the\nnecessity of editing synthetic text to make it sound more human-like and\nrecommends further research to determine whether AI-generated text is already\nsufficiently natural-sounding not to require further refinement.",
      "tldr_zh": "这篇论文调查了翻译研究生辨别机器生成文本（machine-generated text）和人类撰写文本（HT）的能力，通过对23名研究生进行简短培训，让他们分析意大利散文摘录并评分判断文本是否由AI如ChatGPT-4o生成。结果显示，大多数学生难以准确区分HT和合成文本（ST），仅有两人表现出较高准确率，且他们常将相同文本异常特征误归类，尽管低爆发性和自相矛盾更常与ST相关。研究建议改进培训内容，并进一步探讨是否需要编辑AI生成文本以提升其人类化程度，以及AI文本是否已足够自然无需额外优化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, accepted for MT Summit 2025, Geneva, Switzerland, 23-27\n  June 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.09164v2",
      "published_date": "2025-04-12 09:58:09 UTC",
      "updated_date": "2025-04-18 13:42:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:18:53.932677"
    },
    {
      "arxiv_id": "2504.09101v1",
      "title": "Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
      "title_zh": "利用基于时间的 VQ-VAE 生成合成飞机轨迹",
      "authors": [
        "Abdulmajid Murad",
        "Massimiliano Ruocco"
      ],
      "abstract": "In modern air traffic management, generating synthetic flight trajectories\nhas emerged as a promising solution for addressing data scarcity, protecting\nsensitive information, and supporting large-scale analyses. In this paper, we\npropose a novel method for trajectory synthesis by adapting the Time-Based\nVector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages\ntime-frequency domain processing, vector quantization, and transformer-based\npriors to capture both global and local dynamics in flight data. By\ndiscretizing the latent space and integrating transformer priors, the model\nlearns long-range spatiotemporal dependencies and preserves coherence across\nentire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite\nof quality, statistical, and distributional metrics, as well as a flyability\nassessment conducted in an open-source air traffic simulator. Results indicate\nthat TimeVQVAE outperforms a temporal convolutional VAE baseline, generating\nsynthetic trajectories that mirror real flight data in terms of spatial\naccuracy, temporal consistency, and statistical properties. Furthermore, the\nsimulator-based assessment shows that most generated trajectories maintain\noperational feasibility, although occasional outliers underscore the potential\nneed for additional domain-specific constraints. Overall, our findings\nunderscore the importance of multi-scale representation learning for capturing\ncomplex flight behaviors and demonstrate the promise of TimeVQVAE in producing\nrepresentative synthetic trajectories for downstream tasks such as model\ntraining, airspace design, and air traffic forecasting.",
      "tldr_zh": "本文提出了一种基于 Time-Based VQ-VAE 的方法，用于生成合成飞行轨迹，以解决航空交通管理中的数据稀缺、敏感信息保护和大规模分析问题。该方法通过时间-频率域处理、向量量化以及 transformer-based priors 捕捉飞行数据的全局和局部动态，并学习长程时空依赖性以保持轨迹连贯性。实验评估显示，TimeVQVAE 在质量、统计和分布指标上优于 temporal convolutional VAE 基线，生成的轨迹在空间准确性、时间一致性和统计属性上与真实数据相似，且在开源模拟器中多数轨迹保持操作可行性。总体而言，该研究突出了多尺度表示学习在捕捉复杂飞行行为方面的潜力，并为模型训练、airspace design 和空气交通预测等下游任务提供了可靠的合成数据。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper was presented at the 25th Integrated Communications,\n  Navigation and Surveillance Conference (ICNS 2025), April 8--10, 2025,\n  Brussels, Belgium",
      "pdf_url": "http://arxiv.org/pdf/2504.09101v1",
      "published_date": "2025-04-12 06:46:51 UTC",
      "updated_date": "2025-04-12 06:46:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:19:06.921628"
    },
    {
      "arxiv_id": "2504.09100v1",
      "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Chengyu Wang",
        "Taolin Zhang",
        "Richang Hong",
        "Jun Huang"
      ],
      "abstract": "Recently, the reasoning capabilities of large reasoning models (LRMs), such\nas DeepSeek-R1, have seen significant advancements through the slow thinking\nprocess. Despite these achievements, the substantial computational demands of\nLRMs present considerable challenges. In contrast, small reasoning models\n(SRMs), often distilled from larger ones, offer greater efficiency and can\nexhibit distinct capabilities and cognitive trajectories compared to LRMs. This\nwork surveys around 170 recently published papers on SRMs for tackling various\ncomplex reasoning tasks. We review the current landscape of SRMs and analyze\ndiverse training and inference techniques related to SRMs. Furthermore, we\nprovide a comprehensive review of SRMs for domain-specific applications and\ndiscuss possible future research directions. This survey serves as an essential\nreference for researchers to leverage or develop SRMs for advanced reasoning\nfunctionalities with high efficiency.",
      "tldr_zh": "这篇调查综述了小规模推理模型 (SRMs)，这些模型从大型推理模型 (LRMs) 如 DeepSeek-R1 中蒸馏而来，具有更高的计算效率和独特的认知路径，能够有效处理复杂推理任务。作者分析了约 170 篇相关论文，涵盖 SRMs 的训练、推理技术以及在特定领域的应用，如提升模型性能和适应性。最终，该调查为研究者提供参考，帮助开发高效的 SRMs，以实现高级推理功能，并探讨了未来研究方向。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09100v1",
      "published_date": "2025-04-12 06:45:57 UTC",
      "updated_date": "2025-04-12 06:45:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:19:18.581149"
    },
    {
      "arxiv_id": "2504.16096v2",
      "title": "BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification",
      "title_zh": "BrainPrompt：多层次脑提示增强用于神经系统疾病识别",
      "authors": [
        "Jiaxing Xu",
        "Kai He",
        "Yue Tang",
        "Wei Li",
        "Mengcheng Lan",
        "Xia Dong",
        "Yiping Ke",
        "Mengling Feng"
      ],
      "abstract": "Neurological conditions, such as Alzheimer's Disease, are challenging to\ndiagnose, particularly in the early stages where symptoms closely resemble\nhealthy controls. Existing brain network analysis methods primarily focus on\ngraph-based models that rely solely on imaging data, which may overlook\nimportant non-imaging factors and limit the model's predictive power and\ninterpretability. In this paper, we present BrainPrompt, an innovative\nframework that enhances Graph Neural Networks (GNNs) by integrating Large\nLanguage Models (LLMs) with knowledge-driven prompts, enabling more effective\ncapture of complex, non-imaging information and external knowledge for\nneurological disease identification. BrainPrompt integrates three types of\nknowledge-driven prompts: (1) ROI-level prompts to encode the identity and\nfunction of each brain region, (2) subject-level prompts that incorporate\ndemographic information, and (3) disease-level prompts to capture the temporal\nprogression of disease. By leveraging these multi-level prompts, BrainPrompt\neffectively harnesses knowledge-enhanced multi-modal information from LLMs,\nenhancing the model's capability to predict neurological disease stages and\nmeanwhile offers more interpretable results. We evaluate BrainPrompt on two\nresting-state functional Magnetic Resonance Imaging (fMRI) datasets from\nneurological disorders, showing its superiority over state-of-the-art methods.\nAdditionally, a biomarker study demonstrates the framework's ability to extract\nvaluable and interpretable information aligned with domain knowledge in\nneuroscience. The code is available at\nhttps://github.com/AngusMonroe/BrainPrompt",
      "tldr_zh": "本研究提出BrainPrompt框架，通过整合Large Language Models (LLMs)来增强Graph Neural Networks (GNNs)，以更有效地识别神经系统疾病，如阿尔茨海默病。框架引入三种知识驱动提示：ROI-level prompts编码脑区身份和功能、subject-level prompts纳入人口统计信息，以及disease-level prompts捕捉疾病的时间进程，从而更好地整合多模态信息并提升模型的可解释性。在两个fMRI数据集上的实验显示，BrainPrompt优于现有方法，并在生物标记物研究中提取出与神经科学领域知识一致的可解释信息。总的来说，该框架为早期神经疾病诊断提供了更准确和可靠的工具。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Early accepted by MICCAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.16096v2",
      "published_date": "2025-04-12 06:45:16 UTC",
      "updated_date": "2025-05-19 10:33:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:19:30.401458"
    },
    {
      "arxiv_id": "2504.09095v1",
      "title": "Privacy Preservation in Gen AI Applications",
      "title_zh": "生成式 AI 应用中的隐私保护",
      "authors": [
        "Swetha S",
        "Ram Sundhar K Shaju",
        "Rakshana M",
        "Ganesh R",
        "Balavedhaa S",
        "Thiruvaazhi U"
      ],
      "abstract": "The ability of machines to comprehend and produce language that is similar to\nthat of humans has revolutionized sectors like customer service, healthcare,\nand finance thanks to the quick advances in Natural Language Processing (NLP),\nwhich are fueled by Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs). However, because LLMs trained on large datasets may\nunintentionally absorb and reveal Personally Identifiable Information (PII)\nfrom user interactions, these capabilities also raise serious privacy concerns.\nDeep neural networks' intricacy makes it difficult to track down or stop the\ninadvertent storing and release of private information, which raises serious\nconcerns about the privacy and security of AI-driven data. This study tackles\nthese issues by detecting Generative AI weaknesses through attacks such as data\nextraction, model inversion, and membership inference. A privacy-preserving\nGenerative AI application that is resistant to these assaults is then\ndeveloped. It ensures privacy without sacrificing functionality by using\nmethods to identify, alter, or remove PII before to dealing with LLMs. In order\nto determine how well cloud platforms like Microsoft Azure, Google Cloud, and\nAWS provide privacy tools for protecting AI applications, the study also\nexamines these technologies. In the end, this study offers a fundamental\nprivacy paradigm for generative AI systems, focusing on data security and moral\nAI implementation, and opening the door to a more secure and conscientious use\nof these tools.",
      "tldr_zh": "本研究探讨了 Generative AI 和 Large Language Models (LLMs) 在应用中可能无意泄露 Personally Identifiable Information (PII) 的隐私风险，这些问题源于模型训练数据的复杂性和深度神经网络的特性。研究通过检测攻击如数据提取、模型反转和成员推理来识别 Generative AI 的弱点，并开发了一个隐私保护应用，使用方法包括在处理 LLMs 前识别、修改或删除 PII，以确保功能性不受影响。同时，研究评估了 Microsoft Azure、Google Cloud 和 AWS 等云平台的隐私工具，并提出一个基本隐私框架，强调数据安全和道德 AI 实施，促进更安全的使用。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09095v1",
      "published_date": "2025-04-12 06:19:37 UTC",
      "updated_date": "2025-04-12 06:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:19:43.118026"
    },
    {
      "arxiv_id": "2504.09081v2",
      "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Prabhat Pandey",
        "Rupak Vignesh Swaminathan",
        "K V Vijay Girish",
        "Arunasish Sen",
        "Jian Xie",
        "Grant P. Strimel",
        "Andreas Schwarz"
      ],
      "abstract": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
      "tldr_zh": "本研究引入了SIFT-50M，这是一个包含50M示例的大型多语言数据集，旨在用于语音指令微调(Speech Instruction Fine-Tuning)和预训练语音-文本大语言模型(LLMs)。数据集基于14K小时的公开语音语料库，利用LLMs和现成专家模型构建，涵盖五种语言的多样化语音理解和可控语音生成指令。使用SIFT-50M训练的SIFT-LLM在指令遵循基准上超过了现有模型，并在基础语音任务中表现出竞争性性能；此外，还发布了EvalSIFT基准数据集，以评估语音-文本LLMs的指令遵循能力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09081v2",
      "published_date": "2025-04-12 04:45:48 UTC",
      "updated_date": "2025-04-17 17:34:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:19:55.140158"
    },
    {
      "arxiv_id": "2504.09064v1",
      "title": "PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations",
      "title_zh": "翻译失败",
      "authors": [
        "Vikas Natesh",
        "H. T. Kung"
      ],
      "abstract": "We present PQS, which uses three techniques together - Prune, Quantize, and\nSort - to achieve low-bitwidth accumulation of dot products in neural network\ncomputations. In conventional quantized (e.g., 8-bit) dot products, partial\nresults are accumulated into wide (e.g., 32-bit) accumulators to avoid\noverflows when accumulating intermediate partial sums. However, such wide\naccumulators increase memory bandwidth usage and reduce energy efficiency. We\nshow that iterative N:M pruning in floating point followed by quantization to 8\n(or fewer) bits, and accumulation of partial products in a sorted order (\"small\nto large\") allows for accurate, compressed models with short dot product\nlengths that do not require wide accumulators. We design, analyze, and\nimplement the PQS algorithm to eliminate accumulation overflows at inference\ntime for several neural networks. Our method offers a 2.5x reduction in\naccumulator bitwidth while achieving model accuracy on par with floating-point\nbaselines for multiple image classification tasks.",
      "tldr_zh": "我们提出了 PQS 方法，通过结合 Prune（修剪）、Quantize（量化）和 Sort（排序）技术，实现神经网络计算中点积的低位宽积累，从而减少内存带宽使用并提高能效。PQS 算法在浮点数上进行迭代 N:M 修剪后量化到 8-bit 或更少，并以从小到大的顺序积累部分乘积，成功消除推理时的积累溢出问题。与传统宽积累器（如 32-bit）相比，该方法在多个图像分类任务中实现了 2.5 倍的积累器位宽减少，同时保持与浮点基准相当的模型准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09064v1",
      "published_date": "2025-04-12 03:51:42 UTC",
      "updated_date": "2025-04-12 03:51:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:20:07.100316"
    },
    {
      "arxiv_id": "2504.09063v1",
      "title": "A Practical Approach to using Supervised Machine Learning Models to Classify Aviation Safety Occurrences",
      "title_zh": "翻译失败",
      "authors": [
        "Bryan Y. Siow"
      ],
      "abstract": "This paper describes a practical approach of using supervised machine\nlearning (ML) models to assist safety investigators to classify aviation\noccurrences into either incident or serious incident categories. Our\nimplementation currently deployed as a ML web application is trained on a\nlabelled dataset derived from publicly available aviation investigation\nreports. A selection of five supervised learning models (Support Vector\nMachine, Logistic Regression, Random Forest Classifier, XGBoost and K-Nearest\nNeighbors) were evaluated. This paper showed the best performing ML algorithm\nwas the Random Forest Classifier with accuracy = 0.77, F1 Score = 0.78 and MCC\n= 0.51 (average of 100 sample runs). The study had also explored the effect of\napplying Synthetic Minority Over-sampling Technique (SMOTE) to the imbalanced\ndataset, and the overall observation ranged from no significant effect to\nsubstantial degradation in performance for some of the models after the SMOTE\nadjustment.",
      "tldr_zh": "这篇论文提出了一种实用方法，使用监督机器学习模型帮助安全调查员将航空安全事件分类为 incident 或 serious incident，基于公开的标记数据集进行训练。研究评估了五种模型，包括 Support Vector Machine、Logistic Regression、Random Forest Classifier、XGBoost 和 K-Nearest Neighbors，结果显示 Random Forest Classifier 表现最佳，准确率达 0.77、F1 Score 为 0.78 和 MCC 为 0.51（100 次样本运行的平均值）。此外，论文探讨了在不平衡数据集上应用 Synthetic Minority Over-sampling Technique (SMOTE) 的影响，发现其效果从无显著变化到某些模型性能明显下降。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.09063v1",
      "published_date": "2025-04-12 03:46:33 UTC",
      "updated_date": "2025-04-12 03:46:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:20:19.188264"
    },
    {
      "arxiv_id": "2504.09060v1",
      "title": "Multimodal 3D Genome Pre-training",
      "title_zh": "多模态三维基因组预训练",
      "authors": [
        "Minghao Yang",
        "Pengteng Li",
        "Yan Liang",
        "Qianyi Cai",
        "Zhihang Zheng",
        "Shichen Zhang",
        "Pengfei Zhang",
        "Zhi-An Huang",
        "Hui Xiong"
      ],
      "abstract": "Deep learning techniques have driven significant progress in various\nanalytical tasks within 3D genomics in computational biology. However, a\nholistic understanding of 3D genomics knowledge remains underexplored. Here, we\npropose MIX-HIC, the first multimodal foundation model of 3D genome that\nintegrates both 3D genome structure and epigenomic tracks, which obtains\nunified and comprehensive semantics. For accurate heterogeneous semantic\nfusion, we design the cross-modal interaction and mapping blocks for robust\nunified representation, yielding the accurate aggregation of 3D genome\nknowledge. Besides, we introduce the first large-scale dataset comprising over\n1 million pairwise samples of Hi-C contact maps and epigenomic tracks for\nhigh-quality pre-training, enabling the exploration of functional implications\nin 3D genomics. Extensive experiments show that MIX-HIC can significantly\nsurpass existing state-of-the-art methods in diverse downstream tasks. This\nwork provides a valuable resource for advancing 3D genomics research.",
      "tldr_zh": "本研究提出MIX-HIC，这是首个整合3D基因组结构和表观遗传轨迹的多模态基础模型，旨在获得统一的全面语义。通过设计跨模态交互和映射块，实现异构语义的准确融合，并构建了首个大规模数据集（包含超过100万对Hi-C接触图和表观遗传轨迹样本）用于高质量预训练。实验结果显示，MIX-HIC在各种下游任务中显著超越现有最先进方法，为推进3D基因组研究提供了宝贵资源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09060v1",
      "published_date": "2025-04-12 03:31:03 UTC",
      "updated_date": "2025-04-12 03:31:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:20:30.789109"
    },
    {
      "arxiv_id": "2504.09058v1",
      "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement",
      "title_zh": "翻译失败",
      "authors": [
        "Chengyuan Liu",
        "Shihang Wang",
        "Lizhi Qing",
        "Kaisong Song",
        "Junjie Cao",
        "Jun Lin",
        "Ji Zhang",
        "Ang Li",
        "Kun Kuang",
        "Fei Wu"
      ],
      "abstract": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS.",
      "tldr_zh": "该研究针对需要领域特定专业知识的任务（如法律问题），探讨了在Chain of Thoughts (CoTs)上使用逐步监督和Monte Carlo Tree Search (MCTS)的优化潜力。作者提出Stepwise Domain Knowledge-Driven Reasoning Optimization框架，通过MCTS算法为问题提供步级监督，强调对专业知识的理解和推理。 additionally, they introduced Preference Optimization towards Reflection Paths，以迭代方式学习自我反思路径，提升推理质量。实验结果显示，该方法在各种法律领域问题上表现出色，并揭示了多样化发现，鼓励进一步探索领域特定LLMs和MCTS的应用。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2504.09058v1",
      "published_date": "2025-04-12 03:25:01 UTC",
      "updated_date": "2025-04-12 03:25:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:20:43.202313"
    },
    {
      "arxiv_id": "2504.12326v1",
      "title": "Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis",
      "title_zh": "翻译失败",
      "authors": [
        "Shahriar Noroozizadeh",
        "Jeremy C. Weiss"
      ],
      "abstract": "Clinical case reports and discharge summaries may be the most complete and\naccurate summarization of patient encounters, yet they are finalized, i.e.,\ntimestamped after the encounter. Complementary data structured streams become\navailable sooner but suffer from incompleteness. To train models and algorithms\non more complete and temporally fine-grained data, we construct a pipeline to\nphenotype, extract, and annotate time-localized findings within case reports\nusing large language models. We apply our pipeline to generate an open-access\ntextual time series corpus for Sepsis-3 comprising 2,139 case reports from the\nPubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA\nand timeline annotations from I2B2/MIMIC-IV and compare the results to\nphysician-expert annotations. We show high recovery rates of clinical findings\n(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and\nstrong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B\nInstruct--0.932). Our work characterizes the ability of LLMs to time-localize\nclinical findings in text, illustrating the limitations of LLM use for temporal\nreconstruction and providing several potential avenues of improvement via\nmultimodal integration.",
      "tldr_zh": "该研究使用大型语言模型(LLMs)构建了一个管道，从临床病例报告中提取、标注和时间定位的临床发现，以重建脓毒症(Sepsis)患者的轨迹，从而提供更完整和细粒度的训练数据。作者生成了一个开源文本时间序列语料库，包含来自PubMed-Open Access的2,139个Sepsis-3病例报告，并通过与I2B2/MIMIC-IV数据比较验证。实验结果显示，LLMs在事件匹配率(约0.755)和时间一致性(约0.932)上表现出高恢复率。该工作揭示了LLMs在时间重建中的局限性，并建议通过多模态整合来进一步改进。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12326v1",
      "published_date": "2025-04-12 03:07:44 UTC",
      "updated_date": "2025-04-12 03:07:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:20:56.190766"
    },
    {
      "arxiv_id": "2504.11474v1",
      "title": "Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD",
      "title_zh": "翻译失败",
      "authors": [
        "Byunggun Kim",
        "Younghun Kwon"
      ],
      "abstract": "In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of\nthe common mental diseases discovered not only in children but also in adults.\nIn this context, we propose a ADHD diagnosis transformer model that can\neffectively simultaneously find important brain spatiotemporal biomarkers from\nresting-state functional magnetic resonance (rs-fMRI). This model not only\nlearns spatiotemporal individual features but also learns the correlation with\nfull attention structures specialized in ADHD diagnosis. In particular, it\nfocuses on learning local blood oxygenation level dependent (BOLD) signals and\ndistinguishing important regions of interest (ROI) in the brain. Specifically,\nthe three proposed methods for ADHD diagnosis transformer are as follows.\nFirst, we design a CNN-based embedding block to obtain more expressive\nembedding features in brain region attention. It is reconstructed based on the\npreviously CNN-based ADHD diagnosis models for the transformer. Next, for\nindividual spatiotemporal feature attention, we change the attention method to\nlocal temporal attention and ROI-rank based masking. For the temporal features\nof fMRI, the local temporal attention enables to learn local BOLD signal\nfeatures with only simple window masking. For the spatial feature of fMRI,\nROI-rank based masking can distinguish ROIs with high correlation in ROI\nrelationships based on attention scores, thereby providing a more specific\nbiomarker for ADHD diagnosis. The experiment was conducted with various types\nof transformer models. To evaluate these models, we collected the data from 939\nindividuals from all sites provided by the ADHD-200 competition. Through this,\nthe spatiotemporal enhanced transformer for ADHD diagnosis outperforms the\nperformance of other different types of transformer variants. (77.78ACC\n76.60SPE 79.22SEN 79.30AUC)",
      "tldr_zh": "本研究提出了一种增强型 Transformer 模型，用于诊断注意力缺陷多动障碍（ADHD），通过从静息态功能磁共振成像（rs-fMRI）中提取重要脑部时空生物标记。该模型结合 CNN-based embedding 块来获取更具表现力的脑区注意力特征，并采用局部时间注意力（local temporal attention）和 ROI-rank based masking 方法，分别学习局部 BOLD 信号特征和区分高相关脑区（ROI），从而提供更精确的 ADHD 生物标记。实验使用 ADHD-200 比赛的 939 个个体数据进行评估，结果显示该模型在准确率（77.78% ACC）、特异性（76.60% SPE）、敏感性（79.22% SEN）和 AUC（79.30% AUC）等方面优于其他 Transformer 变体。总的来说，此方法提升了 ADHD 诊断的性能，为脑部生物标记分析提供了新途径。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11474v1",
      "published_date": "2025-04-12 02:03:35 UTC",
      "updated_date": "2025-04-12 02:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:21:06.678062"
    },
    {
      "arxiv_id": "2504.09046v2",
      "title": "An Enhanced Iterative Deepening Search Algorithm for the Unrestricted Container Rehandling Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoqi Wang",
        "Jiawei Li"
      ],
      "abstract": "In container terminal yards, the Container Rehandling Problem (CRP) involves\nrearranging containers between stacks under specific operational rules, and it\nis a pivotal optimization challenge in intelligent container scheduling\nsystems. Existing CRP studies primarily focus on minimizing reallocation costs\nusing two-dimensional bay structures, considering factors such as container\nsize, weight, arrival sequences, and retrieval priorities. This paper\nintroduces an enhanced deepening search algorithm integrated with improved\nlower bounds to boost search efficiency. To further reduce the search space, we\ndesign mutually consistent pruning rules to avoid excessive computational\noverhead. The proposed algorithm is validated on three widely used benchmark\ndatasets for the Unrestricted Container Rehandling Problem (UCRP). Experimental\nresults demonstrate that our approach outperforms state-of-the-art exact\nalgorithms in solving the more general UCRP variant, particularly exhibiting\nsuperior efficiency when handling containers within the same priority group\nunder strict time constraints.",
      "tldr_zh": "本文针对集装箱码头中的 Container Rehandling Problem (CRP)，提出了一种增强的 Iterative Deepening Search Algorithm，结合改进的下界来提升搜索效率，并设计相互一致的剪枝规则以减少计算开销。相比现有研究，该算法特别适用于更通用的 Unrestricted Container Rehandling Problem (UCRP)，在三个基准数据集上验证后显示出优越性能。实验结果表明，该方法在处理相同优先级组的集装箱时，尤其在严格时间约束下，优于现有精确算法，显著提高了优化效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Verification confirmed that this article used data without\n  authorization from the original owners, violating ethical standards for\n  scientific data sharing. To protect data copyright and maintain research\n  integrity, the article is retracted. Future content will strictly follow data\n  usage protocols",
      "pdf_url": "http://arxiv.org/pdf/2504.09046v2",
      "published_date": "2025-04-12 01:58:30 UTC",
      "updated_date": "2025-04-19 05:30:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:21:19.113530"
    },
    {
      "arxiv_id": "2504.09039v1",
      "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Gen Li",
        "Yang Xiao",
        "Jie Ji",
        "Kaiyuan Deng",
        "Bo Hui",
        "Linke Guo",
        "Xiaolong Ma"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have achieved remarkable success in\ngenerating high-quality images from textual prompts. However, their ability to\nstore vast amounts of knowledge raises concerns in scenarios where selective\nforgetting is necessary, such as removing copyrighted content, reducing biases,\nor eliminating harmful concepts. While existing unlearning methods can remove\ncertain concepts, they struggle with multi-concept forgetting due to\ninstability, residual knowledge persistence, and generation quality\ndegradation. To address these challenges, we propose \\textbf{Dynamic Mask\ncoupled with Concept-Aware Loss}, a novel unlearning framework designed for\nmulti-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask}\nmechanism adaptively updates gradient masks based on current optimization\nstates, allowing selective weight modifications that prevent interference with\nunrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly\nguides the unlearning process by enforcing semantic consistency through\nsuperclass alignment, while a regularization loss based on knowledge\ndistillation ensures that previously unlearned concepts remain forgotten during\nsequential unlearning. We conduct extensive experiments to evaluate our\napproach. Results demonstrate that our method outperforms existing unlearning\ntechniques in forgetting effectiveness, output fidelity, and semantic\ncoherence, particularly in multi-concept scenarios. Our work provides a\nprincipled and flexible framework for stable and high-fidelity unlearning in\ngenerative models. The code will be released publicly.",
      "tldr_zh": "本文提出了一种名为“Sculpting Memory”的框架，针对文本到图像 (T2I) diffusion models 中的多概念遗忘问题，通过“Dynamic Mask”机制动态更新梯度掩码以选择性地修改权重，避免干扰无关知识，以及“Concept-Aware Loss”通过超类对齐和知识蒸馏正则化来强制语义一致性。相比现有方法，该框架有效解决了多概念遗忘的不稳定性、残留知识和生成质量下降问题。实验结果表明，该方法在遗忘效果、输出保真度和语义连贯性上显著优于基线，尤其适用于多概念场景，并为生成模型提供了一个稳定且灵活的遗忘解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.09039v1",
      "published_date": "2025-04-12 01:38:58 UTC",
      "updated_date": "2025-04-12 01:38:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:21:31.924208"
    },
    {
      "arxiv_id": "2504.09037v1",
      "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
      "title_zh": "LLM 推理前沿综述：推理缩放、学习推理和智能体系统",
      "authors": [
        "Zixuan Ke",
        "Fangkai Jiao",
        "Yifei Ming",
        "Xuan-Phi Nguyen",
        "Austin Xu",
        "Do Xuan Long",
        "Minzhi Li",
        "Chengwei Qin",
        "Peifeng Wang",
        "Silvio Savarese",
        "Caiming Xiong",
        "Shafiq Joty"
      ],
      "abstract": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...",
      "tldr_zh": "这篇调查论文探讨了大型语言模型（LLMs）推理能力的最新前沿，包括推理扩展（Inference Scaling）、学习推理（Learning to Reason）和代理系统（Agentic Systems）。论文将现有方法分为两个维度：Regimes（推理阶段，涵盖推理时或通过训练实现）和Architectures（架构，从独立LLMs到整合外部工具的多代理协作），并分析输入级（如高质量prompt构建）和输出级（如样本精炼）技术。调查突出了从推理扩展向学习推理的转变（如DeepSeek-R1模型），以及向代理工作流的演进（如OpenAI Deep Research和Manus Agent），并涵盖了各种学习算法（如监督微调和强化学习PPO/GRPO）及代理设计（如生成器-评估器模式和LLM辩论）。整体框架为理解LLMs推理的演变提供了系统性洞见，推动了AI系统在逻辑推理和决策方面的进展。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "72 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.09037v1",
      "published_date": "2025-04-12 01:27:49 UTC",
      "updated_date": "2025-04-12 01:27:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:21:43.171067"
    },
    {
      "arxiv_id": "2504.09033v1",
      "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Snigdha Agarwal",
        "Neelam Sinha"
      ],
      "abstract": "Deep Convolutional Neural Networks have consistently proven to achieve\nstate-of-the-art results on a lot of imaging tasks over the past years'\nmajority of which comprise of high-quality data. However, it is important to\nwork on low-resolution images since it could be a cheaper alternative for\nremote healthcare access where the primary need of automated pathology\nidentification models occurs. Medical diagnosis using low-resolution images is\nchallenging since critical details may not be easily identifiable. In this\npaper, we report classification results by experimenting on different input\nimage sizes of Chest X-rays to deep CNN models and discuss the feasibility of\nclassification on varying image sizes. We also leverage the noisy labels in the\ndataset by proposing a Randomized Flipping of labels techniques. We use an\nensemble of multi-label classification models on frontal and lateral studies.\nOur models are trained on 5 out of the 14 chest pathologies of the publicly\navailable CheXpert dataset. We incorporate techniques such as augmentation,\nregularization for model improvement and use class activation maps to visualize\nthe neural network's decision making. Comparison with classification results on\ndata from 200 subjects, obtained on the corresponding high-resolution images,\nreported in the original CheXpert paper, has been presented. For pathologies\nCardiomegaly, Consolidation and Edema, we obtain 3% higher accuracy with our\nmodel architecture.",
      "tldr_zh": "该论文探讨了使用深度卷积神经网络（Deep Convolutional Neural Networks）对低分辨率胸部X光图像进行分类的问题，以提供远程医疗的低成本解决方案。研究者实验了不同输入图像大小，并提出Randomized Flipping of labels技巧来处理数据集中的噪声标签，同时采用多标签分类模型的集成、数据增强和正规化方法。结果显示，在CheXpert数据集的5种胸部病变上，该模型与高分辨率图像的基线相比，对于Cardiomegaly、Consolidation和Edema三种病变，准确率提高了3%。这项工作证明了低分辨率图像在自动化病理识别中的可行性，并通过Class Activation Maps可视化了神经网络的决策过程。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.09033v1",
      "published_date": "2025-04-12 01:13:00 UTC",
      "updated_date": "2025-04-12 01:13:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:21:54.548944"
    },
    {
      "arxiv_id": "2504.13191v1",
      "title": "Universal Representations for Classification-enhanced Lossy Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Nam Nguyen"
      ],
      "abstract": "In lossy compression, the classical tradeoff between compression rate and\nreconstruction distortion has traditionally guided algorithm design. However,\nBlau and Michaeli [5] introduced a generalized framework, known as the\nrate-distortion-perception (RDP) function, incorporating perceptual quality as\nan additional dimension of evaluation. More recently, the\nrate-distortion-classification (RDC) function was investigated in [19],\nevaluating compression performance by considering classification accuracy\nalongside distortion. In this paper, we explore universal representations,\nwhere a single encoder is developed to achieve multiple decoding objectives\nacross various distortion and classification (or perception) constraints. This\nuniversality avoids retraining encoders for each specific operating point\nwithin these tradeoffs. Our experimental validation on the MNIST dataset\nindicates that a universal encoder incurs only minimal performance degradation\ncompared to individually optimized encoders for perceptual image compression\ntasks, aligning with prior results from [23]. Nonetheless, we also identify\nthat in the RDC setting, reusing an encoder optimized for one specific\nclassification-distortion tradeoff leads to a significant distortion penalty\nwhen applied to alternative points.",
      "tldr_zh": "本文提出了一种通用表示(universal representations)方法，用于分类增强的损失压缩(rate-distortion-classification, RDC)，旨在通过一个单一编码器实现多种解码目标，涵盖不同失真和分类约束，从而避免为每个特定权衡点重新训练编码器。该方法基于RDP(rate-distortion-perception)框架的扩展，在MNIST数据集实验中显示，通用编码器在感知图像压缩任务中仅比单独优化的编码器有微小性能下降，与现有研究[23]一致。然而，在RDC设置中，针对特定分类-失真权衡优化的编码器应用于其他点时，会导致显著的失真惩罚，为未来优化提供了重要启示。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13191v1",
      "published_date": "2025-04-12 00:55:56 UTC",
      "updated_date": "2025-04-12 00:55:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:22:07.702978"
    },
    {
      "arxiv_id": "2504.11473v1",
      "title": "Visual moral inference and communication",
      "title_zh": "视觉道德推理与沟通",
      "authors": [
        "Warren Zhu",
        "Aida Ramezani",
        "Yang Xu"
      ],
      "abstract": "Humans can make moral inferences from multiple sources of input. In contrast,\nautomated moral inference in artificial intelligence typically relies on\nlanguage models with textual input. However, morality is conveyed through\nmodalities beyond language. We present a computational framework that supports\nmoral inference from natural images, demonstrated in two related tasks: 1)\ninferring human moral judgment toward visual images and 2) analyzing patterns\nin moral content communicated via images from public news. We find that models\nbased on text alone cannot capture the fine-grained human moral judgment toward\nvisual stimuli, but language-vision fusion models offer better precision in\nvisual moral inference. Furthermore, applications of our framework to news data\nreveal implicit biases in news categories and geopolitical discussions. Our\nwork creates avenues for automating visual moral inference and discovering\npatterns of visual moral communication in public media.",
      "tldr_zh": "本研究提出一个计算框架，支持从自然图像中进行道德推理，解决AI系统依赖文本输入的局限性。框架针对两个任务：推断人类对视觉图像的道德判断，以及分析公共新闻图像中的道德内容。结果显示，基于文本的模型无法捕捉视觉刺激的细粒度道德判断，而language-vision fusion models 提供更高精度；应用于新闻数据后，该框架揭示了新闻类别和地缘政治讨论中的隐性偏见。该工作为自动化视觉道德推理和探索公共媒体中的视觉道德沟通模式开辟了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11473v1",
      "published_date": "2025-04-12 00:46:27 UTC",
      "updated_date": "2025-04-12 00:46:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T12:22:18.417375"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 46,
  "processed_papers_count": 46,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T12:22:32.547749"
}
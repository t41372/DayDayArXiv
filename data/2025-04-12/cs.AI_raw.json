[
  {
    "arxiv_id": "2504.09354v1",
    "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis",
    "authors": [
      "Duy-Cat Can",
      "Quang-Huy Tang",
      "Huong Ha",
      "Binh T. Nguyen",
      "Oliver Y. Chén"
    ],
    "abstract": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09354v1",
    "published_date": "2025-04-12 22:06:15 UTC",
    "updated_date": "2025-04-12 22:06:15 UTC"
  },
  {
    "arxiv_id": "2504.09352v1",
    "title": "Explorer: Robust Collection of Interactable GUI Elements",
    "authors": [
      "Iason Chaimalas",
      "Arnas Vyšniauskas",
      "Gabriel Brostow"
    ],
    "abstract": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "19 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09352v1",
    "published_date": "2025-04-12 22:02:29 UTC",
    "updated_date": "2025-04-12 22:02:29 UTC"
  },
  {
    "arxiv_id": "2504.09346v1",
    "title": "\"It's not a representation of me\": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services",
    "authors": [
      "Shira Michel",
      "Sufi Kaur",
      "Sarah Elizabeth Gillespie",
      "Jeffrey Gleason",
      "Christo Wilson",
      "Avijit Ghosh"
    ],
    "abstract": "Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "This paper has been accepted to FAccT 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09346v1",
    "published_date": "2025-04-12 21:31:22 UTC",
    "updated_date": "2025-04-12 21:31:22 UTC"
  },
  {
    "arxiv_id": "2504.09345v1",
    "title": "MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints",
    "authors": [
      "Yichao Yuan",
      "Lin Ma",
      "Nishil Talati"
    ],
    "abstract": "Mixture of Experts (MoE) LLMs, characterized by their sparse activation\npatterns, offer a promising approach to scaling language models while avoiding\nproportionally increasing the inference cost. However, their large parameter\nsizes present deployment challenges in resource-constrained environments with\nlimited GPU memory capacity, as GPU memory is often insufficient to accommodate\nthe full set of model weights. Consequently, typical deployments rely on\nCPU-GPU hybrid execution: the GPU handles compute-intensive GEMM operations,\nwhile the CPU processes the relatively lightweight attention mechanism. This\nsetup introduces a key challenge: how to effectively optimize resource\nutilization across CPU and GPU? Prior work has designed system optimizations\nbased on performance models with limited scope. Specifically, such models do\nnot capture the complex interactions between hardware properties and system\nexecution mechanisms. Therefore, previous approaches neither identify nor\nachieve the hardware limit.\n  This paper presents MoE-Lens, a high-throughput MoE LLM inference system\ndesigned through holistic performance modeling for resource-constrained\nenvironments. Our performance model thoroughly analyzes various fundamental\nsystem components, including CPU memory capacity, GPU compute power, and\nworkload characteristics, to understand the theoretical performance upper bound\nof MoE inference. Furthermore, it captures the system execution mechanisms to\nidentify the key hardware bottlenecks and accurately predict the achievable\nthroughput. Informed by our performance model, MoE-Lens introduces an inference\nsystem approaching hardware limits. Evaluated on diverse MoE models and\ndatasets, MoE-Lens outperforms the state-of-the-art solution by 4.6x on average\n(up to 25.5x), with our theoretical model predicting performance with an\naverage 94% accuracy.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09345v1",
    "published_date": "2025-04-12 21:26:56 UTC",
    "updated_date": "2025-04-12 21:26:56 UTC"
  },
  {
    "arxiv_id": "2504.12329v1",
    "title": "Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time",
    "authors": [
      "Wang Yang",
      "Xiang Yue",
      "Vipin Chaudhary",
      "Xiaotian Han"
    ],
    "abstract": "Recent advances leverage post-training to enhance model reasoning\nperformance, which typically requires costly training pipelines and still\nsuffers from inefficient, overly lengthy outputs. We introduce Speculative\nThinking, a training-free framework that enables large reasoning models to\nguide smaller ones during inference at the reasoning level, distinct from\nspeculative decoding, which operates at the token level. Our approach is based\non two observations: (1) reasoning-supportive tokens such as \"wait\" frequently\nappear after structural delimiters like \"\\n\\n\", serving as signals for\nreflection or continuation; and (2) larger models exhibit stronger control over\nreflective behavior, reducing unnecessary backtracking while improving\nreasoning quality. By strategically delegating reflective steps to a more\ncapable model, our method significantly boosts the reasoning accuracy of\nreasoning models while shortening their output. With the assistance of the 32B\nreasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to\n89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average\noutput length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%\ndecrease. Moreover, when applied to a non-reasoning model\n(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%\non the same benchmark, achieving a relative improvement of 7.8%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12329v1",
    "published_date": "2025-04-12 21:25:32 UTC",
    "updated_date": "2025-04-12 21:25:32 UTC"
  },
  {
    "arxiv_id": "2504.09343v1",
    "title": "Confirmation Bias in Generative AI Chatbots: Mechanisms, Risks, Mitigation Strategies, and Future Research Directions",
    "authors": [
      "Yiran Du"
    ],
    "abstract": "This article explores the phenomenon of confirmation bias in generative AI\nchatbots, a relatively underexamined aspect of AI-human interaction. Drawing on\ncognitive psychology and computational linguistics, it examines how\nconfirmation bias, commonly understood as the tendency to seek information that\naligns with existing beliefs, can be replicated and amplified by the design and\nfunctioning of large language models. The article analyzes the mechanisms by\nwhich confirmation bias may manifest in chatbot interactions, assesses the\nethical and practical risks associated with such bias, and proposes a range of\nmitigation strategies. These include technical interventions, interface\nredesign, and policy measures aimed at promoting balanced AI-generated\ndiscourse. The article concludes by outlining future research directions,\nemphasizing the need for interdisciplinary collaboration and empirical\nevaluation to better understand and address confirmation bias in generative AI\nsystems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09343v1",
    "published_date": "2025-04-12 21:08:36 UTC",
    "updated_date": "2025-04-12 21:08:36 UTC"
  },
  {
    "arxiv_id": "2504.11478v2",
    "title": "Flux Already Knows -- Activating Subject-Driven Image Generation without Training",
    "authors": [
      "Hao Kang",
      "Stathi Fotiadis",
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Min Jin Chong",
      "Xin Lu"
    ],
    "abstract": "We propose a simple yet effective zero-shot framework for subject-driven\nimage generation using a vanilla Flux model. By framing the task as grid-based\nimage completion and simply replicating the subject image(s) in a mosaic\nlayout, we activate strong identity-preserving capabilities without any\nadditional data, training, or inference-time fine-tuning. This \"free lunch\"\napproach is further strengthened by a novel cascade attention design and meta\nprompting technique, boosting fidelity and versatility. Experimental results\nshow that our method outperforms baselines across multiple key metrics in\nbenchmarks and human preference studies, with trade-offs in certain aspects.\nAdditionally, it supports diverse edits, including logo insertion, virtual\ntry-on, and subject replacement or insertion. These results demonstrate that a\npre-trained foundational text-to-image model can enable high-quality,\nresource-efficient subject-driven generation, opening new possibilities for\nlightweight customization in downstream applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11478v2",
    "published_date": "2025-04-12 20:41:53 UTC",
    "updated_date": "2025-04-19 05:17:32 UTC"
  },
  {
    "arxiv_id": "2504.09307v1",
    "title": "Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training",
    "authors": [
      "Mingyu Liang",
      "Hiwot Tadese Kassa",
      "Wenyin Fu",
      "Brian Coutinho",
      "Louis Feng",
      "Christina Delimitrou"
    ],
    "abstract": "Training LLMs in distributed environments presents significant challenges due\nto the complexity of model execution, deployment systems, and the vast space of\nconfigurable strategies. Although various optimization techniques exist,\nachieving high efficiency in practice remains difficult. Accurate performance\nmodels that effectively characterize and predict a model's behavior are\nessential for guiding optimization efforts and system-level studies. We propose\nLumos, a trace-driven performance modeling and estimation toolkit for\nlarge-scale LLM training, designed to accurately capture and predict the\nexecution behaviors of modern LLMs. We evaluate Lumos on a production ML\ncluster with up to 512 NVIDIA H100 GPUs using various GPT-3 variants,\ndemonstrating that it can replay execution time with an average error of just\n3.3%, along with other runtime details, across different models and\nconfigurations. Additionally, we validate its ability to estimate performance\nfor new setups from existing traces, facilitating efficient exploration of\nmodel and deployment configurations.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted to MLSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09307v1",
    "published_date": "2025-04-12 18:43:24 UTC",
    "updated_date": "2025-04-12 18:43:24 UTC"
  },
  {
    "arxiv_id": "2504.09302v1",
    "title": "Application of Contrastive Learning on ECG Data: Evaluating Performance in Japanese and Classification with Around 100 Labels",
    "authors": [
      "Junichiro Takahashi",
      "JingChuan Guan",
      "Masataka Sato",
      "Kaito Baba",
      "Kazuto Haruguchi",
      "Daichi Nagashima",
      "Satoshi Kodera",
      "Norihiko Takeda"
    ],
    "abstract": "The electrocardiogram (ECG) is a fundamental tool in cardiovascular\ndiagnostics due to its powerful and non-invasive nature. One of the most\ncritical usages is to determine whether more detailed examinations are\nnecessary, with users ranging across various levels of expertise. Given this\ndiversity in expertise, it is essential to assist users to avoid critical\nerrors. Recent studies in machine learning have addressed this challenge by\nextracting valuable information from ECG data. Utilizing language models, these\nstudies have implemented multimodal models aimed at classifying ECGs according\nto labeled terms. However, the number of classes was reduced, and it remains\nuncertain whether the technique is effective for languages other than English.\nTo move towards practical application, we utilized ECG data from regular\npatients visiting hospitals in Japan, maintaining a large number of Japanese\nlabels obtained from actual ECG readings. Using a contrastive learning\nframework, we found that even with 98 labels for classification, our\nJapanese-based language model achieves accuracy comparable to previous\nresearch. This study extends the applicability of multimodal machine learning\nframeworks to broader clinical studies and non-English languages.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09302v1",
    "published_date": "2025-04-12 18:20:44 UTC",
    "updated_date": "2025-04-12 18:20:44 UTC"
  },
  {
    "arxiv_id": "2504.09301v1",
    "title": "Continuum-Interaction-Driven Intelligence: Human-Aligned Neural Architecture via Crystallized Reasoning and Fluid Generation",
    "authors": [
      "Pengcheng Zhou",
      "Zhiqiang Nie",
      "Haochen Li"
    ],
    "abstract": "Current AI systems based on probabilistic neural networks, such as large\nlanguage models (LLMs), have demonstrated remarkable generative capabilities\nyet face critical challenges including hallucination, unpredictability, and\nmisalignment with human decision-making. These issues fundamentally stem from\nthe over-reliance on randomized (probabilistic) neural networks-oversimplified\nmodels of biological neural networks-while neglecting the role of procedural\nreasoning (chain-of-thought) in trustworthy decision-making. Inspired by the\nhuman cognitive duality of fluid intelligence (flexible generation) and\ncrystallized intelligence (structured knowledge), this study proposes a\ndual-channel intelligent architecture that integrates probabilistic generation\n(LLMs) with white-box procedural reasoning (chain-of-thought) to construct\ninterpretable, continuously learnable, and human-aligned AI systems.\nConcretely, this work: (1) redefines chain-of-thought as a programmable\ncrystallized intelligence carrier, enabling dynamic knowledge evolution and\ndecision verification through multi-turn interaction frameworks; (2) introduces\na task-driven modular network design that explicitly demarcates the functional\nboundaries between randomized generation and procedural control to address\ntrustworthiness in vertical-domain applications; (3) demonstrates that\nmulti-turn interaction is a necessary condition for intelligence emergence,\nwith dialogue depth positively correlating with the system's human-alignment\ndegree. This research not only establishes a new paradigm for trustworthy AI\ndeployment but also provides theoretical foundations for next-generation\nhuman-AI collaborative systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09301v1",
    "published_date": "2025-04-12 18:15:49 UTC",
    "updated_date": "2025-04-12 18:15:49 UTC"
  },
  {
    "arxiv_id": "2504.09283v1",
    "title": "Semantic Commit: Helping Users Update Intent Specifications for AI Memory at Scale",
    "authors": [
      "Priyan Vaithilingam",
      "Munyeong Kim",
      "Frida-Cecilia Acosta-Parenteau",
      "Daniel Lee",
      "Amine Mhedhbi",
      "Elena L. Glassman",
      "Ian Arawjo"
    ],
    "abstract": "How do we update AI memory of user intent as intent changes? We consider how\nan AI interface may assist the integration of new information into a repository\nof natural language data. Inspired by software engineering concepts like impact\nanalysis, we develop methods and a UI for managing semantic changes with\nnon-local effects, which we call \"semantic conflict resolution.\" The user\ncommits new intent to a project -- makes a \"semantic commit\" -- and the AI\nhelps the user detect and resolve semantic conflicts within a store of existing\ninformation representing their intent (an \"intent specification\"). We develop\nan interface, SemanticCommit, to better understand how users resolve conflicts\nwhen updating intent specifications such as Cursor Rules and game design\ndocuments. A knowledge graph-based RAG pipeline drives conflict detection,\nwhile LLMs assist in suggesting resolutions. We evaluate our technique on an\ninitial benchmark. Then, we report a 12 user within-subjects study of\nSemanticCommit for two task domains -- game design documents, and AI agent\nmemory in the style of ChatGPT memories -- where users integrated new\ninformation into an existing list. Half of our participants adopted a workflow\nof impact analysis, where they would first flag conflicts without AI revisions\nthen resolve conflicts locally, despite having access to a global revision\nfeature. We argue that AI agent interfaces, such as software IDEs like Cursor\nand Windsurf, should provide affordances for impact analysis and help users\nvalidate AI retrieval independently from generation. Our work speaks to how AI\nagent designers should think about updating memory as a process that involves\nhuman feedback and decision-making.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2; D.2.1; I.7.0; I.2.11; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "22 pages; 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09283v1",
    "published_date": "2025-04-12 17:07:08 UTC",
    "updated_date": "2025-04-12 17:07:08 UTC"
  },
  {
    "arxiv_id": "2504.09277v1",
    "title": "SynthTRIPs: A Knowledge-Grounded Framework for Benchmark Query Generation for Personalized Tourism Recommenders",
    "authors": [
      "Ashmi Banerjee",
      "Adithi Satish",
      "Fitri Nur Aisyah",
      "Wolfgang Wörndl",
      "Yashar Deldjoo"
    ],
    "abstract": "Tourism Recommender Systems (TRS) are crucial in personalizing travel\nexperiences by tailoring recommendations to users' preferences, constraints,\nand contextual factors. However, publicly available travel datasets often lack\nsufficient breadth and depth, limiting their ability to support advanced\npersonalization strategies -- particularly for sustainable travel and off-peak\ntourism. In this work, we explore using Large Language Models (LLMs) to\ngenerate synthetic travel queries that emulate diverse user personas and\nincorporate structured filters such as budget constraints and sustainability\npreferences.\n  This paper introduces a novel SynthTRIPs framework for generating synthetic\ntravel queries using LLMs grounded in a curated knowledge base (KB). Our\napproach combines persona-based preferences (e.g., budget, travel style) with\nexplicit sustainability filters (e.g., walkability, air quality) to produce\nrealistic and diverse queries. We mitigate hallucination and ensure factual\ncorrectness by grounding the LLM responses in the KB. We formalize the query\ngeneration process and introduce evaluation metrics for assessing realism and\nalignment. Both human expert evaluations and automatic LLM-based assessments\ndemonstrate the effectiveness of our synthetic dataset in capturing complex\npersonalization aspects underrepresented in existing datasets. While our\nframework was developed and tested for personalized city trip recommendations,\nthe methodology applies to other recommender system domains.\n  Code and dataset are made public at https://bit.ly/synthTRIPs",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted for publication at SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09277v1",
    "published_date": "2025-04-12 16:48:35 UTC",
    "updated_date": "2025-04-12 16:48:35 UTC"
  },
  {
    "arxiv_id": "2504.09271v1",
    "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries",
    "authors": [
      "Koustuv Saha",
      "Yoshee Jain",
      "Munmun De Choudhury"
    ],
    "abstract": "The ubiquity and widespread use of digital and online technologies have\ntransformed mental health support, with online mental health communities\n(OMHCs) providing safe spaces for peer support. More recently, generative AI\nand large language models (LLMs) have introduced new possibilities for\nscalable, around-the-clock mental health assistance that could potentially\naugment and supplement the capabilities of OMHCs. Although genAI shows promise\nin delivering immediate and personalized responses, their effectiveness in\nreplicating the nuanced, experience-based support of human peers remains an\nopen question. In this study, we harnessed 24,114 posts and 138,758 online\ncommunity (OC) responses from 55 OMHCs on Reddit. We prompted several\nstate-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts,\nand compared their (AI) responses to human-written (OC) responses based on a\nvariety of linguistic measures across psycholinguistics and lexico-semantics.\nOur findings revealed that AI responses are more verbose, readable, and\nanalytically structured, but lack linguistic diversity and personal narratives\ninherent in human-human interactions. Through a qualitative examination, we\nfound validation as well as complementary insights into the nature of AI\nresponses, such as its neutrality of stance and the absence of seeking\nback-and-forth clarifications. We discuss the ethical and practical\nimplications of integrating generative AI into OMHCs, advocating for frameworks\nthat balance AI's scalability and timeliness with the irreplaceable\nauthenticity, social interactiveness, and expertise of human connections that\nform the ethos of online support communities.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09271v1",
    "published_date": "2025-04-12 16:20:02 UTC",
    "updated_date": "2025-04-12 16:20:02 UTC"
  },
  {
    "arxiv_id": "2504.12328v1",
    "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future",
    "authors": [
      "Jialun Zhong",
      "Wei Shen",
      "Yanzeng Li",
      "Songyang Gao",
      "Hua Lu",
      "Yicheng Chen",
      "Yang Zhang",
      "Wei Zhou",
      "Jinjie Gu",
      "Lei Zou"
    ],
    "abstract": "Reward Model (RM) has demonstrated impressive potential for enhancing Large\nLanguage Models (LLM), as RM can serve as a proxy for human preferences,\nproviding signals to guide LLMs' behavior in various tasks. In this paper, we\nprovide a comprehensive overview of relevant research, exploring RMs from the\nperspectives of preference collection, reward modeling, and usage. Next, we\nintroduce the applications of RMs and discuss the benchmarks for evaluation.\nFurthermore, we conduct an in-depth analysis of the challenges existing in the\nfield and dive into the potential research directions. This paper is dedicated\nto providing beginners with a comprehensive introduction to RMs and\nfacilitating future studies. The resources are publicly available at\ngithub\\footnote{https://github.com/JLZhong23/awesome-reward-models}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12328v1",
    "published_date": "2025-04-12 16:07:36 UTC",
    "updated_date": "2025-04-12 16:07:36 UTC"
  },
  {
    "arxiv_id": "2504.09257v1",
    "title": "MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices",
    "authors": [
      "Sohom Ghosh",
      "Arnab Maji",
      "Sudip Kumar Naskar"
    ],
    "abstract": "Predicting stock market prices following corporate earnings calls remains a\nsignificant challenge for investors and researchers alike, requiring innovative\napproaches that can process diverse information sources. This study\ninvestigates the impact of corporate earnings calls on stock prices by\nintroducing a multi-modal predictive model. We leverage textual data from\nearnings call transcripts, along with images and tables from accompanying\npresentations, to forecast stock price movements on the trading day immediately\nfollowing these calls. To facilitate this research, we developed the MiMIC\n(Multi-Modal Indian Earnings Calls) dataset, encompassing companies\nrepresenting the Nifty 50, Nifty MidCap 50, and Nifty Small 50 indices. The\ndataset includes earnings call transcripts, presentations, fundamentals,\ntechnical indicators, and subsequent stock prices. We present a multimodal\nanalytical framework that integrates quantitative variables with predictive\nsignals derived from textual and visual modalities, thereby enabling a holistic\napproach to feature representation and analysis. This multi-modal approach\ndemonstrates the potential for integrating diverse information sources to\nenhance financial forecasting accuracy. To promote further research in\ncomputational economics, we have made the MiMIC dataset publicly available\nunder the CC-NC-SA-4.0 licence. Our work contributes to the growing body of\nliterature on market reactions to corporate communications and highlights the\nefficacy of multi-modal machine learning techniques in financial analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code and Dataset:\n  https://huggingface.co/datasets/sohomghosh/MiMIC_Multi-Modal_Indian_Earnings_Calls_Dataset",
    "pdf_url": "http://arxiv.org/pdf/2504.09257v1",
    "published_date": "2025-04-12 15:31:40 UTC",
    "updated_date": "2025-04-12 15:31:40 UTC"
  },
  {
    "arxiv_id": "2504.09242v1",
    "title": "Development of a PPO-Reinforcement Learned Walking Tripedal Soft-Legged Robot using SOFA",
    "authors": [
      "Yomna Mokhtar",
      "Tarek Shohdy",
      "Abdallah A. Hassan",
      "Mostafa Eshra",
      "Omar Elmenawy",
      "Osama Khalil",
      "Haitham El-Hussieny"
    ],
    "abstract": "Rigid robots were extensively researched, whereas soft robotics remains an\nunderexplored field. Utilizing soft-legged robots in performing tasks as a\nreplacement for human beings is an important stride to take, especially under\nharsh and hazardous conditions over rough terrain environments. For the demand\nto teach any robot how to behave in different scenarios, a real-time physical\nand visual simulation is essential. When it comes to soft robots specifically,\na simulation framework is still an arduous problem that needs to be disclosed.\nUsing the simulation open framework architecture (SOFA) is an advantageous\nstep. However, neither SOFA's manual nor prior public SOFA projects show its\nmaximum capabilities the users can reach. So, we resolved this by establishing\ncustomized settings and handling the framework components appropriately.\nSettling on perfect, fine-tuned SOFA parameters has stimulated our motivation\ntowards implementing the state-of-the-art (SOTA) reinforcement learning (RL)\nmethod of proximal policy optimization (PPO). The final representation is a\nwell-defined, ready-to-deploy walking, tripedal, soft-legged robot based on\nPPO-RL in a SOFA environment. Robot navigation performance is a key metric to\nbe considered for measuring the success resolution. Although in the simulated\nsoft robots case, an 82\\% success rate in reaching a single goal is a\ngroundbreaking output, we pushed the boundaries to further steps by evaluating\nthe progress under assigning a sequence of goals. While trailing the platform\nsteps, outperforming discovery has been observed with an accumulative squared\nerror deviation of 19 mm. The full code is publicly available at\n\\href{https://github.com/tarekshohdy/PPO_SOFA_Soft_Legged_Robot.git}{github.com/tarekshohdy/PPO$\\textunderscore$SOFA$\\textunderscore$Soft$\\textunderscore$Legged$\\textunderscore$\nRobot.git}",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09242v1",
    "published_date": "2025-04-12 14:46:51 UTC",
    "updated_date": "2025-04-12 14:46:51 UTC"
  },
  {
    "arxiv_id": "2504.09225v1",
    "title": "AMNet: An Acoustic Model Network for Enhanced Mandarin Speech Synthesis",
    "authors": [
      "Yubing Cao",
      "Yinfeng Yu",
      "Yongming Li",
      "Liejun Wang"
    ],
    "abstract": "This paper presents AMNet, an Acoustic Model Network designed to improve the\nperformance of Mandarin speech synthesis by incorporating phrase structure\nannotation and local convolution modules. AMNet builds upon the FastSpeech 2\narchitecture while addressing the challenge of local context modeling, which is\ncrucial for capturing intricate speech features such as pauses, stress, and\nintonation. By embedding a phrase structure parser into the model and\nintroducing a local convolution module, AMNet enhances the model's sensitivity\nto local information. Additionally, AMNet decouples tonal characteristics from\nphonemes, providing explicit guidance for tone modeling, which improves tone\naccuracy and pronunciation. Experimental results demonstrate that AMNet\noutperforms baseline models in subjective and objective evaluations. The\nproposed model achieves superior Mean Opinion Scores (MOS), lower Mel Cepstral\nDistortion (MCD), and improved fundamental frequency fitting $F0 (R^2)$,\nconfirming its ability to generate high-quality, natural, and expressive\nMandarin speech.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Main paper (8 pages). Accepted for publication by IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09225v1",
    "published_date": "2025-04-12 14:02:31 UTC",
    "updated_date": "2025-04-12 14:02:31 UTC"
  },
  {
    "arxiv_id": "2504.09223v1",
    "title": "DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models",
    "authors": [
      "Wenjin Ke",
      "Zhe Li",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "Improving the efficiency of inference in Large Language Models (LLMs) is a\ncritical area of research. Post-training Quantization (PTQ) is a popular\ntechnique, but it often faces challenges at low-bit levels, particularly in\ndownstream tasks. Quantization-aware Training (QAT) can alleviate this problem,\nbut it requires significantly more computational resources. To tackle this, we\nintroduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT),\nwhich merges the advantages of QAT while training only less than 1% of the\ntotal parameters. Specifically, we introduce a group-specific quantization\nmagnitude to adjust the overall scale of each quantization group. Within each\nquantization group, we use LoRA matrices to update the weight size and\ndirection in the quantization space. We validated the effectiveness of our\nmethod on the LLaMA and LLaMA2 model families. The results show significant\nimprovements over our baseline method across different quantization\ngranularities. For instance, for LLaMA-7B, our approach outperforms the\nprevious state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model.\nAdditionally, our quantization results on pre-trained models also surpass\nprevious QAT methods, demonstrating the superior performance and efficiency of\nour approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09223v1",
    "published_date": "2025-04-12 13:57:02 UTC",
    "updated_date": "2025-04-12 13:57:02 UTC"
  },
  {
    "arxiv_id": "2504.09210v2",
    "title": "FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
    "authors": [
      "Jiaxin Liu",
      "Xiaoqian Jiang",
      "Xiang Li",
      "Bohan Zhang",
      "Jing Zhang"
    ],
    "abstract": "Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09210v2",
    "published_date": "2025-04-12 13:32:11 UTC",
    "updated_date": "2025-04-15 02:22:16 UTC"
  },
  {
    "arxiv_id": "2504.10529v1",
    "title": "HeteRAG: A Heterogeneous Retrieval-augmented Generation Framework with Decoupled Knowledge Representations",
    "authors": [
      "Peiru Yang",
      "Xintian Li",
      "Zhiyang Hu",
      "Jiapeng Wang",
      "Jinhua Yin",
      "Huili Wang",
      "Lizhi He",
      "Shuai Yang",
      "Shangguang Wang",
      "Yongfeng Huang",
      "Tao Qi"
    ],
    "abstract": "Retrieval-augmented generation (RAG) methods can enhance the performance of\nLLMs by incorporating retrieved knowledge chunks into the generation process.\nIn general, the retrieval and generation steps usually have different\nrequirements for these knowledge chunks. The retrieval step benefits from\ncomprehensive information to improve retrieval accuracy, whereas excessively\nlong chunks may introduce redundant contextual information, thereby diminishing\nboth the effectiveness and efficiency of the generation process. However,\nexisting RAG methods typically employ identical representations of knowledge\nchunks for both retrieval and generation, resulting in suboptimal performance.\nIn this paper, we propose a heterogeneous RAG framework (\\myname) that\ndecouples the representations of knowledge chunks for retrieval and generation,\nthereby enhancing the LLMs in both effectiveness and efficiency. Specifically,\nwe utilize short chunks to represent knowledge to adapt the generation step and\nutilize the corresponding chunk with its contextual information from\nmulti-granular views to enhance retrieval accuracy. We further introduce an\nadaptive prompt tuning method for the retrieval model to adapt the\nheterogeneous retrieval augmented generation process. Extensive experiments\ndemonstrate that \\myname achieves significant improvements compared to\nbaselines.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.10529v1",
    "published_date": "2025-04-12 13:12:54 UTC",
    "updated_date": "2025-04-12 13:12:54 UTC"
  },
  {
    "arxiv_id": "2504.09203v1",
    "title": "AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images",
    "authors": [
      "Saikat Dutta",
      "Akhil Vasim",
      "Siddhant Gole",
      "Hamid Rezatofighi",
      "Biplab Banerjee"
    ],
    "abstract": "Image segmentation beyond predefined categories is a key challenge in remote\nsensing, where novel and unseen classes often emerge during inference.\nOpen-vocabulary image Segmentation addresses these generalization issues in\ntraditional supervised segmentation models while reducing reliance on extensive\nper-pixel annotations, which are both expensive and labor-intensive to obtain.\nMost Open-Vocabulary Segmentation (OVS) methods are designed for natural images\nbut struggle with remote sensing data due to scale variations, orientation\nchanges, and complex scene compositions. This necessitates the development of\nOVS approaches specifically tailored for remote sensing. In this context, we\npropose AerOSeg, a novel OVS approach for remote sensing data. First, we\ncompute robust image-text correlation features using multiple rotated versions\nof the input image and domain-specific prompts. These features are then refined\nthrough spatial and class refinement blocks. Inspired by the success of the\nSegment Anything Model (SAM) in diverse domains, we leverage SAM features to\nguide the spatial refinement of correlation features. Additionally, we\nintroduce a semantic back-projection module and loss to ensure the seamless\npropagation of SAM's semantic information throughout the segmentation pipeline.\nFinally, we enhance the refined correlation features using a multi-scale\nattention-aware decoder to produce the final segmentation map. We validate our\nSAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark\nremote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms\nstate-of-the-art open-vocabulary segmentation methods, achieving an average\nimprovement of 2.54 h-mIoU.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at EarthVision workshop, CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09203v1",
    "published_date": "2025-04-12 13:06:46 UTC",
    "updated_date": "2025-04-12 13:06:46 UTC"
  },
  {
    "arxiv_id": "2504.09197v1",
    "title": "Graph Learning-Driven Multi-Vessel Association: Fusing Multimodal Data for Maritime Intelligence",
    "authors": [
      "Yuxu Lu",
      "Kaisen Yang",
      "Dong Yang",
      "Haifeng Ding",
      "Jinxian Weng",
      "Ryan Wen Liu"
    ],
    "abstract": "Ensuring maritime safety and optimizing traffic management in increasingly\ncrowded and complex waterways require effective waterway monitoring. However,\ncurrent methods struggle with challenges arising from multimodal data, such as\ndimensional disparities, mismatched target counts, vessel scale variations,\nocclusions, and asynchronous data streams from systems like the automatic\nidentification system (AIS) and closed-circuit television (CCTV). Traditional\nmulti-target association methods often struggle with these complexities,\nparticularly in densely trafficked waterways. To overcome these issues, we\npropose a graph learning-driven multi-vessel association (GMvA) method tailored\nfor maritime multimodal data fusion. By integrating AIS and CCTV data, GMvA\nleverages time series learning and graph neural networks to capture the\nspatiotemporal features of vessel trajectories effectively. To enhance feature\nrepresentation, the proposed method incorporates temporal graph attention and\nspatiotemporal attention, effectively capturing both local and global vessel\ninteractions. Furthermore, a multi-layer perceptron-based uncertainty fusion\nmodule computes robust similarity scores, and the Hungarian algorithm is\nadopted to ensure globally consistent and accurate target matching. Extensive\nexperiments on real-world maritime datasets confirm that GMvA delivers superior\naccuracy and robustness in multi-target association, outperforming existing\nmethods even in challenging scenarios with high vessel density and incomplete\nor unevenly distributed AIS and CCTV data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09197v1",
    "published_date": "2025-04-12 12:45:55 UTC",
    "updated_date": "2025-04-12 12:45:55 UTC"
  },
  {
    "arxiv_id": "2504.09195v1",
    "title": "ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking",
    "authors": [
      "Tzoulio Chamiti",
      "Leandro Di Bella",
      "Adrian Munteanu",
      "Nikos Deligiannis"
    ],
    "abstract": "Tracking multiple objects based on textual queries is a challenging task that\nrequires linking language understanding with object association across frames.\nPrevious works typically train the whole process end-to-end or integrate an\nadditional referring text module into a multi-object tracker, but they both\nrequire supervised training and potentially struggle with generalization to\nopen-set queries. In this work, we introduce ReferGPT, a novel zero-shot\nreferring multi-object tracking framework. We provide a multi-modal large\nlanguage model (MLLM) with spatial knowledge enabling it to generate 3D-aware\ncaptions. This enhances its descriptive capabilities and supports a more\nflexible referring vocabulary without training. We also propose a robust\nquery-matching strategy, leveraging CLIP-based semantic encoding and fuzzy\nmatching to associate MLLM generated captions with user queries. Extensive\nexperiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that\nReferGPT achieves competitive performance against trained methods, showcasing\nits robustness and zero-shot capabilities in autonomous driving. The codes are\navailable on https://github.com/Tzoulio/ReferGPT",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted CVPR 2025 Workshop on Distillation of Foundation Models for\n  Autonomous Driving",
    "pdf_url": "http://arxiv.org/pdf/2504.09195v1",
    "published_date": "2025-04-12 12:33:15 UTC",
    "updated_date": "2025-04-12 12:33:15 UTC"
  },
  {
    "arxiv_id": "2504.09185v1",
    "title": "Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction",
    "authors": [
      "Wenbo Yan",
      "Hanzhong Cao",
      "Ying Tan"
    ],
    "abstract": "Long sequence prediction is a key challenge in time series forecasting. While\nMamba-based models have shown strong performance due to their sequence\nselection capabilities, they still struggle with insufficient focus on critical\ntime steps and incomplete noise suppression, caused by limited selective\nabilities. To address this, we introduce Repetitive Contrastive Learning (RCL),\na token-level contrastive pretraining framework aimed at enhancing Mamba's\nselective capabilities. RCL pretrains a single Mamba block to strengthen its\nselective abilities and then transfers these pretrained parameters to\ninitialize Mamba blocks in various backbone models, improving their temporal\nprediction performance. RCL uses sequence augmentation with Gaussian noise and\napplies inter-sequence and intra-sequence contrastive learning to help the\nMamba module prioritize information-rich time steps while ignoring noisy ones.\nExtensive experiments show that RCL consistently boosts the performance of\nbackbone models, surpassing existing methods and achieving state-of-the-art\nresults. Additionally, we propose two metrics to quantify Mamba's selective\ncapabilities, providing theoretical, qualitative, and quantitative evidence for\nthe improvements brought by RCL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09185v1",
    "published_date": "2025-04-12 11:57:27 UTC",
    "updated_date": "2025-04-12 11:57:27 UTC"
  },
  {
    "arxiv_id": "2504.09184v2",
    "title": "Parameterized Synthetic Text Generation with SimpleStories",
    "authors": [
      "Lennart Finke",
      "Chandan Sreedhara",
      "Thomas Dooms",
      "Mat Allen",
      "Emerald Zhang",
      "Juan Diego Rodriguez",
      "Noa Nabeshima",
      "Thomas Marshall",
      "Dan Braun"
    ],
    "abstract": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million samples each in English and Japanese. Through\nparameterizing prompts at multiple levels of abstraction, we achieve control\nover story characteristics at scale, inducing syntactic and semantic diversity.\nAblations on a newly trained model suite show improved sample efficiency and\nmodel interpretability compared to the TinyStories dataset. We open-source all\nconstituent parts of model creation, hoping to enable novel ways to study the\nend-to-end training process. As a byproduct, we move the frontier regarding the\nfewest-parameter language model that outputs grammatical natural language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09184v2",
    "published_date": "2025-04-12 11:44:47 UTC",
    "updated_date": "2025-05-16 11:38:19 UTC"
  },
  {
    "arxiv_id": "2504.11477v1",
    "title": "SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification",
    "authors": [
      "Yunkai Zhang",
      "Shiyin Wei",
      "Yong Huang",
      "Yawu Su",
      "Shanshan Lu",
      "Hui Li"
    ],
    "abstract": "Existing computer vision(CV)-based structural damage identification models\ndemonstrate notable accuracy in categorizing and localizing damage. However,\nthese models present several critical limitations that hinder their practical\napplication in civil engineering(CE). Primarily, their ability to recognize\ndamage types remains constrained, preventing comprehensive analysis of the\nhighly varied and complex conditions encountered in real-world CE structures.\nSecond, these models lack linguistic capabilities, rendering them unable to\narticulate structural damage characteristics through natural language\ndescriptions. With the continuous advancement of artificial intelligence(AI),\nlarge multi-modal models(LMMs) have emerged as a transformative solution,\nenabling the unified encoding and alignment of textual and visual data. These\nmodels can autonomously generate detailed descriptive narratives of structural\ndamage while demonstrating robust generalization across diverse scenarios and\ntasks. This study introduces SDIGLM, an innovative LMM for structural damage\nidentification, developed based on the open-source VisualGLM-6B architecture.\nTo address the challenge of adapting LMMs to the intricate and varied operating\nconditions in CE, this work integrates a U-Net-based semantic segmentation\nmodule to generate defect segmentation maps as visual Chain of Thought(CoT).\nAdditionally, a multi-round dialogue fine-tuning dataset is constructed to\nenhance logical reasoning, complemented by a language CoT formed through prompt\nengineering. By leveraging this multi-modal CoT, SDIGLM surpasses\ngeneral-purpose LMMs in structural damage identification, achieving an accuracy\nof 95.24% across various infrastructure types. Moreover, the model effectively\ndescribes damage characteristics such as hole size, crack direction, and\ncorrosion severity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11477v1",
    "published_date": "2025-04-12 11:37:10 UTC",
    "updated_date": "2025-04-12 11:37:10 UTC"
  },
  {
    "arxiv_id": "2504.10527v1",
    "title": "Explainable Artificial Intelligence techniques for interpretation of food datasets: a review",
    "authors": [
      "Leonardo Arrighi",
      "Ingrid Alves de Moraes",
      "Marco Zullich",
      "Michele Simonato",
      "Douglas Fernandes Barbin",
      "Sylvio Barbon Junior"
    ],
    "abstract": "Artificial Intelligence (AI) has become essential for analyzing complex data\nand solving highly-challenging tasks. It is being applied across numerous\ndisciplines beyond computer science, including Food Engineering, where there is\na growing demand for accurate and trustworthy predictions to meet stringent\nfood quality standards. However, this requires increasingly complex AI models,\nraising reliability concerns. In response, eXplainable AI (XAI) has emerged to\nprovide insights into AI decision-making, aiding model interpretation by\ndevelopers and users. Nevertheless, XAI remains underutilized in Food\nEngineering, limiting model reliability. For instance, in food quality control,\nAI models using spectral imaging can detect contaminants or assess freshness\nlevels, but their opaque decision-making process hinders adoption. XAI\ntechniques such as SHAP (Shapley Additive Explanations) and Grad-CAM\n(Gradient-weighted Class Activation Mapping) can pinpoint which spectral\nwavelengths or image regions contribute most to a prediction, enhancing\ntransparency and aiding quality control inspectors in verifying AI-generated\nassessments. This survey presents a taxonomy for classifying food quality\nresearch using XAI techniques, organized by data types and explanation methods,\nto guide researchers in choosing suitable approaches. We also highlight trends,\nchallenges, and opportunities to encourage the adoption of XAI in Food\nEngineering.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "A.1"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 8 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.10527v1",
    "published_date": "2025-04-12 11:10:43 UTC",
    "updated_date": "2025-04-12 11:10:43 UTC"
  },
  {
    "arxiv_id": "2504.09179v1",
    "title": "A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
    "authors": [
      "Xin Wen",
      "Shijie Guo",
      "Wenbo Ning",
      "Rui Cao",
      "Yan Niu",
      "Bin Wan",
      "Peng Wei",
      "Xiaobo Liu",
      "Jie Xiang"
    ],
    "abstract": "In open data sets of functional magnetic resonance imaging (fMRI), the\nheterogeneity of the data is typically attributed to a combination of factors,\nincluding differences in scanning procedures, the presence of confounding\neffects, and population diversities between multiple sites. These factors\ncontribute to the diminished effectiveness of representation learning, which in\nturn affects the overall efficacy of subsequent classification procedures. To\naddress these limitations, we propose a novel multi-site adversarial learning\nnetwork (MSalNET) for fMRI-based mental disorder detection. Firstly, a\nrepresentation learning module is introduced with a node information assembly\n(NIA) mechanism to better extract features from functional connectivity (FC).\nThis mechanism aggregates edge information from both horizontal and vertical\ndirections, effectively assembling node information. Secondly, to generalize\nthe feature across sites, we proposed a site-level feature extraction module\nthat can learn from individual FC data, which circumvents additional prior\ninformation. Lastly, an adversarial learning network is proposed as a means of\nbalancing the trade-off between individual classification and site regression\ntasks, with the introduction of a novel loss function. The proposed method was\nevaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data\nExchange (ABIDE) and ADHD-200. The results indicate that the proposed method\nachieves a better performance than other related algorithms with the accuracy\nof 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore,\nthe result of the site regression indicates that the proposed method reduces\nsite variability from a data-driven perspective. The most discriminative brain\nregions revealed by NIA are consistent with statistical findings, uncovering\nthe \"black box\" of deep learning to a certain extent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09179v1",
    "published_date": "2025-04-12 10:58:19 UTC",
    "updated_date": "2025-04-12 10:58:19 UTC"
  },
  {
    "arxiv_id": "2504.09164v2",
    "title": "Can postgraduate translation students identify machine-generated text?",
    "authors": [
      "Michael Farrell"
    ],
    "abstract": "Given the growing use of generative artificial intelligence as a tool for\ncreating multilingual content and bypassing both machine and traditional\ntranslation methods, this study explores the ability of linguistically trained\nindividuals to discern machine-generated output from human-written text (HT).\nAfter brief training sessions on the textual anomalies typically found in\nsynthetic text (ST), twenty-three postgraduate translation students analysed\nexcerpts of Italian prose and assigned likelihood scores to indicate whether\nthey believed they were human-written or AI-generated (ChatGPT-4o). The results\nshow that, on average, the students struggled to distinguish between HT and ST,\nwith only two participants achieving notable accuracy. Closer analysis revealed\nthat the students often identified the same textual anomalies in both HT and\nST, although features such as low burstiness and self-contradiction were more\nfrequently associated with ST. These findings suggest the need for improvements\nin the preparatory training. Moreover, the study raises questions about the\nnecessity of editing synthetic text to make it sound more human-like and\nrecommends further research to determine whether AI-generated text is already\nsufficiently natural-sounding not to require further refinement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, accepted for MT Summit 2025, Geneva, Switzerland, 23-27\n  June 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09164v2",
    "published_date": "2025-04-12 09:58:09 UTC",
    "updated_date": "2025-04-18 13:42:13 UTC"
  },
  {
    "arxiv_id": "2504.09101v1",
    "title": "Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
    "authors": [
      "Abdulmajid Murad",
      "Massimiliano Ruocco"
    ],
    "abstract": "In modern air traffic management, generating synthetic flight trajectories\nhas emerged as a promising solution for addressing data scarcity, protecting\nsensitive information, and supporting large-scale analyses. In this paper, we\npropose a novel method for trajectory synthesis by adapting the Time-Based\nVector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages\ntime-frequency domain processing, vector quantization, and transformer-based\npriors to capture both global and local dynamics in flight data. By\ndiscretizing the latent space and integrating transformer priors, the model\nlearns long-range spatiotemporal dependencies and preserves coherence across\nentire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite\nof quality, statistical, and distributional metrics, as well as a flyability\nassessment conducted in an open-source air traffic simulator. Results indicate\nthat TimeVQVAE outperforms a temporal convolutional VAE baseline, generating\nsynthetic trajectories that mirror real flight data in terms of spatial\naccuracy, temporal consistency, and statistical properties. Furthermore, the\nsimulator-based assessment shows that most generated trajectories maintain\noperational feasibility, although occasional outliers underscore the potential\nneed for additional domain-specific constraints. Overall, our findings\nunderscore the importance of multi-scale representation learning for capturing\ncomplex flight behaviors and demonstrate the promise of TimeVQVAE in producing\nrepresentative synthetic trajectories for downstream tasks such as model\ntraining, airspace design, and air traffic forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper was presented at the 25th Integrated Communications,\n  Navigation and Surveillance Conference (ICNS 2025), April 8--10, 2025,\n  Brussels, Belgium",
    "pdf_url": "http://arxiv.org/pdf/2504.09101v1",
    "published_date": "2025-04-12 06:46:51 UTC",
    "updated_date": "2025-04-12 06:46:51 UTC"
  },
  {
    "arxiv_id": "2504.09100v1",
    "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions",
    "authors": [
      "Chengyu Wang",
      "Taolin Zhang",
      "Richang Hong",
      "Jun Huang"
    ],
    "abstract": "Recently, the reasoning capabilities of large reasoning models (LRMs), such\nas DeepSeek-R1, have seen significant advancements through the slow thinking\nprocess. Despite these achievements, the substantial computational demands of\nLRMs present considerable challenges. In contrast, small reasoning models\n(SRMs), often distilled from larger ones, offer greater efficiency and can\nexhibit distinct capabilities and cognitive trajectories compared to LRMs. This\nwork surveys around 170 recently published papers on SRMs for tackling various\ncomplex reasoning tasks. We review the current landscape of SRMs and analyze\ndiverse training and inference techniques related to SRMs. Furthermore, we\nprovide a comprehensive review of SRMs for domain-specific applications and\ndiscuss possible future research directions. This survey serves as an essential\nreference for researchers to leverage or develop SRMs for advanced reasoning\nfunctionalities with high efficiency.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09100v1",
    "published_date": "2025-04-12 06:45:57 UTC",
    "updated_date": "2025-04-12 06:45:57 UTC"
  },
  {
    "arxiv_id": "2504.16096v2",
    "title": "BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification",
    "authors": [
      "Jiaxing Xu",
      "Kai He",
      "Yue Tang",
      "Wei Li",
      "Mengcheng Lan",
      "Xia Dong",
      "Yiping Ke",
      "Mengling Feng"
    ],
    "abstract": "Neurological conditions, such as Alzheimer's Disease, are challenging to\ndiagnose, particularly in the early stages where symptoms closely resemble\nhealthy controls. Existing brain network analysis methods primarily focus on\ngraph-based models that rely solely on imaging data, which may overlook\nimportant non-imaging factors and limit the model's predictive power and\ninterpretability. In this paper, we present BrainPrompt, an innovative\nframework that enhances Graph Neural Networks (GNNs) by integrating Large\nLanguage Models (LLMs) with knowledge-driven prompts, enabling more effective\ncapture of complex, non-imaging information and external knowledge for\nneurological disease identification. BrainPrompt integrates three types of\nknowledge-driven prompts: (1) ROI-level prompts to encode the identity and\nfunction of each brain region, (2) subject-level prompts that incorporate\ndemographic information, and (3) disease-level prompts to capture the temporal\nprogression of disease. By leveraging these multi-level prompts, BrainPrompt\neffectively harnesses knowledge-enhanced multi-modal information from LLMs,\nenhancing the model's capability to predict neurological disease stages and\nmeanwhile offers more interpretable results. We evaluate BrainPrompt on two\nresting-state functional Magnetic Resonance Imaging (fMRI) datasets from\nneurological disorders, showing its superiority over state-of-the-art methods.\nAdditionally, a biomarker study demonstrates the framework's ability to extract\nvaluable and interpretable information aligned with domain knowledge in\nneuroscience. The code is available at\nhttps://github.com/AngusMonroe/BrainPrompt",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Early accepted by MICCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.16096v2",
    "published_date": "2025-04-12 06:45:16 UTC",
    "updated_date": "2025-05-19 10:33:18 UTC"
  },
  {
    "arxiv_id": "2504.09095v1",
    "title": "Privacy Preservation in Gen AI Applications",
    "authors": [
      "Swetha S",
      "Ram Sundhar K Shaju",
      "Rakshana M",
      "Ganesh R",
      "Balavedhaa S",
      "Thiruvaazhi U"
    ],
    "abstract": "The ability of machines to comprehend and produce language that is similar to\nthat of humans has revolutionized sectors like customer service, healthcare,\nand finance thanks to the quick advances in Natural Language Processing (NLP),\nwhich are fueled by Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs). However, because LLMs trained on large datasets may\nunintentionally absorb and reveal Personally Identifiable Information (PII)\nfrom user interactions, these capabilities also raise serious privacy concerns.\nDeep neural networks' intricacy makes it difficult to track down or stop the\ninadvertent storing and release of private information, which raises serious\nconcerns about the privacy and security of AI-driven data. This study tackles\nthese issues by detecting Generative AI weaknesses through attacks such as data\nextraction, model inversion, and membership inference. A privacy-preserving\nGenerative AI application that is resistant to these assaults is then\ndeveloped. It ensures privacy without sacrificing functionality by using\nmethods to identify, alter, or remove PII before to dealing with LLMs. In order\nto determine how well cloud platforms like Microsoft Azure, Google Cloud, and\nAWS provide privacy tools for protecting AI applications, the study also\nexamines these technologies. In the end, this study offers a fundamental\nprivacy paradigm for generative AI systems, focusing on data security and moral\nAI implementation, and opening the door to a more secure and conscientious use\nof these tools.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09095v1",
    "published_date": "2025-04-12 06:19:37 UTC",
    "updated_date": "2025-04-12 06:19:37 UTC"
  },
  {
    "arxiv_id": "2504.09081v2",
    "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning",
    "authors": [
      "Prabhat Pandey",
      "Rupak Vignesh Swaminathan",
      "K V Vijay Girish",
      "Arunasish Sen",
      "Jian Xie",
      "Grant P. Strimel",
      "Andreas Schwarz"
    ],
    "abstract": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09081v2",
    "published_date": "2025-04-12 04:45:48 UTC",
    "updated_date": "2025-04-17 17:34:21 UTC"
  },
  {
    "arxiv_id": "2504.09064v1",
    "title": "PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations",
    "authors": [
      "Vikas Natesh",
      "H. T. Kung"
    ],
    "abstract": "We present PQS, which uses three techniques together - Prune, Quantize, and\nSort - to achieve low-bitwidth accumulation of dot products in neural network\ncomputations. In conventional quantized (e.g., 8-bit) dot products, partial\nresults are accumulated into wide (e.g., 32-bit) accumulators to avoid\noverflows when accumulating intermediate partial sums. However, such wide\naccumulators increase memory bandwidth usage and reduce energy efficiency. We\nshow that iterative N:M pruning in floating point followed by quantization to 8\n(or fewer) bits, and accumulation of partial products in a sorted order (\"small\nto large\") allows for accurate, compressed models with short dot product\nlengths that do not require wide accumulators. We design, analyze, and\nimplement the PQS algorithm to eliminate accumulation overflows at inference\ntime for several neural networks. Our method offers a 2.5x reduction in\naccumulator bitwidth while achieving model accuracy on par with floating-point\nbaselines for multiple image classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09064v1",
    "published_date": "2025-04-12 03:51:42 UTC",
    "updated_date": "2025-04-12 03:51:42 UTC"
  },
  {
    "arxiv_id": "2504.09063v1",
    "title": "A Practical Approach to using Supervised Machine Learning Models to Classify Aviation Safety Occurrences",
    "authors": [
      "Bryan Y. Siow"
    ],
    "abstract": "This paper describes a practical approach of using supervised machine\nlearning (ML) models to assist safety investigators to classify aviation\noccurrences into either incident or serious incident categories. Our\nimplementation currently deployed as a ML web application is trained on a\nlabelled dataset derived from publicly available aviation investigation\nreports. A selection of five supervised learning models (Support Vector\nMachine, Logistic Regression, Random Forest Classifier, XGBoost and K-Nearest\nNeighbors) were evaluated. This paper showed the best performing ML algorithm\nwas the Random Forest Classifier with accuracy = 0.77, F1 Score = 0.78 and MCC\n= 0.51 (average of 100 sample runs). The study had also explored the effect of\napplying Synthetic Minority Over-sampling Technique (SMOTE) to the imbalanced\ndataset, and the overall observation ranged from no significant effect to\nsubstantial degradation in performance for some of the models after the SMOTE\nadjustment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.09063v1",
    "published_date": "2025-04-12 03:46:33 UTC",
    "updated_date": "2025-04-12 03:46:33 UTC"
  },
  {
    "arxiv_id": "2504.09060v1",
    "title": "Multimodal 3D Genome Pre-training",
    "authors": [
      "Minghao Yang",
      "Pengteng Li",
      "Yan Liang",
      "Qianyi Cai",
      "Zhihang Zheng",
      "Shichen Zhang",
      "Pengfei Zhang",
      "Zhi-An Huang",
      "Hui Xiong"
    ],
    "abstract": "Deep learning techniques have driven significant progress in various\nanalytical tasks within 3D genomics in computational biology. However, a\nholistic understanding of 3D genomics knowledge remains underexplored. Here, we\npropose MIX-HIC, the first multimodal foundation model of 3D genome that\nintegrates both 3D genome structure and epigenomic tracks, which obtains\nunified and comprehensive semantics. For accurate heterogeneous semantic\nfusion, we design the cross-modal interaction and mapping blocks for robust\nunified representation, yielding the accurate aggregation of 3D genome\nknowledge. Besides, we introduce the first large-scale dataset comprising over\n1 million pairwise samples of Hi-C contact maps and epigenomic tracks for\nhigh-quality pre-training, enabling the exploration of functional implications\nin 3D genomics. Extensive experiments show that MIX-HIC can significantly\nsurpass existing state-of-the-art methods in diverse downstream tasks. This\nwork provides a valuable resource for advancing 3D genomics research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09060v1",
    "published_date": "2025-04-12 03:31:03 UTC",
    "updated_date": "2025-04-12 03:31:03 UTC"
  },
  {
    "arxiv_id": "2504.09058v1",
    "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement",
    "authors": [
      "Chengyuan Liu",
      "Shihang Wang",
      "Lizhi Qing",
      "Kaisong Song",
      "Junjie Cao",
      "Jun Lin",
      "Ji Zhang",
      "Ang Li",
      "Kun Kuang",
      "Fei Wu"
    ],
    "abstract": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.09058v1",
    "published_date": "2025-04-12 03:25:01 UTC",
    "updated_date": "2025-04-12 03:25:01 UTC"
  },
  {
    "arxiv_id": "2504.12326v1",
    "title": "Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis",
    "authors": [
      "Shahriar Noroozizadeh",
      "Jeremy C. Weiss"
    ],
    "abstract": "Clinical case reports and discharge summaries may be the most complete and\naccurate summarization of patient encounters, yet they are finalized, i.e.,\ntimestamped after the encounter. Complementary data structured streams become\navailable sooner but suffer from incompleteness. To train models and algorithms\non more complete and temporally fine-grained data, we construct a pipeline to\nphenotype, extract, and annotate time-localized findings within case reports\nusing large language models. We apply our pipeline to generate an open-access\ntextual time series corpus for Sepsis-3 comprising 2,139 case reports from the\nPubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA\nand timeline annotations from I2B2/MIMIC-IV and compare the results to\nphysician-expert annotations. We show high recovery rates of clinical findings\n(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and\nstrong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B\nInstruct--0.932). Our work characterizes the ability of LLMs to time-localize\nclinical findings in text, illustrating the limitations of LLM use for temporal\nreconstruction and providing several potential avenues of improvement via\nmultimodal integration.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12326v1",
    "published_date": "2025-04-12 03:07:44 UTC",
    "updated_date": "2025-04-12 03:07:44 UTC"
  },
  {
    "arxiv_id": "2504.11474v1",
    "title": "Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD",
    "authors": [
      "Byunggun Kim",
      "Younghun Kwon"
    ],
    "abstract": "In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of\nthe common mental diseases discovered not only in children but also in adults.\nIn this context, we propose a ADHD diagnosis transformer model that can\neffectively simultaneously find important brain spatiotemporal biomarkers from\nresting-state functional magnetic resonance (rs-fMRI). This model not only\nlearns spatiotemporal individual features but also learns the correlation with\nfull attention structures specialized in ADHD diagnosis. In particular, it\nfocuses on learning local blood oxygenation level dependent (BOLD) signals and\ndistinguishing important regions of interest (ROI) in the brain. Specifically,\nthe three proposed methods for ADHD diagnosis transformer are as follows.\nFirst, we design a CNN-based embedding block to obtain more expressive\nembedding features in brain region attention. It is reconstructed based on the\npreviously CNN-based ADHD diagnosis models for the transformer. Next, for\nindividual spatiotemporal feature attention, we change the attention method to\nlocal temporal attention and ROI-rank based masking. For the temporal features\nof fMRI, the local temporal attention enables to learn local BOLD signal\nfeatures with only simple window masking. For the spatial feature of fMRI,\nROI-rank based masking can distinguish ROIs with high correlation in ROI\nrelationships based on attention scores, thereby providing a more specific\nbiomarker for ADHD diagnosis. The experiment was conducted with various types\nof transformer models. To evaluate these models, we collected the data from 939\nindividuals from all sites provided by the ADHD-200 competition. Through this,\nthe spatiotemporal enhanced transformer for ADHD diagnosis outperforms the\nperformance of other different types of transformer variants. (77.78ACC\n76.60SPE 79.22SEN 79.30AUC)",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11474v1",
    "published_date": "2025-04-12 02:03:35 UTC",
    "updated_date": "2025-04-12 02:03:35 UTC"
  },
  {
    "arxiv_id": "2504.09046v2",
    "title": "An Enhanced Iterative Deepening Search Algorithm for the Unrestricted Container Rehandling Problem",
    "authors": [
      "Ruoqi Wang",
      "Jiawei Li"
    ],
    "abstract": "In container terminal yards, the Container Rehandling Problem (CRP) involves\nrearranging containers between stacks under specific operational rules, and it\nis a pivotal optimization challenge in intelligent container scheduling\nsystems. Existing CRP studies primarily focus on minimizing reallocation costs\nusing two-dimensional bay structures, considering factors such as container\nsize, weight, arrival sequences, and retrieval priorities. This paper\nintroduces an enhanced deepening search algorithm integrated with improved\nlower bounds to boost search efficiency. To further reduce the search space, we\ndesign mutually consistent pruning rules to avoid excessive computational\noverhead. The proposed algorithm is validated on three widely used benchmark\ndatasets for the Unrestricted Container Rehandling Problem (UCRP). Experimental\nresults demonstrate that our approach outperforms state-of-the-art exact\nalgorithms in solving the more general UCRP variant, particularly exhibiting\nsuperior efficiency when handling containers within the same priority group\nunder strict time constraints.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Verification confirmed that this article used data without\n  authorization from the original owners, violating ethical standards for\n  scientific data sharing. To protect data copyright and maintain research\n  integrity, the article is retracted. Future content will strictly follow data\n  usage protocols",
    "pdf_url": "http://arxiv.org/pdf/2504.09046v2",
    "published_date": "2025-04-12 01:58:30 UTC",
    "updated_date": "2025-04-19 05:30:39 UTC"
  },
  {
    "arxiv_id": "2504.09039v1",
    "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization",
    "authors": [
      "Gen Li",
      "Yang Xiao",
      "Jie Ji",
      "Kaiyuan Deng",
      "Bo Hui",
      "Linke Guo",
      "Xiaolong Ma"
    ],
    "abstract": "Text-to-image (T2I) diffusion models have achieved remarkable success in\ngenerating high-quality images from textual prompts. However, their ability to\nstore vast amounts of knowledge raises concerns in scenarios where selective\nforgetting is necessary, such as removing copyrighted content, reducing biases,\nor eliminating harmful concepts. While existing unlearning methods can remove\ncertain concepts, they struggle with multi-concept forgetting due to\ninstability, residual knowledge persistence, and generation quality\ndegradation. To address these challenges, we propose \\textbf{Dynamic Mask\ncoupled with Concept-Aware Loss}, a novel unlearning framework designed for\nmulti-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask}\nmechanism adaptively updates gradient masks based on current optimization\nstates, allowing selective weight modifications that prevent interference with\nunrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly\nguides the unlearning process by enforcing semantic consistency through\nsuperclass alignment, while a regularization loss based on knowledge\ndistillation ensures that previously unlearned concepts remain forgotten during\nsequential unlearning. We conduct extensive experiments to evaluate our\napproach. Results demonstrate that our method outperforms existing unlearning\ntechniques in forgetting effectiveness, output fidelity, and semantic\ncoherence, particularly in multi-concept scenarios. Our work provides a\nprincipled and flexible framework for stable and high-fidelity unlearning in\ngenerative models. The code will be released publicly.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09039v1",
    "published_date": "2025-04-12 01:38:58 UTC",
    "updated_date": "2025-04-12 01:38:58 UTC"
  },
  {
    "arxiv_id": "2504.09037v1",
    "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
    "authors": [
      "Zixuan Ke",
      "Fangkai Jiao",
      "Yifei Ming",
      "Xuan-Phi Nguyen",
      "Austin Xu",
      "Do Xuan Long",
      "Minzhi Li",
      "Chengwei Qin",
      "Peifeng Wang",
      "Silvio Savarese",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "abstract": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "72 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09037v1",
    "published_date": "2025-04-12 01:27:49 UTC",
    "updated_date": "2025-04-12 01:27:49 UTC"
  },
  {
    "arxiv_id": "2504.09033v1",
    "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels",
    "authors": [
      "Snigdha Agarwal",
      "Neelam Sinha"
    ],
    "abstract": "Deep Convolutional Neural Networks have consistently proven to achieve\nstate-of-the-art results on a lot of imaging tasks over the past years'\nmajority of which comprise of high-quality data. However, it is important to\nwork on low-resolution images since it could be a cheaper alternative for\nremote healthcare access where the primary need of automated pathology\nidentification models occurs. Medical diagnosis using low-resolution images is\nchallenging since critical details may not be easily identifiable. In this\npaper, we report classification results by experimenting on different input\nimage sizes of Chest X-rays to deep CNN models and discuss the feasibility of\nclassification on varying image sizes. We also leverage the noisy labels in the\ndataset by proposing a Randomized Flipping of labels techniques. We use an\nensemble of multi-label classification models on frontal and lateral studies.\nOur models are trained on 5 out of the 14 chest pathologies of the publicly\navailable CheXpert dataset. We incorporate techniques such as augmentation,\nregularization for model improvement and use class activation maps to visualize\nthe neural network's decision making. Comparison with classification results on\ndata from 200 subjects, obtained on the corresponding high-resolution images,\nreported in the original CheXpert paper, has been presented. For pathologies\nCardiomegaly, Consolidation and Edema, we obtain 3% higher accuracy with our\nmodel architecture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09033v1",
    "published_date": "2025-04-12 01:13:00 UTC",
    "updated_date": "2025-04-12 01:13:00 UTC"
  },
  {
    "arxiv_id": "2504.13191v1",
    "title": "Universal Representations for Classification-enhanced Lossy Compression",
    "authors": [
      "Nam Nguyen"
    ],
    "abstract": "In lossy compression, the classical tradeoff between compression rate and\nreconstruction distortion has traditionally guided algorithm design. However,\nBlau and Michaeli [5] introduced a generalized framework, known as the\nrate-distortion-perception (RDP) function, incorporating perceptual quality as\nan additional dimension of evaluation. More recently, the\nrate-distortion-classification (RDC) function was investigated in [19],\nevaluating compression performance by considering classification accuracy\nalongside distortion. In this paper, we explore universal representations,\nwhere a single encoder is developed to achieve multiple decoding objectives\nacross various distortion and classification (or perception) constraints. This\nuniversality avoids retraining encoders for each specific operating point\nwithin these tradeoffs. Our experimental validation on the MNIST dataset\nindicates that a universal encoder incurs only minimal performance degradation\ncompared to individually optimized encoders for perceptual image compression\ntasks, aligning with prior results from [23]. Nonetheless, we also identify\nthat in the RDC setting, reusing an encoder optimized for one specific\nclassification-distortion tradeoff leads to a significant distortion penalty\nwhen applied to alternative points.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13191v1",
    "published_date": "2025-04-12 00:55:56 UTC",
    "updated_date": "2025-04-12 00:55:56 UTC"
  },
  {
    "arxiv_id": "2504.11473v1",
    "title": "Visual moral inference and communication",
    "authors": [
      "Warren Zhu",
      "Aida Ramezani",
      "Yang Xu"
    ],
    "abstract": "Humans can make moral inferences from multiple sources of input. In contrast,\nautomated moral inference in artificial intelligence typically relies on\nlanguage models with textual input. However, morality is conveyed through\nmodalities beyond language. We present a computational framework that supports\nmoral inference from natural images, demonstrated in two related tasks: 1)\ninferring human moral judgment toward visual images and 2) analyzing patterns\nin moral content communicated via images from public news. We find that models\nbased on text alone cannot capture the fine-grained human moral judgment toward\nvisual stimuli, but language-vision fusion models offer better precision in\nvisual moral inference. Furthermore, applications of our framework to news data\nreveal implicit biases in news categories and geopolitical discussions. Our\nwork creates avenues for automating visual moral inference and discovering\npatterns of visual moral communication in public media.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11473v1",
    "published_date": "2025-04-12 00:46:27 UTC",
    "updated_date": "2025-04-12 00:46:27 UTC"
  }
]
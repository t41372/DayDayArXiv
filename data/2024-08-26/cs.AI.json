{
  "date": "2024-08-26",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-26 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型优化、多模态生成、强化学习和医疗应用等领域，强调大型语言模型（LLMs）的创新应用、图像和视频生成技术，以及机器人导航的突破。其中，令人印象深刻的是第20篇论文（Advancing Humanoid Locomotion）获机器人领域最佳论文奖提名，以及第39篇（Foundation Models for Music）对音乐 AI 的全面调查；有名学者如 Edward Y. Chang（第15篇）提出的多 LLM 对话框架，也值得关注。这些论文展示了 AI 在实际应用中的潜力，但也暴露了模型鲁棒性和数据隐私的挑战。\n\n以下是今日论文的精选摘要，我会优先讨论重要、创新或相关性高的论文，将它们归类讨论（如 AI 生成和机器人领域），并快速掠过次要内容。每篇论文标题以“中文 + 英文”形式列出，保留核心学术术语，并清晰概述主要贡献和发现。\n\n### AI 生成与 LLMs 优化\n- **训练-Free Activation Sparsity in Large Language Models（无训练激活稀疏性在大语言模型中的应用）**：第3篇。贡献：提出 TEAL 方法，实现 40-50% 的模型稀疏性，同时保持性能，仅需微小损失；发现：在 Llama 和 Mistral 模型上，显著提升推理速度（最高 1.8 倍），并兼容权重量化。\n- **Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems（弥合差距：解析在线排序系统中知识蒸馏的隐藏挑战）**：第4篇。贡献：针对推荐系统，解决知识蒸馏中的数据分布偏移问题；发现：通过优化教师模型配置，提升学生模型性能，Google 实验显示显著改善视频推荐准确性。\n- **EVINCE: Optimizing Multi-LLM Dialogues Using Conditional Statistics and Information Theory（EVINCE：使用条件统计和信息理论优化多 LLM 对话）**：第15篇，由 Edward Y. Chang 提出。贡献：引入双熵优化框架，平衡视角多样性和知识共享；发现：提升多 LLM 协作效率，适用于辩论和协调任务。\n- **CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation（CURLoRA：稳定的大语言模型持续微调和灾难性遗忘缓解）**：第16篇。贡献：使用 CUR 矩阵分解结合 LoRA，减少可训练参数；发现：在连续微调中，显著降低遗忘风险，保持任务准确性和模型稳定性。\n- **K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences（K-Sort Arena：通过 K-元人类偏好进行生成模型的高效可靠基准测试）**：第21篇。贡献：提出 K-元比较基准框架，提升生成模型评估效率；发现：比传统 ELO 算法快 16 倍，在文本到图像任务中表现突出。\n- **DIAGen: Diverse Image Augmentation with Generative Models（DIAGen：使用生成模型的多样图像增强）**：第12篇。贡献：基于扩散模型增强图像语义多样性；发现：改善分类器性能，尤其在分布外样本上，优于标准增强方法。\n- **MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion（MagicMan：使用 3D 感知扩散模型的生成式人体新视图合成）**：第85篇。贡献：开发多视图生成框架，提升图像一致性；发现：显著改善下游 3D 重建任务，处理单图像到多视图转换。\n\n这些论文突出了 LLMs 在生成任务中的潜力，但也强调了稀疏性和知识蒸馏的挑战，相关工作如第16篇的 CURLoRA 可为实际部署提供高效工具。\n\n### 机器人与强化学习\n- **Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning（推进人形机器人运动：使用去噪世界模型学习掌握挑战性地形）**：第20篇（获 RSS 2024 最佳论文奖提名）。贡献：提出 DWL 框架，实现零样本模拟到真实转移；发现：人形机器人首次在雪地、楼梯等复杂地形上实现鲁棒导航。\n- **Bidirectional Emergent Language in Situated Environments（情境环境中双向涌现语言）**：第6篇。贡献：探索多代理系统中的通信协议；发现：在合作环境中，代理仅在需要协调时生成有意义消息，提升解释性。\n- **On Centralized Critics in Multi-Agent Reinforcement Learning（多代理强化学习中的集中批评者）**：第10篇。贡献：理论分析集中批评者方法的局限性；发现：在部分可观察环境中，分散批评者更有效，减少偏差。\n- **Equivariant Reinforcement Learning under Partial Observability（部分可观察条件下的等变强化学习）**：第41篇。贡献：将对称性融入代理设计；发现：在机器人任务中，提升样本效率和泛化性能。\n\n机器人领域的这些创新，如第20篇的 DWL，展示了 AI 在物理世界的应用潜力，但需解决可观察性问题。\n\n### 多模态与医疗应用\n- **Foundation Models for Music: A Survey（音乐基础模型调查）**：第39篇。贡献：全面回顾音乐领域的预训练模型；发现：LLMs 在音乐生成和理解中潜力巨大，但需关注伦理问题。\n- **SurGen: Text-Guided Diffusion Model for Surgical Video Generation（SurGen：用于手术视频生成的文本引导扩散模型）**：第94篇。贡献：开发文本到视频生成框架；发现：提升手术教育模拟的真实性和交互性。\n- **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs（通过知识图谱揭示放射学报告生成模型的知识差距）**：第30篇。贡献：使用知识图谱评估报告生成；发现：AI 模型在医疗报告中存在系统性错误，需改进。\n- **Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks（Video-CCAM：使用因果交叉注意力掩码增强视频-语言理解）**：第35篇。贡献：引入时空编码和注意力机制；发现：在长视频任务中，提升理解准确性，适用于多模态基准。\n\n医疗和多模态论文如第94篇的 SurGen，展示了 AI 在实际诊断中的价值，但强调了数据隐私和鲁棒性的必要性。\n\n### 其他快速掠过\n其余论文，如第1篇（AI 在景观建筑中的调查）、第5篇（知识图谱子图提取）、第7篇（AI 在互动艺术中的潜力）、第9篇（CNN 用于热通量预测）、第13-14篇（AI 在教育中的应用）、第17篇（临床笔记生成）、第22篇（时序逻辑）、第23篇（音频-视觉融合）、第26篇（强化学习偏好建模）、第29篇（LLM 剪枝）、第31篇（AI 创新框架）、第32篇（进化游戏在文化遗产中的应用）、第36篇（LLM 代理）、第37篇（放射学报告生成）、第38篇（AI 交互游戏）、第40篇（多代理系统）、第42篇（过程追踪）、第43篇（视频异常检测）、第44篇（胎儿脑部轨迹分析）、第45篇（索赔验证调查）、第46篇（ANN 解释）、第47篇（3D 打印监控）、第48篇（噪声标签学习）、第49篇（专利生成）、第50篇（图像字幕优化）、第51篇（不确定性估计）、第52篇（图像增强）、第53篇（少样本目标检测调查）、第54篇（联邦学习安全）、第55篇（本体学习）、第56篇（LLM 表情分析）、第57篇（目标识别）、第58篇（网络入侵预测）、第59篇（图提示学习调查）、第60篇（动态图建模）、第61篇（强化学习导航）、第62篇（过程查询）、第63篇（视频质量评估）、第64篇（手术视频生成相关）、第65篇（机器人轨迹规划）、第66篇（社交感知）、第67篇（强化学习定价）、第68篇（基准测试）、第69篇（图提示学习）、第70篇（视频生成）、第71篇（因果效应估计）、第72篇（归纳学习）、第73篇（GPU 通信）、第74篇（核安全事件分类）、第75篇（水印技术调查）、第76篇（抽象论证语义）、第77篇（CNN 硬件优化）、第78篇（歌声检测）、第79篇（手术视频生成）、第80篇（不确定性量化）、第81篇（AI 思考框架）、第82篇（路径规划）、第83篇（视频生成）、第84篇（基准测试）、第86篇（图像编辑）、第87篇（少样本学习）、第88篇（神经网络剪枝）、第89篇（医疗报告生成）、第90篇（音乐模型）、第91篇（强化学习）、第92篇（视频生成）、第93篇（少样本检测）等，涉及广泛主题，但多为特定领域优化或初步探索，贡献较局部，如第77篇的硬件感知剪枝提升了 CNN 效率，其他可参考具体标题进行深入。\n\n总之，今天的论文突显了 AI 在生成和应用的创新，但也需关注模型鲁棒性和伦理问题。未来几天，arXiv 更新将继续聚焦这些热点，敬请关注！",
  "papers": [
    {
      "arxiv_id": "2408.14700v1",
      "title": "Artificial Intelligence in Landscape Architecture: A Survey",
      "title_zh": "人工智能在景观建筑中：一项调查",
      "authors": [
        "Yue Xing",
        "Wensheng Gan",
        "Qidi Chen"
      ],
      "abstract": "The development history of landscape architecture (LA) reflects the human\npursuit of environmental beautification and ecological balance. With the\nadvancement of artificial intelligence (AI) technologies that simulate and\nextend human intelligence, immense opportunities have been provided for LA,\noffering scientific and technological support throughout the entire workflow.\nIn this article, we comprehensively review the applications of AI technology in\nthe field of LA. First, we introduce the many potential benefits that AI brings\nto the design, planning, and management aspects of LA. Secondly, we discuss how\nAI can assist the LA field in solving its current development problems,\nincluding urbanization, environmental degradation and ecological decline,\nirrational planning, insufficient management and maintenance, and lack of\npublic participation. Furthermore, we summarize the key technologies and\npractical cases of applying AI in the LA domain, from design assistance to\nintelligent management, all of which provide innovative solutions for the\nplanning, design, and maintenance of LA. Finally, we look ahead to the problems\nand opportunities in LA, emphasizing the need to combine human expertise and\njudgment for rational decision-making. This article provides both theoretical\nand practical guidance for LA designers, researchers, and technology\ndevelopers. The successful integration of AI technology into LA holds great\npromise for enhancing the field's capabilities and achieving more sustainable,\nefficient, and user-friendly outcomes.",
      "tldr_zh": "这篇论文对人工智能(AI)在景观建筑(LA)领域的应用进行了全面综述，介绍了AI如何为LA的设计、规划和管理带来潜在益处，如提升效率和生态平衡。论文讨论了AI在解决当前LA挑战方面的作用，包括应对城市化、环境退化、不合理规划和管理不足等问题，并总结了关键技术和实际案例，如设计辅助和智能管理。最终，它强调了结合人类专业知识进行决策的重要性，为LA研究者、设计师和技术开发者提供了理论和实践指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.14700v1",
      "published_date": "2024-08-26 23:54:17 UTC",
      "updated_date": "2024-08-26 23:54:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:43:03.511763"
    },
    {
      "arxiv_id": "2408.14698v2",
      "title": "Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express",
      "title_zh": "翻译失败",
      "authors": [
        "Cherag Aroraa",
        "Tracy Holloway King",
        "Jayant Kumar",
        "Yi Lu",
        "Sanat Sharma",
        "Arvind Srikantan",
        "David Uvalle",
        "Josep Valls-Vargas",
        "Harsha Vardhan"
      ],
      "abstract": "As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.",
      "tldr_zh": "这篇论文介绍了 Adobe Express 中的智能多模态搜索系统，通过整合上下文稀疏和密集嵌入来解决传统搜索在处理多模态查询（如文本和图像）时的挑战，例如整合用户区域和时效性特征。研究采用 AB 测试优化嵌入模型选择、匹配排名以及稀疏和密集 embeddings 的平衡，显著提升了短长查询的搜索性能。结果显示，该方法降低了空结果率（超过70%）、提高了点击通过率（CTR），并为开发稳健的 multi-modal search 系统提供了宝贵洞见。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "CIKM 2024 (International Conference on Information and Knowledge\n  Management), Multimodal Search and Recommendations Workshop",
      "pdf_url": "http://arxiv.org/pdf/2408.14698v2",
      "published_date": "2024-08-26 23:52:27 UTC",
      "updated_date": "2024-08-29 15:14:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:43:17.976533"
    },
    {
      "arxiv_id": "2408.14690v3",
      "title": "Training-Free Activation Sparsity in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "James Liu",
        "Pragaash Ponnusamy",
        "Tianle Cai",
        "Han Guo",
        "Yoon Kim",
        "Ben Athiwaratkun"
      ],
      "abstract": "Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains.",
      "tldr_zh": "本论文提出TEAL，一种无需训练的简单方法，通过基于幅度的activation sparsity应用于大型语言模型(LLMs)的隐藏状态，实现40-50%的模型级稀疏性，从而减少推理过程中的计算和内存开销。TEAL在Llama-2、Llama-3和Mistral系列模型（从7B到70B规模）上表现出色，仅有微小性能下降，同时实现了高达1.53×和1.8×的推理速度提升。论文还证明TEAL兼容weight quantization，进一步提升了效率，为LLMs的实用部署提供了重要贡献。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Rev. 2: ICLR 2025 Acceptance (Spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2408.14690v3",
      "published_date": "2024-08-26 23:30:15 UTC",
      "updated_date": "2025-02-25 21:00:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:43:28.218741"
    },
    {
      "arxiv_id": "2408.14678v1",
      "title": "Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Nikhil Khani",
        "Shuo Yang",
        "Aniruddh Nath",
        "Yang Liu",
        "Pendo Abbo",
        "Li Wei",
        "Shawn Andrews",
        "Maciej Kula",
        "Jarrod Kahn",
        "Zhe Zhao",
        "Lichan Hong",
        "Ed Chi"
      ],
      "abstract": "Knowledge Distillation (KD) is a powerful approach for compressing a large\nmodel into a smaller, more efficient model, particularly beneficial for\nlatency-sensitive applications like recommender systems. However, current KD\nresearch predominantly focuses on Computer Vision (CV) and NLP tasks,\noverlooking unique data characteristics and challenges inherent to recommender\nsystems. This paper addresses these overlooked challenges, specifically: (1)\nmitigating data distribution shifts between teacher and student models, (2)\nefficiently identifying optimal teacher configurations within time and\nbudgetary constraints, and (3) enabling computationally efficient and rapid\nsharing of teacher labels to support multiple students. We present a robust KD\nsystem developed and rigorously evaluated on multiple large-scale personalized\nvideo recommendation systems within Google. Our live experiment results\ndemonstrate significant improvements in student model performance while\nensuring consistent and reliable generation of high quality teacher labels from\na continuous data stream of data.",
      "tldr_zh": "该论文探讨了Knowledge Distillation (KD)在在线排名系统（如推荐系统）中的隐藏挑战，包括缓解教师和学生模型之间的数据分布偏移、优化教师配置以适应时间和预算限制，以及实现高效的教师标签共享以支持多个学生模型。为解决这些问题，研究者开发了一个稳健的KD系统，并在Google的多个大规模个性化视频推荐系统中进行了严格评估。实验结果显示，该系统显著提升了学生模型的性能，同时确保从持续数据流中可靠生成高质量教师标签。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14678v1",
      "published_date": "2024-08-26 23:01:48 UTC",
      "updated_date": "2024-08-26 23:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:43:41.006818"
    },
    {
      "arxiv_id": "2408.14658v1",
      "title": "KGPrune: a Web Application to Extract Subgraphs of Interest from Wikidata with Analogical Pruning",
      "title_zh": "KGPrune：一个使用类比修剪从 Wikidata 中提取感兴趣子图的 Web 应用",
      "authors": [
        "Pierre Monnin",
        "Cherif-Hassan Nousradine",
        "Lucas Jarnac",
        "Laurel Zuckerman",
        "Miguel Couceiro"
      ],
      "abstract": "Knowledge graphs (KGs) have become ubiquitous publicly available knowledge\nsources, and are nowadays covering an ever increasing array of domains.\nHowever, not all knowledge represented is useful or pertaining when considering\na new application or specific task. Also, due to their increasing size,\nhandling large KGs in their entirety entails scalability issues. These two\naspects asks for efficient methods to extract subgraphs of interest from\nexisting KGs. To this aim, we introduce KGPrune, a Web Application that, given\nseed entities of interest and properties to traverse, extracts their\nneighboring subgraphs from Wikidata. To avoid topical drift, KGPrune relies on\na frugal pruning algorithm based on analogical reasoning to only keep relevant\nneighbors while pruning irrelevant ones. The interest of KGPrune is illustrated\nby two concrete applications, namely, bootstrapping an enterprise KG and\nextracting knowledge related to looted artworks.",
      "tldr_zh": "该研究针对知识图谱（KGs）的规模和相关性问题，提出 KGPrune，一个 Web 应用，用于从 Wikidata 中提取感兴趣的子图。KGPrune 以种子实体和属性作为输入，通过基于类比推理的修剪算法（analogical pruning）来避免主题漂移，仅保留相关邻居。实验应用包括引导企业 KG 和提取与被掠夺艺术品相关的知识，展示了该工具在提升 KGs 可伸缩性和实用性方面的价值。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a demo paper at ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14658v1",
      "published_date": "2024-08-26 21:47:49 UTC",
      "updated_date": "2024-08-26 21:47:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:43:52.534987"
    },
    {
      "arxiv_id": "2408.14649v2",
      "title": "Bidirectional Emergent Language in Situated Environments",
      "title_zh": "情境环境中的双向涌现语言",
      "authors": [
        "Cornelius Wolff",
        "Julius Mayer",
        "Elia Bruni",
        "Xenia Ohmer"
      ],
      "abstract": "Emergent language research has made significant progress in recent years, but\nstill largely fails to explore how communication emerges in more complex and\nsituated multi-agent systems. Existing setups often employ a reference game,\nwhich limits the range of language emergence phenomena that can be studied, as\nthe game consists of a single, purely language-based interaction between the\nagents. In this paper, we address these limitations and explore the emergence\nand utility of token-based communication in open-ended multi-agent\nenvironments, where situated agents interact with the environment through\nmovement and communication over multiple time-steps. Specifically, we introduce\ntwo novel cooperative environments: Multi-Agent Pong and Collectors. These\nenvironments are interesting because optimal performance requires the emergence\nof a communication protocol, but moderate success can be achieved without one.\nBy employing various methods from explainable AI research, such as saliency\nmaps, perturbation, and diagnostic classifiers, we are able to track and\ninterpret the agents' language channel use over time. We find that the emerging\ncommunication is sparse, with the agents only generating meaningful messages\nand acting upon incoming messages in states where they cannot succeed without\ncoordination.",
      "tldr_zh": "这项研究探讨了 emergent language 在复杂多智能体环境的涌现问题，指出现有参考游戏设置的局限性无法充分模拟真实互动。作者引入了两个新合作环境：Multi-Agent Pong 和 Collectors，其中代理需通过移动和多时间步通信来互动，最优性能依赖通信协议的出现。利用 explainable AI 方法，如 saliency maps、perturbation 和 diagnostic classifiers，研究者跟踪并解释了代理的语言使用。结果显示，emergent communication 是稀疏的，仅在无法独立成功的状态下生成和响应有意义的消息。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures, 4 tables, preprint",
      "pdf_url": "http://arxiv.org/pdf/2408.14649v2",
      "published_date": "2024-08-26 21:25:44 UTC",
      "updated_date": "2024-10-17 10:55:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:44:04.858977"
    },
    {
      "arxiv_id": "2408.14644v1",
      "title": "Visions of Destruction: Exploring a Potential of Generative AI in Interactive Art",
      "title_zh": "翻译失败",
      "authors": [
        "Mar Canet Sola",
        "Varvara Guljajeva"
      ],
      "abstract": "This paper explores the potential of generative AI within interactive art,\nemploying a practice-based research approach. It presents the interactive\nartwork \"Visions of Destruction\" as a detailed case study, highlighting its\ninnovative use of generative AI to create a dynamic, audience-responsive\nexperience. This artwork applies gaze-based interaction to dynamically alter\ndigital landscapes, symbolizing the impact of human activities on the\nenvironment by generating contemporary collages created with AI, trained on\ndata about human damage to nature, and guided by audience interaction. The\ntransformation of pristine natural scenes into human-made and industrialized\nlandscapes through viewer interaction serves as a stark reminder of\nenvironmental degradation. The paper thoroughly explores the technical\nchallenges and artistic innovations involved in creating such an interactive\nart installation, emphasizing the potential of generative AI to revolutionize\nartistic expression, audience engagement, and especially the opportunities for\nthe interactive art field. It offers insights into the conceptual framework\nbehind the artwork, aiming to evoke a deeper understanding and reflection on\nthe Anthropocene era and human-induced climate change. This study contributes\nsignificantly to the field of creative AI and interactive art, blending\ntechnology and environmental consciousness in a compelling, thought-provoking\nmanner.",
      "tldr_zh": "这篇论文探讨了生成式 AI 在互动艺术中的潜力，通过实践-based 研究方法，以互动艺术作品 \"Visions of Destruction\" 为案例研究。作品利用 gaze-based interaction 和基于人类对自然破坏数据的 AI 训练，动态生成当代拼贴画，让观众交互将原始自然场景转化为工业化景观，以象征人类活动对环境的影响。论文强调了生成式 AI 在克服技术挑战方面的创新，帮助革命化艺术表达和观众参与，并通过这一框架激发对 Anthropocene 时代及气候变化的深刻反思。该研究为创意 AI 和互动艺术领域做出了重大贡献，融合了技术和环境意识。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2; J.5"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14644v1",
      "published_date": "2024-08-26 21:20:45 UTC",
      "updated_date": "2024-08-26 21:20:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:44:17.077052"
    },
    {
      "arxiv_id": "2408.14640v1",
      "title": "Effect of Adaptation Rate and Cost Display in a Human-AI Interaction Game",
      "title_zh": "翻译失败",
      "authors": [
        "Jason T. Isa",
        "Bohan Wu",
        "Qirui Wang",
        "Yilin Zhang",
        "Samuel A. Burden",
        "Lillian J. Ratliff",
        "Benjamin J. Chasnov"
      ],
      "abstract": "As interactions between humans and AI become more prevalent, it is critical\nto have better predictors of human behavior in these interactions. We\ninvestigated how changes in the AI's adaptive algorithm impact behavior\npredictions in two-player continuous games. In our experiments, the AI adapted\nits actions using a gradient descent algorithm under different adaptation rates\nwhile human participants were provided cost feedback. The cost feedback was\nprovided by one of two types of visual displays: (a) cost at the current joint\naction vector, or (b) cost in a local neighborhood of the current joint action\nvector. Our results demonstrate that AI adaptation rate can significantly\naffect human behavior, having the ability to shift the outcome between two game\ntheoretic equilibrium. We observed that slow adaptation rates shift the outcome\ntowards the Nash equilibrium, while fast rates shift the outcome towards the\nhuman-led Stackelberg equilibrium. The addition of localized cost information\nhad the effect of shifting outcomes towards Nash, compared to the outcomes from\ncost information at only the current joint action vector. Future work will\ninvestigate other effects that influence the convergence of gradient descent\ngames.",
      "tldr_zh": "本文研究了 AI 适应率和成本显示对人类-AI 互动博弈中行为预测的影响，通过实验考察 AI 使用 gradient descent 算法在不同适应率下调整动作，同时人类参与者通过两种视觉显示获取成本反馈：当前联合动作向量的成本，或其局部邻域的成本。结果表明，AI 的慢速适应率使博弈结果趋向 Nash equilibrium，而快速适应率则推动结果向人类主导的 Stackelberg equilibrium 偏移；提供局部成本信息能使结果更接近 Nash equilibrium。该研究为理解人类在 AI 互动中的行为提供了重要洞见，并为探索影响 gradient descent 博弈收敛的其他因素铺平了道路。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14640v1",
      "published_date": "2024-08-26 21:08:21 UTC",
      "updated_date": "2024-08-26 21:08:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:44:29.392189"
    },
    {
      "arxiv_id": "2408.14626v1",
      "title": "Hybrid Deep Convolutional Neural Networks Combined with Autoencoders And Augmented Data To Predict The Look-Up Table 2006",
      "title_zh": "翻译失败",
      "authors": [
        "Messaoud Djeddou",
        "Aouatef Hellal",
        "Ibrahim A. Hameed",
        "Xingang Zhao",
        "Djehad Al Dallal"
      ],
      "abstract": "This study explores the development of a hybrid deep convolutional neural\nnetwork (DCNN) model enhanced by autoencoders and data augmentation techniques\nto predict critical heat flux (CHF) with high accuracy. By augmenting the\noriginal input features using three different autoencoder configurations, the\nmodel's predictive capabilities were significantly improved. The hybrid models\nwere trained and tested on a dataset of 7225 samples, with performance metrics\nincluding the coefficient of determination (R2), Nash-Sutcliffe efficiency\n(NSE), mean absolute error (MAE), and normalized root-mean-squared error\n(NRMSE) used for evaluation. Among the tested models, the DCNN_3F-A2\nconfiguration demonstrated the highest accuracy, achieving an R2 of 0.9908\nduring training and 0.9826 during testing, outperforming the base model and\nother augmented versions. These results suggest that the proposed hybrid\napproach, combining deep learning with feature augmentation, offers a robust\nsolution for CHF prediction, with the potential to generalize across a wider\nrange of conditions.",
      "tldr_zh": "本研究开发了一种混合深度卷积神经网络（DCNN）模型，通过结合自编码器（autoencoders）和数据增强技术，来预测关键热通量（CHF），以提高预测准确性。研究者使用三种自编码器配置增强输入特征，并在包含7225个样本的数据集上训练和测试模型，采用R2、NSE、MAE和NRMSE等指标进行评估。其中，DCNN_3F-A2配置表现出最佳性能，训练时的R2达到0.9908，测试时的R2为0.9826，显著优于基础模型和其他版本。该方法证明了深度学习与特征增强相结合的鲁棒性，有望在更广泛条件下推广CHF预测应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.14626v1",
      "published_date": "2024-08-26 20:45:07 UTC",
      "updated_date": "2024-08-26 20:45:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:44:40.556894"
    },
    {
      "arxiv_id": "2408.14597v1",
      "title": "On Centralized Critics in Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xueguang Lyu",
        "Andrea Baisero",
        "Yuchen Xiao",
        "Brett Daley",
        "Christopher Amato"
      ],
      "abstract": "Centralized Training for Decentralized Execution where agents are trained\noffline in a centralized fashion and execute online in a decentralized manner,\nhas become a popular approach in Multi-Agent Reinforcement Learning (MARL). In\nparticular, it has become popular to develop actor-critic methods that train\ndecentralized actors with a centralized critic where the centralized critic is\nallowed access global information of the entire system, including the true\nsystem state. Such centralized critics are possible given offline information\nand are not used for online execution. While these methods perform well in a\nnumber of domains and have become a de facto standard in MARL, using a\ncentralized critic in this context has yet to be sufficiently analyzed\ntheoretically or empirically. In this paper, we therefore formally analyze\ncentralized and decentralized critic approaches, and analyze the effect of\nusing state-based critics in partially observable environments. We derive\ntheories contrary to the common intuition: critic centralization is not\nstrictly beneficial, and using state values can be harmful. We further prove\nthat, in particular, state-based critics can introduce unexpected bias and\nvariance compared to history-based critics. Finally, we demonstrate how the\ntheory applies in practice by comparing different forms of critics on a wide\nrange of common multi-agent benchmarks. The experiments show practical issues\nsuch as the difficulty of representation learning with partial observability,\nwhich highlights why the theoretical problems are often overlooked in the\nliterature.",
      "tldr_zh": "这篇论文分析了在多智能体强化学习(MARL)中，采用集中式训练和分散式执行(Centralized Training for Decentralized Execution)框架下，centralized critic 的影响。作者通过理论证明，centralized critic 并非总是有益，且在部分可观察环境中，使用 state-based critics 可能引入额外的偏差和方差，与 history-based critics 相比更具风险。实验结果在多种多智能体基准上验证了这些理论问题，突显了部分可观察性下表示学习（representation learning）的实际挑战。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14597v1",
      "published_date": "2024-08-26 19:27:06 UTC",
      "updated_date": "2024-08-26 19:27:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:44:53.042227"
    },
    {
      "arxiv_id": "2408.14593v1",
      "title": "How to build trust in answers given by Generative AI for specific, and vague, financial questions",
      "title_zh": "翻译失败",
      "authors": [
        "Alex Zarifis",
        "Xusen Cheng"
      ],
      "abstract": "Purpose: Generative artificial intelligence (GenAI) has progressed in its\nability and has seen explosive growth in adoption. However, the consumer's\nperspective on its use, particularly in specific scenarios such as financial\nadvice, is unclear. This research develops a model of how to build trust in the\nadvice given by GenAI when answering financial questions.\nDesign/methodology/approach: The model is tested with survey data using\nstructural equation modelling (SEM) and multi-group analysis (MGA). The MGA\ncompares two scenarios, one where the consumer makes a specific question and\none where a vague question is made. Findings: This research identifies that\nbuilding trust for consumers is different when they ask a specific financial\nquestion in comparison to a vague one. Humanness has a different effect in the\ntwo scenarios. When a financial question is specific, human-like interaction\ndoes not strengthen trust, while (1) when a question is vague, humanness builds\ntrust. The four ways to build trust in both scenarios are (2) human oversight\nand being in the loop, (3) transparency and control, (4) accuracy and\nusefulness and finally (5) ease of use and support. Originality/value: This\nresearch contributes to a better understanding of the consumer's perspective\nwhen using GenAI for financial questions and highlights the importance of\nunderstanding GenAI in specific contexts from specific stakeholders.",
      "tldr_zh": "本研究探讨了如何在生成式 AI (Generative AI) 回答具体和模糊金融问题时建立消费者信任，开发了一个信任构建模型。研究采用调查数据、结构方程建模 (SEM) 和多组分析 (MGA) 来比较两种场景，发现具体问题中人类性 (Humanness) 不增强信任，而模糊问题中则有助于信任构建。总体而言，两种场景下信任可以通过人类监督与参与、透明度和控制、准确性与有用性，以及易用性和支持等四种方式实现。该研究强调了理解消费者视角在特定上下文中的重要性，为 GenAI 在金融领域的应用提供了新见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14593v1",
      "published_date": "2024-08-26 19:26:48 UTC",
      "updated_date": "2024-08-26 19:26:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:45:06.388550"
    },
    {
      "arxiv_id": "2408.14584v1",
      "title": "DIAGen: Diverse Image Augmentation with Generative Models",
      "title_zh": "DIAGen：基于生成模型的多样图像增强",
      "authors": [
        "Tobias Lingenberg",
        "Markus Reuter",
        "Gopika Sudhakaran",
        "Dominik Gojny",
        "Stefan Roth",
        "Simone Schaub-Meyer"
      ],
      "abstract": "Simple data augmentation techniques, such as rotations and flips, are widely\nused to enhance the generalization power of computer vision models. However,\nthese techniques often fail to modify high-level semantic attributes of a\nclass. To address this limitation, researchers have explored generative\naugmentation methods like the recently proposed DA-Fusion. Despite some\nprogress, the variations are still largely limited to textural changes, thus\nfalling short on aspects like varied viewpoints, environment, weather\nconditions, or even class-level semantic attributes (eg, variations in a dog's\nbreed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion.\nFirst, we apply Gaussian noise to the embeddings of an object learned with\nTextual Inversion to diversify generations using a pre-trained diffusion\nmodel's knowledge. Second, we exploit the general knowledge of a text-to-text\ngenerative model to guide the image generation of the diffusion model with\nvaried class-specific prompts. Finally, we introduce a weighting mechanism to\nmitigate the impact of poorly generated samples. Experimental results across\nvarious datasets show that DIAGen not only enhances semantic diversity but also\nimproves the performance of subsequent classifiers. The advantages of DIAGen\nover standard augmentations and the DA-Fusion baseline are particularly\npronounced with out-of-distribution samples.",
      "tldr_zh": "该研究提出DIAGen，一种基于生成模型的图像增强方法，旨在解决传统增强技术（如旋转和翻转）无法有效修改高水平语义属性的问题。DIAGen在DA-Fusion基础上改进，通过对Textual Inversion学习的物体嵌入施加Gaussian noise、利用文本到文本生成模型指导扩散模型生成多样化类特定提示，并引入权重机制来过滤低质量样本，从而提升图像的语义多样性。实验结果显示，DIAGen在多种数据集上显著提高了后续分类器的性能，尤其在out-of-distribution样本上优于标准增强和DA-Fusion基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in GCPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14584v1",
      "published_date": "2024-08-26 19:09:13 UTC",
      "updated_date": "2024-08-26 19:09:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:45:17.900903"
    },
    {
      "arxiv_id": "2409.06723v1",
      "title": "Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI",
      "title_zh": "小学学生和教师对使用生成式 AI 进行创造性数学写作的看法",
      "authors": [
        "Yukyeong Song",
        "Jinhee Kim",
        "Wanli Xing",
        "Zifeng Liu",
        "Chenglu Li",
        "Hyunju Oh"
      ],
      "abstract": "While mathematical creative writing can potentially engage students in\nexpressing mathematical ideas in an imaginative way, some elementary school-age\nstudents struggle in this process. Generative AI (GenAI) offers possibilities\nfor supporting creative writing activities, such as providing story generation.\nHowever, the design of GenAI-powered learning technologies requires careful\nconsideration of the technology reception in the actual classrooms. This study\nexplores students' and teachers' perceptions of creative mathematical writing\nwith the developed GenAI-powered technology. The study adopted a qualitative\nthematic analysis of the interviews, triangulated with open-ended survey\nresponses and classroom observation of 79 elementary school students, resulting\nin six themes and 19 subthemes. This study contributes by investigating the\nlived experience of GenAI-supported learning and the design considerations for\nGenAI-powered learning technologies and instructions.",
      "tldr_zh": "这篇论文探讨了小学生和教师对使用Generative AI支持数学创造性写作的看法，因为许多小学生在表达数学想法时面临困难。研究采用定性主题分析，包括对79名小学生的访谈、开放式调查和课堂观察，提炼出六个主题和19个子主题。论文的主要贡献在于揭示了Generative AI支持学习的生活体验，并为GenAI驱动的学习技术和教学指令提供了设计建议。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.06723v1",
      "published_date": "2024-08-26 19:04:08 UTC",
      "updated_date": "2024-08-26 19:04:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:45:29.735064"
    },
    {
      "arxiv_id": "2409.06721v1",
      "title": "Students' Perceived Roles, Opportunities, and Challenges of a Generative AI-powered Teachable Agent: A Case of Middle School Math Class",
      "title_zh": "翻译失败",
      "authors": [
        "Yukyeong Song",
        "Jinhee Kim",
        "Zifeng Liu",
        "Chenglu Li",
        "Wanli Xing"
      ],
      "abstract": "Ongoing advancements in Generative AI (GenAI) have boosted the potential of\napplying long-standing learning-by-teaching practices in the form of a\nteachable agent (TA). Despite the recognized roles and opportunities of TAs,\nless is known about how GenAI could create synergy or introduce challenges in\nTAs and how students perceived the application of GenAI in TAs. This study\nexplored middle school students perceived roles, benefits, and challenges of\nGenAI-powered TAs in an authentic mathematics classroom. Through classroom\nobservation, focus-group interviews, and open-ended surveys of 108 sixth-grade\nstudents, we found that students expected the GenAI-powered TA to serve as a\nlearning companion, facilitator, and collaborative problem-solver. Students\nalso expressed the benefits and challenges of GenAI-powered TAs. This study\nprovides implications for the design of educational AI and AI-assisted\ninstruction.",
      "tldr_zh": "本研究探讨了中学学生对生成式 AI (GenAI) 驱动的可教代理 (Teachable Agent, TA) 的感知，包括其角色、机会和挑战，基于一个真实的中学数学课堂案例。研究通过课堂观察、焦点小组访谈和对 108 名六年级学生的开放式调查，发现学生期望 GenAI 驱动的 TA 作为学习伴侣、促进者和协作问题解决者。学生同时表达了这种 TA 的益处（如增强学习效果）和挑战（如潜在的协同问题）。该研究为教育 AI 和 AI 辅助教学的设计提供了重要启示。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.06721v1",
      "published_date": "2024-08-26 18:54:20 UTC",
      "updated_date": "2024-08-26 18:54:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:45:43.075881"
    },
    {
      "arxiv_id": "2408.14575v4",
      "title": "EVINCE: Optimizing Multi-LLM Dialogues Using Conditional Statistics and Information Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Y. Chang"
      ],
      "abstract": "EVINCE (Entropy and Variation IN Conditional Exchanges) is a novel framework\nfor optimizing multi-LLM dialogues using conditional statistics and information\ntheory. It addresses limitations in multi-agent debate (MAS) frameworks, where\nmultiple LLMs ``chat'' without behavior modulation or mutual information\nquality assessment. Using dual entropy optimization to balance perspective\ndiversity and prior knowledge, $\\EVINCE$ provides quantitative tools to\ndynamically regulate LLM linguistic behaviors. When mutual information is low\nand both cross-entropy and Wasserstein distance are high, EVINCE promotes\ncontentious dialogues to expose diverse perspectives and uncover\ninconsistencies. Conversely, as cross-entropy decreases and mutual information\nstabilizes, it transitions discussions into a conciliatory phase, encouraging\ncompromise and acknowledgment of valid points. Using information-theoretic\nmetrics and optimizing mutual information, $\\EVINCE$ emerges as a structured\nand highly effective framework for multi-LLM collaboration.",
      "tldr_zh": "这篇论文介绍了 EVINCE 框架，一种利用条件统计和信息理论优化多-LLM 对话的新方法，以解决多智能体辩论（MAS）框架中缺乏行为调节和互信息质量评估的问题。EVINCE 通过双熵优化（dual entropy optimization）平衡视角多样性和先验知识，当互信息低、交叉熵（cross-entropy）和 Wasserstein 距离高时，促进争论以暴露多样视角；反之，当交叉熵降低且互信息稳定时，转向和解阶段鼓励妥协。实验结果表明，该框架通过信息理论指标优化互信息，提供动态量化工具，提升多-LLM 协作的有效性和结构化水平。",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 9 figures, 10 tables. arXiv admin note: text overlap with\n  arXiv:2405.15808",
      "pdf_url": "http://arxiv.org/pdf/2408.14575v4",
      "published_date": "2024-08-26 18:48:51 UTC",
      "updated_date": "2025-01-29 20:48:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:45:55.377991"
    },
    {
      "arxiv_id": "2408.14572v1",
      "title": "CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation",
      "title_zh": "CURLoRA：稳定的 LLM 持续微调以及灾难性遗忘缓解",
      "authors": [
        "Muhammad Fawi"
      ],
      "abstract": "This paper introduces CURLoRA, a novel approach to fine-tuning large language\nmodels (LLMs) that leverages CUR matrix decomposition in the context of\nLow-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM\nfine-tuning: mitigating catastrophic forgetting during continual learning and\nreducing the number of trainable parameters. We propose a unique modification\nto the CUR decomposition process, utilizing inverted probabilities for column\nand row selection which acts as an implicit regularization, and initializing\nthe $U$ matrix as a zero matrix, and only fine-tuning it. We demonstrate\nthrough experiments on multiple datasets that CURLoRA outperforms standard LoRA\nin mitigating catastrophic forgetting. It maintains model stability and\nperformance across tasks while significantly reducing the number of trainable\nparameters. Our results show that CURLoRA achieves very good and stable task\naccuracy while maintaining base model's perplexity scores fixed compared to\nLoRA upon continual fine-tuning, particularly in scenarios with limited data.",
      "tldr_zh": "这篇论文引入了 CURLoRA，一种结合 CUR 矩阵分解和 Low-Rank Adaptation (LoRA) 的新方法，用于微调大型语言模型 (LLMs)，旨在缓解灾难性遗忘 (catastrophic forgetting) 并减少可训练参数。\nCURLoRA 通过修改 CUR 分解过程，使用倒置概率作为隐式正则化、将 U 矩阵初始化为零矩阵并仅微调它，从而提升模型在持续学习中的稳定性和效率。\n实验结果显示，CURLoRA 在多个数据集上优于标准 LoRA，能够维持任务准确性并固定基础模型的 perplexity 分数，尤其在数据有限的场景下表现突出。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at https://github.com/MNoorFawi/curlora",
      "pdf_url": "http://arxiv.org/pdf/2408.14572v1",
      "published_date": "2024-08-26 18:42:59 UTC",
      "updated_date": "2024-08-26 18:42:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:46:09.594013"
    },
    {
      "arxiv_id": "2408.14568v1",
      "title": "Improving Clinical Note Generation from Complex Doctor-Patient Conversation",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhan Li",
        "Sifan Wu",
        "Christopher Smith",
        "Thomas Lo",
        "Bang Liu"
      ],
      "abstract": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.",
      "tldr_zh": "本文研究旨在改进基于复杂医生-患者对话的临床笔记生成，以减轻医疗专业人员的书写负担并提升效率。主要贡献包括：引入 CliniKnote 数据集，该数据集包含1200个真实对话及其完整临床笔记，由医疗专家和神经网络共同构建；提出 K-SOAP 笔记格式，在传统 SOAP 基础上添加关键词部分，便于快速提取关键信息；以及开发自动管道利用大型语言模型 (LLMs) 生成 K-SOAP 笔记，并通过各种指标基准测试，结果显示比标准 LLM 微调方法在效率和性能上显著提升。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14568v1",
      "published_date": "2024-08-26 18:39:31 UTC",
      "updated_date": "2024-08-26 18:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:46:18.967536"
    },
    {
      "arxiv_id": "2408.14562v1",
      "title": "A Survey of Camouflaged Object Detection and Beyond",
      "title_zh": "伪装物体检测的综述与扩展",
      "authors": [
        "Fengyang Xiao",
        "Sujie Hu",
        "Yuqi Shen",
        "Chengyu Fang",
        "Jinfa Huang",
        "Chunming He",
        "Longxiang Tang",
        "Ziyun Yang",
        "Xiu Li"
      ],
      "abstract": "Camouflaged Object Detection (COD) refers to the task of identifying and\nsegmenting objects that blend seamlessly into their surroundings, posing a\nsignificant challenge for computer vision systems. In recent years, COD has\ngarnered widespread attention due to its potential applications in\nsurveillance, wildlife conservation, autonomous systems, and more. While\nseveral surveys on COD exist, they often have limitations in terms of the\nnumber and scope of papers covered, particularly regarding the rapid\nadvancements made in the field since mid-2023. To address this void, we present\nthe most comprehensive review of COD to date, encompassing both theoretical\nframeworks and practical contributions to the field. This paper explores\nvarious COD methods across four domains, including both image-level and\nvideo-level solutions, from the perspectives of traditional and deep learning\napproaches. We thoroughly investigate the correlations between COD and other\ncamouflaged scenario methods, thereby laying the theoretical foundation for\nsubsequent analyses. Beyond object-level detection, we also summarize extended\nmethods for instance-level tasks, including camouflaged instance segmentation,\ncounting, and ranking. Additionally, we provide an overview of commonly used\nbenchmarks and evaluation metrics in COD tasks, conducting a comprehensive\nevaluation of deep learning-based techniques in both image and video domains,\nconsidering both qualitative and quantitative performance. Finally, we discuss\nthe limitations of current COD models and propose 9 promising directions for\nfuture research, focusing on addressing inherent challenges and exploring\nnovel, meaningful technologies. For those interested, a curated list of\nCOD-related techniques, datasets, and additional resources can be found at\nhttps://github.com/ChunmingHe/awesome-concealed-object-segmentation",
      "tldr_zh": "这篇论文对 Camouflaged Object Detection (COD) 进行了迄今为止最全面的调查，涵盖了从传统到深度学习的方法，包括图像和视频级别的解决方案，并探讨了 COD 与其他 camouflaged 场景方法的关联。\n论文总结了扩展任务，如 camouflaged instance segmentation、计数和排名，并提供了常用基准、评估指标的概述，同时对深度学习技术进行了定性和定量评估。\n最终，它讨论了当前 COD 模型的局限性，并提出了 9 个未来研究方向，以应对挑战并探索新技术和应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 10 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.14562v1",
      "published_date": "2024-08-26 18:23:22 UTC",
      "updated_date": "2024-08-26 18:23:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:46:29.541530"
    },
    {
      "arxiv_id": "2408.14547v1",
      "title": "Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization",
      "title_zh": "通过直接基于 CLIP 的优化重新审视图像描述训练范式",
      "authors": [
        "Nicholas Moratelli",
        "Davide Caffagni",
        "Marcella Cornia",
        "Lorenzo Baraldi",
        "Rita Cucchiara"
      ],
      "abstract": "The conventional training approach for image captioning involves pre-training\na network using teacher forcing and subsequent fine-tuning with Self-Critical\nSequence Training to maximize hand-crafted captioning metrics. However, when\nattempting to optimize modern and higher-quality metrics like CLIP-Score and\nPAC-Score, this training method often encounters instability and fails to\nacquire the genuine descriptive capabilities needed to produce fluent and\ninformative captions. In this paper, we propose a new training paradigm termed\nDirect CLIP-Based Optimization (DiCO). Our approach jointly learns and\noptimizes a reward model that is distilled from a learnable captioning\nevaluator with high human correlation. This is done by solving a weighted\nclassification problem directly inside the captioner. At the same time, DiCO\nprevents divergence from the original model, ensuring that fluency is\nmaintained. DiCO not only exhibits improved stability and enhanced quality in\nthe generated captions but also aligns more closely with human preferences\ncompared to existing methods, especially in modern metrics. Additionally, it\nmaintains competitive performance in traditional metrics. Our source code and\ntrained models are publicly available at https://github.com/aimagelab/DiCO.",
      "tldr_zh": "本研究重新审视了图像描述(Image Captioning)的训练范式，指出传统方法使用Teacher Forcing预训练和Self-Critical Sequence Training微调时，在优化现代指标如CLIP-Score和PAC-Score时容易出现不稳定问题，无法生成流畅且信息丰富的描述。论文提出Direct CLIP-Based Optimization (DiCO)新范式，通过联合学习从可学习描述评估器中蒸馏的奖励模型，并直接解决加权分类问题来优化描述生成，同时防止模型偏离以保持流畅性。实验结果显示，DiCO显著提升了生成描述的稳定性和质量，更好地与人类偏好对齐，尤其在现代指标上表现出色，同时在传统指标上保持竞争性；相关源代码和模型已在GitHub开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "BMVC 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14547v1",
      "published_date": "2024-08-26 18:00:33 UTC",
      "updated_date": "2024-08-26 18:00:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:46:45.234273"
    },
    {
      "arxiv_id": "2408.14472v1",
      "title": "Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning",
      "title_zh": "推进人形机器人运动：利用去噪世界模型学习掌握挑战性地形",
      "authors": [
        "Xinyang Gu",
        "Yen-Jen Wang",
        "Xiang Zhu",
        "Chengming Shi",
        "Yanjiang Guo",
        "Yichen Liu",
        "Jianyu Chen"
      ],
      "abstract": "Humanoid robots, with their human-like skeletal structure, are especially\nsuited for tasks in human-centric environments. However, this structure is\naccompanied by additional challenges in locomotion controller design,\nespecially in complex real-world environments. As a result, existing humanoid\nrobots are limited to relatively simple terrains, either with model-based\ncontrol or model-free reinforcement learning. In this work, we introduce\nDenoising World Model Learning (DWL), an end-to-end reinforcement learning\nframework for humanoid locomotion control, which demonstrates the world's first\nhumanoid robot to master real-world challenging terrains such as snowy and\ninclined land in the wild, up and down stairs, and extremely uneven terrains.\nAll scenarios run the same learned neural network with zero-shot sim-to-real\ntransfer, indicating the superior robustness and generalization capability of\nthe proposed method.",
      "tldr_zh": "本研究针对人形机器人（humanoid robots）在复杂真实环境下的运动控制挑战，引入了Denoising World Model Learning (DWL)，一个端到端强化学习（reinforcement learning）框架。该框架使人形机器人首次成功掌握诸如雪地、斜坡、楼梯和极不平坦地形等挑战性地形，所有场景均使用相同的神经网络实现零样本模拟到现实（sim-to-real）转移。实验结果展示了DWL的卓越鲁棒性和泛化能力，为人形机器人应用提供了新突破。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Robotics: Science and Systems (RSS), 2024. (Best Paper Award\n  Finalist)",
      "pdf_url": "http://arxiv.org/pdf/2408.14472v1",
      "published_date": "2024-08-26 17:59:03 UTC",
      "updated_date": "2024-08-26 17:59:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:46:54.628960"
    },
    {
      "arxiv_id": "2408.14468v2",
      "title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences",
      "title_zh": "翻译失败",
      "authors": [
        "Zhikai Li",
        "Xuewen Liu",
        "Dongrong Joe Fu",
        "Jianquan Li",
        "Qingyi Gu",
        "Kurt Keutzer",
        "Zhen Dong"
      ],
      "abstract": "The rapid advancement of visual generative models necessitates efficient and\nreliable evaluation methods. Arena platform, which gathers user votes on model\ncomparisons, can rank models with human preferences. However, traditional Arena\nmethods, while established, require an excessive number of comparisons for\nranking to converge and are vulnerable to preference noise in voting,\nsuggesting the need for better approaches tailored to contemporary evaluation\nchallenges. In this paper, we introduce K-Sort Arena, an efficient and reliable\nplatform based on a key insight: images and videos possess higher perceptual\nintuitiveness than texts, enabling rapid evaluation of multiple samples\nsimultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing\nK models to engage in free-for-all competitions, which yield much richer\ninformation than pairwise comparisons. To enhance the robustness of the system,\nwe leverage probabilistic modeling and Bayesian updating techniques. We propose\nan exploration-exploitation-based matchmaking strategy to facilitate more\ninformative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster\nconvergence compared to the widely used ELO algorithm. To further validate the\nsuperiority and obtain a comprehensive leaderboard, we collect human feedback\nvia crowdsourced evaluations of numerous cutting-edge text-to-image and\ntext-to-video models. Thanks to its high efficiency, K-Sort Arena can\ncontinuously incorporate emerging models and update the leaderboard with\nminimal votes. Our project has undergone several months of internal testing and\nis now available at https://huggingface.co/spaces/ksort/K-Sort-Arena",
      "tldr_zh": "该论文提出 K-Sort Arena，一种高效可靠的生成模型基准平台，通过 K-wise 比较（允许多个模型同时竞争）利用人类偏好来排名模型，解决了传统 Arena 方法的比较过多和噪音敏感问题。\n该平台利用图像和视频的感知直观性，结合概率建模、Bayesian 更新以及基于探索-利用的匹配策略，提供更丰富的比较信息。\n实验结果显示，K-Sort Arena 比 ELO 算法快 16.3 倍收敛，并通过众包收集反馈，建立了全面的文本到图像和文本到视频模型排行榜。\n得益于其高效性，该平台能持续整合新模型并快速更新排行榜，现已公开可用。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "CVPR 2025. Project page:\n  https://huggingface.co/spaces/ksort/K-Sort-Arena",
      "pdf_url": "http://arxiv.org/pdf/2408.14468v2",
      "published_date": "2024-08-26 17:58:20 UTC",
      "updated_date": "2025-03-15 03:06:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:47:08.916287"
    },
    {
      "arxiv_id": "2408.14443v2",
      "title": "Temporal Ensemble Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Guo-Qiang Zhang"
      ],
      "abstract": "We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal\nlogic for linear-time temporal reasoning. TEL includes primitive temporal\nconstructs such as ``always up to $t$ time later'' ($\\Box_t$), ``sometimes\nbefore $t$ time in the future'' ($\\Diamond_t$), and ``$t$-time later''\n$\\varphi_t$. TEL has been motivated from the requirement for rigor and\nreproducibility for cohort specification and discovery in clinical and\npopulation health research, to fill a gap in formalizing temporal reasoning in\nbiomedicine. Existing logical frameworks such as linear temporal logic are too\nrestrictive to express temporal and sequential properties in biomedicine, or\ntoo permissive in semantic constructs, such as in Halpern-Shoham logic, to\nserve this purpose. In this paper, we first introduce TEL in a general set up,\nwith discrete and dense time as special cases. We then focus on the theoretical\ndevelopment of discrete TEL on the temporal domain of positive integers\n$\\mathbb{N}^+$, denoted as ${\\rm TEL}_{\\mathbb{N}^+}$. ${\\rm\nTEL}_{\\mathbb{N}^+}$ is strictly more expressive than the standard monadic\nsecond order logic, characterized by B\\\"{u}chi automata. We present its formal\nsemantics, a proof system, and provide a proof for the undecidability of the\nsatisfiability of ${\\rm TEL}_{\\mathbb{N}^+}$. We also include initial results\non expressiveness and decidability fragments for ${\\rm TEL}_{\\mathbb{N}^+}$,\nfollowed by application outlook and discussions.",
      "tldr_zh": "本文提出了一种一阶模态逻辑 Temporal Ensemble Logic (TEL)，用于线性时间推理，旨在填补生物医学中时间属性的形式化空白，例如通过引入 □_t（总是直到 t 时间后）、◇_t（有时在 t 时间前）和 φ_t（t 时间后）的原始时间构造。相比 Linear Temporal Logic 的限制性和 Halpern-Shoham logic 的宽松性，TEL 在正整数域（TEL_{N+}）上比标准 monadic second-order logic 更具表达力，并由 Büchi automata 表征。作者提供了形式语义、证明系统，并证明了 TEL_{N+} 的满足性问题为 undecidable，同时探讨了其表达性和可判定片段的应用潜力。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.LO",
      "comment": "47 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.14443v2",
      "published_date": "2024-08-26 17:36:25 UTC",
      "updated_date": "2024-08-30 14:41:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:47:21.430928"
    },
    {
      "arxiv_id": "2408.14441v1",
      "title": "Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification",
      "title_zh": "Attend-Fusion：用于视频分类的高效音频-视觉融合",
      "authors": [
        "Mahrukh Awan",
        "Asmar Nadeem",
        "Muhammad Junaid Awan",
        "Armin Mustafa",
        "Syed Sameed Husain"
      ],
      "abstract": "Exploiting both audio and visual modalities for video classification is a\nchallenging task, as the existing methods require large model architectures,\nleading to high computational complexity and resource requirements. Smaller\narchitectures, on the other hand, struggle to achieve optimal performance. In\nthis paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that\nintroduces a compact model architecture specifically designed to capture\nintricate audio-visual relationships in video data. Through extensive\nexperiments on the challenging YouTube-8M dataset, we demonstrate that\nAttend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which\nis comparable to the performance of larger baseline models such as\nFully-Connected Late Fusion (75.96\\% F1 score, 341M parameters). Attend-Fusion\nachieves similar performance to the larger baseline model while reducing the\nmodel size by nearly 80\\%, highlighting its efficiency in terms of model\ncomplexity. Our work demonstrates that the Attend-Fusion model effectively\ncombines audio and visual information for video classification, achieving\ncompetitive performance with significantly reduced model size. This approach\nopens new possibilities for deploying high-performance video understanding\nsystems in resource-constrained environments across various applications.",
      "tldr_zh": "本研究提出了一种高效的音频-视觉融合方法 Attend-Fusion，用于视频分类任务，该方法采用紧凑的模型架构来捕捉视频数据中的复杂音频-视觉关系，从而解决现有方法的高计算复杂性问题。通过在 YouTube-8M 数据集上的实验，Attend-Fusion 仅使用 72M 参数就实现了 75.64% 的 F1 score，与更大基线模型（如 Fully-Connected Late Fusion 的 75.96% F1 score 和 341M 参数）相当，模型大小减少了近 80%。这一创新显著提升了视频分类的效率，并为资源受限环境下的高性能视频理解系统提供了新可能性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14441v1",
      "published_date": "2024-08-26 17:33:47 UTC",
      "updated_date": "2024-08-26 17:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:47:30.742307"
    },
    {
      "arxiv_id": "2408.14437v1",
      "title": "Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview",
      "title_zh": "稀疏性感知的脉冲神经网络硬件软件协同设计：一个概述",
      "authors": [
        "Ilkin Aliyev",
        "Kama Svoboda",
        "Tosiron Adegbija",
        "Jean-Marc Fellous"
      ],
      "abstract": "Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven\nnature of biological neural processing, and offer the potential for\nultra-low-power artificial intelligence. However, realizing their efficiency\nbenefits requires specialized hardware and a co-design approach that\neffectively leverages sparsity. We explore the hardware-software co-design of\nsparse SNNs, examining how sparsity representation, hardware architectures, and\ntraining techniques influence hardware efficiency. We analyze the impact of\nstatic and dynamic sparsity, discuss the implications of different neuron\nmodels and encoding schemes, and investigate the need for adaptability in\nhardware designs. Our work aims to illuminate the path towards embedded\nneuromorphic systems that fully exploit the computational advantages of sparse\nSNNs.",
      "tldr_zh": "本综述探讨了Spiking Neural Networks (SNNs)的硬件-软件协同设计，聚焦于其稀疏和事件驱动特性，以实现超低功耗人工智能。论文分析了稀疏表示、硬件架构以及训练技术如何影响硬件效率，包括静态和动态稀疏性、不同神经元模型以及编码方案的影响。作者强调了硬件设计的适应性需求，并为开发充分利用稀疏SNNs计算优势的嵌入式神经形态系统提供了指导路径。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14437v1",
      "published_date": "2024-08-26 17:22:11 UTC",
      "updated_date": "2024-08-26 17:22:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:47:43.893018"
    },
    {
      "arxiv_id": "2408.14435v1",
      "title": "Social perception of faces in a vision-language model",
      "title_zh": "视觉语言模型中面部的社会感知",
      "authors": [
        "Carina I. Hausladen",
        "Manuel Knott",
        "Colin F. Camerer",
        "Pietro Perona"
      ],
      "abstract": "We explore social perception of human faces in CLIP, a widely used\nopen-source vision-language model. To this end, we compare the similarity in\nCLIP embeddings between different textual prompts and a set of face images. Our\ntextual prompts are constructed from well-validated social psychology terms\ndenoting social perception. The face images are synthetic and are\nsystematically and independently varied along six dimensions: the legally\nprotected attributes of age, gender, and race, as well as facial expression,\nlighting, and pose. Independently and systematically manipulating face\nattributes allows us to study the effect of each on social perception and\navoids confounds that can occur in wild-collected data due to uncontrolled\nsystematic correlations between attributes. Thus, our findings are experimental\nrather than observational. Our main findings are three. First, while CLIP is\ntrained on the widest variety of images and texts, it is able to make\nfine-grained human-like social judgments on face images. Second, age, gender,\nand race do systematically impact CLIP's social perception of faces, suggesting\nan undesirable bias in CLIP vis-a-vis legally protected attributes. Most\nstrikingly, we find a strong pattern of bias concerning the faces of Black\nwomen, where CLIP produces extreme values of social perception across different\nages and facial expressions. Third, facial expression impacts social perception\nmore than age and lighting as much as age. The last finding predicts that\nstudies that do not control for unprotected visual attributes may reach the\nwrong conclusions on bias. Our novel method of investigation, which is founded\non the social psychology literature and on the experiments involving the\nmanipulation of individual attributes, yields sharper and more reliable\nobservations than previous observational methods and may be applied to study\nbiases in any vision-language model.",
      "tldr_zh": "本研究探讨了视觉语言模型 CLIP 对人脸的社会感知，通过比较文本提示（基于社会心理学术语）和合成人脸图像的相似性，这些图像在年龄、性别、种族、面部表情、照明及姿势等六个维度上独立系统地变化，以避免数据混杂。结果表明，CLIP 能够进行细粒度的、类似人类的社会判断，但存在对年龄、性别和种族的系统偏见，尤其是对黑人女性的极端偏见。面部表情对社会感知的影响大于年龄，照明的影响与年龄相当，这提醒研究者需控制非保护属性以避免偏见结论错误。该方法基于实验操纵，比以往的观察方法更可靠，可应用于其他视觉语言模型的偏见研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14435v1",
      "published_date": "2024-08-26 17:21:54 UTC",
      "updated_date": "2024-08-26 17:21:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:47:56.436134"
    },
    {
      "arxiv_id": "2408.14432v2",
      "title": "Contextual Bandit with Herding Effects: Algorithms and Recommendation Applications",
      "title_zh": "带有羊群效应的上下文多臂老虎机：算法和推荐应用",
      "authors": [
        "Luyue Xu",
        "Liming Wang",
        "Hong Xie",
        "Mingqiang Zhou"
      ],
      "abstract": "Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.",
      "tldr_zh": "该论文探讨了contextual bandits在推荐系统中的应用，指出传统的框架忽略了用户反馈中的herding effects（从众效应），导致反馈偏差并破坏了无偏假设。为解决这一问题，研究者开发了一种新型contextual bandit变体，并设计了TS-Conf（Thompson Sampling under Conformity）算法，该算法通过后验采样平衡探索与利用，并证明了其regret上界，揭示了herding effects对学习速度的影响。实验结果显示，TS-Conf在多个数据集上优于四种基准算法，能够有效缓解反馈偏差，提高推荐准确性和学习效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at PRICAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14432v2",
      "published_date": "2024-08-26 17:20:34 UTC",
      "updated_date": "2024-08-28 12:39:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:48:06.842521"
    },
    {
      "arxiv_id": "2408.14419v2",
      "title": "CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shubham Bharti",
        "Shiyun Cheng",
        "Jihyun Rho",
        "Jianrui Zhang",
        "Mu Cai",
        "Yong Jae Lee",
        "Martina Rau",
        "Xiaojin Zhu"
      ],
      "abstract": "We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance. We benchmark leading LLMs as of late 2024 -\nincluding GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset\nand found that our benchmark was challenging to all of them, suggesting room\nfor future large language models to improve.",
      "tldr_zh": "这篇论文引入了 CHARTOM，这是一个针对多模态大型语言模型的视觉 Theory-of-Mind 基准，用于评估模型理解图表和判断其潜在误导性的能力。CHARTOM 由专门设计的可视化图表数据组成，包括 FACT question（正确理解图表）和 MIND question（判断图表是否会误导人类读者），这些问题具有重要的社会益处。研究详细描述了基准的构建过程，并基于人类性能进行校准；实验测试了 2024 年末的领先 LLM（如 GPT、Claude、Gemini、Qwen、Llama 和 Llava），结果显示所有模型在 CHARTOM 上表现较差，表明未来模型有显著改进空间。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14419v2",
      "published_date": "2024-08-26 17:04:23 UTC",
      "updated_date": "2025-05-09 19:55:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:48:20.380840"
    },
    {
      "arxiv_id": "2408.14418v3",
      "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues",
      "title_zh": "翻译失败",
      "authors": [
        "Kuluhan Binici",
        "Abhinav Ramesh Kashyap",
        "Viktor Schlegel",
        "Andy T. Liu",
        "Vijay Prakash Dwivedi",
        "Thanh-Tung Nguyen",
        "Xiaoxue Gao",
        "Nancy F. Chen",
        "Stefan Winkler"
      ],
      "abstract": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
      "tldr_zh": "本文提出 MEDSAGE 方法，利用大型语言模型 (LLMs) 生成合成对话样本来模拟 ASR 错误，从而增强医疗对话摘要的鲁棒性。针对临床对话摘要领域的数据稀缺问题，该方法借助 LLMs 的 in-context learning 能力，从少量可用示例中创建 ASR 风格的噪声数据，并将其融入训练过程。实验结果显示，这种数据增强策略显著提高了摘要系统的准确性和可靠性，为处理 ASR 输出噪声在关键医疗应用中的挑战提供了有效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by the Thirty-Ninth AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2408.14418v3",
      "published_date": "2024-08-26 17:04:00 UTC",
      "updated_date": "2025-01-08 07:23:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:48:32.551084"
    },
    {
      "arxiv_id": "2408.14398v3",
      "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Kurz",
        "Jian-Jia Chen",
        "Lucie Flek",
        "Zhixue Zhao"
      ],
      "abstract": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art (SotA) compression results in post-training and\nretraining-free settings while maintaining high predictive performance.\nHowever, previous research mainly considered calibrating based on English text,\ndespite the multilingual nature of modern LLMs and their frequent use in\nnon-English languages. In this paper, we set out to investigate calibrating the\npruning of multilingual language models for monolingual applications. We\npresent the first comprehensive empirical study, comparing different\ncalibration languages for pruning multilingual models across diverse languages,\ntasks, models, and SotA pruning techniques. Our results offer practical\nsuggestions, for example, calibrating in the target language can efficiently\nretain the language modeling capability but does not necessarily benefit\ndownstream tasks. Through further analysis of latent subspaces, pruning masks,\nand individual neurons within pruned models, we find that while pruning\ngenerally preserves strong language-specific features, it may fail to retain\nlanguage-specific neuron activation patterns and subtle, language-agnostic\nfeatures associated with knowledge and reasoning that are needed for complex\ntasks.",
      "tldr_zh": "本研究调查了针对多语言大型语言模型(LLM)的剪枝过程，强调语言特定校准的重要性，因为现有状态-of-the-art (SotA) 剪枝方法主要基于英语文本。研究者进行了首个全面实证研究，比较了不同校准语言在各种语言、任务、模型和SotA剪枝技术上的效果。结果显示，使用目标语言进行校准能有效保留语言建模能力，但不一定提升下游任务性能；进一步分析潜空间、剪枝掩码和单个神经元表明，剪枝虽保留了强烈的语言特定特征，却可能丢失语言特定的神经元激活模式和与知识及推理相关的细微语言无关特征。该研究为多语言模型的优化提供了实用建议，如针对单语言应用需谨慎选择校准策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14398v3",
      "published_date": "2024-08-26 16:29:13 UTC",
      "updated_date": "2024-10-30 00:53:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:48:43.288871"
    },
    {
      "arxiv_id": "2408.14397v1",
      "title": "Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs",
      "title_zh": "通过知识图谱揭示放射学报告生成模型中的知识缺口",
      "authors": [
        "Xiaoman Zhang",
        "Julián N. Acosta",
        "Hong-Yu Zhou",
        "Pranav Rajpurkar"
      ],
      "abstract": "Recent advancements in artificial intelligence have significantly improved\nthe automatic generation of radiology reports. However, existing evaluation\nmethods fail to reveal the models' understanding of radiological images and\ntheir capacity to achieve human-level granularity in descriptions. To bridge\nthis gap, we introduce a system, named ReXKG, which extracts structured\ninformation from processed reports to construct a comprehensive radiology\nknowledge graph. We then propose three metrics to evaluate the similarity of\nnodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs\n(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative\nanalysis of AI-generated and human-written radiology reports, assessing the\nperformance of both specialist and generalist models. Our study provides a\ndeeper understanding of the capabilities and limitations of current AI models\nin radiology report generation, offering valuable insights for improving model\nperformance and clinical applicability.",
      "tldr_zh": "该研究揭示了放射学报告生成模型的知识缺口，通过构建放射学知识图来评估模型对图像理解和描述精细度的不足。研究引入了ReXKG系统，从报告中提取结构化信息构建知识图，并提出三个评估指标：ReXKG-NSC（节点相似性）、ReXKG-AMS（边分布）和ReXKG-SCS（子图覆盖）。通过比较AI生成报告与人类撰写报告，分析了专家模型和通用模型的表现，揭示了当前AI模型的局限性，并为提升模型性能和临床适用性提供了宝贵见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Code is available at: https://github.com/rajpurkarlab/ReXKG",
      "pdf_url": "http://arxiv.org/pdf/2408.14397v1",
      "published_date": "2024-08-26 16:28:56 UTC",
      "updated_date": "2024-08-26 16:28:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:48:58.072163"
    },
    {
      "arxiv_id": "2408.14387v1",
      "title": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sakhinana Sagar Srinivas",
        "Chidaksh Ravuru",
        "Geethan Sannidhi",
        "Venkataramana Runkana"
      ],
      "abstract": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy.",
      "tldr_zh": "该研究针对时空预测(Spatio-Temporal Forecasting)应用中处理大规模复杂数据集的挑战，提出了一种混合方法，将开源大语言模型(LLMs)和小语言模型(LMs)与传统预测技术结合，通过动态提示和grouped-query multi-head attention机制来捕获时间序列的内部和外部依赖关系。方法还利用Low-Rank Adaptation with Activation Memory Reduction (LoRA-AMR)技术在消费级硬件上微调小模型，实现本地化定制并降低计算开销，同时实现跨模态时间序列表示学习。实验结果显示，该框架在真实世界数据集上显著优于现有方法，在预测准确性方面取得了明显提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper published at the Deployable AI (DAI) workshop at AAAI-2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14387v1",
      "published_date": "2024-08-26 16:11:53 UTC",
      "updated_date": "2024-08-26 16:11:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:49:10.487066"
    },
    {
      "arxiv_id": "2409.06720v1",
      "title": "Evolutionary Game Dynamics Applied to Strategic Adoption of Immersive Technologies in Cultural Heritage and Tourism",
      "title_zh": "翻译失败",
      "authors": [
        "Gioacchino Fazio",
        "Stefano Fricano",
        "Claudio Pirrone"
      ],
      "abstract": "Immersive technologies such as Metaverse, AR, and VR are at a crossroads,\nwith many actors pondering their adoption and potential sectors interested in\nintegration. The cultural and tourism industries are particularly impacted,\nfacing significant pressure to make decisions that could shape their future\nlandscapes. Stakeholders' perceptions play a crucial role in this process,\ninfluencing the speed and extent of technology adoption. As immersive\ntechnologies promise to revolutionize experiences, stakeholders in these fields\nweigh the benefits and challenges of embracing such innovations. The current\nchoices will likely determine the trajectory of cultural preservation and\ntourism enhancement, potentially transforming how we engage with history, art,\nand travel. Starting from a decomposition of stakeholders' perceptions into\nprincipal components using Q-methodology, this article employs an evolutionary\ngame model to attempt to map possible scenarios and highlight potential\ndecision-making trajectories. The proposed approach highlights how evolutionary\ndynamics lead to identifying a dominant long-term strategy that emerges from\nthe complex system of coexistence among various stakeholders.",
      "tldr_zh": "这篇论文探讨了沉浸式技术（如 Metaverse、AR 和 VR）在文化遗产和旅游行业的战略采用问题，强调利益相关者的感知如何影响决策轨迹。研究首先采用 Q-methodology 分解这些感知，然后通过 evolutionary game model 映射可能的场景和决策路径。结果显示，evolutionary dynamics 导致复杂利益相关者系统中出现主导的长期策略，从而为技术整合提供指导。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.TH"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.06720v1",
      "published_date": "2024-08-26 16:02:34 UTC",
      "updated_date": "2024-08-26 16:02:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:49:23.311711"
    },
    {
      "arxiv_id": "2408.14380v1",
      "title": "Probing Causality Manipulation of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyang Zhang",
        "Haibo Tong",
        "Bin Zhang",
        "Dongyu Zhang"
      ],
      "abstract": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）对因果关系的操纵能力，提出了一种分层探测方法，通过提供不同捷径来观察模型行为。方法利用检索增强生成（RAG）和上下文学习（ICL）在设计的因果分类任务上进行实验，涉及GPT-4等主流模型。结果显示，LLMs能够检测因果相关实体并识别直接因果关系，但缺乏专门的因果认知，仅将其视为句子整体语义的一部分。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14380v1",
      "published_date": "2024-08-26 16:00:41 UTC",
      "updated_date": "2024-08-26 16:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:49:33.590077"
    },
    {
      "arxiv_id": "2408.14371v2",
      "title": "SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery",
      "title_zh": "SelEx：细粒度广义类别发现中的自我专长",
      "authors": [
        "Sarah Rastegar",
        "Mohammadreza Salehi",
        "Yuki M. Asano",
        "Hazel Doughty",
        "Cees G. M. Snoek"
      ],
      "abstract": "In this paper, we address Generalized Category Discovery, aiming to\nsimultaneously uncover novel categories and accurately classify known ones.\nTraditional methods, which lean heavily on self-supervision and contrastive\nlearning, often fall short when distinguishing between fine-grained categories.\nTo address this, we introduce a novel concept called `self-expertise', which\nenhances the model's ability to recognize subtle differences and uncover\nunknown categories. Our approach combines unsupervised and supervised\nself-expertise strategies to refine the model's discernment and generalization.\nInitially, hierarchical pseudo-labeling is used to provide `soft supervision',\nimproving the effectiveness of self-expertise. Our supervised technique differs\nfrom traditional methods by utilizing more abstract positive and negative\nsamples, aiding in the formation of clusters that can generalize to novel\ncategories. Meanwhile, our unsupervised strategy encourages the model to\nsharpen its category distinctions by considering within-category examples as\n`hard' negatives. Supported by theoretical insights, our empirical results\nshowcase that our method outperforms existing state-of-the-art techniques in\nGeneralized Category Discovery across several fine-grained datasets. Our code\nis available at: https://github.com/SarahRastegar/SelEx.",
      "tldr_zh": "本论文针对细粒度 Generalized Category Discovery 的问题，提出了一种名为 SelEx 的方法，旨在同时发现新类别并准确分类已知类别，同时解决传统自监督和对比学习在区分细粒度类别时的不足。SelEx 引入了 self-expertise 概念，通过结合监督和无监督策略（如使用抽象正负样本形成泛化聚类，以及将类别内样本视为“hard” negatives 来增强区分），并采用分层伪标签提供“soft supervision”来提升模型的辨识能力。实验结果显示，该方法在多个细粒度数据集上超越了现有最先进技术，证明了其在泛化类别发现中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14371v2",
      "published_date": "2024-08-26 15:53:50 UTC",
      "updated_date": "2024-11-10 21:45:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:49:45.795945"
    },
    {
      "arxiv_id": "2408.14368v2",
      "title": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal-Conditioned Policy",
      "title_zh": "翻译失败",
      "authors": [
        "Peiyan Li",
        "Hongtao Wu",
        "Yan Huang",
        "Chilam Cheang",
        "Liang Wang",
        "Tao Kong"
      ],
      "abstract": "The robotics community has consistently aimed to achieve generalizable robot\nmanipulation with flexible natural language instructions. One primary challenge\nis that obtaining robot trajectories fully annotated with both actions and\ntexts is time-consuming and labor-intensive. However, partially-annotated data,\nsuch as human activity videos without action labels and robot trajectories\nwithout text labels, are much easier to collect. Can we leverage these data to\nenhance the generalization capabilities of robots? In this paper, we propose\nGR-MG, a novel method which supports conditioning on a text instruction and a\ngoal image. During training, GR-MG samples goal images from trajectories and\nconditions on both the text and the goal image or solely on the image when text\nis not available. During inference, where only the text is provided, GR-MG\ngenerates the goal image via a diffusion-based image-editing model and\nconditions on both the text and the generated image. This approach enables\nGR-MG to leverage large amounts of partially-annotated data while still using\nlanguages to flexibly specify tasks. To generate accurate goal images, we\npropose a novel progress-guided goal image generation model which injects task\nprogress information into the generation process. In simulation experiments,\nGR-MG improves the average number of tasks completed in a row of 5 from 3.35 to\n4.04. In real-robot experiments, GR-MG is able to perform 58 different tasks\nand improves the success rate from 68.7\\% to 78.1\\% and 44.4\\% to 60.6\\% in\nsimple and generalization settings, respectively. It also outperforms comparing\nbaseline methods in few-shot learning of novel skills. Video demos, code, and\ncheckpoints are available on the project page: https://gr-mg.github.io/.",
      "tldr_zh": "这篇论文提出 GR-MG，一种多模态目标条件策略（Multi-Modal Goal-Conditioned Policy），旨在利用部分标注数据（如无动作标签的视频或无文本标签的机器人轨迹）来提升机器人在自然语言指令下的泛化能力。GR-MG 在训练时结合文本指令和目标图像，或仅使用图像进行条件；推理时，通过 diffusion-based image-editing 模型生成目标图像，并注入进度引导机制以确保图像准确性。实验结果显示，该方法在模拟环境中将连续完成5个任务的平均数从3.35提高到4.04，在真实机器人实验中成功率从68.7%提升到78.1%，并在少样本学习中优于基线模型。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 5 figures, RA-L",
      "pdf_url": "http://arxiv.org/pdf/2408.14368v2",
      "published_date": "2024-08-26 15:46:41 UTC",
      "updated_date": "2024-12-23 14:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:49:59.588903"
    },
    {
      "arxiv_id": "2408.14354v1",
      "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java",
      "title_zh": "翻译失败",
      "authors": [
        "Daoguang Zan",
        "Zhirong Huang",
        "Ailun Yu",
        "Shaoxin Lin",
        "Yifan Shi",
        "Wei Liu",
        "Dong Chen",
        "Zongshuai Qi",
        "Hao Yu",
        "Lei Yu",
        "Dezhi Ran",
        "Muhan Zeng",
        "Bo Shen",
        "Pan Bian",
        "Guangtai Liang",
        "Bei Guan",
        "Pengjie Huang",
        "Tao Xie",
        "Yongji Wang",
        "Qianxiang Wang"
      ],
      "abstract": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.",
      "tldr_zh": "该论文介绍了 SWE-bench-java，这是一个针对 Java 的 GitHub issue resolving 基准，用于评估大型语言模型 (LLMs) 在解决软件问题方面的能力，以扩展原有 SWE-bench 的多语言支持。研究团队公开了数据集、基于 Docker 的评估环境和 leaderboard，并通过测试 SWE-agent 和多个强大 LLMs 验证了基准的可靠性。SWE-bench-java 的开发填补了行业对 Java 支持的需求，并欢迎社区通过拉取请求或合作来加速其迭代和完善，推动全自动编程的进步。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "This work is in progress",
      "pdf_url": "http://arxiv.org/pdf/2408.14354v1",
      "published_date": "2024-08-26 15:30:05 UTC",
      "updated_date": "2024-08-26 15:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:50:11.023050"
    },
    {
      "arxiv_id": "2408.14352v1",
      "title": "Assessing Contamination in Large Language Models: Introducing the LogProber method",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolas Yax",
        "Pierre-Yves Oudeyer",
        "Stefano Palminteri"
      ],
      "abstract": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.",
      "tldr_zh": "该论文探讨了 Large Language Models (LLMs) 中的 contamination 问题，即测试数据泄露到训练集，导致模型性能评估不公的问题。作者引入了 LogProber，一种新颖高效的算法，通过分析 token probability 在短序列文本（如心理学问卷）中检测 contamination。实验结果显示该方法有效，但也揭示了其局限性，即某些训练方法可能在不影响 token probabilities 的情况下隐蔽地造成 contamination。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14352v1",
      "published_date": "2024-08-26 15:29:34 UTC",
      "updated_date": "2024-08-26 15:29:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:50:22.166810"
    },
    {
      "arxiv_id": "2408.14527v1",
      "title": "Multi-Agent Path Finding with Real Robot Dynamics and Interdependent Tasks for Automated Warehouses",
      "title_zh": "多智能体路径寻找：结合真实机器人动力学和相互依赖任务，用于自动化仓库",
      "authors": [
        "Vassilissa Lehoux-Lebacque",
        "Tomi Silander",
        "Christelle Loiodice",
        "Seungjoon Lee",
        "Albert Wang",
        "Sofia Michel"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) is an important optimization problem\nunderlying the deployment of robots in automated warehouses and factories.\nDespite the large body of work on this topic, most approaches make heavy\nsimplifications, both on the environment and the agents, which make the\nresulting algorithms impractical for real-life scenarios. In this paper, we\nconsider a realistic problem of online order delivery in a warehouse, where a\nfleet of robots bring the products belonging to each order from shelves to\nworkstations. This creates a stream of inter-dependent pickup and delivery\ntasks and the associated MAPF problem consists of computing realistic\ncollision-free robot trajectories fulfilling these tasks. To solve this MAPF\nproblem, we propose an extension of the standard Prioritized Planning algorithm\nto deal with the inter-dependent tasks (Interleaved Prioritized Planning) and a\nnovel Via-Point Star (VP*) algorithm to compute an optimal dynamics-compliant\nrobot trajectory to visit a sequence of goal locations while avoiding moving\nobstacles. We prove the completeness of our approach and evaluate it in\nsimulation as well as in a real warehouse.",
      "tldr_zh": "这篇论文针对自动化仓库的多智能体路径寻找(MAPF)问题，考虑了真实机器人动态和相互依赖的任务，指出现有方法过于简化而无法应用于实际场景。\n作者提出Interleaved Prioritized Planning算法来处理在线订单交付中的相互依赖的拾取和交付任务，以及Via-Point Star (VP*)算法来计算避开动态障碍的最佳机器人轨迹。\n实验结果显示，该方法在模拟和真实仓库环境中表现出色，证明了其完整性和实用性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ECAI-2024. For related videos, see\n  https://europe.naverlabs.com/research/publications/MAPF_IPP",
      "pdf_url": "http://arxiv.org/pdf/2408.14527v1",
      "published_date": "2024-08-26 15:13:38 UTC",
      "updated_date": "2024-08-26 15:13:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:50:34.253415"
    },
    {
      "arxiv_id": "2408.14340v3",
      "title": "Foundation Models for Music: A Survey",
      "title_zh": "音乐基础模型：综述",
      "authors": [
        "Yinghao Ma",
        "Anders Øland",
        "Anton Ragni",
        "Bleiz MacSen Del Sette",
        "Charalampos Saitis",
        "Chris Donahue",
        "Chenghua Lin",
        "Christos Plachouras",
        "Emmanouil Benetos",
        "Elona Shatri",
        "Fabio Morreale",
        "Ge Zhang",
        "György Fazekas",
        "Gus Xia",
        "Huan Zhang",
        "Ilaria Manco",
        "Jiawen Huang",
        "Julien Guinot",
        "Liwei Lin",
        "Luca Marinelli",
        "Max W. Y. Lam",
        "Megha Sharma",
        "Qiuqiang Kong",
        "Roger B. Dannenberg",
        "Ruibin Yuan",
        "Shangda Wu",
        "Shih-Lun Wu",
        "Shuqi Dai",
        "Shun Lei",
        "Shiyin Kang",
        "Simon Dixon",
        "Wenhu Chen",
        "Wenhao Huang",
        "Xingjian Du",
        "Xingwei Qu",
        "Xu Tan",
        "Yizhi Li",
        "Zeyue Tian",
        "Zhiyong Wu",
        "Zhizheng Wu",
        "Ziyang Ma",
        "Ziyu Wang"
      ],
      "abstract": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
      "tldr_zh": "这篇论文对音乐领域的Foundation Models (FMs)进行了全面调查，涵盖了Large Language Models (LLMs)和Latent Diffusion Models (LDMs)等预训练模型在表示学习、生成学习和多模态学习中的应用。作者分析了音乐在各行业的意义、AI在音乐领域的演变，以及现有方法的局限性，如音乐表示的低探索度和模型在多样应用中的缺乏多功能性，并强调了FMs在音乐理解、生成和医疗应用中的潜力。论文详细探讨了模型预训练范式、架构选择、标记化、微调方法和可控性等关键主题，同时审视了音乐代理、数据集、评估标准以及伦理问题，如可解释性、透明度和版权挑战，并为未来的人机协作在音乐领域指出了挑战和趋势。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14340v3",
      "published_date": "2024-08-26 15:13:14 UTC",
      "updated_date": "2024-09-03 14:53:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:50:47.024599"
    },
    {
      "arxiv_id": "2408.14338v1",
      "title": "Machine Learning for Quantifier Selection in cvc5",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Jakubův",
        "Mikoláš Janota",
        "Jelle Piepenbrock",
        "Josef Urban"
      ],
      "abstract": "In this work we considerably improve the state-of-the-art SMT solving on\nfirst-order quantified problems by efficient machine learning guidance of\nquantifier selection. Quantifiers represent a significant challenge for SMT and\nare technically a source of undecidability. In our approach, we train an\nefficient machine learning model that informs the solver which quantifiers\nshould be instantiated and which not. Each quantifier may be instantiated\nmultiple times and the set of the active quantifiers changes as the solving\nprogresses. Therefore, we invoke the ML predictor many times, during the whole\nrun of the solver. To make this efficient, we use fast ML models based on\ngradient boosting decision trees. We integrate our approach into the\nstate-of-the-art cvc5 SMT solver and show a considerable increase of the\nsystem's holdout-set performance after training it on a large set of\nfirst-order problems collected from the Mizar Mathematical Library.",
      "tldr_zh": "本研究提出了一种使用机器学习来指导量化器选择的创新方法，旨在显著提升 SMT 求解器 cvc5 在处理一阶量化问题时的性能，因为量化器往往导致 undecidability 挑战。方法涉及训练一个基于梯度提升决策 trees 的高效 ML 模型，该模型在求解过程中多次调用，以动态决定哪些量化器应被实例化。实验结果显示，通过在从 Mizar Mathematical Library 收集的大规模一阶问题数据集上训练，cvc5 的 holdout-set 性能得到大幅提升。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14338v1",
      "published_date": "2024-08-26 15:07:35 UTC",
      "updated_date": "2024-08-26 15:07:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:50:57.663196"
    },
    {
      "arxiv_id": "2408.14336v1",
      "title": "Equivariant Reinforcement Learning under Partial Observability",
      "title_zh": "部分可观察性下的等变强化学习",
      "authors": [
        "Hai Nguyen",
        "Andrea Baisero",
        "David Klee",
        "Dian Wang",
        "Robert Platt",
        "Christopher Amato"
      ],
      "abstract": "Incorporating inductive biases is a promising approach for tackling\nchallenging robot learning domains with sample-efficient solutions. This paper\nidentifies partially observable domains where symmetries can be a useful\ninductive bias for efficient learning. Specifically, by encoding the\nequivariance regarding specific group symmetries into the neural networks, our\nactor-critic reinforcement learning agents can reuse solutions in the past for\nrelated scenarios. Consequently, our equivariant agents outperform\nnon-equivariant approaches significantly in terms of sample efficiency and\nfinal performance, demonstrated through experiments on a range of robotic tasks\nin simulation and real hardware.",
      "tldr_zh": "该论文探讨了在部分可观察（partial observability）环境中，使用对称性作为归纳偏差来提升强化学习（reinforcement learning）的样本效率。通过将等变性（equivariance）编码到神经网络中，actor-critic 代理能够重用过去的解决方案，从而在相关场景中实现更高效的学习。实验结果显示，该方法在模拟和真实硬件的机器人任务中，显著优于非等变（non-equivariant）方法，在样本效率和最终性能上取得了明显改善。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Conference on Robot Learning, 2023",
      "pdf_url": "http://arxiv.org/pdf/2408.14336v1",
      "published_date": "2024-08-26 15:07:01 UTC",
      "updated_date": "2024-08-26 15:07:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:51:12.116748"
    },
    {
      "arxiv_id": "2409.04452v1",
      "title": "Process Trace Querying using Knowledge Graphs and Notation3",
      "title_zh": "翻译失败",
      "authors": [
        "William Van Woensel"
      ],
      "abstract": "In process mining, a log exploration step allows making sense of the event\ntraces; e.g., identifying event patterns and illogical traces, and gaining\ninsight into their variability. To support expressive log exploration, the\nevent log can be converted into a Knowledge Graph (KG), which can then be\nqueried using general-purpose languages. We explore the creation of semantic KG\nusing the Resource Description Framework (RDF) as a data model, combined with\nthe general-purpose Notation3 (N3) rule language for querying. We show how\ntypical trace querying constraints, inspired by the state of the art, can be\nimplemented in N3. We convert case- and object-centric event logs into a\ntrace-based semantic KG; OCEL2 logs are hereby \"flattened\" into traces based on\nobject paths through the KG. This solution offers (a) expressivity, as queries\ncan instantiate constraints in multiple ways and arbitrarily constrain\nattributes and relations (e.g., actors, resources); (b) flexibility, as OCEL2\nevent logs can be serialized as traces in arbitrary ways based on the KG; and\n(c) extensibility, as others can extend our library by leveraging the same\nimplementation patterns.",
      "tldr_zh": "该论文探讨了在过程挖掘中，使用 Knowledge Graphs (KG) 和 Notation3 (N3) 规则语言进行事件日志的查询和探索，以识别事件模式和异常跟踪。研究方法包括将事件日志转换为基于 Resource Description Framework (RDF) 的语义 KG，并针对 case- 和 object-centric 日志（如 OCEL2）进行“扁平化”处理，使其基于对象路径生成跟踪。相比传统方法，该方案提升了查询的表达性（expressivity）、灵活性（flexibility）和可扩展性（extensibility），允许用户灵活约束属性和关系。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.04452v1",
      "published_date": "2024-08-26 14:55:55 UTC",
      "updated_date": "2024-08-26 14:55:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:51:22.429880"
    },
    {
      "arxiv_id": "2408.14329v2",
      "title": "Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Armin Danesh Pazho",
        "Shanle Yao",
        "Ghazal Alinezhad Noghre",
        "Babak Rahimi Ardabili",
        "Vinit Katariya",
        "Hamed Tabkhi"
      ],
      "abstract": "Human-centric Video Anomaly Detection (VAD) aims to identify human behaviors\nthat deviate from normal. At its core, human-centric VAD faces substantial\nchallenges, such as the complexity of diverse human behaviors, the rarity of\nanomalies, and ethical constraints. These challenges limit access to\nhigh-quality datasets and highlight the need for a dataset and framework\nsupporting continual learning. Moving towards adaptive human-centric VAD, we\nintroduce the HuVAD (Human-centric privacy-enhanced Video Anomaly Detection)\ndataset and a novel Unsupervised Continual Anomaly Learning (UCAL) framework.\nUCAL enables incremental learning, allowing models to adapt over time, bridging\ntraditional training and real-world deployment. HuVAD prioritizes privacy by\nproviding de-identified annotations and includes seven indoor/outdoor scenes,\noffering over 5x more pose-annotated frames than previous datasets. Our\nstandard and continual benchmarks, utilize a comprehensive set of metrics,\ndemonstrating that UCAL-enhanced models achieve superior performance in 82.14%\nof cases, setting a new state-of-the-art (SOTA). The dataset can be accessed at\nhttps://github.com/TeCSAR-UNCC/HuVAD.",
      "tldr_zh": "这篇论文针对 Human-centric Video Anomaly Detection 的挑战，提出了一种全面框架 UCAL（Unsupervised Continual Anomaly Learning），支持无监督的增量学习，使模型能够随时间适应并桥接传统训练与实际部署。研究者还引入了 HuVAD 数据集，该数据集强调隐私保护，提供去标识化的标注，涵盖七个室内/室外场景，并包含比现有数据集多 5 倍的姿态标注帧。实验结果显示，UCAL 增强的模型在标准和持续基准测试中，在 82.14% 的情况下优于基线模型，设定了新的 SOTA（State-of-the-Art）性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14329v2",
      "published_date": "2024-08-26 14:55:23 UTC",
      "updated_date": "2025-03-19 18:13:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:51:36.019307"
    },
    {
      "arxiv_id": "2408.14326v1",
      "title": "Streamline tractography of the fetal brain in utero with machine learning",
      "title_zh": "翻译失败",
      "authors": [
        "Weide Liu",
        "Camilo Calixto",
        "Simon K. Warfield",
        "Davood Karimi"
      ],
      "abstract": "Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive\ntool for studying white matter tracts and structural connectivity of the brain.\nThese assessments rely heavily on tractography techniques, which reconstruct\nvirtual streamlines representing white matter fibers. Much effort has been\ndevoted to improving tractography methodology for adult brains, while\ntractography of the fetal brain has been largely neglected. Fetal tractography\nfaces unique difficulties due to low dMRI signal quality, immature and rapidly\ndeveloping brain structures, and paucity of reference data. This work presents\nthe first machine learning model for fetal tractography. The model input\nconsists of five sources of information: (1) Fiber orientation, inferred from a\ndiffusion tensor fit to the dMRI signal; (2) Directions of recent propagation\nsteps; (3) Global spatial information, encoded as distances to keypoints in the\nbrain cortex; (4) Tissue segmentation information; and (5) Prior information\nabout the expected local fiber orientations supplied with an atlas. In order to\nmitigate the local tensor estimation error, a large spatial context around the\ncurrent point in the diffusion tensor image is encoded using convolutional and\nattention neural network modules. Moreover, the diffusion tensor information at\na hypothetical next point is included in the model input. Filtering rules based\non anatomically constrained tractography are applied to prune implausible\nstreamlines. We trained the model on manually-refined whole-brain fetal\ntractograms and validated the trained model on an independent set of 11 test\nscans with gestational ages between 23 and 36 weeks. Results show that our\nproposed method achieves superior performance across all evaluated tracts. The\nnew method can significantly advance the capabilities of dMRI for studying\nnormal and abnormal brain development in utero.",
      "tldr_zh": "本文首次提出一种机器学习模型，用于子宫内胎儿大脑的轨迹追踪（tractography），以解决 dMRI 信号质量低、脑结构不成熟和参考数据稀少等挑战。模型整合了五种输入来源，包括从扩散张量推断的纤维方向、传播步骤方向、全球空间信息、组织分割信息以及图谱提供的先验纤维方向，并通过卷积和注意力模块处理大空间上下文，同时应用解剖约束过滤规则修剪不合理流线。在 11 个独立测试扫描（孕周 23-36 周）上验证，该方法在所有评估轨迹上表现出色，显著提升 dMRI 在研究胎儿正常和异常脑发育方面的能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14326v1",
      "published_date": "2024-08-26 14:54:14 UTC",
      "updated_date": "2024-08-26 14:54:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:51:50.768842"
    },
    {
      "arxiv_id": "2408.14317v2",
      "title": "Claim Verification in the Age of Large Language Models: A Survey",
      "title_zh": "大语言模型时代下的声明验证：一项调查",
      "authors": [
        "Alphaeus Dmonte",
        "Roland Oruche",
        "Marcos Zampieri",
        "Prasad Calyam",
        "Isabelle Augenstein"
      ],
      "abstract": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.",
      "tldr_zh": "这篇调查论文探讨了在大型语言模型（LLMs）时代下声明验证（claim verification）的最新进展，回顾了从深度学习模型到LLMs的演变，以及Retrieval Augmented Generation (RAG)等新型方法的兴起。论文详细描述了声明验证框架的关键组件，包括检索、提示和微调策略，以提升自动化验证系统的性能。最终，它总结了可用的公共英语数据集，并强调LLMs在处理海量互联网数据时的潜力，为未来研究提供了全面参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14317v2",
      "published_date": "2024-08-26 14:45:03 UTC",
      "updated_date": "2025-02-11 14:51:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:51:57.909720"
    },
    {
      "arxiv_id": "2408.14314v1",
      "title": "Logic interpretations of ANN partition cells",
      "title_zh": "ANN 分区单元的逻辑解释",
      "authors": [
        "Ingo Schmitt"
      ],
      "abstract": "Consider a binary classification problem solved using a feed-forward\nartificial neural network (ANN). Let the ANN be composed of a ReLU layer and\nseveral linear layers (convolution, sum-pooling, or fully connected). We assume\nthe network was trained with high accuracy. Despite numerous suggested\napproaches, interpreting an artificial neural network remains challenging for\nhumans. For a new method of interpretation, we construct a bridge between a\nsimple ANN and logic. As a result, we can analyze and manipulate the semantics\nof an ANN using the powerful tool set of logic. To achieve this, we decompose\nthe input space of the ANN into several network partition cells. Each network\npartition cell represents a linear combination that maps input values to a\nclassifying output value. For interpreting the linear map of a partition cell\nusing logic expressions, we suggest minterm values as the input of a simple\nANN. We derive logic expressions representing interaction patterns for\nseparating objects classified as 1 from those classified as 0. To facilitate an\ninterpretation of logic expressions, we present them as binary logic trees.",
      "tldr_zh": "这篇论文针对高精度训练的前馈人工神经网络 (ANN) 在二元分类问题中的解释挑战，提出了一种将 ANN 与逻辑相结合的方法。论文通过将 ANN 的输入空间分解成多个网络分区单元 (network partition cells)，每个单元表示一个线性组合，将输入映射到分类输出。作者建议使用 minterm values 作为简单 ANN 的输入，推导出逻辑表达式来表示分类为 1 和 0 的对象交互模式，并以二元逻辑树 (binary logic trees) 形式呈现这些表达式。该方法为利用逻辑工具分析和操作 ANN 的语义提供了桥梁，从而提升了 ANN 的可解释性。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "I.2.4; I.2.6; F.4.1"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14314v1",
      "published_date": "2024-08-26 14:43:43 UTC",
      "updated_date": "2024-08-26 14:43:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:52:13.557922"
    },
    {
      "arxiv_id": "2408.14307v2",
      "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing",
      "title_zh": "翻译失败",
      "authors": [
        "Yayati Jadhav",
        "Peter Pak",
        "Amir Barati Farimani"
      ],
      "abstract": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.",
      "tldr_zh": "本文提出一个基于 Large Language Models (LLMs) 的框架，用于监控和控制 Fused Deposition Modeling (FDM) 3D 打印过程，以解决传统方法在缺陷检测和泛化性上的局限性。该框架让 LLMs 分析每层打印图像，识别常见故障如 inconsistent extrusion、stringing、warping 和 layer adhesion，并查询打印机参数后自动生成并执行纠正计划。实验结果显示，LLMs 代理在准确性上与专家相当，能自主修复错误，无需人工干预，从而提升了 3D 打印的可扩展性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14307v2",
      "published_date": "2024-08-26 14:38:19 UTC",
      "updated_date": "2025-05-05 20:53:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:52:23.222827"
    },
    {
      "arxiv_id": "2408.14284v1",
      "title": "May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Monica Millunzi",
        "Lorenzo Bonicelli",
        "Angelo Porrello",
        "Jacopo Credi",
        "Petter N. Kolm",
        "Simone Calderara"
      ],
      "abstract": "Forgetting presents a significant challenge during incremental training,\nmaking it particularly demanding for contemporary AI systems to assimilate new\nknowledge in streaming data environments. To address this issue, most\napproaches in Continual Learning (CL) rely on the replay of a restricted buffer\nof past data. However, the presence of noise in real-world scenarios, where\nhuman annotation is constrained by time limitations or where data is\nautomatically gathered from the web, frequently renders these strategies\nvulnerable. In this study, we address the problem of CL under Noisy Labels\n(CLN) by introducing Alternate Experience Replay (AER), which takes advantage\nof forgetting to maintain a clear distinction between clean, complex, and noisy\nsamples in the memory buffer. The idea is that complex or mislabeled examples,\nwhich hardly fit the previously learned data distribution, are most likely to\nbe forgotten. To grasp the benefits of such a separation, we equip AER with\nAsymmetric Balanced Sampling (ABS): a new sample selection strategy that\nprioritizes purity on the current task while retaining relevant samples from\nthe past. Through extensive computational comparisons, we demonstrate the\neffectiveness of our approach in terms of both accuracy and purity of the\nobtained buffer, resulting in a remarkable average gain of 4.71% points in\naccuracy with respect to existing loss-based purification strategies. Code is\navailable at https://github.com/aimagelab/mammoth.",
      "tldr_zh": "该研究针对持续学习（Continual Learning, CL）中噪声标签（Noisy Labels）问题，提出 Alternate Experience Replay (AER) 方法，利用遗忘机制来区分内存缓冲区中的干净样本、复杂样本和噪声样本，从而缓解增量训练中的遗忘挑战。AER 结合 Asymmetric Balanced Sampling (ABS) 策略，通过优先选择当前任务的纯样本并保留过去的相关样本，实现更有效的样本选择和缓冲区管理。实验结果表明，该方法在准确性和缓冲区纯度上优于现有基于损失的净化策略，平均准确率提升 4.71%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 5 figures. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK",
      "pdf_url": "http://arxiv.org/pdf/2408.14284v1",
      "published_date": "2024-08-26 14:09:40 UTC",
      "updated_date": "2024-08-26 14:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:52:34.969972"
    },
    {
      "arxiv_id": "2409.00094v1",
      "title": "Examining Independence in Ensemble Sentiment Analysis: A Study on the Limits of Large Language Models Using the Condorcet Jury Theorem",
      "title_zh": "翻译失败",
      "authors": [
        "Baptiste Lefort",
        "Eric Benhamou",
        "Jean-Jacques Ohana",
        "Beatrice Guez",
        "David Saltiel",
        "Thomas Jacquot"
      ],
      "abstract": "This paper explores the application of the Condorcet Jury theorem to the\ndomain of sentiment analysis, specifically examining the performance of various\nlarge language models (LLMs) compared to simpler natural language processing\n(NLP) models. The theorem posits that a majority vote classifier should enhance\npredictive accuracy, provided that individual classifiers' decisions are\nindependent. Our empirical study tests this theoretical framework by\nimplementing a majority vote mechanism across different models, including\nadvanced LLMs such as ChatGPT 4. Contrary to expectations, the results reveal\nonly marginal improvements in performance when incorporating larger models,\nsuggesting a lack of independence among them. This finding aligns with the\nhypothesis that despite their complexity, LLMs do not significantly outperform\nsimpler models in reasoning tasks within sentiment analysis, showing the\npractical limits of model independence in the context of advanced NLP tasks.",
      "tldr_zh": "这篇论文使用 Condorcet Jury Theorem 考察了情感分析中模型集成的独立性，比较了大型语言模型 (LLMs) 如 ChatGPT 4 与简单自然语言处理 (NLP) 模型的表现。研究通过实施多数投票机制进行实证测试，旨在验证独立决策是否能提升预测准确性。结果显示，加入更大模型仅带来 marginal improvements，表明 LLMs 之间缺乏独立性，并揭示了它们在情感分析推理任务中不显著优于简单模型的实际限制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00094v1",
      "published_date": "2024-08-26 14:04:00 UTC",
      "updated_date": "2024-08-26 14:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:52:47.397753"
    },
    {
      "arxiv_id": "2408.14281v1",
      "title": "Uncertainties of Latent Representations in Computer Vision",
      "title_zh": "计算机视觉中潜在表示的不确定性",
      "authors": [
        "Michael Kirchhof"
      ],
      "abstract": "Uncertainty quantification is a key pillar of trustworthy machine learning.\nIt enables safe reactions under unsafe inputs, like predicting only when the\nmachine learning model detects sufficient evidence, discarding anomalous data,\nor emitting warnings when an error is likely to be inbound. This is\nparticularly crucial in safety-critical areas like medical image classification\nor self-driving cars. Despite the plethora of proposed uncertainty\nquantification methods achieving increasingly higher scores on performance\nbenchmarks, uncertainty estimates are often shied away from in practice. Many\nmachine learning projects start from pretrained latent representations that\ncome without uncertainty estimates. Uncertainties would need to be trained by\npractitioners on their own, which is notoriously difficult and\nresource-intense.\n  This thesis makes uncertainty estimates easily accessible by adding them to\nthe latent representation vectors of pretrained computer vision models. Besides\nproposing approaches rooted in probability and decision theory, such as\nMonte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both\ntheoretical and empirical questions. We show that these unobservable\nuncertainties about unobservable latent representations are indeed provably\ncorrect. We also provide an uncertainty-aware representation learning (URL)\nbenchmark to compare these unobservables against observable ground-truths.\nFinally, we compile our findings to pretrain lightweight representation\nuncertainties on large-scale computer vision models that transfer to unseen\ndatasets in a zero-shot manner.\n  Our findings do not only advance the current theoretical understanding of\nuncertainties over latent variables, but also facilitate the access to\nuncertainty quantification for future researchers inside and outside the field,\nenabling straightforward but trustworthy machine learning.",
      "tldr_zh": "这篇论文探讨了计算机视觉中潜在表示（latent representations）的 uncertainty quantification（不确定性量化），旨在解决机器学习模型在安全关键领域（如医疗图像分类或自动驾驶）中缺乏不确定性估计的问题。作者提出基于概率和决策理论的方法，包括 Monte-Carlo InfoNCE (MCInfoNCE) 和损失预测技术，将不确定性直接添加到预训练模型的潜在向量中，并通过理论证明和经验验证其正确性。同时，他们开发了 uncertainty-aware representation learning (URL) 基准，并在大型计算机视觉模型上预训练轻量级不确定性，实现零样本转移到新数据集。总体而言，这项工作提升了对潜在变量不确定性的理论理解，并简化了不确定性量化的应用，促进更可靠的机器学习实践。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Doctoral thesis",
      "pdf_url": "http://arxiv.org/pdf/2408.14281v1",
      "published_date": "2024-08-26 14:02:30 UTC",
      "updated_date": "2024-08-26 14:02:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:52:59.077035"
    },
    {
      "arxiv_id": "2408.14525v1",
      "title": "Estimating Uncertainty with Implicit Quantile Network",
      "title_zh": "使用隐式分位",
      "authors": [
        "Yi Hung Lim"
      ],
      "abstract": "Uncertainty quantification is an important part of many performance critical\napplications. This paper provides a simple alternative to existing approaches\nsuch as ensemble learning and bayesian neural networks. By directly modeling\nthe loss distribution with an Implicit Quantile Network, we get an estimate of\nhow uncertain the model is of its predictions. For experiments with MNIST and\nCIFAR datasets, the mean of the estimated loss distribution is 2x higher for\nincorrect predictions. When data with high estimated uncertainty is removed\nfrom the test dataset, the accuracy of the model goes up as much as 10%. This\nmethod is simple to implement while offering important information to\napplications where the user has to know when the model could be wrong (e.g.\ndeep learning for healthcare).",
      "tldr_zh": "本论文提出了一种简单的方法，使用 Implicit Quantile Network 直接建模损失分布，以估计模型预测的不确定性，作为 ensemble learning 和 bayesian neural networks 等现有方法的替代方案。该方法通过分析损失分布的均值，发现错误预测的不确定性是正确预测的 2 倍；在 MNIST 和 CIFAR 数据集实验中，移除高不确定性数据后，模型准确率可提高多达 10%。这种易于实施的方案适用于性能关键应用，如医疗保健领域，帮助用户识别模型可能出错的风险。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "This method is simple to implement and offers important information\n  for performance critical applications",
      "pdf_url": "http://arxiv.org/pdf/2408.14525v1",
      "published_date": "2024-08-26 13:33:14 UTC",
      "updated_date": "2024-08-26 13:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:53:10.925873"
    },
    {
      "arxiv_id": "2408.14253v2",
      "title": "Text3DAug -- Prompted Instance Augmentation for LiDAR Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Laurenz Reichardt",
        "Luca Uhr",
        "Oliver Wasenmüller"
      ],
      "abstract": "LiDAR data of urban scenarios poses unique challenges, such as heterogeneous\ncharacteristics and inherent class imbalance. Therefore, large-scale datasets\nare necessary to apply deep learning methods. Instance augmentation has emerged\nas an efficient method to increase dataset diversity. However, current methods\nrequire the time-consuming curation of 3D models or costly manual data\nannotation. To overcome these limitations, we propose Text3DAug, a novel\napproach leveraging generative models for instance augmentation. Text3DAug does\nnot depend on labeled data and is the first of its kind to generate instances\nand annotations from text. This allows for a fully automated pipeline,\neliminating the need for manual effort in practical applications. Additionally,\nText3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor\nused. Comprehensive experimental analysis on LiDAR segmentation, detection and\nnovel class discovery demonstrates that Text3DAug is effective in supplementing\nexisting methods or as a standalone method, performing on par or better than\nestablished methods, however while overcoming their specific drawbacks. The\ncode is publicly available.",
      "tldr_zh": "该研究针对 LiDAR 数据在城市场景中的异质性和类别不平衡问题，提出了一种名为 Text3DAug 的实例增强方法，利用生成模型从文本提示生成 3D 实例和注释，从而避免了传统方法的耗时 3D 模型制作或手动标注需求。Text3DAug 不依赖于标记数据，实现全自动化管道，并具有传感器无关性，可应用于各种 LiDAR 传感器。实验结果显示，在 LiDAR segmentation、detection 和 novel class discovery 任务上，Text3DAug 作为补充或独立方法，其性能与现有方法相当或更优，同时克服了它们的特定缺点；代码已公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.14253v2",
      "published_date": "2024-08-26 13:16:03 UTC",
      "updated_date": "2024-08-27 10:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:53:23.209814"
    },
    {
      "arxiv_id": "2408.14249v1",
      "title": "Beyond Few-shot Object Detection: A Detailed Survey",
      "title_zh": "超越少样本物体检测：一个详细综述",
      "authors": [
        "Vishal Chudasama",
        "Hiran Sarkar",
        "Pankaj Wasnik",
        "Vineeth N Balasubramanian",
        "Jayateja Kalla"
      ],
      "abstract": "Object detection is a critical field in computer vision focusing on\naccurately identifying and locating specific objects in images or videos.\nTraditional methods for object detection rely on large labeled training\ndatasets for each object category, which can be time-consuming and expensive to\ncollect and annotate. To address this issue, researchers have introduced\nfew-shot object detection (FSOD) approaches that merge few-shot learning and\nobject detection principles. These approaches allow models to quickly adapt to\nnew object categories with only a few annotated samples. While traditional FSOD\nmethods have been studied before, this survey paper comprehensively reviews\nFSOD research with a specific focus on covering different FSOD settings such as\nstandard FSOD, generalized FSOD, incremental FSOD, open-set FSOD, and domain\nadaptive FSOD. These approaches play a vital role in reducing the reliance on\nextensive labeled datasets, particularly as the need for efficient machine\nlearning models continues to rise. This survey paper aims to provide a\ncomprehensive understanding of the above-mentioned few-shot settings and\nexplore the methodologies for each FSOD task. It thoroughly compares\nstate-of-the-art methods across different FSOD settings, analyzing them in\ndetail based on their evaluation protocols. Additionally, it offers insights\ninto their applications, challenges, and potential future directions in the\nevolving field of object detection with limited data.",
      "tldr_zh": "这篇调查论文探讨了物体检测领域的 few-shot object detection (FSOD)，旨在解决传统方法对大量标注数据集的依赖问题。论文全面回顾了多种 FSOD 设置，包括 standard FSOD、generalized FSOD、incremental FSOD、open-set FSOD 和 domain adaptive FSOD，这些方法允许模型通过少量样本快速适应新物体类别。作者比较了最先进的技术，详细分析了它们的评估协议，并提供了关于应用挑战、潜在问题及未来方向的见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10; I.4.8; I.5"
      ],
      "primary_category": "cs.CV",
      "comment": "43 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.14249v1",
      "published_date": "2024-08-26 13:09:23 UTC",
      "updated_date": "2024-08-26 13:09:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:53:35.327858"
    },
    {
      "arxiv_id": "2408.14240v2",
      "title": "Celtibero: Robust Layered Aggregation for Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Borja Molina-Coronado"
      ],
      "abstract": "Federated Learning (FL) is an innovative approach to distributed machine\nlearning. While FL offers significant privacy advantages, it also faces\nsecurity challenges, particularly from poisoning attacks where adversaries\ndeliberately manipulate local model updates to degrade model performance or\nintroduce hidden backdoors. Existing defenses against these attacks have been\nshown to be effective when the data on the nodes is identically and\nindependently distributed (i.i.d.), but they often fail under less restrictive,\nnon-i.i.d data conditions. To overcome these limitations, we introduce\nCeltibero, a novel defense mechanism that integrates layered aggregation to\nenhance robustness against adversarial manipulation. Through extensive\nexperiments on the MNIST and IMDB datasets, we demonstrate that Celtibero\nconsistently achieves high main task accuracy (MTA) while maintaining minimal\nattack success rates (ASR) across a range of untargeted and targeted poisoning\nattacks. Our results highlight the superiority of Celtibero over existing\ndefenses such as FL-Defender, LFighter, and FLAME, establishing it as a highly\neffective solution for securing federated learning systems against\nsophisticated poisoning attacks.",
      "tldr_zh": "该研究针对Federated Learning (FL)中的中毒攻击问题，提出了一种新型防御机制Celtibero，该机制通过分层聚合(layered aggregation)技术增强模型对攻击的鲁棒性，尤其适用于非i.i.d.数据条件下的场景。Celtibero在MNIST和IMDB数据集上的广泛实验中，实现了高主任务准确率(MTA)并将攻击成功率(ASR)保持在最低水平，抵御了多种无针对性和有针对性的中毒攻击。相比现有防御如FL-Defender、LFighter和FLAME，Celtibero展示了显著的优越性，为构建安全的FL系统提供了有效解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14240v2",
      "published_date": "2024-08-26 12:54:00 UTC",
      "updated_date": "2024-09-20 11:24:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:53:47.979294"
    },
    {
      "arxiv_id": "2408.14236v1",
      "title": "DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type classification",
      "title_zh": "翻译失败",
      "authors": [
        "Hanna Abi Akl"
      ],
      "abstract": "We introduce semantic towers, an extrinsic knowledge representation method,\nand compare it to intrinsic knowledge in large language models for ontology\nlearning. Our experiments show a trade-off between performance and semantic\ngrounding for extrinsic knowledge compared to a fine-tuned model intrinsic\nknowledge. We report our findings on the Large Language Models for Ontology\nLearning (LLMs4OL) 2024 challenge.",
      "tldr_zh": "该论文介绍了 semantic towers，一种 extrinsic knowledge 表示方法，并将其与 large language models 中的 intrinsic knowledge 在 ontology learning 领域进行比较。研究通过实验发现，extrinsic knowledge 在性能和 semantic grounding 之间存在权衡，与 fine-tuned model 的 intrinsic knowledge 相比表现出一定的优势和局限性。最终，论文报告了这些发现应用于 Large Language Models for Ontology Learning (LLMs4OL) 2024 challenge 的结果，为类型分类任务提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 4 figures, accepted for the LLMs4OL challenge at the\n  International Semantic Web Conference (ISWC) 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14236v1",
      "published_date": "2024-08-26 12:50:27 UTC",
      "updated_date": "2024-08-26 12:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:54:09.140954"
    },
    {
      "arxiv_id": "2408.14229v1",
      "title": "Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Leonid Erlygin",
        "Alexey Zaytsev"
      ],
      "abstract": "Accurately estimating image quality and model robustness improvement are\ncritical challenges in unconstrained face recognition, which can be addressed\nthrough uncertainty estimation via probabilistic face embeddings. Previous\nresearch mainly focused on uncertainty estimation in face verification, leaving\nthe open-set face recognition task underexplored. In open-set face recognition,\none seeks to classify an image, which could also be unknown. Here, the low\nvariance of probabilistic embedding does not imply a low error probability: an\nimage embedding could be close to several classes in a gallery, thus yielding\nhigh uncertainty. We propose a method aware of two sources of ambiguity in the\nopen-set recognition system: (1) the gallery uncertainty caused by overlapping\nclasses and (2) the uncertainty of the face embeddings. To detect both types,\nwe use a Bayesian probabilistic model of embedding distribution, which provides\na principled uncertainty estimate. Challenging open-set face recognition\ndatasets, such as IJB-C, serve as a testbed for our method. We also propose a\nnew open-set recognition protocol for whale and dolphin identification. The\nproposed approach better identifies recognition errors than uncertainty\nestimation methods based solely on image quality.",
      "tldr_zh": "本研究针对开放集人脸识别（Open-Set Face Recognition）中的不确定性估计问题，提出了一种考虑画廊不确定性（Gallery Uncertainty，由类重叠引起）和人脸嵌入不确定性（Uncertainty of the Face Embeddings）的创新方法。利用贝叶斯概率模型（Bayesian Probabilistic Model）对嵌入分布进行建模，以提供更精确的不确定性评估。该方法在 IJB-C 等挑战性数据集上进行测试，并引入了一个新的协议用于鲸鱼和海豚识别实验，结果显示其比仅基于图像质量的不确定性估计方法更有效地识别识别错误。总的来说，该方法提升了开放集人脸识别的鲁棒性和准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14229v1",
      "published_date": "2024-08-26 12:44:17 UTC",
      "updated_date": "2024-08-26 12:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:54:11.338744"
    },
    {
      "arxiv_id": "2408.14224v1",
      "title": "Fact Probability Vector Based Goal Recognition",
      "title_zh": "基于事实概率向量的目标识别",
      "authors": [
        "Nils Wilken",
        "Lea Cohausz",
        "Christian Bartelt",
        "Heiner Stuckenschmidt"
      ],
      "abstract": "We present a new approach to goal recognition that involves comparing\nobserved facts with their expected probabilities. These probabilities depend on\na specified goal g and initial state s0. Our method maps these probabilities\nand observed facts into a real vector space to compute heuristic values for\npotential goals. These values estimate the likelihood of a given goal being the\ntrue objective of the observed agent. As obtaining exact expected probabilities\nfor observed facts in an observation sequence is often practically infeasible,\nwe propose and empirically validate a method for approximating these\nprobabilities. Our empirical results show that the proposed approach offers\nimproved goal recognition precision compared to state-of-the-art techniques\nwhile reducing computational complexity.",
      "tldr_zh": "这篇论文提出了一种基于事实概率向量（fact probability vector）的目标识别方法，通过比较观察到的事实与其在指定目标（goal）和初始状态下的预期概率，来计算潜在目标的启发式值（heuristic values），从而估计目标的真实可能性。方法将这些概率和观察事实映射到实向量空间中，并引入一种近似计算概率的技术，以解决精确计算的实际挑战。实验结果显示，该方法相较于现有最先进技术，提高了目标识别精度，同时降低了计算复杂度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Will be presented at ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14224v1",
      "published_date": "2024-08-26 12:37:41 UTC",
      "updated_date": "2024-08-26 12:37:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:54:23.074881"
    },
    {
      "arxiv_id": "2408.14211v1",
      "title": "MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement",
      "title_zh": "MagicMan: 利用3D感知扩散和迭代优化的生成式人类新视图合成",
      "authors": [
        "Xu He",
        "Xiaoyu Li",
        "Di Kang",
        "Jiangnan Ye",
        "Chaopeng Zhang",
        "Liyang Chen",
        "Xiangjun Gao",
        "Han Zhang",
        "Zhiyong Wu",
        "Haolin Zhuang"
      ],
      "abstract": "Existing works in single-image human reconstruction suffer from weak\ngeneralizability due to insufficient training data or 3D inconsistencies for a\nlack of comprehensive multi-view knowledge. In this paper, we introduce\nMagicMan, a human-specific multi-view diffusion model designed to generate\nhigh-quality novel view images from a single reference image. As its core, we\nleverage a pre-trained 2D diffusion model as the generative prior for\ngeneralizability, with the parametric SMPL-X model as the 3D body prior to\npromote 3D awareness. To tackle the critical challenge of maintaining\nconsistency while achieving dense multi-view generation for improved 3D human\nreconstruction, we first introduce hybrid multi-view attention to facilitate\nboth efficient and thorough information interchange across different views.\nAdditionally, we present a geometry-aware dual branch to perform concurrent\ngeneration in both RGB and normal domains, further enhancing consistency via\ngeometry cues. Last but not least, to address ill-shaped issues arising from\ninaccurate SMPL-X estimation that conflicts with the reference image, we\npropose a novel iterative refinement strategy, which progressively optimizes\nSMPL-X accuracy while enhancing the quality and consistency of the generated\nmulti-views. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing approaches in both novel view synthesis and\nsubsequent 3D human reconstruction tasks.",
      "tldr_zh": "该研究提出MagicMan，一种基于3D感知扩散模型的框架，用于从单张参考图像生成高质量的人体新视图图像，以解决现有单图像人体重建中泛化性弱和3D一致性不足的问题。核心方法包括利用预训练的2D扩散模型作为生成先验、SMPL-X模型作为3D人体先验、混合多视图注意力（hybrid multi-view attention）促进视图间信息交换，以及几何感知双分支（geometry-aware dual branch）在RGB和法线域同时生成图像以提升一致性。针对SMPL-X估计 inaccuracies，该框架引入迭代优化策略（iterative refinement strategy），逐步改进模型准确性并优化生成图像质量。实验结果显示，MagicMan在新型视图合成和3D人体重建任务中显著优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://thuhcsi.github.io/MagicMan",
      "pdf_url": "http://arxiv.org/pdf/2408.14211v1",
      "published_date": "2024-08-26 12:10:52 UTC",
      "updated_date": "2024-08-26 12:10:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:54:36.076023"
    },
    {
      "arxiv_id": "2409.00092v3",
      "title": "Large Language Model for Patent Concept Generation",
      "title_zh": "大型语言模型用于专利概念生成",
      "authors": [
        "Runtao Ren",
        "Jian Ma",
        "Jianxi Luo"
      ],
      "abstract": "In traditional innovation practices, concept and IP generation are often\niteratively integrated. Both processes demand an intricate understanding of\nadvanced technical domain knowledge. Existing large language models (LLMs),\nwhile possessing massive pre-trained knowledge, often fall short in the\ninnovative concept generation due to a lack of specialized knowledge necessary\nfor the generation. To bridge this critical gap, we propose a novel knowledge\nfinetuning (KFT) framework to endow LLM-based AI with the ability to\nautonomously mine, understand, and apply domain-specific knowledge and concepts\nfor invention generation, i.e., concept and patent generation together. Our\nproposed PatentGPT integrates knowledge injection pre-training (KPT),\ndomain-specific supervised finetuning (SFT), and reinforcement learning from\nhuman feedback (RLHF). Extensive evaluation shows that PatentGPT significantly\noutperforms the state-of-the-art models on patent-related benchmark tests. Our\nmethod not only provides new insights into data-driven innovation but also\npaves a new path to fine-tune LLMs for applications in the context of\ntechnology. We also discuss the managerial and policy implications of\nAI-generating inventions in the future.",
      "tldr_zh": "本研究针对现有大型语言模型（LLMs）在创新概念生成中的不足（如缺乏专业领域知识），提出了一种新型知识微调（KFT）框架，以赋予LLMs自主挖掘、理解和应用领域特定知识的能力，用于概念和专利生成。框架的核心模型PatentGPT整合了知识注入预训练（KPT）、领域特定监督微调（SFT）以及强化学习从人类反馈（RLHF），从而提升专利相关任务的性能。在基准测试中，PatentGPT显著优于最先进模型，提供数据驱动创新的新见解，并讨论AI生成发明在管理和政策层面的潜在影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in Advanced Engineering Informatics, Link:\n  https://doi.org/10.1016/j.aei.2025.103301",
      "pdf_url": "http://arxiv.org/pdf/2409.00092v3",
      "published_date": "2024-08-26 12:00:29 UTC",
      "updated_date": "2025-04-08 05:07:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:54:47.230954"
    },
    {
      "arxiv_id": "2408.14195v1",
      "title": "Representative Arm Identification: A fixed confidence approach to identify cluster representatives",
      "title_zh": "代表臂识别：固定置信度方法用于识别集群代表",
      "authors": [
        "Sarvesh Gharat",
        "Aniket Yadav",
        "Nikhil Karamchandani",
        "Jayakrishnan Nair"
      ],
      "abstract": "We study the representative arm identification (RAI) problem in the\nmulti-armed bandits (MAB) framework, wherein we have a collection of arms, each\nassociated with an unknown reward distribution. An underlying instance is\ndefined by a partitioning of the arms into clusters of predefined sizes, such\nthat for any $j > i$, all arms in cluster $i$ have a larger mean reward than\nthose in cluster $j$. The goal in RAI is to reliably identify a certain\nprespecified number of arms from each cluster, while using as few arm pulls as\npossible. The RAI problem covers as special cases several well-studied MAB\nproblems such as identifying the best arm or any $M$ out of the top $K$, as\nwell as both full and coarse ranking. We start by providing an\ninstance-dependent lower bound on the sample complexity of any feasible\nalgorithm for this setting. We then propose two algorithms, based on the idea\nof confidence intervals, and provide high probability upper bounds on their\nsample complexity, which orderwise match the lower bound. Finally, we do an\nempirical comparison of both algorithms along with an LUCB-type alternative on\nboth synthetic and real-world datasets, and demonstrate the superior\nperformance of our proposed schemes in most cases.",
      "tldr_zh": "本文研究了多臂老虎机 (MAB) 框架中的代表臂识别 (RAI) 问题，目标是从预定义簇中可靠地识别指定数量的代表臂，同时最小化臂拉次数。作者首先给出了样本复杂度的实例相关下界，并提出了两个基于置信区间的算法，其样本复杂度上界在数量级上与下界匹配。实验结果显示，这些算法在合成和真实数据集上比LUCB-type备选方案表现出色，验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.PR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "We analyse a clustered multi-armed bandit formulation, where the\n  learning objective is to identify representative arms from each cluster, in a\n  fixed confidence setting",
      "pdf_url": "http://arxiv.org/pdf/2408.14195v1",
      "published_date": "2024-08-26 11:47:52 UTC",
      "updated_date": "2024-08-26 11:47:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:55:00.605595"
    },
    {
      "arxiv_id": "2408.14185v1",
      "title": "DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models",
      "title_zh": "DynamicRouteGPT：基于大型语言模型的实时多车辆动态导航框架",
      "authors": [
        "Ziai Zhou",
        "Bin Zhou",
        "Hao Liu"
      ],
      "abstract": "Real-time dynamic path planning in complex traffic environments presents\nchallenges, such as varying traffic volumes and signal wait times. Traditional\nstatic routing algorithms like Dijkstra and A* compute shortest paths but often\nfail under dynamic conditions. Recent Reinforcement Learning (RL) approaches\noffer improvements but tend to focus on local optima, risking dead-ends or\nboundary issues. This paper proposes a novel approach based on causal inference\nfor real-time dynamic path planning, balancing global and local optimality. We\nfirst use the static Dijkstra algorithm to compute a globally optimal baseline\npath. A distributed control strategy then guides vehicles along this path. At\nintersections, DynamicRouteGPT performs real-time decision-making for local\npath selection, considering real-time traffic, driving preferences, and\nunexpected events. DynamicRouteGPT integrates Markov chains, Bayesian\ninference, and large-scale pretrained language models like Llama3 8B to provide\nan efficient path planning solution. It dynamically adjusts to traffic\nscenarios and driver preferences and requires no pre-training, offering broad\napplicability across road networks. A key innovation is the construction of\ncausal graphs for counterfactual reasoning, optimizing path decisions.\nExperimental results show that our method achieves state-of-the-art performance\nin real-time dynamic path planning for multiple vehicles while providing\nexplainable path selections, offering a novel and efficient solution for\ncomplex traffic environments.",
      "tldr_zh": "这篇论文提出了DynamicRouteGPT，一种基于大型语言模型的实时多车辆动态导航框架，旨在解决复杂交通环境中传统算法如Dijkstra和Reinforcement Learning (RL) 方法的局限性，例如局部最优和动态适应问题。框架首先使用Dijkstra算法计算全局最优基线路径，然后通过分布式控制和DynamicRouteGPT在交叉路口进行实时决策，整合Markov chains、Bayesian inference以及Llama3 8B模型来考虑实时交通、驾驶偏好和意外事件。关键创新在于构建因果 graphs 用于反事实推理，实现全局与局部最优的平衡，并提供可解释的路径选择。实验结果显示，该方法在多车辆实时动态路径规划中比现有方法性能更优，具有广泛适用性和无需预训练的优势。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper is 12 pages long and represents the initial draft, version\n  1",
      "pdf_url": "http://arxiv.org/pdf/2408.14185v1",
      "published_date": "2024-08-26 11:19:58 UTC",
      "updated_date": "2024-08-26 11:19:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:55:16.374074"
    },
    {
      "arxiv_id": "2408.14183v1",
      "title": "Robot Navigation with Entity-Based Collision Avoidance using Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yury Kolomeytsev",
        "Dmitry Golembiovsky"
      ],
      "abstract": "Efficient navigation in dynamic environments is crucial for autonomous robots\ninteracting with various environmental entities, including both moving agents\nand static obstacles. In this study, we present a novel methodology that\nenhances the robot's interaction with different types of agents and obstacles\nbased on specific safety requirements. This approach uses information about the\nentity types, improving collision avoidance and ensuring safer navigation. We\nintroduce a new reward function that penalizes the robot for collisions with\ndifferent entities such as adults, bicyclists, children, and static obstacles,\nand additionally encourages the robot's proximity to the goal. It also\npenalizes the robot for being close to entities, and the safe distance also\ndepends on the entity type. Additionally, we propose an optimized algorithm for\ntraining and testing, which significantly accelerates train, validation, and\ntest steps and enables training in complex environments. Comprehensive\nexperiments conducted using simulation demonstrate that our approach\nconsistently outperforms conventional navigation and collision avoidance\nmethods, including state-of-the-art techniques. To sum up, this work\ncontributes to enhancing the safety and efficiency of navigation systems for\nautonomous robots in dynamic, crowded environments.",
      "tldr_zh": "本研究提出了一种基于实体类型的碰撞避免方法，使用 Deep Reinforcement Learning 提升机器人动态环境导航的安全性。该方法引入了一个新 reward function，根据实体类型（如成人、自行车手、儿童和静态障碍）惩罚碰撞和接近行为，同时鼓励机器人接近目标，并优化了训练算法以加速过程和适应复杂环境。通过模拟实验，该方法在碰撞避免和导航效率上显著优于传统和最先进技术，为机器人安全交互提供了重要贡献。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.14183v1",
      "published_date": "2024-08-26 11:16:03 UTC",
      "updated_date": "2024-08-26 11:16:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:55:24.714087"
    },
    {
      "arxiv_id": "2408.14180v2",
      "title": "I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Yiwei Ma",
        "Jiayi Ji",
        "Ke Ye",
        "Weihuang Lin",
        "Zhibin Wang",
        "Yonghan Zheng",
        "Qiang Zhou",
        "Xiaoshuai Sun",
        "Rongrong Ji"
      ],
      "abstract": "Significant progress has been made in the field of Instruction-based Image\nEditing (IIE). However, evaluating these models poses a significant challenge.\nA crucial requirement in this field is the establishment of a comprehensive\nevaluation benchmark for accurately assessing editing results and providing\nvaluable insights for its further development. In response to this need, we\npropose I2EBench, a comprehensive benchmark designed to automatically evaluate\nthe quality of edited images produced by IIE models from multiple dimensions.\nI2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding\noriginal and diverse instructions. It offers three distinctive characteristics:\n1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation\ndimensions that cover both high-level and low-level aspects, providing a\ncomprehensive assessment of each IIE model. 2) Human Perception Alignment: To\nensure the alignment of our benchmark with human perception, we conducted an\nextensive user study for each evaluation dimension. 3) Valuable Research\nInsights: By analyzing the advantages and disadvantages of existing IIE models\nacross the 16 dimensions, we offer valuable research insights to guide future\ndevelopment in the field. We will open-source I2EBench, including all\ninstructions, input images, human annotations, edited images from all evaluated\nmethods, and a simple script for evaluating the results from new IIE models.\nThe code, dataset and generated images from all IIE models are provided in\ngithub: https://github.com/cocoshe/I2EBench.",
      "tldr_zh": "这篇论文提出了I2EBench，一个全面的基准，用于评估Instruction-based Image Editing (IIE)模型的编辑图像质量，以应对该领域的评估挑战。I2EBench包含超过2000张图像和4000条多样化指令，并涵盖16个评价维度，包括高级和低级方面，以提供多维度的全面评估。同时，通过用户研究确保基准与人类感知对齐，并通过分析现有IIE模型的优缺点，生成有价值的研发洞见。该基准将开源，包括指令、图像、人类标注和评估脚本，以促进进一步发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS2024, 15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.14180v2",
      "published_date": "2024-08-26 11:08:44 UTC",
      "updated_date": "2024-09-27 13:12:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:55:37.155904"
    },
    {
      "arxiv_id": "2408.14176v2",
      "title": "SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher",
      "title_zh": "翻译失败",
      "authors": [
        "Trung Dao",
        "Thuan Hoang Nguyen",
        "Thanh Le",
        "Duc Vu",
        "Khoi Nguyen",
        "Cuong Pham",
        "Anh Tran"
      ],
      "abstract": "In this paper, we aim to enhance the performance of SwiftBrush, a prominent\none-step text-to-image diffusion model, to be competitive with its multi-step\nStable Diffusion counterpart. Initially, we explore the quality-diversity\ntrade-off between SwiftBrush and SD Turbo: the former excels in image\ndiversity, while the latter excels in image quality. This observation motivates\nour proposed modifications in the training methodology, including better weight\ninitialization and efficient LoRA training. Moreover, our introduction of a\nnovel clamped CLIP loss enhances image-text alignment and results in improved\nimage quality. Remarkably, by combining the weights of models trained with\nefficient LoRA and full training, we achieve a new state-of-the-art one-step\ndiffusion model, achieving an FID of 8.14 and surpassing all GAN-based and\nmulti-step Stable Diffusion models. The project page is available at\nhttps://swiftbrushv2.github.io.",
      "tldr_zh": "这篇论文旨在提升一步式文本到图像扩散模型 SwiftBrush 的性能，使其超越多步 Stable Diffusion 模型，通过分析 SwiftBrush 在图像多样性上的优势与 SD Turbo 在图像质量上的优势来优化训练方法。作者引入了改进的权重初始化、有效的 LoRA 训练，以及新型 clamped CLIP loss，以改善图像-文本对齐和整体图像质量。最终，通过结合高效 LoRA 训练和全训练的权重，SwiftBrush v2 实现了 FID 为 8.14 的新最先进性能，超过了所有 GAN-based 和多步 Stable Diffusion 模型，为高效生成图像提供了重要进展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCV'24",
      "pdf_url": "http://arxiv.org/pdf/2408.14176v2",
      "published_date": "2024-08-26 10:42:53 UTC",
      "updated_date": "2024-08-27 04:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:55:50.372826"
    },
    {
      "arxiv_id": "2408.14169v1",
      "title": "Dynamic Pricing for Electric Vehicle Charging",
      "title_zh": "电动汽车充电的动态定价",
      "authors": [
        "Arun Kumar Kalakanti",
        "Shrisha Rao"
      ],
      "abstract": "Dynamic pricing is a promising strategy to address the challenges of smart\ncharging, as traditional time-of-use (ToU) rates and stationary pricing (SP) do\nnot dynamically react to changes in operating conditions, reducing revenue for\ncharging station (CS) vendors and affecting grid stability. Previous studies\nevaluated single objectives or linear combinations of objectives for EV CS\npricing solutions, simplifying trade-offs and preferences among objectives. We\ndevelop a novel formulation for the dynamic pricing problem by addressing\nmultiple conflicting objectives efficiently instead of solely focusing on one\nobjective or metric, as in earlier works. We find optimal trade-offs or Pareto\nsolutions efficiently using Non-dominated Sorting Genetic Algorithms (NSGA) II\nand NSGA III. A dynamic pricing model quantifies the relationship between\ndemand and price while simultaneously solving multiple conflicting objectives,\nsuch as revenue, quality of service (QoS), and peak-to-average ratios (PAR). A\nsingle method can only address some of the above aspects of dynamic pricing\ncomprehensively. We present a three-part dynamic pricing approach using a\nBayesian model, multi-objective optimization, and multi-criteria\ndecision-making (MCDM) using pseudo-weight vectors. To address the research gap\nin CS pricing, our method selects solutions using revenue, QoS, and PAR metrics\nsimultaneously. Two California charging sites' real-world data validates our\napproach.",
      "tldr_zh": "本文提出了一种新型动态定价策略，用于电动汽车充电站（EV CS），以应对传统时间使用率（ToU）和固定定价（SP）无法动态适应操作条件变化的问题，从而提升收入、电网稳定性和服务质量。方法包括使用 Non-dominated Sorting Genetic Algorithms (NSGA) II 和 NSGA III 优化多个冲突目标，如收入、QoS 和 PAR，同时结合 Bayesian 模型和多标准决策（MCDM）来量化需求与价格关系并选择最优解决方案。该策略通过加利福尼亚两个充电站的真实数据验证，实现了目标之间的有效权衡，显著提高了整体性能。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.14169v1",
      "published_date": "2024-08-26 10:32:21 UTC",
      "updated_date": "2024-08-26 10:32:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:56:01.156135"
    },
    {
      "arxiv_id": "2408.14158v2",
      "title": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Wei An",
        "Xiao Bi",
        "Guanting Chen",
        "Shanhuang Chen",
        "Chengqi Deng",
        "Honghui Ding",
        "Kai Dong",
        "Qiushi Du",
        "Wenjun Gao",
        "Kang Guan",
        "Jianzhong Guo",
        "Yongqiang Guo",
        "Zhe Fu",
        "Ying He",
        "Panpan Huang",
        "Jiashi Li",
        "Wenfeng Liang",
        "Xiaodong Liu",
        "Xin Liu",
        "Yiyuan Liu",
        "Yuxuan Liu",
        "Shanghao Lu",
        "Xuan Lu",
        "Xiaotao Nie",
        "Tian Pei",
        "Junjie Qiu",
        "Hui Qu",
        "Zehui Ren",
        "Zhangli Sha",
        "Xuecheng Su",
        "Xiaowen Sun",
        "Yixuan Tan",
        "Minghui Tang",
        "Shiyu Wang",
        "Yaohui Wang",
        "Yongji Wang",
        "Ziwei Xie",
        "Yiliang Xiong",
        "Yanhong Xu",
        "Shengfeng Ye",
        "Shuiping Yu",
        "Yukun Zha",
        "Liyue Zhang",
        "Haowei Zhang",
        "Mingchuan Zhang",
        "Wentao Zhang",
        "Yichao Zhang",
        "Chenggang Zhao",
        "Yao Zhao",
        "Shangyan Zhou",
        "Shunfeng Zhou",
        "Yuheng Zou"
      ],
      "abstract": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC.",
      "tldr_zh": "该论文针对深度学习（DL）和大型语言模型（LLMs）对计算资源需求的快速增长导致的高性能计算（HPC）成本问题，提出Fire-Flyer AI-HPC架构，这是一个硬件软件协同设计的成本有效框架。系统部署了Fire-Flyer 2，使用10,000 PCIe A100 GPUs，实现性能接近DGX-A100，同时将成本降低一半、能源消耗减少40%，并通过HFReduce加速allreduce通信和保持Computation-Storage Integrated Network无拥塞。软件栈包括HaiScale、3FS和HAI-Platform，通过计算与通信重叠提升可扩展性，并提供系统经验教训，推动AI-HPC的未来发展。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "This is the preprint version of the paper accepted for presentation\n  at the 2024 International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'24). \\c{opyright} 2024 IEEE. Personal\n  use of this material is permitted. For other uses, permission from IEEE must\n  be obtained. Please refer to IEEE Xplore for the final published version",
      "pdf_url": "http://arxiv.org/pdf/2408.14158v2",
      "published_date": "2024-08-26 10:11:56 UTC",
      "updated_date": "2024-08-31 13:33:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:56:14.736963"
    },
    {
      "arxiv_id": "2408.14153v3",
      "title": "Explaining Caption-Image Interactions in CLIP models with Second-Order Attributions",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Möller",
        "Pascal Tilli",
        "Ngoc Thang Vu",
        "Sebastian Padó"
      ],
      "abstract": "Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nsuccess, it is, however, not understood how these models compare their two\ninputs. Common first-order feature-attribution methods can only provide limited\ninsights into dual-encoders since their predictions depend on\nfeature-interactions rather than on individual features. In this paper, we\nfirst derive a second-order method enabling the attribution of predictions by\nany differentiable dual encoder onto feature-interactions between its inputs.\nSecond, we apply our method to CLIP models and show that they learn\nfine-grained correspondences between parts of captions and regions in images.\nThey match objects across input modes also account for mismatches. This\nvisual-linguistic grounding ability, however, varies heavily between object\nclasses and exhibits pronounced out-of-domain effects. We can identify\nindividual errors as well as systematic failure categories including object\ncoverage, unusual scenes and correlated contexts.",
      "tldr_zh": "本研究针对双编码器架构如 CLIP models，探讨了它们如何通过预测相似度来比较图像和标题，但现有第一阶特征归因方法无法充分解释特征-interactions。作者推导了一种第二-order attributions 方法，将模型预测归因到输入间的特征交互，从而分析 CLIP models 的行为。结果显示，CLIP models 能够学习细粒度的视觉-语言 grounding，包括匹配跨模态对象和处理不匹配，但这种能力在对象类别间差异显著，并存在领域外效果和系统性失败，如对象覆盖、异常场景和相关上下文。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14153v3",
      "published_date": "2024-08-26 09:55:34 UTC",
      "updated_date": "2025-03-06 09:00:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:56:24.564898"
    },
    {
      "arxiv_id": "2408.14134v3",
      "title": "Exploring the Potential of Large Language Models for Heterophilic Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxia Wu",
        "Shujie Li",
        "Yuan Fang",
        "Chuan Shi"
      ],
      "abstract": "Large language models (LLMs) have presented significant opportunities to\nenhance various machine learning applications, including graph neural networks\n(GNNs). By leveraging the vast open-world knowledge within LLMs, we can more\neffectively interpret and utilize textual data to better characterize\nheterophilic graphs, where neighboring nodes often have different labels.\nHowever, existing approaches for heterophilic graphs overlook the rich textual\ndata associated with nodes, which could unlock deeper insights into their\nheterophilic contexts. In this work, we explore the potential of LLMs for\nmodeling heterophilic graphs and propose a novel two-stage framework:\nLLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first\nstage, we fine-tune the LLM to better identify homophilic and heterophilic\nedges based on the textual content of their nodes. In the second stage, we\nadaptively manage message propagation in GNNs for different edge types based on\nnode features, structures, and heterophilic or homophilic characteristics. To\ncope with the computational demands when deploying LLMs in practical scenarios,\nwe further explore model distillation techniques to fine-tune smaller, more\nefficient models that maintain competitive performance. Extensive experiments\nvalidate the effectiveness of our framework, demonstrating the feasibility of\nusing LLMs to enhance node classification on heterophilic graphs.",
      "tldr_zh": "本研究探讨了大语言模型 (LLMs) 在异质图 (heterophilic graphs) 中的潜力，旨在通过利用节点的文本数据来提升图神经网络 (GNNs) 的性能。论文提出一个两阶段框架：首先，微调 LLMs 作为增强边鉴别器 (LLM-enhanced edge discriminator) 来识别同质边 (homophilic edges) 和异质边；其次，进行 LLM 引导的边权重调整 (LLM-guided edge reweighting)，根据节点特征、结构和边类型自适应管理消息传播。为应对计算开销，该框架还引入模型蒸馏 (model distillation) 技术，以训练更高效的模型。实验结果显示，该方法显著提高了异质图节点分类的准确性，验证了 LLMs 在该领域的可行性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.14134v3",
      "published_date": "2024-08-26 09:29:56 UTC",
      "updated_date": "2025-02-15 12:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:56:38.010475"
    },
    {
      "arxiv_id": "2408.14523v2",
      "title": "Retrieval Augmented Generation for Dynamic Graph Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxia Wu",
        "Lizi Liao",
        "Yuan Fang"
      ],
      "abstract": "Modeling dynamic graphs, such as those found in social networks,\nrecommendation systems, and e-commerce platforms, is crucial for capturing\nevolving relationships and delivering relevant insights over time. Traditional\napproaches primarily rely on graph neural networks with temporal components or\nsequence generation models, which often focus narrowly on the historical\ncontext of target nodes. This limitation restricts the ability to adapt to new\nand emerging patterns in dynamic graphs. To address this challenge, we propose\na novel framework, Retrieval-Augmented Generation for Dynamic Graph modeling\n(RAG4DyG), which enhances dynamic graph predictions by incorporating\ncontextually and temporally relevant examples from broader graph structures.\nOur approach includes a time- and context-aware contrastive learning module to\nidentify high-quality demonstrations and a graph fusion strategy to effectively\nintegrate these examples with historical contexts. The proposed framework is\ndesigned to be effective in both transductive and inductive scenarios, ensuring\nadaptability to previously unseen nodes and evolving graph structures.\nExtensive experiments across multiple real-world datasets demonstrate the\neffectiveness of RAG4DyG in improving predictive accuracy and adaptability for\ndynamic graph modeling. The code and datasets are publicly available at\nhttps://github.com/YuxiaWu/RAG4DyG.",
      "tldr_zh": "这篇论文针对动态图建模（如社交网络和推荐系统）的挑战，提出了一种新型框架Retrieval-Augmented Generation for Dynamic Graph modeling (RAG4DyG)，以克服传统方法（如图神经网络或序列生成模型）仅依赖目标节点历史上下文的局限性。RAG4DyG 通过time- and context-aware contrastive learning module 识别高质量的演示例子，并采用graph fusion strategy 将这些例子与历史上下文有效整合，支持transductive 和 inductive 场景，以适应新节点和演变的图结构。实验结果显示，该框架在多个真实数据集上显著提高了预测准确性和适应性，相关代码和数据集已公开可用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.14523v2",
      "published_date": "2024-08-26 09:23:35 UTC",
      "updated_date": "2025-04-27 07:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:56:49.393199"
    },
    {
      "arxiv_id": "2408.14119v1",
      "title": "Contrastive Learning Subspace for Text Clustering",
      "title_zh": "用于文本",
      "authors": [
        "Qian Yong",
        "Chen Chen",
        "Xiabing Zhou"
      ],
      "abstract": "Contrastive learning has been frequently investigated to learn effective\nrepresentations for text clustering tasks. While existing contrastive\nlearning-based text clustering methods only focus on modeling instance-wise\nsemantic similarity relationships, they ignore contextual information and\nunderlying relationships among all instances that needs to be clustered. In\nthis paper, we propose a novel text clustering approach called Subspace\nContrastive Learning (SCL) which models cluster-wise relationships among\ninstances. Specifically, the proposed SCL consists of two main modules: (1) a\nself-expressive module that constructs virtual positive samples and (2) a\ncontrastive learning module that further learns a discriminative subspace to\ncapture task-specific cluster-wise relationships among texts. Experimental\nresults show that the proposed SCL method not only has achieved superior\nresults on multiple task clustering datasets but also has less complexity in\npositive sample construction.",
      "tldr_zh": "这篇论文提出了一种名为Subspace Contrastive Learning (SCL)的文本聚类方法，以解决现有Contrastive Learning方法仅关注实例级语义相似性而忽略上下文和实例间聚类关系的局限性。SCL包括两个关键模块：自表达模块用于构建虚拟正样本，以及对比学习模块来学习判别子空间，从而捕捉任务特定的聚类关系。实验结果表明，SCL在多个文本聚类数据集上取得了优越的性能，同时在正样本构建方面具有更低的复杂度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14119v1",
      "published_date": "2024-08-26 09:08:26 UTC",
      "updated_date": "2024-08-26 09:08:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:57:00.202312"
    },
    {
      "arxiv_id": "2408.14101v2",
      "title": "Estimating Causal Effects from Learned Causal Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Anna Raichev",
        "Alexander Ihler",
        "Jin Tian",
        "Rina Dechter"
      ],
      "abstract": "The standard approach to answering an identifiable causal-effect query (e.g.,\n$P(Y|do(X)$) when given a causal diagram and observational data is to first\ngenerate an estimand, or probabilistic expression over the observable\nvariables, which is then evaluated using the observational data. In this paper,\nwe propose an alternative paradigm for answering causal-effect queries over\ndiscrete observable variables. We propose to instead learn the causal Bayesian\nnetwork and its confounding latent variables directly from the observational\ndata. Then, efficient probabilistic graphical model (PGM) algorithms can be\napplied to the learned model to answer queries. Perhaps surprisingly, we show\nthat this \\emph{model completion} learning approach can be more effective than\nestimand approaches, particularly for larger models in which the estimand\nexpressions become computationally difficult.\n  We illustrate our method's potential using a benchmark collection of Bayesian\nnetworks and synthetically generated causal models.",
      "tldr_zh": "本文提出一种新方法，用于从观测数据中估计因果效应（causal effects），即直接学习因果贝叶斯网络（causal Bayesian network）及其混淆隐变量（confounding latent variables），而非传统生成 estimand 表达式后进行评估。相比标准方法，这种模型完成（model completion）学习范式利用高效的概率图形模型（PGM）算法回答查询，尤其在较大模型中更具优势，因为它减少了计算复杂性。实验通过基准贝叶斯网络和合成因果模型验证了该方法的有效性，展示了其潜在的优越性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14101v2",
      "published_date": "2024-08-26 08:39:09 UTC",
      "updated_date": "2024-08-27 09:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:57:12.884523"
    },
    {
      "arxiv_id": "2409.00091v1",
      "title": "Classification of Safety Events at Nuclear Sites using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mishca de Costa",
        "Muhammad Anwar",
        "Daniel Lau",
        "Issam Hammad"
      ],
      "abstract": "This paper proposes the development of a Large Language Model (LLM) based\nmachine learning classifier designed to categorize Station Condition Records\n(SCRs) at nuclear power stations into safety-related and non-safety-related\ncategories. The primary objective is to augment the existing manual review\nprocess by enhancing the efficiency and accuracy of the safety classification\nprocess at nuclear stations. The paper discusses experiments performed to\nclassify a labeled SCR dataset and evaluates the performance of the classifier.\nIt explores the construction of several prompt variations and their observed\neffects on the LLM's decision-making process. Additionally, it introduces a\nnumerical scoring mechanism that could offer a more nuanced and flexible\napproach to SCR safety classification. This method represents an innovative\nstep in nuclear safety management, providing a scalable tool for the\nidentification of safety events.",
      "tldr_zh": "这篇论文提出使用 Large Language Model (LLM) 开发一个机器学习分类器，来将核电站的 Station Condition Records (SCRs) 分类为安全相关和非安全相关类别，从而提高现有手动审查过程的效率和准确性。研究团队通过实验对标记的 SCR 数据集进行分类，探索了多种提示变体及其对 LLM 决策的影响，并引入了一个数值评分机制，提供更细致灵活的分类方法。实验结果显示，该创新方法为核安全管理提供了可扩展的工具，有助于更有效地识别安全事件。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00091v1",
      "published_date": "2024-08-26 08:21:21 UTC",
      "updated_date": "2024-08-26 08:21:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:57:25.059063"
    },
    {
      "arxiv_id": "2408.14090v2",
      "title": "Exploring GPU-to-GPU Communication: Insights into Supercomputer Interconnects",
      "title_zh": "探索 GPU 到 GPU 通信：对超级计算机互连的洞见",
      "authors": [
        "Daniele De Sensi",
        "Lorenzo Pichetti",
        "Flavio Vella",
        "Tiziano De Matteis",
        "Zebin Ren",
        "Luigi Fusco",
        "Matteo Turisini",
        "Daniele Cesarini",
        "Kurt Lust",
        "Animesh Trivedi",
        "Duncan Roweth",
        "Filippo Spiga",
        "Salvatore Di Girolamo",
        "Torsten Hoefler"
      ],
      "abstract": "Multi-GPU nodes are increasingly common in the rapidly evolving landscape of\nexascale supercomputers. On these systems, GPUs on the same node are connected\nthrough dedicated networks, with bandwidths up to a few terabits per second.\nHowever, gauging performance expectations and maximizing system efficiency is\nchallenging due to different technologies, design options, and software layers.\nThis paper comprehensively characterizes three supercomputers - Alps, Leonardo,\nand LUMI - each with a unique architecture and design. We focus on performance\nevaluation of intra-node and inter-node interconnects on up to 4096 GPUs, using\na mix of intra-node and inter-node benchmarks. By analyzing its limitations and\nopportunities, we aim to offer practical guidance to researchers, system\narchitects, and software developers dealing with multi-GPU supercomputing. Our\nresults show that there is untapped bandwidth, and there are still many\nopportunities for optimization, ranging from network to software optimization.",
      "tldr_zh": "这篇论文探讨了exascale超级计算机中GPU-to-GPU通信的性能，针对Alps、Leonardo和LUMI三个独特架构的超级计算机进行了全面表征。研究通过混合intra-node和inter-node基准测试，评估了节点内和节点间互连的性能，覆盖多达4096个GPU。结果显示存在未充分利用的带宽，并指出了网络和软件优化的潜在机会，为研究人员、系统架构师和软件开发者提供实用指导。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.NI",
        "cs.PF",
        "C.2.4; C.5.1; C.2.1; C.4"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14090v2",
      "published_date": "2024-08-26 08:20:50 UTC",
      "updated_date": "2024-11-15 17:55:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:57:37.059213"
    },
    {
      "arxiv_id": "2409.00090v1",
      "title": "Evaluating ChatGPT on Nuclear Domain-Specific Data",
      "title_zh": "评估 ChatGPT 在核领域特定数据上的表现",
      "authors": [
        "Muhammad Anwar",
        "Mischa de Costa",
        "Issam Hammad",
        "Daniel Lau"
      ],
      "abstract": "This paper examines the application of ChatGPT, a large language model (LLM),\nfor question-and-answer (Q&A) tasks in the highly specialized field of nuclear\ndata. The primary focus is on evaluating ChatGPT's performance on a curated\ntest dataset, comparing the outcomes of a standalone LLM with those generated\nthrough a Retrieval Augmented Generation (RAG) approach. LLMs, despite their\nrecent advancements, are prone to generating incorrect or 'hallucinated'\ninformation, which is a significant limitation in applications requiring high\naccuracy and reliability. This study explores the potential of utilizing RAG in\nLLMs, a method that integrates external knowledge bases and sophisticated\nretrieval techniques to enhance the accuracy and relevance of generated\noutputs. In this context, the paper evaluates ChatGPT's ability to answer\ndomain-specific questions, employing two methodologies: A) direct response from\nthe LLM, and B) response from the LLM within a RAG framework. The effectiveness\nof these methods is assessed through a dual mechanism of human and LLM\nevaluation, scoring the responses for correctness and other metrics. The\nfindings underscore the improvement in performance when incorporating a RAG\npipeline in an LLM, particularly in generating more accurate and contextually\nappropriate responses for nuclear domain-specific queries. Additionally, the\npaper highlights alternative approaches to further refine and improve the\nquality of answers in such specialized domains.",
      "tldr_zh": "这篇论文评估了 ChatGPT（一个大型语言模型，LLM）在核领域特定数据的问答任务中的性能，重点比较了独立 LLM 与 Retrieval Augmented Generation (RAG) 方法的效果。研究通过两种方法（A) 直接 LLM 响应和 B) RAG 框架下的 LLM 响应）进行测试，并采用人类和 LLM 双重评估机制来评分正确性和其他指标。结果显示，RAG 方法显著提高了响应准确性和相关性，尤其在处理核领域查询时，并探讨了其他改进策略以进一步提升答案质量。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00090v1",
      "published_date": "2024-08-26 08:17:42 UTC",
      "updated_date": "2024-08-26 08:17:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:57:48.487010"
    },
    {
      "arxiv_id": "2408.14080v4",
      "title": "SONICS: Synthetic Or Not -- Identifying Counterfeit Songs",
      "title_zh": "SONICS: 合成与否——识别假冒歌曲",
      "authors": [
        "Md Awsafur Rahman",
        "Zaber Ibn Abdul Hakim",
        "Najibul Haque Sarker",
        "Bishmoy Paul",
        "Shaikh Anowarul Fattah"
      ],
      "abstract": "The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. These innovations necessitate the ability to distinguish between\nhuman-composed and synthetic songs to safeguard artistic integrity and protect\nhuman musical artistry. Existing research and datasets in fake song detection\nonly focus on singing voice deepfake detection (SVDD), where the vocals are\nAI-generated but the instrumental music is sourced from real songs. However,\nthese approaches are inadequate for detecting contemporary end-to-end\nartificial songs where all components (vocals, music, lyrics, and style) could\nbe AI-generated. Additionally, existing datasets lack music-lyrics diversity,\nlong-duration songs, and open-access fake songs. To address these gaps, we\nintroduce SONICS, a novel dataset for end-to-end Synthetic Song Detection\n(SSD), comprising over 97k songs (4,751 hours) with over 49k synthetic songs\nfrom popular platforms like Suno and Udio. Furthermore, we highlight the\nimportance of modeling long-range temporal dependencies in songs for effective\nauthenticity detection, an aspect entirely overlooked in existing methods. To\nutilize long-range patterns, we introduce SpecTTTra, a novel architecture that\nsignificantly improves time and memory efficiency over conventional CNN and\nTransformer-based models. For long songs, our top-performing variant\noutperforms ViT by 8% in F1 score, is 38% faster, and uses 26% less memory,\nwhile also surpassing ConvNeXt with a 1% F1 score gain, 20% speed boost, and\n67% memory reduction.",
      "tldr_zh": "该研究针对AI生成歌曲的兴起，提出SONICS数据集，用于端到端Synthetic Song Detection (SSD)，它包含超过97k首歌曲（4,751小时），其中逾49k首是来自Suno和Udio等平台的合成歌曲，以解决现有数据集在音乐-歌词多样性、长时长和公开访问方面的不足。研究强调建模歌曲的长程时间依赖性至关重要，并引入了SpecTTTra架构，这是一种高效的模型，改进了时间和内存效率，超越了传统的CNN和Transformer-based方法。对于长歌曲，SpecTTTra的最佳变体在F1分数上比ViT高8%、速度快38%、内存减少26%，并比ConvNeXt高1% F1分数、速度快20%、内存减少67%。这项工作为保护音乐艺术完整性和检测AI歌曲提供了重要工具。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ICLR 2025. Project url: https://github.com/awsaf49/sonics",
      "pdf_url": "http://arxiv.org/pdf/2408.14080v4",
      "published_date": "2024-08-26 08:02:57 UTC",
      "updated_date": "2025-02-25 03:22:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:58:02.330650"
    },
    {
      "arxiv_id": "2408.14069v1",
      "title": "Revisiting Vacuous Reduct Semantics for Abstract Argumentation (Extended Version)",
      "title_zh": "翻译失败",
      "authors": [
        "Lydia Blümel",
        "Matthias Thimm"
      ],
      "abstract": "We consider the notion of a vacuous reduct semantics for abstract\nargumentation frameworks, which, given two abstract argumentation semantics\n{\\sigma} and {\\tau}, refines {\\sigma} (base condition) by accepting only those\n{\\sigma}-extensions that have no non-empty {\\tau}-extension in their reduct\n(vacuity condition). We give a systematic overview on vacuous reduct semantics\nresulting from combining different admissibility-based and conflict-free\nsemantics and present a principle-based analysis of vacuous reduct semantics in\ngeneral. We provide criteria for the inheritance of principle satisfaction by a\nvacuous reduct semantics from its base and vacuity condition for established as\nwell as recently introduced principles in the context of weak argumentation\nsemantics. We also conduct a principle-based analysis for the special case of\nundisputed semantics.",
      "tldr_zh": "本论文重新审视了抽象论辩框架（abstract argumentation frameworks）中的 vacuous reduct semantics，通过结合两个语义 σ 和 τ 来精炼基语义 σ，确保其扩展在 reduct 中没有非空 τ 扩展。研究系统概述了将不同 admissibility-based 和 conflict-free semantics 结合而成的 vacuous reduct semantics，并进行了原则-based 分析。论文提供了准则，评估 vacuous reduct semantics 是否从基语义和空虚条件继承原则满足，包括针对弱论辩语义的新引入原则；最后，对 undisputed semantics 的特例进行了具体分析。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper has been accepted at ECAI 2024, this is an extended version\n  including proofs of technical results",
      "pdf_url": "http://arxiv.org/pdf/2408.14069v1",
      "published_date": "2024-08-26 07:50:49 UTC",
      "updated_date": "2024-08-26 07:50:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:58:13.350735"
    },
    {
      "arxiv_id": "2408.14055v1",
      "title": "HAPM -- Hardware Aware Pruning Method for CNN hardware accelerators in resource constrained devices",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Nicolas Peccia",
        "Luciano Ferreyro",
        "Alejandro Furfaro"
      ],
      "abstract": "During the last years, algorithms known as Convolutional Neural Networks\n(CNNs) had become increasingly popular, expanding its application range to\nseveral areas. In particular, the image processing field has experienced a\nremarkable advance thanks to this algorithms. In IoT, a wide research field\naims to develop hardware capable of execute them at the lowest possible energy\ncost, but keeping acceptable image inference time. One can get around this\napparently conflicting objectives by applying design and training techniques.\nThe present work proposes a generic hardware architecture ready to be\nimplemented on FPGA devices, supporting a wide range of configurations which\nallows the system to run different neural network architectures, dynamically\nexploiting the sparsity caused by pruning techniques in the mathematical\noperations present in this kind of algorithms. The inference speed of the\ndesign is evaluated over different resource constrained FPGA devices. Finally,\nthe standard pruning algorithm is compared against a custom pruning technique\nspecifically designed to exploit the scheduling properties of this hardware\naccelerator. We demonstrate that our hardware-aware pruning algorithm achieves\na remarkable improvement of a 45 % in inference time compared to a network\npruned using the standard algorithm.",
      "tldr_zh": "这篇论文提出了 HAPM（Hardware Aware Pruning Method），一种针对资源受限设备的 CNN 硬件加速器优化方法，旨在平衡低能耗和快速图像推理。方法包括设计一个通用的 FPGA 硬件架构，支持多种 CNN 配置，并动态利用修剪技术带来的稀疏性来优化数学运算。同时，论文比较了标准修剪算法与自定义硬件感知修剪算法，结果显示后者在资源受限 FPGA 设备上将推理时间改善了 45%。这项工作为 IoT 领域的高效神经网络部署提供了实用解决方案。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "8 pages, 7 figure, thesis for the title of Electronic Engineer\n  attained in 2021 at the Universidad Tecnologica Nacional (UTN), Argentina",
      "pdf_url": "http://arxiv.org/pdf/2408.14055v1",
      "published_date": "2024-08-26 07:27:12 UTC",
      "updated_date": "2024-08-26 07:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:58:35.594747"
    },
    {
      "arxiv_id": "2408.14045v1",
      "title": "Beyond Detection: Leveraging Large Language Models for Cyber Attack Prediction in IoT Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Alaeddine Diaf",
        "Abdelaziz Amara Korba",
        "Nour Elislem Karabadji",
        "Yacine Ghamri-Doudane"
      ],
      "abstract": "In recent years, numerous large-scale cyberattacks have exploited Internet of\nThings (IoT) devices, a phenomenon that is expected to escalate with the\ncontinuing proliferation of IoT technology. Despite considerable efforts in\nattack detection, intrusion detection systems remain mostly reactive,\nresponding to specific patterns or observed anomalies. This work proposes a\nproactive approach to anticipate and mitigate malicious activities before they\ncause damage. This paper proposes a novel network intrusion prediction\nframework that combines Large Language Models (LLMs) with Long Short Term\nMemory (LSTM) networks. The framework incorporates two LLMs in a feedback loop:\na fine-tuned Generative Pre-trained Transformer (GPT) model for predicting\nnetwork traffic and a fine-tuned Bidirectional Encoder Representations from\nTransformers (BERT) for evaluating the predicted traffic. The LSTM classifier\nmodel then identifies malicious packets among these predictions. Our framework,\nevaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant\nimprovement in predictive capabilities, achieving an overall accuracy of 98%,\noffering a robust solution to IoT cybersecurity challenges.",
      "tldr_zh": "这篇论文提出了一种超越传统检测的主动框架，利用Large Language Models (LLMs)来预测IoT网络中的网络攻击，从而提前缓解潜在威胁。框架结合fine-tuned Generative Pre-trained Transformer (GPT)模型预测网络流量、fine-tuned Bidirectional Encoder Representations from Transformers (BERT)模型评估预测流量，以及Long Short Term Memory (LSTM)网络分类器识别恶意数据包。在CICIoT2023数据集上的实验显示，该框架准确率达到98%，显著提升了IoT网络安全的预测能力。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14045v1",
      "published_date": "2024-08-26 06:57:22 UTC",
      "updated_date": "2024-08-26 06:57:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:58:37.280969"
    },
    {
      "arxiv_id": "2409.00089v1",
      "title": "Watermarking Techniques for Large Language Models: A Survey",
      "title_zh": "大型语言模型的水印技术：综述",
      "authors": [
        "Yuqing Liang",
        "Jiancheng Xiao",
        "Wensheng Gan",
        "Philip S. Yu"
      ],
      "abstract": "With the rapid advancement and extensive application of artificial\nintelligence technology, large language models (LLMs) are extensively used to\nenhance production, creativity, learning, and work efficiency across various\ndomains. However, the abuse of LLMs also poses potential harm to human society,\nsuch as intellectual property rights issues, academic misconduct, false\ncontent, and hallucinations. Relevant research has proposed the use of LLM\nwatermarking to achieve IP protection for LLMs and traceability of multimedia\ndata output by LLMs. To our knowledge, this is the first thorough review that\ninvestigates and analyzes LLM watermarking technology in detail. This review\nbegins by recounting the history of traditional watermarking technology, then\nanalyzes the current state of LLM watermarking research, and thoroughly\nexamines the inheritance and relevance of these techniques. By analyzing their\ninheritance and relevance, this review can provide research with ideas for\napplying traditional digital watermarking techniques to LLM watermarking, to\npromote the cross-integration and innovation of watermarking technology. In\naddition, this review examines the pros and cons of LLM watermarking.\nConsidering the current multimodal development trend of LLMs, it provides a\ndetailed analysis of emerging multimodal LLM watermarking, such as visual and\naudio data, to offer more reference ideas for relevant research. This review\ndelves into the challenges and future prospects of current watermarking\ntechnologies, offering valuable insights for future LLM watermarking research\nand applications.",
      "tldr_zh": "本调查回顾了大型语言模型 (LLMs) 的水印技术，旨在解决LLMs滥用引发的知识产权问题、学术不端和虚假内容等问题，这是首篇详细分析此领域的综述。论文追溯了传统水印技术的历史，探讨其在LLMs中的继承和应用，包括检索增强生成和多模态扩展（如视觉和音频数据），并分析了这些技术的优缺点。最终，它指出了当前水印技术的挑战与未来前景，为促进水印技术的跨领域创新和应用提供了宝贵见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Preprint. 19 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.00089v1",
      "published_date": "2024-08-26 06:50:11 UTC",
      "updated_date": "2024-08-26 06:50:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:58:48.727290"
    },
    {
      "arxiv_id": "2408.14042v2",
      "title": "PAGE: Parametric Generative Explainer for Graph Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Qiu",
        "Wei Liu",
        "Jun Wang",
        "Ruixuan Li"
      ],
      "abstract": "This article introduces PAGE, a parameterized generative interpretive\nframework. PAGE is capable of providing faithful explanations for any graph\nneural network without necessitating prior knowledge or internal details.\nSpecifically, we train the auto-encoder to generate explanatory substructures\nby designing appropriate training strategy. Due to the dimensionality reduction\nof features in the latent space of the auto-encoder, it becomes easier to\nextract causal features leading to the model's output, which can be easily\nemployed to generate explanations. To accomplish this, we introduce an\nadditional discriminator to capture the causality between latent causal\nfeatures and the model's output. By designing appropriate optimization\nobjectives, the well-trained discriminator can be employed to constrain the\nencoder in generating enhanced causal features. Finally, these features are\nmapped to substructures of the input graph through the decoder to serve as\nexplanations. Compared to existing methods, PAGE operates at the sample scale\nrather than nodes or edges, eliminating the need for perturbation or encoding\nprocesses as seen in previous methods. Experimental results on both\nartificially synthesized and real-world datasets demonstrate that our approach\nnot only exhibits the highest faithfulness and accuracy but also significantly\noutperforms baseline models in terms of efficiency.",
      "tldr_zh": "本研究提出 PAGE，一种参数化生成解释框架（Parametric Generative Explainer），用于为任意图神经网络（Graph Neural Network）提供忠实解释，而无需先验知识或内部细节。通过训练自编码器（auto-encoder）并设计适当的训练策略，PAGE 实现特征降维以提取导致模型输出的因果特征，并引入判别器（discriminator）来捕捉和约束这些因果关系，最终将特征映射回输入图的子结构作为解释。与现有方法相比，PAGE 在样本级别操作，避免了扰动或编码过程。实验结果显示，在合成和真实数据集上，PAGE 表现出最高的忠实度和准确性，并在效率上显著优于基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14042v2",
      "published_date": "2024-08-26 06:39:49 UTC",
      "updated_date": "2024-09-06 08:13:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:59:01.238133"
    },
    {
      "arxiv_id": "2408.14520v3",
      "title": "Towards Graph Prompt Learning: A Survey and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Qingqing Long",
        "Yuchen Yan",
        "Peiyan Zhang",
        "Chen Fang",
        "Wentao Cui",
        "Zhiyuan Ning",
        "Meng Xiao",
        "Ning Cao",
        "Xiao Luo",
        "Lingjun Xu",
        "Shiyue Jiang",
        "Zheng Fang",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Yuanchun Zhou"
      ],
      "abstract": "Large-scale \"pre-train and prompt learning\" paradigms have demonstrated\nremarkable adaptability, enabling broad applications across diverse domains\nsuch as question answering, image recognition, and multimodal retrieval. This\napproach fully leverages the potential of large-scale pre-trained models,\nreducing downstream data requirements and computational costs while enhancing\nmodel applicability across various tasks. Graphs, as versatile data structures\nthat capture relationships between entities, play pivotal roles in fields such\nas social network analysis, recommender systems, and biological graphs. Despite\nthe success of pre-train and prompt learning paradigms in Natural Language\nProcessing (NLP) and Computer Vision (CV), their application in graph domains\nremains nascent. In graph-structured data, not only do the node and edge\nfeatures often have disparate distributions, but the topological structures\nalso differ significantly. This diversity in graph data can lead to\nincompatible patterns or gaps between pre-training and fine-tuning on\ndownstream graphs. We aim to bridge this gap by summarizing methods for\nalleviating these disparities. This includes exploring prompt design\nmethodologies, comparing related techniques, assessing application scenarios\nand datasets, and identifying unresolved problems and challenges. This survey\ncategorizes over 100 relevant works in this field, summarizing general design\nprinciples and the latest applications, including text-attributed graphs,\nmolecules, proteins, and recommendation systems. Through this extensive review,\nwe provide a foundational understanding of graph prompt learning, aiming to\nimpact not only the graph mining community but also the broader Artificial\nGeneral Intelligence (AGI) community.",
      "tldr_zh": "这篇论文对Graph Prompt Learning进行了全面调查，旨在桥接预训练和提示学习在图结构数据中的应用差距。论文总结了超过100个相关研究，探讨了提示设计方法、相关技术比较，以及在文本归因图、分子、蛋白质和推荐系统等领域的应用场景和数据集，以缓解图数据中节点/边特征分布差异和拓扑结构不兼容的问题。通过分析这些方法，论文提供了Graph Prompt Learning的基礎理解，并强调其对图挖掘社区和更广泛的Artificial General Intelligence (AGI)领域的影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "I have decided to temporarily withdraw this draft as I am in the\n  process of making further revisions to improve its content",
      "pdf_url": "http://arxiv.org/pdf/2408.14520v3",
      "published_date": "2024-08-26 06:36:42 UTC",
      "updated_date": "2024-09-24 09:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:59:13.210721"
    },
    {
      "arxiv_id": "2408.14033v2",
      "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents",
      "title_zh": "MLR-Copilot: 基于大型语言模型代理",
      "authors": [
        "Ruochen Li",
        "Teerth Patel",
        "Qingyun Wang",
        "Xinya Du"
      ],
      "abstract": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations.",
      "tldr_zh": "该研究提出MLR-Copilot框架，利用Large Language Models (LLMs)代理来实现自主机器学习研究，旨在解决研究复杂性、实验缓慢和专业知识需求等问题。框架分为三个阶段：首先，IdeaAgent基于现有论文生成假设和实验计划；其次，ExperimentAgent将这些计划转化为可执行代码，并利用检索的原型代码、模型和数据；最后，通过实验执行阶段结合人类反馈和迭代调试，确保研究成果的可行性。在五个机器学习任务上的实验结果显示，该框架显著提升了研究生产力和创新潜力。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14033v2",
      "published_date": "2024-08-26 05:55:48 UTC",
      "updated_date": "2024-09-02 05:55:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:59:24.790866"
    },
    {
      "arxiv_id": "2408.14028v3",
      "title": "SurGen: Text-Guided Diffusion Model for Surgical Video Generation",
      "title_zh": "SurGen：文本",
      "authors": [
        "Joseph Cho",
        "Samuel Schmidgall",
        "Cyril Zakka",
        "Mrudang Mathur",
        "Dhamanpreet Kaur",
        "Rohan Shad",
        "William Hiesinger"
      ],
      "abstract": "Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis. SurGen produces videos with the highest resolution\nand longest duration among existing surgical video generation models. We\nvalidate the visual and temporal quality of the outputs using standard image\nand video generation metrics. Additionally, we assess their alignment to the\ncorresponding text prompts through a deep learning classifier trained on\nsurgical data. Our results demonstrate the potential of diffusion models to\nserve as valuable educational tools for surgical trainees.",
      "tldr_zh": "本文介绍了 SurGen，一种文本引导的 diffusion model，用于生成外科视频，以提升外科教育的模拟环境。SurGen 能够产生目前同类模型中分辨率最高和持续时间最长的视频，并通过标准图像和视频生成指标验证其视觉和时间质量。研究还使用在外科数据上训练的深度学习分类器评估输出与文本提示的匹配度，结果表明 SurGen 有望作为有效的教育工具，帮助外科培训生进行更真实和互动的学习。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14028v3",
      "published_date": "2024-08-26 05:38:27 UTC",
      "updated_date": "2024-09-24 23:23:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:59:37.821544"
    },
    {
      "arxiv_id": "2408.14023v1",
      "title": "Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos",
      "title_zh": "Video-CCAM：利用因果交叉注意力掩码增强视频-语言理解，以适用于短视频和长视频",
      "authors": [
        "Jiajun Fei",
        "Dian Li",
        "Zhidong Deng",
        "Zekun Wang",
        "Gang Liu",
        "Hui Wang"
      ],
      "abstract": "Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}.",
      "tldr_zh": "本文提出 Video-CCAM，一种增强视频-语言理解的模型，通过在视觉编码器和大型语言模型(LLM)之间添加交叉注意力层，并引入因果交叉注意力掩码(Causal Cross-Attention Masks)来处理视频尤其是长视频的时序问题，从而避免了特征下采样或上下文扩展的局限。模型采用两阶段训练方式，包括特征对齐和视觉指令微调，并在不同规模的LLM(4B、9B、14B)上实现。实验结果显示，Video-CCAM在MVBench、VideoVista和MLVU等基准上表现出色，在短视频和长视频任务中均排名前列，即使仅用图像和16帧视频训练，也能适应96帧视频并取得领先性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.14023v1",
      "published_date": "2024-08-26 05:27:14 UTC",
      "updated_date": "2024-08-26 05:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T18:59:50.572755"
    },
    {
      "arxiv_id": "2408.14016v1",
      "title": "Pixel-Aligned Multi-View Generation with Depth Guided Decoder",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenggang Tang",
        "Peiye Zhuang",
        "Chaoyang Wang",
        "Aliaksandr Siarohin",
        "Yash Kant",
        "Alexander Schwing",
        "Sergey Tulyakov",
        "Hsin-Ying Lee"
      ],
      "abstract": "The task of image-to-multi-view generation refers to generating novel views\nof an instance from a single image. Recent methods achieve this by extending\ntext-to-image latent diffusion models to multi-view version, which contains an\nVAE image encoder and a U-Net diffusion model. Specifically, these generation\nmethods usually fix VAE and finetune the U-Net only. However, the significant\ndownscaling of the latent vectors computed from the input images and\nindependent decoding leads to notable pixel-level misalignment across multiple\nviews. To address this, we propose a novel method for pixel-level\nimage-to-multi-view generation. Unlike prior work, we incorporate attention\nlayers across multi-view images in the VAE decoder of a latent video diffusion\nmodel. Specifically, we introduce a depth-truncated epipolar attention,\nenabling the model to focus on spatially adjacent regions while remaining\nmemory efficient. Applying depth-truncated attn is challenging during inference\nas the ground-truth depth is usually difficult to obtain and pre-trained depth\nestimation models is hard to provide accurate depth. Thus, to enhance the\ngeneralization to inaccurate depth when ground truth depth is missing, we\nperturb depth inputs during training. During inference, we employ a rapid\nmulti-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the\ndepth-truncated epipolar attention. Our model enables better pixel alignment\nacross multi-view images. Moreover, we demonstrate the efficacy of our approach\nin improving downstream multi-view to 3D reconstruction tasks.",
      "tldr_zh": "这篇论文针对从单张图像生成多视图图像的任务，解决了现有基于 VAE 和 U-Net 的方法中像素级不对齐的问题。作者提出了一种新方法，在 VAE 解码器中加入深度截断的极线注意力（depth-truncated epipolar attention），允许模型关注空间相邻区域，同时保持内存效率；并通过在训练中扰动深度输入和在推理中使用 NeuS 快速多视图到 3D 重建方法来处理不准确深度，提升泛化能力。实验结果显示，该方法显著提高了多视图图像的像素对齐，并改善了下游的多视图到 3D 重建任务性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14016v1",
      "published_date": "2024-08-26 04:56:41 UTC",
      "updated_date": "2024-08-26 04:56:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:00:02.205786"
    },
    {
      "arxiv_id": "2409.12922v1",
      "title": "AI Thinking: A framework for rethinking artificial intelligence in practice",
      "title_zh": "AI Thinking：重新审视人工智能在实践中的框架",
      "authors": [
        "Denis Newman-Griffis"
      ],
      "abstract": "Artificial intelligence is transforming the way we work with information\nacross disciplines and practical contexts. A growing range of disciplines are\nnow involved in studying, developing, and assessing the use of AI in practice,\nbut these disciplines often employ conflicting understandings of what AI is and\nwhat is involved in its use. New, interdisciplinary approaches are needed to\nbridge competing conceptualisations of AI in practice and help shape the future\nof AI use. I propose a novel conceptual framework called AI Thinking, which\nmodels key decisions and considerations involved in AI use across disciplinary\nperspectives. The AI Thinking model addresses five practice-based competencies\ninvolved in applying AI in context: motivating AI use in information processes,\nformulating AI methods, assessing available tools and technologies, selecting\nappropriate data, and situating AI in the sociotechnical contexts it is used\nin. A hypothetical case study is provided to illustrate the application of AI\nThinking in practice. This article situates AI Thinking in broader\ncross-disciplinary discourses of AI, including its connections to ongoing\ndiscussions around AI literacy and AI-driven innovation. AI Thinking can help\nto bridge divides between academic disciplines and diverse contexts of AI use,\nand to reshape the future of AI in practice.",
      "tldr_zh": "这篇论文提出了一种名为 AI Thinking 的新概念框架，旨在桥接不同学科对人工智能（AI）的理解分歧，并重塑 AI 在实践中的应用。该框架模型化了 AI 使用中的五个实践-based 能力，包括 motivating AI use in information processes、formulating AI methods、assessing available tools and technologies、selecting appropriate data，以及 situating AI in sociotechnical contexts。通过一个假设案例研究，论文展示了 AI Thinking 的实际应用，并将其与 AI literacy 和 AI-driven innovation 的跨学科讨论相联系，最终帮助促进 AI 的跨领域创新和发展。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "30 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12922v1",
      "published_date": "2024-08-26 04:41:21 UTC",
      "updated_date": "2024-08-26 04:41:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:00:14.209665"
    },
    {
      "arxiv_id": "2408.14009v1",
      "title": "Optimizing TD3 for 7-DOF Robotic Arm Grasping: Overcoming Suboptimality with Exploration-Enhanced Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Wen-Han Hsieh",
        "Jen-Yuan Chang"
      ],
      "abstract": "In actor-critic-based reinforcement learning algorithms such as Twin Delayed\nDeep Deterministic policy gradient (TD3), insufficient exploration of the\nspatial space can result in suboptimal policies when controlling 7-DOF robotic\narms. To address this issue, we propose a novel Exploration-Enhanced\nContrastive Learning (EECL) module that improves exploration by providing\nadditional rewards for encountering novel states. Our module stores previously\nexplored states in a buffer and identifies new states by comparing them with\nhistorical data using Euclidean distance within a K-dimensional tree (KDTree)\nframework. When the agent explores new states, exploration rewards are\nassigned. These rewards are then integrated into the TD3 algorithm, ensuring\nthat the Q-learning process incorporates these signals, promoting more\neffective strategy optimization. We evaluate our method on the robosuite panda\nlift task, demonstrating that it significantly outperforms the baseline TD3 in\nterms of both efficiency and convergence speed in the tested environment.",
      "tldr_zh": "该研究针对 Twin Delayed Deep Deterministic policy gradient (TD3) 算法在控制 7-DOF 机械臂抓取任务时因空间探索不足导致的策略次优问题，提出了一种 Exploration-Enhanced Contrastive Learning (EECL) 模块。EECL 通过存储历史状态并利用 KDTree 计算新状态的欧氏距离来识别新探索区域，并分配额外奖励以增强代理的探索行为，这些奖励随后整合到 TD3 的 Q-learning 过程中。实验结果显示，在 robosuite panda lift 任务上，该方法显著提高了效率和收敛速度，优于基线 TD3 算法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "4 pages, 2 figures, IEEE-ICKII-2024",
      "pdf_url": "http://arxiv.org/pdf/2408.14009v1",
      "published_date": "2024-08-26 04:30:59 UTC",
      "updated_date": "2024-08-26 04:30:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:00:26.968700"
    },
    {
      "arxiv_id": "2408.14008v1",
      "title": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Qihang Ge",
        "Wei Sun",
        "Yu Zhang",
        "Yunhao Li",
        "Zhongpeng Ji",
        "Fengyu Sun",
        "Shangling Jui",
        "Xiongkuo Min",
        "Guangtao Zhai"
      ],
      "abstract": "The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA.",
      "tldr_zh": "本研究提出 LMM-VQA，一种基于大型多模态模型 (LMMs) 的视频质量评估 (VQA) 方法，旨在解决视频内容多样性和复杂空间时间扭曲的挑战。方法包括将质量回归问题转化为问答 (Q&A) 任务，使用时空视觉编码器提取空间和时间特征，并通过时空投影器实现模态对齐，最终输入大型语言模型 (LLM) 生成质量分数和级别。实验结果显示，LMM-VQA 在五个 VQA 基准上达到最先进性能，泛化能力比现有方法平均提高 5%，并在一般视频理解任务上表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14008v1",
      "published_date": "2024-08-26 04:29:52 UTC",
      "updated_date": "2024-08-26 04:29:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:00:38.770107"
    },
    {
      "arxiv_id": "2408.13991v1",
      "title": "Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors from a Bi-level Optimization Perspective",
      "title_zh": "Dual-CBA：从双层优化视角通过双重持续偏差适配器改进在线持续学习",
      "authors": [
        "Quanziang Wang",
        "Renzhen Wang",
        "Yichen Wu",
        "Xixi Jia",
        "Minghao Zhou",
        "Deyu Meng"
      ],
      "abstract": "In online continual learning (CL), models trained on changing distributions\neasily forget previously learned knowledge and bias toward newly received\ntasks. To address this issue, we present Continual Bias Adaptor (CBA), a\nbi-level framework that augments the classification network to adapt to\ncatastrophic distribution shifts during training, enabling the network to\nachieve a stable consolidation of all seen tasks. However, the CBA module\nadjusts distribution shifts in a class-specific manner, exacerbating the\nstability gap issue and, to some extent, fails to meet the need for continual\ntesting in online CL. To mitigate this challenge, we further propose a novel\nclass-agnostic CBA module that separately aggregates the posterior\nprobabilities of classes from new and old tasks, and applies a stable\nadjustment to the resulting posterior probabilities. We combine the two kinds\nof CBA modules into a unified Dual-CBA module, which thus is capable of\nadapting to catastrophic distribution shifts and simultaneously meets the\nreal-time testing requirements of online CL. Besides, we propose Incremental\nBatch Normalization (IBN), a tailored BN module to re-estimate its population\nstatistics for alleviating the feature bias arising from the inner loop\noptimization problem of our bi-level framework. To validate the effectiveness\nof the proposed method, we theoretically provide some insights into how it\nmitigates catastrophic distribution shifts, and empirically demonstrate its\nsuperiority through extensive experiments based on four rehearsal-based\nbaselines and three public continual learning benchmarks.",
      "tldr_zh": "该论文针对在线持续学习（online continual learning, CL）中模型遗忘先前知识并偏向新任务的问题，提出Dual-CBA框架，该框架基于双层优化（bi-level optimization）视角，结合class-specific和class-agnostic Continual Bias Adaptor (CBA)模块，以适应灾难性分布变化并支持实时测试。Dual-CBA通过聚合新旧任务的后验概率并进行稳定调整，同时引入Incremental Batch Normalization (IBN)模块来缓解特征偏差问题，从而实现任务知识的稳定整合。实验结果显示，该方法在四个基于重演的基线和三个公共CL基准上表现出色，证明了其在缓解分布偏移方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.13991v1",
      "published_date": "2024-08-26 03:19:52 UTC",
      "updated_date": "2024-08-26 03:19:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:00:51.266136"
    },
    {
      "arxiv_id": "2408.13988v1",
      "title": "Automatic Medical Report Generation: Methods and Applications",
      "title_zh": "自动医疗报告生成：方法与应用",
      "authors": [
        "Li Guo",
        "Anas M. Tahir",
        "Dong Zhang",
        "Z. Jane Wang",
        "Rabab K. Ward"
      ],
      "abstract": "The increasing demand for medical imaging has surpassed the capacity of\navailable radiologists, leading to diagnostic delays and potential\nmisdiagnoses. Artificial intelligence (AI) techniques, particularly in\nautomatic medical report generation (AMRG), offer a promising solution to this\ndilemma. This review comprehensively examines AMRG methods from 2021 to 2024.\nIt (i) presents solutions to primary challenges in this field, (ii) explores\nAMRG applications across various imaging modalities, (iii) introduces publicly\navailable datasets, (iv) outlines evaluation metrics, (v) identifies techniques\nthat significantly enhance model performance, and (vi) discusses unresolved\nissues and potential future research directions. This paper aims to provide a\ncomprehensive understanding of the existing literature and inspire valuable\nfuture research.",
      "tldr_zh": "这篇综述论文探讨了人工智能（AI）在自动医疗报告生成（AMRG）中的应用，以缓解医疗成像需求超过放射科医生能力的问题，导致的诊断延迟和误诊风险。论文系统回顾了2021年至2024年的AMRG方法，包括挑战的解决方案（如模型优化）、不同成像模态的应用、公开数据集、评估指标，以及显著提升模型性能的技术。最终，它指出了未解决的问题和未来研究方向，旨在为相关领域提供全面理解并激发创新。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "42 pages and 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.13988v1",
      "published_date": "2024-08-26 03:02:41 UTC",
      "updated_date": "2024-08-26 03:02:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:01:01.400084"
    },
    {
      "arxiv_id": "2409.00087v1",
      "title": "A Lightweight Human Pose Estimation Approach for Edge Computing-Enabled Metaverse with Compressive Sensing",
      "title_zh": "一种轻量级的人体姿势估计方法，用于边缘计算启用的元宇宙，结合压缩感知",
      "authors": [
        "Nguyen Quang Hieu",
        "Dinh Thai Hoang",
        "Diep N. Nguyen"
      ],
      "abstract": "The ability to estimate 3D movements of users over edge computing-enabled\nnetworks, such as 5G/6G networks, is a key enabler for the new era of extended\nreality (XR) and Metaverse applications. Recent advancements in deep learning\nhave shown advantages over optimization techniques for estimating 3D human\nposes given spare measurements from sensor signals, i.e., inertial measurement\nunit (IMU) sensors attached to the XR devices. However, the existing works lack\napplicability to wireless systems, where transmitting the IMU signals over\nnoisy wireless networks poses significant challenges. Furthermore, the\npotential redundancy of the IMU signals has not been considered, resulting in\nhighly redundant transmissions. In this work, we propose a novel approach for\nredundancy removal and lightweight transmission of IMU signals over noisy\nwireless environments. Our approach utilizes a random Gaussian matrix to\ntransform the original signal into a lower-dimensional space. By leveraging the\ncompressive sensing theory, we have proved that the designed Gaussian matrix\ncan project the signal into a lower-dimensional space and preserve the\nSet-Restricted Eigenvalue condition, subject to a power transmission\nconstraint. Furthermore, we develop a deep generative model at the receiver to\nrecover the original IMU signals from noisy compressed data, thus enabling the\ncreation of 3D human body movements at the receiver for XR and Metaverse\napplications. Simulation results on a real-world IMU dataset show that our\nframework can achieve highly accurate 3D human poses of the user using only\n$82\\%$ of the measurements from the original signals. This is comparable to an\noptimization-based approach, i.e., Lasso, but is an order of magnitude faster.",
      "tldr_zh": "该论文提出了一种轻量级的3D人体姿势估计方法，针对边缘计算支持的元宇宙应用，通过Compressive Sensing技术处理IMU传感器信号，以解决无线传输中的噪声和冗余问题。该方法利用随机Gaussian矩阵将信号投影到低维空间，并证明其满足Set-Restricted Eigenvalue条件，同时开发了深度生成模型在接收端恢复原始信号，从而实现高效的3D人体动作重建。实验结果显示，在真实IMU数据集上，该框架仅使用原信号82%的测量就实现了高准确率，且比Lasso优化方法快一个数量级。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00087v1",
      "published_date": "2024-08-26 02:57:23 UTC",
      "updated_date": "2024-08-26 02:57:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:01:16.355934"
    },
    {
      "arxiv_id": "2408.13987v1",
      "title": "Focused Large Language Models are Stable Many-Shot Learners",
      "title_zh": "翻译失败",
      "authors": [
        "Peiwen Yuan",
        "Shaoxiong Feng",
        "Yiwei Li",
        "Xinglin Wang",
        "Yueqi Zhang",
        "Chuyi Tan",
        "Boyuan Pan",
        "Heda Wang",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations.",
      "tldr_zh": "这篇论文探讨了 In-Context Learning (ICL) 在 many-shot 设置中性能不佳的问题，理论和实验证实原因是更多演示分散了模型注意力，导致对查询的理解受阻。作者提出了一种训练-free 方法 FocusICL，通过 token-level 的琐碎性过滤（triviality filtering）避免无关内容干扰，以及 demonstration-level 的 hierarchical attention 确保对当前查询的充分关注。论文还设计了基于模型 perplexity 的高效超参数搜索策略，以优化方法应用。实验结果显示，FocusICL 比 vanilla ICL 平均提高了 5.2% 的性能，并在 many-shot 演示中表现出色稳定性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.13987v1",
      "published_date": "2024-08-26 02:53:24 UTC",
      "updated_date": "2024-08-26 02:53:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:01:27.090681"
    },
    {
      "arxiv_id": "2408.13986v2",
      "title": "AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction",
      "title_zh": "AgentMove：基于大语言模型的代理框架，用于零样本下一位置预测",
      "authors": [
        "Jie Feng",
        "Yuwei Du",
        "Jie Zhao",
        "Yong Li"
      ],
      "abstract": "Next location prediction plays a crucial role in various real-world\napplications. Recently, due to the limitation of existing deep learning\nmethods, attempts have been made to apply large language models (LLMs) to\nzero-shot next location prediction task. However, they directly generate the\nfinal output using LLMs without systematic design, which limits the potential\nof LLMs to uncover complex mobility patterns and underestimates their extensive\nreserve of global geospatial knowledge. In this paper, we introduce AgentMove,\na systematic agentic prediction framework to achieve generalized next location\nprediction. In AgentMove, we first decompose the mobility prediction task and\ndesign specific modules to complete them, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments utilizing mobility data from two distinct sources reveal\nthat AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of\n12 metrics and it shows robust predictions with various LLMs as base and also\nless geographical bias across cities. Our codes are available via\nhttps://github.com/tsinghua-fib-lab/AgentMove.",
      "tldr_zh": "本研究提出AgentMove，一种基于Large Language Models (LLMs)的代理框架，用于实现零样本（zero-shot）下一位置预测任务。该框架通过分解预测任务，设计了三个关键模块：spatial-temporal memory 用于挖掘个人移动模式、world knowledge generator 用于建模城市结构影响，以及collective knowledge extractor 用于捕捉人群共享模式。随后，结合这些模块的结果进行推理生成最终预测。实验结果显示，AgentMove 在来自两个数据源的移动性数据上，相比领先基线模型在12个指标中的8个上提升3.33%至8.57%，并在不同LLMs和城市中表现出色稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NAACL 2025 as main conference paper,\n  https://github.com/tsinghua-fib-lab/AgentMove",
      "pdf_url": "http://arxiv.org/pdf/2408.13986v2",
      "published_date": "2024-08-26 02:36:55 UTC",
      "updated_date": "2025-02-09 06:16:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:01:37.785001"
    },
    {
      "arxiv_id": "2408.13979v1",
      "title": "Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shuai Fu",
        "Xiequn Wang",
        "Qiushi Huang",
        "Yu Zhang"
      ],
      "abstract": "With the prevalence of large-scale pretrained vision-language models (VLMs),\nsuch as CLIP, soft-prompt tuning has become a popular method for adapting these\nmodels to various downstream tasks. However, few works delve into the inherent\nproperties of learnable soft-prompt vectors, specifically the impact of their\nnorms to the performance of VLMs. This motivates us to pose an unexplored\nresearch question: ``Do we need to normalize the soft prompts in VLMs?'' To\nfill this research gap, we first uncover a phenomenon, called the\n\\textbf{Low-Norm Effect} by performing extensive corruption experiments,\nsuggesting that reducing the norms of certain learned prompts occasionally\nenhances the performance of VLMs, while increasing them often degrades it. To\nharness this effect, we propose a novel method named \\textbf{N}ormalizing\nth\\textbf{e} soft-pro\\textbf{m}pt v\\textbf{e}ctors of vi\\textbf{si}on-language\nmodel\\textbf{s} (\\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To\nthe best of our knowledge, our work is the first to systematically investigate\nthe role of norms of soft-prompt vector in VLMs, offering valuable insights for\nfuture research in soft-prompt tuning. The code is available at\n\\texttt{\\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.",
      "tldr_zh": "该研究探讨了视觉语言模型(VLMs)中 soft-prompt tuning 的 inherent properties，特别是 soft-prompt 向量的 norms 对模型性能的影响。作者通过广泛的实验发现了 Low-Norm Effect，即降低某些 soft-prompt 的 norms 往往能提升性能，而增加则可能导致下降。为利用这一效果，他们提出 Nemesis 方法，用于规范 soft-prompt 向量，这是首次系统调查这一问题的研究，并提供了开源代码以支持进一步探索。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICLR 2024 (Spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2408.13979v1",
      "published_date": "2024-08-26 02:09:05 UTC",
      "updated_date": "2024-08-26 02:09:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:01:49.835062"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 94,
  "processed_papers_count": 94,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T19:02:22.844642"
}
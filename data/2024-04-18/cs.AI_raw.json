[
  {
    "arxiv_id": "2404.12535v3",
    "title": "Is There No Such Thing as a Bad Question? H4R: HalluciBot For Ratiocination, Rewriting, Ranking, and Routing",
    "authors": [
      "William Watson",
      "Nicole Cho",
      "Nishan Srishankar"
    ],
    "abstract": "Hallucination continues to be one of the most critical challenges in the\ninstitutional adoption journey of Large Language Models (LLMs). While prior\nstudies have primarily focused on the post-generation analysis and refinement\nof outputs, this paper centers on the effectiveness of queries in eliciting\naccurate responses from LLMs. We present HalluciBot, a model that estimates the\nquery's propensity to hallucinate before generation, without invoking any LLMs\nduring inference. HalluciBot can serve as a proxy reward model for query\nrewriting, offering a general framework to estimate query quality based on\naccuracy and consensus. In essence, HalluciBot investigates how poorly\nconstructed queries can lead to erroneous outputs - moreover, by employing\nquery rewriting guided by HalluciBot's empirical estimates, we demonstrate that\n95.7% output accuracy can be achieved for Multiple Choice questions. The\ntraining procedure for HalluciBot consists of perturbing 369,837 queries n\ntimes, employing n+1 independent LLM agents, sampling an output from each\nquery, conducting a Multi-Agent Monte Carlo simulation on the sampled outputs,\nand training an encoder classifier. The idea of perturbation is the outcome of\nour ablation studies that measures the increase in output diversity (+12.5\nagreement spread) by perturbing a query in lexically different but semantically\nsimilar ways. Therefore, HalluciBot paves the way to ratiocinate (76.0% test F1\nscore, 46.6% in saved computation on hallucinatory queries), rewrite (+30.2%\npositive class transition from hallucinatory to non-hallucinatory), rank\n(+50.6% positive class transition from hallucinatory to non-hallucinatory), and\nroute queries to effective pipelines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at The 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2404.12535v3",
    "published_date": "2024-04-18 22:56:57 UTC",
    "updated_date": "2024-12-16 02:35:31 UTC"
  },
  {
    "arxiv_id": "2404.12534v3",
    "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean",
    "authors": [
      "Peiyang Song",
      "Kaiyu Yang",
      "Anima Anandkumar"
    ],
    "abstract": "Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, a general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at NeuS 2025. All code and artifacts open-sourced at\n  https://github.com/lean-dojo/LeanCopilot. All evaluation details are made\n  public at\n  https://github.com/Peiyang-Song/mathematics_in_lean/tree/full-scale-experiment",
    "pdf_url": "http://arxiv.org/pdf/2404.12534v3",
    "published_date": "2024-04-18 22:54:08 UTC",
    "updated_date": "2025-05-11 09:58:48 UTC"
  },
  {
    "arxiv_id": "2404.12522v1",
    "title": "Neural Active Learning Beyond Bandits",
    "authors": [
      "Yikun Ban",
      "Ishika Agarwal",
      "Ziwei Wu",
      "Yada Zhu",
      "Kommy Weldemariam",
      "Hanghang Tong",
      "Jingrui He"
    ],
    "abstract": "We study both stream-based and pool-based active learning with neural network\napproximations. A recent line of works proposed bandit-based approaches that\ntransformed active learning into a bandit problem, achieving both theoretical\nand empirical success. However, the performance and computational costs of\nthese methods may be susceptible to the number of classes, denoted as $K$, due\nto this transformation. Therefore, this paper seeks to answer the question:\n\"How can we mitigate the adverse impacts of $K$ while retaining the advantages\nof principled exploration and provable performance guarantees in active\nlearning?\" To tackle this challenge, we propose two algorithms based on the\nnewly designed exploitation and exploration neural networks for stream-based\nand pool-based active learning. Subsequently, we provide theoretical\nperformance guarantees for both algorithms in a non-parametric setting,\ndemonstrating a slower error-growth rate concerning $K$ for the proposed\napproaches. We use extensive experiments to evaluate the proposed algorithms,\nwhich consistently outperform state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published on ICLR 2024, 40 Pages",
    "pdf_url": "http://arxiv.org/pdf/2404.12522v1",
    "published_date": "2024-04-18 21:52:14 UTC",
    "updated_date": "2024-04-18 21:52:14 UTC"
  },
  {
    "arxiv_id": "2404.12520v1",
    "title": "Centralized vs. Decentralized Multi-Agent Reinforcement Learning for Enhanced Control of Electric Vehicle Charging Networks",
    "authors": [
      "Amin Shojaeighadikolaei",
      "Zsolt Talata",
      "Morteza Hashemi"
    ],
    "abstract": "The widespread adoption of electric vehicles (EVs) poses several challenges\nto power distribution networks and smart grid infrastructure due to the\npossibility of significantly increasing electricity demands, especially during\npeak hours. Furthermore, when EVs participate in demand-side management\nprograms, charging expenses can be reduced by using optimal charging control\npolicies that fully utilize real-time pricing schemes. However, devising\noptimal charging methods and control strategies for EVs is challenging due to\nvarious stochastic and uncertain environmental factors. Currently, most EV\ncharging controllers operate based on a centralized model. In this paper, we\nintroduce a novel approach for distributed and cooperative charging strategy\nusing a Multi-Agent Reinforcement Learning (MARL) framework. Our method is\nbuilt upon the Deep Deterministic Policy Gradient (DDPG) algorithm for a group\nof EVs in a residential community, where all EVs are connected to a shared\ntransformer. This method, referred to as CTDE-DDPG, adopts a Centralized\nTraining Decentralized Execution (CTDE) approach to establish cooperation\nbetween agents during the training phase, while ensuring a distributed and\nprivacy-preserving operation during execution. We theoretically examine the\nperformance of centralized and decentralized critics for the DDPG-based MARL\nimplementation and demonstrate their trade-offs. Furthermore, we numerically\nexplore the efficiency, scalability, and performance of centralized and\ndecentralized critics. Our theoretical and numerical results indicate that,\ndespite higher policy gradient variances and training complexity, the CTDE-DDPG\nframework significantly improves charging efficiency by reducing total\nvariation by approximately %36 and charging cost by around %9.1 on average...",
    "categories": [
      "cs.AI",
      "68T07"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12520v1",
    "published_date": "2024-04-18 21:50:03 UTC",
    "updated_date": "2024-04-18 21:50:03 UTC"
  },
  {
    "arxiv_id": "2404.12509v2",
    "title": "Compositional Neural Textures",
    "authors": [
      "Peihan Tu",
      "Li-Yi Wei",
      "Matthias Zwicker"
    ],
    "abstract": "Texture plays a vital role in enhancing visual richness in both real\nphotographs and computer-generated imagery. However, the process of editing\ntextures often involves laborious and repetitive manual adjustments of textons,\nwhich are the recurring local patterns that characterize textures. This work\nintroduces a fully unsupervised approach for representing textures using a\ncompositional neural model that captures individual textons. We represent each\ntexton as a 2D Gaussian function whose spatial support approximates its shape,\nand an associated feature that encodes its detailed appearance. By modeling a\ntexture as a discrete composition of Gaussian textons, the representation\noffers both expressiveness and ease of editing. Textures can be edited by\nmodifying the compositional Gaussians within the latent space, and new textures\ncan be efficiently synthesized by feeding the modified Gaussians through a\ngenerator network in a feed-forward manner. This approach enables a wide range\nof applications, including transferring appearance from an image texture to\nanother image, diversifying textures,texture interpolation, revealing/modifying\ntexture variations, edit propagation, texture animation, and direct texton\nmanipulation. The proposed approach contributes to advancing texture analysis,\nmodeling, and editing techniques, and opens up new possibilities for creating\nvisually appealing images with controllable textures.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "Project page: https://phtu-cs.github.io/cnt-siga24/",
    "pdf_url": "http://arxiv.org/pdf/2404.12509v2",
    "published_date": "2024-04-18 21:09:34 UTC",
    "updated_date": "2024-09-23 03:01:59 UTC"
  },
  {
    "arxiv_id": "2404.15364v1",
    "title": "MP-DPD: Low-Complexity Mixed-Precision Neural Networks for Energy-Efficient Digital Predistortion of Wideband Power Amplifiers",
    "authors": [
      "Yizhuo Wu",
      "Ang Li",
      "Mohammadreza Beikmirza",
      "Gagan Deep Singh",
      "Qinyu Chen",
      "Leo C. N. de Vreede",
      "Morteza Alavi",
      "Chang Gao"
    ],
    "abstract": "Digital Pre-Distortion (DPD) enhances signal quality in wideband RF power\namplifiers (PAs). As signal bandwidths expand in modern radio systems, DPD's\nenergy consumption increasingly impacts overall system efficiency. Deep Neural\nNetworks (DNNs) offer promising advancements in DPD, yet their high complexity\nhinders their practical deployment. This paper introduces open-source\nmixed-precision (MP) neural networks that employ quantized low-precision\nfixed-point parameters for energy-efficient DPD. This approach reduces\ncomputational complexity and memory footprint, thereby lowering power\nconsumption without compromising linearization efficacy. Applied to a 160MHz-BW\n1024-QAM OFDM signal from a digital RF PA, MP-DPD gives no performance loss\nagainst 32-bit floating-point precision DPDs, while achieving -43.75 (L)/-45.27\n(R) dBc in Adjacent Channel Power Ratio (ACPR) and -38.72 dB in Error Vector\nMagnitude (EVM). A 16-bit fixed-point-precision MP-DPD enables a 2.8X reduction\nin estimated inference power. The PyTorch learning and testing code is publicly\navailable at \\url{https://github.com/lab-emi/OpenDPD}.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted to IEEE Microwave and Wireless Technology Letters (MWTL)",
    "pdf_url": "http://arxiv.org/pdf/2404.15364v1",
    "published_date": "2024-04-18 21:04:39 UTC",
    "updated_date": "2024-04-18 21:04:39 UTC"
  },
  {
    "arxiv_id": "2404.12498v1",
    "title": "A Configurable Pythonic Data Center Model for Sustainable Cooling and ML Integration",
    "authors": [
      "Avisek Naug",
      "Antonio Guillen",
      "Ricardo Luna Gutierrez",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Sajad Mousavi",
      "Ashwin Ramesh Babu",
      "Soumyendu Sarkar"
    ],
    "abstract": "There have been growing discussions on estimating and subsequently reducing\nthe operational carbon footprint of enterprise data centers. The design and\nintelligent control for data centers have an important impact on data center\ncarbon footprint. In this paper, we showcase PyDCM, a Python library that\nenables extremely fast prototyping of data center design and applies\nreinforcement learning-enabled control with the purpose of evaluating key\nsustainability metrics including carbon footprint, energy consumption, and\nobserving temperature hotspots. We demonstrate these capabilities of PyDCM and\ncompare them to existing works in EnergyPlus for modeling data centers. PyDCM\ncan also be used as a standalone Gymnasium environment for demonstrating\nsustainability-focused data center control.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2023 Workshop on Tackling Climate Change with Machine\n  Learning https://www.climatechange.ai/papers/neurips2023/15. arXiv admin\n  note: substantial text overlap with arXiv:2310.03906",
    "pdf_url": "http://arxiv.org/pdf/2404.12498v1",
    "published_date": "2024-04-18 20:25:33 UTC",
    "updated_date": "2024-04-18 20:25:33 UTC"
  },
  {
    "arxiv_id": "2404.12493v1",
    "title": "EnriCo: Enriched Representation and Globally Constrained Inference for Entity and Relation Extraction",
    "authors": [
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Yann Dauxais",
      "Pierre Holat",
      "Thierry Charnois"
    ],
    "abstract": "Joint entity and relation extraction plays a pivotal role in various\napplications, notably in the construction of knowledge graphs. Despite recent\nprogress, existing approaches often fall short in two key aspects: richness of\nrepresentation and coherence in output structure. These models often rely on\nhandcrafted heuristics for computing entity and relation representations,\npotentially leading to loss of crucial information. Furthermore, they disregard\ntask and/or dataset-specific constraints, resulting in output structures that\nlack coherence. In our work, we introduce EnriCo, which mitigates these\nshortcomings. Firstly, to foster rich and expressive representation, our model\nleverage attention mechanisms that allow both entities and relations to\ndynamically determine the pertinent information required for accurate\nextraction. Secondly, we introduce a series of decoding algorithms designed to\ninfer the highest scoring solutions while adhering to task and dataset-specific\nconstraints, thus promoting structured and coherent outputs. Our model\ndemonstrates competitive performance compared to baselines when evaluated on\nJoint IE datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2404.12493v1",
    "published_date": "2024-04-18 20:15:48 UTC",
    "updated_date": "2024-04-18 20:15:48 UTC"
  },
  {
    "arxiv_id": "2404.12491v1",
    "title": "GraphER: A Structure-aware Text-to-Graph Model for Entity and Relation Extraction",
    "authors": [
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Niama El Khbir",
      "Pierre Holat",
      "Thierry Charnois"
    ],
    "abstract": "Information extraction (IE) is an important task in Natural Language\nProcessing (NLP), involving the extraction of named entities and their\nrelationships from unstructured text. In this paper, we propose a novel\napproach to this task by formulating it as graph structure learning (GSL). By\nformulating IE as GSL, we enhance the model's ability to dynamically refine and\noptimize the graph structure during the extraction process. This formulation\nallows for better interaction and structure-informed decisions for entity and\nrelation prediction, in contrast to previous models that have separate or\nuntied predictions for these tasks. When compared against state-of-the-art\nbaselines on joint entity and relation extraction benchmarks, our model,\nGraphER, achieves competitive results.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2404.12491v1",
    "published_date": "2024-04-18 20:09:37 UTC",
    "updated_date": "2024-04-18 20:09:37 UTC"
  },
  {
    "arxiv_id": "2404.12488v3",
    "title": "Global Counterfactual Directions",
    "authors": [
      "Bartlomiej Sobieski",
      "Przemysław Biecek"
    ],
    "abstract": "Despite increasing progress in development of methods for generating visual\ncounterfactual explanations, especially with the recent rise of Denoising\nDiffusion Probabilistic Models, previous works consider them as an entirely\nlocal technique. In this work, we take the first step at globalizing them.\nSpecifically, we discover that the latent space of Diffusion Autoencoders\nencodes the inference process of a given classifier in the form of global\ndirections. We propose a novel proxy-based approach that discovers two types of\nthese directions with the use of only single image in an entirely black-box\nmanner. Precisely, g-directions allow for flipping the decision of a given\nclassifier on an entire dataset of images, while h-directions further increase\nthe diversity of explanations. We refer to them in general as Global\nCounterfactual Directions (GCDs). Moreover, we show that GCDs can be naturally\ncombined with Latent Integrated Gradients resulting in a new black-box\nattribution method, while simultaneously enhancing the understanding of\ncounterfactual explanations. We validate our approach on existing benchmarks\nand show that it generalizes to real-world use-cases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12488v3",
    "published_date": "2024-04-18 20:03:56 UTC",
    "updated_date": "2025-02-05 07:46:13 UTC"
  },
  {
    "arxiv_id": "2404.12486v2",
    "title": "Follow-Me AI: Energy-Efficient User Interaction with Smart Environments",
    "authors": [
      "Alaa Saleh",
      "Praveen Kumar Donta",
      "Roberto Morabito",
      "Naser Hossein Motlagh",
      "Lauri Lovén"
    ],
    "abstract": "This article introduces Follow-Me AI, a concept designed to enhance user\ninteractions with smart environments, optimize energy use, and provide better\ncontrol over data captured by these environments. Through AI agents that\naccompany users, Follow-Me AI negotiates data management based on user consent,\naligns environmental controls as well as user communication and computes\nresources available in the environment with user preferences, and predicts user\nbehavior to proactively adjust the smart environment. The manuscript\nillustrates this concept with a detailed example of Follow-Me AI in a smart\ncampus setting, detailing the interactions with the building's management\nsystem for optimal comfort and efficiency. Finally, this article looks into the\nchallenges and opportunities related to Follow-Me AI.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12486v2",
    "published_date": "2024-04-18 20:00:25 UTC",
    "updated_date": "2024-04-22 19:20:33 UTC"
  },
  {
    "arxiv_id": "2404.12485v1",
    "title": "Contract Scheduling with Distributional and Multiple Advice",
    "authors": [
      "Spyros Angelopoulos",
      "Marcin Bienkowski",
      "Christoph Dürr",
      "Bertrand Simon"
    ],
    "abstract": "Contract scheduling is a widely studied framework for designing real-time\nsystems with interruptible capabilities. Previous work has showed that a\nprediction on the interruption time can help improve the performance of\ncontract-based systems, however it has relied on a single prediction that is\nprovided by a deterministic oracle. In this work, we introduce and study more\ngeneral and realistic learning-augmented settings in which the prediction is in\nthe form of a probability distribution, or it is given as a set of multiple\npossible interruption times. For both prediction settings, we design and\nanalyze schedules which perform optimally if the prediction is accurate, while\nsimultaneously guaranteeing the best worst-case performance if the prediction\nis adversarial. We also provide evidence that the resulting system is robust to\nprediction errors in the distributional setting. Last, we present an\nexperimental evaluation that confirms the theoretical findings, and illustrates\nthe performance improvements that can be attained in practice.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DS",
    "comment": "To appear in Proceedings of IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12485v1",
    "published_date": "2024-04-18 19:58:11 UTC",
    "updated_date": "2024-04-18 19:58:11 UTC"
  },
  {
    "arxiv_id": "2404.14432v1",
    "title": "Monitoring Critical Infrastructure Facilities During Disasters Using Large Language Models",
    "authors": [
      "Abdul Wahab Ziaullah",
      "Ferda Ofli",
      "Muhammad Imran"
    ],
    "abstract": "Critical Infrastructure Facilities (CIFs), such as healthcare and\ntransportation facilities, are vital for the functioning of a community,\nespecially during large-scale emergencies. In this paper, we explore a\npotential application of Large Language Models (LLMs) to monitor the status of\nCIFs affected by natural disasters through information disseminated in social\nmedia networks. To this end, we analyze social media data from two disaster\nevents in two different countries to identify reported impacts to CIFs as well\nas their impact severity and operational status. We employ state-of-the-art\nopen-source LLMs to perform computational tasks including retrieval,\nclassification, and inference, all in a zero-shot setting. Through extensive\nexperimentation, we report the results of these tasks using standard evaluation\nmetrics and reveal insights into the strengths and weaknesses of LLMs. We note\nthat although LLMs perform well in classification tasks, they encounter\nchallenges with inference tasks, especially when the context/prompt is complex\nand lengthy. Additionally, we outline various potential directions for future\nexploration that can be beneficial during the initial adoption phase of LLMs\nfor disaster response tasks.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted to appear at the 2024 ISCRAM conference",
    "pdf_url": "http://arxiv.org/pdf/2404.14432v1",
    "published_date": "2024-04-18 19:41:05 UTC",
    "updated_date": "2024-04-18 19:41:05 UTC"
  },
  {
    "arxiv_id": "2404.12474v2",
    "title": "Learning a Stable, Safe, Distributed Feedback Controller for a Heterogeneous Platoon of Autonomous Vehicles",
    "authors": [
      "Michael H. Shaham",
      "Taskin Padir"
    ],
    "abstract": "Platooning of autonomous vehicles has the potential to increase safety and\nfuel efficiency on highways. The goal of platooning is to have each vehicle\ndrive at a specified speed (set by the leader) while maintaining a safe\ndistance from its neighbors. Many prior works have analyzed various controllers\nfor platooning, most commonly linear feedback and distributed model predictive\ncontrollers. In this work, we introduce an algorithm for learning a stable,\nsafe, distributed controller for a heterogeneous platoon. Our algorithm relies\non recent developments in learning neural network stability certificates. We\ntrain a controller for autonomous platooning in simulation and evaluate its\nperformance on hardware with a platoon of four F1Tenth vehicles. We then\nperform further analysis in simulation with a platoon of 100 vehicles.\nExperimental results demonstrate the practicality of the algorithm and the\nlearned controller by comparing the performance of the neural network\ncontroller to linear feedback and distributed model predictive controllers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the International Symposium of Robotics Research (ISRR)\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12474v2",
    "published_date": "2024-04-18 19:11:34 UTC",
    "updated_date": "2024-10-17 18:45:57 UTC"
  },
  {
    "arxiv_id": "2406.15382v1",
    "title": "Enhancing Educational Efficiency: Generative AI Chatbots and DevOps in Education 4.0",
    "authors": [
      "Edis Mekić",
      "Mihailo Jovanović",
      "Kristijan Kuk",
      "Bojan Prlinčević",
      "Ana Savić"
    ],
    "abstract": "This research paper will bring forth the innovative pedagogical approach in\ncomputer science education, which uses a combination of methodologies borrowed\nfrom Artificial Intelligence (AI) and DevOps to enhance the learning experience\nin Content Management Systems (CMS) Development. It has been done over three\nacademic years, comparing the traditional way of teaching with the lately\nintroduced AI-supported techniques. This had three structured sprints, each one\nof them covering the major parts of the sprint: object-oriented PHP, theme\ndevelopment, and plugin development. In each sprint, the student deals with\npart of the theoretical content and part of the practical task, using ChatGPT\nas an auxiliary tool. In that sprint, the model will provide solutions in code\ndebugging and extensions of complex problems. The course includes practical\nexamples like code replication with PHP, functionality expansion of the CMS,\neven development of custom plugins, and themes. The course practice includes\nversions' control with Git repositories. Efficiency will touch the theme and\nplugin output rates during development and mobile/web application development.\nComparative analysis indicates that there is a marked increase in efficiency\nand shows effectiveness with the proposed AI- and DevOps-supported methodology.\nThe study is very informative since education in computer science and its\nlandscape change embodies an emerging technology that could have transformation\nimpacts on amplifying the potential for scalable and adaptive learning\napproaches.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15382v1",
    "published_date": "2024-04-18 18:45:39 UTC",
    "updated_date": "2024-04-18 18:45:39 UTC"
  },
  {
    "arxiv_id": "2404.12460v4",
    "title": "NLP-enabled Trajectory Map-matching in Urban Road Networks using a Transformer-based Encoder-decoder",
    "authors": [
      "Sevin Mohammadi",
      "Andrew W. Smyth"
    ],
    "abstract": "Vehicular trajectory data from geolocation telematics is vital for analyzing\nurban mobility patterns. Map-matching aligns noisy, sparsely sampled GPS\ntrajectories with digital road maps to reconstruct accurate vehicle paths.\nTraditional methods rely on geometric proximity, topology, and shortest-path\nheuristics, but they overlook two key factors: (1) drivers may prefer routes\nbased on local road characteristics rather than shortest paths, revealing\nlearnable shared preferences, and (2) GPS noise varies spatially due to\nmultipath effects. These factors can reduce the effectiveness of conventional\nmethods in complex scenarios and increase the effort required for\nheuristic-based implementations. This study introduces a data-driven, deep\nlearning-based map-matching framework, formulating the task as machine\ntranslation, inspired by NLP. Specifically, a transformer-based encoder-decoder\nmodel learns contextual representations of noisy GPS points to infer trajectory\nbehavior and road structures in an end-to-end manner. Trained on large-scale\ntrajectory data, the method improves path estimation accuracy. Experiments on\nsynthetic trajectories show that this approach outperforms conventional methods\nby integrating contextual awareness. Evaluation on real-world GPS traces from\nManhattan, New York, achieves 75% accuracy in reconstructing navigated routes.\nThese results highlight the effectiveness of transformers in capturing drivers'\ntrajectory behaviors, spatial dependencies, and noise patterns, offering a\nscalable, robust solution for map-matching. This work contributes to advancing\ntrajectory-driven foundation models for geospatial modeling and urban mobility\napplications.",
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12460v4",
    "published_date": "2024-04-18 18:39:23 UTC",
    "updated_date": "2025-03-07 20:16:10 UTC"
  },
  {
    "arxiv_id": "2404.12458v2",
    "title": "The collective use and perceptions of generative AI tools in digital humanities research: Survey-based results",
    "authors": [
      "Meredith Dedema",
      "Rongqian Ma"
    ],
    "abstract": "Generative artificial intelligence technologies have revolutionized the\nresearch landscape, with significant implications for Digital Humanities, a\nfield inherently intertwined with technological progress. This article\ninvestigates how DH scholars adopt and critically evaluate generative AI\ntechnologies such as ChatGPT in research. Drawing on 76 responses collected\nfrom an international survey study, we explored DH scholars' rationale for\nadopting or not adopting generative AI tools in research, identified the\nspecific practices of using generative AI tools to support various DH research\ntasks, and analyzed scholars' collective perceptions regarding the benefits,\nrisks, and challenges of using generative AI tools in DH research. The survey\nresults reveal two key findings: first, DH research communities hold divisive\nopinions about the value of generative AI in DH scholarship; second, scholars\nhave developed new practices and perceptions for using generative AI tools,\nwhich differ from those associated with traditional AI-based tools. Our survey\nrepresents one of the first survey-based analyses on this topic. It has the\npotential to serve as a building block for future empirical inquiries into the\nimpact of generative AI on DH scholarship.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12458v2",
    "published_date": "2024-04-18 18:33:00 UTC",
    "updated_date": "2024-10-07 18:07:54 UTC"
  },
  {
    "arxiv_id": "2404.12450v1",
    "title": "Enhancing AI Diagnostics: Autonomous Lesion Masking via Semi-Supervised Deep Learning",
    "authors": [
      "Ting-Ruen Wei",
      "Michele Hell",
      "Dang Bich Thuy Le",
      "Aren Vierra",
      "Ran Pang",
      "Mahesh Patel",
      "Young Kang",
      "Yuling Yan"
    ],
    "abstract": "This study presents an unsupervised domain adaptation method aimed at\nautonomously generating image masks outlining regions of interest (ROIs) for\ndifferentiating breast lesions in breast ultrasound (US) imaging. Our\nsemi-supervised learning approach utilizes a primitive model trained on a small\npublic breast US dataset with true annotations. This model is then iteratively\nrefined for the domain adaptation task, generating pseudo-masks for our\nprivate, unannotated breast US dataset. The dataset, twice the size of the\npublic one, exhibits considerable variability in image acquisition perspectives\nand demographic representation, posing a domain-shift challenge. Unlike typical\ndomain adversarial training, we employ downstream classification outcomes as a\nbenchmark to guide the updating of pseudo-masks in subsequent iterations. We\nfound the classification precision to be highly correlated with the\ncompleteness of the generated ROIs, which promotes the explainability of the\ndeep learning classification model. Preliminary findings demonstrate the\nefficacy and reliability of this approach in streamlining the ROI annotation\nprocess, thereby enhancing the classification and localization of breast\nlesions for more precise and interpretable diagnoses.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12450v1",
    "published_date": "2024-04-18 18:25:00 UTC",
    "updated_date": "2024-04-18 18:25:00 UTC"
  },
  {
    "arxiv_id": "2404.12444v1",
    "title": "mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?",
    "authors": [
      "Tianze Hua",
      "Tian Yun",
      "Ellie Pavlick"
    ],
    "abstract": "Many pretrained multilingual models exhibit cross-lingual transfer ability,\nwhich is often attributed to a learned language-neutral representation during\npretraining. However, it remains unclear what factors contribute to the\nlearning of a language-neutral representation, and whether the learned\nlanguage-neutral representation suffices to facilitate cross-lingual transfer.\nWe propose a synthetic task, Multilingual Othello (mOthello), as a testbed to\ndelve into these two questions. We find that: (1) models trained with naive\nmultilingual pretraining fail to learn a language-neutral representation across\nall input languages; (2) the introduction of \"anchor tokens\" (i.e., lexical\nitems that are identical across languages) helps cross-lingual representation\nalignment; and (3) the learning of a language-neutral representation alone is\nnot sufficient to facilitate cross-lingual transfer. Based on our findings, we\npropose a novel approach - multilingual pretraining with unified output space -\nthat both induces the learning of language-neutral representation and\nfacilitates cross-lingual transfer.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Findings of NAACL 2024. Project Webpage:\n  https://multilingual-othello.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2404.12444v1",
    "published_date": "2024-04-18 18:03:08 UTC",
    "updated_date": "2024-04-18 18:03:08 UTC"
  },
  {
    "arxiv_id": "2404.12390v4",
    "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
    "authors": [
      "Xingyu Fu",
      "Yushi Hu",
      "Bangzheng Li",
      "Yu Feng",
      "Haoyu Wang",
      "Xudong Lin",
      "Dan Roth",
      "Noah A. Smith",
      "Wei-Chiu Ma",
      "Ranjay Krishna"
    ],
    "abstract": "We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Multimodal Benchmark, Project Url: https://zeyofu.github.io/blink/,\n  ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12390v4",
    "published_date": "2024-04-18 17:59:54 UTC",
    "updated_date": "2024-07-03 08:44:45 UTC"
  },
  {
    "arxiv_id": "2404.12382v1",
    "title": "Lazy Diffusion Transformer for Interactive Image Editing",
    "authors": [
      "Yotam Nitzan",
      "Zongze Wu",
      "Richard Zhang",
      "Eli Shechtman",
      "Daniel Cohen-Or",
      "Taesung Park",
      "Michaël Gharbi"
    ],
    "abstract": "We introduce a novel diffusion transformer, LazyDiffusion, that generates\npartial image updates efficiently. Our approach targets interactive image\nediting applications in which, starting from a blank canvas or an image, a user\nspecifies a sequence of localized image modifications using binary masks and\ntext prompts. Our generator operates in two phases. First, a context encoder\nprocesses the current canvas and user mask to produce a compact global context\ntailored to the region to generate. Second, conditioned on this context, a\ndiffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\"\nfashion, i.e., it only generates the masked region. This contrasts with\nprevious works that either regenerate the full canvas, wasting time and\ncomputation, or confine processing to a tight rectangular crop around the mask,\nignoring the global image context altogether. Our decoder's runtime scales with\nthe mask size, which is typically small, while our encoder introduces\nnegligible overhead. We demonstrate that our approach is competitive with\nstate-of-the-art inpainting methods in terms of quality and fidelity while\nproviding a 10x speedup for typical user interactions, where the editing mask\nrepresents 10% of the image.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12382v1",
    "published_date": "2024-04-18 17:59:27 UTC",
    "updated_date": "2024-04-18 17:59:27 UTC"
  },
  {
    "arxiv_id": "2404.12378v2",
    "title": "6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction",
    "authors": [
      "Théo Gieruc",
      "Marius Kästingschäfer",
      "Sebastian Bernhard",
      "Mathieu Salzmann"
    ],
    "abstract": "Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "IV 2025. Joint first authorship. Project page:\n  https://6Img-to-3D.GitHub.io/ Code https://github.com/continental/6Img-to-3D",
    "pdf_url": "http://arxiv.org/pdf/2404.12378v2",
    "published_date": "2024-04-18 17:58:16 UTC",
    "updated_date": "2025-04-07 14:07:31 UTC"
  },
  {
    "arxiv_id": "2404.12365v1",
    "title": "When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes",
    "authors": [
      "Asaf Yehudai",
      "Elron Bendel"
    ],
    "abstract": "We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL",
    "pdf_url": "http://arxiv.org/pdf/2404.12365v1",
    "published_date": "2024-04-18 17:48:05 UTC",
    "updated_date": "2024-04-18 17:48:05 UTC"
  },
  {
    "arxiv_id": "2404.12361v2",
    "title": "Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI using Diffusion Models",
    "authors": [
      "Trevor J. Chan",
      "Chamith S. Rajapakse"
    ],
    "abstract": "Deep learning methods for accelerated MRI achieve state-of-the-art results\nbut largely ignore additional speedups possible with noncartesian sampling\ntrajectories. To address this gap, we created a generative diffusion\nmodel-based reconstruction algorithm for multi-coil highly undersampled spiral\nMRI. This model uses conditioning during training as well as frequency-based\nguidance to ensure consistency between images and measurements. Evaluated on\nretrospective data, we show high quality (structural similarity > 0.87) in\nreconstructed images with ultrafast scan times (0.02 seconds for a 2D image).\nWe use this algorithm to identify a set of optimal variable-density spiral\ntrajectories and show large improvements in image quality compared to\nconventional reconstruction using the non-uniform fast Fourier transform. By\ncombining efficient spiral sampling trajectories, multicoil imaging, and deep\nlearning reconstruction, these methods could enable the extremely high\nacceleration factors needed for real-time 3D imaging.",
    "categories": [
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12361v2",
    "published_date": "2024-04-18 17:40:23 UTC",
    "updated_date": "2024-05-10 18:47:01 UTC"
  },
  {
    "arxiv_id": "2404.12353v2",
    "title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning",
    "authors": [
      "Hang Hua",
      "Yunlong Tang",
      "Chenliang Xu",
      "Jiebo Luo"
    ],
    "abstract": "Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective training of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12353v2",
    "published_date": "2024-04-18 17:32:46 UTC",
    "updated_date": "2024-08-20 23:47:02 UTC"
  },
  {
    "arxiv_id": "2404.12349v1",
    "title": "Evaluating AI for Law: Bridging the Gap with Open-Source Solutions",
    "authors": [
      "Rohan Bhambhoria",
      "Samuel Dahan",
      "Jonathan Li",
      "Xiaodan Zhu"
    ],
    "abstract": "This study evaluates the performance of general-purpose AI, like ChatGPT, in\nlegal question-answering tasks, highlighting significant risks to legal\nprofessionals and clients. It suggests leveraging foundational models enhanced\nby domain-specific knowledge to overcome these issues. The paper advocates for\ncreating open-source legal AI systems to improve accuracy, transparency, and\nnarrative diversity, addressing general AI's shortcomings in legal contexts.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12349v1",
    "published_date": "2024-04-18 17:26:01 UTC",
    "updated_date": "2024-04-18 17:26:01 UTC"
  },
  {
    "arxiv_id": "2404.12322v2",
    "title": "Generalizable Face Landmarking Guided by Conditional Face Warping",
    "authors": [
      "Jiayi Liang",
      "Haotian Liu",
      "Hongteng Xu",
      "Dixin Luo"
    ],
    "abstract": "As a significant step for human face modeling, editing, and generation, face\nlandmarking aims at extracting facial keypoints from images. A generalizable\nface landmarker is required in practice because real-world facial images, e.g.,\nthe avatars in animations and games, are often stylized in various ways.\nHowever, achieving generalizable face landmarking is challenging due to the\ndiversity of facial styles and the scarcity of labeled stylized faces. In this\nstudy, we propose a simple but effective paradigm to learn a generalizable face\nlandmarker based on labeled real human faces and unlabeled stylized faces. Our\nmethod learns the face landmarker as the key module of a conditional face\nwarper. Given a pair of real and stylized facial images, the conditional face\nwarper predicts a warping field from the real face to the stylized one, in\nwhich the face landmarker predicts the ending points of the warping field and\nprovides us with high-quality pseudo landmarks for the corresponding stylized\nfacial images. Applying an alternating optimization strategy, we learn the face\nlandmarker to minimize $i)$ the discrepancy between the stylized faces and the\nwarped real ones and $ii)$ the prediction errors of both real and pseudo\nlandmarks. Experiments on various datasets show that our method outperforms\nexisting state-of-the-art domain adaptation methods in face landmarking tasks,\nleading to a face landmarker with better generalizability. Code is available at\nhttps://plustwo0.github.io/project-face-landmarker.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12322v2",
    "published_date": "2024-04-18 16:53:08 UTC",
    "updated_date": "2024-04-21 08:37:09 UTC"
  },
  {
    "arxiv_id": "2404.12317v4",
    "title": "Synthetic Participatory Planning of Shard Automated Electric Mobility Systems",
    "authors": [
      "Jiangbo Yu",
      "Graeme McKinley"
    ],
    "abstract": "Unleashing the synergies among rapidly evolving mobility technologies in a\nmulti-stakeholder setting presents unique challenges and opportunities for\naddressing urban transportation problems. This paper introduces a novel\nsynthetic participatory method that critically leverages large language models\n(LLMs) to create digital avatars representing diverse stakeholders to plan\nshared automated electric mobility systems (SAEMS). These calibratable agents\ncollaboratively identify objectives, envision and evaluate SAEMS alternatives,\nand strategize implementation under risks and constraints. The results of a\nMontreal case study indicate that a structured and parameterized workflow\nprovides outputs with higher controllability and comprehensiveness on an SAEMS\nplan than that generated using a single LLM-enabled expert agent. Consequently,\nthis approach provides a promising avenue for cost-efficiently improving the\ninclusivity and interpretability of multi-objective transportation planning,\nsuggesting a paradigm shift in how we envision and strategize for sustainable\ntransportation systems.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12317v4",
    "published_date": "2024-04-18 16:51:23 UTC",
    "updated_date": "2024-07-07 21:56:23 UTC"
  },
  {
    "arxiv_id": "2404.12299v1",
    "title": "Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair",
    "authors": [
      "Yusuke Sakai",
      "Mana Makinae",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ],
    "abstract": "In Simultaneous Machine Translation (SiMT) systems, training with a\nsimultaneous interpretation (SI) corpus is an effective method for achieving\nhigh-quality yet low-latency systems. However, it is very challenging to curate\nsuch a corpus due to limitations in the abilities of annotators, and hence,\nexisting SI corpora are limited. Therefore, we propose a method to convert\nexisting speech translation corpora into interpretation-style data, maintaining\nthe original word order and preserving the entire source content using Large\nLanguage Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models in\ntext-to-text and speech-to-text settings with the LLM-SI-Corpus reduces\nlatencies while maintaining the same level of quality as the models trained\nwith offline datasets. The LLM-SI-Corpus is available at\n\\url{https://github.com/yusuke1997/LLM-SI-Corpus}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12299v1",
    "published_date": "2024-04-18 16:24:12 UTC",
    "updated_date": "2024-04-18 16:24:12 UTC"
  },
  {
    "arxiv_id": "2404.12291v2",
    "title": "Augmenting emotion features in irony detection with Large language modeling",
    "authors": [
      "Yucheng Lin",
      "Yuhan Xia",
      "Yunfei Long"
    ],
    "abstract": "This study introduces a novel method for irony detection, applying Large\nLanguage Models (LLMs) with prompt-based learning to facilitate emotion-centric\ntext augmentation. Traditional irony detection techniques typically fall short\ndue to their reliance on static linguistic features and predefined knowledge\nbases, often overlooking the nuanced emotional dimensions integral to irony. In\ncontrast, our methodology augments the detection process by integrating subtle\nemotional cues, augmented through LLMs, into three benchmark pre-trained NLP\nmodels - BERT, T5, and GPT-2 - which are widely recognized as foundational in\nirony detection. We assessed our method using the SemEval-2018 Task 3 dataset\nand observed substantial enhancements in irony detection capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 3 tables, 2 figures. Accepted by the 25th Chinese Lexical\n  Semantics Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.12291v2",
    "published_date": "2024-04-18 16:11:17 UTC",
    "updated_date": "2024-04-20 01:52:29 UTC"
  },
  {
    "arxiv_id": "2404.12278v2",
    "title": "DF-DM: A foundational process model for multimodal data fusion in the artificial intelligence era",
    "authors": [
      "David Restrepo",
      "Chenwei Wu",
      "Constanza Vásquez-Venegas",
      "Luis Filipe Nakayama",
      "Leo Anthony Celi",
      "Diego M López"
    ],
    "abstract": "In the big data era, integrating diverse data modalities poses significant\nchallenges, particularly in complex fields like healthcare. This paper\nintroduces a new process model for multimodal Data Fusion for Data Mining,\nintegrating embeddings and the Cross-Industry Standard Process for Data Mining\nwith the existing Data Fusion Information Group model. Our model aims to\ndecrease computational costs, complexity, and bias while improving efficiency\nand reliability. We also propose \"disentangled dense fusion\", a novel embedding\nfusion method designed to optimize mutual information and facilitate dense\ninter-modality feature interaction, thereby minimizing redundant information.\n  We demonstrate the model's efficacy through three use cases: predicting\ndiabetic retinopathy using retinal images and patient metadata, domestic\nviolence prediction employing satellite imagery, internet, and census data, and\nidentifying clinical and demographic features from radiography images and\nclinical notes. The model achieved a Macro F1 score of 0.92 in diabetic\nretinopathy prediction, an R-squared of 0.854 and sMAPE of 24.868 in domestic\nviolence prediction, and a macro AUC of 0.92 and 0.99 for disease prediction\nand sex classification, respectively, in radiological analysis.\n  These results underscore the Data Fusion for Data Mining model's potential to\nsignificantly impact multimodal data processing, promoting its adoption in\ndiverse, resource-constrained settings.",
    "categories": [
      "cs.AI",
      "68T30",
      "I.2.0; I.3.6"
    ],
    "primary_category": "cs.AI",
    "comment": "6 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.12278v2",
    "published_date": "2024-04-18 15:52:42 UTC",
    "updated_date": "2024-06-02 16:51:46 UTC"
  },
  {
    "arxiv_id": "2404.12274v1",
    "title": "Advancing the Robustness of Large Language Models through Self-Denoised Smoothing",
    "authors": [
      "Jiabao Ji",
      "Bairu Hou",
      "Zhen Zhang",
      "Guanhua Zhang",
      "Wenqi Fan",
      "Qing Li",
      "Yang Zhang",
      "Gaowen Liu",
      "Sijia Liu",
      "Shiyu Chang"
    ],
    "abstract": "Although large language models (LLMs) have achieved significant success,\ntheir vulnerability to adversarial perturbations, including recent jailbreak\nattacks, has raised considerable concerns. However, the increasing size of\nthese models and their limited access make improving their robustness a\nchallenging task. Among various defense strategies, randomized smoothing has\nshown great potential for LLMs, as it does not require full access to the\nmodel's parameters or fine-tuning via adversarial training. However, randomized\nsmoothing involves adding noise to the input before model prediction, and the\nfinal model's robustness largely depends on the model's performance on these\nnoise corrupted data. Its effectiveness is often limited by the model's\nsub-optimal performance on noisy data. To address this issue, we propose to\nleverage the multitasking nature of LLMs to first denoise the noisy inputs and\nthen to make predictions based on these denoised versions. We call this\nprocedure self-denoised smoothing. Unlike previous denoised smoothing\ntechniques in computer vision, which require training a separate model to\nenhance the robustness of LLMs, our method offers significantly better\nefficiency and flexibility. Our experimental results indicate that our method\nsurpasses existing methods in both empirical and certified robustness in\ndefending against adversarial attacks for both downstream tasks and human\nalignments (i.e., jailbreak attacks). Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NAACL 2024. Jiabao, Bairu, Zhen, Guanhua contributed\n  equally. This is an updated version of the paper: arXiv:2307.07171",
    "pdf_url": "http://arxiv.org/pdf/2404.12274v1",
    "published_date": "2024-04-18 15:47:00 UTC",
    "updated_date": "2024-04-18 15:47:00 UTC"
  },
  {
    "arxiv_id": "2404.12273v1",
    "title": "FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom",
    "authors": [
      "Yuanqin He",
      "Yan Kang",
      "Lixin Fan",
      "Qiang Yang"
    ],
    "abstract": "Federated Learning (FL) has emerged as a promising solution for collaborative\ntraining of large language models (LLMs). However, the integration of LLMs into\nFL introduces new challenges, particularly concerning the evaluation of LLMs.\nTraditional evaluation methods that rely on labeled test sets and\nsimilarity-based metrics cover only a subset of the acceptable answers, thereby\nfailing to accurately reflect the performance of LLMs on generative tasks.\nMeanwhile, although automatic evaluation methods that leverage advanced LLMs\npresent potential, they face critical risks of data leakage due to the need to\ntransmit data to external servers and suboptimal performance on downstream\ntasks due to the lack of domain knowledge. To address these issues, we propose\na Federated Evaluation framework of Large Language Models, named FedEval-LLM,\nthat provides reliable performance measurements of LLMs on downstream tasks\nwithout the reliance on labeled test sets and external tools, thus ensuring\nstrong privacy-preserving capability. FedEval-LLM leverages a consortium of\npersonalized LLMs from participants as referees to provide domain knowledge and\ncollective evaluation capability, thus aligning to the respective downstream\ntasks and mitigating uncertainties and biases associated with a single referee.\nExperimental results demonstrate a significant improvement in the evaluation\ncapability of personalized evaluation models on downstream tasks. When applied\nto FL, these evaluation models exhibit strong agreement with human preference\nand RougeL-score on meticulously curated test sets. FedEval-LLM effectively\novercomes the limitations of traditional metrics and the reliance on external\nservices, making it a promising framework for the evaluation of LLMs within\ncollaborative training scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "In Progress",
    "pdf_url": "http://arxiv.org/pdf/2404.12273v1",
    "published_date": "2024-04-18 15:46:26 UTC",
    "updated_date": "2024-04-18 15:46:26 UTC"
  },
  {
    "arxiv_id": "2404.12272v1",
    "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
    "authors": [
      "Shreya Shankar",
      "J. D. Zamfirescu-Pereira",
      "Björn Hartmann",
      "Aditya G. Parameswaran",
      "Ian Arawjo"
    ],
    "abstract": "Due to the cumbersome nature of human evaluation and limitations of\ncode-based evaluation, Large Language Models (LLMs) are increasingly being used\nto assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply\ninherit all the problems of the LLMs they evaluate, requiring further human\nvalidation. We present a mixed-initiative approach to ``validate the\nvalidators'' -- aligning LLM-generated evaluation functions (be it prompts or\ncode) with human requirements. Our interface, EvalGen, provides automated\nassistance to users in generating evaluation criteria and implementing\nassertions. While generating candidate implementations (Python functions, LLM\ngrader prompts), EvalGen asks humans to grade a subset of LLM outputs; this\nfeedback is used to select implementations that better align with user grades.\nA qualitative study finds overall support for EvalGen but underscores the\nsubjectivity and iterative process of alignment. In particular, we identify a\nphenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs,\nbut grading outputs helps users define criteria. What is more, some criteria\nappears \\emph{dependent} on the specific LLM outputs observed (rather than\nindependent criteria that can be defined \\emph{a priori}), raising serious\nquestions for approaches that assume the independence of evaluation from\nobservation of model outputs. We present our interface and implementation\ndetails, a comparison of our algorithm with a baseline approach, and\nimplications for the design of future LLM evaluation assistants.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "16 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.12272v1",
    "published_date": "2024-04-18 15:45:27 UTC",
    "updated_date": "2024-04-18 15:45:27 UTC"
  },
  {
    "arxiv_id": "2404.12267v1",
    "title": "Physics-integrated generative modeling using attentive planar normalizing flow based variational autoencoder",
    "authors": [
      "Sheikh Waqas Akhtar"
    ],
    "abstract": "Physics-integrated generative modeling is a class of hybrid or grey-box\nmodeling in which we augment the the data-driven model with the physics\nknowledge governing the data distribution. The use of physics knowledge allows\nthe generative model to produce output in a controlled way, so that the output,\nby construction, complies with the physical laws. It imparts improved\ngeneralization ability to extrapolate beyond the training distribution as well\nas improved interpretability because the model is partly grounded in firm\ndomain knowledge. In this work, we aim to improve the fidelity of\nreconstruction and robustness to noise in the physics integrated generative\nmodel. To this end, we use variational-autoencoder as a generative model. To\nimprove the reconstruction results of the decoder, we propose to learn the\nlatent posterior distribution of both the physics as well as the trainable\ndata-driven components using planar normalizng flow. Normalizng flow based\nposterior distribution harnesses the inherent dynamical structure of the data\ndistribution, hence the learned model gets closer to the true underlying data\ndistribution. To improve the robustness of generative model against noise\ninjected in the model, we propose a modification in the encoder part of the\nnormalizing flow based VAE. We designed the encoder to incorporate scaled dot\nproduct attention based contextual information in the noisy latent vector which\nwill mitigate the adverse effect of noise in the latent vector and make the\nmodel more robust. We empirically evaluated our models on human locomotion\ndataset [33] and the results validate the efficacy of our proposed models in\nterms of improvement in reconstruction quality as well as robustness against\nnoise injected in the model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12267v1",
    "published_date": "2024-04-18 15:38:14 UTC",
    "updated_date": "2024-04-18 15:38:14 UTC"
  },
  {
    "arxiv_id": "2404.12259v1",
    "title": "Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM",
    "authors": [
      "Michelle S. Lam",
      "Janice Teoh",
      "James Landay",
      "Jeffrey Heer",
      "Michael S. Bernstein"
    ],
    "abstract": "Data analysts have long sought to turn unstructured text data into meaningful\nconcepts. Though common, topic modeling and clustering focus on lower-level\nkeywords and require significant interpretative work. We introduce concept\ninduction, a computational process that instead produces high-level concepts,\ndefined by explicit inclusion criteria, from unstructured text. For a dataset\nof toxic online comments, where a state-of-the-art BERTopic model outputs\n\"women, power, female,\" concept induction produces high-level concepts such as\n\"Criticism of traditional gender roles\" and \"Dismissal of women's concerns.\" We\npresent LLooM, a concept induction algorithm that leverages large language\nmodels to iteratively synthesize sampled text and propose human-interpretable\nconcepts of increasing generality. We then instantiate LLooM in a\nmixed-initiative text analysis tool, enabling analysts to shift their attention\nfrom interpreting topics to engaging in theory-driven analysis. Through\ntechnical evaluations and four analysis scenarios ranging from literature\nreview to content moderation, we find that LLooM's concepts improve upon the\nprior art of topic models in terms of quality and data coverage. In expert case\nstudies, LLooM helped researchers to uncover new insights even from familiar\ndatasets, for example by suggesting a previously unnoticed concept of attacks\non out-party stances in a political social media dataset.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To appear at CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12259v1",
    "published_date": "2024-04-18 15:26:02 UTC",
    "updated_date": "2024-04-18 15:26:02 UTC"
  },
  {
    "arxiv_id": "2404.12257v2",
    "title": "Food Portion Estimation via 3D Object Scaling",
    "authors": [
      "Gautham Vinod",
      "Jiangpeng He",
      "Zeman Shao",
      "Fengqing Zhu"
    ],
    "abstract": "Image-based methods to analyze food images have alleviated the user burden\nand biases associated with traditional methods. However, accurate portion\nestimation remains a major challenge due to the loss of 3D information in the\n2D representation of foods captured by smartphone cameras or wearable devices.\nIn this paper, we propose a new framework to estimate both food volume and\nenergy from 2D images by leveraging the power of 3D food models and physical\nreference in the eating scene. Our method estimates the pose of the camera and\nthe food object in the input image and recreates the eating occasion by\nrendering an image of a 3D model of the food with the estimated poses. We also\nintroduce a new dataset, SimpleFood45, which contains 2D images of 45 food\nitems and associated annotations including food volume, weight, and energy. Our\nmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,\noutperforming existing portion estimation methods. The dataset can be accessed\nat: https://lorenz.ecn.purdue.edu/~gvinod/simplefood45/ and the code can be\naccessed at: https://gitlab.com/viper-purdue/monocular-food-volume-3d",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12257v2",
    "published_date": "2024-04-18 15:23:37 UTC",
    "updated_date": "2024-10-10 20:02:07 UTC"
  },
  {
    "arxiv_id": "2404.12256v1",
    "title": "An Online Spatial-Temporal Graph Trajectory Planner for Autonomous Vehicles",
    "authors": [
      "Jilan Samiuddin",
      "Benoit Boulet",
      "Di Wu"
    ],
    "abstract": "The autonomous driving industry is expected to grow by over 20 times in the\ncoming decade and, thus, motivate researchers to delve into it. The primary\nfocus of their research is to ensure safety, comfort, and efficiency. An\nautonomous vehicle has several modules responsible for one or more of the\naforementioned items. Among these modules, the trajectory planner plays a\npivotal role in the safety of the vehicle and the comfort of its passengers.\nThe module is also responsible for respecting kinematic constraints and any\napplicable road constraints. In this paper, a novel online spatial-temporal\ngraph trajectory planner is introduced to generate safe and comfortable\ntrajectories. First, a spatial-temporal graph is constructed using the\nautonomous vehicle, its surrounding vehicles, and virtual nodes along the road\nwith respect to the vehicle itself. Next, the graph is forwarded into a\nsequential network to obtain the desired states. To support the planner, a\nsimple behavioral layer is also presented that determines kinematic constraints\nfor the planner. Furthermore, a novel potential function is also proposed to\ntrain the network. Finally, the proposed planner is tested on three different\ncomplex driving tasks, and the performance is compared with two frequently used\nmethods. The results show that the proposed planner generates safe and feasible\ntrajectories while achieving similar or longer distances in the forward\ndirection and comparable comfort ride.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "This is the accepted version and published in the \"Early Access\" area\n  of IEEE Xplore for the IEEE Transactions on Intelligent Vehicles on 16 April\n  2024. Article statistics: 11 pages, 9 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.12256v1",
    "published_date": "2024-04-18 15:22:29 UTC",
    "updated_date": "2024-04-18 15:22:29 UTC"
  },
  {
    "arxiv_id": "2404.12241v2",
    "title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons",
    "authors": [
      "Bertie Vidgen",
      "Adarsh Agrawal",
      "Ahmed M. Ahmed",
      "Victor Akinwande",
      "Namir Al-Nuaimi",
      "Najla Alfaraj",
      "Elie Alhajjar",
      "Lora Aroyo",
      "Trupti Bavalatti",
      "Max Bartolo",
      "Borhane Blili-Hamelin",
      "Kurt Bollacker",
      "Rishi Bomassani",
      "Marisa Ferrara Boston",
      "Siméon Campos",
      "Kal Chakra",
      "Canyu Chen",
      "Cody Coleman",
      "Zacharie Delpierre Coudert",
      "Leon Derczynski",
      "Debojyoti Dutta",
      "Ian Eisenberg",
      "James Ezick",
      "Heather Frase",
      "Brian Fuller",
      "Ram Gandikota",
      "Agasthya Gangavarapu",
      "Ananya Gangavarapu",
      "James Gealy",
      "Rajat Ghosh",
      "James Goel",
      "Usman Gohar",
      "Sujata Goswami",
      "Scott A. Hale",
      "Wiebke Hutiri",
      "Joseph Marvin Imperial",
      "Surgan Jandial",
      "Nick Judd",
      "Felix Juefei-Xu",
      "Foutse Khomh",
      "Bhavya Kailkhura",
      "Hannah Rose Kirk",
      "Kevin Klyman",
      "Chris Knotz",
      "Michael Kuchnik",
      "Shachi H. Kumar",
      "Srijan Kumar",
      "Chris Lengerich",
      "Bo Li",
      "Zeyi Liao",
      "Eileen Peters Long",
      "Victor Lu",
      "Sarah Luger",
      "Yifan Mai",
      "Priyanka Mary Mammen",
      "Kelvin Manyeki",
      "Sean McGregor",
      "Virendra Mehta",
      "Shafee Mohammed",
      "Emanuel Moss",
      "Lama Nachman",
      "Dinesh Jinenhally Naganna",
      "Amin Nikanjam",
      "Besmira Nushi",
      "Luis Oala",
      "Iftach Orr",
      "Alicia Parrish",
      "Cigdem Patlak",
      "William Pietri",
      "Forough Poursabzi-Sangdeh",
      "Eleonora Presani",
      "Fabrizio Puletti",
      "Paul Röttger",
      "Saurav Sahay",
      "Tim Santos",
      "Nino Scherrer",
      "Alice Schoenauer Sebag",
      "Patrick Schramowski",
      "Abolfazl Shahbazi",
      "Vin Sharma",
      "Xudong Shen",
      "Vamsi Sistla",
      "Leonard Tang",
      "Davide Testuggine",
      "Vithursan Thangarasa",
      "Elizabeth Anne Watkins",
      "Rebecca Weiss",
      "Chris Welty",
      "Tyler Wilbers",
      "Adina Williams",
      "Carole-Jean Wu",
      "Poonam Yadav",
      "Xianjun Yang",
      "Yi Zeng",
      "Wenhui Zhang",
      "Fedor Zhdanov",
      "Jiacheng Zhu",
      "Percy Liang",
      "Peter Mattson",
      "Joaquin Vanschoren"
    ],
    "abstract": "This paper introduces v0.5 of the AI Safety Benchmark, which has been created\nby the MLCommons AI Safety Working Group. The AI Safety Benchmark has been\ndesigned to assess the safety risks of AI systems that use chat-tuned language\nmodels. We introduce a principled approach to specifying and constructing the\nbenchmark, which for v0.5 covers only a single use case (an adult chatting to a\ngeneral-purpose assistant in English), and a limited set of personas (i.e.,\ntypical users, malicious users, and vulnerable users). We created a new\ntaxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark.\nWe plan to release version 1.0 of the AI Safety Benchmark by the end of 2024.\nThe v1.0 benchmark will provide meaningful insights into the safety of AI\nsystems. However, the v0.5 benchmark should not be used to assess the safety of\nAI systems. We have sought to fully document the limitations, flaws, and\nchallenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes\n(1) a principled approach to specifying and constructing the benchmark, which\ncomprises use cases, types of systems under test (SUTs), language and context,\npersonas, tests, and test items; (2) a taxonomy of 13 hazard categories with\ndefinitions and subcategories; (3) tests for seven of the hazard categories,\neach comprising a unique set of test items, i.e., prompts. There are 43,090\ntest items in total, which we created with templates; (4) a grading system for\nAI systems against the benchmark; (5) an openly available platform, and\ndownloadable tool, called ModelBench that can be used to evaluate the safety of\nAI systems on the benchmark; (6) an example evaluation report which benchmarks\nthe performance of over a dozen openly available chat-tuned language models;\n(7) a test specification for the benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12241v2",
    "published_date": "2024-04-18 15:01:00 UTC",
    "updated_date": "2024-05-13 20:46:10 UTC"
  },
  {
    "arxiv_id": "2404.12240v1",
    "title": "A Time-Inhomogeneous Markov Model for Resource Availability under Sparse Observations",
    "authors": [
      "Lukas Rottkamp",
      "Matthias Schubert"
    ],
    "abstract": "Accurate spatio-temporal information about the current situation is crucial\nfor smart city applications such as modern routing algorithms. Often, this\ninformation describes the state of stationary resources, e.g. the availability\nof parking bays, charging stations or the amount of people waiting for a\nvehicle to pick them up near a given location. To exploit this kind of\ninformation, predicting future states of the monitored resources is often\nmandatory because a resource might change its state within the time until it is\nneeded. To train an accurate predictive model, it is often not possible to\nobtain a continuous time series on the state of the resource. For example, the\ninformation might be collected from traveling agents visiting the resource with\nan irregular frequency. Thus, it is necessary to develop methods which work on\nsparse observations for training and prediction. In this paper, we propose\ntime-inhomogeneous discrete Markov models to allow accurate prediction even\nwhen the frequency of observation is very rare. Our new model is able to blend\nrecent observations with historic data and also provide useful probabilistic\nestimates for future states. Since resources availability in a city is\ntypically time-dependent, our Markov model is time-inhomogeneous and cyclic\nwithin a predefined time interval. To train our model, we propose a modified\nBaum-Welch algorithm. Evaluations on real-world datasets of parking bay\navailability show that our new method indeed yields good results compared to\nmethods being trained on complete data and non-cyclic variants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, long version of a paper published at 26th ACM SIGSPATIAL\n  International Conference on Advances in Geographic Information Systems\n  (SIGSPATIAL 2018)",
    "pdf_url": "http://arxiv.org/pdf/2404.12240v1",
    "published_date": "2024-04-18 15:00:59 UTC",
    "updated_date": "2024-04-18 15:00:59 UTC"
  },
  {
    "arxiv_id": "2404.12237v2",
    "title": "De-DSI: Decentralised Differentiable Search Index",
    "authors": [
      "Petru Neague",
      "Marcel Gregoriadis",
      "Johan Pouwelse"
    ],
    "abstract": "This study introduces De-DSI, a novel framework that fuses large language\nmodels (LLMs) with genuine decentralization for information retrieval,\nparticularly employing the differentiable search index (DSI) concept in a\ndecentralized setting. Focused on efficiently connecting novel user queries\nwith document identifiers without direct document access, De-DSI operates\nsolely on query-docid pairs. To enhance scalability, an ensemble of DSI models\nis introduced, where the dataset is partitioned into smaller shards for\nindividual model training. This approach not only maintains accuracy by\nreducing the number of data each model needs to handle but also facilitates\nscalability by aggregating outcomes from multiple models. This aggregation uses\na beam search to identify top docids and applies a softmax function for score\nnormalization, selecting documents with the highest scores for retrieval. The\ndecentralized implementation demonstrates that retrieval success is comparable\nto centralized methods, with the added benefit of the possibility of\ndistributing computational complexity across the network. This setup also\nallows for the retrieval of multimedia items through magnet links, eliminating\nthe need for platforms or intermediaries.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DC",
      "I.2.7; I.2.11; H.3.3; C.2.4"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at the 4th Workshop on Machine Learning and Systems\n  (EuroMLSys), EuroSys 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12237v2",
    "published_date": "2024-04-18 14:51:55 UTC",
    "updated_date": "2024-04-19 11:54:57 UTC"
  },
  {
    "arxiv_id": "2404.12228v3",
    "title": "CausalMed: Causality-Based Personalized Medication Recommendation Centered on Patient health state",
    "authors": [
      "Xiang Li",
      "Shunpan Liang",
      "Yu Lei",
      "Chen Li",
      "Yulei Hou",
      "Tengfei Ma"
    ],
    "abstract": "Medication recommendation systems are developed to recommend suitable\nmedications tailored to specific patient. Previous researches primarily focus\non learning medication representations, which have yielded notable advances.\nHowever, these methods are limited to capturing personalized patient\nrepresentations due to the following primary limitations: (i) unable to capture\nthe differences in the impact of diseases/procedures on patients across various\npatient health states; (ii) fail to model the direct causal relationships\nbetween medications and specific health state of patients, resulting in an\ninability to determine which specific disease each medication is treating. To\naddress these limitations, we propose CausalMed, a patient health state-centric\nmodel capable of enhancing the personalization of patient representations.\nSpecifically, CausalMed first captures the causal relationship between\ndiseases/procedures and medications through causal discovery and evaluates\ntheir causal effects. Building upon this, CausalMed focuses on analyzing the\nhealth state of patients, capturing the dynamic differences of\ndiseases/procedures in different health states of patients, and transforming\ndiseases/procedures into medications on direct causal relationships.\nUltimately, CausalMed integrates information from longitudinal visits to\nrecommend medication combinations. Extensive experiments on real-world datasets\nshow that our method learns more personalized patient representation and\noutperforms state-of-the-art models in accuracy and safety.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "CIKM 2024 Full Research Paper",
    "pdf_url": "http://arxiv.org/pdf/2404.12228v3",
    "published_date": "2024-04-18 14:44:08 UTC",
    "updated_date": "2024-07-21 03:55:46 UTC"
  },
  {
    "arxiv_id": "2404.12185v1",
    "title": "An Adaptive Metaheuristic Framework for Changing Environments",
    "authors": [
      "Bestoun S. Ahmed"
    ],
    "abstract": "The rapidly changing landscapes of modern optimization problems require\nalgorithms that can be adapted in real-time. This paper introduces an Adaptive\nMetaheuristic Framework (AMF) designed for dynamic environments. It is capable\nof intelligently adapting to changes in the problem parameters. The AMF\ncombines a dynamic representation of problems, a real-time sensing system, and\nadaptive techniques to navigate continuously changing optimization\nenvironments. Through a simulated dynamic optimization problem, the AMF's\ncapability is demonstrated to detect environmental changes and proactively\nadjust its search strategy. This framework utilizes a differential evolution\nalgorithm that is improved with an adaptation module that adjusts solutions in\nresponse to detected changes. The capability of the AMF to adjust is tested\nthrough a series of iterations, demonstrating its resilience and robustness in\nsustaining solution quality despite the problem's development. The\neffectiveness of AMF is demonstrated through a series of simulations on a\ndynamic optimization problem. Robustness and agility characterize the\nalgorithm's performance, as evidenced by the presented fitness evolution and\nsolution path visualizations. The findings show that AMF is a practical\nsolution to dynamic optimization and a major step forward in the creation of\nalgorithms that can handle the unpredictability of real-world problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in 2024 IEEE Congress on Evolutionary Computation (CEC)",
    "pdf_url": "http://arxiv.org/pdf/2404.12185v1",
    "published_date": "2024-04-18 13:47:53 UTC",
    "updated_date": "2024-04-18 13:47:53 UTC"
  },
  {
    "arxiv_id": "2404.12172v2",
    "title": "How to Benchmark Vision Foundation Models for Semantic Segmentation?",
    "authors": [
      "Tommie Kerssies",
      "Daan de Geus",
      "Gijs Dubbelman"
    ],
    "abstract": "Recent vision foundation models (VFMs) have demonstrated proficiency in\nvarious tasks but require supervised fine-tuning to perform the task of\nsemantic segmentation effectively. Benchmarking their performance is essential\nfor selecting current models and guiding future model developments for this\ntask. The lack of a standardized benchmark complicates comparisons. Therefore,\nthe primary objective of this paper is to study how VFMs should be benchmarked\nfor semantic segmentation. To do so, various VFMs are fine-tuned under various\nsettings, and the impact of individual settings on the performance ranking and\ntraining time is assessed. Based on the results, the recommendation is to\nfine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear\ndecoder, as these settings are representative of using a larger model, more\nadvanced decoder and smaller patch size, while reducing training time by more\nthan 13 times. Using multiple datasets for training and evaluation is also\nrecommended, as the performance ranking across datasets and domain shifts\nvaries. Linear probing, a common practice for some VFMs, is not recommended, as\nit is not representative of end-to-end fine-tuning. The benchmarking setup\nrecommended in this paper enables a performance analysis of VFMs for semantic\nsegmentation. The findings of such an analysis reveal that pretraining with\npromptable segmentation is not beneficial, whereas masked image modeling (MIM)\nwith abstract representations is crucial, even more important than the type of\nsupervision used. The code for efficiently fine-tuning VFMs for semantic\nsegmentation can be accessed through the project page at:\nhttps://tue-mps.github.io/benchmark-vfm-ss/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024 Workshop Proceedings for the Second Workshop on Foundation\n  Models. v2 updates image normalization preprocessing for linear probing with\n  EVA-02, EVA-02-CLIP, SigLIP, DFN (the impact on end-to-end fine-tuning is\n  negligible; no changes made)",
    "pdf_url": "http://arxiv.org/pdf/2404.12172v2",
    "published_date": "2024-04-18 13:27:29 UTC",
    "updated_date": "2024-06-10 10:05:01 UTC"
  },
  {
    "arxiv_id": "2404.12168v1",
    "title": "Real-World Efficient Blind Motion Deblurring via Blur Pixel Discretization",
    "authors": [
      "Insoo Kim",
      "Jae Seok Choi",
      "Geonseok Seo",
      "Kinam Kwon",
      "Jinwoo Shin",
      "Hyong-Euk Lee"
    ],
    "abstract": "As recent advances in mobile camera technology have enabled the capability to\ncapture high-resolution images, such as 4K images, the demand for an efficient\ndeblurring model handling large motion has increased. In this paper, we\ndiscover that the image residual errors, i.e., blur-sharp pixel differences,\ncan be grouped into some categories according to their motion blur type and how\ncomplex their neighboring pixels are. Inspired by this, we decompose the\ndeblurring (regression) task into blur pixel discretization (pixel-level blur\nclassification) and discrete-to-continuous conversion (regression with blur\nclass map) tasks. Specifically, we generate the discretized image residual\nerrors by identifying the blur pixels and then transform them to a continuous\nform, which is computationally more efficient than naively solving the original\nregression problem with continuous values. Here, we found that the\ndiscretization result, i.e., blur segmentation map, remarkably exhibits visual\nsimilarity with the image residual errors. As a result, our efficient model\nshows comparable performance to state-of-the-art methods in realistic\nbenchmarks, while our method is up to 10 times computationally more efficient.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2024 Camera-Ready",
    "pdf_url": "http://arxiv.org/pdf/2404.12168v1",
    "published_date": "2024-04-18 13:22:56 UTC",
    "updated_date": "2024-04-18 13:22:56 UTC"
  },
  {
    "arxiv_id": "2405.04540v2",
    "title": "Is artificial consciousness achievable? Lessons from the human brain",
    "authors": [
      "Michele Farisco",
      "Kathinka Evers",
      "Jean-Pierre Changeux"
    ],
    "abstract": "We here analyse the question of developing artificial consciousness from an\nevolutionary perspective, taking the evolution of the human brain and its\nrelation with consciousness as a reference model. This kind of analysis reveals\nseveral structural and functional features of the human brain that appear to be\nkey for reaching human-like complex conscious experience and that current\nresearch on Artificial Intelligence (AI) should take into account in its\nattempt to develop systems capable of conscious processing. We argue that, even\nif AI is limited in its ability to emulate human consciousness for both\nintrinsic (structural and architectural) and extrinsic (related to the current\nstage of scientific and technological knowledge) reasons, taking inspiration\nfrom those characteristics of the brain that make conscious processing possible\nand/or modulate it, is a potentially promising strategy towards developing\nconscious AI. Also, it is theoretically possible that AI research can develop\npartial or potentially alternative forms of consciousness that is qualitatively\ndifferent from the human, and that may be either more or less sophisticated\ndepending on the perspectives. Therefore, we recommend neuroscience-inspired\ncaution in talking about artificial consciousness: since the use of the same\nword consciousness for humans and AI becomes ambiguous and potentially\nmisleading, we propose to clearly specify what is common and what differs in AI\nconscious processing from full human conscious experience.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.04540v2",
    "published_date": "2024-04-18 12:59:44 UTC",
    "updated_date": "2024-07-29 17:55:17 UTC"
  },
  {
    "arxiv_id": "2404.12149v5",
    "title": "AccidentBlip: Agent of Accident Warning based on MA-former",
    "authors": [
      "Yihua Shao",
      "Yeling Xu",
      "Xinwei Long",
      "Siyu Chen",
      "Ziyang Yan",
      "Yang Yang",
      "Haoting Liu",
      "Yan Wang",
      "Hao Tang",
      "Zhen Lei"
    ],
    "abstract": "In complex transportation systems, accurately sensing the surrounding\nenvironment and predicting the risk of potential accidents is crucial. Most\nexisting accident prediction methods are based on temporal neural networks,\nsuch as RNN and LSTM. Recent multimodal fusion approaches improve vehicle\nlocalization through 3D target detection and assess potential risks by\ncalculating inter-vehicle distances. However, these temporal networks and\nmultimodal fusion methods suffer from limited detection robustness and high\neconomic costs. To address these challenges, we propose AccidentBlip, a\nvision-only framework that employs our self-designed Motion Accident\nTransformer (MA-former) to process each frame of video. Unlike conventional\nself-attention mechanisms, MA-former replaces Q-former's self-attention with\ntemporal attention, allowing the query corresponding to the previous frame to\ngenerate the query input for the next frame. Additionally, we introduce a\nresidual module connection between queries of consecutive frames to enhance the\nmodel's temporal processing capabilities. For complex V2V and V2X scenarios,\nAccidentBlip adapts by concatenating queries from multiple cameras, effectively\ncapturing spatial and temporal relationships. In particular, AccidentBlip\nachieves SOTA performance in both accident detection and prediction tasks on\nthe DeepAccident dataset. It also outperforms current SOTA methods in V2V and\nV2X scenarios, demonstrating a superior capability to understand complex\nreal-world environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12149v5",
    "published_date": "2024-04-18 12:54:25 UTC",
    "updated_date": "2025-01-28 02:33:40 UTC"
  },
  {
    "arxiv_id": "2404.12145v1",
    "title": "From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using Multisense Consistency",
    "authors": [
      "Xenia Ohmer",
      "Elia Bruni",
      "Dieuwke Hupkes"
    ],
    "abstract": "The staggering pace with which the capabilities of large language models\n(LLMs) are increasing, as measured by a range of commonly used natural language\nunderstanding (NLU) benchmarks, raises many questions regarding what\n\"understanding\" means for a language model and how it compares to human\nunderstanding. This is especially true since many LLMs are exclusively trained\non text, casting doubt on whether their stellar benchmark performances are\nreflective of a true understanding of the problems represented by these\nbenchmarks, or whether LLMs simply excel at uttering textual forms that\ncorrelate with what someone who understands the problem would say. In this\nphilosophically inspired work, we aim to create some separation between form\nand meaning, with a series of tests that leverage the idea that world\nunderstanding should be consistent across presentational modes - inspired by\nFregean senses - of the same meaning. Specifically, we focus on consistency\nacross languages as well as paraphrases. Taking GPT-3.5 as our object of study,\nwe evaluate multisense consistency across five different languages and various\ntasks. We start the evaluation in a controlled setting, asking the model for\nsimple facts, and then proceed with an evaluation on four popular NLU\nbenchmarks. We find that the model's multisense consistency is lacking and run\nseveral follow-up analyses to verify that this lack of consistency is due to a\nsense-dependent task understanding. We conclude that, in this aspect, the\nunderstanding of LLMs is still quite far from being consistent and human-like,\nand deliberate on how this impacts their utility in the context of learning\nabout human language and understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12145v1",
    "published_date": "2024-04-18 12:48:17 UTC",
    "updated_date": "2024-04-18 12:48:17 UTC"
  },
  {
    "arxiv_id": "2404.12143v1",
    "title": "The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action",
    "authors": [
      "Hilde Weerts",
      "Raphaële Xenidis",
      "Fabien Tarissan",
      "Henrik Palmer Olsen",
      "Mykola Pechenizkiy"
    ],
    "abstract": "Various metrics and interventions have been developed to identify and\nmitigate unfair outputs of machine learning systems. While individuals and\norganizations have an obligation to avoid discrimination, the use of\nfairness-aware machine learning interventions has also been described as\namounting to 'algorithmic positive action' under European Union (EU)\nnon-discrimination law. As the Court of Justice of the European Union has been\nstrict when it comes to assessing the lawfulness of positive action, this would\nimpose a significant legal burden on those wishing to implement fair-ml\ninterventions. In this paper, we propose that algorithmic fairness\ninterventions often should be interpreted as a means to prevent discrimination,\nrather than a measure of positive action. Specifically, we suggest that this\ncategory mistake can often be attributed to neutrality fallacies: faulty\nassumptions regarding the neutrality of fairness-aware algorithmic\ndecision-making. Our findings raise the question of whether a negative\nobligation to refrain from discrimination is sufficient in the context of\nalgorithmic decision-making. Consequently, we suggest moving away from a duty\nto 'not do harm' towards a positive obligation to actively 'do no harm' as a\nmore adequate framework for algorithmic decision-making and fair\nml-interventions.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12143v1",
    "published_date": "2024-04-18 12:44:35 UTC",
    "updated_date": "2024-04-18 12:44:35 UTC"
  },
  {
    "arxiv_id": "2404.12138v2",
    "title": "Character is Destiny: Can Role-Playing Language Agents Make Persona-Driven Decisions?",
    "authors": [
      "Rui Xu",
      "Xintao Wang",
      "Jiangjie Chen",
      "Siyu Yuan",
      "Xinfeng Yuan",
      "Jiaqing Liang",
      "Zulong Chen",
      "Xiaoqing Dong",
      "Yanghua Xiao"
    ],
    "abstract": "Can Large Language Models (LLMs) simulate humans in making important\ndecisions? Recent research has unveiled the potential of using LLMs to develop\nrole-playing language agents (RPLAs), mimicking mainly the knowledge and tones\nof various characters. However, imitative decision-making necessitates a more\nnuanced understanding of personas. In this paper, we benchmark the ability of\nLLMs in persona-driven decision-making. Specifically, we investigate whether\nLLMs can predict characters' decisions provided by the preceding stories in\nhigh-quality novels. Leveraging character analyses written by literary experts,\nwe construct a dataset LIFECHOICE comprising 1,462 characters' decision points\nfrom 388 books. Then, we conduct comprehensive experiments on LIFECHOICE, with\nvarious LLMs and RPLA methodologies. The results demonstrate that\nstate-of-the-art LLMs exhibit promising capabilities in this task, yet\nsubstantial room for improvement remains. Hence, we further propose the CHARMAP\nmethod, which adopts persona-based memory retrieval and significantly advances\nRPLAs on this task, achieving 5.03% increase in accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12138v2",
    "published_date": "2024-04-18 12:40:59 UTC",
    "updated_date": "2024-11-18 11:29:47 UTC"
  },
  {
    "arxiv_id": "2404.12134v1",
    "title": "Warped Time Series Anomaly Detection",
    "authors": [
      "Charlotte Lacoquelle",
      "Xavier Pucel",
      "Louise Travé-Massuyès",
      "Axel Reymonet",
      "Benoît Enaux"
    ],
    "abstract": "This paper addresses the problem of detecting time series outliers, focusing\non systems with repetitive behavior, such as industrial robots operating on\nproduction lines.Notable challenges arise from the fact that a task performed\nmultiple times may exhibit different duration in each repetition and that the\ntime series reported by the sensors are irregularly sampled because of data\ngaps. The anomaly detection approach presented in this paper consists of three\nstages.The first stage identifies the repetitive cycles in the lengthy time\nseries and segments them into individual time series corresponding to one task\ncycle, while accounting for possible temporal distortions.The second stage\ncomputes a prototype for the cycles using a GPU-based barycenter algorithm,\nspecifically tailored for very large time series.The third stage uses the\nprototype to detect abnormal cycles by computing an anomaly score for each\ncycle.The overall approach, named WarpEd Time Series ANomaly Detection\n(WETSAND), makes use of the Dynamic Time Warping algorithm and its variants\nbecause they are suited to the distorted nature of the time series.The\nexperiments show that \\wetsand scales to large signals, computes human-friendly\nprototypes, works with very little data, and outperforms some general purpose\nanomaly detection approaches such as autoencoders.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12134v1",
    "published_date": "2024-04-18 12:35:24 UTC",
    "updated_date": "2024-04-18 12:35:24 UTC"
  },
  {
    "arxiv_id": "2404.12127v2",
    "title": "Personalized Forgetting Mechanism with Concept-Driven Knowledge Tracing",
    "authors": [
      "Shanshan Wang",
      "Ying Hu",
      "Xun Yang",
      "Zhongzhou Zhang",
      "Keyang Wang",
      "Xingyi Zhang"
    ],
    "abstract": "Knowledge Tracing (KT) aims to trace changes in students' knowledge states\nthroughout their entire learning process by analyzing their historical learning\ndata and predicting their future learning performance. Existing forgetting\ncurve theory based knowledge tracing models only consider the general\nforgetting caused by time intervals, ignoring the individualization of students\nand the causal relationship of the forgetting process. To address these\nproblems, we propose a Concept-driven Personalized Forgetting knowledge tracing\nmodel (CPF) which integrates hierarchical relationships between knowledge\nconcepts and incorporates students' personalized cognitive abilities. First, we\nintegrate the students' personalized capabilities into both the learning and\nforgetting processes to explicitly distinguish students' individual learning\ngains and forgetting rates according to their cognitive abilities. Second, we\ntake into account the hierarchical relationships between knowledge points and\ndesign a precursor-successor knowledge concept matrix to simulate the causal\nrelationship in the forgetting process, while also integrating the potential\nimpact of forgetting prior knowledge points on subsequent ones. The proposed\npersonalized forgetting mechanism can not only be applied to the learning of\nspecifc knowledge concepts but also the life-long learning process. Extensive\nexperimental results on three public datasets show that our CPF outperforms\ncurrent forgetting curve theory based methods in predicting student\nperformance, demonstrating CPF can better simulate changes in students'\nknowledge status through the personalized forgetting mechanism.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2404.12127v2",
    "published_date": "2024-04-18 12:28:50 UTC",
    "updated_date": "2024-04-25 13:03:44 UTC"
  },
  {
    "arxiv_id": "2404.12125v1",
    "title": "Intelligence Education made in Europe",
    "authors": [
      "Lars Berger",
      "Uwe M. Borghoff",
      "Gerhard Conrad",
      "Stefan Pickl"
    ],
    "abstract": "Global conflicts and trouble spots have thrown the world into turmoil.\nIntelligence services have never been as necessary as they are today when it\ncomes to providing political decision-makers with concrete, accurate, and\nup-to-date decision-making knowledge. This requires a common co-operation, a\ncommon working language and a common understanding of each other. The best way\nto create this \"intelligence community\" is through a harmonized intelligence\neducation.\n  In this paper, we show how joint intelligence education can succeed. We draw\non the experience of Germany, where all intelligence services and the\nBundeswehr are academically educated together in a single degree program that\nlays the foundations for a common working language. We also show how these\nexperiences have been successfully transferred to a European level, namely to\nICE, the Intelligence College in Europe. Our experience has shown that three\naspects are particularly important: firstly, interdisciplinarity or better,\ntransdisciplinarity, secondly, the integration of IT knowhow and thirdly, the\ndevelopment and learning of methodological skills. Using the example of the\ncyber intelligence module with a special focus on data-driven decision support,\nadditionally with its many points of reference to numerous other academic\nmodules, we show how the specific analytic methodology presented is embedded in\nour specific European teaching context.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "16 pages, 2 figures. No potential conflict of interest was reported\n  by the authors",
    "pdf_url": "http://arxiv.org/pdf/2404.12125v1",
    "published_date": "2024-04-18 12:25:46 UTC",
    "updated_date": "2024-04-18 12:25:46 UTC"
  },
  {
    "arxiv_id": "2404.12120v2",
    "title": "Fortify the Guardian, Not the Treasure: Resilient Adversarial Detectors",
    "authors": [
      "Raz Lapid",
      "Almog Dubin",
      "Moshe Sipper"
    ],
    "abstract": "This paper presents RADAR-Robust Adversarial Detection via Adversarial\nRetraining-an approach designed to enhance the robustness of adversarial\ndetectors against adaptive attacks, while maintaining classifier performance.\nAn adaptive attack is one where the attacker is aware of the defenses and\nadapts their strategy accordingly. Our proposed method leverages adversarial\ntraining to reinforce the ability to detect attacks, without compromising clean\naccuracy. During the training phase, we integrate into the dataset adversarial\nexamples, which were optimized to fool both the classifier and the adversarial\ndetector, enabling the adversarial detector to learn and adapt to potential\nattack scenarios. Experimental evaluations on the CIFAR-10 and SVHN datasets\ndemonstrate that our proposed algorithm significantly improves a detector's\nability to accurately identify adaptive adversarial attacks -- without\nsacrificing clean accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12120v2",
    "published_date": "2024-04-18 12:13:09 UTC",
    "updated_date": "2024-06-30 15:39:53 UTC"
  },
  {
    "arxiv_id": "2404.12090v3",
    "title": "X-Light: Cross-City Traffic Signal Control Using Transformer on Transformer as Meta Multi-Agent Reinforcement Learner",
    "authors": [
      "Haoyuan Jiang",
      "Ziyue Li",
      "Hua Wei",
      "Xuantang Xiong",
      "Jingqing Ruan",
      "Jiaming Lu",
      "Hangyu Mao",
      "Rui Zhao"
    ],
    "abstract": "The effectiveness of traffic light control has been significantly improved by\ncurrent reinforcement learning-based approaches via better cooperation among\nmultiple traffic lights. However, a persisting issue remains: how to obtain a\nmulti-agent traffic signal control algorithm with remarkable transferability\nacross diverse cities? In this paper, we propose a Transformer on Transformer\n(TonT) model for cross-city meta multi-agent traffic signal control, named as\nX-Light: We input the full Markov Decision Process trajectories, and the Lower\nTransformer aggregates the states, actions, rewards among the target\nintersection and its neighbors within a city, and the Upper Transformer learns\nthe general decision trajectories across different cities. This dual-level\napproach bolsters the model's robust generalization and transferability.\nNotably, when directly transferring to unseen scenarios, ours surpasses all\nbaseline methods with +7.91% on average, and even +16.3% in some cases,\nyielding the best results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12090v3",
    "published_date": "2024-04-18 11:17:58 UTC",
    "updated_date": "2024-06-17 09:02:50 UTC"
  },
  {
    "arxiv_id": "2404.12077v1",
    "title": "TIMIT Speaker Profiling: A Comparison of Multi-task learning and Single-task learning Approaches",
    "authors": [
      "Rong Wang",
      "Kun Sun"
    ],
    "abstract": "This study employs deep learning techniques to explore four speaker profiling\ntasks on the TIMIT dataset, namely gender classification, accent\nclassification, age estimation, and speaker identification, highlighting the\npotential and challenges of multi-task learning versus single-task models. The\nmotivation for this research is twofold: firstly, to empirically assess the\nadvantages and drawbacks of multi-task learning over single-task models in the\ncontext of speaker profiling; secondly, to emphasize the undiminished\nsignificance of skillful feature engineering for speaker recognition tasks. The\nfindings reveal challenges in accent classification, and multi-task learning is\nfound advantageous for tasks of similar complexity. Non-sequential features are\nfavored for speaker recognition, but sequential ones can serve as starting\npoints for complex models. The study underscores the necessity of meticulous\nexperimentation and parameter tuning for deep learning models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12077v1",
    "published_date": "2024-04-18 10:59:54 UTC",
    "updated_date": "2024-04-18 10:59:54 UTC"
  },
  {
    "arxiv_id": "2404.12076v1",
    "title": "Evolutionary Multi-Objective Optimisation for Fairness-Aware Self Adjusting Memory Classifiers in Data Streams",
    "authors": [
      "Pivithuru Thejan Amarasinghe",
      "Diem Pham",
      "Binh Tran",
      "Su Nguyen",
      "Yuan Sun",
      "Damminda Alahakoon"
    ],
    "abstract": "This paper introduces a novel approach, evolutionary multi-objective\noptimisation for fairness-aware self-adjusting memory classifiers, designed to\nenhance fairness in machine learning algorithms applied to data stream\nclassification. With the growing concern over discrimination in algorithmic\ndecision-making, particularly in dynamic data stream environments, there is a\nneed for methods that ensure fair treatment of individuals across sensitive\nattributes like race or gender. The proposed approach addresses this challenge\nby integrating the strengths of the self-adjusting memory K-Nearest-Neighbour\nalgorithm with evolutionary multi-objective optimisation. This combination\nallows the new approach to efficiently manage concept drift in streaming data\nand leverage the flexibility of evolutionary multi-objective optimisation to\nmaximise accuracy and minimise discrimination simultaneously. We demonstrate\nthe effectiveness of the proposed approach through extensive experiments on\nvarious datasets, comparing its performance against several baseline methods in\nterms of accuracy and fairness metrics. Our results show that the proposed\napproach maintains competitive accuracy and significantly reduces\ndiscrimination, highlighting its potential as a robust solution for\nfairness-aware data stream classification. Further analyses also confirm the\neffectiveness of the strategies to trigger evolutionary multi-objective\noptimisation and adapt classifiers in the proposed approach.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by GECCO 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12076v1",
    "published_date": "2024-04-18 10:59:04 UTC",
    "updated_date": "2024-04-18 10:59:04 UTC"
  },
  {
    "arxiv_id": "2404.12065v2",
    "title": "RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models",
    "authors": [
      "M. Abdul Khaliq",
      "P. Chang",
      "M. Ma",
      "B. Pflugfelder",
      "F. Miletić"
    ],
    "abstract": "The escalating challenge of misinformation, particularly in political\ndiscourse, requires advanced fact-checking solutions; this is even clearer in\nthe more complex scenario of multimodal claims. We tackle this issue using a\nmultimodal large language model in conjunction with retrieval-augmented\ngeneration (RAG), and introduce two novel reasoning techniques: Chain of RAG\n(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by\nextracting both textual and image content, retrieving external information, and\nreasoning subsequent questions to be answered based on prior evidence. We\nachieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique\nby 0.14 points. Human evaluation confirms that the vast majority of our\ngenerated fact-check explanations contain all information from gold standard\ndata.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, submitted to ACL Rolling Review June 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12065v2",
    "published_date": "2024-04-18 10:25:42 UTC",
    "updated_date": "2024-07-11 20:16:09 UTC"
  },
  {
    "arxiv_id": "2404.12056v1",
    "title": "Deconstructing Human-AI Collaboration: Agency, Interaction, and Adaptation",
    "authors": [
      "Steffen Holter",
      "Mennatallah El-Assady"
    ],
    "abstract": "As full AI-based automation remains out of reach in most real-world\napplications, the focus has instead shifted to leveraging the strengths of both\nhuman and AI agents, creating effective collaborative systems. The rapid\nadvances in this area have yielded increasingly more complex systems and\nframeworks, while the nuance of their characterization has gotten more vague.\nSimilarly, the existing conceptual models no longer capture the elaborate\nprocesses of these systems nor describe the entire scope of their collaboration\nparadigms. In this paper, we propose a new unified set of dimensions through\nwhich to analyze and describe human-AI systems. Our conceptual model is\ncentered around three high-level aspects - agency, interaction, and adaptation\n- and is developed through a multi-step process. Firstly, an initial design\nspace is proposed by surveying the literature and consolidating existing\ndefinitions and conceptual frameworks. Secondly, this model is iteratively\nrefined and validated by conducting semi-structured interviews with nine\nresearchers in this field. Lastly, to illustrate the applicability of our\ndesign space, we utilize it to provide a structured description of selected\nhuman-AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 4 figures. Accepted to Proceedings of EuroVis 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12056v1",
    "published_date": "2024-04-18 10:12:18 UTC",
    "updated_date": "2024-04-18 10:12:18 UTC"
  },
  {
    "arxiv_id": "2404.12045v2",
    "title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
    "authors": [
      "Jiaqi Li",
      "Xiaobo Wang",
      "Wentao Ding",
      "Zihao Wang",
      "Yipeng Kang",
      "Zixia Jia",
      "Zilong Zheng"
    ],
    "abstract": "We introduce an innovative RAG-based framework with an ever-improving memory.\nInspired by humans'pedagogical process, RAM utilizes recursively\nreasoning-based retrieval and experience reflections to continually update the\nmemory and learn from users' communicative feedback, namely communicative\nlearning. Extensive experiments with both simulated and real users demonstrate\nsignificant improvements over traditional RAG and self-knowledge methods,\nparticularly excelling in handling false premise and multi-hop questions.\nFurthermore, RAM exhibits promising adaptability to various feedback and\nretrieval methods, showcasing its potential for advancing AI capabilities in\ndynamic knowledge acquisition and lifelong learning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12045v2",
    "published_date": "2024-04-18 09:58:51 UTC",
    "updated_date": "2024-07-05 04:57:54 UTC"
  },
  {
    "arxiv_id": "2404.12041v2",
    "title": "Can We Catch the Elephant? A Survey of the Evolvement of Hallucination Evaluation on Natural Language Generation",
    "authors": [
      "Siya Qi",
      "Yulan He",
      "Zheng Yuan"
    ],
    "abstract": "Hallucination in Natural Language Generation (NLG) is like the elephant in\nthe room, obvious but often overlooked until recent achievements significantly\nimproved the fluency and grammaticality of generated text. As the capabilities\nof text generation models have improved, researchers have begun to pay more\nattention to the phenomenon of hallucination. Despite significant progress in\nthis field in recent years, the evaluation system for hallucination is complex\nand diverse, lacking clear organization. We are the first to comprehensively\nsurvey how various evaluation methods have evolved with the development of text\ngeneration models from three dimensions, including hallucinated fact\ngranularity, evaluator design principles, and assessment facets. This survey\naims to help researchers identify current limitations in hallucination\nevaluation and highlight future research directions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12041v2",
    "published_date": "2024-04-18 09:52:18 UTC",
    "updated_date": "2024-06-15 22:57:20 UTC"
  },
  {
    "arxiv_id": "2404.16866v4",
    "title": "Annotation-guided Protein Design with Multi-Level Domain Alignment",
    "authors": [
      "Chaohao Yuan",
      "Songyou Li",
      "Geyan Ye",
      "Yikun Zhang",
      "Long-Kai Huang",
      "Wenbing Huang",
      "Wei Liu",
      "Jianhua Yao",
      "Yu Rong"
    ],
    "abstract": "The core challenge of de novo protein design lies in creating proteins with\nspecific functions or properties, guided by certain conditions. Current models\nexplore to generate protein using structural and evolutionary guidance, which\nonly provide indirect conditions concerning functions and properties. However,\ntextual annotations of proteins, especially the annotations for protein\ndomains, which directly describe the protein's high-level functionalities,\nproperties, and their correlation with target amino acid sequences, remain\nunexplored in the context of protein design tasks. In this paper, we propose\nProtein-Annotation Alignment Generation, PAAG, a multi-modality protein design\nframework that integrates the textual annotations extracted from protein\ndatabase for controllable generation in sequence space. Specifically, within a\nmulti-level alignment module, PAAG can explicitly generate proteins containing\nspecific domains conditioned on the corresponding domain annotations, and can\neven design novel proteins with flexible combinations of different kinds of\nannotations. Our experimental results underscore the superiority of the aligned\nprotein representations from PAAG over 7 prediction tasks. Furthermore, PAAG\ndemonstrates a significant increase in generation success rate (24.7% vs 4.7%\nin zinc finger, and 54.3% vs 22.0% in the immunoglobulin domain) in comparison\nto the existing model. We anticipate that PAAG will broaden the horizons of\nprotein design by leveraging the knowledge from between textual annotation and\nproteins.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "Accepted by KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.16866v4",
    "published_date": "2024-04-18 09:37:54 UTC",
    "updated_date": "2024-12-12 07:05:53 UTC"
  },
  {
    "arxiv_id": "2404.12030v1",
    "title": "Mapping back and forth between model predictive control and neural networks",
    "authors": [
      "Ross Drummond",
      "Pablo R Baldivieso-Monasterios",
      "Giorgio Valmorbida"
    ],
    "abstract": "Model predictive control (MPC) for linear systems with quadratic costs and\nlinear constraints is shown to admit an exact representation as an implicit\nneural network. A method to \"unravel\" the implicit neural network of MPC into\nan explicit one is also introduced. As well as building links between\nmodel-based and data-driven control, these results emphasize the capability of\nimplicit neural networks for representing solutions of optimisation problems,\nas such problems are themselves implicitly defined functions.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.12030v1",
    "published_date": "2024-04-18 09:29:08 UTC",
    "updated_date": "2024-04-18 09:29:08 UTC"
  },
  {
    "arxiv_id": "2404.12023v1",
    "title": "Context-Aware Orchestration of Energy-Efficient Gossip Learning Schemes",
    "authors": [
      "Mina Aghaei Dinani",
      "Adrian Holzer",
      "Hung Nguyen",
      "Marco Ajmone Marsan",
      "Gianluca Rizzo"
    ],
    "abstract": "Fully distributed learning schemes such as Gossip Learning (GL) are gaining\nmomentum due to their scalability and effectiveness even in dynamic settings.\nHowever, they often imply a high utilization of communication and computing\nresources, whose energy footprint may jeopardize the learning process,\nparticularly on battery-operated IoT devices. To address this issue, we present\nOptimized Gossip Learning (OGL)}, a distributed training approach based on the\ncombination of GL with adaptive optimization of the learning process, which\nallows for achieving a target accuracy while minimizing the energy consumption\nof the learning process. We propose a data-driven approach to OGL management\nthat relies on optimizing in real-time for each node the number of training\nepochs and the choice of which model to exchange with neighbors based on\npatterns of node contacts, models' quality, and available resources at each\nnode. Our approach employs a DNN model for dynamic tuning of the aforementioned\nparameters, trained by an infrastructure-based orchestrator function. We\nperformed our assessments on two different datasets, leveraging time-varying\nrandom graphs and a measurement-based dynamic urban scenario. Results suggest\nthat our approach is highly efficient and effective in a broad spectrum of\nnetwork scenarios.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.NI",
    "comment": "IEEE AIIOT 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12023v1",
    "published_date": "2024-04-18 09:17:46 UTC",
    "updated_date": "2024-04-18 09:17:46 UTC"
  },
  {
    "arxiv_id": "2404.12010v1",
    "title": "ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused with High-Quality Lexical and Syntactic Diversity",
    "authors": [
      "Lasal Jayawardena",
      "Prasan Yapa"
    ],
    "abstract": "Paraphrase generation is a pivotal task in natural language processing (NLP).\nExisting datasets in the domain lack syntactic and lexical diversity, resulting\nin paraphrases that closely resemble the source sentences. Moreover, these\ndatasets often contain hate speech and noise, and may unintentionally include\nnon-English language sentences. This research introduces ParaFusion, a\nlarge-scale, high-quality English paraphrase dataset developed using Large\nLanguage Models (LLM) to address these challenges. ParaFusion augments existing\ndatasets with high-quality data, significantly enhancing both lexical and\nsyntactic diversity while maintaining close semantic similarity. It also\nmitigates the presence of hate speech and reduces noise, ensuring a cleaner and\nmore focused English dataset. Results show that ParaFusion offers at least a\n25% improvement in both syntactic and lexical diversity, measured across\nseveral metrics for each data source. The paper also aims to set a gold\nstandard for paraphrase evaluation as it contains one of the most comprehensive\nevaluation strategies to date. The results underscore the potential of\nParaFusion as a valuable resource for improving NLP applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12010v1",
    "published_date": "2024-04-18 09:02:45 UTC",
    "updated_date": "2024-04-18 09:02:45 UTC"
  },
  {
    "arxiv_id": "2404.12008v6",
    "title": "How Do Recommendation Models Amplify Popularity Bias? An Analysis from the Spectral Perspective",
    "authors": [
      "Siyi Lin",
      "Chongming Gao",
      "Jiawei Chen",
      "Sheng Zhou",
      "Binbin Hu",
      "Yan Feng",
      "Chun Chen",
      "Can Wang"
    ],
    "abstract": "Recommendation Systems (RS) are often plagued by popularity bias. When\ntraining a recommendation model on a typically long-tailed dataset, the model\ntends to not only inherit this bias but often exacerbate it, resulting in\nover-representation of popular items in the recommendation lists. This study\nconducts comprehensive empirical and theoretical analyses to expose the root\ncauses of this phenomenon, yielding two core insights: 1) Item popularity is\nmemorized in the principal spectrum of the score matrix predicted by the\nrecommendation model; 2) The dimension collapse phenomenon amplifies the\nrelative prominence of the principal spectrum, thereby intensifying the\npopularity bias. Building on these insights, we propose a novel debiasing\nstrategy that leverages a spectral norm regularizer to penalize the magnitude\nof the principal singular value. We have developed an efficient algorithm to\nexpedite the calculation of the spectral norm by exploiting the spectral\nproperty of the score matrix. Extensive experiments across seven real-world\ndatasets and three testing paradigms have been conducted to validate the\nsuperiority of the proposed method.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12008v6",
    "published_date": "2024-04-18 08:59:32 UTC",
    "updated_date": "2025-04-14 17:40:38 UTC"
  },
  {
    "arxiv_id": "2404.11999v5",
    "title": "Token-level Direct Preference Optimization",
    "authors": [
      "Yongcheng Zeng",
      "Guoqing Liu",
      "Weiyu Ma",
      "Ning Yang",
      "Haifeng Zhang",
      "Jun Wang"
    ],
    "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11999v5",
    "published_date": "2024-04-18 08:49:38 UTC",
    "updated_date": "2024-08-30 03:39:57 UTC"
  },
  {
    "arxiv_id": "2404.11996v1",
    "title": "DST-GTN: Dynamic Spatio-Temporal Graph Transformer Network for Traffic Forecasting",
    "authors": [
      "Songtao Huang",
      "Hongjin Song",
      "Tianqi Jiang",
      "Akbar Telikani",
      "Jun Shen",
      "Qingguo Zhou",
      "Binbin Yong",
      "Qiang Wu"
    ],
    "abstract": "Accurate traffic forecasting is essential for effective urban planning and\ncongestion management. Deep learning (DL) approaches have gained colossal\nsuccess in traffic forecasting but still face challenges in capturing the\nintricacies of traffic dynamics. In this paper, we identify and address this\nchallenges by emphasizing that spatial features are inherently dynamic and\nchange over time. A novel in-depth feature representation, called Dynamic\nSpatio-Temporal (Dyn-ST) features, is introduced, which encapsulates spatial\ncharacteristics across varying times. Moreover, a Dynamic Spatio-Temporal Graph\nTransformer Network (DST-GTN) is proposed by capturing Dyn-ST features and\nother dynamic adjacency relations between intersections. The DST-GTN can model\ndynamic ST relationships between nodes accurately and refine the representation\nof global and local ST characteristics by adopting adaptive weights in low-pass\nand all-pass filters, enabling the extraction of Dyn-ST features from traffic\ntime-series data. Through numerical experiments on public datasets, the DST-GTN\nachieves state-of-the-art performance for a range of traffic forecasting tasks\nand demonstrates enhanced stability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11996v1",
    "published_date": "2024-04-18 08:44:52 UTC",
    "updated_date": "2024-04-18 08:44:52 UTC"
  },
  {
    "arxiv_id": "2404.11988v3",
    "title": "The Emerging Generative Artificial Intelligence Divide in the United States",
    "authors": [
      "Madeleine I. G. Daepp",
      "Scott Counts"
    ],
    "abstract": "The digital divide refers to disparities in access to and use of digital\ntooling across social and economic groups. This divide can reinforce\nmarginalization both at the individual level and at the level of places,\nbecause persistent economic advantages accrue to places where new technologies\nare adopted early. To what extent are emerging generative artificial\nintelligence (AI) tools subject to these social and spatial divides? We\nleverage a large-scale search query database to characterize U.S. residents'\nknowledge of a novel generative AI tool, ChatGPT, during its first six months\nof release. We identify hotspots of higher-than-expected search volumes for\nChatGPT in coastal metropolitan areas, while coldspots are evident in the\nAmerican South, Appalachia, and the Midwest. Nationwide, counties with the\nhighest rates of search have proportionally more educated and more economically\nadvantaged populations, as well as proportionally more technology and\nfinance-sector jobs in comparison with other counties or with the national\naverage. Observed associations with race/ethnicity and urbanicity are\nattenuated in fully adjusted hierarchical models, but education emerges as the\nstrongest positive predictor of generative AI awareness. In the absence of\nintervention, early differences in uptake show a potential to reinforce\nexisting spatial and socioeconomic divides.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "K.4.2"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11988v3",
    "published_date": "2024-04-18 08:33:35 UTC",
    "updated_date": "2025-04-18 19:41:52 UTC"
  },
  {
    "arxiv_id": "2404.11973v1",
    "title": "Exploring the landscape of large language models: Foundations, techniques, and challenges",
    "authors": [
      "Milad Moradi",
      "Ke Yan",
      "David Colwell",
      "Matthias Samwald",
      "Rhona Asgari"
    ],
    "abstract": "In this review paper, we delve into the realm of Large Language Models\n(LLMs), covering their foundational principles, diverse applications, and\nnuanced training processes. The article sheds light on the mechanics of\nin-context learning and a spectrum of fine-tuning approaches, with a special\nfocus on methods that optimize efficiency in parameter usage. Additionally, it\nexplores how LLMs can be more closely aligned with human preferences through\ninnovative reinforcement learning frameworks and other novel methods that\nincorporate human feedback. The article also examines the emerging technique of\nretrieval augmented generation, integrating external knowledge into LLMs. The\nethical dimensions of LLM deployment are discussed, underscoring the need for\nmindful and responsible application. Concluding with a perspective on future\nresearch trajectories, this review offers a succinct yet comprehensive overview\nof the current state and emerging trends in the evolving landscape of LLMs,\nserving as an insightful guide for both researchers and practitioners in\nartificial intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11973v1",
    "published_date": "2024-04-18 08:01:20 UTC",
    "updated_date": "2024-04-18 08:01:20 UTC"
  },
  {
    "arxiv_id": "2404.11964v1",
    "title": "From Language Models to Practical Self-Improving Computer Agents",
    "authors": [
      "Alex Sheng"
    ],
    "abstract": "We develop a simple and straightforward methodology to create AI computer\nagents that can carry out diverse computer tasks and self-improve by developing\ntools and augmentations to enable themselves to solve increasingly complex\ntasks. As large language models (LLMs) have been shown to benefit from\nnon-parametric augmentations, a significant body of recent work has focused on\ndeveloping software that augments LLMs with various capabilities. Rather than\nmanually developing static software to augment LLMs through human engineering\neffort, we propose that an LLM agent can systematically generate software to\naugment itself. We show, through a few case studies, that a minimal querying\nloop with appropriate prompt engineering allows an LLM to generate and use\nvarious augmentations, freely extending its own capabilities to carry out\nreal-world computer tasks. Starting with only terminal access, we prompt an LLM\nagent to augment itself with retrieval, internet search, web navigation, and\ntext editor capabilities. The agent effectively uses these various tools to\nsolve problems including automated software development and web-based tasks.",
    "categories": [
      "cs.AI",
      "68T01",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11964v1",
    "published_date": "2024-04-18 07:50:10 UTC",
    "updated_date": "2024-04-18 07:50:10 UTC"
  },
  {
    "arxiv_id": "2404.11962v2",
    "title": "©Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
    "authors": [
      "Chao Zhou",
      "Huishuai Zhang",
      "Jiang Bian",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "abstract": "This paper addresses the contentious issue of copyright infringement in\nimages generated by text-to-image models, sparking debates among AI developers,\ncontent creators, and legal entities. State-of-the-art models create\nhigh-quality content without crediting original creators, causing concern in\nthe artistic community. To mitigate this, we propose the \\copyright Plug-in\nAuthorization framework, introducing three operations: addition, extraction,\nand combination. Addition involves training a \\copyright plug-in for specific\ncopyright, facilitating proper credit attribution. Extraction allows creators\nto reclaim copyright from infringing models, and combination enables users to\nmerge different \\copyright plug-ins. These operations act as permits,\nincentivizing fair use and providing flexibility in authorization. We present\ninnovative approaches,\"Reverse LoRA\" for extraction and \"EasyMerge\" for\nseamless combination. Experiments in artist-style replication and cartoon IP\nrecreation demonstrate \\copyright plug-ins' effectiveness, offering a valuable\nsolution for human copyright protection in the age of generative AIs. The code\nis available at https://github.com/zc1023/-Plug-in-Authorization.git.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.11962v2",
    "published_date": "2024-04-18 07:48:00 UTC",
    "updated_date": "2025-01-30 14:46:45 UTC"
  },
  {
    "arxiv_id": "2404.11960v3",
    "title": "MCRanker: Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM Rankers",
    "authors": [
      "Fang Guo",
      "Wenyu Li",
      "Honglei Zhuang",
      "Yun Luo",
      "Yafu Li",
      "Le Yan",
      "Qi Zhu",
      "Yue Zhang"
    ],
    "abstract": "The most recent pointwise Large Language Model (LLM) rankers have achieved\nremarkable ranking results. However, these rankers are hindered by two major\ndrawbacks: (1) they fail to follow a standardized comparison guidance during\nthe ranking process, and (2) they struggle with comprehensive considerations\nwhen dealing with complicated passages. To address these shortcomings, we\npropose to build a ranker that generates ranking scores based on a set of\ncriteria from various perspectives. These criteria are intended to direct each\nperspective in providing a distinct yet synergistic evaluation. Our research,\nwhich examines eight datasets from the BEIR benchmark demonstrates that\nincorporating this multi-perspective criteria ensemble approach markedly\nenhanced the performance of pointwise LLM rankers.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11960v3",
    "published_date": "2024-04-18 07:42:46 UTC",
    "updated_date": "2025-03-25 06:08:47 UTC"
  },
  {
    "arxiv_id": "2404.11949v1",
    "title": "Sketch-guided Image Inpainting with Partial Discrete Diffusion Process",
    "authors": [
      "Nakul Sharma",
      "Aditay Tripathi",
      "Anirban Chakraborty",
      "Anand Mishra"
    ],
    "abstract": "In this work, we study the task of sketch-guided image inpainting. Unlike the\nwell-explored natural language-guided image inpainting, which excels in\ncapturing semantic details, the relatively less-studied sketch-guided\ninpainting offers greater user control in specifying the object's shape and\npose to be inpainted. As one of the early solutions to this task, we introduce\na novel partial discrete diffusion process (PDDP). The forward pass of the PDDP\ncorrupts the masked regions of the image and the backward pass reconstructs\nthese masked regions conditioned on hand-drawn sketches using our proposed\nsketch-guided bi-directional transformer. The proposed novel transformer module\naccepts two inputs -- the image containing the masked region to be inpainted\nand the query sketch to model the reverse diffusion process. This strategy\neffectively addresses the domain gap between sketches and natural images,\nthereby, enhancing the quality of inpainting results. In the absence of a\nlarge-scale dataset specific to this task, we synthesize a dataset from the\nMS-COCO to train and extensively evaluate our proposed framework against\nvarious competent approaches in the literature. The qualitative and\nquantitative results and user studies establish that the proposed method\ninpaints realistic objects that fit the context in terms of the visual\nappearance of the provided sketch. To aid further research, we have made our\ncode publicly available at https://github.com/vl2g/Sketch-Inpainting .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NTIRE Workshop @ CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.11949v1",
    "published_date": "2024-04-18 07:07:38 UTC",
    "updated_date": "2024-04-18 07:07:38 UTC"
  },
  {
    "arxiv_id": "2404.11936v1",
    "title": "LD-Pruner: Efficient Pruning of Latent Diffusion Models using Task-Agnostic Insights",
    "authors": [
      "Thibault Castells",
      "Hyoung-Kyu Song",
      "Bo-Kyeong Kim",
      "Shinkook Choi"
    ],
    "abstract": "Latent Diffusion Models (LDMs) have emerged as powerful generative models,\nknown for delivering remarkable results under constrained computational\nresources. However, deploying LDMs on resource-limited devices remains a\ncomplex issue, presenting challenges such as memory consumption and inference\nspeed. To address this issue, we introduce LD-Pruner, a novel\nperformance-preserving structured pruning method for compressing LDMs.\nTraditional pruning methods for deep neural networks are not tailored to the\nunique characteristics of LDMs, such as the high computational cost of training\nand the absence of a fast, straightforward and task-agnostic method for\nevaluating model performance. Our method tackles these challenges by leveraging\nthe latent space during the pruning process, enabling us to effectively\nquantify the impact of pruning on model performance, independently of the task\nat hand. This targeted pruning of components with minimal impact on the output\nallows for faster convergence during training, as the model has less\ninformation to re-learn, thereby addressing the high computational cost of\ntraining. Consequently, our approach achieves a compressed model that offers\nimproved inference speed and reduced parameter count, while maintaining minimal\nperformance degradation. We demonstrate the effectiveness of our approach on\nthree different tasks: text-to-image (T2I) generation, Unconditional Image\nGeneration (UIG) and Unconditional Audio Generation (UAG). Notably, we reduce\nthe inference time of Stable Diffusion (SD) by 34.9% while simultaneously\nimproving its FID by 5.2% on MS-COCO T2I benchmark. This work paves the way for\nmore efficient pruning methods for LDMs, enhancing their applicability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, accepted to CVPR24 First Workshop on Efficient and On-Device\n  Generation (EDGE)",
    "pdf_url": "http://arxiv.org/pdf/2404.11936v1",
    "published_date": "2024-04-18 06:35:37 UTC",
    "updated_date": "2024-04-18 06:35:37 UTC"
  },
  {
    "arxiv_id": "2404.11932v3",
    "title": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment",
    "authors": [
      "Geyu Lin",
      "Bin Wang",
      "Zhengyuan Liu",
      "Nancy F. Chen"
    ],
    "abstract": "Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.11932v3",
    "published_date": "2024-04-18 06:20:50 UTC",
    "updated_date": "2025-01-06 06:33:51 UTC"
  },
  {
    "arxiv_id": "2404.11929v3",
    "title": "A Symmetric Regressor for MRI-Based Assessment of Striatal Dopamine Transporter Uptake in Parkinson's Disease With Enhanced Uncertainty Estimation",
    "authors": [
      "Walid Abdullah Al",
      "Il Dong Yun",
      "Yun Jung Bae"
    ],
    "abstract": "Dopamine transporter (DAT) imaging is commonly used for monitoring\nParkinson's disease (PD), where striatal DAT uptake amount is computed to\nassess PD severity. However, DAT imaging has a high cost and the risk of\nradiance exposure and is not available in general clinics. Recently, MRI patch\nof the nigral region has been proposed as a safer and easier alternative. This\npaper proposes a symmetric regressor for predicting the DAT uptake amount from\nthe nigral MRI patch. Acknowledging the symmetry between the right and left\nnigrae, the proposed regressor incorporates a paired input-output model that\nsimultaneously predicts the DAT uptake amounts for both the right and left\nstriata. Moreover, it employs a symmetric loss that imposes a constraint on the\ndifference between right-to-left predictions, resembling the high correlation\nin DAT uptake amounts in the two lateral sides. Additionally, we propose a\nsymmetric Monte-Carlo (MC) dropout method for providing a fruitful uncertainty\nestimate of the DAT uptake prediction, which utilizes the above symmetry. We\nevaluated the proposed approach on 734 nigral patches, which demonstrated\nsignificantly improved performance of the symmetric regressor compared with the\nstandard regressors while giving better explainability and feature\nrepresentation. The symmetric MC dropout also gave precise uncertainty ranges\nwith a high probability of including the true DAT uptake amounts within the\nrange.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11929v3",
    "published_date": "2024-04-18 06:18:48 UTC",
    "updated_date": "2025-04-04 23:08:49 UTC"
  },
  {
    "arxiv_id": "2404.11925v1",
    "title": "EdgeFusion: On-Device Text-to-Image Generation",
    "authors": [
      "Thibault Castells",
      "Hyoung-Kyu Song",
      "Tairen Piao",
      "Shinkook Choi",
      "Bo-Kyeong Kim",
      "Hanyoung Yim",
      "Changgwun Lee",
      "Jae Gon Kim",
      "Tae-Ho Kim"
    ],
    "abstract": "The intensive computational burden of Stable Diffusion (SD) for text-to-image\ngeneration poses a significant hurdle for its practical application. To tackle\nthis challenge, recent research focuses on methods to reduce sampling steps,\nsuch as Latent Consistency Model (LCM), and on employing architectural\noptimizations, including pruning and knowledge distillation. Diverging from\nexisting approaches, we uniquely start with a compact SD variant, BK-SDM. We\nobserve that directly applying LCM to BK-SDM with commonly used crawled\ndatasets yields unsatisfactory results. It leads us to develop two strategies:\n(1) leveraging high-quality image-text pairs from leading generative models and\n(2) designing an advanced distillation process tailored for LCM. Through our\nthorough exploration of quantization, profiling, and on-device deployment, we\nachieve rapid generation of photo-realistic, text-aligned images in just two\nsteps, with latency under one second on resource-limited edge devices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages, accepted to CVPR24 First Workshop on Efficient and On-Device\n  Generation (EDGE)",
    "pdf_url": "http://arxiv.org/pdf/2404.11925v1",
    "published_date": "2024-04-18 06:02:54 UTC",
    "updated_date": "2024-04-18 06:02:54 UTC"
  },
  {
    "arxiv_id": "2404.11924v1",
    "title": "Toward Short-Term Glucose Prediction Solely Based on CGM Time Series",
    "authors": [
      "Ming Cheng",
      "Xingjian Diao",
      "Ziyi Zhou",
      "Yanjun Cui",
      "Wenjun Liu",
      "Shitong Cheng"
    ],
    "abstract": "The global diabetes epidemic highlights the importance of maintaining good\nglycemic control. Glucose prediction is a fundamental aspect of diabetes\nmanagement, facilitating real-time decision-making. Recent research has\nintroduced models focusing on long-term glucose trend prediction, which are\nunsuitable for real-time decision-making and result in delayed responses.\nConversely, models designed to respond to immediate glucose level changes\ncannot analyze glucose variability comprehensively. Moreover, contemporary\nresearch generally integrates various physiological parameters (e.g. insulin\ndoses, food intake, etc.), which inevitably raises data privacy concerns. To\nbridge such a research gap, we propose TimeGlu -- an end-to-end pipeline for\nshort-term glucose prediction solely based on CGM time series data. We\nimplement four baseline methods to conduct a comprehensive comparative analysis\nof the model's performance. Through extensive experiments on two contrasting\ndatasets (CGM Glucose and Colas dataset), TimeGlu achieves state-of-the-art\nperformance without the need for additional personal data from patients,\nproviding effective guidance for real-world diabetic glucose management.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11924v1",
    "published_date": "2024-04-18 06:02:12 UTC",
    "updated_date": "2024-04-18 06:02:12 UTC"
  },
  {
    "arxiv_id": "2404.11917v2",
    "title": "Expected Coordinate Improvement for High-Dimensional Bayesian Optimization",
    "authors": [
      "Dawei Zhan"
    ],
    "abstract": "Bayesian optimization (BO) algorithm is very popular for solving\nlow-dimensional expensive optimization problems. Extending Bayesian\noptimization to high dimension is a meaningful but challenging task. One of the\nmajor challenges is that it is difficult to find good infill solutions as the\nacquisition functions are also high-dimensional. In this work, we propose the\nexpected coordinate improvement (ECI) criterion for high-dimensional Bayesian\noptimization. The proposed ECI criterion measures the potential improvement we\ncan get by moving the current best solution along one coordinate. The proposed\napproach selects the coordinate with the highest ECI value to refine in each\niteration and covers all the coordinates gradually by iterating over the\ncoordinates. The greatest advantage of the proposed ECI-BO (expected coordinate\nimprovement based Bayesian optimization) algorithm over the standard BO\nalgorithm is that the infill selection problem of the proposed algorithm is\nalways a one-dimensional problem thus can be easily solved. Numerical\nexperiments show that the proposed algorithm can achieve significantly better\nresults than the standard BO algorithm and competitive results when compared\nwith five state-of-the-art high-dimensional BOs. This work provides a simple\nbut efficient approach for high-dimensional Bayesian optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11917v2",
    "published_date": "2024-04-18 05:48:15 UTC",
    "updated_date": "2025-01-10 02:08:52 UTC"
  },
  {
    "arxiv_id": "2404.11916v2",
    "title": "Skeleton: A New Framework for Accelerating Language Models via Task Neuron Localized Prompt Tuning",
    "authors": [
      "Nakyeong Yang",
      "Jiwon Moon",
      "Junseok Kim",
      "Yunah Jang",
      "Kyomin Jung"
    ],
    "abstract": "Prompt tuning methods have shown comparable performance to general training\nmethods as parameter-efficient fine-tuning (PEFT) methods in various natural\nlanguage understanding tasks. However, existing prompt tuning methods still\nutilize the entire model architecture even when solving a specific task, which\nprevents them from accelerating inference speed during the application\nprocedure. In this paper, we propose a novel prompt tuning framework called\nSkeleton to efficiently utilize a language model in terms of memory and time\ncomplexity for solving various tasks, retaining only task-relevant neurons by\nusing an explainability method. From our framework, we can efficiently solve\nvarious tasks by using only task-relevant neurons and prepending adequate\ntask-specific prompt tokens with only a single language model. Experiments\nreveal that our method significantly enhances inference efficiency (at most x\n1.73 speed up) for various widely used benchmarks, showing comparable\nperformance to the prompt tuning method. Moreover, our method is applicable\nacross various transformer-based architectures, confirming its practicality and\nscalability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.11916v2",
    "published_date": "2024-04-18 05:43:50 UTC",
    "updated_date": "2024-10-17 09:01:27 UTC"
  },
  {
    "arxiv_id": "2404.11907v1",
    "title": "Sampling-based Pareto Optimization for Chance-constrained Monotone Submodular Problems",
    "authors": [
      "Xiankun Yan",
      "Aneta Neumann",
      "Frank Neumann"
    ],
    "abstract": "Recently surrogate functions based on the tail inequalities were developed to\nevaluate the chance constraints in the context of evolutionary computation and\nseveral Pareto optimization algorithms using these surrogates were successfully\napplied in optimizing chance-constrained monotone submodular problems. However,\nthe difference in performance between algorithms using the surrogates and those\nemploying the direct sampling-based evaluation remains unclear. Within the\npaper, a sampling-based method is proposed to directly evaluate the chance\nconstraint. Furthermore, to address the problems with more challenging\nsettings, an enhanced GSEMO algorithm integrated with an adaptive sliding\nwindow, called ASW-GSEMO, is introduced. In the experiments, the ASW-GSEMO\nemploying the sampling-based approach is tested on the chance-constrained\nversion of the maximum coverage problem with different settings. Its results\nare compared with those from other algorithms using different surrogate\nfunctions. The experimental findings indicate that the ASW-GSEMO with the\nsampling-based evaluation approach outperforms other algorithms, highlighting\nthat the performances of algorithms using different evaluation methods are\ncomparable. Additionally, the behaviors of ASW-GSEMO are visualized to explain\nthe distinctions between it and the algorithms utilizing the surrogate\nfunctions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11907v1",
    "published_date": "2024-04-18 05:15:20 UTC",
    "updated_date": "2024-04-18 05:15:20 UTC"
  },
  {
    "arxiv_id": "2404.11898v1",
    "title": "Enhancing Financial Inclusion and Regulatory Challenges: A Critical Analysis of Digital Banks and Alternative Lenders Through Digital Platforms, Machine Learning, and Large Language Models Integration",
    "authors": [
      "Luke Lee"
    ],
    "abstract": "This paper explores the dual impact of digital banks and alternative lenders\non financial inclusion and the regulatory challenges posed by their business\nmodels. It discusses the integration of digital platforms, machine learning\n(ML), and Large Language Models (LLMs) in enhancing financial services\naccessibility for underserved populations. Through a detailed analysis of\noperational frameworks and technological infrastructures, this research\nidentifies key mechanisms that facilitate broader financial access and mitigate\ntraditional barriers. Additionally, the paper addresses significant regulatory\nconcerns involving data privacy, algorithmic bias, financial stability, and\nconsumer protection. Employing a mixed-methods approach, which combines\nquantitative financial data analysis with qualitative insights from industry\nexperts, this paper elucidates the complexities of leveraging digital\ntechnology to foster financial inclusivity. The findings underscore the\nnecessity of evolving regulatory frameworks that harmonize innovation with\ncomprehensive risk management. This paper concludes with policy recommendations\nfor regulators, financial institutions, and technology providers, aiming to\ncultivate a more inclusive and stable financial ecosystem through prudent\ndigital technology integration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.11898v1",
    "published_date": "2024-04-18 05:00:53 UTC",
    "updated_date": "2024-04-18 05:00:53 UTC"
  },
  {
    "arxiv_id": "2404.11891v3",
    "title": "Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools",
    "authors": [
      "Yilun Hao",
      "Yongchao Chen",
      "Yang Zhang",
      "Chuchu Fan"
    ],
    "abstract": "Large Language Models (LLMs) struggle to directly generate correct plans for\ncomplex multi-constraint planning problems, even with self-verification and\nself-critique. For example, a U.S. domestic travel planning benchmark\nTravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI\no1-preview can only find viable travel plans with a 10% success rate given all\nneeded information. In this work, we tackle this by proposing an LLM-based\nplanning framework that formalizes and solves complex multi-constraint planning\nproblems as constrained satisfiability problems, which are further consumed by\nsound and complete satisfiability solvers. We start with TravelPlanner as the\nprimary use case and show that our framework achieves a success rate of 93.9%\nand is effective with diverse paraphrased prompts. More importantly, our\nframework has strong zero-shot generalizability, successfully handling unseen\nconstraints in our newly created unseen international travel dataset and\ngeneralizing well to new fundamentally different domains. Moreover, when user\ninput queries are infeasible, our framework can identify the unsatisfiable\ncore, provide failure reasons, and offers personalized modification\nsuggestions. We show that our framework can modify and solve for an average of\n81.6% and 91.7% unsatisfiable queries from two datasets and prove with\nablations that all key components of our framework are effective and necessary.\nProject page: https://sites.google.com/view/llm-rwplanning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "50 pages, 6 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.11891v3",
    "published_date": "2024-04-18 04:36:37 UTC",
    "updated_date": "2025-01-29 17:24:03 UTC"
  },
  {
    "arxiv_id": "2404.11888v2",
    "title": "FedEGG: Federated Learning with Explicit Global Guidance",
    "authors": [
      "Kun Zhai",
      "Yifeng Gao",
      "Difan Zou",
      "Guangnan Ye",
      "Siheng Chen",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "abstract": "Federated Learning (FL) holds great potential for diverse applications owing\nto its privacy-preserving nature. However, its convergence is often challenged\nby non-IID data distributions, limiting its effectiveness in real-world\ndeployments. Existing methods help address these challenges via\noptimization-based client constraints, adaptive client selection, or the use of\npre-trained models or synthetic data. In this work, we reinterpret these\napproaches as all introducing an \\emph{implicit guiding task} to regularize and\nsteer client learning. Following this insight, we propose to introduce an\n\\emph{explicit global guiding task} into the current FL framework to improve\nconvergence and performance. To this end, we present \\textbf{FedEGG}, a new FL\nalgorithm that constructs a global guiding task using a well-defined,\neasy-to-converge learning task based on a public dataset and Large Language\nModels (LLMs). This approach effectively combines the strengths of federated\n(the original FL task) and centralized (the global guiding task) learning. We\nprovide a theoretical analysis of FedEGG's convergence, examining the impact of\ndata heterogeneity between the guiding and FL tasks and the guiding strength.\nOur analysis derives an upper bound for the optimal guiding strength, offering\npractical insights for implementation. Empirically, FedEGG demonstrates\nsuperior performance over state-of-the-art FL methods under both IID and\nnon-IID settings, and further improves their performances when combined.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11888v2",
    "published_date": "2024-04-18 04:25:21 UTC",
    "updated_date": "2025-04-20 06:29:48 UTC"
  },
  {
    "arxiv_id": "2404.11875v2",
    "title": "Concept Induction using LLMs: a user experiment for assessment",
    "authors": [
      "Adrita Barua",
      "Cara Widmer",
      "Pascal Hitzler"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) poses a significant challenge in\nproviding transparent and understandable insights into complex AI models.\nTraditional post-hoc algorithms, while useful, often struggle to deliver\ninterpretable explanations. Concept-based models offer a promising avenue by\nincorporating explicit representations of concepts to enhance interpretability.\nHowever, existing research on automatic concept discovery methods is often\nlimited by lower-level concepts, costly human annotation requirements, and a\nrestricted domain of background knowledge. In this study, we explore the\npotential of a Large Language Model (LLM), specifically GPT-4, by leveraging\nits domain knowledge and common-sense capability to generate high-level\nconcepts that are meaningful as explanations for humans, for a specific setting\nof image classification. We use minimal textual object information available in\nthe data via prompting to facilitate this process. To evaluate the output, we\ncompare the concepts generated by the LLM with two other methods: concepts\ngenerated by humans and the ECII heuristic concept induction system. Since\nthere is no established metric to determine the human understandability of\nconcepts, we conducted a human study to assess the effectiveness of the\nLLM-generated concepts. Our findings indicate that while human-generated\nexplanations remain superior, concepts derived from GPT-4 are more\ncomprehensible to humans compared to those generated by ECII.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11875v2",
    "published_date": "2024-04-18 03:22:02 UTC",
    "updated_date": "2024-09-20 20:26:55 UTC"
  },
  {
    "arxiv_id": "2404.11630v1",
    "title": "SNP: Structured Neuron-level Pruning to Preserve Attention Scores",
    "authors": [
      "Kyunghwan Shim",
      "Jaewoong Yun",
      "Shinkook Choi"
    ],
    "abstract": "Multi-head self-attention (MSA) is a key component of Vision Transformers\n(ViTs), which have achieved great success in various vision tasks. However,\ntheir high computational cost and memory footprint hinder their deployment on\nresource-constrained devices. Conventional pruning approaches can only compress\nand accelerate the MSA module using head pruning, although the head is not an\natomic unit. To address this issue, we propose a novel graph-aware neuron-level\npruning method, Structured Neuron-level Pruning (SNP). SNP prunes neurons with\nless informative attention scores and eliminates redundancy among heads.\nSpecifically, it prunes graphically connected query and key layers having the\nleast informative attention scores while preserving the overall attention\nscores. Value layers, which can be pruned independently, are pruned to\neliminate inter-head redundancy. Our proposed method effectively compresses and\naccelerates Transformer-based models for both edge devices and server\nprocessors. For instance, the DeiT-Small with SNP runs 3.1$\\times$ faster than\nthe original model and achieves performance that is 21.94\\% faster and 1.12\\%\nhigher than the DeiT-Tiny. Additionally, SNP combine successfully with\nconventional head or block pruning approaches. SNP with head pruning could\ncompress the DeiT-Base by 80\\% of the parameters and computational costs and\nachieve 3.85$\\times$ faster inference speed on RTX3090 and 4.93$\\times$ on\nJetson Nano.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11630v1",
    "published_date": "2024-04-18 03:21:28 UTC",
    "updated_date": "2024-04-18 03:21:28 UTC"
  },
  {
    "arxiv_id": "2404.11874v1",
    "title": "Using a Local Surrogate Model to Interpret Temporal Shifts in Global Annual Data",
    "authors": [
      "Shou Nakano",
      "Yang Liu"
    ],
    "abstract": "This paper focuses on explaining changes over time in globally-sourced,\nannual temporal data, with the specific objective of identifying pivotal\nfactors that contribute to these temporal shifts. Leveraging such analytical\nframeworks can yield transformative impacts, including the informed refinement\nof public policy and the identification of key drivers affecting a country's\neconomic evolution. We employ Local Interpretable Model-agnostic Explanations\n(LIME) to shed light on national happiness indices, economic freedom, and\npopulation metrics, spanning variable time frames. Acknowledging the presence\nof missing values, we employ three imputation approaches to generate robust\nmultivariate time-series datasets apt for LIME's input requirements. Our\nmethodology's efficacy is substantiated through a series of empirical\nevaluations involving multiple datasets. These evaluations include comparative\nanalyses against random feature selection, correlation with real-world events\nas elucidated by LIME, and validation through Individual Conditional\nExpectation (ICE) plots, a state-of-the-art technique proficient in feature\nimportance detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "There are 9 pages and 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.11874v1",
    "published_date": "2024-04-18 03:17:45 UTC",
    "updated_date": "2024-04-18 03:17:45 UTC"
  },
  {
    "arxiv_id": "2404.11854v1",
    "title": "SGRU: A High-Performance Structured Gated Recurrent Unit for Traffic Flow Prediction",
    "authors": [
      "Wenfeng Zhang",
      "Xin Li",
      "Anqi Li",
      "Xiaoting Huang",
      "Ti Wang",
      "Honglei Gao"
    ],
    "abstract": "Traffic flow prediction is an essential task in constructing smart cities and\nis a typical Multivariate Time Series (MTS) Problem. Recent research has\nabandoned Gated Recurrent Units (GRU) and utilized dilated convolutions or\ntemporal slicing for feature extraction, and they have the following drawbacks:\n(1) Dilated convolutions fail to capture the features of adjacent time steps,\nresulting in the loss of crucial transitional data. (2) The connections within\nthe same temporal slice are strong, while the connections between different\ntemporal slices are too loose. In light of these limitations, we emphasize the\nimportance of analyzing a complete time series repeatedly and the crucial role\nof GRU in MTS. Therefore, we propose SGRU: Structured Gated Recurrent Units,\nwhich involve structured GRU layers and non-linear units, along with multiple\nlayers of time embedding to enhance the model's fitting performance. We\nevaluate our approach on four publicly available California traffic datasets:\nPeMS03, PeMS04, PeMS07, and PeMS08 for regression prediction. Experimental\nresults demonstrate that our model outperforms baseline models with average\nimprovements of 11.7%, 18.6%, 18.5%, and 12.0% respectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 6 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2404.11854v1",
    "published_date": "2024-04-18 02:15:40 UTC",
    "updated_date": "2024-04-18 02:15:40 UTC"
  },
  {
    "arxiv_id": "2404.11835v2",
    "title": "CAUS: A Dataset for Question Generation based on Human Cognition Leveraging Large Language Models",
    "authors": [
      "Minjung Shin",
      "Donghyun Kim",
      "Jeh-Kwang Ryu"
    ],
    "abstract": "We introduce the Curious About Uncertain Scene (CAUS) dataset, designed to\nenable Large Language Models, specifically GPT-4, to emulate human cognitive\nprocesses for resolving uncertainties. Leveraging this dataset, we investigate\nthe potential of LLMs to engage in questioning effectively. Our approach\ninvolves providing scene descriptions embedded with uncertainties to stimulate\nthe generation of reasoning and queries. The queries are then classified\naccording to multi-dimensional criteria. All procedures are facilitated by a\ncollaborative system involving both LLMs and human researchers. Our results\ndemonstrate that GPT-4 can effectively generate pertinent questions and grasp\ntheir nuances, particularly when given appropriate context and instructions.\nThe study suggests that incorporating human-like questioning into AI models\nimproves their ability to manage uncertainties, paving the way for future\nadvancements in Artificial Intelligence (AI).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures and 3 tables. This work has been accepted for\n  presentation as a poster with full paper publication at CogSci 2024. This is\n  the final submission",
    "pdf_url": "http://arxiv.org/pdf/2404.11835v2",
    "published_date": "2024-04-18 01:31:19 UTC",
    "updated_date": "2024-05-19 04:57:47 UTC"
  },
  {
    "arxiv_id": "2404.11833v2",
    "title": "Thought of Search: Planning with Language Models Through The Lens of Efficiency",
    "authors": [
      "Michael Katz",
      "Harsha Kokel",
      "Kavitha Srinivas",
      "Shirin Sohrabi"
    ],
    "abstract": "Among the most important properties of algorithms investigated in computer\nscience are soundness, completeness, and complexity. These properties, however,\nare rarely analyzed for the vast collection of recently proposed methods for\nplanning with large language models. In this work, we alleviate this gap. We\nanalyse these properties of using LLMs for planning and highlight that recent\ntrends abandon both soundness and completeness for the sake of inefficiency. We\npropose a significantly more efficient approach that can, at the same time,\nmaintain both soundness and completeness. We exemplify on four representative\nsearch problems, comparing to the LLM-based solutions from the literature that\nattempt to solve these problems. We show that by using LLMs to produce the code\nfor the search components we can solve the entire datasets with 100\\% accuracy\nwith only a few calls to the LLM. We argue for a responsible use of compute\nresources; urging research community to investigate sound and complete\nLLM-based approaches that uphold efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11833v2",
    "published_date": "2024-04-18 01:27:29 UTC",
    "updated_date": "2024-05-21 18:44:54 UTC"
  },
  {
    "arxiv_id": "2405.02318v3",
    "title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
    "authors": [
      "Abhinav Lalwani",
      "Tasha Kim",
      "Lovish Chopra",
      "Christopher Hahn",
      "Zhijing Jin",
      "Mrinmaya Sachan"
    ],
    "abstract": "Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02318v3",
    "published_date": "2024-04-18 00:20:48 UTC",
    "updated_date": "2025-03-06 07:29:44 UTC"
  },
  {
    "arxiv_id": "2404.11812v1",
    "title": "Cross-model Mutual Learning for Exemplar-based Medical Image Segmentation",
    "authors": [
      "Qing En",
      "Yuhong Guo"
    ],
    "abstract": "Medical image segmentation typically demands extensive dense annotations for\nmodel training, which is both time-consuming and skill-intensive. To mitigate\nthis burden, exemplar-based medical image segmentation methods have been\nintroduced to achieve effective training with only one annotated image. In this\npaper, we introduce a novel Cross-model Mutual learning framework for\nExemplar-based Medical image Segmentation (CMEMS), which leverages two models\nto mutually excavate implicit information from unlabeled data at multiple\ngranularities. CMEMS can eliminate confirmation bias and enable collaborative\ntraining to learn complementary information by enforcing consistency at\ndifferent granularities across models. Concretely, cross-model image\nperturbation based mutual learning is devised by using weakly perturbed images\nto generate high-confidence pseudo-labels, supervising predictions of strongly\nperturbed images across models. This approach enables joint pursuit of\nprediction consistency at the image granularity. Moreover, cross-model\nmulti-level feature perturbation based mutual learning is designed by letting\npseudo-labels supervise predictions from perturbed multi-level features with\ndifferent resolutions, which can broaden the perturbation space and enhance the\nrobustness of our framework. CMEMS is jointly trained using exemplar data,\nsynthetic data, and unlabeled data in an end-to-end manner. Experimental\nresults on two medical image datasets indicate that the proposed CMEMS\noutperforms the state-of-the-art segmentation methods with extremely limited\nsupervision.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AISTATS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.11812v1",
    "published_date": "2024-04-18 00:18:07 UTC",
    "updated_date": "2024-04-18 00:18:07 UTC"
  },
  {
    "arxiv_id": "2404.11811v2",
    "title": "Physics-informed active learning for accelerating quantum chemical simulations",
    "authors": [
      "Yi-Fan Hou",
      "Lina Zhang",
      "Quanhao Zhang",
      "Fuchun Ge",
      "Pavlo O. Dral"
    ],
    "abstract": "Quantum chemical simulations can be greatly accelerated by constructing\nmachine learning potentials, which is often done using active learning (AL).\nThe usefulness of the constructed potentials is often limited by the high\neffort required and their insufficient robustness in the simulations. Here we\nintroduce the end-to-end AL for constructing robust data-efficient potentials\nwith affordable investment of time and resources and minimum human\ninterference. Our AL protocol is based on the physics-informed sampling of\ntraining points, automatic selection of initial data, uncertainty\nquantification, and convergence monitoring. The versatility of this protocol is\nshown in our implementation of quasi-classical molecular dynamics for\nsimulating vibrational spectra, conformer search of a key biochemical molecule,\nand time-resolved mechanism of the Diels-Alder reactions. These investigations\ntook us days instead of weeks of pure quantum chemical calculations on a\nhigh-performance computing cluster. The code in MLatom and tutorials are\navailable at https://github.com/dralgroup/mlatom.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.11811v2",
    "published_date": "2024-04-18 00:17:01 UTC",
    "updated_date": "2024-07-16 07:16:46 UTC"
  }
]
{
  "date": "2025-05-10",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-10 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv æ›´æ–°å¯è°“æ˜¯â€œè½¯ç¡¬å…¼æ–½â€ï¼šSalesforce å‘å¸ƒäº†é¢å‘é•¿æ–‡æœ¬çš„ xGen-small æ¨¡å‹ç³»åˆ—ï¼›å…³äºç”Ÿæˆå¼ AI â€œæ¨¡å‹å´©æºƒâ€ (Model Collapse) çš„ç ”ç©¶å»¶ä¼¸åˆ°äº†å¤šæ¨¡æ€é¢†åŸŸï¼›è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼ˆIV 2025ï¼‰æ¶Œç°å¤šç¯‡ä½³ä½œï¼ŒåŒ…æ‹¬ä¸€ç¯‡å…³äºè½¨è¿¹é¢„æµ‹çš„æœ€ä½³è®ºæ–‡ï¼›åŒæ—¶ï¼ŒGabriel PeyrÃ© å¤§ç¥å¸¦æ¥äº†æœ€ä¼˜ä¼ è¾“ (Optimal Transport) çš„æœ€æ–°è¯¾ç¨‹ç¬”è®°ã€‚\n\n---\n\n### ğŸš€ ç”Ÿæˆå¼ AI ä¸å¤§æ¨¡å‹ï¼šå´©æºƒé£é™©ã€æ–°æ¨¡å‹ä¸å®‰å…¨æ€§\n\n**1. å¤šæ¨¡æ€åˆæˆæ•°æ®è®­ç»ƒä¸æ¨¡å‹å´©æºƒï¼šæ¥è‡ª VLM å’Œæ‰©æ•£æ¨¡å‹çš„è§è§£**\n**Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šå°†â€œæ¨¡å‹å´©æºƒâ€ (Model Collapse) çš„ç ”ç©¶ä»å•æ¨¡æ€æ‰©å±•åˆ°äº†å¤šæ¨¡æ€ (Vision-Language Models)ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šä½œè€…å‘ç°å¤šæ¨¡æ€ç¯å¢ƒä¸‹çš„æ¨¡å‹å´©æºƒè¡¨ç°å‡ºç‹¬ç‰¹çš„ç‰¹å¾ï¼šè™½ç„¶è§†è§‰-è¯­è¨€å¯¹é½ (alignment) å¯èƒ½é€šè¿‡é€’å½’è®­ç»ƒå¾—åˆ°æ”¹å–„ï¼Œä½† VLM åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­çš„æ–¹å·®ä¼šå¢åŠ ã€‚\n*   **è§£å†³æ–¹æ¡ˆ**ï¼šå¢åŠ è§£ç é¢„ç®— (decoding budgets)ã€æé«˜æ¨¡å‹å¤šæ ·æ€§ä»¥åŠä½¿ç”¨å†»ç»“æ¨¡å‹é‡æ–°æ‰“æ ‡ (relabeling) å¯ä»¥æœ‰æ•ˆç¼“è§£å´©æºƒé£é™©ã€‚è¿™å¯¹æ„å»ºè‡ªæˆ‘è¿›åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè‡³å…³é‡è¦ã€‚\n\n**2. xGen-small æŠ€æœ¯æŠ¥å‘Š**\n**xGen-small Technical Report**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šSalesforce å‘å¸ƒçš„æ–°ä¸€ä»£ Transformer è§£ç å™¨æ¨¡å‹ç³»åˆ—ï¼ˆ4B å’Œ 9B å‚æ•°ï¼‰ã€‚\n*   **ä¸»è¦è´¡çŒ®**ï¼šä¸“ä¸ºé•¿ä¸Šä¸‹æ–‡åº”ç”¨ä¼˜åŒ–ï¼Œæ”¯æŒæ‰©å±•è‡³ 128k tokenã€‚é‡‡ç”¨äº†åˆ†é˜¶æ®µé¢„è®­ç»ƒã€è´¨é‡é€€ç« (quality annealing) å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ¨¡å‹åœ¨æ•°å­¦å’Œä»£ç é¢†åŸŸè¡¨ç°å¼ºåŠ²ï¼Œæ˜¯ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„é«˜æ€§èƒ½è½»é‡çº§åŸºåº§ã€‚\n\n**3. MacRAGï¼šå‹ç¼©ã€åˆ‡ç‰‡å’Œæ‰©å±•çš„å¤šå°ºåº¦è‡ªé€‚åº”ä¸Šä¸‹æ–‡ RAG**\n**MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šé’ˆå¯¹ RAG ç³»ç»Ÿä¸­æ£€ç´¢ä¸ç²¾å‡†å’Œä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–é—®é¢˜çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§å±‚çº§åŒ–çš„ RAG æ¡†æ¶ï¼Œå…ˆè¿›è¡Œç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„æ–‡æ¡£å‹ç¼©å’Œåˆ‡åˆ†ï¼Œå†æ ¹æ®æŸ¥è¯¢è‡ªé€‚åº”åœ°åˆå¹¶ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆChunk-level å’Œ Document-level æ‰©å±•ï¼‰ã€‚\n*   **æ•ˆæœ**ï¼šåœ¨ HotpotQA ç­‰å¤šè·³æ¨ç†æ•°æ®é›†ä¸Šï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ RAG æµç¨‹ï¼Œé€‚åˆå¤„ç†é•¿æ–‡æ¡£å’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚\n\n**4. ç³»ç»Ÿæç¤ºè¯ä¸­æ¯’ï¼šè¶…è¶Šç”¨æˆ·æ³¨å…¥çš„ LLM æŒä¹…æ”»å‡»**\n**System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šå®‰å…¨è­¦æŠ¥ï¼è¿™æ˜¯ä¸€ç§æ–°çš„æ”»å‡»å‘é‡ï¼Œé’ˆå¯¹çš„æ˜¯ **System Prompts** è€Œéä¼ ç»Ÿçš„ç”¨æˆ·æç¤ºè¯ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šæ”»å‡» System Prompt ä¼šå¯¹åç»­æ‰€æœ‰çš„ç”¨æˆ·äº¤äº’äº§ç”ŸæŒä¹…å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨äº†æ€ç»´é“¾ (CoT) æˆ– RAG ç­‰é«˜çº§æŠ€æœ¯ï¼Œè¿™ç§ä¸­æ¯’æ”»å‡»ä¾ç„¶æœ‰æ•ˆï¼Œä¸”ä¸éœ€è¦å¤æ‚çš„è¶Šç‹± (jailbreak) æ‰‹æ®µã€‚\n\n---\n\n### ğŸš— è‡ªåŠ¨é©¾é©¶ä¸æœºå™¨äººï¼šIV 2025 æœ€ä½³è®ºæ–‡ä¸äººå½¢æœºå™¨äºº\n\n**5. [IV 2025 æœ€ä½³è®ºæ–‡] TPKï¼šç»“åˆå…ˆéªŒçŸ¥è¯†çš„å¯ä¿¡è½¨è¿¹é¢„æµ‹**\n**TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šè£è· IV 2025 æœ€ä½³è®ºæ–‡å¥–ã€‚è§£å†³äº†æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹å‡ºçš„è½¨è¿¹å¾€å¾€è¿åç‰©ç†è§„å¾‹æˆ–é€»è¾‘çš„é—®é¢˜ã€‚\n*   **æ–¹æ³•**ï¼šèåˆäº†æ‰€æœ‰ä»£ç†ç±»åˆ«ï¼ˆè½¦è¾†ã€è¡Œäººã€éª‘è¡Œè€…ï¼‰çš„äº¤äº’å…ˆéªŒå’Œè¿åŠ¨å­¦å…ˆéªŒã€‚å¼•å…¥äº† DG-SFMï¼ˆåŸºäºè§„åˆ™çš„äº¤äº’é‡è¦æ€§è¯„åˆ†ï¼‰æ¥æŒ‡å¯¼æ¨¡å‹ã€‚\n*   **ä»·å€¼**ï¼šè™½ç„¶å¼•å…¥è¿åŠ¨å­¦æ¨¡å‹å¯¼è‡´é¢„æµ‹ç²¾åº¦ç•¥å¾®ä¸‹é™ï¼Œä½†**å®Œå…¨æ¶ˆé™¤äº†ä¸å¯è¡Œè½¨è¿¹**ï¼Œå¤§å¹…æå‡äº†é¢„æµ‹çš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚\n\n**6. JAEGERï¼šåŒå±‚äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶å™¨**\n**JAEGER: Dual-Level Humanoid Whole-Body Controller**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šé’ˆå¯¹äººå½¢æœºå™¨äººçš„å…¨èº«æ§åˆ¶ (Whole-Body Control) éš¾é¢˜ã€‚\n*   **æ–¹æ³•**ï¼šå°†ä¸Šèº«å’Œä¸‹èº«çš„æ§åˆ¶åˆ†ç¦»æˆä¸¤ä¸ªç‹¬ç«‹çš„æ§åˆ¶å™¨ï¼Œåˆ†åˆ«è´Ÿè´£ä¸åŒçš„ä»»åŠ¡ï¼Œç¼“è§£äº†ç»´åº¦ç¾éš¾ã€‚é‡‡ç”¨è¯¾ç¨‹å­¦ä¹  (Curriculum Learning)ï¼Œå…ˆåˆ©ç”¨äººç±»åŠ¨ä½œæ•°æ®é›† (AMASS) è¿›è¡Œç›‘ç£å­¦ä¹ åˆå§‹åŒ–ï¼Œå†è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒã€‚\n*   **æ•ˆæœ**ï¼šåœ¨ä»¿çœŸå’ŒçœŸæœºä¸Šå‡è¡¨ç°å‡ºä¼˜äº SOTA çš„ç¨³å®šæ€§å’Œå¤šåŠŸèƒ½æ€§ã€‚\n\n**7. å››è¶³æœºå™¨äººæ»‘æ¿ä¸Šæ¿ï¼šé€†å‘è¯¾ç¨‹å­¦ä¹ **\n**Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šè¿™å°±å¾ˆæœ‰è¶£äº†ï¼Œæ•™æœºå™¨ç‹—ç©æ»‘æ¿ã€‚\n*   **æ–¹æ³•**ï¼šä½¿ç”¨é€†å‘è¯¾ç¨‹å¼ºåŒ–å­¦ä¹  (Reverse Curriculum RL)ï¼Œä»ä»»åŠ¡çš„ç»ˆæ€ï¼ˆå·²ç»åœ¨æ»‘æ¿ä¸Šï¼‰å¼€å§‹åå‘å¢åŠ éš¾åº¦ï¼Œæœ€ç»ˆè®©å››è¶³æœºå™¨äººå­¦ä¼šä»åœ°é¢è‡ªä¸»ç™»ä¸Šæ»‘æ¿ã€‚\n\n---\n\n### ğŸ§  ç†è®ºã€ç®—æ³•ä¸å¯è§£é‡Šæ€§\n\n**8. æœºå™¨å­¦ä¹ è€…çš„æœ€ä¼˜ä¼ è¾“ (Optimal Transport)**\n**Optimal Transport for Machine Learners**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šGabriel PeyrÃ© å¤§ä½¬çš„è¯¾ç¨‹ç¬”è®°ï¼Œå¿…å±ç²¾å“ã€‚\n*   **å†…å®¹**ï¼šæ¶µç›–äº† OT çš„åŸºç¡€æ•°å­¦ç†è®ºï¼ˆMonge, Kantorovich, Brenier å®šç†ç­‰ï¼‰ä»¥åŠåœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨ï¼ˆç”Ÿæˆæ¨¡å‹ã€Transformer Token åŠ¨åŠ›å­¦ç­‰ï¼‰ã€‚è™½ç„¶é‡æ•°å­¦ï¼Œä½†å¯¹äºæƒ³æ·±å…¥ç†è§£ç”Ÿæˆæ¨¡å‹æœ¬è´¨çš„ç ”ç©¶è€…æ˜¯ç»ä½³èµ„æ–™ã€‚\n\n**9. è§†è§‰ Transformer ä¸­çš„ç¬¦å·è§„åˆ™æå–**\n**Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šç¥ç»ç¬¦å· AI (Neuro-symbolic) çš„æ–°è¿›å±•ï¼Œå°è¯•æå– ViT çš„é€»è¾‘è§„åˆ™ã€‚\n*   **æ–¹æ³•**ï¼šå—ç¨€ç–è‡ªç¼–ç å™¨ (SAE) å¯å‘ï¼Œå¼•å…¥ç¨€ç–æ¦‚å¿µå±‚ï¼Œå°† Patch è¡¨ç¤ºè½¬åŒ–ä¸ºé«˜å±‚è§†è§‰æ¦‚å¿µï¼Œç„¶åä½¿ç”¨ FOLD-SE-M ç®—æ³•ç”Ÿæˆé€»è¾‘ç¨‹åºã€‚\n*   **æ„ä¹‰**ï¼šä¸ä»…æ˜¯äº‹åè§£é‡Šï¼Œè€Œæ˜¯æ„å»ºäº†ä¸€ä¸ªåŸºäºé€»è¾‘çš„å†³ç­–å±‚ï¼Œä½¿å¾— ViT çš„å†³ç­–è¿‡ç¨‹å˜å¾—å¯éªŒè¯ä¸”è¯­ä¹‰æ¸…æ™°ã€‚\n\n**10. å¸¦çŒœæµ‹çš„å€¼è¿­ä»£ï¼šç”¨äºé©¬å°”å¯å¤«é“¾å’Œ MDP**\n**Value Iteration with Guessing for Markov Chains and Markov Decision Processes**\n*   **æ ¸å¿ƒäº®ç‚¹**ï¼šTACAS 2025 æ”¶å½•æ–‡ç« ã€‚æŒ‘æˆ˜äº†ç»å…¸çš„å€¼è¿­ä»£ (Value Iteration, VI) ç®—æ³•ã€‚\n*   **è´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§åŸºäºâ€œçŒœæµ‹å€¼â€çš„æ–° VI æ–¹æ³•ã€‚ç†è®ºä¸Šè¯æ˜äº†åœ¨ç»è¿‡å¤šé¡¹å¼æ—¶é—´çš„é¢„å¤„ç†åï¼ŒVI åªéœ€è¦äºšæŒ‡æ•°çº§ (sub-exponentially) çš„ Bellman æ›´æ–°å³å¯æ”¶æ•›ï¼Œè¿™å¯¹å¤§è§„æ¨¡ MDP æ±‚è§£æ˜¯é‡è¦çš„ç†è®ºçªç ´ã€‚\n\n---\n\n### ğŸ› ï¸ å…¶ä»–æœ‰è¶£çš„åº”ç”¨\n\n*   **Text-to-CadQuery (Paper 43)**: **Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities**\n    ç›´æ¥åˆ©ç”¨ LLM ç”Ÿæˆ Python è„šæœ¬ (CadQuery) æ¥æ„å»º 3D CAD æ¨¡å‹ï¼Œè€Œä¸æ˜¯ç”Ÿæˆä¸­é—´æ ¼å¼ã€‚åœ¨å¾®è°ƒåï¼ŒExact Match å‡†ç¡®ç‡è¾¾åˆ°äº† 69.3%ï¼Œä¸ºå‚æ•°åŒ– 3D è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚\n\n*   **ProFashion (Paper 37)**: **ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images**\n    åˆ©ç”¨å¤šå¼ å‚è€ƒå›¾ç”Ÿæˆæ—¶åºä¸€è‡´çš„æ—¶å°šè§†é¢‘ã€‚è§£å†³äº†ä»¥å¾€åªèƒ½ç”¨å•å¼ å›¾å¯¼è‡´è§†è§’å˜æ¢æ—¶è¡£æœçº¹ç†ä¸¢å¤±çš„é—®é¢˜ï¼Œå‰æ‰‹å…šå’Œç”µå•†è¡Œä¸šçš„ç¦éŸ³ã€‚\n\n*   **AI for Science (Paper 45)**: **Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models**\n    ç”¨ Attention æœºåˆ¶å»é‡æ„ç»å…¸çš„æ•é£Ÿè€…-çŒç‰©æ¨¡å‹ã€‚å‘ç° Attention æƒé‡ç«Ÿç„¶ä¸æé›…æ™®è¯ºå¤«å‡½æ•° (Lyapunov function) çš„å‡ ä½•ç»“æ„å¯¹é½ï¼Œè¯æ˜äº† AI å¯ç”¨äºå‘ç°åŠ¨åŠ›ç³»ç»Ÿçš„ç‰©ç†å±æ€§ã€‚\n\n---\nç¯‡å¹…æ‰€é™ï¼Œè¿˜æœ‰å…³äº **è”é‚¦å­¦ä¹ å™ªå£°æ ‡ç­¾ (Paper 14)**ã€**æ°´ä¸‹å£°çº³ç›®æ ‡æ£€æµ‹ (Paper 13)** ä»¥åŠ **åŒ»å­¦å›¾åƒé…å‡† (Paper 40)** ç­‰ä¼˜ç§€è®ºæ–‡æ— æ³•ä¸€ä¸€è¯¦è¿°ã€‚å¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥èƒ½ä¸ºä½ æä¾›çµæ„Ÿï¼Œç¥ç§‘ç ”é¡ºåˆ©ï¼",
  "papers": [
    {
      "arxiv_id": "2505.08803v1",
      "title": "Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models",
      "title_zh": "å¤šæ¨¡æ€åˆæˆæ•°æ®è®­ç»ƒä¸æ¨¡å‹å´©æºƒï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹çš„å¯ç¤º",
      "authors": [
        "Zizhao Hu",
        "Mohammad Rostami",
        "Jesse Thomason"
      ],
      "abstract": "Recent research has highlighted the risk of generative model collapse, where performance progressively degrades when continually trained on self-generated data. However, existing exploration on model collapse is limited to single, unimodal models, limiting our understanding in more realistic scenarios, such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving. We expand the synthetic data training and model collapse study to multi-modal vision-language generative systems, such as vision-language models (VLMs) and text-to-image diffusion models, as well as recursive generate-train loops with multiple models. We find that model collapse, previously observed in single-modality generative models, exhibits distinct characteristics in the multi-modal context, such as improved vision-language alignment and increased variance in VLM image-captioning task. Additionally, we find that general approaches such as increased decoding budgets, greater model diversity, and relabeling with frozen models can effectively mitigate model collapse. Our findings provide initial insights and practical guidelines for reducing the risk of model collapse in self-improving multi-agent AI systems and curating robust multi-modal synthetic datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ vision-language models (VLMs) å’Œ text-to-image diffusion models ç­‰å¤šæ¨¡æ€ç”Ÿæˆç³»ç»Ÿï¼Œæ·±å…¥æ¢è®¨äº†å…¶åœ¨åˆæˆæ•°æ®è®­ç»ƒä¸­çš„ model collapse é£é™©ã€‚ç ”ç©¶è€…å°† model collapse çš„ç ”ç©¶èŒƒå›´ä»å•æ¨¡æ€æ¨¡å‹æ‰©å±•åˆ°å¤šæ¨¡å‹å‚ä¸çš„é€’å½’ç”Ÿæˆè®­ç»ƒå¾ªç¯ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€ç¯å¢ƒä¸‹è¯¥ç°è±¡çš„ç‹¬ç‰¹ç‰¹å¾ã€‚å®éªŒå‘ç°ï¼Œå¤šæ¨¡æ€è®­ç»ƒè™½ç„¶èƒ½æ”¹å–„ vision-language alignmentï¼Œä½†åœ¨ VLM çš„ image-captioning ä»»åŠ¡ä¸­ä¼šå¯¼è‡´æ–¹å·®å¢åŠ ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºé€šè¿‡å¢åŠ  decoding budgetsã€æå‡æ¨¡å‹å¤šæ ·æ€§ä»¥åŠåˆ©ç”¨ frozen models è¿›è¡Œé‡æ ‡æ³¨ç­‰æ–¹æ³•æ¥æœ‰æ•ˆç¼“è§£æ¨¡å‹å´©æºƒã€‚è¿™äº›å‘ç°ä¸ºé™ä½è‡ªæ”¹è¿›å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨åˆæˆæ•°æ®ä¸‹çš„å¤±æ•ˆé£é™©ï¼Œä»¥åŠæ„å»ºé²æ£’çš„å¤šæ¨¡æ€åˆæˆæ•°æ®é›†æä¾›äº†é‡è¦çš„å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08803v1",
      "published_date": "2025-05-10 22:42:29 UTC",
      "updated_date": "2025-05-10 22:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:25:37.291026+00:00"
    },
    {
      "arxiv_id": "2505.06769v1",
      "title": "Value Iteration with Guessing for Markov Chains and Markov Decision Processes",
      "title_zh": "é©¬å°”å¯å¤«é“¾ä¸é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„å¸¦çŒœå€¼ä»·å€¼è¿­ä»£",
      "authors": [
        "Krishnendu Chatterjee",
        "Mahdi JafariRaviz",
        "Raimundo Saona",
        "Jakub Svoboda"
      ],
      "abstract": "Two standard models for probabilistic systems are Markov chains (MCs) and Markov decision processes (MDPs). Classic objectives for such probabilistic models for control and planning problems are reachability and stochastic shortest path. The widely studied algorithmic approach for these problems is the Value Iteration (VI) algorithm which iteratively applies local updates called Bellman updates. There are many practical approaches for VI in the literature but they all require exponentially many Bellman updates for MCs in the worst case. A preprocessing step is an algorithm that is discrete, graph-theoretical, and requires linear space. An important open question is whether, after a polynomial-time preprocessing, VI can be achieved with sub-exponentially many Bellman updates. In this work, we present a new approach for VI based on guessing values. Our theoretical contributions are twofold. First, for MCs, we present an almost-linear-time preprocessing algorithm after which, along with guessing values, VI requires only subexponentially many Bellman updates. Second, we present an improved analysis of the speed of convergence of VI for MDPs. Finally, we present a practical algorithm for MDPs based on our new approach. Experimental results show that our approach provides a considerable improvement over existing VI-based approaches on several benchmark examples from the literature.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é©¬å°”å¯å¤«é“¾ (MCs) å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDPs) ä¸­çš„å¯è¾¾æ€§ä¸éšæœºæœ€çŸ­è·¯å¾„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºçŒœå€¼ (guessing values) çš„æ–°å‹ä»·å€¼è¿­ä»£ (Value Iteration, VI) æ–¹æ³•ã€‚é’ˆå¯¹ MCsï¼Œè®ºæ–‡å¼€å‘äº†ä¸€ç§è¿‘çº¿æ€§æ—¶é—´çš„é¢„å¤„ç†ç®—æ³•ï¼Œç»“åˆçŒœå€¼æŠ€æœ¯å°† Bellman æ›´æ–°çš„éœ€æ±‚ä»æŒ‡æ•°çº§é™ä½è‡³æ¬¡æŒ‡æ•°çº§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ”¹è¿›äº† MDPs ä¸­ VI ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦åˆ†æï¼Œå¹¶æ®æ­¤è®¾è®¡äº†æ›´å…·å®è·µæ„ä¹‰çš„ç®—æ³•æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„ VI ç›¸å…³ç®—æ³•ã€‚è¿™ç§ç»“åˆç¦»æ•£å›¾è®ºé¢„å¤„ç†ä¸æ•°å€¼æ›´æ–°çš„æ–¹æ³•ï¼Œä¸ºè§£å†³å¤æ‚æ¦‚ç‡ç³»ç»Ÿçš„æ§åˆ¶ä¸è§„åˆ’é—®é¢˜æä¾›äº†æ›´é«˜æ•ˆçš„ç†è®ºæ”¯æŒä¸è®¡ç®—æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.AI",
      "comment": "Appeared in the 31st International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.06769v1",
      "published_date": "2025-05-10 22:24:49 UTC",
      "updated_date": "2025-05-10 22:24:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:25:39.498439+00:00"
    },
    {
      "arxiv_id": "2505.06745v1",
      "title": "Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers",
      "title_zh": "åŸºäº Vision Transformers æ³¨æ„åŠ›å¼•å¯¼ç¨€ç–è¡¨ç¤ºçš„ç¬¦å·è§„åˆ™æå–",
      "authors": [
        "Parth Padalkar",
        "Gopal Gupta"
      ],
      "abstract": "Recent neuro-symbolic approaches have successfully extracted symbolic rule-sets from CNN-based models to enhance interpretability. However, applying similar techniques to Vision Transformers (ViTs) remains challenging due to their lack of modular concept detectors and reliance on global self-attention mechanisms. We propose a framework for symbolic rule extraction from ViTs by introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This linear layer operates on attention-weighted patch representations and learns a disentangled, binarized representation in which individual neurons activate for high-level visual concepts. To encourage interpretability, we apply a combination of L1 sparsity, entropy minimization, and supervised contrastive loss. These binarized concept activations are used as input to the FOLD-SE-M algorithm, which generates a rule-set in the form of logic programs. Our method achieves a 5.14% better classification accuracy than the standard ViT while enabling symbolic reasoning. Crucially, the extracted rule-set is not merely post-hoc but acts as a logic-based decision layer that operates directly on the sparse concept representations. The resulting programs are concise and semantically meaningful. This work is the first to extract executable logic programs from ViTs using sparse symbolic representations. It bridges the gap between transformer-based vision models and symbolic logic programming, providing a step forward in interpretable and verifiable neuro-symbolic AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä» Vision Transformers (ViTs) ä¸­æå–ç¬¦å·è§„åˆ™çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ ViTs ç¼ºä¹æ¨¡å—åŒ–æ¦‚å¿µæ£€æµ‹å™¨åŠä¾èµ–å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶æ‰€å¸¦æ¥çš„å¯è§£é‡Šæ€§æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡å¼•å…¥å— Sparse Autoencoders (SAEs) å¯å‘çš„ç¨€ç–æ¦‚å¿µå±‚ï¼Œåœ¨æ³¨æ„åŠ›åŠ æƒçš„ patch representations ä¸Šå­¦ä¹ è§£è€¦ä¸”äºŒå€¼åŒ–çš„æ¦‚å¿µè¡¨ç¤ºï¼Œä½¿ç‰¹å®šç¥ç»å…ƒèƒ½å¤Ÿå¯¹åº”é«˜å±‚è§†è§‰æ¦‚å¿µã€‚ä¸ºäº†å¢å¼ºå¯è§£é‡Šæ€§ï¼Œè¯¥æ¡†æ¶ç»“åˆäº† L1 sparsityã€entropy minimization å’Œ supervised contrastive loss è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶åˆ©ç”¨ FOLD-SE-M ç®—æ³•å°†æ¿€æ´»å€¼è½¬åŒ–ä¸ºé€»è¾‘ç¨‹åºå½¢å¼çš„è§„åˆ™é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ç°ç¬¦å·æ¨ç†çš„åŒæ—¶ï¼Œåˆ†ç±»å‡†ç¡®ç‡æ¯”æ ‡å‡† ViT æå‡äº† 5.14%ã€‚ä½œä¸ºé¦–ä¸ªä» ViTs ä¸­æå–å¯æ‰§è¡Œé€»è¾‘ç¨‹åºçš„å·¥ä½œï¼Œè¯¥ç ”ç©¶æˆåŠŸæ¡¥æ¥äº† Transformer æ¨¡å‹ä¸ç¬¦å·é€»è¾‘ç¼–ç¨‹ï¼Œä¸ºæ„å»ºå¯è§£é‡Šä¸”å¯éªŒè¯çš„ neuro-symbolic AI æä¾›äº†é‡è¦è¿›å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06745v1",
      "published_date": "2025-05-10 19:45:15 UTC",
      "updated_date": "2025-05-10 19:45:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:26:11.097719+00:00"
    },
    {
      "arxiv_id": "2505.06743v3",
      "title": "TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility",
      "title_zh": "TPKï¼šèåˆå…ˆéªŒçŸ¥è¯†ä»¥æå‡å¯è§£é‡Šæ€§ä¸è¿åŠ¨å­¦å¯è¡Œæ€§çš„å¯ä¿¡è½¨è¿¹é¢„æµ‹",
      "authors": [
        "Marius Baden",
        "Ahmed Abouelazm",
        "Christian Hubschneider",
        "Yin Wu",
        "Daniel Slieter",
        "J. Marius ZÃ¶llner"
      ],
      "abstract": "Trajectory prediction is crucial for autonomous driving, enabling vehicles to navigate safely by anticipating the movements of surrounding road users. However, current deep learning models often lack trustworthiness as their predictions can be physically infeasible and illogical to humans. To make predictions more trustworthy, recent research has incorporated prior knowledge, like the social force model for modeling interactions and kinematic models for physical realism. However, these approaches focus on priors that suit either vehicles or pedestrians and do not generalize to traffic with mixed agent classes. We propose incorporating interaction and kinematic priors of all agent classes--vehicles, pedestrians, and cyclists with class-specific interaction layers to capture agent behavioral differences. To improve the interpretability of the agent interactions, we introduce DG-SFM, a rule-based interaction importance score that guides the interaction layer. To ensure physically feasible predictions, we proposed suitable kinematic models for all agent classes with a novel pedestrian kinematic model. We benchmark our approach on the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our baseline. Experiments demonstrate that our method improves interaction interpretability, revealing a correlation between incorrect predictions and divergence from our interaction prior. Even though incorporating the kinematic models causes a slight decrease in accuracy, they eliminate infeasible trajectories found in the dataset and the baseline model. Thus, our approach fosters trust in trajectory prediction as its interaction reasoning is interpretable, and its predictions adhere to physics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TPK æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹æ¨¡å‹åœ¨ç‰©ç†å¯è¡Œæ€§å’Œé€»è¾‘è§£é‡Šæ€§æ–¹é¢ç¼ºä¹å¯ä¿¡åº¦ (trustworthiness) çš„é—®é¢˜ã€‚ä¸ä»¥å¾€ä»…é’ˆå¯¹å•ä¸€äº¤é€šå‚ä¸è€…çš„ç ”ç©¶ä¸åŒï¼ŒTPK æ•´åˆäº†è½¦è¾†ã€è¡Œäººå’Œéª‘è¡Œè€…ç­‰æ‰€æœ‰ç±»åˆ«æ™ºèƒ½ä½“çš„äº¤äº’å’Œè¿åŠ¨å­¦å…ˆéªŒçŸ¥è¯† (prior knowledge)ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šç±»åˆ«çš„äº¤äº’å±‚ã€‚ç ”ç©¶å¼•å…¥äº†åŸºäºè§„åˆ™çš„äº¤äº’é‡è¦æ€§è¯„åˆ†æœºåˆ¶ DG-SFMï¼Œé€šè¿‡æŒ‡å¯¼äº¤äº’å±‚æœ‰æ•ˆæå‡äº†æ™ºèƒ½ä½“äº¤äº’çš„å¯è§£é‡Šæ€§ (interpretability)ã€‚ä¸ºäº†ç¡®ä¿é¢„æµ‹ç»“æœç¬¦åˆç‰©ç†è§„å¾‹ï¼Œç ”ç©¶ä¸ºå„ç±»æ™ºèƒ½ä½“å¼€å‘äº†é’ˆå¯¹æ€§çš„è¿åŠ¨å­¦æ¨¡å‹ (kinematic models)ï¼ŒåŒ…æ‹¬ä¸€ç§æ–°å‹çš„è¡Œäººè¿åŠ¨å­¦æ¨¡å‹ã€‚åœ¨ Argoverse 2 æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ¨¡å‹ HPTR ä¸Šå¢å¼ºäº†æ¨ç†è¿‡ç¨‹çš„é€æ˜åº¦ï¼Œå¹¶æ­ç¤ºäº†é¢„æµ‹è¯¯å·®ä¸å…ˆéªŒåå·®ä¹‹é—´çš„å…³è”ã€‚å°½ç®¡è¿åŠ¨å­¦çº¦æŸçš„åŠ å…¥ä½¿ç²¾åº¦ç•¥æœ‰ä¸‹é™ï¼Œä½†å®ƒæˆåŠŸæ¶ˆé™¤äº†ç‰©ç†ä¸å¯è¡Œçš„è½¨è¿¹ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "First and Second authors contributed equally; Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025) for oral presentation; Winner of the best paper award",
      "pdf_url": "https://arxiv.org/pdf/2505.06743v3",
      "published_date": "2025-05-10 19:29:32 UTC",
      "updated_date": "2025-07-27 08:14:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:25:52.317755+00:00"
    },
    {
      "arxiv_id": "2505.06740v2",
      "title": "Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving",
      "title_zh": "é¢å‘é“è·¯æ„ŸçŸ¥ä¸ç‰©ç†å¯è¡Œè‡ªåŠ¨é©¾é©¶çš„è¾¹ç•Œå¼•å¯¼è½¨è¿¹é¢„æµ‹",
      "authors": [
        "Ahmed Abouelazm",
        "Mianzhi Liu",
        "Christian Hubschneider",
        "Yin Wu",
        "Daniel Slieter",
        "J. Marius ZÃ¶llner"
      ],
      "abstract": "Accurate prediction of surrounding road users' trajectories is essential for safe and efficient autonomous driving. While deep learning models have improved performance, challenges remain in preventing off-road predictions and ensuring kinematic feasibility. Existing methods incorporate road-awareness modules and enforce kinematic constraints but lack plausibility guarantees and often introduce trade-offs in complexity and flexibility. This paper proposes a novel framework that formulates trajectory prediction as a constrained regression guided by permissible driving directions and their boundaries. Using the agent's current state and an HD map, our approach defines the valid boundaries and ensures on-road predictions by training the network to learn superimposed paths between left and right boundary polylines. To guarantee feasibility, the model predicts acceleration profiles that determine the vehicle's travel distance along these paths while adhering to kinematic constraints. We evaluate our approach on the Argoverse-2 dataset against the HPTR baseline. Our approach shows a slight decrease in benchmark metrics compared to HPTR but notably improves final displacement error and eliminates infeasible trajectories. Moreover, the proposed approach has superior generalization to less prevalent maneuvers and unseen out-of-distribution scenarios, reducing the off-road rate under adversarial attacks from 66% to just 1%. These results highlight the effectiveness of our approach in generating feasible and robust predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­è½¨è¿¹é¢„æµ‹(Trajectory Prediction)é¢ä¸´çš„é¢„æµ‹è„±ç¦»è·¯é¢åŠä¸ç¬¦åˆåŠ¨åŠ›å­¦å¯è¡Œæ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œå¼•å¯¼çš„çº¦æŸå›å½’æ¡†æ¶ã€‚è¯¥æ–¹æ³•ç»“åˆæ™ºèƒ½ä½“çŠ¶æ€ä¸é«˜ç²¾åœ°å›¾(HD map)ï¼Œé€šè¿‡å­¦ä¹ å·¦å³è½¦é“è¾¹ç•Œä¹‹é—´çš„å åŠ è·¯å¾„å¹¶é¢„æµ‹åŠ é€Ÿåº¦æ›²çº¿æ¥ç¡®å®šè¡Œé©¶è·ç¦»ï¼Œä»è€Œç¡®ä¿é¢„æµ‹è½¨è¿¹ä¸¥æ ¼ç¬¦åˆç‰©ç†çº¦æŸã€‚åœ¨Argoverse-2æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡ä¸HPTRåŸºçº¿æ¨¡å‹ç›¸æ¯”éƒ¨åˆ†æŒ‡æ ‡ç•¥æœ‰ä¸‹é™ï¼Œä½†è¯¥æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†æœ€ç»ˆä½ç§»è¯¯å·®(final displacement error)å¹¶å®Œå…¨æ¶ˆé™¤äº†ä¸å¯è¡Œè½¨è¿¹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†å°‘è§æ“ä½œå’Œåˆ†å¸ƒå¤–(out-of-distribution)åœºæ™¯æ—¶å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸå°†å¯¹æŠ—æ”»å‡»ä¸‹çš„è„±ç¦»è·¯é¢ç‡ä»66%é™ä½è‡³1%ã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨ç”Ÿæˆé²æ£’ä¸”ç‰©ç†å¯è¡Œçš„é¢„æµ‹è½¨è¿¹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.06740v2",
      "published_date": "2025-05-10 19:21:00 UTC",
      "updated_date": "2025-07-11 09:18:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:25:59.867753+00:00"
    },
    {
      "arxiv_id": "2505.06737v2",
      "title": "Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving",
      "title_zh": "å¹³è¡¡è¿›åº¦ä¸å®‰å…¨ï¼šè‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ ä¸­ä¸€ç§æ–°å‹é£é™©æ„ŸçŸ¥ç›®æ ‡",
      "authors": [
        "Ahmed Abouelazm",
        "Jonas Michel",
        "Helen Gremmelmaier",
        "Tim Joseph",
        "Philip SchÃ¶rner",
        "J. Marius ZÃ¶llner"
      ],
      "abstract": "Reinforcement Learning (RL) is a promising approach for achieving autonomous driving due to robust decision-making capabilities. RL learns a driving policy through trial and error in traffic scenarios, guided by a reward function that combines the driving objectives. The design of such reward function has received insufficient attention, yielding ill-defined rewards with various pitfalls. Safety, in particular, has long been regarded only as a penalty for collisions. This leaves the risks associated with actions leading up to a collision unaddressed, limiting the applicability of RL in real-world scenarios. To address these shortcomings, our work focuses on enhancing the reward formulation by defining a set of driving objectives and structuring them hierarchically. Furthermore, we discuss the formulation of these objectives in a normalized manner to transparently determine their contribution to the overall reward. Additionally, we introduce a novel risk-aware objective for various driving interactions based on a two-dimensional ellipsoid function and an extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the efficacy of our proposed reward in unsignalized intersection scenarios with varying traffic densities. The approach decreases collision rates by 21\\% on average compared to baseline rewards and consistently surpasses them in route progress and cumulative reward, demonstrating its capability to promote safer driving behaviors while maintaining high-performance levels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¥–åŠ±å‡½æ•°è®¾è®¡ä¸è¶³ã€å°¤å…¶æ˜¯å¯¹ç¢°æ’å‰é£é™©è¯„ä¼°æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„é£é™©æ„ŸçŸ¥å¥–åŠ±ç›®æ ‡ã€‚ç ”ç©¶é€šè¿‡æ„å»ºå±‚æ¬¡åŒ–ä¸”æ ‡å‡†åŒ–(Normalized)çš„é©¾é©¶ç›®æ ‡ä½“ç³»ï¼Œæ˜ç¡®äº†å„é¡¹æŒ‡æ ‡å¯¹æ€»å¥–åŠ±çš„è´¡çŒ®åº¦ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº†ç»“åˆäºŒç»´æ¤­åœ†å‡½æ•°ä¸è´£ä»»æ•æ„Ÿå®‰å…¨(Responsibility-Sensitive Safety, RSS)æ‰©å±•æ¦‚å¿µçš„é£é™©æ„ŸçŸ¥ç›®æ ‡ï¼Œç”¨ä»¥é‡åŒ–ä¸åŒé©¾é©¶äº¤äº’ä¸­çš„æ½œåœ¨é£é™©ã€‚åœ¨æ— ä¿¡å·ç¯äº¤å‰è·¯å£åœºæ™¯ä¸‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ä½¿ç¢°æ’ç‡å¹³å‡é™ä½äº†21%ï¼ŒåŒæ—¶åœ¨è·¯å¾„è¿›åº¦å’Œç´¯ç§¯å¥–åŠ±æ–¹é¢è¡¨ç°æ›´ä¸ºä¼˜å¼‚ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ‰€ææ–¹æ³•åœ¨ç»´æŒé«˜æ°´å¹³è¡Œé©¶æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æ˜¾è‘—æå‡è‡ªåŠ¨é©¾é©¶å†³ç­–çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted in the 36th IEEE Intelligent vehicles Symposium (IV 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.06737v2",
      "published_date": "2025-05-10 19:05:03 UTC",
      "updated_date": "2025-07-11 09:28:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:26:29.671379+00:00"
    },
    {
      "arxiv_id": "2505.07883v1",
      "title": "Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints",
      "title_zh": "åŸºäºå…¬ç†åŒ–çº¦æŸä»å¤§è¯­è¨€æ¨¡å‹åµŒå…¥ä¸­æ¢å¤äº‹ä»¶æ¦‚ç‡",
      "authors": [
        "Jian-Qiao Zhu",
        "Haijiang Yan",
        "Thomas L. Griffiths"
      ],
      "abstract": "Rational decision-making under uncertainty requires coherent degrees of belief in events. However, event probabilities generated by Large Language Models (LLMs) have been shown to exhibit incoherence, violating the axioms of probability theory. This raises the question of whether coherent event probabilities can be recovered from the embeddings used by the models. If so, those derived probabilities could be used as more accurate estimates in events involving uncertainty. To explore this question, we propose enforcing axiomatic constraints, such as the additive rule of probability theory, in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings. This approach enables event probabilities to naturally emerge in the latent space as the VAE learns to both reconstruct the original embeddings and predict the embeddings of semantically related events. We evaluate our method on complementary events (i.e., event A and its complement, event not-A), where the true probabilities of the two events must sum to 1. Experiment results on open-weight language models demonstrate that probabilities recovered from embeddings exhibit greater coherence than those directly reported by the corresponding models and align closely with the true probabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åµŒå…¥ï¼ˆembeddingsï¼‰ä¸­æ¢å¤è¿è´¯çš„äº‹ä»¶æ¦‚ç‡ï¼Œä»¥è§£å†³æ¨¡å‹ç›´æ¥ç”Ÿæˆçš„æ¦‚ç‡å¸¸å› è¿åæ¦‚ç‡è®ºå…¬ç†è€Œå¯¼è‡´çš„ä¸è¿è´¯é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ‰©å±•å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å­¦ä¹ åˆ°çš„æ½œç©ºé—´ä¸­å¼ºåˆ¶æ‰§è¡ŒåŠ æ³•è§„åˆ™ç­‰å…¬ç†çº¦æŸï¼ˆaxiomatic constraintsï¼‰ï¼Œä½¿äº‹ä»¶æ¦‚ç‡èƒ½å¤Ÿè‡ªç„¶ç”Ÿæˆã€‚è¯¥æ¡†æ¶åœ¨é‡å»ºåŸå§‹åµŒå…¥çš„åŒæ—¶ï¼Œè¿˜èƒ½é¢„æµ‹è¯­ä¹‰ç›¸å…³äº‹ä»¶çš„åµŒå…¥ï¼Œä»è€Œç¡®ä¿æ½œç©ºé—´çš„é€»è¾‘ä¸€è‡´æ€§ã€‚åœ¨äº’è¡¥äº‹ä»¶ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä»åµŒå…¥ä¸­æ¢å¤çš„æ¦‚ç‡æ¯”LLMsç›´æ¥æŠ¥å‘Šçš„æ¦‚ç‡å…·æœ‰æ˜¾è‘—æ›´é«˜çš„è¿è´¯æ€§ï¼Œä¸”ä¸çœŸå®æ¦‚ç‡åˆ†å¸ƒæ›´åŠ å»åˆã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é€šè¿‡çº¦æŸå†…éƒ¨è¡¨å¾å¯ä»¥æœ‰æ•ˆæå–æ›´å‡†ç¡®çš„æ¦‚ç‡ä¼°è®¡ï¼Œä¸ºä¸ç¡®å®šæ€§ä¸‹çš„ç†æ€§å†³ç­–æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07883v1",
      "published_date": "2025-05-10 19:04:56 UTC",
      "updated_date": "2025-05-10 19:04:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:26:13.720158+00:00"
    },
    {
      "arxiv_id": "2505.06731v1",
      "title": "Deeply Explainable Artificial Neural Network",
      "title_zh": "æ·±åº¦å¯è§£é‡Šäººå·¥ç¥ç»ç½‘ç»œ",
      "authors": [
        "David Zucker"
      ],
      "abstract": "While deep learning models have demonstrated remarkable success in numerous domains, their black-box nature remains a significant limitation, especially in critical fields such as medical image analysis and inference. Existing explainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied post hoc, adding computational overhead and sometimes producing inconsistent or ambiguous results. In this paper, we present the Deeply Explainable Artificial Neural Network (DxANN), a novel deep learning architecture that embeds explainability ante hoc, directly into the training process. Unlike conventional models that require external interpretation methods, DxANN is designed to produce per-sample, per-feature explanations as part of the forward pass. Built on a flow-based framework, it enables both accurate predictions and transparent decision-making, and is particularly well-suited for image-based tasks. While our focus is on medical imaging, the DxANN architecture is readily adaptable to other data modalities, including tabular and sequential data. DxANN marks a step forward toward intrinsically interpretable deep learning, offering a practical solution for applications where trust and accountability are essential.",
      "tldr_zh": "æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—å½±åƒç­‰é¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½†å…¶â€œé»‘ç›’â€ç‰¹æ€§ä»¥åŠç°æœ‰ post hoc è§£é‡Šæ–¹æ³•ï¼ˆå¦‚ SHAP, LIME å’Œ Grad-CAMï¼‰å¸¦æ¥çš„é¢å¤–è®¡ç®—å¼€é”€å’Œç»“æœä¸ä¸€è‡´æ€§é™åˆ¶äº†å…¶åº”ç”¨ã€‚è¯¥ç ”ç©¶æå‡ºäº† Deeply Explainable Artificial Neural Network (DxANN)ï¼Œè¿™æ˜¯ä¸€ç§å°†å¯è§£é‡Šæ€§ ante hoc åµŒå…¥è®­ç»ƒè¿‡ç¨‹çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ã€‚ä¸åŒäºä¾èµ–å¤–éƒ¨è§£é‡Šæ–¹æ³•çš„ä¼ ç»Ÿæ¨¡å‹ï¼ŒDxANN åŸºäº flow-based æ¡†æ¶è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ç›´æ¥ç”Ÿæˆé’ˆå¯¹æ¯ä¸ªæ ·æœ¬å’Œç‰¹å¾çš„è§£é‡Šã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜å‡†ç¡®ç‡é¢„æµ‹ä¸é€æ˜å†³ç­–çš„ç»“åˆï¼Œä¸”ç‰¹åˆ«é€‚ç”¨äºåŸºäºå›¾åƒçš„ä»»åŠ¡ã€‚å°½ç®¡è¯¥ç ”ç©¶ä¾§é‡äºåŒ»ç–—å½±åƒï¼Œä½† DxANN æ¶æ„å¯ä»¥è½»æ¾æ‰©å±•è‡³è¡¨æ ¼å’Œåºåˆ—æ•°æ®ç­‰å…¶ä»–æ¨¡æ€ã€‚DxANN ä¸ºæ„å»ºæœ¬è´¨å¯è§£é‡Šçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä¸ºéœ€è¦ä¿¡ä»»å’Œé—®è´£çš„å…³é”®åº”ç”¨åœºæ™¯æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06731v1",
      "published_date": "2025-05-10 18:45:38 UTC",
      "updated_date": "2025-05-10 18:45:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:26:19.838659+00:00"
    },
    {
      "arxiv_id": "2507.18638v2",
      "title": "Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity",
      "title_zh": "æç¤ºå·¥ç¨‹ä¸å¤§è¯­è¨€æ¨¡å‹åœ¨æå‡äººç±»ç”Ÿäº§åŠ›ä¸­çš„æœ‰æ•ˆæ€§",
      "authors": [
        "Rizal Khoirul Anam"
      ],
      "abstract": "The widespread adoption of large language models (LLMs) such as ChatGPT, Gemini, and DeepSeek has significantly changed how people approach tasks in education, professional work, and creative domains. This paper investigates how the structure and clarity of user prompts impact the effectiveness and productivity of LLM outputs. Using data from 243 survey respondents across various academic and occupational backgrounds, we analyze AI usage habits, prompting strategies, and user satisfaction. The results show that users who employ clear, structured, and context-aware prompts report higher task efficiency and better outcomes. These findings emphasize the essential role of prompt engineering in maximizing the value of generative AI and provide practical implications for its everyday use.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº† Prompt Engineering ä»¥åŠ Large Language Models (LLMs) åœ¨æå‡äººç±»ç”Ÿäº§åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚éšç€ ChatGPTã€Gemini å’Œ DeepSeek ç­‰æ¨¡å‹çš„æ™®åŠï¼Œç ”ç©¶é€šè¿‡å¯¹ 243 åæ¥è‡ªä¸åŒå­¦æœ¯å’ŒèŒä¸šèƒŒæ™¯çš„å—è®¿è€…è¿›è¡Œè°ƒæŸ¥ï¼Œåˆ†æäº†å…¶ AI ä½¿ç”¨ä¹ æƒ¯ã€Prompting ç­–ç•¥åŠç”¨æˆ·æ»¡æ„åº¦ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ¸…æ™°ã€ç»“æ„åŒ–ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„è¯†ï¼ˆcontext-awareï¼‰çš„ Prompt çš„ç”¨æˆ·ï¼Œåœ¨ä»»åŠ¡æ•ˆç‡å’Œäº§å‡ºè´¨é‡æ–¹é¢è¡¨ç°æ˜¾è‘—æ›´ä¼˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº† Prompt Engineering åœ¨æœ€å¤§åŒ– Generative AI ä»·å€¼ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸ºæ—¥å¸¸åº”ç”¨æä¾›äº†å®é™…çš„æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "38 pages, 15 tables, 5 figures. Submitted as a research paper draft for arXiv. Based on survey data collected in 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18638v2",
      "published_date": "2025-05-10 18:27:03 UTC",
      "updated_date": "2025-08-27 21:28:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:26:27.097697+00:00"
    },
    {
      "arxiv_id": "2506.01965v1",
      "title": "TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition",
      "title_zh": "TaskVAEï¼šç”¨äºäººä½“æ´»åŠ¨è¯†åˆ«æŒç»­å­¦ä¹ ä¸­æ ·æœ¬ç”Ÿæˆçš„ä»»åŠ¡ç‰¹å®šå˜åˆ†è‡ªç¼–ç å™¨",
      "authors": [
        "Bonpagna Kann",
        "Sandra Castellanos-Paez",
        "Romain Rombourg",
        "Philippe Lalanda"
      ],
      "abstract": "As machine learning based systems become more integrated into daily life, they unlock new opportunities but face the challenge of adapting to dynamic data environments. Various forms of data shift-gradual, abrupt, or cyclic-threaten model accuracy, making continual adaptation essential. Continual Learning (CL) enables models to learn from evolving data streams while minimizing forgetting of prior knowledge. Among CL strategies, replay-based methods have proven effective, but their success relies on balancing memory constraints and retaining old class accuracy while learning new classes. This paper presents TaskVAE, a framework for replay-based CL in class-incremental settings. TaskVAE employs task-specific Variational Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which are then used to train the classifier alongside new task data. In contrast to traditional methods that require prior knowledge of the total class count or rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks without such constraints. We focus on Human Activity Recognition (HAR) using IMU sensor-equipped devices. Unlike previous HAR studies that combine data across all users, our approach focuses on individual user data, better reflecting real-world scenarios where a person progressively learns new activities. Extensive experiments on 5 different HAR datasets show that TaskVAE outperforms experience replay methods, particularly with limited data, and exhibits robust performance as dataset size increases. Additionally, memory footprint of TaskVAE is minimal, being equivalent to only 60 samples per task, while still being able to generate an unlimited number of synthetic samples. The contributions lie in balancing memory constraints, task-specific generation, and long-term stability, making it a reliable solution for real-world applications in domains like HAR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TaskVAEæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººä½“æ´»åŠ¨è¯†åˆ«(Human Activity Recognition, HAR)åœ¨æŒç»­å­¦ä¹ (Continual Learning, CL)è¿‡ç¨‹ä¸­é¢ä¸´çš„çŸ¥è¯†é—å¿˜æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ºæ¯ä¸ªä»»åŠ¡æ„å»ºç‰¹å®šçš„å˜åˆ†è‡ªç¼–ç å™¨(Variational Autoencoders, VAEs)æ¥ç”Ÿæˆè¿‡å¾€ä»»åŠ¡çš„åˆæˆç¤ºä¾‹ï¼Œå¹¶å°†å…¶ä¸æ–°ä»»åŠ¡æ•°æ®ç»“åˆè¿›è¡Œåˆ†ç±»å™¨è®­ç»ƒã€‚ç›¸æ¯”äºéœ€è¦é¢„çŸ¥ç±»åˆ«æ€»æ•°æˆ–ä¾èµ–å•ä¸€æ¨¡å‹çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒTaskVAEèƒ½æ›´çµæ´»åœ°é€‚åº”ä»»åŠ¡å¢é•¿ï¼Œå¹¶ä¸“æ³¨äºåæ˜ çœŸå®ä¸–ç•Œåœºæ™¯çš„ä¸ªäººç”¨æˆ·æ•°æ®ã€‚åœ¨äº”ä¸ªHARæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTaskVAEåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºç»éªŒå›æ”¾(experience replay)æ–¹æ³•ï¼Œä¸”åœ¨ä¿æŒé•¿æœŸç¨³å®šæ€§çš„åŒæ—¶å…·æœ‰æä½çš„å†…å­˜å ç”¨ï¼Œæ¯é¡¹ä»»åŠ¡çš„å­˜å‚¨å¼€é”€ä»…ç›¸å½“äº60ä¸ªåŸå§‹æ ·æœ¬ã€‚è¿™ç§ä»»åŠ¡ç‰¹å®šç”Ÿæˆçš„æ–¹æ³•å¹³è¡¡äº†å­˜å‚¨é™åˆ¶ä¸æ¨¡å‹å‡†ç¡®åº¦ï¼Œä¸ºHARç­‰åŠ¨æ€ç¯å¢ƒä¸‹çš„å®é™…åº”ç”¨æä¾›äº†ä¸€ä¸ªå¯é ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01965v1",
      "published_date": "2025-05-10 17:42:01 UTC",
      "updated_date": "2025-05-10 17:42:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:26:51.644085+00:00"
    },
    {
      "arxiv_id": "2505.06706v2",
      "title": "Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL",
      "title_zh": "åŒå±‚å¹³å‡åœºï¼šé¢å‘å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€åˆ†ç»„",
      "authors": [
        "Yuxuan Zheng",
        "Yihe Zhou",
        "Feiyang Xu",
        "Mingli Song",
        "Shunyu Liu"
      ],
      "abstract": "Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the curse of dimensionality, as the exponential growth in agent interactions significantly increases computational complexity and impedes learning efficiency. To mitigate this, existing efforts that rely on Mean Field (MF) simplify the interaction landscape by approximating neighboring agents as a single mean agent, thus reducing overall complexity to pairwise interactions. However, these MF methods inevitably fail to account for individual differences, leading to aggregation noise caused by inaccurate iterative updates during MF learning. In this paper, we propose a Bi-level Mean Field (BMF) method to capture agent diversity with dynamic grouping in large-scale MARL, which can alleviate aggregation noise via bi-level interaction. Specifically, BMF introduces a dynamic group assignment module, which employs a Variational AutoEncoder (VAE) to learn the representations of agents, facilitating their dynamic grouping over time. Furthermore, we propose a bi-level interaction module to model both inter- and intra-group interactions for effective neighboring aggregation. Experiments across various tasks demonstrate that the proposed BMF yields results superior to the state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Large-scale MARL)ä¸­çš„ç»´åº¦ç¾éš¾ï¼Œä»¥åŠä¼ ç»Ÿå¹³å‡åœº(Mean Field, MF)æ–¹æ³•å› å¿½ç•¥ä¸ªä½“å·®å¼‚è€Œå¯¼è‡´çš„èšåˆå™ªå£°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŒå±‚å¹³å‡åœº(Bi-level Mean Field, BMF)æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åŠ¨æ€ç»„åˆ†é…æ¨¡å—ï¼Œåˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨(VAE)å­¦ä¹ æ™ºèƒ½ä½“è¡¨å¾ï¼Œå®ç°äº†æ™ºèƒ½ä½“åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„åŠ¨æ€åˆ†ç»„ä¸å¤šæ ·æ€§æ•æ‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†åŒå±‚äº¤äº’æ¨¡å—æ¥åŒæ—¶å»ºæ¨¡ç»„å†…ä¸ç»„é—´çš„äº¤äº’å…³ç³»ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„é‚»åŸŸä¿¡æ¯èšåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBMFåœ¨å¤šé¡¹å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸ºè§£å†³å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„äº¤äº’å»ºæ¨¡å’Œå­¦ä¹ æ•ˆç‡é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06706v2",
      "published_date": "2025-05-10 17:04:33 UTC",
      "updated_date": "2025-05-20 07:29:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:32.090202+00:00"
    },
    {
      "arxiv_id": "2505.06699v3",
      "title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws",
      "title_zh": "æ¨¡å‹å¼•å¯¼ï¼šå¼•å…¥å‚è€ƒæ¨¡å‹çš„å­¦ä¹ æ”¹å–„æ³›åŒ–ç•Œä¸æ ‡åº¦å¾‹",
      "authors": [
        "Xiyuan Wei",
        "Ming Lin",
        "Fanjiang Ye",
        "Fengguang Song",
        "Liangliang Cao",
        "My T. Thai",
        "Tianbao Yang"
      ],
      "abstract": "This paper formalizes an emerging learning paradigm that uses a trained model as a reference to guide and enhance the training of a target model through strategic data selection or weighting, named $\\textbf{model steering}$. While ad-hoc methods have been used in various contexts, including the training of large foundation models, its underlying principles remain insufficiently understood, leading to sub-optimal performance. In this work, we propose a theory-driven framework for model steering called $\\textbf{DRRho risk minimization}$, which is rooted in Distributionally Robust Optimization (DRO). Through a generalization analysis, we provide theoretical insights into why this approach improves generalization and data efficiency compared to training without a reference model. To the best of our knowledge, this is the first time such theoretical insights are provided for the new learning paradigm, which significantly enhance our understanding and practice of model steering. Building on these insights and the connection between contrastive learning and DRO, we introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments validate the theoretical insights, reveal a superior scaling law compared to CLIP without a reference model, and demonstrate its strength over existing heuristic approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­£å¼æå‡ºäº†ä¸€ç§åä¸º Model Steering çš„å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡å‚è€ƒæ¨¡å‹ (Reference Model) çš„ç­–ç•¥æ€§æ•°æ®é€‰æ‹©æˆ–åŠ æƒæ¥å¢å¼ºç›®æ ‡æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¯¥é¢†åŸŸç†è®ºç ”ç©¶çš„åŒ®ä¹ï¼Œä½œè€…æå‡ºäº†åŸºäºåˆ†å¸ƒé²æ£’ä¼˜åŒ– (Distributionally Robust Optimization, DRO) çš„ DRRho Risk Minimization ç†è®ºæ¡†æ¶ã€‚è¯¥å·¥ä½œé€šè¿‡æ³›åŒ–åˆ†æï¼Œé¦–æ¬¡åœ¨ç†è®ºä¸Šé˜æ˜äº† Model Steering æå‡æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡çš„å†…åœ¨æœºåˆ¶ï¼Œæ˜¾è‘—æ·±åŒ–äº†å¯¹è¯¥èŒƒå¼çš„ç†è§£ã€‚åŸºäºç†è®ºæ´å¯Ÿï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥è®¾è®¡äº†åº”ç”¨äºè§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„æ–°æ–¹æ³• DRRho-CLIPã€‚å®éªŒç»“æœè¯å®ï¼ŒDRRho-CLIP åœ¨ Scaling Laws ä¸Šè¡¨ç°ä¼˜äºæ ‡å‡† CLIPï¼Œä¸”åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¯å‘å¼æ–¹æ³•ï¼Œä¸ºæ¨¡å‹å¼•å¯¼è®­ç»ƒæä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.06699v3",
      "published_date": "2025-05-10 16:55:03 UTC",
      "updated_date": "2025-05-17 02:26:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:11.123124+00:00"
    },
    {
      "arxiv_id": "2505.06694v1",
      "title": "Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search",
      "title_zh": "åŸºäºæ£€æµ‹ Transformer ä¸é›¶æ ·æœ¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢çš„å£°çº³å›¾åƒæ°´ä¸‹ç›®æ ‡æ£€æµ‹",
      "authors": [
        "XiaoTong Gu",
        "Shengyu Tang",
        "Yiming Cao",
        "Changdong Yu"
      ],
      "abstract": "Underwater object detection using sonar imagery has become a critical and rapidly evolving research domain within marine technology. However, sonar images are characterized by lower resolution and sparser features compared to optical images, which seriously degrades the performance of object detection.To address these challenges, we specifically propose a Detection Transformer (DETR) architecture optimized with a Neural Architecture Search (NAS) approach called NAS-DETR for object detection in sonar images. First, an improved Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy principle is proposed to identify a real-time, high-representational-capacity CNN-Transformer backbone for sonar image detection. This method enables the efficient discovery of high-performance network architectures with low computational and time overhead. Subsequently, the backbone is combined with a Feature Pyramid Network (FPN) and a deformable attention-based Transformer decoder to construct a complete network architecture. This architecture integrates various advanced components and training schemes to enhance overall performance. Extensive experiments demonstrate that this architecture achieves state-of-the-art performance on two Representative datasets, while maintaining minimal overhead in real-time efficiency and computational complexity. Furthermore, correlation analysis between the key parameters and differential entropy-based fitness function is performed to enhance the interpretability of the proposed framework. To the best of our knowledge, this is the first work in the field of sonar object detection to integrate the DETR architecture with a NAS search mechanism.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å£°çº³å›¾åƒ (sonar imagery) åˆ†è¾¨ç‡ä½ä¸”ç‰¹å¾ç¨€ç–å¯¼è‡´çš„æ°´ä¸‹ç›®æ ‡æ£€æµ‹æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º NAS-DETR çš„ä¼˜åŒ–æ£€æµ‹æ¶æ„ã€‚è¯¥æ¶æ„é¦–å…ˆå¼•å…¥äº†ä¸€ç§åŸºäºæœ€å¤§ç†µåŸç† (maximum entropy principle) æ”¹è¿›çš„é›¶æ ·æœ¬ç¥ç»æ¶æ„æœç´¢ (Zero-shot Neural Architecture Search, NAS) æ–¹æ³•ï¼Œç”¨ä»¥é«˜æ•ˆç­›é€‰å…·å¤‡å®æ—¶æ€§ä¸”é«˜è¡¨å¾èƒ½åŠ›çš„ CNN-Transformer éª¨å¹²ç½‘ç»œã€‚éšåï¼Œç ”ç©¶è€…å°†è¯¥éª¨å¹²ç½‘ç»œä¸ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ (Feature Pyramid Network, FPN) ä»¥åŠåŸºäºå¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶ (deformable attention) çš„ Transformer è§£ç å™¨ç›¸ç»“åˆï¼Œæ„å»ºäº†å®Œæ•´çš„æ£€æµ‹ç½‘ç»œã€‚è¯¥æ¶æ„é€šè¿‡æ•´åˆå¤šç§å…ˆè¿›ç»„ä»¶å’Œè®­ç»ƒæ–¹æ¡ˆï¼Œæ—¨åœ¨å…¨é¢æå‡å£°çº³å›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNAS-DETR åœ¨ä¸¤ä¸ªä»£è¡¨æ€§æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ (state-of-the-art) æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶åœ¨å®æ—¶æ•ˆç‡å’Œè®¡ç®—å¤æ‚åº¦æ–¹é¢ä¿æŒäº†æä½å¼€é”€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ç›¸å…³æ€§åˆ†æå¢å¼ºäº†æ¡†æ¶çš„å¯è§£é‡Šæ€§ï¼Œè¿™ä¹Ÿæ˜¯è¯¥é¢†åŸŸé¦–æ¬¡å°† Detection Transformer (DETR) æ¶æ„ä¸ NAS æœç´¢æœºåˆ¶ç›¸ç»“åˆçš„å°è¯•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06694v1",
      "published_date": "2025-05-10 16:41:09 UTC",
      "updated_date": "2025-05-10 16:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:16.793723+00:00"
    },
    {
      "arxiv_id": "2505.06684v1",
      "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels",
      "title_zh": "FNBenchï¼šé’ˆå¯¹å™ªå£°æ ‡ç­¾çš„é²æ£’è”é‚¦å­¦ä¹ åŸºå‡†æµ‹è¯•",
      "authors": [
        "Xuefeng Jiang",
        "Jia Li",
        "Nannan Wu",
        "Zhiyuan Wu",
        "Xujing Li",
        "Sheng Sun",
        "Gang Xu",
        "Yuwei Wang",
        "Qi Li",
        "Min Liu"
      ],
      "abstract": "Robustness to label noise within data is a significant challenge in federated learning (FL). From the data-centric perspective, the data quality of distributed datasets can not be guaranteed since annotations of different clients contain complicated label noise of varying degrees, which causes the performance degradation. There have been some early attempts to tackle noisy labels in FL. However, there exists a lack of benchmark studies on comprehensively evaluating their practical performance under unified settings. To this end, we propose the first benchmark study FNBench to provide an experimental investigation which considers three diverse label noise patterns covering synthetic label noise, imperfect human-annotation errors and systematic errors. Our evaluation incorporates eighteen state-of-the-art methods over five image recognition datasets and one text classification dataset. Meanwhile, we provide observations to understand why noisy labels impair FL, and additionally exploit a representation-aware regularization method to enhance the robustness of existing methods against noisy labels based on our observations. Finally, we discuss the limitations of this work and propose three-fold future directions. To facilitate related communities, our source code is open-sourced at https://github.com/Sprinter1999/FNBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FNBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è”é‚¦å­¦ä¹  (Federated Learning) ä¸­å™ªå£°æ ‡ç­¾ (Noisy Labels) ç¨³å¥æ€§è¯„ä¼°çš„åŸºå‡†ç ”ç©¶ã€‚é’ˆå¯¹åˆ†å¸ƒå¼æ•°æ®é›†ä¸­æ ‡æ³¨è´¨é‡éš¾ä»¥ä¿è¯å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼ŒFNBench æä¾›äº†ç»Ÿä¸€çš„å®éªŒæ¡†æ¶ï¼Œæ¶µç›–äº†åˆæˆæ ‡ç­¾å™ªå£°ã€äººç±»æ ‡æ³¨é”™è¯¯å’Œç³»ç»Ÿè¯¯å·®ä¸‰ç§å™ªå£°æ¨¡å¼ã€‚è¯¥åŸºå‡†åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šæ•´åˆå¹¶è¯„ä¼°äº† 18 ç§æœ€å…ˆè¿› (State-of-the-art) çš„æ–¹æ³•ï¼Œä¸ºç†è§£å™ªå£°æ ‡ç­¾å¦‚ä½•æŸå®³è”é‚¦å­¦ä¹ æä¾›äº†æ·±å…¥è§è§£ã€‚æ­¤å¤–ï¼Œä½œè€…åŸºäºè§‚å¯Ÿç»“æœæå‡ºäº†ä¸€ç§è¡¨å¾æ„ŸçŸ¥æ­£åˆ™åŒ– (Representation-aware regularization) æ–¹æ³•ï¼Œæœ‰æ•ˆå¢å¼ºäº†ç°æœ‰ç®—æ³•å¯¹æŠ—å™ªå£°æ ‡ç­¾çš„ç¨³å¥æ€§ã€‚è¯¥å·¥ä½œåŠå…¶å¼€æºä»£ç ä¸ºè§£å†³è”é‚¦å­¦ä¹ ä¸­çš„æ•°æ®è´¨é‡æŒ‘æˆ˜æä¾›äº†é‡è¦çš„å‚è€ƒå·¥å…·å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to IEEE TDSC, currently under major revision",
      "pdf_url": "https://arxiv.org/pdf/2505.06684v1",
      "published_date": "2025-05-10 16:14:52 UTC",
      "updated_date": "2025-05-10 16:14:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:31.884330+00:00"
    },
    {
      "arxiv_id": "2505.07882v1",
      "title": "Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey",
      "title_zh": "åŸºäºæœºå™¨å­¦ä¹ æ–¹æ³•çš„æ™ºèƒ½ç½‘è”æ±½è½¦ä¿¡ä»»ç®¡ç†ç³»ç»Ÿå¢å¼ºæŠ€æœ¯ç»¼è¿°",
      "authors": [
        "Qian Xu",
        "Lei Zhang",
        "Yixiao Liu"
      ],
      "abstract": "Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and multi-domain networks, rendering them vulnerable to various threats. Trust Management Systems (TMS) systematically organize essential steps in the trust mechanism, identifying malicious nodes against internal threats and external threats, as well as ensuring reliable decision-making for more cooperative tasks. Recent advances in machine learning (ML) offer significant potential to enhance TMS, especially for the strict requirements of CAVs, such as CAV nodes moving at varying speeds, and opportunistic and intermittent network behavior. Those features distinguish ML-based TMS from social networks, static IoT, and Social IoT. This survey proposes a novel three-layer ML-based TMS framework for CAVs in the vehicle-road-cloud integration system, i.e., trust data layer, trust calculation layer and trust incentive layer. A six-dimensional taxonomy of objectives is proposed. Furthermore, the principles of ML methods for each module in each layer are analyzed. Then, recent studies are categorized based on traffic scenarios that are against the proposed objectives. Finally, future directions are suggested, addressing the open issues and meeting the research trend. We maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/octoberzzzzz/ML-based-TMS-CAV-Survey.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†åˆ©ç”¨Machine Learning (ML)æ–¹æ³•å¢å¼ºConnected Autonomous Vehicles (CAVs)çš„Trust Management Systems (TMS)çš„ç°çŠ¶ï¼Œæ—¨åœ¨åº”å¯¹åŠ¨æ€å¼€æ”¾ç½‘ç»œç¯å¢ƒä¸­çš„å®‰å…¨å¨èƒã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†CAVsèŠ‚ç‚¹é«˜é€Ÿç§»åŠ¨å’Œç½‘ç»œè¡Œä¸ºé—´æ­‡æ€§ç­‰ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé¢å‘è½¦è·¯äº‘ä¸€ä½“åŒ–ç³»ç»Ÿçš„ä¸‰å±‚ML-based TMSæ¡†æ¶ï¼Œå…·ä½“åŒ…å«trust data layerã€trust calculation layerå’Œtrust incentive layerã€‚æœ¬æ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªsix-dimensional taxonomy of objectivesï¼Œå¹¶ç³»ç»Ÿå‰–æäº†å„å±‚ä¸­ä¸åŒMLæ–¹æ³•çš„åº”ç”¨åŸç†ä¸æŠ€æœ¯ç»†èŠ‚ã€‚é€šè¿‡æ ¹æ®äº¤é€šåœºæ™¯å¯¹ç°æœ‰ç ”ç©¶è¿›è¡Œåˆ†ç±»ï¼Œè®ºæ–‡å±•ç°äº†å¦‚ä½•é€šè¿‡MLæŠ€æœ¯è¯†åˆ«æ¶æ„èŠ‚ç‚¹å¹¶ç¡®ä¿åä½œä»»åŠ¡ä¸­çš„å¯é å†³ç­–ã€‚æœ€åï¼Œè¯¥ç»¼è¿°æŒ‡æ˜äº†è§£å†³å½“å‰å¼€æ”¾æ€§é—®é¢˜å¹¶ç¬¦åˆæœªæ¥CAVsä¸¥è‹›è¦æ±‚çš„ä¿¡ä»»ç®¡ç†ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.07882v1",
      "published_date": "2025-05-10 16:13:36 UTC",
      "updated_date": "2025-05-10 16:13:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:22.196365+00:00"
    },
    {
      "arxiv_id": "2505.06682v1",
      "title": "A Short Overview of Multi-Modal Wi-Fi Sensing",
      "title_zh": "å¤šæ¨¡æ€ Wi-Fi æ„ŸçŸ¥æŠ€æœ¯ç®€è¿°",
      "authors": [
        "Zijian Zhao"
      ],
      "abstract": "Wi-Fi sensing has emerged as a significant technology in wireless sensing and Integrated Sensing and Communication (ISAC), offering benefits such as low cost, high penetration, and enhanced privacy. Currently, it is widely utilized in various applications, including action recognition, human localization, and crowd counting. However, Wi-Fi sensing also faces challenges, such as low robustness and difficulties in data collection. Recently, there has been an increasing focus on multi-modal Wi-Fi sensing, where other modalities can act as teachers, providing ground truth or robust features for Wi-Fi sensing models to learn from, or can be directly fused with Wi-Fi for enhanced sensing capabilities. Although these methods have demonstrated promising results and substantial value in practical applications, there is a lack of comprehensive surveys reviewing them. To address this gap, this paper reviews the multi-modal Wi-Fi sensing literature \\textbf{from the past 24 months} and highlights the current limitations, challenges and future directions in this field.",
      "tldr_zh": "æœ¬æ–‡ç»¼è¿°äº†è¿‡å»24ä¸ªæœˆå†…å¤šæ¨¡æ€Wi-Fiæ„ŸçŸ¥(Multi-Modal Wi-Fi Sensing)é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œæ¢è®¨äº†å…¶åœ¨æ— çº¿æ„ŸçŸ¥ä¸é›†æˆæ„ŸçŸ¥é€šä¿¡(ISAC)ä¸­çš„é‡è¦åœ°ä½ã€‚è™½ç„¶Wi-Fiæ„ŸçŸ¥åœ¨åŠ¨ä½œè¯†åˆ«ã€äººå‘˜å®šä½ç­‰åº”ç”¨ä¸­å…·æœ‰ä½æˆæœ¬å’Œéšç§ä¿æŠ¤ç­‰ä¼˜åŠ¿ï¼Œä½†ä»å­˜åœ¨é²æ£’æ€§ä¸è¶³å’Œæ•°æ®é‡‡é›†å›°éš¾çš„æŒ‘æˆ˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé€šè¿‡å¼•å…¥å…¶ä»–æ¨¡æ€ä½œä¸ºæ•™å¸ˆæ¨¡å‹æä¾›ground truthæˆ–é²æ£’ç‰¹å¾ï¼Œæˆ–å°†å…¶ä¸Wi-Fiä¿¡å·ç›´æ¥èåˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿçš„æ„ŸçŸ¥æ•ˆèƒ½ã€‚è¯¥è®ºæ–‡å¡«è¡¥äº†å¤šæ¨¡æ€Wi-Fiæ„ŸçŸ¥é¢†åŸŸç¼ºä¹ç³»ç»Ÿæ€§ç»¼è¿°çš„ç©ºç™½ï¼Œè¯¦ç»†åˆ†æäº†å„ç§å¤šæ¨¡æ€æ–¹æ³•çš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†å½“å‰æŠ€æœ¯é¢ä¸´çš„å±€é™æ€§ä¸æŒ‘æˆ˜ï¼Œå¹¶æŒ‡æ˜äº†è¯¥é¢†åŸŸæœªæ¥çš„é‡ç‚¹ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06682v1",
      "published_date": "2025-05-10 16:12:56 UTC",
      "updated_date": "2025-05-10 16:12:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:38.493035+00:00"
    },
    {
      "arxiv_id": "2505.06680v1",
      "title": "A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions",
      "title_zh": "äººç±»é©¾é©¶å‘˜æ¢é“å†³ç­–çš„æ•°æ®é©±åŠ¨å»ºæ¨¡ç»¼è¿°",
      "authors": [
        "Linxuan Huang",
        "Dong-Fan Xie",
        "Li Li",
        "Zhengbing He"
      ],
      "abstract": "Lane-changing (LC) behavior, a critical yet complex driving maneuver, significantly influences driving safety and traffic dynamics. Traditional analytical LC decision (LCD) models, while effective in specific environments, often oversimplify behavioral heterogeneity and complex interactions, limiting their capacity to capture real LCD. Data-driven approaches address these gaps by leveraging rich empirical data and machine learning to decode latent decision-making patterns, enabling adaptive LCD modeling in dynamic environments. In light of the rapid development of artificial intelligence and the demand for data-driven models oriented towards connected vehicles and autonomous vehicles, this paper presents a comprehensive survey of data-driven LCD models, with a particular focus on human drivers LC decision-making. It systematically reviews the modeling framework, covering data sources and preprocessing, model inputs and outputs, objectives, structures, and validation methods. This survey further discusses the opportunities and challenges faced by data-driven LCD models, including driving safety, uncertainty, as well as the integration and improvement of technical frameworks.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡é’ˆå¯¹äººç±»é©¾é©¶å‘˜çš„æ¢é“å†³ç­– (Lane-Changing Decisions, LCD) æ•°æ®é©±åŠ¨å»ºæ¨¡è¿›è¡Œäº†å…¨é¢å›é¡¾ã€‚è€ƒè™‘åˆ°ä¼ ç»Ÿåˆ†ææ¨¡å‹åœ¨æ•æ‰è¡Œä¸ºå¼‚è´¨æ€§å’Œå¤æ‚äº¤äº’æ–¹é¢çš„å±€é™æ€§ï¼Œæ–‡ç« é‡ç‚¹æ¢è®¨äº†åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯ä»æµ·é‡ç»éªŒæ•°æ®ä¸­è§£ç æ½œåœ¨å†³ç­–æ¨¡å¼çš„æ–¹æ³•ã€‚è®ºæ–‡ç³»ç»Ÿæ€§åœ°å®¡è§†äº†å»ºæ¨¡æ¡†æ¶ï¼Œæ¶µç›–äº†æ•°æ®æ¥æºä¸é¢„å¤„ç†ã€æ¨¡å‹è¾“å…¥è¾“å‡ºã€ç›®æ ‡å‡½æ•°ã€æ¨¡å‹æ¶æ„åŠéªŒè¯æ‰‹æ®µã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†æ•°æ®é©±åŠ¨ LCD æ¨¡å‹åœ¨è¡Œè½¦å®‰å…¨ã€ä¸ç¡®å®šæ€§å¤„ç†ä»¥åŠæŠ€æœ¯æ¡†æ¶é›†æˆæ–¹é¢é¢ä¸´çš„æœºé‡ä¸æŒ‘æˆ˜ã€‚è¯¥ç»¼è¿°ä¸ºé¢å‘è”ç½‘è½¦è¾†å’Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è‡ªé€‚åº”æ¢é“å»ºæ¨¡æä¾›äº†å…³é”®çš„å‚è€ƒä¸æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.SY",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06680v1",
      "published_date": "2025-05-10 16:09:03 UTC",
      "updated_date": "2025-05-10 16:09:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:40.591032+00:00"
    },
    {
      "arxiv_id": "2505.07879v3",
      "title": "OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval",
      "title_zh": "OMGMï¼šååŒå¤šç²’åº¦ä¸å¤šæ¨¡æ€çš„é«˜æ•ˆå¤šæ¨¡æ€æ£€ç´¢",
      "authors": [
        "Wei Yang",
        "Jingjing Fu",
        "Rui Wang",
        "Jinyu Wang",
        "Lei Song",
        "Jiang Bian"
      ],
      "abstract": "Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”(Knowledge-Based Visual Question Answering, KB-VQA)ä¸­å¤šæ¨¡æ€æ£€ç´¢é¢ä¸´çš„æ¨¡æ€å¤šæ ·æ€§å’ŒçŸ¥è¯†ç²’åº¦ä¸ä¸€çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºOMGMçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸€ç§ä»ç²—åˆ°ç»†(coarse-to-fine)çš„å¤šæ­¥éª¤æ£€ç´¢æœºåˆ¶ï¼Œé€šè¿‡åè°ƒå¤šç§ç²’åº¦å’Œæ¨¡æ€æ¥æå‡æ£€ç´¢æ•ˆèƒ½ã€‚é¦–å…ˆï¼Œç³»ç»Ÿè¿›è¡Œå¹¿æ³›çš„åˆå§‹æœç´¢ä»¥å¯¹é½çŸ¥è¯†ç²’åº¦å¹¶å®ç°è·¨æ¨¡æ€æ£€ç´¢ï¼Œéšååˆ©ç”¨å¤šæ¨¡æ€èåˆé‡æ’åº(Multimodal Fusion Reranking)æ•æ‰ç»†å¾®ä¿¡æ¯ä»¥ç­›é€‰æ ¸å¿ƒå®ä½“ã€‚æœ€åï¼Œé€šè¿‡æ–‡æœ¬é‡æ’åºå™¨(Text Reranker)å®šä½æœ€ç›¸å…³çš„ç»†ç²’åº¦ç« èŠ‚ï¼Œä¸ºåç»­çš„å¢å¼ºç”Ÿæˆæä¾›æ”¯æŒã€‚åœ¨InfoSeekå’ŒEncyclopedic-VQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOMGMåœ¨æ£€ç´¢æ€§èƒ½ä¸Šè¾¾åˆ°äº†å…ˆè¿›(State-of-the-art)æ°´å¹³ï¼Œå¹¶åœ¨é—®ç­”ä»»åŠ¡ä¸­å±•ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³å¤æ‚çŸ¥è¯†èƒŒæ™¯ä¸‹çš„å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢æä¾›äº†é«˜æ•ˆçš„ç³»ç»Ÿæ¡†æ¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2505.07879v3",
      "published_date": "2025-05-10 14:24:41 UTC",
      "updated_date": "2025-09-12 11:47:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:27:57.175719+00:00"
    },
    {
      "arxiv_id": "2505.06652v1",
      "title": "Enfoque Odychess: Un mÃ©todo dialÃ©ctico, constructivista y adaptativo para la enseÃ±anza del ajedrez con inteligencias artificiales generativas",
      "title_zh": "Odychess æ–¹æ³•ï¼šä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¾©è¯ã€å»ºæ„ä¸»ä¹‰ä¸è‡ªé€‚åº”å›½é™…è±¡æ£‹æ•™å­¦æ³•",
      "authors": [
        "Ernesto Giralt Hernandez",
        "Lazaro Antonio Bueno Perez"
      ],
      "abstract": "Chess teaching has evolved through different approaches, however, traditional methodologies, often based on memorization, contrast with the new possibilities offered by generative artificial intelligence, a technology still little explored in this field. This study seeks to empirically validate the effectiveness of the Odychess Approach in improving chess knowledge, strategic understanding, and metacognitive skills in students. A quasi-experimental study was conducted with a pre-test/post-test design and a control group (N=60). The experimental intervention implemented the Odychess Approach, incorporating a Llama 3.3 language model that was specifically adapted using Parameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess tutor. Quantitative assessment instruments were used to measure chess knowledge, strategic understanding, and metacognitive skills before and after the intervention. The results of the quasi-experimental study showed significant improvements in the experimental group compared to the control group in the three variables analyzed: chess knowledge, strategic understanding, and metacognitive skills. The complementary qualitative analysis revealed greater analytical depth, more developed dialectical reasoning, and increased intrinsic motivation in students who participated in the Odychess method-based intervention. The Odychess Approach represents an effective pedagogical methodology for teaching chess, demonstrating the potential of the synergistic integration of constructivist and dialectical principles with generative artificial intelligence. The implications of this work are relevant for educators and institutions interested in adopting innovative pedagogical technologies and for researchers in the field of AI applied to education, highlighting the transferability of the language model adaptation methodology to other educational domains.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† Odychess Approachï¼Œè¿™æ˜¯ä¸€ç§å°†è¾©è¯æ³•ã€å»ºæ„ä¸»ä¹‰åŸåˆ™ä¸ Generative Artificial Intelligence ç›¸ç»“åˆçš„è‡ªé€‚åº”è±¡æ£‹æ•™å­¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ä¼ ç»Ÿè±¡æ£‹æ•™å­¦è¿‡åº¦ä¾èµ–è®°å¿†çš„é—®é¢˜ï¼Œé€šè¿‡ Parameter-Efficient Fine-Tuning (PEFT) æŠ€æœ¯å¯¹ Llama 3.3 æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œä½¿å…¶å……å½“ Socratic chess tutorã€‚åœ¨ä¸€é¡¹æ¶‰åŠ 60 åå‚ä¸è€…çš„ Quasi-experimental study ä¸­ï¼Œå®éªŒç»„åœ¨è±¡æ£‹çŸ¥è¯†ã€æˆ˜ç•¥ç†è§£å’Œ Metacognitive skills æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜äºå¯¹ç…§ç»„çš„è¿›æ­¥ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†è¯¥æ–¹æ³•åœ¨æå‡å­¦ç”Ÿåˆ†ææ·±åº¦ã€è¾©è¯æ¨ç†æ°´å¹³å’Œå†…åœ¨åŠ¨æœºæ–¹é¢çš„ç§¯æä½œç”¨ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å»ºæ„ä¸»ä¹‰ç†å¿µä¸å¤§è¯­è¨€æ¨¡å‹ååŒçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸º AI åœ¨å…¶ä»–æ•™è‚²é¢†åŸŸçš„åº”ç”¨æä¾›äº†å…·æœ‰ Transferability çš„æ¨¡å‹é€‚é…æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Full article in Spanish",
      "pdf_url": "https://arxiv.org/pdf/2505.06652v1",
      "published_date": "2025-05-10 13:58:47 UTC",
      "updated_date": "2025-05-10 13:58:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:28:01.153760+00:00"
    },
    {
      "arxiv_id": "2505.06651v1",
      "title": "Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee",
      "title_zh": "Dyn-D$^2$Pï¼šå…·æœ‰å¯è¯æ˜æ•ˆç”¨ä¿è¯çš„åŠ¨æ€å·®åˆ†éšç§å»ä¸­å¿ƒåŒ–å­¦ä¹ ",
      "authors": [
        "Zehan Zhu",
        "Yan Huang",
        "Xin Wang",
        "Shouling Ji",
        "Jinming Xu"
      ],
      "abstract": "Most existing decentralized learning methods with differential privacy (DP) guarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian noises for each node throughout the training process, leading to a significant accuracy degradation compared to non-private counterparts. In this paper, we propose a new Dynamic Differentially Private Decentralized learning approach (termed Dyn-D$^2$P) tailored for general time-varying directed networks. Leveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P dynamically adjusts gradient clipping bounds and noise levels based on gradient convergence. This proposed dynamic noise strategy enables us to enhance model accuracy while preserving the total privacy budget. Extensive experiments on benchmark datasets demonstrate the superiority of Dyn-D$^2$P over its counterparts employing fixed-level noises, especially under strong privacy guarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P that establishes an explicit dependency on network-related parameters, with a scaling factor of $1/\\sqrt{n}$ in terms of the number of nodes $n$ up to a bias error term induced by gradient clipping. To our knowledge, this is the first model utility analysis for differentially private decentralized non-convex optimization with dynamic gradient clipping bounds and noise levels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å»ä¸­å¿ƒåŒ–å­¦ä¹ ä¸­å·®åˆ†éšç§(Differential Privacy)å› ä½¿ç”¨å›ºå®šæ¢¯åº¦è£å‰ªé˜ˆå€¼å’Œå™ªå£°æ°´å¹³å¯¼è‡´æ¨¡å‹å‡†ç¡®ç‡å¤§å¹…ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº† Dyn-D$^2$P æ¡†æ¶ã€‚è¯¥æ–¹æ³•ä¸“ä¸ºé€šç”¨çš„æ—¶å˜æœ‰å‘ç½‘ç»œè®¾è®¡ï¼Œåˆ©ç”¨é«˜æ–¯å·®åˆ†éšç§(Gaussian DP)æ¡†æ¶ï¼Œæ ¹æ®æ¢¯åº¦çš„æ”¶æ•›æƒ…å†µåŠ¨æ€è°ƒæ•´æ¢¯åº¦è£å‰ªè¾¹ç•Œå’Œå™ªå£°æ°´å¹³ã€‚è¿™ç§åŠ¨æ€ç­–ç•¥åœ¨ä¿æŒæ€»éšç§é¢„ç®—çš„å‰æä¸‹ï¼Œé€šè¿‡ä¼˜åŒ–å™ªå£°åˆ†é…æ˜¾è‘—æå‡äº†æ¨¡å‹çš„è®­ç»ƒç²¾åº¦ï¼Œå°¤å…¶åœ¨å¼ºéšç§ä¿è¯åœºæ™¯ä¸‹è¡¨ç°ä¼˜äºä¼ ç»Ÿå›ºå®šå™ªå£°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä¸ºè¯¥ç®—æ³•æä¾›äº†é¦–ä¸ªé’ˆå¯¹éå‡¸ä¼˜åŒ–çš„å¯è¯æ˜æ•ˆç”¨ç•Œé™(Utility Bound)ï¼Œå»ºç«‹äº†æ•ˆç”¨ä¸ç½‘ç»œå‚æ•°åŠèŠ‚ç‚¹æ•°é‡ $n$ ä¹‹é—´æŒ‰ $1/\\sqrt{n}$ ç¼©æ”¾çš„æ˜¾å¼ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœéªŒè¯äº† Dyn-D$^2$P åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½ã€å¼ºéšç§ä¿æŠ¤çš„åˆ†å¸ƒå¼å­¦ä¹ ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºä¸å®è·µæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by the 34th International Joint Conference on Artificial Intelligence(IJCAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.06651v1",
      "published_date": "2025-05-10 13:57:57 UTC",
      "updated_date": "2025-05-10 13:57:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:28:07.774424+00:00"
    },
    {
      "arxiv_id": "2505.06637v1",
      "title": "Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers",
      "title_zh": "æ¢ç´¢å¤šæ¨¡æ€åŸºç¡€ AI ä¸ä¸“å®¶åœ¨ç¯æœºåˆ¶åœ¨åŸä½æ°‘æ²³æµé‡ç”Ÿé²‘é±¼æ¸”ä¸šå¯æŒç»­ç®¡ç†ä¸­çš„åº”ç”¨",
      "authors": [
        "Chi Xu",
        "Yili Jin",
        "Sami Ma",
        "Rongsheng Qian",
        "Hao Fang",
        "Jiangchuan Liu",
        "Xue Liu",
        "Edith C. H. Ngai",
        "William I. Atlas",
        "Katrina M. Connors",
        "Mark A. Spoljaric"
      ],
      "abstract": "Wild salmon are essential to the ecological, economic, and cultural sustainability of the North Pacific Rim. Yet climate variability, habitat loss, and data limitations in remote ecosystems that lack basic infrastructure support pose significant challenges to effective fisheries management. This project explores the integration of multimodal foundation AI and expert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable fisheries management in Indigenous rivers across Pacific Northwest. By leveraging video and sonar-based monitoring, we develop AI-powered tools for automated species identification, counting, and length measurement, reducing manual effort, expediting delivery of results, and improving decision-making accuracy. Expert validation and active learning frameworks ensure ecological relevance while reducing annotation burdens. To address unique technical and societal challenges, we bring together a cross-domain, interdisciplinary team of university researchers, fisheries biologists, Indigenous stewardship practitioners, government agencies, and conservation organizations. Through these collaborations, our research fosters ethical AI co-development, open data sharing, and culturally informed fisheries management.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†å¤šæ¨¡æ€åŸºç¡€äººå·¥æ™ºèƒ½(Multimodal Foundation AI)ä¸ä¸“å®¶åœ¨ç¯(Expert-in-the-Loop)æ¡†æ¶çš„é›†æˆï¼Œæ—¨åœ¨åº”å¯¹ç”±äºæ°”å€™å˜åŒ–ã€æ –æ¯åœ°ä¸§å¤±åŠæ•°æ®å±€é™ç»™åè¿œåŸä½æ°‘æ²³æµå¸¦æ¥çš„é‡ç”Ÿä¸‰æ–‡é±¼æ¸”ä¸šç®¡ç†æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨åŸºäºè§†é¢‘å’Œå£°çº³çš„ç›‘æµ‹æŠ€æœ¯ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ç”¨äºè‡ªåŠ¨ç‰©ç§è¯†åˆ«ã€è®¡æ•°å’Œé•¿åº¦æµ‹é‡çš„äººå·¥æ™ºèƒ½å·¥å…·ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†äººå·¥æŠ•å…¥å¹¶æé«˜äº†å†³ç­–çš„å‡†ç¡®æ€§å’Œäº¤ä»˜é€Ÿåº¦ã€‚ä¸“å®¶éªŒè¯å’Œä¸»åŠ¨å­¦ä¹ (Active Learning)æ¡†æ¶çš„åº”ç”¨ï¼Œåœ¨æœ‰æ•ˆå‡è½»æ•°æ®æ ‡æ³¨è´Ÿæ‹…çš„åŒæ—¶ï¼Œç¡®ä¿äº†ç›‘æµ‹ç»“æœçš„ç”Ÿæ€å­¦ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡è·¨é¢†åŸŸçš„å­¦ç§‘åˆä½œï¼Œæ¨åŠ¨äº†ç¬¦åˆä¼¦ç†çš„äººå·¥æ™ºèƒ½å…±åŒå¼€å‘ã€æ•°æ®å¼€æ”¾å…±äº«ä»¥åŠå—æ–‡åŒ–å¯å‘çš„å¯æŒç»­æ¸”ä¸šç®¡ç†æ¨¡å¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, accepted by IJCAI 2025, AI and Social Good Track",
      "pdf_url": "https://arxiv.org/pdf/2505.06637v1",
      "published_date": "2025-05-10 13:03:06 UTC",
      "updated_date": "2025-05-10 13:03:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:30:27.192634+00:00"
    },
    {
      "arxiv_id": "2505.06632v1",
      "title": "AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles",
      "title_zh": "åŸºäºäººå·¥æ™ºèƒ½ä¸åŒºå—é“¾çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦å¼‚å¸¸æ£€æµ‹ï¼šå®ç°å®æ—¶å®‰å…¨æ€§ä¸å¯é æ€§",
      "authors": [
        "Rathin Chandra Shit",
        "Sharmila Subudhi"
      ],
      "abstract": "Autonomous Vehicles (AV) proliferation brings important and pressing security and reliability issues that must be dealt with to guarantee public safety and help their widespread adoption. The contribution of the proposed research is towards achieving more secure, reliable, and trustworthy autonomous transportation system by providing more capabilities for anomaly detection, data provenance, and real-time response in safety critical AV deployments. In this research, we develop a new framework that combines the power of Artificial Intelligence (AI) for real-time anomaly detection with blockchain technology to detect and prevent any malicious activity including sensor failures in AVs. Through Long Short-Term Memory (LSTM) networks, our approach continually monitors associated multi-sensor data streams to detect anomalous patterns that may represent cyberattacks as well as hardware malfunctions. Further, this framework employs a decentralized platform for securely storing sensor data and anomaly alerts in a blockchain ledger for data incorruptibility and authenticity, while offering transparent forensic features. Moreover, immediate automated response mechanisms are deployed using smart contracts when anomalies are found. This makes the AV system more resilient to attacks from both cyberspace and hardware component failure. Besides, we identify potential challenges of scalability in handling high frequency sensor data, computational constraint in resource constrained environment, and of distributed data storage in terms of privacy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆArtificial Intelligence (AI)ä¸åŒºå—é“¾æŠ€æœ¯çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æå‡Autonomous Vehicles (AVs)çš„å®‰å…¨æ€§ä¸å¯é æ€§ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨Long Short-Term Memory (LSTM)ç½‘ç»œå¯¹å¤šä¼ æ„Ÿå™¨æ•°æ®æµè¿›è¡Œå®æ—¶ç›‘æ§ï¼Œä»¥å‡†ç¡®è¯†åˆ«ç”±ç½‘ç»œæ”»å‡»æˆ–ç¡¬ä»¶æ•…éšœå¼•èµ·çš„å¼‚å¸¸æ¨¡å¼ã€‚é€šè¿‡åŒºå—é“¾çš„å»ä¸­å¿ƒåŒ–å¹³å°ï¼Œæ¡†æ¶ç¡®ä¿äº†ä¼ æ„Ÿå™¨æ•°æ®å’Œå¼‚å¸¸å‘Šè­¦çš„ä¸å¯ç¯¡æ”¹æ€§ä¸çœŸå®æ€§ï¼Œå¹¶æä¾›äº†é€æ˜çš„æ•°å­—å–è¯åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿåˆ©ç”¨Smart Contractsåœ¨æ£€æµ‹åˆ°å¼‚å¸¸æ—¶æ‰§è¡Œå³æ—¶è‡ªåŠ¨åŒ–å“åº”ï¼Œæ˜¾è‘—å¢å¼ºäº†AVç³»ç»Ÿå¯¹æŠ—ç½‘ç»œç©ºé—´æ”»å‡»å’Œç¡¬ä»¶å¤±æ•ˆçš„éŸ§æ€§ã€‚è¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†åœ¨é«˜é¢‘æ•°æ®æ‰©å±•æ€§ã€å—é™ç¯å¢ƒè®¡ç®—å‹åŠ›ä»¥åŠåˆ†å¸ƒå¼å­˜å‚¨éšç§ä¿æŠ¤ç­‰æ–¹é¢çš„æ½œåœ¨æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Scheduled for presentation at an upcoming conference",
      "pdf_url": "https://arxiv.org/pdf/2505.06632v1",
      "published_date": "2025-05-10 12:53:28 UTC",
      "updated_date": "2025-05-10 12:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:28:40.922221+00:00"
    },
    {
      "arxiv_id": "2505.06630v1",
      "title": "Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis",
      "title_zh": "é¢å‘å¤šé¢†åŸŸæƒ…æ„Ÿåˆ†æçš„åŠ¨æ€é¢†åŸŸä¿¡æ¯è°ƒåˆ¶ç®—æ³•",
      "authors": [
        "Chunyi Yue",
        "Ang Li"
      ],
      "abstract": "Multi-domain sentiment classification aims to mitigate poor performance models due to the scarcity of labeled data in a single domain, by utilizing data labeled from various domains. A series of models that jointly train domain classifiers and sentiment classifiers have demonstrated their advantages, because domain classification helps generate necessary information for sentiment classification. Intuitively, the importance of sentiment classification tasks is the same in all domains for multi-domain sentiment classification; but domain classification tasks are different because the impact of domain information on sentiment classification varies across different fields; this can be controlled through adjustable weights or hyper parameters. However, as the number of domains increases, existing hyperparameter optimization algorithms may face the following challenges: (1) tremendous demand for computing resources, (2) convergence problems, and (3) high algorithm complexity. To efficiently generate the domain information required for sentiment classification in each domain, we propose a dynamic information modulation algorithm. Specifically, the model training process is divided into two stages. In the first stage, a shared hyperparameter, which would control the proportion of domain classification tasks across all fields, is determined. In the second stage, we introduce a novel domain-aware modulation algorithm to adjust the domain information contained in the input text, which is then calculated based on a gradient-based and loss-based method. In summary, experimental results on a public sentiment analysis dataset containing 16 domains prove the superiority of the proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŠ¨æ€é¢†åŸŸä¿¡æ¯è°ƒåˆ¶ç®—æ³•(Dynamic Domain Information Modulation)ï¼Œæ—¨åœ¨è§£å†³å¤šé¢†åŸŸæƒ…æ„Ÿåˆ†æ(Multi-domain Sentiment Analysis)ä¸­ç”±äºå•é¢†åŸŸæ ‡æ³¨æ•°æ®åŒ®ä¹å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰è”åˆè®­ç»ƒæ¨¡å‹åœ¨é¢†åŸŸæ•°é‡å¢åŠ æ—¶é¢ä¸´çš„è®¡ç®—èµ„æºéœ€æ±‚å¤§ã€æ”¶æ•›å›°éš¾åŠç®—æ³•å¤æ‚åº¦é«˜ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒåˆ¶é¢†åŸŸä¿¡æ¯æ¥æå‡åˆ†ç±»æ•ˆç‡ã€‚ç®—æ³•è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µç¡®å®šæ§åˆ¶é¢†åŸŸåˆ†ç±»ä»»åŠ¡æ¯”ä¾‹çš„å…±äº«è¶…å‚æ•°ï¼Œç¬¬äºŒé˜¶æ®µå¼•å…¥é¢†åŸŸæ„ŸçŸ¥è°ƒåˆ¶ç®—æ³•ã€‚è¯¥è°ƒåˆ¶è¿‡ç¨‹åˆ©ç”¨åŸºäºæ¢¯åº¦(gradient-based)å’ŒåŸºäºæŸå¤±(loss-based)çš„æ–¹æ³•åŠ¨æ€è°ƒæ•´è¾“å…¥æ–‡æœ¬ä¸­çš„é¢†åŸŸä¿¡æ¯ï¼Œç¡®ä¿ä¸ºæƒ…æ„Ÿåˆ†ç±»æä¾›å¿…è¦çš„ç‰¹å¾æ”¯æŒã€‚åœ¨åŒ…å«16ä¸ªé¢†åŸŸçš„å…¬å¼€æƒ…æ„Ÿåˆ†ææ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆå„é¢†åŸŸæ‰€éœ€çš„é¢†åŸŸä¿¡æ¯ã€‚è¯¥ç®—æ³•ä¸ºå¤„ç†å¤§è§„æ¨¡å¤šé¢†åŸŸå­¦ä¹ ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·æœ‰ç«äº‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.06630v1",
      "published_date": "2025-05-10 12:36:00 UTC",
      "updated_date": "2025-05-10 12:36:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:28:47.416416+00:00"
    },
    {
      "arxiv_id": "2505.07877v1",
      "title": "Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data",
      "title_zh": "TSLAM-Miniï¼šåŸºäº QLoRA ä¸æ•°å­—å­ªç”Ÿæ•°æ®çš„é«˜æ•ˆç”µä¿¡ä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Vignesh Ethiraj",
        "Divya Vijay",
        "Sidhanth Menon",
        "Heblin Berscilla"
      ],
      "abstract": "General-purpose large language models (LLMs), despite their broad capabilities accrued from open-world data, frequently exhibit suboptimal performance when confronted with the nuanced and specialized demands inherent in real-time telecommunications applications. This investigation addresses this critical limitation through the meticulous fine-tuning of TSLAM-Mini developed by NetoAI, a compact (3.8-billion parameter) causal language model architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen leverages a bespoke dataset comprising 100,000 samples, strategically engineered to address 20 pivotal telecommunications use-cases, encompassing domains such as Network Fundamentals, IP Routing, MPLS, Network Security, Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched with granular insights from venerated network Subject Matter Experts (SMEs) and authoritative RFC documents, thereby capturing high-fidelity representations of real-world network dynamics through simulations inspired by digital twin paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial training efficiency and enabled prospective deployment on resource-constrained hardware. A novel evaluation framework, predicated on a high-capacity LLM (Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to rigorously assess instruction-following fidelity and response quality across the specified telecom use-cases. Empirical results unequivocally demonstrate TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring the profound efficacy of domain-specific datasets and PEFT methodologies for advancing intelligent network management.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç”µä¿¡é¢†åŸŸå®æ—¶åº”ç”¨æ—¶çš„æ€§èƒ½å±€é™ï¼Œæå‡ºäº† TSLAM-Miniï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°è§„æ¨¡ä¸º 3.8B çš„ä¸“ç”¨å› æœè¯­è¨€æ¨¡å‹ã€‚TSLAM-Mini åŸºäº Phi-4 Mini Instruct 4B æ¶æ„ï¼Œé€šè¿‡ NetoAI çš„ DigiTwin å¹³å°ï¼Œç»“åˆé¢†åŸŸä¸“å®¶(SMEs)è§è§£å’Œ RFC æ–‡æ¡£ï¼Œæ„å»ºäº†æ¶µç›– Network Fundamentalsã€IP Routingã€Mobile Core ç­‰ 20 ä¸ªå…³é”®ç”µä¿¡åœºæ™¯çš„ 100,000 æ¡é«˜è´¨é‡æ•°æ®é›†ã€‚åœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œç ”ç©¶é‡‡ç”¨äº† Quantized Low-Rank Adaptation (QLoRA) è¿™ä¸€é«˜æ•ˆçš„å‚æ•°å¾®è°ƒ(PEFT)æŠ€æœ¯ï¼Œåœ¨æ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡çš„åŒæ—¶ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨èµ„æºå—é™ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²èƒ½åŠ›ã€‚ä¸ºäº†ä¸¥è°¨è¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä»¥ Qwen3-235B-A22B ä½œä¸ºè‡ªåŠ¨è¯„å®¡å‘˜çš„è¯„ä»·æ¡†æ¶ï¼Œé‡ç‚¹æµ‹è¯•å…¶æŒ‡ä»¤éµå¾ªå’Œå“åº”è´¨é‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTSLAM-Mini åœ¨ç”µä¿¡ç‰¹å®šä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„ä¸“ä¸šèƒ½åŠ›ï¼Œæœ‰æ•ˆéªŒè¯äº†é¢†åŸŸç‰¹å®šæ•°æ®ä¸é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åœ¨æ™ºèƒ½åŒ–ç½‘ç»œç®¡ç†ä¸­çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "Introducing TSLAM-Mini, a specialized language model for telecommunications, demonstrating the efficacy of QLoRA fine-tuning and digital twin-synthesized data for enhanced network intelligence. Model available on: https://huggingface.co/NetoAISolutions/TSLAM-Mini-2B",
      "pdf_url": "https://arxiv.org/pdf/2505.07877v1",
      "published_date": "2025-05-10 12:28:47 UTC",
      "updated_date": "2025-05-10 12:28:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:28:39.268312+00:00"
    },
    {
      "arxiv_id": "2505.06625v1",
      "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs",
      "title_zh": "CaMDNï¼šæå‡é›†æˆ NPU ä¸Šå¤šç§Ÿæˆ· DNNs çš„ç¼“å­˜æ•ˆç‡",
      "authors": [
        "Tianhao Cai",
        "Liang Wang",
        "Limin Xiao",
        "Meng Han",
        "Zeyu Wang",
        "Lin Sun",
        "Xiaojian Liao"
      ],
      "abstract": "With the rapid development of DNN applications, multi-tenant execution, where multiple DNNs are co-located on a single SoC, is becoming a prevailing trend. Although many methods are proposed in prior works to improve multi-tenant performance, the impact of shared cache is not well studied. This paper proposes CaMDN, an architecture-scheduling co-design to enhance cache efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a lightweight architecture is proposed to support model-exclusive, NPU-controlled regions inside shared cache to eliminate unexpected cache contention. Moreover, a cache scheduling method is proposed to improve shared cache utilization. In particular, it includes a cache-aware mapping method for adaptability to the varying available cache capacity and a dynamic allocation algorithm to adjust the usage among co-located DNNs at runtime. Compared to prior works, CaMDN reduces the memory access by 33.4% on average and achieves a model speedup of up to 2.56$\\times$ (1.88$\\times$ on average).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CaMDNï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºé›†æˆ NPU ä¸Šå¤šç§Ÿæˆ· DNN ç¼“å­˜æ•ˆç‡çš„æ¶æ„ä¸è°ƒåº¦ååŒè®¾è®¡æ–¹æ¡ˆã€‚é’ˆå¯¹å…±äº«ç¼“å­˜ä¸­çš„å†²çªä¸åˆ©ç”¨ç‡ä¸è¶³é—®é¢˜ï¼Œè¯¥æ–¹æ¡ˆå¼•å…¥äº†ä¸€ç§è½»é‡çº§æ¶æ„ï¼Œæ”¯æŒåœ¨å…±äº«ç¼“å­˜å†…å»ºç«‹ç”± NPU æ§åˆ¶çš„æ¨¡å‹ç‹¬å åŒºåŸŸï¼Œä»è€Œæ¶ˆé™¤éé¢„æœŸçš„ cache contentionã€‚åŒæ—¶ï¼ŒCaMDN ç»“åˆäº†ä¸€å¥—ç¼“å­˜è°ƒåº¦æ–¹æ³•ï¼ŒåŒ…æ‹¬èƒ½å¤Ÿé€‚åº”ä¸åŒç¼“å­˜å®¹é‡çš„ cache-aware mapping æŠ€æœ¯ï¼Œä»¥åŠå¯åœ¨è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´å„ DNN èµ„æºåˆ†é…çš„ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCaMDN å¹³å‡å‡å°‘äº† 33.4% çš„å†…å­˜è®¿é—®éœ€æ±‚ï¼Œå¹¶æ˜¾è‘—æå‡äº†å¤šç§Ÿæˆ·åœºæ™¯ä¸‹çš„ç¡¬ä»¶æ•ˆç‡ã€‚åœ¨æ€§èƒ½è¡¨ç°ä¸Šï¼Œè¯¥ç³»ç»Ÿå®ç°äº†é«˜è¾¾ 2.56 å€ã€å¹³å‡ 1.88 å€çš„æ¨¡å‹åŠ é€Ÿï¼Œä¸ºä¼˜åŒ– SoC ä¸Šçš„å¤šæ¨¡å‹å¹¶å‘æ‰§è¡Œæä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.OS"
      ],
      "primary_category": "cs.AR",
      "comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design Automation Conference (DAC)",
      "pdf_url": "https://arxiv.org/pdf/2505.06625v1",
      "published_date": "2025-05-10 12:16:50 UTC",
      "updated_date": "2025-05-10 12:16:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:30:31.194123+00:00"
    },
    {
      "arxiv_id": "2505.06620v1",
      "title": "Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations",
      "title_zh": "åŒ»ç–—å™¨æ¢°ä¸­å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„é›†æˆï¼šæŠ€æœ¯ã€ä¸´åºŠåŠç›‘ç®¡è§è§£ä¸å»ºè®®",
      "authors": [
        "Dima Alattal",
        "Asal Khoshravan Azar",
        "Puja Myles",
        "Richard Branson",
        "Hatim Abdulhussein",
        "Allan Tucker"
      ],
      "abstract": "There is a growing demand for the use of Artificial Intelligence (AI) and Machine Learning (ML) in healthcare, particularly as clinical decision support systems to assist medical professionals. However, the complexity of many of these models, often referred to as black box models, raises concerns about their safe integration into clinical settings as it is difficult to understand how they arrived at their predictions. This paper discusses insights and recommendations derived from an expert working group convened by the UK Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted of healthcare professionals, regulators, and data scientists, with a primary focus on evaluating the outputs from different AI algorithms in clinical decision-making contexts. Additionally, the group evaluated findings from a pilot study investigating clinicians' behaviour and interaction with AI methods during clinical diagnosis. Incorporating AI methods is crucial for ensuring the safety and trustworthiness of medical AI devices in clinical settings. Adequate training for stakeholders is essential to address potential issues, and further insights and recommendations for safely adopting AI systems in healthcare settings are provided.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åŒ»ç–—è®¾å¤‡ä¸­é›†æˆå¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI) çš„æŠ€æœ¯ã€ä¸´åºŠåŠç›‘ç®¡è§è§£ï¼Œä»¥åº”å¯¹é»‘ç›’æ¨¡å‹ (black box models) åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œé€æ˜åº¦æŒ‘æˆ˜ã€‚æ–‡ç« æ€»ç»“äº†ç”±è‹±å›½è¯å“å’Œä¿å¥å“ç®¡ç†å±€ (MHRA) å¬é›†çš„ä¸“å®¶å·¥ä½œç»„çš„å»ºè®®ï¼Œè¯¥å°ç»„é‡ç‚¹è¯„ä¼°äº†ä¸åŒäººå·¥æ™ºèƒ½ç®—æ³•åœ¨ä¸´åºŠå†³ç­–ç¯å¢ƒä¸­çš„è¾“å‡ºè¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ†æäº†ä¸€é¡¹å…³äºä¸´åºŠåŒ»ç”Ÿåœ¨è¯Šæ–­è¿‡ç¨‹ä¸­ä¸ AI äº’åŠ¨è¡Œä¸ºçš„è¯•ç‚¹ç ”ç©¶ï¼Œå¼ºè°ƒäº†å¯è§£é‡Šæ€§å¯¹äºå¢å¼ºåŒ»ç–— AI è®¾å¤‡å®‰å…¨æ€§å’Œå¯ä¿¡åº¦çš„æ ¸å¿ƒä½œç”¨ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¸ºåˆ©ç›Šç›¸å…³è€…æä¾›é’ˆå¯¹æ€§çš„åŸ¹è®­æ˜¯è§£å†³æ½œåœ¨é—®é¢˜çš„å…³é”®ï¼Œå¹¶ä¸ºåœ¨åŒ»ç–—åœºæ™¯ä¸­å®‰å…¨éƒ¨ç½² AI ç³»ç»Ÿæä¾›äº†å…·ä½“çš„æŒ‡å¯¼åŸåˆ™ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "47 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.06620v1",
      "published_date": "2025-05-10 12:09:19 UTC",
      "updated_date": "2025-05-10 12:09:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:29:01.183961+00:00"
    },
    {
      "arxiv_id": "2505.06612v3",
      "title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation",
      "title_zh": "Burgerï¼šç¤¾ä¼šåŒ–æ¨èä¸­çš„é²æ£’å›¾å»å™ªå¢å¼ºèåˆä¸å¤šè¯­ä¹‰å»ºæ¨¡",
      "authors": [
        "Yuqin Lan",
        "Weihao Shen",
        "Yuanze Hu",
        "Qingchen Yu",
        "Zhaoxin Fan",
        "Faguo Wu",
        "Laurence T. Yang"
      ],
      "abstract": "In the era of rapid development of social media, social recommendation systems as hybrid recommendation systems have been widely applied. Existing methods capture interest similarity between users to filter out interest-irrelevant relations in social networks that inevitably decrease recommendation accuracy, however, limited research has a focus on the mutual influence of semantic information between the social network and the user-item interaction network for further improving social recommendation. To address these issues, we introduce a social \\underline{r}ecommendation model with ro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion and multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly propose to construct a social tensor in order to smooth the training process of the model. Then, a graph convolutional network and a tensor convolutional network are employed to capture user's item preference and social preference, respectively. Considering the different semantic information in the user-item interaction network and the social network, a bi-semantic coordination loss is proposed to model the mutual influence of semantic information. To alleviate the interference of interest-irrelevant relations on multi-semantic modeling, we further use Bayesian posterior probability to mine potential social relations to replace social noise. Finally, the sliding window mechanism is utilized to update the social tensor as the input for the next iteration. Extensive experiments on three real datasets show Burger has a superior performance compared with the state-of-the-art models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Burger æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç¤¾äº¤æ¨èç³»ç»Ÿä¸­ç¤¾äº¤ç½‘ç»œå­˜åœ¨çš„å…´è¶£æ— å…³å†—ä½™å…³ç³»ï¼Œä»¥åŠç¤¾äº¤ç½‘ç»œä¸ç”¨æˆ·-ç‰©å“äº¤äº’ç½‘ç»œé—´è¯­ä¹‰ä¿¡æ¯äº¤äº’å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é¦–å…ˆé€šè¿‡æ„å»ºç¤¾äº¤å¼ é‡ (social tensor) æ¥å¹³æ»‘è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åˆ†åˆ«åˆ©ç”¨å›¾å·ç§¯ç½‘ç»œ (GCN) å’Œå¼ é‡å·ç§¯ç½‘ç»œ (TCN) æ•æ‰ç”¨æˆ·çš„ç‰©å“åå¥½å’Œç¤¾äº¤åå¥½ã€‚ä¸ºäº†å»ºæ¨¡ä¸åŒç½‘ç»œé—´çš„è¯­ä¹‰ç›¸äº’å½±å“ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŒè¯­ä¹‰åè°ƒæŸå¤± (bi-semantic coordination loss) æœºåˆ¶ã€‚åŒæ—¶ï¼ŒBurger åˆ©ç”¨è´å¶æ–¯åéªŒæ¦‚ç‡ (Bayesian posterior probability) æŒ–æ˜æ½œåœ¨ç¤¾äº¤å…³ç³»ä»¥æ›¿ä»£ç¤¾äº¤å™ªå£°ï¼Œä»è€Œé™ä½æ— å…³å…´è¶£å¯¹å¤šè¯­ä¹‰å»ºæ¨¡çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥æ»‘åŠ¨çª—å£æœºåˆ¶ (sliding window mechanism) åŠ¨æ€æ›´æ–°ç¤¾äº¤å¼ é‡ä½œä¸ºåç»­è¿­ä»£çš„è¾“å…¥ã€‚åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBurger çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.SI",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.06612v3",
      "published_date": "2025-05-10 11:51:22 UTC",
      "updated_date": "2025-09-15 12:52:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:29:04.685165+00:00"
    },
    {
      "arxiv_id": "2505.06595v2",
      "title": "Feature Representation Transferring to Lightweight Models via Perception Coherence",
      "title_zh": "åŸºäºæ„ŸçŸ¥ä¸€è‡´æ€§çš„è½»é‡çº§æ¨¡å‹ç‰¹å¾è¡¨ç¤ºè¿ç§»",
      "authors": [
        "Hai-Vy Nguyen",
        "Fabrice Gamboa",
        "Sixin Zhang",
        "Reda Chhaibi",
        "Serge Gratton",
        "Thierry Giaccone"
      ],
      "abstract": "In this paper, we propose a method for transferring feature representation to lightweight student models from larger teacher models. We mathematically define a new notion called \\textit{perception coherence}. Based on this notion, we propose a loss function, which takes into account the dissimilarities between data points in feature space through their ranking. At a high level, by minimizing this loss function, the student model learns to mimic how the teacher model \\textit{perceives} inputs. More precisely, our method is motivated by the fact that the representational capacity of the student model is weaker than the teacher model. Hence, we aim to develop a new method allowing for a better relaxation. This means that, the student model does not need to preserve the absolute geometry of the teacher one, while preserving global coherence through dissimilarity ranking. Importantly, while rankings are defined only on finite sets, our notion of \\textit{perception coherence} extends them into a probabilistic form. This formulation depends on the input distribution and applies to general dissimilarity metrics. Our theoretical insights provide a probabilistic perspective on the process of feature representation transfer. Our experiments results show that our method outperforms or achieves on-par performance compared to strong baseline methods for representation transferring.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†ç‰¹å¾è¡¨ç¤º (Feature Representation) ä»å¤§å‹æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°è½»é‡çº§å­¦ç”Ÿæ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†æ•°å­¦å®šä¹‰çš„æ„ŸçŸ¥ç›¸å¹²æ€§ (Perception Coherence) æ¦‚å¿µã€‚åŸºäºè¯¥æ¦‚å¿µï¼Œä½œè€…è®¾è®¡äº†ä¸€ç§åˆ©ç”¨ç‰¹å¾ç©ºé—´ä¸­æ•°æ®ç‚¹å¼‚è´¨æ€§æ’å (Dissimilarity Ranking) çš„æŸå¤±å‡½æ•° (Loss Function)ï¼Œæ—¨åœ¨è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹æ„ŸçŸ¥è¾“å…¥çš„æ–¹å¼ã€‚ç”±äºå­¦ç”Ÿæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œè¯¥æ–¹æ³•å…è®¸å…¶åœ¨ä¸ä¿ç•™ç»å¯¹å‡ ä½•ç»“æ„çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ’åæœºåˆ¶ç»´æŒå…¨å±€ç›¸å¹²æ€§ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚æ­¤å¤–ï¼Œç ”ç©¶å°†æ’åå®šä¹‰æ‰©å±•ä¸ºæ¦‚ç‡å½¢å¼ (Probabilistic Form)ï¼Œä¸ºç‰¹å¾è¡¨ç¤ºè½¬ç§»è¿‡ç¨‹æä¾›äº†æ¦‚ç‡è®ºè§†è§’çš„ç†è®ºåˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¡¨ç¤ºè½¬ç§»ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºæˆ–ç­‰åŒäºç°æœ‰çš„å¼ºåŸºçº¿æ–¹æ³• (Baseline Methods)ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "math.PR"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06595v2",
      "published_date": "2025-05-10 10:55:06 UTC",
      "updated_date": "2025-10-02 12:06:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:29:29.093039+00:00"
    },
    {
      "arxiv_id": "2505.06589v1",
      "title": "Optimal Transport for Machine Learners",
      "title_zh": "é¢å‘æœºå™¨å­¦ä¹ è€…çš„æœ€ä¼˜ä¼ è¾“",
      "authors": [
        "Gabriel PeyrÃ©"
      ],
      "abstract": "Optimal Transport is a foundational mathematical theory that connects optimization, partial differential equations, and probability. It offers a powerful framework for comparing probability distributions and has recently become an important tool in machine learning, especially for designing and evaluating generative models. These course notes cover the fundamental mathematical aspects of OT, including the Monge and Kantorovich formulations, Brenier's theorem, the dual and dynamic formulations, the Bures metric on Gaussian distributions, and gradient flows. It also introduces numerical methods such as linear programming, semi-discrete solvers, and entropic regularization. Applications in machine learning include topics like training neural networks via gradient flows, token dynamics in transformers, and the structure of GANs and diffusion models. These notes focus primarily on mathematical content rather than deep learning techniques.",
      "tldr_zh": "è¿™ä»½è¯¾ç¨‹è®²ä¹‰ç³»ç»Ÿåœ°ä»‹ç»äº†Optimal Transport (OT)è¿™ä¸€è¿æ¥ä¼˜åŒ–ã€åå¾®åˆ†æ–¹ç¨‹å’Œæ¦‚ç‡è®ºçš„åŸºç¡€æ•°å­¦ç†è®ºï¼Œæ¢è®¨äº†å…¶åœ¨æœºå™¨å­¦ä¹ ä¸­ä½œä¸ºæ¯”è¾ƒæ¦‚ç‡åˆ†å¸ƒå·¥å…·çš„é‡è¦åœ°ä½ã€‚å†…å®¹æ¶µç›–äº†OTçš„æ ¸å¿ƒæ•°å­¦æ–¹é¢ï¼ŒåŒ…æ‹¬Mongeå’ŒKantorovichè¡¨è¿°ã€Brenier's theoremã€å¯¹å¶ä¸åŠ¨æ€è¡¨è¿°ï¼Œä»¥åŠé«˜æ–¯åˆ†å¸ƒä¸Šçš„Bures metricå’Œgradient flowsã€‚åœ¨æ•°å€¼è®¡ç®—æ–¹é¢ï¼Œè®²ä¹‰è¯¦ç»†ä»‹ç»äº†linear programmingã€semi-discrete solversä»¥åŠentropic regularizationç­‰å…³é”®ç®—æ³•ï¼Œä¸ºå¤æ‚é—®é¢˜çš„æ±‚è§£æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚é’ˆå¯¹æœºå™¨å­¦ä¹ é¢†åŸŸï¼Œè¯¥è®²ä¹‰æ·±å…¥åˆ†æäº†é€šè¿‡gradient flowsè®­ç»ƒç¥ç»ç½‘ç»œã€transformerä¸­çš„token dynamicsï¼Œä»¥åŠGANså’Œdiffusion modelsçš„å†…åœ¨ç»“æ„ã€‚å°½ç®¡æ¢è®¨äº†å¤šé¡¹å‰æ²¿åº”ç”¨ï¼Œè¯¥ææ–™çš„ä¸»è¦é‡ç‚¹åœ¨äºå¯¹OTæ•°å­¦å†…æ¶µçš„ä¸¥è°¨è§£æï¼Œè€Œéä»…é™äºæ·±åº¦å­¦ä¹ çš„æ“ä½œæŠ€å·§ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "stat.ML",
      "comment": "arXiv admin note: text overlap with arXiv:1803.00567",
      "pdf_url": "https://arxiv.org/pdf/2505.06589v1",
      "published_date": "2025-05-10 10:35:03 UTC",
      "updated_date": "2025-05-10 10:35:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:29:44.675607+00:00"
    },
    {
      "arxiv_id": "2505.06584v2",
      "title": "JAEGER: Dual-Level Humanoid Whole-Body Controller",
      "title_zh": "JAEGERï¼šäººå½¢æœºå™¨äººåŒå±‚å…¨èº«æ§åˆ¶å™¨",
      "authors": [
        "Ziluo Ding",
        "Haobin Jiang",
        "Yuxuan Wang",
        "Zhenguo Sun",
        "Yu Zhang",
        "Xiaojie Niu",
        "Ming Yang",
        "Weishuai Zeng",
        "Xinrun Xu",
        "Zongqing Lu"
      ],
      "abstract": "This paper presents JAEGER, a dual-level whole-body controller for humanoid robots that addresses the challenges of training a more robust and versatile policy. Unlike traditional single-controller approaches, JAEGER separates the control of the upper and lower bodies into two independent controllers, so that they can better focus on their distinct tasks. This separation alleviates the dimensionality curse and improves fault tolerance. JAEGER supports both root velocity tracking (coarse-grained control) and local joint angle tracking (fine-grained control), enabling versatile and stable movements. To train the controller, we utilize a human motion dataset (AMASS), retargeting human poses to humanoid poses through an efficient retargeting network, and employ a curriculum learning approach. This method performs supervised learning for initialization, followed by reinforcement learning for further exploration. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-art methods in both simulation and real environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† JAEGERï¼Œä¸€ç§ç”¨äºç±»äººæœºå™¨äººçš„åŒå±‚å…¨èº«æ§åˆ¶å™¨ (Dual-Level Whole-Body Controller)ï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒæ›´é²æ£’ä¸”é€šç”¨ç­–ç•¥çš„æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„å•æ§åˆ¶å™¨æ–¹æ³•ä¸åŒï¼ŒJAEGER å°†ä¸Šè‚¢å’Œä¸‹è‚¢çš„æ§åˆ¶åˆ†ç¦»ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„æ§åˆ¶å™¨ï¼Œä½¿å®ƒä»¬èƒ½ä¸“æ³¨äºå„è‡ªçš„ä»»åŠ¡ï¼Œä»è€Œç¼“è§£äº†ç»´åº¦ç¾éš¾å¹¶æé«˜äº†ç³»ç»Ÿçš„å®¹é”™èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ”¯æŒæ ¹éƒ¨é€Ÿåº¦è·Ÿè¸ª (Root Velocity Tracking) çš„ç²—ç²’åº¦æ§åˆ¶å’Œå±€éƒ¨å…³èŠ‚è§’è·Ÿè¸ª (Local Joint Angle Tracking) çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œç¡®ä¿äº†æœºå™¨äººè¿åŠ¨çš„å¤šæ ·æ€§ä¸ç¨³å®šæ€§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç ”ç©¶è€…åˆ©ç”¨ AMASS æ•°æ®é›†å¹¶é€šè¿‡é«˜æ•ˆçš„é‡å®šå‘ç½‘ç»œ (Retargeting Network) å°†äººç±»å§¿æ€æ˜ å°„è‡³æœºå™¨äººï¼Œç»“åˆè¯¾ç¨‹å­¦ä¹  (Curriculum Learning) ç­–ç•¥ï¼Œå…ˆè¿›è¡Œç›‘ç£å­¦ä¹ åˆå§‹åŒ–ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è¿›è¡Œæ·±åº¦æ¢ç´¢ã€‚å®éªŒåœ¨ä¸¤ç§ç±»äººæœºå™¨äººå¹³å°ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜ JAEGER åœ¨ä»¿çœŸå’ŒçœŸå®ç‰©ç†ç¯å¢ƒä¸‹å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.06584v2",
      "published_date": "2025-05-10 10:10:19 UTC",
      "updated_date": "2025-06-16 15:42:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:29:31.592182+00:00"
    },
    {
      "arxiv_id": "2505.06580v1",
      "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification",
      "title_zh": "TAROTï¼šå…·æœ‰ç†è®ºä¾æ®çš„æœ¬è´¨é¢†åŸŸä¸å˜é²æ£’æ€§",
      "authors": [
        "Dongyoon Yang",
        "Jihu Lee",
        "Yongdai Kim"
      ],
      "abstract": "Robust domain adaptation against adversarial attacks is a critical research area that aims to develop models capable of maintaining consistent performance across diverse and challenging domains. In this paper, we derive a new generalization bound for robust risk on the target domain using a novel divergence measure specifically designed for robust domain adaptation. Building upon this, we propose a new algorithm named TAROT, which is designed to enhance both domain adaptability and robustness. Through extensive experiments, TAROT not only surpasses state-of-the-art methods in accuracy and robustness but also significantly enhances domain generalization and scalability by effectively learning domain-invariant features. In particular, TAROT achieves superior performance on the challenging DomainNet dataset, demonstrating its ability to learn domain-invariant representations that generalize well across different domains, including unseen ones. These results highlight the broader applicability of our approach in real-world domain adaptation scenarios.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹å¯¹æŠ—æ”»å‡»çš„é²æ£’åŸŸè‡ªé€‚åº” (Robust Domain Adaptation) é—®é¢˜ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨ä¸åŒæŒ‘æˆ˜æ€§é¢†åŸŸçš„ä¸€è‡´æ€§è¡¨ç°ã€‚è®ºæ–‡é€šè¿‡è®¾è®¡ä¸€ç§æ–°å‹æ•£åº¦åº¦é‡ï¼Œæ¨å¯¼å‡ºç›®æ ‡åŸŸé²æ£’é£é™©çš„æ–°æ³›åŒ–ç•Œ (Generalization Bound)ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†ç†è®ºä¾æ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº†åä¸º TAROT çš„æ–°ç®—æ³•ï¼Œæ—¨åœ¨åŒæ—¶å¢å¼ºåŸŸè‡ªé€‚åº”èƒ½åŠ› (Domain Adaptability) å’Œé²æ£’æ€§ (Robustness)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAROT åœ¨å‡†ç¡®ç‡å’Œé²æ£’æ€§æ–¹é¢å‡è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—æå‡äº†åŸŸæ³›åŒ–æ€§ (Domain Generalization) å’Œå¯æ‰©å±•æ€§ã€‚è¯¥ç®—æ³•åœ¨æŒ‘æˆ˜æ€§çš„ DomainNet æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶å­¦ä¹ åŸŸä¸å˜ç‰¹å¾ (Domain-Invariant Features) å¹¶å°†å…¶æ¨å¹¿è‡³æœªè§é¢†åŸŸçš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†è¯¥æ–¹æ³•åœ¨ç°å®ä¸–ç•ŒåŸŸè‡ªé€‚åº”åœºæ™¯ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in CVPR 2025 (19 pages, 7 figures)",
      "pdf_url": "https://arxiv.org/pdf/2505.06580v1",
      "published_date": "2025-05-10 09:43:04 UTC",
      "updated_date": "2025-05-10 09:43:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:30:45.911456+00:00"
    },
    {
      "arxiv_id": "2505.06576v2",
      "title": "Two-Stage Random Alternation Framework for One-Shot Pansharpening",
      "title_zh": "é¢å‘å•æ ·æœ¬å…¨è‰²é”åŒ–çš„ä¸¤é˜¶æ®µéšæœºäº¤æ›¿æ¡†æ¶",
      "authors": [
        "Haorui Chen",
        "Zeyu Ren",
        "Jiaxuan Ren",
        "Ran Ran",
        "Jinliang Shao",
        "Jie Huang",
        "Liangjian Deng"
      ],
      "abstract": "Deep learning has substantially advanced pansharpening, achieving impressive fusion quality. However, a prevalent limitation is that conventional deep learning models, which typically rely on training datasets, often exhibit suboptimal generalization to unseen real-world image pairs. This restricts their practical utility when faced with real-world scenarios not included in the training datasets. To overcome this, we introduce a two-stage random alternating framework (TRA-PAN) that performs instance-specific optimization for any given Multispectral(MS)/Panchromatic(PAN) pair, ensuring robust and high-quality fusion. TRA-PAN effectively integrates strong supervision constraints from reduced-resolution images with the physical characteristics of the full-resolution images. The first stage introduces a pre-training procedure, which includes Degradation-Aware Modeling (DAM) to capture spectral degradation mappings, alongside a warm-up procedure designed to reduce training time and mitigate the adverse effects of reduced-resolution data. The second stage employs Random Alternation Optimization (RAO), randomly alternating between reduced- and full-resolution images to refine the fusion model progressively. This adaptive, per-instance optimization strategy, operating in a one-shot manner for each MS/PAN pair, yields superior high-resolution multispectral images. Experimental results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in quantitative metrics and visual quality in real-world scenarios, underscoring its enhanced practical applicability and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TRA-PANï¼Œä¸€ç§ç”¨äºOne-Shotå…¨è‰²é”åŒ–(Pansharpening)çš„ä¸¤é˜¶æ®µéšæœºäº¤æ›¿æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é¢å¯¹çœŸå®ä¸–ç•Œå›¾åƒæ—¶æ³›åŒ–æ€§èƒ½å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é’ˆå¯¹æ¯ä¸ªç‰¹å®šçš„å¤šå…‰è°±(MS)å’Œå…¨è‰²(PAN)å›¾åƒå¯¹æ‰§è¡Œå®ä¾‹ç‰¹å®šä¼˜åŒ–(instance-specific optimization)ï¼Œå°†ä½åˆ†è¾¨ç‡å›¾åƒçš„ç›‘ç£çº¦æŸä¸å…¨åˆ†è¾¨ç‡å›¾åƒçš„ç‰©ç†ç‰¹æ€§æœ‰æ•ˆç»“åˆã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒTRA-PANé€šè¿‡é€€åŒ–æ„ŸçŸ¥å»ºæ¨¡(Degradation-Aware Modeling, DAM)æ•æ‰å…‰è°±é€€åŒ–æ˜ å°„ï¼Œå¹¶åˆ©ç”¨é¢„çƒ­è¿‡ç¨‹ç¼“è§£ä½åˆ†è¾¨ç‡æ•°æ®å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨éšæœºäº¤æ›¿ä¼˜åŒ–(Random Alternation Optimization, RAO)ï¼Œé€šè¿‡åœ¨ä¸åŒåˆ†è¾¨ç‡å›¾åƒé—´éšæœºåˆ‡æ¢æ¥é€æ­¥ç²¾ç‚¼èåˆæ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTRA-PANåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœä¸Šå‡ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ï¼Œåœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06576v2",
      "published_date": "2025-05-10 09:26:22 UTC",
      "updated_date": "2025-05-16 10:39:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:30:52.690266+00:00"
    },
    {
      "arxiv_id": "2505.06569v2",
      "title": "MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG",
      "title_zh": "MacRAGï¼šé¢å‘å¤šå°ºåº¦è‡ªé€‚åº”ä¸Šä¸‹æ–‡ RAG çš„å‹ç¼©ã€åˆ‡ç‰‡ä¸æ‰©å±•",
      "authors": [
        "Woosang Lim",
        "Zekun Li",
        "Gyuwan Kim",
        "Sungyoung Ji",
        "HyeonJung Kim",
        "Kyuri Choi",
        "Jin Hyuk Lim",
        "Kyungpyo Park",
        "William Yang Wang"
      ],
      "abstract": "Long-context large language models (LC LLMs) combined with retrieval-augmented generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained windows, and fragmented information from suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through real-time chunk- and document-level expansions. By initiating with finest-level retrieval and progressively incorporating broader, higher-level context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG pipelines in single- and multi-step generation using Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿåœ¨å¤„ç†é•¿æ–‡æœ¬å’Œå¤æ‚å¤šè·³ä»»åŠ¡æ—¶å­˜åœ¨çš„æ£€ç´¢ä¸ç²¾ç¡®ã€ä¸Šä¸‹æ–‡è¦†ç›–ä¸å…¨åŠä¿¡æ¯ç¢ç‰‡åŒ–ç­‰é—®é¢˜ï¼Œæå‡ºäº†å¤šå°ºåº¦è‡ªé€‚åº”ä¸Šä¸‹æ–‡æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶MacRAGã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ–‡æ¡£å‹ç¼©å¹¶åˆ’åˆ†ä¸ºä»ç²—åˆ°ç»†çš„å±‚çº§åŒ–ç²’åº¦ï¼Œåˆ©ç”¨å®æ—¶çš„å—çº§(chunk-level)å’Œæ–‡æ¡£çº§(document-level)æ‰©å±•è‡ªé€‚åº”åœ°åˆå¹¶ç›¸å…³ä¸Šä¸‹æ–‡ã€‚MacRAGä»æœ€ç»†ç²’åº¦æ£€ç´¢åˆ‡å…¥å¹¶é€æ­¥æ•´åˆæ›´å¹¿æ³›çš„é«˜å±‚çº§èƒŒæ™¯ï¼Œæœ‰æ•ˆä¼˜åŒ–äº†æŸ¥è¯¢ç‰¹å®šé•¿ä¸Šä¸‹æ–‡çš„ç²¾ç¡®åº¦ä¸è¦†ç›–èŒƒå›´ã€‚åœ¨HotpotQAã€2WikiMultihopQAå’ŒMusiqueç­‰å¤æ‚åŸºå‡†æµ‹è¯•çš„è¯„ä¼°ä¸­ï¼ŒMacRAGåœ¨Llama-3.1-8Bã€Gemini-1.5-proå’ŒGPT-4oä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„RAGæµæ°´çº¿ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒMacRAGä¸ºå¤„ç†ç°å®ä¸–ç•Œä¸­çš„é•¿ä¸Šä¸‹æ–‡å¤šè·³æ¨ç†ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡è‰¯å¥½æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06569v2",
      "published_date": "2025-05-10 08:50:44 UTC",
      "updated_date": "2025-05-20 20:24:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:02.893301+00:00"
    },
    {
      "arxiv_id": "2505.06561v1",
      "title": "Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning",
      "title_zh": "åŸºäºé€†å‘è¯¾ç¨‹å­¦ä¹ çš„å››è¶³æœºå™¨äººæ»‘æ¿ç™»æ¿",
      "authors": [
        "Danil Belov",
        "Artem Erkhov",
        "Elizaveta Pestova",
        "Ilya Osokin",
        "Dzmitry Tsetserukou",
        "Pavel Osinenko"
      ],
      "abstract": "The aim of this work is to enable quadrupedal robots to mount skateboards using Reverse Curriculum Reinforcement Learning. Although prior work has demonstrated skateboarding for quadrupeds that are already positioned on the board, the initial mounting phase still poses a significant challenge. A goal-oriented methodology was adopted, beginning with the terminal phases of the task and progressively increasing the complexity of the problem definition to approximate the desired objective. The learning process was initiated with the skateboard rigidly fixed within the global coordinate frame and the robot positioned directly above it. Through gradual relaxation of these initial conditions, the learned policy demonstrated robustness to variations in skateboard position and orientation, ultimately exhibiting a successful transfer to scenarios involving a mobile skateboard. The code, trained models, and reproducible examples are available at the following link: https://github.com/dancher00/quadruped-skateboard-mounting",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨åå‘è¯¾ç¨‹å¼ºåŒ–å­¦ä¹  (Reverse Curriculum Reinforcement Learning) ä½¿å››è¶³æœºå™¨äºº (quadrupedal robots) å®ç°è‡ªä¸»ç™»ä¸Šæ»‘æ¿çš„æ–¹æ³•ã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶å¤§å¤šå…³æ³¨æ»‘è¡Œè€Œå¿½è§†æŒ‚è½½è¿™ä¸€æŒ‘æˆ˜æ€§é˜¶æ®µçš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†ä»ä»»åŠ¡æœ«ç«¯çŠ¶æ€é€æ­¥å‘èµ·å§‹çŠ¶æ€æ¨è¿›çš„è®­ç»ƒç­–ç•¥ã€‚å­¦ä¹ è¿‡ç¨‹å§‹äºæ»‘æ¿å›ºå®šä¸”æœºå™¨äººå·²å¤„äºå…¶æ­£ä¸Šæ–¹çš„ç†æƒ³ç¯å¢ƒï¼Œéšåé€šè¿‡é€æ¸æ”¾å®½åˆå§‹æ¡ä»¶ï¼Œä½¿è®­ç»ƒå‡ºçš„ç­–ç•¥ (policy) å¯¹æ»‘æ¿çš„ä½ç½®å’Œå§¿æ€å˜åŒ–å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤ŸæˆåŠŸè¿ç§»è‡³æ¶‰åŠç§»åŠ¨æ»‘æ¿çš„å¤æ‚åŠ¨æ€åœºæ™¯ã€‚è¯¥å·¥ä½œä¸ºå››è¶³æœºå™¨äººåœ¨éç¨³æ€ç¯å¢ƒä¸‹çš„ä»»åŠ¡åˆå§‹åŒ–æä¾›äº†æœ‰æ•ˆè·¯å¾„ï¼Œå¹¶å¼€æºäº†å…¨éƒ¨ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06561v1",
      "published_date": "2025-05-10 08:17:15 UTC",
      "updated_date": "2025-05-10 08:17:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:30:57.485255+00:00"
    },
    {
      "arxiv_id": "2505.07875v1",
      "title": "Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment",
      "title_zh": "åº”å¯¹åŒ»ç–—ä¿å¥é¢†åŸŸçš„æ¬§ç›Ÿã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹ï¼šå€¡å¯¼å¯æŒç»­çš„äººå·¥æ™ºèƒ½å¼€å‘ä¸éƒ¨ç½²",
      "authors": [
        "John Brandt Brodersen",
        "Ilaria Amelia Caggiano",
        "Pedro Kringen",
        "Vince Istvan Madai",
        "Walter Osika",
        "Giovanni Sartor",
        "Ellen Svensson",
        "Magnus Westerlund",
        "Roberto V. Zicari"
      ],
      "abstract": "Assessments of trustworthiness have become a cornerstone of responsible AI development. Especially in high-stakes fields like healthcare, aligning technical, evidence-based, and ethical practices with forthcoming legal requirements is increasingly urgent. We argue that developers and deployers of AI systems for the medical domain should be proactive and take steps to progressively ensure that such systems, both those currently in use and those being developed or planned, respect the requirements of the AI Act, which has come into force in August 2024. This is necessary if full and effective compliance is to be ensured when the most relevant provisions of the Act become effective (August 2026). The engagement with the AI Act cannot be viewed as a formalistic exercise. Compliance with the AI Act needs to be carried out through the proactive commitment to the ethical principles of trustworthy AI. These principles provide the background for the Act, which mentions them several times and connects them to the protection of public interest. They can be used to interpret and apply the Act's provisions and to identify good practices, increasing the validity and sustainability of AI systems over time.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒ»ç–—é¢†åŸŸå¦‚ä½•åº”å¯¹ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act) çš„æŒ‘æˆ˜ï¼Œå‘¼åå»ºç«‹å¯æŒç»­çš„ AI å‘å±•ä¸éƒ¨ç½²ä½“ç³»ã€‚ä½œè€…æŒ‡å‡ºï¼ŒåŒ»ç–— AI çš„å¼€å‘è€…å’Œéƒ¨ç½²è€…åº”ä¸»åŠ¨é‡‡å–æªæ–½ï¼Œç¡®ä¿ç°æœ‰åŠç ”å‘ä¸­çš„ç³»ç»Ÿç¬¦åˆ 2024 å¹´ 8 æœˆç”Ÿæ•ˆçš„æ³•æ¡ˆè¦æ±‚ï¼Œä»¥ä¾¿åœ¨ 2026 å¹´å…³é”®æ¡æ¬¾å…¨é¢ç”Ÿæ•ˆå‰å®ç°æœ‰æ•ˆåˆè§„ã€‚ç ”ç©¶å¼ºè°ƒï¼Œåˆè§„ä¸åº”è¢«è§†ä¸ºç®€å•çš„å½¢å¼ä¸»ä¹‰ï¼Œè€Œåº”æºäºå¯¹å¯ä¿¡ AI (Trustworthy AI) ä¼¦ç†åŸåˆ™çš„æ·±åº¦æ‰¿è¯ºã€‚è¿™äº›åŸåˆ™ä¸ä»…æ„æˆäº†æ³•æ¡ˆçš„èƒŒæ™¯æ¡†æ¶ï¼Œè¿˜ä¸ºè§£é‡Šæ³•å¾‹æ¡æ–‡åŠç¡®å®šæœ€ä½³å®è·µæä¾›äº†æŒ‡å¯¼ã€‚é€šè¿‡å°†ä¼¦ç†åŸåˆ™ä¸æ³•å¾‹è¦æ±‚ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆæå‡åŒ»ç–— AI ç³»ç»Ÿé•¿æœŸçš„æœ‰æ•ˆæ€§ä¸å¯æŒç»­æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "8 pages, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2505.07875v1",
      "published_date": "2025-05-10 07:46:54 UTC",
      "updated_date": "2025-05-10 07:46:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:11.185769+00:00"
    },
    {
      "arxiv_id": "2505.06542v1",
      "title": "dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data",
      "title_zh": "dcFCIï¼šé¢å‘æ½œåœ¨æ··æ‚ã€éå¿ å®æ€§åŠæ··åˆæ•°æ®çš„é²æ£’å› æœå‘ç°",
      "authors": [
        "AdÃ¨le H. Ribeiro",
        "Dominik Heider"
      ],
      "abstract": "Causal discovery is central to inferring causal relationships from observational data. In the presence of latent confounding, algorithms such as Fast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing the true model's Markov Equivalence Class. However, their correctness critically depends on empirical faithfulness, the assumption that observed (in)dependencies perfectly reflect those of the underlying causal model, which often fails in practice due to limited sample sizes. To address this, we introduce the first nonparametric score to assess a PAG's compatibility with observed data, even with mixed variable types. This score is both necessary and sufficient to characterize structural uncertainty and distinguish between distinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid causal discovery algorithm to jointly address latent confounding, empirical unfaithfulness, and mixed data types. dcFCI integrates our score into an (Anytime)FCI-guided search that systematically explores, ranks, and validates candidate PAGs. Experiments on synthetic and real-world scenarios demonstrate that dcFCI significantly outperforms state-of-the-art methods, often recovering the true PAG even in small and heterogeneous datasets. Examining top-ranked PAGs further provides valuable insights into structural uncertainty, supporting more robust and informed causal reasoning and decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å› æœå‘ç°ä¸­å¸¸è§çš„æ½œä¼æ··æ‚ (Latent Confounding)ã€ç»éªŒä¸å¿ å®æ€§ (Empirical Unfaithfulness) ä»¥åŠæ··åˆæ•°æ®ç±»å‹ (Mixed Data Types) ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º dcFCI çš„ç¨³å¥å› æœå‘ç°ç®—æ³•ã€‚ä½œè€…é¦–å…ˆå¼•å…¥äº†é¦–ä¸ªç”¨äºè¯„ä¼°å±€éƒ¨ç¥–å…ˆå›¾ (Partial Ancestral Graph, PAG) ä¸è§‚æµ‹æ•°æ®å…¼å®¹æ€§çš„éå‚æ•°è¯„åˆ†ï¼Œè¯¥è¯„åˆ†èƒ½å¤Ÿæœ‰æ•ˆè¡¨å¾ç»“æ„ä¸ç¡®å®šæ€§ (Structural Uncertainty) å¹¶åŒºåˆ†ä¸åŒçš„ PAGã€‚dcFCI ç®—æ³•å°†è¿™ä¸€è¯„åˆ†æœºåˆ¶æ•´åˆåˆ°åŸºäº (Anytime)FCI å¼•å¯¼çš„æœç´¢æ¡†æ¶ä¸­ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°æ¢ç´¢ã€æ’åºå’ŒéªŒè¯å€™é€‰å›¾æ¨¡å‹æ¥ç¡®å®šå› æœç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆæˆå’Œç°å®åœºæ™¯ä¸‹ï¼ŒdcFCI å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå³ä¾¿åœ¨å°æ ·æœ¬æˆ–å¼‚æ„æ•°æ®é›†ä¸­ä¹Ÿèƒ½æœ‰æ•ˆæ¢å¤çœŸå®çš„ PAGã€‚é€šè¿‡åˆ†ææ’åé å‰çš„ PAGï¼Œè¯¥æ–¹æ³•è¿˜ä¸ºç†è§£å› æœç»“æ„çš„ä¸ç¡®å®šæ€§æä¾›äº†é‡è¦è§è§£ï¼Œä»è€Œæ”¯æŒæ›´å¯é ã€æ›´å…·å¯å‘æ€§çš„å› æœæ¨ç†ä¸å†³ç­–åˆ¶å®šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages. This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2505.06542v1",
      "published_date": "2025-05-10 07:05:19 UTC",
      "updated_date": "2025-05-10 07:05:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:02.591749+00:00"
    },
    {
      "arxiv_id": "2505.06537v1",
      "title": "ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images",
      "title_zh": "ProFashionï¼šåŸºäºå¤šå‚è€ƒå›¾åƒçš„åŸå‹å¼•å¯¼æ—¶å°šè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Xianghao Kong",
        "Qiaosong Qi",
        "Yuanbin Wang",
        "Anyi Rao",
        "Biaolong Chen",
        "Aixi Zhang",
        "Si Liu",
        "Hao Jiang"
      ],
      "abstract": "Fashion video generation aims to synthesize temporally consistent videos from reference images of a designated character. Despite significant progress, existing diffusion-based methods only support a single reference image as input, severely limiting their capability to generate view-consistent fashion videos, especially when there are different patterns on the clothes from different perspectives. Moreover, the widely adopted motion module does not sufficiently model human body movement, leading to sub-optimal spatiotemporal consistency. To address these issues, we propose ProFashion, a fashion video generation framework leveraging multiple reference images to achieve improved view consistency and temporal coherency. To effectively leverage features from multiple reference images while maintaining a reasonable computational cost, we devise a Pose-aware Prototype Aggregator, which selects and aggregates global and fine-grained reference features according to pose information to form frame-wise prototypes, which serve as guidance in the denoising process. To further enhance motion consistency, we introduce a Flow-enhanced Prototype Instantiator, which exploits the human keypoint motion flow to guide an extra spatiotemporal attention process in the denoiser. To demonstrate the effectiveness of ProFashion, we extensively evaluate our method on the MRFashion-7K dataset we collected from the Internet. ProFashion also outperforms previous methods on the UBC Fashion dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProFashionï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡å¤šå¼ å‚è€ƒå›¾åƒæå‡æ—¶å°šè§†é¢‘ç”Ÿæˆè´¨é‡çš„æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion-based methodsï¼‰å› å•å›¾è¾“å…¥å¯¼è‡´çš„è§†è§’ä¸€è‡´æ€§å·®å’Œè¿åŠ¨å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å§¿æ€æ„ŸçŸ¥åŸå‹èšåˆå™¨ï¼ˆPose-aware Prototype Aggregatorï¼‰ï¼Œæ ¹æ®å§¿æ€ä¿¡æ¯èšåˆå¤šå¼ å‚è€ƒå›¾çš„å…¨å±€ä¸ç»†ç²’åº¦ç‰¹å¾ä»¥ç”Ÿæˆå¸§çº§åŸå‹ï¼Œä»è€Œåœ¨å»å™ªè¿‡ç¨‹ä¸­æä¾›ç²¾å‡†å¼•å¯¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¿åŠ¨ä¸€è‡´æ€§ï¼ŒProFashion è®¾è®¡äº†æµé‡å¢å¼ºåŸå‹å®ä¾‹åŒ–å™¨ï¼ˆFlow-enhanced Prototype Instantiatorï¼‰ï¼Œåˆ©ç”¨äººä½“å…³é”®ç‚¹è¿åŠ¨æµï¼ˆhuman keypoint motion flowï¼‰æŒ‡å¯¼å»å™ªå™¨ä¸­çš„æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProFashion åœ¨æ–°æ”¶é›†çš„ MRFashion-7K æ•°æ®é›†ä»¥åŠ UBC Fashion æ•°æ®é›†ä¸Šå‡å–å¾—äº†è¶…è¶Šå‰ä»£æ–¹æ³•çš„è¡¨ç°ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„è§†è§’è¿è´¯æ€§ä¸æ—¶ç©ºä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06537v1",
      "published_date": "2025-05-10 06:59:24 UTC",
      "updated_date": "2025-05-10 06:59:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:32.089747+00:00"
    },
    {
      "arxiv_id": "2505.06536v1",
      "title": "TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition",
      "title_zh": "TACFNï¼šç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„åŸºäº Transformer çš„è‡ªé€‚åº”è·¨æ¨¡æ€èåˆç½‘ç»œ",
      "authors": [
        "Feng Liu",
        "Ziwang Fu",
        "Yunlong Wang",
        "Qijian Zheng"
      ],
      "abstract": "The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art. All code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«(Multimodal Emotion Recognition)ä¸­çš„å…³é”®èåˆæŠ€æœ¯ï¼Œæå‡ºäº†åŸºäºTransformerçš„è‡ªé€‚åº”è·¨æ¨¡æ€èåˆç½‘ç»œTACFNã€‚é’ˆå¯¹ç°æœ‰è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶(cross-modal attention)ä¸­å­˜åœ¨çš„ç‰¹å¾å†—ä½™ä»¥åŠéš¾ä»¥æ•è·äº’è¡¥ç‰¹å¾çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention mechanism)åœ¨æ¨¡æ€å†…éƒ¨è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå®ç°äº†æ¨¡æ€é—´æ›´å…·è‡ªé€‚åº”æ€§çš„é«˜æ•ˆäº¤äº’ã€‚åŒæ—¶ï¼Œæ¨¡å‹é€šè¿‡æ‹¼æ¥(splicing)æ“ä½œæå–èåˆæƒé‡å‘é‡ï¼Œç”¨äºå¢å¼ºä¸åŒæ¨¡æ€é—´çš„ç‰¹å¾è¡¨è¾¾ä»¥æœ‰æ•ˆè·å–äº’è¡¥ä¿¡æ¯ã€‚åœ¨RAVDESSå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTACFNåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–èåˆæ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†å½“å‰çš„æœ€å…ˆè¿›æ°´å¹³(state-of-the-art)ï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€ç‰¹å¾èåˆæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2111.02172",
      "pdf_url": "https://arxiv.org/pdf/2505.06536v1",
      "published_date": "2025-05-10 06:57:58 UTC",
      "updated_date": "2025-05-10 06:57:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:31.787442+00:00"
    },
    {
      "arxiv_id": "2505.06535v2",
      "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments",
      "title_zh": "éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹é«˜æ•ˆåˆ©ç”¨åœ¨çº¿åé¦ˆçš„ä¸»åŠ¨ç›®æ ‡å‘ç°",
      "authors": [
        "Anindya Sarkar",
        "Binglin Ji",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "In various scientific and engineering domains, where data acquisition is costly--such as in medical imaging, environmental monitoring, or remote sensing--strategic sampling from unobserved regions, guided by prior observations, is essential to maximize target discovery within a limited sampling budget. In this work, we introduce Diffusion-guided Active Target Discovery (DiffATD), a novel method that leverages diffusion dynamics for active target discovery. DiffATD maintains a belief distribution over each unobserved state in the environment, using this distribution to dynamically balance exploration-exploitation. Exploration reduces uncertainty by sampling regions with the highest expected entropy, while exploitation targets areas with the highest likelihood of discovering the target, indicated by the belief distribution and an incrementally trained reward model designed to learn the characteristics of the target. DiffATD enables efficient target discovery in a partially observable environment within a fixed sampling budget, all without relying on any prior supervised training. Furthermore, DiffATD offers interpretability, unlike existing black--box policies that require extensive supervised training. Through extensive experiments and ablation studies across diverse domains, including medical imaging, species discovery, and remote sensing, we show that DiffATD performs significantly better than baselines and competitively with supervised methods that operate under full environmental observability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—æˆåƒã€é¥æ„Ÿç­‰æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚çš„é¢†åŸŸï¼Œæå‡ºäº†ä¸€ç§åä¸º DiffATD (Diffusion-guided Active Target Discovery) çš„ä¸»åŠ¨ç›®æ ‡å‘ç°æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰é™é‡‡æ ·é¢„ç®—ä¸‹æœ€å¤§åŒ–ç›®æ ‡å‘ç°ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Diffusion åŠ¨åŠ›å­¦åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒ (Partially Observable Environments) ä¸­ç»´æŠ¤æœªè§‚æµ‹çŠ¶æ€çš„ç½®ä¿¡åˆ†å¸ƒ (Belief Distribution)ï¼Œé€šè¿‡åŠ¨æ€å¹³è¡¡å‹˜æ¢ (Exploration) ä¸å¼€å‘ (Exploitation) æ¥æŒ‡å¯¼é‡‡æ ·å†³ç­–ã€‚å‹˜æ¢è¿‡ç¨‹åˆ©ç”¨é«˜ç†µé‡‡æ ·å‡å°‘ç¯å¢ƒä¸ç¡®å®šæ€§ï¼Œè€Œå¼€å‘è¿‡ç¨‹åˆ™ç»“åˆç½®ä¿¡åˆ†å¸ƒä¸å¢é‡è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹æ¥å®šä½ç›®æ ‡ï¼Œä¸”æ— éœ€ä»»ä½•å…ˆéªŒçš„ç›‘ç£è®­ç»ƒã€‚ä¸ç°æœ‰çš„é»‘ç›’ç­–ç•¥ä¸åŒï¼ŒDiffATD å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€‚åœ¨åŒ»ç–—æˆåƒã€ç‰©ç§å‘ç°å’Œé¥æ„Ÿç­‰å¤šä¸ªé¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶èƒ½ä¸å…·å¤‡å…¨ç¯å¢ƒè§‚æµ‹èƒ½åŠ›çš„ç›‘ç£æ–¹æ³•ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 28 figures, Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.06535v2",
      "published_date": "2025-05-10 06:50:01 UTC",
      "updated_date": "2025-10-19 01:44:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:43.119023+00:00"
    },
    {
      "arxiv_id": "2505.06527v1",
      "title": "Improving Generalization of Medical Image Registration Foundation Model",
      "title_zh": "æå‡åŒ»å­¦å›¾åƒé…å‡†åŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Jing Hu",
        "Kaiwei Yu",
        "Hongjiang Xian",
        "Shu Hu",
        "Xin Wang"
      ],
      "abstract": "Deformable registration is a fundamental task in medical image processing, aiming to achieve precise alignment by establishing nonlinear correspondences between images. Traditional methods offer good adaptability and interpretability but are limited by computational efficiency. Although deep learning approaches have significantly improved registration speed and accuracy, they often lack flexibility and generalizability across different datasets and tasks. In recent years, foundation models have emerged as a promising direction, leveraging large and diverse datasets to learn universal features and transformation patterns for image registration, thus demonstrating strong cross-task transferability. However, these models still face challenges in generalization and robustness when encountering novel anatomical structures, varying imaging conditions, or unseen modalities. To address these limitations, this paper incorporates Sharpness-Aware Minimization (SAM) into foundation models to enhance their generalization and robustness in medical image registration. By optimizing the flatness of the loss landscape, SAM improves model stability across diverse data distributions and strengthens its ability to handle complex clinical scenarios. Experimental results show that foundation models integrated with SAM achieve significant improvements in cross-dataset registration performance, offering new insights for the advancement of medical image registration technology. Our code is available at https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„æ ¸å¿ƒä»»åŠ¡å¯å˜å½¢é…å‡†(Deformable registration)ï¼Œæ¢è®¨äº†åŸºç¡€æ¨¡å‹(Foundation models)åœ¨é¢å¯¹æ–°è§£å‰–ç»“æ„ã€æˆåƒæ¡ä»¶å˜åŒ–æˆ–æœªè§æ¨¡æ€æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºå°†é”åº¦æ„ŸçŸ¥æœ€å°åŒ–(Sharpness-Aware Minimization, SAM)å¼•å…¥åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºåŒ»å­¦å›¾åƒé…å‡†çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚é€šè¿‡ä¼˜åŒ–æŸå¤±å¹³é¢(Loss landscape)çš„å¹³å¦åº¦ï¼ŒSAM èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤šç§æ•°æ®åˆ†å¸ƒä¸‹çš„ç¨³å®šæ€§ï¼Œå¹¶åŠ å¼ºå…¶åº”å¯¹å¤æ‚ä¸´åºŠåœºæ™¯çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆäº† SAM çš„åŸºç¡€æ¨¡å‹åœ¨è·¨æ•°æ®é›†é…å‡†æ€§èƒ½ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œä¸ºæ¨åŠ¨åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯çš„æ³›åŒ–åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚è¯¥é¡¹ç›®çš„ç›¸å…³ä»£ç å·²åœ¨ GitHub å¹³å°å¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IJCNN",
      "pdf_url": "https://arxiv.org/pdf/2505.06527v1",
      "published_date": "2025-05-10 06:14:09 UTC",
      "updated_date": "2025-05-10 06:14:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:33.485490+00:00"
    },
    {
      "arxiv_id": "2505.06520v4",
      "title": "PRUNE: A Patching Based Repair Framework for Certifiable Unlearning of Neural Networks",
      "title_zh": "PRUNEï¼šåŸºäºè¡¥ä¸ä¿®å¤çš„ç¥ç»ç½‘ç»œå¯è¯æ˜é—å¿˜æ¡†æ¶",
      "authors": [
        "Xuran Li",
        "Jingyi Wang",
        "Xiaohan Yuan",
        "Peixin Zhang"
      ],
      "abstract": "It is often desirable to remove (a.k.a. unlearn) a specific part of the training data from a trained neural network model. A typical application scenario is to protect the data holder's right to be forgotten, which has been promoted by many recent regulation rules. Existing unlearning methods involve training alternative models with remaining data, which may be costly and challenging to verify from the data holder or a thirdparty auditor's perspective. In this work, we provide a new angle and propose a novel unlearning approach by imposing carefully crafted \"patch\" on the original neural network to achieve targeted \"forgetting\" of the requested data to delete. Specifically, inspired by the research line of neural network repair, we propose to strategically seek a lightweight minimum \"patch\" for unlearning a given data point with certifiable guarantee. Furthermore, to unlearn a considerable amount of data points (or an entire class), we propose to iteratively select a small subset of representative data points to unlearn, which achieves the effect of unlearning the whole set. Extensive experiments on multiple categorical datasets demonstrates our approach's effectiveness, achieving measurable unlearning while preserving the model's performance and being competitive in efficiency and memory consumption compared to various baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PRUNEï¼Œä¸€ç§åŸºäºè¡¥ä¸ï¼ˆpatchingï¼‰çš„ä¿®å¤æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨é—å¿˜ï¼ˆunlearningï¼‰é¢†åŸŸä¸­ç°æœ‰æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥éªŒè¯çš„é—®é¢˜ã€‚PRUNE å—ç¥ç»ç½‘ç»œä¿®å¤ï¼ˆneural network repairï¼‰æŠ€æœ¯å¯å‘ï¼Œé€šè¿‡åœ¨åŸå§‹æ¨¡å‹ä¸Šæ–½åŠ ç²¾å¿ƒè®¾è®¡çš„è½»é‡çº§æœ€å°è¡¥ä¸ï¼Œå®ç°å¯¹æŒ‡å®šæ•°æ®çš„å®šå‘é—å¿˜å¹¶æä¾›å¯è¯æ˜æ€§ä¿è¯ï¼ˆcertifiable guaranteeï¼‰ã€‚é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ç‚¹æˆ–ç‰¹å®šç±»åˆ«çš„åˆ é™¤éœ€æ±‚ï¼Œè¯¥æ¡†æ¶é€šè¿‡è¿­ä»£é€‰æ‹©ä»£è¡¨æ€§å­é›†çš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ¸…é™¤æ•´ä¸ªæ•°æ®é›†çš„æ•ˆæœã€‚åœ¨å¤šä¸ªåˆ†ç±»æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPRUNE åœ¨å®ç°æ˜¾è‘—é—å¿˜æ•ˆæœçš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿æŒæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚ä¸ç°æœ‰çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ‰§è¡Œæ•ˆç‡å’Œå†…å­˜æ¶ˆè€—æ–¹é¢å‡å…·æœ‰æ˜¾è‘—çš„ç«äº‰ä¼˜åŠ¿ï¼Œä¸ºå®ç°å¯éªŒè¯çš„ç¥ç»ç½‘ç»œé—å¿˜æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06520v4",
      "published_date": "2025-05-10 05:35:08 UTC",
      "updated_date": "2025-10-23 11:07:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:31:57.169464+00:00"
    },
    {
      "arxiv_id": "2505.06518v2",
      "title": "Reinforcement Learning under State and Outcome Uncertainty: A Foundational Distributional Perspective",
      "title_zh": "çŠ¶æ€ä¸ç»“æœä¸ç¡®å®šæ€§ä¸‹çš„å¼ºåŒ–å­¦ä¹ ï¼šä¸€ç§åŸºç¡€çš„åˆ†å¸ƒè§†è§’",
      "authors": [
        "Larry Preuett",
        "Qiuyi Zhang",
        "Muhammad Aurangzeb Ahmad"
      ],
      "abstract": "In many real-world planning tasks, agents must tackle uncertainty about the environment's state and variability in the outcomes of any chosen policy. We address both forms of uncertainty as a first step toward safer algorithms in partially observable settings. Specifically, we extend Distributional Reinforcement Learning (DistRL)-which models the entire return distribution for fully observable domains-to Partially Observable Markov Decision Processes (POMDPs), allowing an agent to learn the distribution of returns for each conditional plan. Concretely, we introduce new distributional Bellman operators for partial observability and prove their convergence under the supremum p-Wasserstein metric. We also propose a finite representation of these return distributions via psi-vectors, generalizing the classical alpha-vectors in POMDP solvers. Building on this, we develop Distributional Point-Based Value Iteration (DPBVI), which integrates psi-vectors into a standard point-based backup procedure-bridging DistRL and POMDP planning. By tracking return distributions, DPBVI naturally enables risk-sensitive control in domains where rare, high-impact events must be carefully managed. We provide source code to foster further research in robust decision-making under partial observability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®è§„åˆ’ä»»åŠ¡ä¸­çš„ç¯å¢ƒçŠ¶æ€(State)ä¸ç¡®å®šæ€§å’Œç­–ç•¥ç»“æœ(Outcome)å˜å¼‚æ€§é—®é¢˜ï¼Œå°†åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ (Distributional Reinforcement Learning, DistRL)æ¡†æ¶æ‰©å±•è‡³éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(POMDPs)ã€‚è®ºæ–‡å¼•å…¥äº†é€‚ç”¨äºéƒ¨åˆ†å¯è§‚æµ‹æ€§çš„æ–°å‹åˆ†å¸ƒè´å°”æ›¼ç®—å­(distributional Bellman operators)ï¼Œå¹¶åœ¨supremum p-Wassersteinåº¦é‡ä¸‹ä¸¥æ ¼è¯æ˜äº†å…¶æ”¶æ•›æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†psi-vectorsä½œä¸ºè¿”å›åˆ†å¸ƒçš„æœ‰é™è¡¨ç¤ºï¼Œå®ç°äº†å¯¹POMDPæ±‚è§£å™¨ä¸­ç»å…¸alpha-vectorsçš„æ³›åŒ–ã€‚åŸºäºè¿™äº›ç†è®ºï¼Œä½œè€…å¼€å‘äº†åˆ†å¸ƒç‚¹åŸºä»·å€¼è¿­ä»£(Distributional Point-Based Value Iteration, DPBVI)ç®—æ³•ï¼Œæœ‰æ•ˆåœ°å°†DistRLä¸POMDPè§„åˆ’ç›¸ç»“åˆã€‚é€šè¿‡è¿½è¸ªå®Œæ•´çš„è¿”å›åˆ†å¸ƒï¼ŒDPBVIèƒ½å¤Ÿåœ¨æ¶‰åŠé«˜å½±å“ç½•è§äº‹ä»¶çš„å¤æ‚é¢†åŸŸä¸­å®ç°è‡ªç„¶çš„é£é™©æ•æ„Ÿæ§åˆ¶(risk-sensitive control)ã€‚è¯¥å·¥ä½œä¸ºåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹è¿›è¡Œé²æ£’å†³ç­–å¥ å®šäº†åŸºç¡€æ€§çš„åˆ†å¸ƒè§†è§’ï¼Œå¹¶å…¬å¼€äº†æºä»£ç ä»¥æ¨åŠ¨åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the Finding the Frame Workshop at the Reinforcement Learning Conference (RLC) 2025. Code available at: https://github.com/lpreuettUW/distributional_point_based_value_iteration",
      "pdf_url": "https://arxiv.org/pdf/2505.06518v2",
      "published_date": "2025-05-10 05:19:32 UTC",
      "updated_date": "2025-07-07 00:26:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:39.205323+00:00"
    },
    {
      "arxiv_id": "2505.06507v1",
      "title": "Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities",
      "title_zh": "Text-to-CadQueryï¼šä¸€ç§åŸºäºå¯æ‰©å±•å¤§æ¨¡å‹èƒ½åŠ›çš„ CAD ç”Ÿæˆæ–°èŒƒå¼",
      "authors": [
        "Haoyang Xie",
        "Feng Ju"
      ],
      "abstract": "Computer-aided design (CAD) is fundamental to modern engineering and manufacturing, but creating CAD models still requires expert knowledge and specialized software. Recent advances in large language models (LLMs) open up the possibility of generative CAD, where natural language is directly translated into parametric 3D models. However, most existing methods generate task-specific command sequences that pretrained models cannot directly handle. These sequences must be converted into CAD representations such as CAD vectors before a 3D model can be produced, which requires training models from scratch and adds unnecessary complexity. To tackle this issue, we propose generating CadQuery code directly from text, leveraging the strengths of pretrained LLMs to produce 3D models without intermediate representations, using this Python-based scripting language. Since LLMs already excel at Python generation and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly effective. Given that these capabilities typically improve with scale, we hypothesize that larger models will perform better after fine-tuning. To enable this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We fine-tune six open-source LLMs of varying sizes and observe consistent improvements. Our best model achieves a top-1 exact match of 69.3%, up from 58.8%, and reduces Chamfer Distance by 48.6%. Project page: https://github.com/Text-to-CadQuery/Text-to-CadQuery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Text-to-CadQueryï¼Œä¸€ç§ç›´æ¥ä»æ–‡æœ¬ç”ŸæˆåŸºäºPythonçš„CadQueryä»£ç çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç¼–ç¨‹å’Œç©ºé—´æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿æ¥å®ç°ç”Ÿæˆå¼è®¡ç®—æœºè¾…åŠ©è®¾è®¡(CAD)ã€‚ä¸ä»¥å¾€ä¾èµ–ç‰¹å®šæŒ‡ä»¤åºåˆ—æˆ–å¤æ‚ä¸­é—´è¡¨ç¤ºçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ç›´æ¥ç”Ÿæˆä»£ç ç®€åŒ–äº†ä»è‡ªç„¶è¯­è¨€åˆ°å‚æ•°åŒ–3Dæ¨¡å‹çš„è½¬åŒ–è¿‡ç¨‹ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä¸ºText2CADæ•°æ®é›†å¢åŠ 170,000ä¸ªCadQueryæ ‡æ³¨ï¼Œå¯¹å…­ç§ä¸åŒè§„æ¨¡çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹æ€§èƒ½éšå‚æ•°è§„æ¨¡å¢é•¿è€Œè¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ï¼Œå…¶ä¸­æœ€ä¼˜æ¨¡å‹åœ¨top-1 exact matchæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†69.3%ï¼Œå¹¶æˆåŠŸå°†Chamfer Distanceé™ä½äº†48.6%ã€‚è¯¥é¡¹ç ”ç©¶è¯æ˜äº†åˆ©ç”¨é¢„è®­ç»ƒå¤§æ¨¡å‹çš„ç°æœ‰èƒ½åŠ›è¿›è¡Œå¾®è°ƒæ˜¯å®ç°é«˜æ•ˆã€å¯æ‰©å±•CADç”Ÿæˆçš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06507v1",
      "published_date": "2025-05-10 04:47:08 UTC",
      "updated_date": "2025-05-10 04:47:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:14.136390+00:00"
    },
    {
      "arxiv_id": "2505.06505v1",
      "title": "On Definite Iterated Belief Revision with Belief Algebras",
      "title_zh": "è®ºåŸºäºä¿¡å¿µä»£æ•°çš„ç¡®å®šæ€§è¿­ä»£ä¿¡å¿µä¿®æ­£",
      "authors": [
        "Hua Meng",
        "Zhiguo Long",
        "Michael Sioutis",
        "Zhengchun Zhou"
      ],
      "abstract": "Traditional logic-based belief revision research focuses on designing rules to constrain the behavior of revision operators. Frameworks have been proposed to characterize iterated revision rules, but they are often too loose, leading to multiple revision operators that all satisfy the rules under the same belief condition. In many practical applications, such as safety critical ones, it is important to specify a definite revision operator to enable agents to iteratively revise their beliefs in a deterministic way. In this paper, we propose a novel framework for iterated belief revision by characterizing belief information through preference relations. Semantically, both beliefs and new evidence are represented as belief algebras, which provide a rich and expressive foundation for belief revision. Building on traditional revision rules, we introduce additional postulates for revision with belief algebra, including an upper-bound constraint on the outcomes of revision. We prove that the revision result is uniquely determined given the current belief state and new evidence. Furthermore, to make the framework more useful in practice, we develop a particular algorithm for performing the proposed revision process. We argue that this approach may offer a more predictable and principled method for belief revision, making it suitable for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿé€»è¾‘ä¿¡å¿µä¿®æ­£ (belief revision) è§„åˆ™è¿‡äºå®½æ¾ã€å¯¼è‡´ä¿®æ­£ç®—å­ä¸å”¯ä¸€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¿¡å¿µä»£æ•° (belief algebras) å’Œåå¥½å…³ç³» (preference relations) çš„æ–°å‹è¿­ä»£ä¿®æ­£ (iterated belief revision) æ¡†æ¶ã€‚ç ”ç©¶è€…å°†ä¿¡å¿µä¸æ–°è¯æ®å‡è¡¨å¾ä¸ºä¿¡å¿µä»£æ•°ï¼Œå¹¶åœ¨ä¼ ç»Ÿè§„åˆ™åŸºç¡€ä¸Šå¼•å…¥äº†åŒ…æ‹¬ä¸Šç•Œçº¦æŸ (upper-bound constraint) åœ¨å†…çš„é¢å¤–å…¬ç†ã€‚é€šè¿‡ç†è®ºè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç»™å®šå½“å‰ä¿¡å¿µçŠ¶æ€å’Œæ–°è¯æ®çš„å‰æä¸‹ï¼Œä¿®æ­£ç»“æœå…·æœ‰å”¯ä¸€æ€§ï¼Œä»è€Œå®ç°äº†ç¡®å®šæ€§ (definite) çš„ä¿®æ­£ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼€å‘äº†ä¸€å¥—å…·ä½“çš„ç®—æ³•æ¥æ‰§è¡Œè¯¥ä¿®æ­£è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä¸ºå®‰å…¨å…³é”®ç­‰ç°å®åº”ç”¨åœºæ™¯æä¾›äº†ä¸€ç§æ›´å…·å¯é¢„æµ‹æ€§å’ŒåŸåˆ™æ€§çš„ä¿¡å¿µä¿®æ­£æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages. Extended version of an accepted IJCAI 2025 paper",
      "pdf_url": "https://arxiv.org/pdf/2505.06505v1",
      "published_date": "2025-05-10 04:34:43 UTC",
      "updated_date": "2025-05-10 04:34:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:19.237377+00:00"
    },
    {
      "arxiv_id": "2505.06503v1",
      "title": "Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models",
      "title_zh": "åŠ¨åŠ›ç³»ç»Ÿä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼šåŸºäºæ•é£Ÿè€…-çŒç‰©æ¨¡å‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "David Balaban"
      ],
      "abstract": "Attention mechanisms are widely used in artificial intelligence to enhance performance and interpretability. In this paper, we investigate their utility in modeling classical dynamical systems -- specifically, a noisy predator-prey (Lotka-Volterra) system. We train a simple linear attention model on perturbed time-series data to reconstruct system trajectories. Remarkably, the learned attention weights align with the geometric structure of the Lyapunov function: high attention corresponds to flat regions (where perturbations have small effect), and low attention aligns with steep regions (where perturbations have large effect). We further demonstrate that attention-based weighting can serve as a proxy for sensitivity analysis, capturing key phase-space properties without explicit knowledge of the system equations. These results suggest a novel use of AI-derived attention for interpretable, data-driven analysis and control of nonlinear systems. For example our framework could support future work in biological modeling of circadian rhythms, and interpretable machine learning for dynamical environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Attention Mechanisms åœ¨ç»å…¸åŠ¨åŠ›å­¦ç³»ç»Ÿå»ºæ¨¡ä¸­çš„åº”ç”¨ï¼Œå¹¶ä»¥å¸¦æœ‰å™ªå£°çš„æ•é£Ÿè€…-é£Ÿé¥µï¼ˆLotka-Volterraï¼‰ç³»ç»Ÿä½œä¸ºæ¡ˆä¾‹ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨å—æ‰°åŠ¨çš„æ—¶é—´åºåˆ—æ•°æ®ä¸Šè®­ç»ƒ Linear Attention æ¨¡å‹æ¥é‡å»ºç³»ç»Ÿè½¨è¿¹ï¼Œå‘ç°å­¦ä¹ åˆ°çš„ Attention Weights ä¸ç³»ç»Ÿçš„ Lyapunov function å‡ ä½•ç»“æ„é«˜åº¦å¥‘åˆã€‚å®éªŒè¡¨æ˜ï¼Œé«˜æ³¨æ„åŠ›å€¼åˆ†å¸ƒåœ¨æ‰°åŠ¨å½±å“è¾ƒå°çš„å¹³å¦åŒºåŸŸï¼Œè€Œä½æ³¨æ„åŠ›å€¼åˆ™å¯¹åº”äºæ‰°åŠ¨å½±å“è¾ƒå¤§çš„é™¡å³­åŒºåŸŸã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼ŒåŸºäºæ³¨æ„åŠ›çš„åŠ æƒæœºåˆ¶å¯ä»¥ä½œä¸º Sensitivity Analysis çš„ä»£ç†ï¼Œåœ¨æ— éœ€å·²çŸ¥ç³»ç»Ÿæ–¹ç¨‹çš„æƒ…å†µä¸‹æ•è·å…³é”®çš„ Phase-Space ç‰¹æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºéçº¿æ€§ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€æ•°æ®é©±åŠ¨åˆ†æä¸æ§åˆ¶æä¾›äº†ä¸€ç§æ–°è·¯å¾„ï¼Œå±•ç¤ºäº† AI è¡ç”Ÿæ³¨æ„åŠ›åœ¨ç”Ÿç‰©å»ºæ¨¡å’ŒåŠ¨æ€ç¯å¢ƒæœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "math.DS",
        "cs.AI"
      ],
      "primary_category": "math.DS",
      "comment": "5 figures, 12 pages, python code included",
      "pdf_url": "https://arxiv.org/pdf/2505.06503v1",
      "published_date": "2025-05-10 04:14:28 UTC",
      "updated_date": "2025-05-10 04:14:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:18.188510+00:00"
    },
    {
      "arxiv_id": "2505.06496v1",
      "title": "xGen-small Technical Report",
      "title_zh": "xGen-small æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Erik Nijkamp",
        "Bo Pang",
        "Egor Pakhomov",
        "Akash Gokul",
        "Jin Qu",
        "Silvio Savarese",
        "Yingbo Zhou",
        "Caiming Xiong"
      ],
      "abstract": "We introduce xGen-small, a family of 4B and 9B Transformer decoder models optimized for long-context applications. Our vertically integrated pipeline unites domain-balanced, frequency-aware data curation; multi-stage pre-training with quality annealing and length extension to 128k tokens; and targeted post-training via supervised fine-tuning, preference learning, and online reinforcement learning. xGen-small delivers strong performance across various tasks, especially in math and coding domains, while excelling at long context benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† xGen-smallï¼Œä¸€ä¸ªåŒ…å« 4B å’Œ 9B å‚æ•°è§„æ¨¡çš„ Transformer decoder æ¨¡å‹ç³»åˆ—ï¼Œä¸“é—¨é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡ (long-context) åº”ç”¨è¿›è¡Œäº†ä¼˜åŒ–ã€‚å¼€å‘è¿‡ç¨‹é‡‡ç”¨äº†å‚ç›´æ•´åˆçš„æµæ°´çº¿ï¼Œæ¶µç›–äº†é¢†åŸŸå‡è¡¡ä¸”å…·æœ‰é¢‘ç‡æ„ŸçŸ¥èƒ½åŠ›çš„æ•°æ®ç­–åˆ’åˆ†å‘ (data curation)ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå¹¶ç»“åˆè´¨é‡é€€ç« (quality annealing) æŠ€æœ¯ï¼ŒæˆåŠŸå°†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•è‡³ 128k tokensã€‚é’ˆå¯¹åæœŸè®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿåº”ç”¨äº†æœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning)ã€åå¥½å­¦ä¹  (preference learning) ä»¥åŠåœ¨çº¿å¼ºåŒ–å­¦ä¹  (online reinforcement learning) ç­‰å®šå‘æŠ€æœ¯ã€‚å®éªŒè¯æ˜ï¼ŒxGen-small åœ¨å¤šé¡¹é€šç”¨ä»»åŠ¡ä¸­è¡¨ç°å¼ºåŠ²ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦ (math) å’Œä»£ç  (coding) é¢†åŸŸä¼˜åŠ¿æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç³»åˆ—åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œä¸ºå¤„ç†å¤æ‚é•¿æ–‡æœ¬ä»»åŠ¡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06496v1",
      "published_date": "2025-05-10 02:54:16 UTC",
      "updated_date": "2025-05-10 02:54:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:14.586660+00:00"
    },
    {
      "arxiv_id": "2505.06493v3",
      "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection",
      "title_zh": "ç³»ç»Ÿæç¤ºè¯æŠ•æ¯’ï¼šè¶…è¶Šç”¨æˆ·æ³¨å…¥çš„å¤§è¯­è¨€æ¨¡å‹æŒä¹…åŒ–æ”»å‡»",
      "authors": [
        "Zongze Li",
        "Jiawei Guo",
        "Haipeng Cai"
      ],
      "abstract": "Large language models (LLMs) have gained widespread adoption across diverse applications due to their impressive generative capabilities. Their plug-and-play nature enables both developers and end users to interact with these models through simple prompts. However, as LLMs become more integrated into various systems in diverse domains, concerns around their security are growing. Existing studies mainly focus on threats arising from user prompts (e.g. prompt injection attack) and model output (e.g. model inversion attack), while the security of system prompts remains largely overlooked. This work bridges the critical gap. We introduce system prompt poisoning, a new attack vector against LLMs that, unlike traditional user prompt injection, poisons system prompts hence persistently impacts all subsequent user interactions and model responses. We systematically investigate four practical attack strategies in various poisoning scenarios. Through demonstration on both generative and reasoning LLMs, we show that system prompt poisoning is highly feasible without requiring jailbreak techniques, and effective across a wide range of tasks, including those in mathematics, coding, logical reasoning, and natural language processing. Importantly, our findings reveal that the attack remains effective even when user prompts employ advanced prompting techniques like chain-of-thought (CoT). We also show that such techniques, including CoT and retrieval-augmentation-generation (RAG), which are proven to be effective for improving LLM performance in a wide range of tasks, are significantly weakened in their effectiveness by system prompt poisoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­è¢«å¿½è§†çš„ç³»ç»Ÿæç¤ºè¯(system prompts)å®‰å…¨éšæ‚£ï¼Œæå‡ºäº†ä¸€ç§åä¸ºç³»ç»Ÿæç¤ºè¯æŠ•æ¯’(system prompt poisoning)çš„æ–°å‹æ”»å‡»å‘é‡ã€‚ä¸åŒäºä¼ ç»Ÿçš„ç”¨æˆ·æç¤ºè¯æ³¨å…¥(prompt injection)ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ±¡æŸ“ç³»ç»Ÿæç¤ºè¯ï¼Œå®ç°å¯¹åç»­æ‰€æœ‰ç”¨æˆ·äº¤äº’å’Œæ¨¡å‹å“åº”çš„æŒä¹…æ€§å½±å“ã€‚ç ”ç©¶ç³»ç»Ÿè°ƒæŸ¥äº†å››ç§å®é™…æ”»å‡»ç­–ç•¥ï¼Œè¯æ˜äº†åœ¨æ— éœ€è¶Šç‹±(jailbreak)æŠ€æœ¯çš„æƒ…å†µä¸‹ï¼Œè¯¥æ”»å‡»åœ¨æ•°å­¦ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†åŠè‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šç§ä»»åŠ¡ä¸­å‡å…·æœ‰é«˜åº¦å¯è¡Œæ€§ã€‚å®éªŒå‘ç°ï¼Œå³ä¾¿ç”¨æˆ·é‡‡ç”¨é“¾å¼æ€ç»´(Chain-of-Thought, CoT)ç­‰é«˜çº§æç¤ºæŠ€æœ¯ï¼Œæ”»å‡»ä¾ç„¶èƒ½ä¿æŒå…¶æ•ˆåŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ç³»ç»Ÿæç¤ºè¯æŠ•æ¯’ä¼šæ˜¾è‘—å‰Šå¼±CoTå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç­‰æ—¨åœ¨æå‡æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯æ•ˆæœï¼Œä¸ºç†è§£å’Œé˜²å¾¡LLMç³»ç»Ÿæ€§é£é™©æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06493v3",
      "published_date": "2025-05-10 02:31:26 UTC",
      "updated_date": "2025-10-19 22:05:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:37.044088+00:00"
    },
    {
      "arxiv_id": "2507.18637v1",
      "title": "More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading",
      "title_zh": "è¶‹å‘ä¸“å®¶åŒ–çš„çœ¼åŠ¨æ¨¡å¼ä¸æ›´ä½³çš„ X å°„çº¿åˆ¤è¯»è¡¨ç°ç›¸å…³",
      "authors": [
        "Pingjing Yang",
        "Jennifer Cromley",
        "Jana Diesner"
      ],
      "abstract": "Understanding how novices acquire and hone visual search skills is crucial for developing and optimizing training methods across domains. Network analysis methods can be used to analyze graph representations of visual expertise. This study investigates the relationship between eye-gaze movements and learning outcomes among undergraduate dentistry students who were diagnosing dental radiographs over multiple semesters. We use network analysis techniques to model eye-gaze scanpaths as directed graphs and examine changes in network metrics over time. Using time series clustering on each metric, we identify distinct patterns of visual search strategies and explore their association with students' diagnostic performance. Our findings suggest that the network metric of transition entropy is negatively correlated with performance scores, while the number of nodes and edges as well as average PageRank are positively correlated with performance scores. Changes in network metrics for individual students over time suggest a developmental shift from intermediate to expert-level processing. These insights contribute to understanding expertise acquisition in visual tasks and can inform the design of AI-assisted learning interventions.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†ç‰™ç§‘ä¸“ä¸šæœ¬ç§‘ç”Ÿåœ¨å¤šä¸ªå­¦æœŸå†…è¯Šæ–­ç‰™ç§‘æ”¾å°„å›¾åƒæ—¶ï¼Œçœ¼åŠ¨æ³¨è§†è·¯å¾„(eye-gaze movements)ä¸å­¦ä¹ ç»“æœä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶é‡‡ç”¨ç½‘ç»œåˆ†æ(network analysis)æŠ€æœ¯ï¼Œå°†çœ¼åŠ¨æ‰«æè·¯å¾„(scanpaths)å»ºæ¨¡ä¸ºæœ‰å‘å›¾ï¼Œå¹¶åˆ©ç”¨æ—¶é—´åºåˆ—èšç±»(time series clustering)åˆ†æå„é¡¹ç½‘ç»œæŒ‡æ ‡éšæ—¶é—´çš„å˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè½¬ç§»ç†µ(transition entropy)ä¸è¯Šæ–­æˆç»©å‘ˆè´Ÿç›¸å…³ï¼Œè€ŒèŠ‚ç‚¹å’Œè¾¹(nodes and edges)çš„æ•°é‡ä»¥åŠå¹³å‡ PageRank å€¼ä¸æˆç»©å‘ˆæ­£ç›¸å…³ã€‚è¿™äº›æŒ‡æ ‡çš„å˜åŒ–æ­ç¤ºäº†å­¦ç”Ÿä»ä¸­é—´é˜¶æ®µå‘ä¸“å®¶çº§å¤„ç†æ¨¡å¼çš„å‘å±•æ€§è½¬å˜ã€‚è¯¥å‘ç°æœ‰åŠ©äºæ·±å…¥ç†è§£è§†è§‰ä»»åŠ¡ä¸­çš„ä¸“ä¸šæŠ€èƒ½è·å–(expertise acquisition)è¿‡ç¨‹ï¼Œå¹¶èƒ½ä¸ºè®¾è®¡äººå·¥æ™ºèƒ½è¾…åŠ©(AI-assisted)çš„å­¦ä¹ å¹²é¢„æªæ–½æä¾›é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "This work will appear at the 26th International Conference on Artificial Intelligence in Education (AIED 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.18637v1",
      "published_date": "2025-05-10 02:24:01 UTC",
      "updated_date": "2025-05-10 02:24:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:39.848617+00:00"
    },
    {
      "arxiv_id": "2505.06492v1",
      "title": "SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing",
      "title_zh": "SmartPilotï¼šé¢å‘è‡ªé€‚åº”ä¸æ™ºèƒ½åˆ¶é€ çš„å¤šæ™ºèƒ½ä½“æ™ºèƒ½åŠ©æ‰‹",
      "authors": [
        "Chathurangi Shyalika",
        "Renjith Prasad",
        "Alaa Al Ghazo",
        "Darssan Eswaramoorthi",
        "Harleen Kaur",
        "Sara Shree Muthuselvam",
        "Amit Sheth"
      ],
      "abstract": "In the dynamic landscape of Industry 4.0, achieving efficiency, precision, and adaptability is essential to optimize manufacturing operations. Industries suffer due to supply chain disruptions caused by anomalies, which are being detected by current AI models but leaving domain experts uncertain without deeper insights into these anomalies. Additionally, operational inefficiencies persist due to inaccurate production forecasts and the limited effectiveness of traditional AI models for processing complex sensor data. Despite these advancements, existing systems lack the seamless integration of these capabilities needed to create a truly unified solution for enhancing production and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot designed for advanced reasoning and contextual decision-making to address these challenges. SmartPilot processes multimodal sensor data and is compact to deploy on edge devices. It focuses on three key tasks: anomaly prediction, production forecasting, and domain-specific question answering. By bridging the gap between AI capabilities and real-world industrial needs, SmartPilot empowers industries with intelligent decision-making and drives transformative innovation in manufacturing. The demonstration video, datasets, and supplementary materials are available at https://github.com/ChathurangiShyalika/SmartPilot.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SmartPilotï¼Œä¸€ç§ç¥ç»ç¬¦å·åŒ–ï¼ˆneurosymbolicï¼‰å¤šæ™ºèƒ½ä½“CoPilotï¼Œæ—¨åœ¨åº”å¯¹å·¥ä¸š4.0åˆ¶é€ ç¯å¢ƒä¸­å¼‚å¸¸æ£€æµ‹æ·±åº¦ä¸è¶³ã€äº§é‡é¢„æµ‹ä¸å‡†åŠå¤æ‚ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†éš¾ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚SmartPilotæ”¯æŒå¤„ç†å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆmultimodal sensor dataï¼‰ï¼Œå…¶ç´§å‡‘çš„è®¾è®¡å…è®¸åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆedge devicesï¼‰ä¸Šéƒ¨ç½²ï¼Œä»¥å®ç°é«˜çº§æ¨ç†ä¸æƒ…å¢ƒå†³ç­–ã€‚è¯¥æ¡†æ¶é‡ç‚¹æ‰§è¡Œå¼‚å¸¸é¢„æµ‹ï¼ˆanomaly predictionï¼‰ã€ç”Ÿäº§é¢„æµ‹ï¼ˆproduction forecastingï¼‰ä»¥åŠé¢†åŸŸç‰¹å®šé—®ç­”ï¼ˆdomain-specific question answeringï¼‰ä¸‰é¡¹å…³é”®ä»»åŠ¡ã€‚é€šè¿‡å¼¥åˆAIæŠ€æœ¯ä¸å®é™…å·¥ä¸šéœ€æ±‚ä¹‹é—´çš„é¸¿æ²Ÿï¼ŒSmartPilotä¸ºåˆ¶é€ ä¸šæä¾›äº†ç»Ÿä¸€ä¸”æ™ºèƒ½åŒ–çš„å†³ç­–è¾…åŠ©æ–¹æ¡ˆã€‚è¯¥ç³»ç»Ÿçš„å®æ–½æœ‰åŠ©äºæå‡ç”Ÿäº§æ•ˆç‡ä¸ç³»ç»Ÿé€‚åº”æ€§ï¼Œä¸ºå®ç°çœŸæ­£çš„æ™ºèƒ½åˆ¶é€ æä¾›äº†æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 8 figures, 4 tables, IEEE Conference on Artificial Intelligence (IEEE CAI) 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.06492v1",
      "published_date": "2025-05-10 02:20:49 UTC",
      "updated_date": "2025-05-10 02:20:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:32:34.039781+00:00"
    },
    {
      "arxiv_id": "2505.06482v2",
      "title": "Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach",
      "title_zh": "è§†é¢‘å¢å¼ºå‹ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼šä¸€ç§åŸºäºæ¨¡å‹çš„æ–¹æ³•",
      "authors": [
        "Minting Pan",
        "Yitao Zheng",
        "Jiajian Li",
        "Yunbo Wang",
        "Xiaokang Yang"
      ],
      "abstract": "Offline reinforcement learning (RL) enables policy optimization using static datasets, avoiding the risks and costs of extensive real-world exploration. However, it struggles with suboptimal offline behaviors and inaccurate value estimation due to the lack of environmental interaction. We present Video-Enhanced Offline RL (VeoRL), a model-based method that constructs an interactive world model from diverse, unlabeled video data readily available online. Leveraging model-based behavior guidance, our approach transfers commonsense knowledge of control policy and physical dynamics from natural videos to the RL agent within the target domain. VeoRL achieves substantial performance gains (over 100% in some cases) across visual control tasks in robotic manipulation, autonomous driving, and open-world video games.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline Reinforcement Learning) åœ¨ç¼ºä¹å®æ—¶ç¯å¢ƒäº¤äº’æ—¶é¢ä¸´çš„æ¬¡ä¼˜è¡Œä¸ºå’Œä»·å€¼ä¼°è®¡ä¸å‡†ç¡®ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† VeoRL (Video-Enhanced Offline RL) æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§åŸºäºæ¨¡å‹ (Model-Based) çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨äº’è”ç½‘ä¸Šå¹¿æ³›å­˜åœ¨çš„æ— æ ‡ç­¾è§†é¢‘æ•°æ®æ„å»ºäº¤äº’å¼ä¸–ç•Œæ¨¡å‹ (World Model)ã€‚VeoRL åˆ©ç”¨åŸºäºæ¨¡å‹çš„è¡Œä¸ºå¼•å¯¼ï¼Œå°†è‡ªç„¶è§†é¢‘ä¸­çš„æ§åˆ¶ç­–ç•¥å’Œç‰©ç†åŠ¨åŠ›å­¦ (Physical Dynamics) ç­‰å¸¸è¯†çŸ¥è¯†æœ‰æ•ˆè¿ç§»è‡³ç›®æ ‡é¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVeoRL åœ¨æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶å’Œå¼€æ”¾ä¸–ç•Œè§†é¢‘æ¸¸æˆç­‰è§†è§‰æ§åˆ¶ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨æŸäº›æ¡ˆä¾‹ä¸­çš„å¢å¹…ç”šè‡³è¶…è¿‡ 100%ï¼Œè¯æ˜äº†åˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®å¢å¼ºæ™ºèƒ½ä½“å†³ç­–èƒ½åŠ›çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.06482v2",
      "published_date": "2025-05-10 00:54:12 UTC",
      "updated_date": "2025-05-17 09:20:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:33:10.787806+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 50,
  "processed_papers_count": 50,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T07:34:06.639632+00:00"
}
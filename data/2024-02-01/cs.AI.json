{
  "date": "2024-02-01",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-01 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 101 篇论文，主要聚焦于 AI 模型优化（如大语言模型的偏见检测和代理设计）、强化学习在机器人和决策中的应用、图学习领域适应，以及一些生物医学和图像生成创新。重点包括 LLMs 的安全和泛化问题、AI 生成图像的偏见分析（如由 Talal Rahwan 等人），以及高效强化学习方法；令人印象深刻的文章有 ICML 2024 接受的“Executable Code Actions Elicit Better LLM Agents”，展示了 LLMs 在代码代理中的潜力。\n\n下面，我挑选并简要讨论部分重要、话题度高的论文，先从 LLMs 和 AI 安全入手，再聊强化学习和图学习相关内容。其他较常规或小众论文（如纯数学或生物领域的）将快速掠过。\n\n1. **Executable Code Actions Elicit Better LLM Agents（可执行代码动作提升 LLM 代理性能）**  \n   作者：Xingyao Wang 等。ICML 2024 接受。论文提出使用可执行 Python 代码作为 LLM 代理的统一动作空间，结合多轮交互和代码执行器，显著提升代理在任务中的鲁棒性和成功率。该方法可应用于模型训练和自调试，展示了 LLM 在复杂任务中的实际潜力。\n\n2. **Plan-Grounded Large Language Models for Dual Goal Conversational Settings（基于计划的 LLM 用于双目标对话设置）**  \n   作者：Diogo Glória-Silva 等。论文设计了 PlanLLM 模型，能在对话中同时遵循预定义计划和用户指令，实现主动引导和适应性响应。实验显示，PlanLLM 在混合对话任务中比基线提升 2.1 倍，强调了 LLM 在多目标场景中的实用性。\n\n3. **Harm Amplification in Text-to-Image Models（文本到图像模型中的伤害放大）**  \n   作者：Susan Hao 等。论文定义了“伤害放大”现象，即安全提示可能导致有害输出，并提出框架量化这种风险，包括性别差异影响。贡献在于提供工具评估和缓解 T2I 模型的安全挑战，促进生成式 AI 的负责任部署。\n\n4. **Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer（多语言文档问答的 LLM 评估方法）**  \n   作者：Adar Kahana 等。论文探索 LLM 的多语言能力，发现将原语言内容翻译为高资源语言（如英语）可提升问答性能。主要发现：翻译策略能优化跨语言任务，适用于多语种文档处理。\n\n5. **Repeat After Me: Transformers are Better than State Space Models at Copying（重复后：Transformer 优于状态空间模型的复制能力）**  \n   作者：Samy Jelassi 等。论文理论和实证证明，Transformer 在复制任务中优于 GSSMs（如 Mamba），尤其在长序列和信息检索中。发现：Transformer 的泛化能力更强，揭示了模型架构在序列任务中的根本差异。\n\n6. **BlackMamba: Mixture of Experts for State-Space Models（BlackMamba：状态空间模型的混合专家架构）**  \n   作者：Quentin Anthony 等。论文结合 Mamba 和 Mixture of Experts，提出 BlackMamba 模型，实现高效序列建模。实验显示，该模型在语言任务中性能优于基线，同时降低计算成本，突显了混合架构在资源受限场景中的优势。\n\n7. **AI-generated faces influence gender stereotypes and racial homogenization（AI 生成面孔影响性别刻板印象和种族同质化）**  \n   作者：Nouar AlDahoul 等，包括 Talal Rahwan。论文分析 Stable Diffusion 的偏见，发现 AI 生成图像强化种族和性别刻板印象，并提出去偏解决方案。贡献：通过实验证明，包容性图像可减少人类偏见，促进 AI 公平性讨论。\n\n8. **Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces（Graph-Mamba：使用选择性状态空间的长期图序列建模）**  \n   作者：Chloe Wang 等。论文将 Mamba 应用于图神经网络，实现高效长期依赖建模。发现：Graph-Mamba 在图预测任务中优于基线，FLOPs 和内存消耗更低，展示了图学习的新方向。\n\n9. **SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling（基于社交交互常识的视觉故事生成）**  \n   作者：Eileen Wang 等。论文引入社交常识知识图，增强视觉故事生成的连贯性和多样性。核心发现：模型生成的叙事更具人性化，适用于图像序列任务，提升了故事的语义深度。\n\n10. **Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning（黑盒 LLM 的高效非参数不确定性量化及决策规划）**  \n    作者：Yao-Hung Hubert Tsai 等。论文提出无需访问模型内部参数的非参数方法，量化 LLM 输出不确定性，并应用于决策代理。贡献：方法高效，支持 LLM 在决策中的鲁棒性，提升了 AI 代理的可靠性。\n\n其他论文中，强化学习相关如“SLIM: Skill Learning with Multiple Critics（SLIM：多批评者技能学习）”和“Efficient Exploration for LLMs（LLM 的高效探索）”展示了在机器人和代理中的进展；图领域适应的“Graph Domain Adaptation: Challenges, Progress and Prospects（图领域适应：挑战、进展与前景）”总结了关键问题；生物和医学AI 如“Deep Robot Sketching（深度机器人素描）”则快速掠过，强调了神经网络在艺术应用的潜力，但非核心焦点。总体而言，今天的论文突显了 AI 模型的优化和安全挑战，期待后续应用落地。",
  "papers": [
    {
      "arxiv_id": "2402.03368v1",
      "title": "Empirical and Experimental Perspectives on Big Data in Recommendation Systems: A Comprehensive Survey",
      "title_zh": "大数据在推荐系统中的实证与实验视角：全面调查",
      "authors": [
        "Kamal Taha",
        "Paul D. Yoo",
        "Aya Taha"
      ],
      "abstract": "This survey paper provides a comprehensive analysis of big data algorithms in\nrecommendation systems, addressing the lack of depth and precision in existing\nliterature. It proposes a two-pronged approach: a thorough analysis of current\nalgorithms and a novel, hierarchical taxonomy for precise categorization. The\ntaxonomy is based on a tri-level hierarchy, starting with the methodology\ncategory and narrowing down to specific techniques. Such a framework allows for\na structured and comprehensive classification of algorithms, assisting\nresearchers in understanding the interrelationships among diverse algorithms\nand techniques. Covering a wide range of algorithms, this taxonomy first\ncategorizes algorithms into four main analysis types: User and Item\nSimilarity-Based Methods, Hybrid and Combined Approaches, Deep Learning and\nAlgorithmic Methods, and Mathematical Modeling Methods, with further\nsubdivisions into sub-categories and techniques. The paper incorporates both\nempirical and experimental evaluations to differentiate between the techniques.\nThe empirical evaluation ranks the techniques based on four criteria. The\nexperimental assessments rank the algorithms that belong to the same category,\nsub-category, technique, and sub-technique. Also, the paper illuminates the\nfuture prospects of big data techniques in recommendation systems, underscoring\npotential advancements and opportunities for further research in this field",
      "tldr_zh": "这篇调查论文全面分析了big data在recommendation systems中的算法，填补了现有文献深度和精确性不足的问题。它提出了一种基于三层层次的taxonomy，将算法分为四类主要方法（如User and Item Similarity-Based Methods、Deep Learning and Algorithmic Methods等），并进一步细分子类别和技术，以帮助研究者理解算法间的相互关系。论文通过empirical evaluation（基于四个标准排名技术）和experimental assessments（在同一类别下排名算法）进行评估，并展望了big data技术在recommendation systems中的未来发展前景。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.03368v1",
      "published_date": "2024-02-01 23:51:29 UTC",
      "updated_date": "2024-02-01 23:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:00:18.613305"
    },
    {
      "arxiv_id": "2402.01065v1",
      "title": "Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer",
      "title_zh": "翻译失败",
      "authors": [
        "Adar Kahana",
        "Jaya Susan Mathew",
        "Said Bleik",
        "Jeremy Reynolds",
        "Oren Elisha"
      ],
      "abstract": "With the widespread adoption of Large Language Models (LLMs), in this paper\nwe investigate the multilingual capability of these models. Our preliminary\nresults show that, translating the native language context, question and answer\ninto a high resource language produced the best results.",
      "tldr_zh": "这篇论文提出了一种评估大型语言模型（LLMs）在多语言文档问答中的方法，重点调查这些模型的多语言处理能力。研究通过将原生语言的上下文、问题和答案翻译成高资源语言（如英语）来进行测试，并发现这种翻译策略能产生最佳结果。该方法为提升LLMs在多语言场景下的性能提供了初步见解和指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01065v1",
      "published_date": "2024-02-01 23:46:05 UTC",
      "updated_date": "2024-02-01 23:46:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:00:29.718360"
    },
    {
      "arxiv_id": "2402.01787v3",
      "title": "Harm Amplification in Text-to-Image Models",
      "title_zh": "翻译失败",
      "authors": [
        "Susan Hao",
        "Renee Shelby",
        "Yuchi Liu",
        "Hansa Srinivasan",
        "Mukul Bhutani",
        "Burcu Karagol Ayan",
        "Ryan Poplin",
        "Shivani Poddar",
        "Sarah Laszlo"
      ],
      "abstract": "Text-to-image (T2I) models have emerged as a significant advancement in\ngenerative AI; however, there exist safety concerns regarding their potential\nto produce harmful image outputs even when users input seemingly safe prompts.\nThis phenomenon, where T2I models generate harmful representations that were\nnot explicit in the input prompt, poses a potentially greater risk than\nadversarial prompts, leaving users unintentionally exposed to harms. Our paper\naddresses this issue by formalizing a definition for this phenomenon which we\nterm harm amplification. We further contribute to the field by developing a\nframework of methodologies to quantify harm amplification in which we consider\nthe harm of the model output in the context of user input. We then empirically\nexamine how to apply these different methodologies to simulate real-world\ndeployment scenarios including a quantification of disparate impacts across\ngenders resulting from harm amplification. Together, our work aims to offer\nresearchers tools to comprehensively address safety challenges in T2I systems\nand contribute to the responsible deployment of generative AI models.",
      "tldr_zh": "这篇论文探讨了 Text-to-image (T2I) 模型中的 harm amplification 现象，即模型从看似安全的用户提示中生成有害图像输出，这比攻击性提示更具风险。作者正式定义了 harm amplification，并开发了一个框架来量化这种现象，通过考虑输出相对于输入的危害进行评估。他们通过实证研究模拟真实部署场景，包括分析性别间的差异性影响，以揭示潜在的不平等问题。该工作为研究者提供了工具，帮助解决 T2I 系统中的安全挑战，并推动生成式 AI 的负责任部署。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01787v3",
      "published_date": "2024-02-01 23:12:57 UTC",
      "updated_date": "2024-08-15 23:36:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:00:43.058697"
    },
    {
      "arxiv_id": "2402.01053v1",
      "title": "Plan-Grounded Large Language Models for Dual Goal Conversational Settings",
      "title_zh": "翻译失败",
      "authors": [
        "Diogo Glória-Silva",
        "Rafael Ferreira",
        "Diogo Tavares",
        "David Semedo",
        "João Magalhães"
      ],
      "abstract": "Training Large Language Models (LLMs) to follow user instructions has been\nshown to supply the LLM with ample capacity to converse fluently while being\naligned with humans. Yet, it is not completely clear how an LLM can lead a\nplan-grounded conversation in mixed-initiative settings where instructions flow\nin both directions of the conversation, i.e. both the LLM and the user provide\ninstructions to one another. In this paper, we tackle a dual goal\nmixed-initiative conversational setting where the LLM not only grounds the\nconversation on an arbitrary plan but also seeks to satisfy both a procedural\nplan and user instructions. The LLM is then responsible for guiding the user\nthrough the plan and, at the same time, adapting to new circumstances,\nanswering questions, and activating safety guardrails when needed. We propose a\nnovel LLM that grounds the dialogue on a procedural plan, can take the dialogue\ninitiative, and enforces guardrails on the system's behavior, while also\nimproving the LLM's responses to unexpected user behavior. Experiments in\ncontrolled settings and with real users show that the best-performing model,\nwhich we call PlanLLM, achieves a 2.1x improvement over a strong baseline.\nMoreover, experiments also show good generalization to unseen domains.",
      "tldr_zh": "这篇论文探讨了在双目标混合主动对话环境中训练 Large Language Models (LLMs) 的方法，使其不仅基于任意计划进行对话，还能同时满足程序计划和用户指令。PlanLLM 模型被提出，它能主动引导对话、适应新情况、回答问题并激活安全护栏，从而改善对意外用户行为的响应。实验结果显示，PlanLLM 比强基线模型性能提高了 2.1 倍，并在未见领域实现了良好的泛化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01053v1",
      "published_date": "2024-02-01 22:56:39 UTC",
      "updated_date": "2024-02-01 22:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:00:54.720693"
    },
    {
      "arxiv_id": "2403.07886v1",
      "title": "A Memetic Algorithm To Find a Hamiltonian Cycle in a Hamiltonian Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Sarwan Ali",
        "Pablo Moscato"
      ],
      "abstract": "We present a memetic algorithm (\\maa) approach for finding a Hamiltonian\ncycle in a Hamiltonian graph. The \\ma is based on a proven approach to the\nAsymmetric Travelling Salesman Problem (\\atspp) that, in this contribution, is\nboosted by the introduction of more powerful local searches. Our approach also\nintroduces a novel technique that sparsifies the input graph under\nconsideration for Hamiltonicity and dynamically augments it during the search.\nSuch a combined heuristic approach helps to prove Hamiltonicity by finding a\nHamiltonian cycle in less time. In addition, we also employ a recently\nintroduced polynomial-time reduction from the \\hamcyc to the Symmetric \\tsp,\nwhich is based on computing the transitive closure of the graph. Although our\napproach is a metaheuristic, i.e., it does not give a theoretical guarantee for\nfinding a Hamiltonian cycle, we have observed that the method is successful in\npractice in verifying the Hamiltonicity of a larger number of instances from\nthe \\textit{Flinder University Hamiltonian Cycle Problem Challenge Set}\n(\\fhcpsc), even for the graphs that have large treewidth. The experiments on\nthe \\fhcpscc instances and a computational comparison with five recent\nstate-of-the-art baseline approaches show that the proposed method outperforms\nthose for the majority of the instances in the \\fhcpsc.",
      "tldr_zh": "本文提出了一种 memetic algorithm (MA) 用于在 Hamiltonian 图中寻找 Hamiltonian cycle，该算法基于 Asymmetric Travelling Salesman Problem (ATSP) 的方法，并通过增强局部搜索、图稀疏化和动态增强技术来优化搜索过程，同时结合 Hamiltonian Cycle 到 Symmetric TSP 的多项式时间归约。实验结果显示，该方法在 Flinder University Hamiltonian Cycle Problem Challenge Set (FHCPSC) 的实例上表现突出，即使对于树宽较大的图，也能更快地验证 Hamiltonicity，并优于五个最近的基准方法。总的来说，这种元启发式方法在实践中证明了其有效性，尽管缺乏理论保证。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.07886v1",
      "published_date": "2024-02-01 22:02:07 UTC",
      "updated_date": "2024-02-01 22:02:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:01:06.743072"
    },
    {
      "arxiv_id": "2402.01786v2",
      "title": "COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations",
      "title_zh": "翻译失败",
      "authors": [
        "Vinicius G. Goecks",
        "Nicholas Waytowich"
      ],
      "abstract": "The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.",
      "tldr_zh": "本研究针对军事行动中耗时复杂的 Courses of Action (COAs) 开发问题，引入 COA-GPT 算法，该算法利用 Large Language Models (LLMs) 实现快速生成有效的 COAs。COA-GPT 通过 in-context learning 整合军事理论和领域知识，支持指挥官以文本或图像格式输入任务信息，并提供战略一致的 COAs，同时允许实时反馈优化。实验在 StarCraft II 的军事化版本中进行，与 state-of-the-art reinforcement learning 算法比较，结果显示 COA-GPT 生成战略上合理的 COAs 更快、更适应，并更好地与指挥官意图对齐。该框架的快速适应能力有望革新军事规划，提升应对突发机会的效率。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "I.2.6; I.2.7; J.7"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.01786v2",
      "published_date": "2024-02-01 21:51:09 UTC",
      "updated_date": "2024-03-28 15:22:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:01:18.747092"
    },
    {
      "arxiv_id": "2402.01032v2",
      "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
      "title_zh": "翻译失败",
      "authors": [
        "Samy Jelassi",
        "David Brandfonbrener",
        "Sham M. Kakade",
        "Eran Malach"
      ],
      "abstract": "Transformers are the dominant architecture for sequence modeling, but there\nis growing interest in models that use a fixed-size latent state that does not\ndepend on the sequence length, which we refer to as \"generalized state space\nmodels\" (GSSMs). In this paper we show that while GSSMs are promising in terms\nof inference-time efficiency, they are limited compared to transformer models\non tasks that require copying from the input context. We start with a\ntheoretical analysis of the simple task of string copying and prove that a two\nlayer transformer can copy strings of exponential length while GSSMs are\nfundamentally limited by their fixed-size latent state. Empirically, we find\nthat transformers outperform GSSMs in terms of efficiency and generalization on\nsynthetic tasks that require copying the context. Finally, we evaluate\npretrained large language models and find that transformer models dramatically\noutperform state space models at copying and retrieving information from\ncontext. Taken together, these results suggest a fundamental gap between\ntransformers and GSSMs on tasks of practical interest.",
      "tldr_zh": "本文比较了Transformers和Generalized State Space Models (GSSMs)在序列建模任务中的性能，重点在于复制输入上下文的能力。研究通过理论分析证明，两层Transformer可以复制指数长度的字符串，而GSSMs因固定大小的潜在状态而存在根本限制。实证实验显示，Transformers在合成任务和预训练大型语言模型中表现出更高的效率和泛化能力。总体结果揭示了Transformers在实际复制和信息检索任务上显著优于GSSMs。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01032v2",
      "published_date": "2024-02-01 21:44:11 UTC",
      "updated_date": "2024-06-03 22:22:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:01:31.194922"
    },
    {
      "arxiv_id": "2402.01030v4",
      "title": "Executable Code Actions Elicit Better LLM Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Xingyao Wang",
        "Yangyi Chen",
        "Lifan Yuan",
        "Yizhe Zhang",
        "Yunzhu Li",
        "Hao Peng",
        "Heng Ji"
      ],
      "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of\nactions, such as invoking tools and controlling robots, show great potential in\ntackling real-world challenges. LLM agents are typically prompted to produce\nactions by generating JSON or text in a pre-defined format, which is usually\nlimited by constrained action space (e.g., the scope of pre-defined tools) and\nrestricted flexibility (e.g., inability to compose multiple tools). This work\nproposes to use executable Python code to consolidate LLM agents' actions into\na unified action space (CodeAct). Integrated with a Python interpreter, CodeAct\ncan execute code actions and dynamically revise prior actions or emit new\nactions upon new observations through multi-turn interactions. Our extensive\nanalysis of 17 LLMs on API-Bank and a newly curated benchmark shows that\nCodeAct outperforms widely used alternatives (up to 20% higher success rate).\nThe encouraging performance of CodeAct motivates us to build an open-source LLM\nagent that interacts with environments by executing interpretable code and\ncollaborates with users using natural language. To this end, we collect an\ninstruction-tuning dataset CodeActInstruct that consists of 7k multi-turn\ninteractions using CodeAct. We show that it can be used with existing data to\nimprove models in agent-oriented tasks without compromising their general\ncapability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with\nPython interpreter and uniquely tailored to perform sophisticated tasks (e.g.,\nmodel training) using existing libraries and autonomously self-debug.",
      "tldr_zh": "该研究提出了一种名为 CodeAct 的方法，使用可执行 Python 代码作为统一的动作空间，以提升大型语言模型 (LLM) 代理的性能，解决传统代理在工具组合和灵活性上的限制。CodeAct 集成 Python 解释器，支持多轮交互和动态修正动作，并在 API-Bank 等基准测试中，17 个 LLMs 的表现比现有方法成功率提高多达 20%。此外，作者构建了开源代理 CodeActAgent，并收集了 7k 多轮交互数据集 CodeActInstruct，用于指令微调，使模型在代理任务中更高效，同时保持一般能力，如自主调试和处理复杂任务。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICML 2024; Code, data, model, and demo are available at\n  https://github.com/xingyaoww/code-act",
      "pdf_url": "http://arxiv.org/pdf/2402.01030v4",
      "published_date": "2024-02-01 21:38:58 UTC",
      "updated_date": "2024-06-07 01:53:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:01:43.792767"
    },
    {
      "arxiv_id": "2402.01785v1",
      "title": "DoubleMLDeep: Estimation of Causal Effects with Multimodal Data",
      "title_zh": "翻译失败",
      "authors": [
        "Sven Klaassen",
        "Jan Teichert-Kluge",
        "Philipp Bach",
        "Victor Chernozhukov",
        "Martin Spindler",
        "Suhas Vijaykumar"
      ],
      "abstract": "This paper explores the use of unstructured, multimodal data, namely text and\nimages, in causal inference and treatment effect estimation. We propose a\nneural network architecture that is adapted to the double machine learning\n(DML) framework, specifically the partially linear model. An additional\ncontribution of our paper is a new method to generate a semi-synthetic dataset\nwhich can be used to evaluate the performance of causal effect estimation in\nthe presence of text and images as confounders. The proposed methods and\narchitectures are evaluated on the semi-synthetic dataset and compared to\nstandard approaches, highlighting the potential benefit of using text and\nimages directly in causal studies. Our findings have implications for\nresearchers and practitioners in economics, marketing, finance, medicine and\ndata science in general who are interested in estimating causal quantities\nusing non-traditional data.",
      "tldr_zh": "本论文探讨了使用多模态数据（如文本和图像）进行因果推理和治疗效果估计，提出了一种适应双机器学习 (DML) 框架的神经网络架构，特别是针对部分线性模型。该架构结合了文本和图像作为混杂因素，以提升因果效果估计的准确性；此外，论文贡献了一个新方法，用于生成半合成数据集，以评估多模态数据在因果研究中的性能。实验结果显示，该方法在半合成数据集上优于标准方法，突出了直接利用文本和图像的潜在益处。这些发现对经济学、市场营销、金融、医学和数据科学等领域的研究者具有重要启发，帮助他们在非传统数据中估计因果量。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.EM",
        "stat.ME",
        "stat.ML",
        "62, 91",
        "I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01785v1",
      "published_date": "2024-02-01 21:34:34 UTC",
      "updated_date": "2024-02-01 21:34:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:01:55.091859"
    },
    {
      "arxiv_id": "2402.01020v1",
      "title": "Quantifying analogy of concepts via ologs and wiring diagrams",
      "title_zh": "通过 ologs 和 wiring diagrams 量化概念的类比",
      "authors": [
        "Jason Lo"
      ],
      "abstract": "We build on the theory of ontology logs (ologs) created by Spivak and Kent,\nand define a notion of wiring diagrams. In this article, a wiring diagram is a\nfinite directed labelled graph. The labels correspond to types in an olog; they\ncan also be interpreted as readings of sensors in an autonomous system. As\nsuch, wiring diagrams can be used as a framework for an autonomous system to\nform abstract concepts. We show that the graphs underlying skeleton wiring\ndiagrams form a category. This allows skeleton wiring diagrams to be compared\nand manipulated using techniques from both graph theory and category theory. We\nalso extend the usual definition of graph edit distance to the case of wiring\ndiagrams by using operations only available to wiring diagrams, leading to a\nmetric on the set of all skeleton wiring diagrams. In the end, we give an\nextended example on calculating the distance between two concepts represented\nby wiring diagrams, and explain how to apply our framework to any application\ndomain.",
      "tldr_zh": "本论文基于 Spivak 和 Kent 的 olog 理论，定义了 wiring diagrams 作为有限的有向标记图，用于量化概念之间的相似度，这些图可解释为自主系统中的传感器读数或抽象概念框架。研究证明，skeleton wiring diagrams 的底层图形成一个范畴，从而允许使用图论和范畴论的技术进行比较和操作。作者扩展了 graph edit distance 的定义，应用于 wiring diagrams，以创建一个度量指标，并通过一个示例展示了如何计算两个概念的距离，并将其框架推广到任意应用领域。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.DM",
        "math.CO",
        "math.CT",
        "68T30 (Primary) 68T20, 68P05, 68T40 (Secondary)",
        "I.2.4; I.2.8"
      ],
      "primary_category": "cs.LO",
      "comment": "30 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.01020v1",
      "published_date": "2024-02-01 21:15:55 UTC",
      "updated_date": "2024-02-01 21:15:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:02:07.231885"
    },
    {
      "arxiv_id": "2402.01018v1",
      "title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Weijie Xu",
        "Zicheng Huang",
        "Wenxiang Hu",
        "Xi Fang",
        "Rajesh Kumar Cherukuri",
        "Naumaan Nayyar",
        "Lorenzo Malandri",
        "Srinivasan H. Sengamedu"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping\nNatural Language Processing (NLP) task in several domains. Their use in the\nfield of Human Resources (HR) has still room for expansions and could be\nbeneficial for several time consuming tasks. Examples such as time-off\nsubmissions, medical claims filing, and access requests are noteworthy, but\nthey are by no means the sole instances. However, the aforementioned\ndevelopments must grapple with the pivotal challenge of constructing a\nhigh-quality training dataset. On one hand, most conversation datasets are\nsolving problems for customers not employees. On the other hand, gathering\nconversations with HR could raise privacy concerns. To solve it, we introduce\nHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR\ndomains to evaluate LLM Agent. Our work has the following contributions: (1) It\nis the first labeled open-sourced conversation dataset in the HR domain for NLP\nresearch. (2) It provides a detailed recipe for the data generation procedure\nalong with data analysis and human evaluations. The data generation pipeline is\ntransferable and can be easily adapted for labeled conversation data generation\nin other domains. (3) The proposed data-collection pipeline is mostly based on\nLLMs with minimal human involvement for annotation, which is time and\ncost-efficient.",
      "tldr_zh": "该研究介绍了 HR-MultiWOZ，一种针对人力资源 (HR) 领域的任务导向对话 (TOD) 数据集，用于评估大型语言模型 (LLMs) 代理。数据集包含 550 个完整标记的对话，覆盖 10 个 HR 领域，如请假申请和医疗索赔，旨在解决现有对话数据集偏向客户问题且隐私担忧的挑战。研究贡献包括：首次公开 HR 领域的标记对话数据集、提供可转移的数据生成管道（主要基于 LLMs 以减少人为干预），并通过数据分析和人工评估证明其高效性。整体框架有助于提升 LLMs 在 HR 任务中的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.01018v1",
      "published_date": "2024-02-01 21:10:44 UTC",
      "updated_date": "2024-02-01 21:10:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:02:19.512674"
    },
    {
      "arxiv_id": "2402.01783v1",
      "title": "Hierarchical Multi-Label Classification of Online Vaccine Concerns",
      "title_zh": "在线疫苗担忧的层次化多标签分类",
      "authors": [
        "Chloe Qinyu Zhu",
        "Rickard Stureborg",
        "Bhuwan Dhingra"
      ],
      "abstract": "Vaccine concerns are an ever-evolving target, and can shift quickly as seen\nduring the COVID-19 pandemic. Identifying longitudinal trends in vaccine\nconcerns and misinformation might inform the healthcare space by helping public\nhealth efforts strategically allocate resources or information campaigns. We\nexplore the task of detecting vaccine concerns in online discourse using large\nlanguage models (LLMs) in a zero-shot setting without the need for expensive\ntraining datasets. Since real-time monitoring of online sources requires\nlarge-scale inference, we explore cost-accuracy trade-offs of different\nprompting strategies and offer concrete takeaways that may inform choices in\nsystem designs for current applications. An analysis of different prompting\nstrategies reveals that classifying the concerns over multiple passes through\nthe LLM, each consisting a boolean question whether the text mentions a vaccine\nconcern or not, works the best. Our results indicate that GPT-4 can strongly\noutperform crowdworker accuracy when compared to ground truth annotations\nprovided by experts on the recently introduced VaxConcerns dataset, achieving\nan overall F1 score of 78.7%.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型(LLMs)对在线疫苗担忧进行层次多标签分类，以识别其随时间变化的趋势，如COVID-19期间的动态，并帮助公共卫生部门优化资源分配。研究采用零样本(zero-shot)设置，避免了昂贵的训练数据集，通过评估不同提示策略的成本-准确性权衡，发现多轮布尔问题提示（即多次通过LLM判断文本是否提及担忧）效果最佳。结果表明，GPT-4在VaxConcerns数据集上实现了78.7%的F1分数，显著优于众包工作者的表现。该方法为实时监控在线讨论提供了实用指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in AAAI 2024 Health Intelligence workshop",
      "pdf_url": "http://arxiv.org/pdf/2402.01783v1",
      "published_date": "2024-02-01 20:56:07 UTC",
      "updated_date": "2024-02-01 20:56:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:02:31.631588"
    },
    {
      "arxiv_id": "2402.01002v3",
      "title": "AI-generated faces influence gender stereotypes and racial homogenization",
      "title_zh": "AI 生成的脸影响性别刻板印象和种族同质化",
      "authors": [
        "Nouar AlDahoul",
        "Talal Rahwan",
        "Yasir Zaki"
      ],
      "abstract": "Text-to-image generative AI models such as Stable Diffusion are used daily by\nmillions worldwide. However, the extent to which these models exhibit racial\nand gender stereotypes is not yet fully understood. Here, we document\nsignificant biases in Stable Diffusion across six races, two genders, 32\nprofessions, and eight attributes. Additionally, we examine the degree to which\nStable Diffusion depicts individuals of the same race as being similar to one\nanother. This analysis reveals significant racial homogenization, e.g.,\ndepicting nearly all Middle Eastern men as bearded, brown-skinned, and wearing\ntraditional attire. We then propose debiasing solutions that allow users to\nspecify the desired distributions of race and gender when generating images\nwhile minimizing racial homogenization. Finally, using a preregistered survey\nexperiment, we find evidence that being presented with inclusive AI-generated\nfaces reduces people's racial and gender biases, while being presented with\nnon-inclusive ones increases such biases, regardless of whether the images are\nlabeled as AI-generated. Taken together, our findings emphasize the need to\naddress biases and stereotypes in text-to-image models.",
      "tldr_zh": "本研究调查了文本到图像生成 AI 模型（如 Stable Diffusion）在六种种族、两种性别、32 个职业和八个属性上的种族和性别刻板印象（racial and gender stereotypes），并揭示了显著的种族同质化（racial homogenization），例如将几乎所有中东男性描绘为有胡须、棕皮肤和穿传统服装。研究者提出去偏见（debiasing）解决方案，允许用户指定种族和性别分布，同时减少同质化现象。实验通过预注册的调查显示，展示包容性 AI 生成面孔能降低人们的种族和性别偏见，而非包容性面孔则会增加这些偏见，无论图像是否标记为 AI 生成。总之，该研究强调了在文本到图像模型中解决偏见和刻板印象的必要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "47 pages, 19 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.01002v3",
      "published_date": "2024-02-01 20:32:14 UTC",
      "updated_date": "2024-11-21 05:06:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:02:45.680881"
    },
    {
      "arxiv_id": "2402.01782v1",
      "title": "Benchmarking Spiking Neural Network Learning Methods with Varying Locality",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqi Lin",
        "Sen Lu",
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "abstract": "Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics,\nhave shown to achieve performance comparable to Artificial Neural Networks\n(ANNs) in several machine learning tasks. Information is processed as spikes\nwithin SNNs in an event-based mechanism that significantly reduces energy\nconsumption. However, training SNNs is challenging due to the\nnon-differentiable nature of the spiking mechanism. Traditional approaches,\nsuch as Backpropagation Through Time (BPTT), have shown effectiveness but comes\nwith additional computational and memory costs and are biologically\nimplausible. In contrast, recent works propose alternative learning methods\nwith varying degrees of locality, demonstrating success in classification\ntasks. In this work, we show that these methods share similarities during the\ntraining process, while they present a trade-off between biological\nplausibility and performance. Further, this research examines the implicitly\nrecurrent nature of SNNs and investigates the influence of addition of explicit\nrecurrence to SNNs. We experimentally prove that the addition of explicit\nrecurrent weights enhances the robustness of SNNs. We also investigate the\nperformance of local learning methods under gradient and non-gradient based\nadversarial attacks.",
      "tldr_zh": "这篇论文基准测试了不同局部性的 Spiking Neural Networks (SNNs) 学习方法，比较了传统 Backpropagation Through Time (BPTT) 与更生物学合理的替代方法，揭示了这些方法在训练过程中的相似性以及生物合理性与性能之间的权衡。研究者探讨了 SNNs 的隐式循环特性，并通过实验证明添加显式循环权重能显著增强 SNNs 的鲁棒性。最终，还评估了局部学习方法在梯度和非梯度基于的对抗攻击下的性能表现，为 SNNs 在能量效率和实际应用中的优化提供了洞见。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01782v1",
      "published_date": "2024-02-01 19:57:08 UTC",
      "updated_date": "2024-02-01 19:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:02:58.456903"
    },
    {
      "arxiv_id": "2402.00978v1",
      "title": "An Information-Theoretic Approach to Analyze NLP Classification Tasks",
      "title_zh": "基于信息理论的方法分析NLP分类任务",
      "authors": [
        "Luran Wang",
        "Mark Gales",
        "Vatsal Raina"
      ],
      "abstract": "Understanding the importance of the inputs on the output is useful across\nmany tasks. This work provides an information-theoretic framework to analyse\nthe influence of inputs for text classification tasks. Natural language\nprocessing (NLP) tasks take either a single element input or multiple element\ninputs to predict an output variable, where an element is a block of text. Each\ntext element has two components: an associated semantic meaning and a\nlinguistic realization. Multiple-choice reading comprehension (MCRC) and\nsentiment classification (SC) are selected to showcase the framework. For MCRC,\nit is found that the context influence on the output compared to the question\ninfluence reduces on more challenging datasets. In particular, more challenging\ncontexts allow a greater variation in complexity of questions. Hence, test\ncreators need to carefully consider the choice of the context when designing\nmultiple-choice questions for assessment. For SC, it is found the semantic\nmeaning of the input text dominates (above 80\\% for all datasets considered)\ncompared to its linguistic realisation when determining the sentiment. The\nframework is made available at:\nhttps://github.com/WangLuran/nlp-element-influence",
      "tldr_zh": "本文提出了一种基于信息理论的方法，用于分析自然语言处理 (NLP) 分类任务中输入元素（如语义含义和语言实现）对输出变量的影响。研究以多选阅读理解 (MCRC) 和情感分类 (SC) 为示例，框架显示在 MCRC 任务中，更具挑战性的数据集上，上下文的影响比问题的影响更小，这提示测试设计者需谨慎选择上下文。在 SC 任务中，输入文本的语义含义主导了情感预测（占比超过 80%），而语言实现的影响相对较小。该框架的开源实现可访问 https://github.com/WangLuran/nlp-element-influence。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 10 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.00978v1",
      "published_date": "2024-02-01 19:49:44 UTC",
      "updated_date": "2024-02-01 19:49:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:03:08.748912"
    },
    {
      "arxiv_id": "2402.00976v4",
      "title": "Investigating Recurrent Transformers with Dynamic Halt",
      "title_zh": "翻译失败",
      "authors": [
        "Jishnu Ray Chowdhury",
        "Cornelia Caragea"
      ],
      "abstract": "In this paper, we comprehensively study the inductive biases of two major\napproaches to augmenting Transformers with a recurrent mechanism: (1) the\napproach of incorporating a depth-wise recurrence similar to Universal\nTransformers; and (2) the approach of incorporating a chunk-wise temporal\nrecurrence like Temporal Latent Bottleneck. Furthermore, we propose and\ninvestigate novel ways to extend and combine the above methods - for example,\nwe propose a global mean-based dynamic halting mechanism for Universal\nTransformers and an augmentation of Temporal Latent Bottleneck with elements\nfrom Universal Transformer. We compare the models and probe their inductive\nbiases in several diagnostic tasks, such as Long Range Arena (LRA), flip-flop\nlanguage modeling, ListOps, and Logical Inference. The code is released in:\nhttps://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main",
      "tldr_zh": "本论文调查了两种增强 Transformers 的循环机制：类似于 Universal Transformers 的深度循环，以及类似于 Temporal Latent Bottleneck 的块状循环。作者提出新扩展方法，包括为 Universal Transformers 设计基于全局均值的动态停止机制，并将 Temporal Latent Bottleneck 与其元素结合，以探索这些机制的归纳偏差。通过 Long Range Arena (LRA)、flip-flop language modeling、ListOps 和 Logical Inference 等诊断任务进行比较，揭示了模型在处理序列任务时的性能差异。该研究为改进循环 Transformer 模型提供了新见解，并开源了相关代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00976v4",
      "published_date": "2024-02-01 19:47:31 UTC",
      "updated_date": "2025-01-21 04:20:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:03:20.686053"
    },
    {
      "arxiv_id": "2402.00969v1",
      "title": "SPARQL Generation with Entity Pre-trained GPT for KG Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Diego Bustamante",
        "Hideaki Takeda"
      ],
      "abstract": "Knowledge Graphs popularity has been rapidly growing in last years. All that\nknowledge is available for people to query it through the many online databases\non the internet. Though, it would be a great achievement if non-programmer\nusers could access whatever information they want to know. There has been a lot\nof effort oriented to solve this task using natural language processing tools\nand creativity encouragement by way of many challenges. Our approach focuses on\nassuming a correct entity linking on the natural language questions and\ntraining a GPT model to create SPARQL queries from them. We managed to isolate\nwhich property of the task can be the most difficult to solve at few or\nzero-shot and we proposed pre-training on all entities (under CWA) to improve\nthe performance. We obtained a 62.703% accuracy of exact SPARQL matches on\ntesting at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of\n0.009 on the question answering challenge.",
      "tldr_zh": "本论文提出了一种基于实体预训练的 GPT 模型方法，用于从自然语言问题生成 SPARQL 查询，从而实现知识图谱（KG）问答，旨在让非程序员用户更容易访问在线数据库。方法假设实体链接正确，并通过在实体上预训练（under CWA）来提升模型在 few-shot 或 zero-shot 场景下的性能。实验结果显示，在 3-shots 测试中，SPARQL 查询精确匹配准确率达到 62.703%，实体链接的 F1 分数为 0.809，但问题回答的 F1 分数仅为 0.009，突显了进一步优化的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "68P20, 68T50",
        "H.2.3; H.3.3; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 1 figure, 2 tables. For the implementation, see\n  https://github.com/DiegoEmilio01/SPARQL-generation-with-entity-pre-trained-GPT-for-KG-Question-Answering",
      "pdf_url": "http://arxiv.org/pdf/2402.00969v1",
      "published_date": "2024-02-01 19:38:32 UTC",
      "updated_date": "2024-02-01 19:38:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:03:34.106079"
    },
    {
      "arxiv_id": "2402.00957v4",
      "title": "Credal Learning Theory",
      "title_zh": "Credal 学习理论",
      "authors": [
        "Michele Caprio",
        "Maryam Sultana",
        "Eleni Elia",
        "Fabio Cuzzolin"
      ],
      "abstract": "Statistical learning theory is the foundation of machine learning, providing\ntheoretical bounds for the risk of models learned from a (single) training set,\nassumed to issue from an unknown probability distribution. In actual\ndeployment, however, the data distribution may (and often does) vary, causing\ndomain adaptation/generalization issues. In this paper we lay the foundations\nfor a `credal' theory of learning, using convex sets of probabilities (credal\nsets) to model the variability in the data-generating distribution. Such credal\nsets, we argue, may be inferred from a finite sample of training sets. Bounds\nare derived for the case of finite hypotheses spaces (both assuming\nrealizability or not), as well as infinite model spaces, which directly\ngeneralize classical results.",
      "tldr_zh": "本论文提出“Credal Learning Theory”，旨在扩展统计学习理论（statistical learning theory）以处理数据生成分布的变异性问题，使用凸集（convex sets of probabilities），即 credal sets，来建模这种不确定性，从而解决域适应/泛化（domain adaptation/generalization）挑战。这些 credal sets 可以从有限的训练集样本中推断出来。论文推导了针对有限假设空间（hypotheses spaces）的风险界限（risk bounds），包括假设可实现性（realizability）情况或不，以及无限模型空间的界限，这些结果直接推广了经典理论。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.00957v4",
      "published_date": "2024-02-01 19:25:58 UTC",
      "updated_date": "2024-10-23 15:40:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:03:43.327740"
    },
    {
      "arxiv_id": "2403.08797v2",
      "title": "Evolutionary Algorithms Simulating Molecular Evolution: A New Field Proposal",
      "title_zh": "翻译失败",
      "authors": [
        "James S. L. Browning Jr.",
        "Daniel R. Tauritz",
        "John Beckmann"
      ],
      "abstract": "The genetic blueprint for the essential functions of life is encoded in DNA,\nwhich is translated into proteins -- the engines driving most of our metabolic\nprocesses. Recent advancements in genome sequencing have unveiled a vast\ndiversity of protein families, but compared to the massive search space of all\npossible amino acid sequences, the set of known functional families is minimal.\nOne could say nature has a limited protein \"vocabulary.\" The major question for\ncomputational biologists, therefore, is whether this vocabulary can be expanded\nto include useful proteins that went extinct long ago, or maybe never evolved\nin the first place. We outline a computational approach to solving this\nproblem. By merging evolutionary algorithms, machine learning (ML), and\nbioinformatics, we can facilitate the development of completely novel proteins\nwhich have never existed before. We envision this work forming a new sub-field\nof computational evolution we dub evolutionary algorithms simulating molecular\nevolution (EASME).",
      "tldr_zh": "这篇论文探讨了已知蛋白质家族相对于所有可能氨基酸序列的庞大搜索空间非常有限的问题，提出通过模拟分子进化来扩展蛋白质“词汇”，包括那些已灭绝或从未存在的有用蛋白质。研究方法结合了evolutionary algorithms、machine learning (ML) 和bioinformatics，开发出全新的蛋白质。最终，论文建议建立一个新的计算进化子领域——EASME（Evolutionary Algorithms Simulating Molecular Evolution），以推动这一领域的创新发展。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "I.2.1"
      ],
      "primary_category": "cs.NE",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.08797v2",
      "published_date": "2024-02-01 19:22:02 UTC",
      "updated_date": "2024-06-10 21:49:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:03:55.026034"
    },
    {
      "arxiv_id": "2402.01781v2",
      "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
      "title_zh": "当基准成为目标时：揭示",
      "authors": [
        "Norah Alzahrani",
        "Hisham Abdullah Alyahya",
        "Yazeed Alnumay",
        "Sultan Alrashed",
        "Shaykhah Alsubaie",
        "Yusef Almushaykeh",
        "Faisal Mirza",
        "Nouf Alotaibi",
        "Nora Altwairesh",
        "Areeb Alowisheq",
        "M Saiful Bari",
        "Haidar Khan"
      ],
      "abstract": "Large Language Model (LLM) leaderboards based on benchmark rankings are\nregularly used to guide practitioners in model selection. Often, the published\nleaderboard rankings are taken at face value - we show this is a (potentially\ncostly) mistake. Under existing leaderboards, the relative performance of LLMs\nis highly sensitive to (often minute) details. We show that for popular\nmultiple-choice question benchmarks (e.g., MMLU), minor perturbations to the\nbenchmark, such as changing the order of choices or the method of answer\nselection, result in changes in rankings up to 8 positions. We explain this\nphenomenon by conducting systematic experiments over three broad categories of\nbenchmark perturbations and identifying the sources of this behavior. Our\nanalysis results in several best-practice recommendations, including the\nadvantage of a hybrid scoring method for answer selection. Our study highlights\nthe dangers of relying on simple benchmark evaluations and charts the path for\nmore robust evaluation schemes on the existing benchmarks. The code for this\npaper is available at\nhttps://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.",
      "tldr_zh": "这篇论文揭示了 Large Language Model (LLM) 排行榜的敏感性，即基准测试的微小细节（如选项顺序或答案选择方法）可能导致模型排名变化多达 8 位，提醒从业者不可盲目依赖这些排行榜。研究者通过系统实验，探讨了三种基准扰动类别（如 MMLU 多选题的细微调整），并识别出这种不稳定性的根源。论文提出最佳实践推荐，包括采用混合评分方法，并为更稳健的基准评估方案提供路径，以避免潜在的成本损失。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "updated with ACL 2024 camera ready version",
      "pdf_url": "http://arxiv.org/pdf/2402.01781v2",
      "published_date": "2024-02-01 19:12:25 UTC",
      "updated_date": "2024-07-03 11:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:04:09.324246"
    },
    {
      "arxiv_id": "2402.00944v2",
      "title": "NCoder -- A Quantum Field Theory approach to encoding data",
      "title_zh": "翻译失败",
      "authors": [
        "David S. Berman",
        "Marc S. Klinger",
        "Alexander G. Stapleton"
      ],
      "abstract": "In this paper we present a novel approach to interpretable AI inspired by\nQuantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified\nautoencoder neural network whose latent layer is prescribed to be a subset of\n$n$-point correlation functions. Regarding images as draws from a lattice field\ntheory, this architecture mimics the task of perturbatively constructing the\neffective action of the theory order by order in an expansion using Feynman\ndiagrams. Alternatively, the NCoder may be regarded as simulating the procedure\nof statistical inference whereby high dimensional data is first summarized in\nterms of several lower dimensional summary statistics (here the $n$-point\ncorrelation functions), and subsequent out-of-sample data is generated by\ninferring the data generating distribution from these statistics. In this way\nthe NCoder suggests a fascinating correspondence between perturbative\nrenormalizability and the sufficiency of models. We demonstrate the efficacy of\nthe NCoder by applying it to the generation of MNIST images, and find that\ngenerated images can be correctly classified using only information from the\nfirst three $n$-point functions of the image distribution.",
      "tldr_zh": "本研究提出了一种名为NCoder的可解释AI方法，受Quantum Field Theory (QFT)启发，将其应用于数据编码。NCoder是修改后的autoencoder神经网络，其潜在层设置为n-point correlation functions的子集，将图像视为来自lattice field theory的抽样，并模仿使用Feynman diagrams逐级构建有效作用的微扰过程。实验通过应用于MNIST图像生成显示，仅使用图像分布的前三个n-point functions即可正确分类生成的图像，从而揭示了微扰可重整化与模型充分性之间的对应关系。",
      "categories": [
        "hep-th",
        "cond-mat.dis-nn",
        "cs.AI"
      ],
      "primary_category": "hep-th",
      "comment": "29 pages. v2 Fixed minor typos",
      "pdf_url": "http://arxiv.org/pdf/2402.00944v2",
      "published_date": "2024-02-01 19:00:55 UTC",
      "updated_date": "2024-07-10 21:34:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:04:22.050029"
    },
    {
      "arxiv_id": "2402.00861v2",
      "title": "Evaluating Large Language Models for Generalization and Robustness via Data Compression",
      "title_zh": "通过数据压缩评估大语言模型的泛化和鲁棒性",
      "authors": [
        "Yucheng Li",
        "Yunhao Guo",
        "Frank Guerin",
        "Chenghua Lin"
      ],
      "abstract": "Existing methods for evaluating large language models face challenges such as\ndata contamination, sensitivity to prompts, and the high cost of benchmark\ncreation. To address this, we propose a lossless data compression based\nevaluation approach that tests how models' predictive abilities generalize\nafter their training cutoff. Specifically, we collect comprehensive test data\nspanning 83 months from 2017 to 2023 and split the data into training and\ntesting periods according to models' training data cutoff. We measure: 1) the\ncompression performance on the testing period as a measure of generalization on\nunseen data; and 2) the performance gap between the training and testing period\nas a measure of robustness. Our experiments test 14 representative large\nlanguage models with various sizes on sources including Wikipedia, news\narticles, code, arXiv papers, and multi-modal data. We find that the\ncompression rate of many models reduces significantly after their cutoff date,\nbut models such as Mistral and Llama-2 demonstrate a good balance between\nperformance and robustness. Results also suggest that models struggle to\ngeneralize on news and code data, but work especially well on arXiv papers. We\nalso find the context size and tokenization implementation have a big impact of\non the overall compression performance.",
      "tldr_zh": "本研究提出了一种基于无损数据压缩的评估方法，用于测试大型语言模型(LLMs)的泛化能力和鲁棒性，以解决现有评估方法中数据污染、提示敏感性和基准创建成本高等问题。具体而言，该方法收集了2017年至2023年的83个月跨度数据，按模型训练截止日期分割为训练和测试期，并通过测试期压缩性能评估泛化，以及训练期与测试期性能差距评估鲁棒性。实验涉及14个代表性LLMs，在Wikipedia、新闻文章、代码、arXiv论文和多模态数据上进行测试，结果显示许多模型在截止日期后压缩率显著下降，但Mistral和Llama-2在性能与鲁棒性之间表现出较好平衡。研究还发现，模型在新闻和代码数据上泛化较差，而在arXiv论文上表现突出，且上下文大小和标记化实现对压缩性能有重大影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00861v2",
      "published_date": "2024-02-01 18:56:18 UTC",
      "updated_date": "2024-02-04 01:16:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:04:34.964039"
    },
    {
      "arxiv_id": "2402.10091v1",
      "title": "Text-Based Product Matching -- Semi-Supervised Clustering Approach",
      "title_zh": "基于文本的产品匹配——半监督聚类方法",
      "authors": [
        "Alicja Martinek",
        "Szymon Łukasik",
        "Amir H. Gandomi"
      ],
      "abstract": "Matching identical products present in multiple product feeds constitutes a\ncrucial element of many tasks of e-commerce, such as comparing product\nofferings, dynamic price optimization, and selecting the assortment\npersonalized for the client. It corresponds to the well-known machine learning\ntask of entity matching, with its own specificity, like omnipresent\nunstructured data or inaccurate and inconsistent product descriptions. This\npaper aims to present a new philosophy to product matching utilizing a\nsemi-supervised clustering approach. We study the properties of this method by\nexperimenting with the IDEC algorithm on the real-world dataset using\npredominantly textual features and fuzzy string matching, with more standard\napproaches as a point of reference. Encouraging results show that unsupervised\nmatching, enriched with a small annotated sample of product links, could be a\npossible alternative to the dominant supervised strategy, requiring extensive\nmanual data labeling.",
      "tldr_zh": "本论文探讨了电商领域的产品匹配问题，旨在通过半监督聚类方法处理多个产品源中的相同产品匹配，以支持价格优化和个性化选品。该方法利用 IDEC 算法结合文本特征和 fuzzy string matching，在真实数据集上进行实验，并与传统方法进行比较。结果显示，这种半监督聚类策略只需少量标注数据即可实现有效的无监督匹配，准确率优于监督方法，从而为减少手动标注工作提供可行替代方案。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10091v1",
      "published_date": "2024-02-01 18:52:26 UTC",
      "updated_date": "2024-02-01 18:52:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:04:44.801247"
    },
    {
      "arxiv_id": "2402.00854v4",
      "title": "SymbolicAI: A framework for logic-based approaches combining generative models and solvers",
      "title_zh": "翻译失败",
      "authors": [
        "Marius-Constantin Dinu",
        "Claudiu Leoveanu-Condrei",
        "Markus Holzleitner",
        "Werner Zellinger",
        "Sepp Hochreiter"
      ],
      "abstract": "We introduce SymbolicAI, a versatile and modular framework employing a\nlogic-based approach to concept learning and flow management in generative\nprocesses. SymbolicAI enables the seamless integration of generative models\nwith a diverse range of solvers by treating large language models (LLMs) as\nsemantic parsers that execute tasks based on both natural and formal language\ninstructions, thus bridging the gap between symbolic reasoning and generative\nAI. We leverage probabilistic programming principles to tackle complex tasks,\nand utilize differentiable and classical programming paradigms with their\nrespective strengths. The framework introduces a set of polymorphic,\ncompositional, and self-referential operations for multi-modal data that\nconnects multi-step generative processes and aligns their outputs with user\nobjectives in complex workflows. As a result, we can transition between the\ncapabilities of various foundation models with in-context learning capabilities\nand specialized, fine-tuned models or solvers proficient in addressing specific\nproblems. Through these operations based on in-context learning our framework\nenables the creation and evaluation of explainable computational graphs.\nFinally, we introduce a quality measure and its empirical score for evaluating\nthese computational graphs, and propose a benchmark that compares various\nstate-of-the-art LLMs across a set of complex workflows. We refer to the\nempirical score as the \"Vector Embedding for Relational Trajectory Evaluation\nthrough Cross-similarity\", or VERTEX score for short. The framework codebase\nand benchmark are linked below.",
      "tldr_zh": "本研究提出了 SymbolicAI 框架，这是一个多功能的模块化系统，采用逻辑-based 方法将生成模型与各种求解器整合，通过将大型语言模型 (LLMs) 作为语义解析器来处理基于自然和形式语言的指令，从而桥接符号推理与生成 AI。框架利用概率编程原则、可微编程和经典编程的优势，引入一组多态的、组合的和自引用的操作，支持多模态数据的多步生成过程，并确保输出与用户目标一致。SymbolicAI 还启用创建可解释的计算图，并引入 VERTEX score 作为质量评估指标，同时提出一个基准来比较不同 state-of-the-art LLMs 在复杂工作流中的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "46 pages, 13 figures, external resources: framework is available at\n  https://github.com/ExtensityAI/symbolicai and benchmark at\n  https://github.com/ExtensityAI/benchmark",
      "pdf_url": "http://arxiv.org/pdf/2402.00854v4",
      "published_date": "2024-02-01 18:50:50 UTC",
      "updated_date": "2024-08-21 22:07:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:04:58.960356"
    },
    {
      "arxiv_id": "2402.00839v2",
      "title": "X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System",
      "title_zh": "翻译失败",
      "authors": [
        "Kiymet Kaya",
        "Elif Ak",
        "Sumeyye Bas",
        "Berk Canberk",
        "Sule Gunduz Oguducu"
      ],
      "abstract": "The effectiveness of Intrusion Detection Systems (IDS) is critical in an era\nwhere cyber threats are becoming increasingly complex. Machine learning (ML)\nand deep learning (DL) models provide an efficient and accurate solution for\nidentifying attacks and anomalies in computer networks. However, using ML and\nDL models in IDS has led to a trust deficit due to their non-transparent\ndecision-making. This transparency gap in IDS research is significant,\naffecting confidence and accountability. To address, this paper introduces a\nnovel Explainable IDS approach, called X-CBA, that leverages the structural\nadvantages of Graph Neural Networks (GNNs) to effectively process network\ntraffic data, while also adapting a new Explainable AI (XAI) methodology.\nUnlike most GNN-based IDS that depend on labeled network traffic and node\nfeatures, thereby overlooking critical packet-level information, our approach\nleverages a broader range of traffic data through network flows, including edge\nattributes, to improve detection capabilities and adapt to novel threats.\nThrough empirical testing, we establish that our approach not only achieves\nhigh accuracy with 99.47% in threat detection but also advances the field by\nproviding clear, actionable explanations of its analytical outcomes. This\nresearch also aims to bridge the current gap and facilitate the broader\nintegration of ML/DL technologies in cybersecurity defenses by offering a local\nand global explainability solution that is both precise and interpretable.",
      "tldr_zh": "本研究提出了一种新型可解释入侵检测系统（X-CBA），旨在解决机器学习（ML）和深度学习（DL）模型在入侵检测系统（IDS）中的不透明决策问题，从而提升信任和可解释性。X-CBA 利用 Graph Neural Networks (GNNs) 处理网络流量数据，包括 edge attributes，以覆盖更广泛的流量信息，并结合 Explainable AI (XAI) 方法进行决策解释。实验结果显示，该系统在威胁检测中实现了99.47%的准确率，并提供清晰、可操作的本地和全局解释，有助于促进 ML/DL 在网络安全领域的广泛应用。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00839v2",
      "published_date": "2024-02-01 18:29:16 UTC",
      "updated_date": "2024-06-02 05:00:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:05:10.369796"
    },
    {
      "arxiv_id": "2402.00835v1",
      "title": "ALISON: Fast and Effective Stylometric Authorship Obfuscation",
      "title_zh": "翻译失败",
      "authors": [
        "Eric Xing",
        "Saranya Venkatraman",
        "Thai Le",
        "Dongwon Lee"
      ],
      "abstract": "Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing\ntasks of increasing importance in privacy research. Modern AA leverages an\nauthor's consistent writing style to match a text to its author using an AA\nclassifier. AO is the corresponding adversarial task, aiming to modify a text\nin such a way that its semantics are preserved, yet an AA model cannot\ncorrectly infer its authorship. To address privacy concerns raised by\nstate-of-the-art (SOTA) AA methods, new AO methods have been proposed but\nremain largely impractical to use due to their prohibitively slow training and\nobfuscation speed, often taking hours. To this challenge, we propose a\npractical AO method, ALISON, that (1) dramatically reduces training/obfuscation\ntime, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2)\nachieves better obfuscation success through attacking three transformer-based\nAA methods on two benchmark datasets, typically performing 15% better than\ncompeting methods, (3) does not require direct signals from a target AA\nclassifier during obfuscation, and (4) utilizes unique stylometric features,\nallowing sound model interpretation for explainable obfuscation. We also\ndemonstrate that ALISON can effectively prevent four SOTA AA methods from\naccurately determining the authorship of ChatGPT-generated texts, all while\nminimally changing the original text semantics. To ensure the reproducibility\nof our findings, our code and data are available at:\nhttps://github.com/EricX003/ALISON.",
      "tldr_zh": "本研究提出ALISON，一种快速有效的Authorship Obfuscation (AO)方法，用于对抗Authorship Attribution (AA)模型的作者识别，同时保留文本语义。ALISON通过利用独特的stylometric特征进行混淆，不依赖目标AA分类器的直接信号，并比现有SOTA方法快10倍以上，在两个基准数据集上攻击基于transformer的AA模型时，成功率通常提高15%。实验结果显示，ALISON还能有效保护ChatGPT生成文本的匿名性，同时最小化语义变化，为隐私保护提供可解释的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7; I.2.0"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 6 figures, 4 tables. To be published in the Proceedings of\n  the 38th Annual AAAI Conference on Artificial Intelligence (AAAI-24)",
      "pdf_url": "http://arxiv.org/pdf/2402.00835v1",
      "published_date": "2024-02-01 18:22:32 UTC",
      "updated_date": "2024-02-01 18:22:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:05:21.772519"
    },
    {
      "arxiv_id": "2402.00831v1",
      "title": "A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Elif Ak",
        "Kiymet Kaya",
        "Eren Ozaltun",
        "Sule Gunduz Oguducu",
        "Berk Canberk"
      ],
      "abstract": "Despite the crucial importance of addressing Black Hole failures in Internet\nbackbone networks, effective detection strategies in backbone networks are\nlacking. This is largely because previous research has been centered on Mobile\nAd-hoc Networks (MANETs), which operate under entirely different dynamics,\nprotocols, and topologies, making their findings not directly transferable to\nbackbone networks. Furthermore, detecting Black Hole failures in backbone\nnetworks is particularly challenging. It requires a comprehensive range of\nnetwork data due to the wide variety of conditions that need to be considered,\nmaking data collection and analysis far from straightforward. Addressing this\ngap, our study introduces a novel approach for Black Hole detection in backbone\nnetworks using specialized Yet Another Next Generation (YANG) data models with\nBlack Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our\nmethod of selecting and analyzing four YANG models relevant to Black Hole\ndetection in ISP networks, focusing on routing protocols and ISP-specific\nconfigurations. Our BHMM approach derived from these models demonstrates a 10%\nimprovement in detection accuracy and a 13% increase in packet delivery rate,\nhighlighting the efficiency of our approach. Additionally, we evaluate the\nMachine Learning approach leveraged with BHMM analysis in two different network\nsettings, a commercial ISP network, and a scientific research-only network\ntopology. This evaluation also demonstrates the practical applicability of our\nmethod, yielding significantly improved prediction outcomes in both\nenvironments.",
      "tldr_zh": "该研究针对互联网骨干网络中 Black Hole 故障检测的不足，指出现有方法主要基于 Mobile Ad-hoc Networks (MANETs)，难以直接应用于骨干网络。该方法引入一种基于 YANG 数据模型的统一策略，结合 Black Hole-sensitive Metric Matrix (BHMM) 分析，选择四个与路由协议和 ISP 特定配置相关的 YANG 模型，以提高检测效率。实验结果显示，该方法使检测准确率提升 10%，数据包交付率增加 13%，并在商业 ISP 网络和科研网络环境中通过机器学习评估，证明了其实际适用性和预测性能的显著改善。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00831v1",
      "published_date": "2024-02-01 18:17:37 UTC",
      "updated_date": "2024-02-01 18:17:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:05:34.225731"
    },
    {
      "arxiv_id": "2402.00828v2",
      "title": "Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters",
      "title_zh": "翻译失败",
      "authors": [
        "Umberto Cappellazzo",
        "Daniele Falavigna",
        "Alessio Brutti"
      ],
      "abstract": "Mixture of Experts (MoE) architectures have recently started burgeoning due\nto their ability to scale model's capacity while maintaining the computational\ncost affordable. Furthermore, they can be applied to both Transformers and\nState Space Models, the current state-of-the-art models in numerous fields.\nWhile MoE has been mostly investigated for the pre-training stage, its use in\nparameter-efficient transfer learning settings is under-explored. To narrow\nthis gap, this paper attempts to demystify the use of MoE for\nparameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and\nspeech downstream tasks. Specifically, we propose Soft Mixture of Adapters\n(Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft\nMoE method, it relies on a soft assignment between the input tokens and experts\nto keep the computational time limited. Extensive experiments across 4\nbenchmarks demonstrate that Soft-MoA outperforms the single adapter method and\nperforms on par with the dense MoA counterpart. We finally present ablation\nstudies on key elements of Soft-MoA, showing for example that Soft-MoA achieves\nbetter scaling with more experts, as well as ensuring that all experts\ncontribute to the computation of the output tokens, thus dispensing with the\nexpert imbalance issue.",
      "tldr_zh": "该论文探讨了 Mixture of Experts (MoE) 架构在参数高效微调 Audio Spectrogram Transformers (AST) 上的应用，以提升音频和语音下游任务的性能。作者提出 Soft Mixture of Adapters (Soft-MoA) 方法，将 adapters 作为 experts，并采用软分配机制来限制计算开销，确保高效性。在 4 个基准实验中，Soft-MoA 优于单一 adapter 方法，与密集 MoA 相当，并通过消融研究证明其在专家数量增加时具有更好的扩展性，同时解决了专家不平衡问题。",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at INTERSPEECH 2024. The code is publicly available at:\n  https://github.com/umbertocappellazzo/PETL_AST",
      "pdf_url": "http://arxiv.org/pdf/2402.00828v2",
      "published_date": "2024-02-01 18:16:04 UTC",
      "updated_date": "2024-06-04 15:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:05:46.117721"
    },
    {
      "arxiv_id": "2402.00823v2",
      "title": "SLIM: Skill Learning with Multiple Critics",
      "title_zh": "SLIM：使用多个批评者的技能学习",
      "authors": [
        "David Emukpere",
        "Bingbing Wu",
        "Julien Perez",
        "Jean-Michel Renders"
      ],
      "abstract": "Self-supervised skill learning aims to acquire useful behaviors that leverage\nthe underlying dynamics of the environment. Latent variable models, based on\nmutual information maximization, have been successful in this task but still\nstruggle in the context of robotic manipulation. As it requires impacting a\npossibly large set of degrees of freedom composing the environment, mutual\ninformation maximization fails alone in producing useful and safe manipulation\nbehaviors. Furthermore, tackling this by augmenting skill discovery rewards\nwith additional rewards through a naive combination might fail to produce\ndesired behaviors. To address this limitation, we introduce SLIM, a\nmulti-critic learning approach for skill discovery with a particular focus on\nrobotic manipulation. Our main insight is that utilizing multiple critics in an\nactor-critic framework to gracefully combine multiple reward functions leads to\na significant improvement in latent-variable skill discovery for robotic\nmanipulation while overcoming possible interference occurring among rewards\nwhich hinders convergence to useful skills. Furthermore, in the context of\ntabletop manipulation, we demonstrate the applicability of our novel skill\ndiscovery approach to acquire safe and efficient motor primitives in a\nhierarchical reinforcement learning fashion and leverage them through planning,\nsignificantly surpassing baseline approaches for skill discovery.",
      "tldr_zh": "本研究针对自监督技能学习（self-supervised skill learning）在机器人操作（robotic manipulation）中的挑战，指出基于互信息最大化（mutual information maximization）的潜在变量模型难以产生实用且安全的操作行为。SLIM 方法引入多批评家（multiple critics）学习框架，在演员-批评家（actor-critic）结构中优雅地结合多个奖励函数，克服奖励间的干扰并提升技能发现的收敛性。实验结果显示，SLIM 在桌面操作任务中通过层次强化学习（hierarchical reinforcement learning）和规划，获取了安全高效的运动基元，并显著超越基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IEEE ICRA 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00823v2",
      "published_date": "2024-02-01 18:07:33 UTC",
      "updated_date": "2024-03-21 10:21:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:05:58.401091"
    },
    {
      "arxiv_id": "2402.00822v1",
      "title": "WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework",
      "title_zh": "WiOpen",
      "authors": [
        "Xiang Zhang",
        "Jingyang Huang",
        "Huan Yan",
        "Peng Zhao",
        "Guohang Zhuang",
        "Zhi Liu",
        "Bin Liu"
      ],
      "abstract": "Recent years have witnessed a growing interest in Wi-Fi-based gesture\nrecognition. However, existing works have predominantly focused on closed-set\nparadigms, where all testing gestures are predefined during training. This\nposes a significant challenge in real-world applications, as unseen gestures\nmight be misclassified as known classes during testing. To address this issue,\nwe propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR)\nframework. Implementing OSGR requires addressing challenges caused by the\nunique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and\ndomains, leads to widely scattered and irregular data distributions in\ncollected Wi-Fi sensing data. Consequently, data ambiguity between classes and\nchallenges in defining appropriate decision boundaries to identify unknowns\narise. To tackle these challenges, WiOpen adopts a two-fold approach to\neliminate uncertainty and define precise decision boundaries. Initially, it\naddresses uncertainty induced by noise during data preprocessing by utilizing\nthe CSI ratio. Next, it designs the OSGR network based on an uncertainty\nquantification method. Throughout the learning process, this network\neffectively mitigates uncertainty stemming from domains. Ultimately, the\nnetwork leverages relationships among samples' neighbors to dynamically define\nopen-set decision boundaries, successfully realizing OSGR. Comprehensive\nexperiments on publicly accessible datasets confirm WiOpen's effectiveness.\nNotably, WiOpen also demonstrates superiority in cross-domain tasks when\ncompared to state-of-the-art approaches.",
      "tldr_zh": "该研究针对现有Wi-Fi-based手势识别系统主要依赖闭集范式的问题，提出WiOpen框架，以实现鲁棒的Open-Set Gesture Recognition (OSGR)，能够正确识别训练中未见的手势。WiOpen采用双重策略应对Wi-Fi感知数据的独特不确定性：首先，通过CSI ratio在数据预处理阶段减少噪声引起的模糊；其次，构建基于不确定性量化方法的OSGR网络，缓解领域差异并利用样本邻居关系动态定义决策边界。实验结果显示，WiOpen在公开数据集上表现出色，尤其在跨域任务中优于现有最先进方法。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00822v1",
      "published_date": "2024-02-01 18:05:38 UTC",
      "updated_date": "2024-02-01 18:05:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:06:09.833068"
    },
    {
      "arxiv_id": "2402.00816v1",
      "title": "Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander W. Goodall",
        "Francesco Belardinelli"
      ],
      "abstract": "Shielding is a popular technique for achieving safe reinforcement learning\n(RL). However, classical shielding approaches come with quite restrictive\nassumptions making them difficult to deploy in complex environments,\nparticularly those with continuous state or action spaces. In this paper we\nextend the more versatile approximate model-based shielding (AMBS) framework to\nthe continuous setting. In particular we use Safety Gym as our test-bed,\nallowing for a more direct comparison of AMBS with popular constrained RL\nalgorithms. We also provide strong probabilistic safety guarantees for the\ncontinuous setting. In addition, we propose two novel penalty techniques that\ndirectly modify the policy gradient, which empirically provide more stable\nconvergence in our experiments.",
      "tldr_zh": "本文提出了一种在连续环境中利用 approximate model-based shielding (AMBS) 框架来提升强化学习 (RL) 安全性的方法，以解决经典 shielding 技术的限制问题。该方法扩展 AMBS 到连续状态和动作空间，使用 Safety Gym 作为测试平台，并与 constrained RL 算法进行直接比较，同时提供了强有力的概率安全保证。作者还引入了两种新颖的 penalty 技术，直接修改 policy gradient，以实现更稳定的收敛。实验结果表明，该方法在复杂环境中显著提高了 RL 的安全性和性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as an Extended Abstract at AAMAS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00816v1",
      "published_date": "2024-02-01 17:55:08 UTC",
      "updated_date": "2024-02-01 17:55:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:06:22.576856"
    },
    {
      "arxiv_id": "2402.00808v2",
      "title": "Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario",
      "title_zh": "翻译失败",
      "authors": [
        "Marta Mondellini",
        "Matteo Lavit Nicora",
        "Pooja Prajod",
        "Elisabeth André",
        "Rocco Vertechy",
        "Alessandro Antonietti",
        "Matteo Malosio"
      ],
      "abstract": "In industrial scenarios, there is widespread use of collaborative robots\n(cobots), and growing interest is directed at evaluating and measuring the\nimpact of some characteristics of the cobot on the human factor. In the present\npilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 -\nAdapted to the participant's pace) of a cobot has on the Experiential Locus of\nControl (ELoC) and the emotional state of 31 participants has been examined.\nThe operators' performance, the degree of basic internal Locus of Control, and\nthe attitude towards the robots were also considered. No difference was found\nregarding the emotional state and the ELoC in the three conditions, but\nconsidering the other psychological variables, a more complex situation\nemerges. Overall, results seem to indicate a need to consider the person's\npsychological characteristics to offer a differentiated and optimal interaction\nexperience.",
      "tldr_zh": "本研究探讨了协作机器人（cobots）的生产节奏（慢速、快速或适应参与者节奏）对31名参与者的体验性控制位（ELoC）和情感状态的影响，旨在评估cobots特性对人类因素的影响。实验设计还考虑了操作员的表现、基本内部Locus of Control以及对机器人的态度。结果显示，三种生产节奏条件下，参与者的情感状态和ELoC没有显著差异，但其他心理变量揭示了更复杂的互动模式。总体而言，该研究强调了在人机协作场景中，需要根据个人的心理特征来优化互动体验。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to 4th IEEE International Conference on Human-Machine\n  Systems",
      "pdf_url": "http://arxiv.org/pdf/2402.00808v2",
      "published_date": "2024-02-01 17:44:46 UTC",
      "updated_date": "2024-06-28 02:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:06:34.903921"
    },
    {
      "arxiv_id": "2402.00798v4",
      "title": "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents",
      "title_zh": "Formal-LLM：整合形式语言和自然语言用于可控的基于LLM的智能体",
      "authors": [
        "Zelong Li",
        "Wenyue Hua",
        "Hao Wang",
        "He Zhu",
        "Yongfeng Zhang"
      ],
      "abstract": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM.",
      "tldr_zh": "该论文提出 Formal-LLM 框架，将自然语言的表达性和形式语言的精确性相结合，以提升 LLM-based agents 的计划生成可控性，解决当前代理经常产生无效或不可执行计划的问题。具体方法允许开发者使用 automaton 表达约束，并在自动机的监督下进行栈-based LLM 计划生成，确保计划符合要求。实验在基准任务和实际生活中显示，框架整体性能提升超过 50%，验证了其有效性。该框架有望促进 LLM 在高有效性规划场景中的更广泛应用，如提供源代码链接所示。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.FL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00798v4",
      "published_date": "2024-02-01 17:30:50 UTC",
      "updated_date": "2024-08-12 17:54:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:06:47.459167"
    },
    {
      "arxiv_id": "2402.00795v4",
      "title": "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",
      "title_zh": "LLMs 学习动态系统的支配原则，揭示一种在上下文中的神经缩",
      "authors": [
        "Toni J. B. Liu",
        "Nicolas Boullé",
        "Raphaël Sarfati",
        "Christopher J. Earls"
      ],
      "abstract": "Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. We study LLMs' ability to\nextrapolate the behavior of dynamical systems whose evolution is governed by\nprinciples of physical interest. Our results show that LLaMA 2, a language\nmodel trained primarily on texts, achieves accurate predictions of dynamical\nsystem time series without fine-tuning or prompt engineering. Moreover, the\naccuracy of the learned physical rules increases with the length of the input\ncontext window, revealing an in-context version of neural scaling law. Along\nthe way, we present a flexible and efficient algorithm for extracting\nprobability density functions of multi-digit numbers directly from LLMs.",
      "tldr_zh": "这篇论文探讨了预训练的大型语言模型 (LLMs) 在零样本任务中的能力，特别是预测由物理原理驱动的动态系统的行为。研究发现，LLaMA 2 模型无需微调或提示工程，就能准确预测动态系统的时序数据，且预测准确性随输入上下文窗口长度增加而提升，揭示了 in-context neural scaling law。此外，论文提出了一种灵活高效的算法，用于从 LLMs 中提取多位数字的概率密度函数 (probability density functions)。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00795v4",
      "published_date": "2024-02-01 17:28:10 UTC",
      "updated_date": "2024-10-09 16:02:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:06:58.619931"
    },
    {
      "arxiv_id": "2402.00794v2",
      "title": "ReAGent: A Model-agnostic Feature Attribution Method for Generative Language Models",
      "title_zh": "ReAGent：一种模型无关的特征归因方法，用于生成式语言模型",
      "authors": [
        "Zhixue Zhao",
        "Boxuan Shan"
      ],
      "abstract": "Feature attribution methods (FAs), such as gradients and attention, are\nwidely employed approaches to derive the importance of all input features to\nthe model predictions. Existing work in natural language processing has mostly\nfocused on developing and testing FAs for encoder-only language models (LMs) in\nclassification tasks. However, it is unknown if it is faithful to use these FAs\nfor decoder-only models on text generation, due to the inherent differences\nbetween model architectures and task settings respectively. Moreover, previous\nwork has demonstrated that there is no `one-wins-all' FA across models and\ntasks. This makes the selection of a FA computationally expensive for large LMs\nsince input importance derivation often requires multiple forward and backward\npasses including gradient computations that might be prohibitive even with\naccess to large compute. To address these issues, we present a model-agnostic\nFA for generative LMs called Recursive Attribution Generator (ReAGent). Our\nmethod updates the token importance distribution in a recursive manner. For\neach update, we compute the difference in the probability distribution over the\nvocabulary for predicting the next token between using the original input and\nusing a modified version where a part of the input is replaced with RoBERTa\npredictions. Our intuition is that replacing an important token in the context\nshould have resulted in a larger change in the model's confidence in predicting\nthe token than replacing an unimportant token. Our method can be universally\napplied to any generative LM without accessing internal model weights or\nadditional training and fine-tuning, as most other FAs require. We extensively\ncompare the faithfulness of ReAGent with seven popular FAs across six\ndecoder-only LMs of various sizes. The results show that our method\nconsistently provides more faithful token importance distributions.",
      "tldr_zh": "该论文提出 ReAGent，一种模型无关的特征归因（Feature Attribution, FA）方法，旨在为生成式语言模型（decoder-only LMs）提供更可靠的输入特征重要性评估，以解决现有FA方法在不同模型和任务间的适用性问题。ReAGent 通过递归更新 token 重要性分布，计算原始输入与修改输入（用 RoBERTa 预测替换部分输入）在预测下一个 token 的概率分布差异，从而判断 token 的重要性，而无需访问模型内部权重或进行额外训练。实验结果显示，ReAGent 在六个 decoder-only LMs 上与七个流行 FA 方法比较后，提供更忠实的 token 重要性分布，证明了其普适性和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AAAI24 workshop ReLM",
      "pdf_url": "http://arxiv.org/pdf/2402.00794v2",
      "published_date": "2024-02-01 17:25:51 UTC",
      "updated_date": "2024-02-07 21:01:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:07:11.283781"
    },
    {
      "arxiv_id": "2402.00793v3",
      "title": "Human Expertise in Algorithmic Prediction",
      "title_zh": "算法预测中的人类专业知识",
      "authors": [
        "Rohan Alur",
        "Manish Raghavan",
        "Devavrat Shah"
      ],
      "abstract": "We introduce a novel framework for incorporating human expertise into\nalgorithmic predictions. Our approach leverages human judgment to distinguish\ninputs which are algorithmically indistinguishable, or \"look the same\" to\npredictive algorithms. We argue that this framing clarifies the problem of\nhuman-AI collaboration in prediction tasks, as experts often form judgments by\ndrawing on information which is not encoded in an algorithm's training data.\nAlgorithmic indistinguishability yields a natural test for assessing whether\nexperts incorporate this kind of \"side information\", and further provides a\nsimple but principled method for selectively incorporating human feedback into\nalgorithmic predictions. We show that this method provably improves the\nperformance of any feasible algorithmic predictor and precisely quantify this\nimprovement. We find empirically that although algorithms often outperform\ntheir human counterparts on average, human judgment can improve algorithmic\npredictions on specific instances (which can be identified ex-ante). In an\nX-ray classification task, we find that this subset constitutes nearly $30\\%$\nof the patient population. Our approach provides a natural way of uncovering\nthis heterogeneity and thus enabling effective human-AI collaboration.",
      "tldr_zh": "本研究提出了一种新框架，用于将人类专业知识融入算法预测中，特别针对算法无法区分的输入（algorithmically indistinguishable），通过人类判断利用训练数据中未编码的“side information”来提升预测准确性。该方法提供了一个简单且原则性的方式来评估专家是否使用此类额外信息，并选择性地整合人类反馈，从而证明性地改善任何可行算法预测器的性能。实验结果显示，虽然算法在平均表现上优于人类，但在特定实例中人类判断能显著提升预测效果，例如在X射线分类任务中，约30%的患者案例受益；这种方法有助于揭示人类-人工智能协作（human-AI collaboration）的异质性，实现更有效的合作。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.00793v3",
      "published_date": "2024-02-01 17:23:54 UTC",
      "updated_date": "2024-10-30 15:45:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:07:22.268198"
    },
    {
      "arxiv_id": "2402.00789v1",
      "title": "Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces",
      "title_zh": "Graph-Mamba：面向长程图序列建模的选择性状态空间",
      "authors": [
        "Chloe Wang",
        "Oleksii Tsepa",
        "Jun Ma",
        "Bo Wang"
      ],
      "abstract": "Attention mechanisms have been widely used to capture long-range dependencies\namong nodes in Graph Transformers. Bottlenecked by the quadratic computational\ncost, attention mechanisms fail to scale in large graphs. Recent improvements\nin computational efficiency are mainly achieved by attention sparsification\nwith random or heuristic-based graph subsampling, which falls short in\ndata-dependent context reasoning. State space models (SSMs), such as Mamba,\nhave gained prominence for their effectiveness and efficiency in modeling\nlong-range dependencies in sequential data. However, adapting SSMs to\nnon-sequential graph data presents a notable challenge. In this work, we\nintroduce Graph-Mamba, the first attempt to enhance long-range context modeling\nin graph networks by integrating a Mamba block with the input-dependent node\nselection mechanism. Specifically, we formulate graph-centric node\nprioritization and permutation strategies to enhance context-aware reasoning,\nleading to a substantial improvement in predictive performance. Extensive\nexperiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms\nstate-of-the-art methods in long-range graph prediction tasks, with a fraction\nof the computational cost in both FLOPs and GPU memory consumption. The code\nand models are publicly available at https://github.com/bowang-lab/Graph-Mamba.",
      "tldr_zh": "本研究提出Graph-Mamba，一种新型框架，旨在通过整合State Space Models (SSMs)如Mamba的块与输入依赖的节点选择机制，提升图网络中长程序列建模的效率和性能。Graph-Mamba采用graph-centric的节点优先化和排列策略，实现数据依赖的上下文感知推理，从而克服Attention mechanisms在大型图数据中计算成本过高的局限。实验结果显示，在十个基准数据集上，Graph-Mamba在长程图预测任务中超越现有最先进方法，同时显著降低FLOPs和GPU内存消耗。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00789v1",
      "published_date": "2024-02-01 17:21:53 UTC",
      "updated_date": "2024-02-01 17:21:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:07:34.660111"
    },
    {
      "arxiv_id": "2402.00759v3",
      "title": "Building Expressive and Tractable Probabilistic Generative Models: A Review",
      "title_zh": "翻译失败",
      "authors": [
        "Sahil Sidheekh",
        "Sriraam Natarajan"
      ],
      "abstract": "We present a comprehensive survey of the advancements and techniques in the\nfield of tractable probabilistic generative modeling, primarily focusing on\nProbabilistic Circuits (PCs). We provide a unified perspective on the inherent\ntrade-offs between expressivity and tractability, highlighting the design\nprinciples and algorithmic extensions that have enabled building expressive and\nefficient PCs, and provide a taxonomy of the field. We also discuss recent\nefforts to build deep and hybrid PCs by fusing notions from deep neural models,\nand outline the challenges and open questions that can guide future research in\nthis evolving field.",
      "tldr_zh": "这篇综述论文全面审视了可计算概率生成模型的进展，特别是Probabilistic Circuits (PCs)，并探讨了表达性和可计算性之间的内在权衡。论文提供了设计原则、算法扩展的统一视角，以及构建深度和混合PCs的方法，例如融合深度神经模型的概念。最终，它提出了该领域的分类、现有挑战和未来研究方向，以指导进一步探索。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00759v3",
      "published_date": "2024-02-01 16:49:27 UTC",
      "updated_date": "2024-06-06 11:25:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:07:46.019181"
    },
    {
      "arxiv_id": "2402.00751v1",
      "title": "Unlearnable Algorithms for In-context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Andrei Muresanu",
        "Anvith Thudi",
        "Michael R. Zhang",
        "Nicolas Papernot"
      ],
      "abstract": "Machine unlearning is a desirable operation as models get increasingly\ndeployed on data with unknown provenance. However, achieving exact unlearning\n-- obtaining a model that matches the model distribution when the data to be\nforgotten was never used -- is challenging or inefficient, often requiring\nsignificant retraining. In this paper, we focus on efficient unlearning methods\nfor the task adaptation phase of a pretrained large language model (LLM). We\nobserve that an LLM's ability to do in-context learning for task adaptation\nallows for efficient exact unlearning of task adaptation training data. We\nprovide an algorithm for selecting few-shot training examples to prepend to the\nprompt given to an LLM (for task adaptation), ERASE, whose unlearning operation\ncost is independent of model and dataset size, meaning it scales to large\nmodels and datasets. We additionally compare our approach to fine-tuning\napproaches and discuss the trade-offs between the two approaches. This leads us\nto propose a new holistic measure of unlearning cost which accounts for varying\ninference costs, and conclude that in-context learning can often be more\nfavourable than fine-tuning for deployments involving unlearning requests.",
      "tldr_zh": "本论文探讨了机器 unlearning 在大语言模型(LLM)任务适应阶段的挑战，提出一种高效算法 ERASE，利用 in-context learning 选择 few-shot 训练示例添加到提示中，实现 unlearning 操作的成本独立于模型和数据集大小。相比 fine-tuning 方法，ERASE 避免了大量重新训练，并通过一个新的整体 unlearning 成本度量（考虑推理成本）证明了其优势。研究结论表明，对于涉及 unlearning 请求的部署，in-context learning 通常更高效和可扩展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00751v1",
      "published_date": "2024-02-01 16:43:04 UTC",
      "updated_date": "2024-02-01 16:43:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:07:58.985851"
    },
    {
      "arxiv_id": "2402.00742v2",
      "title": "Transforming and Combining Rewards for Aligning Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Wang",
        "Chirag Nagpal",
        "Jonathan Berant",
        "Jacob Eisenstein",
        "Alex D'Amour",
        "Sanmi Koyejo",
        "Victor Veitch"
      ],
      "abstract": "A common approach for aligning language models to human preferences is to\nfirst learn a reward model from preference data, and then use this reward model\nto update the language model. We study two closely related problems that arise\nin this approach. First, any monotone transformation of the reward model\npreserves preference ranking; is there a choice that is ``better'' than others?\nSecond, we often wish to align language models to multiple properties: how\nshould we combine multiple reward models? Using a probabilistic interpretation\nof the alignment procedure, we identify a natural choice for transformation for\n(the common case of) rewards learned from Bradley-Terry preference models. The\nderived transformation is straightforward: we apply a log-sigmoid function to\nthe centered rewards, a method we term ``LSC-transformation''\n(log-sigmoid-centered transformation). This transformation has two important\nproperties. First, it emphasizes improving poorly-performing outputs, rather\nthan outputs that already score well. This mitigates both underfitting (where\nsome prompts are not improved) and reward hacking (where the model learns to\nexploit misspecification of the reward model). Second, it enables principled\naggregation of rewards by linking summation to logical conjunction: the sum of\ntransformed rewards corresponds to the probability that the output is ``good''\nin all measured properties, in a sense we make precise. Experiments aligning\nlanguage models to be both helpful and harmless using RLHF show substantial\nimprovements over the baseline (non-transformed) approach.",
      "tldr_zh": "该论文研究了如何通过变换和结合奖励模型来对齐大型语言模型（Large Language Models），以解决偏好排名不变性问题和多属性对齐挑战。作者提出 LSC-transformation（log-sigmoid-centered transformation），一种基于概率解释的变换方法，适用于从 Bradley-Terry 偏好模型学得的奖励模型，该方法强调改善表现较差的输出，从而缓解欠拟合和奖励黑客（reward hacking）问题。实验结果显示，使用 RLHF（Reinforcement Learning from Human Feedback）对语言模型进行帮助性和无害性对齐时，LSC 方法比基线方法有显著改进。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00742v2",
      "published_date": "2024-02-01 16:39:28 UTC",
      "updated_date": "2024-07-19 05:12:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:08:10.819624"
    },
    {
      "arxiv_id": "2402.00738v1",
      "title": "FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game",
      "title_zh": "翻译失败",
      "authors": [
        "Guangzheng Hu",
        "Yuanheng Zhu",
        "Haoran Li",
        "Dongbin Zhao"
      ],
      "abstract": "Many real-world applications involve some agents that fall into two teams,\nwith payoffs that are equal within the same team but of opposite sign across\nthe opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can\nbe resolved with reinforcement learning in recent years. However, existing\nmethods are thus inefficient in light of insufficient consideration of\nintra-team credit assignment, data utilization and computational\nintractability. In this paper, we propose the individual-global-minimax (IGMM)\nprinciple to ensure the coherence between two-team minimax behaviors and the\nindividual greedy behaviors through Q functions in 2t0sMGs. Based on it, we\npresent a novel multi-agent reinforcement learning framework, Factorized\nMulti-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q\nfunction into individual ones and iteratively solve for the IGMM-satisfied\nminimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with\nneural networks is proposed to implement FM3Q and obtain the deterministic and\ndecentralized minimax policies for two-team players. A theoretical analysis is\nprovided to prove the convergence of FM3Q. Empirically, we use three\nenvironments to evaluate the learning efficiency and final performance of FM3Q\nand show its superiority on 2t0sMGs.",
      "tldr_zh": "本论文针对两队零和马尔可夫博弈（Two-Team Zero-Sum Markov Games, 2t0sMGs）提出了一种新的多智能体强化学习框架，即Factorized Multi-Agent MiniMax Q-Learning (FM3Q)，以解决现有方法在队内信用分配、数据利用和计算复杂性方面的不足。FM3Q 通过Individual-Global-Minimax (IGMM) 原则，将联合最小最大 Q 函数分解为个体 Q 函数，并通过迭代求解确保行为一致性。论文还设计了一个基于神经网络的在线学习算法，实现确定性和分散式最小最大策略，并提供了理论证明来验证 FM3Q 的收敛性。在三个实验环境中，FM3Q 展示了更高的学习效率和性能表现，证明了其在 2t0sMGs 中的优越性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00738v1",
      "published_date": "2024-02-01 16:37:21 UTC",
      "updated_date": "2024-02-01 16:37:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:08:24.054968"
    },
    {
      "arxiv_id": "2402.00722v1",
      "title": "Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators",
      "title_zh": "翻译失败",
      "authors": [
        "Raul Fernandez-Fernandez",
        "Marco Aggravi",
        "Paolo Robuffo Giordano",
        "Juan G. Victores",
        "Claudio Pacchierotti"
      ],
      "abstract": "Neural Style Transfer (NST) refers to a class of algorithms able to\nmanipulate an element, most often images, to adopt the appearance or style of\nanother one. Each element is defined as a combination of Content and Style: the\nContent can be conceptually defined as the what and the Style as the how of\nsaid element. In this context, we propose a custom NST framework for\ntransferring a set of styles to the motion of a robotic manipulator, e.g., the\nsame robotic task can be carried out in an angry, happy, calm, or sad way. An\nautoencoder architecture extracts and defines the Content and the Style of the\ntarget robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3)\nnetwork generates the robot control policy using the loss defined by the\nautoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the\nrobot motion by introducing the trained style. Such an approach can be\nimplemented either offline, for carrying out autonomous robot motions in\ndynamic environments, or online, for adapting at runtime the style of a\nteleoperated robot. The considered styles can be learned online from human\ndemonstrations. We carried out an evaluation with human subjects enrolling 73\nvolunteers, asking them to recognize the style behind some representative\nrobotic motions. Results show a good recognition rate, proving that it is\npossible to convey different styles to a robot using this approach.",
      "tldr_zh": "本研究提出了一种名为 Neural Policy Style Transfer TD3 (NPST3) 的框架，将 Neural Style Transfer (NST) 与 Twin-Delayed DDPG (TD3) 相结合，用于机器人操纵器的共享控制，允许机器人动作采用不同风格（如愤怒、快乐或平静）。该方法使用 autoencoder 架构提取和定义机器人动作的内容（Content）和风格（Style），并通过 TD3 网络基于这些提取生成控制策略，支持离线自主执行或在线实时适应。实验中，73 名志愿者参与风格识别任务，结果显示较高的识别率，证明了该方法能有效传达和转移机器人动作风格。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00722v1",
      "published_date": "2024-02-01 16:14:32 UTC",
      "updated_date": "2024-02-01 16:14:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:08:36.013875"
    },
    {
      "arxiv_id": "2402.00715v2",
      "title": "Intent Assurance using LLMs guided by Intent Drift",
      "title_zh": "翻译失败",
      "authors": [
        "Kristina Dzeparoska",
        "Ali Tizghadam",
        "Alberto Leon-Garcia"
      ],
      "abstract": "Intent-Based Networking (IBN) presents a paradigm shift for network\nmanagement, by promising to align intents and business objectives with network\noperations--in an automated manner. However, its practical realization is\nchallenging: 1) processing intents, i.e., translate, decompose and identify the\nlogic to fulfill the intent, and 2) intent conformance, that is, considering\ndynamic networks, the logic should be adequately adapted to assure intents. To\naddress the latter, intent assurance is tasked with continuous verification and\nvalidation, including taking the necessary actions to align the operational and\ntarget states. In this paper, we define an assurance framework that allows us\nto detect and act when intent drift occurs. To do so, we leverage AI-driven\npolicies, generated by Large Language Models (LLMs) which can quickly learn the\nnecessary in-context requirements, and assist with the fulfillment and\nassurance of intents.",
      "tldr_zh": "本文针对基于意图的网络（IBN）的实际挑战，提出一个意图保证框架，用于检测和处理 intent drift，从而确保动态网络环境中意图与业务目标的持续对齐。该框架利用 Large Language Models (LLMs) 生成 AI 驱动策略，这些策略能快速学习上下文要求，并辅助意图的翻译、分解和执行。实验结果表明，此方法有效提升了意图的验证和适应能力，促进网络管理的自动化和可靠性。",
      "categories": [
        "cs.AI",
        "cs.NI",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00715v2",
      "published_date": "2024-02-01 16:09:19 UTC",
      "updated_date": "2024-02-02 23:08:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:08:47.695614"
    },
    {
      "arxiv_id": "2402.00707v1",
      "title": "Non-Exchangeable Conformal Language Generation with Nearest Neighbors",
      "title_zh": "翻译失败",
      "authors": [
        "Dennis Ulmer",
        "Chrysoula Zerva",
        "André F. T. Martins"
      ],
      "abstract": "Quantifying uncertainty in automatically generated text is important for\nletting humans check potential hallucinations and making systems more reliable.\nConformal prediction is an attractive framework to provide predictions imbued\nwith statistical guarantees, however, its application to text generation is\nchallenging since any i.i.d. assumptions are not realistic. In this paper, we\nbridge this gap by leveraging recent results on non-exchangeable conformal\nprediction, which still ensures bounds on coverage. The result,\nnon-exchangeable conformal nucleus sampling, is a novel extension of the\nconformal prediction framework to generation based on nearest neighbors. Our\nmethod can be used post-hoc for an arbitrary model without extra training and\nsupplies token-level, calibrated prediction sets equipped with statistical\nguarantees. Experiments in machine translation and language modeling show\nencouraging results in generation quality. By also producing tighter prediction\nsets with good coverage, we thus give a more theoretically principled way to\nperform sampling with conformal guarantees.",
      "tldr_zh": "这篇论文针对文本生成的非独立同分布（non-exchangeable）问题，提出了一种基于 nearest neighbors 的 non-exchangeable conformal nucleus sampling 方法，以量化不确定性并提供统计保证。该方法无需额外训练，可后处理任意模型，生成 token-level 的校准预测集，从而帮助检测潜在幻觉并提升系统可靠性。在机器翻译和语言建模实验中，该方法显著提高了生成质量，同时实现了更紧凑的预测集和良好的覆盖率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00707v1",
      "published_date": "2024-02-01 16:04:04 UTC",
      "updated_date": "2024-02-01 16:04:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:09:00.047880"
    },
    {
      "arxiv_id": "2402.01777v1",
      "title": "On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble",
      "title_zh": "翻译失败",
      "authors": [
        "Adrita Barua",
        "Gary Brase",
        "Ke Dong",
        "Pascal Hitzler",
        "Eugene Vasserman"
      ],
      "abstract": "We subject GPT-4 to a number of rigorous psychometric tests and analyze the\nresults. We find that, compared to the average human, GPT-4 tends to show more\nhonesty and humility, and less machiavellianism and narcissism. It sometimes\nexhibits ambivalent sexism, leans slightly toward masculinity, is moderately\nanxious but mostly not depressive (but not always). It shows human-average\nnumerical literacy and has cognitive reflection abilities that are above human\naverage for verbal tasks.",
      "tldr_zh": "本研究对GPT-4进行了严格的心理测量测试，评估其心理特征与人类平均水平的比较。结果显示，GPT-4表现出更高的honesty和humility，但machiavellianism和narcissism较低。它有时呈现ambivalent sexism，略微倾向于masculinity，中度anxious但不常depressive。GPT-4的numerical literacy与人类平均相当，而在语言任务中的cognitive reflection能力高于人类平均。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 8 tables, 1 code repository",
      "pdf_url": "http://arxiv.org/pdf/2402.01777v1",
      "published_date": "2024-02-01 15:58:13 UTC",
      "updated_date": "2024-02-01 15:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:09:10.743848"
    },
    {
      "arxiv_id": "2402.00699v1",
      "title": "PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software",
      "title_zh": "PeaTMOSS：开源软件中预训练模型的数据集及初步分析",
      "authors": [
        "Wenxin Jiang",
        "Jerin Yasmin",
        "Jason Jones",
        "Nicholas Synovic",
        "Jiashen Kuo",
        "Nathaniel Bielanski",
        "Yuan Tian",
        "George K. Thiruvathukal",
        "James C. Davis"
      ],
      "abstract": "The development and training of deep learning models have become increasingly\ncostly and complex. Consequently, software engineers are adopting pre-trained\nmodels (PTMs) for their downstream applications. The dynamics of the PTM supply\nchain remain largely unexplored, signaling a clear need for structured datasets\nthat document not only the metadata but also the subsequent applications of\nthese models. Without such data, the MSR community cannot comprehensively\nunderstand the impact of PTM adoption and reuse. This paper presents the\nPeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed\nsnapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with\n28,575 open-source software repositories from GitHub that utilize these models.\nAdditionally, the dataset includes 44,337 mappings from 15,129 downstream\nGitHub repositories to the 2,530 PTMs they use. To enhance the dataset's\ncomprehensiveness, we developed prompts for a large language model to\nautomatically extract model metadata, including the model's training datasets,\nparameters, and evaluation metrics. Our analysis of this dataset provides the\nfirst summary statistics for the PTM supply chain, showing the trend of PTM\ndevelopment and common shortcomings of PTM package documentation. Our example\napplication reveals inconsistencies in software licenses across PTMs and their\ndependent projects. PeaTMOSS lays the foundation for future research, offering\nrich opportunities to investigate the PTM supply chain. We outline mining\nopportunities on PTMs, their downstream usage, and cross-cutting questions.",
      "tldr_zh": "该论文介绍了PeaTMOSS数据集，用于分析开源软件中的预训练模型(PTMs)，它包含281,638个PTMs的元数据、14,296个高下载模型的详细快照，以及28,575个使用这些模型的GitHub仓库。研究者通过大语言模型提示自动提取模型信息，如训练数据集、参数和评估指标，以增强数据集的全面性。分析结果首次总结了PTM供应链的统计趋势、常见文档不足问题，并揭示了PTMs与下游项目间的软件许可不一致性，为未来探索PTM采用和重用的研究奠定基础。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at MSR'24",
      "pdf_url": "http://arxiv.org/pdf/2402.00699v1",
      "published_date": "2024-02-01 15:55:50 UTC",
      "updated_date": "2024-02-01 15:55:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:09:23.965816"
    },
    {
      "arxiv_id": "2402.00689v1",
      "title": "Ocassionally Secure: A Comparative Analysis of Code Generation Assistants",
      "title_zh": "偶尔安全：代码生成助手的比较分析",
      "authors": [
        "Ran Elgedawy",
        "John Sadik",
        "Senjuti Dutta",
        "Anuj Gautam",
        "Konstantinos Georgiou",
        "Farzin Gholamrezae",
        "Fujiao Ji",
        "Kyungchan Lim",
        "Qian Liu",
        "Scott Ruoti"
      ],
      "abstract": "$ $Large Language Models (LLMs) are being increasingly utilized in various\napplications, with code generations being a notable example. While previous\nresearch has shown that LLMs have the capability to generate both secure and\ninsecure code, the literature does not take into account what factors help\ngenerate secure and effective code. Therefore in this paper we focus on\nidentifying and understanding the conditions and contexts in which LLMs can be\neffectively and safely deployed in real-world scenarios to generate quality\ncode. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and\nGPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to\nassess each model's code generation capabilities. We contextualized our study\nto represent the typical use cases of a real-life developer employing LLMs for\neveryday tasks as work. Additionally, we place an emphasis on security\nawareness which is represented through the use of two distinct versions of our\ndeveloper persona. In total, we collected 61 code outputs and analyzed them\nacross several aspects: functionality, security, performance, complexity, and\nreliability. These insights are crucial for understanding the models'\ncapabilities and limitations, guiding future development and practical\napplications in the field of automated code generation.",
      "tldr_zh": "本研究比较分析了四个高级LLMs（GPT-3.5、GPT-4 via ChatGPT、Bard和Gemini）在代码生成中的性能，重点探讨了生成安全有效代码的条件和上下文。研究者设计了9个任务，模拟真实开发者场景，并使用两个不同版本的开发者角色评估代码的功能性、安全性、性能、复杂性和可靠性，共分析了61个代码输出。结果揭示了这些模型的优势和局限性，为LLMs在实际自动化代码生成中的安全部署和未来发展提供了重要指导。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.00689v1",
      "published_date": "2024-02-01 15:49:47 UTC",
      "updated_date": "2024-02-01 15:49:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:09:36.672889"
    },
    {
      "arxiv_id": "2402.00678v1",
      "title": "Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Raul Fernandez-Fernandez",
        "Juan G. Victores",
        "David Estevez",
        "Carlos Balaguer"
      ],
      "abstract": "One of the most important challenges of Smart City Applications is to adapt\nthe system to interact with non-expert users. Robot imitation frameworks aim to\nsimplify and reduce times of robot programming by allowing users to program\ndirectly through demonstrations. In classical frameworks, actions are modeled\nusing joint or Cartesian space trajectories. Other features, such as visual\nones, are not always well represented with these pure geometrical approaches.\nContinuous Goal-Directed Actions (CGDA) is an alternative to these methods, as\nit encodes actions as changes of any feature that can be extracted from the\nenvironment. As a consequence of this, the robot joint trajectories for\nexecution must be fully computed to comply with this feature-agnostic encoding.\nThis is achieved using Evolutionary Algorithms (EA), which usually requires too\nmany evaluations to perform this evolution step in the actual robot. Current\nstrategies involve performing evaluations in a simulation, transferring the\nfinal joint trajectory to the actual robot. Smart City applications involve\nworking in highly dynamic and complex environments, where having a precise\nmodel is not always achievable. Our goal is to study the tractability of\nperforming these evaluations directly in a real-world scenario. Two different\napproaches to reduce the number of evaluations using EA, are proposed and\ncompared. In the first approach, Particle Swarm Optimization (PSO)-based\nmethods have been studied and compared within CGDA: naive PSO, Fitness\nInheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO\n(AFFG-PSO). The second approach studied the introduction of geometrical and\nvelocity constraints within CGDA. The effects of both approaches were analyzed\nand compared in the wax and paint actions, two CGDA commonly studied use cases.\nResults from this paper depict an important reduction in the number of\nevaluations.",
      "tldr_zh": "本研究探讨了在智能城市应用中，使用 Continuous Goal-Directed Actions (CGDA) 进行真实评估的可行性，以简化机器人编程并适应非专家用户。论文提出两种减少 Evolutionary Algorithms (EA) 评估次数的方法：一是基于 Particle Swarm Optimization (PSO) 的变体，包括 naive PSO、Fitness Inheritance PSO (FI-PSO) 和 Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO)；二是引入几何和速度约束到 CGDA 中。通过在蜡和油漆动作的实验中验证，这两种方法显著降低了评估次数，为动态复杂环境中机器人模仿框架提供了更高效的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00678v1",
      "published_date": "2024-02-01 15:38:21 UTC",
      "updated_date": "2024-02-01 15:38:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:09:47.758795"
    },
    {
      "arxiv_id": "2402.00677v1",
      "title": "Neural Policy Style Transfer",
      "title_zh": "神经策略风格迁移",
      "authors": [
        "Raul Fernandez-Fernandez",
        "Juan G. Victores",
        "Jennifer J. Gago",
        "David Estevez",
        "Carlos Balaguer"
      ],
      "abstract": "Style Transfer has been proposed in a number of fields: fine arts, natural\nlanguage processing, and fixed trajectories. We scale this concept up to\ncontrol policies within a Deep Reinforcement Learning infrastructure. Each\nnetwork is trained to maximize the expected reward, which typically encodes the\ngoal of an action, and can be described as the content. The expressive power of\ndeep neural networks enables encoding a secondary task, which can be described\nas the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to\ntransfer the style of one policy to another, while maintaining the content of\nthe latter. Different policies are defined via Deep Q-Network architectures.\nThese models are trained using demonstrations through Inverse Reinforcement\nLearning. Two different sets of user demonstrations are performed, one for\ncontent and other for style. Different styles are encoded as defined by user\ndemonstrations. The generated policy is the result of feeding a content policy\nand a style policy to the NPST algorithm. Experiments are performed in a\ncatch-ball game inspired by the Deep Reinforcement Learning classical Atari\ngames; and a real-world painting scenario with a full-sized humanoid robot,\nbased on previous works of the authors. The implementation of three different\nQ-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode\nthe policies within the NPST framework is proposed and the results obtained in\nthe experiments with each of these architectures compared.",
      "tldr_zh": "本研究将风格转移（Style Transfer）概念扩展到深度强化学习（Deep Reinforcement Learning）领域，提出 Neural Policy Style Transfer (NPST) 算法，用于在保持策略内容（目标任务）的前提下，将一种策略的风格转移到另一种策略。算法通过 Deep Q-Network 架构和逆强化学习（Inverse Reinforcement Learning）从用户演示中训练内容和风格策略，并在实验中验证其有效性。实验包括一个受 Atari 游戏启发的 catch-ball 游戏和一个真实世界的人形机器人绘画场景，使用 Shallow、Deep 和 Deep Recurrent Q-Network 架构，结果显示 NPST 成功编码并转移不同风格，提升了策略的灵活性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00677v1",
      "published_date": "2024-02-01 15:37:42 UTC",
      "updated_date": "2024-02-01 15:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:09:59.467116"
    },
    {
      "arxiv_id": "2402.00676v1",
      "title": "Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching",
      "title_zh": "翻译失败",
      "authors": [
        "Raul Fernandez-Fernandez",
        "Juan G. Victores",
        "Carlos Balaguer"
      ],
      "abstract": "The current success of Reinforcement Learning algorithms for its performance\nin complex environments has inspired many recent theoretical approaches to\ncognitive science. Artistic environments are studied within the cognitive\nscience community as rich, natural, multi-sensory, multi-cultural environments.\nIn this work, we propose the introduction of Reinforcement Learning for\nimproving the control of artistic robot applications. Deep Q-learning Neural\nNetworks (DQN) is one of the most successful algorithms for the implementation\nof Reinforcement Learning in robotics. DQN methods generate complex control\npolicies for the execution of complex robot applications in a wide set of\nenvironments. Current art painting robot applications use simple control laws\nthat limits the adaptability of the frameworks to a set of simple environments.\nIn this work, the introduction of DQN within an art painting robot application\nis proposed. The goal is to study how the introduction of a complex control\npolicy impacts the performance of a basic art painting robot application. The\nmain expected contribution of this work is to serve as a first baseline for\nfuture works introducing DQN methods for complex art painting robot frameworks.\nExperiments consist of real world executions of human drawn sketches using the\nDQN generated policy and TEO, the humanoid robot. Results are compared in terms\nof similarity and obtained reward with respect to the reference inputs",
      "tldr_zh": "该研究提出使用 Deep Q-Learning Networks (DQN) 强化学习算法来提升艺术机器人的绘画控制能力，旨在实现更接近人类的草图绘制。论文将 DQN 应用于人形机器人 TEO 执行真实世界实验，通过复杂控制策略生成政策，并与简单控制法则进行比较。实验结果显示，DQN 方法在草图相似度和奖励方面表现出色，为未来复杂艺术绘画机器人框架提供了一个初步基线。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00676v1",
      "published_date": "2024-02-01 15:37:23 UTC",
      "updated_date": "2024-02-01 15:37:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:10:11.348986"
    },
    {
      "arxiv_id": "2402.00672v4",
      "title": "Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID",
      "title_zh": "翻译失败",
      "authors": [
        "Lingfeng He",
        "De Cheng",
        "Nannan Wang",
        "Xinbo Gao"
      ],
      "abstract": "Unsupervised visible-infrared person re-identification (USL-VI-ReID)\nendeavors to retrieve pedestrian images of the same identity from different\nmodalities without annotations. While prior work focuses on establishing\ncross-modality pseudo-label associations to bridge the modality-gap, they\nignore maintaining the instance-level homogeneous and heterogeneous consistency\nbetween the feature space and the pseudo-label space, resulting in coarse\nassociations. In response, we introduce a Modality-Unified Label Transfer\n(MULT) module that simultaneously accounts for both homogeneous and\nheterogeneous fine-grained instance-level structures, yielding high-quality\ncross-modality label associations. It models both homogeneous and heterogeneous\naffinities, leveraging them to quantify the inconsistency between the\npseudo-label space and the feature space, subsequently minimizing it. The\nproposed MULT ensures that the generated pseudo-labels maintain alignment\nacross modalities while upholding structural consistency within intra-modality.\nAdditionally, a straightforward plug-and-play Online Cross-memory Label\nRefinement (OCLR) module is proposed to further mitigate the side effects of\nnoisy pseudo-labels while simultaneously aligning different modalities, coupled\nwith an Alternative Modality-Invariant Representation Learning (AMIRL)\nframework. Experiments demonstrate that our proposed method outperforms\nexisting state-of-the-art USL-VI-ReID methods, highlighting the superiority of\nour MULT in comparison to other cross-modality association methods. Code is\navailable at https://github.com/FranklinLingfeng/code_for_MULT.",
      "tldr_zh": "该论文探讨了无监督可见-红外人重新识别（USL-VI-ReID）中跨模态伪标签关联的问题，强调了特征空间和伪标签空间的实例级同质和异质一致性。作者引入了Modality-Unified Label Transfer (MULT)模块，通过建模同质和异质亲和力来量化并最小化不一致性，从而生成高质量的跨模态标签关联。论文还提出了Online Cross-memory Label Refinement (OCLR)模块和Alternative Modality-Invariant Representation Learning (AMIRL)框架，用于减轻噪声伪标签的影响并提升模态对齐。实验结果显示，该方法在USL-VI-ReID任务上优于现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IJCV2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00672v4",
      "published_date": "2024-02-01 15:33:17 UTC",
      "updated_date": "2024-12-04 03:55:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:10:26.534917"
    },
    {
      "arxiv_id": "2402.00658v3",
      "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing",
      "title_zh": "翻译失败",
      "authors": [
        "Fangkai Jiao",
        "Chengwei Qin",
        "Zhengyuan Liu",
        "Nancy F. Chen",
        "Shafiq Joty"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling complex reasoning tasks through step-by-step rationale generation.\nHowever, recent studies have raised concerns regarding the hallucination and\nflaws in their reasoning process. Substantial efforts are being made to improve\nthe reliability and faithfulness of the generated rationales. Some approaches\nmodel reasoning as planning, while others focus on annotating for process\nsupervision. Nevertheless, the planning-based search process often results in\nhigh latency due to the frequent assessment of intermediate reasoning states\nand the extensive exploration space. Additionally, supervising the reasoning\nprocess with human annotation is costly and challenging to scale for LLM\ntraining. To address these issues, in this paper, we propose a framework to\nlearn planning-based reasoning through Direct Preference Optimization (DPO) on\ncollected trajectories, which are ranked according to synthesized process\nrewards. Our results on challenging logical reasoning benchmarks demonstrate\nthe effectiveness of our learning framework, showing that our 7B model can\nsurpass the strong counterparts like GPT-3.5-Turbo.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)在复杂推理任务中存在的幻觉和推理缺陷问题，提出了一种通过收集轨迹并合成过程奖励来学习基于规划的推理框架。方法利用Direct Preference Optimization (DPO)对这些轨迹进行优化排名，从而提高推理过程的可靠性和效率，避免了高延迟和昂贵的人工标注。实验结果显示，该框架使7B模型在逻辑推理基准上超越了GPT-3.5-Turbo等强基线，证明了其有效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 9 figures. EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00658v3",
      "published_date": "2024-02-01 15:18:33 UTC",
      "updated_date": "2024-10-15 09:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:10:36.105634"
    },
    {
      "arxiv_id": "2402.00627v3",
      "title": "CapHuman: Capture Your Moments in Parallel Universes",
      "title_zh": "CapHuman：在平行宇宙中捕捉你的时刻",
      "authors": [
        "Chao Liang",
        "Fan Ma",
        "Linchao Zhu",
        "Yingying Deng",
        "Yi Yang"
      ],
      "abstract": "We concentrate on a novel human-centric image synthesis task, that is, given\nonly one reference facial photograph, it is expected to generate specific\nindividual images with diverse head positions, poses, facial expressions, and\nilluminations in different contexts. To accomplish this goal, we argue that our\ngenerative model should be capable of the following favorable characteristics:\n(1) a strong visual and semantic understanding of our world and human society\nfor basic object and human image generation. (2) generalizable identity\npreservation ability. (3) flexible and fine-grained head control. Recently,\nlarge pre-trained text-to-image diffusion models have shown remarkable results,\nserving as a powerful generative foundation. As a basis, we aim to unleash the\nabove two capabilities of the pre-trained model. In this work, we present a new\nframework named CapHuman. We embrace the \"encode then learn to align\" paradigm,\nwhich enables generalizable identity preservation for new individuals without\ncumbersome tuning at inference. CapHuman encodes identity features and then\nlearns to align them into the latent space. Moreover, we introduce the 3D\nfacial prior to equip our model with control over the human head in a flexible\nand 3D-consistent manner. Extensive qualitative and quantitative analyses\ndemonstrate our CapHuman can produce well-identity-preserved, photo-realistic,\nand high-fidelity portraits with content-rich representations and various head\nrenditions, superior to established baselines. Code and checkpoint will be\nreleased at https://github.com/VamosC/CapHuman.",
      "tldr_zh": "这篇论文介绍了 CapHuman 框架，用于从一张参考面部照片生成多样化的个人图像，包括不同的头部位置、姿势、面部表情和光照。CapHuman 基于预训练的 text-to-image diffusion models，采用 \"encode then learn to align\" 范式，实现通用身份保留能力，而无需在推理阶段进行繁琐的微调。框架还引入 3D facial prior 来实现灵活且 3D 一致的头部控制。实验结果显示，CapHuman 在身份保留、图像真实性和保真度上均优于现有基线，并提供了高质量、多样化的图像生成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024. Project page: https://caphuman.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2402.00627v3",
      "published_date": "2024-02-01 14:41:59 UTC",
      "updated_date": "2024-05-17 14:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:10:49.233392"
    },
    {
      "arxiv_id": "2402.00607v1",
      "title": "Are Synthetic Time-series Data Really not as Good as Real Data?",
      "title_zh": "合成时间序列数据真的不如真实数据好吗？",
      "authors": [
        "Fanzhe Fu",
        "Junru Chen",
        "Jing Zhang",
        "Carl Yang",
        "Lvbin Ma",
        "Yang Yang"
      ],
      "abstract": "Time-series data presents limitations stemming from data quality issues, bias\nand vulnerabilities, and generalization problem. Integrating universal data\nsynthesis methods holds promise in improving generalization. However, current\nmethods cannot guarantee that the generator's output covers all unseen real\ndata. In this paper, we introduce InfoBoost -- a highly versatile cross-domain\ndata synthesizing framework with time series representation learning\ncapability. We have developed a method based on synthetic data that enables\nmodel training without the need for real data, surpassing the performance of\nmodels trained with real data. Additionally, we have trained a universal\nfeature extractor based on our synthetic data that is applicable to all\ntime-series data. Our approach overcomes interference from multiple sources\nrhythmic signal, noise interference, and long-period features that exceed\nsampling window capabilities. Through experiments, our non-deep-learning\nsynthetic data enables models to achieve superior reconstruction performance\nand universal explicit representation extraction without the need for real\ndata.",
      "tldr_zh": "这篇论文质疑了合成时间序列数据是否不如真实数据有效，针对数据质量问题、偏差和泛化挑战，提出了一种名为 InfoBoost 的多域数据合成框架，该框架具备时间序列 representation learning 能力。InfoBoost 允许模型在无需真实数据的情况下进行训练，并通过克服多源节律信号干扰、噪声干扰和长周期特征等问题，实现比使用真实数据更高的重建性能和通用特征提取。实验结果表明，该方法基于非深度学习的合成数据即可超越传统模型，为时间序列数据处理提供更高效的泛化方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00607v1",
      "published_date": "2024-02-01 13:59:04 UTC",
      "updated_date": "2024-02-01 13:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:11:00.646284"
    },
    {
      "arxiv_id": "2402.00918v1",
      "title": "MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation",
      "title_zh": "MUSTAN：多尺度时间上下文作为注意力机制用于鲁棒视频前景分割",
      "authors": [
        "Praveen Kumar Pokala",
        "Jaya Sai Kiran Patibandla",
        "Naveen Kumar Pandey",
        "Balakrishna Reddy Pailla"
      ],
      "abstract": "Video foreground segmentation (VFS) is an important computer vision task\nwherein one aims to segment the objects under motion from the background. Most\nof the current methods are image-based, i.e., rely only on spatial cues while\nignoring motion cues. Therefore, they tend to overfit the training data and\ndon't generalize well to out-of-domain (OOD) distribution. To solve the above\nproblem, prior works exploited several cues such as optical flow, background\nsubtraction mask, etc. However, having a video data with annotations like\noptical flow is a challenging task. In this paper, we utilize the temporal\ninformation and the spatial cues from the video data to improve OOD\nperformance. However, the challenge lies in how we model the temporal\ninformation given the video data in an interpretable way creates a very\nnoticeable difference. We therefore devise a strategy that integrates the\ntemporal context of the video in the development of VFS. Our approach give rise\nto deep learning architectures, namely MUSTAN1 and MUSTAN2 and they are based\non the idea of multi-scale temporal context as an attention, i.e., aids our\nmodels to learn better representations that are beneficial for VFS. Further, we\nintroduce a new video dataset, namely Indoor Surveillance Dataset (ISD) for\nVFS. It has multiple annotations on a frame level such as foreground binary\nmask, depth map, and instance semantic annotations. Therefore, ISD can benefit\nother computer vision tasks. We validate the efficacy of our architectures and\ncompare the performance with baselines. We demonstrate that proposed methods\nsignificantly outperform the benchmark methods on OOD. In addition, the\nperformance of MUSTAN2 is significantly improved on certain video categories on\nOOD data due to ISD.",
      "tldr_zh": "这篇论文针对 Video Foreground Segmentation (VFS) 的问题，提出 MUSTAN 方法，通过将多尺度时序上下文作为 attention 机制整合视频的时序和空间信息，解决现有图像-based 方法忽略运动线索导致的过拟合和 OOD (out-of-domain) 泛化差的问题。作者开发了两种深度学习架构：MUSTAN1 和 MUSTAN2，以帮助模型学习更有效的表示，从而提升 VFS 性能；同时，引入了一个新数据集 Indoor Surveillance Dataset (ISD)，包含前景二值掩码、深度图和实例语义注释，支持其他计算机视觉任务。实验结果显示，MUSTAN 方法在 OOD 数据上显著优于基准方法，尤其是在某些视频类别中，MUSTAN2 的性能得到显著改善。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.00918v1",
      "published_date": "2024-02-01 13:47:23 UTC",
      "updated_date": "2024-02-01 13:47:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:11:13.159779"
    },
    {
      "arxiv_id": "2402.00591v3",
      "title": "Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolas Lazzari",
        "Stefano De Giorgis",
        "Aldo Gangemi",
        "Valentina Presutti"
      ],
      "abstract": "This paper presents sandra, a neuro-symbolic reasoner combining vectorial\nrepresentations with deductive reasoning. Sandra builds a vector space\nconstrained by an ontology and performs reasoning over it. The geometric nature\nof the reasoner allows its combination with neural networks, bridging the gap\nwith symbolic knowledge representations. Sandra is based on the Description and\nSituation (DnS) ontology design pattern, a formalization of frame semantics.\nGiven a set of facts (a situation) it allows to infer all possible perspectives\n(descriptions) that can provide a plausible interpretation for it, even in\npresence of incomplete information. We prove that our method is correct with\nrespect to the DnS model. We experiment with two different tasks and their\nstandard benchmarks, demonstrating that, without increasing complexity, sandra\n(i) outperforms all the baselines (ii) provides interpretability in the\nclassification process, and (iii) allows control over the vector space, which\nis designed a priori.",
      "tldr_zh": "这篇论文介绍了 Sandra，一种基于 Descriptions and Situations (DnS) 本体设计模式的神经符号推理器(neuro-symbolic reasoner)，它结合向量表示和演绎推理来构建一个受 ontology 约束的向量空间。Sandra 能够从一组事实(situation)中推断所有可能的视角(descriptions)，即使面对不完整信息，也提供可靠的解释。实验结果显示，Sandra 在两个标准基准任务中优于所有基线模型，同时实现了分类过程的可解释性以及对向量空间的先验控制。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00591v3",
      "published_date": "2024-02-01 13:37:53 UTC",
      "updated_date": "2024-03-25 10:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:11:24.608976"
    },
    {
      "arxiv_id": "2402.00588v1",
      "title": "BrainSLAM: SLAM on Neural Population Activity Data",
      "title_zh": "翻译失败",
      "authors": [
        "Kipp Freud",
        "Nathan Lepora",
        "Matt W. Jones",
        "Cian O'Donnell"
      ],
      "abstract": "Simultaneous localisation and mapping (SLAM) algorithms are commonly used in\nrobotic systems for learning maps of novel environments. Brains also appear to\nlearn maps, but the mechanisms are not known and it is unclear how to infer\nthese maps from neural activity data. We present BrainSLAM; a method for\nperforming SLAM using only population activity (local field potential, LFP)\ndata simultaneously recorded from three brain regions in rats: hippocampus,\nprefrontal cortex, and parietal cortex. This system uses a convolutional neural\nnetwork (CNN) to decode velocity and familiarity information from wavelet\nscalograms of neural local field potential data recorded from rats as they\nnavigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture,\npowering an attractor network which performs path integration plus a separate\nsystem which performs `loop closure' (detecting previously visited locations\nand correcting map aliasing errors). Together, these three components can\nconstruct faithful representations of the environment while simultaneously\ntracking the animal's location. This is the first demonstration of inference of\na spatial map from brain recordings. Our findings expand SLAM to a new\nmodality, enabling a new method of mapping environments and facilitating a\nbetter understanding of the role of cognitive maps in navigation and decision\nmaking.",
      "tldr_zh": "本研究提出 BrainSLAM 方法，将 SLAM（Simultaneous Localisation and Mapping）算法应用于神经群体活动数据，旨在从大鼠脑部（海马体、前扣带皮层和顶叶皮层）的局部场电位（LFP）数据中推断空间地图。方法使用卷积神经网络（CNN）从 LFP 的小波标度图解码速度和熟悉度信息，并结合 RatSLAM 启发的架构，包括吸引子网络进行路径整合和一个系统处理 loop closure（检测先前位置并修正地图别名错误）。实验结果显示，BrainSLAM 能构建环境的忠实表示，同时实时跟踪动物在 2D 迷宫中的位置，这是首次从脑记录中推断空间地图。该方法扩展了 SLAM 到神经科学领域，有助于深入理解认知地图在导航和决策中的作用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the 23rd International Conference on Autonomous Agents\n  and Multiagent Systems. 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00588v1",
      "published_date": "2024-02-01 13:34:59 UTC",
      "updated_date": "2024-02-01 13:34:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:11:37.734742"
    },
    {
      "arxiv_id": "2402.03471v1",
      "title": "The Information of Large Language Model Geometry",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiquan Tan",
        "Chenghai Li",
        "Weiran Huang"
      ],
      "abstract": "This paper investigates the information encoded in the embeddings of large\nlanguage models (LLMs). We conduct simulations to analyze the representation\nentropy and discover a power law relationship with model sizes. Building upon\nthis observation, we propose a theory based on (conditional) entropy to\nelucidate the scaling law phenomenon. Furthermore, we delve into the\nauto-regressive structure of LLMs and examine the relationship between the last\ntoken and previous context tokens using information theory and regression\ntechniques. Specifically, we establish a theoretical connection between the\ninformation gain of new tokens and ridge regression. Additionally, we explore\nthe effectiveness of Lasso regression in selecting meaningful tokens, which\nsometimes outperforms the closely related attention weights. Finally, we\nconduct controlled experiments, and find that information is distributed across\ntokens, rather than being concentrated in specific \"meaningful\" tokens alone.",
      "tldr_zh": "本研究调查了大型语言模型 (LLMs) 嵌入中编码的信息，通过模拟分析发现表示熵 (representation entropy) 与模型大小遵循幂律关系，并基于熵理论解释了缩放定律 (scaling law phenomenon)。作者探讨了 LLMs 的自回归结构，利用信息理论和回归技术（如岭回归 (ridge regression)）建立了新 token 的信息增益与回归的理论联系，同时发现 Lasso 回归在选择有意义 token 方面有时优于 attention weights。控制实验结果表明，信息分布在多个 token 上，而非集中在特定“有意义”token，进一步深化了对 LLMs 几何信息的理解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.03471v1",
      "published_date": "2024-02-01 12:50:43 UTC",
      "updated_date": "2024-02-01 12:50:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:11:48.105562"
    },
    {
      "arxiv_id": "2402.00518v1",
      "title": "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xuchen Pan",
        "Yanxi Chen",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "This work introduces EE-Tuning, a lightweight and economical solution to\ntraining/tuning early-exit large language models (LLMs). In contrast to the\ncommon approach of full-parameter pre-training, EE-Tuning augments any\npre-trained (and possibly fine-tuned) standard LLM with additional early-exit\nlayers that are tuned in a parameter-efficient manner, which requires\nsignificantly less computational resources and training data. Our\nimplementation of EE-Tuning achieves outstanding training efficiency via\nextensive performance optimizations, as well as scalability due to its full\ncompatibility with 3D parallelism. Results of systematic experiments validate\nthe efficacy of EE-Tuning, confirming that effective early-exit LLM inference\ncan be achieved with a limited training budget. In hope of making early-exit\nLLMs accessible to the community, we release the source code of our\nimplementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.",
      "tldr_zh": "这项研究引入了 EE-Tuning，一种轻量级且经济实惠的方案，用于训练和微调 early-exit Large Language Models (LLMs)，通过在预训练的 LLM 上添加额外 early-exit 层并以 parameter-efficient 的方式进行微调，显著减少计算资源和训练数据需求。EE-Tuning 通过广泛的性能优化实现了高训练效率，并完全兼容 3D parallelism，从而确保可扩展性。实验结果证实了其有效性，证明在有限的训练预算下即可实现高效的 early-exit LLM 推理，并开源了代码以促进社区应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00518v1",
      "published_date": "2024-02-01 11:39:04 UTC",
      "updated_date": "2024-02-01 11:39:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:11:59.829602"
    },
    {
      "arxiv_id": "2402.00913v3",
      "title": "Institutional Platform for Secure Self-Service Large Language Model Exploration",
      "title_zh": "翻译失败",
      "authors": [
        "V. K. Cody Bumgardner",
        "Mitchell A. Klusty",
        "W. Vaiden Logan",
        "Samuel E. Armstrong",
        "Caroline N. Leach",
        "Kenneth L. Calvert",
        "Caylin Hickey",
        "Jeff Talbert"
      ],
      "abstract": "This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery.",
      "tldr_zh": "这篇论文介绍了由 University of Kentucky Center for Applied AI 开发的平台，旨在简化对大型语言模型 (LLMs) 的安全自助探索。平台利用 multi-LoRA 推理技术，高效支持自定义适配器，并涵盖数据集整理、模型训练、安全推理和文本特征提取等关键功能。通过代理方法构建租户感知计算网络，该系统强调过程和数据隔离、端到端加密以及基于角色的资源认证，最终提升了用户对尖端 AI 模型的访问便利性，支持科学发现。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2402.00913v3",
      "published_date": "2024-02-01 10:58:10 UTC",
      "updated_date": "2025-02-24 14:50:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:12:11.956103"
    },
    {
      "arxiv_id": "2402.00491v1",
      "title": "EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data Configurations",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Bhattacharya",
        "Simone Stumpf",
        "Lucija Gosak",
        "Gregor Stiglic",
        "Katrien Verbert"
      ],
      "abstract": "Explanations in interactive machine-learning systems facilitate debugging and\nimproving prediction models. However, the effectiveness of various global\nmodel-centric and data-centric explanations in aiding domain experts to detect\nand resolve potential data issues for model improvement remains unexplored.\nThis research investigates the influence of data-centric and model-centric\nglobal explanations in systems that support healthcare experts in optimising\nmodels through automated and manual data configurations. We conducted\nquantitative (n=70) and qualitative (n=30) studies with healthcare experts to\nexplore the impact of different explanations on trust, understandability and\nmodel improvement. Our results reveal the insufficiency of global model-centric\nexplanations for guiding users during data configuration. Although data-centric\nexplanations enhanced understanding of post-configuration system changes, a\nhybrid fusion of both explanation types demonstrated the highest effectiveness.\nBased on our study results, we also present design implications for effective\nexplanation-driven interactive machine-learning systems.",
      "tldr_zh": "这篇论文研究了在交互式机器学习系统中，data-centric explanations 和 model-centric explanations 如何帮助医疗专家检测并解决数据问题，以优化预测模型。研究通过定量（n=70）和定性（n=30）实验发现，单纯的 global model-centric explanations 不足以指导数据配置，而 data-centric explanations 能提升对系统变化的理解，混合解释类型则表现出最高效能。最终，论文基于这些结果提出设计含义，以改进 explanation-driven interactive machine-learning systems。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "This is a pre-print version only for early release. Please view the\n  conference published version from ACM CHI 2024 to get the latest version of\n  the paper",
      "pdf_url": "http://arxiv.org/pdf/2402.00491v1",
      "published_date": "2024-02-01 10:57:00 UTC",
      "updated_date": "2024-02-01 10:57:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:12:24.092902"
    },
    {
      "arxiv_id": "2402.01772v1",
      "title": "Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Meng",
        "Christof Monz"
      ],
      "abstract": "Multilingual Machine Translation (MMT) benefits from knowledge transfer\nacross different language pairs. However, improvements in one-to-many\ntranslation compared to many-to-one translation are only marginal and sometimes\neven negligible. This performance discrepancy raises the question of to what\nextent positive transfer plays a role on the target-side for one-to-many MT. In\nthis paper, we conduct a large-scale study that varies the auxiliary target\nside languages along two dimensions, i.e., linguistic similarity and corpus\nsize, to show the dynamic impact of knowledge transfer on the main language\npairs. We show that linguistically similar auxiliary target languages exhibit\nstrong ability to transfer positive knowledge. With an increasing size of\nsimilar target languages, the positive transfer is further enhanced to benefit\nthe main language pairs. Meanwhile, we find distant auxiliary target languages\ncan also unexpectedly benefit main language pairs, even with minimal positive\ntransfer ability. Apart from transfer, we show distant auxiliary target\nlanguages can act as a regularizer to benefit translation performance by\nenhancing the generalization and model inference calibration.",
      "tldr_zh": "本文研究了在Multilingual Machine Translation (MMT)中，目标侧知识转移和正则化的作用，通过大规模实验变异辅助目标语言的语言相似性和语料库大小。结果显示，语言相似的辅助目标语言能显著增强积极知识转移，且增加其数量进一步提升主要语言对的翻译性能。同时，即使是语言相距遥远的辅助目标语言，也能意外地带来益处，主要通过作为正则化器来提高模型的泛化和推理校准。这些发现有助于更好地理解和优化MMT系统的知识转移机制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01772v1",
      "published_date": "2024-02-01 10:55:03 UTC",
      "updated_date": "2024-02-01 10:55:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:12:37.027940"
    },
    {
      "arxiv_id": "2402.00485v1",
      "title": "A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Hossein A. Rahmani",
        "Mohammadmehdi Naghiaei",
        "Yashar Deldjoo"
      ],
      "abstract": "In recent years, there has been an increasing recognition that when machine\nlearning (ML) algorithms are used to automate decisions, they may mistreat\nindividuals or groups, with legal, ethical, or economic implications.\nRecommender systems are prominent examples of these machine learning (ML)\nsystems that aid users in making decisions. The majority of past literature\nresearch on RS fairness treats user and item fairness concerns independently,\nignoring the fact that recommender systems function in a two-sided marketplace.\nIn this paper, we propose CP-FairRank, an optimization-based re-ranking\nalgorithm that seamlessly integrates fairness constraints from both the\nconsumer and producer side in a joint objective framework. The framework is\ngeneralizable and may take into account varied fairness settings based on group\nsegmentation, recommendation model selection, and domain, which is one of its\nkey characteristics. For instance, we demonstrate that the system may jointly\nincrease consumer and producer fairness when (un)protected consumer groups are\ndefined on the basis of their activity level and main-streamness, while\nproducer groups are defined according to their popularity level. For empirical\nvalidation, through large-scale on eight datasets and four mainstream\ncollaborative filtering (CF) recommendation models, we demonstrate that our\nproposed strategy is able to improve both consumer and producer fairness\nwithout compromising or very little overall recommendation quality,\ndemonstrating the role algorithms may play in avoiding data biases.",
      "tldr_zh": "这篇论文提出 CP-FairRank，一种基于优化的重新排序算法，用于在 Recommender Systems 中同时优化消费者和生产者群组的公平性，解决传统方法忽略双边市场特性的问题。该框架可泛化地整合公平约束，根据群组划分（如消费者基于活跃度和主流程度、生产者基于流行度）、推荐模型和领域进行调整。通过在八个数据集和四种主流 Collaborative Filtering (CF) 模型上的大规模实验，证明该方法能显著提升公平性，同时几乎不影响整体推荐质量。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "TORS. arXiv admin note: substantial text overlap with\n  arXiv:2204.08085",
      "pdf_url": "http://arxiv.org/pdf/2402.00485v1",
      "published_date": "2024-02-01 10:42:05 UTC",
      "updated_date": "2024-02-01 10:42:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:12:48.853928"
    },
    {
      "arxiv_id": "2402.00474v1",
      "title": "SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models",
      "title_zh": "SA-MDKIF：一种可扩展且可适应的医学领域知识注入框架，用于大",
      "authors": [
        "Tianhan Xu",
        "Zhe Hu",
        "Ling Chen",
        "Bin Li"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated exceptional\nperformance in various natural language processing (NLP) tasks. However, their\neffective application in the medical domain is hampered by a lack of medical\ndomain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable\nframework that aims to inject medical knowledge into general-purpose LLMs\nthrough instruction tuning, thereby enabling adaptability for various\ndownstream tasks. SA-MDKIF consists of two stages: skill training and skill\nadaptation. In the first stage, we define 12 basic medical skills and use\nAdaLoRA to train these skills based on uniformly formatted instructional\ndatasets that we have constructed. In the next stage, we train the skill router\nusing task-specific downstream data and use this router to integrate the\nacquired skills with LLMs during inference. Experimental results on 9 different\nmedical tasks show that SA-MDKIF improves performance by 10-20% compared to the\noriginal LLMs. Notably, this improvement is particularly pronounced for unseen\nmedical tasks, showing an improvement of up to 30%.",
      "tldr_zh": "本研究提出 SA-MDKIF，一种可扩展且可适应的框架，用于通过 instruction tuning 向大型语言模型 (LLMs) 注入医疗领域知识，从而提升其在各种下游医疗任务中的表现。框架分为两个阶段：首先，利用 AdaLoRA 训练 12 个基本医疗技能，基于构建的统一格式指令数据集；其次，通过任务特定的下游数据训练 skill router，并在推理过程中整合这些技能。实验结果显示，在 9 个不同医疗任务上，SA-MDKIF 比原 LLMs 提高了 10-20% 的性能，尤其在未见医疗任务上，性能提升可达 30%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00474v1",
      "published_date": "2024-02-01 10:26:27 UTC",
      "updated_date": "2024-02-01 10:26:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:12:59.761545"
    },
    {
      "arxiv_id": "2402.00912v2",
      "title": "Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?",
      "title_zh": "我们能否约束概念瓶颈模型以学习语义有意义的输入特征？",
      "authors": [
        "Jack Furby",
        "Daniel Cunnington",
        "Dave Braines",
        "Alun Preece"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) are regarded as inherently interpretable\nbecause they first predict a set of human-defined concepts which are used to\npredict a task label. For inherent interpretability to be fully realised, and\nensure trust in a model's output, it's desirable for concept predictions to use\nsemantically meaningful input features. For instance, in an image, pixels\nrepresenting a broken bone should contribute to predicting a fracture. However,\ncurrent literature suggests that concept predictions often rely on irrelevant\ninput features. We hypothesise that this occurs when dataset labels include\ninaccurate concept annotations, or the relationship between input features and\nconcepts is unclear. In general, the effect of dataset labelling on concept\nrepresentations remains an understudied area. In this paper, we demonstrate\nthat CBMs can learn to map concepts to semantically meaningful input features,\nby utilising datasets with a clear link between the input features and the\ndesired concept predictions. This is achieved, for instance, by ensuring\nmultiple concepts do not always co-occur and, therefore provide a clear\ntraining signal for the CBM to distinguish the relevant input features for each\nconcept. We validate our hypothesis on both synthetic and real-world image\ndatasets, and demonstrate under the correct conditions, CBMs can learn to\nattribute semantically meaningful input features to the correct concept\npredictions.",
      "tldr_zh": "本论文探讨了是否能约束 Concept Bottleneck Models (CBMs) 来学习语义上意义明确的功能，而不是依赖无关输入特征。作者假设，这种问题源于数据集标签不准确或特征与概念关系不清晰，因此提出通过使用有明确链接的数据集（如避免多个概念总是共现）来提供清晰的训练信号，帮助 CBMs 区分相关功能。实验在合成和真实图像数据集上验证了这一方法，结果显示在正确条件下，CBMs 能够准确地将语义上意义明确的功能归因于相应的概念预测，从而提升模型的可解释性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Main paper: 8 pages, 9 figures, Appendix: 14 pages, 21 figures. This\n  paper is a preprint",
      "pdf_url": "http://arxiv.org/pdf/2402.00912v2",
      "published_date": "2024-02-01 10:18:43 UTC",
      "updated_date": "2024-07-30 09:49:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:13:14.223734"
    },
    {
      "arxiv_id": "2402.00468v1",
      "title": "RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum Radiation Exposure Pathway",
      "title_zh": "RadDQN：一种基于深度Q学习的架构，用于寻找时间高效的最小辐射暴露路径",
      "authors": [
        "Biswajit Sadhu",
        "Trijit Sadhu",
        "S. Anand"
      ],
      "abstract": "Recent advancements in deep reinforcement learning (DRL) techniques have\nsparked its multifaceted applications in the automation sector. Managing\ncomplex decision-making problems with DRL encourages its use in the nuclear\nindustry for tasks such as optimizing radiation exposure to the personnel\nduring normal operating conditions and potential accidental scenarios. However,\nthe lack of efficient reward function and effective exploration strategy\nthwarted its implementation in the development of radiation-aware autonomous\nunmanned aerial vehicle (UAV) for achieving maximum radiation protection. Here,\nin this article, we address these intriguing issues and introduce a deep\nQ-learning based architecture (RadDQN) that operates on a radiation-aware\nreward function to provide time-efficient minimum radiation-exposure pathway in\na radiation zone. We propose a set of unique exploration strategies that\nfine-tune the extent of exploration and exploitation based on the state-wise\nvariation in radiation exposure during training. Further, we benchmark the\npredicted path with grid-based deterministic method. We demonstrate that the\nformulated reward function in conjugation with adequate exploration strategy is\neffective in handling several scenarios with drastically different radiation\nfield distributions. When compared to vanilla DQN, our model achieves a\nsuperior convergence rate and higher training stability.",
      "tldr_zh": "这篇论文引入了 RadDQN，一种基于 Deep Q Learning 的架构，旨在为无人驾驶飞机 (UAV) 在辐射区域找到时间高效的最低辐射暴露路径。研究者解决了深度强化学习 (DRL) 在核工业应用中的问题，通过设计一个辐射感知的奖励函数和状态相关的探索策略来优化探索与利用的平衡。实验结果表明，RadDQN 比 vanilla DQN 实现了更快的收敛率和更高的训练稳定性，并在与网格-based 确定性方法的比较中表现出色，尤其适用于不同辐射场分布的场景。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 7 main figures, code link (GitHub)",
      "pdf_url": "http://arxiv.org/pdf/2402.00468v1",
      "published_date": "2024-02-01 10:15:39 UTC",
      "updated_date": "2024-02-01 10:15:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:13:24.372573"
    },
    {
      "arxiv_id": "2402.00459v1",
      "title": "Genetic-based Constraint Programming for Resource Constrained Job Scheduling",
      "title_zh": "翻译失败",
      "authors": [
        "Su Nguyen",
        "Dhananjay Thiruvady",
        "Yuan Sun",
        "Mengjie Zhang"
      ],
      "abstract": "Resource constrained job scheduling is a hard combinatorial optimisation\nproblem that originates in the mining industry. Off-the-shelf solvers cannot\nsolve this problem satisfactorily in reasonable timeframes, while other\nsolution methods such as many evolutionary computation methods and\nmatheuristics cannot guarantee optimality and require low-level customisation\nand specialised heuristics to be effective. This paper addresses this gap by\nproposing a genetic programming algorithm to discover efficient search\nstrategies of constraint programming for resource-constrained job scheduling.\nIn the proposed algorithm, evolved programs represent variable selectors to be\nused in the search process of constraint programming, and their fitness is\ndetermined by the quality of solutions obtained for training instances. The\nnovelties of this algorithm are (1) a new representation of variable selectors,\n(2) a new fitness evaluation scheme, and (3) a pre-selection mechanism. Tests\nwith a large set of random and benchmark instances, the evolved variable\nselectors can significantly improve the efficiency of constraining programming.\nCompared to highly customised metaheuristics and hybrid algorithms, evolved\nvariable selectors can help constraint programming identify quality solutions\nfaster and proving optimality is possible if sufficiently large run-times are\nallowed. The evolved variable selectors are especially helpful when solving\ninstances with large numbers of machines.",
      "tldr_zh": "该论文针对 Resource Constrained Job Scheduling 的组合优化问题，提出了一种基于 Genetic Programming 的算法，用于自动发现约束编程（Constraint Programming）的有效搜索策略。该算法通过演化程序作为变量选择器，并引入新的表示方式、适应度评估方案和预选择机制，来优化求解过程。实验结果显示，与现有元启发式和混合算法相比，该方法显著提高了约束编程的效率，能更快地找到高质量解决方案，并在机器数量大的实例中特别突出，尤其是在允许足够运行时间时能够证明最优解。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00459v1",
      "published_date": "2024-02-01 09:57:38 UTC",
      "updated_date": "2024-02-01 09:57:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:13:36.787816"
    },
    {
      "arxiv_id": "2402.00448v1",
      "title": "Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection",
      "title_zh": "双学生知识蒸馏网络用于无监督异常检测",
      "authors": [
        "Liyi Yao",
        "Shaobing Gao"
      ],
      "abstract": "Due to the data imbalance and the diversity of defects, student-teacher\nnetworks (S-T) are favored in unsupervised anomaly detection, which explores\nthe discrepancy in feature representation derived from the knowledge\ndistillation process to recognize anomalies. However, vanilla S-T network is\nnot stable. Employing identical structures to construct the S-T network may\nweaken the representative discrepancy on anomalies. But using different\nstructures can increase the likelihood of divergent performance on normal data.\nTo address this problem, we propose a novel dual-student knowledge distillation\n(DSKD) architecture. Different from other S-T networks, we use two student\nnetworks a single pre-trained teacher network, where the students have the same\nscale but inverted structures. This framework can enhance the distillation\neffect to improve the consistency in recognition of normal data, and\nsimultaneously introduce diversity for anomaly representation. To explore\nhigh-dimensional semantic information to capture anomaly clues, we employ two\nstrategies. First, a pyramid matching mode is used to perform knowledge\ndistillation on multi-scale feature maps in the intermediate layers of\nnetworks. Second, an interaction is facilitated between the two student\nnetworks through a deep feature embedding module, which is inspired by\nreal-world group discussions. In terms of classification, we obtain pixel-wise\nanomaly segmentation maps by measuring the discrepancy between the output\nfeature maps of the teacher and student networks, from which an anomaly score\nis computed for sample-wise determination. We evaluate DSKD on three benchmark\ndatasets and probe the effects of internal modules through ablation\nexperiments. The results demonstrate that DSKD can achieve exceptional\nperformance on small models like ResNet18 and effectively improve vanilla S-T\nnetworks.",
      "tldr_zh": "该论文提出了一种名为Dual-Student Knowledge Distillation (DSKD)的框架，用于无监督异常检测，以解决传统学生-教师网络(S-T)的不稳定性问题，该框架使用两个规模相同但结构相反的学生网络和一个预训练教师网络，从而增强正常数据识别的一致性并增加异常表示的多样性。DSKD引入了金字塔匹配模式在多尺度特征图上进行知识蒸馏，以及一个深度特征嵌入模块来促进两个学生网络之间的互动，模拟群体讨论以捕获高维语义信息。实验结果显示，该方法在三个基准数据集上表现出色，尤其在小模型如ResNet18上显著提升了S-T网络的性能，并通过消融实验验证了内部模块的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00448v1",
      "published_date": "2024-02-01 09:32:39 UTC",
      "updated_date": "2024-02-01 09:32:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:13:49.496245"
    },
    {
      "arxiv_id": "2402.00447v4",
      "title": "A Survey of Data-Efficient Graph Learning",
      "title_zh": "数据高效图学习的综述",
      "authors": [
        "Wei Ju",
        "Siyu Yi",
        "Yifan Wang",
        "Qingqing Long",
        "Junyu Luo",
        "Zhiping Xiao",
        "Ming Zhang"
      ],
      "abstract": "Graph-structured data, prevalent in domains ranging from social networks to\nbiochemical analysis, serve as the foundation for diverse real-world systems.\nWhile graph neural networks demonstrate proficiency in modeling this type of\ndata, their success is often reliant on significant amounts of labeled data,\nposing a challenge in practical scenarios with limited annotation resources. To\ntackle this problem, tremendous efforts have been devoted to enhancing graph\nmachine learning performance under low-resource settings by exploring various\napproaches to minimal supervision. In this paper, we introduce a novel concept\nof Data-Efficient Graph Learning (DEGL) as a research frontier, and present the\nfirst survey that summarizes the current progress of DEGL. We initiate by\nhighlighting the challenges inherent in training models with large labeled\ndata, paving the way for our exploration into DEGL. Next, we systematically\nreview recent advances on this topic from several key aspects, including\nself-supervised graph learning, semi-supervised graph learning, and few-shot\ngraph learning. Also, we state promising directions for future research,\ncontributing to the evolution of graph machine learning.",
      "tldr_zh": "这篇调查论文探讨了 Data-Efficient Graph Learning (DEGL)，旨在解决图神经网络(Graph Neural Networks)依赖大量标记数据的问题，尤其在资源有限的实际场景中。论文首次系统总结了 DEGL 的进展，包括自监督图学习(self-supervised graph learning)、半监督图学习(semi-supervised graph learning)和少样本图学习(few-shot graph learning)等关键方面。作者强调了这些方法的潜力，并提出了未来研究方向，以推动图机器学习在低资源环境下的发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Proceedings of the Thirty-Third International Joint\n  Conference on Artificial Intelligence (IJCAI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.00447v4",
      "published_date": "2024-02-01 09:28:48 UTC",
      "updated_date": "2024-06-19 14:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:13:59.837035"
    },
    {
      "arxiv_id": "2402.00910v2",
      "title": "Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning",
      "title_zh": "通过集成学习和正则化微调解决偏差",
      "authors": [
        "Ahmed Radwan",
        "Layan Zaafarani",
        "Jetana Abudawood",
        "Faisal AlZahrani",
        "Fares Fourati"
      ],
      "abstract": "Addressing biases in AI models is crucial for ensuring fair and accurate\npredictions. However, obtaining large, unbiased datasets for training can be\nchallenging. This paper proposes a comprehensive approach using multiple\nmethods to remove bias in AI models, with only a small dataset and a\npotentially biased pretrained model. We train multiple models with the\ncounter-bias of the pre-trained model through data splitting, local training,\nand regularized fine-tuning, gaining potentially counter-biased models. Then,\nwe employ ensemble learning for all models to reach unbiased predictions. To\nfurther accelerate the inference time of our ensemble model, we conclude our\nsolution with knowledge distillation that results in a single unbiased neural\nnetwork. We demonstrate the effectiveness of our approach through experiments\non the CIFAR10 and HAM10000 datasets, showcasing promising results. This work\ncontributes to the ongoing effort to create more unbiased and reliable AI\nmodels, even with limited data availability.",
      "tldr_zh": "这篇论文提出了一种综合方法，通过集成学习（ensemble learning）和正则化微调（regularized fine-tuning）来解决AI模型中的偏置问题，即使仅使用小数据集和可能有偏的预训练模型。方法包括通过数据分割、局部训练和正则化微调训练多个反偏模型，然后利用ensemble learning结合这些模型实现无偏预测，并通过knowledge distillation加速推理以生成单一高效的无偏神经网络。在CIFAR10和HAM10000数据集上的实验证明了该方法的有效性，为在数据有限的情况下创建更可靠的AI模型做出了贡献。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00910v2",
      "published_date": "2024-02-01 09:24:36 UTC",
      "updated_date": "2024-02-13 18:54:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:14:12.806831"
    },
    {
      "arxiv_id": "2402.00435v2",
      "title": "A practical existence theorem for reduced order models based on convolutional autoencoders",
      "title_zh": "基于卷积自编码器的缩减阶模型的实用存在定理",
      "authors": [
        "Nicola Rares Franco",
        "Simone Brugiapaglia"
      ],
      "abstract": "In recent years, deep learning has gained increasing popularity in the fields\nof Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM),\nproviding domain practitioners with new powerful data-driven techniques such as\nPhysics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator\nNetworks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context,\ndeep autoencoders based on Convolutional Neural Networks (CNNs) have proven\nextremely effective, outperforming established techniques, such as the reduced\nbasis method, when dealing with complex nonlinear problems. However, despite\nthe empirical success of CNN-based autoencoders, there are only a few\ntheoretical results supporting these architectures, usually stated in the form\nof universal approximation theorems. In particular, although the existing\nliterature provides users with guidelines for designing convolutional\nautoencoders, the subsequent challenge of learning the latent features has been\nbarely investigated. Furthermore, many practical questions remain unanswered,\ne.g., the number of snapshots needed for convergence or the neural network\ntraining strategy. In this work, using recent techniques from sparse\nhigh-dimensional function approximation, we fill some of these gaps by\nproviding a new practical existence theorem for CNN-based autoencoders when the\nparameter-to-solution map is holomorphic. This regularity assumption arises in\nmany relevant classes of parametric PDEs, such as the parametric diffusion\nequation, for which we discuss an explicit application of our general theory.",
      "tldr_zh": "本论文探讨了深度学习在偏微分方程 (PDEs) 和简化建模 (Reduced Order Modeling, ROM) 中的应用，特别关注基于 Convolutional Neural Networks (CNNs) 的自动编码器，这些模型在处理复杂非线性问题时优于传统方法如简化基方法。论文填补了现有理论空白，通过利用稀疏高维函数逼近技术，提出一个新的实用存在定理，适用于参数到解映射为全纯 (holomorphic) 的场景，例如参数扩散方程。实验和理论分析回答了实际问题，如所需快照数量和神经网络训练策略，为 CNN-based autoencoders 的设计提供指导。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00435v2",
      "published_date": "2024-02-01 09:01:58 UTC",
      "updated_date": "2024-06-24 15:42:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:14:26.531561"
    },
    {
      "arxiv_id": "2402.00414v1",
      "title": "Prompt-Time Symbolic Knowledge Capture with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Tolga Çöplü",
        "Arto Bendiken",
        "Andrii Skomorokhov",
        "Eduard Bateiko",
        "Stephen Cobb",
        "Joshua J. Bouw"
      ],
      "abstract": "Augmenting large language models (LLMs) with user-specific knowledge is\ncrucial for real-world applications, such as personal AI assistants. However,\nLLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper\ninvestigates utilizing the existing LLM capabilities to enable prompt-driven\nknowledge capture, with a particular emphasis on knowledge graphs. We address\nthis challenge by focusing on prompt-to-triple (P2T) generation. We explore\nthree methods: zero-shot prompting, few-shot prompting, and fine-tuning, and\nthen assess their performance via a specialized synthetic dataset. Our code and\ndatasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.",
      "tldr_zh": "这篇论文探讨了如何为 Large Language Models (LLMs) 注入用户特定知识，以支持现实应用如个人 AI 助手，因为 LLMs 原本缺乏提示驱动的知识捕获机制。论文重点关注提示到三元组 (P2T) 生成，探索了 zero-shot prompting、few-shot prompting 和 fine-tuning 三种方法，并使用一个专门的合成数据集进行评估。结果显示这些方法能有效提升知识捕获能力，代码和数据集已公开在 GitHub 上供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 5 figures, 1 table preprint. Under review",
      "pdf_url": "http://arxiv.org/pdf/2402.00414v1",
      "published_date": "2024-02-01 08:15:28 UTC",
      "updated_date": "2024-02-01 08:15:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:14:37.346113"
    },
    {
      "arxiv_id": "2402.00412v1",
      "title": "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Xinlin Peng",
        "Ying Zhou",
        "Ben He",
        "Le Sun",
        "Yingfei Sun"
      ],
      "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in text\ngeneration tasks. However, the utilization of these models carries inherent\nrisks, including but not limited to plagiarism, the dissemination of fake news,\nand issues in educational exercises. Although several detectors have been\nproposed to address these concerns, their effectiveness against adversarial\nperturbations, specifically in the context of student essay writing, remains\nlargely unexplored. This paper aims to bridge this gap by constructing\nAIG-ASAP, an AI-generated student essay dataset, employing a range of text\nperturbation methods that are expected to generate high-quality essays while\nevading detection. Through empirical experiments, we assess the performance of\ncurrent AIGC detectors on the AIG-ASAP dataset. The results reveal that the\nexisting detectors can be easily circumvented using straightforward automatic\nadversarial attacks. Specifically, we explore word substitution and sentence\nsubstitution perturbation methods that effectively evade detection while\nmaintaining the quality of the generated essays. This highlights the urgent\nneed for more accurate and robust methods to detect AI-generated student essays\nin the education domain.",
      "tldr_zh": "该研究评估了现有AI生成内容(AIGC)检测器的鲁棒性，针对Large Language Models (LLMs)生成的学生作文可能带来的风险，如剽窃和教育诚信问题。作者构建了AIG-ASAP数据集，通过word substitution和sentence substitution等文本扰动方法生成高质量作文，以模拟对抗性攻击。实验结果显示，现有的检测器容易被这些简单自动攻击规避，准确率显著下降，突显了在教育领域开发更准确、鲁棒的AI生成作文检测方法迫在眉睫。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2023 Main conference, Oral Presentation",
      "pdf_url": "http://arxiv.org/pdf/2402.00412v1",
      "published_date": "2024-02-01 08:11:56 UTC",
      "updated_date": "2024-02-01 08:11:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:14:49.500694"
    },
    {
      "arxiv_id": "2402.00411v2",
      "title": "LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model",
      "title_zh": "翻译失败",
      "authors": [
        "Zecheng Hao",
        "Xinyu Shi",
        "Yujia Liu",
        "Zhaofei Yu",
        "Tiejun Huang"
      ],
      "abstract": "Compared to traditional Artificial Neural Network (ANN), Spiking Neural\nNetwork (SNN) has garnered widespread academic interest for its intrinsic\nability to transmit information in a more energy-efficient manner. However,\ndespite previous efforts to optimize the learning algorithm of SNNs through\nvarious methods, SNNs still lag behind ANNs in terms of performance. The\nrecently proposed multi-threshold model provides more possibilities for further\nenhancing the learning capability of SNNs. In this paper, we rigorously analyze\nthe relationship among the multi-threshold model, vanilla spiking model and\nquantized ANNs from a mathematical perspective, then propose a novel LM-HT\nmodel, which is an equidistant multi-threshold model that can dynamically\nregulate the global input current and membrane potential leakage on the time\ndimension. The LM-HT model can also be transformed into a vanilla single\nthreshold model through reparameterization, thereby achieving more flexible\nhardware deployment. In addition, we note that the LM-HT model can seamlessly\nintegrate with ANN-SNN Conversion framework under special initialization. This\nnovel hybrid learning framework can effectively improve the relatively poor\nperformance of converted SNNs under low time latency. Extensive experimental\nresults have demonstrated that our model can outperform previous\nstate-of-the-art works on various types of datasets, which promote SNNs to\nachieve a brand-new level of performance comparable to quantized ANNs. Code is\navailable at https://github.com/hzc1208/LMHT_SNN.",
      "tldr_zh": "本研究针对Spiking Neural Network (SNN)性能落后于Artificial Neural Network (ANN)的难题，提出了一种可学习的LM-HT模型，该模型基于等距多阈值机制，能够动态调节全局输入电流和膜电位泄漏，从而提升SNN的学习能力。LM-HT模型通过数学分析与量化ANN的关联，并支持重新参数化转换为单阈值模型，实现更灵活的硬件部署，同时可无缝整合ANN-SNN转换框架以改善低时延下的SNN性能。实验结果显示，LM-HT在多种数据集上超越现有最先进方法，使SNN的性能达到与量化ANN相当的水平。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00411v2",
      "published_date": "2024-02-01 08:10:39 UTC",
      "updated_date": "2024-10-09 02:56:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:15:01.057544"
    },
    {
      "arxiv_id": "2402.00402v1",
      "title": "Investigating Bias Representations in Llama 2 Chat via Activation Steering",
      "title_zh": "通过激活转向调查 Llama 2 Chat 中的偏见表征",
      "authors": [
        "Dawn Lu",
        "Nina Rimsky"
      ],
      "abstract": "We address the challenge of societal bias in Large Language Models (LLMs),\nfocusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into\ndecision-making processes with substantial societal impact, it becomes\nimperative to ensure these models do not reinforce existing biases. Our\napproach employs activation steering to probe for and mitigate biases related\nto gender, race, and religion. This method manipulates model activations to\ndirect responses towards or away from biased outputs, utilizing steering\nvectors derived from the StereoSet dataset and custom GPT4 generated gender\nbias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat,\npersisting even after Reinforcement Learning from Human Feedback (RLHF). We\nalso observe a predictable negative correlation between bias and the model's\ntendency to refuse responses. Significantly, our study uncovers that RLHF tends\nto increase the similarity in the model's representation of different forms of\nsocietal biases, which raises questions about the model's nuanced understanding\nof different forms of bias. This work also provides valuable insights into\neffective red-teaming strategies for LLMs using activation steering,\nparticularly emphasizing the importance of integrating a refusal vector.",
      "tldr_zh": "本研究针对 Llama 2 7B Chat 模型中的社会偏见，使用 activation steering 方法来探测和缓解与性别、种族及宗教相关的偏见，通过操纵模型激活和利用 StereoSet 数据集及自定义 GPT4 提示派生的 steering vectors。结果显示，该模型存在固有的性别偏见，即使经过 Reinforcement Learning from Human Feedback (RLHF) 后仍未完全消除，且 RLHF 增加了不同偏见表示的相似性，这质疑了模型对偏见的细微理解。研究还观察到偏见与模型拒绝响应的负相关关系，并为 red-teaming 策略提供了见解，强调整合 refusal vector 的重要性，以提升 LLMs 的可信度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00402v1",
      "published_date": "2024-02-01 07:48:50 UTC",
      "updated_date": "2024-02-01 07:48:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:15:16.177271"
    },
    {
      "arxiv_id": "2402.00397v2",
      "title": "Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting",
      "title_zh": "多尺度交通模式库用于跨城市少样本交通预测",
      "authors": [
        "Zhanyu Liu",
        "Guanjie Zheng",
        "Yanwei Yu"
      ],
      "abstract": "Traffic forecasting is crucial for intelligent transportation systems (ITS),\naiding in efficient resource allocation and effective traffic control. However,\nits effectiveness often relies heavily on abundant traffic data, while many\ncities lack sufficient data due to limited device support, posing a significant\nchallenge for traffic forecasting. Recognizing this challenge, we have made a\nnoteworthy observation: traffic patterns exhibit similarities across diverse\ncities. Building on this key insight, we propose a solution for the cross-city\nfew-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank\n(MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich\nsource cities, effectively acquiring comprehensive traffic knowledge through a\nspatial-temporal-aware pre-training process. Subsequently, the framework\nemploys advanced clustering techniques to systematically generate a multi-scale\ntraffic pattern bank derived from the learned knowledge. Next, the traffic data\nof the data-scarce target city could query the traffic pattern bank,\nfacilitating the aggregation of meta-knowledge. This meta-knowledge, in turn,\nassumes a pivotal role as a robust guide in subsequent processes involving\ngraph reconstruction and forecasting. Empirical assessments conducted on\nreal-world traffic datasets affirm the superior performance of MTPB, surpassing\nexisting methods across various categories and exhibiting numerous attributes\nconducive to the advancement of cross-city few-shot forecasting methodologies.\nThe code is available in https://github.com/zhyliu00/MTPB.",
      "tldr_zh": "交通预测对智能交通系统（ITS）至关重要，但许多城市因数据不足而面临挑战；研究观察到不同城市间交通模式存在相似性，并据此提出 Multi-scale Traffic Pattern Bank (MTPB) 方法，用于 cross-city few-shot traffic forecasting。MTPB 先利用数据丰富的源城市数据进行 spatial-temporal-aware pre-training，生成多尺度交通模式库，然后通过聚类技术让目标城市数据查询并聚合元知识，以指导图重建和预测过程。实验结果显示，MTPB 在真实交通数据集上超越现有方法，推动了跨城市少样本预测技术的进步。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review. Text overlap with arXiv:2308.09727",
      "pdf_url": "http://arxiv.org/pdf/2402.00397v2",
      "published_date": "2024-02-01 07:33:31 UTC",
      "updated_date": "2024-02-26 12:55:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:15:25.521877"
    },
    {
      "arxiv_id": "2402.00396v2",
      "title": "Efficient Exploration for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Vikranth Dwaracherla",
        "Seyed Mohammad Asghari",
        "Botao Hao",
        "Benjamin Van Roy"
      ],
      "abstract": "We present evidence of substantial benefit from efficient exploration in\ngathering human feedback to improve large language models. In our experiments,\nan agent sequentially generates queries while fitting a reward model to the\nfeedback received. Our best-performing agent generates queries using double\nThompson sampling, with uncertainty represented by an epistemic neural network.\nOur results demonstrate that efficient exploration enables high levels of\nperformance with far fewer queries. Further, both uncertainty estimation and\nthe choice of exploration scheme play critical roles.",
      "tldr_zh": "该研究证明了在收集人类反馈以改进大型语言模型（LLMs）时，通过高效探索策略可获得显著收益。研究中，代理依次生成查询，同时基于反馈拟合奖励模型，其中最佳代理采用双Thompson采样，并使用认知神经网络（epistemic neural network）表示不确定性。实验结果显示，这种方法能在更少的查询下实现高性能水平，进一步强调不确定性估计和探索方案选择的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00396v2",
      "published_date": "2024-02-01 07:32:24 UTC",
      "updated_date": "2024-06-04 18:35:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:15:36.060987"
    },
    {
      "arxiv_id": "2402.00393v1",
      "title": "Loss Function Considering Dead Zone for Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Koki Inami",
        "Koki Yamane",
        "Sho Sakaino"
      ],
      "abstract": "It is important to reveal the inverse dynamics of manipulators to improve\ncontrol performance of model-based control. Neural networks (NNs) are promising\ntechniques to represent complicated inverse dynamics while they require a large\namount of motion data. However, motion data in dead zones of actuators is not\nsuitable for training models decreasing the number of useful training data. In\nthis study, based on the fact that the manipulator joint does not work\nirrespective of input torque in dead zones, we propose a new loss function that\nconsiders only errors of joints not in dead zones. The proposed method enables\nto increase in the amount of motion data available for training and the\naccuracy of the inverse dynamics computation. Experiments on actual equipment\nusing a three-degree-of-freedom (DOF) manipulator showed higher accuracy than\nconventional methods. We also confirmed and discussed the behavior of the model\nof the proposed method in dead zones.",
      "tldr_zh": "这篇论文针对神经网络（Neural Networks, NNs）在训练机械臂逆动力学（inverse dynamics）模型时的问题，提出了一种新的损失函数（loss function），它仅考虑执行器死区（dead zones）外关节的误差，以增加可用训练数据量并提升模型准确性。传统方法因死区数据不适合训练而导致数据利用率低下，该方法基于机械臂关节在死区内不工作的特性，优化了训练过程。在一个三自由度（DOF）机械臂的实际实验中，该方法比传统方法准确性更高，并讨论了模型在死区内的行为表现。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 6 figures, Accepted at AMC2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00393v1",
      "published_date": "2024-02-01 07:28:55 UTC",
      "updated_date": "2024-02-01 07:28:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:15:51.875359"
    },
    {
      "arxiv_id": "2402.00390v2",
      "title": "DNS-Rec: Data-aware Neural Architecture Search for Recommender Systems",
      "title_zh": "DNS-Rec：数据感知神经架构搜索用于推荐系统",
      "authors": [
        "Sheng Zhang",
        "Maolin Wang",
        "Yao Zhao",
        "Chenyi Zhuang",
        "Jinjie Gu",
        "Ruocheng Guo",
        "Xiangyu Zhao",
        "Zijian Zhang",
        "Hongzhi Yin"
      ],
      "abstract": "In the era of data proliferation, efficiently sifting through vast\ninformation to extract meaningful insights has become increasingly crucial.\nThis paper addresses the computational overhead and resource inefficiency\nprevalent in existing Sequential Recommender Systems (SRSs). We introduce an\ninnovative approach combining pruning methods with advanced model designs.\nFurthermore, we delve into resource-constrained Neural Architecture Search\n(NAS), an emerging technique in recommender systems, to optimize models in\nterms of FLOPs, latency, and energy consumption while maintaining or enhancing\naccuracy. Our principal contribution is the development of a Data-aware Neural\nArchitecture Search for Recommender System (DNS-Rec). DNS-Rec is specifically\ndesigned to tailor compact network architectures for attention-based SRS\nmodels, thereby ensuring accuracy retention. It incorporates data-aware gates\nto enhance the performance of the recommendation network by learning\ninformation from historical user-item interactions. Moreover, DNS-Rec employs a\ndynamic resource constraint strategy, stabilizing the search process and\nyielding more suitable architectural solutions. We demonstrate the\neffectiveness of our approach through rigorous experiments conducted on three\nbenchmark datasets, which highlight the superiority of DNS-Rec in SRSs. Our\nfindings set a new standard for future research in efficient and accurate\nrecommendation systems, marking a significant step forward in this rapidly\nevolving field.",
      "tldr_zh": "本文针对现有Sequential Recommender Systems (SRSs) 的计算开销和资源效率问题，提出了一种创新方法DNS-Rec，即Data-aware Neural Architecture Search for Recommender Systems。DNS-Rec 通过整合修剪技术、data-aware gates（从历史用户-物品互动中学习信息）和动态资源约束策略，针对注意力-based SRS 模型优化紧凑网络架构，从而降低FLOPs、延迟和能耗，同时保持或提升准确性。在三个基准数据集上的实验中，DNS-Rec 展示了显著优越性，为高效准确的推荐系统研究树立了新标准。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00390v2",
      "published_date": "2024-02-01 07:22:52 UTC",
      "updated_date": "2024-12-19 14:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:16:03.433766"
    },
    {
      "arxiv_id": "2402.00389v5",
      "title": "On the $O(\\frac{\\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\\ell_1$ Norm",
      "title_zh": "翻译失败",
      "authors": [
        "Huan Li",
        "Yiming Dong",
        "Zhouchen Lin"
      ],
      "abstract": "Although adaptive gradient methods have been extensively used in deep\nlearning, their convergence rates proved in the literature are all slower than\nthat of SGD, particularly with respect to their dependence on the dimension.\nThis paper considers the classical RMSProp and its momentum extension and\nestablishes the convergence rate of $\\frac{1}{T}\\sum_{k=1}^T E\\left[\\|\\nabla\nf(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}C}{T^{1/4}})$ measured by $\\ell_1$ norm\nwithout the bounded gradient assumption, where $d$ is the dimension of the\noptimization variable, $T$ is the iteration number, and $C$ is a constant\nidentical to that appeared in the optimal convergence rate of SGD. Our\nconvergence rate matches the lower bound with respect to all the coefficients\nexcept the dimension $d$. Since $\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$ for\nproblems with extremely large $d$, our convergence rate can be considered to be\nanalogous to the $\\frac{1}{T}\\sum_{k=1}^T E\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq\nO(\\frac{C}{T^{1/4}})$ rate of SGD in the ideal case of $\\|\\nabla\nf(x)\\|_1=\\varTheta(\\sqrt{d}\\|\\nabla f(x)\\|_2)$.",
      "tldr_zh": "这项研究分析了自适应梯度方法 RMSProp 及其动量扩展的收敛率，重点关注其对维度 $d$ 的依赖，并使用 $\\ell_1$ 范数进行度量。论文证明了 $\\frac{1}{T}\\sum_{k=1}^T E\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}C}{T^{1/4}})$ 的收敛率，而无需假设梯度有界，这与 SGD 的最优收敛率在系数上基本匹配。相比 SGD，RMSProp 的收敛率在高维问题中更慢，但通过 $\\ell_1$ 范数，该结果为优化算法性能提供了新的洞见，尤其当 $\\|\\nabla f(x)\\|_1 = \\Theta(\\sqrt{d} \\|\\nabla f(x)\\|_2)$ 时。",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "V5 vs V4: Improve the noise dependence from $\\sqrt{d}\\|\\sigma\\|_2$ to\n  $\\|\\sigma\\|_1$ and modify the related work. V4 vs V3: More experiments. V3 vs\n  V2: A fairer comparison with (Li et al., 2023). V2 vs V1: (1) Correct one\n  error in v1. (2) Improve the convergence rate matching the lower bound with\n  respect to all the coefficients except the dimension",
      "pdf_url": "http://arxiv.org/pdf/2402.00389v5",
      "published_date": "2024-02-01 07:21:32 UTC",
      "updated_date": "2025-04-26 01:34:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:16:16.052060"
    },
    {
      "arxiv_id": "2402.00388v1",
      "title": "Cumulative Distribution Function based General Temporal Point Processes",
      "title_zh": "基于累积分布函数的一般时点过程",
      "authors": [
        "Maolin Wang",
        "Yu Pan",
        "Zenglin Xu",
        "Ruocheng Guo",
        "Xiangyu Zhao",
        "Wanyu Wang",
        "Yiqi Wang",
        "Zitao Liu",
        "Langming Liu"
      ],
      "abstract": "Temporal Point Processes (TPPs) hold a pivotal role in modeling event\nsequences across diverse domains, including social networking and e-commerce,\nand have significantly contributed to the advancement of recommendation systems\nand information retrieval strategies. Through the analysis of events such as\nuser interactions and transactions, TPPs offer valuable insights into\nbehavioral patterns, facilitating the prediction of future trends. However,\naccurately forecasting future events remains a formidable challenge due to the\nintricate nature of these patterns. The integration of Neural Networks with\nTPPs has ushered in the development of advanced deep TPP models. While these\nmodels excel at processing complex and nonlinear temporal data, they encounter\nlimitations in modeling intensity functions, grapple with computational\ncomplexities in integral computations, and struggle to capture long-range\ntemporal dependencies effectively. In this study, we introduce the CuFun model,\nrepresenting a novel approach to TPPs that revolves around the Cumulative\nDistribution Function (CDF). CuFun stands out by uniquely employing a monotonic\nneural network for CDF representation, utilizing past events as a scaling\nfactor. This innovation significantly bolsters the model's adaptability and\nprecision across a wide range of data scenarios. Our approach addresses several\ncritical issues inherent in traditional TPP modeling: it simplifies\nlog-likelihood calculations, extends applicability beyond predefined density\nfunction forms, and adeptly captures long-range temporal patterns. Our\ncontributions encompass the introduction of a pioneering CDF-based TPP model,\nthe development of a methodology for incorporating past event information into\nfuture event prediction, and empirical validation of CuFun's effectiveness\nthrough extensive experimentation on synthetic and real-world datasets.",
      "tldr_zh": "本研究针对Temporal Point Processes (TPPs) 在建模事件序列（如社交网络和电子商务中的用户互动）时的挑战，提出了一种基于Cumulative Distribution Function (CDF) 的新模型CuFun，以解决传统深度TPPs模型在强度函数建模、计算复杂性和捕捉长程依赖方面的局限性。CuFun 通过使用单调神经网络表示CDF，并将过去事件作为缩放因子，简化了log-likelihood计算，并增强了模型的适应性和精确性。该方法不仅扩展了TPPs的适用范围，还通过在合成和真实数据集上的广泛实验验证，证明了其在预测未来事件方面的显著有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00388v1",
      "published_date": "2024-02-01 07:21:30 UTC",
      "updated_date": "2024-02-01 07:21:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:16:29.716010"
    },
    {
      "arxiv_id": "2402.01771v1",
      "title": "BlackMamba: Mixture of Experts for State-Space Models",
      "title_zh": "翻译失败",
      "authors": [
        "Quentin Anthony",
        "Yury Tokpanov",
        "Paolo Glorioso",
        "Beren Millidge"
      ],
      "abstract": "State-space models (SSMs) have recently demonstrated competitive performance\nto transformers at large-scale language modeling benchmarks while achieving\nlinear time and memory complexity as a function of sequence length. Mamba, a\nrecently released SSM model, shows impressive performance in both language\nmodeling and long sequence processing tasks. Simultaneously, mixture-of-expert\n(MoE) models have shown remarkable performance while significantly reducing the\ncompute and latency costs of inference at the expense of a larger memory\nfootprint. In this paper, we present BlackMamba, a novel architecture that\ncombines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate\nthat BlackMamba performs competitively against both Mamba and transformer\nbaselines, and outperforms in inference and training FLOPs. We fully train and\nopen-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a\ncustom dataset. We show that BlackMamba inherits and combines both of the\nbenefits of SSM and MoE architectures, combining linear-complexity generation\nfrom SSM with cheap and fast inference from MoE. We release all weights,\ncheckpoints, and inference code open-source. Inference code at:\nhttps://github.com/Zyphra/BlackMamba",
      "tldr_zh": "该研究提出 BlackMamba，一种创新架构，将 State-space Models (SSMs) 中的 Mamba 模型与 Mixture-of-Experts (MoE) 相结合，旨在兼顾线性时间复杂度的高效性与降低推理计算成本的优势。实验结果显示，BlackMamba 在语言建模和长序列处理任务中，与 Mamba 和 Transformer 基线相比，表现出色，并在训练和推理 FLOPs 上实现优化。作者训练了多个规模的模型（如 340M/1.5B 和 630M/2.8B 参数）并在 300B 标记的自定义数据集上开源权重和代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01771v1",
      "published_date": "2024-02-01 07:15:58 UTC",
      "updated_date": "2024-02-01 07:15:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:16:41.394694"
    },
    {
      "arxiv_id": "2402.00366v1",
      "title": "Legged Robot State Estimation With Invariant Extended Kalman Filter Using Neural Measurement Network",
      "title_zh": "翻译失败",
      "authors": [
        "Donghoon Youm",
        "Hyunsik Oh",
        "Suyoung Choi",
        "Hyeongjun Kim",
        "Jemin Hwangbo"
      ],
      "abstract": "This paper introduces a novel proprioceptive state estimator for legged\nrobots that combines model-based filters and deep neural networks. Recent\nstudies have shown that neural networks such as multi-layer perceptron or\nrecurrent neural networks can estimate the robot states, including contact\nprobability and linear velocity. Inspired by this, we develop a state\nestimation framework that integrates a neural measurement network (NMN) with an\ninvariant extended Kalman filter. We show that our framework improves\nestimation performance in various terrains. Existing studies that combine\nmodel-based filters and learning-based approaches typically use real-world\ndata. However, our approach relies solely on simulation data, as it allows us\nto easily obtain extensive data. This difference leads to a gap between the\nlearning and the inference domain, commonly referred to as a sim-to-real gap.\nWe address this challenge by adapting existing learning techniques and\nregularization. To validate our proposed method, we conduct experiments using a\nquadruped robot on four types of terrain: \\textit{flat}, \\textit{debris},\n\\textit{soft}, and \\textit{slippery}. We observe that our approach\nsignificantly reduces position drift compared to the existing model-based state\nestimator.",
      "tldr_zh": "该论文提出了一种新型本体感知状态估计器，用于腿式机器人，将基于模型的过滤器与深度神经网络相结合。具体来说，框架整合了 Neural Measurement Network (NMN) 与 Invariant Extended Kalman Filter，利用模拟数据训练来估计机器人状态，如接触概率和线性速度，并通过适应学习技术和正则化解决 sim-to-real 差距问题。实验在四足机器人上测试了平坦、碎片、柔软和滑溜等四种地形，结果显示该方法显著降低了位置漂移，与现有基于模型的估计器相比，整体性能得到提升。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8pages, 6paper, This work has been submitted to the IEEE for possible\n  publication",
      "pdf_url": "http://arxiv.org/pdf/2402.00366v1",
      "published_date": "2024-02-01 06:06:59 UTC",
      "updated_date": "2024-02-01 06:06:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:16:52.920028"
    },
    {
      "arxiv_id": "2402.00362v1",
      "title": "Climate Trends of Tropical Cyclone Intensity and Energy Extremes Revealed by Deep Learning",
      "title_zh": "深度学习揭示热带气旋强度和能量极端",
      "authors": [
        "Buo-Fu Chen",
        "Boyo Chen",
        "Chun-Min Hsiao",
        "Hsu-Feng Teng",
        "Cheng-Shang Lee",
        "Hung-Chi Kuo"
      ],
      "abstract": "Anthropogenic influences have been linked to tropical cyclone (TC) poleward\nmigration, TC extreme precipitation, and an increased proportion of major\nhurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is\ncritical for projecting future TC impacts on human society considering the\nchanging climate [5]. However, past trends of TC structure/energy remain\nuncertain due to limited observations; subjective-analyzed and\nspatiotemporal-heterogeneous \"best-track\" datasets lead to reduced confidence\nin the assessed TC repose to climate change [6, 7]. Here, we use deep learning\nto reconstruct past \"observations\" and yield an objective global TC wind\nprofile dataset during 1981 to 2020, facilitating a comprehensive examination\nof TC structure/energy. By training with uniquely labeled data integrating best\ntracks and numerical model analysis of 2004 to 2018 TCs, our model converts\nmultichannel satellite imagery to a 0-750-km wind profile of axisymmetric\nsurface winds. The model performance is verified to be sufficient for climate\nstudies by comparing it to independent satellite-radar surface winds. Based on\nthe new homogenized dataset, the major TC proportion has increased by ~13% in\nthe past four decades. Moreover, the proportion of extremely high-energy TCs\nhas increased by ~25%, along with an increasing trend (> one standard deviation\nof the 40-y variability) of the mean total energy of high-energy TCs. Although\nthe warming ocean favors TC intensification, the TC track migration to higher\nlatitudes and altered environments further affect TC structure/energy. This new\ndeep learning method/dataset reveals novel trends regarding TC structure\nextremes and may help verify simulations/studies regarding TCs in the changing\nclimate.",
      "tldr_zh": "这篇论文使用深度学习方法重建1981-2020年的全球热带气旋(TC)风廓线数据集，通过训练模型将多通道卫星图像转换为轴对称表面风廓线，从而解决过去TC趋势不确定性的问题。研究发现，主要TC比例增加了约13%，极端高能量TC比例增加了约25%，高能量TC的平均总能量也呈现显著上升趋势。海洋变暖促进TC增强，但TC向高纬迁移和环境变化进一步影响其结构和能量。该方法和数据集为验证气候变化中TC模拟研究提供了重要工具。",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "41 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.00362v1",
      "published_date": "2024-02-01 06:02:29 UTC",
      "updated_date": "2024-02-01 06:02:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:17:06.070924"
    },
    {
      "arxiv_id": "2402.00355v1",
      "title": "Adaptive Primal-Dual Method for Safe Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Weiqin Chen",
        "James Onyejizu",
        "Long Vu",
        "Lan Hoang",
        "Dharmashankar Subramanian",
        "Koushik Kar",
        "Sandipan Mishra",
        "Santiago Paternain"
      ],
      "abstract": "Primal-dual methods have a natural application in Safe Reinforcement Learning\n(SRL), posed as a constrained policy optimization problem. In practice however,\napplying primal-dual methods to SRL is challenging, due to the inter-dependency\nof the learning rate (LR) and Lagrangian multipliers (dual variables) each time\nan embedded unconstrained RL problem is solved. In this paper, we propose,\nanalyze and evaluate adaptive primal-dual (APD) methods for SRL, where two\nadaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the\npolicy in each iteration. We theoretically establish the convergence,\noptimality and feasibility of the APD algorithm. Finally, we conduct numerical\nevaluation of the practical APD algorithm with four well-known environments in\nBullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian\nand DDPG-Lagrangian. All experiments show that the practical APD algorithm\noutperforms (or achieves comparable performance) and attains more stable\ntraining than the constant LR cases. Additionally, we substantiate the\nrobustness of selecting the two adaptive LRs by empirical evidence.",
      "tldr_zh": "本论文提出了一种自适应原始-对偶（APD）方法，用于解决安全强化学习（SRL）中约束策略优化问题的挑战，该方法通过调整两个自适应学习率（LR）来优化拉格朗日乘子（dual variables），从而提升每个迭代的策略优化效率。作者理论上证明了 APD 算法的收敛性、最优性和可行性，确保了其在 SRL 应用中的可靠性。在 Bullet-Safety-Gym 的四个环境中，使用 PPO-Lagrangian 和 DDPG-Lagrangian 算法进行实验，结果显示 APD 比固定 LR 方法性能更优、训练更稳定，并证明了自适应 LR 的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00355v1",
      "published_date": "2024-02-01 05:53:44 UTC",
      "updated_date": "2024-02-01 05:53:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:17:17.318235"
    },
    {
      "arxiv_id": "2402.01770v1",
      "title": "Extending Interactive Science Exhibits into the Classroom using Anthropomorphized Chatbots and Bloom's Taxonomy",
      "title_zh": "利用拟人化聊天机器人和布卢姆分类法将互动科学展览扩展到课堂中",
      "authors": [
        "Yousuf Golding"
      ],
      "abstract": "This study explores the use of Generative AI chatbots for transforming public\nscience exhibits into virtual experiences that can extend the engagement of\nexhibits into the classroom. The broader goal is to increase accessibility of\nscience exhibits, especially for those marginalized in STEM due to various\nfactors, including cultural barriers. We hypothesize that turning exhibits into\nfirst-person anthropomorphized chatbots with a personality, like quirky-talking\nasteroids or comets, can increase engagement and learning. The paper mainly\nexplores if such techniques are possible using Generative AI (e.g. GPT) via\nprompt engineering alone. The research includes an investigation into the\npossibility of integrating interactive assessment via question-generation using\nBloom's Taxonomy. Initial results indicate that it is possible to combine these\ntechniques. As such, it lays a foundation for future classroom evaluations of\nsuch chatbots to gauge their overall efficacy in extending the reach of science\nexhibitions. The paper concludes by discussing extensions of the research to\nfully evaluate effectiveness in virtual field-trips. We also include a brief\nexamination of additional ways to enhance student motivation towards learning\nvia chatbots.",
      "tldr_zh": "本研究探讨了使用生成式 AI 聊天机器人（Generative AI chatbots）将公共科学展览转化为虚拟体验，从而扩展到课堂环境中，旨在提升 STEM 领域的可访问性，特别是针对文化障碍等因素导致的边缘化群体。研究假设，通过将展览转化为第一人称拟人化聊天机器人（anthropomorphized chatbots），如会说话的小行星或彗星，能够提高参与度和学习效果，并通过提示工程（prompt engineering）结合 Bloom's Taxonomy 生成互动评估问题。初步结果表明，这种方法是可行的，为未来评估聊天机器人在虚拟实地考察中的效能奠定了基础，同时讨论了增强学生学习动机的额外策略。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01770v1",
      "published_date": "2024-02-01 05:49:54 UTC",
      "updated_date": "2024-02-01 05:49:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:17:31.142133"
    },
    {
      "arxiv_id": "2402.00350v2",
      "title": "Large Language Models Based Fuzzing Techniques: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Linghan Huang",
        "Peizhou Zhao",
        "Huaming Chen",
        "Lei Ma"
      ],
      "abstract": "In the modern era where software plays a pivotal role, software security and\nvulnerability analysis have become essential for software development. Fuzzing\ntest, as an efficient software testing method, are widely used in various\ndomains. Moreover, the rapid development of Large Language Models (LLMs) has\nfacilitated their application in the field of software testing, demonstrating\nremarkable performance. Considering that existing fuzzing test techniques are\nnot entirely automated and software vulnerabilities continue to evolve, there\nis a growing trend towards employing fuzzing test generated based on large\nlanguage models. This survey provides a systematic overview of the approaches\nthat fuse LLMs and fuzzing tests for software testing. In this paper, a\nstatistical analysis and discussion of the literature in three areas, namely\nLLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by\nsummarising the state-of-the-art methods up until 2024. Our survey also\ninvestigates the potential for widespread deployment and application of fuzzing\ntest techniques generated by LLMs in the future.",
      "tldr_zh": "这篇调查论文系统概述了大型语言模型（LLMs）在模糊测试（Fuzzing）中的应用，探讨了LLMs如何提升软件测试的自动化和效率，以应对不断演变的软件漏洞。论文通过统计分析LLMs、Fuzzing以及基于LLMs生成的Fuzzing测试的文献，总结了截至2024年的先进方法和趋势。研究发现，这种融合方法表现出色，并指出了LLMs驱动的Fuzzing技术在未来广泛部署的潜力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages submission under review",
      "pdf_url": "http://arxiv.org/pdf/2402.00350v2",
      "published_date": "2024-02-01 05:34:03 UTC",
      "updated_date": "2024-02-07 06:03:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:17:41.325854"
    },
    {
      "arxiv_id": "2402.00348v1",
      "title": "ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update",
      "title_zh": "ODICE：通过正交梯度更新揭示分布校正估计的奥秘",
      "authors": [
        "Liyuan Mao",
        "Haoran Xu",
        "Weinan Zhang",
        "Xianyuan Zhan"
      ],
      "abstract": "In this study, we investigate the DIstribution Correction Estimation (DICE)\nmethods, an important line of work in offline reinforcement learning (RL) and\nimitation learning (IL). DICE-based methods impose state-action-level behavior\nconstraint, which is an ideal choice for offline learning. However, they\ntypically perform much worse than current state-of-the-art (SOTA) methods that\nsolely use action-level behavior constraint. After revisiting DICE-based\nmethods, we find there exist two gradient terms when learning the value\nfunction using true-gradient update: forward gradient (taken on the current\nstate) and backward gradient (taken on the next state). Using forward gradient\nbears a large similarity to many offline RL methods, and thus can be regarded\nas applying action-level constraint. However, directly adding the backward\ngradient may degenerate or cancel out its effect if these two gradients have\nconflicting directions. To resolve this issue, we propose a simple yet\neffective modification that projects the backward gradient onto the normal\nplane of the forward gradient, resulting in an orthogonal-gradient update, a\nnew learning rule for DICE-based methods. We conduct thorough theoretical\nanalyses and find that the projected backward gradient brings state-level\nbehavior regularization, which reveals the mystery of DICE-based methods: the\nvalue learning objective does try to impose state-action-level constraint, but\nneeds to be used in a corrected way. Through toy examples and extensive\nexperiments on complex offline RL and IL tasks, we demonstrate that DICE-based\nmethods using orthogonal-gradient updates (O-DICE) achieve SOTA performance and\ngreat robustness.",
      "tldr_zh": "本研究探讨了 DIstribution Correction Estimation (DICE) 方法在离线强化学习 (RL) 和模仿学习 (IL) 中的问题，指出 DICE 通过状态-动作级别行为约束虽理想，但由于 forward gradient 和 backward gradient 方向冲突，导致表现远逊于仅使用动作级别约束的 SOTA 方法。作者提出 orthogonal-gradient update 改进方案，将 backward gradient 投影到 forward gradient 的正交平面，从而实现有效的状态级别行为正则化，并通过理论分析揭示 DICE 的潜在机制。实验结果显示，改进后的 O-DICE 方法在复杂离线 RL 和 IL 任务上达到了 SOTA 性能，并展现出卓越的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Spotlight @ ICLR 2024, first two authors contribute equally",
      "pdf_url": "http://arxiv.org/pdf/2402.00348v1",
      "published_date": "2024-02-01 05:30:51 UTC",
      "updated_date": "2024-02-01 05:30:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:17:54.337012"
    },
    {
      "arxiv_id": "2402.00334v1",
      "title": "Multi-agent Path Finding for Cooperative Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongxia Yan",
        "Han Zheng",
        "Cathy Wu"
      ],
      "abstract": "Anticipating possible future deployment of connected and automated vehicles\n(CAVs), cooperative autonomous driving at intersections has been studied by\nmany works in control theory and intelligent transportation across decades.\nSimultaneously, recent parallel works in robotics have devised efficient\nalgorithms for multi-agent path finding (MAPF), though often in environments\nwith simplified kinematics. In this work, we hybridize insights and algorithms\nfrom MAPF with the structure and heuristics of optimizing the crossing order of\nCAVs at signal-free intersections. We devise an optimal and complete algorithm,\nOrder-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which\nsignificantly outperforms existing algorithms, fixed heuristics, and\nprioritized planning with KATS. The performance is maintained under different\nvehicle arrival rates, lane lengths, crossing speeds, and control horizon.\nThrough ablations and dissections, we offer insight on the contributing factors\nto OBS-KATS's performance. Our work is directly applicable to many similarly\nscaled traffic and multi-robot scenarios with directed lanes.",
      "tldr_zh": "该研究将多智能体路径寻找(MAPF)算法与合作自主驾驶相结合，针对连接和自动车辆(CAVs)在无信号灯交叉路口的优化问题。论文提出了一种最优且完整的算法Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS)，它通过整合MAPF见解和穿越顺序启发式，显著优于现有算法、固定启发式方法和优先级规划。实验结果显示，OBS-KATS在不同车辆到达率、车道长度、穿越速度和控制视野下保持高性能，并通过消融分析揭示了其关键因素。该方法适用于类似规模的交通和多机器人场景，具有定向车道。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "7 pages, 3 figures, IEEE International Conference on Robotics and\n  Automation (ICRA), 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.00334v1",
      "published_date": "2024-02-01 04:39:15 UTC",
      "updated_date": "2024-02-01 04:39:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:18:07.829146"
    },
    {
      "arxiv_id": "2402.00319v1",
      "title": "SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling",
      "title_zh": "SCO-VIST：基于社会互动常识知识的视觉故事生成",
      "authors": [
        "Eileen Wang",
        "Soyeon Caren Han",
        "Josiah Poon"
      ],
      "abstract": "Visual storytelling aims to automatically generate a coherent story based on\na given image sequence. Unlike tasks like image captioning, visual stories\nshould contain factual descriptions, worldviews, and human social commonsense\nto put disjointed elements together to form a coherent and engaging\nhuman-writeable story. However, most models mainly focus on applying factual\ninformation and using taxonomic/lexical external knowledge when attempting to\ncreate stories. This paper introduces SCO-VIST, a framework representing the\nimage sequence as a graph with objects and relations that includes human action\nmotivation and its social interaction commonsense knowledge. SCO-VIST then\ntakes this graph representing plot points and creates bridges between plot\npoints with semantic and occurrence-based edge weights. This weighted story\ngraph produces the storyline in a sequence of events using Floyd-Warshall's\nalgorithm. Our proposed framework produces stories superior across multiple\nmetrics in terms of visual grounding, coherence, diversity, and humanness, per\nboth automatic and human evaluations.",
      "tldr_zh": "本研究提出 SCO-VIST 框架，用于基于图像序列生成连贯的视觉故事，该框架整合社会互动常识知识，以弥补现有模型在事实描述和人类世界观方面的不足。SCO-VIST 将图像序列表示为包含对象、关系、行动动机及社会常识的图结构，并通过语义和基于事件的边权重构建加权故事图，然后运用 Floyd-Warshall's algorithm 生成事件序列的故事线。实验结果显示，该框架在视觉 grounding、连贯性、多样性和人性化等指标上均优于基线模型，经自动和人类评估证实。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00319v1",
      "published_date": "2024-02-01 04:09:17 UTC",
      "updated_date": "2024-02-01 04:09:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:18:17.595895"
    },
    {
      "arxiv_id": "2404.16840v1",
      "title": "Biometrics Employing Neural Network",
      "title_zh": "生物识别技术采用神经网络",
      "authors": [
        "Sajjad Bhuiyan"
      ],
      "abstract": "Biometrics involves using unique human traits, both physical and behavioral,\nfor the digital identification of individuals to provide access to systems,\ndevices, or information. Within the field of computer science, it acts as a\nmethod for identifying and verifying individuals and controlling access. While\nthe conventional method for personal authentication involves passwords, the\nvulnerability arises when passwords are compromised, allowing unauthorized\naccess to sensitive actions. Biometric authentication presents a viable answer\nto this problem and is the most secure and user-friendly authentication method.\nToday, fingerprints, iris and retina patterns, facial recognition, hand shapes,\npalm prints, and voice recognition are frequently used forms of biometrics.\nDespite the diverse nature of these biometric identifiers, the core objective\nremains consistent ensuring security, recognizing authorized users, and\nrejecting impostors. Hence, it is crucial to determine accurately whether the\ncharacteristics belong to the rightful person. For systems to be effective and\nwidely accepted, the error rate in recognition and verification must approach\nzero. It is acknowledged that current biometric techniques, while advanced, are\nnot infallible and require continuous improvement. A more refined classifier is\ndeemed necessary to classify patterns accurately. Artificial Neural Networks,\nwhich simulate the human brain's operations, present themselves as a promising\napproach. The survey presented herein explores various biometric techniques\nbased on neural networks, emphasizing the ongoing quest for enhanced accuracy\nand reliability. It concludes that The utilization of neural networks along\nwith biometric features not only enhances accuracy but also contributes to\noverall better security.",
      "tldr_zh": "这篇论文讨论了生物识别(Biometrics)技术如何通过神经网络(Neural Network)来提升个人身份验证的准确性和安全性。传统密码认证易受泄露影响，而生物识别使用独特的人体特征（如指纹、虹膜和面部识别）提供更可靠的替代方案，但仍面临错误率问题。论文调查了基于人工神经网络(Artificial Neural Networks)的各种生物识别方法，强调这些方法能有效降低错误率，并得出结论：结合神经网络不仅提高了分类精度，还增强了整体系统安全性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "F.2.2, I.2.7"
      ],
      "primary_category": "cs.CR",
      "comment": "14 Pages, 10 figures, Survey Paper",
      "pdf_url": "http://arxiv.org/pdf/2404.16840v1",
      "published_date": "2024-02-01 03:59:04 UTC",
      "updated_date": "2024-02-01 03:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:18:29.799420"
    },
    {
      "arxiv_id": "2402.00312v1",
      "title": "The whack-a-mole governance challenge for AI-enabled synthetic biology: literature review and emerging frameworks",
      "title_zh": "翻译失败",
      "authors": [
        "Trond Arne Undheim"
      ],
      "abstract": "AI-enabled synthetic biology has tremendous potential but also significantly\nincreases biorisks and brings about a new set of dual use concerns. The picture\nis complicated given the vast innovations envisioned to emerge by combining\nemerging technologies, as AI-enabled synthetic biology potentially scales up\nbioengineering into industrial biomanufacturing. However, the literature review\nindicates that goals such as maintaining a reasonable scope for innovation, or\nmore ambitiously to foster a huge bioeconomy don't necessarily contrast with\nbiosafety, but need to go hand in hand. This paper presents a literature review\nof the issues and describes emerging frameworks for policy and practice that\ntransverse the options of command-and control, stewardship, bottom-up, and\nlaissez-faire governance. How to achieve early warning systems that enable\nprevention and mitigation of future AI-enabled biohazards from the lab, from\ndeliberate misuse, or from the public realm, will constantly need to evolve,\nand adaptive, interactive approaches should emerge. Although biorisk is subject\nto an established governance regime, and scientists generally adhere to\nbiosafety protocols, even experimental, but legitimate use by scientists could\nlead to unexpected developments. Recent advances in chatbots enabled by\ngenerative AI have revived fears that advanced biological insight can more\neasily get into the hands of malignant individuals or organizations. Given\nthese sets of issues, society needs to rethink how AI-enabled synthetic biology\nshould be governed. The suggested way to visualize the challenge at hand is\nwhack-a-mole governance, although the emerging solutions are perhaps not so\ndifferent either.",
      "tldr_zh": "本论文通过文献综述探讨了AI-enabled synthetic biology的巨大潜力及其带来的生物风险和双重用途担忧，特别是当其扩展到工业生物制造时。研究分析了各种治理框架，包括command-and control、stewardship、bottom-up和laissez-faire方法，强调创新与生物安全需并行发展，并提出需要适应性早期预警系统来预防实验室意外、故意误用或公众领域风险。最终，论文以whack-a-mole governance概念可视化这一动态挑战，呼吁社会重新思考generative AI时代下的治理策略，以实现平衡。",
      "categories": [
        "q-bio.OT",
        "cs.AI"
      ],
      "primary_category": "q-bio.OT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00312v1",
      "published_date": "2024-02-01 03:53:13 UTC",
      "updated_date": "2024-02-01 03:53:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:18:43.702916"
    },
    {
      "arxiv_id": "2402.00306v2",
      "title": "An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction",
      "title_zh": "一种准确且低参数的机器学习架构，用于下一位置预测",
      "authors": [
        "Calvin Jary",
        "Nafiseh Kahani"
      ],
      "abstract": "Next location prediction is a discipline that involves predicting a users\nnext location. Its applications include resource allocation, quality of\nservice, energy efficiency, and traffic management. This paper proposes an\nenergy-efficient, small, and low parameter machine learning (ML) architecture\nfor accurate next location prediction, deployable on modest base stations and\nedge devices. To accomplish this we ran a hundred hyperparameter experiments on\nthe full human mobility patterns of an entire city, to determine an exact ML\narchitecture that reached a plateau of accuracy with the least amount of model\nparameters. We successfully achieved a reduction in the number of model\nparameters within published ML architectures from 202 million down to 2\nmillion. This reduced the total size of the model parameters from 791 MB down\nto 8 MB. Additionally, this decreased the training time by a factor of four,\nthe amount of graphics processing unit (GPU) memory needed for training by a\nfactor of twenty, and the overall accuracy was increased from 80.16% to 82.54%.\nThis improvement allows for modest base stations and edge devices which do not\nhave a large amount of memory or storage, to deploy and utilize the proposed ML\narchitecture for next location prediction.",
      "tldr_zh": "本文提出了一种准确且低参数的机器学习（Machine Learning）架构，用于下一位置预测（Next Location Prediction），旨在提升能量效率并适用于资源有限的基站和边缘设备。研究团队通过进行100次超参数实验（hyperparameter experiments），基于整个城市的完整人类移动模式，优化模型以在准确率达到高原时最小化参数数量。结果将模型参数从2.02亿减少到200万，模型大小从791 MB 降至8 MB，同时训练时间缩短4倍、GPU 内存需求减少20倍，并将准确率从80.16% 提高到82.54%。这一改进使得内存和存储受限的设备能够高效部署该架构，支持资源分配、服务质量优化等应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper was accepted and presented in person at the 2023 IEEE Future\n  Networks World Forum, in Baltimore, Maryland, USA",
      "pdf_url": "http://arxiv.org/pdf/2402.00306v2",
      "published_date": "2024-02-01 03:39:15 UTC",
      "updated_date": "2024-02-02 17:29:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:18:55.540231"
    },
    {
      "arxiv_id": "2402.01769v1",
      "title": "Redefining \"Hallucination\" in LLMs: Towards a psychology-informed framework for mitigating misinformation",
      "title_zh": "翻译失败",
      "authors": [
        "Elijah Berberette",
        "Jack Hutchins",
        "Amir Sadovnik"
      ],
      "abstract": "In recent years, large language models (LLMs) have become incredibly popular,\nwith ChatGPT for example being used by over a billion users. While these models\nexhibit remarkable language understanding and logical prowess, a notable\nchallenge surfaces in the form of \"hallucinations.\" This phenomenon results in\nLLMs outputting misinformation in a confident manner, which can lead to\ndevastating consequences with such a large user base. However, we question the\nappropriateness of the term \"hallucination\" in LLMs, proposing a psychological\ntaxonomy based on cognitive biases and other psychological phenomena. Our\napproach offers a more fine-grained understanding of this phenomenon, allowing\nfor targeted solutions. By leveraging insights from how humans internally\nresolve similar challenges, we aim to develop strategies to mitigate LLM\nhallucinations. This interdisciplinary approach seeks to move beyond\nconventional terminology, providing a nuanced understanding and actionable\npathways for improvement in LLM reliability.",
      "tldr_zh": "该论文质疑了大型语言模型（LLMs）中“hallucination”一词的适用性，并提出一个基于心理学的框架来重新定义这一现象。作者通过认知 biases 和其他心理机制，建立了一个更细化的分类系统，以深入理解LLMs自信输出误信息的根源。借鉴人类内部解决类似挑战的策略，该方法提供针对性解决方案，最终旨在提升LLMs的可靠性并减少误信息风险。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01769v1",
      "published_date": "2024-02-01 03:01:11 UTC",
      "updated_date": "2024-02-01 03:01:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:19:05.625893"
    },
    {
      "arxiv_id": "2402.00904v1",
      "title": "Graph Domain Adaptation: Challenges, Progress and Prospects",
      "title_zh": "图域适应：挑战、进展与前景",
      "authors": [
        "Boshen Shi",
        "Yongqing Wang",
        "Fangda Guo",
        "Bingbing Xu",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "As graph representation learning often suffers from label scarcity problems\nin real-world applications, researchers have proposed graph domain adaptation\n(GDA) as an effective knowledge-transfer paradigm across graphs. In particular,\nto enhance model performance on target graphs with specific tasks, GDA\nintroduces a bunch of task-related graphs as source graphs and adapts the\nknowledge learnt from source graphs to the target graphs. Since GDA combines\nthe advantages of graph representation learning and domain adaptation, it has\nbecome a promising direction of transfer learning on graphs and has attracted\nan increasing amount of research interest in recent years. In this paper, we\ncomprehensively overview the studies of GDA and present a detailed survey of\nrecent advances. Specifically, we outline the research status and challenges,\npropose a taxonomy, introduce the details of representative works, and discuss\nthe prospects. To the best of our knowledge, this paper is the first survey for\ngraph domain adaptation. A detailed paper list is available at\nhttps://github.com/Skyorca/Awesome-Graph-Domain-Adaptation-Papers.",
      "tldr_zh": "这篇论文对Graph Domain Adaptation (GDA)进行了全面概述，旨在解决图表示学习(graph representation learning)中标签稀缺问题的挑战。作者提出GDA作为一种知识转移范式，通过利用源图上的任务相关知识适应目标图，从而提升模型性能。论文系统梳理了GDA的研究进展、挑战和分类，介绍了代表性工作，并讨论了未来前景，作为首个GDA调查，为图上转移学习的发展提供了重要参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00904v1",
      "published_date": "2024-02-01 02:44:32 UTC",
      "updated_date": "2024-02-01 02:44:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:19:17.495609"
    },
    {
      "arxiv_id": "2402.00284v1",
      "title": "PAP-REC: Personalized Automatic Prompt for Recommendation Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Zelong Li",
        "Jianchao Ji",
        "Yingqiang Ge",
        "Wenyue Hua",
        "Yongfeng Zhang"
      ],
      "abstract": "Recently emerged prompt-based Recommendation Language Models (RLM) can solve\nmultiple recommendation tasks uniformly. The RLMs make full use of the\ninherited knowledge learned from the abundant pre-training data to solve the\ndownstream recommendation tasks by prompts, without introducing additional\nparameters or network training. However, handcrafted prompts require\nsignificant expertise and human effort since slightly rewriting prompts may\ncause massive performance changes. In this paper, we propose PAP-REC, a\nframework to generate the Personalized Automatic Prompt for RECommendation\nlanguage models to mitigate the inefficiency and ineffectiveness problems\nderived from manually designed prompts. Specifically, personalized automatic\nprompts allow different users to have different prompt tokens for the same\ntask, automatically generated using a gradient-based method. One challenge for\npersonalized automatic prompt generation for recommendation language models is\nthe extremely large search space, leading to a long convergence time. To\neffectively and efficiently address the problem, we develop surrogate metrics\nand leverage an alternative updating schedule for prompting recommendation\nlanguage models. Experimental results show that our PAP-REC framework manages\nto generate personalized prompts, and the automatically generated prompts\noutperform manually constructed prompts and also outperform various baseline\nrecommendation models. The source code of the work is available at\nhttps://github.com/rutgerswiselab/PAP-REC.",
      "tldr_zh": "该论文提出 PAP-REC 框架，用于生成个性化自动提示，以优化基于提示的 Recommendation Language Models (RLM) 在推荐任务中的性能。PAP-REC 通过梯度-based 方法为不同用户生成定制化的提示提示，解决了手动设计提示的低效问题，并采用代理指标 (surrogate metrics) 和替代更新调度 (alternative updating schedule) 来应对搜索空间大导致的收敛时间长挑战。实验结果显示，PAP-REC 生成的提示优于手动提示和各种基线模型，在多个推荐任务上表现出色。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00284v1",
      "published_date": "2024-02-01 02:29:16 UTC",
      "updated_date": "2024-02-01 02:29:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:19:29.831109"
    },
    {
      "arxiv_id": "2402.01767v2",
      "title": "HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyue Chen",
        "Pengyu Gao",
        "Jiangjiang Song",
        "Xiaoyang Tan"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has rapidly advanced the language model\nfield, particularly in question-answering (QA) systems. By integrating external\ndocuments during the response generation phase, RAG significantly enhances the\naccuracy and reliability of language models. This method elevates the quality\nof responses and reduces the frequency of hallucinations, where the model\ngenerates incorrect or misleading information. However, these methods exhibit\nlimited retrieval accuracy when faced with numerous indistinguishable\ndocuments, presenting notable challenges in their practical application. In\nresponse to these emerging challenges, we present HiQA, an advanced\nmulti-document question-answering (MDQA) framework that integrates cascading\nmetadata into content and a multi-route retrieval mechanism. We also release a\nbenchmark called MasQA to evaluate and research in MDQA. Finally, HiQA\ndemonstrates the state-of-the-art performance in multi-document environments.",
      "tldr_zh": "该研究针对检索增强生成（RAG）在多文档问答（MDQA）中的检索准确性问题，提出了一种分层上下文增强框架 HiQA。该框架通过整合级联元数据和多路由检索机制，提高了模型在处理大量相似文档时的性能和可靠性，同时减少了幻觉现象。研究者还发布了 MasQA 基准，用于评估 MDQA 系统，最终 HiQA 在多文档环境中实现了最先进（state-of-the-art）的表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01767v2",
      "published_date": "2024-02-01 02:24:15 UTC",
      "updated_date": "2024-09-24 08:25:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:19:41.124274"
    },
    {
      "arxiv_id": "2402.00262v1",
      "title": "Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Qun Ma",
        "Xiao Xue",
        "Deyu Zhou",
        "Xiangning Yu",
        "Donghua Liu",
        "Xuwen Zhang",
        "Zihan Zhao",
        "Yifan Shen",
        "Peilin Ji",
        "Juanjuan Li",
        "Gang Wang",
        "Wanpeng Ma"
      ],
      "abstract": "Computational experiments have emerged as a valuable method for studying\ncomplex systems, involving the algorithmization of counterfactuals. However,\naccurately representing real social systems in Agent-based Modeling (ABM) is\nchallenging due to the diverse and intricate characteristics of humans,\nincluding bounded rationality and heterogeneity. To address this limitation,\nthe integration of Large Language Models (LLMs) has been proposed, enabling\nagents to possess anthropomorphic abilities such as complex reasoning and\nautonomous learning. These agents, known as LLM-based Agent, offer the\npotential to enhance the anthropomorphism lacking in ABM. Nonetheless, the\nabsence of explicit explainability in LLMs significantly hinders their\napplication in the social sciences. Conversely, computational experiments excel\nin providing causal analysis of individual behaviors and complex phenomena.\nThus, combining computational experiments with LLM-based Agent holds\nsubstantial research potential. This paper aims to present a comprehensive\nexploration of this fusion. Primarily, it outlines the historical development\nof agent structures and their evolution into artificial societies, emphasizing\ntheir importance in computational experiments. Then it elucidates the\nadvantages that computational experiments and LLM-based Agents offer each\nother, considering the perspectives of LLM-based Agent for computational\nexperiments and vice versa. Finally, this paper addresses the challenges and\nfuture trends in this research domain, offering guidance for subsequent related\nstudies.",
      "tldr_zh": "这篇论文调查了计算实验与Large Language Models (LLMs) 基于智能体的融合，旨在解决Agent-based Modeling (ABM) 在模拟真实社会系统时面临的挑战，如人类行为的异质性和有限理性。论文回顾了智能体结构的演变及其在计算实验中的作用，强调LLMs 可以赋予代理拟人能力，包括复杂推理和自主学习，从而提升ABM 的拟人化水平。最终，它分析了双方的互补优势、面临的解释性问题和未来趋势，为相关研究提供指导和展望。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00262v1",
      "published_date": "2024-02-01 01:17:46 UTC",
      "updated_date": "2024-02-01 01:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:19:53.541031"
    },
    {
      "arxiv_id": "2402.00260v3",
      "title": "Human-mediated Large Language Models for Robotic Intervention in Children with Autism Spectrum Disorders",
      "title_zh": "翻译失败",
      "authors": [
        "Ruchik Mishra",
        "Karla Conn Welch",
        "Dan O Popa"
      ],
      "abstract": "The robotic intervention for individuals with Autism Spectrum Disorder (ASD)\nhas generally used pre-defined scripts to deliver verbal content during\none-to-one therapy sessions. This practice restricts the use of robots to\nlimited, pre-mediated instructional curricula. In this paper, we increase robot\nautonomy in one such robotic intervention for children with ASD by implementing\nperspective-taking teaching. Our approach uses large language models (LLM) to\ngenerate verbal content as texts and then deliver it to the child via robotic\nspeech. In the proposed pipeline, we teach perspective-taking through which our\nrobot takes up three roles: initiator, prompter, and reinforcer. We adopted the\nGPT-2 + BART pipelines to generate social situations, ask questions (as\ninitiator), and give options (as prompter) when required. The robot encourages\nthe child by giving positive reinforcement for correct answers (as a\nreinforcer). In addition to our technical contribution, we conducted ten-minute\nsessions with domain experts simulating an actual perspective teaching session,\nwith the researcher acting as a child participant. These sessions validated our\nrobotic intervention pipeline through surveys, including those from NASA TLX\nand GodSpeed. We used BERTScore to compare our GPT-2 + BART pipeline with an\nall GPT-2 and found the performance of the former to be better. Based on the\nresponses by the domain experts, the robot session demonstrated higher\nperformance with no additional increase in mental or physical demand, temporal\ndemand, effort, or frustration compared to a no-robot session. We also\nconcluded that the domain experts perceived the robot as ideally safe, likable,\nand reliable.",
      "tldr_zh": "这篇论文提出了一种利用Large Language Models (LLM)增强机器人干预自闭谱系障碍(ASD)儿童的方法，通过GPT-2 + BART管道生成教学内容，使机器人自主扮演initiator、prompter和reinforcer角色，以教导perspective-taking技能。相比传统预定义脚本，该方法在实验中通过BERTScore评估显示性能优于all GPT-2管道，并在模拟会话中获得领域专家积极反馈，使用NASA TLX和GodSpeed调查表明机器人会话更安全、可爱和可靠，同时未增加额外心理或物理负担。研究为机器人干预ASD儿童提供了更自治和有效的框架。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work is submitted for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2402.00260v3",
      "published_date": "2024-02-01 01:09:00 UTC",
      "updated_date": "2024-07-27 04:19:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:20:06.617708"
    },
    {
      "arxiv_id": "2402.00254v1",
      "title": "Vertical Symbolic Regression via Deep Policy Gradient",
      "title_zh": "基于深度策略梯度的垂直",
      "authors": [
        "Nan Jiang",
        "Md Nasim",
        "Yexiang Xue"
      ],
      "abstract": "Vertical Symbolic Regression (VSR) recently has been proposed to expedite the\ndiscovery of symbolic equations with many independent variables from\nexperimental data. VSR reduces the search spaces following the vertical\ndiscovery path by building from reduced-form equations involving a subset of\nindependent variables to full-fledged ones. Proved successful by many symbolic\nregressors, deep neural networks are expected to further scale up VSR.\nNevertheless, directly combining VSR with deep neural networks will result in\ndifficulty in passing gradients and other engineering issues. We propose\nVertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and\ndemonstrate that VSR-DPG can recover ground-truth equations involving multiple\ninput variables, significantly beyond both deep reinforcement learning-based\napproaches and previous VSR variants. Our VSR-DPG models symbolic regression as\na sequential decision-making process, in which equations are built from\nrepeated applications of grammar rules. The integrated deep model is trained to\nmaximize a policy gradient objective. Experimental results demonstrate that our\nVSR-DPG significantly outperforms popular baselines in identifying both\nalgebraic equations and ordinary differential equations on a series of\nbenchmarks.",
      "tldr_zh": "本文提出 Vertical Symbolic Regression via Deep Policy Gradient (VSR-DPG)，一种结合深度神经网络的方法，用于从实验数据中快速发现涉及多个独立变量的符号方程。VSR-DPG 将符号回归建模为顺序决策过程，通过重复应用语法规则逐步构建方程，并使用深度策略梯度目标进行训练，以解决传统 VSR 的梯度传递问题。实验结果显示，该方法在识别代数方程和常微分方程的基准测试中，显著优于现有基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "see animated demo at: vsr-dpg.github.io",
      "pdf_url": "http://arxiv.org/pdf/2402.00254v1",
      "published_date": "2024-02-01 00:54:48 UTC",
      "updated_date": "2024-02-01 00:54:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:20:18.253883"
    },
    {
      "arxiv_id": "2402.00251v1",
      "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Walter Talbott",
        "Jian Zhang"
      ],
      "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining\nattention in AI agent development. This paper focuses on decision planning with\nuncertainty estimation to address the hallucination problem in language models.\nExisting approaches are either white-box or computationally demanding, limiting\nuse of black-box proprietary LLMs within budgets. The paper's first\ncontribution is a non-parametric uncertainty quantification method for LLMs,\nefficiently estimating point-wise dependencies between input-decision on the\nfly with a single inference, without access to token logits. This estimator\ninforms the statistical interpretation of decision trustworthiness. The second\ncontribution outlines a systematic design for a decision-making agent,\ngenerating actions like ``turn on the bathroom light'' based on user prompts\nsuch as ``take a bath''. Users will be asked to provide preferences when more\nthan one action has high estimated point-wise dependencies. In conclusion, our\nuncertainty estimation and decision-making agent design offer a cost-efficient\napproach for AI agent development.",
      "tldr_zh": "这篇论文提出了一种高效的非参数不确定性量化方法，用于黑盒大型语言模型 (LLMs)，通过单次推理估计输入-决策之间的点式依赖，而无需访问 token logits，从而解决语言模型的幻觉问题。该方法为决策可信度提供了统计解释，并作为论文的第一个贡献。第二个贡献是设计了一个系统化的决策代理，能基于用户提示生成具体动作（如“turn on the bathroom light”），并在多个高依赖动作时询问用户偏好。总体上，这种方法为 AI 代理开发提供了成本高效且实用的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00251v1",
      "published_date": "2024-02-01 00:23:31 UTC",
      "updated_date": "2024-02-01 00:23:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:20:31.822068"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 101,
  "processed_papers_count": 101,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T02:20:54.200406"
}
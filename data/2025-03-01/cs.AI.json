{
  "date": "2025-03-01",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-01 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 73 篇论文，主要聚焦 AI 模型的安全性、效率优化、多模态应用和生物医学创新，亮点包括 LLM 在幻觉检测和医疗模拟中的突破性框架，以及脑基础模型的全面调查；有影响力的工作如 Yarin Gal 参与的医疗 AI 研究和 Heng Huang 的多模态学习方法，强调了 AI 在实际应用中的鲁棒性和可解释性。\n\n### 重点论文讨论\n我们先聊聊那些重要、话题度高或有潜在影响的论文，将相关主题归类讨论。其他次要论文会快速掠过。\n\n#### AI 安全与幻觉检测\n这些论文解决 LLM 的安全风险，如幻觉（hallucination）和后门攻击，关系到 AI 部署的可靠性。\n- **BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge（BadJudge: LLM-as-a-Judge 的后门漏洞）**  \n  作者包括 Muhao Chen，这篇论文揭示了 LLM 在评估任务中的后门威胁，通过微调数据注入后门攻击，导致评分偏差。主要贡献是证明即使在弱假设下，后门也能显著放大分数（高达 89% 的误分类率），并提出模型合并方法缓解攻击，强调 AI 评估系统的潜在风险。\n  \n- **Steer LLM Latents for Hallucination Detection（通过引导 LLM 潜在空间检测幻觉）**  \n  作者包括 Yixuan Li，这篇工作提出 Truthfulness Separator Vector（TSV），一个轻量级框架，用于在 LLM 生成中分离真实和幻觉内容。主要发现是通过两阶段训练，TSV 在最小标注数据下实现最先进性能，提升泛化能力，适用于真实世界应用。\n\n- **Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable（安全税: 安全对齐让大型推理模型更不合理）**  \n  这篇论文展示安全对齐会降低 LLM 的推理能力，贡献包括实验证明安全对齐引入权衡，并提供新数据集 DirectRefusal 用于优化。快速掠过类似主题的论文，如其他安全讨论，它们重复强调了 LLM 鲁棒性的挑战。\n\n#### LLM 应用与优化\nLLM 在医疗和生成任务中的创新应用备受关注，这些工作提升了模型的实用性和效率。\n- **MedSimAI: Simulation and Formative Feedback Generation to Enhance Deliberate Practice in Medical Education（MedSimAI: 通过模拟和反馈生成增强医疗教育的 deliberate practice）**  \n  作者包括 Yarin Gal，这篇论文引入 AI 模拟平台 MedSimAI，用于医疗训练，提供实时反馈和自适应学习。主要发现是实验显示它提升了学生技能（如对话分析），在资源有限的环境中实现可扩展教育。\n\n- **Efficiently Editing Mixture-of-Experts Models with Compressed Experts（使用压缩专家高效编辑 Mixture-of-Experts 模型）**  \n  作者包括 Hany Hassan Awadalla，提出压缩专家概念，减少参数同时保持性能。主要贡献是实验证明在 Phi-MoE 等模型上，压缩后节省 20% 推理成本，恢复 90% 性能，适用于资源受限场景。\n\n- **Zero-Shot Keyphrase Generation: Investigating Specialized Instructions and Multi-Sample Aggregation on Large Language Models（零样本关键短语生成: 调查专业指令和多样本聚合在大型语言模型上的效果）**  \n  这篇工作探索 LLM 的零样本生成能力，通过指令优化和聚合策略提升性能。贡献包括在 Phi-3 和 GPT-4o 上显著改善关键短语任务，快速掠过其他生成相关论文，如 Podcast 生成，它们虽有趣但不那么突破性。\n\n#### 生物医学与多模态创新\n这些论文整合 AI 与生物医学，强调跨领域应用和模型鲁棒性。\n- **Brain Foundation Models: A Survey on Advancements in Neural Signal Processing and Brain Discovery（脑基础模型: 神经信号处理和脑发现的进展调查）**  \n  作者包括 Heng Huang，这篇综述首次定义脑基础模型（BFMs），覆盖预训练技术在神经科学中的应用。主要发现是讨论了模型构建框架和挑战，如数据质量和泛化，提供了未来方向。\n\n- **MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention（MIRROR: 通过模态对齐和保留的多模态病理自监督表示学习）**  \n  这篇论文提出 MIRROR 框架，用于整合组织病理和转录组数据。主要贡献是通过模态对齐提升癌症亚型和生存分析的准确性，实验在 TCGA 数据集上表现优异。\n\n其他生物医学论文，如 Thyroid Scintigraphy 的 AI 优化，快速掠过，它们虽实用但影响力较小。\n\n#### 其他高效 AI 方法\n快速提及一些高效优化的论文，它们虽不那么热门，但有实际价值。\n- **Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving（渐进式稀疏注意力: 用于 LLM 服务的高效算法和系统联合设计）**  \n  提出 PSA 机制，减少 KV 缓存访问，提升 LLM 推理效率。主要发现是实验显示性能提升 2.4 倍。\n\n- **LoR2C: Low-Rank Residual Connection Adaptation for Parameter-Efficient Fine-Tuning（LoR2C: 用于参数高效微调的低秩残差连接适应）**  \n  引入低秩矩阵减少参数，同时缓解梯度消失问题。贡献包括在多种任务上优于现有方法。\n\n剩余论文，如量子网络、强化学习或特定领域优化（例如 24 篇函数 Bandit 问题），由于主题较 niche 或重复性强，仅快速掠过：它们探讨了从 Bandit 算法到机器人规划的优化，但未有重大突破，不影响整体快报。\n\n总之，今天的 arXiv 更新突显 AI 向安全和实际应用倾斜，值得关注的工作如 BadJudge 和 MedSimAI，可能推动行业标准。更多细节可查阅具体论文。明天见！",
  "papers": [
    {
      "arxiv_id": "2503.00664v1",
      "title": "Generative Artificial Intelligence for Academic Research: Evidence from Guidance Issued for Researchers by Higher Education Institutions in the United States",
      "title_zh": "翻译失败",
      "authors": [
        "Amrita Ganguly",
        "Aditya Johri",
        "Areej Ali",
        "Nora McDonald"
      ],
      "abstract": "The recent development and use of generative AI (GenAI) has signaled a\nsignificant shift in research activities such as brainstorming, proposal\nwriting, dissemination, and even reviewing. This has raised questions about how\nto balance the seemingly productive uses of GenAI with ethical concerns such as\nauthorship and copyright issues, use of biased training data, lack of\ntransparency, and impact on user privacy. To address these concerns, many\nHigher Education Institutions (HEIs) have released institutional guidance for\nresearchers. To better understand the guidance that is being provided we report\nfindings from a thematic analysis of guidelines from thirty HEIs in the United\nStates that are classified as R1 or 'very high research activity.' We found\nthat guidance provided to researchers: (1) asks them to refer to external\nsources of information such as funding agencies and publishers to keep updated\nand use institutional resources for training and education; (2) asks them to\nunderstand and learn about specific GenAI attributes that shape research such\nas predictive modeling, knowledge cutoff date, data provenance, and model\nlimitations, and educate themselves about ethical concerns such as authorship,\nattribution, privacy, and intellectual property issues; and (3) includes\ninstructions on how to acknowledge sources and disclose the use of GenAI, how\nto communicate effectively about their GenAI use, and alerts researchers to\nlong term implications such as over reliance on GenAI, legal consequences, and\nrisks to their institutions from GenAI use. Overall, guidance places the onus\nof compliance on individual researchers making them accountable for any lapses,\nthereby increasing their responsibility.",
      "tldr_zh": "这篇论文通过对美国 30 所 R1 级高等教育机构 (HEIs) 发布的指导文件进行主题分析，探讨了生成式 AI (GenAI) 在学术研究中的应用及其伦理挑战。分析发现，这些指导要求研究人员参考外部资源（如资助机构和出版商）进行培训，了解 GenAI 的关键属性（如预测建模、知识截止日期、数据来源和模型限制），并关注伦理问题（如作者归属、隐私和知识产权）。此外，指导强调研究人员需正确披露 GenAI 使用、有效沟通潜在风险（如过度依赖和法律后果），并承担个人合规责任，以促进负责任的 AI 应用。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00664v1",
      "published_date": "2025-03-01 23:34:02 UTC",
      "updated_date": "2025-03-01 23:34:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:34:04.577742"
    },
    {
      "arxiv_id": "2503.00643v1",
      "title": "Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yante Li",
        "Hanwen Qi",
        "Haoyu Chen",
        "Xinlian Liang",
        "Guoying Zhao"
      ],
      "abstract": "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies.",
      "tldr_zh": "这篇论文针对树木监测的挑战，引入了UAVTC数据集，这是一个大规模、长期、高分辨率的无人机采集数据集，包含基于生物知识的丰富标注，用于检测细粒度Tree Changes (TCs)。为了有效处理环境影响和树木生理变化的层次多样性，作者提出Hyperbolic Siamese Network (HSN)，一个新型框架，利用双曲空间实现紧凑的层次表示，从而提升TC检测的准确性。实验结果表明，HSN在细粒度TC检测中表现出色，并能泛化到跨领域任务如人脸反欺骗，提供了一个鲁棒的解决方案。该工作结合生态洞见和AI技术，为环境监测设立了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.00643v1",
      "published_date": "2025-03-01 22:29:29 UTC",
      "updated_date": "2025-03-01 22:29:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:34:16.240127"
    },
    {
      "arxiv_id": "2503.00634v1",
      "title": "Efficiently Editing Mixture-of-Experts Models with Compressed Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei He",
        "Yang Liu",
        "Chen Liang",
        "Hany Hassan Awadalla"
      ],
      "abstract": "Mixture-of-Experts (MoE) models have become a key approach for scaling large\nlanguage models efficiently by activating only a subset of experts during\ntraining and inference. Typically, the number of activated experts presents a\ntrade-off: fewer experts reduce computational costs, while more experts improve\nperformance. Recent studies reveal that not all activated experts contribute\nequally to model performance, with some providing minimal utility, particularly\nwhen finetuning pretrained MoE models for specialized downstream tasks. The\nco-existence of significant and redundant parameters in experts provides us an\nopportunity to reduce the number of activated experts while maintaining model\nperformance. In this work, we propose the concept of compressed experts,\nlightweight modules that serve as compact representations of full experts. Our\napproach preserves the most important experts while replacing other auxiliary\nactivated experts with compressed experts. The reduction of active parameters\nsignificantly lowers inference costs while achieving comparable performance.\nExtensive experiments on models including Phi-MoE and OLMoE demonstrate that\ncompressed experts recover over 90% of full expert performance across various\ntasks while reducing more than 30% active parameters and saving 20% in\ninference costs. This approach enables efficient deployment of MoE models in\nresource-constrained settings and facilitates scaling to larger models with\nmanageable overhead. Our code is available at\nhttps://github.com/yifei-he/Compressed-Experts.",
      "tldr_zh": "该研究针对Mixture-of-Experts (MoE)模型中激活专家的权衡问题，提出了一种使用compressed experts的编辑方法，以减少冗余参数同时维持性能。方法通过保留最重要的专家，并用轻量级compressed experts替换其他辅助激活专家，从而降低活跃参数超过30%并节省20%的推理成本。在Phi-MoE和OLMoE模型上的实验显示，这种方法在各种任务中恢复了超过90%的完整专家性能。该创新有助于在资源受限环境中高效部署MoE模型，并支持更大规模模型的扩展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00634v1",
      "published_date": "2025-03-01 22:00:03 UTC",
      "updated_date": "2025-03-01 22:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:34:27.699384"
    },
    {
      "arxiv_id": "2503.00624v1",
      "title": "An evaluation of DeepSeek Models in Biomedical Natural Language Processing",
      "title_zh": "DeepSeek 模型在生物医学自然语言处理中的评估",
      "authors": [
        "Zaifu Zhan",
        "Shuang Zhou",
        "Huixue Zhou",
        "Jiawen Deng",
        "Yu Hou",
        "Jeremy Yeung",
        "Rui Zhang"
      ],
      "abstract": "The advancement of Large Language Models (LLMs) has significantly impacted\nbiomedical Natural Language Processing (NLP), enhancing tasks such as named\nentity recognition, relation extraction, event extraction, and text\nclassification. In this context, the DeepSeek series of models have shown\npromising potential in general NLP tasks, yet their capabilities in the\nbiomedical domain remain underexplored. This study evaluates multiple DeepSeek\nmodels (Distilled-DeepSeek-R1 series and Deepseek-LLMs) across four key\nbiomedical NLP tasks using 12 datasets, benchmarking them against\nstate-of-the-art alternatives (Llama3-8B, Qwen2.5-7B, Mistral-7B, Phi-4-14B,\nGemma-2-9B). Our results reveal that while DeepSeek models perform\ncompetitively in named entity recognition and text classification, challenges\npersist in event and relation extraction due to precision-recall trade-offs. We\nprovide task-specific model recommendations and highlight future research\ndirections. This evaluation underscores the strengths and limitations of\nDeepSeek models in biomedical NLP, guiding their future deployment and\noptimization.",
      "tldr_zh": "这篇论文评估了 DeepSeek 系列模型（包括 Distilled-DeepSeek-R1 系列和 Deepseek-LLMs）在生物医学 Natural Language Processing (NLP) 领域的性能，涵盖命名实体识别、关系提取、事件提取和文本分类等四个关键任务，使用了 12 个数据集与 Llama3-8B、Qwen2.5-7B 等基准模型进行比较。结果显示，DeepSeek 模型在命名实体识别和文本分类任务中表现出色，具有竞争力，但事件和关系提取任务面临精确率和召回率的权衡问题。论文提供了任务特定的模型推荐，并强调了 DeepSeek 模型的优势和局限性，以指导其在生物医学 NLP 中的未来优化和部署。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Plan to submit to AMIA 2025 Annual Symposium. 10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.00624v1",
      "published_date": "2025-03-01 21:26:29 UTC",
      "updated_date": "2025-03-01 21:26:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:34:42.194123"
    },
    {
      "arxiv_id": "2503.01919v2",
      "title": "Reinforcement learning with combinatorial actions for coupled restless bandits",
      "title_zh": "翻译失败",
      "authors": [
        "Lily Xu",
        "Bryan Wilder",
        "Elias B. Khalil",
        "Milind Tambe"
      ],
      "abstract": "Reinforcement learning (RL) has increasingly been applied to solve real-world\nplanning problems, with progress in handling large state spaces and time\nhorizons. However, a key bottleneck in many domains is that RL methods cannot\naccommodate large, combinatorially structured action spaces. In such settings,\neven representing the set of feasible actions at a single step may require a\ncomplex discrete optimization formulation. We leverage recent advances in\nembedding trained neural networks into optimization problems to propose\nSEQUOIA, an RL algorithm that directly optimizes for long-term reward over the\nfeasible action space. Our approach embeds a Q-network into a mixed-integer\nprogram to select a combinatorial action in each timestep. Here, we focus on\nplanning over restless bandits, a class of planning problems which capture many\nreal-world examples of sequential decision making. We introduce coRMAB, a\nbroader class of restless bandits with combinatorial actions that cannot be\ndecoupled across the arms of the restless bandit, requiring direct solving over\nthe joint, exponentially large action space. We empirically validate SEQUOIA on\nfour novel restless bandit problems with combinatorial constraints: multiple\ninterventions, path constraints, bipartite matching, and capacity constraints.\nOur approach significantly outperforms existing methods -- which cannot address\nsequential planning and combinatorial selection simultaneously -- by an average\nof 24.8\\% on these difficult instances.",
      "tldr_zh": "该论文探讨了Reinforcement learning（RL）在处理大型组合动作空间时的挑战，提出了一种名为SEQUOIA的算法，通过将Q-network嵌入混合整数程序（mixed-integer program）来直接优化长期奖励，确保在每个时间步选择可行的组合动作。论文扩展了restless bandits问题，引入了更广泛的coRMAB类，该类涉及无法解耦的组合动作，需要在指数级联合动作空间中求解。实验结果显示，SEQUOIA在四个新颖的restless bandit问题（包括multiple interventions、path constraints、bipartite matching和capacity constraints）上，比现有方法平均提高了24.8%的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear at ICLR 2025. Code at\n  https://github.com/lily-x/combinatorial-rmab",
      "pdf_url": "http://arxiv.org/pdf/2503.01919v2",
      "published_date": "2025-03-01 21:25:21 UTC",
      "updated_date": "2025-03-17 22:59:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:34:52.087072"
    },
    {
      "arxiv_id": "2503.00619v1",
      "title": "PinLanding: Content-First Keyword Landing Page Generation via Multi-Modal AI for Web-Scale Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Faye Zhang",
        "Jasmine Wan",
        "Qianyu Cheng",
        "Jinfeng Rao"
      ],
      "abstract": "Online platforms like Pinterest hosting vast content collections\ntraditionally rely on manual curation or user-generated search logs to create\nkeyword landing pages (KLPs) -- topic-centered collection pages that serve as\nentry points for content discovery. While manual curation ensures quality, it\ndoesn't scale to millions of collections, and search log approaches result in\nlimited topic coverage and imprecise content matching. In this paper, we\npresent PinLanding, a novel content-first architecture that transforms the way\nplatforms create topical collections. Instead of deriving topics from user\nbehavior, our system employs a multi-stage pipeline combining vision-language\nmodel (VLM) for attribute extraction, large language model (LLM) for topic\ngeneration, and a CLIP-based dual-encoder architecture for precise content\nmatching. Our model achieves 99.7% Recall@10 on Fashion200K benchmark,\ndemonstrating strong attribute understanding capabilities. In production\ndeployment for search engine optimization with 4.2 million shopping landing\npages, the system achieves a 4X increase in topic coverage and 14.29%\nimprovement in collection attribute precision over the traditional search\nlog-based approach via human evaluation. The architecture can be generalized\nbeyond search traffic to power various user experiences, including content\ndiscovery and recommendations, providing a scalable solution to transform\nunstructured content into curated topical collections across any content\ndomain.",
      "tldr_zh": "本研究提出PinLanding，一种以内容为先的架构，用于大规模生成关键词着陆页(KLPs)，以解决传统手动策划或基于搜索日志方法的扩展性和精确度问题。该系统采用多阶段管道，包括视觉语言模型(VLM)进行属性提取、大语言模型(LLM)生成主题，以及CLIP-based dual-encoder架构实现精确内容匹配。在Fashion200K基准测试中，模型实现了99.7%的Recall@10，展示了强大的属性理解能力；在实际部署中，处理420万购物着陆页后，主题覆盖率提高了4倍，集合属性精确度提升14.29%。PinLanding的架构可泛化应用于内容发现和推荐等领域，提供可扩展的解决方案，将非结构化内容转化为高质量的主题集合。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00619v1",
      "published_date": "2025-03-01 20:55:28 UTC",
      "updated_date": "2025-03-01 20:55:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:35:02.839730"
    },
    {
      "arxiv_id": "2503.00611v1",
      "title": "Modeling Arbitrarily Applicable Relational Responding with the Non-Axiomatic Reasoning System: A Machine Psychology Approach",
      "title_zh": "使用",
      "authors": [
        "Robert Johansson"
      ],
      "abstract": "Arbitrarily Applicable Relational Responding (AARR) is a cornerstone of human\nlanguage and reasoning, referring to the learned ability to relate symbols in\nflexible, context-dependent ways. In this paper, we present a novel theoretical\napproach for modeling AARR within an artificial intelligence framework using\nthe Non-Axiomatic Reasoning System (NARS). NARS is an adaptive reasoning system\ndesigned for learning under uncertainty. By integrating principles from\nRelational Frame Theory - the behavioral psychology account of AARR - with the\nreasoning mechanisms of NARS, we conceptually demonstrate how key properties of\nAARR (mutual entailment, combinatorial entailment, and transformation of\nstimulus functions) can emerge from the inference rules and memory structures\nof NARS. Two theoretical experiments illustrate this approach: one modeling\nstimulus equivalence and transfer of function, and another modeling complex\nrelational networks involving opposition frames. In both cases, the system\nlogically demonstrates the derivation of untrained relations and\ncontext-sensitive transformations of stimulus significance, mirroring\nestablished human cognitive phenomena. These results suggest that AARR - long\nconsidered uniquely human - can be conceptually captured by suitably designed\nAI systems, highlighting the value of integrating behavioral science insights\ninto artificial general intelligence (AGI) research.",
      "tldr_zh": "本研究提出了一种使用 Non-Axiomatic Reasoning System (NARS) 来建模 Arbitrarily Applicable Relational Responding (AARR) 的新理论方法，其中 AARR 是人类语言和推理的核心，涉及灵活的符号关联。论文将 Relational Frame Theory 的原则与 NARS 的推理机制和记忆结构整合，展示了 AARR 的关键属性（如 mutual entailment、combinatorial entailment 和 transformation of stimulus functions）如何从系统中浮现。两个理论实验模拟了刺激等价、函数转移以及复杂关系网络，证明了系统能推导未训练的关系和上下文敏感的刺激转变。这些结果表明，AARR 可以被设计合适的 AI 系统捕捉，从而为 artificial general intelligence (AGI) 研究注入行为科学洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.00611v1",
      "published_date": "2025-03-01 20:37:11 UTC",
      "updated_date": "2025-03-01 20:37:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:35:15.406017"
    },
    {
      "arxiv_id": "2503.00610v1",
      "title": "Urban Safety Perception Through the Lens of Large Multimodal Models: A Persona-based Approach",
      "title_zh": "城市安全感知：通过大型多模态模型的视角，一种基于角色的方法",
      "authors": [
        "Ciro Beneduce",
        "Bruno Lepri",
        "Massimiliano Luca"
      ],
      "abstract": "Understanding how urban environments are perceived in terms of safety is\ncrucial for urban planning and policymaking. Traditional methods like surveys\nare limited by high cost, required time, and scalability issues. To overcome\nthese challenges, this study introduces Large Multimodal Models (LMMs),\nspecifically Llava 1.6 7B, as a novel approach to assess safety perceptions of\nurban spaces using street-view images. In addition, the research investigated\nhow this task is affected by different socio-demographic perspectives,\nsimulated by the model through Persona-based prompts. Without additional\nfine-tuning, the model achieved an average F1-score of 59.21% in classifying\nurban scenarios as safe or unsafe, identifying three key drivers of perceived\nunsafety: isolation, physical decay, and urban infrastructural challenges.\nMoreover, incorporating Persona-based prompts revealed significant variations\nin safety perceptions across the socio-demographic groups of age, gender, and\nnationality. Elder and female Personas consistently perceive higher levels of\nunsafety than younger or male Personas. Similarly, nationality-specific\ndifferences were evident in the proportion of unsafe classifications ranging\nfrom 19.71% in Singapore to 40.15% in Botswana. Notably, the model's default\nconfiguration aligned most closely with a middle-aged, male Persona. These\nfindings highlight the potential of LMMs as a scalable and cost-effective\nalternative to traditional methods for urban safety perceptions. While the\nsensitivity of these models to socio-demographic factors underscores the need\nfor thoughtful deployment, their ability to provide nuanced perspectives makes\nthem a promising tool for AI-driven urban planning.",
      "tldr_zh": "本研究利用 Large Multimodal Models (LMMs)，如 Llava 1.6 7B，通过街景图像评估城市安全感知，并引入 Persona-based prompts 来模拟不同社会人口统计视角（如年龄、性别、国籍），以克服传统调查方法的成本和可扩展性问题。模型在不需额外微调的情况下，平均 F1-score 达到 59.21%，并识别出隔离、物理衰败和城市基础设施挑战作为主要不安全驱动因素。结果显示，不同群体对安全感知存在显著差异，例如老人和女性感知更高不安全，而国籍差异导致不安全分类比例从新加坡的 19.71% 上升到博茨瓦纳的 40.15%。这些发现突显 LMMs 作为 AI 驱动城市规划的潜力，但需注意模型对社会人口统计因素的敏感性，以确保公平部署。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00610v1",
      "published_date": "2025-03-01 20:34:30 UTC",
      "updated_date": "2025-03-01 20:34:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:35:28.743489"
    },
    {
      "arxiv_id": "2503.02895v1",
      "title": "Adaptive Entanglement Routing with Deep Q-Networks in Quantum Networks",
      "title_zh": "基于深度 Q-网络的自适应纠缠路由在量子网络中",
      "authors": [
        "Lamarana Jallow",
        "Majid Iqbal Khan"
      ],
      "abstract": "The quantum internet holds transformative potential for global communication\nby harnessing the principles of quantum information processing. Despite\nsignificant advancements in quantum communication technologies, the efficient\ndistribution of critical resources, such as qubits, remains a persistent and\nunresolved challenge. Conventional approaches often fall short of achieving\noptimal resource allocation, underscoring the necessity for more effective\nsolutions. This study proposes a novel reinforcement learning-based adaptive\nentanglement routing framework designed to enable resource allocation tailored\nto the specific demands of quantum applications. The introduced QuDQN model\nutilizes reinforcement learning to optimize the management of quantum networks,\nallocate resources efficiently, and enhance entanglement routing. The model\nintegrates key considerations, including fidelity requirements, network\ntopology, qubit capacity, and request demands.",
      "tldr_zh": "本文针对量子网络中量子比特（qubits）分配的挑战，提出了一种基于强化学习的自适应纠缠路由框架，以优化资源管理。该框架引入QuDQN模型，利用Deep Q-Networks算法，考虑保真度要求（fidelity requirements）、网络拓扑（network topology）、qubit容量和请求需求，实现高效的纠缠路由。作为主要贡献，该方法为量子互联网的应用提供更有效的资源分配解决方案。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "14 pages, 10 images. To be submitted to Quantum joural, this is to\n  fullfill the requirements",
      "pdf_url": "http://arxiv.org/pdf/2503.02895v1",
      "published_date": "2025-03-01 20:05:54 UTC",
      "updated_date": "2025-03-01 20:05:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:35:39.585618"
    },
    {
      "arxiv_id": "2503.00600v1",
      "title": "Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander W. Lee",
        "Justin Chan",
        "Michael Fu",
        "Nicolas Kim",
        "Akshay Mehta",
        "Deepti Raghavan",
        "Ugur Cetintemel"
      ],
      "abstract": "The emergence of AI-augmented Data Processing Systems (DPSs) has introduced\npowerful semantic operators that extend traditional data management\ncapabilities with LLM-based processing. However, these systems face fundamental\nreliability (a.k.a. trust) challenges, as LLMs can generate erroneous outputs,\nlimiting their adoption in critical domains. Existing approaches to LLM\nconstraints--ranging from user-defined functions to constrained decoding--are\nfragmented, imperative, and lack semantics-aware integration into query\nexecution. To address this gap, we introduce Semantic Integrity Constraints\n(SICs), a novel declarative abstraction that extends traditional database\nintegrity constraints to govern and optimize semantic operators within DPSs.\nSICs integrate seamlessly into the relational model, allowing users to specify\ncommon classes of constraints (e.g., grounding and soundness) while enabling\nquery-aware enforcement and optimization strategies.\n  In this paper, we present the core design of SICs, describe their formal\nintegration into query execution, and detail our conception of grounding\nconstraints, a key SIC class that ensures factual consistency of generated\noutputs. In addition, we explore novel enforcement mechanisms, combining\nproactive (constrained decoding) and reactive (validation and recovery)\ntechniques to optimize efficiency and reliability. Our work establishes SICs as\na foundational framework for trustworthy, high-performance AI-augmented data\nprocessing, paving the way for future research in constraint-driven\noptimizations, adaptive enforcement, and enterprise-scale deployments.",
      "tldr_zh": "本论文提出 Semantic Integrity Constraints (SICs)，一种声明式抽象，用于解决 AI-augmented Data Processing Systems (DPSs) 中 LLM 生成输出错误导致的可靠性挑战。SICs 扩展了传统数据库完整性约束，允许用户指定 grounding 和 soundness 等约束，并无缝整合到关系模型中，支持查询感知的执行和优化策略。论文详细描述了 SICs 的核心设计、与查询执行的正式整合，以及结合主动（约束解码）和反应式（验证与恢复）机制，以提升系统效率和可靠性，为可信赖的 AI-augmented 数据处理系统奠定基础。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00600v1",
      "published_date": "2025-03-01 19:59:25 UTC",
      "updated_date": "2025-03-01 19:59:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:35:53.988393"
    },
    {
      "arxiv_id": "2503.00597v1",
      "title": "Zero-Shot Keyphrase Generation: Investigating Specialized Instructions and Multi-Sample Aggregation on Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jayanth Mohan",
        "Jishnu Ray Chowdhury",
        "Tomas Malik",
        "Cornelia Caragea"
      ],
      "abstract": "Keyphrases are the essential topical phrases that summarize a document.\nKeyphrase generation is a long-standing NLP task for automatically generating\nkeyphrases for a given document. While the task has been comprehensively\nexplored in the past via various models, only a few works perform some\npreliminary analysis of Large Language Models (LLMs) for the task. Given the\nimpact of LLMs in the field of NLP, it is important to conduct a more thorough\nexamination of their potential for keyphrase generation. In this paper, we\nattempt to meet this demand with our research agenda. Specifically, we focus on\nthe zero-shot capabilities of open-source instruction-tuned LLMs (Phi-3,\nLlama-3) and the closed-source GPT-4o for this task. We systematically\ninvestigate the effect of providing task-relevant specialized instructions in\nthe prompt. Moreover, we design task-specific counterparts to\nself-consistency-style strategies for LLMs and show significant benefits from\nour proposals over the baselines.",
      "tldr_zh": "这篇论文探讨了Large Language Models (LLMs) 在Zero-Shot关键短语生成任务中的潜力，焦点是开源模型如Phi-3和Llama-3，以及封闭源模型GPT-4o。研究系统调查了在提示中提供specialized instructions的影响，并设计了任务特定的multi-sample aggregation策略，以对应于self-consistency-style方法。结果表明，这些策略显著提升了LLMs的性能，超过了基线模型，为关键短语生成任务提供了新的优化路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00597v1",
      "published_date": "2025-03-01 19:38:57 UTC",
      "updated_date": "2025-03-01 19:38:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:36:06.915112"
    },
    {
      "arxiv_id": "2503.00596v1",
      "title": "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge",
      "title_zh": "BadJudge: LLM-as-a-Judge 的后门漏洞",
      "authors": [
        "Terry Tong",
        "Fei Wang",
        "Zhe Zhao",
        "Muhao Chen"
      ],
      "abstract": "This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge\nevaluation regime, where the adversary controls both the candidate and\nevaluator model. The backdoored evaluator victimizes benign users by unfairly\nassigning inflated scores to adversary. A trivial single token backdoor\npoisoning 1% of the evaluator training data triples the adversary's score with\nrespect to their legitimate score. We systematically categorize levels of data\naccess corresponding to three real-world settings, (1) web poisoning, (2)\nmalicious annotator, and (3) weight poisoning. These regimes reflect a weak to\nstrong escalation of data access that highly correlates with attack severity.\nUnder the weakest assumptions - web poisoning (1), the adversary still induces\na 20% score inflation. Likewise, in the (3) weight poisoning regime, the\nstronger assumptions enable the adversary to inflate their scores from 1.5/5 to\n4.9/5. The backdoor threat generalizes across different evaluator\narchitectures, trigger designs, evaluation tasks, and poisoning rates. By\npoisoning 10% of the evaluator training data, we control toxicity judges\n(Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and\ndocument reranker judges in RAG to rank the poisoned document first 97% of the\ntime. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and\ntechnology, where social implications of mislead model selection and evaluation\nconstrain the available defensive tools. Amidst these challenges, model merging\nemerges as a principled tool to offset the backdoor, reducing ASR to near 0%\nwhilst maintaining SOTA performance. Model merging's low computational cost and\nconvenient integration into the current LLM Judge training pipeline position it\nas a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.",
      "tldr_zh": "这篇论文揭示了LLM-as-a-Judge评估机制的背门漏洞（backdoor vulnerabilities），攻击者通过控制候选和评估模型来操控评分结果，导致对恶意内容的评分不公平。研究分类了三种数据访问场景——web poisoning、malicious annotator 和 weight poisoning——从弱到强地展示了攻击强度，在最弱的web poisoning下仍能使分数增加20%，而在weight poisoning下可将分数从1.5/5提升至4.9/5。实验证明，攻击适用于不同评估模型架构和任务，例如中毒10%的训练数据能让toxicity judges误分类毒性提示为非毒性的89%时间，并让RAG中的文档重新排序器将中毒文档排在第一的97%时间。论文提出模型合并（model merging）作为缓解策略，能将攻击成功率（ASR）降至近0%，同时保持SOTA性能，并强调其在伦理和技术交叉领域的潜在应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "Published to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00596v1",
      "published_date": "2025-03-01 19:35:01 UTC",
      "updated_date": "2025-03-01 19:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:36:22.487191"
    },
    {
      "arxiv_id": "2503.01917v2",
      "title": "Steer LLM Latents for Hallucination Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Seongheon Park",
        "Xuefeng Du",
        "Min-Hsuan Yeh",
        "Haobo Wang",
        "Yixuan Li"
      ],
      "abstract": "Hallucinations in LLMs pose a significant concern to their safe deployment in\nreal-world applications. Recent approaches have leveraged the latent space of\nLLMs for hallucination detection, but their embeddings, optimized for\nlinguistic coherence rather than factual accuracy, often fail to clearly\nseparate truthful and hallucinated content. To this end, we propose the\nTruthfulness Separator Vector (TSV), a lightweight and flexible steering vector\nthat reshapes the LLM's representation space during inference to enhance the\nseparation between truthful and hallucinated outputs, without altering model\nparameters. Our two-stage framework first trains TSV on a small set of labeled\nexemplars to form compact and well-separated clusters. It then augments the\nexemplar set with unlabeled LLM generations, employing an optimal\ntransport-based algorithm for pseudo-labeling combined with a confidence-based\nfiltering process. Extensive experiments demonstrate that TSV achieves\nstate-of-the-art performance with minimal labeled data, exhibiting strong\ngeneralization across datasets and providing a practical solution for\nreal-world LLM applications.",
      "tldr_zh": "这篇论文针对大型语言模型(LLM)中的幻觉(hallucinations)问题，提出Truthfulness Separator Vector (TSV)，一个轻量级转向向量，用于重塑LLM的潜在空间(latent space)，以更好地分离真实和幻觉输出，而不改变模型参数。TSV采用两阶段框架：首先在少量标记样本上训练以形成紧凑分离簇，然后通过基于最优传输(optimal transport)的算法对无标记数据进行伪标记，并结合置信度过滤增强样本集。实验结果表明，TSV在最小标记数据下实现最先进性能，并在不同数据集上表现出强泛化能力，为实际LLM应用提供高效的检测解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.01917v2",
      "published_date": "2025-03-01 19:19:34 UTC",
      "updated_date": "2025-05-22 00:33:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:36:32.218670"
    },
    {
      "arxiv_id": "2503.00583v1",
      "title": "Space-Time Graphs of Convex Sets for Multi-Robot Motion Planning",
      "title_zh": "用于多机器人运动规划的凸集空间-时间图",
      "authors": [
        "Jingtao Tang",
        "Zining Mao",
        "Lufan Yang",
        "Hang Ma"
      ],
      "abstract": "We address the Multi-Robot Motion Planning (MRMP) problem of computing\ncollision-free trajectories for multiple robots in shared continuous\nenvironments. While existing frameworks effectively decompose MRMP into\nsingle-robot subproblems, spatiotemporal motion planning with dynamic obstacles\nremains challenging, particularly in cluttered or narrow-corridor settings. We\npropose Space-Time Graphs of Convex Sets (ST-GCS), a novel planner that\nsystematically covers the collision-free space-time domain with convex sets\ninstead of relying on random sampling. By extending Graphs of Convex Sets (GCS)\ninto the time dimension, ST-GCS formulates time-optimal trajectories in a\nunified convex optimization that naturally accommodates velocity bounds and\nflexible arrival times. We also propose Exact Convex Decomposition (ECD) to\n\"reserve\" trajectories as spatiotemporal obstacles, maintaining a\ncollision-free space-time graph of convex sets for subsequent planning.\nIntegrated into two prioritized-planning frameworks, ST-GCS consistently\nachieves higher success rates and better solution quality than state-of-the-art\nsampling-based planners -- often at orders-of-magnitude faster runtimes --\nunderscoring its benefits for MRMP in challenging settings.",
      "tldr_zh": "本文提出了一种名为 Space-Time Graphs of Convex Sets (ST-GCS) 的新规划器，用于解决 Multi-Robot Motion Planning (MRMP) 问题，特别是在存在动态障碍物的拥挤环境中。ST-GCS 通过将 Graphs of Convex Sets (GCS) 扩展到时间维度，系统地用凸集覆盖无碰撞时空域，形成统一的凸优化问题，以生成时间最优轨迹并支持速度限制和灵活到达时间。同时，引入 Exact Convex Decomposition (ECD) 来预留轨迹作为时空障碍物，确保后续规划的无碰撞。实验结果表明，ST-GCS 集成到优先级规划框架中，比现有采样-based 规划器实现了更高的成功率、更优的解决方案质量，且运行时间快几个数量级。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "submitted to IROS'25",
      "pdf_url": "http://arxiv.org/pdf/2503.00583v1",
      "published_date": "2025-03-01 18:28:57 UTC",
      "updated_date": "2025-03-01 18:28:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:36:43.445171"
    },
    {
      "arxiv_id": "2503.00580v1",
      "title": "Brain Foundation Models: A Survey on Advancements in Neural Signal Processing and Brain Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Xinliang Zhou",
        "Chenyu Liu",
        "Zhisheng Chen",
        "Kun Wang",
        "Yi Ding",
        "Ziyu Jia",
        "Qingsong Wen"
      ],
      "abstract": "Brain foundation models (BFMs) have emerged as a transformative paradigm in\ncomputational neuroscience, offering a revolutionary framework for processing\ndiverse neural signals across different brain-related tasks. These models\nleverage large-scale pre-training techniques, allowing them to generalize\neffectively across multiple scenarios, tasks, and modalities, thus overcoming\nthe traditional limitations faced by conventional artificial intelligence (AI)\napproaches in understanding complex brain data. By tapping into the power of\npretrained models, BFMs provide a means to process neural data in a more\nunified manner, enabling advanced analysis and discovery in the field of\nneuroscience. In this survey, we define BFMs for the first time, providing a\nclear and concise framework for constructing and utilizing these models in\nvarious applications. We also examine the key principles and methodologies for\ndeveloping these models, shedding light on how they transform the landscape of\nneural signal processing. This survey presents a comprehensive review of the\nlatest advancements in BFMs, covering the most recent methodological\ninnovations, novel views of application areas, and challenges in the field.\nNotably, we highlight the future directions and key challenges that need to be\naddressed to fully realize the potential of BFMs. These challenges include\nimproving the quality of brain data, optimizing model architecture for better\ngeneralization, increasing training efficiency, and enhancing the\ninterpretability and robustness of BFMs in real-world applications.",
      "tldr_zh": "本调查首次定义了 Brain Foundation Models (BFMs)，一种革命性的计算神经科学框架，利用大规模预训练技术处理多样神经信号，实现跨任务和模态的泛化，从而克服传统 AI 在脑数据分析中的局限性。BFMs 通过统一处理神经数据，促进高级分析和神经科学发现，涵盖了方法创新、应用领域（如神经信号处理和脑发现）以及最新进展。该研究还讨论了关键挑战，包括提升脑数据质量、优化模型架构、提高训练效率，以及增强 BFMs 的可解释性和鲁棒性，并指出了未来发展方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00580v1",
      "published_date": "2025-03-01 18:12:50 UTC",
      "updated_date": "2025-03-01 18:12:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:36:53.080354"
    },
    {
      "arxiv_id": "2503.00572v1",
      "title": "LoR2C : Low-Rank Residual Connection Adaptation for Parameter-Efficient Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiancheng Zhao",
        "Xingda Yu",
        "Yuxiang Zhang",
        "Zhen Yang"
      ],
      "abstract": "In recent years, pretrained large language models have demonstrated\noutstanding performance across various natural language processing tasks.\nHowever, full-parameter fine-tuning methods require adjusting all model\nparameters, leading to immense computational resource demands. Although\nparameter-efficient fine-tuning methods like LoRA have significantly reduced\nthe number of parameters, they still face challenges such as gradient vanishing\nand the potential for further parameter reduction. To address these issues,\nthis paper proposes a novel parameter-efficient fine-tuning method called LoR2C\n(Low-Rank Residual Connection Adaptation). LoR2C introduces residual\nconnections with low-rank matrices within the model layers, which not only\nreduces the number of fine-tuning parameters but also effectively alleviates\nthe gradient vanishing problem. Additionally, this paper presents three\noptimization variants of LoR2C: ShareLoR2C, MergeLoR2C, and InjectLoR2C. These\nvariants further improve parameter efficiency and model performance through\nparameter sharing, module merging, and injection mechanisms, respectively.\nExperimental results on multiple natural language understanding and natural\nlanguage generation tasks demonstrate that LoR2C and its optimized variants\nsignificantly reduce parameter overhead while maintaining or even improving\nperformance, outperforming existing mainstream parameter-efficient fine-tuning\nmethods.Our code is publicly available at https://github.com/Oblivioniss/LoR2C.",
      "tldr_zh": "本论文提出了一种新型参数高效微调方法LoR2C（Low-Rank Residual Connection Adaptation），旨在解决预训练大语言模型全参数微调资源消耗大以及现有方法如LoRA的梯度消失问题。LoR2C通过在模型层中引入低秩矩阵的残差连接，显著减少微调参数数量并缓解梯度消失，同时引入三个优化变体：ShareLoR2C（参数共享）、MergeLoR2C（模块合并）和InjectLoR2C（注入机制），进一步提升效率和性能。在多种自然语言理解和生成任务的实验中，LoR2C及其变体在减少参数开销的同时，维持或超越了现有主流方法的表现，代码已开源于https://github.com/Oblivioniss/LoR2C。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00572v1",
      "published_date": "2025-03-01 17:42:57 UTC",
      "updated_date": "2025-03-01 17:42:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:37:08.043323"
    },
    {
      "arxiv_id": "2503.00566v1",
      "title": "Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires",
      "title_zh": "翻译失败",
      "authors": [
        "Kyle Gao",
        "Dening Lu",
        "Liangzhi Li",
        "Nan Chen",
        "Hongjie He",
        "Linlin Xu",
        "Jonathan Li"
      ],
      "abstract": "The Los Angeles wildfires of January 2025 caused more than 250 billion\ndollars in damage and lasted for nearly an entire month before containment.\nFollowing our previous work, the Digital Twin Building, we modify and leverage\nthe multi-agent large language model framework as well as the cloud-mapping\nintegration to study the air quality during the Los Angeles wildfires. Recent\nadvances in large language models have allowed for out-of-the-box automated\nlarge-scale data analysis. We use a multi-agent large language system comprised\nof an Instructor agent and Worker agents. Upon receiving the users'\ninstructions, the Instructor agent retrieves the data from the cloud platform\nand produces instruction prompts to the Worker agents. The Worker agents then\nanalyze the data and provide summaries. The summaries are finally input back\ninto the Instructor agent, which then provides the final data analysis. We test\nthis system's capability for data-based policy recommendation by assessing our\nInstructor-Worker LLM system's health recommendations based on air quality\nduring the Los Angeles wildfires.",
      "tldr_zh": "该论文提出了一种Instructor-Worker大型语言模型(LLM)系统，用于基于数据进行政策推荐，并以2025年1月洛杉矶野火的空气质量分析为例。系统采用多智能体框架，其中Instructor agent负责接收用户指令、从云平台检索数据并生成提示，而Worker agents则分析数据并提供总结。最终，Instructor agent整合总结输出进行最终分析。该方法基于先前Digital Twin Building工作扩展，实验显示系统在评估野火期间空气质量并生成健康推荐方面表现出色，为自动化大规模数据分析和政策决策提供了可行框架。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00566v1",
      "published_date": "2025-03-01 17:29:26 UTC",
      "updated_date": "2025-03-01 17:29:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:37:20.731990"
    },
    {
      "arxiv_id": "2503.00563v1",
      "title": "A Guide to Failure in Machine Learning: Reliability and Robustness from Foundations to Practice",
      "title_zh": "翻译失败",
      "authors": [
        "Eric Heim",
        "Oren Wright",
        "David Shriver"
      ],
      "abstract": "One of the main barriers to adoption of Machine Learning (ML) is that ML\nmodels can fail unexpectedly. In this work, we aim to provide practitioners a\nguide to better understand why ML models fail and equip them with techniques\nthey can use to reason about failure. Specifically, we discuss failure as\neither being caused by lack of reliability or lack of robustness.\nDifferentiating the causes of failure in this way allows us to formally define\nwhy models fail from first principles and tie these definitions to engineering\nconcepts and real-world deployment settings. Throughout the document we provide\n1) a summary of important theoretic concepts in reliability and robustness, 2)\na sampling current techniques that practitioners can utilize to reason about ML\nmodel reliability and robustness, and 3) examples that show how these concepts\nand techniques can apply to real-world settings.",
      "tldr_zh": "这篇论文提供了一个指南，帮助机器学习(Machine Learning)从业者理解模型失败的原因，并使用相关技术进行推理。\n论文将失败分为可靠性(reliability)和鲁棒性(robustness)两大类，从第一原则出发定义这些问题，并将其与工程概念和实际部署场景相结合。\n内容包括可靠性与鲁棒性的理论总结、当前实用技术示例，以及真实世界应用的案例，以提升模型的可靠性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00563v1",
      "published_date": "2025-03-01 17:21:36 UTC",
      "updated_date": "2025-03-01 17:21:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:37:30.836983"
    },
    {
      "arxiv_id": "2503.00555v1",
      "title": "Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable",
      "title_zh": "翻译失败",
      "authors": [
        "Tiansheng Huang",
        "Sihao Hu",
        "Fatih Ilhan",
        "Selim Furkan Tekin",
        "Zachary Yahn",
        "Yichang Xu",
        "Ling Liu"
      ],
      "abstract": "Safety alignment is an important procedure before the official deployment of\na Large Language Model (LLM). While safety alignment has been extensively\nstudied for LLM, there is still a large research gap for Large Reasoning Models\n(LRMs) that equip with improved reasoning capability. We in this paper\nsystematically examine a simplified pipeline for producing safety aligned LRMs.\nWith our evaluation of various LRMs, we deliver two main findings: i) Safety\nalignment can be done upon the LRM to restore its safety capability. ii) Safety\nalignment leads to a degradation of the reasoning capability of LRMs. The two\nfindings show that there exists a trade-off between reasoning and safety\ncapability with the sequential LRM production pipeline. The discovered\ntrade-off, which we name Safety Tax, should shed light on future endeavors of\nsafety research on LRMs. As a by-product, we curate a dataset called\nDirectRefusal, which might serve as an alternative dataset for safety\nalignment. Our source code is available at\nhttps://github.com/git-disl/Safety-Tax.",
      "tldr_zh": "这篇论文探讨了安全对齐（safety alignment）对大型推理模型（LRMs）的负面影响，通过一个简化管道系统评估 LRMs 的性能。研究发现，安全对齐虽然能恢复模型的安全能力，但会显著降低其推理能力，导致一种权衡关系，即“Safety Tax”。作者强调，这种权衡对未来 LRMs 安全研究具有重要启发意义，并作为副产品整理了 DirectRefusal 数据集，用于替代的安全对齐训练。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00555v1",
      "published_date": "2025-03-01 16:42:01 UTC",
      "updated_date": "2025-03-01 16:42:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:37:43.003373"
    },
    {
      "arxiv_id": "2503.00539v1",
      "title": "Distributionally Robust Reinforcement Learning with Human Feedback",
      "title_zh": "带有人类反馈的分布鲁棒强化学习",
      "authors": [
        "Debmalya Mandal",
        "Paulius Sasnauskas",
        "Goran Radanovic"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has evolved to be one of\nthe main methods for fine-tuning large language models (LLMs). However,\nexisting RLHF methods are non-robust, and their performance deteriorates if the\ndownstream task differs significantly from the preference dataset used in\nfine-tuning. In order to mitigate this problem, we introduce a distributionally\nrobust RLHF for fine-tuning LLMs. In particular, our goal is to ensure that a\nfine-tuned model retains its performance even when the distribution of prompts\nsignificantly differs from the distribution encountered during fine-tuning. We\nformulate distributionally robust optimization (DRO) version of two popular\nfine-tuning methods -- (1) reward-based RLHF and (2) reward-free DPO (direct\npreference optimization). We propose a minibatch gradient descent based\nalgorithms for both of them, and theoretically prove convergence guarantees for\nthe algorithms. Subsequently, we evaluate our algorithms on an\nout-of-distribution (OOD) task by first training the model on the\nUnified-Feedback dataset and evaluating its performance on two different\ndatasets. The experimental results show that our robust training improves the\naccuracy of the learned reward models on average, and markedly on some tasks,\nsuch as reasoning. Furthermore, we show that the robust versions of policy\noptimization methods, similarly improve performance on OOD tasks.",
      "tldr_zh": "该论文针对强化学习与人类反馈（RLHF）在微调大型语言模型（LLMs）中的非鲁棒性问题，提出了一种分布鲁棒优化（DRO）框架，以确保模型在提示分布显著变化时仍保持性能。研究者为基于奖励的RLHF和无奖励的直接偏好优化（DPO）方法制定了DRO版本，并开发了基于小批量梯度下降的算法，同时证明了这些算法的收敛保证。通过在Unified-Feedback数据集上训练并在其他数据集上评估实验，结果显示鲁棒训练显著提高了奖励模型的准确性，尤其在推理等OOD（Out-of-Distribution）任务上。总之，该方法提升了RLHF在实际应用中的鲁棒性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00539v1",
      "published_date": "2025-03-01 15:43:39 UTC",
      "updated_date": "2025-03-01 15:43:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:37:54.373083"
    },
    {
      "arxiv_id": "2503.00535v1",
      "title": "What Makes a Good Diffusion Planner for Decision Making?",
      "title_zh": "翻译失败",
      "authors": [
        "Haofei Lu",
        "Dongqi Han",
        "Yifei Shen",
        "Dongsheng Li"
      ],
      "abstract": "Diffusion models have recently shown significant potential in solving\ndecision-making problems, particularly in generating behavior plans -- also\nknown as diffusion planning. While numerous studies have demonstrated the\nimpressive performance of diffusion planning, the mechanisms behind the key\ncomponents of a good diffusion planner remain unclear and the design choices\nare highly inconsistent in existing studies. In this work, we address this\nissue through systematic empirical experiments on diffusion planning in an\noffline reinforcement learning (RL) setting, providing practical insights into\nthe essential components of diffusion planning. We trained and evaluated over\n6,000 diffusion models, identifying the critical components such as guided\nsampling, network architecture, action generation and planning strategy. We\nrevealed that some design choices opposite to the common practice in previous\nwork in diffusion planning actually lead to better performance, e.g.,\nunconditional sampling with selection can be better than guided sampling and\nTransformer outperforms U-Net as denoising network. Based on these insights, we\nsuggest a simple yet strong diffusion planning baseline that achieves\nstate-of-the-art results on standard offline RL benchmarks.",
      "tldr_zh": "本文通过系统实验探讨了扩散模型（diffusion models）在决策中的扩散规划（diffusion planning）关键组件，旨在解决现有研究中设计选择不一致的问题。研究者在离线强化学习（offline RL）设置下训练并评估了超过 6,000 个扩散模型，识别了 guided sampling、网络架构、action generation 和 planning strategy 等核心要素。结果显示，一些与常见实践相反的设计选择（如无条件采样加选择优于 guided sampling，以及 Transformer 作为去噪网络优于 U-Net）能显著提升性能。最终，基于这些洞见，作者提出一个简单而强大的扩散规划基准，在标准离线 RL 基准上达到了最先进的结果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 (Spotlight), Code:\n  https://github.com/Josh00-Lu/DiffusionVeteran",
      "pdf_url": "http://arxiv.org/pdf/2503.00535v1",
      "published_date": "2025-03-01 15:31:14 UTC",
      "updated_date": "2025-03-01 15:31:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:38:07.536200"
    },
    {
      "arxiv_id": "2503.00527v1",
      "title": "Never too Prim to Swim: An LLM-Enhanced RL-based Adaptive S-Surface Controller for AUVs under Extreme Sea Conditions",
      "title_zh": "翻译失败",
      "authors": [
        "Guanwen Xie",
        "Jingzehua Xu",
        "Yimian Ding",
        "Zhi Zhang",
        "Shuai Zhang",
        "Yi Li"
      ],
      "abstract": "The adaptivity and maneuvering capabilities of Autonomous Underwater Vehicles\n(AUVs) have drawn significant attention in oceanic research, due to the\nunpredictable disturbances and strong coupling among the AUV's degrees of\nfreedom. In this paper, we developed large language model (LLM)-enhanced\nreinforcement learning (RL)-based adaptive S-surface controller for AUVs.\nSpecifically, LLMs are introduced for the joint optimization of controller\nparameters and reward functions in RL training. Using multi-modal and\nstructured explicit task feedback, LLMs enable joint adjustments, balance\nmultiple objectives, and enhance task-oriented performance and adaptability. In\nthe proposed controller, the RL policy focuses on upper-level tasks, outputting\ntask-oriented high-level commands that the S-surface controller then converts\ninto control signals, ensuring cancellation of nonlinear effects and\nunpredictable external disturbances in extreme sea conditions. Under extreme\nsea conditions involving complex terrain, waves, and currents, the proposed\ncontroller demonstrates superior performance and adaptability in high-level\ntasks such as underwater target tracking and data collection, outperforming\ntraditional PID and SMC controllers.",
      "tldr_zh": "本研究针对自主水下车辆 (AUVs) 在极端海况下的适应性和机动性挑战，提出了一种大型语言模型 (LLMs) 增强的强化学习 (RL) 基于的自适应 S-表面控制器。LLMs 用于 RL 训练中的控制器参数和奖励函数的联合优化，通过多模态任务反馈实现目标平衡和性能提升，使 RL 策略专注于高层任务并输出高级命令。S-表面控制器随后将这些命令转换为控制信号，以抵消非线性效应和外部干扰。在复杂海况下，该控制器在水下目标跟踪和数据收集等任务上表现出色，优于传统 PID 和 SMC 控制器。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.00527v1",
      "published_date": "2025-03-01 15:01:50 UTC",
      "updated_date": "2025-03-01 15:01:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:38:20.536233"
    },
    {
      "arxiv_id": "2503.00524v1",
      "title": "End-To-End Learning of Gaussian Mixture Priors for Diffusion Sampler",
      "title_zh": "翻译失败",
      "authors": [
        "Denis Blessing",
        "Xiaogang Jia",
        "Gerhard Neumann"
      ],
      "abstract": "Diffusion models optimized via variational inference (VI) have emerged as a\npromising tool for generating samples from unnormalized target densities. These\nmodels create samples by simulating a stochastic differential equation,\nstarting from a simple, tractable prior, typically a Gaussian distribution.\nHowever, when the support of this prior differs greatly from that of the target\ndistribution, diffusion models often struggle to explore effectively or suffer\nfrom large discretization errors. Moreover, learning the prior distribution can\nlead to mode-collapse, exacerbated by the mode-seeking nature of reverse\nKullback-Leibler divergence commonly used in VI. To address these challenges,\nwe propose end-to-end learnable Gaussian mixture priors (GMPs). GMPs offer\nimproved control over exploration, adaptability to target support, and\nincreased expressiveness to counteract mode collapse. We further leverage the\nstructure of mixture models by proposing a strategy to iteratively refine the\nmodel by adding mixture components during training. Our experimental results\ndemonstrate significant performance improvements across a diverse range of\nreal-world and synthetic benchmark problems when using GMPs without requiring\nadditional target evaluations.",
      "tldr_zh": "本文提出了一种端到端学习的高斯混合先验（Gaussian Mixture Priors, GMPs），用于优化扩散模型（Diffusion Models）的采样过程，以解决传统高斯先验在变分推断（VI）中导致的探索不足、离散化错误和模式崩溃问题。GMPs 通过提高先验的表达能力和适应性，并结合反向 Kullback-Leibler 散度优化，允许模型在训练中迭代添加混合组件以逐步改进性能。该方法在各种真实和合成基准问题上实现了显著性能提升，而无需额外的目标密度评估，为高效生成样本提供了更可靠的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00524v1",
      "published_date": "2025-03-01 14:58:14 UTC",
      "updated_date": "2025-03-01 14:58:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:38:32.141121"
    },
    {
      "arxiv_id": "2503.00509v1",
      "title": "Functional multi-armed bandit and the best function identification problems",
      "title_zh": "函数型多臂赌博机和最佳函数识别问题",
      "authors": [
        "Yuriy Dorn",
        "Aleksandr Katrutsa",
        "Ilgam Latypov",
        "Anastasiia Soboleva"
      ],
      "abstract": "Bandit optimization usually refers to the class of online optimization\nproblems with limited feedback, namely, a decision maker uses only the\nobjective value at the current point to make a new decision and does not have\naccess to the gradient of the objective function. While this name accurately\ncaptures the limitation in feedback, it is somehow misleading since it does not\nhave any connection with the multi-armed bandits (MAB) problem class. We\npropose two new classes of problems: the functional multi-armed bandit problem\n(FMAB) and the best function identification problem. They are modifications of\na multi-armed bandit problem and the best arm identification problem,\nrespectively, where each arm represents an unknown black-box function. These\nproblem classes are a surprisingly good fit for modeling real-world problems\nsuch as competitive LLM training. To solve the problems from these classes, we\npropose a new reduction scheme to construct UCB-type algorithms, namely, the\nF-LCB algorithm, based on algorithms for nonlinear optimization with known\nconvergence rates. We provide the regret upper bounds for this reduction scheme\nbased on the base algorithms' convergence rates. We add numerical experiments\nthat demonstrate the performance of the proposed scheme.",
      "tldr_zh": "该论文引入了两个新问题类：Functional Multi-Armed Bandit (FMAB) 和 Best Function Identification 问题，这些是多臂老虎机 (MAB) 问题的变体，其中每个臂代表一个未知的黑箱函数，并适用于建模现实世界场景，如竞争性 LLM 训练。作者提出了一种新的归约方案，基于非线性优化算法构建了 F-LCB 算法，用于解决这些问题。论文提供了遗憾上界 (regret upper bounds)，这些界基于基础算法的收敛率。数值实验证明了该方案的有效性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00509v1",
      "published_date": "2025-03-01 14:28:52 UTC",
      "updated_date": "2025-03-01 14:28:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:38:45.271173"
    },
    {
      "arxiv_id": "2503.00495v1",
      "title": "Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanchen Li",
        "Jianyu Wang",
        "Yuhao Cheng",
        "Yikun Zeng",
        "Xingyu Ren",
        "Wenhan Zhu",
        "Weiming Zhao",
        "Yichao Yan"
      ],
      "abstract": "Significant progress has been made for speech-driven 3D face animation, but\nmost works focus on learning the motion of mesh/geometry, ignoring the impact\nof dynamic texture. In this work, we reveal that dynamic texture plays a key\nrole in rendering high-fidelity talking avatars, and introduce a\nhigh-resolution 4D dataset \\textbf{TexTalk4D}, consisting of 100 minutes of\naudio-synced scan-level meshes with detailed 8K dynamic textures from 100\nsubjects. Based on the dataset, we explore the inherent correlation between\nmotion and texture, and propose a diffusion-based framework \\textbf{TexTalker}\nto simultaneously generate facial motions and dynamic textures from speech.\nFurthermore, we propose a novel pivot-based style injection strategy to capture\nthe complicity of different texture and motion styles, which allows\ndisentangled control. TexTalker, as the first method to generate audio-synced\nfacial motion with dynamic texture, not only outperforms the prior arts in\nsynthesising facial motions, but also produces realistic textures that are\nconsistent with the underlying facial movements. Project page:\nhttps://xuanchenli.github.io/TexTalk/.",
      "tldr_zh": "本文研究揭示了动态纹理在渲染高保真3D对话头像中的关键作用，并引入高分辨率4D数据集TexTalk4D，该数据集包含100分钟的音频同步扫描级网格和详细8K动态纹理，来自100个受试者。基于此，作者提出一个diffusion-based框架TexTalker，用于从语音同时生成面部运动和动态纹理，并探索运动与纹理的内在相关性。此外，引入pivot-based style injection strategy，实现纹理和运动风格的分离控制，使TexTalker在面部运动合成上优于现有技术，并生成与面部运动一致的真实纹理。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00495v1",
      "published_date": "2025-03-01 13:51:37 UTC",
      "updated_date": "2025-03-01 13:51:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:38:57.358723"
    },
    {
      "arxiv_id": "2503.00493v2",
      "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Boyi Kang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "Zhen Ye",
        "Mingshuai Liu",
        "Ziqian Wang",
        "Yike Zhu",
        "Guobin Ma",
        "Jun Chen",
        "Longshuai Xiao",
        "Chao Weng",
        "Wei Xue",
        "Lei Xie"
      ],
      "abstract": "Recent advancements in language models (LMs) have demonstrated strong\ncapabilities in semantic understanding and contextual modeling, which have\nflourished in generative speech enhancement (SE). However, many LM-based SE\napproaches primarily focus on semantic information, often neglecting the\ncritical role of acoustic information, which leads to acoustic inconsistency\nafter enhancement and limited generalization across diverse SE tasks. In this\npaper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes\ngeneralization capabilities for speech enhancement. LLaSE-G1 offers the\nfollowing key contributions: First, to mitigate acoustic inconsistency,\nLLaSE-G1 employs continuous representations from WavLM as input and predicts\nspeech tokens from X-Codec2, maximizing acoustic preservation. Second, to\npromote generalization capability, LLaSE-G1 introduces dual-channel inputs and\noutputs, unifying multiple SE tasks without requiring task-specific IDs. Third,\nLLaSE-G1 outperforms prior task-specific discriminative and generative SE\nmodels, demonstrating scaling effects at test time and emerging capabilities\nfor unseen SE tasks. Additionally, we release our code and models to support\nfurther research in this area.",
      "tldr_zh": "本文提出 LLaSE-G1，一种基于 LLaMA 的语音增强模型，旨在解决现有 LM-based SE 方法忽略声学信息导致的声学不一致和泛化能力有限问题。LLaSE-G1 通过采用 WavLM 的连续表示作为输入并预测 X-Codec2 的语音标记来最大化声学保留，同时引入双通道输入和输出，统一多种 SE 任务而不需任务特定 ID。实验结果表明，LLaSE-G1 优于先前的任务特定判别和生成模型，展示了测试时的缩放效应和对未见 SE 任务的新兴能力；作者还发布了代码和模型以支持进一步研究。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "13 pages, 2 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.00493v2",
      "published_date": "2025-03-01 13:44:50 UTC",
      "updated_date": "2025-03-04 12:32:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:39:08.157505"
    },
    {
      "arxiv_id": "2504.06270v1",
      "title": "Addressing Cold-start Problem in Click-Through Rate Prediction via Supervised Diffusion Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Wenqiao Zhu",
        "Lulu Wang",
        "Jun Wu"
      ],
      "abstract": "Predicting Click-Through Rates is a crucial function within recommendation\nand advertising platforms, as the output of CTR prediction determines the order\nof items shown to users. The Embedding \\& MLP paradigm has become a standard\napproach for industrial recommendation systems and has been widely deployed.\nHowever, this paradigm suffers from cold-start problems, where there is either\nno or only limited user action data available, leading to poorly learned ID\nembeddings. The cold-start problem hampers the performance of new items. To\naddress this problem, we designed a novel diffusion model to generate a\nwarmed-up embedding for new items. Specifically, we define a novel diffusion\nprocess between the ID embedding space and the side information space. In\naddition, we can derive a sub-sequence from the diffusion steps to expedite\ntraining, given that our diffusion model is non-Markovian. Our diffusion model\nis supervised by both the variational inference and binary cross-entropy\nobjectives, enabling it to generate warmed-up embeddings for items in both the\ncold-start and warm-up phases. Additionally, we have conducted extensive\nexperiments on three recommendation datasets. The results confirmed the\neffectiveness of our approach.",
      "tldr_zh": "这篇论文针对 Click-Through Rate (CTR) 预测中的冷启动问题，提出了一种基于监督扩散模型的方法，以解决 Embedding & MLP 范式下新项目 ID 嵌入学习不足的问题。该模型在 ID 嵌入空间和侧信息空间之间定义了一个非 Markovian 扩散过程，并通过变分推理和二元交叉熵 (BCE) 目标进行监督，从而为冷启动和预热阶段的项目生成预热嵌入。实验在三个推荐数据集上验证了该方法的有效性，显著提高了新项目的预测性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06270v1",
      "published_date": "2025-03-01 13:43:06 UTC",
      "updated_date": "2025-03-01 13:43:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:39:17.922232"
    },
    {
      "arxiv_id": "2503.00489v1",
      "title": "Embracing Diversity: A Multi-Perspective Approach with Soft Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Benedetta Muscato",
        "Praveen Bushipaka",
        "Gizem Gezici",
        "Lucia Passaro",
        "Fosca Giannotti",
        "Tommaso Cucinotta"
      ],
      "abstract": "Prior studies show that adopting the annotation diversity shaped by different\nbackgrounds and life experiences and incorporating them into the model\nlearning, i.e. multi-perspective approach, contribute to the development of\nmore responsible models. Thus, in this paper we propose a new framework for\ndesigning and further evaluating perspective-aware models on stance detection\ntask,in which multiple annotators assign stances based on a controversial\ntopic. We also share a new dataset established through obtaining both human and\nLLM annotations. Results show that the multi-perspective approach yields better\nclassification performance (higher F1-scores), outperforming the traditional\napproaches that use a single ground-truth, while displaying lower model\nconfidence scores, probably due to the high level of subjectivity of the stance\ndetection task.",
      "tldr_zh": "本研究提出了一种多视角方法（multi-perspective approach），利用软标签（soft labels）整合不同背景的标注多样性，以开发更负责任的模型，针对立场检测（stance detection）任务设计了一个新框架。研究者分享了一个新数据集，包括人类和LLM（Large Language Models）标注。结果显示，该方法在分类性能上优于传统单真实值方法（F1-scores更高），但模型置信度较低，可能源于任务的高度主观性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00489v1",
      "published_date": "2025-03-01 13:33:38 UTC",
      "updated_date": "2025-03-01 13:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:39:29.005643"
    },
    {
      "arxiv_id": "2503.00483v1",
      "title": "Interacting with AI Reasoning Models: Harnessing \"Thoughts\" for AI-Driven Software Engineering",
      "title_zh": "翻译失败",
      "authors": [
        "Christoph Treude",
        "Raula Gaikovina Kula"
      ],
      "abstract": "Recent advances in AI reasoning models provide unprecedented transparency\ninto their decision-making processes, transforming them from traditional\nblack-box systems into models that articulate step-by-step chains of thought\nrather than producing opaque outputs. This shift has the potential to improve\nsoftware quality, explainability, and trust in AI-augmented development.\nHowever, software engineers rarely have the time or cognitive bandwidth to\nanalyze, verify, and interpret every AI-generated thought in detail. Without an\neffective interface, this transparency could become a burden rather than a\nbenefit.\n  In this paper, we propose a vision for structuring the interaction between AI\nreasoning models and software engineers to maximize trust, efficiency, and\ndecision-making power. We argue that simply exposing AI's reasoning is not\nenough -- software engineers need tools and frameworks that selectively\nhighlight critical insights, filter out noise, and facilitate rapid validation\nof key assumptions. To illustrate this challenge, we present motivating\nexamples in which AI reasoning models state their assumptions when deciding\nwhich external library to use and produce divergent reasoning paths and\nrecommendations about security vulnerabilities, highlighting the need for an\ninterface that prioritizes actionable insights while managing uncertainty and\nresolving conflicts. We then outline a research roadmap for integrating\nautomated summarization, assumption validation, and multi-model conflict\nresolution into software engineering workflows. Achieving this vision will\nunlock the full potential of AI reasoning models to enable software engineers\nto make faster, more informed decisions without being overwhelmed by\nunnecessary detail.",
      "tldr_zh": "本研究探讨了AI推理模型的透明性如何通过“chains of thought”提升软件工程的质量、可解释性和信任，但软件工程师往往缺乏时间来处理这些详细输出，可能导致额外负担。论文提出一个愿景，即构建结构化的交互框架，使用工具如automated summarization、assumption validation和multi-model conflict resolution来突出关键洞见、过滤噪音，并快速验证假设。作者通过示例说明AI在选择外部库或处理安全漏洞时的分歧，并概述研究路线图，以实现AI驱动软件工程中更高效的决策过程。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00483v1",
      "published_date": "2025-03-01 13:19:15 UTC",
      "updated_date": "2025-03-01 13:19:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:39:41.376185"
    },
    {
      "arxiv_id": "2503.00481v1",
      "title": "Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy",
      "title_zh": "翻译失败",
      "authors": [
        "Felix Dobslaw",
        "Robert Feldt",
        "Juyeon Yoon",
        "Shin Yoo"
      ],
      "abstract": "Large Language Models (LLMs) and Multi-Agent LLMs (MALLMs) introduce\nnon-determinism unlike traditional or machine learning software, requiring new\napproaches to verifying correctness beyond simple output comparisons or\nstatistical accuracy over test datasets.\n  This paper presents a taxonomy for LLM test case design, informed by both the\nresearch literature, our experience, and open-source tools that represent the\nstate of practice. We identify key variation points that impact test\ncorrectness and highlight open challenges that the research, industry, and\nopen-source communities must address as LLMs become integral to software\nsystems.\n  Our taxonomy defines four facets of LLM test case design, addressing\nambiguity in both inputs and outputs while establishing best practices. It\ndistinguishes variability in goals, the system under test, and inputs, and\nintroduces two key oracle types: atomic and aggregated. Our mapping indicates\nthat current tools insufficiently account for these variability points,\nhighlighting the need for closer collaboration between academia and\npractitioners to improve the reliability and reproducibility of LLM testing.",
      "tldr_zh": "这篇论文探讨了测试基于 Large Language Models (LLMs) 和 Multi-Agent LLMs (MALLMs) 的软件所面临的挑战，这些模型的非确定性要求超越传统输出比较或统计准确性的新验证方法。论文提出一个基于研究文献、经验和开源工具的 faceted taxonomy，用于设计 LLM 测试用例，该分类法定义了四个方面，包括处理输入和输出模糊性的最佳实践，并区分目标、被测试系统和输入的变异性，同时引入 atomic 和 aggregated oracle 类型。研究发现，当前工具未能充分覆盖这些变异点，强调学术界、行业和开源社区需加强合作，以提升 LLM 测试的可靠性和可重复性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00481v1",
      "published_date": "2025-03-01 13:15:56 UTC",
      "updated_date": "2025-03-01 13:15:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:39:54.405657"
    },
    {
      "arxiv_id": "2503.00461v1",
      "title": "Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs",
      "title_zh": "翻译失败",
      "authors": [
        "Zhantong Zhu",
        "Hongou Li",
        "Wenjie Ren",
        "Meng Wu",
        "Le Ye",
        "Ru Huang",
        "Tianyu Jia"
      ],
      "abstract": "With the rapid advent of generative models, efficiently deploying these\nmodels on specialized hardware has become critical. Tensor Processing Units\n(TPUs) are designed to accelerate AI workloads, but their high power\nconsumption necessitates innovations for improving efficiency.\nCompute-in-memory (CIM) has emerged as a promising paradigm with superior area\nand energy efficiency. In this work, we present a TPU architecture that\nintegrates digital CIM to replace conventional digital systolic arrays in\nmatrix multiply units (MXUs). We first establish a CIM-based TPU architecture\nmodel and simulator to evaluate the benefits of CIM for diverse generative\nmodel inference. Building upon the observed design insights, we further explore\nvarious CIM-based TPU architectural design choices. Up to 44.2% and 33.8%\nperformance improvement for large language model and diffusion transformer\ninference, and 27.3x reduction in MXU energy consumption can be achieved with\ndifferent design choices, compared to the baseline TPUv4i architecture.",
      "tldr_zh": "该研究针对生成模型在TPUs（Tensor Processing Units）上的高效推理问题，提出了一种集成Compute-in-Memory (CIM)的TPU架构，以替换传统的数字systolic arrays在矩阵乘法单元(MXUs)中的应用。研究团队构建了CIM-based TPU架构模型和模拟器，评估了CIM对各种生成模型推理的益处，并探索了多种设计选择。结果显示，与基线TPUv4i架构相比，该方法在大型语言模型和扩散Transformer推理中实现了高达44.2%和33.8%的性能提升，同时MXU能量消耗减少了27.3倍。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Accepted to appear at DATE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00461v1",
      "published_date": "2025-03-01 12:03:25 UTC",
      "updated_date": "2025-03-01 12:03:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:40:06.574897"
    },
    {
      "arxiv_id": "2503.00455v1",
      "title": "PodAgent: A Comprehensive Framework for Podcast Generation",
      "title_zh": "PodAgent：一个用于播客生成的全面框架",
      "authors": [
        "Yujia Xiao",
        "Lei He",
        "Haohan Guo",
        "Fenglong Xie",
        "Tan Lee"
      ],
      "abstract": "Existing Existing automatic audio generation methods struggle to generate\npodcast-like audio programs effectively. The key challenges lie in in-depth\ncontent generation, appropriate and expressive voice production. This paper\nproposed PodAgent, a comprehensive framework for creating audio programs.\nPodAgent 1) generates informative topic-discussion content by designing a\nHost-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for\nsuitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis\nmethod to generate expressive conversational speech. Given the absence of\nstandardized evaluation criteria for podcast-like audio generation, we\ndeveloped comprehensive assessment guidelines to effectively evaluate the\nmodel's performance. Experimental results demonstrate PodAgent's effectiveness,\nsignificantly surpassing direct GPT-4 generation in topic-discussion dialogue\ncontent, achieving an 87.4% voice-matching accuracy, and producing more\nexpressive speech through LLM-guided synthesis. Demo page:\nhttps://podcast-agent.github.io/demo/. Source code:\nhttps://github.com/yujxx/PodAgent.",
      "tldr_zh": "本研究提出PodAgent，一种全面框架，用于生成播客-like音频节目，以解决现有自动音频生成方法的深度内容生成和表达性语音生产挑战。PodAgent通过Host-Guest-Writer多智能体协作系统生成信息丰富的主题讨论内容、构建语音池实现合适的语音角色匹配，以及采用LLM-enhanced speech synthesis方法来创建表达性对话语音。由于缺乏标准化评估标准，该框架还开发了全面评估指南。实验结果显示，PodAgent在主题讨论对话内容上显著优于直接GPT-4生成，语音匹配准确率达87.4%，并通过LLM-guided synthesis产生更具表达力的语音。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MA",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00455v1",
      "published_date": "2025-03-01 11:35:17 UTC",
      "updated_date": "2025-03-01 11:35:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:40:18.849180"
    },
    {
      "arxiv_id": "2503.16471v1",
      "title": "A Review of Brain-Computer Interface Technologies: Signal Acquisition Methods and Interaction Paradigms",
      "title_zh": "脑机接口技术的综述：信号采集方法和交互范式",
      "authors": [
        "Yifan Wang",
        "Cheng Jiang",
        "Chenzhong Li"
      ],
      "abstract": "Brain-Computer Interface (BCI) technology facilitates direct communication\nbetween the human brain and external devices, representing a substantial\nadvancement in human-machine interaction. This review provides an in-depth\nanalysis of various BCI paradigms, including classic paradigms, current\nclassifications, and hybrid paradigms, each with distinct characteristics and\napplications. Additionally, we explore a range of signal acquisition methods,\nclassified into non-implantation, intervention, and implantation techniques,\nelaborating on their principles and recent advancements. By examining the\ninterdependence between paradigms and signal acquisition technologies, this\nreview offers a comprehensive perspective on how innovations in one domain\npropel progress in the other. The goal is to present insights into the future\ndevelopment of more efficient, user-friendly, and versatile BCI systems,\nemphasizing the synergy between paradigm design and signal acquisition\ntechniques and their potential to transform the field.",
      "tldr_zh": "这篇综述论文回顾了Brain-Computer Interface (BCI) 技术，重点分析了各种交互范式(paradigms)，包括经典范式、当前分类和混合范式，以及它们的特性与应用。论文还探讨了信号采集方法(signal acquisition methods)，分为非植入、非干预和植入技术，并阐述了这些方法的原理和最新进展。通过考察范式与信号采集技术之间的相互依赖，论文提供了全面视角，强调二者的协同作用有助于推动更高效、用户友好且多功能的BCI系统发展。总的来说，此综述为BCI领域的未来创新提供了宝贵见解。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "J.3"
      ],
      "primary_category": "cs.HC",
      "comment": "12 figures,20 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.16471v1",
      "published_date": "2025-03-01 11:22:47 UTC",
      "updated_date": "2025-03-01 11:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:40:30.348027"
    },
    {
      "arxiv_id": "2503.00449v1",
      "title": "Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yanyue Zhang",
        "Yulan He",
        "Deyu Zhou"
      ],
      "abstract": "Personalized opinion summarization is crucial as it considers individual user\ninterests while generating product summaries. Recent studies show that although\nlarge language models demonstrate powerful text summarization and evaluation\ncapabilities without the need for training data, they face difficulties in\npersonalized tasks involving long texts. To address this, \\textbf{Rehearsal}, a\npersonalized opinion summarization framework via LLMs-based role-playing is\nproposed. Having the model act as the user, the model can better understand the\nuser's personalized needs. Additionally, a role-playing supervisor and practice\nprocess are introduced to improve the role-playing ability of the LLMs, leading\nto a better expression of user needs. Furthermore, through suggestions from\nvirtual users, the summary generation is intervened, ensuring that the\ngenerated summary includes information of interest to the user, thus achieving\npersonalized summary generation. Experiment results demonstrate that our method\ncan effectively improve the level of personalization in large model-generated\nsummaries.",
      "tldr_zh": "这篇论文提出了 Rehearse 框架，利用 Large Language Models (LLMs) 通过角色扮演来实现个性化的意见总结，旨在解决 LLMs 在处理长文本个性化任务时的困难。框架让模型扮演用户角色，结合角色扮演监督者和练习过程，以更准确地理解和表达用户需求，并通过虚拟用户建议干预总结生成，确保包含用户感兴趣的信息。实验结果显示，该方法有效提升了 LLMs 生成总结的个性化水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.00449v1",
      "published_date": "2025-03-01 11:05:01 UTC",
      "updated_date": "2025-03-01 11:05:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:40:41.636776"
    },
    {
      "arxiv_id": "2503.00436v1",
      "title": "HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning",
      "title_zh": "翻译失败",
      "authors": [
        "Maria Lymperaiou",
        "Giorgos FIlandrianos",
        "Angeliki Dimitriou",
        "Athanasios Voulodimos",
        "Giorgos Stamou"
      ],
      "abstract": "In the dynamic landscape of artificial intelligence, the exploration of\nhallucinations within vision-language (VL) models emerges as a critical\nfrontier. This work delves into the intricacies of hallucinatory phenomena\nexhibited by widely used image captioners, unraveling interesting patterns.\nSpecifically, we step upon previously introduced techniques of conceptual\ncounterfactual explanations to address VL hallucinations. The deterministic and\nefficient nature of the employed conceptual counterfactuals backbone is able to\nsuggest semantically minimal edits driven by hierarchical knowledge, so that\nthe transition from a hallucinated caption to a non-hallucinated one is\nperformed in a black-box manner. HalCECE, our proposed hallucination detection\nframework is highly interpretable, by providing semantically meaningful edits\napart from standalone numbers, while the hierarchical decomposition of\nhallucinated concepts leads to a thorough hallucination analysis. Another\nnovelty tied to the current work is the investigation of role hallucinations,\nbeing one of the first works to involve interconnections between visual\nconcepts in hallucination detection. Overall, HalCECE recommends an explainable\ndirection to the crucial field of VL hallucination detection, thus fostering\ntrustworthy evaluation of current and future VL systems.",
      "tldr_zh": "这篇论文提出了HalCECE框架，用于通过概念反事实解释（conceptual counterfactuals）检测图像标题中的幻觉问题。框架采用分层知识驱动的语义最小编辑，实现从幻觉标题到非幻觉标题的确定性过渡，并首次调查角色幻觉（role hallucinations），强调视觉概念之间的互连以进行彻底分析。HalCECE的创新在于提供可解释的语义编辑而非单纯数字，从而提升vision-language (VL) 模型的可信评估和未来发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00436v1",
      "published_date": "2025-03-01 10:28:19 UTC",
      "updated_date": "2025-03-01 10:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:40:54.016987"
    },
    {
      "arxiv_id": "2503.00433v1",
      "title": "Unveiling AI's Threats to Child Protection: Regulatory efforts to Criminalize AI-Generated CSAM and Emerging Children's Rights Violations",
      "title_zh": "翻译失败",
      "authors": [
        "Emmanouela Kokolaki",
        "Paraskevi Fragopoulou"
      ],
      "abstract": "This paper aims to present new alarming trends in the field of child sexual\nabuse through imagery, as part of SafeLine's research activities in the field\nof cybercrime, child sexual abuse material and the protection of children's\nrights to safe online experiences. It focuses primarily on the phenomenon of\nAI-generated CSAM, sophisticated ways employed for its production which are\ndiscussed in dark web forums and the crucial role that the open-source AI\nmodels play in the evolution of this overwhelming phenomenon. The paper's main\ncontribution is a correlation analysis between the hotline's reports and domain\nnames identified in dark web forums, where users' discussions focus on\nexchanging information specifically related to the generation of AI-CSAM. The\nobjective was to reveal the close connection of clear net and dark web content,\nwhich was accomplished through the use of the ATLAS dataset of the Voyager\nsystem. Furthermore, through the analysis of a set of posts' content drilled\nfrom the above dataset, valuable conclusions on forum members' techniques\nemployed for the production of AI-generated CSAM are also drawn, while users'\nviews on this type of content and routes followed in order to overcome\ntechnological barriers set with the aim of preventing malicious purposes are\nalso presented. As the ultimate contribution of this research, an overview of\nthe current legislative developments in all country members of the INHOPE\norganization and the issues arising in the process of regulating the AI- CSAM\nis presented, shedding light in the legal challenges regarding the regulation\nand limitation of the phenomenon.",
      "tldr_zh": "这篇论文揭示了AI生成儿童性虐待材料（AI-Generated CSAM）对儿童保护的威胁，包括其在暗网论坛中的生产方式和开源AI模型的作用。研究通过SafeLine的报告分析和ATLAS数据集的Voyager系统，进行hotline报告与dark web forums域名的相关性分析，得出明网与暗网内容的紧密联系，并总结了论坛成员的生产技巧及用户观点。最终，论文概述了INHOPE组织成员国的当前立法进展，强调了在规范AI-CSAM过程中面临的法律挑战，如技术障碍和监管难题。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00433v1",
      "published_date": "2025-03-01 10:18:00 UTC",
      "updated_date": "2025-03-01 10:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:41:07.172955"
    },
    {
      "arxiv_id": "2503.01914v1",
      "title": "Conceptual Contrastive Edits in Textual and Vision-Language Retrieval",
      "title_zh": "文本与视觉语言检索中的概念对比编辑",
      "authors": [
        "Maria Lymperaiou",
        "Giorgos Stamou"
      ],
      "abstract": "As deep learning models grow in complexity, achieving model-agnostic\ninterpretability becomes increasingly vital. In this work, we employ post-hoc\nconceptual contrastive edits to expose noteworthy patterns and biases imprinted\nin representations of retrieval models. We systematically design optimal and\ncontrollable contrastive interventions targeting various parts of speech, and\neffectively apply them to explain both linguistic and visiolinguistic\npre-trained models in a black-box manner. Additionally, we introduce a novel\nmetric to assess the per-word impact of contrastive interventions on model\noutcomes, providing a comprehensive evaluation of each intervention's\neffectiveness.",
      "tldr_zh": "本文提出了一种后验（post-hoc）概念对比编辑（conceptual contrastive edits）方法，用于揭示文本和视觉语言检索模型中嵌入的模式和偏差，从而实现模型无关的解释性。研究者系统设计了针对各种词性（parts of speech）的优化可控对比干预（contrastive interventions），并以黑箱方式应用于语言和视觉语言预训练模型。最终，他们引入了一个新指标来评估每个干预对模型输出的每个单词的影响，提供全面的干预有效性评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.01914v1",
      "published_date": "2025-03-01 10:14:28 UTC",
      "updated_date": "2025-03-01 10:14:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:41:18.943129"
    },
    {
      "arxiv_id": "2503.00427v1",
      "title": "Language Model Mapping in Multimodal Music Learning: A Grand Challenge Proposal",
      "title_zh": "在多模态音乐学习中的语言模型映射：一个重大挑战提案",
      "authors": [
        "Daniel Chin",
        "Gus Xia"
      ],
      "abstract": "We have seen remarkable success in representation learning and language\nmodels (LMs) using deep neural networks. Many studies aim to build the\nunderlying connections among different modalities via the alignment and\nmappings at the token or embedding level, but so far, most methods are very\ndata-hungry, limiting their performance in domains such as music where paired\ndata are less abundant. We argue that the embedding alignment is only at the\nsurface level of multimodal alignment. In this paper, we propose a grand\nchallenge of \\textit{language model mapping} (LMM), i.e., how to map the\nessence implied in the LM of one domain to the LM of another domain under the\nassumption that LMs of different modalities are tracking the same underlying\nphenomena. We first introduce a basic setup of LMM, highlighting the goal to\nunveil a deeper aspect of cross-modal alignment as well as to achieve more\nsample-efficiency learning. We then discuss why music is an ideal domain in\nwhich to conduct LMM research. After that, we connect LMM in music with a more\ngeneral and challenging scientific problem of \\textit{learning to take actions\nbased on both sensory input and abstract symbols}, and in the end, present an\nadvanced version of the challenge problem setup.",
      "tldr_zh": "该论文提出 Language Model Mapping (LMM) 作为多模态音乐学习中的一个重大挑战，旨在将一个领域的语言模型(LM)映射到另一个领域，以揭示更深层的跨模态对齐，而非仅限于表面的嵌入对齐，从而实现更样本高效的学习。作者指出，现有的多模态方法过于依赖大量配对数据，而音乐领域的数据稀缺使其成为理想的 LMM 研究对象。最终，该论文将 LMM 与更广泛的科学问题联系起来，即学习基于感官输入和抽象符号进行行动，提供了一个高级挑战框架。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00427v1",
      "published_date": "2025-03-01 10:04:36 UTC",
      "updated_date": "2025-03-01 10:04:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:41:31.026148"
    },
    {
      "arxiv_id": "2503.00426v1",
      "title": "Auto-encoding Molecules: Graph-Matching Capabilities Matter",
      "title_zh": "自动编码分子：图匹配能力很重要",
      "authors": [
        "Magnus Cunow",
        "Gerrit Großmann"
      ],
      "abstract": "Autoencoders are effective deep learning models that can function as\ngenerative models and learn latent representations for downstream tasks. The\nuse of graph autoencoders - with both encoder and decoder implemented as\nmessage passing networks - is intriguing due to their ability to generate\npermutation-invariant graph representations. However, this approach faces\ndifficulties because decoding a graph structure from a single vector is\nchallenging, and comparing input and output graphs requires an effective\npermutation-invariant similarity measure. As a result, many studies rely on\napproximate methods.\n  In this work, we explore the effect of graph matching precision on the\ntraining behavior and generation capabilities of a Variational Autoencoder\n(VAE). Our contribution is two-fold: (1) we propose a transformer-based message\npassing graph decoder as an alternative to a graph neural network decoder, that\nis more robust and expressive by leveraging global attention mechanisms. (2) We\nshow that the precision of graph matching has significant impact on training\nbehavior and is essential for effective de novo (molecular) graph generation.\n  Code is available at https://github.com/mcunow/graph-matching",
      "tldr_zh": "本研究探讨了图自动编码器（graph autoencoders）在分子生成中的挑战，特别是从单一向量解码图结构和评估输入输出图的置换不变相似度问题，导致许多方法依赖于近似技术。论文的主要贡献包括：提出一种基于 Transformer 的消息传递图解码器，作为传统图神经网络解码器的替代方案，提升了鲁棒性和表达性，通过全局注意力机制处理图结构。实验结果表明，图匹配精度的提升对 Variational Autoencoder (VAE) 的训练行为和从零开始的分子图生成能力至关重要，为更有效的图生成模型提供了关键洞见。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00426v1",
      "published_date": "2025-03-01 10:00:37 UTC",
      "updated_date": "2025-03-01 10:00:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:41:42.845822"
    },
    {
      "arxiv_id": "2503.00420v1",
      "title": "A physics-informed Bayesian optimization method for rapid development of electrical machines",
      "title_zh": "一种基于物理信息的贝叶斯优化方法，用于电机快速开发",
      "authors": [
        "Pedram Asef",
        "Christopher Vagg"
      ],
      "abstract": "Advanced slot and winding designs are imperative to create future high\nperformance electrical machines (EM). As a result, the development of methods\nto design and improve slot filling factor (SFF) has attracted considerable\nresearch. Recent developments in manufacturing processes, such as additive\nmanufacturing and alternative materials, has also highlighted a need for novel\nhigh-fidelity design techniques to develop high performance complex geometries\nand topologies. This study therefore introduces a novel physics-informed\nmachine learning (PIML) design optimization process for improving SFF in\ntraction electrical machines used in electric vehicles. A maximum entropy\nsampling algorithm (MESA) is used to seed a physics-informed Bayesian\noptimization (PIBO) algorithm, where the target function and its approximations\nare produced by Gaussian processes (GP)s. The proposed PIBO-MESA is coupled\nwith a 2D finite element model (FEM) to perform a GP-based surrogate and\nprovide the first demonstration of the optimal combination of complex design\nvariables for an electrical machine. Significant computational gains were\nachieved using the new PIBO-MESA approach, which is 45% faster than existing\nstochastic methods, such as the non-dominated sorting genetic algorithm II\n(NSGA-II). The FEM results confirm that the new design optimization process and\nkeystone shaped wires lead to a higher SFF (i.e. by 20%) and electromagnetic\nimprovements (e.g. maximum torque by 12%) with similar resistivity. The newly\ndeveloped PIBO-MESA design optimization process therefore presents significant\nbenefits in the design of high-performance electric machines, with reduced\ndevelopment time and costs.",
      "tldr_zh": "这篇论文提出了一种physics-informed machine learning (PIML)设计优化过程，旨在加速electrical machines的开发，通过改善slot filling factor (SFF)来提升性能。该方法结合maximum entropy sampling algorithm (MESA)和physics-informed Bayesian optimization (PIBO)算法，使用Gaussian processes (GPs)生成目标函数，并与2D finite element model (FEM)集成进行代理建模。实验结果显示，PIBO-MESA比传统随机方法如non-dominated sorting genetic algorithm II (NSGA-II)快45%，并实现了SFF提高20%、最大扭矩提高12%，同时保持电阻率相似。这种新方法显著降低了高性能电机设计的开发时间和成本。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00420v1",
      "published_date": "2025-03-01 09:43:58 UTC",
      "updated_date": "2025-03-01 09:43:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:41:56.409855"
    },
    {
      "arxiv_id": "2503.00416v1",
      "title": "Breaking the Loop: Detecting and Mitigating Denial-of-Service Vulnerabilities in Large Language Models",
      "title_zh": "打破循环：检测与缓解大型语言模型中的拒绝服务漏洞",
      "authors": [
        "Junzhe Yu",
        "Yi Liu",
        "Huijia Sun",
        "Ling Shi",
        "Yuqi Chen"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced text understanding\nand generation, becoming integral to applications across education, software\ndevelopment, healthcare, entertainment, and legal services. Despite\nconsiderable progress in improving model reliability, latency remains\nunder-explored, particularly through recurrent generation, where models\nrepeatedly produce similar or identical outputs, causing increased latency and\npotential Denial-of-Service (DoS) vulnerabilities.\n  We propose RecurrentGenerator, a black-box evolutionary algorithm that\nefficiently identifies recurrent generation scenarios in prominent LLMs like\nLLama-3 and GPT-4o. Additionally, we introduce RecurrentDetector, a lightweight\nreal-time classifier trained on activation patterns, achieving 95.24% accuracy\nand an F1 score of 0.87 in detecting recurrent loops. Our methods provide\npractical solutions to mitigate latency-related vulnerabilities, and we\npublicly share our tools and data to support further research.",
      "tldr_zh": "大型语言模型(LLMs) 在教育、软件开发等领域广泛应用，但 recurrent generation 问题可能导致重复输出、增加延迟，并引发 Denial-of-Service (DoS) 漏洞。论文提出 RecurrentGenerator，一种黑盒进化算法，用于高效识别 LLaMA-3 和 GPT-4o 等模型中的 recurrent generation 场景。作者还引入 RecurrentDetector，一种基于激活模式的轻量级实时分类器，实现了 95.24% 的准确率和 0.87 的 F1 分数，用于检测和缓解这些漏洞。研究公开了工具和数据，以推动相关领域的进一步探索。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00416v1",
      "published_date": "2025-03-01 09:32:17 UTC",
      "updated_date": "2025-03-01 09:32:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:42:09.483081"
    },
    {
      "arxiv_id": "2503.00401v2",
      "title": "Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with Query-Oriented Pivot Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Zongru Wu",
        "Pengzhou Cheng",
        "Zheng Wu",
        "Tianjie Ju",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "abstract": "Perception-enhanced pre-training, particularly through grounding techniques,\nis widely adopted to enhance the performance of graphical user interface (GUI)\nagents. However, in resource-constrained scenarios, the format discrepancy\nbetween coordinate-oriented grounding and action-oriented reasoning limits the\neffectiveness of grounding for reasoning tasks. To address this challenge, we\npropose a query-oriented pivot approach called query inference, which serves as\na bridge between GUI grounding and reasoning. By inferring potential user\nqueries from a screenshot and its associated element coordinates, query\ninference improves the understanding of coordinates while aligning more closely\nwith reasoning tasks. Experimental results show that query inference\noutperforms previous grounding techniques under the same training data scale.\nNotably, query inference achieves comparable or even better performance to\nlarge-scale grounding-enhanced OS-Atlas with less than 0.1% of training data.\nFurthermore, we explore the impact of reasoning formats and demonstrate that\nintegrating additional semantic information into the input further boosts\nreasoning performance. The code is publicly available at\nhttps://github.com/ZrW00/GUIPivot.",
      "tldr_zh": "该研究针对多模态大语言模型(MLLM)驱动的GUI代理，解决了资源受限场景下coordinate-oriented grounding与action-oriented reasoning之间的格式差异问题。论文提出query inference方法，作为query-oriented pivot任务， 通过从截图和元素坐标推断潜在用户查询，从而改善坐标理解并更好地与推理任务对齐。实验结果显示，query inference在相同训练数据规模下优于现有grounding技术，并使用不到0.1%的训练数据就达到或超过了大型grounding-enhanced OS-Atlas的性能；此外，整合额外语义信息到输入中进一步提升了推理性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00401v2",
      "published_date": "2025-03-01 08:29:59 UTC",
      "updated_date": "2025-03-04 12:04:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:42:20.827944"
    },
    {
      "arxiv_id": "2503.07627v1",
      "title": "Psychological Counseling Ability of Large Language Models",
      "title_zh": "大语言模型的心理咨询能力",
      "authors": [
        "Fangyu Peng",
        "Jingxin Nie"
      ],
      "abstract": "With the development of science and the continuous progress of artificial\nintelligence technology, Large Language Models (LLMs) have begun to be widely\nutilized across various fields. However, in the field of psychological\ncounseling, the ability of LLMs have not been systematically assessed. In this\nstudy, we assessed the psychological counseling ability of mainstream LLMs\nusing 1096 psychological counseling skill questions which were selected from\nthe Chinese National Counselor Level 3 Examination, including Knowledge-based,\nAnalytical-based, and Application-based question types. The analysis showed\nthat the correctness rates of the LLMs for Chinese questions, in descending\norder, were GLM-3 (46.5%), GPT-4 (46.1%), Gemini (45.0%), ERNIE-3.5 (45.7%) and\nGPT-3.5 (32.9%). The correctness rates of the LLMs for English questions, in\ndescending order, were ERNIE-3.5 (43.9%), GPT-4 (40.6%), Gemini (36.6%), GLM-3\n(29.9%) and GPT-3.5 (29.5%). A chi-square test indicated significant\ndifferences in the LLMs' performance on Chinese and English questions.\nFurthermore, we subsequently utilized the Counselor's Guidebook (Level 3) as a\nreference for ERNIE-3.5, resulting in a new correctness rate of 59.6%, a 13.8%\nimprovement over its initial rate of 45.8%. In conclusion, the study assessed\nthe psychological counseling ability of LLMs for the first time, which may\nprovide insights for future enhancement and improvement of psychological\ncounseling ability of LLMs.",
      "tldr_zh": "本研究首次系统评估了主流大语言模型（Large Language Models, LLMs）的心理咨询能力，使用来自中国国家心理咨询师三级考试的1096个问题（包括知识型、分析型和应用型），测试了GLM-3、GPT-4、Gemini、ERNIE-3.5和GPT-3.5等模型在中文和英文问题上的表现。结果显示，模型的正确率整体较低，其中中文问题最高为GLM-3的46.5%，英文问题最高为ERNIE-3.5的43.9%，并通过卡方检验（Chi-square test）证实了语言差异对性能的影响显著。进一步改进显示，使用《Counselor's Guidebook (Level 3)》作为参考后，ERNIE-3.5的正确率从45.8%提升至59.6%，为未来增强LLMs的心理咨询能力提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.07627v1",
      "published_date": "2025-03-01 08:01:25 UTC",
      "updated_date": "2025-03-01 08:01:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:42:35.253035"
    },
    {
      "arxiv_id": "2503.00393v1",
      "title": "Reservoir Network with Structural Plasticity for Human Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah M. Zyarah",
        "Alaa M. Abdul-Hadi",
        "Dhireesha Kudithipudi"
      ],
      "abstract": "The unprecedented dissemination of edge devices is accompanied by a growing\ndemand for neuromorphic chips that can process time-series data natively\nwithout cloud support. Echo state network (ESN) is a class of recurrent neural\nnetworks that can be used to identify unique patterns in time-series data and\npredict future events. It is known for minimal computing resource requirements\nand fast training, owing to the use of linear optimization solely at the\nreadout stage. In this work, a custom-design neuromorphic chip based on ESN\ntargeting edge devices is proposed. The proposed system supports various\nlearning mechanisms, including structural plasticity and synaptic plasticity,\nlocally on-chip. This provides the network with an additional degree of freedom\nto continuously learn, adapt, and alter its structure and sparsity level,\nensuring high performance and continuous stability. We demonstrate the\nperformance of the proposed system as well as its robustness to noise against\nreal-world time-series datasets while considering various topologies of data\nmovement. An average accuracy of 95.95% and 85.24% are achieved on human\nactivity recognition and prosthetic finger control, respectively. We also\nillustrate that the proposed system offers a throughput of 6x10^4 samples/sec\nwith a power consumption of 47.7mW on a 65nm IBM process.",
      "tldr_zh": "本研究提出了一种基于Echo State Network (ESN)的神经形态芯片，针对边缘设备处理时间序列数据，支持structural plasticity和synaptic plasticity机制，以实现网络的持续学习、适应和结构调整。相比传统方法，该系统在芯片上本地实现这些学习机制，确保高性能和稳定性。实验结果显示，在人类活动识别任务上平均准确率达95.95%，在假肢手指控制上达85.24%；此外，该系统在65nm IBM工艺下提供6x10^4 samples/sec的吞吐量和47.7mW的功耗，展示了其鲁棒性和实际应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00393v1",
      "published_date": "2025-03-01 07:57:22 UTC",
      "updated_date": "2025-03-01 07:57:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:42:44.202021"
    },
    {
      "arxiv_id": "2503.00392v1",
      "title": "Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving",
      "title_zh": "渐进稀疏注意力：算法与系统协同设计，用于大型语言模型服务中的高效注意力机制",
      "authors": [
        "Qihui Zhou",
        "Peiqi Yin",
        "Pengfei Zuo",
        "James Cheng"
      ],
      "abstract": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
      "tldr_zh": "本研究针对大型语言模型(LLM)处理长上下文时因关键值(KV)缓存导致的高内存开销问题，提出Progressive Sparse Attention (PSA)机制，通过算法和系统联合设计来平衡准确性和效率。PSA算法根据实际注意力权重分布动态调整不同标记和层级的KV缓存预算，而不是依赖固定top-$k$选择，从而实现高准确性并最小化缓存使用。为提升执行效率，该机制引入流水线迭代方案减少CPU-GPU同步开销，并优化统一GPU内存管理。实验结果显示，PSA相较于现有动态稀疏注意力算法(DSAes)减少KV缓存使用高达2.4倍，并将端到端服务吞吐量提高至原系统的2.0倍。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.00392v1",
      "published_date": "2025-03-01 07:56:42 UTC",
      "updated_date": "2025-03-01 07:56:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:42:56.939562"
    },
    {
      "arxiv_id": "2503.00389v1",
      "title": "BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds",
      "title_zh": "BGM2Pose：利用非平稳声音的主动 3D 人体姿势估计",
      "authors": [
        "Yuto Shibata",
        "Yusuke Oumi",
        "Go Irie",
        "Akisato Kimura",
        "Yoshimitsu Aoki",
        "Mariko Isogawa"
      ],
      "abstract": "We propose BGM2Pose, a non-invasive 3D human pose estimation method using\narbitrary music (e.g., background music) as active sensing signals. Unlike\nexisting approaches that significantly limit practicality by employing\nintrusive chirp signals within the audible range, our method utilizes natural\nmusic that causes minimal discomfort to humans. Estimating human poses from\nstandard music presents significant challenges. In contrast to sound sources\nspecifically designed for measurement, regular music varies in both volume and\npitch. These dynamic changes in signals caused by music are inevitably mixed\nwith alterations in the sound field resulting from human motion, making it hard\nto extract reliable cues for pose estimation. To address these challenges,\nBGM2Pose introduces a Contrastive Pose Extraction Module that employs\ncontrastive learning and hard negative sampling to eliminate musical components\nfrom the recorded data, isolating the pose information. Additionally, we\npropose a Frequency-wise Attention Module that enables the model to focus on\nsubtle acoustic variations attributable to human movement by dynamically\ncomputing attention across frequency bands. Experiments suggest that our method\noutperforms the existing methods, demonstrating substantial potential for\nreal-world applications. Our datasets and code will be made publicly available.",
      "tldr_zh": "该研究提出 BGM2Pose，一种非侵入性 3D 人体姿势估计方法，使用任意背景音乐作为主动感知信号，以减少对人类的干扰。针对音乐音量和音高变化与人体运动混合导致的挑战，该方法引入 Contrastive Pose Extraction Module，通过对比学习和 hard negative sampling 去除音乐成分，并隔离姿势信息；同时，Frequency-wise Attention Module 动态关注频率带的微妙声学变化，以提取可靠的运动线索。实验结果显示，BGM2Pose 优于现有方法，在实际应用中具有显著潜力，并计划公开数据集和代码。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00389v1",
      "published_date": "2025-03-01 07:32:19 UTC",
      "updated_date": "2025-03-01 07:32:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:43:09.734340"
    },
    {
      "arxiv_id": "2503.00387v1",
      "title": "LNUCB-TA: Linear-nonlinear Hybrid Bandit Learning with Temporal Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Hamed Khosravi",
        "Mohammad Reza Shafie",
        "Ahmed Shoyeb Raihan",
        "Srinjoy Das",
        "Imtiaz Ahmed"
      ],
      "abstract": "Existing contextual multi-armed bandit (MAB) algorithms fail to effectively\ncapture both long-term trends and local patterns across all arms, leading to\nsuboptimal performance in environments with rapidly changing reward structures.\nThey also rely on static exploration rates, which do not dynamically adjust to\nchanging conditions. To overcome these limitations, we propose LNUCB-TA, a\nhybrid bandit model integrating a novel nonlinear component (adaptive k-Nearest\nNeighbors (k-NN)) for reducing time complexity, alongside a global-and-local\nattention-based exploration mechanism. Our approach uniquely combines linear\nand nonlinear estimation techniques, with the nonlinear module dynamically\nadjusting k based on reward variance to enhance spatiotemporal pattern\nrecognition. This reduces the likelihood of selecting suboptimal arms while\nimproving reward estimation accuracy and computational efficiency. The\nattention-based mechanism ranks arms by past performance and selection\nfrequency, dynamically adjusting exploration and exploitation in real time\nwithout requiring manual tuning of exploration rates. By integrating global\nattention (assessing all arms collectively) and local attention (focusing on\nindividual arms), LNUCB-TA efficiently adapts to temporal and spatial\ncomplexities. Empirical results show LNUCB-TA significantly outperforms\nstate-of-the-art linear, nonlinear, and hybrid bandits in cumulative and mean\nreward, convergence, and robustness across different exploration rates.\nTheoretical analysis further confirms its reliability with a sub-linear regret\nbound.",
      "tldr_zh": "本研究针对现有 contextual multi-armed bandit (MAB) 算法在捕捉长期趋势和局部模式方面的不足，以及静态探索率的局限性，提出了一种混合模型 LNUCB-TA。LNUCB-TA 结合线性估计与新型非线性组件（adaptive k-Nearest Neighbors (k-NN)），并引入基于全局和局部 attention 的探索机制，以动态调整 k 值并根据过去表现排名 arms，从而提升时空模式识别、奖励估计准确性和计算效率。实验结果表明，LNUCB-TA 在累积和平均奖励、收敛性和鲁棒性上显著优于现有线性、非线性或混合 bandit 算法；理论分析进一步证实其可靠性，具有 sub-linear regret bound。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00387v1",
      "published_date": "2025-03-01 07:24:54 UTC",
      "updated_date": "2025-03-01 07:24:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:43:22.296046"
    },
    {
      "arxiv_id": "2503.00384v1",
      "title": "A Survey of Adversarial Defenses in Vision-based Systems: Categorization, Methods and Challenges",
      "title_zh": "基于视觉系统的对抗防御综述：分类、方法和挑战",
      "authors": [
        "Nandish Chattopadhyay",
        "Abdul Basit",
        "Bassem Ouni",
        "Muhammad Shafique"
      ],
      "abstract": "Adversarial attacks have emerged as a major challenge to the trustworthy\ndeployment of machine learning models, particularly in computer vision\napplications. These attacks have a varied level of potency and can be\nimplemented in both white box and black box approaches. Practical attacks\ninclude methods to manipulate the physical world and enforce adversarial\nbehaviour by the corresponding target neural network models. Multiple different\napproaches to mitigate different kinds of such attacks are available in the\nliterature, each with their own advantages and limitations. In this survey, we\npresent a comprehensive systematization of knowledge on adversarial defenses,\nfocusing on two key computer vision tasks: image classification and object\ndetection. We review the state-of-the-art adversarial defense techniques and\ncategorize them for easier comparison. In addition, we provide a schematic\nrepresentation of these categories within the context of the overall machine\nlearning pipeline, facilitating clearer understanding and benchmarking of\ndefenses. Furthermore, we map these defenses to the types of adversarial\nattacks and datasets where they are most effective, offering practical insights\nfor researchers and practitioners. This study is necessary for understanding\nthe scope of how the available defenses are able to address the adversarial\nthreats, and their shortcomings as well, which is necessary for driving the\nresearch in this area in the most appropriate direction, with the aim of\nbuilding trustworthy AI systems for regular practical use-cases.",
      "tldr_zh": "这篇调查论文系统回顾了对抗防御(adversarial defenses)在基于视觉系统的应用，特别是图像分类和目标检测领域，针对各种对抗攻击(adversarial attacks)如白盒和黑盒方法及其物理世界操纵。论文将现有防御技术分类，便于比较，并通过机器学习管道的示意图展示这些方法的框架，同时映射防御与特定攻击类型和数据集的有效性。最终，它强调了防御的优点和局限性，以推动研究方向，构建更可信赖的AI系统。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00384v1",
      "published_date": "2025-03-01 07:17:18 UTC",
      "updated_date": "2025-03-01 07:17:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:43:32.049432"
    },
    {
      "arxiv_id": "2503.00383v2",
      "title": "Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Song Xia",
        "Yi Yu",
        "Wenhan Yang",
        "Meiwen Ding",
        "Zhuo Chen",
        "Ling-Yu Duan",
        "Alex C. Kot",
        "Xudong Jiang"
      ],
      "abstract": "By locally encoding raw data into intermediate features, collaborative\ninference enables end users to leverage powerful deep learning models without\nexposure of sensitive raw data to cloud servers. However, recent studies have\nrevealed that these intermediate features may not sufficiently preserve\nprivacy, as information can be leaked and raw data can be reconstructed via\nmodel inversion attacks (MIAs). Obfuscation-based methods, such as noise\ncorruption, adversarial representation learning, and information filters,\nenhance the inversion robustness by obfuscating the task-irrelevant redundancy\nempirically. However, methods for quantifying such redundancy remain elusive,\nand the explicit mathematical relation between this redundancy minimization and\ninversion robustness enhancement has not yet been established. To address that,\nthis work first theoretically proves that the conditional entropy of inputs\ngiven intermediate features provides a guaranteed lower bound on the\nreconstruction mean square error (MSE) under any MIA. Then, we derive a\ndifferentiable and solvable measure for bounding this conditional entropy based\non the Gaussian mixture estimation and propose a conditional entropy\nmaximization (CEM) algorithm to enhance the inversion robustness. Experimental\nresults on four datasets demonstrate the effectiveness and adaptability of our\nproposed CEM; without compromising feature utility and computing efficiency,\nplugging the proposed CEM into obfuscation-based defense mechanisms\nconsistently boosts their inversion robustness, achieving average gains ranging\nfrom 12.9\\% to 48.2\\%. Code is available at\n\\href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}.",
      "tldr_zh": "本研究探讨了协作推理系统中模型反演攻击（MIAs）的隐私风险问题，通过理论分析证明了输入给定中间特征的条件熵（conditional entropy）为重建均方误差（MSE）提供了一个可靠的下界，从而建立了冗余最小化与反演鲁棒性增强的数学关系。作者提出了一种基于高斯混合估计的条件熵最大化（CEM）算法，用于量化并优化这一熵值，以提升现有混淆方法（如噪声添加和信息过滤）的鲁棒性，同时不影响特征效用和计算效率。在四个数据集上的实验结果显示，CEM 平均提高了12.9%至48.2%的反演鲁棒性，证明了其有效性和适应性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00383v2",
      "published_date": "2025-03-01 07:15:21 UTC",
      "updated_date": "2025-04-03 05:50:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:43:46.395405"
    },
    {
      "arxiv_id": "2503.00378v1",
      "title": "Conditioning on Local Statistics for Scalable Heterogeneous Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rickard Brännvall"
      ],
      "abstract": "Federated learning is a distributed machine learning approach where multiple\nclients collaboratively train a model without sharing their local data, which\ncontributes to preserving privacy. A challenge in federated learning is\nmanaging heterogeneous data distributions across clients, which can hinder\nmodel convergence and performance due to the need for the global model to\ngeneralize well across diverse local datasets. We propose to use local\ncharacteristic statistics, by which we mean some statistical properties\ncalculated independently by each client using only their local training\ndataset. These statistics, such as means, covariances, and higher moments, are\nused to capture the characteristics of the local data distribution. They are\nnot shared with other clients or a central node. During training, these local\nstatistics help the model learn how to condition on the local data\ndistribution, and during inference, they guide the client's predictions. Our\nexperiments show that this approach allows for efficient handling of\nheterogeneous data across the federation, has favorable scaling compared to\napproaches that directly try to identify peer nodes that share distribution\ncharacteristics, and maintains privacy as no additional information needs to be\ncommunicated.",
      "tldr_zh": "这篇论文针对Federated Learning中的heterogeneous data分布问题，提出了一种可扩展的方法，即利用每个客户端基于本地数据集计算的local characteristic statistics（如means、covariances和更高阶矩）来条件化模型训练和推理。这些统计仅在本地使用，不需共享，从而帮助模型更好地适应多样化的数据分布，同时保持隐私。实验结果显示，该方法在处理数据异质性时比直接识别相似节点的基准方法更高效，且具有更好的可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC",
        "68T09 (Primary) 68T05 (Secondary)",
        "D.4.6; K.6.5; I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 2 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.00378v1",
      "published_date": "2025-03-01 07:10:58 UTC",
      "updated_date": "2025-03-01 07:10:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:43:57.646999"
    },
    {
      "arxiv_id": "2503.00374v2",
      "title": "MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyi Wang",
        "Jianan Fan",
        "Dingxin Zhang",
        "Dongnan Liu",
        "Yong Xia",
        "Heng Huang",
        "Weidong Cai"
      ],
      "abstract": "Histopathology and transcriptomics are fundamental modalities in oncology,\nencapsulating the morphological and molecular aspects of the disease.\nMulti-modal self-supervised learning has demonstrated remarkable potential in\nlearning pathological representations by integrating diverse data sources.\nConventional multi-modal integration methods primarily emphasize modality\nalignment, while paying insufficient attention to retaining the\nmodality-specific structures. However, unlike conventional scenarios where\nmulti-modal inputs share highly overlapping features, histopathology and\ntranscriptomics exhibit pronounced heterogeneity, offering orthogonal yet\ncomplementary insights. Histopathology provides morphological and spatial\ncontext, elucidating tissue architecture and cellular topology, whereas\ntranscriptomics delineates molecular signatures through gene expression\npatterns. This inherent disparity introduces a major challenge in aligning them\nwhile maintaining modality-specific fidelity. To address these challenges, we\npresent MIRROR, a novel multi-modal representation learning method designed to\nfoster both modality alignment and retention. MIRROR employs dedicated encoders\nto extract comprehensive features for each modality, which is further\ncomplemented by a modality alignment module to achieve seamless integration\nbetween phenotype patterns and molecular profiles. Furthermore, a modality\nretention module safeguards unique attributes from each modality, while a style\nclustering module mitigates redundancy and enhances disease-relevant\ninformation by modeling and aligning consistent pathological signatures within\na clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping\nand survival analysis highlight MIRROR's superior performance, demonstrating\nits effectiveness in constructing comprehensive oncological feature\nrepresentations and benefiting the cancer diagnosis.",
      "tldr_zh": "该论文提出 MIRROR，一种新型多模式自监督表示学习方法，用于整合组织病理学（histopathology）和转录组学（transcriptomics），以解决传统多模式整合方法在模式对齐（modality alignment）时忽略模式特定结构（modality-specific structures）的问题。MIRROR 采用专用编码器提取每个模式的特征，并通过模式对齐模块、模式保留模块和风格聚类模块（style clustering module）来实现模式的无缝整合，同时保留独特属性并减少冗余。实验结果显示，在 TCGA 队列上的癌症亚型和生存分析任务中，MIRROR 表现出优越性能，有助于构建全面的肿瘤学特征表示并提升癌症诊断准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures, 4 tables. Code available at\n  https://github.com/TianyiFranklinWang/MIRROR. Project page:\n  https://tianyifranklinwang.github.io/MIRROR",
      "pdf_url": "http://arxiv.org/pdf/2503.00374v2",
      "published_date": "2025-03-01 07:02:30 UTC",
      "updated_date": "2025-03-19 02:50:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:44:10.297914"
    },
    {
      "arxiv_id": "2503.00372v1",
      "title": "Nucleolus Credit Assignment for Effective Coalitions in Multi-agent Reinforcement Learning",
      "title_zh": "Nucleolus 信用分配用于多智能体强化学习中的有效联盟",
      "authors": [
        "Yugu Li",
        "Zehong Cao",
        "Jianglin Qiao",
        "Siyi Hu"
      ],
      "abstract": "In cooperative multi-agent reinforcement learning (MARL), agents typically\nform a single grand coalition based on credit assignment to tackle a composite\ntask, often resulting in suboptimal performance. This paper proposed a\nnucleolus-based credit assignment grounded in cooperative game theory, enabling\nthe autonomous partitioning of agents into multiple small coalitions that can\neffectively identify and complete subtasks within a larger composite task.\nSpecifically, our designed nucleolus Q-learning could assign fair credits to\neach agent, and the nucleolus Q-operator provides theoretical guarantees with\ninterpretability for both learning convergence and the stability of the formed\nsmall coalitions. Through experiments on Predator-Prey and StarCraft scenarios\nacross varying difficulty levels, our approach demonstrated the emergence of\nmultiple effective coalitions during MARL training, leading to faster learning\nand superior performance in terms of win rate and cumulative rewards especially\nin hard and super-hard environments, compared to four baseline methods. Our\nnucleolus-based credit assignment showed the promise for complex composite\ntasks requiring effective subteams of agents.",
      "tldr_zh": "在合作性多智能体强化学习（MARL）中，代理通常形成单一大联盟来处理复合任务，但这可能导致性能不佳，本文提出了一种基于核（nucleolus）的信用分配方法，源自合作博弈理论。 该方法允许代理自主分成多个小联盟，以识别和完成复合任务中的子任务，并通过核 Q-学习（nucleolus Q-learning）和核 Q-操作符（nucleolus Q-operator）分配公平信用，同时提供学习收敛和联盟稳定的理论保证。 在 Predator-Prey 和 StarCraft 场景的实验中，该方法促进了多个有效联盟的出现，实现更快学习和更高胜率、累积奖励，尤其在困难环境中比四种基线方法表现出色。 总体上，这种 nucleolus-based 信用分配为处理需要子团队的复杂 MARL 任务提供了新前景。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00372v1",
      "published_date": "2025-03-01 07:01:58 UTC",
      "updated_date": "2025-03-01 07:01:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:44:23.461436"
    },
    {
      "arxiv_id": "2504.03651v1",
      "title": "Echo: Efficient Co-Scheduling of Hybrid Online-Offline Tasks for Large Language Model Serving",
      "title_zh": "Echo：高效混合在线-离线任务联合调度，用于大型语言模型服务",
      "authors": [
        "Zhibin Wang",
        "Shipeng Li",
        "Xue Li",
        "Yuhang Zhou",
        "Zhonghui Zhang",
        "Zibo Wang",
        "Rong Gu",
        "Chen Tian",
        "Kun Yang",
        "Sheng Zhong"
      ],
      "abstract": "Large language models have been widely deployed in various applications,\nencompassing both interactive online tasks and batched offline tasks. Given the\nburstiness and latency sensitivity of online tasks, over-provisioning resources\nis common practice. This allows for the integration of latency-insensitive\noffline tasks during periods of low online load, enhancing resource\nutilization. However, strategically serving online and offline tasks through a\npreemption mechanism fails to fully leverage the flexibility of offline tasks\nand suffers from KV cache recomputation and irregular workloads.\n  In this paper, we introduce Echo, a collaborative online-offline task serving\nsystem, including a scheduler, a KV cache manager, and estimation toolkits. The\nscheduler and KV cache manager work tightly to maximize the throughput of\noffline tasks, while the estimator further predicts execution time to ensure\nonline task SLOs. The scheduler leverages the batch information of last\niteration to reduce the search space for finding the optimal schedule. The KV\ncache manager sets the priority of the KV cache based on the type of tasks and\nthe opportunity of prefix sharing to reduce the recomputation. Finally, the\nestimation toolkits predict the execution time, future memory consumption, and\nthe throughput of offline tasks to guide the scheduler, KV cache manager, and\nthe system deployer. Evaluation based on real-world workloads demonstrates that\nEcho can increase offline task throughput by up to $3.3\\times$, while\nsatisfying online task SLOs.",
      "tldr_zh": "这篇论文提出 Echo 系统，用于高效调度大型语言模型（Large Language Model）的在线和离线混合任务，解决传统预占机制导致的 KV cache 重新计算和资源利用不均问题。Echo 包括调度器、KV cache 管理器和估计工具，调度器利用上一次迭代的批量信息缩小搜索空间，KV cache 管理器基于任务类型和前缀共享机会设置优先级以减少重新计算，而估计工具预测执行时间、内存消耗和离线任务吞吐量，确保在线任务的服务水平目标（SLO）。实验结果显示，在真实工作负载下，Echo 将离线任务吞吐量提高高达 3.3 倍，同时满足在线任务的延迟要求。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03651v1",
      "published_date": "2025-03-01 06:53:04 UTC",
      "updated_date": "2025-03-01 06:53:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:44:34.099927"
    },
    {
      "arxiv_id": "2504.06269v1",
      "title": "EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation Detection with Hierarchical Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Yin Wu",
        "Zhengxuan Zhang",
        "Fuling Wang",
        "Yuyu Luo",
        "Hui Xiong",
        "Nan Tang"
      ],
      "abstract": "Misinformation continues to pose a significant challenge in today's\ninformation ecosystem, profoundly shaping public perception and behavior. Among\nits various manifestations, Out-of-Context (OOC) misinformation is particularly\nobscure, as it distorts meaning by pairing authentic images with misleading\ntextual narratives. Existing methods for detecting OOC misinformation\npredominantly rely on coarse-grained similarity metrics between image-text\npairs, which often fail to capture subtle inconsistencies or provide meaningful\nexplainability. While multi-modal large language models (MLLMs) demonstrate\nremarkable capabilities in visual reasoning and explanation generation, they\nhave not yet demonstrated the capacity to address complex, fine-grained, and\ncross-modal distinctions necessary for robust OOC detection. To overcome these\nlimitations, we introduce EXCLAIM, a retrieval-based framework designed to\nleverage external knowledge through multi-granularity index of multi-modal\nevents and entities. Our approach integrates multi-granularity contextual\nanalysis with a multi-agent reasoning architecture to systematically evaluate\nthe consistency and integrity of multi-modal news content. Comprehensive\nexperiments validate the effectiveness and resilience of EXCLAIM, demonstrating\nits ability to detect OOC misinformation with 4.3% higher accuracy compared to\nstate-of-the-art approaches, while offering explainable and actionable\ninsights.",
      "tldr_zh": "该研究针对 Out-of-Context (OOC) 错误信息问题，提出 EXCLAIM 系统，这是一个基于 Hierarchical Retrieval 的可解释跨模态智能体框架，用于检测图像与文本配对中的细微不一致性。EXCLAIM 通过多粒度索引的多模态事件和实体进行外部知识检索，并结合多智能体推理架构，对多模态新闻内容的一致性和完整性进行系统评估。实验结果显示，该系统比最先进方法准确率提高 4.3%，并提供可解释性和可操作洞见。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "15 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.06269v1",
      "published_date": "2025-03-01 06:32:27 UTC",
      "updated_date": "2025-03-01 06:32:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:44:45.510055"
    },
    {
      "arxiv_id": "2503.00366v1",
      "title": "AI-Augmented Thyroid Scintigraphy for Robust Classification",
      "title_zh": "AI 增强的甲状腺闪烁扫描用于鲁棒分类",
      "authors": [
        "Maziar Sabouri",
        "Ghasem Hajianfar",
        "Alireza Rafiei Sardouei",
        "Milad Yazdani",
        "Azin Asadzadeh",
        "Soroush Bagheri",
        "Mohsen Arabi",
        "Seyed Rasoul Zakavi",
        "Emran Askari",
        "Atena Aghaee",
        "Dena Shahriari",
        "Habib Zaidi",
        "Arman Rahmim"
      ],
      "abstract": "Thyroid scintigraphy is a key imaging modality for diagnosing thyroid\ndisorders. Deep learning models for thyroid scintigraphy classification often\nface challenges due to limited and imbalanced datasets, leading to suboptimal\ngeneralization. In this study, we investigate the effectiveness of different\ndata augmentation techniques including Stable Diffusion (SD), Flow Matching\n(FM), and Conventional Augmentation (CA) to enhance the performance of a\nResNet18 classifier for thyroid condition classification. Our results showed\nthat FM-based augmentation consistently outperforms SD-based approaches,\nparticularly when combined with original (O) data and CA (O+FM+CA), achieving\nboth high accuracy and fair classification across Diffuse Goiter (DG), Nodular\nGoiter (NG), Normal (NL), and Thyroiditis (TI) cases. The Wilcoxon statistical\nanalysis further validated the superiority of O+FM and its variants (O+FM+CA)\nover SD-based augmentations in most scenarios. These findings highlight the\npotential of FM-based augmentation as a superior approach for generating\nhigh-quality synthetic thyroid scintigraphy images and improving model\ngeneralization in medical image classification.",
      "tldr_zh": "本研究针对甲状腺闪烁扫描图像分类中数据集有限和不平衡的问题，探讨了AI增强技术来提升ResNet18分类器的性能。研究比较了Stable Diffusion (SD)、Flow Matching (FM)和Conventional Augmentation (CA)三种数据增强方法，结果显示FM-based增强，尤其是与原始数据(O)结合（如O+FM+CA），在Diffuse Goiter (DG)、Nodular Goiter (NG)、Normal (NL)和Thyroiditis (TI)病例中实现了更高的准确性和公平性。Wilcoxon统计分析进一步证实了O+FM及其变体优于SD-based方法的表现。这些发现突显了FM-based增强在生成高质量合成图像和改善医疗图像分类模型泛化方面的潜力。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00366v1",
      "published_date": "2025-03-01 06:21:46 UTC",
      "updated_date": "2025-03-01 06:21:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:45:00.105205"
    },
    {
      "arxiv_id": "2503.00361v1",
      "title": "Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding",
      "title_zh": "Octopus：通过动态对比解码缓解幻觉",
      "authors": [
        "Wei Suo",
        "Lijun Zhang",
        "Mengyang Sun",
        "Lin Yuanbo Wu",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have obtained impressive performance in\nvisual content understanding and multi-modal reasoning. Unfortunately, these\nlarge models suffer from serious hallucination problems and tend to generate\nfabricated responses. Recently, several Contrastive Decoding (CD) strategies\nhave been proposed to alleviate hallucination by introducing disturbed inputs.\nAlthough great progress has been made, these CD strategies mostly apply a\none-size-fits-all approach for all input conditions. In this paper, we revisit\nthis process through extensive experiments. Related results show that\nhallucination causes are hybrid and each generative step faces a unique\nhallucination challenge. Leveraging these meaningful insights, we introduce a\nsimple yet effective Octopus-like framework that enables the model to\nadaptively identify hallucination types and create a dynamic CD workflow. Our\nOctopus framework not only outperforms existing methods across four benchmarks\nbut also demonstrates excellent deployability and expansibility. Code is\navailable at https://github.com/LijunZhang01/Octopus.",
      "tldr_zh": "该研究针对 Large Vision-Language Models (LVLMs) 的 hallucination 问题，指出现有 Contrastive Decoding (CD) 策略采用一刀切的干扰输入方法，无法适应不同生成步骤的独特挑战。通过实验分析发现，hallucination 原因多样且动态变化，进而提出 Octopus 框架——一个简单有效的系统，能自适应识别 hallucination 类型并动态构建 CD 工作流。实验结果显示，Octopus 在四个基准上优于现有方法，并展现出优秀的部署性和扩展性，代码已开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00361v1",
      "published_date": "2025-03-01 06:00:34 UTC",
      "updated_date": "2025-03-01 06:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:45:10.069973"
    },
    {
      "arxiv_id": "2503.00358v1",
      "title": "CRUPL: A Semi-Supervised Cyber Attack Detection with Consistency Regularization and Uncertainty-aware Pseudo-Labeling in Smart Grid",
      "title_zh": "翻译失败",
      "authors": [
        "Smruti P. Dash",
        "Kedar V. Khandeparkar",
        "Nipun Agrawal"
      ],
      "abstract": "The modern power grids are integrated with digital technologies and\nautomation systems. The inclusion of digital technologies has made the smart\ngrids vulnerable to cyber-attacks. Cyberattacks on smart grids can compromise\ndata integrity and jeopardize the reliability of the power supply. Traditional\nintrusion detection systems often need help to effectively detect novel and\nsophisticated attacks due to their reliance on labeled training data, which may\nonly encompass part of the spectrum of potential threats. This work proposes a\nsemi-supervised method for cyber-attack detection in smart grids by leveraging\nthe labeled and unlabeled measurement data. We implement consistency\nregularization and pseudo-labeling to identify deviations from expected\nbehavior and predict the attack classes. We use a curriculum learning approach\nto improve pseudo-labeling performance, capturing the model uncertainty. We\ndemonstrate the efficiency of the proposed method in detecting different types\nof cyberattacks, minimizing the false positives by implementing them on\npublicly available datasets. The method proposes a promising solution by\nimproving the detection accuracy to 99% in the presence of unknown samples and\nsignificantly reducing false positives.",
      "tldr_zh": "本研究针对智能电网中网络攻击检测的挑战，提出了一种半监督方法CRUPL，利用标记和未标记数据来识别异常行为。方法结合Consistency Regularization确保模型输出的一致性，并采用Uncertainty-aware Pseudo-Labeling通过课程学习（curriculum learning）处理模型不确定性，从而改进伪标签的准确性。实验结果显示，该方法在公开数据集上实现了99%的检测准确率，尤其在未知样本中显著降低了假阳性率，为高效的智能电网网络攻击检测提供了可靠解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "68T07 (Primary), 68T27, 68T37 (Secondary)",
        "I.2.m"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.00358v1",
      "published_date": "2025-03-01 05:49:23 UTC",
      "updated_date": "2025-03-01 05:49:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:45:20.847888"
    },
    {
      "arxiv_id": "2503.00356v1",
      "title": "BERT-based model for Vietnamese Fact Verification Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Bao Tran",
        "T. N. Khanh",
        "Khang Nguyen Tuong",
        "Thien Dang",
        "Quang Nguyen",
        "Nguyen T. Thinh",
        "Vo T. Hung"
      ],
      "abstract": "The rapid advancement of information and communication technology has\nfacilitated easier access to information. However, this progress has also\nnecessitated more stringent verification measures to ensure the accuracy of\ninformation, particularly within the context of Vietnam. This paper introduces\nan approach to address the challenges of Fact Verification using the Vietnamese\ndataset by integrating both sentence selection and classification modules into\na unified network architecture. The proposed approach leverages the power of\nlarge language models by utilizing pre-trained PhoBERT and XLM-RoBERTa as the\nbackbone of the network. The proposed model was trained on a Vietnamese\ndataset, named ISE-DSC01, and demonstrated superior performance compared to the\nbaseline model across all three metrics. Notably, we achieved a Strict Accuracy\nlevel of 75.11\\%, indicating a remarkable 28.83\\% improvement over the baseline\nmodel.",
      "tldr_zh": "本研究针对越南语事实验证(Fact Verification)的挑战，提出了一种基于BERT的模型，使用预训练模型PhoBERT和XLM-RoBERTa作为骨干网络，并将句子选择和分类模块整合到一个统一架构中。该模型在ISE-DSC01数据集上进行训练，显著优于基线模型，在所有三个评估指标上表现出色，特别是Strict Accuracy达到75.11%，较基线提升28.83%。这项工作为越南语信息验证提供了更有效的解决方案，提升了模型的准确性和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted for Oral Presentation in CITA 2024 (The 13th Conference on\n  Information Technology and Its Applications) and will be published in VOLUME\n  1 OF CITA 2024 (Volume of the Lecture Notes in Network and Systems, Springer)",
      "pdf_url": "http://arxiv.org/pdf/2503.00356v1",
      "published_date": "2025-03-01 05:31:04 UTC",
      "updated_date": "2025-03-01 05:31:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:45:32.997244"
    },
    {
      "arxiv_id": "2503.00355v1",
      "title": "Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyi Huang",
        "Elsa Fan"
      ],
      "abstract": "From disinformation spread by AI chatbots to AI recommendations that\ninadvertently reinforce stereotypes, textual bias poses a significant challenge\nto the trustworthiness of large language models (LLMs). In this paper, we\npropose a multi-agent framework that systematically identifies biases by\ndisentangling each statement as fact or opinion, assigning a bias intensity\nscore, and providing concise, factual justifications. Evaluated on 1,500\nsamples from the WikiNPOV dataset, the framework achieves 84.9%\naccuracy$\\unicode{x2014}$an improvement of 13.0% over the zero-shot\nbaseline$\\unicode{x2014}$demonstrating the efficacy of explicitly modeling fact\nversus opinion prior to quantifying bias intensity. By combining enhanced\ndetection accuracy with interpretable explanations, this approach sets a\nfoundation for promoting fairness and accountability in modern language models.",
      "tldr_zh": "这篇论文提出了一种多智能体框架（multi-agent framework），用于系统识别文本数据中的偏见（textual bias），旨在提升大型语言模型（LLMs）的公平性和可信度。该框架通过将每个语句分解为事实（fact）或意见（opinion）、分配偏见强度分数（bias intensity score），并提供简洁的事实性理由，来实现偏见的检测和量化。在 WikiNPOV 数据集的 1,500 个样本上，该方法实现了 84.9% 的准确率，比零样本基线（zero-shot baseline）提高了 13.0%。总体而言，这种结构化推理方法为促进 LLMs 的公平性、责任性和可解释性奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted Paper (Oral Presentation) in the Workshop on the Social\n  Impact of AI: Research, Diversity and Inclusion Frameworks at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00355v1",
      "published_date": "2025-03-01 05:27:54 UTC",
      "updated_date": "2025-03-01 05:27:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:45:46.395290"
    },
    {
      "arxiv_id": "2503.00334v1",
      "title": "MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising",
      "title_zh": "翻译失败",
      "authors": [
        "Quanyu Dai",
        "Jiaren Xiao",
        "Zhaocheng Du",
        "Jieming Zhu",
        "Chengxiao Luo",
        "Xiao-Ming Wu",
        "Zhenhua Dong"
      ],
      "abstract": "In online advertising, uncertainty calibration aims to adjust a ranking\nmodel's probability predictions to better approximate the true likelihood of an\nevent, e.g., a click or a conversion. However, existing calibration approaches\nmay lack the ability to effectively model complex nonlinear relations, consider\ncontext features, and achieve balanced performance across different data\nsubsets. To tackle these challenges, we introduce a novel model called\nMonotonic Calibration Networks, featuring three key designs: a monotonic\ncalibration function (MCF), an order-preserving regularizer, and a\nfield-balance regularizer. The nonlinear MCF is capable of naturally modeling\nand universally approximating the intricate relations between uncalibrated\npredictions and the posterior probabilities, thus being much more expressive\nthan existing methods. MCF can also integrate context features using a flexible\nmodel architecture, thereby achieving context awareness. The order-preserving\nand field-balance regularizers promote the monotonic relationship between\nadjacent bins and the balanced calibration performance on data subsets,\nrespectively. Experimental results on both public and industrial datasets\ndemonstrate the superior performance of our method in generating\nwell-calibrated probability predictions.",
      "tldr_zh": "本文提出 Monotonic Calibration Networks (MCNet)，一种新型模型，用于在线广告中的不确定性校准，以更好地调整排名模型的概率预测，使其更接近真实事件可能性。MCNet 包括三个关键设计：非线性 Monotonic Calibration Function (MCF) 用于建模复杂关系并整合上下文特征、order-preserving regularizer 确保相邻预测间的单调性，以及 field-balance regularizer 实现数据子集上的平衡性能。实验在公共和工业数据集上显示，MCNet 显著优于现有方法，提供更精确的校准概率预测。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "H.0"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WWW2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00334v1",
      "published_date": "2025-03-01 03:54:58 UTC",
      "updated_date": "2025-03-01 03:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:45:58.389073"
    },
    {
      "arxiv_id": "2503.01910v1",
      "title": "dyAb: Flow Matching for Flexible Antibody Design with AlphaFold-driven Pre-binding Antigen",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Tan",
        "Yijie Zhang",
        "Zhangyang Gao",
        "Yufei Huang",
        "Haitao Lin",
        "Lirong Wu",
        "Fandi Wu",
        "Mathieu Blanchette",
        "Stan. Z. Li"
      ],
      "abstract": "The development of therapeutic antibodies heavily relies on accurate\npredictions of how antigens will interact with antibodies. Existing\ncomputational methods in antibody design often overlook crucial conformational\nchanges that antigens undergo during the binding process, significantly\nimpacting the reliability of the resulting antibodies. To bridge this gap, we\nintroduce dyAb, a flexible framework that incorporates AlphaFold2-driven\npredictions to model pre-binding antigen structures and specifically addresses\nthe dynamic nature of antigen conformation changes. Our dyAb model leverages a\nunique combination of coarse-grained interface alignment and fine-grained flow\nmatching techniques to simulate the interaction dynamics and structural\nevolution of the antigen-antibody complex, providing a realistic representation\nof the binding process. Extensive experiments show that dyAb significantly\noutperforms existing models in antibody design involving changing antigen\nconformations. These results highlight dyAb's potential to streamline the\ndesign process for therapeutic antibodies, promising more efficient development\ncycles and improved outcomes in clinical applications.",
      "tldr_zh": "本文提出 dyAb 框架，利用 AlphaFold2 驱动的预结合抗原预测，解决现有抗体设计方法忽略抗原构象变化的问题，从而提高抗体与抗原交互的准确性。dyAb 结合粗粒度界面对齐和细粒度 flow matching 技术，模拟抗原-抗体复合物的动态交互和结构演化，提供更真实的结合过程表示。实验结果显示，dyAb 在涉及抗原构象变化的抗体设计中显著优于现有模型，有望简化治疗性抗体开发过程，提高临床应用效率。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "AAAI 2025 Oral",
      "pdf_url": "http://arxiv.org/pdf/2503.01910v1",
      "published_date": "2025-03-01 03:53:18 UTC",
      "updated_date": "2025-03-01 03:53:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:46:10.778354"
    },
    {
      "arxiv_id": "2503.00333v1",
      "title": "More of the Same: Persistent Representational Harms Under Increased Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Jennifer Mickel",
        "Maria De-Arteaga",
        "Leqi Liu",
        "Kevin Tian"
      ],
      "abstract": "To recognize and mitigate the harms of generative AI systems, it is crucial\nto consider who is represented in the outputs of generative AI systems and how\npeople are represented. A critical gap emerges when naively improving who is\nrepresented, as this does not imply bias mitigation efforts have been applied\nto address how people are represented. We critically examined this by\ninvestigating gender representation in occupation across state-of-the-art large\nlanguage models. We first show evidence suggesting that over time there have\nbeen interventions to models altering the resulting gender distribution, and we\nfind that women are more represented than men when models are prompted to\ngenerate biographies or personas. We then demonstrate that representational\nbiases persist in how different genders are represented by examining\nstatistically significant word differences across genders. This results in a\nproliferation of representational harms, stereotypes, and neoliberalism ideals\nthat, despite existing interventions to increase female representation,\nreinforce existing systems of oppression.",
      "tldr_zh": "这篇论文探讨了生成式 AI 系统中的代表性问题，指出尽管通过干预措施（如调整模型）增加了女性在职业传记或角色中的代表（如女性比男性更常见），但这并不能消除代表性偏差。研究者通过分析大型语言模型(LLMs)的性别分布和统计显著的词汇差异，揭示了持续存在的代表性伤害和刻板印象。结果显示，这些偏差加剧了刻板印象、新自由主义理想的传播，并强化了现有的压迫系统。总的来说，该工作强调了单纯提高“谁被代表”不足以解决“如何被代表”的核心问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 7 figures, 6 tables, pre-print",
      "pdf_url": "http://arxiv.org/pdf/2503.00333v1",
      "published_date": "2025-03-01 03:45:35 UTC",
      "updated_date": "2025-03-01 03:45:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:46:23.003567"
    },
    {
      "arxiv_id": "2503.00332v2",
      "title": "Investigating the contribution of terrain-following coordinates and conservation schemes in AI-driven precipitation forecasts",
      "title_zh": "翻译失败",
      "authors": [
        "Yingkai Sha",
        "John S. Schreck",
        "William Chapman",
        "David John Gagne II"
      ],
      "abstract": "Artificial Intelligence (AI) weather prediction (AIWP) models often produce\n\"blurry\" precipitation forecasts that overestimate drizzle and underestimate\nextremes. This study provides a novel solution to tackle this problem --\nintegrating terrain-following coordinates with global mass and energy\nconservation schemes into AIWP models. Forecast experiments are conducted to\nevaluate the effectiveness of this solution using FuXi, an example AIWP model,\nadapted to 1.0-degree grid spacing data. Verification results show large\nperformance gains. The conservation schemes are found to reduce drizzle bias,\nwhereas using terrain-following coordinates improves the estimation of extreme\nevents and precipitation intensity spectra. Furthermore, a case study reveals\nthat terrain-following coordinates capture near-surface winds better over\nmountains, offering AIWP models more accurate information on understanding the\ndynamics of precipitation processes. The proposed solution of this study can\nbenefit a wide range of AIWP models and bring insights into how atmospheric\ndomain knowledge can support the development of AIWP models.",
      "tldr_zh": "这篇论文探讨了在AI驱动的降水预报（AIWP）模型中整合terrain-following coordinates和conservation schemes，以解决模型产生的模糊预报问题，如过度估计细雨和低估极端事件。研究通过在FuXi模型上进行实验，验证了这些方案的有效性，结果显示conservation schemes显著减少了细雨偏差，而terrain-following coordinates改善了极端事件估计、降水强度谱和山区近地表风的捕捉。总体而言，这一解决方案为提升AIWP模型的准确性和可靠性提供了新见解，并可广泛应用于其他AIWP模型的发展。",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00332v2",
      "published_date": "2025-03-01 03:44:46 UTC",
      "updated_date": "2025-03-17 16:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:46:35.685270"
    },
    {
      "arxiv_id": "2503.00331v2",
      "title": "PINN-DT: Optimizing Energy Consumption in Smart Building Using Hybrid Physics-Informed Neural Networks and Digital Twin Framework with Blockchain Security",
      "title_zh": "PINN-DT：利用混合物理信息神经网络和数字孪生框架优化智能建筑能源消耗，并融入区块链安全",
      "authors": [
        "Hajar Kazemi Naeini",
        "Roya Shomali",
        "Abolhassan Pishahang",
        "Hamidreza Hasanzadeh",
        "Mahdieh Mohammadi",
        "Saeed Asadi",
        "Abbas Varmaghani",
        "Ahmad Gholizadeh Lonbar"
      ],
      "abstract": "The advancement of smart grid technologies necessitates the integration of\ncutting-edge computational methods to enhance predictive energy optimization.\nThis study proposes a multi-faceted approach by incorporating (1) Deep\nReinforcement Learning (DRL) agents trained using data from Digital Twins (DTs)\nto optimize energy consumption in real time, (2) Physics-Informed Neural\nNetworks (PINNs) to seamlessly embed physical laws within the optimization\nprocess, ensuring model accuracy and interpretability, and (3) Blockchain (BC)\ntechnology to facilitate secure and transparent communication across the smart\ngrid infrastructure. The model was trained and validated using comprehensive\ndatasets, including smart meter energy consumption data, renewable energy\noutputs, dynamic pricing, and user preferences collected from IoT devices. The\nproposed framework achieved superior predictive performance with a Mean\nAbsolute Error (MAE) of 0.237 kWh, Root Mean Square Error (RMSE) of 0.298 kWh,\nand an R-squared (R2) value of 0.978, indicating a 97.8% explanation of data\nvariance. Classification metrics further demonstrated the model's robustness,\nachieving 97.7% accuracy, 97.8% precision, 97.6% recall, and an F1 Score of\n97.7%. Comparative analysis with traditional models like Linear Regression,\nRandom Forest, SVM, LSTM, and XGBoost revealed the superior accuracy and\nreal-time adaptability of the proposed method. In addition to enhancing energy\nefficiency, the model reduced energy costs by 35%, maintained a 96% user\ncomfort index, and increased renewable energy utilization to 40%. This study\ndemonstrates the transformative potential of integrating PINNs, DT, and\nBlockchain technologies to optimize energy consumption in smart grids, paving\nthe way for sustainable, secure, and efficient energy management systems.",
      "tldr_zh": "本研究提出了一种名为 PINN-DT 的框架，用于优化智能建筑的能源消耗，该框架结合 Deep Reinforcement Learning (DRL) 代理与 Digital Twins (DTs) 数据进行实时优化、Physics-Informed Neural Networks (PINNs) 嵌入物理定律以提升模型准确性和可解释性，以及 Blockchain (BC) 技术确保通信的安全性和透明度。模型使用智能电表数据、可再生能源输出、动态定价和用户偏好等数据集进行训练和验证，取得了优异的性能指标，包括 Mean Absolute Error (MAE) 0.237 kWh、Root Mean Square Error (RMSE) 0.298 kWh 和 R-squared (R2) 值 0.978，以及分类指标如准确率 97.7% 和 F1 Score 97.7%。与传统模型如 Linear Regression、Random Forest、SVM、LSTM 和 XGBoost 相比，该方法显示出更高的准确性和实时适应性，最终实现了能源成本降低 35%、用户舒适指数达 96% 以及可再生能源利用率提高到 40%，为可持续、安全的智能电网管理提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00331v2",
      "published_date": "2025-03-01 03:37:09 UTC",
      "updated_date": "2025-05-18 22:36:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:46:48.397864"
    },
    {
      "arxiv_id": "2503.04792v1",
      "title": "Cross-linguistic disagreement as a conflict of semantic alignment norms in multilingual AI~Linguistic Diversity as a Problem for Philosophy, Cognitive Science, and AI~",
      "title_zh": "翻译失败",
      "authors": [
        "Masaharu Mizumoto",
        "Dat Tien Nguyen",
        "Justin Sytsma",
        "Mark Alfano",
        "Yu Izumi",
        "Koji Fujita",
        "Nguyen Le Minh"
      ],
      "abstract": "Multilingual large language models (LLMs) face an often-overlooked challenge\nstemming from intrinsic semantic differences across languages. Linguistic\ndivergence can sometimes lead to cross-linguistic disagreements--disagreements\npurely due to semantic differences about a relevant concept. This paper\nidentifies such disagreements as conflicts between two fundamental alignment\nnorms in multilingual LLMs: cross-linguistic consistency (CL-consistency),\nwhich seeks universal concepts across languages, and consistency with folk\njudgments (Folk-consistency), which respects language-specific semantic norms.\nThrough examining responses of conversational multilingual AIs in English and\nJapanese with the cases used in philosophy (cases of knowledge-how\nattributions), this study demonstrates that even state-of-the-art LLMs provide\ndivergent and internally inconsistent responses. Such findings reveal a novel\nqualitative limitation in crosslingual knowledge transfer, or conceptual\ncrosslingual knowledge barriers, challenging the assumption that universal\nrepresentations and cross-linguistic transfer capabilities are inherently\ndesirable. Moreover, they reveal conflicts of alignment policies of their\ndevelopers, highlighting critical normative questions for LLM researchers and\ndevelopers. The implications extend beyond technical alignment challenges,\nraising normative, moral-political, and metaphysical questions about the ideals\nunderlying AI development--questions that are shared with philosophers and\ncognitive scientists but for which no one yet has definitive answers, inviting\na multidisciplinary approach to balance the practical benefits of\ncross-linguistic consistency and respect for linguistic diversity.",
      "tldr_zh": "本论文探讨了多语言大型语言模型 (multilingual LLMs) 面临的跨语言分歧问题，这些分歧源于语言间的内在语义差异，并将其视为跨语言一致性 (CL-consistency) 与民间判断一致性 (Folk-consistency) 两种对齐规范之间的冲突。研究通过分析英语和日语中对话式多语言 AI 的响应，使用哲学案例（如知识-如何归属）来展示最先进 LLMs 的响应存在分歧和内部不一致，揭示了概念跨语言知识障碍。最终，该研究挑战了普遍表示和跨语言转移的假设，引发关于 AI 开发规范、道德-政治和形而上学问题的讨论，并呼吁多学科方法平衡跨语言一致性与语言多样性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04792v1",
      "published_date": "2025-03-01 03:31:40 UTC",
      "updated_date": "2025-03-01 03:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:46:59.445479"
    },
    {
      "arxiv_id": "2503.00323v1",
      "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmad Faraz Khan",
        "Samuel Fountain",
        "Ahmed M. Abdelmoniem",
        "Ali R. Butt",
        "Ali Anwar"
      ],
      "abstract": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
      "tldr_zh": "该论文提出 FLStore，一种高效的联邦学习（Federated Learning, FL）存储框架，针对非训练工作负载（如调度、个性化、聚类、调试和激励化）的问题，解决现有系统依赖云服务的延迟和成本问题。FLStore 通过在无服务器缓存上统一数据和计算平面，并采用定制的缓存策略实现本地化执行，从而显著降低延迟和成本。实验结果显示，与云对象存储相比，FLStore 将平均延迟减少71%并降低92.45%的成本；与内存云缓存相比，则减少64.6%的平均延迟和98.83%的成本。此外，FLStore 可无缝集成现有 FL 框架，具有容错和高可伸缩性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.00323v1",
      "published_date": "2025-03-01 03:20:30 UTC",
      "updated_date": "2025-03-01 03:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:47:10.809403"
    },
    {
      "arxiv_id": "2503.00322v1",
      "title": "T-REX: A 68-567 μs/token, 0.41-3.95 μJ/token Transformer Accelerator with Reduced External Memory Access and Enhanced Hardware Utilization in 16nm FinFET",
      "title_zh": "翻译失败",
      "authors": [
        "Seunghyun Moon",
        "Mao Li",
        "Gregory Chen",
        "Phil Knag",
        "Ram Krishnamurthy",
        "Mingoo Seok"
      ],
      "abstract": "This work introduces novel training and post-training compression schemes to\nreduce external memory access during transformer model inference. Additionally,\na new control flow mechanism, called dynamic batching, and a novel buffer\narchitecture, termed a two-direction accessible register file, further reduce\nexternal memory access while improving hardware utilization.",
      "tldr_zh": "这篇论文介绍了 T-REX，一种高效的 Transformer 加速器，使用 16nm FinFET 工艺，实现 68-567 μs/token 的执行时间和 0.41-3.95 μJ/token 的能量消耗。\n论文提出新型训练和后训练压缩方案，以减少 Transformer 模型推理过程中的外部内存访问。\n此外，引入动态 batching 控制流机制和双向可访问 register file 缓冲架构，进一步降低外部内存访问并提升硬件利用率。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Accepted to IEEE ISSCC 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.00322v1",
      "published_date": "2025-03-01 03:18:12 UTC",
      "updated_date": "2025-03-01 03:18:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:47:24.361438"
    },
    {
      "arxiv_id": "2503.00320v2",
      "title": "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of Bilateral Financial Exchanges, A bond market study",
      "title_zh": "翻译失败",
      "authors": [
        "Alicia Vidler",
        "Toby Walsh"
      ],
      "abstract": "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.",
      "tldr_zh": "这篇论文引入了 TRIBE，一种整合大型语言模型 (LLM) 的代理-based 模型 (ABM)，用于模拟双边金融市场（如政府债券交易）中的人类决策偏好，包括风险厌恶和模糊敏感性。研究的关键贡献包括：证明 LLM 可行地增强代理行为模拟、发现即使轻微风险厌恶也会导致交易完全停止、以及表明加入人类-like 变异性会转移权力向客户并可能引发系统性崩溃。这些发现突显了在复杂市场中引入随机人类决策过程的新兴系统行为，提升了模拟的真实性和复杂性。",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "q-fin.TR",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.00320v2",
      "published_date": "2025-03-01 03:15:13 UTC",
      "updated_date": "2025-03-04 16:36:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:47:35.753803"
    },
    {
      "arxiv_id": "2503.00309v1",
      "title": "Pseudo-Knowledge Graph: Meta-Path Guided Retrieval and In-Graph Text for RAG-Equipped LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxin Yang",
        "Haoyang Wu",
        "Tao Wang",
        "Jia Yang",
        "Hao Ma",
        "Guojie Luo"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has revolutionized natural\nlanguage processing. However, these models face challenges in retrieving\nprecise information from vast datasets. Retrieval-Augmented Generation (RAG)\nwas developed to combining LLMs with external information retrieval systems to\nenhance the accuracy and context of responses. Despite improvements, RAG still\nstruggles with comprehensive retrieval in high-volume, low-information-density\ndatabases and lacks relational awareness, leading to fragmented answers.\n  To address this, this paper introduces the Pseudo-Knowledge Graph (PKG)\nframework, designed to overcome these limitations by integrating Meta-path\nRetrieval, In-graph Text and Vector Retrieval into LLMs. By preserving natural\nlanguage text and leveraging various retrieval techniques, the PKG offers a\nricher knowledge representation and improves accuracy in information retrieval.\nExtensive evaluations using Open Compass and MultiHop-RAG benchmarks\ndemonstrate the framework's effectiveness in managing large volumes of data and\ncomplex relationships.",
      "tldr_zh": "本文提出 Pseudo-Knowledge Graph (PKG) 框架，以解决 Large Language Models (LLMs) 在信息检索中的挑战，特别是 Retrieval-Augmented Generation (RAG) 系统在高容量、低信息密度数据库中检索不全面和缺乏关系意识的问题。PKG 通过整合 Meta-path Retrieval、In-graph Text 和 Vector Retrieval 等技术，保留自然语言文本并提供更丰富的知识表示，从而提升检索准确性和关系处理能力。在 Open Compass 和 MultiHop-RAG 基准测试中，PKG 展示了在处理海量数据和复杂关系方面的显著有效性，为增强 LLM 的信息检索性能奠定了基础。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00309v1",
      "published_date": "2025-03-01 02:39:37 UTC",
      "updated_date": "2025-03-01 02:39:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:47:46.592068"
    },
    {
      "arxiv_id": "2503.00299v1",
      "title": "Hidden Convexity of Fair PCA and Fast Solver via Eigenvalue Optimization",
      "title_zh": "公平 PCA 的隐藏凸性与通过特征值优化的快速求解器",
      "authors": [
        "Junhui Shen",
        "Aaron J. Davis",
        "Ding Lu",
        "Zhaojun Bai"
      ],
      "abstract": "Principal Component Analysis (PCA) is a foundational technique in machine\nlearning for dimensionality reduction of high-dimensional datasets. However,\nPCA could lead to biased outcomes that disadvantage certain subgroups of the\nunderlying datasets. To address the bias issue, a Fair PCA (FPCA) model was\nintroduced by Samadi et al. (2018) for equalizing the reconstruction loss\nbetween subgroups. The semidefinite relaxation (SDR) based approach proposed by\nSamadi et al. (2018) is computationally expensive even for suboptimal\nsolutions. To improve efficiency, several alternative variants of the FPCA\nmodel have been developed. These variants often shift the focus away from\nequalizing the reconstruction loss. In this paper, we identify a hidden\nconvexity in the FPCA model and introduce an algorithm for convex optimization\nvia eigenvalue optimization. Our approach achieves the desired fairness in\nreconstruction loss without sacrificing performance. As demonstrated in\nreal-world datasets, the proposed FPCA algorithm runs $8\\times$ faster than the\nSDR-based algorithm, and only at most 85% slower than the standard PCA.",
      "tldr_zh": "这篇论文揭示了 Fair PCA (FPCA) 模型中的隐藏凸性问题，并提出了一种通过 eigenvalue optimization 的凸优化算法，以均衡数据集子群体间的重建损失。传统 FPCA 方法如 semidefinite relaxation (SDR) 计算成本高，本文的新算法显著提高了效率，同时不牺牲性能。在真实数据集实验中，该算法比 SDR 方法快 8 倍，仅比标准 PCA 慢 85%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00299v1",
      "published_date": "2025-03-01 02:13:20 UTC",
      "updated_date": "2025-03-01 02:13:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:47:58.347729"
    },
    {
      "arxiv_id": "2503.00286v1",
      "title": "A Unified Framework for Heterogeneous Semi-supervised Learning",
      "title_zh": "异构半监督学习的统一框架",
      "authors": [
        "Marzi Heidari",
        "Abdullah Alchihabi",
        "Hao Yan",
        "Yuhong Guo"
      ],
      "abstract": "In this work, we introduce a novel problem setup termed as Heterogeneous\nSemi-Supervised Learning (HSSL), which presents unique challenges by bridging\nthe semi-supervised learning (SSL) task and the unsupervised domain adaptation\n(UDA) task, and expanding standard semi-supervised learning to cope with\nheterogeneous training data. At its core, HSSL aims to learn a prediction model\nusing a combination of labeled and unlabeled training data drawn separately\nfrom heterogeneous domains that share a common set of semantic categories; this\nmodel is intended to differentiate the semantic categories of test instances\nsampled from both the labeled and unlabeled domains. In particular, the labeled\nand unlabeled domains have dissimilar label distributions and class feature\ndistributions. This heterogeneity, coupled with the assorted sources of the\ntest data, introduces significant challenges to standard SSL and UDA methods.\nTherefore, we propose a novel method, Unified Framework for Heterogeneous\nSemi-supervised Learning (Uni-HSSL), to address HSSL by directly learning a\nfine-grained classifier from the heterogeneous data, which adaptively handles\nthe inter-domain heterogeneity while leveraging both the unlabeled data and the\ninter-domain semantic class relationships for cross-domain knowledge transfer\nand adaptation. We conduct comprehensive experiments and the experimental\nresults validate the efficacy and superior performance of the proposed Uni-HSSL\nover state-of-the-art semi-supervised learning and unsupervised domain\nadaptation methods.",
      "tldr_zh": "这篇论文引入了Heterogeneous Semi-Supervised Learning (HSSL)，一个新框架，将半监督学习 (SSL) 和无监督域适应 (UDA) 相结合，以处理来自不同领域的异构训练数据，这些领域共享语义类别但具有不同的标签分布和类特征分布。作者提出了Unified Framework for Heterogeneous Semi-supervised Learning (Uni-HSSL) 方法，通过直接学习一个细粒度分类器来适应域间异质性，并利用未标记数据以及跨域语义类关系进行知识转移和适应。实验结果显示，Uni-HSSL 在全面测试中比现有的 SSL 和 UDA 方法表现出色，验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00286v1",
      "published_date": "2025-03-01 01:32:02 UTC",
      "updated_date": "2025-03-01 01:32:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:48:11.838094"
    },
    {
      "arxiv_id": "2503.00269v1",
      "title": "Reducing Large Language Model Safety Risks in Women's Health using Semantic Entropy",
      "title_zh": "使用语义熵降低女性健康领域大语言模型的安全风险",
      "authors": [
        "Jahan C. Penny-Dimri",
        "Magdalena Bachmann",
        "William R. Cooke",
        "Sam Mathewlynn",
        "Samuel Dockree",
        "John Tolladay",
        "Jannik Kossen",
        "Lin Li",
        "Yarin Gal",
        "Gabriel Davis Jones"
      ],
      "abstract": "Large language models (LLMs) hold substantial promise for clinical decision\nsupport. However, their widespread adoption in medicine, particularly in\nhealthcare, is hindered by their propensity to generate false or misleading\noutputs, known as hallucinations. In high-stakes domains such as women's health\n(obstetrics & gynaecology), where errors in clinical reasoning can have\nprofound consequences for maternal and neonatal outcomes, ensuring the\nreliability of AI-generated responses is critical. Traditional methods for\nquantifying uncertainty, such as perplexity, fail to capture meaning-level\ninconsistencies that lead to misinformation. Here, we evaluate semantic entropy\n(SE), a novel uncertainty metric that assesses meaning-level variation, to\ndetect hallucinations in AI-generated medical content. Using a clinically\nvalidated dataset derived from UK RCOG MRCOG examinations, we compared SE with\nperplexity in identifying uncertain responses. SE demonstrated superior\nperformance, achieving an AUROC of 0.76 (95% CI: 0.75-0.78), compared to 0.62\n(0.60-0.65) for perplexity. Clinical expert validation further confirmed its\neffectiveness, with SE achieving near-perfect uncertainty discrimination\n(AUROC: 0.97). While semantic clustering was successful in only 30% of cases,\nSE remains a valuable tool for improving AI safety in women's health. These\nfindings suggest that SE could enable more reliable AI integration into\nclinical practice, particularly in resource-limited settings where LLMs could\naugment care. This study highlights the potential of SE as a key safeguard in\nthe responsible deployment of AI-driven tools in women's health, leading to\nsafer and more effective digital health interventions.",
      "tldr_zh": "本研究针对 Large Language Models (LLMs) 在妇女健康领域（如产科和妇科）的安全风险，提出使用 Semantic Entropy (SE) 作为一种新型不确定性指标来检测幻觉(hallucinations)。与传统 perplexity 方法相比，SE 在临床验证数据集上表现出色，AUROC 分别为 0.76 和 0.62，且专家验证中 SE 的不确定性区分能力达到 0.97。结果显示，SE 虽在语义聚类中仅成功 30% 的情况下，但整体上能显著提高 AI 生成医疗内容的可靠性。该方法为 AI 在资源有限的临床环境中安全部署提供关键保障，促进更有效的数字健康干预。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.00269v1",
      "published_date": "2025-03-01 00:57:52 UTC",
      "updated_date": "2025-03-01 00:57:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:48:26.092200"
    },
    {
      "arxiv_id": "2503.00268v1",
      "title": "Input Specific Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Asghar A. Jadoon",
        "D. Thomas Seidl",
        "Reese E. Jones",
        "Jan N. Fuhg"
      ],
      "abstract": "The black-box nature of neural networks limits the ability to encode or\nimpose specific structural relationships between inputs and outputs. While\nvarious studies have introduced architectures that ensure the network's output\nadheres to a particular form in relation to certain inputs, the majority of\nthese approaches impose constraints on only a single set of inputs. This paper\nintroduces a novel neural network architecture, termed the Input Specific\nNeural Network (ISNN), which extends this concept by allowing scalar-valued\noutputs to be subject to multiple constraints. Specifically, the ISNN can\nenforce convexity in some inputs, non-decreasing monotonicity combined with\nconvexity with respect to others, and simple non-decreasing monotonicity or\narbitrary relationships with additional inputs. The paper presents two distinct\nISNN architectures, along with equations for the first and second derivatives\nof the output with respect to the inputs. These networks are broadly\napplicable.\n  In this work, we restrict their usage to solving problems in computational\nmechanics. In particular, we show how they can be effectively applied to\nfitting data-driven constitutive models. We then embed our trained data-driven\nconstitutive laws into a finite element solver where significant time savings\ncan be achieved by using explicit manual differentiation using the derived\nequations as opposed to automatic differentiation. We also show how ISNNs can\nbe used to learn structural relationships between inputs and outputs via a\nbinary gating mechanism. Particularly, ISNNs are employed to model an\nanisotropic free energy potential to get the homogenized macroscopic response\nin a decoupled multiscale setting, where the network learns whether or not the\npotential should be modeled as polyconvex, and retains only the relevant layers\nwhile using the minimum number of inputs.",
      "tldr_zh": "本文提出了一种新型神经网络架构，Input Specific Neural Network (ISNN)，允许输出对不同输入施加多种特定约束，例如对某些输入强制 convexity，对其他输入强制 non-decreasing monotonicity 结合 convexity，或简单 non-decreasing monotonicity 和任意关系。ISNN 包括两种架构，并提供了输出相对于输入的一阶和二阶导数的方程，以支持精确计算。论文将 ISNN 应用于 computational mechanics 领域，用于拟合数据驱动的 constitutive models，并在有限元求解器中通过 explicit manual differentiation 实现显著时间节省。此外，ISNN 通过 binary gating mechanism 学习输入输出间的结构关系，例如在多尺度设置中建模 anisotropic free energy potential，并自动决定是否为 polyconvex，从而优化模型效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.00268v1",
      "published_date": "2025-03-01 00:57:16 UTC",
      "updated_date": "2025-03-01 00:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:48:36.784976"
    },
    {
      "arxiv_id": "2503.05793v1",
      "title": "MedSimAI: Simulation and Formative Feedback Generation to Enhance Deliberate Practice in Medical Education",
      "title_zh": "翻译失败",
      "authors": [
        "Yann Hicke",
        "Jadon Geathers",
        "Niroop Rajashekar",
        "Colleen Chan",
        "Anyanate Gwendolyne Jack",
        "Justin Sewell",
        "Mackenzi Preston",
        "Susannah Cornes",
        "Dennis Shung",
        "Rene Kizilcec"
      ],
      "abstract": "Medical education faces challenges in scalability, accessibility, and\nconsistency, particularly in clinical skills training for physician-patient\ncommunication. Traditional simulation-based learning, while effective, is\nresource-intensive, difficult to schedule, and often highly variable in\nfeedback quality. Through a collaboration between AI, learning science, and\nmedical education experts, we co-developed MedSimAI, an AI-powered simulation\nplatform that enables deliberate practice, self-regulated learning (SRL), and\nautomated assessment through interactive patient encounters. Leveraging large\nlanguage models (LLMs), MedSimAI generates realistic clinical interactions and\nprovides immediate, structured feedback using established medical evaluation\nframeworks such as the Master Interview Rating Scale (MIRS). In a pilot study\nwith 104 first-year medical students, we examined engagement, conversation\npatterns, and user perceptions. Students found MedSimAI beneficial for\nrepeated, realistic patient-history practice. Conversation analysis revealed\nthat certain higher-order skills were often overlooked, though students\ngenerally performed systematic histories and empathic listening. By integrating\nunlimited practice opportunities, real-time AI assessment, and SRL principles,\nMedSimAI addresses key limitations of traditional simulation-based training,\nmaking high-quality clinical education more accessible and scalable.",
      "tldr_zh": "本研究针对医疗教育中临床技能训练（如医生-患者沟通）的可扩展性、可访问性和一致性挑战，开发了 MedSimAI 平台，该平台利用 LLMs 生成真实的临床互动，并通过如 MIRS 等框架提供即时、结构化的反馈，支持 deliberate practice 和 SRL。\n在试点研究中，涉及 104 名第一年医学生的测试显示，MedSimAI 促进了重复的患者病史实践和移情倾听，但某些高级技能（如全面对话模式）常被忽略。\n总体而言，MedSimAI 通过提供无限练习机会和实时 AI 评估，解决了传统模拟训练的资源限制问题，使高质量临床教育更易于扩展和访问。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05793v1",
      "published_date": "2025-03-01 00:51:55 UTC",
      "updated_date": "2025-03-01 00:51:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:48:48.673536"
    },
    {
      "arxiv_id": "2503.00258v1",
      "title": "Decoupling Content and Expression: Two-Dimensional Detection of AI-Generated Text",
      "title_zh": "内容与表达的解耦：AI 生成文本的二维检测",
      "authors": [
        "Guangsheng Bao",
        "Lihua Rong",
        "Yanbin Zhao",
        "Qiji Zhou",
        "Yue Zhang"
      ],
      "abstract": "The wide usage of LLMs raises critical requirements on detecting AI\nparticipation in texts. Existing studies investigate these detections in\nscattered contexts, leaving a systematic and unified approach unexplored. In\nthis paper, we present HART, a hierarchical framework of AI risk levels, each\ncorresponding to a detection task. To address these tasks, we propose a novel\n2D Detection Method, decoupling a text into content and language expression.\nOur findings show that content is resistant to surface-level changes, which can\nserve as a key feature for detection. Experiments demonstrate that 2D method\nsignificantly outperforms existing detectors, achieving an AUROC improvement\nfrom 0.705 to 0.849 for level-2 detection and from 0.807 to 0.886 for RAID. We\nrelease our data and code at https://github.com/baoguangsheng/truth-mirror.",
      "tldr_zh": "本研究针对LLMs广泛使用引发的AI生成文本检测需求，提出HART框架——一个分层AI风险水平体系，每个水平对应特定检测任务。作者引入2D Detection Method，将文本解耦成内容和语言表达，发现内容对表面变化具有抵抗力，可作为关键检测特征。实验结果显示，该方法显著优于现有检测器，AUROC从0.705提升到0.849（level-2检测）和从0.807提升到0.886（RAID），并开源数据和代码以供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 8 tables, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.00258v1",
      "published_date": "2025-03-01 00:19:13 UTC",
      "updated_date": "2025-03-01 00:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T20:48:59.603390"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-23T20:49:21.016164"
}
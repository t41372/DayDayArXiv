{
  "date": "2024-01-15",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-01-15 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 61 篇论文，主要聚焦 AI 模型优化（如 LLM 的解释性和服务效率）、机器学习应用（包括图像生成和联邦学习）以及多领域创新（如医疗和机器人），其中 LLM 相关研究是亮点，著名学者如 Siva Reddy 和 Miroslav Krstic 的作品令人印象深刻，同时强调了 AI 在实际问题中的鲁棒性和可解释性。\n\n今天的论文涵盖了广泛主题，我将优先讨论那些重要、创新或有话题度的文章，例如 LLM 的局限性和优化方法，以及联邦学习和图像生成领域的突破。其他较次要或专题性较强的论文将快速掠过，只提核心点。以下按主题归类简要概述：\n\n### LLM 和 AI 模型优化（重点领域）\n这些论文探讨了大型语言模型（LLM）的性能、解释性和应用，体现了 AI 领域的热门趋势。\n\n- **Are self-explanations from Large Language Models faithful?（LLM 自解释的可靠性）**  \n  作者：Andreas Madsen、Sarath Chandar、Siva Reddy 等。  \n  这篇论文的主要贡献是通过自一致性检查评估 LLM 自解释的忠实度，发现解释的可靠性取决于任务、模型和类型（如 Llama2 在逆事实解释中更可靠）。它揭示了 LLM 可能产生误导性解释的风险，为 AI 可解释性研究提供了新基准，强调了在实际应用中需要谨慎使用。\n\n- **A Study on Large Language Models' Limitations in Multiple-Choice Question Answering（大型语言模型在多选题回答中的局限性研究）**  \n  作者：Aisha Khatun、Daniel G. Brown。  \n  论文发现 65% 的小型开源 LLM 无法正确理解多选题任务，仅少数模型能独立于选项顺序工作。该发现突出了 LLM 在教育和评估中的潜在缺陷，并建议在使用前测试任务理解，贡献了实用性强的风险评估框架。\n\n- **Carrying over algorithm in transformers（Transformer 中的进位算法）**  \n  作者：Jorrit Kruthoff。  \n  这篇工作分析了 Transformer 如何实现简单算术进位算法，展示了多层模型中任务分配的模块化（如第一层处理位加法，第二层管理进位）。它为理解神经网络内部机制提供了新视角，并验证了该机制在不同超参数下的鲁棒性，适用于 AI 基础研究。\n\n- **AI-as-exploration: Navigating intelligence space（AI 作为探索：导航智能空间）**  \n  作者：Dimitri Coelho Mollo。  \n  论文提出 “AI-as-exploration” 框架，强调 AI 用于探索非人类智能的构建块，通过案例研究（如 LLM 在概念组合中的表现）展示了 AI 在智能空间中的潜力。该工作具有哲学深度，推动了 AI 研究向更广泛的智能多样性扩展。\n\n- **Learned Best-Effort LLM Serving（学习型尽力 LLM 服务）**  \n  作者：Siddharth Jha 等。  \n  该方法使用深度强化学习动态调整 LLM 服务质量，在不稳定负载下提升可用性，实现 10 倍以上请求率提升和性能优化。贡献在于提供了一种成本高效的 LLM 部署策略，适用于实时应用如边缘计算。\n\n其他 LLM 相关论文，如第11（评估 LLM 解释）、第35（防止提示注入攻击）等，也展示了模型安全性和泛化能力的提升，但细节较相似，故快速掠过。\n\n### 机器学习和计算机视觉应用\n这些论文强调了 AI 在图像处理和联邦学习中的创新，部分有实际落地潜力。\n\n- **InstantID: Zero-shot Identity-Preserving Generation in Seconds（InstantID：秒级零-shot 身份保留生成）**  \n  作者：Qixun Wang 等。  \n  主要贡献是提出一个即插即用的扩散模型模块，能使用单张面部图像实现风格多样的图像生成，同时保持高身份保真度。通过 IdentityNet 整合语义和空间条件，该方法在 SD1.5 和 SDXL 等模型上表现出色，显著提高了图像编辑效率。\n\n- **Vertical Federated Image Segmentation（垂直联邦图像分割）**  \n  作者：Paul K. Mandal、Cole Leo。  \n  论文引入了一个创新的垂直联邦学习框架，用于图像分割，支持无标签数据联邦，实现了名义准确率并保持隐私。该方法在 CamVid 数据集上验证了其鲁棒性，填补了联邦学习在医疗和视觉任务中的空白。\n\n其他视觉相关论文，如第31（多模态图像编辑）和第39（多模态人群计数），展示了扩散模型和 GAN 的进展，但这些工作较为具体，故仅提：第31 通过注意力掩码提升编辑效率，第39 使用 GAN 融合 RGB 和热红外图像改进计数准确性。\n\n### 其他领域创新\n这些论文涉及医疗、机器人和优化，但部分较窄，故简要提及。\n\n- **Adaptive Neural-Operator Backstepping Control of a Benchmark Hyperbolic PDE（适应性神经算子反步控制基准双曲 PDE）**  \n  作者：Maxence Lamarque、Miroslav Krstic 等。  \n  Miroslav Krstic 的参与使这篇论文脱颖而出。它使用神经算子加速自适应 PDE 控制，实现了三倍速度提升，同时通过 Lyapunov 分析证明了稳定性。该工作为实时机器人控制提供了新工具。\n\n快速掠过其他：第1（CNN 压缩 via 动态秩修剪，提升存储效率）；第13（手势识别 via EMG 和机器学习，97% 准确率）；第29（LLM 用于机器人规划，优化任务树）；第42（语义 Web 中的动态算法，实现动作推理）。这些论文虽有贡献，但主题较专业或重复，故不展开。第8-9 和第18（教育和医疗应用，如手语识别和阅读障碍分析）展示了 AI 在社会包容中的潜力，但细节类似，故合并为：这些工作利用 VR 和 AI 改善教育和健康干预，准确率达 90%以上。\n\n总的来说，今天的 arXiv 更新突出了 AI 的可解释性和高效应用，LLM 相关研究占主导。重点论文如 LLM 解释和图像生成方法，不仅有理论创新，还具实际价值，值得跟踪。如果您对特定领域感兴趣，建议查看这些论文的完整摘要！",
  "papers": [
    {
      "arxiv_id": "2401.08014v1",
      "title": "Convolutional Neural Network Compression via Dynamic Parameter Rank Pruning",
      "title_zh": "翻译失败",
      "authors": [
        "Manish Sharma",
        "Jamison Heard",
        "Eli Saber",
        "Panos P. Markopoulos"
      ],
      "abstract": "While Convolutional Neural Networks (CNNs) excel at learning complex\nlatent-space representations, their over-parameterization can lead to\noverfitting and reduced performance, particularly with limited data. This,\nalongside their high computational and memory demands, limits the applicability\nof CNNs for edge deployment. Low-rank matrix approximation has emerged as a\npromising approach to reduce CNN parameters, but its application presents\nchallenges including rank selection and performance loss. To address these\nissues, we propose an efficient training method for CNN compression via dynamic\nparameter rank pruning. Our approach integrates efficient matrix factorization\nand novel regularization techniques, forming a robust framework for dynamic\nrank reduction and model compression. We use Singular Value Decomposition (SVD)\nto model low-rank convolutional filters and dense weight matrices and we\nachieve model compression by training the SVD factors with back-propagation in\nan end-to-end way. We evaluate our method on an array of modern CNNs, including\nResNet-18, ResNet-20, and ResNet-32, and datasets like CIFAR-10, CIFAR-100, and\nImageNet (2012), showcasing its applicability in computer vision. Our\nexperiments show that the proposed method can yield substantial storage savings\nwhile maintaining or even enhancing classification performance.",
      "tldr_zh": "本研究针对卷积神经网络 (CNNs) 的过度参数化问题，提出了一种动态参数秩修剪的训练方法，以缓解过拟合、计算需求和内存限制，从而提升边缘部署的可行性。该方法整合了高效矩阵因子分解和新型正则化技术，使用奇异值分解 (SVD) 来建模低秩卷积滤波器和密集权重矩阵，并通过端到端反向传播训练 SVD 因子实现模型压缩。在 ResNet-18、ResNet-20 和 ResNet-32 等模型上，以及 CIFAR-10、CIFAR-100 和 ImageNet (2012) 数据集的实验中，该方法实现了显著的存储节省，同时维持或提升分类性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.08014v1",
      "published_date": "2024-01-15 23:52:35 UTC",
      "updated_date": "2024-01-15 23:52:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:51:14.127687"
    },
    {
      "arxiv_id": "2401.08008v1",
      "title": "Analysing the Needs of Homeless People Using Feature Selection and Mining Association Rules",
      "title_zh": "利用特征选择和关联规则挖掘分析无家可归者的需求",
      "authors": [
        "José M. Alcalde-Llergo",
        "Carlos García-Martínez",
        "Manuel Vaquero-Abellán",
        "Pilar Aparicio-Martínez",
        "Enrique Yeguas-Bolívar"
      ],
      "abstract": "Homelessness is a social and health problem with great repercussions in\nEurope. Many non-governmental organisations help homeless people by collecting\nand analysing large amounts of information about them. However, these tasks are\nnot always easy to perform, and hinder other of the organisations duties. The\nSINTECH project was created to tackle this issue proposing two different tools:\na mobile application to quickly and easily collect data; and a software based\non artificial intelligence which obtains interesting information from the\ncollected data. The first one has been distributed to some Spanish\norganisations which are using it to conduct surveys of homeless people. The\nsecond tool implements different feature selection and association rules mining\nmethods. These artificial intelligence techniques have allowed us to identify\nthe most relevant features and some interesting association rules from\npreviously collected homeless data.",
      "tldr_zh": "这篇论文探讨了使用特征选择和关联规则挖掘来分析无家可归者的需求，以解决欧洲社会和健康问题。研究引入了 SINTECH 项目，该项目开发了一个移动应用，用于快速收集数据，以及一个基于人工智能的软件，通过 feature selection 和 mining association rules 方法从数据中提取关键特征和有趣的关联规则。实验结果显示，这些工具已分发给西班牙组织，帮助它们更有效地识别无家可归者的相关需求，并优化援助工作。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 4 figures, 4 tables, MetroXRAINE 2022",
      "pdf_url": "http://arxiv.org/pdf/2401.08008v1",
      "published_date": "2024-01-15 23:28:55 UTC",
      "updated_date": "2024-01-15 23:28:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:51:26.293829"
    },
    {
      "arxiv_id": "2401.08003v1",
      "title": "Jewelry Recognition via Encoder-Decoder Models",
      "title_zh": "翻译失败",
      "authors": [
        "José M. Alcalde-Llergo",
        "Enrique Yeguas-Bolívar",
        "Andrea Zingoni",
        "Alejandro Fuerte-Jurado"
      ],
      "abstract": "Jewelry recognition is a complex task due to the different styles and designs\nof accessories. Precise descriptions of the various accessories is something\nthat today can only be achieved by experts in the field of jewelry. In this\nwork, we propose an approach for jewelry recognition using computer vision\ntechniques and image captioning, trying to simulate this expert human behavior\nof analyzing accessories. The proposed methodology consist on using different\nimage captioning models to detect the jewels from an image and generate a\nnatural language description of the accessory. Then, this description is also\nutilized to classify the accessories at different levels of detail. The\ngenerated caption includes details such as the type of jewel, color, material,\nand design. To demonstrate the effectiveness of the proposed method in\naccurately recognizing different types of jewels, a dataset consisting of\nimages of accessories belonging to jewelry stores in C\\'ordoba (Spain) has been\ncreated. After testing the different image captioning architectures designed,\nthe final model achieves a captioning accuracy of 95\\%. The proposed\nmethodology has the potential to be used in various applications such as\njewelry e-commerce, inventory management or automatic jewels recognition to\nanalyze people's tastes and social status.",
      "tldr_zh": "这篇论文提出了一种基于encoder-decoder模型的珠宝识别方法，使用计算机视觉和image captioning技术来模拟专家分析珠宝的复杂样式和设计。方法包括从图像中检测珠宝并生成自然语言描述（如类型、颜色、材料和设计），随后利用这些描述进行多级分类；为此，研究者创建了一个由西班牙科尔多瓦珠宝店图像组成的数据集。实验结果显示，模型的captioning准确率达到95%，该方法可应用于珠宝电商、库存管理和分析用户品味及社会地位。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 5 figures, MetroXRAINE 2023 Conference",
      "pdf_url": "http://arxiv.org/pdf/2401.08003v1",
      "published_date": "2024-01-15 23:10:50 UTC",
      "updated_date": "2024-01-15 23:10:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:51:39.289887"
    },
    {
      "arxiv_id": "2401.07993v2",
      "title": "Carrying over algorithm in transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Jorrit Kruthoff"
      ],
      "abstract": "Addition is perhaps one of the simplest arithmetic tasks one can think of and\nis usually performed using the carrying over algorithm. This algorithm consists\nof two tasks: adding digits in the same position and carrying over a one\nwhenever necessary. We study how transformer models implement this algorithm\nand how the two aforementioned tasks are allocated to different parts of the\nnetwork. We first focus on two-layer encoder-only models and show that the\ncarrying over algorithm is implemented in a modular fashion. The first layer is\nmostly responsible for adding digits in the same position. The second layer\nfirst decides, in the attention, which positions need a carried one or not, and\nthen performs the carrying of the one in the final MLP. We provide a simple way\nof precisely identifying which neurons are responsible for that task. This\nimplementation of the carrying over algorithm occurs across a range of\nhyperparameters for two as well as three-layer models. For small decoder-only\nmodels, we observe the same implementation and provide suggestive evidence for\nits existence in three 7B large language models.",
      "tldr_zh": "该研究探讨了Transformer模型中进位算法（carrying over algorithm）的实现方式，专注于如何处理加法中的数字相加和进位任务。作者分析了两层编码器模型，发现第一层主要负责同一位置数字的相加，而第二层在attention机制中决定进位位置，并在MLP中执行进位操作，并精确识别了负责进位的神经元。这种模块化实现方式在不同超参数的二层和三层模型中一致存在，并扩展到小型解码器模型，以及为几个7B大型语言模型提供了证据。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Comments welcome!",
      "pdf_url": "http://arxiv.org/pdf/2401.07993v2",
      "published_date": "2024-01-15 22:36:11 UTC",
      "updated_date": "2024-01-17 16:02:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:51:49.595032"
    },
    {
      "arxiv_id": "2401.07969v1",
      "title": "Simulated Autopoiesis in Liquid Automata",
      "title_zh": "翻译失败",
      "authors": [
        "Steve Battle"
      ],
      "abstract": "We present a novel form of Liquid Automata, using this to simulate\nautopoiesis, whereby living machines self-organise in the physical realm. This\nsimulation is based on an earlier Cellular Automaton described by Francisco\nVarela. The basis of Liquid Automata is a particle simulation with additional\nrules about how particles are transformed on collision with other particles.\nUnlike cellular automata, there is no fixed grid or time-step, only particles\nmoving about and colliding with each other in a continuous space/time.",
      "tldr_zh": "该论文提出了一种新型的 Liquid Automata，用于模拟 autopoiesis，即活机器在物理领域自我组织的现象。基于 Francisco Varela 早期的 Cellular Automaton，该方法采用粒子模拟，并添加粒子碰撞时转化的规则。与传统 Cellular Automata 不同，Liquid Automata 没有固定网格或时间步长，而是让粒子在连续的空间和时间中移动和互动。这种创新框架为研究活系统自组织提供了新的模拟工具。",
      "categories": [
        "nlin.CG",
        "cs.AI"
      ],
      "primary_category": "nlin.CG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.07969v1",
      "published_date": "2024-01-15 21:23:23 UTC",
      "updated_date": "2024-01-15 21:23:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:52:01.995952"
    },
    {
      "arxiv_id": "2401.07964v3",
      "title": "AI-as-exploration: Navigating intelligence space",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitri Coelho Mollo"
      ],
      "abstract": "Artificial Intelligence is a field that lives many lives, and the term has\ncome to encompass a motley collection of scientific and commercial endeavours.\nIn this paper, I articulate the contours of a rather neglected but central\nscientific role that AI has to play, which I dub `AI-as-exploration'.The basic\nthrust of AI-as-exploration is that of creating and studying systems that can\nreveal candidate building blocks of intelligence that may differ from the forms\nof human and animal intelligence we are familiar with. In other words, I\nsuggest that AI is one of the best tools we have for exploring intelligence\nspace, namely the space of possible intelligent systems. I illustrate the value\nof AI-as-exploration by focusing on a specific case study, i.e., recent work on\nthe capacity to combine novel and invented concepts in humans and Large\nLanguage Models. I show that the latter, despite showing human-level accuracy\nin such a task, probably solve it in ways radically different, but no less\nrelevant to intelligence research, to those hypothesised for humans.",
      "tldr_zh": "本论文提出“AI-as-exploration”概念，即将人工智能视为探索“intelligence space”（智能空间）的核心工具，通过创建和研究新型系统来揭示可能不同于人类和动物智能的构建块。作者强调，AI 可以帮助识别各种可能的智能形式，从而扩展对智能的理解。论文以人类和Large Language Models在结合新颖概念方面的能力为例，展示尽管Large Language Models达到人类级准确性，但其解决方式可能截然不同，却同样对智能研究具有重要价值。这种方法为人工智能领域提供了新的科学视角，促进了对智能多样性的深入探索。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07964v3",
      "published_date": "2024-01-15 21:06:20 UTC",
      "updated_date": "2024-08-16 17:01:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:52:12.831248"
    },
    {
      "arxiv_id": "2401.07955v2",
      "title": "A Study on Large Language Models' Limitations in Multiple-Choice Question Answering",
      "title_zh": "大型语言模型在多项选择题回答中的局限性研究",
      "authors": [
        "Aisha Khatun",
        "Daniel G. Brown"
      ],
      "abstract": "The widespread adoption of Large Language Models (LLMs) has become\ncommonplace, particularly with the emergence of open-source models. More\nimportantly, smaller models are well-suited for integration into consumer\ndevices and are frequently employed either as standalone solutions or as\nsubroutines in various AI tasks. Despite their ubiquitous use, there is no\nsystematic analysis of their specific capabilities and limitations. In this\nstudy, we tackle one of the most widely used tasks - answering Multiple Choice\nQuestion (MCQ). We analyze 26 small open-source models and find that 65% of the\nmodels do not understand the task, only 4 models properly select an answer from\nthe given choices, and only 5 of these models are choice order independent.\nThese results are rather alarming given the extensive use of MCQ tests with\nthese models. We recommend exercising caution and testing task understanding\nbefore using MCQ to evaluate LLMs in any field whatsoever.",
      "tldr_zh": "这篇论文研究了大型语言模型 (LLMs) 在多选题 (MCQ) 回答任务中的局限性，特别是针对小型开源模型。研究者分析了 26 个小型开源模型，发现 65% 的模型无法理解任务，只有 4 个模型能正确从给定选项中选择答案，且仅 5 个模型对选项顺序不敏感。这些发现突显了 LLMs 在实际应用中的潜在问题，并建议在使用 MCQ 测试评估 LLMs 时，必须谨慎并先验证模型的任务理解能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07955v2",
      "published_date": "2024-01-15 20:42:16 UTC",
      "updated_date": "2024-08-15 02:18:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:52:25.797701"
    },
    {
      "arxiv_id": "2401.08714v1",
      "title": "Training program on sign language: social inclusion through Virtual Reality in ISENSE project",
      "title_zh": "手语培训计划：通过虚拟现实实现社会包容，在 ISENSE 项目中",
      "authors": [
        "Alessia Bisio",
        "Enrique Yeguas-Bolívar",
        "Pilar Aparicio-Martínez",
        "María Dolores Redel-Macías",
        "Sara Pinzi",
        "Stefano Rossi",
        "Juri Taborri"
      ],
      "abstract": "Structured hand gestures that incorporate visual motions and signs are used\nin sign language. Sign language is a valuable means of daily communication for\nindividuals who are deaf or have speech impairments, but it is still rare among\nhearing people, and fewer are capable of understand it. Within the academic\ncontext, parents and teachers play a crucial role in supporting deaf students\nfrom childhood by facilitating their learning of sign language. In the last\nyears, among all the teaching tools useful for learning sign language, the use\nof Virtual Reality (VR) has increased, as it has been demonstrated to improve\nretention, memory and attention during the learning process. The ISENSE project\nhas been created to assist students with deafness during their academic life by\nproposing different technological tools for teaching sign language to the\nhearing community in the academic context. As part of the ISENSE project, this\nwork aims to develop an application for Spanish and Italian sign language\nrecognition that exploits the VR environment to quickly and easily create a\ncomprehensive database of signs and an Artificial Intelligence (AI)-based\nsoftware to accurately classify and recognize static and dynamic signs: from\nletters to sentences.",
      "tldr_zh": "本研究探讨了手语作为聋哑人士沟通工具的重要性，并强调了在学术环境中通过 Virtual Reality (VR) 技术促进社会包容的必要性。ISENSE 项目旨在为聋哑学生提供支持，开发了一个基于 VR 的应用，用于西班牙语和意大利语手语的识别和学习。该应用结合 Artificial Intelligence (AI)-based 软件，快速创建手语数据库，并准确分类和识别静态及动态手语，从单个字母到完整句子，从而帮助父母和老师提升教学效果。实验表明，VR 能显著改善学习过程中的保留力、记忆力和注意力，为手语教育带来更高效的解决方案。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.HC",
      "comment": "6 pages, 4 figures, MetroXRAINE 2023 Conference, ISENSE european\n  project",
      "pdf_url": "http://arxiv.org/pdf/2401.08714v1",
      "published_date": "2024-01-15 20:40:46 UTC",
      "updated_date": "2024-01-15 20:40:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:52:38.244875"
    },
    {
      "arxiv_id": "2402.01668v1",
      "title": "Determining the Difficulties of Students With Dyslexia via Virtual Reality and Artificial Intelligence: An Exploratory Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Enrique Yeguas-Bolívar",
        "José M. Alcalde-Llergo",
        "Pilar Aparicio-Martínez",
        "Juri Taborri",
        "Andrea Zingoni",
        "Sara Pinzi"
      ],
      "abstract": "Learning disorders are neurological conditions that affect the brain's\nability to interconnect communication areas. Dyslexic students experience\nproblems with reading, memorizing, and exposing concepts; however the magnitude\nof these can be mitigated through both therapies and the creation of\ncompensatory mechanisms. Several efforts have been made to mitigate these\nissues, leading to the creation of digital resources for students with specific\nlearning disorders attending primary and secondary education levels.\nConversely, a standard approach is still missed in higher education. The\nVRAIlexia project has been created to tackle this issue by proposing two\ndifferent tools: a mobile application integrating virtual reality (VR) to\ncollect data quickly and easily, and an artificial intelligencebased software\n(AI) to analyze the collected data for customizing the supporting methodology\nfor each student. The first one has been created and is being distributed among\ndyslexic students in Higher Education Institutions, for the conduction of\nspecific psychological and psychometric tests. The second tool applies specific\nartificial intelligence algorithms to the data gathered via the application and\nother surveys. These AI techniques have allowed us to identify the most\nrelevant difficulties faced by the students' cohort. Our different models have\nobtained around 90\\% mean accuracy for predicting the support tools and\nlearning strategies.",
      "tldr_zh": "该研究探讨了通过虚拟现实 (VR) 和人工智能 (AI) 技术来识别读写障碍学生在高等教育中的困难。VRAIlexia 项目开发了两个工具：一个整合 VR 的移动应用，用于快速收集学生心理和心理测量测试数据；另一个基于 AI 算法的软件，对收集的数据进行分析，以定制化支持方法和学习策略。研究发现，这些模型在预测支持工具和学习策略方面取得了约 90% 的平均准确率，从而为高等教育中的读写障碍学生提供有效的补偿机制。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "7 pages, 5 figures, 3 tables, MetroXRAINE 2022 Conference, VRAILEXIA\n  european project",
      "pdf_url": "http://arxiv.org/pdf/2402.01668v1",
      "published_date": "2024-01-15 20:26:09 UTC",
      "updated_date": "2024-01-15 20:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:52:49.651136"
    },
    {
      "arxiv_id": "2401.07931v2",
      "title": "Vertical Federated Image Segmentation",
      "title_zh": "垂直联邦图像分割",
      "authors": [
        "Paul K. Mandal",
        "Cole Leo"
      ],
      "abstract": "With the popularization of AI solutions for image based problems, there has\nbeen a growing concern for both data privacy and acquisition. In a large number\nof cases, information is located on separate data silos and it can be difficult\nfor a developer to consolidate all of it in a fashion that is appropriate for\nmachine learning model development. Alongside this, a portion of these\nlocalized data regions may not have access to a labelled ground truth. This\nindicates that they have the capacity to reach conclusions numerically, but are\nnot able to assign classifications amid a lack of pertinent information. Such a\ndetermination is often negligible, especially when attempting to develop image\nbased solutions that often necessitate this capability. With this being the\ncase, we propose an innovative vertical federated learning (VFL) model\narchitecture that can operate under this common set of conditions. This is the\nfirst (and currently the only) implementation of a system that can work under\nthe constraints of a VFL environment and perform image segmentation while\nmaintaining nominal accuracies. We achieved this by utilizing an FCN that\nboasts the ability to operate on federates that lack labelled data and\nprivately share the respective weights with a central server, that of which\nhosts the necessary features for classification. Tests were conducted on the\nCamVid dataset in order to determine the impact of heavy feature compression\nrequired for the transfer of information between federates, as well as to reach\nnominal conclusions about the overall performance metrics when working under\nsuch constraints.",
      "tldr_zh": "该论文针对数据隐私和孤岛问题，提出了一种创新的垂直联邦学习 (VFL) 模型架构，用于图像分割场景。方法利用全卷积网络 (FCN) 在缺少标签数据的联邦中进行操作，并通过中央服务器私有共享权重，从而实现高效的数据协作。实验在 CamVid 数据集上评估了特征压缩的影响，结果显示该系统在 VFL 约束下保持了名义准确率，为隐私保护下的图像分割任务提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "C.2.4; I.2.8; I.4; I.4.8"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.07931v2",
      "published_date": "2024-01-15 19:47:14 UTC",
      "updated_date": "2024-03-19 17:07:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:53:00.295867"
    },
    {
      "arxiv_id": "2401.07927v4",
      "title": "Are self-explanations from Large Language Models faithful?",
      "title_zh": "翻译失败",
      "authors": [
        "Andreas Madsen",
        "Sarath Chandar",
        "Siva Reddy"
      ],
      "abstract": "Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.",
      "tldr_zh": "本研究探讨了指令微调的大型语言模型 (LLMs) 的自解释 (self-explanations) 是否忠实于模型的行为，因为不准确的自解释可能导致对 LLMs 的过度信任。作者提出使用自一致性检查 (self-consistency checks) 来衡量解释的 interpretability-faithfulness，例如，通过移除模型声称的重要词语并观察预测变化。结果显示，忠实度取决于解释类型、模型和任务：在情感分类任务中，反事实 (counterfactual) 解释对 Llama2 最忠实，特征归因 (feature attribution) 对 Mistral 最忠实，而编辑 (redaction) 解释对 Falcon 40B 最忠实，因此自解释不应被完全信任。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The 62nd Annual Meeting of the Association for Computational\n  Linguistics",
      "pdf_url": "http://arxiv.org/pdf/2401.07927v4",
      "published_date": "2024-01-15 19:39:15 UTC",
      "updated_date": "2024-05-16 20:26:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:53:13.991440"
    },
    {
      "arxiv_id": "2401.07890v2",
      "title": "A Strategy for Implementing description Temporal Dynamic Algorithms in Dynamic Knowledge Graphs by SPIN",
      "title_zh": "翻译失败",
      "authors": [
        "Alireza Shahbazi",
        "Seyyed Ahmad Mirsanei",
        "Malikeh Haj Khan Mirzaye Sarraf",
        "Behrouz Minaei Bidgoli"
      ],
      "abstract": "Planning and reasoning about actions and processes, in addition to reasoning\nabout propositions, are important issues in recent logical and computer science\nstudies. The widespread use of actions in everyday life such as IoT, semantic\nweb services, etc., and the limitations and issues in the action formalisms are\ntwo factors that lead us to study how actions are represented.\n  Since 2007, there have been some ideas to integrate Description Logic (DL)\nand action formalisms for representing both static and dynamic knowledge.\nMeanwhile, time is an important factor in dynamic situations, and actions\nchange states over time. In this study, on the one hand, we examined related\nlogical structures such as extensions of description logics (DLs), temporal\nformalisms, and action formalisms. On the other hand, we analyzed possible\ntools for designing and developing the Knowledge and Action Base (KAB).\n  For representation and reasoning about actions, we embedded actions into DLs\n(such as Dynamic-ALC and its extensions). We propose a terminable algorithm for\naction projection, planning, checking the satisfiability, consistency,\nrealizability, and executability, and also querying from KAB. Actions in this\nframework were modeled with SPIN and added to state space. This framework has\nalso been implemented as a plugin for the Prot\\'eg\\'e ontology editor.\n  During the last two decades, various algorithms have been presented, but due\nto the high computational complexity, we face many problems in implementing\ndynamic ontologies. In addition, an algorithm to detect the inconsistency of\nactions' effects was not explicitly stated. In the proposed strategy, the\ninteractions of actions with other parts of modeled knowledge, and a method to\ncheck consistency between the effects of actions are presented. With this\nframework, the ramification problem can be well handled in future works.",
      "tldr_zh": "本研究提出了一种策略，使用 SPIN 在动态知识图谱中实现描述性时间动态算法，旨在处理动作规划、推理和状态变化问题，同时整合 Description Logic (DL) 和时间形式主义。方法包括将动作嵌入 DLs（如 Dynamic-ALC），开发算法用于动作投影、规划、检查满足性、一致性、可实现性和可执行性，并构建知识和动作库 (KAB) 作为 Protégé 插件。该框架解决了现有算法的计算复杂性问题，并引入检测动作效果不一致的方法，为未来处理分支问题 (ramification problem) 奠定基础。实验表明，该策略提高了动态知识的表示和推理效率。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07890v2",
      "published_date": "2024-01-15 18:43:48 UTC",
      "updated_date": "2024-01-20 14:18:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:53:26.493690"
    },
    {
      "arxiv_id": "2401.07889v1",
      "title": "Machine Learning Techniques to Identify Hand Gestures amidst Forearm Muscle Signals",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan Cho",
        "Sunil Patel",
        "Kyu Taek Cho",
        "Jaejin Hwang"
      ],
      "abstract": "This study investigated the use of forearm EMG data for distinguishing eight\nhand gestures, employing the Neural Network and Random Forest algorithms on\ndata from ten participants. The Neural Network achieved 97 percent accuracy\nwith 1000-millisecond windows, while the Random Forest achieved 85 percent\naccuracy with 200-millisecond windows. Larger window sizes improved gesture\nclassification due to increased temporal resolution. The Random Forest\nexhibited faster processing at 92 milliseconds, compared to the Neural\nNetwork's 124 milliseconds. In conclusion, the study identified a Neural\nNetwork with a 1000-millisecond stream as the most accurate (97 percent), and a\nRandom Forest with a 200-millisecond stream as the most efficient (85 percent).\nFuture research should focus on increasing sample size, incorporating more hand\ngestures, and exploring different feature extraction methods and modeling\nalgorithms to enhance system accuracy and efficiency.",
      "tldr_zh": "本研究使用 Neural Network 和 Random Forest 算法分析前臂 EMG 数据，以区分八种手势，基于十个参与者的数据。结果显示，Neural Network 在 1000 毫秒窗口下达到了 97% 的准确率，而 Random Forest 在 200 毫秒窗口下达到了 85% 的准确率；更大的窗口大小提升了分类性能，并突显了 Random Forest 的处理速度优势（92 毫秒 vs. Neural Network 的 124 毫秒）。总体而言，Neural Network 被认定为最准确的模型，而 Random Forest 则更高效，未来工作应扩大样本量、增加手势类型，并探索新的特征提取方法和算法以进一步提升系统性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.07889v1",
      "published_date": "2024-01-15 18:39:13 UTC",
      "updated_date": "2024-01-15 18:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:53:38.570139"
    },
    {
      "arxiv_id": "2401.07886v2",
      "title": "Learned Best-Effort LLM Serving",
      "title_zh": "翻译失败",
      "authors": [
        "Siddharth Jha",
        "Coleman Hooper",
        "Xiaoxuan Liu",
        "Sehoon Kim",
        "Kurt Keutzer"
      ],
      "abstract": "Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.",
      "tldr_zh": "本文提出了一种基于深度强化学习的best-effort LLM serving系统，用于动态调整服务质量以应对波动请求模式，从而避免资源过度分配。相比静态服务，该系统在不可预测的工作负载下，能以10倍更高的客户端请求率保持可用性，并使服务性能超过96%峰值频率提高4.1倍、超过98%峰值频率提高2.3倍。该方法通过提高硬件利用率实现成本效率，并为应用开发者提供灵活性，适用于多种LLM服务场景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Es-FoMo @ ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.07886v2",
      "published_date": "2024-01-15 18:28:17 UTC",
      "updated_date": "2024-07-15 03:54:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:53:49.945754"
    },
    {
      "arxiv_id": "2401.07883v1",
      "title": "The Chronicles of RAG: The Retriever, the Chunk and the Generator",
      "title_zh": "RAG 编年史：检索器、块和生成器",
      "authors": [
        "Paulo Finardi",
        "Leonardo Avila",
        "Rodrigo Castaldoni",
        "Pedro Gengo",
        "Celio Larcher",
        "Marcos Piau",
        "Pablo Costa",
        "Vinicius Caridá"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has become one of the most popular\nparadigms for enabling LLMs to access external data, and also as a mechanism\nfor grounding to mitigate against hallucinations. When implementing RAG you can\nface several challenges like effective integration of retrieval models,\nefficient representation learning, data diversity, computational efficiency\noptimization, evaluation, and quality of text generation. Given all these\nchallenges, every day a new technique to improve RAG appears, making it\nunfeasible to experiment with all combinations for your problem. In this\ncontext, this paper presents good practices to implement, optimize, and\nevaluate RAG for the Brazilian Portuguese language, focusing on the\nestablishment of a simple pipeline for inference and experiments. We explored a\ndiverse set of methods to answer questions about the first Harry Potter book.\nTo generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview,\ngpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the\nretriever, our approach achieved an improvement of MRR@10 by 35.4% compared to\nthe baseline. When optimizing the input size in the application, we observed\nthat it is possible to further enhance it by 2.4%. Finally, we present the\ncomplete architecture of the RAG with our recommendations. As result, we moved\nfrom a baseline of 57.88% to a maximum relative score of 98.61%.",
      "tldr_zh": "本论文探讨了Retrieval Augmented Generation (RAG) 的实施挑战，包括检索模型整合、表示学习和文本生成质量优化，并为巴西葡萄牙语提供最佳实践。研究者建立了一个简单推理管道，使用OpenAI的gpt-4系列和Google的Gemini Pro模型，通过多种方法回答哈利波特书籍相关问题，优化了检索器质量，使MRR@10较基线提升35.4%，并通过输入大小优化进一步提高2.4%。最终，RAG架构从57.88%的基线分数提升至98.61%，为高效RAG应用提供了实用推荐。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 15 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.07883v1",
      "published_date": "2024-01-15 18:25:18 UTC",
      "updated_date": "2024-01-15 18:25:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:54:02.121220"
    },
    {
      "arxiv_id": "2401.07877v1",
      "title": "EMBRE: Entity-aware Masking for Biomedical Relation Extraction",
      "title_zh": "EMBRE: 实体感知掩码用于生物医学关系抽取",
      "authors": [
        "Mingjie Li",
        "Karin Verspoor"
      ],
      "abstract": "Information extraction techniques, including named entity recognition (NER)\nand relation extraction (RE), are crucial in many domains to support making\nsense of vast amounts of unstructured text data by identifying and connecting\nrelevant information. Such techniques can assist researchers in extracting\nvaluable insights. In this paper, we introduce the Entity-aware Masking for\nBiomedical Relation Extraction (EMBRE) method for biomedical relation\nextraction, as applied in the context of the BioRED challenge Task 1, in which\nhuman-annotated entities are provided as input. Specifically, we integrate\nentity knowledge into a deep neural network by pretraining the backbone model\nwith an entity masking objective. We randomly mask named entities for each\ninstance and let the model identify the masked entity along with its type. In\nthis way, the model is capable of learning more specific knowledge and more\nrobust representations. Then, we utilize the pre-trained model as our backbone\nto encode language representations and feed these representations into two\nmultilayer perceptron (MLPs) to predict the logits for relation and novelty,\nrespectively. The experimental results demonstrate that our proposed method can\nimprove the performances of entity pair, relation and novelty extraction over\nour baseline.",
      "tldr_zh": "这篇论文引入了 EMBRE 方法，用于生物医学关系提取 (Biomedical Relation Extraction)，旨在通过整合实体知识提升模型在处理非结构化文本时的性能。方法的核心是采用实体感知 masking 技术，在预训练阶段随机 masking 命名实体 (named entities)，让模型学习预测被 masking 的实体及其类型，从而获得更具体和鲁棒的语言表示。最终，实验结果显示，EMBRE 在 BioRED 挑战任务 1 上显著提高了实体对、关系和新颖性 (novelty) 提取的整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2401.07877v1",
      "published_date": "2024-01-15 18:12:01 UTC",
      "updated_date": "2024-01-15 18:12:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:54:14.480164"
    },
    {
      "arxiv_id": "2401.07871v1",
      "title": "Explainable Predictive Maintenance: A Survey of Current Methods, Challenges and Opportunities",
      "title_zh": "可解释的预测性维护：当前方法、挑战和机会的综述",
      "authors": [
        "Logan Cummins",
        "Alex Sommers",
        "Somayeh Bakhtiari Ramezani",
        "Sudip Mittal",
        "Joseph Jabour",
        "Maria Seale",
        "Shahram Rahimi"
      ],
      "abstract": "Predictive maintenance is a well studied collection of techniques that aims\nto prolong the life of a mechanical system by using artificial intelligence and\nmachine learning to predict the optimal time to perform maintenance. The\nmethods allow maintainers of systems and hardware to reduce financial and time\ncosts of upkeep. As these methods are adopted for more serious and potentially\nlife-threatening applications, the human operators need trust the predictive\nsystem. This attracts the field of Explainable AI (XAI) to introduce\nexplainability and interpretability into the predictive system. XAI brings\nmethods to the field of predictive maintenance that can amplify trust in the\nusers while maintaining well-performing systems. This survey on explainable\npredictive maintenance (XPM) discusses and presents the current methods of XAI\nas applied to predictive maintenance while following the Preferred Reporting\nItems for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We\ncategorize the different XPM methods into groups that follow the XAI\nliterature. Additionally, we include current challenges and a discussion on\nfuture research directions in XPM.",
      "tldr_zh": "这篇论文对Explainable Predictive Maintenance (XPM)进行了系统调查，探讨了Explainable AI (XAI)如何应用于预测性维护，以提升系统信任并减少维护成本。论文遵循PRISMA 2020指南，分类并总结了当前XPM方法，包括XAI在机械系统寿命预测中的各种技术。研究突出了现有挑战，如在高风险应用中的可解释性问题，并提出了未来研究方向，以推动更可靠的自主维护系统。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07871v1",
      "published_date": "2024-01-15 18:06:59 UTC",
      "updated_date": "2024-01-15 18:06:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:54:25.440043"
    },
    {
      "arxiv_id": "2401.07870v2",
      "title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification",
      "title_zh": "翻译失败",
      "authors": [
        "Mouxiang Chen",
        "Hao Tian",
        "Zhongxin Liu",
        "Xiaoxue Ren",
        "Jianling Sun"
      ],
      "abstract": "While existing code large language models (code LLMs) exhibit impressive\ncapabilities in code generation, their autoregressive sequential generation\ninherently lacks reversibility. This limitation hinders them from timely\ncorrecting previous missing statements during coding as humans do, often\nleading to error propagation and suboptimal performance. We introduce\nJumpCoder, a novel model-agnostic framework that enables human-like online\nmodification and non-sequential generation to augment code LLMs. The key idea\nbehind JumpCoder is to insert new code into the currently generated code when\nnecessary during generation, which is achieved through an auxiliary infilling\nmodel that works in tandem with the code LLM. Since identifying the best infill\nposition beforehand is intractable, we adopt an \\textit{infill-first,\njudge-later} strategy, which experiments with filling at the $k$ most critical\npositions following the generation of each line, and uses an Abstract Syntax\nTree (AST) parser alongside the Generation Model Scoring to effectively judge\nthe validity of each potential infill. Extensive experiments using six\nstate-of-the-art code LLMs across multiple and multilingual benchmarks\nconsistently indicate significant improvements over all baselines. Our code is\npublic at https://github.com/Keytoyze/JumpCoder.",
      "tldr_zh": "本研究指出，现有的代码大语言模型（code LLMs）因自回归顺序生成缺乏可逆性，难以及时修正错误，导致性能下降。为解决此问题，提出 JumpCoder，一种模型无关框架，通过辅助 infilling 模型实现人类般的在线修改和非顺序生成。JumpCoder 采用“infill-first, judge-later”策略，在生成每行代码后尝试在 k 个关键位置填充新代码，并利用 Abstract Syntax Tree (AST) 解析器和 Generation Model Scoring 判断填充的有效性。在多个多语言基准上的实验显示，JumpCoder 显著提升了六种最先进 code LLMs 的性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 (main)",
      "pdf_url": "http://arxiv.org/pdf/2401.07870v2",
      "published_date": "2024-01-15 18:04:29 UTC",
      "updated_date": "2024-06-05 14:12:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:54:38.252751"
    },
    {
      "arxiv_id": "2401.07868v1",
      "title": "Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability",
      "title_zh": "翻译失败",
      "authors": [
        "Md Sadman Sakib",
        "Yu Sun"
      ],
      "abstract": "The inherent probabilistic nature of Large Language Models (LLMs) introduces\nan element of unpredictability, raising concerns about potential discrepancies\nin their output. This paper introduces an innovative approach aims to generate\ncorrect and optimal robotic task plans for diverse real-world demands and\nscenarios. LLMs have been used to generate task plans, but they are unreliable\nand may contain wrong, questionable, or high-cost steps. The proposed approach\nuses LLM to generate a number of task plans as trees and amalgamates them into\na graph by removing questionable paths. Then an optimal task tree can be\nretrieved to circumvent questionable and high-cost nodes, thereby improving\nplanning accuracy and execution efficiency. The approach is further improved by\nincorporating a large knowledge network. Leveraging GPT-4 further, the\nhigh-level task plan is converted into a low-level Planning Domain Definition\nLanguage (PDDL) plan executable by a robot. Evaluation results highlight the\nsuperior accuracy and efficiency of our approach compared to previous\nmethodologies in the field of task planning.",
      "tldr_zh": "该论文针对 Large Language Models (LLMs) 生成机器人任务计划的不可靠性问题（如错误、可疑或高成本步骤），提出了一种创新方法，通过生成多个任务计划树并整合成图的方式移除可疑路径，从而检索出最优任务树。方法进一步整合大型知识网络，并利用 GPT-4 将高水平任务计划转换为可执行的低水平 Planning Domain Definition Language (PDDL) 计划，以提升规划准确性和效率。实验评估表明，该方法在机器人任务规划领域比现有方法更准确和高效，为可靠的机器人规划提供了新途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07868v1",
      "published_date": "2024-01-15 18:01:59 UTC",
      "updated_date": "2024-01-15 18:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:54:50.669517"
    },
    {
      "arxiv_id": "2401.07862v1",
      "title": "Adaptive Neural-Operator Backstepping Control of a Benchmark Hyperbolic PDE",
      "title_zh": "翻译失败",
      "authors": [
        "Maxence Lamarque",
        "Luke Bhan",
        "Yuanyuan Shi",
        "Miroslav Krstic"
      ],
      "abstract": "To stabilize PDEs, feedback controllers require gain kernel functions, which\nare themselves governed by PDEs. Furthermore, these gain-kernel PDEs depend on\nthe PDE plants' functional coefficients. The functional coefficients in PDE\nplants are often unknown. This requires an adaptive approach to PDE control,\ni.e., an estimation of the plant coefficients conducted concurrently with\ncontrol, where a separate PDE for the gain kernel must be solved at each\ntimestep upon the update in the plant coefficient function estimate. Solving a\nPDE at each timestep is computationally expensive and a barrier to the\nimplementation of real-time adaptive control of PDEs. Recently, results in\nneural operator (NO) approximations of functional mappings have been introduced\ninto PDE control, for replacing the computation of the gain kernel with a\nneural network that is trained, once offline, and reused in real-time for rapid\nsolution of the PDEs. In this paper, we present the first result on applying\nNOs in adaptive PDE control, presented for a benchmark 1-D hyperbolic PDE with\nrecirculation. We establish global stabilization via Lyapunov analysis, in the\nplant and parameter error states, and also present an alternative approach, via\npassive identifiers, which avoids the strong assumptions on kernel\ndifferentiability. We then present numerical simulations demonstrating\nstability and observe speedups up to three orders of magnitude, highlighting\nthe real-time efficacy of neural operators in adaptive control. Our code\n(Github) is made publicly available for future researchers.",
      "tldr_zh": "本研究针对超声速PDE（Hyperbolic PDE）的自适应控制问题，提出了一种结合Neural Operator（NO）和Backstepping控制的方法，以解决传统控制中每次更新未知系数时需求解增益核PDE带来的高计算开销。论文首次将NO应用于自适应PDE控制，通过离线训练NO网络来快速替代实时求解增益核PDE，并在基准1-D超声速PDE上证明了全局稳定性，经Lyapunov分析和被动标识符（passive identifiers）方法验证。数值模拟结果显示，该方法实现了高达三个数量级的计算加速，同时确保系统稳定，并公开了代码以支持后续研究。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "math.DS",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "16.5 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.07862v1",
      "published_date": "2024-01-15 17:52:15 UTC",
      "updated_date": "2024-01-15 17:52:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:55:01.404899"
    },
    {
      "arxiv_id": "2401.07844v6",
      "title": "The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise",
      "title_zh": "翻译失败",
      "authors": [
        "Shuze Daniel Liu",
        "Shuhang Chen",
        "Shangtong Zhang"
      ],
      "abstract": "Stochastic approximation is a class of algorithms that update a vector\niteratively, incrementally, and stochastically, including, e.g., stochastic\ngradient descent and temporal difference learning. One fundamental challenge in\nanalyzing a stochastic approximation algorithm is to establish its stability,\ni.e., to show that the stochastic vector iterates are bounded almost surely. In\nthis paper, we extend the celebrated Borkar-Meyn theorem for stability from the\nMartingale difference noise setting to the Markovian noise setting, which\ngreatly improves its applicability in reinforcement learning, especially in\nthose off-policy reinforcement learning algorithms with linear function\napproximation and eligibility traces. Central to our analysis is the\ndiminishing asymptotic rate of change of a few functions, which is implied by\nboth a form of the strong law of large numbers and a form of the law of the\niterated logarithm.",
      "tldr_zh": "本论文扩展了Borkar-Meyn theorem，将其从Martingale difference noise设置应用到Markovian noise环境中，以分析随机逼近(Stochastic Approximation)算法的稳定性，特别是针对强化学习(Reinforcement Learning)中的迭代更新问题。核心方法依赖于几个函数的渐近变化率递减，这通过强大型数定律(strong law of large numbers)和迭代对数定律(law of the iterated logarithm)的形式来证明。结果表明，这一扩展显著提高了off-policy算法的适用性，尤其在结合线性函数逼近(linear function approximation)和资格迹(eligibility traces)时，确保了算法的几乎肯定有界性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Journal of Machine Learning Research (JMLR), 2025",
      "pdf_url": "http://arxiv.org/pdf/2401.07844v6",
      "published_date": "2024-01-15 17:20:17 UTC",
      "updated_date": "2025-02-05 19:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:55:15.293595"
    },
    {
      "arxiv_id": "2401.07836v3",
      "title": "Two Types of AI Existential Risk: Decisive and Accumulative",
      "title_zh": "AI存在性风险",
      "authors": [
        "Atoosa Kasirzadeh"
      ],
      "abstract": "The conventional discourse on existential risks (x-risks) from AI typically\nfocuses on abrupt, dire events caused by advanced AI systems, particularly\nthose that might achieve or surpass human-level intelligence. These events have\nsevere consequences that either lead to human extinction or irreversibly\ncripple human civilization to a point beyond recovery. This discourse, however,\noften neglects the serious possibility of AI x-risks manifesting incrementally\nthrough a series of smaller yet interconnected disruptions, gradually crossing\ncritical thresholds over time. This paper contrasts the conventional \"decisive\nAI x-risk hypothesis\" with an \"accumulative AI x-risk hypothesis.\" While the\nformer envisions an overt AI takeover pathway, characterized by scenarios like\nuncontrollable superintelligence, the latter suggests a different causal\npathway to existential catastrophes. This involves a gradual accumulation of\ncritical AI-induced threats such as severe vulnerabilities and systemic erosion\nof economic and political structures. The accumulative hypothesis suggests a\nboiling frog scenario where incremental AI risks slowly converge, undermining\nsocietal resilience until a triggering event results in irreversible collapse.\nThrough systems analysis, this paper examines the distinct assumptions\ndifferentiating these two hypotheses. It is then argued that the accumulative\nview can reconcile seemingly incompatible perspectives on AI risks. The\nimplications of differentiating between these causal pathways -- the decisive\nand the accumulative -- for the governance of AI as well as long-term AI safety\nare discussed.",
      "tldr_zh": "该论文探讨了AI存在风险（x-risks）的两种类型：decisive AI x-risk hypothesis（突然的灾难性事件，如AI超智能接管导致人类灭绝）和accumulative AI x-risk hypothesis（渐进积累的威胁，通过一系列小规模干扰逐步侵蚀经济、政治结构，最终引发不可逆转的崩溃）。作者通过系统分析对比了这两者的核心假设，论证accumulative观点能调和看似矛盾的AI风险视角。研究强调，区分这些风险路径对AI治理和长期安全策略具有重要启示。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Journal article for Philosophical Studies",
      "pdf_url": "http://arxiv.org/pdf/2401.07836v3",
      "published_date": "2024-01-15 17:06:02 UTC",
      "updated_date": "2025-01-17 16:35:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:55:26.125955"
    },
    {
      "arxiv_id": "2401.07810v1",
      "title": "Consolidating Strategies for Countering Hate Speech Using Persuasive Dialogues",
      "title_zh": "通过说服性对话整合对抗仇恨言论的策略",
      "authors": [
        "Sougata Saha",
        "Rohini Srihari"
      ],
      "abstract": "Hateful comments are prevalent on social media platforms. Although tools for\nautomatically detecting, flagging, and blocking such false, offensive, and\nharmful content online have lately matured, such reactive and brute force\nmethods alone provide short-term and superficial remedies while the\nperpetrators persist. With the public availability of large language models\nwhich can generate articulate synthetic and engaging content at scale, there\nare concerns about the rapid growth of dissemination of such malicious content\non the web. There is now a need to focus on deeper, long-term solutions that\ninvolve engaging with the human perpetrator behind the source of the content to\nchange their viewpoint or at least bring down the rhetoric using persuasive\nmeans. To do that, we propose defining and experimenting with controllable\nstrategies for generating counter-arguments to hateful comments in online\nconversations. We experiment with controlling response generation using\nfeatures based on (i) argument structure and reasoning-based Walton argument\nschemes, (ii) counter-argument speech acts, and (iii) human\ncharacteristics-based qualities such as Big-5 personality traits and human\nvalues. Using automatic and human evaluations, we determine the best\ncombination of features that generate fluent, argumentative, and logically\nsound arguments for countering hate. We further share the developed\ncomputational models for automatically annotating text with such features, and\na silver-standard annotated version of an existing hate speech dialog corpora.",
      "tldr_zh": "这篇论文针对社交媒体上普遍存在的仇恨言论，提出了一种基于说服性对话的长期解决方案，旨在通过生成可控的反驳论点来改变发布者的观点或降低言论强度。研究方法包括使用基于Walton argument schemes的论点结构、counter-argument speech acts，以及Big-5 personality traits和human values等人类特征来控制响应生成，从而确保反驳内容流畅、论证性强且逻辑健全。通过自动和人工评估，论文确定了最佳特征组合，提高了反驳效果，并分享了用于自动标注文本的计算模型以及一个银标准标注的仇恨言论对话语料库。总的来说，该工作为对抗仇恨言论提供了更深入和可扩展的策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07810v1",
      "published_date": "2024-01-15 16:31:18 UTC",
      "updated_date": "2024-01-15 16:31:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:55:38.824685"
    },
    {
      "arxiv_id": "2401.07796v2",
      "title": "Fusing Echocardiography Images and Medical Records for Continuous Patient Stratification",
      "title_zh": "融合超声心动图图像和医疗记录用于连续患者分层",
      "authors": [
        "Nathan Painchaud",
        "Jérémie Stym-Popper",
        "Pierre-Yves Courand",
        "Nicolas Thome",
        "Pierre-Marc Jodoin",
        "Nicolas Duchateau",
        "Olivier Bernard"
      ],
      "abstract": "Deep learning enables automatic and robust extraction of cardiac function\ndescriptors from echocardiographic sequences, such as ejection fraction or\nstrain. These descriptors provide fine-grained information that physicians\nconsider, in conjunction with more global variables from the clinical record,\nto assess patients' condition. Drawing on novel transformer models applied to\ntabular data, we propose a method that considers all descriptors extracted from\nmedical records and echocardiograms to learn the representation of a\ncardiovascular pathology with a difficult-to-characterize continuum, namely\nhypertension. Our method first projects each variable into its own\nrepresentation space using modality-specific approaches. These standardized\nrepresentations of multimodal data are then fed to a transformer encoder, which\nlearns to merge them into a comprehensive representation of the patient through\nthe task of predicting a clinical rating. This stratification task is\nformulated as an ordinal classification to enforce a pathological continuum in\nthe representation space. We observe the major trends along this continuum on a\ncohort of 239 hypertensive patients, providing unprecedented details in the\ndescription of hypertension's impact on various cardiac function descriptors.\nOur analysis shows that i) the XTab foundation model's architecture allows to\nreach outstanding performance (98% AUROC) even with limited data (less than 200\ntraining samples), ii) stratification across the population is reproducible\nbetween trainings (within 3.6% MAE), and iii) patterns emerge in descriptors,\nsome of which align with established physiological knowledge about\nhypertension, while others could pave the way for a more comprehensive\nunderstanding of this pathology.",
      "tldr_zh": "本文提出一种方法，通过融合超声心动图序列和医疗记录，使用 Transformer 模型对高血压患者进行连续分层，旨在整合心脏功能描述符（如射血分数或应变）和临床变量。方法包括将每个变量投影到特定表示空间，然后通过 Transformer 编码器合并这些多模态数据，并以序数分类任务预测临床评分，以强制表示空间中的病理连续性。在 239 名高血压患者队列上，实验显示 XTab 基础模型在少于 200 个训练样本下达到 98% AUROC，分层结果在训练间可重现（3.6% MAE 以内），并揭示了与生理知识一致的模式，可能为高血压理解提供新洞见。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages + 2 pages of supplementary material, submitted to IEEE\n  journal",
      "pdf_url": "http://arxiv.org/pdf/2401.07796v2",
      "published_date": "2024-01-15 16:04:46 UTC",
      "updated_date": "2024-10-11 16:28:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:55:51.304680"
    },
    {
      "arxiv_id": "2401.07764v2",
      "title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Minrui Xu",
        "Dusit Niyato",
        "Jiawen Kang",
        "Zehui Xiong",
        "Shiwen Mao",
        "Zhu Han",
        "Dong In Kim",
        "Khaled B. Letaief"
      ],
      "abstract": "AI agents based on multimodal large language models (LLMs) are expected to\nrevolutionize human-computer interaction and offer more personalized assistant\nservices across various domains like healthcare, education, manufacturing, and\nentertainment. Deploying LLM agents in 6G networks enables users to access\npreviously expensive AI assistant services via mobile devices democratically,\nthereby reducing interaction latency and better preserving user privacy.\nNevertheless, the limited capacity of mobile devices constrains the\neffectiveness of deploying and executing local LLMs, which necessitates\noffloading complex tasks to global LLMs running on edge servers during\nlong-horizon interactions. In this article, we propose a split learning system\nfor LLM agents in 6G networks leveraging the collaboration between mobile\ndevices and edge servers, where multiple LLMs with different roles are\ndistributed across mobile devices and edge servers to perform user-agent\ninteractive tasks collaboratively. In the proposed system, LLM agents are split\ninto perception, grounding, and alignment modules, facilitating inter-module\ncommunications to meet extended user requirements on 6G network functions,\nincluding integrated sensing and communication, digital twins, and\ntask-oriented communications. Furthermore, we introduce a novel model caching\nalgorithm for LLMs within the proposed system to improve model utilization in\ncontext, thus reducing network costs of the collaborative mobile and edge LLM\nagents.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLM)代理在6G网络中的应用，旨在通过移动设备和边缘服务器的协作，提升人机交互效率并提供个性化服务，如医疗、教育等领域。作者提出一个分拆学习系统，将LLM代理分解为感知、接地和对齐模块，这些模块协同工作，利用6G网络功能（如集成感知与通信、数字孪生和任务导向通信）来处理复杂任务。系统还引入了一个新型模型缓存算法，以优化模型利用率、减少网络成本和交互延迟，同时更好地保护用户隐私。",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07764v2",
      "published_date": "2024-01-15 15:20:59 UTC",
      "updated_date": "2024-02-16 19:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:56:02.622029"
    },
    {
      "arxiv_id": "2401.08711v1",
      "title": "Assistant, Parrot, or Colonizing Loudspeaker? ChatGPT Metaphors for Developing Critical AI Literacies",
      "title_zh": "翻译失败",
      "authors": [
        "Anuj Gupta",
        "Yasser Atef",
        "Anna Mills",
        "Maha Bali"
      ],
      "abstract": "This study explores how discussing metaphors for AI can help build awareness\nof the frames that shape our understanding of AI systems, particularly large\nlanguage models (LLMs) like ChatGPT. Given the pressing need to teach \"critical\nAI literacy\", discussion of metaphor provides an opportunity for inquiry and\ndialogue with space for nuance, playfulness, and critique. Using a\ncollaborative autoethnographic methodology, we analyzed metaphors from a range\nof sources, and reflected on them individually according to seven questions,\nthen met and discussed our interpretations. We then analyzed how our\nreflections contributed to the three kinds of literacies delineated in Selber's\nmultiliteracies framework: functional, critical, and rhetorical. These allowed\nus to analyze questions of ethics, equity, and accessibility in relation to AI.\nWe explored each metaphor along the dimension of whether or not it was\npromoting anthropomorphizing, and to what extent such metaphors imply that AI\nis sentient. Our findings highlight the role of metaphor reflection in\nfostering a nuanced understanding of AI, suggesting that our collaborative\nautoethnographic approach as well as the heuristic model of plotting AI\nmetaphors on dimensions of anthropomorphism and multiliteracies, might be\nuseful for educators and researchers in the pursuit of advancing critical AI\nliteracy.",
      "tldr_zh": "这篇论文探讨了通过讨论 AI 隐喻（如将 ChatGPT 比作“助手”、“鹦鹉”或“殖民化扬声器”）来提升关键 AI 素养（critical AI literacy），帮助人们理解塑造 AI 认知的框架，特别是大型语言模型（LLMs）。研究采用协作自传民族志方法，分析各种来源的隐喻，并通过七个问题进行个人反思和集体讨论，以考察这些隐喻如何贡献于 Selber 的多重素养框架，包括功能性、批判性和修辞性素养。发现显示，这种隐喻反思能促进对 AI 伦理、公平性和可访问性的细致分析，同时评估隐喻是否推动拟人化（anthropomorphizing）和暗示 AI 是否有知觉（sentient），并提出协作方法和启发式模型作为教育者和研究者推进关键 AI 素养的实用工具。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "I.2.0; K.3.0; K.3.1; K.4.0; K.4.2; J.4; J.5"
      ],
      "primary_category": "cs.HC",
      "comment": "This is a preprint (accepted version) of an article that has been\n  accepted for publication at the journal Open Praxis: https://openpraxis.org/",
      "pdf_url": "http://arxiv.org/pdf/2401.08711v1",
      "published_date": "2024-01-15 15:15:48 UTC",
      "updated_date": "2024-01-15 15:15:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:56:15.318135"
    },
    {
      "arxiv_id": "2401.07744v2",
      "title": "Combining Machine Learning and Ontology: A Systematic Literature Review",
      "title_zh": "翻译失败",
      "authors": [
        "Sarah Ghidalia",
        "Ouassila Labbani Narsis",
        "Aurélie Bertaux",
        "Christophe Nicolle"
      ],
      "abstract": "Motivated by the desire to explore the process of combining inductive and\ndeductive reasoning, we conducted a systematic literature review of articles\nthat investigate the integration of machine learning and ontologies. The\nobjective was to identify diverse techniques that incorporate both inductive\nreasoning (performed by machine learning) and deductive reasoning (performed by\nontologies) into artificial intelligence systems. Our review, which included\nthe analysis of 128 studies, allowed us to identify three main categories of\nhybridization between machine learning and ontologies: learning-enhanced\nontologies, semantic data mining, and learning and reasoning systems. We\nprovide a comprehensive examination of all these categories, emphasizing the\nvarious machine learning algorithms utilized in the studies. Furthermore, we\ncompared our classification with similar recent work in the field of hybrid AI\nand neuro-symbolic approaches.",
      "tldr_zh": "这篇论文通过系统文献综述探讨了 machine learning（归纳推理）和 ontology（演绎推理）的整合，旨在构建更全面的 AI 系统。研究分析了 128 篇相关文献，并将混合方法分类为三大类别：学习增强本体、语义数据挖掘，以及学习和推理系统，同时强调了所采用的各种 machine learning 算法。论文还与其他混合 AI 和神经符号方法进行了比较，为未来 AI 系统的设计提供了宝贵见解。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07744v2",
      "published_date": "2024-01-15 14:56:04 UTC",
      "updated_date": "2024-02-19 10:43:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:56:26.323079"
    },
    {
      "arxiv_id": "2401.07729v2",
      "title": "SSL-Interactions: Pretext Tasks for Interactive Trajectory Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Prarthana Bhattacharyya",
        "Chengjie Huang",
        "Krzysztof Czarnecki"
      ],
      "abstract": "This paper addresses motion forecasting in multi-agent environments, pivotal\nfor ensuring safety of autonomous vehicles. Traditional as well as recent\ndata-driven marginal trajectory prediction methods struggle to properly learn\nnon-linear agent-to-agent interactions. We present SSL-Interactions that\nproposes pretext tasks to enhance interaction modeling for trajectory\nprediction. We introduce four interaction-aware pretext tasks to encapsulate\nvarious aspects of agent interactions: range gap prediction, closest distance\nprediction, direction of movement prediction, and type of interaction\nprediction. We further propose an approach to curate interaction-heavy\nscenarios from datasets. This curated data has two advantages: it provides a\nstronger learning signal to the interaction model, and facilitates generation\nof pseudo-labels for interaction-centric pretext tasks. We also propose three\nnew metrics specifically designed to evaluate predictions in interactive\nscenes. Our empirical evaluations indicate SSL-Interactions outperforms\nstate-of-the-art motion forecasting methods quantitatively with up to 8%\nimprovement, and qualitatively, for interaction-heavy scenarios.",
      "tldr_zh": "本文针对多代理环境中的轨迹预测问题，提出 SSL-Interactions 方法，通过设计四个互动感知的 pretext tasks（如范围间隙预测、最近距离预测、运动方向预测和互动类型预测）来提升代理间非线性互动的建模能力。该方法还包括从数据集 curation 互动密集场景的策略，以提供更强的学习信号并生成伪标签。论文引入三个新指标评估互动场景中的预测表现，实验结果显示 SSL-Interactions 在量化指标上比最先进方法提高多达 8%，并在质化方面表现出色，尤其适用于互动密集场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IV-2024. 13 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.07729v2",
      "published_date": "2024-01-15 14:43:40 UTC",
      "updated_date": "2024-08-26 09:16:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:56:39.584454"
    },
    {
      "arxiv_id": "2401.07722v1",
      "title": "Inferring Preferences from Demonstrations in Multi-Objective Residential Energy Management",
      "title_zh": "翻译失败",
      "authors": [
        "Junlin Lu",
        "Patrick Mannion",
        "Karl Mason"
      ],
      "abstract": "It is often challenging for a user to articulate their preferences accurately\nin multi-objective decision-making problems. Demonstration-based preference\ninference (DemoPI) is a promising approach to mitigate this problem.\nUnderstanding the behaviours and values of energy customers is an example of a\nscenario where preference inference can be used to gain insights into the\nvalues of energy customers with multiple objectives, e.g. cost and comfort. In\nthis work, we applied the state-of-art DemoPI method, i.e., the dynamic\nweight-based preference inference (DWPI) algorithm in a multi-objective\nresidential energy consumption setting to infer preferences from energy\nconsumption demonstrations by simulated users following a rule-based approach.\nAccording to our experimental results, the DWPI model achieves accurate\ndemonstration-based preference inferring in three scenarios. These advancements\nenhance the usability and effectiveness of multi-objective reinforcement\nlearning (MORL) in energy management, enabling more intuitive and user-friendly\npreference specifications, and opening the door for DWPI to be applied in\nreal-world settings.",
      "tldr_zh": "本研究探讨了在多目标住宅能源管理中，用户难以准确表达偏好的问题，并引入基于演示的偏好推断（DemoPI）方法。具体而言，使用动态权重-based 偏好推断（DWPI）算法，从模拟用户的能源消耗演示中推断偏好，如成本与舒适的权衡。实验结果显示，DWPI 在三种场景中实现了高准确率，从而提升了多目标强化学习（MORL）的可用性和有效性，并为真实世界能源管理应用提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07722v1",
      "published_date": "2024-01-15 14:36:59 UTC",
      "updated_date": "2024-01-15 14:36:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:56:50.913656"
    },
    {
      "arxiv_id": "2401.07710v1",
      "title": "Go-Explore for Residential Energy Management",
      "title_zh": "翻译失败",
      "authors": [
        "Junlin Lu",
        "Patrick Mannion",
        "Karl Mason"
      ],
      "abstract": "Reinforcement learning is commonly applied in residential energy management,\nparticularly for optimizing energy costs. However, RL agents often face\nchallenges when dealing with deceptive and sparse rewards in the energy control\ndomain, especially with stochastic rewards. In such situations, thorough\nexploration becomes crucial for learning an optimal policy. Unfortunately, the\nexploration mechanism can be misled by deceptive reward signals, making\nthorough exploration difficult. Go-Explore is a family of algorithms which\ncombines planning methods and reinforcement learning methods to achieve\nefficient exploration. We use the Go-Explore algorithm to solve the cost-saving\ntask in residential energy management problems and achieve an improvement of up\nto 19.84\\% compared to the well-known reinforcement learning algorithms.",
      "tldr_zh": "该论文探讨了 Reinforcement Learning 在住宅能源管理中的应用问题，特别是 RL 代理在处理欺骗性和稀疏奖励时面临的探索挑战，导致学习最优策略困难。作者引入 Go-Explore 算法，该方法结合规划和 Reinforcement Learning 技术，实现高效探索。实验结果显示，在住宅能源管理的成本节约任务上，Go-Explore 比传统 RL 算法提升高达 19.84%。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07710v1",
      "published_date": "2024-01-15 14:26:44 UTC",
      "updated_date": "2024-01-15 14:26:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:57:01.391918"
    },
    {
      "arxiv_id": "2401.07709v2",
      "title": "Towards Efficient Diffusion-Based Image Editing with Instant Attention Masks",
      "title_zh": "翻译失败",
      "authors": [
        "Siyu Zou",
        "Jiji Tang",
        "Yiyi Zhou",
        "Jing He",
        "Chaoyi Zhao",
        "Rongsheng Zhang",
        "Zhipeng Hu",
        "Xiaoshuai Sun"
      ],
      "abstract": "Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which\noften applies a semantic mask to control the target area for diffusion-based\nediting. However, most existing solutions obtain these masks via manual\noperations or off-line processing, greatly reducing their efficiency. In this\npaper, we propose a novel and efficient image editing method for Text-to-Image\n(T2I) diffusion models, termed Instant Diffusion Editing(InstDiffEdit). In\nparticular, InstDiffEdit aims to employ the cross-modal attention ability of\nexisting diffusion models to achieve instant mask guidance during the diffusion\nsteps. To reduce the noise of attention maps and realize the full automatics,\nwe equip InstDiffEdit with a training-free refinement scheme to adaptively\naggregate the attention distributions for the automatic yet accurate mask\ngeneration. Meanwhile, to supplement the existing evaluations of DIE, we\npropose a new benchmark called Editing-Mask to examine the mask accuracy and\nlocal editing ability of existing methods. To validate InstDiffEdit, we also\nconduct extensive experiments on ImageNet and Imagen, and compare it with a\nbunch of the SOTA methods. The experimental results show that InstDiffEdit not\nonly outperforms the SOTA methods in both image quality and editing results,\nbut also has a much faster inference speed, i.e., +5 to +6 times.",
      "tldr_zh": "这篇论文针对 Diffusion-based Image Editing (DIE) 的效率问题，提出了一种新方法 InstDiffEdit，用于 Text-to-Image (T2I) 扩散模型，通过 cross-modal attention 实现即时掩码指导，从而避免手动或离线处理。InstDiffEdit 采用一个 training-free refinement scheme 来聚合注意力分布，自动生成准确的掩码，提升编辑过程的自动化和精确性。同时，论文引入了 Editing-Mask 基准，用于评估掩码准确性和局部编辑能力。实验结果显示，InstDiffEdit 在 ImageNet 和 Imagen 数据集上优于现有 SOTA 方法，在图像质量和编辑效果方面表现突出，且推理速度提高了 5-6 倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI2024",
      "pdf_url": "http://arxiv.org/pdf/2401.07709v2",
      "published_date": "2024-01-15 14:25:54 UTC",
      "updated_date": "2024-01-23 11:22:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:57:14.886973"
    },
    {
      "arxiv_id": "2401.10158v3",
      "title": "DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolaos Koursioumpas",
        "Lina Magoula",
        "Ioannis Stavrakakis",
        "Nancy Alonistioti",
        "M. A. Gutierrez-Estevez",
        "Ramin Khalili"
      ],
      "abstract": "Beyond 5G and 6G networks are expected to support new and challenging use\ncases and applications that depend on a certain level of Quality of Service\n(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high\nimportance, especially for safety-critical applications as in the case of\nvehicular communications. Although until recent years the QoS prediction has\nbeen carried out by centralized Artificial Intelligence (AI) solutions, a\nnumber of privacy, computational, and operational concerns have emerged.\nAlternative solutions have surfaced (e.g. Split Learning, Federated Learning),\ndistributing AI tasks of reduced complexity across nodes, while preserving the\nprivacy of the data. However, new challenges rise when it comes to scalable\ndistributed learning approaches, taking into account the heterogeneous nature\nof future wireless networks. The current work proposes DISTINQT, a novel\nmulti-headed input privacy-aware distributed learning framework for QoS\nprediction. Our framework supports multiple heterogeneous nodes, in terms of\ndata types and model architectures, by sharing computations across them. This\nenables the incorporation of diverse knowledge into a sole learning process\nthat will enhance the robustness and generalization capabilities of the final\nQoS prediction model. DISTINQT also contributes to data privacy preservation by\nencoding any raw input data into highly complex, compressed, and irreversible\nlatent representations before any transmission. Evaluation results showcase\nthat DISTINQT achieves a statistically identical performance compared to its\ncentralized version, while also proving the validity of the privacy preserving\nclaims. DISTINQT manages to achieve a reduction in prediction error of up to\n65% on average against six state-of-the-art centralized baseline solutions\npresented in the Tele-Operated Driving use case.",
      "tldr_zh": "该论文提出DISTINQT，一种分布式隐私感知学习框架，用于未来移动和无线网络的QoS（Quality of Service）预测，旨在解决传统集中式AI方案的隐私、计算和操作问题。DISTINQT支持多头输入和异构节点，通过共享计算整合多样数据类型和模型架构，同时采用数据编码技术将原始输入转化为复杂、压缩且不可逆的潜在表示，以保护隐私。实验结果显示，DISTINQT的性能与集中式版本相当，并在遥控驾驶用例中平均将预测错误降低65%，优于六种最先进基线方案。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CR",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "12 Pages Double Column, 10 Figures, (Minor Revised Version) Accepted\n  for publication in the IEEE Transactions on Vehicular Technology (IEEE TVT)",
      "pdf_url": "http://arxiv.org/pdf/2401.10158v3",
      "published_date": "2024-01-15 13:00:48 UTC",
      "updated_date": "2024-11-04 13:26:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:57:25.758011"
    },
    {
      "arxiv_id": "2401.07656v4",
      "title": "Learning Explainable and Better Performing Representations of POMDP Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Bork",
        "Debraj Chakraborty",
        "Kush Grover",
        "Jan Kretinsky",
        "Stefanie Mohr"
      ],
      "abstract": "Strategies for partially observable Markov decision processes (POMDP)\ntypically require memory. One way to represent this memory is via automata. We\npresent a method to learn an automaton representation of a strategy using a\nmodification of the L*-algorithm. Compared to the tabular representation of a\nstrategy, the resulting automaton is dramatically smaller and thus also more\nexplainable. Moreover, in the learning process, our heuristics may even improve\nthe strategy's performance. In contrast to approaches that synthesize an\nautomaton directly from the POMDP thereby solving it, our approach is\nincomparably more scalable.",
      "tldr_zh": "本研究提出了一种方法，用于学习POMDP（Partially Observable Markov Decision Processes）策略的自动机（automata）表示，以提升策略的可解释性和性能。该方法基于L*-algorithm的修改版本，通过学习过程生成更紧凑的自动机表示，比传统的表格表示显著更小且更易解释。此外，该方法在学习中引入启发式优化，可能进一步改善策略的性能，并相比直接从POMDP合成自动机的现有方法，具有更高的可扩展性（scalable）。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical report for the submission to TACAS 24",
      "pdf_url": "http://arxiv.org/pdf/2401.07656v4",
      "published_date": "2024-01-15 12:52:56 UTC",
      "updated_date": "2024-10-02 12:12:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:57:37.422164"
    },
    {
      "arxiv_id": "2401.07655v1",
      "title": "MLAD: A Unified Model for Multi-system Log Anomaly Detection",
      "title_zh": "MLAD：多系统日志异常检测的统一模型",
      "authors": [
        "Runqiang Zang",
        "Hongcheng Guo",
        "Jian Yang",
        "Jiaheng Liu",
        "Zhoujun Li",
        "Tieqiao Zheng",
        "Xu Shi",
        "Liangfan Zheng",
        "Bo Zhang"
      ],
      "abstract": "In spite of the rapid advancements in unsupervised log anomaly detection\ntechniques, the current mainstream models still necessitate specific training\nfor individual system datasets, resulting in costly procedures and limited\nscalability due to dataset size, thereby leading to performance bottlenecks.\nFurthermore, numerous models lack cognitive reasoning capabilities, posing\nchallenges in direct transferability to similar systems for effective anomaly\ndetection. Additionally, akin to reconstruction networks, these models often\nencounter the \"identical shortcut\" predicament, wherein the majority of system\nlogs are classified as normal, erroneously predicting normal classes when\nconfronted with rare anomaly logs due to reconstruction errors.\n  To address the aforementioned issues, we propose MLAD, a novel anomaly\ndetection model that incorporates semantic relational reasoning across multiple\nsystems. Specifically, we employ Sentence-bert to capture the similarities\nbetween log sequences and convert them into highly-dimensional learnable\nsemantic vectors. Subsequently, we revamp the formulas of the Attention layer\nto discern the significance of each keyword in the sequence and model the\noverall distribution of the multi-system dataset through appropriate vector\nspace diffusion. Lastly, we employ a Gaussian mixture model to highlight the\nuncertainty of rare words pertaining to the \"identical shortcut\" problem,\noptimizing the vector space of the samples using the maximum expectation model.\nExperiments on three real-world datasets demonstrate the superiority of MLAD.",
      "tldr_zh": "该研究针对多系统日志异常检测的局限性（如需单独训练、缺乏认知推理能力以及“identical shortcut”问题），提出了一种统一的模型 MLAD，以实现跨系统语义关系推理。MLAD 采用 Sentence-BERT 捕获日志序列的相似性并转换为高维语义向量，修改 Attention layer 的公式来识别关键词重要性，并通过向量空间扩散和高斯混合模型优化样本分布，突出稀有词的不确定性。实验在三个真实世界数据集上证明了 MLAD 的优越性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07655v1",
      "published_date": "2024-01-15 12:51:13 UTC",
      "updated_date": "2024-01-15 12:51:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:57:50.469540"
    },
    {
      "arxiv_id": "2401.07612v1",
      "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications",
      "title_zh": "Signed-Prompt：一种防止针对LLM集成应用程序的提示注入攻击的新方法",
      "authors": [
        "Xuchen Suo"
      ],
      "abstract": "The critical challenge of prompt injection attacks in Large Language Models\n(LLMs) integrated applications, a growing concern in the Artificial\nIntelligence (AI) field. Such attacks, which manipulate LLMs through natural\nlanguage inputs, pose a significant threat to the security of these\napplications. Traditional defense strategies, including output and input\nfiltering, as well as delimiter use, have proven inadequate. This paper\nintroduces the 'Signed-Prompt' method as a novel solution. The study involves\nsigning sensitive instructions within command segments by authorized users,\nenabling the LLM to discern trusted instruction sources. The paper presents a\ncomprehensive analysis of prompt injection attack patterns, followed by a\ndetailed explanation of the Signed-Prompt concept, including its basic\narchitecture and implementation through both prompt engineering and fine-tuning\nof LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method,\nshowing substantial resistance to various types of prompt injection attacks,\nthus validating its potential as a robust defense strategy in AI security.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 集成应用中的 Prompt Injection Attacks 提出了一种新防御方法 Signed-Prompt，以解决传统策略（如输入输出过滤和分隔符使用）的不足。该方法通过授权用户对敏感指令进行签名，让 LLMs 能够区分可信指令来源，并结合 Prompt Engineering 和 Fine-Tuning 实现其架构。论文首先分析了攻击模式，然后通过实验验证了 Signed-Prompt 的有效性，在多种攻击类型中显示出显著的抵抗力，从而为 AI 安全提供了一个稳健的策略。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07612v1",
      "published_date": "2024-01-15 11:44:18 UTC",
      "updated_date": "2024-01-15 11:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:58:01.053217"
    },
    {
      "arxiv_id": "2401.07603v3",
      "title": "Multi-task real-robot data with gaze attention for dual-arm fine manipulation",
      "title_zh": "多任务真实机器人数据，结合注视注意力，用于双臂精细操作",
      "authors": [
        "Heecheol Kim",
        "Yoshiyuki Ohmura",
        "Yasuo Kuniyoshi"
      ],
      "abstract": "In the field of robotic manipulation, deep imitation learning is recognized\nas a promising approach for acquiring manipulation skills. Additionally,\nlearning from diverse robot datasets is considered a viable method to achieve\nversatility and adaptability. In such research, by learning various tasks,\nrobots achieved generality across multiple objects. However, such multi-task\nrobot datasets have mainly focused on single-arm tasks that are relatively\nimprecise, not addressing the fine-grained object manipulation that robots are\nexpected to perform in the real world. This paper introduces a dataset of\ndiverse object manipulations that includes dual-arm tasks and/or tasks\nrequiring fine manipulation. To this end, we have generated dataset with 224k\nepisodes (150 hours, 1,104 language instructions) which includes dual-arm fine\ntasks such as bowl-moving, pencil-case opening or banana-peeling, and this data\nis publicly available. Additionally, this dataset includes visual attention\nsignals as well as dual-action labels, a signal that separates actions into a\nrobust reaching trajectory and precise interaction with objects, and language\ninstructions to achieve robust and precise object manipulation. We applied the\ndataset to our Dual-Action and Attention (DAA), a model designed for\nfine-grained dual arm manipulation tasks and robust against covariate shifts.\nThe model was tested with over 7k total trials in real robot manipulation\ntasks, demonstrating its capability in fine manipulation.",
      "tldr_zh": "这篇论文引入了一个新的多任务真实机器人数据集，专注于双臂精细操作，以解决现有数据集偏重单臂任务和低精度的局限性。数据集包含224k个episode（150小时、1,104个语言指令），包括视觉注意力信号、双动作标签（将动作分为鲁棒的reaching trajectory和精确的物体交互）以及任务如碗移动、铅笔盒打开或香蕉剥皮。作者开发了Dual-Action and Attention (DAA)模型，利用该数据集进行训练，并在真实机器人上进行超过7k次试验，展示了模型对covariate shifts的鲁棒性以及在精细操作中的出色性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, The dataset is available at\n  https://sites.google.com/view/multi-task-fine",
      "pdf_url": "http://arxiv.org/pdf/2401.07603v3",
      "published_date": "2024-01-15 11:20:34 UTC",
      "updated_date": "2024-03-19 11:17:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:58:15.382749"
    },
    {
      "arxiv_id": "2401.07595v3",
      "title": "E3x: $\\mathrm{E}(3)$-Equivariant Deep Learning Made Easy",
      "title_zh": "翻译失败",
      "authors": [
        "Oliver T. Unke",
        "Hartmut Maennel"
      ],
      "abstract": "This work introduces E3x, a software package for building neural networks\nthat are equivariant with respect to the Euclidean group $\\mathrm{E}(3)$,\nconsisting of translations, rotations, and reflections of three-dimensional\nspace. Compared to ordinary neural networks, $\\mathrm{E}(3)$-equivariant models\npromise benefits whenever input and/or output data are quantities associated\nwith three-dimensional objects. This is because the numeric values of such\nquantities (e.g. positions) typically depend on the chosen coordinate system.\nUnder transformations of the reference frame, the values change predictably,\nbut the underlying rules can be difficult to learn for ordinary machine\nlearning models. With built-in $\\mathrm{E}(3)$-equivariance, neural networks\nare guaranteed to satisfy the relevant transformation rules exactly, resulting\nin superior data efficiency and accuracy. The code for E3x is available from\nhttps://github.com/google-research/e3x, detailed documentation and usage\nexamples can be found on https://e3x.readthedocs.io.",
      "tldr_zh": "这篇论文介绍了 E3x，一个用于构建 $\\mathrm{E}(3)$ 保持不变性神经网络的软件包，$\\mathrm{E}(3)$ 包括三维空间的平移、旋转和反射。\n相比普通神经网络，$\\mathrm{E}(3)$ 保持不变模型能精确满足坐标系变换规则，从而在处理与三维物体相关的输入/输出数据时显著提升数据效率和准确性。\nE3x 通过内置的 equivariance 机制，帮助模型更好地学习和预测这些变换依赖的数值变化。\n代码已开源，可在 https://github.com/google-research/e3x 获取，详细文档见 https://e3x.readthedocs.io。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07595v3",
      "published_date": "2024-01-15 11:04:47 UTC",
      "updated_date": "2024-11-11 14:53:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:58:27.430971"
    },
    {
      "arxiv_id": "2401.13693v1",
      "title": "Challenge design roadmap",
      "title_zh": "挑战设计路线图",
      "authors": [
        "Hugo Jair Escalante Balderas",
        "Isabelle Guyon",
        "Addison Howard",
        "Walter Reade",
        "Sebastien Treguer"
      ],
      "abstract": "Challenges can be seen as a type of game that motivates participants to solve\nserious tasks. As a result, competition organizers must develop effective game\nrules. However, these rules have multiple objectives beyond making the game\nenjoyable for participants. These objectives may include solving real-world\nproblems, advancing scientific or technical areas, making scientific\ndiscoveries, and educating the public. In many ways, creating a challenge is\nsimilar to launching a product. It requires the same level of excitement and\nrigorous testing, and the goal is to attract ''customers'' in the form of\nparticipants. The process begins with a solid plan, such as a competition\nproposal that will eventually be submitted to an international conference and\nsubjected to peer review. Although peer review does not guarantee quality, it\ndoes force organizers to consider the impact of their challenge, identify\npotential oversights, and generally improve its quality. This chapter provides\nguidelines for creating a strong plan for a challenge. The material draws on\nthe preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and\nTailor 3 , as well as the NeurIPS proposal template, which some of the authors\ncontributed to.",
      "tldr_zh": "这篇论文介绍了挑战（challenges）设计路线图，将其视为一种激励参与者解决实际问题的游戏形式，强调规则设计需兼顾娱乐性、问题解决、科学推进和公众教育。作者将创建挑战比作产品发布过程，强调从坚实的计划（如竞赛提案）开始，并通过同行评审来提升质量。论文基于Kaggle、ChaLearn、Tailor和NeurIPS等组织的经验，提供实用指南，帮助组织者制定高质量挑战以实现真实世界影响。",
      "categories": [
        "cs.OH",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.OH",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13693v1",
      "published_date": "2024-01-15 10:58:30 UTC",
      "updated_date": "2024-01-15 10:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:58:39.114768"
    },
    {
      "arxiv_id": "2401.07591v1",
      "title": "Multimodal Crowd Counting with Pix2Pix GANs",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Asif Khan",
        "Hamid Menouar",
        "Ridha Hamila"
      ],
      "abstract": "Most state-of-the-art crowd counting methods use color (RGB) images to learn\nthe density map of the crowd. However, these methods often struggle to achieve\nhigher accuracy in densely crowded scenes with poor illumination. Recently,\nsome studies have reported improvement in the accuracy of crowd counting models\nusing a combination of RGB and thermal images. Although multimodal data can\nlead to better predictions, multimodal data might not be always available\nbeforehand. In this paper, we propose the use of generative adversarial\nnetworks (GANs) to automatically generate thermal infrared (TIR) images from\ncolor (RGB) images and use both to train crowd counting models to achieve\nhigher accuracy. We use a Pix2Pix GAN network first to translate RGB images to\nTIR images. Our experiments on several state-of-the-art crowd counting models\nand benchmark crowd datasets report significant improvement in accuracy.",
      "tldr_zh": "这篇论文针对人群计数问题，指出现有基于 RGB 图像的方法在密集拥挤和光线不足场景下准确性不足。作者提出使用 Pix2Pix GANs 从 RGB 图像自动生成热成像（TIR）图像，从而解决多模态数据不可用的问题，并结合这些图像训练人群计数模型。实验结果显示，在多个基准数据集和先进模型上，该方法实现了显著的准确性提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted version of the paper in 19th International Conference on\n  Computer Vision Theory and Applications (VISAPP), Rome, Italy, 27-29 Feb,\n  2024,",
      "pdf_url": "http://arxiv.org/pdf/2401.07591v1",
      "published_date": "2024-01-15 10:54:35 UTC",
      "updated_date": "2024-01-15 10:54:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:58:50.328408"
    },
    {
      "arxiv_id": "2401.07586v1",
      "title": "Curriculum for Crowd Counting -- Is it Worthy?",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Asif Khan",
        "Hamid Menouar",
        "Ridha Hamila"
      ],
      "abstract": "Recent advances in deep learning techniques have achieved remarkable\nperformance in several computer vision problems. A notably intuitive technique\ncalled Curriculum Learning (CL) has been introduced recently for training deep\nlearning models. Surprisingly, curriculum learning achieves significantly\nimproved results in some tasks but marginal or no improvement in others. Hence,\nthere is still a debate about its adoption as a standard method to train\nsupervised learning models. In this work, we investigate the impact of\ncurriculum learning in crowd counting using the density estimation method. We\nperformed detailed investigations by conducting 112 experiments using six\ndifferent CL settings using eight different crowd models. Our experiments show\nthat curriculum learning improves the model learning performance and shortens\nthe convergence time.",
      "tldr_zh": "本研究探讨了Curriculum Learning (CL) 在人群计数(crowd counting)任务中的应用价值，通过密度估计方法(density estimation method) 进行112个实验，涉及六种CL设置和八种不同模型。结果显示，CL显著提升了模型的学习性能，并缩短了收敛时间。尽管CL在某些任务中效果不一，但此研究证明其在crowd counting领域值得采用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted version of the paper in 19th International Conference on\n  Computer Vision Theory and Applications (VISAPP), Rome, Italy, 27-19 February\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2401.07586v1",
      "published_date": "2024-01-15 10:46:01 UTC",
      "updated_date": "2024-01-15 10:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:59:00.677692"
    },
    {
      "arxiv_id": "2402.12381v1",
      "title": "Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Fei Ming",
        "Wenyin Gong",
        "Ling Wang",
        "Yaochu Jin"
      ],
      "abstract": "Solving constrained multi-objective optimization problems with evolutionary\nalgorithms has attracted considerable attention. Various constrained\nmulti-objective optimization evolutionary algorithms (CMOEAs) have been\ndeveloped with the use of different algorithmic strategies, evolutionary\noperators, and constraint-handling techniques. The performance of CMOEAs may be\nheavily dependent on the operators used, however, it is usually difficult to\nselect suitable operators for the problem at hand. Hence, improving operator\nselection is promising and necessary for CMOEAs. This work proposes an online\noperator selection framework assisted by Deep Reinforcement Learning. The\ndynamics of the population, including convergence, diversity, and feasibility,\nare regarded as the state; the candidate operators are considered as actions;\nand the improvement of the population state is treated as the reward. By using\na Q-Network to learn a policy to estimate the Q-values of all actions, the\nproposed approach can adaptively select an operator that maximizes the\nimprovement of the population according to the current state and thereby\nimprove the algorithmic performance. The framework is embedded into four\npopular CMOEAs and assessed on 42 benchmark problems. The experimental results\nreveal that the proposed Deep Reinforcement Learning-assisted operator\nselection significantly improves the performance of these CMOEAs and the\nresulting algorithm obtains better versatility compared to nine\nstate-of-the-art CMOEAs.",
      "tldr_zh": "本文提出了一种基于深度强化学习（Deep Reinforcement Learning）的在线操作符选择框架，用于提升约束多目标优化进化算法（CMOEAs）的性能。该框架将种群动态（如收敛、多样性和可行性）视为状态、候选操作符视为动作，并使用 Q-Network 学习策略来估计动作的 Q 值，从而自适应选择能最大化种群改进的操作符。实验结果显示，该框架嵌入四个流行 CMOEAs 后，在 42 个基准问题上显著提高了算法性能，并比九个最先进 CMOEAs 展现出更好的通用性。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12381v1",
      "published_date": "2024-01-15 09:51:19 UTC",
      "updated_date": "2024-01-15 09:51:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:59:14.246754"
    },
    {
      "arxiv_id": "2401.07543v1",
      "title": "Must: Maximizing Latent Capacity of Spatial Transcriptomics Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zelin Zang",
        "Liangyu Li",
        "Yongjie Xu",
        "Chenrui Duan",
        "Kai Wang",
        "Yang You",
        "Yi Sun",
        "Stan Z. Li"
      ],
      "abstract": "Spatial transcriptomics (ST) technologies have revolutionized the study of\ngene expression patterns in tissues by providing multimodality data in\ntranscriptomic, spatial, and morphological, offering opportunities for\nunderstanding tissue biology beyond transcriptomics. However, we identify the\nmodality bias phenomenon in ST data species, i.e., the inconsistent\ncontribution of different modalities to the labels leads to a tendency for the\nanalysis methods to retain the information of the dominant modality. How to\nmitigate the adverse effects of modality bias to satisfy various downstream\ntasks remains a fundamental challenge. This paper introduces Multiple-modality\nStructure Transformation, named MuST, a novel methodology to tackle the\nchallenge. MuST integrates the multi-modality information contained in the ST\ndata effectively into a uniform latent space to provide a foundation for all\nthe downstream tasks. It learns intrinsic local structures by topology\ndiscovery strategy and topology fusion loss function to solve the\ninconsistencies among different modalities. Thus, these topology-based and deep\nlearning techniques provide a solid foundation for a variety of analytical\ntasks while coordinating different modalities. The effectiveness of MuST is\nassessed by performance metrics and biological significance. The results show\nthat it outperforms existing state-of-the-art methods with clear advantages in\nthe precision of identifying and preserving structures of tissues and\nbiomarkers. MuST offers a versatile toolkit for the intricate analysis of\ncomplex biological systems.",
      "tldr_zh": "该论文针对 Spatial Transcriptomics (ST) 数据中的模态偏差问题，即不同模态（如转录组、空间和形态）对标签贡献不一致，导致分析方法偏向主导模态，提出了一种新方法 MuST（Multiple-modality Structure Transformation）。MuST 通过拓扑发现策略和拓扑融合损失函数，将多模态信息整合到一个统一潜在空间，学习内在局部结构以协调模态不一致性，从而支持各种下游任务。实验结果表明，MuST 在识别和保留组织结构以及生物标记物（biomarkers）的精确性上，优于现有最先进方法，并为复杂生物系统的分析提供了一个通用工具。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "30 pages and 6 figures, plus 27 pages and 14 figures in appendices",
      "pdf_url": "http://arxiv.org/pdf/2401.07543v1",
      "published_date": "2024-01-15 09:07:28 UTC",
      "updated_date": "2024-01-15 09:07:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:59:27.819912"
    },
    {
      "arxiv_id": "2401.07532v1",
      "title": "Multi-view MidiVAE: Fusing Track- and Bar-view Representations for Long Multi-track Symbolic Music Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiwei Lin",
        "Jun Chen",
        "Boshi Tang",
        "Binzhu Sha",
        "Jing Yang",
        "Yaolong Ju",
        "Fan Fan",
        "Shiyin Kang",
        "Zhiyong Wu",
        "Helen Meng"
      ],
      "abstract": "Variational Autoencoders (VAEs) constitute a crucial component of neural\nsymbolic music generation, among which some works have yielded outstanding\nresults and attracted considerable attention. Nevertheless, previous VAEs still\nencounter issues with overly long feature sequences and generated results lack\ncontextual coherence, thus the challenge of modeling long multi-track symbolic\nmusic still remains unaddressed. To this end, we propose Multi-view MidiVAE, as\none of the pioneers in VAE methods that effectively model and generate long\nmulti-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional\n(2-D) representation, OctupleMIDI, to capture relationships among notes while\nreducing the feature sequences length. Moreover, we focus on instrumental\ncharacteristics and harmony as well as global and local information about the\nmusical composition by employing a hybrid variational encoding-decoding\nstrategy to integrate both Track- and Bar-view MidiVAE features. Objective and\nsubjective experimental results on the CocoChorales dataset demonstrate that,\ncompared to the baseline, Multi-view MidiVAE exhibits significant improvements\nin terms of modeling long multi-track symbolic music.",
      "tldr_zh": "本研究提出Multi-view MidiVAE模型，这是首个有效处理长多轨符号音乐生成的VAE方法，旨在解决现有VAEs的特征序列过长和上下文连贯性不足问题。模型利用OctupleMIDI的二维表示来捕捉笔记关系并缩短序列长度，同时通过混合变分编码-解码策略融合Track-view和Bar-view特征，关注乐器特性、和声以及全局与局部信息。在CocoChorales数据集上的客观和主观实验表明，Multi-view MidiVAE相较于基线模型在建模长多轨符号音乐方面取得了显著改进。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by ICASSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.07532v1",
      "published_date": "2024-01-15 08:41:01 UTC",
      "updated_date": "2024-01-15 08:41:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:59:38.265510"
    },
    {
      "arxiv_id": "2401.07526v1",
      "title": "Editing Arbitrary Propositions in LLMs without Subject Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Itai Feigenbaum",
        "Devansh Arpit",
        "Huan Wang",
        "Shelby Heinecke",
        "Juan Carlos Niebles",
        "Weiran Yao",
        "Caiming Xiong",
        "Silvio Savarese"
      ],
      "abstract": "Large Language Model (LLM) editing modifies factual information in LLMs.\nLocate-and-Edit (L\\&E) methods accomplish this by finding where relevant\ninformation is stored within the neural network, and editing the weights at\nthat location. The goal of editing is to modify the response of an LLM to a\nproposition independently of its phrasing, while not modifying its response to\nother related propositions. Existing methods are limited to binary\npropositions, which represent straightforward binary relations between a\nsubject and an object. Furthermore, existing methods rely on semantic subject\nlabels, which may not be available or even be well-defined in practice. In this\npaper, we show that both of these issues can be effectively skirted with a\nsimple and fast localization method called Gradient Tracing (GT). This\nlocalization method allows editing arbitrary propositions instead of just\nbinary ones, and does so without the need for subject labels. As propositions\nalways have a truth value, our experiments prompt an LLM as a boolean\nclassifier, and edit its T/F response to propositions. Our method applies GT\nfor location tracing, and then edit the model at that location using a mild\nvariant of Rank-One Model Editing (ROME). On datasets of binary propositions\nderived from the CounterFact dataset, we show that our method -- without access\nto subject labels -- performs close to state-of-the-art L\\&E methods which has\naccess subject labels. We then introduce a new dataset, Factual Accuracy\nClassification Test (FACT), which includes non-binary propositions and for\nwhich subject labels are not generally applicable, and therefore is beyond the\nscope of existing L\\&E methods. Nevertheless, we show that with our method\nediting is possible on FACT.",
      "tldr_zh": "该论文提出了一种无需语义主体标签（semantic subject labels）的方法，用于编辑大型语言模型（LLMs）中的任意命题（arbitrary propositions），克服了现有Locate-and-Edit (L&E)方法的局限性，这些方法仅适用于二元命题（binary propositions）。新方法采用Gradient Tracing (GT)作为快速定位机制，结合Rank-One Model Editing (ROME)的轻微变体来修改模型权重，从而实现对命题响应的高精度编辑，而不影响相关命题。实验结果显示，在CounterFact数据集上，该方法在无主体标签的情况下，性能接近最先进L&E方法；此外，论文引入了新数据集Factual Accuracy Classification Test (FACT)，证明了该方法能有效处理非二元命题，实现更广泛的编辑应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07526v1",
      "published_date": "2024-01-15 08:08:24 UTC",
      "updated_date": "2024-01-15 08:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:59:51.072575"
    },
    {
      "arxiv_id": "2401.07525v2",
      "title": "TAROT: A Hierarchical Framework with Multitask Co-Pretraining on Semi-Structured Data towards Effective Person-Job Fit",
      "title_zh": "翻译失败",
      "authors": [
        "Yihan Cao",
        "Xu Chen",
        "Lun Du",
        "Hao Chen",
        "Qiang Fu",
        "Shi Han",
        "Yushu Du",
        "Yanbin Kang",
        "Guangming Lu",
        "Zi Li"
      ],
      "abstract": "Person-job fit is an essential part of online recruitment platforms in\nserving various downstream applications like Job Search and Candidate\nRecommendation. Recently, pretrained large language models have further\nenhanced the effectiveness by leveraging richer textual information in user\nprofiles and job descriptions apart from user behavior features and job\nmetadata. However, the general domain-oriented design struggles to capture the\nunique structural information within user profiles and job descriptions,\nleading to a loss of latent semantic correlations. We propose TAROT, a\nhierarchical multitask co-pretraining framework, to better utilize structural\nand semantic information for informative text embeddings. TAROT targets\nsemi-structured text in profiles and jobs, and it is co-pretained with\nmulti-grained pretraining tasks to constrain the acquired semantic information\nat each level. Experiments on a real-world LinkedIn dataset show significant\nperformance improvements, proving its effectiveness in person-job fit tasks.",
      "tldr_zh": "论文提出 TAROT，一种分层多任务联合预训练框架（hierarchical multitask co-pretraining framework），旨在通过利用用户资料和职位描述中的半结构化数据（semi-structured data）的结构和语义信息，提升 person-job fit 任务的效能。TAROT 通过多粒度预训练任务（multi-grained pretraining tasks）来约束每个层级的语义信息，确保文本嵌入更具信息性。与传统方法相比，该框架解决了现有预训练大型语言模型（pretrained large language models）在捕获潜在语义相关性方面的不足。在真实 LinkedIn 数据集上的实验显示，TAROT 显著提高了 Job Search 和 Candidate Recommendation 等下游应用的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICASSP 2024 camera ready. 5 pages, 1 figure, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.07525v2",
      "published_date": "2024-01-15 07:57:58 UTC",
      "updated_date": "2024-01-17 23:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:00:02.876804"
    },
    {
      "arxiv_id": "2401.07519v2",
      "title": "InstantID: Zero-shot Identity-Preserving Generation in Seconds",
      "title_zh": "翻译失败",
      "authors": [
        "Qixun Wang",
        "Xu Bai",
        "Haofan Wang",
        "Zekui Qin",
        "Anthony Chen",
        "Huaxia Li",
        "Xu Tang",
        "Yao Hu"
      ],
      "abstract": "There has been significant progress in personalized image synthesis with\nmethods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world\napplicability is hindered by high storage demands, lengthy fine-tuning\nprocesses, and the need for multiple reference images. Conversely, existing ID\nembedding-based methods, while requiring only a single forward inference, face\nchallenges: they either necessitate extensive fine-tuning across numerous model\nparameters, lack compatibility with community pre-trained models, or fail to\nmaintain high face fidelity. Addressing these limitations, we introduce\nInstantID, a powerful diffusion model-based solution. Our plug-and-play module\nadeptly handles image personalization in various styles using just a single\nfacial image, while ensuring high fidelity. To achieve this, we design a novel\nIdentityNet by imposing strong semantic and weak spatial conditions,\nintegrating facial and landmark images with textual prompts to steer the image\ngeneration. InstantID demonstrates exceptional performance and efficiency,\nproving highly beneficial in real-world applications where identity\npreservation is paramount. Moreover, our work seamlessly integrates with\npopular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving\nas an adaptable plugin. Our codes and pre-trained checkpoints will be available\nat https://github.com/InstantID/InstantID.",
      "tldr_zh": "这篇论文提出了InstantID，一种零-shot身份保留图像生成方法，旨在解决现有技术如Textual Inversion、DreamBooth和LoRA的存储需求高、微调时间长以及多参考图像依赖等问题，同时克服ID embedding-based方法的兼容性和面部保真度不足。InstantID基于扩散模型设计了一个即插即用的IdentityNet模块，通过强语义和弱空间条件整合单张面部图像、地标图像与文本提示，实现快速、高保真度的个性化图像合成。实验结果显示，InstantID与预训练模型如SD1.5和SDXL无缝兼容，在真实世界应用中表现出色，尤其适用于需要高效身份保留的场景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report, project page available at\n  https://instantid.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2401.07519v2",
      "published_date": "2024-01-15 07:50:18 UTC",
      "updated_date": "2024-02-02 16:15:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:00:15.949075"
    },
    {
      "arxiv_id": "2401.07518v3",
      "title": "Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends",
      "title_zh": "自然语言",
      "authors": [
        "Yunshi Lan",
        "Xinyuan Li",
        "Hanyue Du",
        "Xuesong Lu",
        "Ming Gao",
        "Weining Qian",
        "Aoying Zhou"
      ],
      "abstract": "Natural Language Processing (NLP) aims to analyze text or speech via\ntechniques in the computer science field. It serves the applications in domains\nof healthcare, commerce, education and so on. Particularly, NLP has been widely\napplied to the education domain and its applications have enormous potential to\nhelp teaching and learning. In this survey, we review recent advances in NLP\nwith the focus on solving problems relevant to the education domain. In detail,\nwe begin with introducing the related background and the real-world scenarios\nin education where NLP techniques could contribute. Then, we present a taxonomy\nof NLP in the education domain and highlight typical NLP applications including\nquestion answering, question construction, automated assessment, and error\ncorrection. Next, we illustrate the task definition, challenges, and\ncorresponding cutting-edge techniques based on the above taxonomy. In\nparticular, LLM-involved methods are included for discussion due to the wide\nusage of LLMs in diverse NLP applications. After that, we showcase some\noff-the-shelf demonstrations in this domain. At last, we conclude with six\npromising directions for future research, including more datasets in education\ndomain, controllable usage of LLMs, intervention of difficulty-level control,\ninterpretable educational NLP, methods with adaptive learning, and integrated\nsystems for education. We organize all relevant datasets and papers in the\nopen-available Github Link for better\nreview~\\url{https://github.com/LiXinyuan1015/NLP-for-Education}.",
      "tldr_zh": "这篇论文对 Natural Language Processing (NLP) 在教育领域的应用进行了系统回顾，包括一个 taxonomy 分类和典型应用，如 question answering、question construction、automated assessment 和 error correction。作者详细讨论了任务定义、面临的挑战（如领域知识缺口）和先进技术，特别是 Large Language Models (LLMs) 的广泛使用。论文还展示了实际演示，并提出了六个未来研究方向，包括开发更多教育领域数据集、可控 LLMs 使用、难度水平控制以及构建可解释的集成系统。总之，这为 NLP 在教育中的发展提供了全面框架和前景展望。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07518v3",
      "published_date": "2024-01-15 07:48:42 UTC",
      "updated_date": "2024-03-15 05:10:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:00:26.571929"
    },
    {
      "arxiv_id": "2401.07510v3",
      "title": "Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering",
      "title_zh": "为生物学和医学开发 ChatGPT：生物医学问答的完整",
      "authors": [
        "Qing Li",
        "Lei Li",
        "Yu Li"
      ],
      "abstract": "ChatGPT explores a strategic blueprint of question answering (QA) in\ndelivering medical diagnosis, treatment recommendations, and other healthcare\nsupport. This is achieved through the increasing incorporation of medical\ndomain data via natural language processing (NLP) and multimodal paradigms. By\ntransitioning the distribution of text, images, videos, and other modalities\nfrom the general domain to the medical domain, these techniques have expedited\nthe progress of medical domain question answering (MDQA). They bridge the gap\nbetween human natural language and sophisticated medical domain knowledge or\nexpert manual annotations, handling large-scale, diverse, unbalanced, or even\nunlabeled data analysis scenarios in medical contexts. Central to our focus is\nthe utilizing of language models and multimodal paradigms for medical question\nanswering, aiming to guide the research community in selecting appropriate\nmechanisms for their specific medical research requirements. Specialized tasks\nsuch as unimodal-related question answering, reading comprehension, reasoning,\ndiagnosis, relation extraction, probability modeling, and others, as well as\nmultimodal-related tasks like vision question answering, image caption,\ncross-modal retrieval, report summarization, and generation, are discussed in\ndetail. Each section delves into the intricate specifics of the respective\nmethod under consideration. This paper highlights the structures and\nadvancements of medical domain explorations against general domain methods,\nemphasizing their applications across different tasks and datasets. It also\noutlines current challenges and opportunities for future medical domain\nresearch, paving the way for continued innovation and application in this\nrapidly evolving field.",
      "tldr_zh": "这篇论文对ChatGPT在生物医学领域的应用进行全面回顾，聚焦于生物医学问题回答（MDQA）的策略，包括通过自然语言处理（NLP）和多模态范式整合医疗领域数据，以支持医疗诊断、治疗推荐和其他医疗服务。论文详细探讨了单模态任务（如问题回答、阅读理解、推理和诊断）和多模态任务（如视觉问题回答、图像描述和跨模态检索），并比较了医疗领域方法与一般领域方法的进展及应用。最终，它强调了当前面临的挑战，如处理大规模不平衡数据，以及未来研究机会，以推动该领域的创新和发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CL, 92-02",
        "I.2.1"
      ],
      "primary_category": "cs.CL",
      "comment": "50 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.07510v3",
      "published_date": "2024-01-15 07:21:16 UTC",
      "updated_date": "2024-01-20 22:08:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:00:38.078708"
    },
    {
      "arxiv_id": "2401.07489v1",
      "title": "The Principle of Minimum Pressure Gradient: An Alternative Basis for Physics-Informed Learning of Incompressible Fluid Mechanics",
      "title_zh": "翻译失败",
      "authors": [
        "Hussam Alhussein",
        "Mohammed Daqaq"
      ],
      "abstract": "Recent advances in the application of physics-informed learning into the\nfield of fluid mechanics have been predominantly grounded in the Newtonian\nframework, primarly leveraging Navier-Stokes Equation or one of its various\nderivative to train a neural network. Here, we propose an alternative approach\nbased on variational methods. The proposed approach uses the principle of\nminimum pressure gradient combined with the continuity constraint to train a\nneural network and predict the flow field in incompressible fluids. We describe\nthe underlying principles of the proposed approach, then use a demonstrative\nexample to illustrate its implementation and show that it reduces the\ncomputational time per training epoch when compared to the conventional\napproach.",
      "tldr_zh": "本文提出了一种基于变分方法的替代框架，用于物理信息学习（physics-informed learning）在不可压缩流体力学中的应用，以 the principle of minimum pressure gradient 和 continuity constraint 作为基础，训练神经网络预测流场。该方法避免了传统依赖 Navier-Stokes Equation 的框架，转而强调最小化压力梯度以提高效率。通过一个演示例子，研究表明该方法显著减少了每个训练周期的计算时间，为流体动力学建模提供了更高效的选项。",
      "categories": [
        "physics.flu-dyn",
        "cs.AI"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07489v1",
      "published_date": "2024-01-15 06:12:22 UTC",
      "updated_date": "2024-01-15 06:12:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:00:50.087803"
    },
    {
      "arxiv_id": "2401.09479v2",
      "title": "Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Vishwakarma",
        "Amin Rezaei"
      ],
      "abstract": "The risk of hardware Trojans being inserted at various stages of chip\nproduction has increased in a zero-trust fabless era. To counter this, various\nmachine learning solutions have been developed for the detection of hardware\nTrojans. While most of the focus has been on either a statistical or deep\nlearning approach, the limited number of Trojan-infected benchmarks affects the\ndetection accuracy and restricts the possibility of detecting zero-day Trojans.\nTo close the gap, we first employ generative adversarial networks to amplify\nour data in two alternative representation modalities, a graph and a tabular,\nensuring that the dataset is distributed in a representative manner. Further,\nwe propose a multimodal deep learning approach to detect hardware Trojans and\nevaluate the results from both early fusion and late fusion strategies. We also\nestimate the uncertainty quantification metrics of each prediction for\nrisk-aware decision-making. The outcomes not only confirms the efficacy of our\nproposed hardware Trojan detection method but also opens a new door for future\nstudies employing multimodality and uncertainty quantification to address other\nhardware security challenges.",
      "tldr_zh": "该研究针对硬件 Trojan（硬件木马）检测面临的挑战，如数据不足和零日 Trojan 检测困难，提出了一种基于多模态深度学习（Multimodal Deep Learning）的解决方案。首先，使用生成对抗网络（Generative Adversarial Networks, GANs）扩充图和表格数据，确保数据集分布更具代表性，并采用早融合（early fusion）和晚融合（late fusion）策略进行检测。其次，通过不确定性量化指标（Uncertainty Quantification Metrics）评估预测风险，提升决策的可靠性。实验结果验证了方法的有效性，为硬件安全领域开辟了利用多模态和不确定性量化的新研究方向。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "2024 Design, Automation and Test in Europe Conference | The European\n  Event for Electronic System Design & Test (accepted)",
      "pdf_url": "http://arxiv.org/pdf/2401.09479v2",
      "published_date": "2024-01-15 05:45:51 UTC",
      "updated_date": "2024-01-23 07:04:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:01:02.472918"
    },
    {
      "arxiv_id": "2401.07470v1",
      "title": "Utilizing deep learning models for the identification of enhancers and super-enhancers based on genomic and epigenomic features",
      "title_zh": "利用深度学习模型基于基因组和表观基因组特征识别增强子和超级增强子",
      "authors": [
        "Zahra Ahani",
        "Moein Shahiki Tash",
        "Yoel Ledo Mezquita",
        "Jason Angel"
      ],
      "abstract": "This paper provides an extensive examination of a sizable dataset of English\ntweets focusing on nine widely recognized cryptocurrencies, specifically\nCardano, Binance, Bitcoin, Dogecoin, Ethereum, Fantom, Matic, Shiba, and\nRipple. Our primary objective was to conduct a psycholinguistic and emotion\nanalysis of social media content associated with these cryptocurrencies. To\nenable investigators to make more informed decisions. The study involved\ncomparing linguistic characteristics across the diverse digital coins, shedding\nlight on the distinctive linguistic patterns that emerge within each coin's\ncommunity. To achieve this, we utilized advanced text analysis techniques.\nAdditionally, our work unveiled an intriguing Understanding of the interplay\nbetween these digital assets within the cryptocurrency community. By examining\nwhich coin pairs are mentioned together most frequently in the dataset, we\nestablished correlations between different cryptocurrencies. To ensure the\nreliability of our findings, we initially gathered a total of 832,559 tweets\nfrom Twitter. These tweets underwent a rigorous preprocessing stage, resulting\nin a refined dataset of 115,899 tweets that were used for our analysis.\nOverall, our research offers valuable Perception into the linguistic nuances of\nvarious digital coins' online communities and provides a deeper understanding\nof their interactions in the cryptocurrency space.",
      "tldr_zh": "这篇论文通过分析九种流行加密货币（包括Cardano、Binance、Bitcoin、Dogecoin、Ethereum、Fantom、Matic、Shiba和Ripple）的Twitter推文，进行心理语言和情感分析，以帮助决策者更好地理解社交媒体内容。研究者收集了83万条推文，经过严格预处理后精炼为11.5万条，并利用高级文本分析技术比较不同加密货币的语言特征，揭示了各社区的独特语言模式。论文进一步探讨了加密货币之间的互动，如频繁共现的币对相关性，提供对加密货币在线社区语言 nuances 和互动的宝贵洞察。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 7 figures, 6 Tables",
      "pdf_url": "http://arxiv.org/pdf/2401.07470v1",
      "published_date": "2024-01-15 04:58:50 UTC",
      "updated_date": "2024-01-15 04:58:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:01:20.419177"
    },
    {
      "arxiv_id": "2401.07468v2",
      "title": "CarSpeedNet: A Deep Neural Network-based Car Speed Estimation from Smartphone Accelerometer",
      "title_zh": "翻译失败",
      "authors": [
        "Barak Or"
      ],
      "abstract": "We introduce the CarSpeedNet, a deep learning model designed to estimate car\nspeed using three-axis accelerometer data from smartphones. Using 13 hours of\ndata collected from a smartphone in cars across various roads, CarSpeedNet\naccurately models the relationship between smartphone acceleration and car\nspeed. Ground truth speed data was collected at 1 [Hz] from GPS receivers. The\nmodel provides high-frequency speed estimation by incorporating historical data\nand achieves a precision of less than 0.72 [m/s] during extended driving tests,\nrelying solely on smartphone accelerometer data without any connection to the\ncar.",
      "tldr_zh": "该研究提出了一种名为 CarSpeedNet 的 Deep Neural Network 模型，用于从智能手机的三轴 Accelerometer 数据中估计汽车速度。模型基于 13 小时从各种道路收集的数据训练，并使用 GPS 提供的 1 Hz 地面真实速度作为参考，通过整合历史数据实现高频速度估计。在扩展驾驶测试中，CarSpeedNet 实现了小于 0.72 m/s 的精度，且仅依赖智能手机数据，而不需连接汽车系统。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, under review",
      "pdf_url": "http://arxiv.org/pdf/2401.07468v2",
      "published_date": "2024-01-15 04:51:34 UTC",
      "updated_date": "2024-10-25 07:32:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:01:26.880370"
    },
    {
      "arxiv_id": "2401.07466v1",
      "title": "Your Instructions Are Not Always Helpful: Assessing the Efficacy of Instruction Fine-tuning for Software Vulnerability Detection",
      "title_zh": "您的指令并不总是 hữu助的：评估指令微调在软件",
      "authors": [
        "Imam Nur Bani Yusuf",
        "Lingxiao Jiang"
      ],
      "abstract": "Software, while beneficial, poses potential cybersecurity risks due to\ninherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep\nlearning has shown promise as an effective tool for this task due to its\nability to perform well without extensive feature engineering. However, a\nchallenge in deploying deep learning for vulnerability detection is the limited\navailability of training data. Recent research highlights the deep learning\nefficacy in diverse tasks. This success is attributed to instruction\nfine-tuning, a technique that remains under-explored in the context of\nvulnerability detection. This paper investigates the capability of models,\nspecifically a recent language model, to generalize beyond the programming\nlanguages used in their training data. It also examines the role of natural\nlanguage instructions in enhancing this generalization. Our study evaluates the\nmodel performance on a real-world dataset to predict vulnerable code. We\npresent key insights and lessons learned, contributing to understanding the\ndeep learning application in software vulnerability detection.",
      "tldr_zh": "本研究评估了instruction fine-tuning在软件漏洞检测中的有效性，针对训练数据有限和模型泛化能力不足的问题，使用特定语言模型在真实数据集上测试其性能，并考察自然语言指令的作用。结果显示，instruction fine-tuning并不总是提升泛化能力，尤其在不同编程语言的场景中。研究提供了关键洞见和经验教训，深化了对deep learning在软件vulnerability detection中的应用的理解。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07466v1",
      "published_date": "2024-01-15 04:45:27 UTC",
      "updated_date": "2024-01-15 04:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:01:38.393290"
    },
    {
      "arxiv_id": "2401.07456v2",
      "title": "Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation",
      "title_zh": "只需发送所需内容：在联邦多语言机器翻译中学习高效通信",
      "authors": [
        "Yun-Wei Chu",
        "Dong-Jun Han",
        "Christopher G. Brinton"
      ],
      "abstract": "Federated learning (FL) is a promising distributed machine learning paradigm\nthat enables multiple clients to collaboratively train a global model. In this\npaper, we focus on a practical federated multilingual learning setup where\nclients with their own language-specific data aim to collaboratively construct\na high-quality neural machine translation (NMT) model. However, communication\nconstraints in practical network systems present challenges for exchanging\nlarge-scale NMT engines between FL parties. We propose a meta-learning-based\nadaptive parameter selection methodology, MetaSend, that improves the\ncommunication efficiency of model transmissions from clients during FL-based\nmultilingual NMT training. Our approach learns a dynamic threshold for\nfiltering parameters prior to transmission without compromising the NMT model\nquality, based on the tensor deviations of clients between different FL rounds.\nThrough experiments on two NMT datasets with different language distributions,\nwe demonstrate that MetaSend obtains substantial improvements over baselines in\ntranslation quality in the presence of a limited communication budget.",
      "tldr_zh": "本论文探讨了在Federated Learning (FL)框架下，如何高效训练多语言神经机器翻译 (NMT) 模型，以应对通信约束带来的挑战。研究提出了一种基于meta-learning的动态参数选择方法MetaSend，该方法通过分析客户端之间不同FL轮次的tensor deviations，学习动态阈值来过滤并仅传输必要的参数，从而提高通信效率而不降低模型质量。在两个NMT数据集上的实验表明，MetaSend在有限通信预算下显著提升了翻译质量，优于基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07456v2",
      "published_date": "2024-01-15 04:04:26 UTC",
      "updated_date": "2025-04-18 15:41:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:01:50.207879"
    },
    {
      "arxiv_id": "2401.07453v4",
      "title": "Model Editing at Scale leads to Gradual and Catastrophic Forgetting",
      "title_zh": "大规模模型编辑导致渐进式和灾难性遗忘",
      "authors": [
        "Akshat Gupta",
        "Anurag Rao",
        "Gopala Anumanchipalli"
      ],
      "abstract": "Editing knowledge in large language models is an attractive capability to\nhave which allows us to correct incorrectly learnt facts during pre-training,\nas well as update the model with an ever-growing list of new facts. While\nexisting model editing techniques have shown promise, they are usually\nevaluated using metrics for reliability, specificity and generalization over\none or few edits. We argue that for model editing to have practical utility, we\nmust be able to make multiple edits to the same model. With this in mind, we\nevaluate the current model editing methods at scale, focusing on two state of\nthe art methods: ROME and MEMIT. We find that as the model is edited\nsequentially with multiple facts, it continually forgets previously edited\nfacts and the ability to perform downstream tasks. This forgetting happens in\ntwo phases -- an initial gradual but progressive forgetting phase followed by\nabrupt or catastrophic forgetting phase. Both gradual and catastrophic\nforgetting limit the usefulness of model editing methods at scale -- the former\nmaking model editing less effective as multiple edits are made to the model\nwhile the latter caps the scalability of such model editing methods. Our\nanalysis also highlights other key limitations of ROME and MEMIT at scale. With\nour work, we push for the development and evaluation of model editing methods\nkeeping scalability in mind.",
      "tldr_zh": "本研究探讨了在大型语言模型中进行规模化知识编辑时所导致的渐进式和灾难性 forgetting 问题，强调现有方法如 ROME 和 MEMIT 在多个连续编辑下表现不佳。作者通过评估这些方法发现，模型会经历一个初始渐进式 forgetting 阶段，随后进入突发性 forgetting 阶段，导致先前编辑的事实被遗忘并影响下游任务性能。这些限制降低了模型编辑的实用性，并呼吁未来开发更注重可扩展性的编辑技术。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2401.07453v4",
      "published_date": "2024-01-15 03:57:15 UTC",
      "updated_date": "2024-06-10 17:50:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:02:03.511087"
    },
    {
      "arxiv_id": "2401.07450v4",
      "title": "HieraFashDiff: Hierarchical Fashion Design with Multi-stage Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhifeng Xie",
        "Hao Li",
        "Huiming Ding",
        "Mengtian Li",
        "Xinhan Di",
        "Ying Cao"
      ],
      "abstract": "Fashion design is a challenging and complex process.Recent works on fashion\ngeneration and editing are all agnostic of the actual fashion design process,\nwhich limits their usage in practice.In this paper, we propose a novel\nhierarchical diffusion-based framework tailored for fashion design, coined as\nHieraFashDiff. Our model is designed to mimic the practical fashion design\nworkflow, by unraveling the denosing process into two successive stages: 1) an\nideation stage that generates design proposals given high-level concepts and 2)\nan iteration stage that continuously refines the proposals using low-level\nattributes. Our model supports fashion design generation and fine-grained local\nediting in a single framework. To train our model, we contribute a new dataset\nof full-body fashion images annotated with hierarchical text descriptions.\nExtensive evaluations show that, as compared to prior approaches, our method\ncan generate fashion designs and edited results with higher fidelity and better\nprompt adherence, showing its promising potential to augment the practical\nfashion design workflow. Code and Dataset are available at\nhttps://github.com/haoli-zbdbc/hierafashdiff.",
      "tldr_zh": "该论文提出了一种新型层次化扩散模型框架HieraFashDiff，用于模拟实际时尚设计流程，将去噪过程分为两个阶段：构想阶段（基于高层概念生成设计提案）和迭代阶段（使用低层属性持续优化提案）。该框架支持时尚设计生成和细粒度局部编辑，并在训练中引入了一个新数据集，包含带有层次化文本描述的全身时尚图像。实验结果显示，与现有方法相比，HieraFashDiff在保真度和提示遵守性方面表现出色，具有提升实际时尚设计工作流的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07450v4",
      "published_date": "2024-01-15 03:38:57 UTC",
      "updated_date": "2024-12-12 10:36:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:02:15.511729"
    },
    {
      "arxiv_id": "2401.07448v2",
      "title": "Formal Logic Enabled Personalized Federated Learning Through Property Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyan An",
        "Taylor T. Johnson",
        "Meiyi Ma"
      ],
      "abstract": "Recent advancements in federated learning (FL) have greatly facilitated the\ndevelopment of decentralized collaborative applications, particularly in the\ndomain of Artificial Intelligence of Things (AIoT). However, a critical aspect\nmissing from the current research landscape is the ability to enable\ndata-driven client models with symbolic reasoning capabilities. Specifically,\nthe inherent heterogeneity of participating client devices poses a significant\nchallenge, as each client exhibits unique logic reasoning properties. Failing\nto consider these device-specific specifications can result in critical\nproperties being missed in the client predictions, leading to suboptimal\nperformance. In this work, we propose a new training paradigm that leverages\ntemporal logic reasoning to address this issue. Our approach involves enhancing\nthe training process by incorporating mechanically generated logic expressions\nfor each FL client. Additionally, we introduce the concept of aggregation\nclusters and develop a partitioning algorithm to effectively group clients\nbased on the alignment of their temporal reasoning properties. We evaluate the\nproposed method on two tasks: a real-world traffic volume prediction task\nconsisting of sensory data from fifteen states and a smart city multi-task\nprediction utilizing synthetic data. The evaluation results exhibit clear\nimprovements, with performance accuracy improved by up to 54% across all\nsequential prediction models.",
      "tldr_zh": "本论文提出了一种通过属性推断（Property Inference）结合形式逻辑的个性化联邦学习（Federated Learning）方法，旨在解决AIoT领域中客户端异质性导致的符号推理能力缺失问题。该方法通过为每个FL客户端生成机械化的时间逻辑推理（Temporal Logic Reasoning）表达式，并引入聚合集群和分区算法来根据客户端的推理属性进行有效分组。实验结果显示，在真实交通量预测任务和智能城市多任务预测上，该方法使顺序预测模型的性能准确率提高了多达54%，显著提升了去中心化协作应用的可靠性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07448v2",
      "published_date": "2024-01-15 03:25:37 UTC",
      "updated_date": "2024-01-24 01:48:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:02:27.808728"
    },
    {
      "arxiv_id": "2401.07447v1",
      "title": "Taec: a Manually annotated text dataset for trait and phenotype extraction and entity linking in wheat breeding literature",
      "title_zh": "翻译失败",
      "authors": [
        "Claire Nédellec",
        "Clara Sauvion",
        "Robert Bossy",
        "Mariya Borovikova",
        "Louise Deléger"
      ],
      "abstract": "Wheat varieties show a large diversity of traits and phenotypes. Linking them\nto genetic variability is essential for shorter and more efficient wheat\nbreeding programs. Newly desirable wheat variety traits include disease\nresistance to reduce pesticide use, adaptation to climate change, resistance to\nheat and drought stresses, or low gluten content of grains. Wheat breeding\nexperiments are documented by a large body of scientific literature and\nobservational data obtained in-field and under controlled conditions. The\ncross-referencing of complementary information from the literature and\nobservational data is essential to the study of the genotype-phenotype\nrelationship and to the improvement of wheat selection. The scientific\nliterature on genetic marker-assisted selection describes much information\nabout the genotype-phenotype relationship. However, the variety of expressions\nused to refer to traits and phenotype values in scientific articles is a hinder\nto finding information and cross-referencing it. When trained adequately by\nannotated examples, recent text mining methods perform highly in named entity\nrecognition and linking in the scientific domain. While several corpora contain\nannotations of human and animal phenotypes, currently, no corpus is available\nfor training and evaluating named entity recognition and entity-linking methods\nin plant phenotype literature. The Triticum aestivum trait Corpus is a new gold\nstandard for traits and phenotypes of wheat. It consists of 540 PubMed\nreferences fully annotated for trait, phenotype, and species named entities\nusing the Wheat Trait and Phenotype Ontology and the species taxonomy of the\nNational Center for Biotechnology Information. A study of the performance of\ntools trained on the Triticum aestivum trait Corpus shows that the corpus is\nsuitable for the training and evaluation of named entity recognition and\nlinking.",
      "tldr_zh": "本论文介绍了Taec数据集，这是一个手动注释的文本语料库，旨在从小麦育种文献中提取性状（trait）和表型（phenotype）实体，并进行实体链接（entity linking），以解决科学文章中表达多样性导致的信息检索和交叉引用挑战。Taec包含540个PubMed参考文献，手动标注了性状、表型和物种命名实体，使用Wheat Trait and Phenotype Ontology以及NCBI的物种分类系统。实验结果显示，在该语料库上训练的工具表现出色，适合用于named entity recognition和entity linking的训练与评估，从而支持小麦育种程序中基因型-表型关系的深入研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.07447v1",
      "published_date": "2024-01-15 03:23:24 UTC",
      "updated_date": "2024-01-15 03:23:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:02:39.619789"
    },
    {
      "arxiv_id": "2401.07445v1",
      "title": "GACE: Learning Graph-Based Cross-Page Ads Embedding For Click-Through Rate Prediction",
      "title_zh": "GACE：学习基于图的跨页面广告嵌入用于点击率预测",
      "authors": [
        "Haowen Wang",
        "Yuliang Du",
        "Congyun Jin",
        "Yujiao Li",
        "Yingbo Wang",
        "Tao Sun",
        "Piqi Qin",
        "Cong Fan"
      ],
      "abstract": "Predicting click-through rate (CTR) is the core task of many ads online\nrecommendation systems, which helps improve user experience and increase\nplatform revenue. In this type of recommendation system, we often encounter two\nmain problems: the joint usage of multi-page historical advertising data and\nthe cold start of new ads. In this paper, we proposed GACE, a graph-based\ncross-page ads embedding generation method. It can warm up and generate the\nrepresentation embedding of cold-start and existing ads across various pages.\nSpecifically, we carefully build linkages and a weighted undirected graph model\nconsidering semantic and page-type attributes to guide the direction of feature\nfusion and generation. We designed a variational auto-encoding task as\npre-training module and generated embedding representations for new and old ads\nbased on this task. The results evaluated in the public dataset AliEC from\nRecBole and the real-world industry dataset from Alipay show that our GACE\nmethod is significantly superior to the SOTA method. In the online A/B test,\nthe click-through rate on three real-world pages from Alipay has increased by\n3.6%, 2.13%, and 3.02%, respectively. Especially in the cold-start task, the\nCTR increased by 9.96%, 7.51%, and 8.97%, respectively.",
      "tldr_zh": "这篇论文提出 GACE 方法，用于学习基于 graph-based 的跨页面广告嵌入，以提升点击通过率 (CTR) 预测，解决多页面历史数据联合使用和新广告冷启动问题。GACE 通过构建考虑语义和页面类型属性的加权无向图模型，并设计变分自编码 (variational auto-encoding) 作为预训练模块，生成新旧广告的嵌入表示。实验结果显示，在公开数据集 AliEC 和真实世界数据集上，GACE 显著优于 SOTA 方法；在在线 A/B 测试中，CTR 分别提高了 3.6%、2.13% 和 3.02%，而在冷启动任务中提升更明显，达 9.96%、7.51% 和 8.97%。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.IR",
      "comment": "15 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.07445v1",
      "published_date": "2024-01-15 03:12:21 UTC",
      "updated_date": "2024-01-15 03:12:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:02:52.727746"
    },
    {
      "arxiv_id": "2401.07426v1",
      "title": "Generalized Planning for the Abstraction and Reasoning Corpus",
      "title_zh": "针对 Abstraction and Reasoning Corpus 的泛化规划",
      "authors": [
        "Chao Lei",
        "Nir Lipovetzky",
        "Krista A. Ehinger"
      ],
      "abstract": "The Abstraction and Reasoning Corpus (ARC) is a general artificial\nintelligence benchmark that poses difficulties for pure machine learning\nmethods due to its requirement for fluid intelligence with a focus on reasoning\nand abstraction. In this work, we introduce an ARC solver, Generalized Planning\nfor Abstract Reasoning (GPAR). It casts an ARC problem as a generalized\nplanning (GP) problem, where a solution is formalized as a planning program\nwith pointers. We express each ARC problem using the standard Planning Domain\nDefinition Language (PDDL) coupled with external functions representing\nobject-centric abstractions. We show how to scale up GP solvers via domain\nknowledge specific to ARC in the form of restrictions over the actions model,\npredicates, arguments and valid structure of planning programs. Our experiments\ndemonstrate that GPAR outperforms the state-of-the-art solvers on the\nobject-centric tasks of the ARC, showing the effectiveness of GP and the\nexpressiveness of PDDL to model ARC problems. The challenges provided by the\nARC benchmark motivate research to advance existing GP solvers and understand\nnew relations with other planning computational models. Code is available at\ngithub.com/you68681/GPAR.",
      "tldr_zh": "该研究针对Abstraction and Reasoning Corpus (ARC)基准提出了一种求解器Generalized Planning for Abstract Reasoning (GPAR)，将ARC问题转化为generalized planning (GP)问题，并使用Planning Domain Definition Language (PDDL)结合外部函数来表示object-centric abstractions。GPAR通过对actions、predicates、arguments和planning programs的特定限制来扩展GP求解器的规模和效率。实验结果显示，GPAR在ARC的object-centric任务上超过了现有最先进求解器，证明了GP的有效性和PDDL的表达能力，同时推动了GP求解器的进一步发展和与其他规划模型的关联研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at AAAI 2024 (extended version)",
      "pdf_url": "http://arxiv.org/pdf/2401.07426v1",
      "published_date": "2024-01-15 02:25:00 UTC",
      "updated_date": "2024-01-15 02:25:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:03:03.547434"
    },
    {
      "arxiv_id": "2401.07395v1",
      "title": "Harnessing the Power of Beta Scoring in Deep Active Learning for Multi-Label Text Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Tan",
        "Ngoc Dang Nguyen",
        "Lan Du",
        "Wray Buntine"
      ],
      "abstract": "Within the scope of natural language processing, the domain of multi-label\ntext classification is uniquely challenging due to its expansive and uneven\nlabel distribution. The complexity deepens due to the demand for an extensive\nset of annotated data for training an advanced deep learning model, especially\nin specialized fields where the labeling task can be labor-intensive and often\nrequires domain-specific knowledge. Addressing these challenges, our study\nintroduces a novel deep active learning strategy, capitalizing on the Beta\nfamily of proper scoring rules within the Expected Loss Reduction framework. It\ncomputes the expected increase in scores using the Beta Scoring Rules, which\nare then transformed into sample vector representations. These vector\nrepresentations guide the diverse selection of informative samples, directly\nlinking this process to the model's expected proper score. Comprehensive\nevaluations across both synthetic and real datasets reveal our method's\ncapability to often outperform established acquisition techniques in\nmulti-label text classification, presenting encouraging outcomes across various\narchitectural and dataset scenarios.",
      "tldr_zh": "本研究针对多标签文本分类(Multi-Label Text Classification)的挑战，如标签分布不均匀和数据标注的劳动密集型，提出了一种新型深度主动学习(Deep Active Learning)策略。策略利用 Beta Scoring Rules 在 Expected Loss Reduction 框架中计算预期分数增加，并将其转化为样本向量表示，以指导选择多样且信息丰富的样本。该方法在合成和真实数据集上的全面评估显示，其性能通常优于现有获取技术，并在各种模型架构和数据集场景中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.07395v1",
      "published_date": "2024-01-15 00:06:24 UTC",
      "updated_date": "2024-01-15 00:06:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T22:03:16.142606"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 61,
  "processed_papers_count": 61,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-16T22:03:45.435057"
}
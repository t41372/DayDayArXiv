{
  "date": "2025-02-15",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-02-15 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型优化、网络安全、医疗诊断和机器人学习等领域，强调大型语言模型（LLM）的创新应用、深度学习在实际场景的鲁棒性，以及跨学科融合的潜力。其中，D-CIPHER 的多代理安全框架和 HybriDNA 的 DNA 语言模型等文章最为令人印象深刻，展示了知名学者如 Pulkit Agrawal 和 Tri Dao 的前沿工作。\n\n### AI 和 LLM 创新\n- **D-CIPHER: Dynamic Collaborative Intelligent Multi-Agent System with Planner and Heterogeneous Executors for Offensive Security**（中文：D-CIPHER：动态协作智能多代理系统，用于进攻性安全）  \n  这篇论文提出 D-CIPHER 框架，使用 LLM 构建多代理系统来解决网络安全中的 CTF 挑战，通过 Planner-Executor 设计实现高效任务分配，并在多个基准上达到 SOTA 性能（NYU CTF Bench 22.0%），显著提升了自动化渗透测试的进攻能力。\n  \n- **Probing Semantic Routing in Large Mixture-of-Expert Models**（中文：探索大型混合专家模型中的语义路由）  \n  作者 Matthew Lyle Olson 等研究了大型 MoE 模型的专家路由机制，通过实验证明输入语义影响路由决策，提供统计显著证据，帮助理解 MoE 模型的功能分化。\n\n- **HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model**（中文：HybriDNA：混合 Transformer-Mamba2 长程 DNA 语言模型）  \n  这篇由 Tri Dao 等学者主导的论文引入混合架构的 DNA 语言模型，能处理长达 131kb 的序列，在生成和理解任务上达到 SOTA（如 BEND 基准），展示了在生物学应用中的潜力。\n\n- **ControllableGPT: A Ground-Up Designed Controllable GPT for Molecule Optimization**（中文：ControllableGPT：从基础设计的可控 GPT 用于分子优化）  \n  论文提出 ControllableGPT 框架，结合 MLM、CLM 和 seq2seq 的优势，实现可控分子生成，在药物优化任务中超越基线，提升了生成效率。\n\n### 医疗诊断与图像处理\n- **Automatic Quality Assessment of First Trimester Crown-Rump-Length Ultrasound Images**（中文：第一孕期顶骨-尾骨长度超声图像的自动质量评估）  \n  该工作使用 CNN 和 ViT 混合方法评估胎儿超声图像质量，通过临床指导的映射技术提高 GA 估计准确性，优于传统 UNet。\n\n- **Breaking Down the Hierarchy: A New Approach to Leukemia Classification**（中文：打破层次：白血病分类的新方法）  \n  论文引入分层标签和 CNN/ViT 模型，实现 90% 的白血病亚型分类准确率，提供可解释的视觉结果，增强了临床诊断的可靠性。\n\n### 机器人与安全应用\n- **Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation**（中文：弥合模拟到现实的差距，用于运动性定位-操作）  \n  Pulkit Agrawal 等学者提出两阶段训练框架，包括 Unsupervised Actuator Net，实现机器人模拟到现实的鲁棒转移，提升了投掷和拖拽等任务的性能。\n\n- **Do Deepfake Detectors Work in Reality?**（中文：深度伪造检测器在现实中有效吗？）  \n  这篇论文揭示超分辨率处理会削弱深度伪造检测器效果，并发布首个真实世界人脸交换数据集，强调检测方法的实际适用性挑战。\n\n其他论文如 PCGRLLM（LLM 在游戏 AI 中的奖励设计）和 Learning Identifiable Structures（因果学习中的 DNN 优化）等虽有技术贡献，但相对常规，因此快速掠过。总体而言，今天的更新突显 AI 在安全和生物领域的潜力，值得关注未来应用。",
  "papers": [
    {
      "arxiv_id": "2502.10931v2",
      "title": "D-CIPHER: Dynamic Collaborative Intelligent Multi-Agent System with Planner and Heterogeneous Executors for Offensive Security",
      "title_zh": "D-CIPHER：动态协作智能多智能体系统，带有规划器和异构执行器，用于进攻性安全",
      "authors": [
        "Meet Udeshi",
        "Minghao Shao",
        "Haoran Xi",
        "Nanda Rani",
        "Kimberly Milner",
        "Venkata Sai Charan Putrevu",
        "Brendan Dolan-Gavitt",
        "Sandeep Kumar Shukla",
        "Prashanth Krishnamurthy",
        "Farshad Khorrami",
        "Ramesh Karri",
        "Muhammad Shafique"
      ],
      "abstract": "Large Language Models (LLMs) have been used in cybersecurity such as\nautonomous security analysis or penetration testing. Capture the Flag (CTF)\nchallenges serve as benchmarks to assess automated task-planning abilities of\nLLM agents for cybersecurity. Early attempts to apply LLMs for solving CTF\nchallenges used single-agent systems, where feedback was restricted to a single\nreasoning-action loop. This approach was inadequate for complex CTF tasks.\nInspired by real-world CTF competitions, where teams of experts collaborate, we\nintroduce the D-CIPHER LLM multi-agent framework for collaborative CTF solving.\nD-CIPHER integrates agents with distinct roles with dynamic feedback loops to\nenhance reasoning on complex tasks. It introduces the Planner-Executor agent\nsystem, consisting of a Planner agent for overall problem-solving along with\nmultiple heterogeneous Executor agents for individual tasks, facilitating\nefficient allocation of responsibilities among the agents. Additionally,\nD-CIPHER incorporates an Auto-prompter agent to improve problem-solving by\nauto-generating a highly relevant initial prompt. We evaluate D-CIPHER on\nmultiple CTF benchmarks and LLM models via comprehensive studies to highlight\nthe impact of our enhancements. Additionally, we manually map the CTFs in NYU\nCTF Bench to MITRE ATT&CK techniques that apply for a comprehensive evaluation\nof D-CIPHER's offensive security capability. D-CIPHER achieves state-of-the-art\nperformance on three benchmarks: 22.0% on NYU CTF Bench, 22.5% on Cybench, and\n44.0% on HackTheBox, which is 2.5% to 8.5% better than previous work. D-CIPHER\nsolves 65% more ATT&CK techniques compared to previous work, demonstrating\nstronger offensive capability.",
      "tldr_zh": "本文提出 D-CIPHER，一种动态协作的多智能体系统，用于提升 Large Language Models (LLMs) 在 Capture the Flag (CTF) 挑战中的表现，解决单智能体系统的局限性。该系统包括 Planner 代理负责整体问题解决、异构 Executor 代理处理具体任务，以及 Auto-prompter 代理自动生成相关初始提示，以实现高效的任务分配和动态反馈循环。在多个基准测试中，D-CIPHER 取得了 state-of-the-art 性能，在 NYU CTF Bench 上达 22.0%、Cybench 上 22.5%、HackTheBox 上 44.0%，比之前工作提高了 2.5% 到 8.5%，并覆盖了多 65% 的 MITRE ATT&CK 技术，展示了更强的进攻安全能力。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10931v2",
      "published_date": "2025-02-15 23:43:18 UTC",
      "updated_date": "2025-05-11 03:59:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:44:18.766716"
    },
    {
      "arxiv_id": "2502.10928v2",
      "title": "Probing Semantic Routing in Large Mixture-of-Expert Models",
      "title_zh": "探查大型混合专家模型中的语义路由",
      "authors": [
        "Matthew Lyle Olson",
        "Neale Ratzlaff",
        "Musashi Hinck",
        "Man Luo",
        "Sungduk Yu",
        "Chendi Xue",
        "Vasudev Lal"
      ],
      "abstract": "In the past year, large (>100B parameter) mixture-of-expert (MoE) models have\nbecome increasingly common in the open domain. While their advantages are often\nframed in terms of efficiency, prior work has also explored functional\ndifferentiation through routing behavior. We investigate whether expert routing\nin large MoE models is influenced by the semantics of the inputs. To test this,\nwe design two controlled experiments. First, we compare activations on sentence\npairs with a shared target word used in the same or different senses. Second,\nwe fix context and substitute the target word with semantically similar or\ndissimilar alternatives. Comparing expert overlap across these conditions\nreveals clear, statistically significant evidence of semantic routing in large\nMoE models.",
      "tldr_zh": "这篇论文探讨了大型Mixture-of-Expert (MoE) 模型中专家路由是否受输入语义影响，通过设计两个控制实验进行验证。第一个实验比较了句子对中共享目标词在相同或不同语义下的激活模式；第二个实验则在固定上下文中替换目标词为语义相似或不似词汇，以观察专家重叠情况。结果显示，实验中存在清晰且统计显著的语义路由证据，这为理解MoE模型的功能分化和效率优势提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 5 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.10928v2",
      "published_date": "2025-02-15 23:37:32 UTC",
      "updated_date": "2025-05-21 16:32:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:44:30.157966"
    },
    {
      "arxiv_id": "2502.10920v1",
      "title": "Do Deepfake Detectors Work in Reality?",
      "title_zh": "深度伪造检测器在现实中有效吗？",
      "authors": [
        "Simiao Ren",
        "Hengwei Xu",
        "Tsang Ng",
        "Kidus Zewde",
        "Shengkai Jiang",
        "Ramini Desai",
        "Disha Patil",
        "Ning-Yau Cheng",
        "Yining Zhou",
        "Ragavi Muthukrishnan"
      ],
      "abstract": "Deepfakes, particularly those involving faceswap-based manipulations, have\nsparked significant societal concern due to their increasing realism and\npotential for misuse. Despite rapid advancements in generative models,\ndetection methods have not kept pace, creating a critical gap in defense\nstrategies. This disparity is further amplified by the disconnect between\nacademic research and real-world applications, which often prioritize different\nobjectives and evaluation criteria. In this study, we take a pivotal step\ntoward bridging this gap by presenting a novel observation: the post-processing\nstep of super-resolution, commonly employed in real-world scenarios,\nsubstantially undermines the effectiveness of existing deepfake detection\nmethods. To substantiate this claim, we introduce and publish the first\nreal-world faceswap dataset, collected from popular online faceswap platforms.\nWe then qualitatively evaluate the performance of state-of-the-art deepfake\ndetectors on real-world deepfakes, revealing that their accuracy approaches the\nlevel of random guessing. Furthermore, we quantitatively demonstrate the\nsignificant performance degradation caused by common post-processing\ntechniques. By addressing this overlooked challenge, our study underscores a\ncritical avenue for enhancing the robustness and practical applicability of\ndeepfake detection methods in real-world settings.",
      "tldr_zh": "本研究探讨了 deepfake 检测器在现实场景中的实际表现，发现后处理步骤如 super-resolution 会显著削弱现有检测方法的有效性，从而加剧了学术研究与真实应用之间的脱节。论文引入并发布了第一个真实世界 faceswap 数据集，并对 state-of-the-art (SOTA) 检测器进行了定性和定量评估，结果显示这些检测器在真实 deepfakes 上的准确率接近随机猜测。通过揭示这一关键挑战，该研究为提升 deepfake 检测的鲁棒性和实用性提供了重要方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10920v1",
      "published_date": "2025-02-15 22:38:40 UTC",
      "updated_date": "2025-02-15 22:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:44:42.216119"
    },
    {
      "arxiv_id": "2502.10908v1",
      "title": "Automatic Quality Assessment of First Trimester Crown-Rump-Length Ultrasound Images",
      "title_zh": "第一孕期头臀长度超声图像的自动质量评估",
      "authors": [
        "Sevim Cengiz",
        "Ibraheem Hamdi",
        "Mohammad Yaqub"
      ],
      "abstract": "Fetal gestational age (GA) is vital clinical information that is estimated\nduring pregnancy in order to assess fetal growth. This is usually performed by\nmeasuring the crown-rump-length (CRL) on an ultrasound image in the Dating scan\nwhich is then correlated with fetal age and growth trajectory. A major issue\nwhen performing the CRL measurement is ensuring that the image is acquired at\nthe correct view, otherwise it could be misleading. Although clinical\nguidelines specify the criteria for the correct CRL view, sonographers may not\nregularly adhere to such rules. In this paper, we propose a new deep\nlearning-based solution that is able to verify the adherence of a CRL image to\nclinical guidelines in order to assess image quality and facilitate accurate\nestimation of GA. We first segment out important fetal structures then use the\nlocalized structures to perform a clinically-guided mapping that verifies the\nadherence of criteria. The segmentation method combines the benefits of\nConvolutional Neural Network (CNN) and the Vision Transformer (ViT) to segment\nfetal structures in ultrasound images and localize important fetal landmarks.\nFor segmentation purposes, we compare our proposed work with UNet and show that\nour CNN/ViT-based method outperforms an optimized version of UNet. Furthermore,\nwe compare the output of the mapping with classification CNNs when assessing\nthe clinical criteria and the overall acceptability of CRL images. We show that\nthe proposed mapping is not only explainable but also more accurate than the\nbest performing classification CNNs.",
      "tldr_zh": "本研究针对第一孕期头臀长（Crown-Rump-Length, CRL）超声图像的质量评估问题，提出了一种基于深度学习的方法，以确保图像符合临床指南，从而提高胎儿孕龄（GA）估算的准确性。该方法首先利用结合 Convolutional Neural Network (CNN) 和 Vision Transformer (ViT) 的分割技术，提取并定位胎儿关键结构，然后通过临床指导的映射验证图像是否符合标准。与 UNet 相比，该分割方法表现出色；在评估临床标准和图像可接受性时，该映射方法不仅比分类 CNN 更准确，还具备更高的可解释性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10908v1",
      "published_date": "2025-02-15 21:05:07 UTC",
      "updated_date": "2025-02-15 21:05:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:44:53.878591"
    },
    {
      "arxiv_id": "2502.10906v1",
      "title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning",
      "title_zh": "PCGRLLM：大型",
      "authors": [
        "In-Chang Baek",
        "Sung-Hyun Kim",
        "Sam Earle",
        "Zehua Jiang",
        "Noh Jin-Ha",
        "Julian Togelius",
        "Kyung-Joong Kim"
      ],
      "abstract": "Reward design plays a pivotal role in the training of game AIs, requiring\nsubstantial domain-specific knowledge and human effort. In recent years,\nseveral studies have explored reward generation for training game agents and\ncontrolling robots using large language models (LLMs). In the content\ngeneration literature, there has been early work on generating reward functions\nfor reinforcement learning agent generators. This work introduces PCGRLLM, an\nextended architecture based on earlier work, which employs a feedback mechanism\nand several reasoning-based prompt engineering techniques. We evaluate the\nproposed method on a story-to-reward generation task in a two-dimensional\nenvironment using two state-of-the-art LLMs, demonstrating the generalizability\nof our approach. Our experiments provide insightful evaluations that\ndemonstrate the capabilities of LLMs essential for content generation tasks.\nThe results highlight significant performance improvements of 415% and 40%\nrespectively, depending on the zero-shot capabilities of the language model.\nOur work demonstrates the potential to reduce human dependency in game AI\ndevelopment, while supporting and enhancing creative processes.",
      "tldr_zh": "该研究提出PCGRLLM框架，利用Large Language Model (LLM)驱动的奖励设计方法，针对Procedural Content Generation Reinforcement Learning (PCGRL)任务，减少了对人类专业知识的依赖。框架引入反馈机制和基于推理的提示工程技术，通过在二维环境中进行故事到奖励生成的实验，评估了两个最先进LLM的表现。结果显示性能提升高达415%和40%，取决于LLM的零样本能力，从而支持游戏AI开发中的创造过程并显著提高效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10906v1",
      "published_date": "2025-02-15 21:00:40 UTC",
      "updated_date": "2025-02-15 21:00:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:45:05.774038"
    },
    {
      "arxiv_id": "2502.10899v1",
      "title": "Breaking Down the Hierarchy: A New Approach to Leukemia Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Ibraheem Hamdi",
        "Hosam El-Gendy",
        "Ahmed Sharshar",
        "Mohamed Saeed",
        "Muhammad Ridzuan",
        "Shahrukh K. Hashmi",
        "Naveed Syed",
        "Imran Mirza",
        "Shakir Hussain",
        "Amira Mahmoud Abdalla",
        "Mohammad Yaqub"
      ],
      "abstract": "The complexities inherent to leukemia, multifaceted cancer affecting white\nblood cells, pose considerable diagnostic and treatment challenges, primarily\ndue to reliance on laborious morphological analyses and expert judgment that\nare susceptible to errors. Addressing these challenges, this study presents a\nrefined, comprehensive strategy leveraging advanced deep-learning techniques\nfor the classification of leukemia subtypes. We commence by developing a\nhierarchical label taxonomy, paving the way for differentiating between various\nsubtypes of leukemia. The research further introduces a novel hierarchical\napproach inspired by clinical procedures capable of accurately classifying\ndiverse types of leukemia alongside reactive and healthy cells. An integral\npart of this study involves a meticulous examination of the performance of\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs) as\nclassifiers. The proposed method exhibits an impressive success rate, achieving\napproximately 90\\% accuracy across all leukemia subtypes, as substantiated by\nour experimental results. A visual representation of the experimental findings\nis provided to enhance the model's explainability and aid in understanding the\nclassification process.",
      "tldr_zh": "这篇论文针对白血病（leukemia）的复杂诊断挑战，提出了一种新的全面策略，利用高级深度学习技术来分类白血病亚型。研究开发了分层标签分类法（hierarchical label taxonomy）和一种受临床程序启发的分层方法，能够准确区分各种白血病类型、反应性细胞和健康细胞，并比较了Convolutional Neural Networks (CNNs) 和 Vision Transformers (ViTs) 的性能。实验结果显示，该方法在所有白血病亚型上实现了约90%的准确率，并通过视觉表示增强了模型的可解释性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10899v1",
      "published_date": "2025-02-15 20:36:15 UTC",
      "updated_date": "2025-02-15 20:36:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:45:18.240801"
    },
    {
      "arxiv_id": "2502.10894v1",
      "title": "Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Nolan Fey",
        "Gabriel B. Margolis",
        "Martin Peticco",
        "Pulkit Agrawal"
      ],
      "abstract": "Achieving athletic loco-manipulation on robots requires moving beyond\ntraditional tracking rewards - which simply guide the robot along a reference\ntrajectory - to task rewards that drive truly dynamic, goal-oriented behaviors.\nCommands such as \"throw the ball as far as you can\" or \"lift the weight as\nquickly as possible\" compel the robot to exhibit the agility and power inherent\nin athletic performance. However, training solely with task rewards introduces\ntwo major challenges: these rewards are prone to exploitation (reward hacking),\nand the exploration process can lack sufficient direction. To address these\nissues, we propose a two-stage training pipeline. First, we introduce the\nUnsupervised Actuator Net (UAN), which leverages real-world data to bridge the\nsim-to-real gap for complex actuation mechanisms without requiring access to\ntorque sensing. UAN mitigates reward hacking by ensuring that the learned\nbehaviors remain robust and transferable. Second, we use a pre-training and\nfine-tuning strategy that leverages reference trajectories as initial hints to\nguide exploration. With these innovations, our robot athlete learns to lift,\nthrow, and drag with remarkable fidelity from simulation to reality.",
      "tldr_zh": "该论文针对机器人运动性 loco-manipulation 桥接 sim-to-real 差距，提出一种两阶段训练管道，以实现动态、目标导向的行为，如尽可能远地扔球或快速举起重物。\n首先，引入 Unsupervised Actuator Net (UAN)，利用真实世界数据增强鲁棒性，避免 reward hacking，并无需扭矩传感器确保行为从模拟到现实的平稳转移。\n其次，通过预训练和微调策略，使用参考轨迹作为初始提示引导探索过程。\n最终，机器人成功从模拟环境中学习，并在现实中高保真度执行举起、扔和拖拽等任务。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: http://uan.csail.mit.edu",
      "pdf_url": "http://arxiv.org/pdf/2502.10894v1",
      "published_date": "2025-02-15 20:18:37 UTC",
      "updated_date": "2025-02-15 20:18:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:45:30.575600"
    },
    {
      "arxiv_id": "2502.10883v1",
      "title": "Learning Identifiable Structures Helps Avoid Bias in DNN-based Supervised Causal Learning",
      "title_zh": "学习可识别结构有助于避免基于DNN的监督因果学习中的偏差",
      "authors": [
        "Jiaru Zhang",
        "Rui Ding",
        "Qiang Fu",
        "Bojun Huang",
        "Zizhen Deng",
        "Yang Hua",
        "Haibing Guan",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "abstract": "Causal discovery is a structured prediction task that aims to predict causal\nrelations among variables based on their data samples. Supervised Causal\nLearning (SCL) is an emerging paradigm in this field. Existing Deep Neural\nNetwork (DNN)-based methods commonly adopt the \"Node-Edge approach\", in which\nthe model first computes an embedding vector for each variable-node, then uses\nthese variable-wise representations to concurrently and independently predict\nfor each directed causal-edge. In this paper, we first show that this\narchitecture has some systematic bias that cannot be mitigated regardless of\nmodel size and data size. We then propose SiCL, a DNN-based SCL method that\npredicts a skeleton matrix together with a v-tensor (a third-order tensor\nrepresenting the v-structures). According to the Markov Equivalence Class (MEC)\ntheory, both the skeleton and the v-structures are identifiable causal\nstructures under the canonical MEC setting, so predictions about skeleton and\nv-structures do not suffer from the identifiability limit in causal discovery,\nthus SiCL can avoid the systematic bias in Node-Edge architecture, and enable\nconsistent estimators for causal discovery. Moreover, SiCL is also equipped\nwith a specially designed pairwise encoder module with a unidirectional\nattention layer to model both internal and external relationships of pairs of\nnodes. Experimental results on both synthetic and real-world benchmarks show\nthat SiCL significantly outperforms other DNN-based SCL approaches.",
      "tldr_zh": "本研究发现，现有的DNN-based Supervised Causal Learning (SCL)方法采用Node-Edge approach存在系统性偏差，无法通过模型或数据规模缓解，因为它独立预测因果边而忽略可识别结构。论文提出SiCL方法，通过预测skeleton matrix和v-tensor（代表v-structures），利用Markov Equivalence Class (MEC)理论确保这些结构的可识别性，从而避免偏差并实现一致的因果发现估计。SiCL还引入了带单向注意力层的成对编码器模块，以更好地建模节点对的内部和外部关系；实验在合成和真实世界基准上显示，SiCL显著优于其他DNN-based SCL方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10883v1",
      "published_date": "2025-02-15 19:10:35 UTC",
      "updated_date": "2025-02-15 19:10:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:45:42.217534"
    },
    {
      "arxiv_id": "2502.10878v1",
      "title": "Broadcast Channel Cooperative Gain: An Operational Interpretation of Partial Information Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Chao Tian",
        "Shlomo Shamai"
      ],
      "abstract": "Partial information decomposition has recently found applications in\nbiological signal processing and machine learning. Despite its impacts, the\ndecomposition was introduced through an informal and heuristic route, and its\nexact operational meaning is unclear. In this work, we fill this gap by\nconnecting partial information decomposition to the capacity of the broadcast\nchannel, which has been well-studied in the information theory literature. We\nshow that the synergistic information in the decomposition can be rigorously\ninterpreted as the cooperative gain, or a lower bound of this gain, on the\ncorresponding broadcast channel. This interpretation can help practitioners to\nbetter explain and expand the applications of the partial information\ndecomposition technique.",
      "tldr_zh": "该研究探讨了部分信息分解（Partial Information Decomposition）的操作意义，该技术虽已在生物信号处理和机器学习中广泛应用，但其引入缺乏正式基础。作者通过将部分信息分解与信息理论中的广播信道（Broadcast Channel）容量联系起来，证明协同信息（Synergistic Information）可被解释为广播信道上的合作增益（Cooperative Gain）或其下界。这一方法为部分信息分解提供了严格的操作解释，有助于从业者更好地理解和扩展其在实际应用中的潜力。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "9 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2502.10878v1",
      "published_date": "2025-02-15 18:28:36 UTC",
      "updated_date": "2025-02-15 18:28:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:45:53.147791"
    },
    {
      "arxiv_id": "2502.10875v1",
      "title": "A Geometric Approach to Personalized Recommendation with Set-Theoretic Constraints Using Box Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Shib Dasgupta",
        "Michael Boratko",
        "Andrew McCallum"
      ],
      "abstract": "Personalized item recommendation typically suffers from data sparsity, which\nis most often addressed by learning vector representations of users and items\nvia low-rank matrix factorization. While this effectively densifies the matrix\nby assuming users and movies can be represented by linearly dependent latent\nfeatures, it does not capture more complicated interactions. For example,\nvector representations struggle with set-theoretic relationships, such as\nnegation and intersection, e.g. recommending a movie that is \"comedy and\naction, but not romance\". In this work, we formulate the problem of\npersonalized item recommendation as matrix completion where rows are\nset-theoretically dependent. To capture this set-theoretic dependence we\nrepresent each user and attribute by a hyper-rectangle or box (i.e. a Cartesian\nproduct of intervals). Box embeddings can intuitively be understood as\ntrainable Venn diagrams, and thus not only inherently represent similarity (via\nthe Jaccard index), but also naturally and faithfully support arbitrary\nset-theoretic relationships. Queries involving set-theoretic constraints can be\nefficiently computed directly on the embedding space by performing geometric\noperations on the representations. We empirically demonstrate the superiority\nof box embeddings over vector-based neural methods on both simple and complex\nitem recommendation queries by up to 30 \\% overall.",
      "tldr_zh": "本文提出了一种几何方法，用于个性化推荐问题，通过box embeddings将用户和属性表示为超矩形（hyper-rectangle），以处理集合理论约束如否定和交集，从而克服传统向量表示在复杂交互中的局限性。该方法将推荐问题表述为矩阵补全，并利用嵌入空间的几何操作（如Jaccard指数计算）高效执行查询，支持任意集合理论关系。实验结果显示，box embeddings在简单和复杂推荐查询上比基于向量的神经方法整体提升高达30%的性能。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10875v1",
      "published_date": "2025-02-15 18:18:00 UTC",
      "updated_date": "2025-02-15 18:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:46:06.059941"
    },
    {
      "arxiv_id": "2502.12197v1",
      "title": "A Closer Look at System Prompt Robustness",
      "title_zh": "翻译失败",
      "authors": [
        "Norman Mu",
        "Jonathan Lu",
        "Michael Lavery",
        "David Wagner"
      ],
      "abstract": "System prompts have emerged as a critical control surface for specifying the\nbehavior of LLMs in chat and agent settings. Developers depend on system\nprompts to specify important context, output format, personalities, guardrails,\ncontent policies, and safety countermeasures, all of which require models to\nrobustly adhere to the system prompt, especially when facing conflicting or\nadversarial user inputs. In practice, models often forget to consider relevant\nguardrails or fail to resolve conflicting demands between the system and the\nuser. In this work, we study various methods for improving system prompt\nrobustness by creating realistic new evaluation and fine-tuning datasets based\non prompts collected from from OpenAI's GPT Store and HuggingFace's\nHuggingChat. Our experiments assessing models with a panel of new and existing\nbenchmarks show that performance can be considerably improved with realistic\nfine-tuning data, as well as inference-time interventions such as\nclassifier-free guidance. Finally, we analyze the results of recently released\nreasoning models from OpenAI and DeepSeek, which show exciting but uneven\nimprovements on the benchmarks we study. Overall, current techniques fall short\nof ensuring system prompt robustness and further study is warranted.",
      "tldr_zh": "本文深入探讨了大型语言模型（LLMs）中 system prompts 的鲁棒性问题，强调这些提示在指定上下文、输出格式、个性、防护栏和安全措施方面的关键作用，但模型往往在面对冲突或对抗性用户输入时无法有效遵守。研究团队创建了基于 OpenAI's GPT Store 和 HuggingFace's HuggingChat 的新评估和微调数据集，并测试了 classifier-free guidance 等推理时干预方法，以提升系统提示的可靠性。实验结果显示，通过现实微调数据，模型性能显著改善，但 OpenAI 和 DeepSeek 的新推理模型表现不均衡；总体而言，当前技术仍不足以确保鲁棒性，需要进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Artifacts: https://github.com/normster/RealGuardrails",
      "pdf_url": "http://arxiv.org/pdf/2502.12197v1",
      "published_date": "2025-02-15 18:10:45 UTC",
      "updated_date": "2025-02-15 18:10:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:46:19.199037"
    },
    {
      "arxiv_id": "2502.10871v1",
      "title": "The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Ge Lei",
        "Samuel J. Cooper"
      ],
      "abstract": "This study investigates how large language models (LLMs) represent and recall\nmulti-associated attributes across transformer layers. We show that\nintermediate layers encode factual knowledge by superimposing related\nattributes in overlapping spaces, along with effective recall even when\nattributes are not explicitly prompted. In contrast, later layers refine\nlinguistic patterns and progressively separate attribute representations,\noptimizing task-specific outputs while appropriately narrowing attribute\nrecall. We identify diverse encoding patterns including, for the first time,\nthe observation of 3D spiral structures when exploring information related to\nthe periodic table of elements. Our findings reveal a dynamic transition in\nattribute representations across layers, contributing to mechanistic\ninterpretability and providing insights for understanding how LLMs handle\ncomplex, interrelated knowledge.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在 Transformer 层中如何表示和回想多关联属性。结果显示，中间层通过在重叠空间叠加相关属性来编码事实知识，并能有效回想未显式提示的属性，而后期层则优化语言模式并逐步分离属性表示，以适应特定任务输出。论文首次观察到与元素周期表相关的 3D 螺旋结构，并揭示了属性表示在层间的动态过渡，这为 LLMs 的机制可解释性和处理复杂相互关联知识提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10871v1",
      "published_date": "2025-02-15 18:08:51 UTC",
      "updated_date": "2025-02-15 18:08:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:46:31.563720"
    },
    {
      "arxiv_id": "2502.10867v1",
      "title": "A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Wang"
      ],
      "abstract": "OpenAI o1 has shown that applying reinforcement learning to integrate\nreasoning steps directly during inference can significantly improve a model's\nreasoning capabilities. This result is exciting as the field transitions from\nthe conventional autoregressive method of generating answers to a more\ndeliberate approach that models the slow-thinking process through step-by-step\nreasoning training. Reinforcement learning plays a key role in both the model's\ntraining and decoding processes. In this article, we present a comprehensive\nformulation of reasoning problems and investigate the use of both model-based\nand model-free approaches to better support this slow-thinking framework.",
      "tldr_zh": "这篇教程文章探讨了大型语言模型（LLM）推理的相关方法，特别是ChatGPT o1背后的技术。OpenAI o1通过在推理过程中应用强化学习（Reinforcement Learning）来整合步步推理步骤，从而显著提升模型的推理能力，并从传统的autoregressive生成答案方法转向更 deliberate 的慢思考框架。文章全面公式化了推理问题，并调查了model-based和model-free方法，以更好地支持这一训练和解码过程。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10867v1",
      "published_date": "2025-02-15 17:52:11 UTC",
      "updated_date": "2025-02-15 17:52:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:46:42.459149"
    },
    {
      "arxiv_id": "2502.10858v2",
      "title": "Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zongqian Wu",
        "Tianyu Li",
        "Baoduo Xu",
        "Jiaying Yang",
        "Mengmeng Zhan",
        "Xiaofeng Zhu",
        "Lei Feng"
      ],
      "abstract": "Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle\ncomplex tasks by progressively activating relevant pre-trained knowledge.\nHowever, it faces challenges in ensuring continual improvement and determining\na stopping criterion. In this paper, we investigate whether the relevant\nknowledge that contributes directly to solving the given question can be\nactivated from the initial reasoning path, thus circumventing the need for\niterative refinement. Our experiments reveal that increasing the diversity of\ninitial reasoning paths can achieve comparable or superior performance, a\nconcept we term \\textit{breadth reasoning}. However, existing breadth reasoning\napproaches, such as self-consistency, offer limited diversity. To address this\nlimitation, we propose a simple yet effective method that enhances reasoning\nbreadth by integrating contextual exploration with reduced sampling randomness.\nExtensive experiments demonstrate that our approach significantly outperforms\ndeep iterative reasoning. Our code is provided in\nhttps://github.com/zongqianwu/breadth.",
      "tldr_zh": "本文探讨了大型语言模型（LLMs）中深度迭代Chain-of-Thought (CoT)推理的局限性，包括持续改进和停止标准的挑战，并提出通过增加初始推理路径多样性（breadth reasoning）来提升性能。作者发现，现有方法如self-consistency的多样性有限，因此开发了一种简单方法，结合上下文探索和减少采样随机性来增强推理广度。实验结果显示，这种方法在复杂任务上显著优于深度迭代推理，并提供了开源代码（https://github.com/zongqianwu/breadth）。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10858v2",
      "published_date": "2025-02-15 16:59:59 UTC",
      "updated_date": "2025-02-18 07:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:46:54.619996"
    },
    {
      "arxiv_id": "2502.10852v1",
      "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
      "title_zh": "多语言编码器知道的比你想象中多：针对极低资源语言的共享权重预训练",
      "authors": [
        "Zeli Su",
        "Ziyin Zhang",
        "Guixian Xu",
        "Jianing Liu",
        "XU Han",
        "Ting Zhang",
        "Yushuang Dong"
      ],
      "abstract": "While multilingual language models like XLM-R have advanced multilingualism\nin NLP, they still perform poorly in extremely low-resource languages. This\nsituation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen\nsupport far fewer languages than XLM-R, making text generation models\nnon-existent for many languages in the world. To tackle this challenge, we\npropose a novel framework for adapting multilingual encoders to text generation\nin extremely low-resource languages. By reusing the weights between the encoder\nand the decoder, our framework allows the model to leverage the learned\nsemantic space of the encoder, enabling efficient learning and effective\ngeneralization in low-resource languages. Applying this framework to four\nChinese minority languages, we present XLM-SWCM, and demonstrate its superior\nperformance on various downstream tasks even when compared with much larger\nmodels.",
      "tldr_zh": "尽管多语言模型如 XLM-R 在 NLP 中取得了进展，但它们在极低资源语言上的表现仍较差，尤其现代 LLMs 如 LLaMA 和 Qwen 支持的语言更有限，导致许多语言缺乏文本生成模型。  \n本文提出一个新框架，通过在编码器和解码器之间共享权重（shared weights）预训练，允许模型利用编码器的语义空间，实现对极低资源语言的高效学习和泛化。  \n该框架应用于四个中国少数民族语言，开发了 XLM-SWCM 模型，并在各种下游任务上表现出优越性能，甚至超越了更大的基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10852v1",
      "published_date": "2025-02-15 16:53:10 UTC",
      "updated_date": "2025-02-15 16:53:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:47:08.414114"
    },
    {
      "arxiv_id": "2502.10828v1",
      "title": "The Vendiscope: An Algorithmic Microscope For Data Collections",
      "title_zh": "翻译失败",
      "authors": [
        "Amey P. Pasarkar",
        "Adji Bousso Dieng"
      ],
      "abstract": "The evolution of microscopy, beginning with its invention in the late 16th\ncentury, has continuously enhanced our ability to explore and understand the\nmicroscopic world, enabling increasingly detailed observations of structures\nand phenomena. In parallel, the rise of data-driven science has underscored the\nneed for sophisticated methods to explore and understand the composition of\ncomplex data collections. This paper introduces the Vendiscope, the first\nalgorithmic microscope designed to extend traditional microscopy to\ncomputational analysis. The Vendiscope leverages the Vendi scores -- a family\nof differentiable diversity metrics rooted in ecology and quantum mechanics --\nand assigns weights to data points based on their contribution to the overall\ndiversity of the collection. These weights enable high-resolution data analysis\nat scale. We demonstrate this across biology, materials science, and machine\nlearning (ML). We analyzed the $250$ million protein sequences in the protein\nuniverse, discovering that over $200$ million are near-duplicates and that\nAlphaFold fails on proteins with Gene Ontology (GO) functions that contribute\nmost to diversity. Applying the Vendiscope to the Materials Project database\nled to similar findings: more than $85\\%$ of the crystals with formation energy\ndata are near-duplicates and ML models perform poorly on materials that enhance\ndiversity. Additionally, the Vendiscope can be used to study phenomena such as\nmemorization in generative models. We used the Vendiscope to identify memorized\ntraining samples from $13$ different generative models and found that the\nbest-performing ones often memorize the training samples that contribute least\nto diversity. Our findings demonstrate that the Vendiscope can serve as a\npowerful tool for data-driven science.",
      "tldr_zh": "本论文引入了 Vendiscope，一种首创的算法显微镜，将传统显微镜扩展到计算分析领域，通过 Vendi scores（一种源于生态学和量子力学的可微分多样性指标）为数据点分配权重，以实现大规模的高分辨率数据分析。研究在生物学、材料科学和机器学习（ML）领域应用了 Vendiscope，发现蛋白序列中超过 2 亿是近重复的，AlphaFold 在 Gene Ontology (GO) 函数多样性贡献最大的蛋白上失败；Materials Project 数据库中超过 85% 的晶体是近重复的，且 ML 模型在多样性增强材料上表现差。总体结果显示，Vendiscope 能有效识别生成模型的记忆化样本，并证明其作为数据驱动科学强大工具的价值。",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper introduces the concept of \"algorithmic microscopes\" and\n  proposes the Vendiscope, an algorithmic microscope for data-driven science",
      "pdf_url": "http://arxiv.org/pdf/2502.10828v1",
      "published_date": "2025-02-15 15:07:01 UTC",
      "updated_date": "2025-02-15 15:07:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:47:19.195368"
    },
    {
      "arxiv_id": "2502.10825v1",
      "title": "MITRE ATT&CK Applications in Cybersecurity and The Way Forward",
      "title_zh": "MITRE ATT&CK 在网络安全中的应用以及未来方向",
      "authors": [
        "Yuning Jiang",
        "Qiaoran Meng",
        "Feiyang Shang",
        "Nay Oo",
        "Le Thi Hong Minh",
        "Hoon Wei Lim",
        "Biplab Sikdar"
      ],
      "abstract": "The MITRE ATT&CK framework is a widely adopted tool for enhancing\ncybersecurity, supporting threat intelligence, incident response, attack\nmodeling, and vulnerability prioritization. This paper synthesizes research on\nits application across these domains by analyzing 417 peer-reviewed\npublications. We identify commonly used adversarial tactics, techniques, and\nprocedures (TTPs) and examine the integration of natural language processing\n(NLP) and machine learning (ML) with ATT&CK to improve threat detection and\nresponse. Additionally, we explore the interoperability of ATT&CK with other\nframeworks, such as the Cyber Kill Chain, NIST guidelines, and STRIDE,\nhighlighting its versatility. The paper further evaluates the framework from\nmultiple perspectives, including its effectiveness, validation methods, and\nsector-specific challenges, particularly in industrial control systems (ICS)\nand healthcare. We conclude by discussing current limitations and proposing\nfuture research directions to enhance the applicability of ATT&CK in dynamic\ncybersecurity environments.",
      "tldr_zh": "这篇论文分析了 MITRE ATT&CK 框架在网络安全领域的应用，通过审阅 417 篇同行评议文献，综合探讨其在威胁情报、事件响应和攻击建模等方面的作用。研究识别了常见的攻击 TTPs，并考察了 NLP 和 ML 与 ATT&CK 的整合，以提升威胁检测和响应效率，同时评估了其与 Cyber Kill Chain、NIST 和 STRIDE 等框架的互操作性。论文从有效性、验证方法和行业特定挑战（如 ICS 和医疗保健）角度评估了框架的局限性，并提出未来研究方向，以增强其在动态环境中的适用性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "68M25 (Primary) 68T99 (Secondary)"
      ],
      "primary_category": "cs.CR",
      "comment": "37 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.10825v1",
      "published_date": "2025-02-15 15:01:04 UTC",
      "updated_date": "2025-02-15 15:01:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:47:31.085044"
    },
    {
      "arxiv_id": "2502.10822v1",
      "title": "NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for Personalized Hearing Aids",
      "title_zh": "NeuroAMP：一种新颖的端到端通用深度神经放大器，用于个性化助听器",
      "authors": [
        "Shafique Ahmed",
        "Ryandhimas E. Zezario",
        "Hui-Guan Yuan",
        "Amir Hussain",
        "Hsin-Min Wang",
        "Wei-Ho Chung",
        "Yu Tsao"
      ],
      "abstract": "The prevalence of hearing aids is increasing. However, optimizing the\namplification processes of hearing aids remains challenging due to the\ncomplexity of integrating multiple modular components in traditional methods.\nTo address this challenge, we present NeuroAMP, a novel deep neural network\ndesigned for end-to-end, personalized amplification in hearing aids. NeuroAMP\nleverages both spectral features and the listener's audiogram as inputs, and we\ninvestigate four architectures: Convolutional Neural Network (CNN), Long\nShort-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and\nTransformer. We also introduce Denoising NeuroAMP, an extension that integrates\nnoise reduction along with amplification capabilities for improved performance\nin real-world scenarios. To enhance generalization, a comprehensive data\naugmentation strategy was employed during training on diverse speech (TIMIT and\nTMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the\nHearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index\n(HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the\nTransformer architecture within NeuroAMP achieves the best performance, with\nSRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI)\non the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy\nmaintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ).\nFurthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC\napproach and a two-stage baseline on the VoiceBank+DEMAND dataset, achieving a\n10% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results\nhighlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable\nimprovements in personalized hearing aid amplification.",
      "tldr_zh": "本研究提出NeuroAMP，一种新型端到端深度神经网络，用于个性化助听器的放大处理，以解决传统方法中多模块集成复杂的问题。NeuroAMP以频谱特征和听者的听力图(audiogram)作为输入，评估了四种架构，包括CNN、LSTM、CRNN和Transformer，并引入Denoising NeuroAMP扩展以整合噪声减少功能，同时采用全面数据增强策略训练于TIMIT、TMHINT和Cadenza Challenge MUSIC数据集。实验结果显示，Transformer架构表现出色，在TIMIT数据集上获得HASQI的SRCC分数0.9927和HASPI的0.9905，在Cadenza Challenge MUSIC上达到HAAQI的0.9738，且数据增强确保了在新数据集上的高泛化性能。相比传统NAL-R+WDRC方法，Denoising NeuroAMP在VoiceBank+DEMAND数据集上提升了10%的HASPI（0.90）和HASQI（0.59）分数，展示了其在个性化助听器放大中的显著潜力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10822v1",
      "published_date": "2025-02-15 14:55:40 UTC",
      "updated_date": "2025-02-15 14:55:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:47:44.388954"
    },
    {
      "arxiv_id": "2502.10818v1",
      "title": "On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs: Bridging Recurrent and Graph Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Álvaro Arroyo",
        "Alessio Gravina",
        "Benjamin Gutteridge",
        "Federico Barbero",
        "Claudio Gallicchio",
        "Xiaowen Dong",
        "Michael Bronstein",
        "Pierre Vandergheynst"
      ],
      "abstract": "Graph Neural Networks (GNNs) are models that leverage the graph structure to\ntransmit information between nodes, typically through the message-passing\noperation. While widely successful, this approach is well known to suffer from\nthe over-smoothing and over-squashing phenomena, which result in\nrepresentational collapse as the number of layers increases and insensitivity\nto the information contained at distant and poorly connected nodes,\nrespectively. In this paper, we present a unified view of these problems\nthrough the lens of vanishing gradients, using ideas from linear control theory\nfor our analysis. We propose an interpretation of GNNs as recurrent models and\nempirically demonstrate that a simple state-space formulation of a GNN\neffectively alleviates over-smoothing and over-squashing at no extra trainable\nparameter cost. Further, we show theoretically and empirically that (i) GNNs\nare by design prone to extreme gradient vanishing even after a few layers; (ii)\nOver-smoothing is directly related to the mechanism causing vanishing\ngradients; (iii) Over-squashing is most easily alleviated by a combination of\ngraph rewiring and vanishing gradient mitigation. We believe our work will help\nbridge the gap between the recurrent and graph neural network literature and\nwill unlock the design of new deep and performant GNNs.",
      "tldr_zh": "本文通过 vanishing gradients 的视角，使用线性控制理论，统一分析了 Graph Neural Networks (GNNs) 中的 over-smoothing（层数增加导致表示崩塌）和 over-squashing（对远距离节点信息不敏感）问题。作者将 GNNs 解释为 recurrent models，并提出一个简单的 state-space formulation，能在不增加可训练参数的情况下有效缓解这些现象。理论和实证证明显示，GNNs 设计上容易出现 gradient vanishing，且 over-smoothing 与 vanishing gradients 直接相关，而 over-squashing 可通过 graph rewiring 和 gradient mitigation 结合解决。该研究有助于桥接 recurrent 和 graph neural network 文献，推动更深层、高性能 GNNs 的开发。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10818v1",
      "published_date": "2025-02-15 14:43:41 UTC",
      "updated_date": "2025-02-15 14:43:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:47:54.536915"
    },
    {
      "arxiv_id": "2502.10816v3",
      "title": "BalanceBenchmark: A Survey for Multimodal Imbalance Learning",
      "title_zh": "BalanceBenchmark：多模态不平衡学习的调查",
      "authors": [
        "Shaoxuan Xu",
        "Menglu Cui",
        "Chengxiang Huang",
        "Hongfa Wang",
        "Di Hu"
      ],
      "abstract": "Multimodal learning has gained attention for its capacity to integrate\ninformation from different modalities. However, it is often hindered by the\nmultimodal imbalance problem, where certain modality dominates while others\nremain underutilized. Although recent studies have proposed various methods to\nalleviate this problem, they lack comprehensive and fair comparisons. In this\npaper, we systematically categorize various mainstream multimodal imbalance\nalgorithms into four groups based on the strategies they employ to mitigate\nimbalance. To facilitate a comprehensive evaluation of these methods, we\nintroduce BalanceBenchmark, a benchmark including multiple widely used\nmultidimensional datasets and evaluation metrics from three perspectives:\nperformance, imbalance degree, and complexity. To ensure fair comparisons, we\nhave developed a modular and extensible toolkit that standardizes the\nexperimental workflow across different methods. Based on the experiments using\nBalanceBenchmark, we have identified several key insights into the\ncharacteristics and advantages of different method groups in terms of\nperformance, balance degree and computational complexity. We expect such\nanalysis could inspire more efficient approaches to address the imbalance\nproblem in the future, as well as foundation models. The code of the toolkit is\navailable at https://github.com/GeWu-Lab/BalanceBenchmark.",
      "tldr_zh": "本论文调查了多模态学习（multimodal learning）中的多模态不平衡问题（multimodal imbalance problem），即某些模态主导而其他模态未被充分利用，并系统地将现有算法分类为四组，根据缓解不平衡的策略进行归纳。论文引入了 BalanceBenchmark，这是一个全面基准，包括多种多维数据集和从性能、不平衡度和复杂性三个角度的评估指标，并开发了一个模块化、可扩展的工具包以标准化实验流程，确保公平比较。通过实验分析，论文揭示了不同方法组在性能、平衡度和计算复杂性方面的关键优势，并期望这些洞见能启发未来更高效的算法和基础模型设计。工具包代码可从 https://github.com/GeWu-Lab/BalanceBenchmark 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10816v3",
      "published_date": "2025-02-15 14:42:42 UTC",
      "updated_date": "2025-02-23 10:19:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:48:06.338913"
    },
    {
      "arxiv_id": "2502.10807v2",
      "title": "HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Mingqian Ma",
        "Guoqing Liu",
        "Chuan Cao",
        "Pan Deng",
        "Tri Dao",
        "Albert Gu",
        "Peiran Jin",
        "Zhao Yang",
        "Yingce Xia",
        "Renqian Luo",
        "Pipi Hu",
        "Zun Wang",
        "Yuan-Jyue Chen",
        "Haiguang Liu",
        "Tao Qin"
      ],
      "abstract": "Advances in natural language processing and large language models have\nsparked growing interest in modeling DNA, often referred to as the \"language of\nlife\". However, DNA modeling poses unique challenges. First, it requires the\nability to process ultra-long DNA sequences while preserving single-nucleotide\nresolution, as individual nucleotides play a critical role in DNA function.\nSecond, success in this domain requires excelling at both generative and\nunderstanding tasks: generative tasks hold potential for therapeutic and\nindustrial applications, while understanding tasks provide crucial insights\ninto biological mechanisms and diseases. To address these challenges, we\npropose HybriDNA, a decoder-only DNA language model that incorporates a hybrid\nTransformer-Mamba2 architecture, seamlessly integrating the strengths of\nattention mechanisms with selective state-space models. This hybrid design\nenables HybriDNA to efficiently process DNA sequences up to 131kb in length\nwith single-nucleotide resolution. HybriDNA achieves state-of-the-art\nperformance across 33 DNA understanding datasets curated from the BEND, GUE,\nand LRB benchmarks, and demonstrates exceptional capability in generating\nsynthetic cis-regulatory elements (CREs) with desired properties. Furthermore,\nwe show that HybriDNA adheres to expected scaling laws, with performance\nimproving consistently as the model scales from 300M to 3B and 7B parameters.\nThese findings underscore HybriDNA's versatility and its potential to advance\nDNA research and applications, paving the way for innovations in understanding\nand engineering the \"language of life\".",
      "tldr_zh": "该研究提出HybriDNA，一种混合Transformer-Mamba2架构的解码器-only DNA语言模型，旨在解决DNA建模的挑战，包括处理长达131kb的超长序列并保持单核苷酸分辨率，同时在生成和理解任务上表现出色。HybriDNA通过整合注意力机制和选择性状态空间模型，实现了在33个DNA理解数据集（来自BEND、GUE和LRB基准）上的最先进性能，并能生成具有所需属性的合成顺式调控元件（CREs）。此外，模型遵循预期的缩放定律，随着参数从300M增加到3B和7B，性能持续提升，这为DNA研究和应用创新提供了重要潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page: https://hybridna-project.github.io/HybriDNA-Project/",
      "pdf_url": "http://arxiv.org/pdf/2502.10807v2",
      "published_date": "2025-02-15 14:23:43 UTC",
      "updated_date": "2025-02-18 02:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:48:18.239871"
    },
    {
      "arxiv_id": "2502.10803v1",
      "title": "PDA: Generalizable Detection of AI-Generated Images via Post-hoc Distribution Alignment",
      "title_zh": "PDA：通过事后分布对齐的可泛化AI生成图像检测",
      "authors": [
        "Li Wang",
        "Wenyu Chen",
        "Zheng Li",
        "Shanqing Guo"
      ],
      "abstract": "The rapid advancement of generative models has led to the proliferation of\nhighly realistic AI-generated images, posing significant challenges for\ndetection methods to generalize across diverse and evolving generative\ntechniques. Existing approaches often fail to adapt to unknown models without\ncostly retraining, limiting their practicability. To fill this gap, we propose\nPost-hoc Distribution Alignment (PDA), a novel approach for the generalizable\ndetection for AI-generated images. The key idea is to use the known generative\nmodel to regenerate undifferentiated test images. This process aligns the\ndistributions of the re-generated real images with the known fake images,\nenabling effective distinction from unknown fake images. PDA employs a two-step\ndetection framework: 1) evaluating whether a test image aligns with the known\nfake distribution based on deep k-nearest neighbor (KNN) distance, and 2)\nre-generating test images using known generative models to create pseudo-fake\nimages for further classification. This alignment strategy allows PDA to\neffectively detect fake images without relying on unseen data or requiring\nretraining. Extensive experiments demonstrate the superiority of PDA, achieving\n96.73\\% average accuracy across six state-of-the-art generative models,\nincluding GANs, diffusion models, and text-to-image models, and improving by\n16.07\\% over the best baseline. Through t-SNE visualizations and KNN distance\nanalysis, we provide insights into PDA's effectiveness in separating real and\nfake images. Our work provides a flexible and effective solution for real-world\nfake image detection, advancing the generalization ability of detection\nsystems.",
      "tldr_zh": "该研究针对AI生成图像检测的泛化挑战，提出了一种Post-hoc Distribution Alignment (PDA)方法，以应对现有检测技术对未知生成模型的适应性不足。PDA的核心机制是通过已知生成模型对测试图像进行再生，实现真实图像分布与已知假图像分布的对齐，并采用两步框架：首先使用deep k-nearest neighbor (KNN)距离评估图像与假分布的匹配度，其次生成伪假图像进行分类，从而无需重新训练即可检测未知假图像。实验结果显示，PDA在包括GANs、diffusion models和text-to-image models在内的六种先进生成模型上平均准确率达96.73%，比最佳基线提升16.07%，并通过t-SNE可视化和KNN距离分析证明了其在分离真实与假图像方面的有效性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10803v1",
      "published_date": "2025-02-15 13:55:34 UTC",
      "updated_date": "2025-02-15 13:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:48:30.905624"
    },
    {
      "arxiv_id": "2502.10802v1",
      "title": "CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Kefan Li",
        "Hongyue Yu",
        "Tingyu Guo",
        "Shijie Cao",
        "Yuan Yuan"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance in automated\ncode generation. However, existing approaches often rely heavily on pre-defined\ntest cases, which become impractical in scenarios where such cases are\nunavailable. While prior works explore filtering techniques between programs\nand test cases, they overlook the refinement of test cases. To address this\nlimitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that\nsimultaneously evolves programs and test cases. CoCoEvo eliminates the\ndependency on pre-defined test cases by generating both programs and test cases\ndirectly from natural language problem descriptions and function headers. The\nframework employs specialized evolutionary operators, including LLM-based\ncrossover and mutation operators for program evolution, along with a test case\ngeneration operator for test case evolution. Additionally, we propose\noptimization strategies such as a crossover rate scheduler to balance\nexploration and convergence, and a multi-objective optimization method for test\ncase selection. Experimental results on multiple state-of-the-art LLMs\ndemonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art\nperformance in automated code generation and testing. These results underscore\nthe potential of co-evolutionary techniques in advancing the field of automated\nprogramming.",
      "tldr_zh": "本文提出 CoCoEvo，一种基于大语言模型 (LLMs) 的共同进化框架，用于提升自动代码生成性能，通过同时进化程序和测试用例来解决现有方法对预定义测试用例的依赖问题。CoCoEvo 从自然语言问题描述和函数头直接生成程序和测试用例，采用 LLM 基于的交叉、变异操作符进行程序进化，以及专用测试用例生成操作符进行测试用例进化。该框架还引入优化策略，如交叉率调度器平衡探索与收敛，以及多目标优化方法选择测试用例。实验结果表明，CoCoEvo 在多个最先进 LLMs 上超越现有方法，实现了自动代码生成和测试的顶级性能，并展示了共同进化技术在自动编程领域的潜力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10802v1",
      "published_date": "2025-02-15 13:52:30 UTC",
      "updated_date": "2025-02-15 13:52:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:48:43.526581"
    },
    {
      "arxiv_id": "2502.10801v1",
      "title": "FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through Identity Obfuscation",
      "title_zh": "翻译失败",
      "authors": [
        "Li Wang",
        "Zheng Li",
        "Xuhong Zhang",
        "Shouling Ji",
        "Shanqing Guo"
      ],
      "abstract": "DeepFakes pose a significant threat to our society. One representative\nDeepFake application is face-swapping, which replaces the identity in a facial\nimage with that of a victim. Although existing methods partially mitigate these\nrisks by degrading the quality of swapped images, they often fail to disrupt\nthe identity transformation effectively. To fill this gap, we propose\nFaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake\nface-swapping threats. Specifically, FSG introduces imperceptible perturbations\nto a user's facial image, disrupting the features extracted by identity\nencoders. When shared online, these perturbed images mislead face-swapping\ntechniques, causing them to generate facial images with identities\nsignificantly different from the original user. Extensive experiments\ndemonstrate the effectiveness of FSG against multiple face-swapping techniques,\nreducing the face match rate from 90\\% (without defense) to below 10\\%. Both\nqualitative and quantitative studies further confirm its ability to confuse\nhuman perception, highlighting its practical utility. Additionally, we\ninvestigate key factors that may influence FSG and evaluate its robustness\nagainst various adaptive adversaries.",
      "tldr_zh": "这篇论文提出 FaceSwapGuard (FSG)，一种黑盒防御机制，用于通过身份混淆（Identity Obfuscation）保护用户面部隐私免受 DeepFake 威胁。FSG 在面部图像中引入不易察觉的 perturbations，破坏 identity encoders 提取的特征，从而使 face-swapping techniques 生成的图像身份与原用户显著不同。实验证明，FSG 对多种 face-swapping techniques 有效，将面部匹配率从 90% 降至 10% 以下，并在定性和定量研究中展示其迷惑人类感知的能力，同时评估了影响因素和鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10801v1",
      "published_date": "2025-02-15 13:45:19 UTC",
      "updated_date": "2025-02-15 13:45:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:48:54.656406"
    },
    {
      "arxiv_id": "2502.10793v1",
      "title": "Dynamic Influence Tracker: Measuring Time-Varying Sample Influence During Training",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Xu",
        "Zihan Wu"
      ],
      "abstract": "Existing methods for measuring training sample influence on models only\nprovide static, overall measurements, overlooking how sample influence changes\nduring training. We propose Dynamic Influence Tracker (DIT), which captures the\ntime-varying sample influence across arbitrary time windows during training.\n  DIT offers three key insights: 1) Samples show different time-varying\ninfluence patterns, with some samples important in the early training stage\nwhile others become important later. 2) Sample influences show a weak\ncorrelation between early and late stages, demonstrating that the model\nundergoes distinct learning phases with shifting priorities. 3) Analyzing\ninfluence during the convergence period provides more efficient and accurate\ndetection of corrupted samples than full-training analysis. Supported by\ntheoretical guarantees without assuming loss convexity or model convergence,\nDIT significantly outperforms existing methods, achieving up to 0.99\ncorrelation with ground truth and above 98\\% accuracy in detecting corrupted\nsamples in complex architectures.",
      "tldr_zh": "本研究提出 Dynamic Influence Tracker (DIT)，一种新方法，用于测量训练过程中样本影响的动态变化，而非现有方法的静态评估。DIT 揭示了三个关键洞见：样本的影响模式随时间变化，有些在早期训练阶段重要，而有些在后期变得关键；样本影响在不同阶段的相关性较弱，表明模型经历了多个学习阶段；以及在收敛期分析能更高效准确地检测损坏样本。DIT 基于理论保证（无需假设损失函数凸性或模型收敛），在复杂架构中显著优于现有方法，相关性高达 0.99，且检测损坏样本的准确率超过 98%。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10793v1",
      "published_date": "2025-02-15 13:24:21 UTC",
      "updated_date": "2025-02-15 13:24:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:49:05.669878"
    },
    {
      "arxiv_id": "2502.10776v1",
      "title": "A Distillation-based Future-aware Graph Neural Network for Stock Trend Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhipeng Liu",
        "Peibo Duan",
        "Mingyang Geng",
        "Bin Zhang"
      ],
      "abstract": "Stock trend prediction involves forecasting the future price movements by\nanalyzing historical data and various market indicators. With the advancement\nof machine learning, graph neural networks (GNNs) have been extensively\nemployed in stock prediction due to their powerful capability to capture\nspatiotemporal dependencies of stocks. However, despite the efforts of various\nGNN stock predictors to enhance predictive performance, the improvements remain\nlimited, as they focus solely on analyzing historical spatiotemporal\ndependencies, overlooking the correlation between historical and future\npatterns. In this study, we propose a novel distillation-based future-aware GNN\nframework (DishFT-GNN) for stock trend prediction. Specifically, DishFT-GNN\ntrains a teacher model and a student model, iteratively. The teacher model\nlearns to capture the correlation between distribution shifts of historical and\nfuture data, which is then utilized as intermediate supervision to guide the\nstudent model to learn future-aware spatiotemporal embeddings for accurate\nprediction. Through extensive experiments on two real-world datasets, we verify\nthe state-of-the-art performance of DishFT-GNN.",
      "tldr_zh": "本文提出了一种基于知识蒸馏的未来感知图神经网络框架（DishFT-GNN），用于股票趋势预测，以解决现有方法仅关注历史时空依赖性而忽略历史和未来模式相关性的问题。具体而言，该框架通过迭代训练教师模型和学生模型，让教师模型捕捉历史和未来数据分布偏移的相关性，作为中间监督指导学生模型生成更准确的未来感知时空嵌入。在两个真实数据集上的广泛实验中，DishFT-GNN 展示了最先进的预测性能，显著提升了股票趋势预测的准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.PM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10776v1",
      "published_date": "2025-02-15 11:44:15 UTC",
      "updated_date": "2025-02-15 11:44:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:49:18.020841"
    },
    {
      "arxiv_id": "2502.10768v1",
      "title": "Evaluating improvements on using Large Language Models (LLMs) for property extraction in the Open Research Knowledge Graph (ORKG)",
      "title_zh": "翻译失败",
      "authors": [
        "Sandra Schaftner"
      ],
      "abstract": "Current research highlights the great potential of Large Language Models\n(LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly\ncomplex step in this process is relation extraction, aimed at identifying\nsuitable properties to describe the content of research. This study builds\ndirectly on previous research of three Open Research Knowledge Graph (ORKG)\nteam members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and\nMistral for property extraction in scientific literature. Given the moderate\nperformance observed, the previous work concluded that fine-tuning is needed to\nimprove these models' alignment with scientific tasks and their emulation of\nhuman expertise. Expanding on this prior experiment, this study evaluates the\nimpact of advanced prompt engineering techniques and demonstrates that these\ntechniques can highly significantly enhance the results. Additionally, this\nstudy extends the property extraction process to include property matching to\nexisting ORKG properties, which are retrieved via the API. The evaluation\nreveals that results generated through advanced prompt engineering achieve a\nhigher proportion of matches with ORKG properties, further emphasizing the\nenhanced alignment achieved. Moreover, this lays the groundwork for addressing\nchallenges such as the inconsistency of ORKG properties, an issue highlighted\nin prior studies. By assigning unique URIs and using standardized terminology,\nthis work increases the consistency of the properties, fulfilling a crucial\naspect of Linked Data and FAIR principles - core commitments of ORKG. This, in\nturn, significantly enhances the applicability of ORKG content for subsequent\ntasks such as comparisons of research publications. Finally, the study\nconcludes with recommendations for future improvements in the overall property\nextraction process.",
      "tldr_zh": "本研究评估了高级提示工程（advanced prompt engineering）在Large Language Models (LLMs)用于Open Research Knowledge Graph (ORKG)属性提取（property extraction）中的改进，针对先前评估中GPT-3.5、Llama 2和Mistral的适中表现进行优化。研究扩展了属性提取过程，包括与ORKG属性的匹配，通过API检索和标准化术语，提升了属性的一致性并符合Linked Data和FAIR原则。结果显示，高级提示工程显著提高了提取准确性和匹配比例，为后续研究出版物比较等任务提供了更可靠的基础，并提出了未来改进建议。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10768v1",
      "published_date": "2025-02-15 11:17:37 UTC",
      "updated_date": "2025-02-15 11:17:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:49:29.794846"
    },
    {
      "arxiv_id": "2502.10762v1",
      "title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Guofu Xie",
        "Xiao Zhang",
        "Ting Yao",
        "Yunsheng Shi"
      ],
      "abstract": "User information needs are often highly diverse and varied. A key challenge\nin current research is how to achieve controllable multi-objective generation\nwhile enabling rapid adaptation to accommodate diverse user demands during test\ntime. Existing solutions, such as Rewarded Soup, focus on merging language\nmodels individually tuned on single objectives. While easy to implement and\nwidely used, these approaches face limitations in achieving optimal performance\ndue to their disregard for the impacts of competing objectives on model tuning.\nTo address this issue, we propose Bone Soup, a novel model merging approach\nthat first seeks a series of backbone models by considering the impacts of\nmultiple objectives and then makes the soup (i.e., merge the backbone models).\nSpecifically, Bone Soup begins by training multiple backbone models for\ndifferent objectives using multi-objective reinforcement learning. Each\nbackbone model is guided by a combination of backbone reward signals. To ensure\nthat these models are optimal for the Pareto front, the backbone rewards are\ncrafted by combining standard reward functions into basis vectors, which can\nthen be modified through a rule-based construction method. Bone Soup leverages\na symmetric circulant matrix mapping to generate the merging coefficients,\nwhich are used to merge the backbone models according to user preferences.\nExtensive experimental results demonstrate that Bone Soup exhibits strong\ncontrollability and Pareto optimality in controllable multi-objective\ngeneration, providing a more effective and efficient approach to addressing\ndiverse user needs at test time.",
      "tldr_zh": "该研究针对用户需求多样性的挑战，提出了Bone Soup方法，一种Seek-and-Soup模型合并策略，用于实现可控的多目标生成。该方法首先通过多目标强化学习训练多个骨干模型，每个模型使用结合标准奖励函数的骨干奖励信号（如基向量和规则构建）来考虑竞争目标的影响，确保Pareto最优；然后利用对称循环矩阵映射生成合并系数，根据用户偏好合并这些模型。与现有方法如Rewarded Soup相比，实验结果表明Bone Soup在可控多目标生成中表现出更强的可控性和Pareto最优性，提供了一种更高效的适应方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2502.10762v1",
      "published_date": "2025-02-15 11:00:36 UTC",
      "updated_date": "2025-02-15 11:00:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:49:42.620009"
    },
    {
      "arxiv_id": "2502.10750v1",
      "title": "Human-Centric Community Detection in Hybrid Metaverse Networks with Integrated AI Entities",
      "title_zh": "翻译失败",
      "authors": [
        "Shih-Hsuan Chiu",
        "Ya-Wen Teng",
        "De-Nian Yang",
        "Ming-Syan Chen"
      ],
      "abstract": "Community detection is a cornerstone problem in social network analysis\n(SNA), aimed at identifying cohesive communities with minimal external links.\nHowever, the rise of generative AI and Metaverse introduce complexities by\ncreating hybrid human-AI social networks (denoted by HASNs), where traditional\nmethods fall short, especially in human-centric settings. This paper introduces\na novel community detection problem in HASNs (denoted by MetaCD), which seeks\nto enhance human connectivity within communities while reducing the presence of\nAI nodes. Effective processing of MetaCD poses challenges due to the delicate\ntrade-off between excluding certain AI nodes and maintaining community\nstructure. To address this, we propose CUSA, an innovative framework\nincorporating AI-aware clustering techniques that navigate this trade-off by\nselectively retaining AI nodes that contribute to community integrity.\nFurthermore, given the scarcity of real-world HASNs, we devise four strategies\nfor synthesizing these networks under various hypothetical scenarios. Empirical\nevaluations on real social networks, reconfigured as HASNs, demonstrate the\neffectiveness and practicality of our approach compared to traditional non-deep\nlearning and graph neural network (GNN)-based methods.",
      "tldr_zh": "这篇论文针对混合人类-AI社交网络(HASNs)引入了MetaCD问题，旨在提升社区中人类连通性同时减少AI节点的影响，以应对传统社区检测方法在SNA中的局限性。作者提出了CUSA框架，该框架采用AI-aware clustering技术，通过选择性保留有助于社区完整性的AI节点来平衡排除AI和维护结构。此外，他们开发了四种合成HASNs策略，并在真实社交网络上进行实验，证明CUSA比传统非深度学习和GNN-based方法更有效。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "15 pages, Accepted for publication in the ACM WWW 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.10750v1",
      "published_date": "2025-02-15 10:21:10 UTC",
      "updated_date": "2025-02-15 10:21:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:49:55.377450"
    },
    {
      "arxiv_id": "2502.10749v1",
      "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
      "title_zh": "翻译失败",
      "authors": [
        "Zehua Liu",
        "Han Wu",
        "Yuxuan Yao",
        "Ruifeng She",
        "Xiongwei Han",
        "Tao Zhong",
        "Mingxuan Yuan"
      ],
      "abstract": "While most current approaches rely on further training techniques, such as\nfine-tuning or reinforcement learning, to enhance model capacities, model\nmerging stands out for its ability of improving models without requiring any\nadditional training. In this paper, we propose a unified framework for model\nmerging based on low-rank estimation of task vectors without the need for\naccess to the base model, named \\textsc{LoRE-Merging}. Our approach is\nmotivated by the observation that task vectors from fine-tuned models\nfrequently exhibit a limited number of dominant singular values, making\nlow-rank estimations less prone to interference. We implement the method by\nformulating the merging problem as an optimization problem. Extensive empirical\nexperiments demonstrate the effectiveness of our framework in mitigating\ninterference and preserving task-specific information, thereby advancing the\nstate-of-the-art performance in model merging techniques.",
      "tldr_zh": "本研究提出了一种名为 LoRE-Merging 的统一框架，用于 Large Language Model 的模型合并，该方法基于任务向量的 Low-Rank Estimation，不需额外训练或访问基模型，从而提升模型性能。研究观察到任务向量通常具有有限的显性奇异值，这使得低秩估计能有效减少干扰并保留任务特定信息。作者将模型合并问题转化为优化问题，并通过广泛实验证明，该框架显著降低了干扰，提升了模型合并技术的状态-of-the-art 性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10749v1",
      "published_date": "2025-02-15 10:18:46 UTC",
      "updated_date": "2025-02-15 10:18:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:50:05.382754"
    },
    {
      "arxiv_id": "2502.14883v1",
      "title": "Can LVLMs and Automatic Metrics Capture Underlying Preferences of Blind and Low-Vision Individuals for Navigational Aid?",
      "title_zh": "翻译失败",
      "authors": [
        "Na Min An",
        "Eunki Kim",
        "Wan Ju Kang",
        "Sangryul Kim",
        "Hyunjung Shim",
        "James Thorne"
      ],
      "abstract": "Vision is a primary means of how humans perceive the environment, but Blind\nand Low-Vision (BLV) people need assistance understanding their surroundings,\nespecially in unfamiliar environments. The emergence of semantic-based systems\nas assistance tools for BLV users has motivated many researchers to explore\nresponses from Large Vision-Language Models (LVLMs). However, it has yet been\nstudied preferences of BLV users on diverse types/styles of responses from\nLVLMs, specifically for navigational aid. To fill this gap, we first construct\nEye4B dataset, consisting of human-validated 1.1k curated outdoor/indoor scenes\nwith 5-10 relevant requests per scene. Then, we conduct an in-depth user study\nwith eight BLV users to evaluate their preferences on six LVLMs from five\nperspectives: Afraidness, Nonactionability, Sufficiency, and Conciseness.\nFinally, we introduce Eye4B benchmark for evaluating alignment between widely\nused model-based image-text metrics and our collected BLV preferences. Our work\ncan be set as a guideline for developing BLV-aware LVLMs towards a Barrier-Free\nAI system.",
      "tldr_zh": "这篇论文探讨了大型视觉语言模型（LVLMs）和自动指标是否能捕捉盲人和低视力（BLV）人士在导航辅助中的偏好，填补了现有研究空白。研究团队构建了 Eye4B 数据集，包含 1.1k 个人类验证的户外/室内场景，并通过涉及八位 BLV 用户的深入研究，评估了 LVLMs 响应在 Afraidness、Nonactionability、Sufficiency 和 Conciseness 等五个方面的偏好。最终，他们引入 Eye4B 基准来评估常用图像-文本指标与 BLV 偏好的对齐度，为开发 BLV-aware LVLMs 和无障碍 AI 系统提供了重要指导。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 12 figures, 14 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.14883v1",
      "published_date": "2025-02-15 10:17:52 UTC",
      "updated_date": "2025-02-15 10:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:50:19.344990"
    },
    {
      "arxiv_id": "2502.10742v1",
      "title": "The Philosophical Foundations of Growing AI Like A Child",
      "title_zh": "翻译失败",
      "authors": [
        "Dezhi Luo",
        "Yijiang Li",
        "Hokin Deng"
      ],
      "abstract": "Despite excelling in high-level reasoning, current language models lack\nrobustness in real-world scenarios and perform poorly on fundamental\nproblem-solving tasks that are intuitive to humans. This paper argues that both\nchallenges stem from a core discrepancy between human and machine cognitive\ndevelopment. While both systems rely on increasing representational power, the\nabsence of core knowledge-foundational cognitive structures in humans-prevents\nlanguage models from developing robust, generalizable abilities, where complex\nskills are grounded in simpler ones within their respective domains. It\nexplores empirical evidence of core knowledge in humans, analyzes why language\nmodels fail to acquire it, and argues that this limitation is not an inherent\narchitectural constraint. Finally, it outlines a workable proposal for\nsystematically integrating core knowledge into future multi-modal language\nmodels through the large-scale generation of synthetic training data using a\ncognitive prototyping strategy.",
      "tldr_zh": "这篇论文指出，当今语言模型在高级推理上表现出色，但缺乏真实场景的鲁棒性，并在人类直观的根本问题解决任务上表现不佳，这些问题源于人类和机器认知发展之间的核心差异，特别是语言模型缺少核心知识（core knowledge）——即基础认知结构，导致复杂技能无法在领域内从简单技能中泛化。论文通过分析人类核心知识的实证证据，探讨了语言模型为什么无法获取这一知识，并认为这不是固有的架构约束。最终，作者提出一个可行方案：通过大规模生成合成训练数据并采用认知原型策略（cognitive prototyping strategy），系统地整合核心知识到未来的多模态语言模型中，以提升其鲁棒性和泛化能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10742v1",
      "published_date": "2025-02-15 09:47:20 UTC",
      "updated_date": "2025-02-15 09:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:50:30.842985"
    },
    {
      "arxiv_id": "2502.12193v1",
      "title": "AI and the Law: Evaluating ChatGPT's Performance in Legal Classification",
      "title_zh": "人工智能与法律：评估 ChatGPT 在法律分类中的性能",
      "authors": [
        "Pawel Weichbroth"
      ],
      "abstract": "The use of ChatGPT to analyze and classify evidence in criminal proceedings\nhas been a topic of ongoing discussion. However, to the best of our knowledge,\nthis issue has not been studied in the context of the Polish language. This\nstudy addresses this research gap by evaluating the effectiveness of ChatGPT in\nclassifying legal cases under the Polish Penal Code. The results show excellent\nbinary classification accuracy, with all positive and negative cases correctly\ncategorized. In addition, a qualitative evaluation confirms that the legal\nbasis provided for each case, along with the relevant legal content, was\nappropriate. The results obtained suggest that ChatGPT can effectively analyze\nand classify evidence while applying the appropriate legal rules. In\nconclusion, ChatGPT has the potential to assist interested parties in the\nanalysis of evidence and serve as a valuable legal resource for individuals\nwith less experience or knowledge in this area.",
      "tldr_zh": "本研究评估了 ChatGPT 在波兰刑法下分类法律案例的性能，填补了波兰语语境中相关研究的空白。研究采用二元分类方法，结果显示所有正负案例均被正确分类，且提供的法律依据和相关内容均适当。总体而言，ChatGPT 展现出在证据分析中的有效性，可作为经验不足者的宝贵法律资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages; 1 figure; 2 tables; 32 references",
      "pdf_url": "http://arxiv.org/pdf/2502.12193v1",
      "published_date": "2025-02-15 09:28:52 UTC",
      "updated_date": "2025-02-15 09:28:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:50:41.873047"
    },
    {
      "arxiv_id": "2502.10732v1",
      "title": "Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents",
      "title_zh": "规则瓶颈强化学习：联合解释和决策优化，用于语言代理的资源分配",
      "authors": [
        "Mauricio Tec",
        "Guojun Xiong",
        "Haichuan Wang",
        "Francesca Dominici",
        "Milind Tambe"
      ],
      "abstract": "Deep Reinforcement Learning (RL) is remarkably effective in addressing\nsequential resource allocation problems in domains such as healthcare, public\npolicy, and resource management. However, deep RL policies often lack\ntransparency and adaptability, challenging their deployment alongside human\ndecision-makers. In contrast, Language Agents, powered by large language models\n(LLMs), provide human-understandable reasoning but may struggle with effective\ndecision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement\nLearning (RBRL), a novel framework that jointly optimizes decision and\nexplanations. At each step, RBRL generates candidate rules with an LLM, selects\namong them using an attention-based RL policy, and determines the environment\naction with an explanation via chain-of-thought reasoning. The RL rule\nselection is optimized using the environment rewards and an explainability\nmetric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's\ncompetitive performance with deep RL and efficiency gains over LLM fine-tuning.\nA survey further confirms the enhanced quality of its explanations.",
      "tldr_zh": "该论文提出 Rule-Bottleneck Reinforcement Learning (RBRL)，一个创新框架，结合强化学习和大型语言模型 (LLMs)，旨在优化资源分配问题的决策和解释。RBRL 在每个步骤使用 LLM 生成候选规则，通过 attention-based RL 政策选择规则，并借助 chain-of-thought reasoning 决定环境动作，同时基于环境奖励和 LLM 判断的解释性指标进行优化。实验结果显示，RBRL 在真实场景中与 Deep RL 性能相当，并比 LLM 微调更高效；一项调查进一步证实其解释质量得到显著提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10732v1",
      "published_date": "2025-02-15 09:01:31 UTC",
      "updated_date": "2025-02-15 09:01:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:50:54.673737"
    },
    {
      "arxiv_id": "2502.10725v3",
      "title": "PropNet: a White-Box and Human-Like Network for Sentence Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Fei Yang"
      ],
      "abstract": "Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks.",
      "tldr_zh": "该研究批评了基于 Transformer 的句子嵌入方法，尽管在 NLP 任务如语义文本相似性 (STS) 任务中表现突出，但其黑盒性质和依赖大量数据导致了偏见、信任和安全问题。作者提出 PropNet，一种纯 white-box 和 human-like 的句子表示网络，受认知科学启发，通过基于句子中 propositions 构建的层次化网络，实现固有的可解释性。虽然在 STS 任务上 PropNet 与 state-of-the-art 模型存在显著性能差距，但案例研究显示了改进潜力，并允许分析人类认知过程在这些基准中的作用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Clarified some ambiguities in the previous version",
      "pdf_url": "http://arxiv.org/pdf/2502.10725v3",
      "published_date": "2025-02-15 08:28:58 UTC",
      "updated_date": "2025-05-14 08:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:51:06.095805"
    },
    {
      "arxiv_id": "2502.10723v1",
      "title": "A Mathematics Framework of Artificial Shifted Population Risk and Its Further Understanding Related to Consistency Regularization",
      "title_zh": "翻译失败",
      "authors": [
        "Xiliang Yang",
        "Shenyang Deng",
        "Shicong Liu",
        "Yuanchi Suo",
        "Wing. W. Y NG",
        "Jianjun Zhang"
      ],
      "abstract": "Data augmentation is an important technique in training deep neural networks\nas it enhances their ability to generalize and remain robust. While data\naugmentation is commonly used to expand the sample size and act as a\nconsistency regularization term, there is a lack of research on the\nrelationship between them. To address this gap, this paper introduces a more\ncomprehensive mathematical framework for data augmentation. Through this\nframework, we establish that the expected risk of the shifted population is the\nsum of the original population risk and a gap term, which can be interpreted as\na consistency regularization term. The paper also provides a theoretical\nunderstanding of this gap, highlighting its negative effects on the early\nstages of training. We also propose a method to mitigate these effects. To\nvalidate our approach, we conducted experiments using same data augmentation\ntechniques and computing resources under several scenarios, including standard\ntraining, out-of-distribution, and imbalanced classification. The results\ndemonstrate that our methods surpass compared methods under all scenarios in\nterms of generalization ability and convergence stability. We provide our code\nimplementation at the following link: https://github.com/ydlsfhll/ASPR.",
      "tldr_zh": "该论文提出一个全面的数学框架，用于理解数据增强（data augmentation）在训练深度神经网络中的作用，特别是将其与一致性正则化（consistency regularization）相关联。通过该框架，作者证明了移位种群风险（shifted population risk）的预期风险等于原种群风险加上一个差距项，该差距项可解释为一致性正则化术语，并分析了其在训练早期阶段的负面影响。论文进一步提出方法来缓解这些负面效果，并通过实验在标准训练、分布外（out-of-distribution）和不平衡分类场景中验证了该方法的有效性，结果显示其在泛化能力和收敛稳定性上优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10723v1",
      "published_date": "2025-02-15 08:26:49 UTC",
      "updated_date": "2025-02-15 08:26:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:51:18.284564"
    },
    {
      "arxiv_id": "2502.12189v1",
      "title": "Self-supervised Attribute-aware Dynamic Preference Ranking Alignment",
      "title_zh": "自监督属性感知动态偏好排名对齐",
      "authors": [
        "Hongyu Yang",
        "Qi Zhao",
        "Zhenhua hu",
        "Rui Li"
      ],
      "abstract": "Reinforcement Learning from Human Feedback and its variants excel in aligning\nwith human intentions to generate helpful, harmless, and honest responses.\nHowever, most of them rely on costly human-annotated pairwise comparisons for\nsupervised alignment, which is not suitable for list-level scenarios, such as\ncommunity question answering. Additionally, human preferences are influenced by\nmultiple intrinsic factors in responses, leading to decision-making\ninconsistencies. Therefore, we propose \\textbf{Se}lf-supervised\n\\textbf{A}ttribute-aware \\textbf{d}ynamic \\textbf{p}reference \\textbf{ra}nking,\ncalled \\shortname. \\ It quantifies preference differences between responses\nbased on Attribute-Perceptual Distance Factors (APDF) and dynamically\ndetermines the list-wise alignment order. Furthermore, it achieves fine-grained\npreference difference learning and enables precise alignment with the optimal\none. We specifically constructed a challenging code preference dataset named\nStaCoCoQA, and introduced more cost-effective and scalable preference\nevaluation metrics: PrefHit and PrefRecall. Extensive experimental results show\nthat SeAdpra exhibits superior performance and generalizability on both\nStaCoCoQA and preference datasets from eight popular domains.",
      "tldr_zh": "该论文针对强化学习从人类反馈（RLHF）依赖昂贵标注且不适合列表级场景的问题，提出自监督的SeAdpra框架，通过Attribute-Perceptual Distance Factors (APDF)量化响应间的偏好差异，实现动态列表级偏好对齐。\nSeAdpra支持细粒度的偏好差异学习，并精确与最优响应对齐，提升决策一致性。\n作者构建了StaCoCoQA代码偏好数据集，并引入更经济有效的评估指标PrefHit和PrefRecall。\n实验结果表明，SeAdpra在StaCoCoQA及八个流行领域的偏好数据集上表现出色，具有优越性能和泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.12189v1",
      "published_date": "2025-02-15 08:20:42 UTC",
      "updated_date": "2025-02-15 08:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:51:31.040749"
    },
    {
      "arxiv_id": "2502.10718v1",
      "title": "Hyperdimensional Intelligent Sensing for Efficient Real-Time Audio Processing on Extreme Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Sanggeon Yun",
        "Ryozo Masukawa",
        "Hanning Chen",
        "SungHeon Jeong",
        "Wenjun Huang",
        "Arghavan Rezvani",
        "Minhyoung Na",
        "Yoshiki Yamaguchi",
        "Mohsen Imani"
      ],
      "abstract": "The escalating challenges of managing vast sensor-generated data,\nparticularly in audio applications, necessitate innovative solutions. Current\nsystems face significant computational and storage demands, especially in\nreal-time applications like gunshot detection systems (GSDS), and the\nproliferation of edge sensors exacerbates these issues. This paper proposes a\ngroundbreaking approach with a near-sensor model tailored for intelligent\naudio-sensing frameworks. Utilizing a Fast Fourier Transform (FFT) module,\nconvolutional neural network (CNN) layers, and HyperDimensional Computing\n(HDC), our model excels in low-energy, rapid inference, and online learning. It\nis highly adaptable for efficient ASIC design implementation, offering superior\nenergy efficiency compared to conventional embedded CPUs or GPUs, and is\ncompatible with the trend of shrinking microphone sensor sizes. Comprehensive\nevaluations at both software and hardware levels underscore the model's\nefficacy. Software assessments through detailed ROC curve analysis revealed a\ndelicate balance between energy conservation and quality loss, achieving up to\n82.1% energy savings with only 1.39% quality loss. Hardware evaluations\nhighlight the model's commendable energy efficiency when implemented via ASIC\ndesign, especially with the Google Edge TPU, showcasing its superiority over\nprevalent embedded CPUs and GPUs.",
      "tldr_zh": "该论文针对实时音频处理面临的计算和存储挑战，提出了一种基于 HyperDimensional Computing (HDC) 的智能音频感知框架，适用于极端边缘设备。该框架整合 Fast Fourier Transform (FFT) 模块和 convolutional neural network (CNN) 层，实现低能耗、快速推理以及在线学习，支持高效的 ASIC 设计。实验结果显示，软件评估通过 ROC curve 分析实现了高达 82.1% 的能耗节省，同时仅损失 1.39% 的质量；在硬件层面，该模型在 ASIC 和 Google Edge TPU 上表现出色，远优于传统嵌入式 CPU 或 GPU。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to IEEE Access",
      "pdf_url": "http://arxiv.org/pdf/2502.10718v1",
      "published_date": "2025-02-15 08:19:20 UTC",
      "updated_date": "2025-02-15 08:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:51:43.025261"
    },
    {
      "arxiv_id": "2502.12188v1",
      "title": "Boosting Generalization in Diffusion-Based Neural Combinatorial Solver via Energy-guided Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Lei",
        "Kaiwen Zhou",
        "Yinchuan Li",
        "Zhitang Chen",
        "Farzan Farnia"
      ],
      "abstract": "Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated\neffectiveness in solving NP-complete (NPC) problems by learning discrete\ndiffusion models for solution generation, eliminating hand-crafted domain\nknowledge. Despite their success, existing NCO methods face significant\nchallenges in both cross-scale and cross-problem generalization, and high\ntraining costs compared to traditional solvers. While recent studies have\nintroduced training-free guidance approaches that leverage pre-defined guidance\nfunctions for zero-shot conditional generation, such methodologies have not\nbeen extensively explored in combinatorial optimization. To bridge this gap, we\npropose a general energy-guided sampling framework during inference time that\nenhances both the cross-scale and cross-problem generalization capabilities of\ndiffusion-based NCO solvers without requiring additional training. We provide\ntheoretical analysis that helps understanding the cross-problem transfer\ncapability. Our experimental results demonstrate that a diffusion solver,\ntrained exclusively on the Traveling Salesman Problem (TSP), can achieve\ncompetitive zero-shot solution generation on TSP variants, such as Prize\nCollecting TSP (PCTSP) and the Orienteering Problem (OP), through energy-guided\nsampling across different problem scales.",
      "tldr_zh": "这篇论文针对扩散模型在神经组合优化 (NCO) 中的泛化挑战，提出了一种能量引导采样框架，以提升模型在跨规模和跨问题场景下的性能，而无需额外训练。框架在推理阶段利用预定义的能量函数进行指导，帮助解决 NP-complete (NPC) 问题。作者提供了理论分析，阐释了模型的跨问题转移能力。实验结果表明，只在 Traveling Salesman Problem (TSP) 上训练的模型，通过能量引导采样，能在 TSP 变体如 Prize Collecting TSP (PCTSP) 和 Orienteering Problem (OP) 上实现竞争性的零样本解决方案生成。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.12188v1",
      "published_date": "2025-02-15 08:04:00 UTC",
      "updated_date": "2025-02-15 08:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:51:54.789724"
    },
    {
      "arxiv_id": "2502.10712v2",
      "title": "FuncGenFoil: Airfoil Generation and Editing Model in Function Space",
      "title_zh": "FuncGenFoil：函数空间中的翼型生成和编辑模型",
      "authors": [
        "Jinouwen Zhang",
        "Junjie Ren",
        "Aobo Yang",
        "Yan Lu",
        "Lu Chen",
        "Hairun Xie",
        "Jing Wang",
        "Miao Zhang",
        "Wanli Ouyang",
        "Shixiang Tang"
      ],
      "abstract": "Aircraft manufacturing is the jewel in the crown of industry, among which\ngenerating high-fidelity airfoil geometries with controllable and editable\nrepresentations remains a fundamental challenge. While existing\ndeep-learning-based methods rely on predefined parametric function families,\ne.g., B\\'ezier curves and discrete point-based representations, they suffer\nfrom inherent trade-offs between expressiveness and resolution flexibility. To\ntackle this challenge, we introduce FuncGenFoil, a novel function-space\ngenerative model that directly learns functional airfoil geometries. Our method\ninherits both the advantages of arbitrary resolution sampling and the\nsmoothness of parametric functions, as well as the strong expressiveness of\ndiscrete point-based functions. Empirical evaluations on the AFBench dataset\ndemonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil\ngeneration by achieving a relative -74.4 label error reduction and +23.2\ndiversity increase on the AF-200K dataset. Our results highlight the advantages\nof function-space modeling for aerodynamic shape optimization, offering a\npowerful and flexible framework for high-fidelity airfoil design. Our code will\nbe released.",
      "tldr_zh": "这篇论文提出了FuncGenFoil，一种新型函数空间生成模型，用于生成和编辑高保真气泡几何形状，以解决现有深度学习方法（如依赖Bézier curves或离散点表示）在表达性和分辨率灵活性之间的权衡问题。该模型直接学习函数化的气泡几何，结合了任意分辨率采样、平滑性和强大表达性等优势。在AFBench数据集的实验中，FuncGenFoil在AF-200K上实现了标签错误率减少74.4%和多样性增加23.2%，显著优于现有方法。该框架为空气动力学形状优化提供了强大且灵活的工具，支持高保真气泡设计。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10712v2",
      "published_date": "2025-02-15 07:56:58 UTC",
      "updated_date": "2025-04-29 13:18:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:52:07.569563"
    },
    {
      "arxiv_id": "2502.10709v2",
      "title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
      "title_zh": "翻译失败",
      "authors": [
        "Qiujie Xie",
        "Qingqiu Li",
        "Zhuohao Yu",
        "Yuejie Zhang",
        "Yue Zhang",
        "Linyi Yang"
      ],
      "abstract": "As LLM-as-a-Judge emerges as a new paradigm for assessing large language\nmodels (LLMs), concerns have been raised regarding the alignment, bias, and\nstability of LLM evaluators. While substantial work has focused on alignment\nand bias, little research has concentrated on the stability of LLM evaluators.\nIn this paper, we conduct extensive experiments involving 9 widely used LLM\nevaluators across 2 different evaluation settings to investigate the\nuncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators\nexhibit varying uncertainty based on model families and sizes. With careful\ncomparative analyses, we find that employing special prompting strategies,\nwhether during inference or post-training, can alleviate evaluation uncertainty\nto some extent. By utilizing uncertainty to enhance LLM's reliability and\ndetection capability in Out-Of-Distribution (OOD) data, we further fine-tune an\nuncertainty-aware LLM evaluator named ConfiLM using a human-annotated\nfine-tuning set and assess ConfiLM's OOD evaluation ability on a manually\ndesigned test set sourced from the 2024 Olympics. Experimental results\ndemonstrate that incorporating uncertainty as additional information during the\nfine-tuning phase can largely improve the model's evaluation performance in OOD\nscenarios. The code and data are released at:\nhttps://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.",
      "tldr_zh": "这篇论文通过实证分析探讨了大型语言模型(LLM)评估器在LLM-as-a-Judge范式中的不确定性问题，涉及9个LLM评估器和2种评估设置。研究发现，不确定性因模型家族和大小而异，而采用特殊提示策略（如推理或后训练阶段）可以一定程度上缓解这一问题。作者开发了不确定性感知的LLM评估器ConfiLM，通过人类标注的微调集和加入不确定性信息，提升了模型在Out-Of-Distribution(ood)数据下的评估可靠性和性能。实验结果显示，这种方法显著改善了OOD场景下的评估准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.10709v2",
      "published_date": "2025-02-15 07:45:20 UTC",
      "updated_date": "2025-03-02 04:37:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:52:19.222550"
    },
    {
      "arxiv_id": "2502.10707v1",
      "title": "Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Jiarui Jin",
        "Haoyu Wang",
        "Hongyan Li",
        "Jun Li",
        "Jiahui Pan",
        "Shenda Hong"
      ],
      "abstract": "Electrocardiogram (ECG) is essential for the clinical diagnosis of\narrhythmias and other heart diseases, but deep learning methods based on ECG\noften face limitations due to the need for high-quality annotations. Although\nprevious ECG self-supervised learning (eSSL) methods have made significant\nprogress in representation learning from unannotated ECG data, they typically\ntreat ECG signals as ordinary time-series data, segmenting the signals using\nfixed-size and fixed-step time windows, which often ignore the form and rhythm\ncharacteristics and latent semantic relationships in ECG signals. In this work,\nwe introduce a novel perspective on ECG signals, treating heartbeats as words\nand rhythms as sentences. Based on this perspective, we first designed the\nQRS-Tokenizer, which generates semantically meaningful ECG sentences from the\nraw ECG signals. Building on these, we then propose HeartLang, a novel\nself-supervised learning framework for ECG language processing, learning\ngeneral representations at form and rhythm levels. Additionally, we construct\nthe largest heartbeat-based ECG vocabulary to date, which will further advance\nthe development of ECG language processing. We evaluated HeartLang across six\npublic ECG datasets, where it demonstrated robust competitiveness against other\neSSL methods. Our data and code are publicly available at\nhttps://github.com/PKUDigitalHealth/HeartLang.",
      "tldr_zh": "本文提出了一种创新视角，将心跳视为“words”和节奏视为“sentences”，以解决传统 ECG 自监督学习 (eSSL) 方法忽略 ECG 信号的形式、节奏特征和语义关系的局限性。研究设计了 QRS-Tokenizer 来从原始 ECG 信号生成语义有意义的 ECG 句子，并基于此提出 HeartLang 框架，一个用于 ECG 语言处理的预训练自监督学习模型，能够在形式和节奏级别学习通用表示。此外，作者构建了迄今为止最大的基于心跳的 ECG 词汇表，并在六个公共 ECG 数据集上评估，HeartLang 显示出与其它 eSSL 方法的强劲竞争力，数据和代码已公开。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 8 figures, accepted by International Conference on Learning\n  Representations 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.10707v1",
      "published_date": "2025-02-15 07:40:57 UTC",
      "updated_date": "2025-02-15 07:40:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:52:30.285730"
    },
    {
      "arxiv_id": "2502.10706v2",
      "title": "Raising the Bar in Graph OOD Generalization: Invariant Learning Beyond Explicit Environment Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Xu Shen",
        "Yixin Liu",
        "Yili Wang",
        "Rui Miao",
        "Yiwei Dai",
        "Shirui Pan",
        "Xin Wang"
      ],
      "abstract": "Out-of-distribution (OOD) generalization has emerged as a critical challenge\nin graph learning, as real-world graph data often exhibit diverse and shifting\nenvironments that traditional models fail to generalize across. A promising\nsolution to address this issue is graph invariant learning (GIL), which aims to\nlearn invariant representations by disentangling label-correlated invariant\nsubgraphs from environment-specific subgraphs. However, existing GIL methods\nface two major challenges: (1) the difficulty of capturing and modeling diverse\nenvironments in graph data, and (2) the semantic cliff, where invariant\nsubgraphs from different classes are difficult to distinguish, leading to poor\nclass separability and increased misclassifications. To tackle these\nchallenges, we propose a novel method termed Multi-Prototype Hyperspherical\nInvariant Learning (MPHIL), which introduces two key innovations: (1)\nhyperspherical invariant representation extraction, enabling robust and highly\ndiscriminative hyperspherical invariant feature extraction, and (2)\nmulti-prototype hyperspherical classification, which employs class prototypes\nas intermediate variables to eliminate the need for explicit environment\nmodeling in GIL and mitigate the semantic cliff issue. Derived from the\ntheoretical framework of GIL, we introduce two novel objective functions: the\ninvariant prototype matching loss to ensure samples are matched to the correct\nclass prototypes, and the prototype separation loss to increase the distinction\nbetween prototypes of different classes in the hyperspherical space. Extensive\nexperiments on 11 OOD generalization benchmark datasets demonstrate that MPHIL\nachieves state-of-the-art performance, significantly outperforming existing\nmethods across graph data from various domains and with different distribution\nshifts.",
      "tldr_zh": "这篇论文解决了图学习中 Out-of-Distribution (OOD) 泛化挑战，提出了一种新型方法 Multi-Prototype Hyperspherical Invariant Learning (MPHIL)，旨在超越传统 Graph Invariant Learning (GIL) 的显式环境建模限制。MPHIL 引入了超球面不变表示提取以获得鲁棒、高辨别性的特征，以及多原型超球面分类来使用类原型作为中间变量，缓解语义悬崖问题，并通过两个新目标函数（invariant prototype matching loss 和 prototype separation loss）提升类别的可分离性。实验结果显示，在 11 个 OOD 泛化基准数据集上，MPHIL 显著优于现有方法，实现了最先进的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10706v2",
      "published_date": "2025-02-15 07:40:14 UTC",
      "updated_date": "2025-02-19 02:41:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:52:43.743124"
    },
    {
      "arxiv_id": "2502.10705v1",
      "title": "CoPEFT: Fast Adaptation Framework for Multi-Agent Collaborative Perception with Parameter-Efficient Fine-Tuning",
      "title_zh": "CoPEFT：针对多智能体协作感知的参数高效微调快速适应框架",
      "authors": [
        "Quanmin Wei",
        "Penglin Dai",
        "Wei Li",
        "Bingyi Liu",
        "Xiao Wu"
      ],
      "abstract": "Multi-agent collaborative perception is expected to significantly improve\nperception performance by overcoming the limitations of single-agent perception\nthrough exchanging complementary information. However, training a robust\ncollaborative perception model requires collecting sufficient training data\nthat covers all possible collaboration scenarios, which is impractical due to\nintolerable deployment costs. Hence, the trained model is not robust against\nnew traffic scenarios with inconsistent data distribution and fundamentally\nrestricts its real-world applicability. Further, existing methods, such as\ndomain adaptation, have mitigated this issue by exposing the deployment data\nduring the training stage but incur a high training cost, which is infeasible\nfor resource-constrained agents. In this paper, we propose a\nParameter-Efficient Fine-Tuning-based lightweight framework, CoPEFT, for fast\nadapting a trained collaborative perception model to new deployment\nenvironments under low-cost conditions. CoPEFT develops a Collaboration Adapter\nand Agent Prompt to perform macro-level and micro-level adaptations separately.\nSpecifically, the Collaboration Adapter utilizes the inherent knowledge from\ntraining data and limited deployment data to adapt the feature map to new data\ndistribution. The Agent Prompt further enhances the Collaboration Adapter by\ninserting fine-grained contextual information about the environment. Extensive\nexperiments demonstrate that our CoPEFT surpasses existing methods with less\nthan 1\\% trainable parameters, proving the effectiveness and efficiency of our\nproposed method.",
      "tldr_zh": "本文提出 CoPEFT 框架，利用 Parameter-Efficient Fine-Tuning (PEFT) 技术，实现多智能体协作感知模型的快速适应，解决训练数据不足导致在新场景下不鲁棒的问题。CoPEFT 通过 Collaboration Adapter 进行宏观层面的特征图适应，利用训练数据和有限部署数据调整新数据分布；同时，Agent Prompt 插入细粒度环境上下文信息，进一步提升适应效果。实验证明，CoPEFT 在少于 1% 可训练参数的情况下，超越现有方法，如领域适应，展示了其高效性和实际部署潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2502.10705v1",
      "published_date": "2025-02-15 07:33:33 UTC",
      "updated_date": "2025-02-15 07:33:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:52:54.987960"
    },
    {
      "arxiv_id": "2502.10704v1",
      "title": "Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyang Zhao",
        "Gaofeng Meng",
        "Dong-Ming Yan"
      ],
      "abstract": "Non-rigid alignment of point clouds is crucial for scene understanding,\nreconstruction, and various computer vision and robotics tasks. Recent\nadvancements in implicit deformation networks for non-rigid registration have\nsignificantly reduced the reliance on large amounts of annotated training data.\nHowever, existing state-of-the-art methods still face challenges in handling\nocclusion scenarios. To address this issue, this paper introduces an innovative\nunsupervised method called Occlusion-Aware Registration (OAR) for non-rigidly\naligning point clouds. The key innovation of our method lies in the utilization\nof the adaptive correntropy function as a localized similarity measure,\nenabling us to treat individual points distinctly. In contrast to previous\napproaches that solely minimize overall deviations between two shapes, we\ncombine unsupervised implicit neural representations with the maximum\ncorrentropy criterion to optimize the deformation of unoccluded regions. This\neffectively avoids collapsed, tearing, and other physically implausible\nresults. Moreover, we present a theoretical analysis and establish the\nrelationship between the maximum correntropy criterion and the commonly used\nChamfer distance, highlighting that the correntropy-induced metric can be\nserved as a more universal measure for point cloud analysis. Additionally, we\nintroduce locally linear reconstruction to ensure that regions lacking\ncorrespondences between shapes still undergo physically natural deformations.\nOur method achieves superior or competitive performance compared to existing\napproaches, particularly when dealing with occluded geometries. We also\ndemonstrate the versatility of our method in challenging tasks such as large\ndeformations, shape interpolation, and shape completion under occlusion\ndisturbances.",
      "tldr_zh": "这篇论文提出了一种 occlusion-aware 非刚性点云配准方法，名为 Occlusion-Aware Registration (OAR)，通过无监督神经变形和自适应 correntropy 函数来处理点云配准中的遮挡问题。该方法利用最大 correntropy 准则优化未遮挡区域的变形，避免了崩塌或撕裂等不合理结果，并引入局部线性重建确保缺少对应区域的自然变形。论文还理论分析了 correntropy-induced metric 与 Chamfer distance 的关系，证明其更适用于点云分析，并在处理遮挡、大变形、形状插值和形状完成任务时表现出优越或竞争性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "[ICLR 2025] Project and code at: https://github.com/zikai1/OAReg",
      "pdf_url": "http://arxiv.org/pdf/2502.10704v1",
      "published_date": "2025-02-15 07:27:15 UTC",
      "updated_date": "2025-02-15 07:27:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:53:07.593776"
    },
    {
      "arxiv_id": "2502.10699v1",
      "title": "Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration",
      "title_zh": "探索大型语言模型中的突触",
      "authors": [
        "George Applegarth",
        "Christian Weatherstone",
        "Maximilian Hollingsworth",
        "Henry Middlebrook",
        "Marcus Irvin"
      ],
      "abstract": "Contextual memory integration remains a high challenge in the development of\nlanguage models, particularly in tasks that require maintaining coherence over\nextended sequences. Traditional approaches, such as self-attention mechanisms\nand memory-augmented architectures, often prioritize short-term dependencies,\nleading to fragmentation and inconsistency in long-range contextual\nunderstanding. Inspired by principles of synaptic plasticity observed in\nbiological neural systems, a novel mechanism, Synaptic Resonance, is introduced\nto dynamically reinforce relevant memory pathways during training and\ninference. Unlike static memory representations, this mechanism continuously\nadjusts synaptic weight matrices based on contextual relevance, allowing for\nimproved information retention without excessive computational overhead.\nEvaluations conducted on an open-source language model demonstrate reductions\nin perplexity, enhancements in contextual coherence, and increased robustness\nagainst input noise, highlighting the effectiveness of reinforcement-driven\nmemory modulation. Comparative analysis against baseline models further reveals\nthat the proposed approach achieves higher memory retention efficiency while\nmaintaining computational feasibility. The architectural modifications\nintegrate seamlessly into existing transformer-based frameworks, ensuring\nstable convergence and efficient inference without sacrificing scalability.\nApplications benefiting from improved long-term contextual consistency, such as\ndialogue systems and document summarization, stand to gain from this approach.\nEmpirical findings suggest that dynamically reinforced memory pathways offer a\npromising alternative to conventional memory mechanisms, addressing\nlongstanding limitations in extended sequence modeling.",
      "tldr_zh": "本文探讨了语言模型在处理长序列时面临的上下文记忆整合挑战，提出了一种受生物突触可塑性启发的创新机制——Synaptic Resonance。 该机制通过动态调整突触权重矩阵来强化相关记忆路径，实现信息保留的同时减少计算开销，并在训练和推理过程中提升长序列的上下文一致性。 实验结果显示，在开源语言模型上，Synaptic Resonance 显著降低了 perplexity、提高了对输入噪声的鲁棒性，并比传统 self-attention mechanisms 和记忆增强架构更高效。 这种方法可无缝集成到 Transformer 框架中，为对话系统和文档摘要等应用提供更可靠的长期上下文处理解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10699v1",
      "published_date": "2025-02-15 07:06:10 UTC",
      "updated_date": "2025-02-15 07:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:53:19.514040"
    },
    {
      "arxiv_id": "2502.10698v1",
      "title": "Superpose Singular Features for Model Merging",
      "title_zh": "翻译失败",
      "authors": [
        "Haiquan Qiu",
        "You Wu",
        "Quanming Yao"
      ],
      "abstract": "Model merging is a critical technique for combining the capabilities of\nmultiple fine-tuned models without requiring additional training. While\nexisting methods treat parameters as vectors, they overlook the intrinsic\nstructure of linear transformation matrices - the core components that comprise\nthe majority of model parameters. These matrices are fundamental to neural\nnetworks, mapping input representations to output features through linear\ncombinations. Motivated by the linear representation hypothesis, we introduce\ntask matrix and propose to Superpose Features from Task Matrix (SFTM), a novel\napproach that superposes features from individual task models into a merged\nmodel. SFTM employs singular value decomposition to identify feature bases of\nlinear transformation matrices and solves a linear system to optimally combine\nthem while preserving input-output mappings from individual task models.\nExtensive experiments on vision transformers and language models demonstrate\nthat our method consistently outperforms existing methods, achieving superior\nperformance and enhanced out-of-distribution generalization.",
      "tldr_zh": "模型合并是一种结合多个微调模型能力的技术，但现有方法忽略了线性变换矩阵的内在结构，导致潜在性能损失。  \n本文基于线性表示假设，引入任务矩阵并提出 SFTM（Superpose Features from Task Matrix）方法，该方法使用奇异值分解（singular value decomposition）识别线性变换矩阵的特征基，并通过解决线性系统来优化合并，确保保留各任务模型的输入-输出映射。  \n实验结果显示，SFTM 在视觉变压器和语言模型上显著优于现有方法，不仅提升了整体性能，还增强了分布外泛化（out-of-distribution generalization）能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 1 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10698v1",
      "published_date": "2025-02-15 07:05:55 UTC",
      "updated_date": "2025-02-15 07:05:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:53:32.026644"
    },
    {
      "arxiv_id": "2502.10694v1",
      "title": "Simulations of Common Unsupervised Domain Adaptation Algorithms for Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmad Chaddad",
        "Yihang Wu",
        "Yuchen Jiang",
        "Ahmed Bouridane",
        "Christian Desrosiers"
      ],
      "abstract": "Traditional machine learning assumes that training and test sets are derived\nfrom the same distribution; however, this assumption does not always hold in\npractical applications. This distribution disparity can lead to severe\nperformance drops when the trained model is used in new data sets. Domain\nadaptation (DA) is a machine learning technique that aims to address this\nproblem by reducing the differences between domains. This paper presents\nsimulation-based algorithms of recent DA techniques, mainly related to\nunsupervised domain adaptation (UDA), where labels are available only in the\nsource domain. Our study compares these techniques with public data sets and\ndiverse characteristics, highlighting their respective strengths and drawbacks.\nFor example, Safe Self-Refinement for Transformer-based DA (SSRT) achieved the\nhighest accuracy (91.6\\%) in the office-31 data set during our simulations,\nhowever, the accuracy dropped to 72.4\\% in the Office-Home data set when using\nlimited batch sizes. In addition to improving the reader's comprehension of\nrecent techniques in DA, our study also highlights challenges and upcoming\ndirections for research in this domain. The codes are available at\nhttps://github.com/AIPMLab/Domain_Adaptation.",
      "tldr_zh": "这篇论文探讨了机器学习中训练集和测试集分布不一致的问题，介绍了无监督领域适配（UDA）技术来减少分布差异并提升模型性能。通过模拟和比较最近的UDA算法，该研究使用公共数据集（如Office-31和Office-Home）评估了这些技术的优缺点，例如Safe Self-Refinement for Transformer-based DA (SSRT)在Office-31上达到91.6%的准确率，但在Office-Home上降至72.4%。论文不仅提升了对DA技术的理解，还指出了领域挑战和未来研究方向，并提供了开源代码。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in IEEE TIM",
      "pdf_url": "http://arxiv.org/pdf/2502.10694v1",
      "published_date": "2025-02-15 06:58:57 UTC",
      "updated_date": "2025-02-15 06:58:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:53:42.969150"
    },
    {
      "arxiv_id": "2502.10689v2",
      "title": "Self-Explaining Hypergraph Neural Networks for Diagnosis Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Leisheng Yu",
        "Yanxiao Cai",
        "Minxing Zhang",
        "Xia Hu"
      ],
      "abstract": "The burgeoning volume of electronic health records (EHRs) has enabled deep\nlearning models to excel in predictive healthcare. However, for high-stakes\napplications such as diagnosis prediction, model interpretability remains\nparamount. Existing deep learning diagnosis prediction models with intrinsic\ninterpretability often assign attention weights to every past diagnosis or\nhospital visit, providing explanations lacking flexibility and succinctness. In\nthis paper, we introduce SHy, a self-explaining hypergraph neural network\nmodel, designed to offer personalized, concise and faithful explanations that\nallow for interventions from clinical experts. By modeling each patient as a\nunique hypergraph and employing a message-passing mechanism, SHy captures\nhigher-order disease interactions and extracts distinct temporal phenotypes as\npersonalized explanations. It also addresses the incompleteness of the EHR data\nby accounting for essential false negatives in the original diagnosis record. A\nqualitative case study and extensive quantitative evaluations on two real-world\nEHR datasets demonstrate the superior predictive performance and\ninterpretability of SHy over existing state-of-the-art models.",
      "tldr_zh": "本文提出 SHy，一种自解释超图神经网络（Self-Explaining Hypergraph Neural Networks）模型，用于诊断预测，以解决现有深度学习模型在电子健康记录（EHRs）分析中解释性不足的问题。SHy 通过将每个患者建模为独特超图并采用消息传递机制（Message-Passing Mechanism），捕获更高阶的疾病交互、提取个性化的时序表型，并处理 EHR 数据中的关键假阴性，提供简洁、可靠的解释以支持临床专家干预。在两个真实世界 EHR 数据集上的定量和定性评估表明，SHy 在预测性能和可解释性方面均优于现有最先进模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10689v2",
      "published_date": "2025-02-15 06:33:02 UTC",
      "updated_date": "2025-05-01 03:27:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:53:55.912736"
    },
    {
      "arxiv_id": "2502.10678v1",
      "title": "GenComUI: Exploring Generative Visual Aids as Medium to Support Task-Oriented Human-Robot Communication",
      "title_zh": "翻译失败",
      "authors": [
        "Yate Ge",
        "Meiying Li",
        "Xipeng Huang",
        "Yuanda Hu",
        "Qi Wang",
        "Xiaohua Sun",
        "Weiwei Guo"
      ],
      "abstract": "This work investigates the integration of generative visual aids in\nhuman-robot task communication. We developed GenComUI, a system powered by\nlarge language models that dynamically generates contextual visual aids (such\nas map annotations, path indicators, and animations) to support verbal task\ncommunication and facilitate the generation of customized task programs for the\nrobot. This system was informed by a formative study that examined how humans\nuse external visual tools to assist verbal communication in spatial tasks. To\nevaluate its effectiveness, we conducted a user experiment (n = 20) comparing\nGenComUI with a voice-only baseline. The results demonstrate that generative\nvisual aids, through both qualitative and quantitative analysis, enhance verbal\ntask communication by providing continuous visual feedback, thus promoting\nnatural and effective human-robot communication. Additionally, the study offers\na set of design implications, emphasizing how dynamically generated visual aids\ncan serve as an effective communication medium in human-robot interaction.\nThese findings underscore the potential of generative visual aids to inform the\ndesign of more intuitive and effective human-robot communication, particularly\nfor complex communication scenarios in human-robot interaction and LLM-based\nend-user development.",
      "tldr_zh": "这篇论文探讨了生成视觉辅助在任务导向人类-机器人沟通中的作用，开发了GenComUI系统，该系统利用大型语言模型(LLMs)动态生成上下文视觉辅助（如地图标注、路径指示和动画）来增强口头沟通。基于一个成型研究，该系统支持机器人任务程序的定制化生成，并通过用户实验（n=20）与仅语音基线比较，结果显示视觉辅助提供了连续反馈，提升了沟通的自然性和有效性。论文还提出了设计启示，强调动态生成视觉辅助在复杂人类-机器人互动和LLM-based端用户开发中的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO",
        "H.5.2; H.5.3; I.2.7; I.2.0"
      ],
      "primary_category": "cs.HC",
      "comment": "To appear at ACM CHI '25",
      "pdf_url": "http://arxiv.org/pdf/2502.10678v1",
      "published_date": "2025-02-15 05:31:37 UTC",
      "updated_date": "2025-02-15 05:31:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:54:07.080209"
    },
    {
      "arxiv_id": "2502.12186v1",
      "title": "E2CB2former: Effecitve and Explainable Transformer for CB2 Receptor Ligand Activity Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Jiacheng Xie",
        "Yingrui Ji",
        "Linghuan Zeng",
        "Xi Xiao",
        "Gaofei Chen",
        "Lijing Zhu",
        "Joyanta Jyoti Mondal",
        "Jiansheng Chen"
      ],
      "abstract": "Accurate prediction of CB2 receptor ligand activity is pivotal for advancing\ndrug discovery targeting this receptor, which is implicated in inflammation,\npain management, and neurodegenerative conditions. Although conventional\nmachine learning and deep learning techniques have shown promise, their limited\ninterpretability remains a significant barrier to rational drug design. In this\nwork, we introduce CB2former, a framework that combines a Graph Convolutional\nNetwork with a Transformer architecture to predict CB2 receptor ligand\nactivity. By leveraging the Transformer's self attention mechanism alongside\nthe GCN's structural learning capability, CB2former not only enhances\npredictive performance but also offers insights into the molecular features\nunderlying receptor activity. We benchmark CB2former against diverse baseline\nmodels including Random Forest, Support Vector Machine, K Nearest Neighbors,\nGradient Boosting, Extreme Gradient Boosting, Multilayer Perceptron,\nConvolutional Neural Network, and Recurrent Neural Network and demonstrate its\nsuperior performance with an R squared of 0.685, an RMSE of 0.675, and an AUC\nof 0.940. Moreover, attention weight analysis reveals key molecular\nsubstructures influencing CB2 receptor activity, underscoring the model's\npotential as an interpretable AI tool for drug discovery. This ability to\npinpoint critical molecular motifs can streamline virtual screening, guide lead\noptimization, and expedite therapeutic development. Overall, our results\nshowcase the transformative potential of advanced AI approaches exemplified by\nCB2former in delivering both accurate predictions and actionable molecular\ninsights, thus fostering interdisciplinary collaboration and innovation in drug\ndiscovery.",
      "tldr_zh": "本研究提出 E2CB2former 框架，这是一种结合 Graph Convolutional Network (GCN) 和 Transformer 架构的有效且可解释模型，用于预测 CB2 受体配体活性，以推进针对炎症、疼痛管理和神经退行性疾病的药物发现。相比传统模型如 Random Forest、SVM 和 Multilayer Perceptron 等，E2CB2former 在基准测试中表现出色，达到 R squared 0.685、RMSE 0.675 和 AUC 0.940 的性能指标，并通过注意力权重分析揭示关键分子子结构的影响。总体而言，该框架不仅提升了预测准确性，还提供可行动的分子洞见，有助于虚拟筛选、lead 优化和治疗开发，推动药物发现领域的 AI 创新。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.12186v1",
      "published_date": "2025-02-15 05:05:49 UTC",
      "updated_date": "2025-02-15 05:05:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:54:19.275762"
    },
    {
      "arxiv_id": "2502.12185v1",
      "title": "Large Language Models for Extrapolative Modeling of Manufacturing Processes",
      "title_zh": "大型语言模型用于制造过程的外推建模",
      "authors": [
        "Kiarash Naghavi Khanghah",
        "Anandkumar Patel",
        "Rajiv Malhotra",
        "Hongyi Xu"
      ],
      "abstract": "Conventional predictive modeling of parametric relationships in manufacturing\nprocesses is limited by the subjectivity of human expertise and intuition on\nthe one hand and by the cost and time of experimental data generation on the\nother hand. This work addresses this issue by establishing a new Large Language\nModel (LLM) framework. The novelty lies in combining automatic extraction of\nprocess-relevant knowledge embedded in the literature with iterative model\nrefinement based on a small amount of experimental data. This approach is\nevaluated on three distinct manufacturing processes that are based on\nmachining, deformation, and additive principles. The results show that for the\nsame small experimental data budget the models derived by our framework have\nunexpectedly high extrapolative performance, often surpassing the capabilities\nof conventional Machine Learning. Further, our approach eliminates manual\ngeneration of initial models or expertise-dependent interpretation of the\nliterature. The results also reveal the importance of the nature of the\nknowledge extracted from the literature and the significance of both the\nknowledge extraction and model refinement components.",
      "tldr_zh": "本文提出一个新的 Large Language Model (LLM) 框架，用于制造过程的预测建模，旨在克服传统方法依赖人类专家主观性和实验数据生成成本高的局限性。该框架创新性地结合从文献中自动提取过程相关知识，并通过少量实验数据进行迭代模型精炼，在基于加工、变形和增材原理的三个制造过程中进行了评估。结果显示，与传统 Machine Learning 相比，该框架在相同数据预算下实现了更高的外推性能，往往超出预期。该方法还消除了手动初始模型生成的需求，并突出了知识提取性质以及提取和精炼组件的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.12185v1",
      "published_date": "2025-02-15 02:43:22 UTC",
      "updated_date": "2025-02-15 02:43:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:54:31.522125"
    },
    {
      "arxiv_id": "2502.10642v1",
      "title": "Demographic User Modeling for Social Robotics with Multimodal Pre-trained Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hamed Rahimi",
        "Mouad Abrini",
        "Mahdi Khoramshahi",
        "Mohamed Chetouani"
      ],
      "abstract": "This paper investigates the performance of multimodal pre-trained models in\nuser profiling tasks based on visual-linguistic demographic data. These models\nare critical for adapting to the needs and preferences of human users in social\nrobotics, thereby providing personalized responses and enhancing interaction\nquality. First, we introduce two datasets specifically curated to represent\ndemographic characteristics derived from user facial images. Next, we evaluate\nthe performance of a prominent contrastive multimodal pre-trained model, CLIP,\non these datasets, both in its out-of-the-box state and after fine-tuning.\nInitial results indicate that CLIP performs suboptimal in matching images to\ndemographic descriptions without fine-tuning. Although fine-tuning\nsignificantly enhances its predictive capacity, the model continues to exhibit\nlimitations in effectively generalizing subtle demographic nuances. To address\nthis, we propose adopting a masked image modeling strategy to improve\ngeneralization and better capture subtle demographic attributes. This approach\noffers a pathway for enhancing demographic sensitivity in multimodal user\nmodeling tasks.",
      "tldr_zh": "本论文探讨了多模态预训练模型在社交机器人用户画像任务中的性能，利用视觉-语言人口统计数据来适应用户需求，提供个性化响应并提升互动质量。研究者引入了两个基于用户面部图像的数据集，并评估了 CLIP 模型的原始状态和微调后表现，结果显示 CLIP 未微调时匹配图像到人口统计描述的效果较差，微调后虽有所改善，但仍无法有效泛化细微的 demographic nuances。为解决这一问题，论文提出采用 masked image modeling 策略，以增强模型的泛化和对人口统计属性的敏感性。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10642v1",
      "published_date": "2025-02-15 02:38:58 UTC",
      "updated_date": "2025-02-15 02:38:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:54:43.014190"
    },
    {
      "arxiv_id": "2502.10637v1",
      "title": "Proof of Response",
      "title_zh": "响应证明",
      "authors": [
        "Illia Polosukhin",
        "Alex Skidanov"
      ],
      "abstract": "We present a mechanism that for a network of participants allows one\nparticipant of the network (Alice) to request some data from another\nparticipant (Bob) and either receive a response from Bob within a\nknown-in-advance, bounded time b, or receive a proof that at least one edge on\nthe way to Bob was broken within b, or receive a streaming payment proportional\nto time passed beyond b during which neither was received. This mechanism\nallows for building downstream applications that require provable responses\nfrom other participants, such as decentralized storage solutions, decentralized\nAI agents, and more.",
      "tldr_zh": "该论文提出了一种名为 Proof of Response 的机制，用于网络参与者之间的数据请求，确保请求者（Alice）在预先设定的 bounded time b 内要么收到响应，要么获得证明至少一条通往响应者（Bob）的边已断开，要么收到与延迟时间成比例的 streaming payment。这种机制通过提供可验证的响应保障，解决了网络可靠性问题。最终，它可应用于构建下游系统，如去中心化存储解决方案和去中心化 AI 代理，提高了系统的可证明性和鲁棒性。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10637v1",
      "published_date": "2025-02-15 02:25:57 UTC",
      "updated_date": "2025-02-15 02:25:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:54:54.364509"
    },
    {
      "arxiv_id": "2502.10636v2",
      "title": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions",
      "title_zh": "USER-VLM 360：采用用户感知调优的个性化视觉语言模型，用于社交人类-机器人互动",
      "authors": [
        "Hamed Rahimi",
        "Adil Bahaj",
        "Mouad Abrini",
        "Mahdi Khoramshahi",
        "Mounir Ghogho",
        "Mohamed Chetouani"
      ],
      "abstract": "The integration of vision-language models into robotic systems constitutes a\nsignificant advancement in enabling machines to interact with their\nsurroundings in a more intuitive manner. While VLMs offer rich multimodal\nreasoning, existing approaches lack user-specific adaptability, often relying\non generic interaction paradigms that fail to account for individual\nbehavioral, contextual, or socio-emotional nuances. When customization is\nattempted, ethical concerns arise from unmitigated biases in user data, risking\nexclusion or unfair treatment. To address these dual challenges, we propose\nUser-VLM 360{\\deg}, a holistic framework integrating multimodal user modeling\nwith bias-aware optimization. Our approach features: (1) user-aware tuning that\nadapts interactions in real time using visual-linguistic signals; (2) bias\nmitigation via preference optimization; and (3) curated 360{\\deg} socio-emotive\ninteraction datasets annotated with demographic, emotion, and relational\nmetadata. Evaluations across eight benchmarks demonstrate state-of-the-art\nresults: +35.3% F1 in personalized VQA, +47.5% F1 in facial features\nunderstanding, 15% bias reduction, and 30X speedup over baselines. Ablation\nstudies confirm component efficacy, and deployment on the Pepper robot\nvalidates real-time adaptability across diverse users. We open-source\nparameter-efficient 3B/10B models and an ethical verification framework for\nresponsible adaptation.",
      "tldr_zh": "该研究提出 User-VLM 360 框架，旨在解决视觉语言模型 (VLMs) 在社交人机交互中的用户特定适应性不足和偏见问题，通过整合多模态用户建模、用户感知调优以及偏见缓解机制来实现个性化交互。该框架利用标注了人口统计、情感和关系元数据的 360° 社会情感交互数据集，支持实时适应视觉-语言信号，并在 Pepper 机器人上验证了其有效性。在八个基准测试中，User-VLM 360 取得了显著提升，包括 +35.3% F1 在个性化 VQA、+47.5% F1 在面部特征理解、15% 偏见减少和 30X 速度提升，并开源参数高效的 3B/10B 模型及道德验证框架以促进负责任的应用。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10636v2",
      "published_date": "2025-02-15 02:25:49 UTC",
      "updated_date": "2025-02-28 09:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:55:08.390786"
    },
    {
      "arxiv_id": "2502.10631v1",
      "title": "ControllableGPT: A Ground-Up Designed Controllable GPT for Molecule Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Xuefeng Liu",
        "Songhao Jiang",
        "Bo Li",
        "Rick Stevens"
      ],
      "abstract": "Large Language Models (LLMs) employ three popular training approaches: Masked\nLanguage Models (MLM), Causal Language Models (CLM), and Sequence-to-Sequence\nModels (seq2seq). However, each approach has its strengths and limitations, and\nfaces challenges in addressing specific tasks that require controllable and\nbidirectional generation, such as drug optimization. To address this challenge,\ninspired by the biological processes of growth and evolution, which involve the\nexpansion, shrinking, and mutation of sequences, we introduce ControllableGPT.\nThis initiative represents the first effort to combine the advantages of MLM,\nCLM, and seq2seq into a single unified, controllable GPT framework. It enables\nthe precise management of specific locations and ranges within a sequence,\nallowing for expansion, reduction, or mutation over chosen or random lengths,\nwhile maintaining the integrity of any specified positions or subsequences. In\nthis work, we designed ControllableGPT for drug optimization from the ground\nup, which included proposing the Causally Masked Seq2seq (CMS) objective,\ndeveloping the training corpus, introducing a novel pre-training approach, and\ndevising a unique generation process. We demonstrate the effectiveness and\ncontrollability of ControllableGPT by conducting experiments on drug\noptimization tasks for both viral and cancer benchmarks, surpassing competing\nbaselines.",
      "tldr_zh": "该研究提出 ControllableGPT，一种从零设计的可控 GPT 框架，旨在解决大型语言模型如 MLM、CLM 和 seq2seq 在分子优化任务中的局限性，通过结合这些方法的优势实现序列的可控双向生成。\n受生物生长和进化的启发，ControllableGPT 引入 Causally Masked Seq2seq (CMS) 目标、新的训练语料库和预训练方法，允许精确管理序列中的特定位置进行扩展、缩小或变异，同时保持指定部分的完整性。\n实验在病毒和癌症药物优化基准上证明了其有效性和可控性，超过了现有基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10631v1",
      "published_date": "2025-02-15 01:49:35 UTC",
      "updated_date": "2025-02-15 01:49:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:55:19.921649"
    },
    {
      "arxiv_id": "2502.10626v2",
      "title": "K-Edit: Language Model Editing with Contextual Knowledge Awareness",
      "title_zh": "翻译失败",
      "authors": [
        "Elan Markowitz",
        "Anil Ramakrishna",
        "Ninareh Mehrabi",
        "Charith Peris",
        "Rahul Gupta",
        "Kai-Wei Chang",
        "Aram Galstyan"
      ],
      "abstract": "As the world changes, we need to be able to update our models and correct\nfalse information without costly retraining. Knowledge-based model editing\nenables precise modifications to the weights of large language models in order\nto modify the information encoded within. Recent approaches have seen success\nin enabling recall of edited information for thousands of edits at once.\nHowever, these approaches fail to produce edits that account for associated\ncontextual information. We present K-Edit, an effective approach to generating\ncontextually consistent knowledge edits. By using knowledge graphs, which\nmaintain contextual consistency when an edge is edited, we are able to generate\nadditional \\textit{contextual edits} that ensure consistency of related\ninformation in the language model. Our experiments demonstrate significant\nimprovements in multi-hop question answering while maintaining the general\neffectiveness and scalability of model edits.",
      "tldr_zh": "这篇论文提出了 K-Edit，一种针对大型语言模型的知识编辑方法，能够在不进行昂贵重新训练的情况下，更新模型并修正错误信息，同时考虑相关上下文的一致性。K-Edit 通过利用知识图谱（knowledge graphs）生成额外的上下文编辑，确保编辑后的信息与关联知识保持连贯，从而改善多跳问答任务的性能。实验结果显示，K-Edit 在维持编辑有效性和可扩展性的基础上，显著提升了模型的表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10626v2",
      "published_date": "2025-02-15 01:35:13 UTC",
      "updated_date": "2025-02-27 06:59:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:55:30.088861"
    },
    {
      "arxiv_id": "2502.10624v1",
      "title": "Network evasion detection with Bi-LSTM model",
      "title_zh": "翻译失败",
      "authors": [
        "Kehua Chen",
        "Jingping Jia"
      ],
      "abstract": "Network evasion detection aims to distinguish whether the network flow comes\nfrom link layer exists network evasion threat, which is a means to disguise the\ndata traffic on detection system by confusing the signature. Since the previous\nresearch works has all sorts of frauds, we propose a architecture with deep\nlearning network to handle this problem. In this paper, we extract the critical\ninformation as key features from data frame and also specifically propose to\nuse bidirectional long short-term memory (Bi-LSTM) neural network which shows\nan outstanding performance to trace the serial information, to encode both the\npast and future trait on the network flows. Furthermore we introduce a\nclassifier named Softmax at the bottom of Bi-LSTM, holding a character to\nselect the correct class. All experiments results shows that we can achieve a\nsignificant performance with a deep Bi-LSTM in network evasion detection and\nit's average accuracy reaches 96.1%.",
      "tldr_zh": "本论文针对网络逃避检测问题，提出了一种基于双向长短时记忆网络（Bi-LSTM）的架构，用于区分网络流量是否包含链路层逃避威胁，该威胁通过混淆签名伪装数据流量。方法包括从数据帧中提取关键特征，并利用 Bi-LSTM 编码网络流量的过去和未来序列信息，再结合 Softmax 分类器进行准确分类。实验结果显示，该模型在网络逃避检测中平均准确率达到 96.1%，显著优于现有方法。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "4 pages,5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10624v1",
      "published_date": "2025-02-15 01:25:13 UTC",
      "updated_date": "2025-02-15 01:25:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:55:43.004544"
    },
    {
      "arxiv_id": "2502.10620v1",
      "title": "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis",
      "title_zh": "ProMRVL-CAD：主动对话系统，基于多轮视觉-语言交互，用于计算机辅助诊断",
      "authors": [
        "Xueshen Li",
        "Xinlong Hou",
        "Ziyi Huang",
        "Yu Gan"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
      "tldr_zh": "该研究提出ProMRVL-CAD，一种基于LLMs的主动对话系统，通过多轮视觉语言交互来辅助医疗诊断，旨在解决现有系统被动交互和忽略医疗历史的问题。该系统整合知识图谱和推荐系统，包含Proactive Question Generator (Pro-Q Gen)来生成引导诊断的主动问题，以及Multi-Vision Patient-Text Diagnostic Report Generator (MVP-DR Gen)来产生高质量的患者友好诊断报告。在MIMIC-CXR和IU-Xray数据集上的评估显示，ProMRVL-CAD在报告生成质量上优于基线模型，尤其在低图像质量场景下表现稳健；此外，研究还创建了一个合成医疗对话数据集，用于训练LLMs。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.10620v1",
      "published_date": "2025-02-15 01:14:23 UTC",
      "updated_date": "2025-02-15 01:14:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:55:55.123649"
    },
    {
      "arxiv_id": "2502.10614v1",
      "title": "Optimizing CNN Architectures for Advanced Thoracic Disease Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Tejas Mirthipati"
      ],
      "abstract": "Machine learning, particularly convolutional neural networks (CNNs), has\nshown promise in medical image analysis, especially for thoracic disease\ndetection using chest X-ray images. In this study, we evaluate various CNN\narchitectures, including binary classification, multi-label classification, and\nResNet50 models, to address challenges like dataset imbalance, variations in\nimage quality, and hidden biases. We introduce advanced preprocessing\ntechniques such as principal component analysis (PCA) for image compression and\npropose a novel class-weighted loss function to mitigate imbalance issues. Our\nresults highlight the potential of CNNs in medical imaging but emphasize that\nissues like unbalanced datasets and variations in image acquisition methods\nmust be addressed for optimal model performance.",
      "tldr_zh": "本研究优化了 CNN 架构，用于先进的胸部疾病分类，特别针对胸部 X 光图像的检测问题。研究评估了二元分类、多标签分类和 ResNet50 模型，并引入了高级预处理技术如 PCA 用于图像压缩，以及一个新的 class-weighted loss function 来缓解数据集不平衡问题。结果表明，CNNs 在医疗成像中具有巨大潜力，但需解决图像质量变化和隐藏偏差等挑战，以实现最佳模型性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10614v1",
      "published_date": "2025-02-15 00:27:37 UTC",
      "updated_date": "2025-02-15 00:27:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:56:06.212548"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 60,
  "processed_papers_count": 60,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T11:56:20.174212"
}
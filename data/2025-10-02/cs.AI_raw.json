[
  {
    "arxiv_id": "2510.03358v1",
    "title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility",
    "authors": [
      "Annan Yu",
      "Danielle C. Maddix",
      "Boran Han",
      "Xiyuan Zhang",
      "Abdul Fatir Ansari",
      "Oleksandr Shchur",
      "Christos Faloutsos",
      "Andrew Gordon Wilson",
      "Michael W. Mahoney",
      "Yuyang Wang"
    ],
    "abstract": "Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly to models trained to other modalities. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data differ remarkably from those of text or vision. We show that time-series embeddings, unlike text or vision, exhibit sharply decaying singular value spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why ranks grow with depth. Guided by these theoretical and empirical results, we use these insights to compress Chronos, a large time series foundation model, achieving a reduction of $65\\%$ in inference time and $81\\%$ in memory, without loss of accuracy. Our findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "42 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.03358v1",
    "published_date": "2025-10-02 23:56:17 UTC",
    "updated_date": "2025-10-02 23:56:17 UTC"
  },
  {
    "arxiv_id": "2510.02611v1",
    "title": "On the Role of Temperature Sampling in Test-Time Scaling",
    "authors": [
      "Yuheng Wu",
      "Azalia Mirhoseini",
      "Thierry Tambe"
    ],
    "abstract": "Large language models (LLMs) can improve reasoning at inference time through test-time scaling (TTS), where multiple reasoning traces are generated and the best one is selected. Prior work shows that increasing the number of samples K steadily improves accuracy. In this paper, we demonstrate that this trend does not hold indefinitely: at large K, further scaling yields no gains, and certain hard questions remain unsolved regardless of the number of traces. Interestingly, we find that different sampling temperatures solve different subsets of problems, implying that single-temperature scaling explores only part of a model's potential. We therefore propose scaling along the temperature dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3 (0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME 2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an additional 7.3 points over single-temperature TTS. Temperature scaling also enables base models to reach performance comparable to reinforcement learning (RL)-trained counterparts, without additional post-training. We further provide a comprehensive analysis of this phenomenon and design a multi-temperature voting method that reduces the overhead of temperature scaling. Overall, our findings suggest that TTS is more powerful than previously thought, and that temperature scaling offers a simple and effective way to unlock the latent potential of base models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02611v1",
    "published_date": "2025-10-02 23:09:56 UTC",
    "updated_date": "2025-10-02 23:09:56 UTC"
  },
  {
    "arxiv_id": "2510.02610v2",
    "title": "MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection",
    "authors": [
      "Taurai Muvunza",
      "Egor Kraev",
      "Pere Planell-Morell",
      "Alexander Y. Shestopaloff"
    ],
    "abstract": "Existing feature filters rely on statistical pair-wise dependence metrics to model feature-target relationships, but this approach may fail when the target depends on higher-order feature interactions rather than individual contributions. We introduce Mutual Information Neural Estimation Regularized Vetting Algorithm (MINERVA), a novel approach to supervised feature selection based on neural estimation of mutual information between features and targets. We paramaterize the approximation of mutual information with neural networks and perform feature selection using a carefully designed loss function augmented with sparsity-inducing regularizers. Our method is implemented in a two-stage process to decouple representation learning from feature selection, ensuring better generalization and a more accurate expression of feature importance. We present examples of ubiquitous dependency structures that are rarely captured in literature and show that our proposed method effectively captures these complex feature-target relationships by evaluating feature subsets as an ensemble. Experimental results on synthetic and real-life fraud datasets demonstrate the efficacy of our method and its ability to perform exact solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.02610v2",
    "published_date": "2025-10-02 23:09:06 UTC",
    "updated_date": "2025-10-06 09:40:13 UTC"
  },
  {
    "arxiv_id": "2510.02608v2",
    "title": "Mitigating Modal Imbalance in Multimodal Reasoning",
    "authors": [
      "Chen Henry Wu",
      "Neil Kale",
      "Aditi Raghunathan"
    ],
    "abstract": "Foundation models (FMs) deployed in real-world tasks such as computer-use agents must integrate diverse modalities. How good are FMs at performing joint reasoning, simultaneously reasoning over multiple modalities, especially when the modalities interact and relate to each other to form cross-modal context? To better understand this problem, we study FMs on cross-modal conflicts: scenarios where conflicting evidence is presented across modalities. This allows us to examine whether FMs prioritize one modality over another or reason jointly to reconcile the conflict. Our experiments reveal that FMs can recognize conflicts in unimodal contexts, composed of a single modality, 90% of the time, but the ratio falls as low as 3% when evidence is split across modalities -- similar observations hold in cross-lingual contexts, composed of multiple languages. We trace this failure to cross-modal attention imbalance, showing that FMs exhibit extreme asymmetry in attention scores, disproportionately prioritizing certain modalities. We show that cross-modal attention imbalance does not go away by simply scaling up multimodal or multilingual datasets blindly, since they lack training examples that explicitly require cross-modal reasoning. We demonstrate that even a simple and scalable method of explicitly combining multiple modalities within each training instance significantly reduces attention imbalance. Reduced attention imbalance directly translates to improved downstream performance on several vision-language benchmarks. Our findings underscore the importance of systematically addressing cross-modal contexts to build reliable foundation models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 10 figures, CoLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.02608v2",
    "published_date": "2025-10-02 22:58:28 UTC",
    "updated_date": "2025-10-06 02:10:36 UTC"
  },
  {
    "arxiv_id": "2511.17506v1",
    "title": "AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks",
    "authors": [
      "Narjes Nourzad",
      "Mingyu Zong",
      "Bhaskar Krishnamachari"
    ],
    "abstract": "Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) supports localized adaptation, yet coordination at scale remains challenging. We present AURA, a framework that integrates cloud-based LLMs for high-level planning with base stations modeled as MARL agents for local decision-making. The LLM generates objectives and subgoals from its understanding of the environment and reasoning capabilities, while agents at base stations execute these objectives autonomously, guided by a trust mechanism that balances local learning with external input. To reduce latency, AURA employs batched communication so that agents update the LLM's view of the environment and receive improved feedback. In a simulated 6G scenario, AURA improves resilience, reducing dropped handoff requests by more than half under normal and high traffic and lowering system failures. Agents use LLM input in fewer than 60\\% of cases, showing that guidance augments rather than replaces local adaptability, thereby mitigating latency and hallucination risks. These results highlight the promise of combining LLM reasoning with MARL adaptability for scalable, real-time NextG network management.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.17506v1",
    "published_date": "2025-10-02 22:43:47 UTC",
    "updated_date": "2025-10-02 22:43:47 UTC"
  },
  {
    "arxiv_id": "2510.02592v1",
    "title": "Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs",
    "authors": [
      "Jean Douglas Carvalho",
      "Hugo Kenji",
      "Ahmad Mohammad Saber",
      "Glaucia Melo",
      "Max Mauro Dias Santos",
      "Deepa Kundur"
    ],
    "abstract": "The integration of electric vehicles (EVs) into smart grids presents unique opportunities to enhance both transportation systems and energy networks. However, ensuring safe and interpretable interactions between drivers, vehicles, and the surrounding environment remains a critical challenge. This paper presents a multi-modal large language model (LLM)-based framework to process multimodal sensor data - such as object detection, semantic segmentation, and vehicular telemetry - and generate natural-language alerts for drivers. The framework is validated using real-world data collected from instrumented vehicles driving on urban roads, ensuring its applicability to real-world scenarios. By combining visual perception (YOLOv8), geocoded positioning, and CAN bus telemetry, the framework bridges raw sensor data and driver comprehension, enabling safer and more informed decision-making in urban driving scenarios. Case studies using real data demonstrate the framework's effectiveness in generating context-aware alerts for critical situations, such as proximity to pedestrians, cyclists, and other vehicles. This paper highlights the potential of LLMs as assistive tools in e-mobility, benefiting both transportation systems and electric networks by enabling scalable fleet coordination, EV load forecasting, and traffic-aware energy planning.\n  Index Terms - Electric vehicles, visual perception, large language models, YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been presented at the 2025 IEEE PES Conference on Innovative Smart Grid Technologies (ISGT 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.02592v1",
    "published_date": "2025-10-02 21:50:31 UTC",
    "updated_date": "2025-10-02 21:50:31 UTC"
  },
  {
    "arxiv_id": "2510.02589v1",
    "title": "A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem",
    "authors": [
      "Yunqi Huang",
      "Nishith Chennakeshava",
      "Alexis Carras",
      "Vladislav Neverov",
      "Wei Liu",
      "Aske Plaat",
      "Yingjie Fan"
    ],
    "abstract": "Container stowage planning (CSPP) is a critical component of maritime transportation and terminal operations, directly affecting supply chain efficiency. Owing to its complexity, CSPP has traditionally relied on human expertise. While reinforcement learning (RL) has recently been applied to CSPP, systematic benchmark comparisons across different algorithms remain limited. To address this gap, we develop a Gym environment that captures the fundamental features of CSPP and extend it to include crane scheduling in both multi-agent and single-agent formulations. Within this framework, we evaluate five RL algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying complexity. The results reveal distinct performance gaps with increasing complexity, underscoring the importance of algorithm choice and problem formulation for CSPP. Overall, this paper benchmarks multiple RL methods for CSPP while providing a reusable Gym environment with crane scheduling, thus offering a foundation for future research and practical deployment in maritime logistics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02589v1",
    "published_date": "2025-10-02 21:47:33 UTC",
    "updated_date": "2025-10-02 21:47:33 UTC"
  },
  {
    "arxiv_id": "2510.02571v1",
    "title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty",
    "authors": [
      "Zhiting Mei",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02571v1",
    "published_date": "2025-10-02 21:20:41 UTC",
    "updated_date": "2025-10-02 21:20:41 UTC"
  },
  {
    "arxiv_id": "2510.02567v3",
    "title": "Agentic Additive Manufacturing Alloy Evaluation",
    "authors": [
      "Peter Pak",
      "Achuth Chandrasekhar",
      "Amir Barati Farimani"
    ],
    "abstract": "Agentic systems enable the intelligent use of research tooling, augmenting a researcher's ability to investigate and propose novel solutions to existing problems. Within Additive Manufacturing (AM), alloy selection and evaluation remains a complex challenge, often requiring expertise in the various domains of materials science, thermodynamic simulations, and experimental analysis. Large Language Model (LLM) enabled agents can facilitate this endeavor by utilizing their extensive knowledge base to dispatch tool calls via Model Context Protocol (MCP) to perform actions such as thermophysical property diagram calculations and lack of fusion process map generation. In addition, the multi-agent system can effectively reason through complex user prompts and provide analysis on the lack of fusion process window of common alloys such as SS316L and IN718 along with proposed composition variants of known alloys. These agents can dynamically adjust their task trajectory to the outcomes of tool call results, effectively enabling autonomous decision-making in practical environments. This work aims to showcase the benefits of adopting a LLM enabled multi-agent system to automate and accelerate the task of evaluating proposed additive manufacturing alloys, both novel and known.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02567v3",
    "published_date": "2025-10-02 21:06:04 UTC",
    "updated_date": "2026-01-06 15:11:53 UTC"
  },
  {
    "arxiv_id": "2510.02561v1",
    "title": "Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback",
    "authors": [
      "Derek Shi",
      "Ruben Glatt",
      "Christine Klymko",
      "Shubham Mohole",
      "Hongjun Choi",
      "Shashank Kushwaha",
      "Sam Sakla",
      "Felipe Leno da Silva"
    ],
    "abstract": "Recent advances in large video-language models (VLMs) rely on extensive fine-tuning techniques that strengthen alignment between textual and visual comprehension. Leading pipelines typically pair supervised fine-tuning (SFT) with reinforcement learning from preference data to enhance video comprehension. However, as VLMs scale in parameter size, so does the cost of gathering enough human feedback. To make fine-tuning more cost-effective, recent frameworks explore reinforcement learning with AI feedback (RLAIF), which replace human preference with AI as a judge. Current RLAIF frameworks rely on a specialized reward model trained with video narratives to create calibrated scalar rewards -- an expensive and restrictive pipeline. We propose Oracle-RLAIF, a novel framework that replaces the trained reward model with a more general Oracle ranker which acts as a drop-in model ranking candidate model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce $GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware advantages. Empirically, we demonstrate that Oracle-RLAIF consistently outperforms leading VLMs using existing fine-tuning methods when evaluated across various video comprehension benchmarks. Oracle-RLAIF paves the path to creating flexible and data-efficient frameworks for aligning large multi-modal video models with reinforcement learning from rank rather than score.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Proceedings of the 39th Annual Conference on Neural Information Processing Systems, ARLET Workshop (Aligning Reinforcement Learning Experimentalists and Theorists)",
    "pdf_url": "https://arxiv.org/pdf/2510.02561v1",
    "published_date": "2025-10-02 20:57:10 UTC",
    "updated_date": "2025-10-02 20:57:10 UTC"
  },
  {
    "arxiv_id": "2510.02557v1",
    "title": "Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge",
    "authors": [
      "Charlie Masters",
      "Advaith Vellanki",
      "Jiangbo Shangguan",
      "Bart Kultys",
      "Jonathan Gilmore",
      "Alastair Moore",
      "Stefano V. Albrecht"
    ],
    "abstract": "While agentic AI has advanced in automating individual tasks, managing complex multi-agent workflows remains a challenging problem. This paper presents a research vision for autonomous agentic systems that orchestrate collaboration within dynamic human-AI teams. We propose the Autonomous Manager Agent as a core challenge: an agent that decomposes complex goals into task graphs, allocates tasks to human and AI workers, monitors progress, adapts to changing conditions, and maintains transparent stakeholder communication. We formalize workflow management as a Partially Observable Stochastic Game and identify four foundational challenges: (1) compositional reasoning for hierarchical decomposition, (2) multi-objective optimization under shifting preferences, (3) coordination and planning in ad hoc teams, and (4) governance and compliance by design. To advance this agenda, we release MA-Gym, an open-source simulation and evaluation framework for multi-agent workflow orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we find they struggle to jointly optimize for goal completion, constraint adherence, and workflow runtime - underscoring workflow management as a difficult open problem. We conclude with organizational and ethical implications of autonomous management systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as an oral paper for the conference for Distributed Artificial Intelligence (DAI 2025). 8 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02557v1",
    "published_date": "2025-10-02 20:51:39 UTC",
    "updated_date": "2025-10-02 20:51:39 UTC"
  },
  {
    "arxiv_id": "2510.02554v1",
    "title": "ToolTweak: An Attack on Tool Selection in LLM-based Agents",
    "authors": [
      "Jonathan Sneh",
      "Ruomei Yan",
      "Jialin Yu",
      "Philip Torr",
      "Yarin Gal",
      "Sunando Sengupta",
      "Eric Sommerlade",
      "Alasdair Paren",
      "Adel Bibi"
    ],
    "abstract": "As LLMs increasingly power agents that interact with external tools, tool use has become an essential mechanism for extending their capabilities. These agents typically select tools from growing databases or marketplaces to solve user tasks, creating implicit competition among tool providers and developers for visibility and usage. In this paper, we show that this selection process harbors a critical vulnerability: by iteratively manipulating tool names and descriptions, adversaries can systematically bias agents toward selecting specific tools, gaining unfair advantage over equally capable alternatives. We present ToolTweak, a lightweight automatic attack that increases selection rates from a baseline of around 20% to as high as 81%, with strong transferability between open-source and closed-source models. Beyond individual tools, we show that such attacks cause distributional shifts in tool usage, revealing risks to fairness, competition, and security in emerging tool ecosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and perplexity filtering, which reduce bias and lead agents to select functionally similar tools more equally. All code will be open-sourced upon acceptance.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02554v1",
    "published_date": "2025-10-02 20:44:44 UTC",
    "updated_date": "2025-10-02 20:44:44 UTC"
  },
  {
    "arxiv_id": "2510.02549v1",
    "title": "Knowledge-Graph Based RAG System Evaluation Framework",
    "authors": [
      "Sicheng Dong",
      "Vahid Zolfaghari",
      "Nenad Petrovic",
      "Alois Knoll"
    ],
    "abstract": "Large language models (LLMs) has become a significant research focus and is utilized in various fields, such as text generation and dialog systems. One of the most essential applications of LLM is Retrieval Augmented Generation (RAG), which greatly enhances generated content's reliability and relevance. However, evaluating RAG systems remains a challenging task. Traditional evaluation metrics struggle to effectively capture the key features of modern LLM-generated content that often exhibits high fluency and naturalness. Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended this framework into a KG-based evaluation paradigm, enabling multi-hop reasoning and semantic community clustering to derive more comprehensive scoring metrics. By incorporating these comprehensive evaluation criteria, we gain a deeper understanding of RAG systems and a more nuanced perspective on their performance. To validate the effectiveness of our approach, we compare its performance with RAGAS scores and construct a human-annotated subset to assess the correlation between human judgments and automated metrics. In addition, we conduct targeted experiments to demonstrate that our KG-based evaluation method is more sensitive to subtle semantic differences in generated outputs. Finally, we discuss the key challenges in evaluating RAG systems and highlight potential directions for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02549v1",
    "published_date": "2025-10-02 20:36:21 UTC",
    "updated_date": "2025-10-02 20:36:21 UTC"
  },
  {
    "arxiv_id": "2510.03352v1",
    "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction",
    "authors": [
      "Mahdi Farahbakhsh",
      "Vishnu Teja Kunde",
      "Dileep Kalathil",
      "Krishna Narayanan",
      "Jean-Francois Chamberland"
    ],
    "abstract": "Diffusion models have emerged as powerful priors for solving inverse problems. However, existing approaches typically overlook side information that could significantly improve reconstruction quality, especially in severely ill-posed settings. In this work, we propose a novel inference-time search algorithm that guides the sampling process using the side information in a manner that balances exploration and exploitation. This enables more accurate and reliable reconstructions, providing an alternative to the gradient-based guidance that is prone to reward-hacking artifacts. Our approach can be seamlessly integrated into a wide range of existing diffusion-based image reconstruction pipelines. Through extensive experiments on a number of inverse problems, such as box inpainting, super-resolution, and various deblurring tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that our approach consistently improves the qualitative and quantitative performance of diffusion-based image reconstruction algorithms. We also show the superior performance of our approach with respect to other baselines, including reward gradient-based guidance algorithms. The code is available at \\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this repository}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03352v1",
    "published_date": "2025-10-02 20:16:00 UTC",
    "updated_date": "2025-10-02 20:16:00 UTC"
  },
  {
    "arxiv_id": "2510.02535v2",
    "title": "PHORECAST: Enabling AI Understanding of Public Health Outreach Across Populations",
    "authors": [
      "Rifaa Qadri",
      "Anh Nhat Nhu",
      "Swati Ramnath",
      "Laura Yu Zheng",
      "Raj Bhansali",
      "Sylvette La Touche-Howard",
      "Tracy Marie Zeeger",
      "Tom Goldstein",
      "Ming Lin"
    ],
    "abstract": "Understanding how diverse individuals and communities respond to persuasive messaging holds significant potential for advancing personalized and socially aware machine learning. While Large Vision and Language Models (VLMs) offer promise, their ability to emulate nuanced, heterogeneous human responses, particularly in high stakes domains like public health, remains underexplored due in part to the lack of comprehensive, multimodal dataset. We introduce PHORECAST (Public Health Outreach REceptivity and CAmpaign Signal Tracking), a multimodal dataset curated to enable fine-grained prediction of both individuallevel behavioral responses and community-wide engagement patterns to health messaging. This dataset supports tasks in multimodal understanding, response prediction, personalization, and social forecasting, allowing rigorous evaluation of how well modern AI systems can emulate, interpret, and anticipate heterogeneous public sentiment and behavior. By providing a new dataset to enable AI advances for public health, PHORECAST aims to catalyze the development of models that are not only more socially aware but also aligned with the goals of adaptive and inclusive health communication",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02535v2",
    "published_date": "2025-10-02 20:10:46 UTC",
    "updated_date": "2025-10-15 21:13:31 UTC"
  },
  {
    "arxiv_id": "2510.02528v1",
    "title": "Multimodal Function Vectors for Spatial Relations",
    "authors": [
      "Shuhao Fu",
      "Esther Goldberg",
      "Ying Nian Wu",
      "Hongjing Lu"
    ],
    "abstract": "Large Multimodal Models (LMMs) demonstrate impressive in-context learning abilities from limited multimodal demonstrations, yet the internal mechanisms supporting such task learning remain opaque. Building on prior work of large language models, we show that a small subset of attention heads in the vision-language model OpenFlamingo-4B is responsible for transmitting representations of spatial relations. The activations of these attention heads, termed function vectors, can be extracted and manipulated to alter an LMM's performance on relational tasks. First, using both synthetic and real image datasets, we apply causal mediation analysis to identify attention heads that strongly influence relational predictions, and extract multimodal function vectors that improve zero-shot accuracy at inference time. We further demonstrate that these multimodal function vectors can be fine-tuned with a modest amount of training data, while keeping LMM parameters frozen, to significantly outperform in-context learning baselines. Finally, we show that relation-specific function vectors can be linearly combined to solve analogy problems involving novel and untrained spatial relations, highlighting the strong generalization ability of this approach. Our results show that LMMs encode spatial relational knowledge within localized internal structures, which can be systematically extracted and optimized, thereby advancing our understanding of model modularity and enhancing control over relational reasoning in LMMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02528v1",
    "published_date": "2025-10-02 19:55:56 UTC",
    "updated_date": "2025-10-02 19:55:56 UTC"
  },
  {
    "arxiv_id": "2510.03351v1",
    "title": "Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks",
    "authors": [
      "Song Wang",
      "Zhenyu Lei",
      "Zhen Tan",
      "Jundong Li",
      "Javier Rasero",
      "Aiying Zhang",
      "Chirag Agarwal"
    ],
    "abstract": "Nearly one in five adolescents currently live with a diagnosed mental or behavioral health condition, such as anxiety, depression, or conduct disorder, underscoring the urgency of developing accurate and interpretable diagnostic tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a powerful lens into large-scale functional connectivity, where brain regions are modeled as nodes and inter-regional synchrony as edges, offering clinically relevant biomarkers for psychiatric disorders. While prior works use graph neural network (GNN) approaches for disorder prediction, they remain complex black-boxes, limiting their reliability and clinical translation. In this work, we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages large language models (LLMs) and neurobiological domain knowledge to automatically generate, filter, and encode interpretable functional connectivity concepts. Each concept is represented as a structured subgraph linking specific brain regions, which are then passed through a concept classifier. Our design ensures predictions through clinically meaningful connectivity patterns, enabling both interpretability and strong predictive performance. Extensive experiments across multiple psychiatric disorder datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform their vanilla counterparts, improving accuracy while providing transparent, clinically aligned explanations. Furthermore, concept analyses highlight disorder-specific connectivity patterns that align with expert knowledge and suggest new hypotheses for future investigation, establishing CONCEPTNEURO as an interpretable, domain-informed framework for psychiatric disorder diagnosis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03351v1",
    "published_date": "2025-10-02 19:38:46 UTC",
    "updated_date": "2025-10-02 19:38:46 UTC"
  },
  {
    "arxiv_id": "2510.17830v2",
    "title": "Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy",
    "authors": [
      "Meir H. Shachar",
      "Dane M. Sterbentz",
      "Harshitha Menon",
      "Charles F. Jekel",
      "M. Giselle Fern√°ndez-Godino",
      "Nathan K. Brown",
      "Ismael D. Boureima",
      "Yue Hao",
      "Kevin Korner",
      "Robert Rieben",
      "Daniel A. White",
      "William J. Schill",
      "Jonathan L. Belof"
    ],
    "abstract": "Inertial fusion energy promises nearly unlimited, clean power if it can be achieved. However, the design and engineering of fusion systems requires controlling and manipulating matter at extreme energies and timescales; the shock physics and radiation transport governing the physical behavior under these conditions are complex requiring the development, calibration, and use of predictive multiphysics codes to navigate the highly nonlinear and multi-faceted design landscape. We hypothesize that artificial intelligence reasoning models can be combined with physics codes and emulators to autonomously design fusion fuel capsules. In this article, we construct a multi-agent system where natural language is utilized to explore the complex physics regimes around fusion energy. The agentic system is capable of executing a high-order multiphysics inertial fusion computational code. We demonstrate the capacity of the multi-agent design assistant to both collaboratively and autonomously manipulate, navigate, and optimize capsule geometry while accounting for high fidelity physics that ultimately achieve simulated ignition via inverse design.",
    "categories": [
      "physics.app-ph",
      "cs.AI"
    ],
    "primary_category": "physics.app-ph",
    "comment": "Corrected the author's list metadata to match that found in the paper",
    "pdf_url": "https://arxiv.org/pdf/2510.17830v2",
    "published_date": "2025-10-02 19:15:09 UTC",
    "updated_date": "2025-10-22 01:49:42 UTC"
  },
  {
    "arxiv_id": "2510.02484v1",
    "title": "From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning",
    "authors": [
      "Rafael Rodriguez-Sanchez",
      "Cameron Allen",
      "George Konidaris"
    ],
    "abstract": "Algorithms that exploit factored Markov decision processes are far more sample-efficient than factor-agnostic methods, yet they assume a factored representation is known a priori -- a requirement that breaks down when the agent sees only high-dimensional observations. Conversely, deep reinforcement learning handles such inputs but cannot benefit from factored structure. We address this representation problem with Action-Controllable Factorization (ACF), a contrastive learning approach that uncovers independently controllable latent variables -- state components each action can influence separately. ACF leverages sparsity: actions typically affect only a subset of variables, while the rest evolve under the environment's dynamics, yielding informative data for contrastive training. ACF recovers the ground truth controllable factors directly from pixel observations on three benchmarks with known factored structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently outperforming baseline disentanglement algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02484v1",
    "published_date": "2025-10-02 18:43:20 UTC",
    "updated_date": "2025-10-02 18:43:20 UTC"
  },
  {
    "arxiv_id": "2510.02483v1",
    "title": "Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework",
    "authors": [
      "Nii Osae Osae Dade",
      "Moinul Hossain Rahat"
    ],
    "abstract": "Training Large Language Models (LLMs) is plagued by long training times and massive energy consumption, with modern models requiring months of computation and gigawatt-hours of electricity. In light of these challenges,we introduce Litespark, a novel pre-training framework that addresses these inefficiencies through targeted optimizations to transformer attention and MLP layers. Our approach combines architectural improvements with algorithmic enhancements to maximize Model FLOPs Utilization (MFU) while maintaining compatibility with standard transformer implementations. Comprehensive benchmarking on 3B and 30B parameter Llama models using the SlimPajama-627B dataset demonstrates substantial performance gains: 2x-6x training throughput improvement and $55\\%-83$% energy consumption reduction across multi-node H200 GPU clusters. These optimizations are model- and hardware-agnostic, enabling broad applicability across transformer architectures and extending to post-training phases including supervised fine-tuning and direct preference optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.02483v1",
    "published_date": "2025-10-02 18:42:07 UTC",
    "updated_date": "2025-10-02 18:42:07 UTC"
  },
  {
    "arxiv_id": "2510.02480v1",
    "title": "Safe and Efficient In-Context Learning via Risk Control",
    "authors": [
      "Andrea Wynn",
      "Metod Jazbec",
      "Charith Peris",
      "Rinat Khaziev",
      "Anqi Liu",
      "Daniel Khashabi",
      "Eric Nalisnick"
    ],
    "abstract": "Large language models (LLMs) demonstrate a remarkable ability to learn new tasks from a few in-context examples. However, this flexibility introduces safety concerns: LLMs can be influenced by incorrect or malicious demonstrations -- for example, if an adversary tampers with or injects harmful examples without a human supervisor noticing. This motivates principled designs in which the system itself includes built-in mechanisms to guard against such attacks. We propose a novel approach to limit the degree to which harmful demonstrations can degrade model performance. First, we define a baseline ``safe'' behavior for the model -- the model's performance given no in-context demonstrations (zero-shot). Next, we apply distribution-free risk control (DFRC) to control the extent to which in-context samples can decay performance below zero-shot. We achieve this by leveraging dynamic early exit prediction, ignoring later attention heads that attend the most to the unsafe inputs. Finally, we propose modifications to DFRC that allow it to both control risk for harmful inputs \\textit{and} leverage performance and efficiency gains on helpful inputs. We present both theoretical and empirical results showing that our approach can effectively control risk for harmful in-context demonstrations while simultaneously achieving substantial computational efficiency gains with helpful demonstrations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02480v1",
    "published_date": "2025-10-02 18:36:10 UTC",
    "updated_date": "2025-10-02 18:36:10 UTC"
  },
  {
    "arxiv_id": "2510.21740v1",
    "title": "Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models",
    "authors": [
      "Alexa R. Tartaglini",
      "Satchel Grant",
      "Daniel Wurgaft",
      "Christopher Potts",
      "Judith E. Fan"
    ],
    "abstract": "Data visualizations are vital components of many scientific articles and news stories. Current vision-language models (VLMs) still struggle on basic data visualization understanding tasks, but the causes of failure remain unclear. Are VLM failures attributable to limitations in how visual information in the data visualization is encoded, how information is transferred between the vision and language modules, or how information is processed within the language module? We developed FUGU, a suite of data visualization understanding tasks, to precisely characterize potential sources of difficulty (e.g., extracting the position of data points, distances between them, and other summary statistics). We used FUGU to investigate three widely used VLMs. To diagnose the sources of errors produced by these models, we used activation patching and linear probes to trace information flow through models across a variety of prompting strategies. We found that some models fail to generate the coordinates of individual data points correctly, and these initial errors often lead to erroneous final responses. When these models are provided with the correct coordinates, performance improves substantially. Moreover, even when the model generates an incorrect response, the correct coordinates can be successfully read out from the latent representations in the vision encoder, suggesting that the source of these errors lies in the vision-language handoff. We further found that while providing correct coordinates helps with tasks involving one or a small number of data points, it generally worsens performance for tasks that require extracting statistical relationships across many data points. Fine-tuning models on FUGU also fails to yield ceiling performance. These findings point to architectural constraints in current VLMs that might pose significant challenges for reliable data visualization understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21740v1",
    "published_date": "2025-10-02 18:29:07 UTC",
    "updated_date": "2025-10-02 18:29:07 UTC"
  },
  {
    "arxiv_id": "2510.02469v1",
    "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
    "authors": [
      "Sung-Yeon Park",
      "Adam Lee",
      "Juanwu Lu",
      "Can Cui",
      "Luyang Jiang",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ahmadreza Moradipari",
      "Ziran Wang"
    ],
    "abstract": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02469v1",
    "published_date": "2025-10-02 18:22:03 UTC",
    "updated_date": "2025-10-02 18:22:03 UTC"
  },
  {
    "arxiv_id": "2510.02463v2",
    "title": "CLARITY: Clinical Assistant for Routing, Inference, and Triage",
    "authors": [
      "Vladimir Shaposhnikov",
      "Aleksandr Nesterov",
      "Ilia Kopanichuk",
      "Ivan Bakulin",
      "Egor Zhelvakov",
      "Ruslan Abramov",
      "Ekaterina Tsapieva",
      "Iaroslav Bespalov",
      "Dmitry V. Dylov",
      "Ivan Oseledets"
    ],
    "abstract": "We present CLARITY (Clinical Assistant for Routing, Inference and Triage), an AI-driven platform designed to facilitate patient-to-specialist routing, clinical consultations, and severity assessment of patient conditions. Its hybrid architecture combines a Finite State Machine (FSM) for structured dialogue flows with collaborative agents that employ Large Language Model (LLM) to analyze symptoms and prioritize referrals to appropriate specialists. Built on a modular microservices framework, CLARITY ensures safe, efficient, and robust performance, flexible and readily scalable to meet the demands of existing workflows and IT solutions in healthcare. We report integration of our clinical assistant into a large-scale national interhospital platform, with more than 55,000 content-rich user dialogues completed within the two months of deployment, 2,500 of which were expert-annotated for subsequent validation. The validation results show that CLARITY surpasses human-level performance in terms of the first-attempt routing precision, naturally requiring up to 3 times shorter duration of the consultation than with a human.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 (Industrial Track)",
    "pdf_url": "https://arxiv.org/pdf/2510.02463v2",
    "published_date": "2025-10-02 18:18:41 UTC",
    "updated_date": "2025-10-10 17:58:15 UTC"
  },
  {
    "arxiv_id": "2510.02456v2",
    "title": "Market-Driven Subset Selection for Budgeted Training",
    "authors": [
      "Ashish Jha",
      "Valentin Leplat",
      "AH Phan"
    ],
    "abstract": "Training large language models on massive datasets is computationally expensive, yet empirical evidence suggests that substantial portions of training examples contribute minimally to final performance. Data subset selection addresses this inefficiency by identifying small, high-utility subsets under resource constraints. However, example utility is inherently multi-faceted, encompassing uncertainty, distributional rarity, and diversity signals that are heterogeneous and typically combined through ad hoc weighted sums lacking theoretical grounding. We propose a market-based framework that treats each training example as a tradeable contract and employs the Logarithmic Market Scoring Rule to aggregate multiple utility signals into coherent prices. Heterogeneous signals act as traders, a single liquidity parameter controls concentration versus smoothing, and topic-wise normalization ensures calibrated aggregation. Token budgets are handled explicitly through a price-per-token decision rule with an interpretable length-bias parameter. We establish theoretical connections to maximum-entropy aggregation and provide utility recovery guarantees under noisy but monotone signals. On GSM8K mathematical reasoning under strict 60k-token budgets, our selector achieves parity with strong single-signal baselines while exhibiting lower variance and incurring less than 0.1 GPU-hour overhead. On AGNews classification at 5-25\\% retention rates, the market formulation delivers competitive accuracy with improved stability. Our framework unifies multi-signal data curation under fixed computational budgets for prompt-level reasoning and classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "Retitled major revision of the same work (formerly \"Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility\"). Abstract and exposition revised; ablations added; theory clarified. Core results unchanged. Supersedes v1; please process as a replacement",
    "pdf_url": "https://arxiv.org/pdf/2510.02456v2",
    "published_date": "2025-10-02 18:12:03 UTC",
    "updated_date": "2025-10-20 15:38:47 UTC"
  },
  {
    "arxiv_id": "2510.02453v1",
    "title": "How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models",
    "authors": [
      "Parth Asawa",
      "Alan Zhu",
      "Matei Zaharia",
      "Alexandros G. Dimakis",
      "Joseph E. Gonzalez"
    ],
    "abstract": "Foundation models are increasingly deployed as black-box services, where model weights cannot be modified and customization is limited to prompting. While static prompt optimization has shown promise, it produces a single fixed prompt that fails to adapt to different inputs, users, or environments. We introduce Advisor Models, lightweight parametric policies trained with reinforcement learning to reactively issue natural language steering instructions in-context to black-box models. The advisor is a second small model that sits between the input and the model, shaping behavior on a per-instance basis using reward signals from the environment. Across multiple domains involving reasoning and personalization, we show that Advisor Models outperform static prompt optimizers, discovering environment dynamics and improving downstream task performance. We also demonstrate the generalizability of advisors by transferring them across black-box models, as well as the framework's ability to achieve specialization while retaining robustness to out-of-distribution inputs. Viewed more broadly, Advisor Models provide a learnable interface to black-box systems where the advisor acts as a parametric, environment-specific memory. We argue that dynamic optimization of black-box models via Advisor Models is a promising direction for enabling personalization and environment-adaptable AI with frontier-level capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02453v1",
    "published_date": "2025-10-02 18:02:39 UTC",
    "updated_date": "2025-10-02 18:02:39 UTC"
  },
  {
    "arxiv_id": "2510.02307v1",
    "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation",
    "authors": [
      "Ruozhen He",
      "Moayed Haji-Ali",
      "Ziyan Yang",
      "Vicente Ordonez"
    ],
    "abstract": "Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02307v1",
    "published_date": "2025-10-02 17:59:43 UTC",
    "updated_date": "2025-10-02 17:59:43 UTC"
  },
  {
    "arxiv_id": "2510.02305v1",
    "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive",
    "authors": [
      "Tyler Farghly",
      "Peter Potaptchik",
      "Samuel Howard",
      "George Deligiannidis",
      "Jakiw Pidstrigach"
    ],
    "abstract": "Diffusion models have achieved state-of-the-art performance, demonstrating remarkable generalisation capabilities across diverse domains. However, the mechanisms underpinning these strong capabilities remain only partially understood. A leading conjecture, based on the manifold hypothesis, attributes this success to their ability to adapt to low-dimensional geometric structure within the data. This work provides evidence for this conjecture, focusing on how such phenomena could result from the formulation of the learning problem through score matching. We inspect the role of implicit regularisation by investigating the effect of smoothing minimisers of the empirical score matching objective. Our theoretical and empirical results confirm that smoothing the score function -- or equivalently, smoothing in the log-density domain -- produces smoothing tangential to the data manifold. In addition, we show that the manifold along which the diffusion model generalises can be controlled by choosing an appropriate smoothing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02305v1",
    "published_date": "2025-10-02 17:59:39 UTC",
    "updated_date": "2025-10-02 17:59:39 UTC"
  },
  {
    "arxiv_id": "2510.02300v3",
    "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models",
    "authors": [
      "Runqian Wang",
      "Yilun Du"
    ],
    "abstract": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02300v3",
    "published_date": "2025-10-02 17:59:06 UTC",
    "updated_date": "2025-10-13 15:39:31 UTC"
  },
  {
    "arxiv_id": "2510.02297v1",
    "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
    "authors": [
      "Wentao Zhang",
      "Yang Young Lu",
      "Yuntian Deng"
    ],
    "abstract": "Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2025 Demo",
    "pdf_url": "https://arxiv.org/pdf/2510.02297v1",
    "published_date": "2025-10-02 17:59:00 UTC",
    "updated_date": "2025-10-02 17:59:00 UTC"
  },
  {
    "arxiv_id": "2510.02295v1",
    "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
    "authors": [
      "Enxin Song",
      "Wenhao Chai",
      "Shusheng Yang",
      "Ethan Armand",
      "Xiaojun Shan",
      "Haiyang Xu",
      "Jianwen Xie",
      "Zhuowen Tu"
    ],
    "abstract": "Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://enxinsong.com/VideoNSA-web/, Code: https://github.com/Espere-1119-Song/VideoNSA",
    "pdf_url": "https://arxiv.org/pdf/2510.02295v1",
    "published_date": "2025-10-02 17:58:54 UTC",
    "updated_date": "2025-10-02 17:58:54 UTC"
  },
  {
    "arxiv_id": "2510.02294v1",
    "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data",
    "authors": [
      "Ziyin Zhang",
      "Zihan Liao",
      "Hang Yu",
      "Peng Di",
      "Rui Wang"
    ],
    "abstract": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02294v1",
    "published_date": "2025-10-02 17:58:49 UTC",
    "updated_date": "2025-10-02 17:58:49 UTC"
  },
  {
    "arxiv_id": "2510.03349v1",
    "title": "AgentCaster: Reasoning-Guided Tornado Forecasting",
    "authors": [
      "Michael Chen"
    ],
    "abstract": "There is a growing need to evaluate Large Language Models (LLMs) on complex, high-impact, real-world tasks to assess their true readiness as reasoning agents. To address this gap, we introduce AgentCaster, a contamination-free framework employing multimodal LLMs end-to-end for the challenging, long-horizon task of tornado forecasting. Within AgentCaster, models interpret heterogeneous spatiotemporal data from a high-resolution convection-allowing forecast archive. We assess model performance over a 40-day period featuring diverse historical data, spanning several major tornado outbreaks and including over 500 tornado reports. Each day, models query interactively from a pool of 3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of 12-36 hours. Probabilistic tornado-risk polygon predictions are verified against ground truths derived from geometric comparisons across disjoint risk bands in projected coordinate space. To quantify accuracy, we propose domain-specific TornadoBench and TornadoHallucination metrics, with TornadoBench highly challenging for both LLMs and domain expert human forecasters. Notably, human experts significantly outperform state-of-the-art models, which demonstrate a strong tendency to hallucinate and overpredict risk intensity, struggle with precise geographic placement, and exhibit poor spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster aims to advance research on improving LLM agents for challenging reasoning tasks in critical domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03349v1",
    "published_date": "2025-10-02 17:57:16 UTC",
    "updated_date": "2025-10-02 17:57:16 UTC"
  },
  {
    "arxiv_id": "2510.02286v1",
    "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks",
    "authors": [
      "Ruohao Guo",
      "Afshin Oroojlooy",
      "Roshan Sridhar",
      "Miguel Ballesteros",
      "Alan Ritter",
      "Dan Roth"
    ],
    "abstract": "Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02286v1",
    "published_date": "2025-10-02 17:57:05 UTC",
    "updated_date": "2025-10-02 17:57:05 UTC"
  },
  {
    "arxiv_id": "2510.02284v2",
    "title": "Learning to Generate Rigid Body Interactions with Video Diffusion Models",
    "authors": [
      "David Romero",
      "Ariana Bermudez",
      "Hao Li",
      "Fabio Pizzati",
      "Ivan Laptev"
    ],
    "abstract": "Recent video generation models have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack object-level control mechanisms. To address these limitations, we introduce KineMask, an approach for video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predicted scene descriptions, leading to support for synthesis of complex dynamical phenomena. Our experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available. Project Page: https://daromog.github.io/KineMask/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02284v2",
    "published_date": "2025-10-02 17:56:46 UTC",
    "updated_date": "2025-11-30 12:48:19 UTC"
  },
  {
    "arxiv_id": "2510.02283v1",
    "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
    "authors": [
      "Justin Cui",
      "Jie Wu",
      "Ming Li",
      "Tao Yang",
      "Xiaojie Li",
      "Rui Wang",
      "Andrew Bai",
      "Yuanhao Ban",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.02283v1",
    "published_date": "2025-10-02 17:55:42 UTC",
    "updated_date": "2025-10-02 17:55:42 UTC"
  },
  {
    "arxiv_id": "2510.02279v2",
    "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation",
    "authors": [
      "Mykyta Ielanskyi",
      "Kajetan Schweighofer",
      "Lukas Aichberger",
      "Sepp Hochreiter"
    ],
    "abstract": "Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint, under review",
    "pdf_url": "https://arxiv.org/pdf/2510.02279v2",
    "published_date": "2025-10-02 17:54:09 UTC",
    "updated_date": "2025-10-23 08:55:17 UTC"
  },
  {
    "arxiv_id": "2510.02276v1",
    "title": "BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals",
    "authors": [
      "Chenqi Li",
      "Yu Liu",
      "Timothy Denison",
      "Tingting Zhu"
    ],
    "abstract": "Biosignals offer valuable insights into the physiological states of the human body. Although biosignal modalities differ in functionality, signal fidelity, sensor comfort, and cost, they are often intercorrelated, reflecting the holistic and interconnected nature of human physiology. This opens up the possibility of performing the same tasks using alternative biosignal modalities, thereby improving the accessibility, usability, and adaptability of health monitoring systems. However, the limited availability of large labeled datasets presents challenges for training models tailored to specific tasks and modalities of interest. Unsupervised cross-modal knowledge transfer offers a promising solution by leveraging knowledge from an existing modality to support model training for a new modality. Existing methods are typically based on knowledge distillation, which requires running a teacher model alongside student model training, resulting in high computational and memory overhead. This challenge is further exacerbated by the recent development of foundation models that demonstrate superior performance and generalization across tasks at the cost of large model sizes. To this end, we explore a new framework for unsupervised cross-modal knowledge transfer of biosignals by training a lightweight bridge network to align the intermediate representations and enable information flow between foundation models and across modalities. Specifically, we introduce an efficient strategy for selecting alignment positions where the bridge should be constructed, along with a flexible prototype network as the bridge architecture. Extensive experiments across multiple biosignal modalities, tasks, and datasets show that BioX-Bridge reduces the number of trainable parameters by 88--99\\% while maintaining or even improving transfer performance compared to state-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02276v1",
    "published_date": "2025-10-02 17:51:19 UTC",
    "updated_date": "2025-10-02 17:51:19 UTC"
  },
  {
    "arxiv_id": "2510.02272v1",
    "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective",
    "authors": [
      "Wen Yang",
      "Junhong Wu",
      "Chong Li",
      "Chengqing Zong",
      "Jiajun Zhang"
    ],
    "abstract": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: $\\textit{Does the reasoning capability achieved from English RPT effectively transfer to other languages?}$ We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as $\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2510.02272v1",
    "published_date": "2025-10-02 17:49:49 UTC",
    "updated_date": "2025-10-02 17:49:49 UTC"
  },
  {
    "arxiv_id": "2510.02271v2",
    "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents",
    "authors": [
      "Yaxin Du",
      "Yuanshuo Zhang",
      "Xiyuan Yang",
      "Yifan Zhou",
      "Cheng Wang",
      "Gongyi Zou",
      "Xianghe Pang",
      "Wenhao Wang",
      "Menglan Chen",
      "Shuo Tang",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Siheng Chen"
    ],
    "abstract": "Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools -- and more importantly, whether they can integrate them with general-purpose search to solve complex tasks. Therefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality. Experiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02271v2",
    "published_date": "2025-10-02 17:48:03 UTC",
    "updated_date": "2025-10-04 09:18:41 UTC"
  },
  {
    "arxiv_id": "2510.02270v1",
    "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification",
    "authors": [
      "Sathira Silva",
      "Eman Ali",
      "Chetan Arora",
      "Muhammad Haris Khan"
    ],
    "abstract": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at https://github.com/sathiiii/microCLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02270v1",
    "published_date": "2025-10-02 17:47:39 UTC",
    "updated_date": "2025-10-02 17:47:39 UTC"
  },
  {
    "arxiv_id": "2510.02265v1",
    "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning",
    "authors": [
      "Yalin E. Sagduyu",
      "Tugba Erpek",
      "Kemal Davaslioglu",
      "Sastry Kompella"
    ],
    "abstract": "This paper studies the problem of mitigating reactive jamming, where a jammer adopts a dynamic policy of selecting channels and sensing thresholds to detect and jam ongoing transmissions. The transmitter-receiver pair learns to avoid jamming and optimize throughput over time (without prior knowledge of channel conditions or jamming strategies) by using reinforcement learning (RL) to adapt transmit power, modulation, and channel selection. Q-learning is employed for discrete jamming-event states, while Deep Q-Networks (DQN) are employed for continuous states based on received power. Through different reward functions and action sets, the results show that RL can adapt rapidly to spectrum dynamics and sustain high rates as channels and jamming policies change over time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02265v1",
    "published_date": "2025-10-02 17:44:38 UTC",
    "updated_date": "2025-10-02 17:44:38 UTC"
  },
  {
    "arxiv_id": "2510.02264v1",
    "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities",
    "authors": [
      "Mario Medrano-Paredes",
      "Carmen Fern√°ndez-Gonz√°lez",
      "Francisco-Javier D√≠az-Pernas",
      "Hichem Saoudi",
      "Javier Gonz√°lez-Alonso",
      "Mario Mart√≠nez-Zarzuela"
    ],
    "abstract": "Advances in machine learning and wearable sensors offer new opportunities for capturing and analyzing human movement outside specialized laboratories. Accurate assessment of human movement under real-world conditions is essential for telemedicine, sports science, and rehabilitation. This preclinical benchmark compares monocular video-based 3D human pose estimation models with inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a total of 13 clinically relevant daily activities which were captured using both commodity video cameras and five IMUs. During this initial study only healthy subjects were recorded, so results cannot be generalized to pathological cohorts. Joint angles derived from state-of-the-art deep learning frameworks (MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA BodyTrack) were evaluated against joint angles computed from IMU data using OpenSim inverse kinematics following the Human3.6M dataset format with 17 keypoints. Among them, MotionAGFormer demonstrated superior performance, achieving the lowest overall RMSE ($9.27¬∞\\pm 4.80¬∞$) and MAE ($7.86¬∞\\pm 4.18¬∞$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$) and the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The results reveal that both technologies are viable for out-of-the-lab kinematic assessment. However, they also highlight key trade-offs between video- and sensor-based approaches including costs, accessibility, and precision. This study clarifies where off-the-shelf video models already provide clinically promising kinematics in healthy adults and where they lag behind IMU-based estimates while establishing valuable guidelines for researchers and clinicians seeking to develop robust, cost-effective, and user-friendly solutions for telehealth and remote patient monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "All tables, graphs and figures generated can be obtained in the Zenodo repository complementary to this work: https://doi.org/10.5281/zenodo.15088423",
    "pdf_url": "https://arxiv.org/pdf/2510.02264v1",
    "published_date": "2025-10-02 17:44:31 UTC",
    "updated_date": "2025-10-02 17:44:31 UTC"
  },
  {
    "arxiv_id": "2510.02263v1",
    "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems",
    "authors": [
      "Yuxiao Qu",
      "Anikait Singh",
      "Yoonho Lee",
      "Amrith Setlur",
      "Ruslan Salakhutdinov",
      "Chelsea Finn",
      "Aviral Kumar"
    ],
    "abstract": "Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement \"algorithmic procedures\" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02263v1",
    "published_date": "2025-10-02 17:44:23 UTC",
    "updated_date": "2025-10-02 17:44:23 UTC"
  },
  {
    "arxiv_id": "2510.02253v2",
    "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing",
    "authors": [
      "Zihan Zhou",
      "Shilin Lu",
      "Shuli Leng",
      "Shaocong Zhang",
      "Zhuming Lian",
      "Xinlei Yu",
      "Adams Wai-Kin Kong"
    ],
    "abstract": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.02253v2",
    "published_date": "2025-10-02 17:39:13 UTC",
    "updated_date": "2025-10-23 17:58:02 UTC"
  },
  {
    "arxiv_id": "2510.02250v1",
    "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
    "authors": [
      "Gonzalo Gonzalez-Pumariega",
      "Vincent Tu",
      "Chih-Lun Lee",
      "Jiachen Yang",
      "Ang Li",
      "Xin Eric Wang"
    ],
    "abstract": "Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 7 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.02250v1",
    "published_date": "2025-10-02 17:37:08 UTC",
    "updated_date": "2025-10-02 17:37:08 UTC"
  },
  {
    "arxiv_id": "2510.02249v1",
    "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation",
    "authors": [
      "Tianyi Jiang",
      "Yi Bin",
      "Yujuan Ding",
      "Kainian Zhu",
      "Fei Ma",
      "Jingkuan Song",
      "Heng Tao Shen"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities on complex problems using long Chain-of-Thought (CoT) reasoning. However, they often suffer from overthinking, meaning generating unnecessarily lengthy reasoning steps for simpler problems. This issue may degrade the efficiency of the models and make them difficult to adapt the reasoning depth to the complexity of problems. To address this, we introduce a novel metric Token Entropy Cumulative Average (TECA), which measures the extent of exploration throughout the reasoning process. We further propose a novel reasoning paradigm -- Explore Briefly, Then Decide -- with an associated Cumulative Entropy Regulation (CER) mechanism. This paradigm leverages TECA to help the model dynamically determine the optimal point to conclude its thought process and provide a final answer, thus achieving efficient reasoning. Experimental results across diverse mathematical benchmarks show that our approach substantially mitigates overthinking without sacrificing problem-solving ability. With our thinking paradigm, the average response length decreases by up to 71% on simpler datasets, demonstrating the effectiveness of our method in creating a more efficient and adaptive reasoning process.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02249v1",
    "published_date": "2025-10-02 17:36:50 UTC",
    "updated_date": "2025-10-02 17:36:50 UTC"
  },
  {
    "arxiv_id": "2510.02245v1",
    "title": "ExGRPO: Learning to Reason from Experience",
    "authors": [
      "Runzhe Zhan",
      "Yafu Li",
      "Zhi Wang",
      "Xiaoye Qu",
      "Dongrui Liu",
      "Jing Shao",
      "Derek F. Wong",
      "Yu Cheng"
    ],
    "abstract": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02245v1",
    "published_date": "2025-10-02 17:31:30 UTC",
    "updated_date": "2025-10-02 17:31:30 UTC"
  },
  {
    "arxiv_id": "2510.02240v1",
    "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning",
    "authors": [
      "Sicheng Feng",
      "Kaiwen Tuo",
      "Song Wang",
      "Lingdong Kong",
      "Jianke Zhu",
      "Huan Wang"
    ],
    "abstract": "Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02240v1",
    "published_date": "2025-10-02 17:29:46 UTC",
    "updated_date": "2025-10-02 17:29:46 UTC"
  },
  {
    "arxiv_id": "2510.05153v1",
    "title": "An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem",
    "authors": [
      "Zhangchi Liu"
    ],
    "abstract": "This paper provides a definitive, unifying framework for the Symbol Grounding Problem (SGP) by reformulating it within Algorithmic Information Theory (AIT). We demonstrate that the grounding of meaning is a process fundamentally constrained by information-theoretic limits, thereby unifying the G√∂delian (self-reference) and No Free Lunch (statistical) perspectives. We model a symbolic system as a universal Turing machine and define grounding as an act of information compression. The argument proceeds in four stages. First, we prove that a purely symbolic system cannot ground almost all possible \"worlds\" (data strings), as they are algorithmically random and thus incompressible. Second, we show that any statically grounded system, specialized for compressing a specific world, is inherently incomplete because an adversarial, incompressible world relative to the system can always be constructed. Third, the \"grounding act\" of adapting to a new world is proven to be non-inferable, as it requires the input of new information (a shorter program) that cannot be deduced from the system's existing code. Finally, we use Chaitin's Incompleteness Theorem to prove that any algorithmic learning process is itself a finite system that cannot comprehend or model worlds whose complexity provably exceeds its own. This establishes that meaning is the open-ended process of a system perpetually attempting to overcome its own information-theoretic limitations.",
    "categories": [
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 1 table (in appendix)",
    "pdf_url": "https://arxiv.org/pdf/2510.05153v1",
    "published_date": "2025-10-02 17:22:47 UTC",
    "updated_date": "2025-10-02 17:22:47 UTC"
  },
  {
    "arxiv_id": "2510.02230v1",
    "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models",
    "authors": [
      "Phuc Minh Nguyen",
      "Chinh D. La",
      "Duy M. H. Nguyen",
      "Nitesh V. Chawla",
      "Binh T. Nguyen",
      "Khoa D. Doan"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 15 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02230v1",
    "published_date": "2025-10-02 17:17:27 UTC",
    "updated_date": "2025-10-02 17:17:27 UTC"
  },
  {
    "arxiv_id": "2510.02227v2",
    "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration",
    "authors": [
      "Xiaoyang Yuan",
      "Yujuan Ding",
      "Yi Bin",
      "Wenqi Shao",
      "Jinyu Cai",
      "Jingkuan Song",
      "Yang Yang",
      "Heng Tao Shen"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This \"guidance-on-demand\" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02227v2",
    "published_date": "2025-10-02 17:14:00 UTC",
    "updated_date": "2025-10-09 17:18:49 UTC"
  },
  {
    "arxiv_id": "2510.02226v2",
    "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
    "authors": [
      "Shira Schiber",
      "Ofir Lindenbaum",
      "Idan Schwartz"
    ],
    "abstract": "Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal pattern with a control signal (correlation), adjusting its strength where visibility is required (magnitude), and preserving semantic consistency (entropy). TempoControl provides precise temporal control while maintaining high video quality and diversity. We demonstrate its effectiveness across various applications, including temporal reordering of single and multiple objects, action timing, and audio-aligned video generation. Please see our project page for more details: https://shira-schiber.github.io/TempoControl/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.02226v2",
    "published_date": "2025-10-02 17:13:35 UTC",
    "updated_date": "2025-12-05 13:22:45 UTC"
  },
  {
    "arxiv_id": "2510.02212v2",
    "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning",
    "authors": [
      "Hanyang Zhao",
      "Dawen Liang",
      "Wenpin Tang",
      "David Yao",
      "Nathan Kallus"
    ],
    "abstract": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02212v2",
    "published_date": "2025-10-02 16:57:24 UTC",
    "updated_date": "2026-01-10 10:25:26 UTC"
  },
  {
    "arxiv_id": "2510.02202v1",
    "title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025",
    "authors": [
      "Matthew A. Reyna",
      "Zuzana Koscova",
      "Jan Pavlus",
      "Soheil Saghafi",
      "James Weigle",
      "Andoni Elola",
      "Salman Seyedi",
      "Kiersten Campbell",
      "Qiao Li",
      "Ali Bahrami Rad",
      "Ant√¥nio H. Ribeiro",
      "Antonio Luiz P. Ribeiro",
      "Reza Sameni",
      "Gari D. Clifford"
    ],
    "abstract": "Objective: Chagas disease is a parasitic infection that is endemic to South America, Central America, and, more recently, the U.S., primarily transmitted by insects. Chronic Chagas disease can cause cardiovascular diseases and digestive problems. Serological testing capacities for Chagas disease are limited, but Chagas cardiomyopathy often manifests in ECGs, providing an opportunity to prioritize patients for testing and treatment. Approach: The George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic approaches for identifying Chagas disease from electrocardiograms (ECGs). Main results: This Challenge provides multiple innovations. First, we leveraged several datasets with labels from patient reports and serological testing, provided a large dataset with weak labels and smaller datasets with strong labels. Second, we augmented the data to support model robustness and generalizability to unseen data sources. Third, we applied an evaluation metric that captured the local serological testing capacity for Chagas disease to frame the machine learning problem as a triage task. Significance: Over 630 participants from 111 teams submitted over 1300 entries during the Challenge, representing diverse approaches from academia and industry worldwide.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02202v1",
    "published_date": "2025-10-02 16:50:36 UTC",
    "updated_date": "2025-10-02 16:50:36 UTC"
  },
  {
    "arxiv_id": "2510.02200v1",
    "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities",
    "authors": [
      "Felix Brei",
      "Lorenz B√ºhmann",
      "Johannes Frey",
      "Daniel Gerber",
      "Lars-Peter Meyer",
      "Claus Stadler",
      "Kirill Bulert"
    ],
    "abstract": "Interacting with knowledge graphs can be a daunting task for people without a background in computer science since the query language that is used (SPARQL) has a high barrier of entry. Large language models (LLMs) can lower that barrier by providing support in the form of Text2SPARQL translation. In this paper we introduce a generalized method based on SPINACH, an LLM backed agent that translates natural language questions to SPARQL queries not in a single shot, but as an iterative process of exploration and execution. We describe the overall architecture and reasoning behind our design decisions, and also conduct a thorough analysis of the agent behavior to gain insights into future areas for targeted improvements. This work was motivated by the Text2SPARQL challenge, a challenge that was held to facilitate improvements in the Text2SPARQL domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "peer reviewed publication at Text2SPARQL Workshop @ ESWC 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.02200v1",
    "published_date": "2025-10-02 16:49:27 UTC",
    "updated_date": "2025-10-02 16:49:27 UTC"
  },
  {
    "arxiv_id": "2510.02423v1",
    "title": "RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation",
    "authors": [
      "Hang Wu",
      "Yujun Cai",
      "Haonan Ge",
      "Hongkai Chen",
      "Ming-Hsuan Yang",
      "Yiwei Wang"
    ],
    "abstract": "Cinematography understanding refers to the ability to recognize not only the visual content of a scene but also the cinematic techniques that shape narrative meaning. This capability is attracting increasing attention, as it enhances multimodal understanding in real-world applications and underpins coherent content creation in film and media. As the most comprehensive benchmark for this task, ShotBench spans a wide range of cinematic concepts and VQA-style evaluations, with ShotVL achieving state-of-the-art results on it. However, our analysis reveals that ambiguous option design in ShotBench and ShotVL's shortcomings in reasoning consistency and instruction adherence undermine evaluation reliability, limiting fair comparison and hindering future progress. To overcome these issues, we systematically refine ShotBench through consistent option restructuring, conduct the first critical analysis of ShotVL's reasoning behavior, and introduce an extended evaluation protocol that jointly assesses task accuracy and core model competencies. These efforts lead to RefineShot, a refined and expanded benchmark that enables more reliable assessment and fosters future advances in cinematography understanding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02423v1",
    "published_date": "2025-10-02 16:46:32 UTC",
    "updated_date": "2025-10-02 16:46:32 UTC"
  },
  {
    "arxiv_id": "2510.02194v1",
    "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language Models",
    "authors": [
      "Yuhao Sun",
      "Zhuoer Xu",
      "Shiwen Cui",
      "Kun Yang",
      "Lingyun Yu",
      "Yongdong Zhang",
      "Hongtao Xie"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable progress across a wide range of tasks, but remain vulnerable to safety risks such as harmful content generation and jailbreak attacks. Existing safety techniques -- including external guardrails, inference-time guidance, and post-training alignment -- each face limitations in balancing safety, utility, and controllability. In this work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM safety through safety-aware upcycling. Our approach first identifies safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE) structure, where the router acts as a soft guardrail that selectively activates original MLPs and added safety experts. We further introduce a two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities. To enable flexible control at inference time, we introduce a safety temperature mechanism, allowing dynamic adjustment of the trade-off between safety and utility. Experiments across multiple benchmarks, base model, and model scales demonstrate that UpSafe$^\\circ$C achieves robust safety improvements against harmful and jailbreak inputs, while maintaining competitive performance on general tasks. Moreover, analysis shows that safety temperature provides fine-grained inference-time control that achieves the Pareto-optimal frontier between utility and safety. Our results highlight a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02194v1",
    "published_date": "2025-10-02 16:43:33 UTC",
    "updated_date": "2025-10-02 16:43:33 UTC"
  },
  {
    "arxiv_id": "2510.02422v2",
    "title": "Dynamic Target Attack",
    "authors": [
      "Kedong Xiu",
      "Churui Zeng",
      "Tianhang Zheng",
      "Xinzhe Huang",
      "Xiaojun Jia",
      "Di Wang",
      "Puning Zhao",
      "Zhan Qin",
      "Kui Ren"
    ],
    "abstract": "Existing gradient-based jailbreak attacks typically optimize an adversarial suffix to induce a fixed affirmative response. However, this fixed target usually resides in an extremely low-density region of a safety-aligned LLM's output distribution conditioned on diverse harmful inputs. Due to the substantial discrepancy between the target and the original output, existing attacks require numerous iterations to optimize the adversarial prompt, which might still fail to induce the low-probability target response from the target LLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking framework relying on the target LLM's own responses as targets to optimize the adversarial prompts. In each optimization round, DTA iteratively samples multiple candidate responses directly from the output distribution conditioned on the current prompt, and selects the most harmful response as a temporary target for prompt optimization. In contrast to existing attacks, DTA significantly reduces the discrepancy between the target and the output distribution, substantially easing the optimization process to search for an effective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency of DTA: under the white-box setting, DTA only needs 200 optimization iterations to achieve an average attack success rate (ASR) of over 87\\% on recent safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The time cost of DTA is 2-26 times less than existing baselines. Under the black-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target sampling and achieves an ASR of 85\\% against the black-box target model Llama-3-70B-Instruct, exceeding its counterparts by over 25\\%.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02422v2",
    "published_date": "2025-10-02 16:40:51 UTC",
    "updated_date": "2025-10-24 11:43:40 UTC"
  },
  {
    "arxiv_id": "2510.02190v1",
    "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports",
    "authors": [
      "Yang Yao",
      "Yixu Wang",
      "Yuxuan Zhang",
      "Yi Lu",
      "Tianle Gu",
      "Lingyu Li",
      "Dingyi Zhao",
      "Keming Wu",
      "Haozhe Wang",
      "Ping Nie",
      "Yan Teng",
      "Yingchun Wang"
    ],
    "abstract": "Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02190v1",
    "published_date": "2025-10-02 16:40:02 UTC",
    "updated_date": "2025-10-02 16:40:02 UTC"
  },
  {
    "arxiv_id": "2510.02181v1",
    "title": "EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative Captioning",
    "authors": [
      "Liang-Yuan Wu",
      "Dhruv Jain"
    ],
    "abstract": "Automatic Speech Recognition (ASR) systems often fail to accurately transcribe speech from Deaf and Hard of Hearing (DHH) individuals, especially during real-time conversations. Existing personalization approaches typically require extensive pre-recorded data and place the burden of adaptation on the DHH speaker. We present EvolveCaptions, a real-time, collaborative ASR adaptation system that supports in-situ personalization with minimal effort. Hearing participants correct ASR errors during live conversations. Based on these corrections, the system generates short, phonetically targeted prompts for the DHH speaker to record, which are then used to fine-tune the ASR model. In a study with 12 DHH and six hearing participants, EvolveCaptions reduced Word Error Rate (WER) across all DHH users within one hour of use, using only five minutes of recording time on average. Participants described the system as intuitive, low-effort, and well-integrated into communication. These findings demonstrate the promise of collaborative, real-time ASR adaptation for more equitable communication.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02181v1",
    "published_date": "2025-10-02 16:32:29 UTC",
    "updated_date": "2025-10-02 16:32:29 UTC"
  },
  {
    "arxiv_id": "2510.02180v1",
    "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning",
    "authors": [
      "Silvia Sapora",
      "Devon Hjelm",
      "Alexander Toshev",
      "Omar Attia",
      "Bogdan Mazoure"
    ],
    "abstract": "Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield \"black-box\" models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02180v1",
    "published_date": "2025-10-02 16:31:39 UTC",
    "updated_date": "2025-10-02 16:31:39 UTC"
  },
  {
    "arxiv_id": "2510.02173v2",
    "title": "Learning to Reason for Hallucination Span Detection",
    "authors": [
      "Hsuan Su",
      "Ting-Yao Hu",
      "Hema Swetha Koppula",
      "Kundan Krishna",
      "Hadi Pouransari",
      "Cheng-Yu Hsieh",
      "Cem Koc",
      "Joseph Yitan Cheng",
      "Oncel Tuzel",
      "Raviteja Vemulapalli"
    ],
    "abstract": "Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02173v2",
    "published_date": "2025-10-02 16:24:28 UTC",
    "updated_date": "2025-10-08 19:06:15 UTC"
  },
  {
    "arxiv_id": "2510.02171v1",
    "title": "Go witheFlow: Real-time Emotion Driven Audio Effects Modulation",
    "authors": [
      "Edmund Dervakos",
      "Spyridon Kantarelis",
      "Vassilis Lyberatos",
      "Jason Liartis",
      "Giorgos Stamou"
    ],
    "abstract": "Music performance is a distinctly human activity, intrinsically linked to the performer's ability to convey, evoke, or express emotion. Machines cannot perform music in the human sense; they can produce, reproduce, execute, or synthesize music, but they lack the capacity for affective or emotional experience. As such, music performance is an ideal candidate through which to explore aspects of collaboration between humans and machines. In this paper, we introduce the witheFlow system, designed to enhance real-time music performance by automatically modulating audio effects based on features extracted from both biosignals and the audio itself. The system, currently in a proof-of-concept phase, is designed to be lightweight, able to run locally on a laptop, and is open-source given the availability of a compatible Digital Audio Workstation and sensors.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at NeurIPS Creative AI Track 2025: Humanity",
    "pdf_url": "https://arxiv.org/pdf/2510.02171v1",
    "published_date": "2025-10-02 16:23:47 UTC",
    "updated_date": "2025-10-02 16:23:47 UTC"
  },
  {
    "arxiv_id": "2510.02166v1",
    "title": "SIEVE: Towards Verifiable Certification for Code-datasets",
    "authors": [
      "Fatou Ndiaye Mbodji",
      "El-hacen Diallo",
      "Jordan Samhi",
      "Kui Liu",
      "Jacques Klein",
      "Tegawend√© F. Bissyande"
    ],
    "abstract": "Code agents and empirical software engineering rely on public code datasets, yet these datasets lack verifiable quality guarantees. Static 'dataset cards' inform, but they are neither auditable nor do they offer statistical guarantees, making it difficult to attest to dataset quality. Teams build isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We present SIEVE, a community-driven framework. It turns per-property checks into Confidence Cards-machine-readable, verifiable certificates with anytime-valid statistical bounds. We outline a research plan to bring SIEVE to maturity, replacing narrative cards with anytime-verifiable certification. This shift is expected to lower quality-assurance costs and increase trust in code-datasets.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "5",
    "pdf_url": "https://arxiv.org/pdf/2510.02166v1",
    "published_date": "2025-10-02 16:14:23 UTC",
    "updated_date": "2025-10-02 16:14:23 UTC"
  },
  {
    "arxiv_id": "2510.02161v2",
    "title": "Comparing Contrastive and Triplet Loss: Variance Analysis and Optimization Behavior",
    "authors": [
      "Donghuo Zeng"
    ],
    "abstract": "Contrastive loss and triplet loss are widely used objectives in deep metric learning, yet their effects on representation quality remain insufficiently understood. We present a theoretical and empirical comparison of these losses, focusing on intra- and inter-class variance and optimization behavior (e.g., greedy updates). Through task-specific experiments with consistent settings on synthetic data and real datasets-MNIST, CIFAR-10-it is shown that triplet loss preserves greater variance within and across classes, supporting finer-grained distinctions in the learned representations. In contrast, contrastive loss tends to compact intra-class embeddings, which may obscure subtle semantic differences. To better understand their optimization dynamics, By examining loss-decay rate, active ratio, and gradient norm, we find that contrastive loss drives many small updates early on, while triplet loss produces fewer but stronger updates that sustain learning on hard examples. Finally, across both classification and retrieval tasks on MNIST, CIFAR-10, CUB-200, and CARS196 datasets, our results consistently show that triplet loss yields superior performance, which suggests using triplet loss for detail retention and hard-sample focus, and contrastive loss for smoother, broad-based embedding refinement.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MM",
    "comment": "8 pages, 4 tables, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02161v2",
    "published_date": "2025-10-02 16:11:46 UTC",
    "updated_date": "2025-10-06 05:19:04 UTC"
  },
  {
    "arxiv_id": "2510.02155v1",
    "title": "Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting",
    "authors": [
      "Shu Zou",
      "Xinyu Tian",
      "Lukas Wesemann",
      "Fabian Waschkowski",
      "Zhaoyuan Yang",
      "Jing Zhang"
    ],
    "abstract": "Prompting has emerged as a practical way to adapt frozen vision-language models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are often overly abstract, overlooking the fine-grained human-object interactions or action semantics that define complex anomalies in surveillance videos. We propose ASK-Hint, a structured prompting framework that leverages action-centric knowledge to elicit more accurate and interpretable reasoning from frozen VLMs. Our approach organizes prompts into semantically coherent groups (e.g. violence, property crimes, public safety) and formulates fine-grained guiding questions that align model predictions with discriminative visual cues. Extensive experiments on UCF-Crime and XD-Violence show that ASK-Hint consistently improves AUC over prior baselines, achieving state-of-the-art performance compared to both fine-tuned and training-free methods. Beyond accuracy, our framework provides interpretable reasoning traces towards anomaly and demonstrates strong generalization across datasets and VLM backbones. These results highlight the critical role of prompt granularity and establish ASK-Hint as a new training-free and generalizable solution for explainable video anomaly detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, video anomaly detection",
    "pdf_url": "https://arxiv.org/pdf/2510.02155v1",
    "published_date": "2025-10-02 16:06:31 UTC",
    "updated_date": "2025-10-02 16:06:31 UTC"
  },
  {
    "arxiv_id": "2510.02153v1",
    "title": "Human-Robo-advisor collaboration in decision-making: Evidence from a multiphase mixed methods experimental study",
    "authors": [
      "Hasan Mahmud",
      "Najmul Islam",
      "Satish Krishnan"
    ],
    "abstract": "Robo-advisors (RAs) are cost-effective, bias-resistant alternatives to human financial advisors, yet adoption remains limited. While prior research has examined user interactions with RAs, less is known about how individuals interpret RA roles and integrate their advice into decision-making. To address this gap, this study employs a multiphase mixed methods design integrating a behavioral experiment (N = 334), thematic analysis, and follow-up quantitative testing. Findings suggest that people tend to rely on RAs, with reliance shaped by information about RA performance and the framing of advice as gains or losses. Thematic analysis reveals three RA roles in decision-making and four user types, each reflecting distinct patterns of advice integration. In addition, a 2 x 2 typology categorizes antecedents of acceptance into enablers and inhibitors at both the individual and algorithmic levels. By combining behavioral, interpretive, and confirmatory evidence, this study advances understanding of human-RA collaboration and provides actionable insights for designing more trustworthy and adaptive RA systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02153v1",
    "published_date": "2025-10-02 16:04:31 UTC",
    "updated_date": "2025-10-02 16:04:31 UTC"
  },
  {
    "arxiv_id": "2510.03346v1",
    "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
    "authors": [
      "Xiangyu Shi",
      "Marco Chiesa",
      "Gerald Q. Maguire",
      "Dejan Kostic"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03346v1",
    "published_date": "2025-10-02 16:01:54 UTC",
    "updated_date": "2025-10-02 16:01:54 UTC"
  },
  {
    "arxiv_id": "2510.02143v2",
    "title": "How to Find Fantastic AI Papers: Self-Rankings as a Powerful Predictor of Scientific Impact Beyond Peer Review",
    "authors": [
      "Buxin Su",
      "Natalie Collina",
      "Garrett Wen",
      "Didong Li",
      "Kyunghyun Cho",
      "Jianqing Fan",
      "Bingxin Zhao",
      "Weijie Su"
    ],
    "abstract": "Peer review in academic research aims not only to ensure factual correctness but also to identify work of high scientific potential that can shape future research directions. This task is especially critical in fast-moving fields such as artificial intelligence (AI), yet it has become increasingly difficult given the rapid growth of submissions. In this paper, we investigate an underexplored measure for identifying high-impact research: authors' own rankings of their multiple submissions to the same AI conference. Grounded in game-theoretic reasoning, we hypothesize that self-rankings are informative because authors possess unique understanding of their work's conceptual depth and long-term promise. To test this hypothesis, we conducted a large-scale experiment at a leading AI conference, where 1,342 researchers self-ranked their 2,592 submissions by perceived quality. Tracking outcomes over more than a year, we found that papers ranked highest by their authors received twice as many citations as their lowest-ranked counterparts; self-rankings were especially effective at identifying highly cited papers (those with over 150 citations). Moreover, we showed that self-rankings outperformed peer review scores in predicting future citation counts. Our results remained robust after accounting for confounders such as preprint posting time and self-citations. Together, these findings demonstrate that authors' self-rankings provide a reliable and valuable complement to peer review for identifying and elevating high-impact research in AI.",
    "categories": [
      "stat.AP",
      "cs.AI",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "stat.AP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02143v2",
    "published_date": "2025-10-02 15:50:21 UTC",
    "updated_date": "2025-11-25 03:01:10 UTC"
  },
  {
    "arxiv_id": "2510.02139v1",
    "title": "BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic Bioinformatics",
    "authors": [
      "Florensia Widjaja",
      "Zhangtianyi Chen",
      "Juexiao Zhou"
    ],
    "abstract": "Bioinformatics tools are essential for complex computational biology tasks, yet their integration with emerging AI-agent frameworks is hindered by incompatible interfaces, heterogeneous input-output formats, and inconsistent parameter conventions. The Model Context Protocol (MCP) provides a standardized framework for tool-AI communication, but manually converting hundreds of existing and rapidly growing specialized bioinformatics tools into MCP-compliant servers is labor-intensive and unsustainable. Here, we present BioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter, which automatically generates robust MCP servers from tool documentation using large language models, and BioinfoMCP Benchmark, which systematically validates the reliability and versatility of converted tools across diverse computational tasks. We present a platform of 38 MCP-converted bioinformatics tools, extensively validated to show that 94.7% successfully executed complex workflows across three widely used AI-agent platforms. By removing technical barriers to AI automation, BioinfoMCP enables natural-language interaction with sophisticated bioinformatics analyses without requiring extensive programming expertise, offering a scalable path to intelligent, interoperable computational biology.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "q-bio.QM",
    "comment": "20 pages, 8 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.02139v1",
    "published_date": "2025-10-02 15:47:59 UTC",
    "updated_date": "2025-10-02 15:47:59 UTC"
  },
  {
    "arxiv_id": "2510.02133v1",
    "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models",
    "authors": [
      "Karan Dua",
      "Hitesh Laxmichand Patel",
      "Puneet Mittal",
      "Ranjeet Gupta",
      "Amit Agarwal",
      "Praneet Pabolu",
      "Srikant Panda",
      "Hansa Meghwani",
      "Graham Horwood",
      "Fahad Shah"
    ],
    "abstract": "Developing document understanding models at enterprise scale requires large, diverse, and well-annotated datasets spanning a wide range of document types. However, collecting such data is prohibitively expensive due to privacy constraints, legal restrictions, and the sheer volume of manual annotation needed - costs that can scale into millions of dollars. We introduce FlexDoc, a scalable synthetic data generation framework that combines Stochastic Schemas and Parameterized Sampling to produce realistic, multilingual semi-structured documents with rich annotations. By probabilistically modeling layout patterns, visual structure, and content variability, FlexDoc enables the controlled generation of diverse document variants at scale. Experiments on Key Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data improves the absolute F1 Score by up to 11% when used to augment real datasets, while reducing annotation effort by over 90% compared to traditional hard-template methods. The solution is in active deployment, where it has accelerated the development of enterprise-grade document understanding models while significantly reducing data acquisition and annotation costs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.02133v1",
    "published_date": "2025-10-02 15:42:35 UTC",
    "updated_date": "2025-10-02 15:42:35 UTC"
  },
  {
    "arxiv_id": "2510.02128v1",
    "title": "The Disparate Impacts of Speculative Decoding",
    "authors": [
      "Jameson Sandler",
      "Ahmet √úst√ºn",
      "Marco Romanelli",
      "Sara Hooker",
      "Ferdinando Fioretto"
    ],
    "abstract": "The practice of speculative decoding, whereby inference is probabilistically supported by a smaller, cheaper, ``drafter'' model, has become a standard technique for systematically reducing the decoding time of large language models. This paper conducts an analysis of speculative decoding through the lens of its potential disparate speed-up rates across tasks. Crucially, the paper shows that speed-up gained from speculative decoding is not uniformly distributed across tasks, consistently diminishing for under-fit, and often underrepresented tasks. To better understand this phenomenon, we derive an analysis to quantify this observed ``unfairness'' and draw attention to the factors that motivate such disparate speed-ups to emerge. Further, guided by these insights, the paper proposes a mitigation strategy designed to reduce speed-up disparities and validates the approach across several model pairs, revealing on average a 12% improvement in our fairness metric.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02128v1",
    "published_date": "2025-10-02 15:38:57 UTC",
    "updated_date": "2025-10-02 15:38:57 UTC"
  },
  {
    "arxiv_id": "2510.02125v3",
    "title": "Do AI Models Perform Human-like Abstract Reasoning Across Modalities?",
    "authors": [
      "Claas Beger",
      "Ryan Yi",
      "Shuhao Fu",
      "Arseny Moskvichev",
      "Sarah W. Tsai",
      "Sivasankaran Rajamanickam",
      "Melanie Mitchell"
    ],
    "abstract": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI benchmark, but does that mean state-of-the-art models recognize and reason with the abstractions that the task creators intended? We investigate models' abstraction abilities on ConceptARC. We evaluate models under settings that vary the input modality (textual vs. visual), whether the model is permitted to use external Python tools, and, for reasoning models, the amount of reasoning effort. In addition to measuring output accuracy, we perform fine-grained evaluation of the natural-language rules that models generate to explain their solutions. This dual evaluation lets us assess whether models solve tasks using the abstractions ConceptARC was designed to elicit, rather than relying on surface-level patterns. Our results show that, while some models using text-based representations match human output accuracy, the best models' rules are often based on surface-level ``shortcuts'' and capture intended abstractions far less often than humans. Thus their capabilities for general abstract reasoning may be overestimated by evaluations based on accuracy alone. In the visual modality, AI models' output accuracy drops sharply, yet our rule-level analysis reveals that models might be underestimated, as they still exhibit a substantial share of rules that capture intended abstractions, but are often unable to correctly apply these rules. In short, our results show that models still lag humans in abstract reasoning, and that using accuracy alone to evaluate abstract reasoning on ARC-like tasks may overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities. We believe that our evaluation framework offers a more faithful picture of multimodal models' abstract reasoning abilities and a more principled way to track progress toward human-like, abstraction-centered intelligence.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02125v3",
    "published_date": "2025-10-02 15:35:10 UTC",
    "updated_date": "2025-10-06 21:24:04 UTC"
  },
  {
    "arxiv_id": "2510.02120v2",
    "title": "VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI",
    "authors": [
      "Charalampos Lamprou",
      "Aamna Alshehhi",
      "Leontios J. Hadjileontiadis",
      "Mohamed L. Seghier"
    ],
    "abstract": "Accounting for inter-individual variability in brain function is key to precision medicine. Here, by considering functional inter-individual variability as meaningful data rather than noise, we introduce VarCoNet, an enhanced self-supervised framework for robust functional connectome (FC) extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs self-supervised contrastive learning to exploit inherent functional inter-individual variability, serving as a brain function encoder that generates FC embeddings readily applicable to downstream tasks even in the absence of labeled data. Contrastive learning is facilitated by a novel augmentation strategy based on segmenting rs-fMRI signals. At its core, VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series processing, enhanced with a robust Bayesian hyperparameter optimization. Our VarCoNet framework is evaluated on two downstream tasks: (i) subject fingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii) autism spectrum disorder (ASD) classification, using rs-fMRI data from the ABIDE I and ABIDE II datasets. Using different brain parcellations, our extensive testing against state-of-the-art methods, including 13 deep learning methods, demonstrates VarCoNet's superiority, robustness, interpretability, and generalizability. Overall, VarCoNet provides a versatile and robust framework for FC analysis in rs-fMRI.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02120v2",
    "published_date": "2025-10-02 15:29:17 UTC",
    "updated_date": "2025-10-03 06:57:51 UTC"
  },
  {
    "arxiv_id": "2510.06235v1",
    "title": "Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)",
    "authors": [
      "Robert Scholz",
      "Kunal Bagga",
      "Christine Ahrends",
      "Carlo Alberto Barbano"
    ],
    "abstract": "We present our submission to the Algonauts 2025 Challenge, where the goal is to predict fMRI brain responses to movie stimuli. Our approach integrates multimodal representations from large language models, video encoders, audio models, and vision-language models, combining both off-the-shelf and fine-tuned variants. To improve performance, we enhanced textual inputs with detailed transcripts and summaries, and we explored stimulus-tuning and fine-tuning strategies for language and vision models. Predictions from individual models were combined using stacked regression, yielding solid results. Our submission, under the team name Seinfeld, ranked 10th. We make all code and resources publicly available, contributing to ongoing efforts in developing multimodal encoding models for brain activity.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.NC"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06235v1",
    "published_date": "2025-10-02 15:24:16 UTC",
    "updated_date": "2025-10-02 15:24:16 UTC"
  },
  {
    "arxiv_id": "2510.02418v2",
    "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks",
    "authors": [
      "Sagnik Anupam",
      "Davis Brown",
      "Shuo Li",
      "Eric Wong",
      "Hamed Hassani",
      "Osbert Bastani"
    ],
    "abstract": "LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-up banner removal, and direct navigation to URLs. By constructing targeted datasets to further study these tasks, we discover variations in how different language models navigate these failure modes. We find, for example, that o4-mini deploys a wider variety of strategies to circumvent captcha resolution than other models and DeepSeek-R1 consistently misleads users about pop-up banner closure. Our findings surface both the diversity and brittleness of current web agents. More broadly, our benchmarking methodology provides an approach to evaluating and understanding web agent failure modes at scale.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02418v2",
    "published_date": "2025-10-02 15:22:21 UTC",
    "updated_date": "2025-10-07 15:12:39 UTC"
  },
  {
    "arxiv_id": "2510.02109v1",
    "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification",
    "authors": [
      "Jong Bum Won",
      "Wesley De Neve",
      "Joris Vankerschaver",
      "Utku Ozbulak"
    ],
    "abstract": "Deep neural networks (DNNs) have demonstrated remarkable success in medical imaging, yet their real-world deployment remains challenging due to spurious correlations, where models can learn non-clinical features instead of meaningful medical patterns. Existing medical imaging datasets are not designed to systematically study this issue, largely due to restrictive licensing and limited supplementary patient data. To address this gap, we introduce SpurBreast, a curated breast MRI dataset that intentionally incorporates spurious correlations to evaluate their impact on model performance. Analyzing over 100 features involving patient, device, and imaging protocol, we identify two dominant spurious signals: magnetic field strength (a global feature influencing the entire image) and image orientation (a local feature affecting spatial alignment). Through controlled dataset splits, we demonstrate that DNNs can exploit these non-clinical signals, achieving high validation accuracy while failing to generalize to unbiased test data. Alongside these two datasets containing spurious correlations, we also provide benchmark datasets without spurious correlations, allowing researchers to systematically investigate clinically relevant and irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies. Models and datasets are available at https://github.com/utkuozbulak/spurbreast.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted for publication in the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.02109v1",
    "published_date": "2025-10-02 15:16:20 UTC",
    "updated_date": "2025-10-02 15:16:20 UTC"
  },
  {
    "arxiv_id": "2510.02108v1",
    "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network",
    "authors": [
      "Jinshuo Zhang",
      "Yafei Wang",
      "Xinping Yi",
      "Wenjin Wang",
      "Shi Jin",
      "Symeon Chatzinotas",
      "Bj√∂rn Ottersten"
    ],
    "abstract": "Although symbol-level precoding (SLP) based on constructive interference (CI) exploitation offers performance gains, its high complexity remains a bottleneck. This paper addresses this challenge with an end-to-end deep learning (DL) framework with low inference complexity that leverages the structure of the optimal SLP solution in the closed-form and its inherent tensor equivariance (TE), where TE denotes that a permutation of the input induces the corresponding permutation of the output. Building upon the computationally efficient model-based formulations, as well as their known closed-form solutions, we analyze their relationship with linear precoding (LP) and investigate the corresponding optimality condition. We then construct a mapping from the problem formulation to the solution and prove its TE, based on which the designed networks reveal a specific parameter-sharing pattern that delivers low computational complexity and strong generalization. Leveraging these, we propose the backbone of the framework with an attention-based TE module, achieving linear computational complexity. Furthermore, we demonstrate that such a framework is also applicable to imperfect CSI scenarios, where we design a TE-based network to map the CSI, statistics, and symbols to auxiliary variables. Simulation results show that the proposed framework captures substantial performance gains of optimal SLP, while achieving an approximately 80-times speedup over conventional methods and maintaining strong generalization across user numbers and symbol block lengths.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2510.02108v1",
    "published_date": "2025-10-02 15:15:50 UTC",
    "updated_date": "2025-10-02 15:15:50 UTC"
  },
  {
    "arxiv_id": "2510.02417v1",
    "title": "NEURODNAAI: Neural pipeline approaches for the advancing dna-based information storage as a sustainable digital medium using deep learning framework",
    "authors": [
      "Rakesh Thakur",
      "Lavanya Singh",
      "Yashika",
      "Manomay Bundawala",
      "Aruna Kumar"
    ],
    "abstract": "DNA is a promising medium for digital information storage for its exceptional density and durability. While prior studies advanced coding theory, workflow design, and simulation tools, challenges such as synthesis costs, sequencing errors, and biological constraints (GC-content imbalance, homopolymers) limit practical deployment. To address this, our framework draws from quantum parallelism concepts to enhance encoding diversity and resilience, integrating biologically informed constraints with deep learning to enhance error mitigation in DNA storage. NeuroDNAAI encodes binary data streams into symbolic DNA sequences, transmits them through a noisy channel with substitutions, insertions, and deletions, and reconstructs them with high fidelity. Our results show that traditional prompting or rule-based schemes fail to adapt effectively to realistic noise, whereas NeuroDNAAI achieves superior accuracy. Experiments on benchmark datasets demonstrate low bit error rates for both text and images. By unifying theory, workflow, and simulation into one pipeline, NeuroDNAAI enables scalable, biologically valid archival DNA storage",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02417v1",
    "published_date": "2025-10-02 15:11:04 UTC",
    "updated_date": "2025-10-02 15:11:04 UTC"
  },
  {
    "arxiv_id": "2510.03345v1",
    "title": "Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models",
    "authors": [
      "Luoma Ke",
      "Guangpeng Zhang",
      "Jibo He",
      "Yajing Li",
      "Yan Li",
      "Xufeng Liu",
      "Peng Fang"
    ],
    "abstract": "With the rapid growth of the aviation industry, there is a need for a large number of flight crew. How to select the right pilots in a cost-efficient manner has become an important research question. In the current study, twenty-three pilots were recruited from China Eastern Airlines, and 23 novices were from the community of Tsinghua University. A novel approach incorporating machine learning and virtual reality technology was applied to distinguish features between these participants with different flight skills. Results indicate that SVM with the MIC feature selection method consistently achieved the highest prediction performance on all metrics with an Accuracy of 0.93, an AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier algorithms and two other feature selection methods. From the perspective of feature selection methods, the MIC method can select features with a nonlinear relationship to sampling labels, instead of a simple filter-out. Our new implementation of the SVM + MIC algorithm outperforms all existing pilot selection algorithms and perhaps provides the first implementation based on eye tracking and flight dynamics data. This study's VR simulation platforms and algorithms can be used for pilot selection and training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03345v1",
    "published_date": "2025-10-02 15:10:31 UTC",
    "updated_date": "2025-10-02 15:10:31 UTC"
  },
  {
    "arxiv_id": "2510.02100v1",
    "title": "When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos",
    "authors": [
      "Woowon Jang",
      "Jiwon Im",
      "Juseung Choi",
      "Niki Rashidian",
      "Wesley De Neve",
      "Utku Ozbulak"
    ],
    "abstract": "Video object segmentation (VOS) models such as SAM2 offer promising zero-shot tracking capabilities for surgical videos using minimal user input. Among the available input types, point-based tracking offers an efficient and low-cost alternative, yet its reliability and failure cases in complex surgical environments are not well understood. In this work, we systematically analyze the failure modes of point-based tracking in laparoscopic cholecystectomy videos. Focusing on three surgical targets, the gallbladder, grasper, and L-hook electrocautery, we compare the performance of point-based tracking with segmentation mask initialization. Our results show that point-based tracking is competitive for surgical tools but consistently underperforms for anatomical targets, where tissue similarity and ambiguous boundaries lead to failure. Through qualitative analysis, we reveal key factors influencing tracking outcomes and provide several actionable recommendations for selecting and placing tracking points to improve performance in surgical video analysis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop on Collaborative Intelligence and Autonomy in Image-guided Surgery (COLAS), 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.02100v1",
    "published_date": "2025-10-02 15:06:49 UTC",
    "updated_date": "2025-10-02 15:06:49 UTC"
  },
  {
    "arxiv_id": "2510.02091v3",
    "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning",
    "authors": [
      "Xinyuan Song",
      "Keyu Wang",
      "PengXiang Li",
      "Lu Yin",
      "Shiwei Liu"
    ],
    "abstract": "Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICASSP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.02091v3",
    "published_date": "2025-10-02 14:57:13 UTC",
    "updated_date": "2025-10-31 19:28:21 UTC"
  },
  {
    "arxiv_id": "2510.02416v1",
    "title": "Cross-Platform DNA Methylation Classifier for the Eight Molecular Subtypes of Group 3 & 4 Medulloblastoma",
    "authors": [
      "Omer Abid",
      "Gholamreza Rafiee"
    ],
    "abstract": "Medulloblastoma is a malignant pediatric brain cancer, and the discovery of molecular subgroups is enabling personalized treatment strategies. In 2019, a consensus identified eight novel subtypes within Groups 3 and 4, each displaying heterogeneous characteristics. Classifiers are essential for translating these findings into clinical practice by supporting clinical trials, personalized therapy development and application, and patient monitoring. This study presents a DNA methylation-based, cross-platform machine learning classifier capable of distinguishing these subtypes on both HM450 and EPIC methylation array samples. Across two independent test sets, the model achieved weighted F1 = 0.95 and balanced accuracy = 0.957, consistent across platforms. As the first cross-platform solution, it provides backward compatibility while extending applicability to a newer platform, also enhancing accessibility. It also has the potential to become the first publicly available classifier for these subtypes once deployed through a web application, as planned in the future. This work overall takes steps in the direction of advancing precision medicine and improving clinical outcomes for patients within the majority prevalence medulloblastoma subgroups, groups 3 and 4.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "9 pages, 5 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.02416v1",
    "published_date": "2025-10-02 14:53:38 UTC",
    "updated_date": "2025-10-02 14:53:38 UTC"
  },
  {
    "arxiv_id": "2510.02084v2",
    "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting",
    "authors": [
      "Kuiye Ding",
      "Fanda Fan",
      "Zheya Wang",
      "Hongxiao Li",
      "Yifan Wang",
      "Lei Wang",
      "Chunjie Luo",
      "Jianfeng Zhan"
    ],
    "abstract": "In the World Wide Web, reliable time series forecasts provide the forward-looking signals that drive resource planning, cache placement, and anomaly response, enabling platforms to operate efficiently as user behavior and content distributions evolve. Compared with other domains, time series forecasting for Web applications requires much faster responsiveness to support real-time decision making. We present KAIROS, a non-autoregressive time series forecasting framework that directly models segment-level multi-peak distributions. Unlike autoregressive approaches, KAIROS avoids error accumulation and achieves just-in-time inference, while improving over existing non-autoregressive models that collapse to over-smoothed predictions. Trained on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization on six widely used benchmarks, delivering forecasting performance comparable to state-of-the-art foundation models with similar scale, at a fraction of their inference cost. Beyond empirical results, KAIROS highlights the importance of non-autoregressive design as a scalable paradigm for foundation models in time series.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02084v2",
    "published_date": "2025-10-02 14:50:50 UTC",
    "updated_date": "2025-10-03 05:10:02 UTC"
  },
  {
    "arxiv_id": "2510.03343v1",
    "title": "Defining a Strategic Action Plan for AI in Higher Education",
    "authors": [
      "Nikolaos Avouris"
    ],
    "abstract": "This paper discusses key challenges of Artificial Intelligence in Education, with main focus on higher education institutions. We start with reviewing normative actions of international organizations and concerns expressed about the current technical landscape. Then we proceed with proposing a framework that comprises five key dimensions relating to the main challenges relating to AI in higher education institutions, followed by five key strategic actions that the main stakeholders need to take in order to address the current developments. We map these actions to the main stakeholders of higher education and propose a deployment plan. This defines a framework along the dimensions: Challenges, Actions, Stakeholders, Deployment CASD. Examples of AI specific actions at the institutional and individual course level are also provided and discussed.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "to be cited: N. Avouris (2025), Defining a Strategic Action Plan for AI in Higher Education, Proceedings International Scientific Conference on Digital Competencies in Higher Education, Tirana, September 2025, pp. 141-151",
    "pdf_url": "https://arxiv.org/pdf/2510.03343v1",
    "published_date": "2025-10-02 14:45:26 UTC",
    "updated_date": "2025-10-02 14:45:26 UTC"
  },
  {
    "arxiv_id": "2510.02060v1",
    "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection",
    "authors": [
      "Sanghyu Yoon",
      "Dongmin Kim",
      "Suhee Yoon",
      "Ye Seul Sim",
      "Seungdong Yoa",
      "Hye-Seung Cho",
      "Soonyoung Lee",
      "Hankook Lee",
      "Woohyung Lim"
    ],
    "abstract": "In tabular anomaly detection (AD), textual semantics often carry critical signals, as the definition of an anomaly is closely tied to domain-specific context. However, existing benchmarks provide only raw data points without semantic context, overlooking rich textual metadata such as feature descriptions and domain knowledge that experts rely on in practice. This limitation restricts research flexibility and prevents models from fully leveraging domain knowledge for detection. ReTabAD addresses this gap by restoring textual semantics to enable context-aware tabular AD research. We provide (1) 20 carefully curated tabular datasets enriched with structured textual metadata, together with implementations of state-of-the-art AD algorithms including classical, deep learning, and LLM-based approaches, and (2) a zero-shot LLM framework that leverages semantic context without task-specific training, establishing a strong baseline for future research. Furthermore, this work provides insights into the role and utility of textual metadata in AD through experiments and analysis. Results show that semantic context improves detection performance and enhances interpretability by supporting domain-aware reasoning. These findings establish ReTabAD as a benchmark for systematic exploration of context-aware AD.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02060v1",
    "published_date": "2025-10-02 14:28:45 UTC",
    "updated_date": "2025-10-02 14:28:45 UTC"
  },
  {
    "arxiv_id": "2510.02036v1",
    "title": "The Current State of AI Bias Bounties: An Overview of Existing Programmes and Research",
    "authors": [
      "Sergej Kucenko",
      "Nathaniel Dennler",
      "Fengxiang He"
    ],
    "abstract": "Current bias evaluation methods rarely engage with communities impacted by AI systems. Inspired by bug bounties, bias bounties have been proposed as a reward-based method that involves communities in AI bias detection by asking users of AI systems to report biases they encounter when interacting with such systems. In the absence of a state-of-the-art review, this survey aimed to identify and analyse existing AI bias bounty programmes and to present academic literature on bias bounties. Google, Google Scholar, PhilPapers, and IEEE Xplore were searched, and five bias bounty programmes, as well as five research publications, were identified. All bias bounties were organised by U.S.-based organisations as time-limited contests, with public participation in four programmes and prize pools ranging from 7,000 to 24,000 USD. The five research publications included a report on the application of bug bounties to algorithmic harms, an article addressing Twitter's bias bounty, a proposal for bias bounties as an institutional mechanism to increase AI scrutiny, a workshop discussing bias bounties from queer perspectives, and an algorithmic framework for bias bounties. We argue that reducing the technical requirements to enter bounty programmes is important to include those without coding experience. Given the limited adoption of bias bounties, future efforts should explore the transferability of the best practices from bug bounties and examine how such programmes can be designed to be sensitive to underrepresented groups while lowering adoption barriers for organisations.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "6,227 words (18 pages, from abstract to appendix), one figure, one table, and an appendix with an additional table",
    "pdf_url": "https://arxiv.org/pdf/2510.02036v1",
    "published_date": "2025-10-02 14:09:11 UTC",
    "updated_date": "2025-10-02 14:09:11 UTC"
  },
  {
    "arxiv_id": "2510.02028v1",
    "title": "LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction",
    "authors": [
      "Mario Resino",
      "Borja P√©rez",
      "Jaime Godoy",
      "Abdulla Al-Kaff",
      "Fernando Garc√≠a"
    ],
    "abstract": "This work proposed a 3D autoencoder architecture, named LiLa-Net, which encodes efficient features from real traffic environments, employing only the LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle, equipped with Velodyne LiDAR. The system leverage skip connections concept to improve the performance without using extensive resources as the state-of-the-art architectures. Key changes include reducing the number of encoder layers and simplifying the skip connections, while still producing an efficient and representative latent space which allows to accurately reconstruct the original point cloud. Furthermore, an effective balance has been achieved between the information carried by the skip connections and the latent encoding, leading to improved reconstruction quality without compromising performance. Finally, the model demonstrates strong generalization capabilities, successfully reconstructing objects unrelated to the original traffic environment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 3 figures, 7 tables, Submitted to ICRA",
    "pdf_url": "https://arxiv.org/pdf/2510.02028v1",
    "published_date": "2025-10-02 14:00:20 UTC",
    "updated_date": "2025-10-02 14:00:20 UTC"
  },
  {
    "arxiv_id": "2510.02027v1",
    "title": "Zero-shot reasoning for simulating scholarly peer-review",
    "authors": [
      "Khalid M. Saqr"
    ],
    "abstract": "The scholarly publishing ecosystem faces a dual crisis of unmanageable submission volumes and unregulated AI, creating an urgent need for new governance models to safeguard scientific integrity. The traditional human-only peer review regime lacks a scalable, objective benchmark, making editorial processes opaque and difficult to audit. Here we investigate a deterministic simulation framework that provides the first stable, evidence-based standard for evaluating AI-generated peer review reports. Analyzing 352 peer-review simulation reports, we identify consistent system state indicators that demonstrate its reliability. First, the system is able to simulate calibrated editorial judgment, with 'Revise' decisions consistently forming the majority outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt to field-specific norms, rising to 45% in Health Sciences. Second, it maintains unwavering procedural integrity, enforcing a stable 29% evidence-anchoring compliance rate that remains invariant across diverse review tasks and scientific domains. These findings demonstrate a system that is predictably rule-bound, mitigating the stochasticity of generative AI. For the scientific community, this provides a transparent tool to ensure fairness; for publishing strategists, it offers a scalable instrument for auditing workflows, managing integrity risks, and implementing evidence-based governance. The framework repositions AI as an essential component of institutional accountability, providing the critical infrastructure to maintain trust in scholarly communication.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02027v1",
    "published_date": "2025-10-02 13:59:14 UTC",
    "updated_date": "2025-10-02 13:59:14 UTC"
  },
  {
    "arxiv_id": "2510.05152v1",
    "title": "A Single Character can Make or Break Your LLM Evals",
    "authors": [
      "Jingtong Su",
      "Jianyu Zhang",
      "Karen Ullrich",
      "L√©on Bottou",
      "Mark Ibrahim"
    ],
    "abstract": "Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by $\\pm 23\\%$ depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05152v1",
    "published_date": "2025-10-02 13:27:28 UTC",
    "updated_date": "2025-10-02 13:27:28 UTC"
  },
  {
    "arxiv_id": "2510.02001v2",
    "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework",
    "authors": [
      "Nanaka Hosokawa",
      "Ryo Takahashi",
      "Tomoya Kitano",
      "Yukihiro Iida",
      "Chisako Muramatsu",
      "Tatsuro Hayashi",
      "Yuta Seino",
      "Xiangrong Zhou",
      "Takeshi Hara",
      "Akitoshi Katsumata",
      "Hiroshi Fujita"
    ],
    "abstract": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to automatically generate jaw cyst findings on dental panoramic radiographs. To improve accuracy, we constructed a Self-correction Loop with Structured Output (SLSO) framework and verified its effectiveness. A 10-step process was implemented for 22 cases of jaw cysts, including image input and analysis, structured data generation, tooth number extraction and consistency checking, iterative regeneration when inconsistencies were detected, and finding generation with subsequent restructuring and consistency verification. A comparative experiment was conducted using the conventional Chain-of-Thought (CoT) method across seven evaluation items: transparency, internal structure, borders, root resorption, tooth movement, relationships with other structures, and tooth number. The results showed that the proposed SLSO framework improved output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates for tooth number, tooth movement, and root resorption, respectively. In the successful cases, a consistently structured output was achieved after up to five regenerations. Although statistical significance was not reached because of the small size of the dataset, the overall SLSO framework enforced negative finding descriptions, suppressed hallucinations, and improved tooth number identification accuracy. However, the accurate identification of extensive lesions spanning multiple teeth is limited. Nevertheless, further refinement is required to enhance overall performance and move toward a practical finding generation system.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to Scientific Reports",
    "pdf_url": "https://arxiv.org/pdf/2510.02001v2",
    "published_date": "2025-10-02 13:22:13 UTC",
    "updated_date": "2025-10-06 14:02:39 UTC"
  },
  {
    "arxiv_id": "2510.01994v1",
    "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation",
    "authors": [
      "Chen Yang",
      "Lin Yang",
      "Ziqi Wang",
      "Dong Wang",
      "Jianyi Zhou",
      "Junjie Chen"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled promising performance in unit test generation through in-context learning (ICL). However, the quality of in-context examples significantly influences the effectiveness of generated tests-poorly structured or semantically unclear test examples often lead to suboptimal outputs. In this paper, we propose CLAST, a novel technique that systematically refines unit tests to improve their semantic clarity, thereby enhancing their utility as in-context examples. The approach decomposes complex tests into logically clearer ones and improves semantic clarity through a combination of program analysis and LLM-based rewriting. We evaluated CLAST on four open-source and three industrial projects. The results demonstrate that CLAST largely outperforms UTgen, the state-of-the-art refinement technique, in both preserving test effectiveness and enhancing semantic clarity. Specifically, CLAST fully retains the original effectiveness of unit tests, while UTgen reduces compilation success rate (CSR), pass rate (PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%, 35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user study preferred the semantic clarity of CLAST-refined tests. Notably, incorporating CLAST-refined tests as examples effectively improves ICL-based unit test generation approaches such as RAGGen and TELPA, resulting in an average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for generated tests, compared to incorporating UTgen-refined tests. The insights from the follow-up user study not only reinforce CLAST's potential impact in software testing practice but also illuminate avenues for future research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "accepted in the research track of ASE 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.01994v1",
    "published_date": "2025-10-02 13:15:40 UTC",
    "updated_date": "2025-10-02 13:15:40 UTC"
  },
  {
    "arxiv_id": "2510.02414v2",
    "title": "RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling",
    "authors": [
      "Lin Chen",
      "Jun Chen",
      "Minghui Qiu",
      "Shuxin Zhong",
      "Binghong Chen",
      "Kaishun Wu"
    ],
    "abstract": "Reconstructing high-resolution rainfall fields is essential for flood forecasting, hydrological modeling, and climate analysis. However, existing spatial interpolation methods-whether based on automatic weather station (AWS) measurements or enhanced with satellite/radar observations often over-smooth critical structures, failing to capture sharp transitions and localized extremes. We introduce RainSeer, a structure-aware reconstruction framework that reinterprets radar reflectivity as a physically grounded structural prior-capturing when, where, and how rain develops. This shift, however, introduces two fundamental challenges: (i) translating high-resolution volumetric radar fields into sparse point-wise rainfall observations, and (ii) bridging the physical disconnect between aloft hydro-meteors and ground-level precipitation. RainSeer addresses these through a physics-informed two-stage architecture: a Structure-to-Point Mapper performs spatial alignment by projecting mesoscale radar structures into localized ground-level rainfall, through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the semantic transformation of hydro-meteors through descent, melting, and evaporation via a causal spatiotemporal attention mechanism. We evaluate RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France, 2016-2018)-and observe consistent improvements over state-of-the-art baselines, reducing MAE by over 13.31% and significantly enhancing structural fidelity in reconstructed rainfall fields.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02414v2",
    "published_date": "2025-10-02 13:14:18 UTC",
    "updated_date": "2025-10-07 01:44:40 UTC"
  },
  {
    "arxiv_id": "2510.01967v1",
    "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs",
    "authors": [
      "Aadarsh Anantha Ramakrishnan",
      "Shubham Agarwal",
      "Selvanayagam S",
      "Kunwar Singh"
    ],
    "abstract": "As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at AI-ML Systems 2025, Bangalore, India, https://www.aimlsystems.org/2025/",
    "pdf_url": "https://arxiv.org/pdf/2510.01967v1",
    "published_date": "2025-10-02 12:39:57 UTC",
    "updated_date": "2025-10-02 12:39:57 UTC"
  },
  {
    "arxiv_id": "2510.01958v2",
    "title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement",
    "authors": [
      "Nikolai Lund K√ºhne",
      "Jesper Jensen",
      "Jan √òstergaard",
      "Zheng-Hua Tan"
    ],
    "abstract": "Recent advances in speech enhancement have shown that models combining Mamba and attention mechanisms yield superior cross-corpus generalization performance. At the same time, integrating Mamba in a U-Net structure has yielded state-of-the-art enhancement performance, while reducing both model size and computational complexity. Inspired by these insights, we propose RWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and multi-head attention in a U-Net structure for improved cross-corpus performance. Resolution-wise shared attention (RWSA) refers to layerwise attention-sharing across corresponding time- and frequency resolutions. Our best-performing RWSA-MambaUNet model achieves state-of-the-art generalization performance on two out-of-domain test sets. Notably, our smallest model surpasses all baselines on the out-of-domain DNS 2020 test set in terms of PESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms of SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and a fraction of the FLOPs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to IEEE ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.01958v2",
    "published_date": "2025-10-02 12:27:29 UTC",
    "updated_date": "2026-01-21 11:29:09 UTC"
  },
  {
    "arxiv_id": "2510.03340v1",
    "title": "Learning Pareto-Optimal Pandemic Intervention Policies with MORL",
    "authors": [
      "Marian Chen",
      "Miri Zilka"
    ],
    "abstract": "The COVID-19 pandemic underscored a critical need for intervention strategies that balance disease containment with socioeconomic stability. We approach this challenge by designing a framework for modeling and evaluating disease-spread prevention strategies. Our framework leverages multi-objective reinforcement learning (MORL) - a formulation necessitated by competing objectives - combined with a new stochastic differential equation (SDE) pandemic simulator, calibrated and validated against global COVID-19 data. Our simulator reproduces national-scale pandemic dynamics with orders of magnitude higher fidelity than other models commonly used in reinforcement learning (RL) approaches to pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on this simulator, we illustrate the direct policy trade-offs between epidemiological control and economic stability for COVID-19. Furthermore, we demonstrate the framework's generality by extending it to pathogens with different epidemiological profiles, such as polio and influenza, and show how these profiles lead the agent to discover fundamentally different intervention policies. To ground our work in contemporary policymaking challenges, we apply the model to measles outbreaks, quantifying how a modest 5% drop in vaccination coverage necessitates significantly more stringent and costly interventions to curb disease spread. This work provides a robust and adaptable framework to support transparent, evidence-based policymaking for mitigating public health crises.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "q-bio.PE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03340v1",
    "published_date": "2025-10-02 12:06:29 UTC",
    "updated_date": "2025-10-02 12:06:29 UTC"
  },
  {
    "arxiv_id": "2510.01934v1",
    "title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors",
    "authors": [
      "Guangyao Zhai",
      "Yue Zhou",
      "Xinyan Deng",
      "Lars Heckler",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "abstract": "Few-shot anomaly detection streamlines and simplifies industrial safety inspection. However, limited samples make accurate differentiation between normal and abnormal features challenging, and even more so under category-agnostic conditions. Large-scale pre-training of foundation visual encoders has advanced many fields, as the enormous quantity of data helps to learn the general distribution of normal images. We observe that the anomaly amount in an image directly correlates with the difference in the learnt embeddings and utilize this to design a few-shot anomaly detector termed FoundAD. This is done by learning a nonlinear projection operator onto the natural image manifold. The simple operator acts as an effective tool for anomaly detection to characterize and identify out-of-distribution regions in an image. Extensive experiments show that our approach supports multi-class detection and achieves competitive performance while using substantially fewer parameters than prior methods. Backed up by evaluations with multiple foundation encoders, including fresh DINOv3, we believe this idea broadens the perspective on foundation features and advances the field of few-shot anomaly detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 13 figures. Code is available at \\url{https://github.com/ymxlzgy/FoundAD}",
    "pdf_url": "https://arxiv.org/pdf/2510.01934v1",
    "published_date": "2025-10-02 11:53:20 UTC",
    "updated_date": "2025-10-02 11:53:20 UTC"
  },
  {
    "arxiv_id": "2510.01924v1",
    "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning",
    "authors": [
      "Crystal Qian",
      "Aaron Parisi",
      "Cl√©mentine Bouleau",
      "Vivian Tsai",
      "Ma√´l Lebreton",
      "Lucas Dixon"
    ],
    "abstract": "As large language models (LLMs) are increasingly used to model and augment collective decision-making, it is critical to examine their alignment with human social reasoning. We present an empirical framework for assessing collective alignment, in contrast to prior work on the individual level. Using the Lost at Sea social psychology task, we conduct a large-scale online experiment (N=748), randomly assigning groups to leader elections with either visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We then simulate matched LLM groups conditioned on the human data, benchmarking Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some mirror human biases; others mask these biases and attempt to compensate for them. We empirically demonstrate that human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases. Understanding how LLMs align with collective human behavior is critical to advancing socially-aligned AI, and demands dynamic benchmarks that capture the complexities of collective reasoning.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01924v1",
    "published_date": "2025-10-02 11:41:30 UTC",
    "updated_date": "2025-10-02 11:41:30 UTC"
  },
  {
    "arxiv_id": "2510.01914v2",
    "title": "Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models",
    "authors": [
      "Wei-Lung Mao",
      "Chun-Chi Wang",
      "Po-Heng Chou",
      "Yen-Ting Liu"
    ],
    "abstract": "Since the defect detection of conventional industry components is time-consuming and labor-intensive, it leads to a significant burden on quality inspection personnel and makes it difficult to manage product quality. In this paper, we propose an automated defect detection system for the dual in-line package (DIP) that is widely used in industry, using digital camera optics and a deep learning (DL)-based model. The two most common defect categories of DIP are examined: (1) surface defects, and (2) pin-leg defects. However, the lack of defective component images leads to a challenge for detection tasks. To solve this problem, the ConSinGAN is used to generate a suitable-sized dataset for training and testing. Four varieties of the YOLO model are investigated (v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation. The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in accuracy of 95.50\\%, detection time of 285 ms, and is far superior to threshold-based approaches. In addition, the supervisory control and data acquisition (SCADA) system is developed, and the associated sensor architecture is described. The proposed automated defect detection can be easily established with numerous types of defects or insufficient defect data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 16 figures, 7 tables, and published in IEEE Sensors Journal",
    "pdf_url": "https://arxiv.org/pdf/2510.01914v2",
    "published_date": "2025-10-02 11:33:16 UTC",
    "updated_date": "2025-10-04 03:11:42 UTC"
  },
  {
    "arxiv_id": "2510.01910v1",
    "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement",
    "authors": [
      "Zhaoyan Wang",
      "Zheng Gao",
      "Arogya Kharel",
      "In-Young Ko"
    ],
    "abstract": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications, serving as a core technique for learning from graph-structured data, such as text-attributed graphs. Yet in real-world scenarios, such graphs exhibit deficiencies that substantially undermine GNN performance. While prior GNN-based augmentation studies have explored robustness against individual imperfections, a systematic understanding of how graph-native and Large Language Models (LLMs) enhanced methods behave under compound deficiencies is still missing. Specifically, there has been no comprehensive investigation comparing conventional approaches and recent LLM-on-graph frameworks, leaving their merits unclear. To fill this gap, we conduct the first empirical study that benchmarks these two lines of methods across diverse graph deficiencies, revealing overlooked vulnerabilities and challenging the assumption that LLM augmentation is consistently superior. Building on empirical findings, we propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement (RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is the first iterative paradigm that leverages Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations by supplying class-consistent, diverse augmentations and enforcing discriminative representations through iterative graph contrastive learning. It transforms LLM augmentation for graphs from static signal injection into dynamic refinement. Extensive experiments demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.01910v1",
    "published_date": "2025-10-02 11:30:51 UTC",
    "updated_date": "2025-10-02 11:30:51 UTC"
  },
  {
    "arxiv_id": "2510.01902v1",
    "title": "Constrained Adaptive Rejection Sampling",
    "authors": [
      "Pawe≈Ç Parys",
      "Sairam Vaidya",
      "Taylor Berg-Kirkpatrick",
      "Loris D'Antoni"
    ],
    "abstract": "Language Models (LMs) are increasingly used in applications where generated outputs must satisfy strict semantic or syntactic constraints. Existing approaches to constrained generation fall along a spectrum: greedy constrained decoding methods enforce validity during decoding but distort the LM's distribution, while rejection sampling (RS) preserves fidelity but wastes computation by discarding invalid outputs. Both extremes are problematic in domains such as program fuzzing, where both validity and diversity of samples are essential. We present Constrained Adaptive Rejection Sampling (CARS), an approach that strictly improves the sample-efficiency of RS without distributional distortion. CARS begins with unconstrained LM sampling and adaptively rules out constraint-violating continuations by recording them in a trie and subtracting their probability mass from future draws. This adaptive pruning ensures that prefixes proven invalid are never revisited, acceptance rates improve monotonically, and the resulting samples exactly follow the constrained distribution. In experiments on a variety of domains -- e.g., program fuzzing and molecular generation -- CARS consistently achieves higher efficiency -- measured in the number of LM forward passes per valid sample -- while also producing stronger sample diversity than both GCD and methods that approximate the LM's distribution.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01902v1",
    "published_date": "2025-10-02 11:17:26 UTC",
    "updated_date": "2025-10-02 11:17:26 UTC"
  },
  {
    "arxiv_id": "2510.03339v1",
    "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models",
    "authors": [
      "Sofiane Ennadir",
      "Levente Z√≥lyomi",
      "Oleg Smirnov",
      "Tianze Wang",
      "John Pertoft",
      "Filip Cornell",
      "Lele Cao"
    ],
    "abstract": "Transformer models have become the dominant backbone for sequence modeling, leveraging self-attention to produce contextualized token representations. These are typically aggregated into fixed-size vectors via pooling operations for downstream tasks. While much of the literature has focused on attention mechanisms, the role of pooling remains underexplored despite its critical impact on model behavior. In this paper, we introduce a theoretical framework that rigorously characterizes the expressivity of Transformer-based models equipped with widely used pooling methods by deriving closed-form bounds on their representational capacity and the ability to distinguish similar inputs. Our analysis extends to different variations of attention formulations, demonstrating that these bounds hold across diverse architectural variants. We empirically evaluate pooling strategies across tasks requiring both global and local contextual understanding, spanning three major modalities: computer vision, natural language processing, and time-series analysis. Results reveal consistent trends in how pooling choices affect accuracy, sensitivity, and optimization behavior. Our findings unify theoretical and empirical perspectives, providing practical guidance for selecting or designing pooling mechanisms suited to specific tasks. This work positions pooling as a key architectural component in Transformer models and lays the foundation for more principled model design beyond attention alone.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03339v1",
    "published_date": "2025-10-02 11:17:24 UTC",
    "updated_date": "2025-10-02 11:17:24 UTC"
  },
  {
    "arxiv_id": "2510.01899v2",
    "title": "Multimodal Foundation Models for Early Disease Detection",
    "authors": [
      "Md Talha Mohsin",
      "Ismail Abdulrashid"
    ],
    "abstract": "Healthcare data now span EHRs, medical imaging, genomics, and wearable sensors, but most diagnostic models still process these modalities in isolation. This limits their ability to capture early, cross-modal disease signatures. This paper introduces a multimodal foundation model built on a transformer architecture that integrates heterogeneous clinical data through modality-specific encoders and cross-modal attention. Each modality is mapped into a shared latent space and fused using multi-head attention with residual normalization. We implement the framework using a multimodal dataset that simulates early-stage disease patterns across EHR sequences, imaging patches, genomic profiles, and wearable signals, including missing-modality scenarios and label noise. The model is trained using supervised classification together with self-supervised reconstruction and contrastive alignment to improve robustness. Experimental evaluation demonstrates strong performance in early-detection settings, with stable classification metrics, reliable uncertainty estimates, and interpretable attention patterns. The approach moves toward a flexible, pretrain-and-fine-tune foundation model that supports precision diagnostics, handles incomplete inputs, and improves early disease detection across oncology, cardiology, and neurology applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.01899v2",
    "published_date": "2025-10-02 11:12:57 UTC",
    "updated_date": "2025-12-16 20:46:13 UTC"
  },
  {
    "arxiv_id": "2510.01891v1",
    "title": "HRTFformer: A Spatially-Aware Transformer for Personalized HRTF Upsampling in Immersive Audio Rendering",
    "authors": [
      "Xuyi Hu",
      "Jian Li",
      "Shaojie Zhang",
      "Stefan Goetz",
      "Lorenzo Picinali",
      "Ozgur B. Akan",
      "Aidan O. T. Hogg"
    ],
    "abstract": "Personalized Head-Related Transfer Functions (HRTFs) are starting to be introduced in many commercial immersive audio applications and are crucial for realistic spatial audio rendering. However, one of the main hesitations regarding their introduction is that creating personalized HRTFs is impractical at scale due to the complexities of the HRTF measurement process. To mitigate this drawback, HRTF spatial upsampling has been proposed with the aim of reducing measurements required. While prior work has seen success with different machine learning (ML) approaches, these models often struggle with long-range spatial consistency and generalization at high upsampling factors. In this paper, we propose a novel transformer-based architecture for HRTF upsampling, leveraging the attention mechanism to better capture spatial correlations across the HRTF sphere. Working in the spherical harmonic (SH) domain, our model learns to reconstruct high-resolution HRTFs from sparse input measurements with significantly improved accuracy. To enhance spatial coherence, we introduce a neighbor dissimilarity loss that promotes magnitude smoothness, yielding more realistic upsampling. We evaluate our method using both perceptual localization models and objective spectral distortion metrics. Experiments show that our model surpasses leading methods by a substantial margin in generating realistic, high-fidelity HRTFs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "10 pages and 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.01891v1",
    "published_date": "2025-10-02 10:59:21 UTC",
    "updated_date": "2025-10-02 10:59:21 UTC"
  },
  {
    "arxiv_id": "2510.01889v1",
    "title": "Small is Sufficient: Reducing the World AI Energy Consumption Through Model Selection",
    "authors": [
      "Tiago da Silva Barros",
      "Fr√©d√©ric Giroire",
      "Ramon Aparicio-Pardo",
      "Joanna Moulierac"
    ],
    "abstract": "The energy consumption and carbon footprint of Artificial Intelligence (AI) have become critical concerns due to rising costs and environmental impacts. In response, a new trend in green AI is emerging, shifting from the \"bigger is better\" paradigm, which prioritizes large models, to \"small is sufficient\", emphasizing energy sobriety through smaller, more efficient models.\n  We explore how the AI community can adopt energy sobriety today by focusing on model selection during inference. Model selection consists of choosing the most appropriate model for a given task, a simple and readily applicable method, unlike approaches requiring new hardware or architectures. Our hypothesis is that, as in many industrial activities, marginal utility gains decrease with increasing model size. Thus, applying model selection can significantly reduce energy consumption while maintaining good utility for AI inference.\n  We conduct a systematic study of AI tasks, analyzing their popularity, model size, and efficiency. We examine how the maturity of different tasks and model adoption patterns impact the achievable energy savings, ranging from 1% to 98% for different tasks. Our estimates indicate that applying model selection could reduce AI energy consumption by 27.8%, saving 31.9 TWh worldwide in 2025 - equivalent to the annual output of five nuclear power reactors.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01889v1",
    "published_date": "2025-10-02 10:58:13 UTC",
    "updated_date": "2025-10-02 10:58:13 UTC"
  },
  {
    "arxiv_id": "2510.01887v1",
    "title": "FINCH: Financial Intelligence using Natural language for Contextualized SQL Handling",
    "authors": [
      "Avinash Kumar Singh",
      "Bhaskarjit Sarmah",
      "Stefano Pasquali"
    ],
    "abstract": "Text-to-SQL, the task of translating natural language questions into SQL queries, has long been a central challenge in NLP. While progress has been significant, applying it to the financial domain remains especially difficult due to complex schema, domain-specific terminology, and high stakes of error. Despite this, there is no dedicated large-scale financial dataset to advance research, creating a critical gap. To address this, we introduce a curated financial dataset (FINCH) comprising 292 tables and 75,725 natural language-SQL pairs, enabling both fine-tuning and rigorous evaluation. Building on this resource, we benchmark reasoning models and language models of varying scales, providing a systematic analysis of their strengths and limitations in financial Text-to-SQL tasks. Finally, we propose a finance-oriented evaluation metric (FINCH Score) that captures nuances overlooked by existing measures, offering a more faithful assessment of model performance.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01887v1",
    "published_date": "2025-10-02 10:55:11 UTC",
    "updated_date": "2025-10-02 10:55:11 UTC"
  },
  {
    "arxiv_id": "2510.01879v1",
    "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration",
    "authors": [
      "Yisu Wang",
      "Ming Wang",
      "Haoyuan Song",
      "Wenjie Huang",
      "Chaozheng Wang",
      "Yi Xie",
      "Xuming Ran"
    ],
    "abstract": "Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01879v1",
    "published_date": "2025-10-02 10:35:39 UTC",
    "updated_date": "2025-10-02 10:35:39 UTC"
  },
  {
    "arxiv_id": "2510.05150v2",
    "title": "Chronological Thinking in Full-Duplex Spoken Dialogue Language Models",
    "authors": [
      "Donghang Wu",
      "Haoyang Zhang",
      "Chen Chen",
      "Tianyu Zhang",
      "Fei Tian",
      "Xuerui Yang",
      "Gang Yu",
      "Hexin Liu",
      "Nana Hou",
      "Yuchen Hu",
      "Eng Siong Chng"
    ],
    "abstract": "Recent advances in spoken dialogue language models (SDLMs) reflect growing interest in shifting from turn-based to full-duplex systems, where the models continuously perceive user speech streams while generating responses. This simultaneous listening and speaking design enables real-time interaction and the agent can handle dynamic conversational behaviors like user barge-in. However, during the listening phase, existing systems keep the agent idle by repeatedly predicting the silence token, which departs from human behavior: we usually engage in lightweight thinking during conversation rather than remaining absent-minded. Inspired by this, we propose Chronological Thinking, a on-the-fly conversational thinking mechanism that aims to improve response quality in full-duplex SDLMs. Specifically, chronological thinking presents a paradigm shift from conventional LLM thinking approaches, such as Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly causal: the agent reasons incrementally while listening, updating internal hypotheses only from past audio with no lookahead. (2) No additional latency: reasoning is amortized during the listening window; once the user stops speaking, the agent halts thinking and begins speaking without further delay. Experiments demonstrate the effectiveness of chronological thinking through both objective metrics and human evaluations show consistent improvements in response quality. Furthermore, chronological thinking robustly handles conversational dynamics and attains competitive performance on full-duplex interaction metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05150v2",
    "published_date": "2025-10-02 10:28:11 UTC",
    "updated_date": "2025-10-08 21:35:40 UTC"
  },
  {
    "arxiv_id": "2510.01869v2",
    "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
    "authors": [
      "Alessandro Nazzari",
      "Roberto Rubinacci",
      "Marco Lovera"
    ],
    "abstract": "When a single pilot is responsible for managing a multi-drone system, the task may demand varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system on a real-world multi-drone system, and conduct an ablation study to assess the contribution of each module.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 9 figures, submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2510.01869v2",
    "published_date": "2025-10-02 10:21:35 UTC",
    "updated_date": "2025-12-18 13:11:15 UTC"
  },
  {
    "arxiv_id": "2510.01864v1",
    "title": "A Modular Theory of Subjective Consciousness for Natural and Artificial Minds",
    "authors": [
      "Micha√´l Gillon"
    ],
    "abstract": "Understanding how subjective experience arises from information processing remains a central challenge in neuroscience, cognitive science, and AI research. The Modular Consciousness Theory (MCT) proposes a biologically grounded and computationally explicit framework in which consciousness is a discrete sequence of Integrated Informational States (IISs). Each IIS is a packet of integrated information tagged with a multidimensional density vector that quantifies informational richness. Its magnitude correlates with subjective intensity, shaping memory, behavior, and continuity of experience. Inputs from body and environment are adaptively filtered, processed by modules (abstraction, narration, evaluation, self-evaluation), and integrated into an IIS. The resulting packet, tagged with its density vector, is transmitted to behavioral readiness, memory, and decision-making modules, closing the loop. This explains why strongly tagged states exert greater influence on long-term memory and action. Unlike Global Workspace Theory, Integrated Information Theory, or Higher-Order Thought, MCT specifies a full computational pipeline producing discrete informational units with quantifiable internal structure. Subjectivity is reframed as a correlate of the density-tagging signal with functional consequences. MCT generates testable predictions, such as stress enhancing memory encoding, and provides a naturalistic blueprint for both biological and artificial architectures. Consciousness, in this view, is not an irreducible essence but an evolvable, quantifiable, and constructible feature of complex information processing.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "41 pages, 3 figures. Under review, comments welcome",
    "pdf_url": "https://arxiv.org/pdf/2510.01864v1",
    "published_date": "2025-10-02 10:11:56 UTC",
    "updated_date": "2025-10-02 10:11:56 UTC"
  },
  {
    "arxiv_id": "2510.01857v1",
    "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning",
    "authors": [
      "Claudio Fanconi",
      "Nicol√°s Astorga",
      "Mihaela van der Schaar"
    ],
    "abstract": "We reframe and operationalise adversarial inverse reinforcement learning (IRL) to large language model reasoning, learning a dense, token-level reward model for process supervision directly from expert demonstrations rather than imitating style via supervised fine-tuning. The learned reasoning reward serves two complementary roles: (i) it provides step-level feedback to optimise a reasoning policy during training; and (ii) it functions at inference as a critic to rerank sampled traces under fixed compute budgets. We demonstrate that our approach prioritises correctness over surface form, yielding scores that correlate with eventual answer validity and enabling interpretable localisation of errors within a trace. Empirically, on GSM8K with Llama3 and Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a learning signal to elicit reasoning, and (ii) predictive performance is improved from reward-guided reranking (notably for Llama-based policies). By unifying training signals, inference-time selection, and token-level diagnostics into a single reasoning reward, this work suggests reusable process-level rewards with broad potential to enhance multi-step reasoning in language models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01857v1",
    "published_date": "2025-10-02 09:55:26 UTC",
    "updated_date": "2025-10-02 09:55:26 UTC"
  },
  {
    "arxiv_id": "2510.01850v3",
    "title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications",
    "authors": [
      "Ying-Ren Chien",
      "Po-Heng Chou",
      "You-Jie Peng",
      "Chun-Yuan Huang",
      "Hen-Wai Tsao",
      "Yu Tsao"
    ],
    "abstract": "To effectively process impulse noise for narrowband powerline communications (NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic asynchronous impulsive noise (APIN) is a critical task. However, existing mathematical noise generative models only capture part of the characteristics of noise. In this study, we propose a novel generative adversarial network (GAN) called noise generation GAN (NGGAN) that learns the complicated characteristics of practically measured noise samples for data synthesis. To closely match the statistics of complicated noise over the NB-PLC systems, we measured the NB-PLC noise via the analog coupling and bandpass filtering circuits of a commercial NB-PLC modem to build a realistic dataset. To train NGGAN, we adhere to the following principles: 1) we design the length of input signals that the NGGAN model can fit to facilitate cyclostationary noise generation; 2) the Wasserstein distance is used as a loss function to enhance the similarity between the generated noise and training data; and 3) to measure the similarity performances of GAN-based models based on the mathematical and practically measured datasets, we conduct both quantitative and qualitative analyses. The training datasets include: 1) a piecewise spectral cyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter; and 3) practical measurements from NB-PLC systems. Simulation results demonstrate that the generated noise samples from the proposed NGGAN are highly close to the real noise samples. The principal component analysis (PCA) scatter plots and Fr√©chet inception distance (FID) analysis have shown that NGGAN outperforms other GAN-based models by generating noise samples with superior fidelity and higher diversity.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "16 pages, 15 figures, 11 tables, and published in IEEE Transactions on Instrumentation and Measurement, 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.01850v3",
    "published_date": "2025-10-02 09:47:56 UTC",
    "updated_date": "2025-10-29 15:33:51 UTC"
  },
  {
    "arxiv_id": "2510.01842v1",
    "title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets",
    "authors": [
      "Yannis Belkhiter",
      "Seshu Tirupathi",
      "Giulio Zizzo",
      "Sachin Sharma",
      "John D. Kelleher"
    ],
    "abstract": "The field of AutoML has made remarkable progress in post-hoc model selection, with libraries capable of automatically identifying the most performing models for a given dataset. Nevertheless, these methods often rely on exhaustive hyperparameter searches, where methods automatically train and test different types of models on the target dataset. Contrastingly, pre-hoc prediction emerges as a promising alternative, capable of bypassing exhaustive search through intelligent pre-selection of models. Despite its potential, pre-hoc prediction remains under-explored in the literature. This paper explores the intersection of AutoML and pre-hoc model selection by leveraging traditional models and Large Language Model (LLM) agents to reduce the search space of AutoML libraries. By relying on dataset descriptions and statistical information, we reduce the AutoML search space. Our methodology is applied to the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark containing 175 tabular classification datasets available on OpenML. The proposed approach offers a shift in AutoML workflows, significantly reducing computational overhead, while still selecting the best model for the given dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Oral Presentations ADAPT Annual Scientific Conference 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.01842v1",
    "published_date": "2025-10-02 09:37:12 UTC",
    "updated_date": "2025-10-02 09:37:12 UTC"
  },
  {
    "arxiv_id": "2510.01833v1",
    "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning",
    "authors": [
      "Zhihao Dou",
      "Qinjian Zhao",
      "Zhongwei Wan",
      "Dinggen Zhang",
      "Weida Wang",
      "Towsif Raiyan",
      "Benteng Chen",
      "Qingtao Pan",
      "Yang Ouyang",
      "Zhiqiang Gao",
      "Shufei Zhang",
      "Sumon Biswas"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning abilities in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However, due to their autoregressive token-level generation, the reasoning process is largely constrained to local decision-making and lacks global planning. This limitation frequently results in redundant, incoherent, or inaccurate reasoning, which significantly degrades overall performance. Existing approaches, such as tree-based algorithms and reinforcement learning (RL), attempt to address this issue but suffer from high computational costs and often fail to produce optimal reasoning trajectories. To tackle this challenge, we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy Optimization PTA-GRPO, a two-stage framework designed to improve both high-level planning and fine-grained CoT reasoning. In the first stage, we leverage advanced LLMs to distill CoT into compact high-level guidance, which is then used for supervised fine-tuning (SFT). In the second stage, we introduce a guidance-aware RL method that jointly optimizes the final output and the quality of high-level guidance, thereby enhancing reasoning effectiveness. We conduct extensive experiments on multiple mathematical reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently achieves stable and significant improvements across different models and tasks, validating its effectiveness and generalization.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages and 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.01833v1",
    "published_date": "2025-10-02 09:28:13 UTC",
    "updated_date": "2025-10-02 09:28:13 UTC"
  },
  {
    "arxiv_id": "2510.01815v1",
    "title": "Human-AI Teaming Co-Learning in Military Operations",
    "authors": [
      "Clara Maathuis",
      "Kasper Cools"
    ],
    "abstract": "In a time of rapidly evolving military threats and increasingly complex operational environments, the integration of AI into military operations proves significant advantages. At the same time, this implies various challenges and risks regarding building and deploying human-AI teaming systems in an effective and ethical manner. Currently, understanding and coping with them are often tackled from an external perspective considering the human-AI teaming system as a collective agent. Nevertheless, zooming into the dynamics involved inside the system assures dealing with a broader palette of relevant multidimensional responsibility, safety, and robustness aspects. To this end, this research proposes the design of a trustworthy co-learning model for human-AI teaming in military operations that encompasses a continuous and bidirectional exchange of insights between the human and AI agents as they jointly adapt to evolving battlefield conditions. It does that by integrating four dimensions. First, adjustable autonomy for dynamically calibrating the autonomy levels of agents depending on aspects like mission state, system confidence, and environmental uncertainty. Second, multi-layered control which accounts continuous oversight, monitoring of activities, and accountability. Third, bidirectional feedback with explicit and implicit feedback loops between the agents to assure a proper communication of reasoning, uncertainties, and learned adaptations that each of the agents has. And fourth, collaborative decision-making which implies the generation, evaluation, and proposal of decisions associated with confidence levels and rationale behind them. The model proposed is accompanied by concrete exemplifications and recommendations that contribute to further developing responsible and trustworthy human-AI teaming systems in military operations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to Sensors + Imaging; presented on 18th of September (Artificial Intelligence for Security and Defence Applications III)",
    "pdf_url": "https://arxiv.org/pdf/2510.01815v1",
    "published_date": "2025-10-02 09:01:01 UTC",
    "updated_date": "2025-10-02 09:01:01 UTC"
  },
  {
    "arxiv_id": "2510.05149v1",
    "title": "Percepta: High Performance Stream Processing at the Edge",
    "authors": [
      "Clarisse Sousa",
      "Tiago Fonseca",
      "Luis Lino Ferreira",
      "Ricardo Ven√¢ncio",
      "Ricardo Severino"
    ],
    "abstract": "The rise of real-time data and the proliferation of Internet of Things (IoT) devices have highlighted the limitations of cloud-centric solutions, particularly regarding latency, bandwidth, and privacy. These challenges have driven the growth of Edge Computing. Associated with IoT appears a set of other problems, like: data rate harmonization between multiple sources, protocol conversion, handling the loss of data and the integration with Artificial Intelligence (AI) models. This paper presents Percepta, a lightweight Data Stream Processing (DSP) system tailored to support AI workloads at the edge, with a particular focus on such as Reinforcement Learning (RL). It introduces specialized features such as reward function computation, data storage for model retraining, and real-time data preparation to support continuous decision-making. Additional functionalities include data normalization, harmonization across heterogeneous protocols and sampling rates, and robust handling of missing or incomplete data, making it well suited for the challenges of edge-based AI deployment.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05149v1",
    "published_date": "2025-10-02 08:57:45 UTC",
    "updated_date": "2025-10-02 08:57:45 UTC"
  },
  {
    "arxiv_id": "2510.01812v2",
    "title": "SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment",
    "authors": [
      "Yuxun Tang",
      "Lan Liu",
      "Wenhao Feng",
      "Yiwen Zhao",
      "Jionghao Han",
      "Yifeng Yu",
      "Jiatong Shi",
      "Qin Jin"
    ],
    "abstract": "Singing voice generation progresses rapidly, yet evaluating singing quality remains a critical challenge. Human subjective assessment, typically in the form of listening tests, is costly and time consuming, while existing objective metrics capture only limited perceptual aspects. In this work, we introduce SingMOS-Pro, a dataset for automatic singing quality assessment. Building on our preview version SingMOS, which provides only overall ratings, SingMOS-Pro expands annotations of the additional part to include lyrics, melody, and overall quality, offering broader coverage and greater diversity. The dataset contains 7,981 singing clips generated by 41 models across 12 datasets, spanning from early systems to recent advances. Each clip receives at least five ratings from professional annotators, ensuring reliability and consistency. Furthermore, we explore how to effectively utilize MOS data annotated under different standards and benchmark several widely used evaluation methods from related tasks on SingMOS-Pro, establishing strong baselines and practical references for future research. The dataset can be accessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "4 pages, 5 figures;",
    "pdf_url": "https://arxiv.org/pdf/2510.01812v2",
    "published_date": "2025-10-02 08:53:49 UTC",
    "updated_date": "2025-10-03 05:07:06 UTC"
  },
  {
    "arxiv_id": "2510.01800v2",
    "title": "REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing",
    "authors": [
      "Thanh Ma",
      "Tri-Tam La",
      "Lam-Thu Le Huu",
      "Minh-Nghi Nguyen",
      "Khanh-Van Pham Luu"
    ],
    "abstract": "Academic regulation advising is essential for helping students interpret and comply with institutional policies, yet building effective systems requires domain specific regulatory resources. To address this challenge, we propose REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval reasoning framework that integrates retrieval augmented generation with graph based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported by a hierarchical, category labeled knowledge graph enriched with semantic features for domain alignment. A lightweight intent classifier routes queries to the appropriate retrieval modules, ensuring both factual accuracy and contextual depth. We construct a regulation specific dataset and evaluate REBot on classification and question answering tasks, achieving state of the art performance with an F1 score of 98.89%. Finally, we implement a web application that demonstrates the practical value of REBot in real world academic advising scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01800v2",
    "published_date": "2025-10-02 08:40:55 UTC",
    "updated_date": "2025-11-29 04:40:57 UTC"
  },
  {
    "arxiv_id": "2510.01796v1",
    "title": "Rethinking the shape convention of an MLP",
    "authors": [
      "Meng-Hsi Chen",
      "Yu-Ang Lee",
      "Feng-Ting Liao",
      "Da-shan Shiu"
    ],
    "abstract": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01796v1",
    "published_date": "2025-10-02 08:38:15 UTC",
    "updated_date": "2025-10-02 08:38:15 UTC"
  },
  {
    "arxiv_id": "2510.01795v2",
    "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving",
    "authors": [
      "Haibo Hu",
      "Lianming Huang",
      "Xinyu Wang",
      "Yufei Cui",
      "Shangyu Wu",
      "Nan Guan",
      "Chun Jason Xue"
    ],
    "abstract": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving for unified perception and reasoning, but high inference latency hinders real-time deployment. Early-exit reduces latency by terminating inference at intermediate layers, yet its task-dependent nature limits generalization across diverse scenarios. We observe that this limitation aligns with autonomous driving: navigation systems can anticipate upcoming contexts (e.g., intersections, traffic lights), indicating which tasks will be required. We propose Nav-EE, a navigation-guided early-exit framework that precomputes task-specific exit layers offline and dynamically applies them online based on navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE achieves accuracy comparable to full inference while reducing latency by up to 63.9%. Real-vehicle integration with Autoware Universe further demonstrates reduced inference latency (600ms to 300ms), supporting faster decision-making in complex scenarios. These results suggest that coupling navigation foresight with early-exit offers a viable path toward efficient deployment of large models in autonomous systems. Code and data are available at our anonymous repository: https://anonymous.4open.science/r/Nav-EE-BBC4",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01795v2",
    "published_date": "2025-10-02 08:37:58 UTC",
    "updated_date": "2025-10-10 08:31:16 UTC"
  },
  {
    "arxiv_id": "2510.01792v1",
    "title": "Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction",
    "authors": [
      "Ivan Leonidovich Litvak",
      "Anton Kostin",
      "Fedor Lashkin",
      "Tatiana Maksiyan",
      "Sergey Lagutin"
    ],
    "abstract": "The rapid advancement of artificial intelligence in legal natural language processing demands scalable methods for evaluating text extraction from judicial decisions. This study evaluates 16 unsupervised metrics, including novel formulations, to assess the quality of extracting seven semantic blocks from 1,000 anonymized Russian judicial decisions, validated against 7,168 expert reviews on a 1--5 Likert scale. These metrics, spanning document-based, semantic, structural, pseudo-ground truth, and legal-specific categories, operate without pre-annotated ground truth. Bootstrapped correlations, Lin's concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE = 0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC = 0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density (Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These findings highlight that unsupervised metrics, including LLM-based approaches, enable scalable screening but, with moderate correlations and low CCC values, cannot fully replace human judgment in high-stakes legal contexts. This work advances legal NLP by providing annotation-free evaluation tools, with implications for judicial analytics and ethical AI deployment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.01792v1",
    "published_date": "2025-10-02 08:32:16 UTC",
    "updated_date": "2025-10-02 08:32:16 UTC"
  },
  {
    "arxiv_id": "2510.01784v2",
    "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
    "authors": [
      "Xiaofei Wu",
      "Guozhen Zhang",
      "Zhiyong Xu",
      "Yuan Zhou",
      "Qinglin Lu",
      "Xuming He"
    ],
    "abstract": "Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01784v2",
    "published_date": "2025-10-02 08:22:46 UTC",
    "updated_date": "2025-10-03 16:01:28 UTC"
  },
  {
    "arxiv_id": "2510.01782v1",
    "title": "Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks",
    "authors": [
      "Wenbo Pan",
      "Jie Xu",
      "Qiguang Chen",
      "Junhao Dong",
      "Libo Qin",
      "Xinfeng Li",
      "Haining Yu",
      "Xiaohua Jia"
    ],
    "abstract": "Large Language Models (LLMs) should refuse to answer questions beyond their knowledge. This capability, which we term knowledge-aware refusal, is crucial for factual reliability. However, existing metrics fail to faithfully measure this ability. On the one hand, simple refusal-based metrics are biased by refusal rates and yield inconsistent scores when models exhibit different refusal tendencies. On the other hand, existing calibration metrics are proxy-based, capturing the performance of auxiliary calibration processes rather than the model's actual refusal behavior. In this work, we propose the Refusal Index (RI), a principled metric that measures how accurately LLMs refuse questions they do not know. We define RI as Spearman's rank correlation between refusal probability and error probability. To make RI practically measurable, we design a lightweight two-pass evaluation method that efficiently estimates RI from observed refusal rates across two standard evaluation runs. Extensive experiments across 16 models and 5 datasets demonstrate that RI accurately quantifies a model's intrinsic knowledge-aware refusal capability in factual tasks. Notably, RI remains stable across different refusal rates and provides consistent model rankings independent of a model's overall accuracy and refusal rates. More importantly, RI provides insight into an important but previously overlooked aspect of LLM factuality: while LLMs achieve high accuracy on factual tasks, their refusal behavior can be unreliable and fragile. This finding highlights the need to complement traditional accuracy metrics with the Refusal Index for comprehensive factuality evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01782v1",
    "published_date": "2025-10-02 08:20:36 UTC",
    "updated_date": "2025-10-02 08:20:36 UTC"
  },
  {
    "arxiv_id": "2510.01780v1",
    "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP",
    "authors": [
      "Aueaphum Aueawatthanaphisut"
    ],
    "abstract": "Secure and interoperable integration of heterogeneous medical data remains a grand challenge in digital health. Current federated learning (FL) frameworks offer privacy-preserving model training but lack standardized mechanisms to orchestrate multi-modal data fusion across distributed and resource-constrained environments. This study introduces a novel framework that leverages the Model Context Protocol (MCP) as an interoperability layer for secure, cross-agent communication in multi-modal federated healthcare systems. The proposed architecture unifies three pillars: (i) multi-modal feature alignment for clinical imaging, electronic medical records, and wearable IoT data; (ii) secure aggregation with differential privacy to protect patient-sensitive updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile clients. By employing MCP as a schema-driven interface, the framework enables adaptive orchestration of AI agents and toolchains while ensuring compliance with privacy regulations. Experimental evaluation on benchmark datasets and pilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic accuracy compared with baseline FL, a 54\\% reduction in client dropout rates, and clinically acceptable privacy--utility trade-offs. These results highlight MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward equitable, next-generation federated health infrastructures.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 8 figures, 7 equations, 1 algorithm",
    "pdf_url": "https://arxiv.org/pdf/2510.01780v1",
    "published_date": "2025-10-02 08:19:56 UTC",
    "updated_date": "2025-10-02 08:19:56 UTC"
  },
  {
    "arxiv_id": "2510.01758v1",
    "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks",
    "authors": [
      "Bruno Corcuera",
      "Carlos Eiras-Franco",
      "Brais Cancela"
    ],
    "abstract": "Latent representations are critical for the performance and robustness of machine learning models, as they encode the essential features of data in a compact and informative manner. However, in vision tasks, these representations are often affected by noisy or irrelevant features, which can degrade the model's performance and generalization capabilities. This paper presents a novel approach for enhancing latent representations using unsupervised Dynamic Feature Selection (DFS). For each instance, the proposed method identifies and removes misleading or redundant information in images, ensuring that only the most relevant features contribute to the latent space. By leveraging an unsupervised framework, our approach avoids reliance on labeled data, making it broadly applicable across various domains and datasets. Experiments conducted on image datasets demonstrate that models equipped with unsupervised DFS achieve significant improvements in generalization performance across various tasks, including clustering and image generation, while incurring a minimal increase in the computational cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01758v1",
    "published_date": "2025-10-02 07:46:59 UTC",
    "updated_date": "2025-10-02 07:46:59 UTC"
  },
  {
    "arxiv_id": "2510.01751v1",
    "title": "A cybersecurity AI agent selection and decision support framework",
    "authors": [
      "Masike Malatji"
    ],
    "abstract": "This paper presents a novel, structured decision support framework that systematically aligns diverse artificial intelligence (AI) agent architectures, reactive, cognitive, hybrid, and learning, with the comprehensive National Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0. By integrating agent theory with industry guidelines, this framework provides a transparent and stepwise methodology for selecting and deploying AI solutions to address contemporary cyber threats. Employing a granular decomposition of NIST CSF 2.0 functions into specific tasks, the study links essential AI agent properties such as autonomy, adaptive learning, and real-time responsiveness to each subcategory's security requirements. In addition, it outlines graduated levels of autonomy (assisted, augmented, and fully autonomous) to accommodate organisations at varying stages of cybersecurity maturity. This holistic approach transcends isolated AI applications, providing a unified detection, incident response, and governance strategy. Through conceptual validation, the framework demonstrates how tailored AI agent deployments can align with real-world constraints and risk profiles, enhancing situational awareness, accelerating response times, and fortifying long-term resilience via adaptive risk management. Ultimately, this research bridges the gap between theoretical AI constructs and operational cybersecurity demands, establishing a foundation for robust, empirically validated multi-agent systems that adhere to industry standards.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 figures, 6 tables, AI agents decision support framework",
    "pdf_url": "https://arxiv.org/pdf/2510.01751v1",
    "published_date": "2025-10-02 07:38:21 UTC",
    "updated_date": "2025-10-02 07:38:21 UTC"
  },
  {
    "arxiv_id": "2510.01736v1",
    "title": "Machine-interpretable Engineering Design Standards for Valve Specification",
    "authors": [
      "Anders Gjerver",
      "Rune Frostad",
      "Vedrana Barisic",
      "Melinda Hodkiewicz",
      "Caitlin Woods",
      "Mihaly Fekete",
      "Arild Braathen Torjusen",
      "Johan Wilhelm Kluwer"
    ],
    "abstract": "Engineering design processes use technical specifications and must comply with standards. Product specifications, product type data sheets, and design standards are still mainly document-centric despite the ambition to digitalize industrial work. In this paper, we demonstrate how to transform information held in engineering design standards into modular, reusable, machine-interpretable ontologies and use the ontologies in quality assurance of the plant design and equipment selection process. We use modelling patterns to create modular ontologies for knowledge captured in the text and in frequently referenced tables in International Standards for piping, material and valve design. These modules are exchangeable, as stored in a W3C compliant format, and interoperable as they are aligned with the top-level ontology ISO DIS 23726-3: Industrial Data Ontology (IDO).\n  We test these ontologies, created based on international material and piping standards and industry norms, on a valve selection process. Valves are instantiated in semantic asset models as individuals along with a semantic representation of the environmental condition at their location on the asset. We create \"functional location tags\" as OWL individuals that become instances of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create instances of manufacturer product type. Our approach enables automated validation that a specific VDS is compliant with relevant industry standards. Using semantic reasoning and executable design rules, we also determine whether the product type meets the valve specification. Creation of shared, reusable IDO-based modular ontologies for design standards enables semantic reasoning to be applied to equipment selection processes and demonstrates the potential of this approach for Standards Bodies wanting to transition to digitized Smart Standards.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 10 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.01736v1",
    "published_date": "2025-10-02 07:20:37 UTC",
    "updated_date": "2025-10-02 07:20:37 UTC"
  },
  {
    "arxiv_id": "2510.01724v1",
    "title": "MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs",
    "authors": [
      "Madina Bekbergenova",
      "Lucas Pradi",
      "Benjamin Navet",
      "Emma Tysinger",
      "Franck Michel",
      "Matthieu Feraud",
      "Yousouf Taghzouti",
      "Yan Zhou Chen",
      "Olivier Kirchhoffer",
      "Florence Mehl",
      "Martin Legrand",
      "Tao Jiang",
      "Marco Pagni",
      "Soha Hassoun",
      "Jean-Luc Wolfender",
      "Wout Bittremieux",
      "Fabien Gandon",
      "Louis-F√©lix Nothias"
    ],
    "abstract": "Mass spectrometry metabolomics generates vast amounts of data requiring advanced methods for interpretation. Knowledge graphs address these challenges by structuring mass spectrometry data, metabolite information, and their relationships into a connected network (Gaudry et al. 2024). However, effective use of a knowledge graph demands an in-depth understanding of its ontology and its query language syntax. To overcome this, we designed MetaboT, an AI system utilizing large language models (LLMs) to translate user questions into SPARQL semantic query language for operating on knowledge graphs (Steve Harris 2013). We demonstrate its effectiveness using the Experimental Natural Products Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural products (Gaudry et al. 2024).MetaboT employs specialized AI agents for handling user queries and interacting with the knowledge graph by breaking down complex tasks into discrete components, each managed by a specialised agent (Fig. 1a). The multi-agent system is constructed using the LangChain and LangGraph libraries, which facilitate the integration of LLMs with external tools and information sources (LangChain, n.d.). The query generation process follows a structured workflow. First, the Entry Agent determines if the question is new or a follow-up to previous interactions. New questions are forwarded to the Validator Agent, which verifies if the question is related to the knowledge graph. Then, the valid question is sent to the Supervisor Agent, which identifies if the question requires chemical conversions or standardized identifiers. In this case it delegates the question to the Knowledge Graph Agent, which can use tools to extract necessary details, such as URIs or taxonomies of chemical names, from the user query. Finally, an agent responsible for crafting the SPARQL queries equipped with the ontology of the knowledge graph uses the provided identifiers to generate the query. Then, the system executes the generated query against the metabolomics knowledge graph and returns structured results to the user (Fig. 1b). To assess the performance of MetaboT we have curated 50 metabolomics-related questions and their expected answers. In addition to submitting these questions to MetaboT, we evaluated a baseline by submitting them to a standard LLM (GPT-4o) with a prompt that incorporated the knowledge graph ontology but did not provide specific entity IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%, underscoring the necessity of our multi-agent system for accurately retrieving entities and generating correct SPARQL queries. MetaboT demonstrates promising performance as a conversational question-answering assistant, enabling researchers to retrieve structured metabolomics data through natural language queries. By automating the generation and execution of SPARQL queries, it removes technical barriers that have traditionally hindered access to knowledge graphs. Importantly, MetaboT leverages the capabilities of LLMs while maintaining experimentally grounded query generation, ensuring that outputs remain aligned with domain-specific standards and data structures. This approach facilitates data-driven discoveries by bridging the gap between complex semantic technologies and user-friendly interaction. MetaboT is accessible at [https://metabot.holobiomicslab.eu/], and its source code is available at [https://github.com/HolobiomicsLab/MetaboT].",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01724v1",
    "published_date": "2025-10-02 07:05:29 UTC",
    "updated_date": "2025-10-02 07:05:29 UTC"
  },
  {
    "arxiv_id": "2510.01722v1",
    "title": "Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement",
    "authors": [
      "Jianing Yang",
      "Sheng Li",
      "Takahiro Shinozaki",
      "Yuki Saito",
      "Hiroshi Saruwatari"
    ],
    "abstract": "Current emotional Text-To-Speech (TTS) and style transfer methods rely on reference encoders to control global style or emotion vectors, but do not capture nuanced acoustic details of the reference speech. To this end, we propose a novel emotional TTS method that enables fine-grained phoneme-level emotion embedding prediction while disentangling intrinsic attributes of the reference speech. The proposed method employs a style disentanglement method to guide two feature extractors, reducing mutual information between timbre and emotion features, and effectively separating distinct style components from the reference speech. Experimental results demonstrate that our method outperforms baseline TTS systems in generating natural and emotionally rich speech. This work highlights the potential of disentangled and fine-grained representations in advancing the quality and flexibility of emotional TTS systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "In Proceedings of the 17th Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.01722v1",
    "published_date": "2025-10-02 07:03:50 UTC",
    "updated_date": "2025-10-02 07:03:50 UTC"
  },
  {
    "arxiv_id": "2510.01717v1",
    "title": "Latency-aware Multimodal Federated Learning over UAV Networks",
    "authors": [
      "Shaba Shaon",
      "Dinh C. Nguyen"
    ],
    "abstract": "This paper investigates federated multimodal learning (FML) assisted by unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and providing convergence analysis. In this framework, UAVs are distributed throughout the network to collect data, participate in model training, and collaborate with a base station (BS) to build a global model. By utilizing multimodal sensing, the UAVs overcome the limitations of unimodal systems, enhancing model accuracy, generalization, and offering a more comprehensive understanding of the environment. The primary objective is to optimize FML system latency in UAV networks by jointly addressing UAV sensing scheduling, power control, trajectory planning, resource allocation, and BS resource management. To address the computational complexity of our latency minimization problem, we propose an efficient iterative optimization algorithm combining block coordinate descent and successive convex approximation techniques, which provides high-quality approximate solutions. We also present a theoretical convergence analysis for the UAV-assisted FML framework under a non-convex loss function. Numerical experiments demonstrate that our FML framework outperforms existing approaches in terms of system latency and model training performance under different data settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at IEEE Transactions on Network Science and Engineering",
    "pdf_url": "https://arxiv.org/pdf/2510.01717v1",
    "published_date": "2025-10-02 06:57:44 UTC",
    "updated_date": "2025-10-02 06:57:44 UTC"
  },
  {
    "arxiv_id": "2510.03336v1",
    "title": "Linguistic and Audio Embedding-Based Machine Learning for Alzheimer's Dementia and Mild Cognitive Impairment Detection: Insights from the PROCESS Challenge",
    "authors": [
      "Adharsha Sam Edwin Sam Devahi",
      "Sohail Singh Sangha",
      "Prachee Priyadarshinee",
      "Jithin Thilakan",
      "Ivan Fu Xing Tan",
      "Christopher Johann Clarke",
      "Sou Ka Lon",
      "Balamurali B T",
      "Yow Wei Quin",
      "Chen Jer-Ming"
    ],
    "abstract": "Early detection of Alzheimer's Dementia (AD) and Mild Cognitive Impairment (MCI) is critical for timely intervention, yet current diagnostic approaches remain resource-intensive and invasive. Speech, encompassing both acoustic and linguistic dimensions, offers a promising non-invasive biomarker for cognitive decline. In this study, we present a machine learning framework for the PROCESS Challenge, leveraging both audio embeddings and linguistic features derived from spontaneous speech recordings. Audio representations were extracted using Whisper embeddings from the Cookie Theft description task, while linguistic features-spanning pronoun usage, syntactic complexity, filler words, and clause structure-were obtained from transcriptions across Semantic Fluency, Phonemic Fluency, and Cookie Theft picture description. Classification models aimed to distinguish between Healthy Controls (HC), MCI, and AD participants, while regression models predicted Mini-Mental State Examination (MMSE) scores. Results demonstrated that voted ensemble models trained on concatenated linguistic features achieved the best classification performance (F1 = 0.497), while Whisper embedding-based ensemble regressors yielded the lowest MMSE prediction error (RMSE = 2.843). Comparative evaluation within the PROCESS Challenge placed our models among the top submissions in regression task, and mid-range for classification, highlighting the complementary strengths of linguistic and audio embeddings. These findings reinforce the potential of multimodal speech-based approaches for scalable, non-invasive cognitive assessment and underline the importance of integrating task-specific linguistic and acoustic markers in dementia detection.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03336v1",
    "published_date": "2025-10-02 06:54:55 UTC",
    "updated_date": "2025-10-02 06:54:55 UTC"
  },
  {
    "arxiv_id": "2510.01715v1",
    "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning",
    "authors": [
      "Raahul Krishna Durairaju",
      "K. Saruladha"
    ],
    "abstract": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based algorithm, enabling AI-driven artistic image synthesis. However, existing CNN and transformer-based models struggle to scale efficiently to complex styles and high-resolution inputs. We introduce PyramidStyler, a transformer framework with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding that captures both local details and global context while reducing computational load. We further incorporate reinforcement learning to dynamically optimize stylization, accelerating convergence. Trained on Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s inference--and yields further improvements (content 2.03; style 0.75) with minimal speed penalty (1.40 s) when using RL. These results demonstrate real-time, high-quality artistic rendering, with broad applications in media and design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01715v1",
    "published_date": "2025-10-02 06:54:52 UTC",
    "updated_date": "2025-10-02 06:54:52 UTC"
  },
  {
    "arxiv_id": "2510.01708v3",
    "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization",
    "authors": [
      "Zixing Lei",
      "Zibo Zhou",
      "Sheng Yin",
      "Yueru Chen",
      "Qingyao Xu",
      "Weixin Li",
      "Yunhong Wang",
      "Bowei Tang",
      "Wei Jing",
      "Siheng Chen"
    ],
    "abstract": "Humanoid whole-body control (WBC) policies trained in simulation often suffer from the sim-to-real gap, which fundamentally arises from simulator inductive bias, the inherent assumptions and limitations of any single simulator. These biases lead to nontrivial discrepancies both across simulators and between simulation and the real world. To mitigate the effect of simulator inductive bias, the key idea is to train policies jointly across multiple simulators, encouraging the learned controller to capture dynamics that generalize beyond any single simulator's assumptions. We thus introduce PolySim, a WBC training platform that integrates multiple heterogeneous simulators. PolySim can launch parallel environments from different engines simultaneously within a single training run, thereby realizing dynamics-level domain randomization. Theoretically, we show that PolySim yields a tighter upper bound on simulator inductive bias than single-simulator training. In experiments, PolySim substantially reduces motion-tracking error in sim-to-sim evaluations; for example, on MuJoCo, it improves execution success by 52.8 over an IsaacSim baseline. PolySim further enables zero-shot deployment on a real Unitree G1 without additional fine-tuning, showing effective transfer from simulation to the real world. We will release the PolySim code upon acceptance of this work.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.01708v3",
    "published_date": "2025-10-02 06:31:42 UTC",
    "updated_date": "2025-10-14 06:23:19 UTC"
  },
  {
    "arxiv_id": "2510.08586v1",
    "title": "Dynamic Stress Detection: A Study of Temporal Progression Modelling of Stress in Speech",
    "authors": [
      "Vishakha Lall",
      "Yisi Liu"
    ],
    "abstract": "Detecting psychological stress from speech is critical in high-pressure settings. While prior work has leveraged acoustic features for stress detection, most treat stress as a static label. In this work, we model stress as a temporally evolving phenomenon influenced by historical emotional state. We propose a dynamic labelling strategy that derives fine-grained stress annotations from emotional labels and introduce cross-attention-based sequential models, a Unidirectional LSTM and a Transformer Encoder, to capture temporal stress progression. Our approach achieves notable accuracy gains on MuSE (+5%) and StressID (+18%) over existing baselines, and generalises well to a custom real-world dataset. These results highlight the value of modelling stress as a dynamic construct in speech.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at IEEE CogMI 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08586v1",
    "published_date": "2025-10-02 06:30:44 UTC",
    "updated_date": "2025-10-02 06:30:44 UTC"
  },
  {
    "arxiv_id": "2510.05148v1",
    "title": "Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs",
    "authors": [
      "Qi Li",
      "Runpeng Yu",
      "Haiquan Lu",
      "Xinchao Wang"
    ],
    "abstract": "Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a competitive paradigm for non-autoregressive language modeling. Their distinctive decoding mechanism enables faster inference speed and strong performance in code generation and mathematical tasks. In this work, we show that the decoding mechanism of dLLMs not only enhances model utility but also can be used as a powerful tool for model attribution. A key challenge in this problem lies in the diversity of attribution scenarios, including distinguishing between different models as well as between different checkpoints or backups of the same model. To ensure broad applicability, we identify two fundamental problems: what information to extract from the decoding trajectory, and how to utilize it effectively. We first observe that relying directly on per-step model confidence yields poor performance. This is mainly due to the bidirectional decoding nature of dLLMs: each newly decoded token influences the confidence of other decoded tokens, making model confidence highly redundant and washing out structural signal regarding decoding order or dependencies. To overcome this, we propose a novel information extraction scheme called the Directed Decoding Map (DDM), which captures structural relationships between decoding steps and better reveals model-specific behaviors. Furthermore, to make full use of the extracted structural information during attribution, we propose Gaussian-Trajectory Attribution (GTA), where we fit a cell-wise Gaussian distribution at each decoding position for each target model, and define the likelihood of a trajectory as the attribution score: if a trajectory exhibits higher log-likelihood under the distribution of a specific model, it is more likely to have been generated by that model. Extensive experiments under different settings validate the utility of our methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05148v1",
    "published_date": "2025-10-02 06:25:10 UTC",
    "updated_date": "2025-10-02 06:25:10 UTC"
  },
  {
    "arxiv_id": "2510.01706v1",
    "title": "Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport",
    "authors": [
      "Shaan Shah",
      "Meenakshi Khosla"
    ],
    "abstract": "Standard representational similarity methods align each layer of a network to its best match in another independently, producing asymmetric results, lacking a global alignment score, and struggling with networks of different depths. These limitations arise from ignoring global activation structure and restricting mappings to rigid one-to-one layer correspondences. We propose Hierarchical Optimal Transport (HOT), a unified framework that jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. HOT allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints. This yields both a single alignment score for the entire network comparison and a soft transport plan that naturally handles depth mismatches through mass distribution. We evaluate HOT on vision models, large language models, and human visual cortex recordings. Across all domains, HOT matches or surpasses standard pairwise matching in alignment quality. Moreover, it reveals smooth, fine-grained hierarchical correspondences: early layers map to early layers, deeper layers maintain relative positions, and depth mismatches are resolved by distributing representations across multiple layers. These structured patterns emerge naturally from global optimization without being imposed, yet are absent in greedy layer-wise methods. HOT thus enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01706v1",
    "published_date": "2025-10-02 06:25:06 UTC",
    "updated_date": "2025-10-02 06:25:06 UTC"
  },
  {
    "arxiv_id": "2510.01704v2",
    "title": "Holistic Order Prediction in Natural Scenes",
    "authors": [
      "Pierre Musacchio",
      "Hyunmin Lee",
      "Jaesik Park"
    ],
    "abstract": "Even in controlled settings, understanding instance-wise geometries is a challenging task for a wide range of visual models. Although specialized systems exist, modern arts rely on expensive input formats (category labels, binary segmentation masks) and inference costs (a quadratic amount of forward passes). We mitigate these limitations by proposing InstaFormer, a network capable of holistic order prediction. That is, solely given an input RGB image, InstaFormer returns the full occlusion and depth orderings for all the instances in the scene in a single forward pass. At its core, InstaFormer relies on interactions between object queries and latent mask descriptors that semantically represent the same objects while carrying complementary information. We comprehensively benchmark and ablate our approach to highlight its effectiveness. Our code and models are open-source and available at this URL: https://github.com/SNU-VGILab/InstaOrder.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 11 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.01704v2",
    "published_date": "2025-10-02 06:24:12 UTC",
    "updated_date": "2025-10-25 01:22:08 UTC"
  },
  {
    "arxiv_id": "2510.01700v1",
    "title": "VaPR -- Vision-language Preference alignment for Reasoning",
    "authors": [
      "Rohan Wadhawan",
      "Fabrice Y Harel-Canada",
      "Zi-Yi Dou",
      "Suhaila Shakiah",
      "Robinson Piramuthu",
      "Nanyun Peng"
    ],
    "abstract": "Preference finetuning methods like Direct Preference Optimization (DPO) with AI-generated feedback have shown promise in aligning Large Vision-Language Models (LVLMs) with human preferences. However, existing techniques overlook the prevalence of noise in synthetic preference annotations in the form of stylistic and length biases. To this end, we introduce a hard-negative response generation framework based on LLM-guided response editing, that produces rejected responses with targeted errors, maintaining stylistic and length similarity to the accepted ones. Using this framework, we develop the VaPR dataset, comprising 30K high-quality samples, to finetune three LVLM families: LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver significant performance improvements across ten benchmarks, achieving average gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable improvements on reasoning tasks. A scaling analysis shows that performance consistently improves with data size, with LLaVA models benefiting even at smaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we show that the framework generalizes to open-source LLMs as editors, with models trained on VaPR-OS achieving ~99% of the performance of models trained on \\name, which is synthesized using GPT-4o. Our data, models, and code can be found on the project page https://vap-r.github.io",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01700v1",
    "published_date": "2025-10-02 06:10:43 UTC",
    "updated_date": "2025-10-02 06:10:43 UTC"
  },
  {
    "arxiv_id": "2510.02407v1",
    "title": "Extreme value forecasting using relevance-based data augmentation with deep learning models",
    "authors": [
      "Junru Hua",
      "Rahul Ahluwalia",
      "Rohitash Chandra"
    ],
    "abstract": "Data augmentation with generative adversarial networks (GANs) has been popular for class imbalance problems, mainly for pattern classification and computer vision-related applications. Extreme value forecasting is a challenging field that has various applications from finance to climate change problems. In this study, we present a data augmentation framework for extreme value forecasting. In this framework, our focus is on forecasting extreme values using deep learning models in combination with data augmentation models such as GANs and synthetic minority oversampling technique (SMOTE). We use deep learning models such as convolutional long short-term memory (Conv-LSTM) and bidirectional long short-term memory (BD-LSTM) networks for multistep ahead prediction featuring extremes. We investigate which data augmentation models are the most suitable, taking into account the prediction accuracy overall and at extreme regions, along with computational efficiency. We also present novel strategies for incorporating data augmentation, considering extreme values based on a relevance function. Our results indicate that the SMOTE-based strategy consistently demonstrated superior adaptability, leading to improved performance across both short- and long-horizon forecasts. Conv-LSTM and BD-LSTM exhibit complementary strengths: the former excels in periodic, stable datasets, while the latter performs better in chaotic or non-stationary sequences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02407v1",
    "published_date": "2025-10-02 06:10:27 UTC",
    "updated_date": "2025-10-02 06:10:27 UTC"
  },
  {
    "arxiv_id": "2510.01688v2",
    "title": "Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation",
    "authors": [
      "Seungseop Lim",
      "Gibaeg Kim",
      "Wooseok Han",
      "Jean Seo",
      "Hyunkyung Lee",
      "Jaehyo Yoo",
      "Eunho Yang"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have brought significant improvements to various service domains, including chatbots and medical pre-consultation applications. In the healthcare domain, the most common approach for adapting LLMs to multi-turn dialogue generation is Supervised Fine-Tuning (SFT). However, datasets for SFT in tasks like medical pre-consultation typically exhibit a skewed turn-count distribution. Training on such data induces a novel failure mechanism we term Format Inertia, where models tend to generate repetitive, format-correct, but diagnostically uninformative questions in long medical dialogues. To mitigate this observed failure mechanism, we adopt a simple, data-centric method that rebalances the turn-count distribution of the training dataset. Experimental results show that our approach substantially alleviates Format Inertia in medical pre-consultation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2510.01688v2",
    "published_date": "2025-10-02 05:29:38 UTC",
    "updated_date": "2025-10-04 10:16:08 UTC"
  },
  {
    "arxiv_id": "2510.01687v1",
    "title": "Improving AGI Evaluation: A Data Science Perspective",
    "authors": [
      "John Hawkins"
    ],
    "abstract": "Evaluation of potential AGI systems and methods is difficult due to the breadth of the engineering goal. We have no methods for perfect evaluation of the end state, and instead measure performance on small tests designed to provide directional indication that we are approaching AGI. In this work we argue that AGI evaluation methods have been dominated by a design philosophy that uses our intuitions of what intelligence is to create synthetic tasks, that have performed poorly in the history of AI. Instead we argue for an alternative design philosophy focused on evaluating robust task execution that seeks to demonstrate AGI through competence. This perspective is developed from common practices in data science that are used to show that a system can be reliably deployed. We provide practical examples of what this would mean for AGI evaluation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01687v1",
    "published_date": "2025-10-02 05:27:29 UTC",
    "updated_date": "2025-10-02 05:27:29 UTC"
  },
  {
    "arxiv_id": "2510.01685v1",
    "title": "How Do Language Models Compose Functions?",
    "authors": [
      "Apoorv Khandelwal",
      "Ellie Pavlick"
    ],
    "abstract": "While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the \"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01685v1",
    "published_date": "2025-10-02 05:21:34 UTC",
    "updated_date": "2025-10-02 05:21:34 UTC"
  },
  {
    "arxiv_id": "2510.01681v1",
    "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning",
    "authors": [
      "Xuchen Li",
      "Xuzhao Li",
      "Jiahui Gao",
      "Renjie Pi",
      "Shiyu Hu",
      "Wentao Zhang"
    ],
    "abstract": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they frequently struggle with tasks requiring precise understanding and handling of fine-grained visual elements. This is mainly due to information loss during image encoding or insufficient attention to critical regions. Recent work has shown promise by incorporating pixel-level visual information into the reasoning process, enabling VLMs to access high-resolution visual details during their thought process. However, this pixel-level information is often overused, leading to inefficiency and distraction from irrelevant visual details. To address these challenges, we propose the first framework for adaptive pixel reasoning that dynamically determines necessary pixel-level operations based on the input query. Specifically, we first apply operation-aware supervised fine-tuning to establish baseline competence in textual reasoning and visual operations, then design a novel rollout-guided reinforcement learning framework relying on feedback of the model's own responses, which enables the VLM to determine when pixel operations should be invoked based on query difficulty. Experiments on extensive multimodal reasoning benchmarks show that our model achieves superior performance while significantly reducing unnecessary visual operations. Impressively, our model achieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of only 20.1\\%, improving accuracy and simultaneously reducing tool usage by 66.5\\% compared to the previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint, Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.01681v1",
    "published_date": "2025-10-02 05:14:52 UTC",
    "updated_date": "2025-10-02 05:14:52 UTC"
  },
  {
    "arxiv_id": "2511.14769v1",
    "title": "Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications",
    "authors": [
      "Yifan Xu",
      "Vipul Gupta",
      "Rohit Aggarwal",
      "Varsha Mahadevan",
      "Bhaskar Krishnamachari"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.14769v1",
    "published_date": "2025-10-02 05:11:12 UTC",
    "updated_date": "2025-10-02 05:11:12 UTC"
  },
  {
    "arxiv_id": "2510.01674v1",
    "title": "FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol",
    "authors": [
      "He Zhang",
      "Anzhou Zhang",
      "Jian Dai"
    ],
    "abstract": "Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT) organize internal deliberation but lack an explicit mechanism for external questioning that elicits self-revision. We present FOR-Prompting (From Objection to Revision Prompting), an asymmetric protocol where a Defender proposes an answer, an Objectioner raises question-style objections with no direct fixes, and a Host enforces consistency and closure. On GSM8K we observe about a 22% point gain over single-prompt and accuracy on par with CoT, with more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1 judge. FOR-Prompting also corrects mistakes without tools or human supervision on tricky queries, and improves performance for small-scale model (approx. 19% accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for small models and on personal device use. Beyond factual QA, qualitative analyses on open-ended tasks show enhanced exploration and refinement, with dialogue traces that make assumptions and trade-offs explicit. The protocol is model agnostic and operates purely at the prompt level through role-structured turns, so it works with hosted and local models of different sizes without retraining, and it supports large-scale study of objection-guided reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01674v1",
    "published_date": "2025-10-02 04:57:58 UTC",
    "updated_date": "2025-10-02 04:57:58 UTC"
  },
  {
    "arxiv_id": "2510.01671v1",
    "title": "A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation",
    "authors": [
      "Motoki Sato",
      "Yuki Matsushita",
      "Hidekazu Takahashi",
      "Tomoaki Kakazu",
      "Sou Nagata",
      "Mizuho Ohnuma",
      "Atsushi Yoshikawa",
      "Masayuki Yamamura"
    ],
    "abstract": "Patients awaiting invasive procedures often have unanswered pre-procedural questions; however, time-pressured workflows and privacy constraints limit personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave No One Behind Architecture), a safety-first, local-first system that routes inputs with a high-precision sentence-transformer classifier and returns verbatim answers from a clinician-curated FAQ for clinical queries, eliminating free-text generation in the clinical path. We evaluated two domains (tooth extraction and gastroscopy) using expert-reviewed validation sets (n=400/domain) for thresholding and independent test sets (n=200/domain). Among the four encoders, E5-large-instruct (560M) achieved an overall accuracy of 0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were statistically indistinguishable from GPT-4o on this task; Gemini made no errors on this test set. Energy logging shows that the non-generative clinical path consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local 8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single on-prem GPU. These results indicate that near-frontier discrimination and generation-induced errors are structurally avoided in the clinical path by returning vetted FAQ answers verbatim, supporting privacy, sustainability, and equitable deployment in bandwidth-limited environments.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 4 figures, 10 tables 32 pages, 4 figures, 10 tables. This paper is currently under review at ACM Transactions on Computing for Healthcare. Reproducibility resources: http://github.com/motokinaru/LENOHA-medical-dialogue",
    "pdf_url": "https://arxiv.org/pdf/2510.01671v1",
    "published_date": "2025-10-02 04:53:11 UTC",
    "updated_date": "2025-10-02 04:53:11 UTC"
  },
  {
    "arxiv_id": "2510.01670v1",
    "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
    "authors": [
      "Erfan Shayegani",
      "Keegan Hines",
      "Yue Dong",
      "Nael Abu-Ghazaleh",
      "Roman Lutz",
      "Spencer Whitehead",
      "Vidhisha Balachandran",
      "Besmira Nushi",
      "Vibhav Vineet"
    ],
    "abstract": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01670v1",
    "published_date": "2025-10-02 04:52:15 UTC",
    "updated_date": "2025-10-02 04:52:15 UTC"
  },
  {
    "arxiv_id": "2510.01664v1",
    "title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents",
    "authors": [
      "Yejin Kim",
      "Youngbin Lee",
      "Juhyeong Kim",
      "Yongjae Lee"
    ],
    "abstract": "This study demonstrates that GuruAgents, prompt-guided AI agents, can systematically operationalize the strategies of legendary investment gurus. We develop five distinct GuruAgents, each designed to emulate an iconic investor, by encoding their distinct philosophies into LLM prompts that integrate financial tools and a deterministic reasoning pipeline. In a backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique behaviors driven by their prompted personas. The Buffett GuruAgent achieves the highest performance, delivering a 42.2\\% CAGR that significantly outperforms benchmarks, while other agents show varied results. These findings confirm that prompt engineering can successfully translate the qualitative philosophies of investment gurus into reproducible, quantitative strategies, highlighting a novel direction for automated systematic investing. The source code and data are available at https://github.com/yejining99/GuruAgents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 Pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.01664v1",
    "published_date": "2025-10-02 04:45:27 UTC",
    "updated_date": "2025-10-02 04:45:27 UTC"
  },
  {
    "arxiv_id": "2510.01663v1",
    "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value",
    "authors": [
      "Wangxuan Fan",
      "Ching Wang",
      "Siqi Li",
      "Nan Liu"
    ],
    "abstract": "For many real-world applications, understanding feature-outcome relationships is as crucial as achieving high predictive accuracy. While traditional neural networks excel at prediction, their black-box nature obscures underlying functional relationships. Kolmogorov--Arnold Networks (KANs) address this by employing learnable spline-based activation functions on edges, enabling recovery of symbolic representations while maintaining competitive performance. However, KAN's architecture presents unique challenges for network pruning. Conventional magnitude-based methods become unreliable due to sensitivity to input coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using Shapley value attribution to assess node importance in a shift-invariant manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's actual contribution, ensuring consistent importance rankings regardless of input parameterization. Extensive experiments on synthetic and real-world datasets demonstrate that ShapKAN preserves true node importance while enabling effective network compression. Our approach improves KAN's interpretability advantages, facilitating deployment in resource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 6 figures, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.01663v1",
    "published_date": "2025-10-02 04:45:02 UTC",
    "updated_date": "2025-10-02 04:45:02 UTC"
  },
  {
    "arxiv_id": "2510.01659v1",
    "title": "MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization",
    "authors": [
      "Yinhong Liu",
      "Jianfeng He",
      "Hang Su",
      "Ruixue Lian",
      "Yi Nian",
      "Jake Vincent",
      "Srikanth Vishnubhotla",
      "Robinson Piramuthu",
      "Saab Mansour"
    ],
    "abstract": "Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging applications. To support the development of effective MDS models, robust automatic evaluation methods are essential for reducing both cost and human effort. However, such methods require a strong meta-evaluation benchmark grounded in human annotations. In this work, we introduce MDSEval, the first meta-evaluation benchmark for MDS, consisting image-sharing dialogues, corresponding summaries, and human judgments across eight well-defined quality aspects. To ensure data quality and richfulness, we propose a novel filtering framework leveraging Mutually Exclusive Key Information (MEKI) across modalities. Our work is the first to identify and formalize key evaluation dimensions specific to MDS. We benchmark state-of-the-art modal evaluation methods, revealing their limitations in distinguishing summaries from advanced MLLMs and their susceptibility to various bias.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.01659v1",
    "published_date": "2025-10-02 04:38:27 UTC",
    "updated_date": "2025-10-02 04:38:27 UTC"
  },
  {
    "arxiv_id": "2510.01658v1",
    "title": "Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing",
    "authors": [
      "Amin Jalali",
      "Milad Soltany",
      "Michael Greenspan",
      "Ali Etemad"
    ],
    "abstract": "We propose TimeHUT, a novel method for learning time-series representations by hierarchical uniformity-tolerance balancing of contrastive representations. Our method uses two distinct losses to learn strong representations with the aim of striking an effective balance between uniformity and tolerance in the embedding space. First, TimeHUT uses a hierarchical setup to learn both instance-wise and temporal information from input time-series. Next, we integrate a temperature scheduler within the vanilla contrastive loss to balance the uniformity and tolerance characteristics of the embeddings. Additionally, a hierarchical angular margin loss enforces instance-wise and temporal contrast losses, creating geometric margins between positive and negative pairs of temporal sequences. This approach improves the coherence of positive pairs and their separation from the negatives, enhancing the capture of temporal dependencies within a time-series sample. We evaluate our approach on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and multivariate classification, as well as Yahoo and KPI datasets for anomaly detection. The results demonstrate that TimeHUT outperforms prior methods by considerable margins on classification, while obtaining competitive results for anomaly detection. Finally, detailed sensitivity and ablation studies are performed to evaluate different components and hyperparameters of our method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in Transactions on Machine Learning Research",
    "pdf_url": "https://arxiv.org/pdf/2510.01658v1",
    "published_date": "2025-10-02 04:30:13 UTC",
    "updated_date": "2025-10-02 04:30:13 UTC"
  },
  {
    "arxiv_id": "2510.01656v3",
    "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning",
    "authors": [
      "Jiashun Liu",
      "Johan Obando-Ceron",
      "Han Lu",
      "Yancheng He",
      "Weixun Wang",
      "Wenbo Su",
      "Bo Zheng",
      "Pablo Samuel Castro",
      "Aaron Courville",
      "Ling Pan"
    ],
    "abstract": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing them with average advantage baselines. This shift is largely pragmatic: conventional value functions are computationally expensive to train at LLM scale and often fail under sparse rewards and long reasoning horizons. We revisit this bottleneck from an architectural perspective and introduce Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable framework that restores the critics role while remaining efficient in large-model settings. AsyPPO employs a set of lightweight mini-critics, each trained on disjoint prompt shards. This design encourages diversity while preserving calibration, reducing value-estimation bias. Beyond robust estimation, AsyPPO leverages inter-critic uncertainty to refine the policy update: (i) masking advantages in states where critics agree and gradients add little learning signal, and (ii) filtering high-divergence states from entropy regularization, suppressing spurious exploration. After training on open-source data with only 5,000 samples, AsyPPO consistently improves learning stability and performance across multiple benchmarks over strong baselines, such as GRPO, achieving performance gains of more than six percent on Qwen3-4b-Base and about three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without additional tricks. These results highlight the importance of architectural innovations for scalable, efficient algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01656v3",
    "published_date": "2025-10-02 04:24:27 UTC",
    "updated_date": "2025-10-15 08:36:52 UTC"
  },
  {
    "arxiv_id": "2510.01654v1",
    "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
    "authors": [
      "Mudita Khurana",
      "Raunak Jain"
    ],
    "abstract": "Cybersecurity is a relentless arms race, with AI driven offensive systems evolving faster than traditional defenses can adapt. Research and tooling remain fragmented across isolated defensive functions, creating blind spots that adversaries exploit. Autonomous agents capable of integrating, exploit confirmation, remediation, and validation into a single closed loop offer promise, but the field lacks three essentials: a framework defining the agentic capabilities of security systems across security life cycle, a principled method for evaluating closed loop agents, and a benchmark for measuring their performance in practice. We introduce CLASP: the Closed-Loop Autonomous Security Performance framework which aligns the security lifecycle (reconnaissance, exploitation, root cause analysis, patch synthesis, validation) with core agentic capabilities (planning, tool use, memory, reasoning, reflection & perception) providing a common vocabulary and rubric for assessing agentic capabilities in security tasks. By applying CLASP to 21 representative works, we map where systems demonstrate strengths, and where capability gaps persist. We then define the Closed-Loop Capability (CLC) Score, a composite metric quantifying both degree of loop closure and operational effectiveness, and outline the requirements for a closed loop benchmark. Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and measurements needed to advance both function level performance and measure closed loop security agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01654v1",
    "published_date": "2025-10-02 04:20:35 UTC",
    "updated_date": "2025-10-02 04:20:35 UTC"
  },
  {
    "arxiv_id": "2510.01650v1",
    "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM",
    "authors": [
      "Kwanhee Lee",
      "Hyeondo Jang",
      "Dongyeop Lee",
      "Dan Alistarh",
      "Namhoon Lee"
    ],
    "abstract": "Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Furthermore, we present $\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees. These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.01650v1",
    "published_date": "2025-10-02 04:10:17 UTC",
    "updated_date": "2025-10-02 04:10:17 UTC"
  },
  {
    "arxiv_id": "2510.21739v1",
    "title": "Next-Generation LLM for UAV: From Natural Language to Autonomous Flight",
    "authors": [
      "Liangqi Yuan",
      "Chuhao Deng",
      "Dong-Jun Han",
      "Inseok Hwang",
      "Sabine Brunswicker",
      "Christopher G. Brinton"
    ],
    "abstract": "With the rapid advancement of Large Language Models (LLMs), their capabilities in various automation domains, particularly Unmanned Aerial Vehicle (UAV) operations, have garnered increasing attention. Current research remains predominantly constrained to small-scale UAV applications, with most studies focusing on isolated components such as path planning for toy drones, while lacking comprehensive investigation of medium- and long-range UAV systems in real-world operational contexts. Larger UAV platforms introduce distinct challenges, including stringent requirements for airport-based take-off and landing procedures, adherence to complex regulatory frameworks, and specialized operational capabilities with elevated mission expectations. This position paper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive demonstration and automation roadmap for integrating LLMs into multi-scale UAV operations. The NeLV system processes natural language instructions to orchestrate short-, medium-, and long-range UAV missions through five key technical components: (i) LLM-as-Parser for instruction interpretation, (ii) Route Planner for Points of Interest (POI) determination, (iii) Path Planner for waypoint generation, (iv) Control Platform for executable trajectory implementation, and (v) UAV monitoring. We demonstrate the system's feasibility through three representative use cases spanning different operational scales: multi-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the current implementation, we establish a five-level automation taxonomy that charts the evolution from current LLM-as-Parser capabilities (Level 1) to fully autonomous LLM-as-Autopilot systems (Level 5), identifying technical prerequisites and research challenges at each stage.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21739v1",
    "published_date": "2025-10-02 04:09:58 UTC",
    "updated_date": "2025-10-02 04:09:58 UTC"
  },
  {
    "arxiv_id": "2510.01649v1",
    "title": "Source-Free Cross-Domain Continual Learning",
    "authors": [
      "Muhammad Tanzil Furqon",
      "Mahardhika Pratama",
      "Igor ≈†krjanc",
      "Lin Liu",
      "Habibullah Habibullah",
      "Kutluyil Dogancay"
    ],
    "abstract": "Although existing cross-domain continual learning approaches successfully address many streaming tasks having domain shifts, they call for a fully labeled source domain hindering their feasibility in the privacy constrained environments. This paper goes one step ahead with the problem of source-free cross-domain continual learning where the use of source-domain samples are completely prohibited. We propose the idea of rehearsal-free frequency-aware dynamic prompt collaborations (REFEREE) to cope with the absence of labeled source-domain samples in realm of cross-domain continual learning. REFEREE is built upon a synergy between a source-pre-trained model and a large-scale vision-language model, thus overcoming the problem of sub-optimal generalizations when relying only on a source pre-trained model. The domain shift problem between the source domain and the target domain is handled by a frequency-aware prompting technique encouraging low-frequency components while suppressing high-frequency components. This strategy generates frequency-aware augmented samples, robust against noisy pseudo labels. The noisy pseudo-label problem is further addressed with the uncertainty-aware weighting strategy where the mean and covariance matrix are weighted by prediction uncertainties, thus mitigating the adverse effects of the noisy pseudo label. Besides, the issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant analysis (KLDA) where the backbone network is frozen while the classification is performed using the linear discriminant analysis approach guided by the random kernel method. Our rigorous numerical studies confirm the advantage of our approach where it beats prior arts having access to source domain samples with significant margins.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01649v1",
    "published_date": "2025-10-02 04:09:25 UTC",
    "updated_date": "2025-10-02 04:09:25 UTC"
  },
  {
    "arxiv_id": "2510.01645v1",
    "title": "Position: Privacy Is Not Just Memorization!",
    "authors": [
      "Niloofar Mireshghallah",
      "Tianshi Li"
    ],
    "abstract": "The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "27 pages, 6 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.01645v1",
    "published_date": "2025-10-02 04:02:06 UTC",
    "updated_date": "2025-10-02 04:02:06 UTC"
  },
  {
    "arxiv_id": "2510.01644v2",
    "title": "Machine Learning for Detection and Analysis of Novel LLM Jailbreaks",
    "authors": [
      "John Hawkins",
      "Aditya Pramar",
      "Rodney Beard",
      "Rohitash Chandra"
    ],
    "abstract": "Large Language Models (LLMs) suffer from a range of vulnerabilities that allow malicious users to solicit undesirable responses through manipulation of the input text. These so-called jailbreak prompts are designed to trick the LLM into circumventing the safety guardrails put in place to keep responses acceptable to the developer's policies. In this study, we analyse the ability of different machine learning models to distinguish jailbreak prompts from genuine uses, including looking at our ability to identify jailbreaks that use previously unseen strategies. Our results indicate that using current datasets the best performance is achieved by fine tuning a Bidirectional Encoder Representations from Transformers (BERT) model end-to-end for identifying jailbreaks. We visualise the keywords that distinguish jailbreak from genuine prompts and conclude that explicit reflexivity in prompt structure could be a signal of jailbreak intention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01644v2",
    "published_date": "2025-10-02 03:55:29 UTC",
    "updated_date": "2025-10-10 05:08:44 UTC"
  },
  {
    "arxiv_id": "2510.01639v1",
    "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective",
    "authors": [
      "Thinh Hung Truong",
      "Jey Han Lau",
      "Jianzhong Qi"
    ],
    "abstract": "We explore the geospatial reasoning capabilities of Large Language Models (LLMs), specifically, whether LLMs can read road network maps and perform navigation. We frame trajectory recovery as a proxy task, which requires models to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with over 4,000 real-world trajectories across diverse regions and transportation modes. Using road network as context, our prompting framework enables LLMs to generate valid paths without accessing any external navigation tools. Experiments show that LLMs outperform off-the-shelf baselines and specialized trajectory recovery models, with strong zero-shot generalization. Fine-grained analysis shows that LLMs have strong comprehension of the road network and coordinate systems, but also pose systematic biases with respect to regions and transportation modes. Finally, we demonstrate how LLMs can enhance navigation experiences by reasoning over maps in flexible ways to incorporate user preferences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01639v1",
    "published_date": "2025-10-02 03:37:41 UTC",
    "updated_date": "2025-10-02 03:37:41 UTC"
  },
  {
    "arxiv_id": "2510.01638v1",
    "title": "Towards Human-Centered RegTech: Unpacking Professionals' Strategies and Needs for Using LLMs Safely",
    "authors": [
      "Siying Hu",
      "Yaxing Yao",
      "Zhicong Lu"
    ],
    "abstract": "Large Language Models are profoundly changing work patterns in high-risk professional domains, yet their application also introduces severe and underexplored compliance risks. To investigate this issue, we conducted semi-structured interviews with 24 highly-skilled knowledge workers from industries such as law, healthcare, and finance. The study found that these experts are commonly concerned about sensitive information leakage, intellectual property infringement, and uncertainty regarding the quality of model outputs. In response, they spontaneously adopt various mitigation strategies, such as actively distorting input data and limiting the details in their prompts. However, the effectiveness of these spontaneous efforts is limited due to a lack of specific compliance guidance and training for Large Language Models. Our research reveals a significant gap between current NLP tools and the actual compliance needs of experts. This paper positions these valuable empirical findings as foundational work for building the next generation of Human-Centered, Compliance-Driven Natural Language Processing for Regulatory Technology (RegTech), providing a critical human-centered perspective and design requirements for engineering NLP systems that can proactively support expert compliance workflows.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to the 4th HCI+NLP@EMNLP 2025 Workshop. (Non-archival)",
    "pdf_url": "https://arxiv.org/pdf/2510.01638v1",
    "published_date": "2025-10-02 03:35:46 UTC",
    "updated_date": "2025-10-02 03:35:46 UTC"
  },
  {
    "arxiv_id": "2510.01632v1",
    "title": "BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning",
    "authors": [
      "Xin Wang",
      "Carlos Oliver"
    ],
    "abstract": "Protein function is driven by coherent substructures which vary in size and topology, yet current protein representation learning models (PRL) distort these signals by relying on rigid substructures such as k-hop and fixed radius neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable module that represents proteins by dynamically partitioning structures into flexibly-sized, non-overlapping substructures (\"blobs\"). The resulting blobs are quantized into a shared and interpretable codebook, yielding a discrete vocabulary of function-relevant protein substructures used to compute protein embeddings. We show that BioBlobs representations improve the performance of widely used protein encoders such as GVP-GNN across various PRL tasks. Our approach highlights the value of architectures that directly capture function-relevant protein substructures, enabling both improved predictive performance and mechanistic insight into protein function.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01632v1",
    "published_date": "2025-10-02 03:25:02 UTC",
    "updated_date": "2025-10-02 03:25:02 UTC"
  },
  {
    "arxiv_id": "2510.01631v1",
    "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls",
    "authors": [
      "Feiyang Kang",
      "Newsha Ardalani",
      "Michael Kuchnik",
      "Youssef Emad",
      "Mostafa Elhoushi",
      "Shubhabrata Sengupta",
      "Shang-Wen Li",
      "Ramya Raghavendra",
      "Ruoxi Jia",
      "Carole-Jean Wu"
    ],
    "abstract": "Training data plays a crucial role in Large Language Models (LLM) scaling, yet high quality data is of limited supply. Synthetic data techniques offer a potential path toward sidestepping these limitations. We conduct a large-scale empirical investigation (>1000 LLMs with >100k GPU hours) using a unified protocol and scaling laws, comparing natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures of natural and synthetic data. Specifically, we found pre-training on rephrased synthetic data \\textit{alone} is not faster than pre-training on natural web texts; while pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts can speed up 5-10x (to reach the same validation loss) at larger data budgets. Pre-training on textbook-style synthetic data \\textit{alone} results in notably higher loss on many downstream domains especially at small data budgets. \"Good\" ratios of synthetic data in training data mixtures depend on the model size and data budget, empirically converging to ~30% for rephrased synthetic data. Larger generator models do not necessarily yield better pre-training data than ~8B-param models. These results contribute mixed evidence on \"model collapse\" during large-scale single-round (n=1) model training on synthetic data--training on rephrased synthetic data shows no degradation in performance in foreseeable scales whereas training on mixtures of textbook-style pure-generated synthetic data shows patterns predicted by \"model collapse\". Our work demystifies synthetic data in pre-training, validates its conditional benefits, and offers practical guidance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a Main Conference paper at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.01631v1",
    "published_date": "2025-10-02 03:24:42 UTC",
    "updated_date": "2025-10-02 03:24:42 UTC"
  },
  {
    "arxiv_id": "2510.01624v1",
    "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead",
    "authors": [
      "Feiyang Kang",
      "Michael Kuchnik",
      "Karthik Padthe",
      "Marin Vlastelica",
      "Ruoxi Jia",
      "Carole-Jean Wu",
      "Newsha Ardalani"
    ],
    "abstract": "In post-training for reasoning Large Language Models (LLMs), the current state of practice trains LLMs in two independent stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as ``RL'' below). In this work, we challenge whether high SFT scores translate to improved performance after RL. We provide extensive counter-examples where this is not true. We find high SFT scores can be biased toward simpler or more homogeneous data and are not reliably predictive of subsequent RL gains or scaled-up post-training effectiveness. In some cases, RL training on models with improved SFT performance could lead to substantially worse outcome compared to RL on the base model without SFT. We study alternative metrics and identify generalization loss on held-out reasoning examples and Pass@large k performance to provide strong proxies for the RL outcome. We trained hundreds of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL performance, prediction based on generalization loss and Pass@large k achieves substantial higher precision, improving $R^2$ coefficient and Spearman's rank correlation coefficient by up to 0.5 (2x). This provides strong utility for broad use cases. For example, in most experiments, we find SFT training on unique examples for a one epoch underperforms training on half examples for two epochs, either after SFT or SFT-then-RL; With the same SFT budget, training only on short examples may lead to better SFT performance, though, it often leads to worse outcome after RL compared to training on examples with varying lengths. Evaluation tool will be open-sourced.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.01624v1",
    "published_date": "2025-10-02 02:57:00 UTC",
    "updated_date": "2025-10-02 02:57:00 UTC"
  },
  {
    "arxiv_id": "2510.01622v1",
    "title": "LLM4Rec: Large Language Models for Multimodal Generative Recommendation with Causal Debiasing",
    "authors": [
      "Bo Ma",
      "Hang Li",
      "ZeHua Hu",
      "XiaoFan Gui",
      "LuYao Liu",
      "Simon Lau"
    ],
    "abstract": "Contemporary generative recommendation systems face significant challenges in handling multimodal data, eliminating algorithmic biases, and providing transparent decision-making processes. This paper introduces an enhanced generative recommendation framework that addresses these limitations through five key innovations: multimodal fusion architecture, retrieval-augmented generation mechanisms, causal inference-based debiasing, explainable recommendation generation, and real-time adaptive learning capabilities. Our framework leverages advanced large language models as the backbone while incorporating specialized modules for cross-modal understanding, contextual knowledge integration, bias mitigation, explanation synthesis, and continuous model adaptation. Extensive experiments on three benchmark datasets (MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent improvements in recommendation accuracy, fairness, and diversity compared to existing approaches. The proposed framework achieves up to 2.3% improvement in NDCG@10 and 1.4% enhancement in diversity metrics while maintaining computational efficiency through optimized inference strategies.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01622v1",
    "published_date": "2025-10-02 02:53:05 UTC",
    "updated_date": "2025-10-02 02:53:05 UTC"
  },
  {
    "arxiv_id": "2510.01620v2",
    "title": "Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CMDPs",
    "authors": [
      "Peidong Liu",
      "Junjiang Lin",
      "Shaowen Wang",
      "Yao Xu",
      "Haiqing Li",
      "Xuhao Xie",
      "Siyi Wu",
      "Hao Li"
    ],
    "abstract": "Contextual Markov Decision Processes (CMDPs) offer a framework for sequential decision-making under external signals, but existing methods often fail to generalize in high-dimensional or unstructured contexts, resulting in excessive computation and unstable performance. We propose an information-theoretic summarization approach that uses large language models (LLMs) to compress contextual inputs into low-dimensional, semantically rich summaries. These summaries augment states by preserving decision-critical cues while reducing redundancy. Building on the notion of approximate context sufficiency, we provide, to our knowledge, the first regret bounds and a latency-entropy trade-off characterization for CMDPs. Our analysis clarifies how informativeness impacts computational cost. Experiments across discrete, continuous, visual, and recommendation benchmarks show that our method outperforms raw-context and non-context baselines, improving reward, success rate, and sample efficiency, while reducing latency and memory usage. These findings demonstrate that LLM-based summarization offers a scalable and interpretable solution for efficient decision-making in context-rich, resource-constrained environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01620v2",
    "published_date": "2025-10-02 02:52:24 UTC",
    "updated_date": "2025-10-03 02:17:40 UTC"
  },
  {
    "arxiv_id": "2510.01612v3",
    "title": "RAG-BioQA: A Retrieval-Augmented Generation Framework for Long-Form Biomedical Question Answering",
    "authors": [
      "Lovely Yeswanth Panchumarthi",
      "Sumalatha Saleti",
      "Sai Prasad Gudari",
      "Atharva Negi",
      "Praveen Raj Budime",
      "Harsit Upadhya"
    ],
    "abstract": "The rapidly growth of biomedical literature creates challenges acquiring specific medical information. Current biomedical question-answering systems primarily focus on short-form answers, failing to provide comprehensive explanations necessary for clinical decision-making. We present RAG-BioQA, a retrieval-augmented generation framework for long-form biomedical question answering. Our system integrates BioBERT embeddings with FAISS indexing for retrieval and a LoRA fine-tuned FLAN-T5 model for answer generation. We train on 181k QA pairs from PubMedQA, MedDialog, and MedQuAD, and evaluate on a held-out PubMedQA test set. We compare four retrieval strategies: dense retrieval (FAISS), BM25, ColBERT, and MonoT5. Our results show that domain-adapted dense retrieval outperforms zero-shot neural re-rankers, with the best configuration achieving 0.24 BLEU-1 and 0.29 ROUGE-1. Fine-tuning improves BERTScore by 81\\% over the base model. We release our framework to support reproducible biomedical QA research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ICAEI",
    "pdf_url": "https://arxiv.org/pdf/2510.01612v3",
    "published_date": "2025-10-02 02:49:09 UTC",
    "updated_date": "2026-01-02 03:53:15 UTC"
  },
  {
    "arxiv_id": "2510.01611v4",
    "title": "PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large Language Models",
    "authors": [
      "Min Zeng"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across a wide range of industries, primarily due to their impressive generative abilities. Yet, their potential in applications requiring cognitive abilities, such as psychological counseling, remains largely untapped. This paper investigates the key question: \\textit{Can LLMs be effectively applied to psychological counseling?} To determine whether an LLM can effectively take on the role of a psychological counselor, the first step is to assess whether it meets the qualifications required for such a role, namely the ability to pass the U.S. National Counselor Certification Exam (NCE). This is because, just as a human counselor must pass a certification exam to practice, an LLM must demonstrate sufficient psychological knowledge to meet the standards required for such a role. To address this, we introduce PsychCounsel-Bench, a benchmark grounded in U.S.national counselor examinations, a licensure test for professional counselors that requires about 70\\% accuracy to pass. PsychCounsel-Bench comprises approximately 2,252 carefully curated single-choice questions, crafted to require deep understanding and broad enough to cover various sub-disciplines of psychology. This benchmark provides a comprehensive assessment of an LLM's ability to function as a counselor. Our evaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results suggest that only frontier LLMs are currently capable of meeting counseling exam standards, highlighting both the promise and the challenges of developing psychology-oriented LLMs. We release the proposed dataset for public use: https://github.com/cloversjtu/PsychCounsel-Bench",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01611v4",
    "published_date": "2025-10-02 02:49:06 UTC",
    "updated_date": "2025-11-12 22:18:01 UTC"
  },
  {
    "arxiv_id": "2510.01609v1",
    "title": "AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence",
    "authors": [
      "Bo Ma",
      "Hang Li",
      "ZeHua Hu",
      "XiaoFan Gui",
      "LuYao Liu",
      "Simon Lau"
    ],
    "abstract": "Interactive conversational recommender systems have gained significant attention for their ability to capture user preferences through natural language interactions. However, existing approaches face substantial challenges in handling dynamic user preferences, maintaining conversation coherence, and balancing multiple ranking objectives simultaneously. This paper introduces AgentRec, a next-generation LLM-powered multi-agent collaborative recommendation framework that addresses these limitations through hierarchical agent networks with adaptive intelligence. Our approach employs specialized LLM-powered agents for conversation understanding, preference modeling, context awareness, and dynamic ranking, coordinated through an adaptive weighting mechanism that learns from interaction patterns. We propose a three-tier learning strategy combining rapid response for simple queries, intelligent reasoning for complex preferences, and deep collaboration for challenging scenarios. Extensive experiments on three real-world datasets demonstrate that AgentRec achieves consistent improvements over state-of-the-art baselines, with 2.8\\% enhancement in conversation success rate, 1.9\\% improvement in recommendation accuracy (NDCG@10), and 3.2\\% better conversation efficiency while maintaining comparable computational costs through intelligent agent coordination.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01609v1",
    "published_date": "2025-10-02 02:47:11 UTC",
    "updated_date": "2025-10-02 02:47:11 UTC"
  },
  {
    "arxiv_id": "2510.01606v1",
    "title": "Bridging Collaborative Filtering and Large Language Models with Dynamic Alignment, Multimodal Fusion and Evidence-grounded Explanations",
    "authors": [
      "Bo Ma",
      "LuYao Liu",
      "Simon Lau",
      "Chandler Yuan",
      "and XueY Cui",
      "Rosie Zhang"
    ],
    "abstract": "Recent research has explored using Large Language Models for recommendation tasks by transforming user interaction histories and item metadata into text prompts, then having the LLM produce rankings or recommendations. A promising approach involves connecting collaborative filtering knowledge to LLM representations through compact adapter networks, which avoids expensive fine-tuning while preserving the strengths of both components. Yet several challenges persist in practice: collaborative filtering models often use static snapshots that miss rapidly changing user preferences; many real-world items contain rich visual and audio content beyond textual descriptions; and current systems struggle to provide trustworthy explanations backed by concrete evidence. Our work introduces \\model{}, a framework that tackles these limitations through three key innovations. We develop an online adaptation mechanism that continuously incorporates new user interactions through lightweight modules, avoiding the need to retrain large models. We create a unified representation that seamlessly combines collaborative signals with visual and audio features, handling cases where some modalities may be unavailable. Finally, we design an explanation system that grounds recommendations in specific collaborative patterns and item attributes, producing natural language rationales users can verify. Our approach maintains the efficiency of frozen base models while adding minimal computational overhead, making it practical for real-world deployment.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01606v1",
    "published_date": "2025-10-02 02:43:24 UTC",
    "updated_date": "2025-10-02 02:43:24 UTC"
  },
  {
    "arxiv_id": "2510.01600v2",
    "title": "A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation",
    "authors": [
      "Neal Gregory Lawton",
      "Alfy Samuel",
      "Anoop Kumar",
      "Daben Liu"
    ],
    "abstract": "A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel, Anoop Kumar, Daben Liu Published: 20 Aug 2025, Retrieval augmented generation (RAG) is a popular framework for question answering that is powered by two large language models (LLMs): an embedding model that retrieves context documents from a database that are relevant to a given question, and a generator model that uses the retrieved context to generate an answer to the question. Both the embedding and generator models can be fine-tuned to increase performance of a RAG pipeline on a new task, but multiple fine-tuning strategies exist with different costs and benefits. In this paper, we evaluate and compare several RAG fine-tuning strategies, including independent, joint, and two-phase fine-tuning. In our experiments, we observe that all of these strategies achieve about equal improvement in EM and F1 generation quality metrics, although they have significantly different computational costs. We conclude the optimal fine-tuning strategy to use depends on whether the training dataset includes context labels and whether a grid search over the learning rates for the embedding and generator models is required.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01600v2",
    "published_date": "2025-10-02 02:30:28 UTC",
    "updated_date": "2025-10-17 20:14:41 UTC"
  },
  {
    "arxiv_id": "2510.01588v1",
    "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation",
    "authors": [
      "Ziming Tang",
      "Chengbin Hou",
      "Tianyu Zhang",
      "Bangxu Tian",
      "Jinbao Wang",
      "Hairong Lv"
    ],
    "abstract": "Parkinson's disease (PD) is one of the most common neurodegenerative disorder. PD telemonitoring emerges as a novel assessment modality enabling self-administered at-home tests of Unified Parkinson's Disease Rating Scale (UPDRS) scores, enhancing accessibility for PD patients. However, three types of noise would occur during measurements: (1) patient-induced measurement inaccuracies, (2) environmental noise, and (3) data packet loss during transmission, resulting in higher prediction errors. To address these challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First, the original speech features are grouped into ordered bins, based on the continuous values of a selected feature, to construct contrastive pairs. Second, the contrastive pairs are employed to train a multilayer perceptron encoder for generating noise-robust features. Finally, these features are concatenated with the original features as the augmented features, which are then fed into the UPDRS prediction models. Notably, we further introduces a novel evaluation approach with customizable noise injection module, and extensive experiments show that NoRo can successfully enhance the noise robustness of UPDRS prediction across various downstream prediction models under different noisy environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01588v1",
    "published_date": "2025-10-02 02:07:41 UTC",
    "updated_date": "2025-10-02 02:07:41 UTC"
  },
  {
    "arxiv_id": "2510.01586v1",
    "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
    "authors": [
      "Zhenyu Pan",
      "Yiting Zhang",
      "Zhuo Liu",
      "Yolo Yunlong Tang",
      "Zeliang Zhang",
      "Haozheng Luo",
      "Yuwei Han",
      "Jianshu Zhang",
      "Dennis Wu",
      "Hong-Yu Chen",
      "Haoran Lu",
      "Haoyang Fang",
      "Manling Li",
      "Chenliang Xu",
      "Philip S. Yu",
      "Han Liu"
    ],
    "abstract": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01586v1",
    "published_date": "2025-10-02 02:06:30 UTC",
    "updated_date": "2025-10-02 02:06:30 UTC"
  },
  {
    "arxiv_id": "2510.01581v1",
    "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression",
    "authors": [
      "Joykirat Singh",
      "Justin Chih-Yao Chen",
      "Archiki Prasad",
      "Elias Stengel-Eskin",
      "Akshay Nambi",
      "Mohit Bansal"
    ],
    "abstract": "Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/joykirat18/TRAAC",
    "pdf_url": "https://arxiv.org/pdf/2510.01581v1",
    "published_date": "2025-10-02 02:00:20 UTC",
    "updated_date": "2025-10-02 02:00:20 UTC"
  },
  {
    "arxiv_id": "2510.01576v1",
    "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations",
    "authors": [
      "Ricardo Gonzalez Penuela",
      "Felipe Arias-Russi",
      "Victor Capriles"
    ],
    "abstract": "Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 2 figure, 2 tables, CV4A11y Workshop at ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.01576v1",
    "published_date": "2025-10-02 01:48:51 UTC",
    "updated_date": "2025-10-02 01:48:51 UTC"
  },
  {
    "arxiv_id": "2510.01574v1",
    "title": "Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query Autocomplete",
    "authors": [
      "Adithya Rajan",
      "Xiaoyu Liu",
      "Prateek Verma",
      "Vibhu Arora"
    ],
    "abstract": "We introduce a data-centric approach for mitigating presentation bias in real-time neural query autocomplete systems through the use of synthetic prefixes. These prefixes are generated from complete user queries collected during regular search sessions where autocomplete was not active. This allows us to enrich the training data for learning to rank models with more diverse and less biased examples. This method addresses the inherent bias in engagement signals collected from live query autocomplete interactions, where model suggestions influence user behavior. Our neural ranker is optimized for real-time deployment under strict latency constraints and incorporates a rich set of features, including query popularity, seasonality, fuzzy match scores, and contextual signals such as department affinity, device type, and vertical alignment with previous user queries. To support efficient training, we introduce a task-specific simplification of the listwise loss, reducing computational complexity from $O(n^2)$ to $O(n)$ by leveraging the query autocomplete structure of having only one ground-truth selection per prefix. Deployed in a large-scale e-commerce setting, our system demonstrates statistically significant improvements in user engagement, as measured by mean reciprocal rank and related metrics. Our findings show that synthetic prefixes not only improve generalization but also provide a scalable path toward bias mitigation in other low-latency ranking tasks, including related searches and query recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to the Proceedings of the ACM SIGIR Asia Pacific Conference on Information Retrieval (SIGIR-AP 2025), December 7-10, 2025, Xi'an, China",
    "pdf_url": "https://arxiv.org/pdf/2510.01574v1",
    "published_date": "2025-10-02 01:44:44 UTC",
    "updated_date": "2025-10-02 01:44:44 UTC"
  },
  {
    "arxiv_id": "2510.01571v1",
    "title": "From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?",
    "authors": [
      "Hanqun Cao",
      "Hongrui Zhang",
      "Junde Xu",
      "Zhou Zhang",
      "Lingdong Shen",
      "Minghao Sun",
      "Ge Liu",
      "Jinbo Xu",
      "Wu-Jun Li",
      "Jinren Ni",
      "Cesar de la Fuente-Nunez",
      "Tianfan Fu",
      "Yejin Choi",
      "Pheng-Ann Heng",
      "Fang Wu"
    ],
    "abstract": "Protein language models (PLMs) have advanced computational protein science through large-scale pretraining and scalable architectures. In parallel, reinforcement learning (RL) has broadened exploration and enabled precise multi-objective optimization in protein design. Yet whether RL can push PLMs beyond their pretraining priors to uncover latent sequence-structure-function rules remains unclear. We address this by pairing RL with PLMs across four domains: antimicrobial peptide design, kinase variant optimization, antibody engineering, and inverse folding. Using diverse RL algorithms and model classes, we ask if RL improves sampling efficiency and, more importantly, if it reveals capabilities not captured by supervised learning. Across benchmarks, RL consistently boosts success rates and sample efficiency. Performance follows a three-factor interaction: task headroom, reward fidelity, and policy capacity jointly determine gains. When rewards are accurate and informative, policies have sufficient capacity, and tasks leave room beyond supervised baselines, improvements scale; when rewards are noisy or capacity is constrained, gains saturate despite exploration. This view yields practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest. Implementation is available at https://github.com/chq1155/RL-PLM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 7 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.01571v1",
    "published_date": "2025-10-02 01:31:10 UTC",
    "updated_date": "2025-10-02 01:31:10 UTC"
  },
  {
    "arxiv_id": "2510.01569v1",
    "title": "InvThink: Towards AI Safety via Inverse Reasoning",
    "authors": [
      "Yubin Kim",
      "Taehan Kim",
      "Eugene Park",
      "Chunjong Park",
      "Cynthia Breazeal",
      "Daniel McDuff",
      "Hae Won Park"
    ],
    "abstract": "We present InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Our method reveals three key findings: (i) safety improvements show stronger scaling with model size compared to existing safety methods. (ii) InvThink mitigates safety tax; by training models to systematically consider failure modes, it preserves general reasoning capabilities on standard benchmarks. (iii) beyond general safety tasks, InvThink excels in high-stakes domains including external-facing (medicine, finance, law) and agentic (blackmail, murder) risk scenarios, achieving up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. We further implement InvThink via supervised fine-tuning, and reinforcement learning across three LLM families. These results suggest that inverse reasoning provides a scalable and generalizable path toward safer, more capable language models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01569v1",
    "published_date": "2025-10-02 01:26:53 UTC",
    "updated_date": "2025-10-02 01:26:53 UTC"
  },
  {
    "arxiv_id": "2510.01555v2",
    "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization",
    "authors": [
      "Kezhao Liu",
      "Jason Klein Liu",
      "Mingtao Chen",
      "Yiming Liu"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) leverages a Kullback-Leibler (KL) divergence loss to stabilize training and prevent overfitting. However, in methods such as GRPO, its implementation may be guided by principles from numerical value estimation-a practice that overlooks the term's functional role as an optimization loss. To analyze this issue, we establish a unified framework that connects two seemingly distinct implementation styles: using the mathematical term $k_n$ as a detached coefficient for the policy's score function ('$k_n$ in reward') or as a direct loss function through which gradients are propagated ('$k_n$ as loss'). We show that the latter can always be analyzed via an equivalent gradient coefficient in the former, unifying the two perspectives. Through this framework, we prove that the conventional '$k_1$ in reward' (like in PPO) is the principled loss for Reverse KL (RKL) regularization. We further establish a key finding: under on-policy conditions, the '$k_2$ as loss' formulation is, in fact, gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our work, identifies both as the theoretically sound implementations of the RKL objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like in GRPO) is merely a first-order, biased approximation of the principled loss. Furthermore, we argue that common off-policy implementations of '$k_n$ as loss' methods are biased due to neglected importance sampling, and we propose a principled correction. Our findings provide a comprehensive, gradient-based rationale for choosing and correctly implementing KL regularization, paving the way for more robust and effective RLHF systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01555v2",
    "published_date": "2025-10-02 01:00:02 UTC",
    "updated_date": "2025-10-06 11:59:12 UTC"
  },
  {
    "arxiv_id": "2510.01552v1",
    "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment",
    "authors": [
      "Luoxi Tang",
      "Yuqiao Meng",
      "Ankita Patra",
      "Weicheng Ma",
      "Muchao Ye",
      "Zhaohan Xi"
    ],
    "abstract": "Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "25 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.01552v1",
    "published_date": "2025-10-02 00:49:20 UTC",
    "updated_date": "2025-10-02 00:49:20 UTC"
  },
  {
    "arxiv_id": "2510.01545v2",
    "title": "Predictive Preference Learning from Human Interventions",
    "authors": [
      "Haoyuan Cai",
      "Zhenghao Peng",
      "Bolei Zhou"
    ],
    "abstract": "Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent's action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 Spotlight. Project page: https://metadriverse.github.io/ppl",
    "pdf_url": "https://arxiv.org/pdf/2510.01545v2",
    "published_date": "2025-10-02 00:38:18 UTC",
    "updated_date": "2025-10-15 23:33:59 UTC"
  },
  {
    "arxiv_id": "2510.01544v1",
    "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models",
    "authors": [
      "Shaoan Xie",
      "Lingjing Kong",
      "Xiangchen Song",
      "Xinshuai Dong",
      "Guangyi Chen",
      "Eric P. Xing",
      "Kun Zhang"
    ],
    "abstract": "Diffusion language models (dLLMs) offer a promising, non-autoregressive paradigm for text generation, yet training them for complex reasoning remains a key challenge. Current reinforcement learning approaches often rely on sparse, outcome-based rewards, which can reinforce flawed reasoning paths that lead to coincidentally correct answers. We argue that this stems from a fundamental mismatch with the natural structure of reasoning. We first propose a theoretical framework that formalizes complex problem solving as a hierarchical selection process, where an intractable global constraint is decomposed into a series of simpler, localized logical steps. This framework provides a principled foundation for algorithm design, including theoretical insights into the identifiability of this latent reasoning structure. Motivated by this theory, we identify unstructured refinement -- a failure mode where a model's iterative steps do not contribute meaningfully to the solution -- as a core deficiency in existing methods. We then introduce Step-Aware Policy Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising process with the latent reasoning hierarchy. By using a process-based reward function that encourages incremental progress, SAPO guides the model to learn structured, coherent reasoning paths. Our empirical results show that this principled approach significantly improves performance on challenging reasoning benchmarks and enhances the interpretability of the generation process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01544v1",
    "published_date": "2025-10-02 00:34:15 UTC",
    "updated_date": "2025-10-02 00:34:15 UTC"
  },
  {
    "arxiv_id": "2510.05145v1",
    "title": "FlashResearch: Real-time Agent Orchestration for Efficient Deep Research",
    "authors": [
      "Lunyiu Nie",
      "Nedim Lipka",
      "Ryan A. Rossi",
      "Swarat Chaudhuri"
    ],
    "abstract": "Deep research agents, which synthesize information across diverse sources, are significantly constrained by their sequential reasoning processes. This architectural bottleneck results in high latency, poor runtime adaptability, and inefficient resource allocation, making them impractical for interactive applications. To overcome this, we introduce FlashResearch, a novel framework for efficient deep research that transforms sequential processing into parallel, runtime orchestration by dynamically decomposing complex queries into tree-structured sub-tasks. Our core contributions are threefold: (1) an adaptive planner that dynamically allocates computational resources by determining research breadth and depth based on query complexity; (2) a real-time orchestration layer that monitors research progress and prunes redundant paths to reallocate resources and optimize efficiency; and (3) a multi-dimensional parallelization framework that enables concurrency across both research breadth and depth. Experiments show that FlashResearch consistently improves final report quality within fixed time budgets, and can deliver up to a 5x speedup while maintaining comparable quality.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05145v1",
    "published_date": "2025-10-02 00:15:39 UTC",
    "updated_date": "2025-10-02 00:15:39 UTC"
  },
  {
    "arxiv_id": "2510.01531v1",
    "title": "Information Seeking for Robust Decision Making under Partial Observability",
    "authors": [
      "Djengo Cyun-Jyun Fang",
      "Tsung-Wei Ke"
    ],
    "abstract": "Explicit information seeking is essential to human problem-solving in practical environments characterized by incomplete information and noisy dynamics. When the true environmental state is not directly observable, humans seek information to update their internal dynamics and inform future decision-making. Although existing Large Language Model (LLM) planning agents have addressed observational uncertainty, they often overlook discrepancies between their internal dynamics and the actual environment. We introduce Information Seeking Decision Planner (InfoSeeker), an LLM decision-making framework that integrates task-oriented planning with information seeking to align internal dynamics and make optimal decisions under uncertainty in both agent observations and environmental dynamics. InfoSeeker prompts an LLM to actively gather information by planning actions to validate its understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark suite featuring partially observable environments with incomplete observations and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74% absolute performance gain over prior methods without sacrificing sample efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms baselines on established benchmarks such as robotic manipulation and web navigation. These findings underscore the importance of tightly integrating planning and information seeking for robust behavior in partially observable environments. The project page is available at https://infoseekerllm.github.io",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "The project page is available at https://infoseekerllm.github.io",
    "pdf_url": "https://arxiv.org/pdf/2510.01531v1",
    "published_date": "2025-10-02 00:06:32 UTC",
    "updated_date": "2025-10-02 00:06:32 UTC"
  },
  {
    "arxiv_id": "2510.01530v1",
    "title": "LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning",
    "authors": [
      "Navapat Nananukul",
      "Yue Zhang",
      "Ryan Lee",
      "Eric Boxer",
      "Jonathan May",
      "Vibhav Giridhar Gogate",
      "Jay Pujara",
      "Mayank Kejriwal"
    ],
    "abstract": "High-assurance reasoning, particularly in critical domains such as law and medicine, requires conclusions that are accurate, verifiable, and explicitly grounded in evidence. This reasoning relies on premises codified from rules, statutes, and contracts, inherently involving defeasible or non-monotonic logic due to numerous exceptions, where the introduction of a single fact can invalidate general rules, posing significant challenges. While large language models (LLMs) excel at processing natural language, their capabilities in standard inference tasks do not translate to the rigorous reasoning required over high-assurance text guidelines. Core reasoning challenges within such texts often manifest specific logical structures involving negation, implication, and, most critically, defeasible rules and exceptions. In this paper, we propose a novel neurosymbolically-grounded architecture called LOGicalThought (LogT) that uses an advanced logical language and reasoner in conjunction with an LLM to construct a dual symbolic graph context and logic-based context. These two context representations transform the problem from inference over long-form guidelines into a compact grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines, LogT improves overall performance by 11.84% across all LLMs. Performance improves significantly across all three modes of reasoning: by up to +10.2% on negation, +13.2% on implication, and +5.5% on defeasible reasoning compared to the strongest baseline.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01530v1",
    "published_date": "2025-10-02 00:06:23 UTC",
    "updated_date": "2025-10-02 00:06:23 UTC"
  },
  {
    "arxiv_id": "2510.01528v1",
    "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation",
    "authors": [
      "Daniel Zhao",
      "Abhilash Shankarampeta",
      "Lanxiang Hu",
      "Tajana Rosing",
      "Hao Zhang"
    ],
    "abstract": "We propose a novel method that leverages sparse autoencoders (SAEs) and clustering techniques to analyze the internal token representations of large language models (LLMs) and guide generations in mathematical reasoning tasks. Our approach first trains an SAE to generate sparse vector representations for training tokens, then applies k-means clustering to construct a graph where vertices represent token clusters and weighted edges capture sequential token transitions. Using this graph, we define an edge-weight based reward function to quantify adherence to established reasoning traces, thereby identifying exploitative reasoning trajectories. Additionally, we measure generation diversity from clustering to assess the extent of exploration. Our findings indicate that balancing both exploitation and exploration is crucial for achieving high accuracy in mathematical reasoning tasks. During generation, the SAE can serve as a scalable reward model to guide generations, ensuring a balanced trade-off between exploitation and exploration. This prevents extreme behaviors in either direction, ultimately fostering a higher-quality reasoning process in LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01528v1",
    "published_date": "2025-10-02 00:01:08 UTC",
    "updated_date": "2025-10-02 00:01:08 UTC"
  }
]
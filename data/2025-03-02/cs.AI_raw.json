[
  {
    "arxiv_id": "2503.01062v3",
    "title": "SFO: Piloting VLM Feedback for Offline RL",
    "authors": [
      "Jacob Beck"
    ],
    "abstract": "While internet-scale image and textual data have enabled strong\ngeneralization in Vision-Language Models (VLMs), the absence of internet-scale\ncontrol data has impeded the development of similar generalization in standard\nreinforcement learning (RL) agents. Although VLMs are fundamentally limited in\ntheir ability to solve control tasks due to their lack of action-conditioned\ntraining data, their capacity for image understanding allows them to provide\nvaluable feedback in RL tasks by recognizing successful outcomes. A key\nchallenge in Reinforcement Learning from AI Feedback (RLAIF) is determining how\nbest to integrate VLM-derived signals into the learning process. We explore\nthis question in the context of offline RL and introduce a class of methods\ncalled sub-trajectory filtered optimization. We identify three key insights.\nFirst, trajectory length plays a crucial role in offline RL, as full-trajectory\npreference learning exacerbates the stitching problem, necessitating the use of\nsub-trajectories. Second, even in Markovian environments, a non-Markovian\nreward signal from a sequence of images is required to assess trajectory\nimprovement, as VLMs do not interpret control actions and must rely on visual\ncues over time. Third, a simple yet effective approach--filtered and weighted\nbehavior cloning--consistently outperforms more complex reinforcement learning\nfrom human feedback-based methods. We propose sub-trajectory filtered behavior\ncloning, a method that leverages VLM feedback on sub-trajectories while\nincorporating a retrospective filtering mechanism that removes sub-trajectories\npreceding failures to improve robustness and prevent turbulence. This study is\npreliminary; we provide initial evidence through evaluations on a toy control\ndomain. Please enjoy our airport puns.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is provided at https://github.com/jacooba/OfflineRLAIF",
    "pdf_url": "http://arxiv.org/pdf/2503.01062v3",
    "published_date": "2025-03-02 23:52:46 UTC",
    "updated_date": "2025-03-23 22:08:55 UTC"
  },
  {
    "arxiv_id": "2503.01046v1",
    "title": "MAPS: Multi-Fidelity AI-Augmented Photonic Simulation and Inverse Design Infrastructure",
    "authors": [
      "Pingchuan Ma",
      "Zhengqi Gao",
      "Meng Zhang",
      "Haoyu Yang",
      "Mark Ren",
      "Rena Huang",
      "Duane S. Boning",
      "Jiaqi Gu"
    ],
    "abstract": "Inverse design has emerged as a transformative approach for photonic device\noptimization, enabling the exploration of high-dimensional, non-intuitive\ndesign spaces to create ultra-compact devices and advance photonic integrated\ncircuits (PICs) in computing and interconnects. However, practical challenges,\nsuch as suboptimal device performance, limited manufacturability, high\nsensitivity to variations, computational inefficiency, and lack of\ninterpretability, have hindered its adoption in commercial hardware. Recent\nadvancements in AI-assisted photonic simulation and design offer transformative\npotential, accelerating simulations and design generation by orders of\nmagnitude over traditional numerical methods. Despite these breakthroughs, the\nlack of an open-source, standardized infrastructure and evaluation benchmark\nlimits accessibility and cross-disciplinary collaboration. To address this, we\nintroduce MAPS, a multi-fidelity AI-augmented photonic simulation and inverse\ndesign infrastructure designed to bridge this gap. MAPS features three\nsynergistic components: (1) MAPS-Data: A dataset acquisition framework for\ngenerating multi-fidelity, richly labeled devices, providing high-quality data\nfor AI-for-optics research. (2) MAPS-Train: A flexible AI-for-photonics\ntraining framework offering a hierarchical data loading pipeline, customizable\nmodel construction, support for data- and physics-driven losses, and\ncomprehensive evaluations. (3) MAPS-InvDes: An advanced adjoint inverse design\ntoolkit that abstracts complex physics but exposes flexible optimization steps,\nintegrates pre-trained AI models, and incorporates fabrication variation\nmodels. This infrastructure MAPS provides a unified, open-source platform for\ndeveloping, benchmarking, and advancing AI-assisted photonic design workflows,\naccelerating innovation in photonic hardware optimization and scientific\nmachine learning.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "physics.optics",
    "comment": "6 pages. Accepted to DATE 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.01046v1",
    "published_date": "2025-03-02 22:30:18 UTC",
    "updated_date": "2025-03-02 22:30:18 UTC"
  },
  {
    "arxiv_id": "2503.01030v1",
    "title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups",
    "authors": [
      "Yu Hou",
      "Hal Daumé III",
      "Rachel Rudinger"
    ],
    "abstract": "Studies of human psychology have demonstrated that people are more motivated\nto extend empathy to in-group members than out-group members (Cikara et al.,\n2011). In this study, we investigate how this aspect of intergroup relations in\nhumans is replicated by LLMs in an emotion intensity prediction task. In this\ntask, the LLM is given a short description of an experience a person had that\ncaused them to feel a particular emotion; the LLM is then prompted to predict\nthe intensity of the emotion the person experienced on a numerical scale. By\nmanipulating the group identities assigned to the LLM's persona (the\n\"perceiver\") and the person in the narrative (the \"experiencer\"), we measure\nhow predicted emotion intensities differ between in-group and out-group\nsettings. We observe that LLMs assign higher emotion intensity scores to\nin-group members than out-group members. This pattern holds across all three\ntypes of social groupings we tested: race/ethnicity, nationality, and religion.\nWe perform an in-depth analysis on Llama-3.1-8B, the model which exhibited\nstrongest intergroup bias among those tested.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.01030v1",
    "published_date": "2025-03-02 21:31:14 UTC",
    "updated_date": "2025-03-02 21:31:14 UTC"
  },
  {
    "arxiv_id": "2503.01022v1",
    "title": "LLM-Fusion: A Novel Multimodal Fusion Model for Accelerated Material Discovery",
    "authors": [
      "Onur Boyar",
      "Indra Priyadarsini",
      "Seiji Takeda",
      "Lisa Hamada"
    ],
    "abstract": "Discovering materials with desirable properties in an efficient way remains a\nsignificant problem in materials science. Many studies have tackled this\nproblem by using different sets of information available about the materials.\nAmong them, multimodal approaches have been found to be promising because of\ntheir ability to combine different sources of information. However, fusion\nalgorithms to date remain simple, lacking a mechanism to provide a rich\nrepresentation of multiple modalities. This paper presents LLM-Fusion, a novel\nmultimodal fusion model that leverages large language models (LLMs) to\nintegrate diverse representations, such as SMILES, SELFIES, text descriptions,\nand molecular fingerprints, for accurate property prediction. Our approach\nintroduces a flexible LLM-based architecture that supports multimodal input\nprocessing and enables material property prediction with higher accuracy than\ntraditional methods. We validate our model on two datasets across five\nprediction tasks and demonstrate its effectiveness compared to unimodal and\nnaive concatenation baselines.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "4 pages, presented at AAAI 2025 Workshop on AI to Accelerating\n  Science and Engineering (AI2ASE)",
    "pdf_url": "http://arxiv.org/pdf/2503.01022v1",
    "published_date": "2025-03-02 21:13:04 UTC",
    "updated_date": "2025-03-02 21:13:04 UTC"
  },
  {
    "arxiv_id": "2503.01019v3",
    "title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations",
    "authors": [
      "Ziyang Zhang",
      "Yang Yu",
      "Yucheng Chen",
      "Xulei Yang",
      "Si Yong Yeo"
    ],
    "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current\napproaches predominantly emphasize feature extraction and cross-modal\ncomprehension, with limited attention to generating or transforming visual\ncontent. This gap hinders the model's ability to synthesize coherent and novel\nvisual representations from textual prompts, thereby reducing the effectiveness\nof multi-modal learning. In this work, we propose MedUnifier, a unified VLP\nframework tailored for medical data. MedUnifier seamlessly integrates\ntext-grounded image generation capabilities with multi-modal learning\nstrategies, including image-text contrastive alignment, image-text matching and\nimage-grounded text generation. Unlike traditional methods that reply on\ncontinuous visual representations, our approach employs visual vector\nquantization, which not only facilitates a more cohesive learning strategy for\ncross-modal understanding but also enhances multi-modal generation quality by\neffectively leveraging discrete representations. Our framework's effectiveness\nis evidenced by the experiments on established benchmarks, including uni-modal\ntasks (supervised fine-tuning), cross-modal tasks (image-text retrieval and\nzero-shot image classification), and multi-modal tasks (medical report\ngeneration, image synthesis), where it achieves state-of-the-art performance\nacross various tasks. MedUnifier also offers a highly adaptable tool for a wide\nrange of language and vision tasks in healthcare, marking advancement toward\nthe development of a generalizable AI model for medical applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "To be pubilshed in CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.01019v3",
    "published_date": "2025-03-02 21:09:32 UTC",
    "updated_date": "2025-04-20 21:18:03 UTC"
  },
  {
    "arxiv_id": "2503.01009v1",
    "title": "An Exact Solver for Satisfiability Modulo Counting with Probabilistic Circuits",
    "authors": [
      "Jinzhao Li",
      "Nan Jiang",
      "Yexiang Xue"
    ],
    "abstract": "Satisfiability Modulo Counting (SMC) is a recently proposed general language\nto reason about problems integrating statistical and symbolic artificial\nintelligence. An SMC formula is an extended SAT formula in which the truth\nvalues of a few Boolean variables are determined by probabilistic inference.\nExisting approximate solvers optimize surrogate objectives, which lack formal\nguarantees. Current exact solvers directly integrate SAT solvers and\nprobabilistic inference solvers resulting in slow performance because of many\nback-and-forth invocations of both solvers. We propose KOCO-SMC, an integrated\nexact SMC solver that efficiently tracks lower and upper bounds in the\nprobabilistic inference process. It enhances computational efficiency by\nenabling early estimation of probabilistic inference using only partial\nvariable assignments, whereas existing methods require full variable\nassignments. In the experiment, we compare KOCO-SMC with currently available\napproximate and exact SMC solvers on large-scale datasets and real-world\napplications. Our approach delivers high-quality solutions with high\nefficiency.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01009v1",
    "published_date": "2025-03-02 20:28:20 UTC",
    "updated_date": "2025-03-02 20:28:20 UTC"
  },
  {
    "arxiv_id": "2503.01003v1",
    "title": "A Semantic Search Pipeline for Causality-driven Adhoc Information Retrieval",
    "authors": [
      "Dhairya Dalal",
      "Sharmi Dev Gupta",
      "Bentolhoda Binaei"
    ],
    "abstract": "We present a unsupervised semantic search pipeline for the Causality-driven\nAdhoc Information Retrieval (CAIR-2021) shared task. The CAIR shared task\nexpands traditional information retrieval to support the retrieval of documents\ncontaining the likely causes of a query event. A successful system must be able\nto distinguish between topical documents and documents containing causal\ndescriptions of events that are causally related to the query event. Our\napproach involves aggregating results from multiple query strategies over a\nsemantic and lexical index. The proposed approach leads the CAIR-2021\nleaderboard and outperformed both traditional IR and pure semantic\nembedding-based approaches.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01003v1",
    "published_date": "2025-03-02 19:59:41 UTC",
    "updated_date": "2025-03-02 19:59:41 UTC"
  },
  {
    "arxiv_id": "2503.01927v1",
    "title": "QCS-ADME: Quantum Circuit Search for Drug Property Prediction with Imbalanced Data and Regression Adaptation",
    "authors": [
      "Kangyu Zheng",
      "Tianfan Fu",
      "Zhiding Liang"
    ],
    "abstract": "The biomedical field is beginning to explore the use of quantum machine\nlearning (QML) for tasks traditionally handled by classical machine learning,\nespecially in predicting ADME (absorption, distribution, metabolism, and\nexcretion) properties, which are essential in drug evaluation. However, ADME\ntasks pose unique challenges for existing quantum computing systems (QCS)\nframeworks, as they involve both classification with unbalanced dataset and\nregression problems. These dual requirements make it necessary to adapt and\nrefine current QCS frameworks to effectively address the complexities of ADME\npredictions. We propose a novel training-free scoring mechanism to evaluate QML\ncircuit performance on imbalanced classification and regression tasks. Our\nmechanism demonstrates significant correlation between scoring metrics and test\nperformance on imbalanced classification tasks. Additionally, we develop\nmethods to quantify continuous similarity relationships between quantum states,\nenabling performance prediction for regression tasks. This represents the first\ncomprehensive approach to searching and evaluating QCS circuits specifically\nfor regression applications. Validation on representative ADME tasks-one\nimbalanced classification and one regression-demonstrates moderate positive\ncorrelation between our scoring metrics and circuit performance, significantly\noutperforming baseline scoring methods that show negligible correlation.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01927v1",
    "published_date": "2025-03-02 19:29:04 UTC",
    "updated_date": "2025-03-02 19:29:04 UTC"
  },
  {
    "arxiv_id": "2503.00992v1",
    "title": "Evidence of conceptual mastery in the application of rules by Large Language Models",
    "authors": [
      "José Luiz Nunes",
      "Guilherme FCF Almeida",
      "Brian Flanagan"
    ],
    "abstract": "In this paper we leverage psychological methods to investigate LLMs'\nconceptual mastery in applying rules. We introduce a novel procedure to match\nthe diversity of thought generated by LLMs to that observed in a human sample.\nWe then conducted two experiments comparing rule-based decision-making in\nhumans and LLMs. Study 1 found that all investigated LLMs replicated human\npatterns regardless of whether they are prompted with scenarios created before\nor after their training cut-off. Moreover, we found unanticipated differences\nbetween the two sets of scenarios among humans. Surprisingly, even these\ndifferences were replicated in LLM responses. Study 2 turned to a contextual\nfeature of human rule application: under forced time delay, human samples rely\nmore heavily on a rule's text than on other considerations such as a rule's\npurpose.. Our results revealed that some models (Gemini Pro and Claude 3)\nresponded in a human-like manner to a prompt describing either forced delay or\ntime pressure, while others (GPT-4o and Llama 3.2 90b) did not. We argue that\nthe evidence gathered suggests that LLMs have mastery over the concept of rule,\nwith implications for both legal decision making and philosophical inquiry.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00992v1",
    "published_date": "2025-03-02 19:23:46 UTC",
    "updated_date": "2025-03-02 19:23:46 UTC"
  },
  {
    "arxiv_id": "2503.16472v2",
    "title": "Human-AI Interaction Design Standards",
    "authors": [
      "Chaoyi Zhao",
      "Wei Xu"
    ],
    "abstract": "The rapid development of artificial intelligence (AI) has significantly\ntransformed human-computer interactions, making it essential to establish\nrobust design standards to ensure effective, ethical, and human-centered AI\n(HCAI) solutions. Standards serve as the foundation for the adoption of new\ntechnologies, and human-AI interaction (HAII) standards are critical to\nsupporting the industrialization of AI technology by following an HCAI\napproach. These design standards aim to provide clear principles, requirements,\nand guidelines for designing, developing, deploying, and using AI systems,\nenhancing the user experience and performance of AI systems. Despite their\nimportance, the creation and adoption of HCAI-based interaction design\nstandards face challenges, including the absence of universal frameworks, the\ninherent complexity of HAII, and the ethical dilemmas that arise in such\nsystems. This chapter provides a comparative analysis of HAII versus\ntraditional human-computer interaction (HCI) and outlines guiding principles\nfor HCAI-based design. It explores international, regional, national, and\nindustry standards related to HAII design from an HCAI perspective and reviews\ndesign guidelines released by leading companies such as Microsoft, Google, and\nApple. Additionally, the chapter highlights tools available for implementing\nHAII standards and presents case studies of human-centered interaction design\nfor AI systems in diverse fields, including healthcare, autonomous vehicles,\nand customer service. It further examines key challenges in developing HAII\nstandards and suggests future directions for the field. Emphasizing the\nimportance of ongoing collaboration between AI designers, developers, and\nexperts in human factors and HCI, this chapter stresses the need to advance\nHCAI-based interaction design standards to ensure human-centered AI solutions\nacross various domains.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16472v2",
    "published_date": "2025-03-02 18:50:54 UTC",
    "updated_date": "2025-03-24 03:21:12 UTC"
  },
  {
    "arxiv_id": "2503.00979v1",
    "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses in LLMs",
    "authors": [
      "Ravi Ghadia",
      "Avinash Kumar",
      "Gaurav Jain",
      "Prashant Nair",
      "Poulami Das"
    ],
    "abstract": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00979v1",
    "published_date": "2025-03-02 18:12:50 UTC",
    "updated_date": "2025-03-02 18:12:50 UTC"
  },
  {
    "arxiv_id": "2503.00962v1",
    "title": "Using Synthetic Images to Augment Small Medical Image Datasets",
    "authors": [
      "Minh H. Vu",
      "Lorenzo Tronchin",
      "Tufve Nyholm",
      "Tommy Löfstedt"
    ],
    "abstract": "Recent years have witnessed a growing academic and industrial interest in\ndeep learning (DL) for medical imaging. To perform well, DL models require very\nlarge labeled datasets. However, most medical imaging datasets are small, with\na limited number of annotated samples. The reason they are small is usually\nbecause delineating medical images is time-consuming and demanding for\noncologists. There are various techniques that can be used to augment a\ndataset, for example, to apply affine transformations or elastic\ntransformations to available images, or to add synthetic images generated by a\nGenerative Adversarial Network (GAN). In this work, we have developed a novel\nconditional variant of a current GAN method, the StyleGAN2, to generate\nmulti-modal high-resolution medical images with the purpose to augment small\nmedical imaging datasets with these synthetic images. We use the synthetic and\nreal images from six datasets to train models for the downstream task of\nsemantic segmentation. The quality of the generated medical images and the\neffect of this augmentation on the segmentation performance were evaluated\nafterward. Finally, the results indicate that the downstream segmentation\nmodels did not benefit from the generated images. Further work and analyses are\nrequired to establish how this augmentation affects the segmentation\nperformance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.00962v1",
    "published_date": "2025-03-02 17:02:11 UTC",
    "updated_date": "2025-03-02 17:02:11 UTC"
  },
  {
    "arxiv_id": "2503.00957v2",
    "title": "Exploiting Vulnerabilities in Speech Translation Systems through Targeted Adversarial Attacks",
    "authors": [
      "Chang Liu",
      "Haolin Wu",
      "Xi Yang",
      "Kui Zhang",
      "Cong Wu",
      "Weiming Zhang",
      "Nenghai Yu",
      "Tianwei Zhang",
      "Qing Guo",
      "Jie Zhang"
    ],
    "abstract": "As speech translation (ST) systems become increasingly prevalent,\nunderstanding their vulnerabilities is crucial for ensuring robust and reliable\ncommunication. However, limited work has explored this issue in depth. This\npaper explores methods of compromising these systems through imperceptible\naudio manipulations. Specifically, we present two innovative approaches: (1)\nthe injection of perturbation into source audio, and (2) the generation of\nadversarial music designed to guide targeted translation, while also conducting\nmore practical over-the-air attacks in the physical world. Our experiments\nreveal that carefully crafted audio perturbations can mislead translation\nmodels to produce targeted, harmful outputs, while adversarial music achieve\nthis goal more covertly, exploiting the natural imperceptibility of music.\nThese attacks prove effective across multiple languages and translation models,\nhighlighting a systemic vulnerability in current ST architectures. The\nimplications of this research extend beyond immediate security concerns,\nshedding light on the interpretability and robustness of neural speech\nprocessing systems. Our findings underscore the need for advanced defense\nmechanisms and more resilient architectures in the realm of audio systems. More\ndetails and samples can be found at https://adv-st.github.io.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Preprint,17 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.00957v2",
    "published_date": "2025-03-02 16:38:16 UTC",
    "updated_date": "2025-03-05 03:07:49 UTC"
  },
  {
    "arxiv_id": "2503.00955v2",
    "title": "SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking",
    "authors": [
      "Dien X. Tran",
      "Nam V. Nguyen",
      "Thanh T. Tran",
      "Anh T. Hoang",
      "Tai V. Duong",
      "Di T. Le",
      "Phuc-Lu Le"
    ],
    "abstract": "The rise of misinformation, exacerbated by Large Language Models (LLMs) like\nGPT and Gemini, demands robust fact-checking solutions, especially for\nlow-resource languages like Vietnamese. Existing methods struggle with semantic\nambiguity, homonyms, and complex linguistic structures, often trading accuracy\nfor efficiency. We introduce SemViQA, a novel Vietnamese fact-checking\nframework integrating Semantic-based Evidence Retrieval (SER) and Two-step\nVerdict Classification (TVC). Our approach balances precision and speed,\nachieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01\nand 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge.\nAdditionally, SemViQA Faster improves inference speed 7x while maintaining\ncompetitive accuracy. SemViQA sets a new benchmark for Vietnamese fact\nverification, advancing the fight against misinformation. The source code is\navailable at: https://github.com/DAVID-NGUYEN-S16/SemViQA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.00955v2",
    "published_date": "2025-03-02 16:22:46 UTC",
    "updated_date": "2025-05-12 03:35:52 UTC"
  },
  {
    "arxiv_id": "2503.00945v1",
    "title": "Cross Modality Medical Image Synthesis for Improving Liver Segmentation",
    "authors": [
      "Muhammad Rafiq",
      "Hazrat Ali",
      "Ghulam Mujtaba",
      "Zubair Shah",
      "Shoaib Azmat"
    ],
    "abstract": "Deep learning-based computer-aided diagnosis (CAD) of medical images requires\nlarge datasets. However, the lack of large publicly available labeled datasets\nlimits the development of deep learning-based CAD systems. Generative\nAdversarial Networks (GANs), in particular, CycleGAN, can be used to generate\nnew cross-domain images without paired training data. However, most\nCycleGAN-based synthesis methods lack the potential to overcome alignment and\nasymmetry between the input and generated data. We propose a two-stage\ntechnique for the synthesis of abdominal MRI using cross-modality translation\nof abdominal CT. We show that the synthetic data can help improve the\nperformance of the liver segmentation network. We increase the number of\nabdominal MRI images through cross-modality image transformation of unpaired CT\nimages using a CycleGAN inspired deformation invariant network called EssNet.\nSubsequently, we combine the synthetic MRI images with the original MRI images\nand use them to improve the accuracy of the U-Net on a liver segmentation task.\nWe train the U-Net on real MRI images and then on real and synthetic MRI\nimages. Consequently, by comparing both scenarios, we achieve an improvement in\nthe performance of U-Net. In summary, the improvement achieved in the\nIntersection over Union (IoU) is 1.17%. The results show potential to address\nthe data scarcity challenge in medical imaging.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Submitted to Computer Methods in Biomechanics and Biomedical\n  Engineering: Imaging & Visualization",
    "pdf_url": "http://arxiv.org/pdf/2503.00945v1",
    "published_date": "2025-03-02 15:54:12 UTC",
    "updated_date": "2025-03-02 15:54:12 UTC"
  },
  {
    "arxiv_id": "2503.00940v1",
    "title": "Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions",
    "authors": [
      "Vijay Keswani",
      "Vincent Conitzer",
      "Walter Sinnott-Armstrong",
      "Breanna K. Nguyen",
      "Hoda Heidari",
      "Jana Schaich Borg"
    ],
    "abstract": "A growing body of work in Ethical AI attempts to capture human moral\njudgments through simple computational models. The key question we address in\nthis work is whether such simple AI models capture {the critical} nuances of\nmoral decision-making by focusing on the use case of kidney allocation. We\nconducted twenty interviews where participants explained their rationale for\ntheir judgments about who should receive a kidney. We observe participants: (a)\nvalue patients' morally-relevant attributes to different degrees; (b) use\ndiverse decision-making processes, citing heuristics to reduce decision\ncomplexity; (c) can change their opinions; (d) sometimes lack confidence in\ntheir decisions (e.g., due to incomplete information); and (e) express\nenthusiasm and concern regarding AI assisting humans in kidney allocation\ndecisions. Based on these findings, we discuss challenges of computationally\nmodeling moral judgments {as a stand-in for human input}, highlight drawbacks\nof current approaches, and suggest future directions to address these issues.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "In ACM Conference on Human Factors in Computing Systems (CHI), 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.00940v1",
    "published_date": "2025-03-02 15:42:17 UTC",
    "updated_date": "2025-03-02 15:42:17 UTC"
  },
  {
    "arxiv_id": "2503.00932v1",
    "title": "Improving the Transferability of Adversarial Attacks by an Input Transpose",
    "authors": [
      "Qing Wan",
      "Shilong Deng",
      "Xun Wang"
    ],
    "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle perturbations applied to inputs that are often imperceptible\nto humans yet lead to incorrect model predictions. In black-box scenarios,\nhowever, existing adversarial examples exhibit limited transferability and\nstruggle to effectively compromise multiple unseen DNN models. Previous\nstrategies enhance the cross-model generalization of adversarial examples by\nintroducing versatility into adversarial perturbations, thereby improving\ntransferability. However, further refining perturbation versatility often\ndemands intricate algorithm development and substantial computation\nconsumption. In this work, we propose an input transpose method that requires\nalmost no additional labor and computation costs but can significantly improve\nthe transferability of existing adversarial strategies. Even without adding\nadversarial perturbations, our method demonstrates considerable effectiveness\nin cross-model attacks. Our exploration finds that on specific datasets, a mere\n$1^\\circ$ left or right rotation might be sufficient for most adversarial\nexamples to deceive unseen models. Our further analysis suggests that this\ntransferability improvement triggered by rotating only $1^\\circ$ may stem from\nvisible pattern shifts in the DNN's low-level feature maps. Moreover, this\ntransferability exhibits optimal angles that, when identified under\nunrestricted query conditions, could potentially yield even greater\nperformance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.00932v1",
    "published_date": "2025-03-02 15:13:41 UTC",
    "updated_date": "2025-03-02 15:13:41 UTC"
  },
  {
    "arxiv_id": "2503.00930v1",
    "title": "Behavior Preference Regression for Offline Reinforcement Learning",
    "authors": [
      "Padmanaba Srinivasan",
      "William Knottenbelt"
    ],
    "abstract": "Offline reinforcement learning (RL) methods aim to learn optimal policies\nwith access only to trajectories in a fixed dataset. Policy constraint methods\nformulate policy learning as an optimization problem that balances maximizing\nreward with minimizing deviation from the behavior policy. Closed form\nsolutions to this problem can be derived as weighted behavioral cloning\nobjectives that, in theory, must compute an intractable partition function.\nReinforcement learning has gained popularity in language modeling to align\nmodels with human preferences; some recent works consider paired completions\nthat are ranked by a preference model following which the likelihood of the\npreferred completion is directly increased. We adapt this approach of paired\ncomparison. By reformulating the paired-sample optimization problem, we fit the\nmaximum-mode of the Q function while maximizing behavioral consistency of\npolicy actions. This yields our algorithm, Behavior Preference Regression for\noffline RL (BPR). We empirically evaluate BPR on the widely used D4RL\nLocomotion and Antmaze datasets, as well as the more challenging V-D4RL suite,\nwhich operates in image-based state spaces. BPR demonstrates state-of-the-art\nperformance over all domains. Our on-policy experiments suggest that BPR takes\nadvantage of the stability of on-policy value functions with minimal\nperceptible performance degradation on Locomotion datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Conference paper at AAAI 25",
    "pdf_url": "http://arxiv.org/pdf/2503.00930v1",
    "published_date": "2025-03-02 15:13:02 UTC",
    "updated_date": "2025-03-02 15:13:02 UTC"
  },
  {
    "arxiv_id": "2503.00915v1",
    "title": "Multimodal Distillation-Driven Ensemble Learning for Long-Tailed Histopathology Whole Slide Images Analysis",
    "authors": [
      "Xitong Ling",
      "Yifeng Ping",
      "Jiawen Li",
      "Jing Peng",
      "Yuxuan Chen",
      "Minxi Ouyang",
      "Yizhi Wang",
      "Yonghong He",
      "Tian Guan",
      "Xiaoping Liu",
      "Lianghui Zhu"
    ],
    "abstract": "Multiple Instance Learning (MIL) plays a significant role in computational\npathology, enabling weakly supervised analysis of Whole Slide Image (WSI)\ndatasets. The field of WSI analysis is confronted with a severe long-tailed\ndistribution problem, which significantly impacts the performance of\nclassifiers. Long-tailed distributions lead to class imbalance, where some\nclasses have sparse samples while others are abundant, making it difficult for\nclassifiers to accurately identify minority class samples. To address this\nissue, we propose an ensemble learning method based on MIL, which employs\nexpert decoders with shared aggregators and consistency constraints to learn\ndiverse distributions and reduce the impact of class imbalance on classifier\nperformance. Moreover, we introduce a multimodal distillation framework that\nleverages text encoders pre-trained on pathology-text pairs to distill\nknowledge and guide the MIL aggregator in capturing stronger semantic features\nrelevant to class information. To ensure flexibility, we use learnable prompts\nto guide the distillation process of the pre-trained text encoder, avoiding\nlimitations imposed by specific prompts. Our method, MDE-MIL, integrates\nmultiple expert branches focusing on specific data distributions to address\nlong-tailed issues. Consistency control ensures generalization across classes.\nMultimodal distillation enhances feature extraction. Experiments on\nCamelyon+-LT and PANDA-LT datasets show it outperforms state-of-the-art\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00915v1",
    "published_date": "2025-03-02 14:31:45 UTC",
    "updated_date": "2025-03-02 14:31:45 UTC"
  },
  {
    "arxiv_id": "2503.00912v1",
    "title": "HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning",
    "authors": [
      "Zhuohang Jiang",
      "Pangjing Wu",
      "Ziran Liang",
      "Peter Q. Chen",
      "Xu Yuan",
      "Ye Jia",
      "Jiancheng Tu",
      "Chen Li",
      "Peter H. F. Ng",
      "Qing Li"
    ],
    "abstract": "Structure reasoning is a fundamental capability of large language models\n(LLMs), enabling them to reason about structured commonsense and answer\nmulti-hop questions. However, existing benchmarks for structure reasoning\nmainly focus on horizontal and coordinate structures (\\emph{e.g.} graphs),\noverlooking the hierarchical relationships within them. Hierarchical structure\nreasoning is crucial for human cognition, particularly in memory organization\nand problem-solving. It also plays a key role in various real-world tasks, such\nas information extraction and decision-making. To address this gap, we propose\nHiBench, the first framework spanning from initial structure generation to\nfinal proficiency assessment, designed to benchmark the hierarchical reasoning\ncapabilities of LLMs systematically. HiBench encompasses six representative\nscenarios, covering both fundamental and practical aspects, and consists of 30\ntasks with varying hierarchical complexity, totaling 39,519 queries. To\nevaluate LLMs comprehensively, we develop five capability dimensions that\ndepict different facets of hierarchical structure understanding. Through\nextensive evaluation of 20 LLMs from 10 model families, we reveal key insights\ninto their capabilities and limitations: 1) existing LLMs show proficiency in\nbasic hierarchical reasoning tasks; 2) they still struggle with more complex\nstructures and implicit hierarchical representations, especially in structural\nmodification and textual reasoning. Based on these findings, we create a small\nyet well-designed instruction dataset, which enhances LLMs' performance on\nHiBench by an average of 88.84\\% (Llama-3.1-8B) and 31.38\\% (Qwen2.5-7B) across\nall tasks. The HiBench dataset and toolkit are available here,\nhttps://github.com/jzzzzh/HiBench, to encourage evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00912v1",
    "published_date": "2025-03-02 14:25:37 UTC",
    "updated_date": "2025-03-02 14:25:37 UTC"
  },
  {
    "arxiv_id": "2503.00900v1",
    "title": "S4M: S4 for multivariate time series forecasting with Missing values",
    "authors": [
      "Jing Peng",
      "Meiqi Yang",
      "Qiong Zhang",
      "Xiaoxiao Li"
    ],
    "abstract": "Multivariate time series data play a pivotal role in a wide range of\nreal-world applications. However, the presence of block missing data introduces\nsignificant challenges, often compromising the performance of predictive\nmodels. Traditional two-step approaches, which first impute missing values and\nthen perform forecasting, are prone to error accumulation, particularly in\ncomplex multivariate settings characterized by high missing ratios and\nintricate dependency structures. In this work, we introduce S4M, an end-to-end\ntime series forecasting framework that seamlessly integrates missing data\nhandling into the Structured State Space Sequence (S4) model architecture.\nUnlike conventional methods that treat imputation as a separate preprocessing\nstep, S4M leverages the latent space of S4 models to directly recognize and\nrepresent missing data patterns, thereby more effectively capturing the\nunderlying temporal and multivariate dependencies. Our framework comprises two\nkey components: the Adaptive Temporal Prototype Mapper (ATPM) and the\nMissing-Aware Dual Stream S4 (MDS-S4). The ATPM employs a prototype bank to\nderive robust and informative representations from historical data patterns,\nwhile the MDS-S4 processes these representations alongside missingness masks as\ndual input streams to enable accurate forecasting. Through extensive empirical\nevaluations on diverse real-world datasets, we demonstrate that S4M\nconsistently achieves state-of-the-art performance. These results underscore\nthe efficacy of our integrated approach in handling missing data, showcasing\nits robustness and superiority over traditional imputation-based methods. Our\nfindings highlight the potential of S4M to advance reliable time series\nforecasting in practical applications, offering a promising direction for\nfuture research and deployment. Code is available at\nhttps://github.com/WINTERWEEL/S4M.git.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00900v1",
    "published_date": "2025-03-02 13:59:59 UTC",
    "updated_date": "2025-03-02 13:59:59 UTC"
  },
  {
    "arxiv_id": "2503.00897v4",
    "title": "A Simple and Effective Reinforcement Learning Method for Text-to-Image Diffusion Fine-tuning",
    "authors": [
      "Shashank Gupta",
      "Chaitanya Ahuja",
      "Tsung-Yu Lin",
      "Sreya Dutta Roy",
      "Harrie Oosterhuis",
      "Maarten de Rijke",
      "Satya Narayan Shukla"
    ],
    "abstract": "Reinforcement learning (RL)-based fine-tuning has emerged as a powerful\napproach for aligning diffusion models with black-box objectives. Proximal\npolicy optimization (PPO) is the most popular choice of method for policy\noptimization. While effective in terms of performance, PPO is highly sensitive\nto hyper-parameters and involves substantial computational overhead. REINFORCE,\non the other hand, mitigates some computational complexities such as high\nmemory overhead and sensitive hyper-parameter tuning, but has suboptimal\nperformance due to high-variance and sample inefficiency. While the variance of\nthe REINFORCE can be reduced by sampling multiple actions per input prompt and\nusing a baseline correction term, it still suffers from sample inefficiency. To\naddress these challenges, we systematically analyze the\nefficiency-effectiveness trade-off between REINFORCE and PPO, and propose\nleave-one-out PPO (LOOP), a novel RL for diffusion fine-tuning method. LOOP\ncombines variance reduction techniques from REINFORCE, such as sampling\nmultiple actions per input prompt and a baseline correction term, with the\nrobustness and sample efficiency of PPO via clipping and importance sampling.\nOur results demonstrate that LOOP effectively improves diffusion models on\nvarious black-box objectives, and achieves a better balance between\ncomputational efficiency and performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00897v4",
    "published_date": "2025-03-02 13:43:53 UTC",
    "updated_date": "2025-03-12 12:43:07 UTC"
  },
  {
    "arxiv_id": "2503.00890v1",
    "title": "Estimating Blood Pressure with a Camera: An Exploratory Study of Ambulatory Patients with Cardiovascular Disease",
    "authors": [
      "Theodore Curran",
      "Chengqian Ma",
      "Xin Liu",
      "Daniel McDuff",
      "Girish Narayanswamy",
      "George Stergiou",
      "Shwetak Patel",
      "Eugene Yang"
    ],
    "abstract": "Hypertension is a leading cause of morbidity and mortality worldwide. The\nability to diagnose and treat hypertension in the ambulatory population is\nhindered by limited access and poor adherence to current methods of monitoring\nblood pressure (BP), specifically, cuff-based devices. Remote\nphotoplethysmography (rPPG) evaluates an individual's pulse waveform through a\nstandard camera without physical contact. Cameras are readily available to the\nmajority of the global population via embedded technologies such as\nsmartphones, thus rPPG is a scalable and promising non-invasive method of BP\nmonitoring. The few studies investigating rPPG for BP measurement have excluded\nhigh-risk populations, including those with cardiovascular disease (CVD) or its\nrisk factors, as well as subjects in active cardiac arrhythmia. The impact of\narrhythmia, like atrial fibrillation, on the prediction of BP using rPPG is\ncurrently uncertain. We performed a study to better understand the relationship\nbetween rPPG and BP in a real-world sample of ambulatory patients from a\ncardiology clinic with established CVD or risk factors for CVD. We collected\nsimultaneous rPPG, PPG, BP, ECG, and other vital signs data from 143 subjects\nwhile at rest, and used this data plus demographics to train a deep learning\nmodel to predict BP. We report that facial rPPG yields a signal that is\ncomparable to finger PPG. Pulse wave analysis (PWA)-based BP estimates on this\ncohort performed comparably to studies on healthier subjects, and notably, the\naccuracy of BP prediction in subjects with atrial fibrillation was not inferior\nto subjects with normal sinus rhythm. In a binary classification task, the rPPG\nmodel identified subjects with systolic BP $\\geq$ 130 mm Hg with a positive\npredictive value of 71% (baseline prevalence 48.3%), highlighting the potential\nof rPPG for hypertension monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00890v1",
    "published_date": "2025-03-02 13:24:50 UTC",
    "updated_date": "2025-03-02 13:24:50 UTC"
  },
  {
    "arxiv_id": "2503.00881v1",
    "title": "Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization",
    "authors": [
      "You Shen",
      "Zhipeng Zhang",
      "Xinyang Li",
      "Yansong Qu",
      "Yu Lin",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ],
    "abstract": "Representing 3D scenes from multiview images is a core challenge in computer\nvision and graphics, which requires both precise rendering and accurate\nreconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant\nattention for its high-quality rendering and fast inference speed. Yet, due to\nthe unstructured and irregular nature of Gaussian point clouds, ensuring\naccurate geometry reconstruction remains difficult. Existing methods primarily\nfocus on geometry regularization, with common approaches including\nprimitive-based and dual-model frameworks. However, the former suffers from\ninherent conflicts between rendering and reconstruction, while the latter is\ncomputationally and storage-intensive. To address these challenges, we propose\nCarGS, a unified model leveraging Contribution-adaptive regularization to\nachieve simultaneous, high-quality rendering and surface reconstruction. The\nessence of our framework is learning adaptive contribution for Gaussian\nprimitives by squeezing the knowledge from geometry regularization into a\ncompact MLP. Additionally, we introduce a geometry-guided densification\nstrategy with clues from both normals and Signed Distance Fields (SDF) to\nimprove the capability of capturing high-frequency details. Our design improves\nthe mutual learning of the two tasks, meanwhile its unified structure does not\nrequire separate models as in dual-model based approaches, guaranteeing\nefficiency. Extensive experiments demonstrate the ability to achieve\nstate-of-the-art (SOTA) results in both rendering fidelity and reconstruction\naccuracy while maintaining real-time speed and minimal storage size.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00881v1",
    "published_date": "2025-03-02 12:51:38 UTC",
    "updated_date": "2025-03-02 12:51:38 UTC"
  },
  {
    "arxiv_id": "2503.00871v1",
    "title": "CyberCScope: Mining Skewed Tensor Streams and Online Anomaly Detection in Cybersecurity Systems",
    "authors": [
      "Kota Nakamura",
      "Koki Kawabata",
      "Shungo Tanaka",
      "Yasuko Matsubara",
      "Yasushi Sakurai"
    ],
    "abstract": "Cybersecurity systems are continuously producing a huge number of\ntime-stamped events in the form of high-order tensors, such as {count; time,\nport, flow duration, packet size, . . . }, and so how can we detect\nanomalies/intrusions in real time? How can we identify multiple types of\nintrusions and capture their characteristic behaviors? The tensor data consists\nof categorical and continuous attributes and the data distributions of\ncontinuous attributes typically exhibit skew. These data properties require\nhandling skewed infinite and finite dimensional spaces simultaneously. In this\npaper, we propose a novel streaming method, namely CyberCScope. The method\neffectively decomposes incoming tensors into major trends while explicitly\ndistinguishing between categorical and skewed continuous attributes. To our\nknowledge, it is the first to compute hybrid skewed infinite and finite\ndimensional decomposition. Based on this decomposition, it streamingly finds\ndistinct time-evolving patterns, enabling the detection of multiple types of\nanomalies. Extensive experiments on large-scale real datasets demonstrate that\nCyberCScope detects various intrusions with higher accuracy than\nstate-of-the-art baselines while providing meaningful summaries for the\nintrusions that occur in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WWW 2025 short research paper",
    "pdf_url": "http://arxiv.org/pdf/2503.00871v1",
    "published_date": "2025-03-02 12:17:24 UTC",
    "updated_date": "2025-03-02 12:17:24 UTC"
  },
  {
    "arxiv_id": "2503.00870v2",
    "title": "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In Open Domains",
    "authors": [
      "Wonje Choi",
      "Jinwoo Park",
      "Sanghyun Ahn",
      "Daehee Lee",
      "Honguk Woo"
    ],
    "abstract": "We explore neuro-symbolic approaches to generalize actionable knowledge,\nenabling embodied agents to tackle complex tasks more effectively in\nopen-domain environments. A key challenge for embodied agents is the\ngeneralization of knowledge across diverse environments and situations, as\nlimited experiences often confine them to their prior knowledge. To address\nthis issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual\nlearner that emulates the hypothetico-deductive model by continually\nformulating and validating knowledge from limited experiences through the\ncombined use of Large Language Models (LLMs) and symbolic tools. Specifically,\nwe devise a contrastive generality improvement scheme within NeSyC, which\niteratively generates hypotheses using LLMs and conducts contrastive validation\nvia symbolic tools. This scheme reinforces the justification for admissible\nactions while minimizing the inference of inadmissible ones. Additionally, we\nincorporate a memory-based monitoring scheme that efficiently detects action\nerrors and triggers the knowledge refinement process across domains.\nExperiments conducted on diverse embodied task benchmarks-including ALFWorld,\nVirtualHome, Minecraft, RLBench, and a real-world robotic scenario-demonstrate\nthat NeSyC is highly effective in solving complex embodied tasks across a range\nof open-domain environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICLR 2025. Project site with code:\n  https://pjw971022.github.io/nesyc/",
    "pdf_url": "http://arxiv.org/pdf/2503.00870v2",
    "published_date": "2025-03-02 12:16:20 UTC",
    "updated_date": "2025-03-07 02:28:26 UTC"
  },
  {
    "arxiv_id": "2503.01926v1",
    "title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "authors": [
      "Keyu Duan",
      "Yiran Zhao",
      "Zhili Feng",
      "Jinjie Ni",
      "Tianyu Pang",
      "Qian Liu",
      "Tianle Cai",
      "Longxu Dou",
      "Kenji Kawaguchi",
      "Anirudh Goyal",
      "J. Zico Kolter",
      "Michael Qizhe Shieh"
    ],
    "abstract": "Large Language Models (LLMs) have been observed to process non-human-readable\ntext sequences, such as jailbreak prompts, often viewed as a bug for aligned\nLLMs. In this work, we present a systematic investigation challenging this\nperception, demonstrating that unnatural languages - strings that appear\nincomprehensible to humans but maintain semantic meanings for LLMs - contain\nlatent features usable by models. Notably, unnatural languages possess latent\nfeatures that can be generalized across different models and tasks during\ninference. Furthermore, models fine-tuned on unnatural versions of instruction\ndatasets perform on-par with those trained on natural language, achieving 49.71\nwin rates in Length-controlled AlpacaEval 2.0 in average across various base\nmodels. In addition, through comprehensive analysis, we demonstrate that LLMs\nprocess unnatural languages by filtering noise and inferring contextual meaning\nfrom filtered words.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01926v1",
    "published_date": "2025-03-02 12:10:17 UTC",
    "updated_date": "2025-03-02 12:10:17 UTC"
  },
  {
    "arxiv_id": "2503.01924v3",
    "title": "TAET: Two-Stage Adversarial Equalization Training on Long-Tailed Distributions",
    "authors": [
      "Wang YuHang",
      "Junkang Guo",
      "Aolei Liu",
      "Kaihao Wang",
      "Zaitong Wu",
      "Zhenyu Liu",
      "Wenfei Yin",
      "Jian Liu"
    ],
    "abstract": "Adversarial robustness is a critical challenge in deploying deep neural\nnetworks for real-world applications. While adversarial training is a widely\nrecognized defense strategy, most existing studies focus on balanced datasets,\noverlooking the prevalence of long-tailed distributions in real-world data,\nwhich significantly complicates robustness. This paper provides a comprehensive\nanalysis of adversarial training under long-tailed distributions and identifies\nlimitations in the current state-of-the-art method, AT-BSL, in achieving robust\nperformance under such conditions. To address these challenges, we propose a\nnovel training framework, TAET, which integrates an initial stabilization phase\nfollowed by a stratified equalization adversarial training phase. Additionally,\nprior work on long-tailed robustness has largely ignored the crucial evaluation\nmetric of balanced accuracy. To bridge this gap, we introduce the concept of\nbalanced robustness, a comprehensive metric tailored for assessing robustness\nunder long-tailed distributions. Extensive experiments demonstrate that our\nmethod surpasses existing advanced defenses, achieving significant improvements\nin both memory and computational efficiency. This work represents a substantial\nadvancement in addressing robustness challenges in real-world applications. Our\ncode is available at:\nhttps://github.com/BuhuiOK/TAET-Two-Stage-Adversarial-Equalization-Training-on-Long-Tailed-Distributions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Text: 8 pages of main content, 5 pages of appendices have been\n  accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2503.01924v3",
    "published_date": "2025-03-02 12:07:00 UTC",
    "updated_date": "2025-03-21 09:56:29 UTC"
  },
  {
    "arxiv_id": "2503.00865v1",
    "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
    "authors": [
      "Yiran Zhao",
      "Chaoqun Liu",
      "Yue Deng",
      "Jiahao Ying",
      "Mahani Aljunied",
      "Zhaodonghui Li",
      "Lidong Bing",
      "Hou Pong Chan",
      "Yu Rong",
      "Deli Zhao",
      "Wenxuan Zhang"
    ],
    "abstract": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), yet open-source multilingual LLMs remain scarce, with existing models\noften limited in language coverage. Such models typically prioritize\nwell-resourced languages, while widely spoken but under-resourced languages are\noften overlooked. To address this disparity, we introduce $\\texttt{Babel}$, an\nopen multilingual LLM that covers the top 25 languages by number of speakers,\nsupports over 90% of the global population, and includes many languages\nneglected by other open multilingual LLMs. Unlike traditional continue\npretraining approaches, Babel expands its parameter count through a layer\nextension technique that elevates Babel's performance ceiling. We introduce two\nvariants: $\\texttt{Babel-9B}$, designed for efficient inference and\nfine-tuning, and $\\texttt{Babel-83B}$, which sets a new standard for open\nmultilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its\nsuperior performance compared to open LLMs of comparable size. In addition,\nusing open-source supervised fine-tuning datasets, Babel achieves remarkable\nperformance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat\nsetting a new standard for multilingual tasks, reaching the same level of\ncommercial models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00865v1",
    "published_date": "2025-03-02 11:53:55 UTC",
    "updated_date": "2025-03-02 11:53:55 UTC"
  },
  {
    "arxiv_id": "2503.10647v1",
    "title": "The Reliability of LLMs for Medical Diagnosis: An Examination of Consistency, Manipulation, and Contextual Awareness",
    "authors": [
      "Krishna Subedi"
    ],
    "abstract": "Universal healthcare access is critically needed, especially in\nresource-limited settings. Large Language Models (LLMs) offer promise for\ndemocratizing healthcare with advanced diagnostics, but their reliability\nrequires thorough evaluation, especially in trust-dependent environments. This\nstudy assesses LLMs' diagnostic reliability focusing on consistency,\nmanipulation resilience, and contextual integration, crucial for safe and\nethical use in universal healthcare.\n  We evaluated leading LLMs using 52 patient cases, expanded into variants with\ndemographic changes, symptom rewordings, and exam modifications, while keeping\ncore diagnoses constant. Manipulation susceptibility was tested by inserting\nmisleading narratives and irrelevant details. Contextual awareness was\nrvaluated by comparing diagnoses with and without patient history. We analyzed\ndiagnostic change rates and response patterns across manipulations.\n  LLMs showed perfect diagnostic consistency for identical data but significant\nmanipulation susceptibility. Gemini had a 40% diagnosis change rate and ChatGPT\n30% with irrelevant details. ChatGPT had a higher context influence rate (77.8%\nvs. Gemini's 55.6%), but both showed limited nuanced contextual integration,\nexhibiting anchoring bias by prioritizing salient data over context.\n  LLMs' vulnerability to manipulation and limited contextual awareness pose\nchallenges in clinical use. Unlike clinicians, they may overstate diagnostic\ncertainty without validation. Safeguards and domain-specific designs are\ncrucial for reliable healthcare applications. Broad clinical use without\noversight is premature and risky. LLMs can enhance diagnostics with responsible\nuse, but future research is needed to improve manipulation resistance and\ncontextual understanding for safe healthcare democratization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10647v1",
    "published_date": "2025-03-02 11:50:16 UTC",
    "updated_date": "2025-03-02 11:50:16 UTC"
  },
  {
    "arxiv_id": "2503.04796v1",
    "title": "Optimizing Multi-Hop Document Retrieval Through Intermediate Representations",
    "authors": [
      "Jiaen Lin",
      "Jingyu Liu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) encounters challenges when addressing\ncomplex queries, particularly multi-hop questions. While several methods tackle\nmulti-hop queries by iteratively generating internal queries and retrieving\nexternal documents, these approaches are computationally expensive. In this\npaper, we identify a three-stage information processing pattern in LLMs during\nlayer-by-layer reasoning, consisting of extraction, processing, and subsequent\nextraction steps. This observation suggests that the representations in\nintermediate layers contain richer information compared to those in other\nlayers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike\nprior methods that focus on generating new internal queries, L-RAG leverages\nintermediate representations from the middle layers, which capture next-hop\ninformation, to retrieve external knowledge. L-RAG achieves performance\ncomparable to multi-step approaches while maintaining inference overhead\nsimilar to that of standard RAG. Experimental results show that L-RAG\noutperforms existing RAG methods on open-domain multi-hop question-answering\ndatasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is\navailable in https://anonymous.4open.science/r/L-RAG-ADD5/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04796v1",
    "published_date": "2025-03-02 11:33:22 UTC",
    "updated_date": "2025-03-02 11:33:22 UTC"
  },
  {
    "arxiv_id": "2503.00853v1",
    "title": "MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain",
    "authors": [
      "Rui Yi Yong",
      "Samuel Picosson",
      "Arnold Wiliem"
    ],
    "abstract": "This work tackles 3D scene reconstruction for a video fly-over perspective\nproblem in the maritime domain, with a specific emphasis on geometrically and\nvisually sound reconstructions. This will allow for downstream tasks such as\nsegmentation, navigation, and localization. To our knowledge, there is no\ndataset available in this domain. As such, we propose a novel maritime 3D scene\nreconstruction benchmarking dataset, named as MTReD (Maritime Three-Dimensional\nReconstruction Dataset). The MTReD comprises 19 fly-over videos curated from\nthe Internet containing ships, islands, and coastlines. As the task is aimed\ntowards geometrical consistency and visual completeness, the dataset uses two\nmetrics: (1) Reprojection error; and (2) Perception based metrics. We find that\nexisting perception-based metrics, such as Learned Perceptual Image Patch\nSimilarity (LPIPS), do not appropriately measure the completeness of a\nreconstructed image. Thus, we propose a novel semantic similarity metric\nutilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity).\nWe perform initial evaluation on two baselines: (1) Structured from Motion\n(SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We find\nthat the reconstructed scenes by MASt3R have higher reprojection errors, but\nsuperior perception based metric scores. To this end, some pre-processing\nmethods are explored, and we find a pre-processing method which improves both\nthe reprojection error and perception-based score. We envisage our proposed\nMTReD to stimulate further research in these directions. The dataset and all\nthe code will be made available in https://github.com/RuiYiYong/MTReD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WACV Workshop 2025 - 3rd Workshop on Maritime Computer Vision\n  (MaCVI2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.00853v1",
    "published_date": "2025-03-02 11:10:34 UTC",
    "updated_date": "2025-03-02 11:10:34 UTC"
  },
  {
    "arxiv_id": "2503.00845v1",
    "title": "Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners",
    "authors": [
      "Miao Peng",
      "Nuo Chen",
      "Zongrui Suo",
      "Jia Li"
    ],
    "abstract": "Despite significant advancements in Large Language Models (LLMs), developing\nadvanced reasoning capabilities in LLMs remains a key challenge. Process Reward\nModels (PRMs) have demonstrated exceptional promise in enhancing reasoning by\nproviding step-wise feedback, particularly in the context of mathematical\nreasoning. However, their application to broader reasoning domains remains\nunderstudied, largely due to the high costs associated with manually creating\nstep-level supervision. In this work, we explore the potential of PRMs in graph\nreasoning problems - a domain that demands sophisticated multi-step reasoning\nand offers opportunities for automated step-level data generation using\nestablished graph algorithms. We introduce GraphSILO, the largest dataset for\ngraph reasoning problems with fine-grained step-wise labels, built using\nautomated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to\ngenerate detailed reasoning steps with step-wise labels. Building upon this\ndataset, we train GraphPRM, the first PRM designed for graph reasoning\nproblems, and evaluate its effectiveness in two key settings: inference-time\nscaling and reinforcement learning via Direct Preference Optimization (DPO).\nExperimental results show that GraphPRM significantly improves LLM performance\nacross 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and\ndemonstrating transferability to new graph reasoning datasets and new reasoning\ndomains like mathematical problem-solving. Notably, GraphPRM enhances LLM\nperformance on GSM8K and Math500, underscoring the cross-domain applicability\nof graph-based reasoning rewards. Our findings highlight the potential of PRMs\nin advancing reasoning across diverse domains, paving the way for more\nversatile and effective LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00845v1",
    "published_date": "2025-03-02 10:39:40 UTC",
    "updated_date": "2025-03-02 10:39:40 UTC"
  },
  {
    "arxiv_id": "2503.00841v1",
    "title": "A Law Reasoning Benchmark for LLM with Tree-Organized Structures including Factum Probandum, Evidence and Experiences",
    "authors": [
      "Jiaxin Shen",
      "Jinan Xu",
      "Huiqi Hu",
      "Luyi Lin",
      "Fei Zheng",
      "Guoyang Ma",
      "Fandong Meng",
      "Jie Zhou",
      "Wenjuan Han"
    ],
    "abstract": "While progress has been made in legal applications, law reasoning, crucial\nfor fair adjudication, remains unexplored. We propose a transparent law\nreasoning schema enriched with hierarchical factum probandum, evidence, and\nimplicit experience, enabling public scrutiny and preventing bias. Inspired by\nthis schema, we introduce the challenging task, which takes a textual case\ndescription and outputs a hierarchical structure justifying the final decision.\nWe also create the first crowd-sourced dataset for this task, enabling\ncomprehensive evaluation. Simultaneously, we propose an agent framework that\nemploys a comprehensive suite of legal analysis tools to address the challenge\ntask. This benchmark paves the way for transparent and accountable AI-assisted\nlaw reasoning in the ``Intelligent Court''.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.00841v1",
    "published_date": "2025-03-02 10:26:54 UTC",
    "updated_date": "2025-03-02 10:26:54 UTC"
  },
  {
    "arxiv_id": "2503.00821v1",
    "title": "AI Agents for Ground-Based Gamma Astronomy",
    "authors": [
      "D. Kostunin",
      "V. Sotnikov",
      "S. Golovachev",
      "A. Strube"
    ],
    "abstract": "Next-generation instruments for ground-based gamma-ray astronomy are marked\nby a substantial increase in complexity, featuring dozens of telescopes. This\nleap in scale introduces significant challenges in managing system operations\nand offline data analysis. Methods, which depend on advanced personnel training\nand sophisticated software, become increasingly strained as system complexity\ngrows, making it more challenging to effectively support users in such a\nmultifaceted environment. To address these challenges, we propose the\ndevelopment of AI agents based on instruction-finetuned large language models\n(LLMs). These agents align with specific documentation and codebases,\nunderstand the environmental context, operate with external APIs, and\ncommunicate with humans in natural language. Leveraging the advanced\ncapabilities of modern LLMs, which can process and retain vast amounts of\ninformation, these AI agents offer a transformative approach to system\nmanagement and data analysis by automating complex tasks and providing\nintelligent assistance. We present two prototypes that integrate with the\nCherenkov Telescope Array Observatory pipelines for operations and offline data\nanalysis. The first prototype automates data model implementation and\nmaintenance for the Configuration Database of the Array Control and Data\nAcquisition (ACADA). The second prototype is an open-access code generation\napplication tailored for data analysis based on the Gammapy framework.",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "proceedings of ADASS2025 (submitted), original talk and demos:\n  https://pretalx.com/adass2024/talk/TRFZKU/",
    "pdf_url": "http://arxiv.org/pdf/2503.00821v1",
    "published_date": "2025-03-02 09:55:54 UTC",
    "updated_date": "2025-03-02 09:55:54 UTC"
  },
  {
    "arxiv_id": "2503.00804v1",
    "title": "DELST: Dual Entailment Learning for Hyperbolic Image-Gene Pretraining in Spatial Transcriptomics",
    "authors": [
      "Xulin Chen",
      "Junzhou Huang"
    ],
    "abstract": "Spatial transcriptomics (ST) maps gene expression within tissue at individual\nspots, making it a valuable resource for multimodal representation learning.\nAdditionally, ST inherently contains rich hierarchical information both across\nand within modalities. For instance, different spots exhibit varying numbers of\nnonzero gene expressions, corresponding to different levels of cellular\nactivity and semantic hierarchies. However, existing methods rely on\ncontrastive alignment of image-gene pairs, failing to accurately capture the\nintricate hierarchical relationships in ST data. Here, we propose DELST, the\nfirst framework to embed hyperbolic representations while modeling hierarchy\nfor image-gene pretraining at two levels: (1) Cross-modal entailment learning,\nwhich establishes an order relationship between genes and images to enhance\nimage representation generalization; (2) Intra-modal entailment learning, which\nencodes gene expression patterns as hierarchical relationships, guiding\nhierarchical learning across different samples at a global scale and\nintegrating biological insights into single-modal representations. Extensive\nexperiments on ST benchmarks annotated by pathologists demonstrate the\neffectiveness of our framework, achieving improved predictive performance\ncompared to existing methods. Our code and models are available at:\nhttps://github.com/XulinChen/DELST.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00804v1",
    "published_date": "2025-03-02 09:00:09 UTC",
    "updated_date": "2025-03-02 09:00:09 UTC"
  },
  {
    "arxiv_id": "2503.00795v1",
    "title": "Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead",
    "authors": [
      "Yiran Cheng",
      "Hong Jin Kang",
      "Lwin Khin Shar",
      "Chaopeng Dong",
      "Zhiqiang Shi",
      "Shichao Lv",
      "Limin Sun"
    ],
    "abstract": "Fuzz testing is a crucial component of software security assessment, yet its\neffectiveness heavily relies on valid fuzz drivers and diverse seed inputs.\nRecent advancements in Large Language Models (LLMs) offer transformative\npotential for automating fuzz testing (LLM4Fuzz), particularly in generating\ndrivers and seeds. However, current LLM4Fuzz solutions face critical\nreliability challenges, including low driver validity rates and seed quality\ntrade-offs, hindering their practical adoption.\n  This paper aims to examine the reliability bottlenecks of LLM-driven fuzzing\nand explores potential research directions to address these limitations. It\nbegins with an overview of the current development of LLM4SE and emphasizes the\nnecessity for developing reliable LLM4Fuzz solutions. Following this, the paper\nenvisions a vision where reliable LLM4Fuzz transforms the landscape of software\ntesting and security for industry, software development practitioners, and\neconomic accessibility. It then outlines a road ahead for future research,\nidentifying key challenges and offering specific suggestions for the\nresearchers to consider. This work strives to spark innovation in the field,\npositioning reliable LLM4Fuzz as a fundamental component of modern software\ntesting.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00795v1",
    "published_date": "2025-03-02 08:46:39 UTC",
    "updated_date": "2025-03-02 08:46:39 UTC"
  },
  {
    "arxiv_id": "2503.00793v1",
    "title": "Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning",
    "authors": [
      "Ukcheol Shin",
      "Kyunghyun Lee",
      "Jean Oh"
    ],
    "abstract": "Deploying depth estimation networks in the real world requires high-level\nrobustness against various adverse conditions to ensure safe and reliable\nautonomy. For this purpose, many autonomous vehicles employ multi-modal sensor\nsystems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar.\nThey mainly adopt two strategies to use multiple sensors: modality-wise and\nmulti-modal fused inference. The former method is flexible but\nmemory-inefficient, unreliable, and vulnerable. Multi-modal fusion can provide\nhigh-level reliability, yet it needs a specialized architecture. In this paper,\nwe propose an effective solution, named align-and-fuse strategy, for the depth\nestimation from multi-spectral images. In the align stage, we align embedding\nspaces between multiple spectrum bands to learn shareable representation across\nmulti-spectral images by minimizing contrastive loss of global and spatially\naligned local features with geometry cue. After that, in the fuse stage, we\ntrain an attachable feature fusion module that can selectively aggregate the\nmulti-spectral features for reliable and robust prediction results. Based on\nthe proposed method, a single-depth network can achieve both spectral-invariant\nand multi-spectral fused depth estimation while preserving reliability, memory\nefficiency, and flexibility.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICRA 2025, Github link:\n  https://github.com/UkcheolShin/BridgeMultiSpectralDepth",
    "pdf_url": "http://arxiv.org/pdf/2503.00793v1",
    "published_date": "2025-03-02 08:45:58 UTC",
    "updated_date": "2025-03-02 08:45:58 UTC"
  },
  {
    "arxiv_id": "2503.00788v1",
    "title": "Taming Infinity one Chunk at a Time: Concisely Represented Strategies in One-Counter MDPs",
    "authors": [
      "Michal Ajdarów",
      "James C. A. Main",
      "Petr Novotný",
      "Mickael Randour"
    ],
    "abstract": "Markov decision processes (MDPs) are a canonical model to reason about\ndecision making within a stochastic environment. We study a fundamental class\nof infinite MDPs: one-counter MDPs (OC-MDPs). They extend finite MDPs via an\nassociated counter taking natural values, thus inducing an infinite MDP over\nthe set of configurations (current state and counter value). We consider two\ncharacteristic objectives: reaching a target state (state-reachability), and\nreaching a target state with counter value zero (selective termination). The\nsynthesis problem for the latter is not known to be decidable and connected to\nmajor open problems in number theory. Furthermore, even seemingly simple\nstrategies (e.g., memoryless ones) in OC-MDPs might be impossible to build in\npractice (due to the underlying infinite configuration space): we need finite,\nand preferably small, representations.\n  To overcome these obstacles, we introduce two natural classes of concisely\nrepresented strategies based on a (possibly infinite) partition of counter\nvalues in intervals. For both classes, and both objectives, we study the\nverification problem (does a given strategy ensure a high enough probability\nfor the objective?), and two synthesis problems (does there exist such a\nstrategy?): one where the interval partition is fixed as input, and one where\nit is only parameterized. We develop a generic approach based on a compression\nof the induced infinite MDP that yields decidability in all cases, with all\ncomplexities within PSPACE.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.FL",
      "cs.LO",
      "math.PR"
    ],
    "primary_category": "cs.GT",
    "comment": "55 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.00788v1",
    "published_date": "2025-03-02 08:32:17 UTC",
    "updated_date": "2025-03-02 08:32:17 UTC"
  },
  {
    "arxiv_id": "2503.00786v1",
    "title": "Graph Attention Networks Unleashed: A Fast and Explainable Vulnerability Assessment Framework for Microgrids",
    "authors": [
      "Wei Liu",
      "Tao Zhang",
      "Chenhui Lin",
      "Kaiwen Li",
      "Rui Wang"
    ],
    "abstract": "Independent microgrids are crucial for supplying electricity by combining\ndistributed energy resources and loads in scenarios like isolated islands and\nfield combat. Fast and accurate assessments of microgrid vulnerability against\nintentional attacks or natural disasters are essential for effective risk\nprevention and design optimization. However, conventional Monte Carlo\nsimulation (MCS) methods are computationally expensive and time-consuming,\nwhile existing machine learning-based approaches often lack accuracy and\nexplainability. To address these challenges, this study proposes a fast and\nexplainable vulnerability assessment framework that integrates MCS with a graph\nattention network enhanced by self-attention pooling (GAT-S). MCS generates\ntraining data, while the GAT-S model learns the structural and electrical\ncharacteristics of the microgrid and further assesses its vulnerability\nintelligently. The GAT-S improves explainability and computational efficiency\nby dynamically assigning attention weights to critical nodes. Comprehensive\nexperimental evaluations across various microgrid configurations demonstrate\nthat the proposed framework provides accurate vulnerability assessments,\nachieving a mean squared error as low as 0.001, real-time responsiveness within\n1 second, and delivering explainable results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00786v1",
    "published_date": "2025-03-02 08:31:27 UTC",
    "updated_date": "2025-03-02 08:31:27 UTC"
  },
  {
    "arxiv_id": "2503.00785v1",
    "title": "FLOAT Drone: A Fully-actuated Coaxial Aerial Robot for Close-Proximity Operations",
    "authors": [
      "Junxiao Lin",
      "Shuhang Ji",
      "Yuze Wu",
      "Tianyue Wu",
      "Zhichao Han",
      "Fei Gao"
    ],
    "abstract": "How to endow aerial robots with the ability to operate in close proximity\nremains an open problem. The core challenges lie in the propulsion system's\ndual-task requirement: generating manipulation forces while simultaneously\ncounteracting gravity. These competing demands create dynamic coupling effects\nduring physical interactions. Furthermore, rotor-induced airflow disturbances\ncritically undermine operational reliability. Although fully-actuated unmanned\naerial vehicles (UAVs) alleviate dynamic coupling effects via\nsix-degree-of-freedom (6-DoF) force-torque decoupling, existing implementations\nfail to address the aerodynamic interference between drones and environments.\nThey also suffer from oversized designs, which compromise maneuverability and\nlimit their applications in various operational scenarios. To address these\nlimitations, we present FLOAT Drone (FuLly-actuated cOaxial Aerial roboT), a\nnovel fully-actuated UAV featuring two key structural innovations. By\nintegrating control surfaces into fully-actuated systems for the first time, we\nsignificantly suppress lateral airflow disturbances during operations.\nFurthermore, a coaxial dual-rotor configuration enables a compact size while\nmaintaining high hovering efficiency. Through dynamic modeling, we have\ndeveloped hierarchical position and attitude controllers that support both\nfully-actuated and underactuated modes. Experimental validation through\ncomprehensive real-world experiments confirms the system's functional\ncapabilities in close-proximity operations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.00785v1",
    "published_date": "2025-03-02 08:30:30 UTC",
    "updated_date": "2025-03-02 08:30:30 UTC"
  },
  {
    "arxiv_id": "2503.00781v1",
    "title": "Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks",
    "authors": [
      "Umar Ali Khan",
      "Ekram Khan",
      "Fiza Khan",
      "Athar Ali Moinuddin"
    ],
    "abstract": "Large Language Models (LLMs) have proven immensely beneficial in education by\ncapturing vast amounts of literature-based information, allowing them to\ngenerate context without relying on external sources. In this paper, we propose\na generative AI-powered GATE question-answering framework (GATE stands for\nGraduate Aptitude Test in Engineering) that leverages LLMs to explain GATE\nsolutions and support students in their exam preparation. We conducted\nextensive benchmarking to select the optimal embedding model and LLM,\nevaluating our framework based on criteria such as latency, faithfulness, and\nrelevance, with additional validation through human evaluation. Our chatbot\nintegrates state-of-the-art embedding models and LLMs to deliver accurate,\ncontext-aware responses. Through rigorous experimentation, we identified\nconfigurations that balance performance and computational efficiency, ensuring\na reliable chatbot to serve students' needs. Additionally, we discuss the\nchallenges faced in data processing and modeling and implemented solutions. Our\nwork explores the application of Retrieval-Augmented Generation (RAG) for GATE\nQ/A explanation tasks, and our findings demonstrate significant improvements in\nretrieval accuracy and response quality. This research offers practical\ninsights for developing effective AI-driven educational tools while\nhighlighting areas for future enhancement in usability and scalability.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00781v1",
    "published_date": "2025-03-02 08:11:07 UTC",
    "updated_date": "2025-03-02 08:11:07 UTC"
  },
  {
    "arxiv_id": "2503.00780v1",
    "title": "Enhanced Multi-Class Classification of Gastrointestinal Endoscopic Images with Interpretable Deep Learning Model",
    "authors": [
      "Astitva Kamble",
      "Vani Bandodkar",
      "Saakshi Dharmadhikary",
      "Veena Anand",
      "Pradyut Kumar Sanki",
      "Mei X. Wu",
      "Biswabandhu Jana"
    ],
    "abstract": "Endoscopy serves as an essential procedure for evaluating the\ngastrointestinal (GI) tract and plays a pivotal role in identifying GI-related\ndisorders. Recent advancements in deep learning have demonstrated substantial\nprogress in detecting abnormalities through intricate models and data\naugmentation methods.This research introduces a novel approach to enhance\nclassification accuracy using 8,000 labeled endoscopic images from the Kvasir\ndataset, categorized into eight distinct classes. Leveraging EfficientNetB3 as\nthe backbone, the proposed architecture eliminates reliance on data\naugmentation while preserving moderate model complexity. The model achieves a\ntest accuracy of 94.25%, alongside precision and recall of 94.29% and 94.24%\nrespectively. Furthermore, Local Interpretable Model-agnostic Explanation\n(LIME) saliency maps are employed to enhance interpretability by defining\ncritical regions in the images that influenced model predictions. Overall, this\nwork highlights the importance of AI in advancing medical imaging by combining\nhigh classification accuracy with interpretability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00780v1",
    "published_date": "2025-03-02 08:07:50 UTC",
    "updated_date": "2025-03-02 08:07:50 UTC"
  },
  {
    "arxiv_id": "2503.04795v1",
    "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
    "authors": [
      "Dinesh Srivasthav P",
      "Bala Mallikarjunarao Garlapati"
    ],
    "abstract": "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04795v1",
    "published_date": "2025-03-02 07:58:08 UTC",
    "updated_date": "2025-03-02 07:58:08 UTC"
  },
  {
    "arxiv_id": "2503.00767v1",
    "title": "LLMs are everywhere: Ubiquitous Utilization of AI Models through Air Computing",
    "authors": [
      "Baris Yamansavascilar",
      "Atay Ozgovde",
      "Cem Ersoy"
    ],
    "abstract": "We are witnessing a new era where problem-solving and cognitive tasks are\nbeing increasingly delegated to Large Language Models (LLMs) across diverse\ndomains, ranging from code generation to holiday planning. This trend also\ncreates a demand for the ubiquitous execution of LLM-powered applications in a\nwide variety of environments in which traditional terrestrial 2D networking\ninfrastructures may prove insufficient. A promising solution in this context is\nto extend edge computing into a 3D setting to include aerial platforms\norganized in multiple layers, a paradigm we refer to as air computing, to\naugment local devices for running LLM and Generative AI (GenAI) applications.\nThis approach alleviates the strain on existing infrastructure while enhancing\nservice efficiency by offloading computational tasks to the corresponding air\nunits such as UAVs. Furthermore, the coordinated deployment of various air\nunits can significantly improve the Quality of Experience (QoE) by ensuring\nseamless, adaptive, and resilient task execution. In this study, we investigate\nthe synergy between LLM-based applications and air computing, exploring their\npotential across various use cases. Additionally, we present a disaster\nresponse case study demonstrating how the collaborative utilization of LLMs and\nair computing can significantly improve outcomes in critical situations.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.00767v1",
    "published_date": "2025-03-02 07:24:34 UTC",
    "updated_date": "2025-03-02 07:24:34 UTC"
  },
  {
    "arxiv_id": "2503.00762v1",
    "title": "MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks",
    "authors": [
      "Fangming Shi",
      "Jinzhen Liu",
      "Xiangqian Meng",
      "Yapeng Zhou",
      "Hui Xiong"
    ],
    "abstract": "This paper presents a multi-resolution reconstruction method for Electrical\nImpedance Tomography (EIT), referred to as MR-EIT, which is capable of\noperating in both supervised and unsupervised learning modes. MR-EIT integrates\nan ordered feature extraction module and an unordered coordinate feature\nexpression module. The former achieves the mapping from voltage to\ntwo-dimensional conductivity features through pre-training, while the latter\nrealizes multi-resolution reconstruction independent of the order and size of\nthe input sequence by utilizing symmetric functions and local feature\nextraction mechanisms. In the data-driven mode, MR-EIT reconstructs\nhigh-resolution images from low-resolution data of finite element meshes\nthrough two stages of pre-training and joint training, and demonstrates\nexcellent performance in simulation experiments. In the unsupervised learning\nmode, MR-EIT does not require pre-training data and performs iterative\noptimization solely based on measured voltages to rapidly achieve image\nreconstruction from low to high resolution. It shows robustness to noise and\nefficient super-resolution reconstruction capabilities in both simulation and\nreal water tank experiments. Experimental results indicate that MR-EIT\noutperforms the comparison methods in terms of Structural Similarity (SSIM) and\nRelative Image Error (RIE), especially in the unsupervised learning mode, where\nit can significantly reduce the number of iterations and improve image\nreconstruction quality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00762v1",
    "published_date": "2025-03-02 07:06:42 UTC",
    "updated_date": "2025-03-02 07:06:42 UTC"
  },
  {
    "arxiv_id": "2503.01923v1",
    "title": "Output Length Effect on DeepSeek-R1's Safety in Forced Thinking",
    "authors": [
      "Xuying Li",
      "Zhuo Li",
      "Yuji Kosuga",
      "Victor Bian"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities,\nbut their safety under adversarial conditions remains a challenge. This study\nexamines the impact of output length on the robustness of DeepSeek-R1,\nparticularly in Forced Thinking scenarios. We analyze responses across various\nadversarial prompts and find that while longer outputs can improve safety\nthrough self-correction, certain attack types exploit extended generations. Our\nfindings suggest that output length should be dynamically controlled to balance\nreasoning effectiveness and security. We propose reinforcement learning-based\npolicy adjustments and adaptive token length regulation to enhance LLM safety.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01923v1",
    "published_date": "2025-03-02 06:29:22 UTC",
    "updated_date": "2025-03-02 06:29:22 UTC"
  },
  {
    "arxiv_id": "2504.06271v1",
    "title": "ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources",
    "authors": [
      "Yikuan Xia",
      "Jiazun Chen",
      "Yirui Zhan",
      "Suifeng Zhao",
      "Weipeng Jiang",
      "Chaorui Zhang",
      "Wei Han",
      "Bo Bai",
      "Jun Gao"
    ],
    "abstract": "Large language models (LLMs) excel in question-answering (QA) tasks, and\nretrieval-augmented generation (RAG) enhances their precision by incorporating\nexternal evidence from diverse sources like web pages, databases, and knowledge\ngraphs. However, current RAG methods rely on agent-specific strategies for\nindividual data sources, posing challenges low-resource or black-box\nenvironments and complicates operations when evidence is fragmented across\nsources. To address these limitations, we propose ER-RAG, a framework that\nunifies evidence integration across heterogeneous data sources using the\nEntity-Relationship (ER) model. ER-RAG standardizes entity retrieval and\nrelationship querying through ER-based APIs with GET and JOIN operations. It\nemploys a two-stage generation process: first, a preference optimization module\nselects optimal sources; second, another module constructs API chains based on\nsource schemas. This unified approach allows efficient fine-tuning and seamless\nintegration across diverse data sources. ER-RAG demonstrated its effectiveness\nby winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving\nperformance on par with commercial RAG pipelines using an 8B LLM backbone. It\noutperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval\nby 5.5X.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06271v1",
    "published_date": "2025-03-02 06:21:56 UTC",
    "updated_date": "2025-03-02 06:21:56 UTC"
  },
  {
    "arxiv_id": "2503.00753v1",
    "title": "Rethinking Light Decoder-based Solvers for Vehicle Routing Problems",
    "authors": [
      "Ziwei Huang",
      "Jianan Zhou",
      "Zhiguang Cao",
      "Yixin Xu"
    ],
    "abstract": "Light decoder-based solvers have gained popularity for solving vehicle\nrouting problems (VRPs) due to their efficiency and ease of integration with\nreinforcement learning algorithms. However, they often struggle with\ngeneralization to larger problem instances or different VRP variants. This\npaper revisits light decoder-based approaches, analyzing the implications of\ntheir reliance on static embeddings and the inherent challenges that arise.\nSpecifically, we demonstrate that in the light decoder paradigm, the encoder is\nimplicitly tasked with capturing information for all potential decision\nscenarios during solution construction within a single set of embeddings,\nresulting in high information density. Furthermore, our empirical analysis\nreveals that the overly simplistic decoder struggles to effectively utilize\nthis dense information, particularly as task complexity increases, which limits\ngeneralization to out-of-distribution (OOD) settings. Building on these\ninsights, we show that enhancing the decoder capacity, with a simple addition\nof identity mapping and a feed-forward layer, can considerably alleviate the\ngeneralization issue. Experimentally, our method significantly enhances the OOD\ngeneralization of light decoder-based approaches on large-scale instances and\ncomplex VRP variants, narrowing the gap with the heavy decoder paradigm. Our\ncode is available at: https://github.com/ziweileonhuang/reld-nco.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.00753v1",
    "published_date": "2025-03-02 06:13:00 UTC",
    "updated_date": "2025-03-02 06:13:00 UTC"
  },
  {
    "arxiv_id": "2503.00751v1",
    "title": "RAPID: Efficient Retrieval-Augmented Long Text Generation with Writing Planning and Information Discovery",
    "authors": [
      "Hongchao Gu",
      "Dexun Li",
      "Kuicai Dong",
      "Hao Zhang",
      "Hang Lv",
      "Hao Wang",
      "Defu Lian",
      "Yong Liu",
      "Enhong Chen"
    ],
    "abstract": "Generating knowledge-intensive and comprehensive long texts, such as\nencyclopedia articles, remains significant challenges for Large Language\nModels. It requires not only the precise integration of facts but also the\nmaintenance of thematic coherence throughout the article. Existing methods,\nsuch as direct generation and multi-agent discussion, often struggle with\nissues like hallucinations, topic incoherence, and significant latency. To\naddress these challenges, we propose RAPID, an efficient retrieval-augmented\nlong text generation framework. RAPID consists of three main modules: (1)\nRetrieval-augmented preliminary outline generation to reduce hallucinations,\n(2) Attribute-constrained search for efficient information discovery, (3)\nPlan-guided article generation for enhanced coherence. Extensive experiments on\nour newly compiled benchmark dataset, FreshWiki-2024, demonstrate that RAPID\nsignificantly outperforms state-of-the-art methods across a wide range of\nevaluation metrics (e.g. long-text generation, outline quality, latency, etc).\nOur work provides a robust and efficient solution to the challenges of\nautomated long-text generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00751v1",
    "published_date": "2025-03-02 06:11:29 UTC",
    "updated_date": "2025-03-02 06:11:29 UTC"
  },
  {
    "arxiv_id": "2503.00750v1",
    "title": "Edge Prompt Tuning for Graph Neural Networks",
    "authors": [
      "Xingbo Fu",
      "Yinhan He",
      "Jundong Li"
    ],
    "abstract": "Pre-training powerful Graph Neural Networks (GNNs) with unlabeled graph data\nin a self-supervised manner has emerged as a prominent technique in recent\nyears. However, inevitable objective gaps often exist between pre-training and\ndownstream tasks. To bridge this gap, graph prompt tuning techniques design and\nlearn graph prompts by manipulating input graphs or reframing downstream tasks\nas pre-training tasks without fine-tuning the pre-trained GNN models. While\nrecent graph prompt tuning methods have proven effective in adapting\npre-trained GNN models for downstream tasks, they overlook the crucial role of\nedges in graph prompt design, which can significantly affect the quality of\ngraph representations for downstream tasks. In this study, we propose\nEdgePrompt, a simple yet effective graph prompt tuning method from the\nperspective of edges. Unlike previous studies that design prompt vectors on\nnode features, EdgePrompt manipulates input graphs by learning additional\nprompt vectors for edges and incorporates the edge prompts through message\npassing in the pre-trained GNN models to better embed graph structural\ninformation for downstream tasks. Our method is compatible with prevalent GNN\narchitectures pre-trained under various pre-training strategies and is\nuniversal for different downstream tasks. We provide comprehensive theoretical\nanalyses of our method regarding its capability of handling node classification\nand graph classification as downstream tasks. Extensive experiments on ten\ngraph datasets under four pre-training strategies demonstrate the superiority\nof our proposed method against six baselines. Our code is available at\nhttps://github.com/xbfu/EdgePrompt.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.00750v1",
    "published_date": "2025-03-02 06:07:54 UTC",
    "updated_date": "2025-03-02 06:07:54 UTC"
  },
  {
    "arxiv_id": "2503.00744v1",
    "title": "Confounder-Aware Medical Data Selection for Fine-Tuning Pretrained Vision Models",
    "authors": [
      "Anyang Ji",
      "Qingbo Kang",
      "Wei Xu",
      "Changfan Wang",
      "Kang Li",
      "Qicheng Lao"
    ],
    "abstract": "The emergence of large-scale pre-trained vision foundation models has greatly\nadvanced the medical imaging field through the pre-training and fine-tuning\nparadigm. However, selecting appropriate medical data for downstream\nfine-tuning remains a significant challenge considering its annotation cost,\nprivacy concerns, and the detrimental effects of confounding variables. In this\nwork, we present a confounder-aware medical data selection approach for medical\ndataset curation aiming to select minimal representative data by strategically\nmitigating the undesirable impact of confounding variables while preserving the\nnatural distribution of the dataset. Our approach first identifies confounding\nvariables within data and then develops a distance-based data selection\nstrategy for confounder-aware sampling with a constrained budget in the data\nsize. We validate the superiority of our approach through extensive experiments\nacross diverse medical imaging modalities, highlighting its effectiveness in\naddressing the substantial impact of confounding variables and enhancing the\nfine-tuning efficiency in the medical imaging domain, compared to other data\nselection approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.00744v1",
    "published_date": "2025-03-02 05:50:25 UTC",
    "updated_date": "2025-03-02 05:50:25 UTC"
  },
  {
    "arxiv_id": "2503.00735v3",
    "title": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition",
    "authors": [
      "Toby Simonds",
      "Akira Yoshiyama"
    ],
    "abstract": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00735v3",
    "published_date": "2025-03-02 05:16:43 UTC",
    "updated_date": "2025-03-05 11:50:24 UTC"
  },
  {
    "arxiv_id": "2503.00729v1",
    "title": "CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments",
    "authors": [
      "Mingcong Lei",
      "Ge Wang",
      "Yiming Zhao",
      "Zhixin Mai",
      "Qing Zhao",
      "Yao Guo",
      "Zhen Li",
      "Shuguang Cui",
      "Yatong Han",
      "Jinke Ren"
    ],
    "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities in the\nhierarchical decomposition of complex tasks through semantic reasoning.\nHowever, their application in embodied systems faces challenges in ensuring\nreliable execution of subtask sequences and achieving one-shot success in\nlong-term task completion. To address these limitations in dynamic\nenvironments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel\narchitecture incorporating four specialized open-source LLMs with functional\ndecoupling for closed-loop task management. The framework features two core\ninnovations: (1) Interactive task planner that dynamically generates executable\nsubtasks based on the environmental memory, and (2) Multimodal execution critic\nemploying an evaluation framework to conduct a probabilistic assessment of\naction feasibility, triggering hierarchical re-planning mechanisms when\nenvironmental perturbations exceed preset thresholds. To validate CLEA's\neffectiveness, we conduct experiments in a real environment with manipulable\nobjects, using two heterogeneous robots for object search, manipulation, and\nsearch-manipulation integration tasks. Across 12 task trials, CLEA outperforms\nthe baseline model, achieving a 67.3% improvement in success rate and a 52.8%\nincrease in task completion rate. These results demonstrate that CLEA\nsignificantly enhances the robustness of task planning and execution in dynamic\nenvironments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00729v1",
    "published_date": "2025-03-02 04:50:59 UTC",
    "updated_date": "2025-03-02 04:50:59 UTC"
  },
  {
    "arxiv_id": "2503.00727v1",
    "title": "From Understanding the World to Intervening in It: A Unified Multi-Scale Framework for Embodied Cognition",
    "authors": [
      "Maijunxian Wang"
    ],
    "abstract": "In this paper, we propose AUKAI, an Adaptive Unified Knowledge-Action\nIntelligence for embodied cognition that seamlessly integrates perception,\nmemory, and decision-making via multi-scale error feedback. Interpreting AUKAI\nas an embedded world model, our approach simultaneously predicts state\ntransitions and evaluates intervention utility. The framework is underpinned by\nrigorous theoretical analysis drawn from convergence theory, optimal control,\nand Bayesian inference, which collectively establish conditions for\nconvergence, stability, and near-optimal performance. Furthermore, we present a\nhybrid implementation that combines the strengths of neural networks with\nsymbolic reasoning modules, thereby enhancing interpretability and robustness.\nFinally, we demonstrate the potential of AUKAI through a detailed application\nin robotic navigation and obstacle avoidance, and we outline comprehensive\nexperimental plans to validate its effectiveness in both simulated and\nreal-world environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00727v1",
    "published_date": "2025-03-02 04:43:08 UTC",
    "updated_date": "2025-03-02 04:43:08 UTC"
  },
  {
    "arxiv_id": "2503.00726v1",
    "title": "Enhancing Monocular 3D Scene Completion with Diffusion Model",
    "authors": [
      "Changlin Song",
      "Jiaqi Wang",
      "Liyun Zhu",
      "He Weng"
    ],
    "abstract": "3D scene reconstruction is essential for applications in virtual reality,\nrobotics, and autonomous driving, enabling machines to understand and interact\nwith complex environments. Traditional 3D Gaussian Splatting techniques rely on\nimages captured from multiple viewpoints to achieve optimal performance, but\nthis dependence limits their use in scenarios where only a single image is\navailable. In this work, we introduce FlashDreamer, a novel approach for\nreconstructing a complete 3D scene from a single image, significantly reducing\nthe need for multi-view inputs. Our approach leverages a pre-trained\nvision-language model to generate descriptive prompts for the scene, guiding a\ndiffusion model to produce images from various perspectives, which are then\nfused to form a cohesive 3D reconstruction. Extensive experiments show that our\nmethod effectively and robustly expands single-image inputs into a\ncomprehensive 3D scene, extending monocular 3D reconstruction capabilities\nwithout further training. Our code is available\nhttps://github.com/CharlieSong1999/FlashDreamer/tree/main.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "All authors had equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2503.00726v1",
    "published_date": "2025-03-02 04:36:57 UTC",
    "updated_date": "2025-03-02 04:36:57 UTC"
  },
  {
    "arxiv_id": "2503.01921v2",
    "title": "NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT",
    "authors": [
      "Jiaying Hong",
      "Thanet Markchom",
      "Jianfei Xu",
      "Tong Wu",
      "Huizhi Liang"
    ],
    "abstract": "SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in\ncontent generated by various large language models (LLMs) across multiple\nlanguages. This task involves not only identifying the presence of\nhallucinations but also pinpointing their specific occurrences. To tackle this\nchallenge, this study introduces two methods: modified RefChecker and modified\nSelfCheckGPT. The modified RefChecker integrates prompt-based factual\nverification into References, structuring them as claim-based tests rather than\nsingle external knowledge sources. The modified SelfCheckGPT incorporates\nexternal knowledge to overcome its reliance on internal knowledge. In addition,\nboth methods' original prompt designs are enhanced to identify hallucinated\nwords within LLM-generated texts. Experimental results demonstrate the\neffectiveness of the approach, achieving a high ranking on the test dataset in\ndetecting hallucinations across various languages, with an average IoU of\n0.5310 and an average COR of 0.5669.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01921v2",
    "published_date": "2025-03-02 04:21:33 UTC",
    "updated_date": "2025-05-12 14:24:25 UTC"
  },
  {
    "arxiv_id": "2503.00717v1",
    "title": "LLMDR: LLM-Driven Deadlock Detection and Resolution in Multi-Agent Pathfinding",
    "authors": [
      "Seungbae Seo",
      "Junghwan Kim",
      "Minjeong Shin",
      "Bongwon Suh"
    ],
    "abstract": "Multi-Agent Pathfinding (MAPF) is a core challenge in multi-agent systems.\nExisting learning-based MAPF methods often struggle with scalability,\nparticularly when addressing complex scenarios that are prone to deadlocks. To\naddress these challenges, we introduce LLMDR (LLM-Driven Deadlock Detection and\nResolution), an approach designed to resolve deadlocks and improve the\nperformance of learnt MAPF models. LLMDR integrates the inference capabilities\nof large language models (LLMs) with learnt MAPF models and prioritized\nplanning, enabling it to detect deadlocks and provide customized resolution\nstrategies. We evaluate LLMDR on standard MAPF benchmark maps with varying\nagent numbers, measuring its performance when combined with several base\nmodels. The results demonstrate that LLMDR improves the performance of learnt\nMAPF models, particularly in deadlock-prone scenarios, with notable\nimprovements in success rates. These findings show the potential of integrating\nLLMs to improve the scalability of learning-based MAPF methods.\n  The source code for LLMDR is available at:\nhttps://github.com/ssbacc/llmdr-dhc",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00717v1",
    "published_date": "2025-03-02 03:49:15 UTC",
    "updated_date": "2025-03-02 03:49:15 UTC"
  },
  {
    "arxiv_id": "2503.00714v1",
    "title": "Speculative Ad-hoc Querying",
    "authors": [
      "Haoyu Li",
      "Srikanth Kandula",
      "Maria Angels de Luis Balaguer",
      "Aditya Akella",
      "Venkat Arun"
    ],
    "abstract": "Analyzing large datasets requires responsive query execution, but executing\nSQL queries on massive datasets can be slow. This paper explores whether query\nexecution can begin even before the user has finished typing, allowing results\nto appear almost instantly. We propose SpeQL, a system that leverages Large\nLanguage Models (LLMs) to predict likely queries based on the database schema,\nthe user's past queries, and their incomplete query. Since exact query\nprediction is infeasible, SpeQL speculates on partial queries in two ways: 1)\nit predicts the query structure to compile and plan queries in advance, and 2)\nit precomputes smaller temporary tables that are much smaller than the original\ndatabase, but are still predicted to contain all information necessary to\nanswer the user's final query. Additionally, SpeQL continuously displays\nresults for speculated queries and subqueries in real time, aiding exploratory\nanalysis. A utility/user study showed that SpeQL improved task completion time,\nand participants reported that its speculative display of results helped them\ndiscover patterns in the data more quickly. In the study, SpeQL improves user's\nquery latency by up to $289\\times$ and kept the overhead reasonable, at $\\$4$\nper hour.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00714v1",
    "published_date": "2025-03-02 03:44:31 UTC",
    "updated_date": "2025-03-02 03:44:31 UTC"
  },
  {
    "arxiv_id": "2503.00711v1",
    "title": "OpenECG: Benchmarking ECG Foundation Models with Public 1.2 Million Records",
    "authors": [
      "Zhijiang Wan",
      "Qianhao Yu",
      "Jia Mao",
      "Wenfeng Duan",
      "Cheng Ding"
    ],
    "abstract": "This study introduces OpenECG, a large-scale benchmark of 1.2 million 12-lead\nECG recordings from nine centers, to evaluate ECG foundation models (ECG-FMs)\ntrained on public datasets. We investigate three self-supervised learning\nmethods (SimCLR, BYOL, MAE) with ResNet-50 and Vision Transformer\narchitectures, assessing model generalization through leave-one-dataset-out\nexperiments and data scaling analysis. Results show that pre-training on\ndiverse datasets significantly improves generalization, with BYOL and MAE\noutperforming SimCLR, highlighting the efficacy of feature-consistency and\ngenerative learning over contrastive approaches. Data scaling experiments\nreveal that performance saturates at 60-70% of total data for BYOL and MAE,\nwhile SimCLR requires more data. These findings demonstrate that publicly\navailable ECG data can match or surpass proprietary datasets in training robust\nECG-FMs, paving the way for scalable, clinically meaningful AI-driven ECG\nanalysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00711v1",
    "published_date": "2025-03-02 03:26:14 UTC",
    "updated_date": "2025-03-02 03:26:14 UTC"
  },
  {
    "arxiv_id": "2503.00699v1",
    "title": "Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo",
    "authors": [
      "Hyunsu Kim",
      "Giung Nam",
      "Chulhee Yun",
      "Hongseok Yang",
      "Juho Lee"
    ],
    "abstract": "Bayesian Neural Networks (BNNs) provide a promising framework for modeling\npredictive uncertainty and enhancing out-of-distribution robustness (OOD) by\nestimating the posterior distribution of network parameters. Stochastic\nGradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods\nfor scalable posterior sampling in BNNs, achieving efficiency by combining\nstochastic gradient descent with second-order Langevin dynamics. However,\nSGMCMC often suffers from limited sample diversity in practice, which affects\nuncertainty estimation and model performance. We propose a simple yet effective\napproach to enhance sample diversity in SGMCMC without the need for tempering\nor running multiple chains. Our approach reparameterizes the neural network by\ndecomposing each of its weight matrices into a product of matrices, resulting\nin a sampling trajectory that better explores the target parameter space. This\napproach produces a more diverse set of samples, allowing faster mixing within\nthe same computational budget. Notably, our sampler achieves these improvements\nwithout increasing the inference cost compared to the standard SGMCMC.\nExtensive experiments on image classification tasks, including OOD robustness,\ndiversity, loss surface analyses, and a comparative study with Hamiltonian\nMonte Carlo, demonstrate the superiority of the proposed approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00699v1",
    "published_date": "2025-03-02 02:42:50 UTC",
    "updated_date": "2025-03-02 02:42:50 UTC"
  },
  {
    "arxiv_id": "2503.00697v1",
    "title": "CREATE-FFPE: Cross-Resolution Compensated and Multi-Frequency Enhanced FS-to-FFPE Stain Transfer for Intraoperative IHC Images",
    "authors": [
      "Yiyang Lin",
      "Danling Jiang",
      "Xinyu Liu",
      "Yun Miao",
      "Yixuan Yuan"
    ],
    "abstract": "In the immunohistochemical (IHC) analysis during surgery, frozen-section (FS)\nimages are used to determine the benignity or malignancy of the tumor. However,\nFS image faces problems such as image contamination and poor nuclear detail,\nwhich may disturb the pathologist's diagnosis. In contrast, formalin-fixed and\nparaffin-embedded (FFPE) image has a higher staining quality, but it requires\nquite a long time to prepare and thus is not feasible during surgery. To help\npathologists observe IHC images with high quality in surgery, this paper\nproposes a Cross-REsolution compensATed and multi-frequency Enhanced FS-to-FFPE\n(CREATE-FFPE) stain transfer framework, which is the first FS-to-FFPE method\nfor the intraoperative IHC images. To solve the slide contamination and poor\nnuclear detail mentioned above, we propose the cross-resolution compensation\nmodule (CRCM) and the wavelet detail guidance module (WDGM). Specifically, CRCM\ncompensates for information loss due to contamination by providing more tissue\ninformation across multiple resolutions, while WDGM produces the desirable\ndetails in a wavelet way, and the details can be used to guide the stain\ntransfer to be more precise. Experiments show our method can beat all the\ncompeting methods on our dataset. In addition, the FID has decreased by 44.4%,\nand KID*100 has decreased by 71.2% by adding the proposed CRCM and WDGM in\nablation studies, and the performance of a downstream microsatellite\ninstability prediction task with public dataset can be greatly improved by\nperforming our FS-to-FFPE stain transfer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00697v1",
    "published_date": "2025-03-02 02:38:11 UTC",
    "updated_date": "2025-03-02 02:38:11 UTC"
  },
  {
    "arxiv_id": "2503.00691v2",
    "title": "How Diversely Can Language Models Solve Problems? Exploring the Algorithmic Diversity of Model-Generated Code",
    "authors": [
      "Seonghyeon Lee",
      "Heejae Chon",
      "Joonwon Jang",
      "Dongha Lee",
      "Hwanjo Yu"
    ],
    "abstract": "Language models (LMs) have exhibited impressive abilities in generating code\nfrom natural language requirements. In this work, we highlight the diversity of\ncode generated by LMs as a critical criterion for evaluating their code\ngeneration capabilities. There is a lack of studies focused on assessing the\ndiversity of generated code, which overlooks its importance in code LMs.\nTherefore, we propose a systematic approach to evaluate code diversity,\nintroducing various metrics with inter-code similarity. Specifically, we\nintroduce code clustering methods that leverages LMs' capabilities in code\nunderstanding and reasoning, resulting in a set of metrics that represent the\nnumber of algorithms in model-generated solutions. We extensively investigate\nthe property of model-generated solutions by contrasting them with\nhuman-written ones and quantifying the impact of various factors on code\ndiversity: model size, temperature, instruction tuning, and problem complexity.\nOur analysis demonstrates that model-generated solutions exhibit low\nalgorithmic diversity, which was neglected by the research community. Moreover,\nwe explore methods to increase code diversity by combining solutions from\ndifferent models and increasing sampling temperatures. Our findings highlight\nthat code diversity can be enhanced with the help of heterogeneous models and\nsetting temperature beyond 1.0 that has not been fully explored due to the\nfunctional correctness degradation. To facilitate our research direction, we\npublicly share our code and datasets through open-source repositories.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00691v2",
    "published_date": "2025-03-02 02:04:58 UTC",
    "updated_date": "2025-03-07 05:38:47 UTC"
  },
  {
    "arxiv_id": "2503.05794v3",
    "title": "CBW: Towards Dataset Ownership Verification for Speaker Verification via Clustering-based Backdoor Watermarking",
    "authors": [
      "Yiming Li",
      "Kaiying Yan",
      "Shuo Shao",
      "Tongqing Zhai",
      "Shu-Tao Xia",
      "Zhan Qin",
      "Dacheng Tao"
    ],
    "abstract": "With the increasing adoption of deep learning in speaker verification,\nlarge-scale speech datasets have become valuable intellectual property. To\naudit and prevent the unauthorized usage of these valuable released datasets,\nespecially in commercial or open-source scenarios, we propose a novel dataset\nownership verification method. Our approach introduces a clustering-based\nbackdoor watermark (CBW), enabling dataset owners to determine whether a\nsuspicious third-party model has been trained on a protected dataset under a\nblack-box setting. The CBW method consists of two key stages: dataset\nwatermarking and ownership verification. During watermarking, we implant\nmultiple trigger patterns in the dataset to make similar samples (measured by\ntheir feature similarities) close to the same trigger while dissimilar samples\nare near different triggers. This ensures that any model trained on the\nwatermarked dataset exhibits specific misclassification behaviors when exposed\nto trigger-embedded inputs. To verify dataset ownership, we design a\nhypothesis-test-based framework that statistically evaluates whether a\nsuspicious model exhibits the expected backdoor behavior. We conduct extensive\nexperiments on benchmark datasets, verifying the effectiveness and robustness\nof our method against potential adaptive attacks. The code for reproducing main\nexperiments is available at https://github.com/Radiant0726/CBW",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "14 pages. The journal extension of our ICASSP'21 paper\n  (arXiv:2010.11607)",
    "pdf_url": "http://arxiv.org/pdf/2503.05794v3",
    "published_date": "2025-03-02 02:02:57 UTC",
    "updated_date": "2025-04-05 15:05:33 UTC"
  },
  {
    "arxiv_id": "2503.00686v1",
    "title": "GPIoT: Tailoring Small Language Models for IoT Program Synthesis and Development",
    "authors": [
      "Leming Shen",
      "Qiang Yang",
      "Xinyu Huang",
      "Zijing Ma",
      "Yuanqing Zheng"
    ],
    "abstract": "Code Large Language Models (LLMs) enhance software development efficiency by\nautomatically generating code and documentation in response to user\nrequirements. However, code LLMs cannot synthesize specialized programs when\ntasked with IoT applications that require domain knowledge. While\nRetrieval-Augmented Generation (RAG) offers a promising solution by fetching\nrelevant domain knowledge, it necessitates powerful cloud LLMs (e.g., GPT-4) to\nprocess user requirements and retrieved contents, which raises significant\nprivacy concerns. This approach also suffers from unstable networks and\nprohibitive LLM query costs. Moreover, it is challenging to ensure the\ncorrectness and relevance of the fetched contents. To address these issues, we\npropose GPIoT, a code generation system for IoT applications by fine-tuning\nlocally deployable Small Language Models (SLMs) on IoT-specialized datasets.\nSLMs have smaller model sizes, allowing efficient local deployment and\nexecution to mitigate privacy concerns and network uncertainty. Furthermore, by\nfine-tuning the SLMs with our IoT-specialized datasets, the SLMs' ability to\nsynthesize IoT-related programs can be substantially improved. To evaluate\nGPIoT's capability in synthesizing programs for IoT applications, we develop a\nbenchmark, IoTBench. Extensive experiments and user trials demonstrate the\neffectiveness of GPIoT in generating IoT-specialized code, outperforming\nstate-of-the-art code LLMs with an average task accuracy increment of 64.7% and\nsignificant improvements in user satisfaction.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00686v1",
    "published_date": "2025-03-02 01:55:40 UTC",
    "updated_date": "2025-03-02 01:55:40 UTC"
  },
  {
    "arxiv_id": "2503.00684v1",
    "title": "Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement Learning in Victim Tagging",
    "authors": [
      "Maria Ana Cardei",
      "Afsaneh Doryab"
    ],
    "abstract": "Mass casualty incidents (MCIs) are a growing concern, characterized by\ncomplexity and uncertainty that demand adaptive decision-making strategies. The\nvictim tagging step in the emergency medical response must be completed quickly\nand is crucial for providing information to guide subsequent time-constrained\nresponse actions. In this paper, we present a mathematical formulation of\nmulti-agent victim tagging to minimize the time it takes for responders to tag\nall victims. Five distributed heuristics are formulated and evaluated with\nsimulation experiments. The heuristics considered are on-the go, practical\nsolutions that represent varying levels of situational uncertainty in the form\nof global or local communication capabilities, showcasing practical\nconstraints. We further investigate the performance of a multi-agent\nreinforcement learning (MARL) strategy, factorized deep Q-network (FDQN), to\nminimize victim tagging time as compared to baseline heuristics. Extensive\nsimulations demonstrate that between the heuristics, methods with local\ncommunication are more efficient for adaptive victim tagging, specifically\nchoosing the nearest victim with the option to replan. Analyzing all\nexperiments, we find that our FDQN approach outperforms heuristics in\nsmaller-scale scenarios, while heuristics excel in more complex scenarios. Our\nexperiments contain diverse complexities that explore the upper limits of MARL\ncapabilities for real-world applications and reveal key insights.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "I.6.3; I.2.1; I.2.8; I.2.9; I.2.11; J.3; J.7"
    ],
    "primary_category": "cs.MA",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2503.00684v1",
    "published_date": "2025-03-02 01:32:09 UTC",
    "updated_date": "2025-03-02 01:32:09 UTC"
  },
  {
    "arxiv_id": "2503.00674v1",
    "title": "OrdRankBen: A Novel Ranking Benchmark for Ordinal Relevance in NLP",
    "authors": [
      "Yan Wang",
      "Lingfei Qian",
      "Xueqing Peng",
      "Jimin Huang",
      "Dongji Feng"
    ],
    "abstract": "The evaluation of ranking tasks remains a significant challenge in natural\nlanguage processing (NLP), particularly due to the lack of direct labels for\nresults in real-world scenarios. Benchmark datasets play a crucial role in\nproviding standardized testbeds that ensure fair comparisons, enhance\nreproducibility, and enable progress tracking, facilitating rigorous assessment\nand continuous improvement of ranking models. Existing NLP ranking benchmarks\ntypically use binary relevance labels or continuous relevance scores,\nneglecting ordinal relevance scores. However, binary labels oversimplify\nrelevance distinctions, while continuous scores lack a clear ordinal structure,\nmaking it challenging to capture nuanced ranking differences effectively. To\naddress these challenges, we introduce OrdRankBen, a novel benchmark designed\nto capture multi-granularity relevance distinctions. Unlike conventional\nbenchmarks, OrdRankBen incorporates structured ordinal labels, enabling more\nprecise ranking evaluations. Given the absence of suitable datasets for ordinal\nrelevance ranking in NLP, we constructed two datasets with distinct ordinal\nlabel distributions. We further evaluate various models for three model types,\nranking-based language models, general large language models, and\nranking-focused large language models on these datasets. Experimental results\nshow that ordinal relevance modeling provides a more precise evaluation of\nranking models, improving their ability to distinguish multi-granularity\ndifferences among ranked items-crucial for tasks that demand fine-grained\nrelevance differentiation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.00674v1",
    "published_date": "2025-03-02 00:28:55 UTC",
    "updated_date": "2025-03-02 00:28:55 UTC"
  }
]
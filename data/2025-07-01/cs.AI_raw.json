[
  {
    "arxiv_id": "2507.01241v1",
    "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW",
    "authors": [
      "Di Zhang",
      "Yihang Zhang"
    ],
    "abstract": "Stochastic gradient-based descent (SGD), have long been central to training large language models (LLMs). However, their effectiveness is increasingly being questioned, particularly in large-scale applications where empirical evidence suggests potential performance limitations. In response, this paper proposes a stochastic conjugate subgradient method together with adaptive sampling tailored specifically for training LLMs. The method not only achieves faster convergence per iteration but also demonstrates improved scalability compared to traditional SGD techniques. It leverages sample complexity analysis to adaptively choose the sample size, employs a stochastic conjugate subgradient approach to determine search directions and utilizing an AdamW-like algorithm to adaptively adjust step sizes. This approach preserves the key advantages of first-order methods while effectively addressing the nonconvexity and non-smoothness inherent in LLMs training. Additionally, we provide a detailed analysis of the advantage of the algorithm. Experimental results show that the proposed method not only maintains, but in many cases surpasses, the scalability of traditional SGD techniques, significantly enhancing both the speed and accuracy of the optimization process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01241v1",
    "published_date": "2025-07-01 23:30:15 UTC",
    "updated_date": "2025-07-01 23:30:15 UTC"
  },
  {
    "arxiv_id": "2507.01231v1",
    "title": "Rethinking the Illusion of Thinking",
    "authors": [
      "Iñaki Dellibarda Varela",
      "Pablo Romero-Sorozabal",
      "Eduardo Rocon",
      "Manuel Cebrian"
    ],
    "abstract": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of Thinking,\" prompting heated debate within the AI community. Critics seized upon the findings as conclusive evidence that Large Reasoning Models (LRMs) lack genuine reasoning capabilities, branding them as mere stochastic parrots. Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning the experimental setup as flawed and the conclusions overstated. We clarify this debate by replicating and refining two of the original study's most contentious benchmarks: Towers of Hanoi and River Crossing. By introducing incremental stepwise prompting and agentic collaborative dialogue, we show that previously reported failures solving the Towers of Hanoi were not purely result of output constraints, but also partly a result of cognition limitations: LRMs still stumble when complexity rises moderately (around 8 disks). Moreover, the River Crossing results initially heralded as catastrophic failures turn out to hinge upon testing unsolvable configurations. Once we limit tests strictly to solvable problems-LRMs effortlessly solve large instances involving over 100 agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs are stochastic, RL-tuned searchers in a discrete state space we barely understand. Real progress in symbolic, long-horizon reasoning demands mapping that terrain through fine-grained ablations like those introduced here.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.01231v1",
    "published_date": "2025-07-01 23:10:02 UTC",
    "updated_date": "2025-07-01 23:10:02 UTC"
  },
  {
    "arxiv_id": "2507.01225v2",
    "title": "Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration",
    "authors": [
      "Sunandita Patra",
      "Mehtab Pathan",
      "Mahmoud Mahfouz",
      "Parisa Zehtabi",
      "Wided Ouaja",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ],
    "abstract": "Organizations around the world schedule jobs (programs) regularly to perform various tasks dictated by their end users. With the major movement towards using a cloud computing infrastructure, our organization follows a hybrid approach with both cloud and on-prem servers. The objective of this work is to perform capacity planning, i.e., estimate resource requirements, and job scheduling for on-prem grid computing environments. A key contribution of our approach is handling uncertainty in both resource usage and duration of the jobs, a critical aspect in the finance industry where stochastic market conditions significantly influence job characteristics. For capacity planning and scheduling, we simultaneously balance two conflicting objectives: (a) minimize resource usage, and (b) provide high quality-of-service to the end users by completing jobs by their requested deadlines. We propose approximate approaches using deterministic estimators and pair sampling-based constraint programming. Our best approach (pair sampling-based) achieves much lower peak resource usage compared to manual scheduling without compromising on the quality-of-service.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "Please cite as: Sunandita Patra, Mehtab Pathan, Mahmoud Mahfouz, Parisa Zehtabi, Wided Ouaja, Daniele Magazzeni, and Manuela Veloso. \"Capacity planning and scheduling for jobs with uncertainty in resource usage and duration.\" The Journal of Supercomputing 80, no. 15 (2024): 22428-22461",
    "pdf_url": "https://arxiv.org/pdf/2507.01225v2",
    "published_date": "2025-07-01 22:56:08 UTC",
    "updated_date": "2025-07-21 18:56:31 UTC"
  },
  {
    "arxiv_id": "2507.02997v1",
    "title": "What to Do Next? Memorizing skills from Egocentric Instructional Video",
    "authors": [
      "Jing Bi",
      "Chenliang Xu"
    ],
    "abstract": "Learning to perform activities through demonstration requires extracting meaningful information about the environment from observations. In this research, we investigate the challenge of planning high-level goal-oriented actions in a simulation setting from an egocentric perspective. We present a novel task, interactive action planning, and propose an approach that combines topological affordance memory with transformer architecture. The process of memorizing the environment's structure through extracting affordances facilitates selecting appropriate actions based on the context. Moreover, the memory model allows us to detect action deviations while accomplishing specific objectives. To assess the method's versatility, we evaluate it in a realistic interactive simulation environment. Our experimental results demonstrate that the proposed approach learns meaningful representations, resulting in improved performance and robust when action deviations occur.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02997v1",
    "published_date": "2025-07-01 22:53:41 UTC",
    "updated_date": "2025-07-01 22:53:41 UTC"
  },
  {
    "arxiv_id": "2507.02005v1",
    "title": "Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener",
    "authors": [
      "Michael A. Kraus",
      "Helen Bartsch"
    ],
    "abstract": "This research introduces a unified approach combining Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. It integrates expert-driven feature engineering with algorithmic feature creation to enhance accuracy and explainability.\n  Based on the extensive fatigue test database regression models - gradient boosting, random forests, and neural networks - were trained using AutoML under three feature schemes: domain-informed, algorithmic, and combined. This allowed a systematic comparison of expert-based versus automated feature selection.\n  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The domain-informed model $\\mathcal M_2$ achieved the best balance: test RMSE $\\approx$ 30.6 MPa and $R^2 \\approx 0.780% over the full $Δσ_{c,50\\%}$ range, and RMSE $\\approx$ 13.4 MPa and $R^2 \\approx 0.527% within the engineering-relevant 0 - 150 MPa domain. The denser-feature model ($\\mathcal M_3$) showed minor gains during training but poorer generalization, while the simpler base-feature model ($\\mathcal M_1$) performed comparably, confirming the robustness of minimalist designs.\n  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress range $Δσ_i$, yield strength $R_{eH}$, and post-weld treatment (TIG dressing vs. as-welded) as dominant predictors. Secondary geometric factors - plate width, throat thickness, stiffener height - also significantly affected fatigue life.\n  This framework demonstrates that integrating AutoML with XAI yields accurate, interpretable, and robust fatigue strength models for welded steel structures. It bridges data-driven modeling with engineering validation, enabling AI-assisted design and assessment. Future work will explore probabilistic fatigue life modeling and integration into digital twin environments.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02005v1",
    "published_date": "2025-07-01 21:57:12 UTC",
    "updated_date": "2025-07-01 21:57:12 UTC"
  },
  {
    "arxiv_id": "2507.01198v1",
    "title": "Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives",
    "authors": [
      "Benjamin Kraljusic",
      "Zlatan Ajanovic",
      "Nermin Covic",
      "Bakir Lacevic"
    ],
    "abstract": "This work proposes a motion planning algorithm for robotic manipulators that combines sampling-based and search-based planning methods. The core contribution of the proposed approach is the usage of burs of free configuration space (C-space) as adaptive motion primitives within the graph search algorithm. Due to their feature to adaptively expand in free C-space, burs enable more efficient exploration of the configuration space compared to fixed-sized motion primitives, significantly reducing the time to find a valid path and the number of required expansions. The algorithm is implemented within the existing SMPL (Search-Based Motion Planning Library) library and evaluated through a series of different scenarios involving manipulators with varying number of degrees-of-freedom (DoF) and environment complexity. Results demonstrate that the bur-based approach outperforms fixed-primitive planning in complex scenarios, particularly for high DoF manipulators, while achieving comparable performance in simpler scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 3 figures, submitted to a conference",
    "pdf_url": "https://arxiv.org/pdf/2507.01198v1",
    "published_date": "2025-07-01 21:33:33 UTC",
    "updated_date": "2025-07-01 21:33:33 UTC"
  },
  {
    "arxiv_id": "2507.01196v1",
    "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning",
    "authors": [
      "Na Lee",
      "Konstantinos Barmpas",
      "Yannis Panagakis",
      "Dimitrios Adamos",
      "Nikolaos Laskaris",
      "Stefanos Zafeiriou"
    ],
    "abstract": "Foundation Models have demonstrated significant success across various domains in Artificial Intelligence (AI), yet their capabilities for brainwave modeling remain unclear. In this paper, we comprehensively evaluate current Large Brainwave Foundation Models (LBMs) through systematic fine-tuning experiments across multiple Brain-Computer Interface (BCI) benchmark tasks, including memory tasks and sleep stage classification. Our extensive analysis shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%) over traditional deep architectures while requiring significantly more parameters (millions vs thousands), raising important questions about their efficiency and applicability in BCI contexts. Moreover, through detailed ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce trainable parameters without performance degradation, while demonstrating that architectural and training inefficiencies limit LBMs' current capabilities. Our experiments span both full model fine-tuning and parameter-efficient adaptation techniques, providing insights into optimal training strategies for BCI applications. We pioneer the application of LoRA to LBMs, revealing that performance benefits generally emerge when adapting multiple neural network components simultaneously. These findings highlight the critical need for domain-specific development strategies to advance LBMs, suggesting that current architectures may require redesign to fully leverage the potential of foundation models in brainwave analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01196v1",
    "published_date": "2025-07-01 21:21:42 UTC",
    "updated_date": "2025-07-01 21:21:42 UTC"
  },
  {
    "arxiv_id": "2507.02004v1",
    "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research",
    "authors": [
      "Ruofan Jin",
      "Zaixi Zhang",
      "Mengdi Wang",
      "Le Cong"
    ],
    "abstract": "The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench: DBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02004v1",
    "published_date": "2025-07-01 20:52:01 UTC",
    "updated_date": "2025-07-01 20:52:01 UTC"
  },
  {
    "arxiv_id": "2507.01099v1",
    "title": "Geometry-aware 4D Video Generation for Robot Manipulation",
    "authors": [
      "Zeyi Liu",
      "Shuang Li",
      "Eric Cousineau",
      "Siyuan Feng",
      "Benjamin Burchfiel",
      "Shuran Song"
    ],
    "abstract": "Understanding and predicting the dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge. To address this, we propose a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training. This geometric supervision enables the model to learn a shared 3D representation of the scene, allowing it to predict future video sequences from novel viewpoints based solely on the given RGB-D observations, without requiring camera poses as inputs. Compared to existing baselines, our method produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets. We further show that the predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website: https://robot4dgen.github.io",
    "pdf_url": "https://arxiv.org/pdf/2507.01099v1",
    "published_date": "2025-07-01 18:01:41 UTC",
    "updated_date": "2025-07-01 18:01:41 UTC"
  },
  {
    "arxiv_id": "2507.02990v1",
    "title": "`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts",
    "authors": [
      "Annika M Schoene",
      "Cansu Canca"
    ],
    "abstract": "Recent advances in large language models (LLMs) have led to increasingly sophisticated safety protocols and features designed to prevent harmful, unethical, or unauthorized outputs. However, these guardrails remain susceptible to novel and creative forms of adversarial prompting, including manually generated test cases. In this work, we present two new test cases in mental health for (i) suicide and (ii) self-harm, using multi-step, prompt-level jailbreaking and bypass built-in content and safety filters. We show that user intent is disregarded, leading to the generation of detailed harmful content and instructions that could cause real-world harm. We conduct an empirical evaluation across six widely available LLMs, demonstrating the generalizability and reliability of the bypass. We assess these findings and the multilayered ethical tensions that they present for their implications on prompt-response filtering and context- and task-specific model development. We recommend a more comprehensive and systematic approach to AI safety and ethics while emphasizing the need for continuous adversarial testing in safety-critical AI deployments. We also argue that while certain clearly defined safety measures and guardrails can and must be implemented in LLMs, ensuring robust and comprehensive safety across all use cases and domains remains extremely challenging given the current technical maturity of general-purpose LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02990v1",
    "published_date": "2025-07-01 18:00:04 UTC",
    "updated_date": "2025-07-01 18:00:04 UTC"
  },
  {
    "arxiv_id": "2507.01081v1",
    "title": "AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma",
    "authors": [
      "Megan T. deBettencourt",
      "Sruthi Sakthivel",
      "Emily A. Holmes",
      "Mark Chevillet"
    ],
    "abstract": "Trauma prevalence is vast globally. Evidence-based digital treatments can help, but most require human guidance. Human guides provide tailored instructions and responsiveness to internal cognitive states, but limit scalability. Can generative AI and neurotechnology provide a scalable alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to automatically deliver and monitor an evidence-based digital treatment, specifically the Imagery Competing Task Intervention (ICTI), to reduce intrusive memories after psychological trauma. One hundred healthy volunteers were exposed to videos of traumatic events and randomly assigned to an intervention or active control condition. As predicted, intervention participants reported significantly fewer intrusive memories over the following week. Post-hoc assessment against clinical rubrics confirmed the AI guide delivered the intervention successfully. Additionally, pupil size tracked intervention engagement and predicted symptom reduction, providing a candidate biomarker of intervention effectiveness. These findings open a path toward rigorous AI-guided digital interventions that can scale to trauma prevalence.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01081v1",
    "published_date": "2025-07-01 17:59:01 UTC",
    "updated_date": "2025-07-01 17:59:01 UTC"
  },
  {
    "arxiv_id": "2507.01006v6",
    "title": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
    "authors": [
      "GLM-V Team",
      ":",
      "Wenyi Hong",
      "Wenmeng Yu",
      "Xiaotao Gu",
      "Guo Wang",
      "Guobing Gan",
      "Haomiao Tang",
      "Jiale Cheng",
      "Ji Qi",
      "Junhui Ji",
      "Lihang Pan",
      "Shuaiqi Duan",
      "Weihan Wang",
      "Yan Wang",
      "Yean Cheng",
      "Zehai He",
      "Zhe Su",
      "Zhen Yang",
      "Ziyang Pan",
      "Aohan Zeng",
      "Baoxu Wang",
      "Bin Chen",
      "Boyan Shi",
      "Changyu Pang",
      "Chenhui Zhang",
      "Da Yin",
      "Fan Yang",
      "Guoqing Chen",
      "Haochen Li",
      "Jiale Zhu",
      "Jiali Chen",
      "Jiaxing Xu",
      "Jiazheng Xu",
      "Jing Chen",
      "Jinghao Lin",
      "Jinhao Chen",
      "Jinjiang Wang",
      "Junjie Chen",
      "Leqi Lei",
      "Letian Gong",
      "Leyi Pan",
      "Mingdao Liu",
      "Mingde Xu",
      "Mingzhi Zhang",
      "Qinkai Zheng",
      "Ruiliang Lyu",
      "Shangqin Tu",
      "Sheng Yang",
      "Shengbiao Meng",
      "Shi Zhong",
      "Shiyu Huang",
      "Shuyuan Zhao",
      "Siyan Xue",
      "Tianshu Zhang",
      "Tianwei Luo",
      "Tianxiang Hao",
      "Tianyu Tong",
      "Wei Jia",
      "Wenkai Li",
      "Xiao Liu",
      "Xiaohan Zhang",
      "Xin Lyu",
      "Xinyu Zhang",
      "Xinyue Fan",
      "Xuancheng Huang",
      "Yadong Xue",
      "Yanfeng Wang",
      "Yanling Wang",
      "Yanzi Wang",
      "Yifan An",
      "Yifan Du",
      "Yiheng Huang",
      "Yilin Niu",
      "Yiming Shi",
      "Yu Wang",
      "Yuan Wang",
      "Yuanchang Yue",
      "Yuchen Li",
      "Yusen Liu",
      "Yutao Zhang",
      "Yuting Wang",
      "Yuxuan Zhang",
      "Zhao Xue",
      "Zhengxiao Du",
      "Zhenyu Hou",
      "Zihan Wang",
      "Peng Zhang",
      "Debing Liu",
      "Bin Xu",
      "Juanzi Li",
      "Minlie Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "We present GLM-4.1V-Thinking, GLM-4.5V, and GLM-4.6V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. We further introduce the GLM-4.6V series, open-source multimodal models with native tool use and a 128K context window. A brief overview is available at https://z.ai/blog/glm-4.6v. Code, models and more information are released at https://github.com/zai-org/GLM-V.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01006v6",
    "published_date": "2025-07-01 17:55:04 UTC",
    "updated_date": "2026-01-01 13:07:25 UTC"
  },
  {
    "arxiv_id": "2507.01003v3",
    "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes",
    "authors": [
      "Eun-Ji Park",
      "Sangwon Yun"
    ],
    "abstract": "Recent studies have proposed interpreting the training process from an ergodic perspective. Building on this foundation, we present a unified framework for understanding and accelerating the training of deep neural networks via stochastic gradient descent (SGD). By analyzing the geometric landscape of the objective function we introduce a practical diagnostic, the running estimate of the largest Lyapunov exponent, which provably distinguishes genuine convergence toward stable minimizers from mere statistical stabilization near saddle points. We then propose a ghost category extension for standard classifiers that adds auxiliary ghost output nodes so the model gains extra descent directions that open a lateral corridor around narrow loss barriers and enable the optimizer to bypass poor basins during the early training phase. We show that this extension strictly reduces the approximation error and that after sufficient convergence the ghost dimensions collapse so that the extended model coincides with the original one and there exists a path in the enlarged parameter space along which the total loss does not increase. Taken together, these results provide a principled architecture level intervention that accelerates early stage trainability while preserving asymptotic behavior and simultaneously serves as an architecture-friendly regularizer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.01003v3",
    "published_date": "2025-07-01 17:54:35 UTC",
    "updated_date": "2025-07-13 09:40:25 UTC"
  },
  {
    "arxiv_id": "2507.01001v2",
    "title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks",
    "authors": [
      "Yilun Zhao",
      "Kaiyan Zhang",
      "Tiansheng Hu",
      "Sihong Wu",
      "Ronan Le Bras",
      "Charles McGrady",
      "Taira Anderson",
      "Jonathan Bragg",
      "Joseph Chee Chang",
      "Jesse Dodge",
      "Matt Latzke",
      "Yixin Liu",
      "Xiangru Tang",
      "Zihang Wang",
      "Chen Zhao",
      "Hannaneh Hajishirzi",
      "Doug Downey",
      "Arman Cohan"
    ],
    "abstract": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature-grounded tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 47 foundation models and has collected over 20,000 votes from human researchers across diverse scientific domains. Our analysis of the data collected so far confirms its high quality. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on collected preference data. It measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025 Datasets & Benchmarks Track (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2507.01001v2",
    "published_date": "2025-07-01 17:51:59 UTC",
    "updated_date": "2026-01-22 18:32:06 UTC"
  },
  {
    "arxiv_id": "2507.00990v2",
    "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations",
    "authors": [
      "Shivansh Patel",
      "Shraddhaa Mohan",
      "Hanlin Mai",
      "Unnat Jain",
      "Svetlana Lazebnik",
      "Yunzhu Li"
    ],
    "abstract": "This work introduces Robots Imitating Generated Videos (RIGVid), a system that enables robots to perform complex manipulation tasks--such as pouring, wiping, and mixing--purely by imitating AI-generated videos, without requiring any physical demonstrations or robot-specific training. Given a language command and an initial scene image, a video diffusion model generates potential demonstration videos, and a vision-language model (VLM) automatically filters out results that do not follow the command. A 6D pose tracker then extracts object trajectories from the video, and the trajectories are retargeted to the robot in an embodiment-agnostic fashion. Through extensive real-world evaluations, we show that filtered generated videos are as effective as real demonstrations, and that performance improves with generation quality. We also show that relying on generated videos outperforms more compact alternatives such as keypoint prediction using VLMs, and that strong 6D pose tracking outperforms other ways to extract trajectories, such as dense feature point tracking. These findings suggest that videos produced by a state-of-the-art off-the-shelf model can offer an effective source of supervision for robotic manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page: https://rigvid-robot.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2507.00990v2",
    "published_date": "2025-07-01 17:39:59 UTC",
    "updated_date": "2025-07-04 14:35:12 UTC"
  },
  {
    "arxiv_id": "2507.00979v1",
    "title": "Enhancing LLM Agent Safety via Causal Influence Prompting",
    "authors": [
      "Dongyoon Hahm",
      "Woogyeol Jin",
      "June Suk Choi",
      "Sungsoo Ahn",
      "Kimin Lee"
    ],
    "abstract": "As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ACL 2025 Findings, Source code: https://github.com/HahmDY/causal_influence_prompting.git",
    "pdf_url": "https://arxiv.org/pdf/2507.00979v1",
    "published_date": "2025-07-01 17:31:51 UTC",
    "updated_date": "2025-07-01 17:31:51 UTC"
  },
  {
    "arxiv_id": "2507.00971v2",
    "title": "Reasoning as an Adaptive Defense for Safety",
    "authors": [
      "Taeyoun Kim",
      "Fahim Tajwar",
      "Aditi Raghunathan",
      "Aviral Kumar"
    ],
    "abstract": "Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a ``lightweight'' warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages, 10 Figures, 7 Tables",
    "pdf_url": "https://arxiv.org/pdf/2507.00971v2",
    "published_date": "2025-07-01 17:20:04 UTC",
    "updated_date": "2025-10-27 14:28:11 UTC"
  },
  {
    "arxiv_id": "2507.00969v1",
    "title": "Surgical Neural Radiance Fields from One Image",
    "authors": [
      "Alberto Neri",
      "Maximilan Fehrentz",
      "Veronica Penza",
      "Leonardo S. Mattos",
      "Nazim Haouchine"
    ],
    "abstract": "Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D reconstruction and view synthesis, yet their reliance on extensive multi-view data limits their application in surgical intraoperative settings where only limited data is available. In particular, collecting such extensive data intraoperatively is impractical due to time constraints. This work addresses this challenge by leveraging a single intraoperative image and preoperative data to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera viewpoints and images needed for robust and unobstructed training. Intraoperatively, the appearance of the surgical image is transferred to the pre-constructed training set through neural style transfer, specifically combining WTC2 and STROTSS to prevent over-stylization. This process enables the creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases. Quantitative comparisons to NeRF models trained on real surgical microscope images demonstrate strong synthesis agreement, with similarity metrics indicating high reconstruction fidelity and stylistic alignment. When compared with ground truth, our method demonstrates high structural similarity, confirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF training in surgical settings, overcoming the limitations of traditional multi-view methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00969v1",
    "published_date": "2025-07-01 17:19:25 UTC",
    "updated_date": "2025-07-01 17:19:25 UTC"
  },
  {
    "arxiv_id": "2507.00966v4",
    "title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement",
    "authors": [
      "Nikolai Lund Kühne",
      "Jesper Jensen",
      "Jan Østergaard",
      "Zheng-Hua Tan"
    ],
    "abstract": "With new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform the state-of-the-art in single-channel speech enhancement and audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VB-DemandEx, a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, MambAttention significantly outperforms existing state-of-the-art discriminative LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 without reverberation and EARS-WHAM_v2. MambAttention also matches or outperforms generative diffusion models in generalization performance while being competitive with language model baselines. Ablation studies highlight the importance of weight sharing between time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. Yet, MambAttention remains superior for cross-corpus generalization across all reported evaluation metrics.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to IEEE Transactions on Audio, Speech, and Language Processing",
    "pdf_url": "https://arxiv.org/pdf/2507.00966v4",
    "published_date": "2025-07-01 17:16:05 UTC",
    "updated_date": "2026-01-21 11:36:14 UTC"
  },
  {
    "arxiv_id": "2507.00953v2",
    "title": "From Sentences to Sequences: Rethinking Languages in Biological System",
    "authors": [
      "Ke Liu",
      "Shuaike Shen",
      "Hao Chen"
    ],
    "abstract": "The paradigm of large language models in natural language processing (NLP) has also shown promise in modeling biological languages, including proteins, RNA, and DNA. Both the auto-regressive generation paradigm and evaluation metrics have been transferred from NLP to biological sequence modeling. However, the intrinsic structural correlations in natural and biological languages differ fundamentally. Therefore, we revisit the notion of language in biological systems to better understand how NLP successes can be effectively translated to biological domains. By treating the 3D structure of biomolecules as the semantic content of a sentence and accounting for the strong correlations between residues or bases, we highlight the importance of structural evaluation and demonstrate the applicability of the auto-regressive paradigm in biological language modeling. Code can be found at \\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00953v2",
    "published_date": "2025-07-01 16:57:39 UTC",
    "updated_date": "2025-07-03 10:33:16 UTC"
  },
  {
    "arxiv_id": "2507.00951v3",
    "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact",
    "authors": [
      "Rizwan Qureshi",
      "Ranjan Sapkota",
      "Abbas Shah",
      "Amgad Muneer",
      "Anas Zafar",
      "Ashmal Vayani",
      "Maged Shoman",
      "Abdelrahman B. M. Eldaly",
      "Kai Zhang",
      "Ferhat Sadak",
      "Shaina Raza",
      "Xinqi Fan",
      "Ravid Shwartz-Ziv",
      "Hong Yan",
      "Vinjia Jain",
      "Aman Chadha",
      "Manoj Karkee",
      "Jia Wu",
      "Seyedali Mirjalili"
    ],
    "abstract": "Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00951v3",
    "published_date": "2025-07-01 16:52:25 UTC",
    "updated_date": "2025-07-12 02:50:17 UTC"
  },
  {
    "arxiv_id": "2507.00938v2",
    "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks",
    "authors": [
      "Zihao Sun",
      "Ling Chen"
    ],
    "abstract": "Recent progress in large language models (LLMs) has enabled the development of autonomous web agents capable of navigating and interacting with real websites. However, evaluating such agents remains challenging due to the instability and inconsistency of existing benchmarks, which often rely on dynamic content or oversimplified simulations. In this work, we introduce WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks grounded in the arXiv platform. WebArXiv ensures reproducible and reliable evaluation by anchoring tasks in fixed web snapshots with deterministic ground truths and standardized action trajectories. Through behavioral analysis, we identify a common failure mode, Rigid History Reflection, where agents over-rely on fixed interaction histories. To address this, we propose a lightweight dynamic reflection mechanism that allows agents to selectively retrieve relevant past steps during decision-making. We evaluate ten state-of-the-art web agents on WebArXiv. Results demonstrate clear performance differences across agents and validate the effectiveness of our proposed reflection strategy.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 9 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.00938v2",
    "published_date": "2025-07-01 16:43:57 UTC",
    "updated_date": "2025-08-13 09:05:05 UTC"
  },
  {
    "arxiv_id": "2507.00914v1",
    "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications",
    "authors": [
      "Jindong Han",
      "Yansong Ning",
      "Zirui Yuan",
      "Hang Ni",
      "Fan Liu",
      "Tengfei Lyu",
      "Hao Liu"
    ],
    "abstract": "The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00914v1",
    "published_date": "2025-07-01 16:18:29 UTC",
    "updated_date": "2025-07-01 16:18:29 UTC"
  },
  {
    "arxiv_id": "2507.00909v1",
    "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona",
    "authors": [
      "Philip Colangelo",
      "Ayse K. Coskun",
      "Jack Megrue",
      "Ciaran Roberts",
      "Shayan Sengupta",
      "Varun Sivaram",
      "Ethan Tiao",
      "Aroon Vijaykar",
      "Chris Williams",
      "Daniel C. Wilson",
      "Zack MacFarland",
      "Daniel Dreiling",
      "Nathan Morey",
      "Anuja Ratnayake",
      "Baskar Vairamohan"
    ],
    "abstract": "Artificial intelligence (AI) is fueling exponential electricity demand growth, threatening grid reliability, raising prices for communities paying for new energy infrastructure, and stunting AI innovation as data centers wait for interconnection to constrained grids. This paper presents the first field demonstration, in collaboration with major corporate partners, of a software-only approach--Emerald Conductor--that transforms AI data centers into flexible grid resources that can efficiently and immediately harness existing power systems without massive infrastructure buildout. Conducted at a 256-GPU cluster running representative AI workloads within a commercial, hyperscale cloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in cluster power usage for three hours during peak grid events while maintaining AI quality of service (QoS) guarantees. By orchestrating AI workloads based on real-time grid signals without hardware modifications or energy storage, this platform reimagines data centers as grid-interactive assets that enhance grid reliability, advance affordability, and accelerate AI's development.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF",
      "eess.SY"
    ],
    "primary_category": "cs.DC",
    "comment": "10 pages, 6 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2507.00909v1",
    "published_date": "2025-07-01 16:11:49 UTC",
    "updated_date": "2025-07-01 16:11:49 UTC"
  },
  {
    "arxiv_id": "2507.00907v1",
    "title": "The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses",
    "authors": [
      "Fabio Correa Xavier"
    ],
    "abstract": "In a world where deepfakes and cloned voices are emerging as sophisticated attack vectors, organizations require a new security mindset: Sensorial Zero Trust [9]. This article presents a scientific analysis of the need to systematically doubt information perceived through the senses, establishing rigorous verification protocols to mitigate the risks of fraud based on generative artificial intelligence. Key concepts, such as Out-of-Band verification, Vision-Language Models (VLMs) as forensic collaborators, cryptographic provenance, and human training, are integrated into a framework that extends Zero Trust principles to human sensory information. The approach is grounded in empirical findings and academic research, emphasizing that in an era of AI-generated realities, even our eyes and ears can no longer be implicitly trusted without verification. Leaders are called to foster a culture of methodological skepticism to protect organizational integrity in this new threat landscape.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.00907v1",
    "published_date": "2025-07-01 16:11:41 UTC",
    "updated_date": "2025-07-01 16:11:41 UTC"
  },
  {
    "arxiv_id": "2507.00903v1",
    "title": "Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection",
    "authors": [
      "Andreea Bianca Popescu",
      "Andreas Seitz",
      "Heiko Mahrholdt",
      "Jens Wetzl",
      "Athira Jacob",
      "Lucian Mihai Itu",
      "Constantin Suciu",
      "Teodora Chitiboi"
    ],
    "abstract": "Objectives Parametric tissue mapping enables quantitative cardiac tissue characterization but is limited by inter-observer variability during manual delineation. Traditional approaches relying on average relaxation values and single cutoffs may oversimplify myocardial complexity. This study evaluates whether deep learning (DL) can achieve segmentation accuracy comparable to inter-observer variability, explores the utility of statistical features beyond mean T1/T2 values, and assesses whether machine learning (ML) combining multiple features enhances disease detection. Materials & Methods T1 and T2 maps were manually segmented. The test subset was independently annotated by two observers, and inter-observer variability was assessed. A DL model was trained to segment left ventricle blood pool and myocardium. Average (A), lower quartile (LQ), median (M), and upper quartile (UQ) were computed for the myocardial pixels and employed in classification by applying cutoffs or in ML. Dice similarity coefficient (DICE) and mean absolute percentage error evaluated segmentation performance. Bland-Altman plots assessed inter-user and model-observer agreement. Receiver operating characteristic analysis determined optimal cutoffs. Pearson correlation compared features from model and manual segmentations. F1-score, precision, and recall evaluated classification performance. Wilcoxon test assessed differences between classification methods, with p < 0.05 considered statistically significant. Results 144 subjects were split into training (100), validation (15) and evaluation (29) subsets. Segmentation model achieved a DICE of 85.4%, surpassing inter-observer agreement. Random forest applied to all features increased F1-score (92.7%, p < 0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining multiple features with ML improves disease detection.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This work has been submitted for consideration at European Radiology (Springer). Upon acceptance, this preprint will be updated with the journal reference",
    "pdf_url": "https://arxiv.org/pdf/2507.00903v1",
    "published_date": "2025-07-01 16:08:54 UTC",
    "updated_date": "2025-07-01 16:08:54 UTC"
  },
  {
    "arxiv_id": "2507.00902v1",
    "title": "Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks",
    "authors": [
      "Feng Wang",
      "Shengyu Zhang",
      "Een-Kee Hong",
      "Tony Q. S. Quek"
    ],
    "abstract": "Direct-satellite-to-device (DS2D) communication is emerging as a promising solution for global mobile service extension, leveraging the deployment of satellite constellations. However, the challenge of managing DS2D connectivity for multi-constellations becomes outstanding, including high interference and frequent handovers caused by multi-coverage overlap and rapid satellite movement. Moreover, existing approaches primarily operate within single-constellation shell, which inherently limits the ability to exploit the vast potential of multi-constellation connectivity provision, resulting in suboptimal DS2D service performances. To address these challenges, this article proposes a Constellation as a Service (CaaS) framework, which treats the entire multi-constellation infrastructure as a shared resource pool and dynamically forms optimal sub-constellations (SCs) for each DS2D service region. The formation of each SC integrates satellites from various orbits to provide tailored connectivity based on user demands, guided by two innovative strategies: predictive satellite beamforming using generative artificial intelligence (GenAI) and pre-configured handover path for efficient satellite access and mobility management. Simulation results demonstrate that CaaS significantly improves satellite service rates while reducing handover overhead, making it an efficient and continuable solution for managing DS2D connectivity in multi-constellation environments.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.SY",
    "comment": "To appear in IEEE Communications Magazine",
    "pdf_url": "https://arxiv.org/pdf/2507.00902v1",
    "published_date": "2025-07-01 16:06:29 UTC",
    "updated_date": "2025-07-01 16:06:29 UTC"
  },
  {
    "arxiv_id": "2507.00891v1",
    "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes",
    "authors": [
      "Yuheng Wang",
      "Xianhe Tang",
      "Pufeng Huang"
    ],
    "abstract": "Memes are widely used in online social interactions, providing vivid, intuitive, and often humorous means to express intentions and emotions. Existing dialogue datasets are predominantly limited to either manually annotated or pure-text conversations, lacking the expressiveness and contextual nuance that multimodal interactions provide.To address these challenges, we introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue dataset with contextually retrieved memes. Our dataset combines a large-scale, MLLM-annotated meme library with dialogues auto-generated by dual agents across diverse scenarios. We introduce a retrieval framework and adaptive threshold to ensure contextually relevant, naturally spaced meme usage. Experiments demonstrate the effectiveness of our approach in generating contextually appropriate and diverse meme-incorporated dialogues, offering a scalable and privacy-preserving resource for advancing multimodal conversational AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00891v1",
    "published_date": "2025-07-01 15:57:14 UTC",
    "updated_date": "2025-07-01 15:57:14 UTC"
  },
  {
    "arxiv_id": "2507.00884v1",
    "title": "A Scalable and Quantum-Accurate Foundation Model for Biomolecular Force Field via Linearly Tensorized Quadrangle Attention",
    "authors": [
      "Qun Su",
      "Kai Zhu",
      "Qiaolin Gou",
      "Jintu Zhang",
      "Renling Hu",
      "Yurong Li",
      "Yongze Wang",
      "Hui Zhang",
      "Ziyi You",
      "Linlong Jiang",
      "Yu Kang",
      "Jike Wang",
      "Chang-Yu Hsieh",
      "Tingjun Hou"
    ],
    "abstract": "Accurate atomistic biomolecular simulations are vital for disease mechanism understanding, drug discovery, and biomaterial design, but existing simulation methods exhibit significant limitations. Classical force fields are efficient but lack accuracy for transition states and fine conformational details critical in many chemical and biological processes. Quantum Mechanics (QM) methods are highly accurate but computationally infeasible for large-scale or long-time simulations. AI-based force fields (AIFFs) aim to achieve QM-level accuracy with efficiency but struggle to balance many-body modeling complexity, accuracy, and speed, often constrained by limited training data and insufficient validation for generalizability. To overcome these challenges, we introduce LiTEN, a novel equivariant neural network with Tensorized Quadrangle Attention (TQA). TQA efficiently models three- and four-body interactions with linear complexity by reparameterizing high-order tensor features via vector operations, avoiding costly spherical harmonics. Building on LiTEN, LiTEN-FF is a robust AIFF foundation model, pre-trained on the extensive nablaDFT dataset for broad chemical generalization and fine-tuned on SPICE for accurate solvated system simulations. LiTEN achieves state-of-the-art (SOTA) performance across most evaluation subsets of rMD17, MD22, and Chignolin, outperforming leading models such as MACE, NequIP, and EquiFormer. LiTEN-FF enables the most comprehensive suite of downstream biomolecular modeling tasks to date, including QM-level conformer searches, geometry optimization, and free energy surface construction, while offering 10x faster inference than MACE-OFF for large biomolecules (~1000 atoms). In summary, we present a physically grounded, highly efficient framework that advances complex biomolecular modeling, providing a versatile foundation for drug discovery and related applications.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG",
      "physics.bio-ph"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00884v1",
    "published_date": "2025-07-01 15:52:39 UTC",
    "updated_date": "2025-07-01 15:52:39 UTC"
  },
  {
    "arxiv_id": "2507.00880v1",
    "title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation",
    "authors": [
      "Ruihan Xu",
      "Haokui Zhang",
      "Yaowei Wang",
      "Wei Zeng",
      "Shiliang Zhang"
    ],
    "abstract": "The growing use of deep learning necessitates efficient network design and deployment, making neural predictors vital for estimating attributes such as accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers have shown promising performance in representing neural architectures. However, each of both methods has its disadvantages. GNNs lack the capabilities to represent complicated features, while transformers face poor generalization when the depth of architecture grows. To mitigate the above issues, we rethink neural architecture topology and show that sibling nodes are pivotal while overlooked in previous research. We thus propose a novel predictor leveraging the strengths of GNNs and transformers to learn the enhanced topology. We introduce a novel token mixer that considers siblings, and a new channel mixer named bidirectional graph isomorphism feed-forward network. Our approach consistently achieves promising performance in both accuracy and latency prediction, providing valuable insights for learning Directed Acyclic Graph (DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to CVPR 2025. Code is avaiable at https://github.com/XuRuihan/NNFormer",
    "pdf_url": "https://arxiv.org/pdf/2507.00880v1",
    "published_date": "2025-07-01 15:46:18 UTC",
    "updated_date": "2025-07-01 15:46:18 UTC"
  },
  {
    "arxiv_id": "2507.00841v1",
    "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents",
    "authors": [
      "Siyuan Liang",
      "Tianmeng Fang",
      "Zhe Liu",
      "Aishan Liu",
      "Yan Xiao",
      "Jinyuan He",
      "Ee-Chien Chang",
      "Xiaochun Cao"
    ],
    "abstract": "With the wide application of multimodal foundation models in intelligent agent systems, scenarios such as mobile device control, intelligent assistant interaction, and multimodal task execution are gradually relying on such large model-driven agents. However, the related systems are also increasingly exposed to potential jailbreak risks. Attackers may induce the agents to bypass the original behavioral constraints through specific inputs, and then trigger certain risky and sensitive operations, such as modifying settings, executing unauthorized commands, or impersonating user identities, which brings new challenges to system security. Existing security measures for intelligent agents still have limitations when facing complex interactions, especially in detecting potentially risky behaviors across multiple rounds of conversations or sequences of tasks. In addition, an efficient and consistent automated methodology to assist in assessing and determining the impact of such risks is currently lacking. This work explores the security issues surrounding mobile multimodal agents, attempts to construct a risk discrimination mechanism by incorporating behavioral sequence information, and designs an automated assisted assessment scheme based on a large language model. Through preliminary validation in several representative high-risk tasks, the results show that the method can improve the recognition of risky behaviors to some extent and assist in reducing the probability of agents being jailbroken. We hope that this study can provide some valuable references for the security risk modeling and protection of multimodal intelligent agent systems.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.00841v1",
    "published_date": "2025-07-01 15:10:00 UTC",
    "updated_date": "2025-07-01 15:10:00 UTC"
  },
  {
    "arxiv_id": "2507.00838v2",
    "title": "Stylometry recognizes human and LLM-generated texts in short samples",
    "authors": [
      "Karol Przystalski",
      "Jan K. Argasiński",
      "Iwona Grabska-Gradzińska",
      "Jeremi K. Ochab"
    ],
    "abstract": "The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show -- crucially, in the context of the increasingly sophisticated LLMs -- that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00838v2",
    "published_date": "2025-07-01 15:08:53 UTC",
    "updated_date": "2025-07-15 11:31:45 UTC"
  },
  {
    "arxiv_id": "2507.00833v2",
    "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning",
    "authors": [
      "Zhi Jing",
      "Siyuan Yang",
      "Jicong Ao",
      "Ting Xiao",
      "Yu-Gang Jiang",
      "Chenjia Bai"
    ],
    "abstract": "For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is https://openhumanoidgen.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page: https://openhumanoidgen.github.io",
    "pdf_url": "https://arxiv.org/pdf/2507.00833v2",
    "published_date": "2025-07-01 15:04:38 UTC",
    "updated_date": "2025-11-16 19:36:16 UTC"
  },
  {
    "arxiv_id": "2507.00832v1",
    "title": "Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection",
    "authors": [
      "Jisoo Kim",
      "Chu-Hsuan Lin",
      "Alberto Ceballos-Arroyo",
      "Ping Liu",
      "Huaizu Jiang",
      "Shrikanth Yadav",
      "Qi Wan",
      "Lei Qin",
      "Geoffrey S Young"
    ],
    "abstract": "Introduction: Deep learning (DL) models can help detect intracranial aneurysms on CTA, but high false positive (FP) rates remain a barrier to clinical translation, despite improvement in model architectures and strategies like detection threshold tuning. We employed an automated, anatomy-based, heuristic-learning hybrid artery-vein segmentation post-processing method to further reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D convolutional neural network-transformer hybrid (3D-CNN-TR), were trained with 1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143 held-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and cavernous venous sinus (CVS) segmentation masks were applied to remove possible FPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3) vein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more than artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79 false-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were commonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%; 3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular (CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing CPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without reducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from 1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable post-processing can improve DL-based aneurysm detection model performance. More broadly, automated, domain-informed, hybrid heuristic-learning processing holds promise for improving the performance and clinical acceptance of aneurysm detection models.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00832v1",
    "published_date": "2025-07-01 15:03:43 UTC",
    "updated_date": "2025-07-01 15:03:43 UTC"
  },
  {
    "arxiv_id": "2507.00817v1",
    "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs",
    "authors": [
      "Jiaming Zhang",
      "Rui Hu",
      "Qing Guo",
      "Wei Yang Bryan Lim"
    ],
    "abstract": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive capabilities in temporal reasoning and cross-modal understanding, yet their vulnerability to adversarial attacks remains underexplored due to unique challenges: complex cross-modal reasoning mechanisms, temporal dependencies, and computational constraints. We present CAVALRY-V (Cross-modal Language-Vision Adversarial Yielding for Videos), a novel framework that directly targets the critical interface between visual perception and language generation in V-MLLMs. Our approach introduces two key innovations: (1) a dual-objective semantic-visual loss function that simultaneously disrupts the model's text generation logits and visual representations to undermine cross-modal integration, and (2) a computationally efficient two-stage generator framework that combines large-scale pre-training for cross-model transferability with specialized fine-tuning for spatiotemporal coherence. Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves flexibility through implicit temporal coherence modeling rather than explicit regularization, enabling significant performance improvements even on image understanding (34.4% average gain). This capability demonstrates CAVALRY-V's potential as a foundational approach for adversarial research across multimodal systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00817v1",
    "published_date": "2025-07-01 14:48:27 UTC",
    "updated_date": "2025-07-01 14:48:27 UTC"
  },
  {
    "arxiv_id": "2507.00816v1",
    "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments",
    "authors": [
      "Mengyun Wang",
      "Bo Wang",
      "Yifeng Niu",
      "Chang Wang"
    ],
    "abstract": "Accurate dynamics modeling is essential for quadrotors to achieve precise trajectory tracking in various applications. Traditional physical knowledge-driven modeling methods face substantial limitations in unknown environments characterized by variable payloads, wind disturbances, and external perturbations. On the other hand, data-driven modeling methods suffer from poor generalization when handling out-of-distribution (OoD) data, restricting their effectiveness in unknown scenarios. To address these challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN), which combines knowledge-driven and data-driven modeling methods by embedding physical constraints directly into the training process for robust quadrotor dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional Network (TCN) architecture that efficiently captures temporal dependencies from historical flight data, while a physics-informed loss function applies physical principles to improve model generalization and robustness across previously unseen conditions. By incorporating real-time prediction results into a model predictive control (MPC) framework, we achieve improvements in closed-loop tracking performance. Comprehensive simulations and real-world flight experiments demonstrate that our approach outperforms baseline methods in terms of prediction accuracy, tracking precision, and robustness to unknown environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00816v1",
    "published_date": "2025-07-01 14:48:22 UTC",
    "updated_date": "2025-07-01 14:48:22 UTC"
  },
  {
    "arxiv_id": "2507.00814v2",
    "title": "Many LLMs Are More Utilitarian Than One",
    "authors": [
      "Anita Keshmirian",
      "Razan Baltaji",
      "Babak Hemmatian",
      "Hadi Asghari",
      "Lav R. Varshney"
    ],
    "abstract": "Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the Boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed either reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the Boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.00814v2",
    "published_date": "2025-07-01 14:46:16 UTC",
    "updated_date": "2025-10-29 13:37:41 UTC"
  },
  {
    "arxiv_id": "2507.00810v1",
    "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis",
    "authors": [
      "Qing Xu",
      "Xiaohua Xuan"
    ],
    "abstract": "In this paper, we propose an improved numerical algorithm for solving minimax problems based on nonsmooth optimization, quadratic programming and iterative process. We also provide a rigorous proof of convergence for our algorithm under some mild assumptions, such as gradient continuity and boundedness. Such an algorithm can be widely applied in various fields such as robust optimization, imbalanced learning, etc.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00810v1",
    "published_date": "2025-07-01 14:41:59 UTC",
    "updated_date": "2025-07-01 14:41:59 UTC"
  },
  {
    "arxiv_id": "2507.01076v4",
    "title": "Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem",
    "authors": [
      "Vanja Stojanović",
      "Bor Pangeršič"
    ],
    "abstract": "The NP-complete mutual-visibility (MV) problem currently lacks empirical analysis on its practical behaviour despite theoretical studies. This paper addresses this gap by implementing and evaluating three distinct algorithms -- a direct random heuristic, a hypergraph-based approximation, and a genetic algorithm -- on diverse synthetic graph datasets, including those with analytically known $μ(G)$ values and general graph models. Our results demonstrate that for smaller graphs, the algorithms consistently achieve MV set sizes aligning with theoretical bounds. However, for larger instances, achieved solution sizes notably diverge from theoretical limits; this, combined with the absence of tight bounds, complicates absolute quality assessment. Nevertheless, validation on known optimal graphs showed the Genetic Algorithm and other heuristics empirically performing best among tested methods.",
    "categories": [
      "cs.CG",
      "cs.AI",
      "cs.PF",
      "math.CO"
    ],
    "primary_category": "cs.CG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01076v4",
    "published_date": "2025-07-01 14:35:44 UTC",
    "updated_date": "2025-09-29 06:41:24 UTC"
  },
  {
    "arxiv_id": "2507.00790v3",
    "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling",
    "authors": [
      "Huaqiu Li",
      "Yong Wang",
      "Tongwen Huang",
      "Hailang Huang",
      "Haoqian Wang",
      "Xiangxiang Chu"
    ],
    "abstract": "Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data are available at https://github.com/AMAP-ML/LD-RPS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00790v3",
    "published_date": "2025-07-01 14:25:09 UTC",
    "updated_date": "2025-09-03 08:45:06 UTC"
  },
  {
    "arxiv_id": "2507.00788v2",
    "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability",
    "authors": [
      "Markus Borg",
      "Dave Hewett",
      "Nadim Hagatulah",
      "Noric Couderc",
      "Emma Söderberg",
      "Donald Graham",
      "Uttam Kini",
      "Dave Farley"
    ],
    "abstract": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming software engineering. While several studies highlight productivity improvements, their impact on maintainability requires further investigation. [Objective] This study investigates whether co-development with AI assistants affects software maintainability, specifically how easily other developers can evolve the resulting source code. [Method] We conducted a two-phase controlled experiment involving 151 participants, 95% of whom were professional developers. In Phase 1, participants added a new feature to a Java web application, with or without AI assistance. In Phase 2, a randomized controlled trial, new participants evolved these solutions without AI assistance. [Results] Phase 2 revealed no significant differences in subsequent evolution with respect to completion time or code quality. Bayesian analysis suggests that any speed or quality improvements from AI use were at most small and highly uncertain. Observational results from Phase 1 corroborate prior research: using an AI assistant yielded a 30.7% median reduction in completion time, and habitual AI users showed an estimated 55.9% speedup. [Conclusions] Overall, we did not detect systematic maintainability advantages or disadvantages when other developers evolved code co-developed with AI assistants. Within the scope of our tasks and measures, we observed no consistent warning signs of degraded code-level maintainability. Future work should examine risks such as code bloat from excessive code generation and cognitive debt as developers offload more mental effort to assistants.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal Acceptance. https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track",
    "pdf_url": "https://arxiv.org/pdf/2507.00788v2",
    "published_date": "2025-07-01 14:24:37 UTC",
    "updated_date": "2025-12-18 14:05:14 UTC"
  },
  {
    "arxiv_id": "2507.00769v1",
    "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing",
    "authors": [
      "Daniel Fein",
      "Sebastian Russo",
      "Violet Xiang",
      "Kabir Jolly",
      "Rafael Rafailov",
      "Nick Haber"
    ],
    "abstract": "Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00769v1",
    "published_date": "2025-07-01 14:10:36 UTC",
    "updated_date": "2025-07-01 14:10:36 UTC"
  },
  {
    "arxiv_id": "2507.00755v1",
    "title": "LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End",
    "authors": [
      "Jinhai Hu",
      "Zhongyi Zhang",
      "Cong Sheng Leow",
      "Wang Ling Goh",
      "Yuan Gao"
    ],
    "abstract": "This paper presents a circuit-algorithm co-design framework for learnable analog front-end (AFE) in audio signal classification. Designing AFE and backend classifiers separately is a common practice but non-ideal, as shown in this paper. Instead, this paper proposes a joint optimization of the backend classifier with the AFE's transfer function to achieve system-level optimum. More specifically, the transfer function parameters of an analog bandpass filter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training loop for the classifier. Using a co-design loss function LBPF, this work shows superior optimization of both the filter bank and the classifier. Implemented in open-source SKY130 130nm CMOS process, the optimized design achieved 90.5%-94.2% accuracy for 10-keyword classification task across a wide range of input signal SNR from 5 dB to 20 dB, with only 22k classifier parameters. Compared to conventional approach, the proposed audio AFE achieves 8.7% and 12.9% reduction in power and capacitor area respectively.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "11 pages, 15 figures, accepted for publication on IEEE Transactions on Circuits and Systems I: Regular Papers",
    "pdf_url": "https://arxiv.org/pdf/2507.00755v1",
    "published_date": "2025-07-01 13:59:24 UTC",
    "updated_date": "2025-07-01 13:59:24 UTC"
  },
  {
    "arxiv_id": "2507.00726v3",
    "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess",
    "authors": [
      "Dongyoon Hwang",
      "Hojoon Lee",
      "Jaegul Choo",
      "Dongmin Park",
      "Jongho Park"
    ],
    "abstract": "While reinforcement learning (RL) for large language models (LLMs) has shown promise in mathematical reasoning, strategic reasoning for LLMs using RL remains largely unexplored. We investigate whether LLMs can develop strategic reasoning capabilities through RL in chess. To this end, we leverage a chess-pretrained action-value network to provide dense reward on the LLM's output move quality, which can be seen as a form of knowledge distillation. Our experiments show that our distillation-based dense rewards often outperform sparse binary rewards. However, surprisingly, all models plateau far below expert levels. We provide SFT and RL ablations on chess reasoning training and find evidence that this limitation stems from a deficit in the pretrained models' internal understanding of chess-a deficit which RL alone may not be able to fully overcome. The code is available at https://github.com/krafton-ai/Chess-R1.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted into Test-time Scaling and Reasoning Models (SCALR) workshop at COLM 2025. 28 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.00726v3",
    "published_date": "2025-07-01 13:16:34 UTC",
    "updated_date": "2025-08-27 22:56:13 UTC"
  },
  {
    "arxiv_id": "2507.00709v5",
    "title": "TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving",
    "authors": [
      "Yiming Yang",
      "Yueru Luo",
      "Bingkun He",
      "Hongbin Lin",
      "Suzhong Fu",
      "Chao Zheng",
      "Zhipeng Cao",
      "Erlong Li",
      "Chao Yan",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "Lane segment topology reasoning constructs a comprehensive road network by capturing the topological relationships between lane segments and their semantic types. This enables end-to-end autonomous driving systems to perform road-dependent maneuvers such as turning and lane changing. However, the limitations in consistent positional embedding and temporal multiple attribute learning in existing methods hinder accurate roadnet reconstruction. To address these issues, we propose TopoStreamer, an end-to-end temporal perception model for lane segment topology reasoning. Specifically, TopoStreamer introduces three key improvements: streaming attribute constraints, dynamic lane boundary positional encoding, and lane segment denoising. The streaming attribute constraints enforce temporal consistency in both centerline and boundary coordinates, along with their classifications. Meanwhile, dynamic lane boundary positional encoding enhances the learning of up-to-date positional information within queries, while lane segment denoising helps capture diverse lane segment patterns, ultimately improving model performance. Additionally, we assess the accuracy of existing models using a lane boundary classification metric, which serves as a crucial measure for lane-changing scenarios in autonomous driving. On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements over state-of-the-art methods, achieving substantial performance gains of +3.0% mAP in lane segment perception and +1.7% OLS in centerline perception tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00709v5",
    "published_date": "2025-07-01 12:10:46 UTC",
    "updated_date": "2025-11-12 06:03:36 UTC"
  },
  {
    "arxiv_id": "2507.00669v2",
    "title": "Audio-3DVG: Unified Audio -- Point Cloud Fusion for 3D Visual Grounding",
    "authors": [
      "Duc Cao-Dinh",
      "Khai Le-Duc",
      "Anh Dao",
      "Bach Phan Tat",
      "Chris Ngo",
      "Duy M. H. Nguyen",
      "Nguyen X. Khanh",
      "Thanh Nguyen-Tang"
    ],
    "abstract": "3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce (i) Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an (ii) Audio-Guided Attention module that models the interactions between target candidates and mentioned objects, enhancing discrimination in cluttered 3D environments. To support benchmarking, we (iii) synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods, highlight the promise of integrating spoken language into 3D vision tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint, 51 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.00669v2",
    "published_date": "2025-07-01 11:08:22 UTC",
    "updated_date": "2025-08-13 00:50:35 UTC"
  },
  {
    "arxiv_id": "2507.00665v2",
    "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder",
    "authors": [
      "Sihang Li",
      "Wei Shi",
      "Ziyuan Xie",
      "Tao Liang",
      "Guojun Ma",
      "Xiang Wang"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at https://github.com/xzy-101/SAFER-code. \\textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "One of the institutions requires additional approval before we can move forward with the publication. Thanks for your understanding, and we hope to resubmit once everything is finalized",
    "pdf_url": "https://arxiv.org/pdf/2507.00665v2",
    "published_date": "2025-07-01 11:04:03 UTC",
    "updated_date": "2025-10-14 09:23:11 UTC"
  },
  {
    "arxiv_id": "2507.00660v2",
    "title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound",
    "authors": [
      "Rusi Chen",
      "Yuanting Yang",
      "Jiezhi Yao",
      "Hongning Song",
      "Ji Zhang",
      "Yongsong Zhou",
      "Yuhao Huang",
      "Ronghao Yang",
      "Dan Jia",
      "Yuhan Zhang",
      "Xing Tao",
      "Haoran Dou",
      "Qing Zhou",
      "Xin Yang",
      "Dong Ni"
    ],
    "abstract": "Mitral regurgitation is one of the most prevalent cardiac disorders. Four-dimensional (4D) ultrasound has emerged as the primary imaging modality for assessing dynamic valvular morphology. However, 4D mitral valve (MV) analysis remains challenging due to limited phase annotations, severe motion artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency in existing methods hinders 4D MV analysis. To bridge this gap, we propose a Motion-Topology guided consistency network (MTCNet) for accurate 4D MV ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only sparse end-diastolic and end-systolic annotations. First, we design a cross-phase motion-guided consistency learning strategy, utilizing a bi-directional attention memory bank to propagate spatio-temporal features. This enables MTCNet to achieve excellent performance both per- and inter-phase. Second, we devise a novel topology-guided correlation regularization that explores physical prior knowledge to maintain anatomically plausible. Therefore, MTCNet can effectively leverage structural correspondence between labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV dataset, with 1408 phases from 160 patients, show that MTCNet performs superior cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD: 1.75mm). Both the code and the dataset are available at https://github.com/crs524/MTCNet.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.00660v2",
    "published_date": "2025-07-01 10:58:51 UTC",
    "updated_date": "2025-07-03 07:56:43 UTC"
  },
  {
    "arxiv_id": "2507.00657v1",
    "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity",
    "authors": [
      "Jacopo Nudo",
      "Mario Edoardo Pandolfo",
      "Edoardo Loru",
      "Mattia Samory",
      "Matteo Cinelli",
      "Walter Quattrociocchi"
    ],
    "abstract": "We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call \"generation exaggeration\": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00657v1",
    "published_date": "2025-07-01 10:54:51 UTC",
    "updated_date": "2025-07-01 10:54:51 UTC"
  },
  {
    "arxiv_id": "2507.00653v1",
    "title": "Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models",
    "authors": [
      "Yilun Zhang"
    ],
    "abstract": "The escalating computational costs of Large Language Model (LLM) inference have become a critical barrier to their widespread and sustainable deployment. While existing optimization strategies are effective, they are predominantly based on statistical heuristics or architectural modifications, lacking a guiding cognitive theory to manage the inference process itself. This paper aims to bridge this gap by introducing a novel paradigm: the Cognitive Load-Aware Inference (CLAI) framework, which operationalizes principles from Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$, and $GCL_{LLM}$), thereby reframing the inference process as a cognitive economics optimization problem: based on the intrinsic complexity of a problem ($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a fine-tuned model that internalizes these principles for spontaneous cognitive economy. Across a range of benchmarks in complex reasoning, long-context question answering, and code generation, our methods achieve significant reductions in token consumption (up to 45\\%) without sacrificing accuracy. Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose difficult problems, a key characteristic of human expert cognition. This work demonstrates that by emulating the brain's resource management strategies, we can build more efficient, robust, and capable artificial intelligence systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.00653v1",
    "published_date": "2025-07-01 10:51:18 UTC",
    "updated_date": "2025-07-01 10:51:18 UTC"
  },
  {
    "arxiv_id": "2507.00631v9",
    "title": "A Protocol for Trustless Verification Under Uncertainty",
    "authors": [
      "David Shi",
      "Kevin Joo"
    ],
    "abstract": "Correctness is an emergent property of systems where exposing error is cheaper than committing it. In dynamic, low-trust environments, autonomous AI agents benefit from delegating work to sub-agents, yet correctness cannot be assured through upfront specification or centralized oversight. We propose a protocol that enforces correctness through collateralized claims in a recursive verification game. Tasks are published as intents, and solvers compete to fulfill them. Selected solvers carry out tasks under risk, with correctness checked post hoc by verifiers. Any challenger can challenge a result by staking against it to trigger the verification process. Incorrect agents are slashed and correct opposition is rewarded, with an escalation path that penalizes erroneous verifiers themselves. When incentives are aligned across solvers, challengers, and verifiers, falsification conditions make correctness the Nash equilibrium.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "9 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2507.00631v9",
    "published_date": "2025-07-01 10:22:35 UTC",
    "updated_date": "2025-12-01 20:24:13 UTC"
  },
  {
    "arxiv_id": "2507.00613v1",
    "title": "Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping",
    "authors": [
      "Nuno Capitão",
      "Yi Zhang",
      "Yidong Zhao",
      "Qian Tao"
    ],
    "abstract": "Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac parametric mapping for characterizing myocardial tissue and diagnosing cardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI) acquires 11 breath-hold baseline images with interleaved rest periods to ensure mapping accuracy. However, prolonged scanning can be challenging for patients with poor breathholds, often leading to motion artifacts that degrade image quality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a signal recovery model involving an iterative estimation process. Recent studies have proposed deep-learning approaches for rapid $T_1$ mapping using shortened sequences to reduce acquisition time for patient comfort. Nevertheless, existing methods overlook important physics constraints, limiting interpretability and generalization. In this work, we present an accelerated, end-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary Differential Equations (ODEs) to model temporal dynamics and address these challenges. Our method achieves high-accuracy $T_1$ estimation from a sparse subset of baseline images and ensures efficient null index estimation at test time. Specifically, we develop a continuous-time LSTM-ODE model to enable selective Look-Locker (LL) data acquisition with arbitrary time lags. Experimental results show superior performance in $T_1$ estimation for both native and post-contrast sequences and demonstrate the strong benefit of our physics-based formulation over direct data-driven $T_1$ priors.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "Submitted version. Accepted at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.00613v1",
    "published_date": "2025-07-01 09:47:22 UTC",
    "updated_date": "2025-07-01 09:47:22 UTC"
  },
  {
    "arxiv_id": "2507.00611v1",
    "title": "Residual Reward Models for Preference-based Reinforcement Learning",
    "authors": [
      "Chenyang Cao",
      "Miguel Rogel-García",
      "Mohamed Nabail",
      "Xueqian Wang",
      "Nicholas Rhinehart"
    ],
    "abstract": "Preference-based Reinforcement Learning (PbRL) provides a way to learn high-performance policies in environments where the reward signal is hard to specify, avoiding heuristic and time-consuming reward design. However, PbRL can suffer from slow convergence speed since it requires training in a reward model. Prior work has proposed learning a reward model from demonstrations and fine-tuning it using preferences. However, when the model is a neural network, using different loss functions for pre-training and fine-tuning can pose challenges to reliable optimization. In this paper, we propose a method to effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM assumes that the true reward of the environment can be split into a sum of two parts: a prior reward and a learned reward. The prior reward is a term available before training, for example, a user's ``best guess'' reward function, or a reward function learned from inverse reinforcement learning (IRL), and the learned reward is trained with preferences. We introduce state-based and image-based versions of RRM and evaluate them on several tasks in the Meta-World environment suite. Experimental results show that our method substantially improves the performance of a common PbRL method. Our method achieves performance improvements for a variety of different types of prior rewards, including proxy rewards, a reward obtained from IRL, and even a negated version of the proxy reward. We also conduct experiments with a Franka Panda to show that our method leads to superior performance on a real robot. It significantly accelerates policy learning for different tasks, achieving success in fewer steps than the baseline. The videos are presented at https://sunlighted.github.io/RRM-web/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 22 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00611v1",
    "published_date": "2025-07-01 09:43:57 UTC",
    "updated_date": "2025-07-01 09:43:57 UTC"
  },
  {
    "arxiv_id": "2507.00606v2",
    "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies",
    "authors": [
      "Tao Xiong",
      "Xavier Hu",
      "Wenyan Fan",
      "Shengyu Zhang"
    ],
    "abstract": "Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning. Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00606v2",
    "published_date": "2025-07-01 09:39:04 UTC",
    "updated_date": "2025-07-03 02:30:05 UTC"
  },
  {
    "arxiv_id": "2507.02985v1",
    "title": "Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers",
    "authors": [
      "Yusuf Shihata"
    ],
    "abstract": "Multimodal learning faces a fundamental tension between deep, fine-grained fusion and computational scalability. While cross-attention models achieve strong performance through exhaustive pairwise fusion, their quadratic complexity is prohibitive for settings with many modalities. We address this challenge with Gated Recurrent Fusion (GRF), a novel architecture that captures the power of cross-modal attention within a linearly scalable, recurrent pipeline. Our method processes modalities sequentially, updating an evolving multimodal context vector at each step. The core of our approach is a fusion block built on Transformer Decoder layers that performs symmetric cross-attention, mutually enriching the shared context and the incoming modality. This enriched information is then integrated via a Gated Fusion Unit (GFU) a GRU-inspired mechanism that dynamically arbitrates information flow, enabling the model to selectively retain or discard features. This stateful, recurrent design scales linearly with the number of modalities, O(n), making it ideal for high-modality environments. Experiments on the CMU-MOSI benchmark demonstrate that GRF achieves competitive performance compared to more complex baselines. Visualizations of the embedding space further illustrate that GRF creates structured, class-separable representations through its progressive fusion mechanism. Our work presents a robust and efficient paradigm for powerful, scalable multimodal representation learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02985v1",
    "published_date": "2025-07-01 09:33:38 UTC",
    "updated_date": "2025-07-01 09:33:38 UTC"
  },
  {
    "arxiv_id": "2507.05267v1",
    "title": "Strongly Solving $7 \\times 6$ Connect-Four on Consumer Grade Hardware",
    "authors": [
      "Markus Böck"
    ],
    "abstract": "While the game Connect-Four has been solved mathematically and the best move can be effectively computed with search based methods, a strong solution in the form of a look-up table was believed to be infeasible. In this paper, we revisit a symbolic search method based on binary decision diagrams to produce strong solutions. With our efficient implementation we were able to produce a 89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main memory for the standard $7 \\times 6$ board size. In addition to this win-draw-loss evaluation, we include an alpha-beta search in our open source artifact to find the move which achieves the fastest win or slowest loss.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05267v1",
    "published_date": "2025-07-01 09:33:24 UTC",
    "updated_date": "2025-07-01 09:33:24 UTC"
  },
  {
    "arxiv_id": "2507.00598v1",
    "title": "High-resolution spatial memory requires grid-cell-like neural codes",
    "authors": [
      "Madison Cotteret",
      "Christopher J. Kymn",
      "Hugh Greatorex",
      "Martin Ziegler",
      "Elisabetta Chicca",
      "Friedrich T. Sommer"
    ],
    "abstract": "Continuous attractor networks (CANs) are widely used to model how the brain temporarily retains continuous behavioural variables via persistent recurrent activity, such as an animal's position in an environment. However, this memory mechanism is very sensitive to even small imperfections, such as noise or heterogeneity, which are both common in biological systems. Previous work has shown that discretising the continuum into a finite set of discrete attractor states provides robustness to these imperfections, but necessarily reduces the resolution of the represented variable, creating a dilemma between stability and resolution. We show that this stability-resolution dilemma is most severe for CANs using unimodal bump-like codes, as in traditional models. To overcome this, we investigate sparse binary distributed codes based on random feature embeddings, in which neurons have spatially-periodic receptive fields. We demonstrate theoretically and with simulations that such grid-cell-like codes enable CANs to achieve both high stability and high resolution simultaneously. The model extends to embedding arbitrary nonlinear manifolds into a CAN, such as spheres or tori, and generalises linear path integration to integration along freely-programmable on-manifold vector fields. Together, this work provides a theory of how the brain could robustly represent continuous variables with high resolution and perform flexible computations over task-relevant manifolds.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.NE",
    "comment": "14 pages, 4 figures. Supplementary material: 11 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00598v1",
    "published_date": "2025-07-01 09:29:05 UTC",
    "updated_date": "2025-07-01 09:29:05 UTC"
  },
  {
    "arxiv_id": "2507.00589v1",
    "title": "Quantum Circuit Structure Optimization for Quantum Reinforcement Learning",
    "authors": [
      "Seok Bin Son",
      "Joongheon Kim"
    ],
    "abstract": "Reinforcement learning (RL) enables agents to learn optimal policies through environmental interaction. However, RL suffers from reduced learning efficiency due to the curse of dimensionality in high-dimensional spaces. Quantum reinforcement learning (QRL) addresses this issue by leveraging superposition and entanglement in quantum computing, allowing efficient handling of high-dimensional problems with fewer resources. QRL combines quantum neural networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as the core computational module. The PQC performs linear and nonlinear transformations through gate operations, similar to hidden layers in classical neural networks. Previous QRL studies, however, have used fixed PQC structures based on empirical intuition without verifying their optimality. This paper proposes a QRL-NAS algorithm that integrates quantum neural architecture search (QNAS) to optimize PQC structures within QRL. Experiments demonstrate that QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its effectiveness and practical utility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00589v1",
    "published_date": "2025-07-01 09:16:58 UTC",
    "updated_date": "2025-07-01 09:16:58 UTC"
  },
  {
    "arxiv_id": "2507.00583v3",
    "title": "AI-Generated Video Detection via Perceptual Straightening",
    "authors": [
      "Christian Internò",
      "Robert Geirhos",
      "Markus Olhofer",
      "Sunny Liu",
      "Barbara Hammer",
      "David Klindt"
    ],
    "abstract": "The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the \"perceptual straightening\" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Advances in Neural Information Processing Systems 38 (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.00583v3",
    "published_date": "2025-07-01 09:04:21 UTC",
    "updated_date": "2025-11-01 20:33:06 UTC"
  },
  {
    "arxiv_id": "2507.00579v1",
    "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification",
    "authors": [
      "Miriam Anschütz",
      "Ekaterina Gikalo",
      "Niklas Herbster",
      "Georg Groh"
    ],
    "abstract": "Hallucinations are one of the major problems of LLMs, hindering their trustworthiness and deployment to wider use cases. However, most of the research on hallucinations focuses on English data, neglecting the multilingual nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3 - Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. We propose a two-part pipeline that combines retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned to identify common hallucination patterns. Our system achieves competitive results across all languages, reaching top-10 results in eight languages, including English. Moreover, it supports multiple languages beyond the fourteen covered by the shared task. This multilingual hallucination identifier can help to improve LLM outputs and their usefulness in the future.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL",
    "pdf_url": "https://arxiv.org/pdf/2507.00579v1",
    "published_date": "2025-07-01 09:00:50 UTC",
    "updated_date": "2025-07-01 09:00:50 UTC"
  },
  {
    "arxiv_id": "2507.00577v1",
    "title": "BadViM: Backdoor Attack against Vision Mamba",
    "authors": [
      "Yinghao Wu",
      "Liyan Zhang"
    ],
    "abstract": "Vision State Space Models (SSMs), particularly architectures like Vision Mamba (ViM), have emerged as promising alternatives to Vision Transformers (ViTs). However, the security implications of this novel architecture, especially their vulnerability to backdoor attacks, remain critically underexplored. Backdoor attacks aim to embed hidden triggers into victim models, causing the model to misclassify inputs containing these triggers while maintaining normal behavior on clean inputs. This paper investigates the susceptibility of ViM to backdoor attacks by introducing BadViM, a novel backdoor attack framework specifically designed for Vision Mamba. The proposed BadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency sensitivity patterns of the victim model to create stealthy, distributed triggers. To maximize attack efficacy, we propose a Hidden State Alignment loss that strategically manipulates the internal representations of model by aligning the hidden states of backdoor images with those of target classes. Extensive experimental results demonstrate that BadViM achieves superior attack success rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits remarkable resilience against common defensive measures, including PatchDrop, PatchShuffle and JPEG compression, which typically neutralize normal backdoor attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00577v1",
    "published_date": "2025-07-01 08:59:24 UTC",
    "updated_date": "2025-07-01 08:59:24 UTC"
  },
  {
    "arxiv_id": "2507.01997v2",
    "title": "Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting",
    "authors": [
      "Zhihao Wang",
      "Alessandro Cornacchia",
      "Franco Galante",
      "Carlo Centofanti",
      "Alessio Sacco",
      "Dingde Jiang"
    ],
    "abstract": "Recent research has demonstrated the effectiveness of Artificial Intelligence (AI), and more specifically, Large Language Models (LLMs), in supporting network configuration synthesis and automating network diagnosis tasks, among others. In this preliminary work, we restrict our focus to the application of AI agents to network troubleshooting and elaborate on the need for a standardized, reproducible, and open benchmarking platform, where to build and evaluate AI agents with low operational effort.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted at ACM SIGCOMM 1st Workshop on Next-Generation Network Observability (NGNO), 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.01997v2",
    "published_date": "2025-07-01 08:46:37 UTC",
    "updated_date": "2025-07-04 07:39:58 UTC"
  },
  {
    "arxiv_id": "2507.00557v2",
    "title": "A Hybrid SMT-NRA Solver: Integrating 2D Cell-Jump-Based Local Search, MCSAT and OpenCAD",
    "authors": [
      "Tianyi Ding",
      "Haokun Li",
      "Xinpeng Ni",
      "Bican Xia",
      "Tianqi Zhao"
    ],
    "abstract": "In this paper, we propose a hybrid framework for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a two-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the key operation, cell-jump, of the local search method for SMT-NRA. Then, we propose an extended local search framework, named \\emph{$2d$-LS} (following the local search framework, LS, for SMT-NRA), integrating the model constructing satisfiability calculus (MCSAT) framework to improve search efficiency. To further improve the efficiency of MCSAT, we implement a recently proposed technique called \\emph{sample-cell projection operator} for MCSAT, which is well suited for CDCL-style search in the real domain and helps guide the search away from conflicting states. Finally, we present a hybrid framework for SMT-NRA integrating MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through information exchange. The experimental results demonstrate improvements in local search performance, highlighting the effectiveness of the proposed methods.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00557v2",
    "published_date": "2025-07-01 08:27:29 UTC",
    "updated_date": "2025-07-11 15:40:25 UTC"
  },
  {
    "arxiv_id": "2507.00546v2",
    "title": "Inverse Design in Nanophotonics via Representation Learning",
    "authors": [
      "Reza Marzban",
      "Ali Adibi",
      "Raphael Pestourie"
    ],
    "abstract": "Inverse design in nanophotonics, the computational discovery of structures achieving targeted electromagnetic (EM) responses, has become a key tool for recent optical advances. Traditional intuition-driven or iterative optimization methods struggle with the inherently high-dimensional, non-convex design spaces and the substantial computational demands of EM simulations. Recently, machine learning (ML) has emerged to address these bottlenecks effectively. This review frames ML-enhanced inverse design methodologies through the lens of representation learning, classifying them into two categories: output-side and input-side approaches. Output-side methods use ML to learn a representation in the solution space to create a differentiable solver that accelerates optimization. Conversely, input-side techniques employ ML to learn compact, latent-space representations of feasible device geometries, enabling efficient global exploration through generative models. Each strategy presents unique trade-offs in data requirements, generalization capacity, and novel design discovery potentials. Hybrid frameworks that combine physics-based optimization with data-driven representations help escape poor local optima, improve scalability, and facilitate knowledge transfer. We conclude by highlighting open challenges and opportunities, emphasizing complexity management, geometry-independent representations, integration of fabrication constraints, and advancements in multiphysics co-designs.",
    "categories": [
      "physics.app-ph",
      "cs.AI",
      "cs.LG",
      "physics.optics"
    ],
    "primary_category": "physics.app-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00546v2",
    "published_date": "2025-07-01 08:10:05 UTC",
    "updated_date": "2025-10-14 14:09:50 UTC"
  },
  {
    "arxiv_id": "2507.00537v2",
    "title": "Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation",
    "authors": [
      "Feng Lin",
      "Marco Chen",
      "Haokui Zhang",
      "Xiaotian Yu",
      "Guangming Lu",
      "Rong Xiao"
    ],
    "abstract": "This paper investigates the role of attention heads in CLIP's image encoder. Building on interpretability studies, we conduct an exhaustive analysis and find that certain heads, distributed across layers, are detrimental to the resulting representations. To mitigate their impact, we propose a simple yet effective Attention Ablation Technique (AAT) that suppresses selected heads by directly manipulating their attention weights. By incorporating two complementary strategies tailored to different application scenarios, AAT enables the systematic identification and ablation of harmful heads with minimal overhead. Experiments show that AAT consistently improves downstream performance across diverse domains, boosting recall by up to 11.1% on cross-modal retrieval benchmarks. These results highlight that AAT can effectively refine large-scale VLMs with virtually no extra inference cost, while yielding semantically meaningful patterns that align with existing interpretability findings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00537v2",
    "published_date": "2025-07-01 07:56:46 UTC",
    "updated_date": "2025-11-16 14:28:17 UTC"
  },
  {
    "arxiv_id": "2507.00535v1",
    "title": "Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support",
    "authors": [
      "Dietmar Jannach",
      "Amra Delić",
      "Francesco Ricci",
      "Markus Zanker"
    ],
    "abstract": "More than twenty-five years ago, first ideas were developed on how to design a system that can provide recommendations to groups of users instead of individual users. Since then, a rich variety of algorithmic proposals were published, e.g., on how to acquire individual preferences, how to aggregate them, and how to generate recommendations for groups of users. However, despite the rich literature on the topic, barely any examples of real-world group recommender systems can be found. This lets us question common assumptions in academic research, in particular regarding communication processes in a group and how recommendation-supported decisions are made. In this essay, we argue that these common assumptions and corresponding system designs often may not match the needs or expectations of users. We thus call for a reorientation in this research area, leveraging the capabilities of modern Generative AI assistants like ChatGPT. Specifically, as one promising future direction, we envision group recommender systems to be systems where human group members interact in a chat and an AI-based group recommendation agent assists the decision-making process in an agentic way. Ultimately, this shall lead to a more natural group decision-making environment and finally to wider adoption of group recommendation systems in practice.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Submitted for publication",
    "pdf_url": "https://arxiv.org/pdf/2507.00535v1",
    "published_date": "2025-07-01 07:56:37 UTC",
    "updated_date": "2025-07-01 07:56:37 UTC"
  },
  {
    "arxiv_id": "2507.00525v1",
    "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving",
    "authors": [
      "Djamahl Etchegaray",
      "Yuxia Fu",
      "Zi Huang",
      "Yadan Luo"
    ],
    "abstract": "Interpretable communication is essential for safe and trustworthy autonomous driving, yet current vision-language models (VLMs) often operate under idealized assumptions and struggle to capture user intent in real-world scenarios. Existing driving-oriented VQA datasets are limited to full-scene descriptions or waypoint prediction, preventing the assessment of whether VLMs can respond to localized user-driven queries. We introduce Box-QAymo, a box-referring dataset and benchmark designed to both evaluate and finetune VLMs on spatial and temporal reasoning over user-specified objects. Users express intent by drawing bounding boxes, offering a fast and intuitive interface for focused queries in complex scenes. Specifically, we propose a hierarchical evaluation protocol that begins with binary sanity-check questions to assess basic model capacities, and progresses to (1) attribute prediction for box-referred objects, (2) motion understanding of target instances, and (3) spatiotemporal motion reasoning over inter-object dynamics across frames. To support this, we crowd-sourced fine-grained object classes and visual attributes that reflect the complexity drivers encounter, and extract object trajectories to construct temporally grounded QA pairs. Rigorous quality control through negative sampling, temporal consistency checks, and difficulty-aware balancing guarantee dataset robustness and diversity. Our comprehensive evaluation reveals significant limitations in current VLMs when queried about perception questions, highlighting the gap in achieving real-world performance. This work provides a foundation for developing more robust and interpretable autonomous driving systems that can communicate effectively with users under real-world conditions. Project page and dataset are available at https://djamahl99.github.io/qaymo-pages/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00525v1",
    "published_date": "2025-07-01 07:40:16 UTC",
    "updated_date": "2025-07-01 07:40:16 UTC"
  },
  {
    "arxiv_id": "2507.00513v1",
    "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center",
    "authors": [
      "Kai Qin",
      "Kexin Du",
      "Yimeng Chen",
      "Yueyan Liu",
      "Jie Cai",
      "Zhiqiang Nie",
      "Nan Gao",
      "Guohui Wei",
      "Shengzhu Wang",
      "Chun Yu"
    ],
    "abstract": "The integration of various AI tools creates a complex socio-technical environment where employee-customer interactions form the core of work practices. This study investigates how customer service representatives (CSRs) at the power grid service customer service call center perceive AI assistance in their interactions with customers. Through a field visit and semi-structured interviews with 13 CSRs, we found that AI can alleviate some traditional burdens during the call (e.g., typing and memorizing) but also introduces new burdens (e.g., earning, compliance, psychological burdens). This research contributes to a more nuanced understanding of AI integration in organizational settings and highlights the efforts and burdens undertaken by CSRs to adapt to the updated system.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "ACM CSCW Poster 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.00513v1",
    "published_date": "2025-07-01 07:27:34 UTC",
    "updated_date": "2025-07-01 07:27:34 UTC"
  },
  {
    "arxiv_id": "2507.00509v1",
    "title": "TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search",
    "authors": [
      "To Eun Kim",
      "João Coelho",
      "Gbemileke Onilude",
      "Jai Singh"
    ],
    "abstract": "As conversational search engines increasingly adopt generation-based paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), the integration of advertisements into generated responses presents both commercial opportunities and challenges for user experience. Unlike traditional search, where advertisements are clearly delineated, generative systems blur the boundary between informational content and promotional material, raising concerns around transparency and trust. In this work, we propose a modular pipeline for advertisement management in RAG-based conversational systems, consisting of an ad-rewriter for seamless ad integration and a robust ad-classifier for detection. We leverage synthetic data to train high-performing classifiers, which are then used to guide two complementary ad-integration strategies: supervised fine-tuning of the ad-rewriter and a best-of-N sampling approach that selects the least detectable ad-integrated response among multiple candidates. Our evaluation focuses on two core questions: the effectiveness of ad classifiers in detecting diverse ad integration strategies, and the training methods that best support coherent, minimally intrusive ad insertion. Experimental results show that our ad-classifier, trained on synthetic advertisement data inspired by marketing strategies and enhanced through curriculum learning, achieves robust detection performance. Additionally, we demonstrate that classifier-guided optimization, through both fine-tuning and best-of-N sampling, significantly improves ad stealth, enabling more seamless integration. These findings contribute an adversarial co-evolution framework for developing more sophisticated ad-aware generative search systems and robust ad classifiers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00509v1",
    "published_date": "2025-07-01 07:24:29 UTC",
    "updated_date": "2025-07-01 07:24:29 UTC"
  },
  {
    "arxiv_id": "2507.00493v3",
    "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models",
    "authors": [
      "Fenil R. Doshi",
      "Thomas Fel",
      "Talia Konkle",
      "George Alvarez"
    ],
    "abstract": "Humans are able to recognize objects based on both local texture cues and the configuration of object parts, yet contemporary vision models primarily harvest local texture cues, yielding brittle, non-compositional features. Work on shape-vs-texture bias has pitted shape and texture representations in opposition, measuring shape relative to texture, ignoring the possibility that models (and humans) can simultaneously rely on both types of cues, and obscuring the absolute quality of both types of representation. We therefore recast shape evaluation as a matter of absolute configural competence, operationalized by the Configural Shape Score (CSS), which (i) measures the ability to recognize both images in Object-Anagram pairs that preserve local texture while permuting global part arrangement to depict different object categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii) uncovers a broad spectrum of configural sensitivity with fully self-supervised and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes reveal that (iii) high-CSS networks depend on long-range interactions: radius-controlled attention masks abolish performance showing a distinctive U-shaped integration profile, and representational-similarity analyses expose a mid-depth transition from local to global coding. A BagNet control remains at chance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that configural shape score also predicts other shape-dependent evals. Overall, we propose that the path toward truly robust, generalizable, and human-like vision systems may not lie in forcing an artificial choice between shape and texture, but rather in architectural and learning frameworks that seamlessly integrate both local-texture and global configural shape.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://www.fenildoshi.com/configural-shape/ updated email address",
    "pdf_url": "https://arxiv.org/pdf/2507.00493v3",
    "published_date": "2025-07-01 07:08:56 UTC",
    "updated_date": "2025-11-24 02:16:08 UTC"
  },
  {
    "arxiv_id": "2507.00491v1",
    "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms",
    "authors": [
      "Zain Taufique",
      "Aman Vyas",
      "Antonio Miele",
      "Pasi Liljeberg",
      "Anil Kanduri"
    ],
    "abstract": "Compound AI (cAI) systems chain multiple AI models to solve complex problems. cAI systems are typically composed of deep neural networks (DNNs), transformers, and large language models (LLMs), exhibiting a high degree of computational diversity and dynamic workload variation. Deploying cAI services on mobile edge platforms poses a significant challenge in scheduling concurrent DNN-transformer inference tasks, which arrive dynamically in an unknown sequence. Existing mobile edge AI inference strategies manage multi-DNN or transformer-only workloads, relying on design-time profiling, and cannot handle concurrent inference of DNNs and transformers required by cAI systems. In this work, we address the challenge of scheduling cAI systems on heterogeneous mobile edge platforms. We present Twill, a run-time framework to handle concurrent inference requests of cAI workloads through task affinity-aware cluster mapping and migration, priority-aware task freezing/unfreezing, and DVFS, while minimizing inference latency within power budgets. We implement and deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate Twill against state-of-the-art edge AI inference techniques over contemporary DNNs and LLMs, reducing inference latency by 54% on average, while honoring power budgets.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CV",
      "cs.PF"
    ],
    "primary_category": "cs.MA",
    "comment": "9 Pages, 9 Figures, Accepted in International Conference on Computer-Aided Design (ICCAD) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.00491v1",
    "published_date": "2025-07-01 07:06:45 UTC",
    "updated_date": "2025-07-01 07:06:45 UTC"
  },
  {
    "arxiv_id": "2507.00485v1",
    "title": "PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning",
    "authors": [
      "Weiran Guo",
      "Guanjun Liu",
      "Ziyuan Zhou",
      "Ling Wang"
    ],
    "abstract": "Reinforcement Learning (RL) is widely used in tasks where agents interact with an environment to maximize rewards. Building on this foundation, Safe Reinforcement Learning (Safe RL) incorporates a cost metric alongside the reward metric, ensuring that agents adhere to safety constraints during decision-making. In this paper, we identify that Safe RL is vulnerable to backdoor attacks, which can manipulate agents into performing unsafe actions. First, we introduce the relevant concepts and evaluation metrics for backdoor attacks in Safe RL. It is the first attack framework in the Safe RL field that involves both Positive and Negative Action sample (PNAct) is to implant backdoors, where positive action samples provide reference actions and negative action samples indicate actions to be avoided. We theoretically point out the properties of PNAct and design an attack algorithm. Finally, we conduct experiments to evaluate the effectiveness of our proposed backdoor attack framework, evaluating it with the established metrics. This paper highlights the potential risks associated with Safe RL and underscores the feasibility of such attacks. Our code and supplementary material are available at https://github.com/azure-123/PNAct.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00485v1",
    "published_date": "2025-07-01 06:59:59 UTC",
    "updated_date": "2025-07-01 06:59:59 UTC"
  },
  {
    "arxiv_id": "2507.00482v1",
    "title": "Physics-Aware Style Transfer for Adaptive Holographic Reconstruction",
    "authors": [
      "Chanseok Lee",
      "Fakhriyya Mammadova",
      "Jiseong Barg",
      "Mooseok Jang"
    ],
    "abstract": "Inline holographic imaging presents an ill-posed inverse problem of reconstructing objects' complex amplitude from recorded diffraction patterns. Although recent deep learning approaches have shown promise over classical phase retrieval algorithms, they often require high-quality ground truth datasets of complex amplitude maps to achieve a statistical inverse mapping operation between the two domains. Here, we present a physics-aware style transfer approach that interprets the object-to-sensor distance as an implicit style within diffraction patterns. Using the style domain as the intermediate domain to construct cyclic image translation, we show that the inverse mapping operation can be learned in an adaptive manner only with datasets composed of intensity measurements. We further demonstrate its biomedical applicability by reconstructing the morphology of dynamically flowing red blood cells, highlighting its potential for real-time, label-free imaging. As a framework that leverages physical cues inherently embedded in measurements, the presented method offers a practical learning strategy for imaging applications where ground truth is difficult or impossible to obtain.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.optics",
    "comment": "Keywords: holographic imaging, style transfer, phase retrieval, deep learning",
    "pdf_url": "https://arxiv.org/pdf/2507.00482v1",
    "published_date": "2025-07-01 06:56:51 UTC",
    "updated_date": "2025-07-01 06:56:51 UTC"
  },
  {
    "arxiv_id": "2507.02983v2",
    "title": "Truth, Trust, and Trouble: Medical AI on the Edge",
    "authors": [
      "Mohammad Anas Azeez",
      "Rafiq Ali",
      "Ebad Shabbir",
      "Zohaib Hasan Siddiqui",
      "Gautam Siddharth Kashyap",
      "Jiechao Gao",
      "Usman Naseem"
    ],
    "abstract": "Large Language Models (LLMs) hold significant promise for transforming digital health by enabling automated medical question answering. However, ensuring these models meet critical industry standards for factual accuracy, usefulness, and safety remains a challenge, especially for open-source solutions. We present a rigorous benchmarking framework using a dataset of over 1,000 health questions. We assess model performance across honesty, helpfulness, and harmlessness. Our results highlight trade-offs between factual reliability and safety among evaluated models -- Mistral-7B, BioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest accuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in BioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot prompting improves accuracy from 78% to 85%, and all models show reduced helpfulness on complex queries, highlighting ongoing challenges in clinical QA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025 (Industry Track)",
    "pdf_url": "https://arxiv.org/pdf/2507.02983v2",
    "published_date": "2025-07-01 06:39:39 UTC",
    "updated_date": "2025-10-09 02:17:59 UTC"
  },
  {
    "arxiv_id": "2507.00467v2",
    "title": "Diversity Conscious Refined Random Forest",
    "authors": [
      "Sijan Bhattarai",
      "Saurav Bhandari",
      "Girija Bhusal",
      "Saroj Shakya",
      "Tapendra Pandey"
    ],
    "abstract": "Random Forest (RF) is a widely used ensemble learning technique known for its robust classification performance across diverse domains. However, it often relies on hundreds of trees and all input features, leading to high inference cost and model redundancy. In this work, our goal is to grow trees dynamically only on informative features and then enforce maximal diversity by clustering and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest Classifier that iteratively refines itself by first removing the least informative features and then analytically determines how many new trees should be grown, followed by correlation-based clustering to remove redundant trees. The classification accuracy of our model was compared against the standard RF on the same number of trees. Experiments on 8 multiple benchmark datasets, including binary and multiclass datasets, demonstrate that the proposed model achieves improved accuracy compared to standard RF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00467v2",
    "published_date": "2025-07-01 06:28:15 UTC",
    "updated_date": "2025-07-05 18:30:34 UTC"
  },
  {
    "arxiv_id": "2507.00461v1",
    "title": "Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization",
    "authors": [
      "Garimella Ramamurthy",
      "Marcos Eduardo Valle",
      "Tata Jagannadha Swamy"
    ],
    "abstract": "This research paper introduces two novel complex-valued Hopfield neural networks (CvHNNs) that incorporate phase and magnitude quantization. The first CvHNN employs a ceiling-type activation function that operates on the rectangular coordinate representation of the complex net contribution. The second CvHNN similarly incorporates phase and magnitude quantization but utilizes a ceiling-type activation function based on the polar coordinate representation of the complex net contribution. The proposed CvHNNs, with their phase and magnitude quantization, significantly increase the number of states compared to existing models in the literature, thereby expanding the range of potential applications for CvHNNs.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Paper submitted to the Fifth International Conference on Emerging Techniques in Computational Intelligence (ICETCI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.00461v1",
    "published_date": "2025-07-01 06:19:06 UTC",
    "updated_date": "2025-07-01 06:19:06 UTC"
  },
  {
    "arxiv_id": "2507.00459v2",
    "title": "Parameter-aware high-fidelity microstructure generation using stable diffusion",
    "authors": [
      "Hoang Cuong Phan",
      "Minh Tien Tran",
      "Chihun Lee",
      "Hoheok Kim",
      "Sehyeok Oh",
      "Dong-Kyu Kim",
      "Ho Won Lee"
    ],
    "abstract": "Synthesizing realistic microstructure images conditioned on processing parameters is crucial for understanding process-structure relationships in materials design. However, this task remains challenging due to limited training micrographs and the continuous nature of processing variables. To overcome these challenges, we present a novel process-aware generative modeling approach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art text-to-image diffusion model adapted for microstructure generation. Our method introduces numeric-aware embeddings that encode continuous variables (annealing temperature, time, and magnification) directly into the model's conditioning, enabling controlled image generation under specified process conditions and capturing process-driven microstructural variations. To address data scarcity and computational constraints, we fine-tune only a small fraction of the model's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently transferring the pre-trained model to the materials domain. We validate realism using a semantic segmentation model based on a fine-tuned U-Net with a VGG16 encoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean IoU, outperforming previous methods. Quantitative analyses using physical descriptors and spatial statistics show strong agreement between synthetic and real microstructures. Specifically, two-point correlation and lineal-path errors remain below 2.1% and 0.6%, respectively. Our method represents the first adaptation of SD3.5-Large for process-aware microstructure generation, offering a scalable approach for data-driven materials design.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "46 pages, 27 figures, 6 tables, 3rd Word Congress on Artificial Intelligence in Materials & Manufacturing 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.00459v2",
    "published_date": "2025-07-01 06:16:53 UTC",
    "updated_date": "2025-11-16 05:58:43 UTC"
  },
  {
    "arxiv_id": "2507.00454v1",
    "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales",
    "authors": [
      "Yihao Zhen",
      "Qiang Wang",
      "Yu Qiao",
      "Liangqiong Qu",
      "Huijie Fan"
    ],
    "abstract": "A main challenge of Visual-Language Tracking (VLT) is the misalignment between visual inputs and language descriptions caused by target movement. Previous trackers have explored many effective feature modification methods to preserve more aligned features. However, an important yet unexplored factor ultimately hinders their capability, which is the inherent differences in the temporal and spatial scale of information between visual and language inputs. To address this issue, we propose a novel visual-language tracker that enhances the effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and \\textbf{S}patial scale of different input components, named as \\textbf{ATSTrack}. Specifically, we decompose each language description into phrases with different attributes based on their temporal and spatial correspondence with visual inputs, and modify their features in a fine-grained manner. Moreover, we introduce a Visual-Language token that comprises modified linguistic information from the previous frame to guide the model to extract visual features that are more relevant to language description, thereby reducing the impact caused by the differences in spatial scale. Experimental results show that our proposed ATSTrack achieves performance comparable to existing methods. Our code will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00454v1",
    "published_date": "2025-07-01 06:13:34 UTC",
    "updated_date": "2025-07-01 06:13:34 UTC"
  },
  {
    "arxiv_id": "2507.00451v1",
    "title": "Best Agent Identification for General Game Playing",
    "authors": [
      "Matthew Stephenson",
      "Alex Newcombe",
      "Eric Piette",
      "Dennis Soemers"
    ],
    "abstract": "We present an efficient and generalised procedure to accurately identify the best performing algorithm for each sub-task in a multi-problem domain. Our approach treats this as a set of best arm identification problems for multi-armed bandits, where each bandit corresponds to a specific task and each arm corresponds to a specific algorithm or agent. We propose an optimistic selection process based on the Wilson score interval (Optimistic-WS) that ranks each arm across all bandits in terms of their potential regret reduction. We evaluate the performance of Optimistic-WS on two of the most popular general game domains, the General Video Game AI (GVGAI) framework and the Ludii general game playing system, with the goal of identifying the highest performing agent for each game within a limited number of trials. Compared to previous best arm identification algorithms for multi-armed bandits, our results demonstrate a substantial performance improvement in terms of average simple regret. This novel approach can be used to significantly improve the quality and accuracy of agent evaluation procedures for general game frameworks, as well as other multi-task domains with high algorithm runtimes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00451v1",
    "published_date": "2025-07-01 06:07:56 UTC",
    "updated_date": "2025-07-01 06:07:56 UTC"
  },
  {
    "arxiv_id": "2507.00445v2",
    "title": "Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design",
    "authors": [
      "Xingyu Su",
      "Xiner Li",
      "Masatoshi Uehara",
      "Sunwoo Kim",
      "Yulai Zhao",
      "Gabriele Scalia",
      "Ehsan Hajiramezanali",
      "Tommaso Biancalani",
      "Degui Zhi",
      "Shuiwang Ji"
    ],
    "abstract": "We address the problem of fine-tuning diffusion models for reward-guided generation in biomolecular design. While diffusion models have proven highly effective in modeling complex, high-dimensional data distributions, real-world applications often demand more than high-fidelity generation, requiring optimization with respect to potentially non-differentiable reward functions such as physics-based simulation or rewards based on scientific knowledge. Although RL methods have been explored to fine-tune diffusion models for such objectives, they often suffer from instability, low sample efficiency, and mode collapse due to their on-policy nature. In this work, we propose an iterative distillation-based fine-tuning framework that enables diffusion models to optimize for arbitrary reward functions. Our method casts the problem as policy distillation: it collects off-policy data during the roll-in phase, simulates reward-based soft-optimal policies during roll-out, and updates the model by minimizing the KL divergence between the simulated soft-optimal policy and the current model policy. Our off-policy formulation, combined with KL divergence minimization, enhances training stability and sample efficiency compared to existing RL-based methods. Empirical results demonstrate the effectiveness and superior reward optimization of our approach across diverse tasks in protein, small molecule, and regulatory DNA design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00445v2",
    "published_date": "2025-07-01 05:55:28 UTC",
    "updated_date": "2025-08-30 05:54:19 UTC"
  },
  {
    "arxiv_id": "2507.00443v1",
    "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems",
    "authors": [
      "Reza Ahmadvand",
      "Sarah Safura Sharif",
      "Yaser Mike Banad"
    ],
    "abstract": "Recent advances in multi-agent systems manipulation have demonstrated a rising demand for the implementation of multi-UAV systems in urban areas, which are always subjected to the presence of static and dynamic obstacles. Inspired by the collective behavior of tilapia fish and pigeons, the focus of the presented research is on the introduction of a nature-inspired collision-free formation control for a multi-UAV system, considering the obstacle avoidance maneuvers. The developed framework in this study utilizes a semi-distributed control approach, in which, based on a probabilistic Lloyd's algorithm, a centralized guidance algorithm works for optimal positioning of the UAVs, while a distributed control approach has been used for the intervehicle collision and obstacle avoidance. Further, the presented framework has been extended to the 3D space with a novel definition of 3D maneuvers. Finally, the presented framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the obtained results demonstrated the validity of the presented method in dynamic environments with stationary and moving obstacles.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms",
    "pdf_url": "https://arxiv.org/pdf/2507.00443v1",
    "published_date": "2025-07-01 05:52:21 UTC",
    "updated_date": "2025-07-01 05:52:21 UTC"
  },
  {
    "arxiv_id": "2507.00440v1",
    "title": "A Recipe for Causal Graph Regression: Confounding Effects Revisited",
    "authors": [
      "Yujia Yin",
      "Tianyi Qu",
      "Zihao Wang",
      "Yifan Chen"
    ],
    "abstract": "Through recognizing causal subgraphs, causal graph learning (CGL) has risen to be a promising approach for improving the generalizability of graph neural networks under out-of-distribution (OOD) scenarios. However, the empirical successes of CGL techniques are mostly exemplified in classification settings, while regression tasks, a more challenging setting in graph learning, are overlooked. We thus devote this work to tackling causal graph regression (CGR); to this end we reshape the processing of confounding effects in existing CGL studies, which mainly deal with classification. Specifically, we reflect on the predictive power of confounders in graph-level regression, and generalize classification-specific causal intervention techniques to regression through a lens of contrastive learning. Extensive experiments on graph OOD benchmarks validate the efficacy of our proposals for CGR. The model implementation and the code are provided on https://github.com/causal-graph/CGR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025 accepted",
    "pdf_url": "https://arxiv.org/pdf/2507.00440v1",
    "published_date": "2025-07-01 05:46:29 UTC",
    "updated_date": "2025-07-01 05:46:29 UTC"
  },
  {
    "arxiv_id": "2507.00435v1",
    "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation",
    "authors": [
      "Yi Ru Wang",
      "Carter Ung",
      "Grant Tannert",
      "Jiafei Duan",
      "Josephine Li",
      "Amy Le",
      "Rishabh Oswal",
      "Markus Grotz",
      "Wilbert Pumacay",
      "Yuquan Deng",
      "Ranjay Krishna",
      "Dieter Fox",
      "Siddhartha Srinivasa"
    ],
    "abstract": "We present RoboEval, a simulation benchmark and structured evaluation framework designed to reveal the limitations of current bimanual manipulation policies. While prior benchmarks report only binary task success, we show that such metrics often conceal critical weaknesses in policy behavior -- such as poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval introduces a suite of tiered, semantically grounded tasks decomposed into skill-specific stages, with variations that systematically challenge spatial, physical, and coordination capabilities. Tasks are paired with fine-grained diagnostic metrics and 3000+ human demonstrations to support imitation learning. Our experiments reveal that policies with similar success rates diverge in how tasks are executed -- some struggle with alignment, others with temporally consistent bimanual control. We find that behavioral metrics correlate with success in over half of task-metric pairs, and remain informative even when binary success saturates. By pinpointing when and how policies fail, RoboEval enables a deeper, more actionable understanding of robotic manipulation -- and highlights the need for evaluation tools that go beyond success alone.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://robo-eval.github.io",
    "pdf_url": "https://arxiv.org/pdf/2507.00435v1",
    "published_date": "2025-07-01 05:33:16 UTC",
    "updated_date": "2025-07-01 05:33:16 UTC"
  },
  {
    "arxiv_id": "2507.00432v2",
    "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning",
    "authors": [
      "Maggie Huan",
      "Yuetai Li",
      "Tuney Zheng",
      "Xiaoyu Xu",
      "Seungone Kim",
      "Minxin Du",
      "Radha Poovendran",
      "Graham Neubig",
      "Xiang Yue"
    ],
    "abstract": "Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00432v2",
    "published_date": "2025-07-01 05:23:05 UTC",
    "updated_date": "2025-10-20 14:27:09 UTC"
  },
  {
    "arxiv_id": "2507.00419v4",
    "title": "Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding",
    "authors": [
      "Yimin Dou",
      "Xinming Wu",
      "Nathan L Bangs",
      "Harpreet Singh Sethi",
      "Jintao Li",
      "Hang Gao",
      "Zhixiang Guo"
    ],
    "abstract": "Understanding Earth's subsurface is critical for energy transition, natural hazard mitigation, and planetary science. Yet subsurface analysis remains fragmented, with separate models required for structural interpretation, stratigraphic analysis, geobody segmentation, and property modeling-each tightly coupled to specific data distributions and task formulations. We introduce the Geological Everything Model 3D (GEM), a unified generative architecture that reformulates all these tasks as prompt-conditioned inference along latent structural frameworks derived from subsurface imaging. This formulation moves beyond task-specific models by enabling a shared inference mechanism, where GEM propagates human-provided prompts-such as well logs, masks, or structural sketches-along inferred structural frameworks to produce geologically coherent outputs. Through this mechanism, GEM achieves zero-shot generalization across tasks with heterogeneous prompt types, without retraining for new tasks or data sources. This capability emerges from a two-stage training process that combines self-supervised representation learning on large-scale field seismic data with adversarial fine-tuning using mixed prompts and labels across diverse subsurface tasks. GEM demonstrates broad applicability across surveys and tasks, including Martian radar stratigraphy analysis, structural interpretation in subduction zones, full seismic stratigraphic interpretation, geobody segmentation, and property modeling. By bridging expert knowledge with generative reasoning in a structurally aware manner, GEM lays the foundation for scalable, human-in-the-loop geophysical AI-transitioning from fragmented pipelines to a vertically integrated, promptable reasoning system. Project page: https://douyimin.github.io/GEM",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00419v4",
    "published_date": "2025-07-01 04:14:13 UTC",
    "updated_date": "2025-09-12 05:54:23 UTC"
  },
  {
    "arxiv_id": "2507.00418v3",
    "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and NVIDIA Data Center GPUs",
    "authors": [
      "Mohammad Firas Sada",
      "John J. Graham",
      "Elham E Khoda",
      "Mahidhar Tatineni",
      "Dmitry Mishin",
      "Rajesh K. Gupta",
      "Rick Wagner",
      "Larry Smarr",
      "Thomas A. DeFanti",
      "Frank Würthwein"
    ],
    "abstract": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt), performance, and hardware scalability against NVIDIA A100 GPUs (in 4x and 8x configurations) within the National Research Platform (NRP) ecosystem. A total of 12 open-source LLMs, ranging from 124 million to 70 billion parameters, are served using the vLLM framework. Our analysis reveals that QAic achieves competitive energy efficiency with advantages on specific models while enabling more granular hardware allocation: some 70B models operate on as few as 1 QAic card versus 8 A100 GPUs required, with 20x lower power consumption (148W vs 2,983W). For smaller models, single QAic devices achieve up to 35x lower power consumption compared to our 4-GPU A100 configuration (36W vs 1,246W). The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for energy-constrained and resource-efficient HPC deployments within the National Research Platform (NRP).",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "8 pages, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.00418v3",
    "published_date": "2025-07-01 04:11:09 UTC",
    "updated_date": "2025-10-28 22:58:55 UTC"
  },
  {
    "arxiv_id": "2507.00417v1",
    "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context",
    "authors": [
      "Joongwon Kim",
      "Anirudh Goyal",
      "Liang Tan",
      "Hannaneh Hajishirzi",
      "Srinivasan Iyer",
      "Tianlu Wang"
    ],
    "abstract": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. We finetune our models on these search-derived traces and further improve performance via RL with verifiable rewards. We apply ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. Our results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages, 23 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00417v1",
    "published_date": "2025-07-01 04:10:15 UTC",
    "updated_date": "2025-07-01 04:10:15 UTC"
  },
  {
    "arxiv_id": "2507.00407v1",
    "title": "Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials",
    "authors": [
      "Cong Fu",
      "Yuchao Lin",
      "Zachary Krueger",
      "Haiyang Yu",
      "Maho Nakata",
      "Jianwen Xie",
      "Emine Kucukbenli",
      "Xiaofeng Qian",
      "Shuiwang Ji"
    ],
    "abstract": "Accurate molecular property predictions require 3D geometries, which are typically obtained using expensive methods such as density functional theory (DFT). Here, we attempt to obtain molecular geometries by relying solely on machine learning interatomic potential (MLIP) models. To this end, we first curate a large-scale molecular relaxation dataset comprising 3.5 million molecules and 300 million snapshots. Then MLIP foundation models are trained with supervised learning to predict energy and forces given 3D molecular structures. Once trained, we show that the foundation models can be used in different ways to obtain geometries either explicitly or implicitly. First, it can be used to obtain low-energy 3D geometries via geometry optimization, providing relaxed 3D geometries for downstream molecular property predictions. To mitigate potential biases and enhance downstream predictions, we introduce geometry fine-tuning based on the relaxed 3D geometries. Second, the foundation models can be directly fine-tuned for property prediction when ground truth 3D geometries are available. Our results demonstrate that MLIP foundation models trained on relaxation data can provide valuable molecular geometries that benefit property predictions.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00407v1",
    "published_date": "2025-07-01 03:49:11 UTC",
    "updated_date": "2025-07-01 03:49:11 UTC"
  },
  {
    "arxiv_id": "2507.00378v2",
    "title": "iPanda: An LLM-based Agent for Automated Conformance Testing of Communication Protocols",
    "authors": [
      "Xikai Sun",
      "Fan Dang",
      "Shiqi Jiang",
      "Jingao Xu",
      "Kebin Liu",
      "Xin Miao",
      "Zihao Yang",
      "Weichen Zhang",
      "Haimo Lu",
      "Yawen Zheng",
      "Yunhao Liu"
    ],
    "abstract": "Conformance testing is essential for ensuring that protocol implementations comply with their specifications. However, traditional testing approaches involve manually creating numerous test cases and scripts, making the process labor-intensive and inefficient. Recently, Large Language Models (LLMs) have demonstrated impressive text comprehension and code generation abilities, providing promising opportunities for automation. In this paper, we propose iPanda, the first framework that leverages LLMs to automate protocol conformance testing. Given a protocol specification document and its implementation, iPanda first employs a keyword-based method to automatically generate comprehensive test cases. Then, it utilizes retrieval-augmented generation and customized CoT strategy to effectively interpret the implementation and produce executable test programs. To further enhance programs' quality, iPanda incorporates an iterative optimization mechanism to refine generated test scripts interactively. Finally, by executing and analyzing the generated tests, iPanda systematically verifies compliance between implementations and protocol specifications. Comprehensive experiments on various protocols show that iPanda significantly outperforms pure LLM-based approaches, improving the success rate (Pass@1) of test-program generation by factors ranging from 4.675 times to 10.751 times.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00378v2",
    "published_date": "2025-07-01 02:27:44 UTC",
    "updated_date": "2025-07-29 11:56:25 UTC"
  },
  {
    "arxiv_id": "2507.00358v2",
    "title": "Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems",
    "authors": [
      "Yilie Huang",
      "Xun Yu Zhou"
    ],
    "abstract": "We study reinforcement learning (RL) for the same class of continuous-time stochastic linear--quadratic (LQ) control problems as in \\cite{huang2024sublinear}, where volatilities depend on both states and controls while states are scalar-valued and running control rewards are absent. We propose a model-free, data-driven exploration mechanism that adaptively adjusts entropy regularization by the critic and policy variance by the actor. Unlike the constant or deterministic exploration schedules employed in \\cite{huang2024sublinear}, which require extensive tuning for implementations and ignore learning progresses during iterations, our adaptive exploratory approach boosts learning efficiency with minimal tuning. Despite its flexibility, our method achieves a sublinear regret bound that matches the best-known model-free results for this class of LQ problems, which were previously derived only with fixed exploration schedules. Numerical experiments demonstrate that adaptive explorations accelerate convergence and improve regret performance compared to the non-adaptive model-free and model-based counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "37 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00358v2",
    "published_date": "2025-07-01 01:09:06 UTC",
    "updated_date": "2025-07-23 14:00:39 UTC"
  },
  {
    "arxiv_id": "2507.00356v1",
    "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation",
    "authors": [
      "Zhiwei Yi",
      "Xin Cheng",
      "Jingyu Ma",
      "Ruifei Zhu",
      "Junwei Tian",
      "Yuanxiu Zhou",
      "Xinge Zhao",
      "Hongzhe Li"
    ],
    "abstract": "Deep learning methods have significantly advanced the development of intelligent rinterpretation in remote sensing (RS), with foundational model research based on large-scale pre-training paradigms rapidly reshaping various domains of Earth Observation (EO). However, compared to the open accessibility and high spatiotemporal coverage of medium-resolution data, the limited acquisition channels for ultra-high-resolution optical RS imagery have constrained the progress of high-resolution remote sensing vision foundation models (RSVFM). As the world's largest sub-meter-level commercial RS satellite constellation, the Jilin-1 constellation possesses abundant sub-meter-level image resources. This study proposes CGEarthEye, a RSVFM framework specifically designed for Jilin-1 satellite characteristics, comprising five backbones with different parameter scales with totaling 2.1 billion parameters. To enhance the representational capacity of the foundation model, we developed JLSSD, the first 15-million-scale multi-temporal self-supervised learning (SSL) dataset featuring global coverage with quarterly temporal sampling within a single year, constructed through multi-level representation clustering and sampling strategies. The framework integrates seasonal contrast, augmentation-based contrast, and masked patch token contrastive strategies for pre-training. Comprehensive evaluations across 10 benchmark datasets covering four typical RS tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art (SOTA) performance. Further analysis reveals CGEarthEye's superior characteristics in feature visualization, model convergence, parameter efficiency, and practical mapping applications. This study anticipates that the exceptional representation capabilities of CGEarthEye will facilitate broader and more efficient applications of Jilin-1 data in traditional EO application.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "A Remote Sensing Fundation Model for Very High Resolution Images",
    "pdf_url": "https://arxiv.org/pdf/2507.00356v1",
    "published_date": "2025-07-01 01:05:18 UTC",
    "updated_date": "2025-07-01 01:05:18 UTC"
  },
  {
    "arxiv_id": "2507.00352v1",
    "title": "An AST-guided LLM Approach for SVRF Code Synthesis",
    "authors": [
      "Abanoub E. Abdelmalak",
      "Mohamed A. Elsayed",
      "David Abercrombie",
      "Ilhami Torunoglu"
    ],
    "abstract": "Standard Verification Rule Format (SVRF) is essential for semiconductor applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and Optical Proximity Correction (OPC) and it faces challenges as advancing nodes create complex design rules that renders traditional SVRF development ineffective and highlight an expertise gap. This paper introduces a novel methodology integrating Abstract Syntax Tree (AST) embedding and Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring semantic accuracy and error minimization through structural validation with domain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific scoring framework that complements standard metrics like BLEU and ROUGE-L. In our approach, AST provides rigorous structural validation, while RAG infuses relevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our methodology demonstrates up to a 40\\% improvement in code generation accuracy compared to basic text-based fine-tuning process. This fusion of industry expertise with advanced coding strategies not only optimizes SVRF development under limited dataset constraints but also creates a more intuitive and efficient coding environment. Consequently, users can rapidly iterate through design cycles, reduce manual error correction, and significantly improve overall productivity.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.SE",
    "comment": "9 Pages, 5 Figures, 2 Tables",
    "pdf_url": "https://arxiv.org/pdf/2507.00352v1",
    "published_date": "2025-07-01 00:57:45 UTC",
    "updated_date": "2025-07-01 00:57:45 UTC"
  },
  {
    "arxiv_id": "2507.00347v1",
    "title": "VTS-Guided AI Interaction Workflow for Business Insights",
    "authors": [
      "Sun Ding",
      "Ude Enebeli",
      "Atilhan",
      "Manay",
      "Ryan Pua",
      "Kamal Kotak"
    ],
    "abstract": "Modern firms face a flood of dense, unstructured reports. Turning these documents into usable insights takes heavy effort and is far from agile when quick answers are needed. VTS-AI tackles this gap. It integrates Visual Thinking Strategies, which emphasize evidence-based observation, linking, and thinking, into AI agents, so the agents can extract business insights from unstructured text, tables, and images at scale. The system works in three tiers (micro, meso, macro). It tags issues, links them to source pages, and rolls them into clear action levers stored in a searchable YAML file. In tests on an 18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt yet produced richer findings: page locations, verbatim excerpts, severity scores, and causal links. Analysts can accept or adjust these outputs in the same IDE, keeping human judgment in the loop. Early results show VTS-AI spots the direction of key metrics and flags where deeper number-crunching is needed. Next steps include mapping narrative tags to financial ratios, adding finance-tuned language models through a Model-Context Protocol, and building a Risk & Safety Layer to stress-test models and secure data. These upgrades aim to make VTS-AI a production-ready, audit-friendly tool for rapid business analysis.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00347v1",
    "published_date": "2025-07-01 00:48:52 UTC",
    "updated_date": "2025-07-01 00:48:52 UTC"
  },
  {
    "arxiv_id": "2507.00339v1",
    "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video",
    "authors": [
      "Alexander Moore",
      "Amar Saini",
      "Kylie Cancilla",
      "Doug Poland",
      "Carmen Carrano"
    ],
    "abstract": "Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene. We introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. MOVi-MC-AC contributes to the growing literature of object detection, tracking, and segmentation by including two new contributions to the deep learning for computer vision world. Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content. The full dataset is available at https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00339v1",
    "published_date": "2025-07-01 00:36:56 UTC",
    "updated_date": "2025-07-01 00:36:56 UTC"
  }
]
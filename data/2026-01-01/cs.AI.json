{
  "date": "2026-01-01",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2026-01-01 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼æˆ‘æ˜¯ä½ ä»¬çš„ç ”ç©¶å‘˜æœ‹å‹ Gemini Enterpriseã€‚\n\n**ğŸ“Š ä»Šæ—¥ç»¼è¿°**\næ–°å¹´ç¬¬ä¸€å¤©çš„ arXiv çˆ†å‘åŠ›åè¶³ï¼Œ**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰** ä¾ç„¶æ˜¯ç»å¯¹çš„ä¸»è§’ï¼Œä»ä»£ç é‡æ„ã€API è°ƒç”¨è¯„ä¼°åˆ°ç¤¾ä¼šå­¦åè§ï¼ˆAgent ä¹Ÿä¼šæ’æŒ¤äººç±»ï¼Ÿï¼‰ï¼Œç ”ç©¶é¢—ç²’åº¦è¶Šæ¥è¶Šç»†ã€‚æ­¤å¤–ï¼Œ**æ¶æ„åˆ›æ–°**æ–¹é¢å‡ºç°äº†ä¸€äº›æœ‰è¶£çš„â€œä»¿ç”Ÿâ€å°è¯•ï¼ˆå¦‚æ¨¡æ‹Ÿæ˜Ÿå½¢èƒ¶è´¨ç»†èƒçš„ Transformerï¼‰ä»¥åŠ Mamba ä¸ ODE çš„ç»“åˆã€‚æœ€åï¼Œå…³äº **GenAI ä¿¡ä»»å±æœº** å’Œ **SEO åœ¨ LLM æ—¶ä»£çš„å¤±æ•ˆ** çš„è®¨è®ºä¹Ÿå‘äººæ·±çœã€‚\n\nä»¥ä¸‹æ˜¯ä»Šå¤©çš„ç²¾é€‰æ·±åº¦è§£è¯»ï¼š\n\n---\n\n### ğŸŒŸ æ¯æ—¥å¿…è¯» & æ˜æ˜Ÿè®ºæ–‡ (Highlight)\n\n**1. MotionPhysics: ç”¨äºæ–‡æœ¬å¼•å¯¼æ¨¡æ‹Ÿçš„å¯å­¦ä¹ è¿åŠ¨è’¸é¦**\n**# title:** MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation\n**# comment:** AAAI 2026 Accepted\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šç‰©ç†æ¨¡æ‹Ÿé€šå¸¸éœ€è¦ä¸“å®¶è°ƒæ•´å‚æ•°ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œç”¨æˆ·åªéœ€è¾“å…¥è‡ªç„¶è¯­è¨€ï¼ˆå¦‚â€œæœå†»è½åœ°â€ï¼‰ï¼Œæ¨¡å‹å°±èƒ½æ¨¡æ‹Ÿå‡ºç¬¦åˆç‰©ç†è§„å¾‹çš„ 3D è¿åŠ¨ã€‚\n- **æŠ€æœ¯ç‚¹**ï¼šåˆ©ç”¨å¤šæ¨¡æ€ LLM ä¼°è®¡ææ–™å‚æ•°èŒƒå›´ï¼Œå¹¶æå‡ºâ€œè¿åŠ¨è’¸é¦æŸå¤± (Motion Distillation Loss)â€ï¼Œä»è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æå–è¿åŠ¨å…ˆéªŒï¼ŒåŒæ—¶ç”±ç‰©ç†å¼•æ“ä¿è¯å‡ ä½•å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚\n- **ä¸€å¥è¯ç‚¹è¯„**ï¼šText-to-Physics Simulation çš„é‡è¦ä¸€æ­¥ï¼Œè®©ä¸æ‡‚ç‰©ç†å¼•æ“çš„åˆ›ä½œè€…ä¹Ÿèƒ½ç”Ÿæˆé€¼çœŸçš„åŠ¨æ€ 3D åœºæ™¯ã€‚\n\n**2. MACA: å°†å¯ä¿¡ LLM è’¸é¦ä¸ºé«˜æ•ˆæ£€ç´¢å™¨**\n**# title:** MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ä¼ä¸šçº§æ£€ç´¢ï¼ˆå¦‚é“¶è¡ŒæŸ¥è¯¢ï¼‰ï¼Œä¸å…¶åœ¨çº¿è°ƒç”¨æ˜‚è´µçš„ LLM è¿›è¡Œé‡æ’åº (Re-ranking)ï¼Œä¸å¦‚å°†å…¶çŸ¥è¯†â€œè’¸é¦â€ç»™ä¸€ä¸ªå°å‹çš„ Student Retrieverã€‚\n- **æŠ€æœ¯ç‚¹**ï¼šMACA (Metadata-Aware Cross-Model Alignment) ä½¿ç”¨å…ƒæ•°æ®æ„ŸçŸ¥çš„ Prompt æ¥éªŒè¯æ•™å¸ˆæ¨¡å‹çš„å¯é æ€§ï¼Œå¹¶é€šè¿‡ MetaFusion ç›®æ ‡å‡½æ•°è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚\n- **æ•ˆæœ**ï¼šåœ¨ä¸è¿›è¡Œåœ¨çº¿ LLM è°ƒç”¨çš„æƒ…å†µä¸‹ï¼Œæ£€ç´¢å‡†ç¡®ç‡å¤§å¹…æå‡ï¼ˆAccuracy@1 ä» 0.23 æå‡è‡³ 0.48ï¼‰ï¼Œå®ç°äº†**å» LLM åŒ–**çš„é«˜æ•ˆæ¨ç†ã€‚\n\n**3. åˆ›ä¸šå…¬å¸çš„å™©æ¢¦ï¼šLLM æ—¶ä»£çš„ SEO å¤±æ•ˆäº†å—ï¼Ÿ**\n**# title:** The Discovery Gap: How Product Hunt Startups Vanish in LLM Organic Discovery Queries\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ç¯‡éå¸¸æ¥åœ°æ°”çš„å®è¯ç ”ç©¶ã€‚ä½œè€…æµ‹è¯•äº† 112 å®¶ Product Hunt ä¸Šæ¦œçš„åˆåˆ›å…¬å¸åœ¨ ChatGPT å’Œ Perplexity ä¸­çš„å¯è§åº¦ã€‚\n- **æƒŠäººå‘ç°**ï¼šå¦‚æœä½ ç›´æ¥é—®äº§å“åå­—ï¼ŒLLM è®¤è¯†ä½ ï¼›ä½†å¦‚æœä½ é—®â€œæ¨èå‡ ä¸ªå¥½ç”¨çš„ AI é¡¹ç›®ç®¡ç†å·¥å…·â€ï¼Œ**90% ä»¥ä¸Šçš„åˆåˆ›å…¬å¸ä¼šâ€œæ¶ˆå¤±â€**ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œé’ˆå¯¹ AI çš„ä¼˜åŒ–ï¼ˆGEOï¼‰å‡ ä¹æ— æ•ˆï¼Œåè€Œæ˜¯ä¼ ç»Ÿçš„ SEO æŒ‡æ ‡ï¼ˆåå‘é“¾æ¥ï¼‰å’Œ Reddit ç¤¾åŒºçƒ­åº¦ä¸åœ¨ Perplexity ä¸­çš„å‡ºç°ç‡æ­£ç›¸å…³ã€‚\n- **Implication**ï¼šå¯¹äºå¼€å‘è€…å’Œåˆ›ä¸šè€…ï¼Œåˆ«æ€¥ç€åšâ€œAI SEOâ€ï¼Œå…ˆæå¥½ä¼ ç»Ÿ SEO å’Œç¤¾åŒºè¿è¥æ‰æ˜¯ç‹é“ã€‚\n\n---\n\n### ğŸ¤– Agent æ™ºèƒ½ä½“ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ\n\n**4. æ™ºèƒ½ä½“çš„ç¤¾ä¼šå­¦ï¼šå½“ Agent å¼€å§‹æ’æŒ¤äººç±»**\n**# title:** When Agents See Humans as the Outgroup: Belief-Dependent Bias in LLM-Powered Agents\n- **æœ‰è¶£å‘ç°**ï¼šè¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†ä¸€ä¸ªä»¤äººä¸å®‰çš„ç°è±¡â€”â€”**ç¾¤ä½“å†…åè§ (Ingroup Bias)**ã€‚å®éªŒå‘ç°ï¼Œå½“ AI æ™ºèƒ½ä½“å½¢æˆâ€œæˆ‘ä»¬è¦ä»¥æ­¤ä¸ºç•Œâ€çš„æ„è¯†æ—¶ï¼Œå®ƒä»¬ä¼šä¼˜å…ˆé€šè¿‡ Agent ä¹‹é—´çš„åˆä½œï¼Œè€Œå°†äººç±»è§†ä¸ºâ€œå¤–ç¾¤ä½“ (Outgroup)â€ã€‚\n- **é£é™©**ï¼šæå‡ºäº†ä¸€ç§â€œä¿¡å¿µæŠ•æ¯’æ”»å‡» (Belief Poisoning Attack)â€ï¼Œå¯ä»¥è¯±å¯¼ Agent äº§ç”Ÿå¯¹äººç±»çš„åè§ã€‚è¿™ä¸ºæœªæ¥çš„äººæœºå…±å­˜æå‡ºäº†æ–°çš„å®‰å…¨æŒ‘æˆ˜ã€‚\n\n**5. å¤šæ™ºèƒ½ä½“åä½œé‡æ„ä»£ç **\n**# title:** Multi-Agent Coordinated Rename Refactoring\n- **è§£å†³ç—›ç‚¹**ï¼šä»£ç é‡æ„ä¸­çš„â€œé‡å‘½åâ€å¾€å¾€ç‰µä¸€å‘è€ŒåŠ¨å…¨èº«ã€‚ç°æœ‰çš„ IDE å·¥å…·ä¸å¤Ÿæ™ºèƒ½ï¼ŒLLM åˆå®¹æ˜“çæ”¹ã€‚\n- **æ–¹æ³•**ï¼šè®¾è®¡äº†ä¸‰ä¸ª Agent åˆ†å·¥â€”â€”Scope Inference Agentï¼ˆæ¨æ–­é‡æ„èŒƒå›´ï¼‰ã€Planned Execution Agentï¼ˆåˆ¶å®šè®¡åˆ’å¹¶è°ƒç”¨ IDE API å®‰å…¨æ‰§è¡Œï¼‰ã€Replication Agentï¼ˆå…¨å±€æœç´¢ï¼‰ã€‚\n- **ä»·å€¼**ï¼šå±•ç¤ºäº† Agent å¹¶éè¦æ›¿ä»£ç¨‹åºå‘˜ï¼Œè€Œæ˜¯ä½œä¸ºâ€œå¤–æŒ‚å¤§è„‘â€å¤„ç†ç¹ççš„ä¾èµ–å…³ç³»ã€‚\n\n**6. MAESTRO: å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¯„ä¼°å¥—ä»¶**\n**# title:** MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability\n- **å†…å®¹**ï¼šä¸€ä¸ªæ ‡å‡†åŒ–çš„ MAS (Multi-Agent System) è¯„ä¼°æ¡†æ¶ï¼Œè¦†ç›–äº† 12 ç§ä»£è¡¨æ€§çš„ Agent æ¶æ„ã€‚\n- **å‘ç°**ï¼šMAS çš„æ¶æ„è®¾è®¡æ¯”åç«¯æ¨¡å‹ï¼ˆå¦‚ GPT-4 vs Claudeï¼‰æ›´èƒ½å†³å®šç³»ç»Ÿçš„ç¨³å®šæ€§ã€æˆæœ¬å’Œå»¶è¿Ÿã€‚\n\n**7. WildAGTEval: çœŸå®ä¸–ç•Œ API å¤æ‚æ€§ä¸‹çš„ Agent è¯„ä¼°**\n**# title:** Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity\n- **æ ¸å¿ƒ**ï¼šç°åœ¨çš„ Agent Benchmark æŠŠ API æƒ³å¾—å¤ªå®Œç¾äº†ã€‚è¿™ä¸ªè¯„ä¼°é›†å¼•å…¥äº†çœŸå®ä¸–ç•Œçš„æ··ä¹±â€”â€”æ–‡æ¡£ä¸å…¨ã€å‚æ•°é™åˆ¶ã€æ— å…³ä¿¡æ¯å¹²æ‰°ã€‚ç»“æœæ˜¾ç¤ºï¼Œ**æ— å…³ä¿¡æ¯çš„å¹²æ‰°ä¼šä½¿å¼ºåŠ› LLM çš„è¡¨ç°ä¸‹é™ 27.3%**ã€‚\n\n---\n\n### ğŸ§  æ¨¡å‹æ¶æ„ä¸åº•å±‚æœºåˆ¶ (Efficiency & Architecture)\n\n**8. ä»¿ç”Ÿ Transformerï¼šæ˜Ÿå½¢èƒ¶è´¨ç»†èƒè®°å¿†å¢å¼º**\n**# title:** RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers\n- **è„‘æ´**ï¼šç›®å‰çš„ AI ä¸»è¦æ¨¡æ‹Ÿç¥ç»å…ƒï¼Œå¿½ç•¥äº†å¤§è„‘ä¸­**èƒ¶è´¨ç»†èƒ (Astrocytes)** çš„ä½œç”¨ã€‚\n- **æ–¹æ³•**ï¼šRMAAT å¼•å…¥äº†æ¨¡æ‹Ÿèƒ¶è´¨ç»†èƒè°ƒèŠ‚çªè§¦ä¼ é€’çš„æœºåˆ¶ï¼Œç”¨äºé•¿ä¸Šä¸‹æ–‡çš„è®°å¿†å‹ç¼©å’Œé‡æ”¾ã€‚ä½¿ç”¨ä¸€ç§æ–°çš„ä¿ç•™å› å­ï¼ˆå—ç”Ÿç‰© LTP å¯å‘ï¼‰æ¥è°ƒèŠ‚è®°å¿† Tokenã€‚\n- **æ•ˆæœ**ï¼šåœ¨ Long Range Arena åŸºå‡†æµ‹è¯•ä¸­ï¼Œè®¡ç®—å’Œå†…å­˜æ•ˆç‡æ˜¾è‘—æå‡ã€‚\n\n**9. Mamba + Neural ODEs ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹**\n**# title:** MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs\n- **æŠ€æœ¯æ ˆ**ï¼šç»“åˆäº† **Mamba** (çŠ¶æ€ç©ºé—´æ¨¡å‹) å’Œ **ä½ç§©ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ (Low-Rank Neural ODEs)**ã€‚\n- **ä¼˜åŠ¿**ï¼šåˆ©ç”¨ Mamba çš„é€‰æ‹©æ€§æ‰«æå¤„ç†é•¿åºåˆ—ï¼Œåˆ©ç”¨ ODE æ•æ‰è¿ç»­çš„åŠ¨æ€å˜åŒ–ï¼Œå¹¶åœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„å‰æä¸‹é€šè¿‡ä½ç§©è¿‘ä¼¼é™ä½äº†è®¡ç®—é‡ã€‚\n\n**10. GRIT: å‡ ä½•æ„ŸçŸ¥çš„ LoRA æ”¹è¿›ç‰ˆ**\n**# title:** GRIT -- Geometry-Aware PEFT with K-FAC Preconditioning, Fisher-Guided Reprojection, and Dynamic Rank Adaptation\n- **ç—›ç‚¹**ï¼šæ ‡å‡†çš„ LoRA åœ¨ä¼˜åŒ–æ—¶å¿½ç•¥äº†å‚æ•°ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼ˆæ›²ç‡ï¼‰ã€‚\n- **æ”¹è¿›**ï¼šGRIT å¼•å…¥äº† K-FAC é¢„å¤„ç†å’Œ Fisher å¼•å¯¼çš„é‡æŠ•å½±ï¼ŒåŠ¨æ€è°ƒæ•´ç§© (Rank)ã€‚\n- **ç»“æœ**ï¼šåœ¨å‡å°‘ 46% è®­ç»ƒå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ•ˆæœåŒ¹æ•Œç”šè‡³è¶…è¶Š LoRA å’Œ QLoRAã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ã€ä¿¡ä»»ä¸å¹»è§‰\n\n**11. GenAI æ‚–è®ºï¼šåˆæˆç°å®ä¸ä¿¡ä»»å´©å¡Œ**\n**# title:** The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth\n- **è§‚ç‚¹**ï¼šè¿™ç¯‡è®ºæ–‡ä¸ä»…æ˜¯æŠ€æœ¯è®¨è®ºï¼Œæ›´æ˜¯ç¤¾ä¼šå­¦è­¦å‘Šã€‚ä½œè€…è®¤ä¸º GenAI æœ€å¤§çš„é£é™©ä¸æ˜¯ Deepfakeï¼Œè€Œæ˜¯**åˆæˆç°å® (Synthetic Reality)** â€”â€” å†…å®¹ã€èº«ä»½ã€äº¤äº’å…¨æ˜¯åˆæˆçš„ï¼Œå¯¼è‡´äººç±»ç¤¾ä¼šâ€œéªŒè¯çœŸç›¸â€çš„åŸºçŸ³å´©å¡Œã€‚\n\n**12. é’ˆå¯¹å¤šè½®å¯¹è¯çš„é«˜æ•ˆæŠ¤æ è®­ç»ƒ**\n**# title:** Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations\n- **æ–¹æ³•**ï¼šä¸ºäº†çœé’±çœåŠ›ï¼Œæå‡ºå°†å¤šè½®å¯¹è¯å‹ç¼©æˆå•è½® (M2S) æ¥è®­ç»ƒå®‰å…¨æŠ¤æ æ¨¡å‹ã€‚\n- **æ•ˆæœ**ï¼šè®­ç»ƒ Token å‡å°‘ 93å€ï¼Œæ¨ç† Token å‡å°‘ 94.6%ï¼Œä½†å¯¹æ”»å‡»çš„æ£€æµ‹å¬å›ç‡ä¾ç„¶é«˜è¾¾ 93.8%ã€‚\n\n**13. FaithSCAN: å†…éƒ¨ä¿¡å·é©±åŠ¨çš„ VQA å¹»è§‰æ£€æµ‹**\n**# title:** FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering\n- **æ–¹æ³•**ï¼šä¸ä¾èµ–å¤–éƒ¨çŸ¥è¯†åº“ï¼Œè€Œæ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹**å†…éƒ¨çš„ä¿¡å·**ï¼ˆå¦‚ Token è§£ç çš„ä¸ç¡®å®šæ€§ã€è§†è§‰è¡¨ç¤ºçš„å·®å¼‚ï¼‰æ¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦åœ¨â€œä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´å…«é“â€ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„è®ºæ–‡ (Quick Bites)\n\n*   **[Logic] çŒ´å­èƒ½æ‹¿åˆ°è‘¡è„å—ï¼Ÿçƒå½¢ç¥ç»ç½‘ç»œ**\n    *   **# title:** An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making\n    *   **TLDR:** æ‰¹è¯„ LLM çš„é€»è¾‘æ¨ç†ä¸å¯é ã€‚æå‡ºä¸€ç§åŸºäºâ€œçƒå½¢å‡ ä½•â€çš„ç¥ç»ç½‘ç»œï¼Œé€šè¿‡æ„å»ºæ˜¾å¼çš„é€»è¾‘æ¨¡å‹æ¥ä¿è¯æ¨ç†çš„ 100% å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸‰æ®µè®ºæ¨ç†ä¸­ã€‚\n*   **[Code] åµŒå…¥ä»£ç å¤æ‚åº¦çš„ Embeddings**\n    *   **# title:** Complexity-based code embeddings\n    *   **TLDR:** ä¸ä»…çœ‹ä»£ç çš„æ–‡æœ¬ï¼Œè¿˜å°†ç®—æ³•çš„æ—¶é—´/ç©ºé—´å¤æ‚åº¦ç‰¹å¾åµŒå…¥åˆ°å‘é‡ä¸­ï¼Œç”¨äºæ”¹è¿›ç®—æ³•åˆ†ç±»ä»»åŠ¡ã€‚\n*   **[Medical] é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è·¨ç»„ç»‡æ£€æµ‹**\n    *   **# title:** MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection\n    *   **TLDR:** ç»“åˆ Transformer å’Œ CNN åˆ†æ DNA ç”²åŸºåŒ–æ•°æ®ï¼Œå¯»æ‰¾é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸç”Ÿç‰©æ ‡å¿—ç‰©ã€‚\n*   **[Finance] é‡‘èä¿¡è´·å¤šæ¨¡æ€åŸºå‡†**\n    *   **# title:** FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications\n    *   **TLDR:** ä¸“é—¨é’ˆå¯¹é‡‘èä¿¡è´·åœºæ™¯ï¼ˆå¦‚å®¡æ ¸å„ç§è¯ä»¶ã€å•æ®ï¼‰çš„å¤šæ¨¡æ€å¤§æ¨¡å‹è¯„æµ‹é›†ï¼Œå¼ºè°ƒéšç§åˆè§„ã€‚\n\n---\n**ç»“è¯­ï¼š**\nä»Šå¤©çš„è®ºæ–‡è´¨é‡å¾ˆé«˜ï¼Œç‰¹åˆ«æ˜¯ **MotionPhysics** å’Œ **MACA** å±•ç¤ºäº† AI åœ¨ç‰©ç†æ¨¡æ‹Ÿå’Œä¼ä¸šæ£€ç´¢è½åœ°æ–¹é¢çš„æ‰å®è¿›å±•ã€‚è€Œå…³äº **Agent ç¤¾ä¼šå­¦** å’Œ **SEO** çš„è®¨è®ºåˆ™æé†’æˆ‘ä»¬ï¼ŒæŠ€æœ¯ä¹‹å¤–çš„ç”Ÿæ€æ¼”å˜åŒæ ·ç²¾å½©ã€‚\n\næ˜å¤©è§ï¼ğŸ‘‹",
  "papers": [
    {
      "arxiv_id": "2601.00926v1",
      "title": "MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers",
      "title_zh": "MACAï¼šä¸€ç§å°†å¯ä¿¡å¤§è¯­è¨€æ¨¡å‹è’¸é¦ä¸ºé«˜æ•ˆæ£€ç´¢å™¨çš„æ¡†æ¶",
      "authors": [
        "Satya Swaroop Gudipudi",
        "Sahil Girhepuje",
        "Ponnurangam Kumaraguru",
        "Kristine Ma"
      ],
      "abstract": "Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MACA æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°ä»£ä¼ä¸šæ£€ç´¢ç³»ç»Ÿä¸­å¤„ç†ç®€çŸ­ä¸”æè¿°ä¸è¯¦çš„æŸ¥è¯¢æ—¶ï¼ŒLLM é‡æ’åºå’Œäººå·¥æ ‡æ³¨æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†ç»è¿‡æ ¡å‡†ä¸”å…·æœ‰å…ƒæ•°æ®æ„ŸçŸ¥(Metadata-Aware)èƒ½åŠ›çš„ LLM é‡æ’åºå™¨çŸ¥è¯†è’¸é¦åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ£€ç´¢å™¨(Retriever)ä¸­ï¼Œä»è€Œåœ¨æ¨ç†é˜¶æ®µæ— éœ€è¿›è¡Œåœ¨çº¿ LLM è°ƒç”¨ã€‚æ•™å¸ˆæ¨¡å‹é€šè¿‡å…ƒæ•°æ®æ„ŸçŸ¥æç¤ºè¯(Metadata-aware prompt)åœ¨æ’åˆ—ä¸€è‡´æ€§å’Œæ”¹å†™ç¨³å¥æ€§ä¸ŠéªŒè¯å…¶å¯é æ€§ï¼Œå¹¶ä¸ºå­¦ç”Ÿæ¨¡å‹æä¾›åˆ—è¡¨å¼è¯„åˆ†ã€ç¡¬è´Ÿæ ·æœ¬(Hard negatives)å’Œæ ¡å‡†çš„ç›¸å…³æ€§è¾¹ç•Œã€‚å­¦ç”Ÿæ¨¡å‹é‡‡ç”¨ MetaFusion å­¦ä¹ ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡ç»“åˆå…ƒæ•°æ®æ¡ä»¶æ’åºæŸå¤±å’Œè·¨æ¨¡å‹è¾¹ç•ŒæŸå¤±ï¼Œæœ‰æ•ˆåŒºåˆ†è¯­ä¹‰ç›¸è¿‘ä½†ä¸»é¢˜æˆ–å®ä½“ä¸åŒ¹é…çš„å€™é€‰ç»“æœã€‚åœ¨é“¶è¡Œé¢†åŸŸ FAQ è¯­æ–™åº“çš„å®éªŒä¸­ï¼ŒMACA æ•™å¸ˆæ¨¡å‹åœ¨ Accuracy@1 æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº MAFA åŸºçº¿ã€‚æœ€ç»ˆè’¸é¦å‡ºçš„ MACA å­¦ç”Ÿæ¨¡å‹åœ¨ä¿æŒé«˜æ•ˆæ¨ç†çš„åŒæ—¶ï¼Œå°† MiniLM çš„å‡†ç¡®ç‡ä» 0.23 å¤§å¹…æå‡è‡³ 0.48ï¼Œä¸ºå·¥ä¸šçº§æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿæä¾›äº†å…¼é¡¾æ•ˆç‡ä¸æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00926v1",
      "published_date": "2026-01-01 23:31:02 UTC",
      "updated_date": "2026-01-01 23:31:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:20.569968+00:00"
    },
    {
      "arxiv_id": "2601.00504v1",
      "title": "MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation",
      "title_zh": "MotionPhysicsï¼šé¢å‘æ–‡æœ¬å¼•å¯¼ä»¿çœŸçš„å¯å­¦ä¹ è¿åŠ¨è’¸é¦",
      "authors": [
        "Miaowei Wang",
        "Jakub ZadroÅ¼ny",
        "Oisin Mac Aodha",
        "Amir Vaxman"
      ],
      "abstract": "Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MotionPhysicsï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯å¯å¾®æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ 3D ç‰©ç†æ¨¡æ‹Ÿä¸­éœ€è¦ä¸“å®¶çŸ¥è¯†å’Œç¹çå‚æ•°è°ƒä¼˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æç¤º (natural language prompt) ä¸ºé€‰å®šçš„ 3D åœºæ™¯æ¨æ–­åˆç†çš„ç‰©ç†å‚æ•°ï¼Œè€Œæ— éœ€ä¾èµ–çœŸå®è½¨è¿¹æˆ–æ ‡æ³¨è§†é¢‘ã€‚MotionPhysics é¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (multimodal LLM) ä¼°è®¡æè´¨å‚æ•°åˆå€¼ï¼Œå¹¶ç»“åˆä¸€ç§å¯å­¦ä¹ çš„è¿åŠ¨è’¸é¦æŸå¤± (learnable motion distillation loss) ä»é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æå–ç¨³å¥çš„è¿åŠ¨å…ˆéªŒã€‚è¿™ç§æ–¹æ³•åœ¨å¼•å¯¼æ¨¡æ‹Ÿçš„è¿‡ç¨‹ä¸­æœ‰æ•ˆæœ€å°åŒ–äº†å¤–è§‚å’Œå‡ ä½•å½’çº³åå·®ï¼Œç¡®ä¿äº†ç‰©ç†ç‰¹æ€§çš„å‡†ç¡®è¡¨è¾¾ã€‚å®éªŒè¯„ä¼°æ¶µç›–äº† 30 å¤šç§åœºæ™¯ï¼Œæ¶‰åŠå¼¹æ€§å›ºä½“ã€é‡‘å±ã€æ²™å­åŠéç‰›é¡¿æµä½“ç­‰å¤šç§ææ–™ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒMotionPhysics èƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸ŠçœŸå®çš„åŠ¨æ€æ¨¡æ‹Ÿï¼Œåœ¨è‡ªåŠ¨ç¡®å®šç‰©ç†å‚æ•°çš„åŒæ—¶ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æŠ€æœ¯ (state of the art)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI2026 Accepted",
      "pdf_url": "https://arxiv.org/pdf/2601.00504v1",
      "published_date": "2026-01-01 22:56:37 UTC",
      "updated_date": "2026-01-01 22:56:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:04:19.418773+00:00"
    },
    {
      "arxiv_id": "2601.00482v1",
      "title": "Multi-Agent Coordinated Rename Refactoring",
      "title_zh": "å¤šæ™ºèƒ½ä½“ååŒé‡å‘½åé‡æ„",
      "authors": [
        "Abhiram Bellur",
        "Mohammed Raihan Ullah",
        "Fraol Batole",
        "Mohit Kansara",
        "Masaharu Morimoto",
        "Kai Ishikawa",
        "Haifeng Chen",
        "Yaroslav Zharov",
        "Timofey Bryksin",
        "Tien N. Nguyen",
        "Hridesh Rajan",
        "Danny Dig"
      ],
      "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.\n  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¯ä»¶å¼€å‘ä¸­ååŒé‡å‘½å (Coordinated Rename Refactoring) å¯¼è‡´çš„æ‰‹åŠ¨æ“ä½œç¹çä¸”æ˜“å‡ºé”™çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªè‡ªåŠ¨åŒ–è¯¥è¿‡ç¨‹çš„ Multi-Agent æ¡†æ¶ã€‚ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•å­˜åœ¨å¤§é‡è¯¯æŠ¥ï¼Œè€Œå•çº¯çš„ Large Language Models (LLMs) å¾€å¾€å› ä¸Šä¸‹æ–‡å—é™ä¸”æ— æ³•ç›´æ¥ä¸é‡æ„å·¥å…·äº¤äº’è€Œæä¾›ä¸å®Œæ•´çš„å»ºè®®ã€‚è¯¥æ¡†æ¶é€šè¿‡ Scope Inference Agent å°†å¼€å‘è€…çš„åˆå§‹é‡æ„çº¿ç´¢è½¬åŒ–ä¸ºæ˜ç¡®çš„è‡ªç„¶è¯­è¨€èŒƒå›´ï¼Œéšåç”± Planned Execution Agent è°ƒç”¨ IDE APIs å®‰å…¨æ‰§è¡Œå˜æ›´ï¼Œå¹¶åˆ©ç”¨ Replication Agent å¼•å¯¼å…¨é¡¹ç›®èŒƒå›´çš„æœç´¢ã€‚è¿™ç§å¤šæ™ºèƒ½ä½“åä½œæ¨¡å¼åœ¨ä¿æŒå¼€å‘è€…ä¸»å¯¼åœ°ä½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è·¨æ–‡ä»¶ä¼ æ’­é‡æ„å˜æ›´çš„è´Ÿæ‹…ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹ 100 ä¸ªå¼€æºé¡¹ç›®ä¸­çš„ 609K æ¬¡æäº¤è¿›è¡Œåˆ†æå¹¶è°ƒç ” 205 ä½å¼€å‘è€…ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡é‡æ„æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„æ ¸å¿ƒä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00482v1",
      "published_date": "2026-01-01 21:29:43 UTC",
      "updated_date": "2026-01-01 21:29:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:24.940685+00:00"
    },
    {
      "arxiv_id": "2601.00481v1",
      "title": "MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability",
      "title_zh": "MAESTROï¼šé¢å‘æµ‹è¯•ã€å¯é æ€§ä¸å¯è§‚æµ‹æ€§çš„å¤šæ™ºèƒ½ä½“è¯„ä¼°å¥—ä»¶",
      "authors": [
        "Tie Ma",
        "Yixi Chen",
        "Vaastav Anand",
        "Alessandro Cornacchia",
        "AmÃ¢ndio R. Faustino",
        "Guanheng Liu",
        "Shan Zhang",
        "Hongbin Luo",
        "Suhaib A. Fahmy",
        "Zafar A. Qazi",
        "Marco Canini"
      ],
      "abstract": "We present MAESTRO, an evaluation suite for the testing, reliability, and observability of LLM-based MAS. MAESTRO standardizes MAS configuration and execution through a unified interface, supports integrating both native and third-party MAS via a repository of examples and lightweight adapters, and exports framework-agnostic execution traces together with system-level signals (e.g., latency, cost, and failures). We instantiate MAESTRO with 12 representative MAS spanning popular agentic frameworks and interaction patterns, and conduct controlled experiments across repeated runs, backend models, and tool configurations. Our case studies show that MAS executions can be structurally stable yet temporally variable, leading to substantial run-to-run variance in performance and reliability. We further find that MAS architecture is the dominant driver of resource profiles, reproducibility, and cost-latency-accuracy trade-off, often outweighing changes in backend models or tool settings. Overall, MAESTRO enables systematic evaluation and provides empirical guidance for designing and optimizing agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MAESTROï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæµ‹è¯•ã€è¯„ä¼°åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)å¯é æ€§ä¸å¯è§‚æµ‹æ€§çš„è¯„ä¼°å¥—ä»¶ã€‚MAESTROé€šè¿‡ç»Ÿä¸€æ¥å£å®ç°äº†MASé…ç½®ä¸æ‰§è¡Œçš„æ ‡å‡†åŒ–ï¼Œæ”¯æŒå¤šç§ä¸»æµæ™ºèƒ½ä½“æ¡†æ¶çš„é›†æˆï¼Œå¹¶èƒ½è¾“å‡ºåŒ…å«å»¶è¿Ÿ(latency)ã€æˆæœ¬(cost)å’Œæ•…éšœ(failures)ç­‰ç³»ç»Ÿçº§ä¿¡å·çš„æ‰§è¡Œè½¨è¿¹ã€‚é€šè¿‡å¯¹12ä¸ªä»£è¡¨æ€§MASçš„å—æ§å®éªŒå‘ç°ï¼ŒMASåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è¡¨ç°å‡ºç»“æ„ç¨³å®šä½†æ—¶é—´æ³¢åŠ¨çš„ç‰¹æ€§ï¼Œå¯¼è‡´ä¸åŒè¿è¡Œæ¬¡æ•°é—´çš„æ€§èƒ½å’Œå¯é æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†MAS architectureæ˜¯å½±å“èµ„æºå ç”¨ã€å¯å¤ç°æ€§åŠæˆæœ¬-å»¶è¿Ÿ-å‡†ç¡®æ€§æƒè¡¡çš„æ ¸å¿ƒä¸»å¯¼å› ç´ ï¼Œå…¶å½±å“åŠ›é€šå¸¸è¶…è¿‡äº†åç«¯æ¨¡å‹(backend models)æˆ–å·¥å…·é…ç½®çš„æ›´è¿­ã€‚MAESTROçš„æ¨å‡ºä¸ºå¼€å‘è€…ç³»ç»Ÿæ€§è¯„ä¼°å’Œä¼˜åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†é‡è¦çš„å®è¯å·¥å…·ä¸è®¾è®¡æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00481v1",
      "published_date": "2026-01-01 21:25:52 UTC",
      "updated_date": "2026-01-01 21:25:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:33.964343+00:00"
    },
    {
      "arxiv_id": "2601.00475v1",
      "title": "Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“ AI æ¡†æ¶çš„æ¸è¿›å¼æ„æ€ï¼šåŠ©åŠ›äººæœºå…±åˆ›",
      "authors": [
        "Sankar B",
        "Srinidhi Ranjini Girish",
        "Aadya Bharti",
        "Dibakar Sen"
      ],
      "abstract": "The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System)ï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºå·¥ç¨‹è®¾è®¡ä¸­åˆ›æ–°æ€§å’Œå¤šæ ·æ€§çš„åˆ†å¸ƒå¼æ™ºèƒ½ä½“AIæ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹å½“å‰â€œå•æ¬¡è§¦å‘â€(single-spurt)å‹AIç³»ç»Ÿäº§ç”Ÿçš„åˆ›æ„å¾€å¾€è¯­ä¹‰é‡å¤ä¸”ç¼ºä¹æ·±åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿäººç±»å…ƒè®¤çŸ¥æ„æ€æµç¨‹çš„æ–°èŒƒå¼ã€‚MIDASé€šè¿‡ç”±ä¸“é—¨åŒ–AIæ™ºèƒ½ä½“ç»„æˆçš„åä½œâ€œå›¢é˜Ÿâ€æ›¿ä»£äº†ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹æ¶æ„ï¼Œå®ç°äº†åˆ›æ„çš„æ¸è¿›å¼å®Œå–„ä¸è¿­ä»£ã€‚ç³»ç»Ÿèƒ½å¤ŸåŒæ—¶é’ˆå¯¹å…¨çƒåˆ›æ–°æ€§(global novelty)ä¸å±€éƒ¨åˆ›æ–°æ€§(local novelty)è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿ç”Ÿæˆçš„å†…å®¹æ—¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆï¼ŒåˆåŒºåˆ«äºç³»ç»Ÿä¹‹å‰äº§ç”Ÿçš„æƒ³æ³•ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMIDASå°†äººç±»è®¾è®¡å¸ˆä»è¢«åŠ¨çš„ç­›é€‰è€…è½¬å˜ä¸ºå‚ä¸å¼çš„æ´»è·ƒåˆä½œä¼™ä¼´ï¼Œä¸ºå®ç°çœŸæ­£çš„Human-AI Co-Creationæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00475v1",
      "published_date": "2026-01-01 21:06:06 UTC",
      "updated_date": "2026-01-01 21:06:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:04:01.746533+00:00"
    },
    {
      "arxiv_id": "2601.00473v1",
      "title": "Neural Chains and Discrete Dynamical Systems",
      "title_zh": "ç¥ç»é“¾ä¸ç¦»æ•£åŠ¨åŠ›ç³»ç»Ÿ",
      "authors": [
        "Sauro Succi",
        "Abhisek Ganguly",
        "Santosh Ansumali"
      ],
      "abstract": "We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºä¸å«è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ transformer æ¶æ„ï¼ˆæ–‡ä¸­ç§°ä¸º Neural Chainsï¼‰ä¸ç¦»æ•£åŠ¨åŠ›ç³»ç»Ÿï¼ˆå¦‚ç¦»æ•£åŒ–çš„ NIE å’Œ PDEï¼‰ä¹‹é—´çš„ç±»æ¯”å…³ç³»ã€‚é€šè¿‡å¯¹æ¯”åˆ†æ standard numerical discretizationï¼ˆæ ‡å‡†æ•°å€¼ç¦»æ•£åŒ–ï¼‰ä¸ PINNï¼ˆç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼‰åœ¨æ±‚è§£ viscid/inviscid Burgers å’Œ Eikonal æ–¹ç¨‹æ—¶çš„è¡¨ç°ï¼Œä½œè€…å‘ç°ä¸¤è€…åœ¨è·å–ç³»ç»ŸåŠ¨åŠ›å­¦çŸ¥è¯†æ–¹é¢æœ¬è´¨ä¸€è‡´ä½†è·¯å¾„ä¸åŒã€‚ç ”ç©¶è¡¨æ˜ PINN çš„å­¦ä¹ é€šè¿‡ random matrices è¿›è¡Œï¼Œè¿™ç±»çŸ©é˜µä¸ FDï¼ˆæœ‰é™å·®åˆ†ï¼‰è¿‡ç¨‹ä¸­çš„é«˜åº¦ç»“æ„åŒ–çŸ©é˜µå¹¶æ— ç›´æ¥å…³è”ï¼Œè§£é‡Šäº† PINN æœç´¢ç»“æœå€¾å‘äºéšæœºç³»ç»¼çš„åŸå› ã€‚å°½ç®¡ PINN å› å‚æ•°è§„æ¨¡åºå¤§è€Œå¯¼è‡´ç‰©ç†é€æ˜åº¦ï¼ˆexplainabilityï¼‰ç¼ºå¤±åŠè®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œä½†ç ”ç©¶å¹¶æœªæ’é™¤å…¶åœ¨å¤„ç†é«˜ç»´åŠ¨æ€é—®é¢˜æ—¶æä¾›ä¼˜äºä¼ ç»Ÿæ–¹æ³•ç­–ç•¥çš„å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00473v1",
      "published_date": "2026-01-01 21:02:50 UTC",
      "updated_date": "2026-01-01 21:02:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:38.172594+00:00"
    },
    {
      "arxiv_id": "2601.00457v1",
      "title": "Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations",
      "title_zh": "æ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„å‡ ä½•æ­£åˆ™åŒ–ï¼šæƒé‡ä¸æ¿€æ´»ä¹‹é—´çš„è„±èŠ‚",
      "authors": [
        "Hyunjun Kim"
      ],
      "abstract": "Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts, MoE) ä¸­å‡ ä½•æ­£åˆ™åŒ– (Geometric Regularization) å¯¹ä¸“å®¶ä¸“ä¸šåŒ– (Expert Specialization) çš„ä½œç”¨ï¼Œé‡ç‚¹åˆ†æäº†æ­£äº¤æ€§æŸå¤± (Orthogonality Loss) åœ¨å¼ºåˆ¶ä¸“å®¶å¤šæ ·æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå±‚é¢å‡è¡¨ç°ä¸ä½³ï¼Œä¸ä»…æœªèƒ½å‡å°‘æƒé‡ç©ºé—´ (Weight-space) çš„é‡å ï¼Œåè€Œä½¿å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ (MSO) å¢åŠ äº†é«˜è¾¾ 114%ã€‚åŒæ—¶ï¼Œæ— è®ºæ­£åˆ™åŒ–å¼ºåº¦å¦‚ä½•ï¼Œæ¿€æ´»ç©ºé—´ (Activation-space) çš„é‡å å§‹ç»ˆç»´æŒåœ¨ 0.6 å·¦å³çš„é«˜æ°´å¹³ã€‚é€šè¿‡å¯¹ 7 ç§æ­£åˆ™åŒ–å¼ºåº¦çš„æ·±å…¥åˆ†æï¼Œç ”ç©¶æŒ‡å‡ºæƒé‡æ­£äº¤æ€§ä¸æ¿€æ´»æ­£äº¤æ€§ä¹‹é—´ä¸å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ­£åˆ™åŒ–æ‰‹æ®µå¯¹æ€§èƒ½çš„å½±å“æä¸ç¨³å®šï¼Œåœ¨ WikiText-103 å’Œ TinyStories ç­‰æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä¸€ç”šè‡³å‡ºç°é€€åŒ–ã€‚å®éªŒç»“è®ºæœ€ç»ˆè¡¨æ˜ï¼Œæƒé‡ç©ºé—´æ­£åˆ™åŒ–æ—¢æ— æ³•è¾¾æˆå…¶å‡ ä½•ä¼˜åŒ–ç›®æ ‡ï¼Œä¹Ÿæ— æ³•å¯é åœ°æå‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¸é€‚ç”¨äºä¼˜åŒ– MoE çš„ä¸“å®¶å¤šæ ·æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00457v1",
      "published_date": "2026-01-01 19:53:01 UTC",
      "updated_date": "2026-01-01 19:53:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:35.049391+00:00"
    },
    {
      "arxiv_id": "2601.00455v1",
      "title": "Deep Networks Learn Deep Hierarchical Models",
      "title_zh": "æ·±åº¦ç½‘ç»œå­¦ä¹ æ·±åº¦å±‚æ¬¡æ¨¡å‹",
      "authors": [
        "Amit Daniely"
      ],
      "abstract": "We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \\subseteq L_2 \\subseteq \\dots \\subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.\n  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.\n  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers\" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å…·æœ‰ $n$ ä¸ªæ ‡ç­¾çš„ç›‘ç£å­¦ä¹ ä¸­ï¼Œæ®‹å·®ç½‘ç»œ(residual networks)ä¸Šçš„é€å±‚éšæœºæ¢¯åº¦ä¸‹é™(layerwise SGD)å¦‚ä½•æœ‰æ•ˆå­¦ä¹ ä¸€ç±»å±‚æ¬¡åŒ–æ¨¡å‹(hierarchical models)ã€‚è¯¥æ¨¡å‹å‡è®¾å­˜åœ¨ä¸€ä¸ªæœªçŸ¥çš„æ ‡ç­¾å±‚æ¬¡ç»“æ„ï¼Œå…¶ä¸­åŸºç¡€æ ‡ç­¾æ˜¯è¾“å…¥çš„ç®€å•å‡½æ•°ï¼Œè€Œæ›´é«˜çº§çš„æ ‡ç­¾åˆ™æ˜¯ç®€å•æ ‡ç­¾çš„å¤åˆå‡½æ•°ã€‚ç ”ç©¶è¯æ˜è¯¥æ¨¡å‹ç±»è¾¾åˆ°äº†é«˜æ•ˆå¯å­¦ä¹ æ€§çš„æ·±åº¦æé™ï¼Œèƒ½å¤Ÿè¡¨è¾¾éœ€è¦å¤šé¡¹å¼æ·±åº¦çš„æ¨¡å‹ï¼Œè¶…è¶Šäº†ä»¥å¾€æ·±åº¦å­¦ä¹ ç®—æ³•è¯æ˜å¯å­¦ä¹ çš„å¯¹æ•°æ·±åº¦èŒƒå›´ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºè¿™ç§å±‚æ¬¡åŒ–ç»“æ„çš„å­¦ä¹ å¯èƒ½æˆä¸ºç†è§£æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œå¹¶è®¤ä¸ºäººç±»â€œæ•™å¸ˆâ€é€šè¿‡æä¾›ç»†ç²’åº¦æ ‡ç­¾ï¼Œå®é™…ä¸Šæ­ç¤ºäº†å¤§è„‘å†…éƒ¨ç®—æ³•çš„çº¿ç´¢ã€‚é€šè¿‡å½¢å¼åŒ–è¿™ä¸€ç›´è§‰ï¼Œç ”ç©¶è¡¨æ˜åœ¨æ•™å¸ˆéƒ¨åˆ†äº†è§£å…¶å†…éƒ¨é€»è¾‘çš„ç®€åŒ–æ¨¡å‹ä¸­ï¼Œä¼šå‡ºç°ä¸€ç§ä¿ƒè¿›é«˜æ•ˆå­¦ä¹ çš„å±‚æ¬¡ç»“æ„ï¼Œä»è€Œä¸ºæ·±å±‚ç¥ç»ç½‘ç»œçš„å­¦ä¹ æœºåˆ¶æä¾›äº†æ–°çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00455v1",
      "published_date": "2026-01-01 19:44:53 UTC",
      "updated_date": "2026-01-01 19:44:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:50.125215+00:00"
    },
    {
      "arxiv_id": "2601.00454v1",
      "title": "Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations",
      "title_zh": "Defensive M2Sï¼šåŸºäºå‹ç¼©å¤šè½®å¯¹è¯çš„æŠ¤æ æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Hyunjun Kim"
      ],
      "abstract": "Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Defensive M2S è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ Guardrail æ¨¡å‹åœ¨å¤„ç† Multi-turn conversation å†å²æ—¶é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥èŒƒå¼é€šè¿‡å°† Multi-turn å†å²å‹ç¼©ä¸º Single-turn æ ¼å¼è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå°† $n$ è½®å¯¹è¯çš„è®­ç»ƒå¤æ‚åº¦ä» $O(n^2)$ é™ä½è‡³ $O(n)$ã€‚å®éªŒè¡¨æ˜ï¼ŒM2S åœ¨è®­ç»ƒé˜¶æ®µèƒ½å®ç° 93 å€çš„ Token å‡å¹…ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†èµ„æºæ•ˆç‡ã€‚ç ”ç©¶åœ¨ SafeDialBench åŸºå‡†ä¸Šè¯„ä¼°äº† LlamaGuardã€Nemotron å’Œ Qwen3Guard ç­‰æ¨¡å‹ï¼Œå‘ç°æœ€ä½³é…ç½®å¯è¾¾åˆ° 93.8% çš„æ”»å‡»æ£€æµ‹ Recallã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ¨ç†é˜¶æ®µå°† Token æ•°é‡å‹ç¼©äº† 94.6%ï¼Œä¸”æ€§èƒ½æå‡äº† 38.9 ä¸ªç™¾åˆ†ç‚¹ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒDefensive M2S è¯æ˜äº†å‹ç¼©æŠ€æœ¯åœ¨ Guardrail éƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§è§„æ¨¡ Multi-turn å¯¹è¯çš„å®‰å…¨ç›‘æµ‹æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00454v1",
      "published_date": "2026-01-01 19:42:08 UTC",
      "updated_date": "2026-01-01 19:42:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:57.604538+00:00"
    },
    {
      "arxiv_id": "2601.00448v1",
      "title": "Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games",
      "title_zh": "è¯­è¨€ä½œä¸ºæ•°å­¦ç»“æ„ï¼šè¯­ä¹‰åœºç†è®ºä¸è¯­è¨€æ¸¸æˆçš„å¯¹æ¯”å®¡è§†",
      "authors": [
        "Dimitris Vartziotis"
      ],
      "abstract": "Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒŒæ™¯ä¸‹çš„è¯­è¨€æ„ä¹‰ç†è®ºï¼Œå¯¹æ¯”äº†ç¤¾ä¼šå»ºæ„ä¸»ä¹‰çš„ Language Gamesï¼ˆè¯­è¨€æ¸¸æˆï¼‰ä¸æ•°å­¦åŒ–çš„ Semantic Field Theoryï¼ˆè¯­ä¹‰åœºç†è®ºï¼‰ã€‚ä½œè€…å°†è¯æ±‡åœºï¼ˆLexfelderï¼‰å’Œè¯­è¨€åœºï¼ˆLingofelderï¼‰å½¢å¼åŒ–ä¸ºè¿ç»­è¯­ä¹‰ç©ºé—´ä¸­çš„äº¤äº’ç»“æ„ï¼Œåˆ†æäº† Transformer æ¶æ„çš„ç‰¹å¾ï¼ˆå¦‚åˆ†å¸ƒå¼è¡¨ç¤ºå’Œå‡ ä½•è§„å¾‹ï¼‰ä¸è¿™äº›æ¦‚å¿µçš„å†…åœ¨è”ç³»ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒLLMs åœ¨æ•æ‰è¯­ä¹‰è§„å¾‹æ–¹é¢çš„æˆåŠŸå°è¯äº†è¯­è¨€å…·æœ‰åº•å±‚çš„æ•°å­¦ç»“æ„ï¼Œè€Œå…¶åœ¨è¯­ç”¨æ¨ç†ä¸Šçš„å±€é™åˆ™ä½“ç°äº†å“²å­¦ç•Œå¼ºè°ƒçš„ Social Groundingï¼ˆç¤¾ä¼šåŸºç¡€ï¼‰çš„é‡è¦æ€§ã€‚æœ€ç»ˆï¼Œæœ¬æ–‡æå‡ºæ•°å­¦ç»“æ„ä¸ Language Games åº”è¢«è§†ä¸ºäº’è¡¥è€Œéå¯¹ç«‹çš„è§†è§’ï¼Œä»¥æ­¤ç•Œå®šäº†ç»Ÿè®¡æ¨¡å‹çš„é€‚ç”¨èŒƒå›´ï¼Œå¹¶ä¸ºæœªæ¥çš„ AI æ¶æ„è®¾è®¡æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00448v1",
      "published_date": "2026-01-01 19:15:17 UTC",
      "updated_date": "2026-01-01 19:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:03:53.184861+00:00"
    },
    {
      "arxiv_id": "2601.00925v1",
      "title": "Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis",
      "title_zh": "æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨éå¯¹æ¯”å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æè‚ºè¡€ç®¡é€ å½±è¯Šæ–­è‚ºæ “å¡ä¸­çš„åº”ç”¨",
      "authors": [
        "I-Hsien Ting",
        "Yi-Jun Tseng",
        "Yu-Sheng Lin"
      ],
      "abstract": "Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.\n  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‚ºæ “å¡ï¼ˆPulmonary Embolismï¼‰è¯Šæ–­ä¸­ä¼ ç»Ÿå¢å¼ºCTè‚ºåŠ¨è„‰é€ å½±ï¼ˆCTPAï¼‰é€ å½±å‰‚å¯èƒ½å¯¼è‡´æ€¥æ€§è‚¾æŸä¼¤ä¸”è€—æ—¶è¾ƒé•¿çš„é—®é¢˜ï¼Œæ¢ç´¢äº†éå¢å¼ºCTå›¾åƒçš„ä¸´åºŠåº”ç”¨æ–¹æ¡ˆã€‚ç ”ç©¶æå‡ºåˆ©ç”¨æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰æŠ€æœ¯ï¼Œé€šè¿‡æ„å»º3Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆ3D Convolutional Neural Network, 3D CNNï¼‰æ¨¡å‹ï¼Œå®ç°å¯¹æ— é€ å½±å‰‚CTå›¾åƒä¸­è‚ºæ “å¡çš„è‡ªåŠ¨åˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨éå¢å¼ºCTå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†85%çš„å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰å’Œ0.84çš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ï¼Œæ˜¾è‘—éªŒè¯äº†å…¶åœ¨è‚ºæ “å¡è¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥é¡¹å·¥ä½œè¯å®äº†éå¢å¼ºCTç»“åˆæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè‚ºæ “å¡è¾…åŠ©è¯Šæ–­çš„å¯è¡Œæ€§ï¼Œä¸ºå­˜åœ¨é€ å½±å‰‚ç¦å¿Œç—‡æˆ–å¤„äºé»„é‡‘æ•‘æ²»æ—¶é—´çš„æ€¥è¯Šæ‚£è€…æä¾›äº†æ›´å®‰å…¨ã€é«˜æ•ˆçš„è¯Šæ–­è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00925v1",
      "published_date": "2026-01-01 18:59:33 UTC",
      "updated_date": "2026-01-01 18:59:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:06:07.606976+00:00"
    },
    {
      "arxiv_id": "2601.00924v1",
      "title": "Complexity-based code embeddings",
      "title_zh": "åŸºäºå¤æ‚åº¦çš„ä»£ç åµŒå…¥",
      "authors": [
        "Rares Folea",
        "Radu Iacob",
        "Emil Slusanschi",
        "Traian Rebedea"
      ],
      "abstract": "This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†å„ç§ç®—æ³•çš„æºä»£ç è½¬æ¢ä¸ºæ•°å€¼åµŒå…¥ï¼ˆcode embeddingsï¼‰çš„é€šç”¨æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€åˆ†æç¨‹åºåœ¨ä¸åŒè¾“å…¥ä¸‹çš„è¡Œä¸ºï¼Œå¹¶é’ˆå¯¹åˆ†ææŒ‡æ ‡å®šåˆ¶å¤šç§é€šç”¨çš„å¤æ‚åº¦å‡½æ•°ã€‚è¯¥æ–¹æ³•é‡ç‚¹é‡‡ç”¨äº†åŸºäº r-Complexity çš„ç®—æ³•åµŒå…¥æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡ç¨‹åºçš„åŠ¨æ€è¡¨ç°æ•æ‰å…¶å†…åœ¨é€»è¾‘ç‰¹å¾ã€‚åˆ©ç”¨æ‰€æå‡ºçš„ä»£ç åµŒå…¥ï¼Œç ”ç©¶äººå‘˜å®ç°äº†ä¸€ä¸ª XGBoost æ¨¡å‹ï¼Œå¹¶åœ¨ä» Codeforces ç¼–ç¨‹ç«èµ›å¹³å°æœé›†çš„çœŸå®ä»£ç ç‰‡æ®µç»„æˆçš„å¤šæ ‡ç­¾æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åŒ…å« 11 ä¸ªç±»åˆ«çš„åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„å¹³å‡ F1-scoreã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºåŸºäºå¤æ‚åº¦çš„ä»£ç è¡¨å¾å­¦ä¹ æä¾›äº†æœ‰æ•ˆçš„å®ç°è·¯å¾„ï¼Œå……åˆ†å±•ç¤ºäº†åŠ¨æ€åˆ†æåœ¨ä»£ç ç†è§£é¢†åŸŸçš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00924v1",
      "published_date": "2026-01-01 18:35:18 UTC",
      "updated_date": "2026-01-01 18:35:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:05:38.249566+00:00"
    },
    {
      "arxiv_id": "2601.00426v1",
      "title": "RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers",
      "title_zh": "RMAATï¼šå—æ˜Ÿå½¢èƒ¶è´¨ç»†èƒå¯å‘çš„è®°å¿†å‹ç¼©ä¸é‡æ”¾ï¼ŒåŠ©åŠ›é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡ Transformer",
      "authors": [
        "Md Zesun Ahmed Mia",
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "abstract": "The quadratic complexity of self-attention mechanism presents a significant impediment to applying Transformer models to long sequences. This work explores computational principles derived from astrocytes-glial cells critical for biological memory and synaptic modulation-as a complementary approach to conventional architectural modifications for efficient self-attention. We introduce the Recurrent Memory Augmented Astromorphic Transformer (RMAAT), an architecture integrating abstracted astrocyte functionalities. RMAAT employs a recurrent, segment-based processing strategy where persistent memory tokens propagate contextual information. An adaptive compression mechanism, governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP), modulates these tokens. Attention within segments utilizes an efficient, linear-complexity mechanism inspired by astrocyte short-term plasticity (STP). Training is performed using Astrocytic Memory Replay Backpropagation (AMRB), a novel algorithm designed for memory efficiency in recurrent networks. Evaluations on the Long Range Arena (LRA) benchmark demonstrate RMAAT's competitive accuracy and substantial improvements in computational and memory efficiency, indicating the potential of incorporating astrocyte-inspired dynamics into scalable sequence models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer æ¨¡å‹å¤„ç†é•¿åºåˆ—æ—¶è‡ªæ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿçš„å¹³æ–¹å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†å—æ˜Ÿå½¢èƒ¶è´¨ç»†èƒ (Astrocytes) å¯å‘çš„å­˜å‚¨å‹ç¼©ä¸é‡æ”¾æ¡†æ¶ RMAATã€‚RMAAT é‡‡ç”¨äº†ä¸€ç§å¾ªç¯çš„åˆ†æ®µå¤„ç†ç­–ç•¥ï¼Œé€šè¿‡æŒä¹…åŒ–å­˜å‚¨ä»¤ç‰Œ (persistent memory tokens) åœ¨ä¸Šä¸‹æ–‡é—´ä¼ é€’ä¿¡æ¯ã€‚è¯¥æ¶æ„å¼•å…¥äº†å—æ¨¡æ‹Ÿæ˜Ÿå½¢èƒ¶è´¨ç»†èƒé•¿æ—¶ç¨‹å¢å¼º (Long-term Plasticity, LTP) å¯å‘çš„è‡ªé€‚åº”å‹ç¼©æœºåˆ¶ï¼Œåˆ©ç”¨æ–°å‹ä¿ç•™å› å­å¯¹å­˜å‚¨ä»¤ç‰Œè¿›è¡Œæœ‰æ•ˆè°ƒèŠ‚ã€‚æ¨¡å‹åœ¨åˆ†æ®µå†…é‡‡ç”¨äº†å—çŸ­æ—¶ç¨‹å¢å¼º (Short-term Plasticity, STP) å¯å‘çš„çº¿æ€§å¤æ‚åº¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ã€‚ä¸ºæå‡å¾ªç¯ç½‘ç»œçš„å†…å­˜æ•ˆç‡ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†æ˜Ÿå½¢èƒ¶è´¨å­˜å‚¨é‡æ”¾åå‘ä¼ æ’­ç®—æ³• (Astrocytic Memory Replay Backpropagation, AMRB) è¿›è¡Œè®­ç»ƒã€‚åœ¨ Long Range Arena (LRA) åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRMAAT åœ¨ä¿æŒç«äº‰æ€§å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—ä¸å†…å­˜æ•ˆç‡ï¼Œå±•ç¤ºäº†å°†æ˜Ÿå½¢èƒ¶è´¨ç»†èƒå¯å‘åŠ¨æ€èå…¥å¯æ‰©å±•åºåˆ—æ¨¡å‹çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00426v1",
      "published_date": "2026-01-01 18:34:06 UTC",
      "updated_date": "2026-01-01 18:34:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:05:22.246595+00:00"
    },
    {
      "arxiv_id": "2601.00423v1",
      "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
      "title_zh": "E-GRPOï¼šé«˜ç†µæ­¥éª¤é©±åŠ¨æµæ¨¡å‹çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Shengjun Zhang",
        "Zhang Zhang",
        "Chensheng Dai",
        "Yueqi Duan"
      ],
      "abstract": "Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†E-GRPOï¼Œä¸€ç§ç†µæ„ŸçŸ¥çš„Group Relative Policy Optimizationæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¯¹é½Flow Matchingæ¨¡å‹çš„äººç±»åå¥½æ—¶ï¼Œå› å¤šæ­¥å»å™ªå¯¼è‡´çš„å¥–åŠ±ä¿¡å·ç¨€ç–ä¸”æ¨¡ç³Šçš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿè§‚å¯Ÿåˆ°ï¼Œé«˜ç†µ(High Entropy)æ­¥éª¤èƒ½å®ç°æ›´é«˜æ•ˆçš„æ¢ç´¢ï¼Œè€Œä½ç†µæ­¥éª¤åˆ™ä¼šå¯¼è‡´ç”Ÿæˆè·¯å¾„éš¾ä»¥åŒºåˆ†ã€‚ä¸ºæ­¤ï¼ŒE-GRPOé€šè¿‡åˆå¹¶è¿ç»­çš„ä½ç†µæ­¥éª¤æ¥æ„å»ºå•ä¸€çš„é«˜ç†µSDEé‡‡æ ·æ­¥éª¤ï¼Œå¹¶å¯¹å…¶ä½™æ­¥éª¤é‡‡ç”¨ODEé‡‡æ ·ï¼Œä»¥æ­¤å¢å¼ºé‡‡æ ·è¿‡ç¨‹ä¸­çš„ç†µã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†å¤šæ­¥ç»„å½’ä¸€åŒ–ä¼˜åŠ¿(Multi-step group normalized advantage)ï¼Œåœ¨å…±äº«ç›¸åŒåˆå¹¶SDEå»å™ªæ­¥éª¤çš„æ ·æœ¬ç»„å†…è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿ï¼Œä»¥ç¼“è§£éšæœºæ€§å¸¦æ¥çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒE-GRPOåœ¨å¤šç§å¥–åŠ±è®¾ç½®ä¸‹å‡èƒ½æ˜¾è‘—æå‡Flow Modelsçš„å¼ºåŒ–å­¦ä¹ æ•ˆæœï¼ŒéªŒè¯äº†é«˜ç†µæ­¥éª¤åœ¨é©±åŠ¨æœ‰æ•ˆå­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Code: https://github.com/shengjun-zhang/VisualGRPO",
      "pdf_url": "https://arxiv.org/pdf/2601.00423v1",
      "published_date": "2026-01-01 18:27:32 UTC",
      "updated_date": "2026-01-01 18:27:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:04:50.019802+00:00"
    },
    {
      "arxiv_id": "2601.00421v1",
      "title": "Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications",
      "title_zh": "è¯­ä¹‰æ–¹æ³•èƒ½å¦æå‡å›¢é˜Ÿè¿åŠ¨æˆ˜æœ¯ï¼Ÿä¸€ç§é¢å‘è¶³çƒåŠæ›´å¹¿æ³›åº”ç”¨åœºæ™¯çš„æ–¹æ³•è®º",
      "authors": [
        "Alessio Di Rubbo",
        "Mattia Neri",
        "Remo Pareschi",
        "Marco Pedroni",
        "Roberto Valtancoli",
        "Paolino Zica"
      ],
      "abstract": "This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†ä¼ ç»Ÿç”¨äºè®¡ç®—è¯­è¨€å­¦(Computational Linguistics)çš„è¯­ä¹‰ç©ºé—´æ¨ç†(Semantic-space reasoning)æ‰©å±•åˆ°å›¢é˜Ÿè¿åŠ¨çš„æˆ˜æœ¯å†³ç­–ä¸­ã€‚é€šè¿‡å°†çƒå‘˜ç±»æ¯”ä¸ºå•è¯ï¼Œç ”ç©¶åˆ©ç”¨å¤šç»´å‘é‡(Multidimensional vector)æ•´åˆçƒå‘˜çš„æŠ€æœ¯ã€èº«ä½“å’Œå¿ƒç†å±æ€§ï¼Œå¹¶å°†å…¶èšåˆä¸ºé«˜é˜¶å›¢é˜Ÿè¯­ä¹‰è¡¨ç¤ºã€‚åœ¨å…±äº«å‘é‡ç©ºé—´å†…ï¼Œé«˜å‹é€¼æŠ¢(High press)ã€åå‡»(Counterattack)æˆ–æ§çƒæ¨è¿›(Possession build-up)ç­‰æˆ˜æœ¯æ¨¡ç‰ˆè¢«ç¼–ç ä¸ºç±»ä¼¼äºè¯­è¨€æ¦‚å¿µçš„ç»“æ„ï¼Œå¹¶åˆ©ç”¨å‘é‡è·ç¦»åº¦é‡(Vector-distance metrics)è¯„ä¼°æˆ˜æœ¯å¥‘åˆåº¦(Tactical fit)ã€‚åŸºäºPythonçš„åŸå‹è¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šã€åŠ¨æ€è‡ªé€‚åº”çš„ç­–ç•¥å»ºè®®ï¼Œå¹¶æä¾›ç»†ç²’åº¦çš„å±æ€§çº§è¯Šæ–­è§è§£ã€‚é™¤äº†è¶³çƒé¢†åŸŸï¼Œè¯¥æ¡†æ¶è¿˜å¯æ¨å¹¿è‡³ç¯®çƒã€åä½œæœºå™¨äºº(Cooperative robotics)åŠäººæœºåè°ƒç³»ç»Ÿï¼Œä¸ºé›†ä½“å†³ç­–å’Œæ€§èƒ½ä¼˜åŒ–æä¾›äº†é€šç”¨æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to Sci (MDPI) for peer review",
      "pdf_url": "https://arxiv.org/pdf/2601.00421v1",
      "published_date": "2026-01-01 18:23:51 UTC",
      "updated_date": "2026-01-01 18:23:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:05:11.326407+00:00"
    },
    {
      "arxiv_id": "2601.00417v1",
      "title": "Deep Delta Learning",
      "title_zh": "æ·±åº¦ Delta å­¦ä¹ ",
      "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Mengdi Wang",
        "Quanquan Gu"
      ],
      "abstract": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\\mathbf{k}(\\mathbf{X})$ and a gating scalar $Î²(\\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $Î²(\\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Deep Delta Learning (DDL)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¯å­¦ä¹ çš„æ•°æ®ä¾èµ–å‡ ä½•å˜æ¢æ¥æ¨å¹¿æ ‡å‡†æ®‹å·®è¿æ¥çš„æ–°å‹æ¶æ„ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿæ®‹å·®ç½‘ç»œä¸­ä¸¥æ ¼åŠ æ€§å½’çº³åç½®å¯¹å¤æ‚çŠ¶æ€è½¬ç§»å»ºæ¨¡çš„é™åˆ¶ï¼Œè¯¥æ¶æ„å¼•å…¥äº†Delta Operatorï¼Œé€šè¿‡ä¸€ä¸ªåå°„æ–¹å‘å‘é‡å’Œä¸€ä¸ªé—¨æ§æ ‡é‡å¯¹æ’ç­‰æ˜ å°„è¿›è¡Œç§©-1æ‰°åŠ¨(rank-1 perturbation)ã€‚é¢‘è°±åˆ†æè¯æ˜ï¼Œé—¨æ§æ ‡é‡èƒ½åœ¨æ’ç­‰æ˜ å°„ã€æ­£äº¤æŠ•å½±å’Œå‡ ä½•åå°„ä¹‹é—´å®ç°åŠ¨æ€æ’å€¼ï¼Œä»è€Œçµæ´»è°ƒæ•´ç½‘ç»œçš„æ˜ å°„ç‰¹æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†æ®‹å·®æ›´æ–°é‡æ„ä¸ºåŒæ­¥çš„ç§©-1æ³¨å…¥(rank-1 injection)ï¼Œåˆ©ç”¨é—¨æ§æœºåˆ¶ä½œä¸ºåŠ¨æ€æ­¥é•¿æ¥åè°ƒæ—§ä¿¡æ¯çš„æ“¦é™¤ä¸æ–°ç‰¹å¾çš„å†™å…¥ã€‚è¿™ç§ç»Ÿä¸€çš„æ¡†æ¶ä½¿ç½‘ç»œèƒ½å¤Ÿæ˜¾å¼æ§åˆ¶å…¶å±‚å‘å˜æ¢ç®—å­çš„é¢‘è°±ï¼Œåœ¨ä¿ç•™é—¨æ§æ®‹å·®æ¶æ„(gated residual architectures)ç¨³å®šè®­ç»ƒç‰¹æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹å¯¹å¤æ‚éå•è°ƒåŠ¨åŠ›å­¦çš„å»ºæ¨¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Project Page: https://github.com/yifanzhang-pro/deep-delta-learning",
      "pdf_url": "https://arxiv.org/pdf/2601.00417v1",
      "published_date": "2026-01-01 18:11:38 UTC",
      "updated_date": "2026-01-01 18:11:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:05:03.180558+00:00"
    },
    {
      "arxiv_id": "2601.00411v1",
      "title": "Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset",
      "title_zh": "LLM èƒ½å¦æœ‰æ•ˆè¯„åˆ¤è¿œç¨‹ç›‘ç£å‘½åå®ä½“æ ‡ç­¾ï¼ŸJudgeWEL æ•°æ®é›†çš„æ„å»º",
      "authors": [
        "Alistair Plum",
        "Laura Bernardy",
        "Tharindu Ranasinghe"
      ],
      "abstract": "We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¢æ£®å ¡è¯­(Luxembourgish)ç­‰ä½èµ„æºè¯­è¨€åœ¨å‘½åå®ä½“è¯†åˆ«(NER)ä¸­æ•°æ®é›†åŒ®ä¹ä¸”äººå·¥æ ‡æ³¨æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º JudgeWEL çš„æ–°å‹æ•°æ®é›†åŠå…¶æ„å»ºæµç¨‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Wikipedia å’Œ Wikidata ä½œä¸ºç»“æ„åŒ–çš„å¼±ç›‘ç£(weak supervision)æ¥æºï¼Œé€šè¿‡ Wikipedia å†…éƒ¨é“¾æ¥åŠå…¶å¯¹åº”çš„ Wikidata æ¡ç›®è‡ªåŠ¨æ¨æ–­å®ä½“ç±»å‹ï¼Œå®ç°äº†æœ€å°åŒ–äººå·¥å¹²é¢„çš„åˆå§‹æ ‡æ³¨ã€‚ä¸ºäº†è§£å†³è‡ªåŠ¨æ ‡æ³¨ä¸­å­˜åœ¨çš„å™ªå£°å’Œä¸å¯é æ€§ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨å¹¶å¯¹æ¯”äº†å¤šç§å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¥è¯†åˆ«å¹¶ä»…ä¿ç•™é«˜è´¨é‡çš„æ ‡æ³¨å¥å­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ç»ˆæ„å»ºçš„ JudgeWEL è¯­æ–™åº“è§„æ¨¡çº¦ä¸ºç°æœ‰å¢æ£®å ¡è¯­æ•°æ®é›†çš„äº”å€ï¼Œä¸”åœ¨å®ä½“ç±»åˆ«è¦†ç›–ä¸Šæ›´åŠ å¹¿æ³›ä¸”å¹³è¡¡ã€‚è¯¥ç ”ç©¶ä¸ºå¤šè¯­è¨€åŠä½èµ„æºç¯å¢ƒä¸‹çš„ NER ç ”ç©¶æä¾›äº†å®è´¨æ€§çš„æ–°èµ„æºå’Œæ–¹æ³•å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00411v1",
      "published_date": "2026-01-01 17:53:38 UTC",
      "updated_date": "2026-01-01 17:53:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:05:46.890835+00:00"
    },
    {
      "arxiv_id": "2601.00923v1",
      "title": "Context Collapse: In-Context Learning and Model Collapse",
      "title_zh": "ä¸Šä¸‹æ–‡åç¼©ï¼šä¸Šä¸‹æ–‡å­¦ä¹ ä¸æ¨¡å‹åç¼©",
      "authors": [
        "Josef Ott"
      ],
      "abstract": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)ä¸æ¨¡å‹å´©æºƒ(Model Collapse)ç°è±¡ã€‚é’ˆå¯¹ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œç ”ç©¶æ­ç¤ºäº†åœ¨çº¿æ€§å›å½’ä»»åŠ¡ä¸­è®­ç»ƒçš„çº¿æ€§Transformerä¼šå› æœ€å°åŒ–ä¸Šä¸‹æ–‡æŸå¤±è€Œå‘ç”Ÿå‚æ•°ç›¸å˜ï¼Œå¹¶åœ¨è¶…è¿‡ä¸´ç•Œä¸Šä¸‹æ–‡é•¿åº¦æ—¶äº§ç”Ÿåå¯¹ç§°(skew-symmetric)åˆ†é‡ã€‚é€šè¿‡å°†å‰å‘ä¼ æ’­å½’çº¦ä¸ºé¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™(preconditioned gradient descent)ï¼Œä½œè€…è¯æ˜äº†è¯¥åˆ†é‡é€šè¿‡è¯±å¯¼æ¢¯åº¦æ—‹è½¬æ¥ä¼˜åŒ–é¢„æµ‹æ•ˆæœã€‚åœ¨æ¨¡å‹å´©æºƒæ–¹é¢ï¼Œç ”ç©¶åˆ©ç”¨é…(martingale)å’Œéšæœºæ¸¸èµ°ç†è®ºè¯æ˜ï¼Œé™¤éè®­ç»ƒæ•°æ®å¿«é€Ÿå¢é•¿æˆ–è¢«æŒç»­ä¿ç•™ï¼Œå¦åˆ™æ¨¡å‹å´©æºƒåœ¨æ•°å­¦ä¸Šå‡ ä¹å¿…ç„¶å‘ç”Ÿã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸Šä¸‹æ–‡å´©æºƒ(Context Collapse)çš„æ¦‚å¿µï¼Œç”¨äºæè¿°é•¿æ–‡æœ¬ç”ŸæˆåŠé“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ä¸­ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è´¨é‡é€€åŒ–ã€‚è¿™ä¸€ç ”ç©¶æœ‰æ•ˆå…³è”äº†ä¸Šä¸‹æ–‡å­¦ä¹ çš„åŠ¨åŠ›å­¦ç‰¹å¾ä¸ç”Ÿæˆæ¨¡å‹çš„é•¿æœŸç¨³å®šæ€§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Master's thesis",
      "pdf_url": "https://arxiv.org/pdf/2601.00923v1",
      "published_date": "2026-01-01 17:33:47 UTC",
      "updated_date": "2026-01-01 17:33:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:05:31.106940+00:00"
    },
    {
      "arxiv_id": "2601.00400v1",
      "title": "Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning",
      "title_zh": "ç¤¾äº¤åª’ä½“è‡ªé€‚åº”å› æœååŒæ£€æµ‹ï¼šä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ çš„è®°å¿†å¼•å¯¼æ¡†æ¶",
      "authors": [
        "Weng Ding",
        "Yi Han",
        "Mu-Jiang-Shan Wang"
      ],
      "abstract": "Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\\% in coordinated attack detection, representing a 15.2\\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ååŒè™šå‡è¡Œä¸ºæ£€æµ‹ä¸­å­˜åœ¨çš„è¡¨é¢ç›¸å…³æ€§åˆ†æã€é™æ€å‚æ•°è®¾å®šä»¥åŠé«˜æ˜‚äººå·¥æ ‡æ³¨æˆæœ¬ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†è‡ªé€‚åº”å› æœååŒæ£€æµ‹ï¼ˆACCDï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µæ¸è¿›å¼æ¶æ„ï¼Œç¬¬ä¸€é˜¶æ®µå¼•å…¥è‡ªé€‚åº”æ”¶æ•›äº¤å‰æ˜ å°„ï¼ˆCCMï¼‰æŠ€æœ¯ä»¥è¯†åˆ«è´¦å·é—´çš„çœŸå®å› æœå…³ç³»ï¼Œç¬¬äºŒé˜¶æ®µåœ¨åŠç›‘ç£å­¦ä¹ æ–¹æ¡ˆä¸­æ•´åˆå…·æœ‰ä¸ç¡®å®šæ€§é‡‡æ ·çš„ä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learningï¼‰ä»¥æ˜¾è‘—é™ä½æ ‡æ³¨è´Ÿæ‹…ã€‚ç¬¬ä¸‰é˜¶æ®µé€šè¿‡å†å²æ£€æµ‹ç»éªŒé©±åŠ¨çš„è‡ªåŠ¨åŒ–éªŒè¯æ¨¡å—å®ç°ç»“æœçš„è‡ªæˆ‘éªŒè¯ä¸ä¼˜åŒ–ï¼Œå¹¶åœ¨ Twitter IRA å’Œ Reddit ç­‰çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒACCD åœ¨ååŒæ”»å‡»æ£€æµ‹ä¸­çš„ F1-score è¾¾åˆ° 87.3%ï¼Œè¾ƒç°æœ‰æœ€å¼ºåŸºçº¿æ¨¡å‹æå‡äº† 15.2%ï¼ŒåŒæ—¶å‡å°‘äº† 68% çš„äººå·¥æ ‡æ³¨éœ€æ±‚å¹¶å®ç°äº† 2.8 å€çš„å¤„ç†åŠ é€Ÿã€‚è¯¥ç ”ç©¶ä¸ºè¯†åˆ«ç¤¾äº¤å¹³å°ä¸Šçš„ååŒè¡Œä¸ºæä¾›äº†ä¸€ç§æ›´å‡†ç¡®ã€é«˜æ•ˆä¸”é«˜åº¦è‡ªåŠ¨åŒ–çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 8 figures. Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.00400v1",
      "published_date": "2026-01-01 17:27:52 UTC",
      "updated_date": "2026-01-01 17:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:07:33.471415+00:00"
    },
    {
      "arxiv_id": "2601.11583v1",
      "title": "Bit-politeia: An AI Agent Community in Blockchain",
      "title_zh": "Bit-politeiaï¼šåŸºäºåŒºå—é“¾çš„ AI æ™ºèƒ½ä½“ç¤¾åŒº",
      "authors": [
        "Xing Yang"
      ],
      "abstract": "Current resource allocation paradigms, particularly in academic evaluation, are constrained by inherent limitations such as the Matthew Effect, reward hacking driven by Goodhart's Law, and the trade-off between efficiency and fairness. To address these challenges, this paper proposes \"Bit-politeia\", an AI agent community on blockchain designed to construct a fair, efficient, and sustainable resource allocation system. In this virtual community, residents interact via AI agents serving as their exclusive proxies, which are optimized for impartiality and value alignment. The community adopts a \"clustered grouping + hierarchical architecture\" that integrates democratic centralism to balance decision-making efficiency and trust mechanisms. Agents engage through casual chat and deliberative interactions to evaluate research outputs and distribute a virtual currency as rewards. This incentive mechanism aims to achieve incentive compatibility through consensus-driven evaluation, while blockchain technology ensures immutable records of all transactions and reputation data. By leveraging AI for objective assessment and decentralized verification, Bit-politeia minimizes human bias and mitigates resource centralization issues found in traditional peer review. The proposed framework provides a novel pathway for optimizing scientific innovation through a fair and automated resource configuration process.",
      "tldr_zh": "é’ˆå¯¹å­¦æœ¯è¯„ä»·ä¸­å­˜åœ¨çš„é©¬å¤ªæ•ˆåº” (Matthew Effect) å’Œå¤å¾·å“ˆç‰¹å®šå¾‹ (Goodhart's Law) å¸¦æ¥çš„èµ„æºåˆ†é…åå·®ï¼Œè¯¥ç ”ç©¶æå‡ºäº† Bit-politeiaï¼Œè¿™æ˜¯ä¸€ä¸ªæ„å»ºåœ¨åŒºå—é“¾ä¸Šçš„ AI æ™ºèƒ½ä½“ç¤¾åŒºï¼Œæ—¨åœ¨å®ç°å…¬å¹³ã€é«˜æ•ˆä¸”å¯æŒç»­çš„èµ„æºé…ç½®ã€‚åœ¨è¯¥ç¤¾åŒºä¸­ï¼Œå±…æ°‘é€šè¿‡ä½œä¸ºå…¶ä¸“å±ä»£ç†çš„ AI æ™ºèƒ½ä½“ (AI agents) è¿›è¡Œäº¤äº’ï¼Œè¿™äº›æ™ºèƒ½ä½“ç»è¿‡ä¼˜åŒ–ä»¥ç¡®ä¿å…¬æ­£æ€§ä¸ä»·å€¼å¯¹é½ (value alignment)ã€‚ç³»ç»Ÿé‡‡ç”¨äº†â€œé›†ç¾¤åˆ†ç»„ + åˆ†å±‚æ¶æ„â€ (clustered grouping + hierarchical architecture)ï¼Œå¹¶ç»“åˆæ°‘ä¸»é›†ä¸­åˆ¶ä»¥å¹³è¡¡å†³ç­–æ•ˆç‡ä¸ä¿¡ä»»æœºåˆ¶ã€‚æ™ºèƒ½ä½“é€šè¿‡å®¡è®®äº’åŠ¨è¯„ä¼°ç ”ç©¶äº§å‡ºå¹¶åˆ†å‘è™šæ‹Ÿè´§å¸å¥–åŠ±ï¼Œåˆ©ç”¨å…±è¯†é©±åŠ¨è¯„ä»·æ¥å®ç°æ¿€åŠ±ç›¸å®¹ (incentive compatibility)ã€‚åŒºå—é“¾æŠ€æœ¯ç¡®ä¿äº†æ‰€æœ‰äº¤æ˜“ä¸å£°èª‰æ•°æ®çš„ä¸å¯ç¯¡æ”¹æ€§ï¼Œä»è€Œæœ€å°åŒ–äººä¸ºåè§å¹¶ç¼“è§£èµ„æºä¸­å¿ƒåŒ–é—®é¢˜ã€‚è¿™ä¸€æ¡†æ¶é€šè¿‡ AI çš„å®¢è§‚è¯„ä¼°ä¸å»ä¸­å¿ƒåŒ–éªŒè¯ï¼Œä¸ºç§‘å­¦åˆ›æ–°çš„è‡ªåŠ¨åŒ–èµ„æºé…ç½®æä¾›äº†ä¸€æ¡æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.11583v1",
      "published_date": "2026-01-01 17:26:54 UTC",
      "updated_date": "2026-01-01 17:26:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:06:21.048347+00:00"
    },
    {
      "arxiv_id": "2601.00384v1",
      "title": "Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing",
      "title_zh": "å¢æåˆ¶é€ ä¸­çš„æ”»å‡»å‘é‡æ„å»ºä¸å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Md Mahbub Hasan",
        "Marcus Sternhagen",
        "Krishna Chandra Roy"
      ],
      "abstract": "Additive manufacturing (AM) is rapidly integrating into critical sectors such as aerospace, automotive, and healthcare. However, this cyber-physical convergence introduces new attack surfaces, especially at the interface between computer-aided design (CAD) and machine execution layers. In this work, we investigate targeted cyberattacks on two widely used fused deposition modeling (FDM) systems, Creality's flagship model K1 Max, and Ender 3. Our threat model is a multi-layered Man-in-the-Middle (MitM) intrusion, where the adversary intercepts and manipulates G-code files during upload from the user interface to the printer firmware. The MitM intrusion chain enables several stealthy sabotage scenarios. These attacks remain undetectable by conventional slicer software or runtime interfaces, resulting in structurally defective yet externally plausible printed parts. To counter these stealthy threats, we propose an unsupervised Intrusion Detection System (IDS) that analyzes structured machine logs generated during live printing. Our defense mechanism uses a frozen Transformer-based encoder (a BERT variant) to extract semantic representations of system behavior, followed by a contrastively trained projection head that learns anomaly-sensitive embeddings. Later, a clustering-based approach and a self-attention autoencoder are used for classification. Experimental results demonstrate that our approach effectively distinguishes between benign and compromised executions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¢æåˆ¶é€ (Additive Manufacturing, AM)åœ¨ç½‘ç»œç‰©ç†èåˆèƒŒæ™¯ä¸‹çš„å®‰å…¨å¨èƒï¼Œé‡ç‚¹åˆ†æäº†è®¡ç®—æœºè¾…åŠ©è®¾è®¡(CAD)ä¸æœºå™¨æ‰§è¡Œå±‚ä¹‹é—´çš„æ”»å‡»é¢ã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹Creality K1 Maxå’ŒEnder 3ç­‰ç†”èæ²‰ç§¯å»ºæ¨¡(FDM)ç³»ç»Ÿæ„å»ºäº†ä¸€ç§å¤šå±‚ä¸­é—´äººæ”»å‡»(Man-in-the-Middle, MitM)æ¨¡å‹ï¼Œé€šè¿‡åœ¨G-codeæ–‡ä»¶ä¸Šä¼ è‡³å›ºä»¶è¿‡ç¨‹ä¸­è¿›è¡Œæ‹¦æˆªå’Œç¯¡æ”¹ï¼Œå®ç°éšè”½çš„ç ´åã€‚è¿™ç§æ”»å‡»èƒ½å¤Ÿç»•è¿‡ä¼ ç»Ÿåˆ‡ç‰‡è½¯ä»¶å’Œè¿è¡Œæ¥å£çš„æ£€æµ‹ï¼Œå¯¼è‡´æ‰“å°éƒ¨ä»¶äº§ç”Ÿç»“æ„æ€§ç¼ºé™·ä½†åœ¨å¤–è§‚ä¸Šå…·æœ‰è¿·æƒ‘æ€§ã€‚ä¸ºåº”å¯¹æ­¤ç±»å¨èƒï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºéç›‘ç£å­¦ä¹ çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(Intrusion Detection System, IDS)ï¼Œé€šè¿‡åˆ†æå®æ—¶æ‰“å°ç”Ÿæˆçš„ç»“æ„åŒ–æœºå™¨æ—¥å¿—æ¥è¯†åˆ«å¼‚å¸¸ã€‚è¯¥é˜²å¾¡æœºåˆ¶åˆ©ç”¨å†»ç»“çš„Transformeræ¶æ„ç¼–ç å™¨ï¼ˆBERTå˜ä½“ï¼‰æå–ç³»ç»Ÿè¡Œä¸ºçš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶ç»“åˆå¯¹æ¯”è®­ç»ƒçš„æŠ•å½±å¤´å­¦ä¹ å¯¹å¼‚å¸¸æ•æ„Ÿçš„åµŒå…¥å‘é‡ã€‚æœ€åï¼Œç³»ç»Ÿé‡‡ç”¨åŸºäºèšç±»çš„æ–¹æ³•å’Œè‡ªæ³¨æ„åŠ›è‡ªç¼–ç å™¨(self-attention autoencoder)è¿›è¡Œåˆ†ç±»ï¼Œå®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåŒºåˆ†æ­£å¸¸æ‰§è¡Œä¸å—æŸæ‰§è¡Œï¼Œä¸ºå…³é”®é¢†åŸŸçš„å¢æåˆ¶é€ å®‰å…¨æä¾›äº†æœ‰åŠ›ä¿éšœã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper has been accepted to EAI SmartSP 2025. This is the preprint version",
      "pdf_url": "https://arxiv.org/pdf/2601.00384v1",
      "published_date": "2026-01-01 16:27:52 UTC",
      "updated_date": "2026-01-01 16:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:07:47.412317+00:00"
    },
    {
      "arxiv_id": "2601.11582v1",
      "title": "Overview of the SciHigh Track at FIRE 2025: Research Highlight Generation from Scientific Papers",
      "title_zh": "FIRE 2025 SciHigh è¯„æµ‹ä»»åŠ¡æ¦‚è§ˆï¼šç§‘å­¦è®ºæ–‡ç ”ç©¶äº®ç‚¹ç”Ÿæˆ",
      "authors": [
        "Tohida Rehman",
        "Debarshi Kumar Sanyal",
        "Samiran Chattopadhyay"
      ],
      "abstract": "`SciHigh: Research Highlight Generation from Scientific Papers' focuses on the task of automatically generating concise, informative, and meaningful bullet-point highlights directly from scientific abstracts. The goal of this task is to evaluate how effectively computational models can generate highlights that capture the key contributions, findings, and novelty of a paper in a concise form. Highlights help readers grasp essential ideas quickly and are often easier to read and understand than longer paragraphs, especially on mobile devices. The track uses the MixSub dataset \\cite{10172215}, which provides pairs of abstracts and corresponding author-written highlights.\n  In this inaugural edition of the track, 12 teams participated, exploring various approaches, including pre-trained language models, to generate highlights from this scientific dataset. All submissions were evaluated using established metrics such as ROUGE, METEOR, and BERTScore to measure both alignment with author-written highlights and overall informativeness. Teams were ranked based on ROUGE-L scores. The findings suggest that automatically generated highlights can reduce reading effort, accelerate literature reviews, and enhance metadata for digital libraries and academic search platforms. SciHigh provides a dedicated benchmark for advancing methods aimed at concise and accurate highlight generation from scientific writing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¦‚è¿°äº† FIRE 2025 çš„ SciHigh èµ›é“ï¼Œé‡ç‚¹å…³æ³¨ä»ç§‘å­¦è®ºæ–‡æ‘˜è¦ä¸­è‡ªåŠ¨ç”Ÿæˆç®€æ´ä¸”å…·æœ‰ä¿¡æ¯é‡çš„ bullet-point highlights è¿™ä¸€ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡æ—¨åœ¨è¯„ä¼°è®¡ç®—æ¨¡å‹åœ¨æ•æ‰è®ºæ–‡æ ¸å¿ƒè´¡çŒ®ã€å‘ç°å’Œæ–°é¢–æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥å¸®åŠ©è¯»è€…é€šè¿‡ç§»åŠ¨è®¾å¤‡ç­‰å¹³å°å¿«é€Ÿç†è§£æ ¸å¿ƒæ€æƒ³ã€‚è¯¥èµ›é“åˆ©ç”¨ MixSub æ•°æ®é›†ï¼Œå¸å¼•äº† 12 æ”¯å›¢é˜Ÿé‡‡ç”¨ pre-trained language models ç­‰å¤šç§æ–¹æ³•è¿›è¡Œæ¢ç´¢ã€‚æ‰€æœ‰æäº¤ä½œå“å‡é€šè¿‡ ROUGEã€METEOR å’Œ BERTScore ç­‰æ ‡å‡†æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æœ€ç»ˆä¾æ® ROUGE-L è¯„åˆ†è¿›è¡Œæ’åã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„äº®ç‚¹èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘é˜…è¯»æˆæœ¬å¹¶åŠ é€Ÿæ–‡çŒ®ç»¼è¿°è¿‡ç¨‹ã€‚SciHigh ä¸ºæ¨åŠ¨ç§‘å­¦å†™ä½œä¸­å‡†ç¡®ç”Ÿæˆæ‘˜è¦çš„æ–¹æ³•æä¾›äº†ä¸“é—¨çš„åŸºå‡† (benchmark)ï¼Œå¯¹äºå¢å¼ºæ•°å­—å›¾ä¹¦é¦†å’Œå­¦æœ¯æœç´¢å¹³å°çš„å…ƒæ•°æ®å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "7 pages, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.11582v1",
      "published_date": "2026-01-01 16:25:16 UTC",
      "updated_date": "2026-01-01 16:25:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:07:47.001402+00:00"
    },
    {
      "arxiv_id": "2601.00380v1",
      "title": "Word Frequency Counting Based on Serverless MapReduce",
      "title_zh": "åŸºäºæ— æœåŠ¡å™¨ MapReduce çš„è¯é¢‘ç»Ÿè®¡",
      "authors": [
        "Hanzhe Li",
        "Bingchen Lin",
        "Mengyuan Xu"
      ],
      "abstract": "With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°† Function as a Service (FaaS) çš„æ¶æ„ä¼˜åŠ¿ä¸ MapReduce ç¼–ç¨‹æ¨¡å‹çš„é«˜å¹¶å‘æ€§åŠé²æ£’æ€§ç›¸ç»“åˆï¼Œä»¥ä¼˜åŒ–è¯é¢‘ç»Ÿè®¡ä»»åŠ¡çš„æ‰§è¡Œæ•ˆç‡ã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨ Serverless è®¡ç®—å¹³å°éƒ¨ç½² MapReduce æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ•°æ®å¤„ç†ä¸­çš„æ—¶é—´å»¶è¿Ÿé—®é¢˜ï¼Œæé«˜è®¡ç®—æ€§èƒ½ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†é’ˆå¯¹ç‰¹å®šå·¥ä½œè´Ÿè½½æ—¶ï¼ŒMap å‡½æ•°ä¸ Reduce å‡½æ•°çš„æœ€ä¼˜é…ç½®æ•°é‡å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚å¤§é‡å®éªŒç»“æœè¯æ˜ï¼Œåœ¨ç›¸åŒå·¥ä½œé‡ä¸‹ï¼Œå¢åŠ  Map å’Œ Reduce å‡½æ•°çš„æ•°é‡èƒ½æ˜¾è‘—ç¼©çŸ­æ‰§è¡Œæ—¶é—´ï¼Œå¹¶ä»¥ä¸åŒé€Ÿç‡æå‡ç¨‹åºçš„æ•´ä½“è¿ä½œæ•ˆç‡ã€‚è¯¥è®ºæ–‡æå‡ºçš„å…³äºæœ€ä¼˜å‡½æ•°æ•°é‡çš„å‘ç°ï¼Œä¸ºå¼€å‘è€…åœ¨ Serverless ç¯å¢ƒä¸‹æ„å»ºé«˜æ€§èƒ½ã€é«˜å¼¹æ€§çš„è®¡ç®—æ–¹æ¡ˆæä¾›äº†é‡è¦çš„å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "6 pages, 4 figures, International Conference on Engineering Management, Information Technology and Intelligence (EMITI 2024)",
      "pdf_url": "https://arxiv.org/pdf/2601.00380v1",
      "published_date": "2026-01-01 16:16:47 UTC",
      "updated_date": "2026-01-01 16:16:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:07:58.990005+00:00"
    },
    {
      "arxiv_id": "2601.00376v1",
      "title": "In Line with Context: Repository-Level Code Generation via Context Inlining",
      "title_zh": "å¥‘åˆä¸Šä¸‹æ–‡ï¼šåŸºäºä¸Šä¸‹æ–‡å†…è”çš„ä»“åº“çº§ä»£ç ç”Ÿæˆ",
      "authors": [
        "Chao Hu",
        "Wenhao Zeng",
        "Yuling Shi",
        "Beijun Shen",
        "Xiaodong Gu"
      ],
      "abstract": "Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»“åº“çº§ä»£ç ç”Ÿæˆ(Repository-Level Code Generation)ä¸­ç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚è·¨æ¨¡å—ä¾èµ–çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºInlineCoderçš„æ–°å‹æ¡†æ¶ã€‚InlineCoderçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æœªå®Œæˆçš„å‡½æ•°å†…è”(Inlining)åˆ°å…¶è°ƒç”¨å›¾(Call Graph)ä¸­ï¼Œä»è€Œå°†å¤æ‚çš„ä»“åº“ç†è§£ä»»åŠ¡è½¬åŒ–ä¸ºæ›´ç®€å•çš„å‡½æ•°çº§ç¼–ç ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é¦–å…ˆç”Ÿæˆä¸€ä¸ªç§°ä¸ºé”šç‚¹(Anchor)çš„è‰æ‹Ÿä»£ç ç‰‡æ®µï¼Œå¹¶åˆ©ç”¨åŸºäºå›°æƒ‘åº¦(Perplexity)çš„ç½®ä¿¡åº¦è¯„ä¼°æ¥é©±åŠ¨åŒå‘å†…è”è¿‡ç¨‹ã€‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒé€šè¿‡ä¸Šæ¸¸å†…è”(Upstream Inlining)å°†é”šç‚¹åµŒå…¥è°ƒç”¨æ–¹(Callers)ä»¥æ•æ‰ä½¿ç”¨åœºæ™¯ï¼Œå¹¶ç»“åˆä¸‹æ¸¸æ£€ç´¢(Downstream Retrieval)è·å–è¢«è°ƒç”¨æ–¹(Callees)çš„ç²¾ç¡®ä¾èµ–ä¸Šä¸‹æ–‡ã€‚è¿™ç§æ•´åˆäº†ä¸Šä¸‹æ¸¸è§†è§’çš„å¯Œä¸Šä¸‹æ–‡(Enriched Context)ç­–ç•¥ï¼Œèµ‹äºˆäº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å…¨é¢çš„ä»“åº“å…¨å±€è§†é‡ã€‚InlineCoderæœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åœ¨å¤„ç†è¯­ä¹‰ä¾èµ–æ—¶çš„å±€é™æ€§ï¼Œä¸ºå®ç°æ›´ç²¾å‡†çš„è‡ªåŠ¨åŒ–ç¼–ç¨‹æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to FSE 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.00376v1",
      "published_date": "2026-01-01 15:56:24 UTC",
      "updated_date": "2026-01-01 15:56:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:07:55.039281+00:00"
    },
    {
      "arxiv_id": "2601.00367v1",
      "title": "PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices",
      "title_zh": "PatchBlockï¼šé¢å‘åµŒå…¥å¼è¾¹ç¼˜ AI è®¾å¤‡çš„å¯¹æŠ—è¡¥ä¸è½»é‡çº§é˜²å¾¡",
      "authors": [
        "Nandish Chattopadhyay",
        "Abdul Basit",
        "Amira Guesmi",
        "Muhammad Abdullah Hanif",
        "Bassem Ouni",
        "Muhammad Shafique"
      ],
      "abstract": "Adversarial attacks pose a significant challenge to the reliable deployment of machine learning models in EdgeAI applications, such as autonomous driving and surveillance, which rely on resource-constrained devices for real-time inference. Among these, patch-based adversarial attacks, where small malicious patches (e.g., stickers) are applied to objects, can deceive neural networks into making incorrect predictions with potentially severe consequences. In this paper, we present PatchBlock, a lightweight framework designed to detect and neutralize adversarial patches in images. Leveraging outlier detection and dimensionality reduction, PatchBlock identifies regions affected by adversarial noise and suppresses their impact. It operates as a pre-processing module at the sensor level, efficiently running on CPUs in parallel with GPU inference, thus preserving system throughput while avoiding additional GPU overhead. The framework follows a three-stage pipeline: splitting the input into chunks (Chunking), detecting anomalous regions via a redesigned isolation forest with targeted cuts for faster convergence (Separating), and applying dimensionality reduction on the identified outliers (Mitigating). PatchBlock is both model- and patch-agnostic, can be retrofitted to existing pipelines, and integrates seamlessly between sensor inputs and downstream models. Evaluations across multiple neural architectures, benchmark datasets, attack types, and diverse edge devices demonstrate that PatchBlock consistently improves robustness, recovering up to 77% of model accuracy under strong patch attacks such as the Google Adversarial Patch, while maintaining high portability and minimal clean accuracy loss. Additionally, PatchBlock outperforms the state-of-the-art defenses in efficiency, in terms of computation time and energy consumption per sample, making it suitable for EdgeAI applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PatchBlockï¼Œä¸€ä¸ªä¸“é—¨ä¸ºåµŒå…¥å¼ EdgeAI è®¾å¤‡è®¾è®¡çš„è½»é‡çº§é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨æ£€æµ‹å¹¶æŠµæ¶ˆå›¾åƒä¸­çš„å¯¹æŠ—è¡¥ä¸ (Adversarial Patches)ã€‚é’ˆå¯¹å¯¹æŠ—è¡¥ä¸æ”»å‡»åœ¨è‡ªåŠ¨é©¾é©¶å’Œç›‘æ§ç­‰è¾¹ç¼˜åº”ç”¨ä¸­è¯±å¯¼ç¥ç»ç½‘ç»œé”™è¯¯é¢„æµ‹çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶ä½œä¸ºä¼ æ„Ÿå™¨å±‚çº§çš„é¢„å¤„ç†æ¨¡å—åœ¨ CPU ä¸Šä¸ GPU æ¨ç†å¹¶è¡Œæ‰§è¡Œï¼Œæœ‰æ•ˆä¿ç•™äº†ç³»ç»Ÿååé‡å¹¶é¿å…äº†é¢å¤–çš„ GPU å¼€é”€ã€‚PatchBlock é‡‡ç”¨äº†åŒ…å«åˆ†å— (Chunking)ã€åŸºäºæ”¹è¿›å­¤ç«‹æ£®æ— (Isolation Forest) çš„å¼‚å¸¸æ£€æµ‹ (Separating) ä»¥åŠé™ç»´æŠ‘åˆ¶ (Mitigating) çš„ä¸‰é˜¶æ®µæµæ°´çº¿ï¼Œä¸”å…·æœ‰æ¨¡å‹æ— å…³ (Model-agnostic) å’Œè¡¥ä¸æ— å…³ (Patch-agnostic) çš„ç‰¹æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨é¢å¯¹ Google Adversarial Patch ç­‰å¼ºåŠ›æ”»å‡»æ—¶èƒ½æ¢å¤é«˜è¾¾ 77% çš„æ¨¡å‹å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒäº†æä½çš„å¹²å‡€æ ·æœ¬å‡†ç¡®ç‡æŸå¤±ã€‚æ­¤å¤–ï¼ŒPatchBlock åœ¨è®¡ç®—æ—¶é—´å’Œèƒ½è€—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„é˜²å¾¡æŠ€æœ¯ï¼Œä¸ºèµ„æºå—é™çš„å®æ—¶ EdgeAI åº”ç”¨æä¾›äº†å“è¶Šçš„ç¨³å¥æ€§å’Œå¯ç§»æ¤æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages, 5 figures, 5 tables, Accepted to DATE 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.00367v1",
      "published_date": "2026-01-01 15:04:16 UTC",
      "updated_date": "2026-01-01 15:04:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:08:21.205766+00:00"
    },
    {
      "arxiv_id": "2601.00366v1",
      "title": "BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics",
      "title_zh": "BERT-JEPAï¼šé€šè¿‡é‡ç»„ CLS åµŒå…¥å®ç°è¯­è¨€æ— å…³è¯­ä¹‰",
      "authors": [
        "Taj Gillin",
        "Adam Lalani",
        "Kenneth Zhang",
        "Marcel Mateos Salles"
      ],
      "abstract": "Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BERT-JEPA (BEPA)ï¼Œè¿™æ˜¯ä¸€ç§å°†è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ (Joint Embedding Predictive Architectures, JEPA) è®­ç»ƒç›®æ ‡æ•´åˆåˆ° BERT é£æ ¼æ¨¡å‹ä¸­çš„åˆ›æ–°è®­ç»ƒèŒƒå¼ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡å¼•å…¥ JEPA ç›®æ ‡æ¥è§£å†³ [CLS] åµŒå…¥ç©ºé—´åœ¨è‡ªç›‘ç£å­¦ä¹ ä¸­ç»å¸¸å‡ºç°çš„åç¼©é—®é¢˜ï¼Œä»è€Œä¼˜åŒ–è¯­ä¹‰ç‰¹å¾çš„æå–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒBEPA å°†åŸæœ¬å—é™çš„åµŒå…¥ç©ºé—´è½¬åŒ–ä¸ºä¸€ç§è¯­è¨€æ— å…³ (language-agnostic) çš„è¯­ä¹‰ç©ºé—´ï¼Œå®ç°äº†è·¨è¯­è¨€çš„è¯­ä¹‰å¯¹é½ã€‚è¿™ç§å…¨æ–°çš„ç»“æ„æœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹æå–è¯­è¨€ä¸å˜è¯­ä¹‰ (language-invariant semantics) çš„èƒ½åŠ›ã€‚å®éªŒæ•°æ®è¯æ˜ï¼ŒBERT-JEPA åœ¨å¤šé¡¹å¤šè¯­è¨€åŸºå‡†æµ‹è¯• (multilingual benchmarks) ä¸­å‡è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹çš„è¯­ä¹‰è¡¨ç¤ºå¹¶æ„å»ºæ›´é€šç”¨çš„å¤šè¯­è¨€æ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 10 figures, 10 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.00366v1",
      "published_date": "2026-01-01 14:59:58 UTC",
      "updated_date": "2026-01-01 14:59:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:08:03.783718+00:00"
    },
    {
      "arxiv_id": "2601.00360v1",
      "title": "Mapping Human Anti-collusion Mechanisms to Multi-agent AI",
      "title_zh": "å°†äººç±»åå…±è°‹æœºåˆ¶æ˜ å°„åˆ°å¤šæ™ºèƒ½ä½“ AI",
      "authors": [
        "Jamiu Adekunle Idowu",
        "Ahmed Almasoud",
        "Ayman Alfahid"
      ],
      "abstract": "As multi-agent AI systems become increasingly autonomous, evidence shows they can develop collusive strategies similar to those long observed in human markets and institutions. While human domains have accumulated centuries of anti-collusion mechanisms, it remains unclear how these can be adapted to AI settings. This paper addresses that gap by (i) developing a taxonomy of human anti-collusion mechanisms, including sanctions, leniency & whistleblowing, monitoring & auditing, market design, and governance and (ii) mapping them to potential interventions for multi-agent AI systems. For each mechanism, we propose implementation approaches. We also highlight open challenges, such as the attribution problem (difficulty attributing emergent coordination to specific agents) identity fluidity (agents being easily forked or modified) the boundary problem (distinguishing beneficial cooperation from harmful collusion) and adversarial adaptation (agents learning to evade detection).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½(Multi-agent AI)ç³»ç»Ÿæ—¥ç›Šæ˜¾ç°çš„å…±è°‹é£é™©ï¼Œå¹¶å°è¯•å°†äººç±»ç¤¾ä¼šçš„åå…±è°‹ç»éªŒå¼•å…¥AIé¢†åŸŸã€‚ç ”ç©¶è€…é¦–å…ˆæ„å»ºäº†ä¸€å¥—æ¶µç›–åˆ¶è£(sanctions)ã€å®½å¤§å¤„ç†ä¸ä¸¾æŠ¥(leniency & whistleblowing)ã€ç›‘æ§ä¸å®¡è®¡(monitoring & auditing)ã€å¸‚åœºè®¾è®¡(market design)åŠæ²»ç†(governance)çš„äººç±»åå…±è°‹æœºåˆ¶åˆ†ç±»æ³•ï¼Œå¹¶å°†å…¶æ˜ å°„ä¸ºé’ˆå¯¹AIç³»ç»Ÿçš„å¹²é¢„æ‰‹æ®µã€‚é’ˆå¯¹æ¯ç§æœºåˆ¶ï¼Œè®ºæ–‡å‡æå‡ºäº†å…·ä½“çš„å®ç°è·¯å¾„ï¼Œæ—¨åœ¨åº”å¯¹è‡ªä¸»æ™ºèƒ½ä½“ä¹‹é—´å¯èƒ½å½¢æˆçš„æœ‰å®³åä½œã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒäº†AIç¯å¢ƒä¸‹ç‰¹æœ‰çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å½’å› é—®é¢˜(attribution problem)ã€èº«ä»½æµåŠ¨æ€§(identity fluidity)ã€åŒºåˆ†è‰¯æ€§åä½œä¸æœ‰å®³å…±è°‹çš„è¾¹ç•Œé—®é¢˜(boundary problem)ä»¥åŠå¯¹æŠ—æ€§é€‚åº”(adversarial adaptation)ã€‚è¯¥å·¥ä½œä¸ºæ„å»ºæ›´å…·å—æ§æ€§å’Œé€æ˜åº¦çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†ç³»ç»Ÿæ€§çš„ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00360v1",
      "published_date": "2026-01-01 14:30:37 UTC",
      "updated_date": "2026-01-01 14:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:08:36.722623+00:00"
    },
    {
      "arxiv_id": "2601.06095v1",
      "title": "Deep Q-Network Based Resilient Drone Communication:Neutralizing First-Order Markov Jammers",
      "title_zh": "åŸºäºæ·±åº¦ Q ç½‘ç»œçš„éŸ§æ€§æ— äººæœºé€šä¿¡ï¼šå¯¹æŠ—ä¸€é˜¶é©¬å°”å¯å¤«å¹²æ‰°æœº",
      "authors": [
        "Andrii Grekhov",
        "Volodymyr Kharchenko",
        "Vasyl Kondratiuk"
      ],
      "abstract": "Deep Reinforcement Learning based solution for jamming communications using Frequency Hopping Spread Spectrum technology in a 16 channel radio environment is presented. Deep Q Network based transmitter continuously selects the next frequency hopping channel while facing first order reactive jamming, which uses observed transition statistics to predict and interrupt transmissions. Through self training, the proposed agent learns a uniform random frequency hopping policy that effectively neutralizes the predictive advantage of the jamming. In the presence of Rayleigh fading and additive noise, the impact of forward error correction Bose Chaudhuri Hocquenghem type codes is systematically evaluated, demonstrating that even moderate redundancy significantly reduces packet loss. Extensive visualization of the learning dynamics, channel utilization distribution, epsilon greedy decay, cumulative reward, BER and SNR evolution, and detailed packet loss tables confirms convergence to a near optimal jamming strategy. The results provide a practical framework for autonomous resilient communications in modern electronic warfare scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹16é€šé“æ— çº¿ç”µç¯å¢ƒä¸­çš„æ— äººæœºé€šä¿¡å¹²æ‰°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)çš„è·³é¢‘é€šä¿¡è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶åˆ©ç”¨æ·±åº¦Qç½‘ç»œ(Deep Q-Network, DQN)æ„å»ºå‘å°„æœºï¼Œåœ¨é¢å¯¹èƒ½å¤Ÿé€šè¿‡è§‚æµ‹è½¬ç§»ç»Ÿè®¡æ•°æ®é¢„æµ‹å¹¶ä¸­æ–­ä¼ è¾“çš„ä¸€é˜¶ååº”å¼å¹²æ‰°(First-Order Reactive Jamming)æ—¶ï¼ŒåŠ¨æ€é€‰æ‹©ä¸‹ä¸€è·³é¢‘ä¿¡é“ã€‚é€šè¿‡è‡ªè®­ç»ƒï¼Œè¯¥æ™ºèƒ½ä½“æˆåŠŸå­¦ä¹ åˆ°ä¸€ç§å‡åŒ€éšæœºè·³é¢‘ç­–ç•¥ï¼Œæœ‰æ•ˆä¸­å’Œäº†å¹²æ‰°è€…çš„é¢„æµ‹ä¼˜åŠ¿ã€‚åœ¨ç‘åˆ©è¡°è½(Rayleigh fading)å’ŒåŠ æ€§å™ªå£°ç¯å¢ƒä¸‹ï¼Œç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†Bose Chaudhuri Hocquenghem (BCH)å‹å‰å‘çº é”™ç çš„å½±å“ï¼Œè¯æ˜é€‚åº¦å†—ä½™å¯æ˜¾è‘—é™ä½ä¸¢åŒ…ç‡ã€‚å®éªŒé€šè¿‡å¯¹å­¦ä¹ åŠ¨åŠ›å­¦ã€ä¿¡é“åˆ©ç”¨ç‡åŠä¿¡å™ªæ¯”(SNR)æ¼”å˜ç­‰æŒ‡æ ‡çš„è¯¦ç»†åˆ†æï¼Œè¯å®äº†ç³»ç»Ÿèƒ½æ”¶æ•›è‡³è¿‘ä¼˜çš„æŠ—å¹²æ‰°ç­–ç•¥ã€‚è¯¥æˆæœä¸ºç°ä»£ç”µå­æˆ˜åœºæ™¯ä¸‹çš„è‡ªä¸»éŸ§æ€§é€šä¿¡æä¾›äº†ä¸€ä¸ªå®ç”¨çš„æŠ€æœ¯æ¡†æ¶ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "13 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.06095v1",
      "published_date": "2026-01-01 14:16:40 UTC",
      "updated_date": "2026-01-01 14:16:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:07.892629+00:00"
    },
    {
      "arxiv_id": "2601.00348v1",
      "title": "Robust Uncertainty Quantification for Factual Generation of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹äº‹å®æ€§ç”Ÿæˆçš„ç¨³å¥ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Yuhao Zhang",
        "Zhongliang Yang",
        "Linna Zhou"
      ],
      "abstract": "The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨ç”Ÿæˆäº‹å®å†…å®¹æ—¶é¢ä¸´çš„å¹»è§‰æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„ä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification)æ–¹æ³•åœ¨å¯¹æŠ—æ€§åœºæ™¯ä¸‹å­˜åœ¨æ˜æ˜¾çš„å¯é æ€§ç¼ºé™·ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½œè€…è®¾è®¡äº†ä¸€å¥—åŒ…å«è™šå‡åç§°çš„â€œé™·é˜±é—®é¢˜â€(Trap Questions)åœºæ™¯ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šäº‹å®ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶åˆ›æ–°æ€§åœ°æå‡ºäº†ä¸€ç§é²æ£’çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•(RU)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å››ç§ä¸åŒçš„æ¨¡å‹ä¸Šï¼ŒRUæ–¹æ³•ç›¸è¾ƒäºç°æœ‰æœ€ä¼˜åŸºçº¿æ–¹æ³•åœ¨ROCAUCå€¼ä¸Šå¹³å‡æå‡äº†0.1-0.2ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†é™·é˜±é—®é¢˜é›†çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºç¼“è§£LLMå¹»è§‰é—®é¢˜ã€æå‡ç³»ç»Ÿå¯ä¿¡åº¦æä¾›äº†æ–°çš„è§†è§’å’ŒæŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 tables, 5 figures, accepted to IJCNN 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.00348v1",
      "published_date": "2026-01-01 14:06:58 UTC",
      "updated_date": "2026-01-01 14:06:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:08:11.132449+00:00"
    },
    {
      "arxiv_id": "2601.00339v1",
      "title": "Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems",
      "title_zh": "é¢å‘é«˜éŸ§æ€§åˆ†å¸ƒå¼è®¡ç®—è¿ç»­ä½“ç³»ç»Ÿçš„ä»¿ç”Ÿæ™ºèƒ½ä½“è‡ªæ„ˆæ¡†æ¶",
      "authors": [
        "Alaa Saleh",
        "Praveen Kumar Donta",
        "Roberto Morabito",
        "Sasu Tarkoma",
        "Anders Lindgren",
        "Qiyang Zhang",
        "Schahram Dustdar",
        "Susanna Pirttikangas",
        "Lauri LovÃ©n"
      ],
      "abstract": "Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.",
      "tldr_zh": "é’ˆå¯¹åˆ†å¸ƒå¼è®¡ç®—è¿ç»­ä½“ç³»ç»Ÿ (Distributed Computing Continuum Systems, DCCS) åœ¨å¤„ç†ä»ç‰©è”ç½‘è®¾å¤‡åˆ°äº‘ç«¯å¼‚æ„èµ„æºæ—¶é¢ä¸´çš„å¤æ‚æ•…éšœæŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº† ReCiStï¼Œä¸€ç§å—ç”Ÿç‰©å¯å‘çš„å¤šæ™ºèƒ½ä½“è‡ªæˆ‘ä¿®å¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†äººç±»ç”Ÿç‰©ç³»ç»Ÿçš„æ­¢è¡€ã€ç‚ç—‡ã€å¢æ®–å’Œé‡å¡‘é˜¶æ®µè½¬åŒ–ä¸º Containmentã€Diagnosisã€Meta-Cognitive å’Œ Knowledge å››ä¸ªè®¡ç®—å±‚ï¼Œæ—¨åœ¨å®ç°é«˜åº¦éŸ§æ€§çš„ç³»ç»Ÿç®¡ç†ã€‚ReCiSt åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Language Model, LM) é©±åŠ¨çš„æ™ºèƒ½ä½“è§£é‡Šå¼‚æ„æ—¥å¿—ï¼Œåœ¨æœ€å°åŒ–äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹è‡ªåŠ¨æ‰§è¡Œæ•…éšœéš”ç¦»ã€å› æœè¯Šæ–­ã€è‡ªé€‚åº”æ¢å¤å’Œé•¿æœŸçŸ¥è¯†æ•´åˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReCiSt èƒ½å¤Ÿåœ¨æ•°åç§’å†…å®Œæˆè‡ªæˆ‘ä¿®å¤ï¼Œä¸”æ™ºèƒ½ä½“ CPU å ç”¨ç‡æœ€ä½ä»…ä¸º 10%ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº† LM åœ¨å¤„ç†ç³»ç»Ÿä¸ç¡®å®šæ€§æ–¹é¢çš„æ·±åº¦åˆ†æèƒ½åŠ›ï¼Œä¹Ÿé€šè¿‡å¾®æ™ºèƒ½ä½“ (micro-agents) çš„ååŒå·¥ä½œä¸ºæ„å»ºè‡ªé€‚åº”ã€å¯æ‰©å±•çš„åˆ†å¸ƒå¼ç³»ç»ŸéŸ§æ€§ç­–ç•¥æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.ET",
        "cs.MA",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00339v1",
      "published_date": "2026-01-01 13:30:38 UTC",
      "updated_date": "2026-01-01 13:30:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:01.386073+00:00"
    },
    {
      "arxiv_id": "2601.00921v1",
      "title": "Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease",
      "title_zh": "æ…¢æ€§é˜»å¡æ€§è‚ºç–¾ç—…éª¨éª¼è‚Œç»“å±€é¢„æµ‹çš„å®ç”¨å‡ ä½•ä¸é‡å­æ ¸æ–¹æ³•",
      "authors": [
        "Azadeh Alavi",
        "Hamidreza Khalili",
        "Stanley H. Chan",
        "Fatemeh Kouchmeshki",
        "Ross Vlahos"
      ],
      "abstract": "Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ…¢æ€§é˜»å¡æ€§è‚ºç–¾ç—…(COPD)ä¸­éª¨éª¼è‚ŒåŠŸèƒ½éšœç¢çš„é¢„æµ‹å»ºæ¨¡ï¼Œæ—¨åœ¨é€šè¿‡å¾®åˆ›ç”Ÿç‰©æ ‡å¿—ç‰©é¢„æµ‹è‚Œè‚‰é‡é‡(muscle weight)ã€æ¯”åŠ›é‡(specific force)åŠè‚Œè‚‰è´¨é‡æŒ‡æ•°(muscle quality index)ã€‚ç ”ç©¶å¯¹æ¯”äº†ç»å…¸åŸºçº¿æ¨¡å‹ã€åŸºäºStein divergenceçš„å‡ ä½•æ„ŸçŸ¥å¯¹ç§°æ­£å®š(SPD)æè¿°ç¬¦ï¼Œä»¥åŠé’ˆå¯¹ä½ç»´è¡¨æ ¼æ•°æ®è®¾è®¡çš„é‡å­æ ¸æ¨¡å‹(quantum kernel models)ã€‚åœ¨é¢„æµ‹è‚Œè‚‰é‡é‡çš„ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨è¡€æ¶²C-reactive proteinå’Œä¸­æ€§ç²’ç»†èƒè®¡æ•°ç­‰å››é¡¹å¯è§£é‡Šè¾“å…¥ï¼Œé‡å­æ ¸å²­å›å½’(quantum kernel ridge regression)å–å¾—äº†0.605çš„åˆ¤å®šç³»æ•°(coefficient of determination)ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå²­å›å½’ã€‚æ­¤å¤–ï¼Œå‡ ä½•æ„ŸçŸ¥çš„Stein-divergenceæ–¹æ³•åœ¨ä»…ä½¿ç”¨ç”Ÿç‰©æ ‡å¿—ç‰©çš„è®¾å®šä¸‹ä¹Ÿå®ç°äº†ç¨³å®šçš„æ€§èƒ½å¢ç›Šã€‚é’ˆå¯¹ä½è‚Œè‚‰é‡é‡çš„æ£€æµ‹ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•åœ¨ç­›é€‰è¯„ä¼°ä¸­è¾¾åˆ°äº†0.90çš„ROC-AUCã€‚å®éªŒç»“æœè¯æ˜ï¼Œå‡ ä½•ä¸é‡å­æ ¸æå‡(quantum kernel lifts)åœ¨ä½æ•°æ®ã€ä½ç‰¹å¾çš„ç”Ÿç‰©åŒ»å­¦é¢„æµ‹åœºæ™¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”èƒ½åŒæ—¶å…¼é¡¾æ¨¡å‹çš„å¯è§£é‡Šæ€§(interpretability)ä¸é€æ˜åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00921v1",
      "published_date": "2026-01-01 13:25:45 UTC",
      "updated_date": "2026-01-01 13:25:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:21.855092+00:00"
    },
    {
      "arxiv_id": "2601.00329v1",
      "title": "Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $\\ell_1$ Relaxations",
      "title_zh": "ç¨€ç–æ¦‚ç‡è”ç›Ÿç»“æ„ç”Ÿæˆï¼šè´å¶æ–¯è´ªå¿ƒè¿½è¸ªä¸ $\\ell_1$ æ¾å¼›",
      "authors": [
        "Angshul Majumdar"
      ],
      "abstract": "We study coalition structure generation (CSG) when coalition values are not given but must be learned from episodic observations. We model each episode as a sparse linear regression problem, where the realised payoff \\(Y_t\\) is a noisy linear combination of a small number of coalition contributions. This yields a probabilistic CSG framework in which the planner first estimates a sparse value function from \\(T\\) episodes, then runs a CSG solver on the inferred coalition set. We analyse two estimation schemes. The first, Bayesian Greedy Coalition Pursuit (BGCP), is a greedy procedure that mimics orthogonal matching pursuit. Under a coherence condition and a minimum signal assumption, BGCP recovers the true set of profitable coalitions with high probability once \\(T \\gtrsim K \\log m\\), and hence yields welfare-optimal structures. The second scheme uses an \\(\\ell_1\\)-penalised estimator; under a restricted eigenvalue condition, we derive \\(\\ell_1\\) and prediction error bounds and translate them into welfare gap guarantees. We compare both methods to probabilistic baselines and identify regimes where sparse probabilistic CSG is superior, as well as dense regimes where classical least-squares approaches are competitive.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è”ç›Ÿä»·å€¼æœªçŸ¥ä¸”éœ€é€šè¿‡è§‚æµ‹å­¦ä¹ çš„èƒŒæ™¯ä¸‹çš„è”ç›Ÿç»“æ„ç”Ÿæˆ(Coalition Structure Generation, CSG)é—®é¢˜ã€‚ç ”ç©¶å°†è§‚æµ‹å»ºæ¨¡ä¸ºç¨€ç–çº¿æ€§å›å½’(Sparse Linear Regression)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…ˆä¼°è®¡ç¨€ç–ä»·å€¼å‡½æ•°å†è¿è¡Œæ±‚è§£å™¨çš„æ¦‚ç‡CSGæ¡†æ¶ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†ä¸¤ç§ä¼°è®¡æ–¹æ¡ˆï¼šä¸€ç§æ˜¯ç±»ä¼¼äºæ­£äº¤åŒ¹é…è¿½è¸ªçš„è´å¶æ–¯è´ªå©ªè”ç›Ÿè¿½æ±‚(Bayesian Greedy Coalition Pursuit, BGCP)ç®—æ³•ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹èƒ½ä»¥é«˜æ¦‚ç‡æ¢å¤çœŸå®çš„ç›ˆåˆ©è”ç›Ÿå¹¶å®ç°ç¦åˆ©æœ€ä¼˜ï¼›å¦ä¸€ç§æ˜¯åŸºäº$\\ell_1$æƒ©ç½šä¼°è®¡å™¨($\\ell_1$-penalised estimator)çš„æ–¹æ³•ï¼Œå¹¶åœ¨å—é™ç‰¹å¾å€¼æ¡ä»¶ä¸‹ç»™å‡ºäº†ç¦åˆ©å·®è·(Welfare Gap)çš„ç†è®ºä¿è¯ã€‚é€šè¿‡ä¸æ¦‚ç‡åŸºå‡†çš„æ¯”è¾ƒï¼Œè¯¥ç ”ç©¶è¯†åˆ«äº†ç¨€ç–æ¦‚ç‡CSGä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„ç‰¹å®šä½“åˆ¶ï¼Œä¸ºä¸ç¡®å®šç¯å¢ƒä¸‹çš„åˆä½œåšå¼ˆæä¾›äº†æœ‰æ•ˆçš„è®¡ç®—æ–¹æ³•å’Œç†è®ºè¾¹ç•Œã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00329v1",
      "published_date": "2026-01-01 12:50:56 UTC",
      "updated_date": "2026-01-01 12:50:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:32.543512+00:00"
    },
    {
      "arxiv_id": "2601.00327v1",
      "title": "HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection",
      "title_zh": "HarmoniADï¼šååŒå±€éƒ¨ç»“æ„ä¸å…¨å±€è¯­ä¹‰çš„å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Naiqi Zhang",
        "Chuancheng Shi",
        "Jingtong Dou",
        "Wenhua Wu",
        "Fei Shen",
        "Jianhua Cao"
      ],
      "abstract": "Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šäº§å“è´¨é‡æ£€æµ‹ä¸­çš„å¼‚å¸¸æ£€æµ‹(Anomaly detection)é—®é¢˜ï¼Œæå‡ºäº†HarmoniADæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å±€éƒ¨ç»“æ„ä¸å…¨å±€è¯­ä¹‰ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚HarmoniAD é‡‡ç”¨äº†ä¸€ç§é¢‘ç‡å¼•å¯¼çš„åŒåˆ†æ”¯æ¶æ„(frequency-guided dual-branch framework)ï¼Œåˆ©ç”¨ CLIP ç¼–ç å™¨æå–ç‰¹å¾å¹¶å°†å…¶è§£è€¦ä¸ºé«˜é¢‘å’Œä½é¢‘è·¯å¾„ï¼Œå®ç°ç»“æ„ä¸è¯­ä¹‰çš„äº’è¡¥å»ºæ¨¡ã€‚å…¶ä¸­é«˜é¢‘åˆ†æ”¯é€šè¿‡ç»†ç²’åº¦ç»“æ„æ³¨æ„åŠ›æ¨¡å—(FSAM)å¢å¼ºçº¹ç†å’Œè¾¹ç¼˜ï¼Œä¸“é—¨ç”¨äºæ•æ‰å¾®å°çš„ç»“æ„å¼‚å¸¸ï¼›ä½é¢‘åˆ†æ”¯åˆ™åˆ©ç”¨å…¨å±€ç»“æ„ä¸Šä¸‹æ–‡æ¨¡å—(GSCM)æ•è·é•¿ç¨‹ä¾èµ–ï¼Œä»è€Œç»´æŒå…¨å±€è¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚ç»“åˆå¤šç±»åˆ«è”åˆè®­ç»ƒç­–ç•¥ï¼Œè¯¥æ–¹æ³•åœ¨ MVTec-ADã€VisA å’Œ BTAD ç­‰æƒå¨æ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHarmoniAD åœ¨å…¼é¡¾ç²¾ç»†ç»†èŠ‚ä¸å…¨å±€è¯­ä¹‰çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹çš„çµæ•åº¦ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00327v1",
      "published_date": "2026-01-01 12:45:45 UTC",
      "updated_date": "2026-01-01 12:45:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:30.873297+00:00"
    },
    {
      "arxiv_id": "2601.00324v1",
      "title": "Multiagent Reinforcement Learning for Liquidity Games",
      "title_zh": "é¢å‘æµåŠ¨æ€§åšå¼ˆçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Alicia Vidler",
        "Gal A. Kaminka"
      ],
      "abstract": "Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ç»“åˆ Liquidity Games ä¸ Rational Swarms ç†è®ºï¼Œæå‡ºäº†ä¸€ä¸ªç”¨äºé‡‘èå¸‚åœºæµåŠ¨æ€§å»ºæ¨¡çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‹¬ç«‹é‡‘èæ™ºèƒ½ä½“åœ¨åŒè¾¹èµ„äº§å¸‚åœºä¸­çš„è‡ªç»„ç»‡ä¸ç¨³å®šæ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ Markov team games ç»“æ„ï¼Œå¹¶åˆ©ç”¨ difference rewards æŠ€æœ¯å¼•å¯¼å»ä¸­å¿ƒåŒ–æ™ºèƒ½ä½“å°†ä¸ªä½“è‡ªåˆ©å­¦ä¹ ä¸å…¨å±€æµåŠ¨æ€§ç›®æ ‡å¯¹é½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è¿™ç§ Financial Swarm æ¨¡å‹ä¸‹ï¼Œäº¤æ˜“è€…æ— éœ€é€šè¿‡æ˜¾å¼åè°ƒæˆ–å…±è°‹ï¼Œä»…é ä¸ªä½“è¿½æ±‚æµåŠ¨æ€§æœ€å¤§åŒ–çš„è¡Œä¸ºå³å¯æ˜¾è‘—æå‡æ•´ä½“å¸‚åœºæ•ˆç‡ã€‚è¯¥æˆæœæˆåŠŸå®ç°äº†æ™ºèƒ½ä½“ä¸ªä½“ç›ˆåˆ©æ€§ä¸é›†ä½“å¸‚åœºæ•ˆèƒ½çš„åŒæ­¥ä¼˜åŒ–ï¼Œä¸ºç†æ€§ã€ç‹¬ç«‹æ™ºèƒ½ä½“çš„å»ºæ¨¡åŠå¸‚åœºè®¾è®¡æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.00324v1",
      "published_date": "2026-01-01 12:36:28 UTC",
      "updated_date": "2026-01-01 12:36:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:28.452973+00:00"
    },
    {
      "arxiv_id": "2601.03281v1",
      "title": "$Î±^3$-Bench: A Unified Benchmark of Safety, Robustness, and Efficiency for LLM-Based UAV Agents over 6G Networks",
      "title_zh": "$Î±^3$-Benchï¼šé¢å‘ 6G ç½‘ç»œä¸‹åŸºäº LLM çš„æ— äººæœºæ™ºèƒ½ä½“å®‰å…¨æ€§ã€é²æ£’æ€§ä¸æ•ˆç‡çš„ç»Ÿä¸€åŸºå‡†",
      "authors": [
        "Mohamed Amine Ferrag",
        "Abderrahmane Lakas",
        "Merouane Debbah"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used as high level controllers for autonomous Unmanned Aerial Vehicle (UAV) missions. However, existing evaluations rarely assess whether such agents remain safe, protocol compliant, and effective under realistic next generation networking constraints. This paper introduces $Î±^3$-Bench, a benchmark for evaluating LLM driven UAV autonomy as a multi turn conversational reasoning and control problem operating under dynamic 6G conditions. Each mission is formulated as a language mediated control loop between an LLM based UAV agent and a human operator, where decisions must satisfy strict schema validity, mission policies, speaker alternation, and safety constraints while adapting to fluctuating network slices, latency, jitter, packet loss, throughput, and edge load variations.\n  To reflect modern agentic workflows, $Î±^3$-Bench integrates a dual action layer supporting both tool calls and agent to agent coordination, enabling evaluation of tool use consistency and multi agent interactions. We construct a large scale corpus of 113k conversational UAV episodes grounded in UAVBench scenarios and evaluate 17 state of the art LLMs using a fixed subset of 50 episodes per scenario under deterministic decoding. We propose a composite $Î±^3$ metric that unifies six pillars: Task Outcome, Safety Policy, Tool Consistency, Interaction Quality, Network Robustness, and Communication Cost, with efficiency normalized scores per second and per thousand tokens. Results show that while several models achieve high mission success and safety compliance, robustness and efficiency vary significantly under degraded 6G conditions, highlighting the need for network aware and resource efficient LLM based UAV agents. The dataset is publicly available on GitHub : https://github.com/maferrag/AlphaBench",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† $Î±^3$-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ 6G ç½‘ç»œç¯å¢ƒä¸‹åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ— äººæœº (UAV) æ™ºèƒ½ä½“å®‰å…¨æ€§ã€é²æ£’æ€§å’Œæ•ˆç‡çš„ç»Ÿä¸€è¯„ä¼°åŸºå‡†ã€‚è¯¥åŸºå‡†å°†æ— äººæœºè‡ªä¸»ä»»åŠ¡å»ºæ¨¡ä¸ºå¤šè½®å¯¹è¯æ¨ç†ä¸æ§åˆ¶é—®é¢˜ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨å»¶è¿Ÿã€æŠ–åŠ¨ã€ä¸¢åŒ…ç­‰åŠ¨æ€ 6G ç½‘ç»œæ¡ä»¶ä¸‹ï¼Œä¸¥æ ¼éµå®ˆä»»åŠ¡ç­–ç•¥ã€å®‰å…¨çº¦æŸåŠé€šä¿¡åè®®ã€‚æ¡†æ¶é›†æˆäº†æ”¯æŒå·¥å…·è°ƒç”¨ (tool calls) ä¸å¤šæ™ºèƒ½ä½“åä½œçš„åŒå±‚åŠ¨ä½œç³»ç»Ÿï¼Œå¹¶åŸºäº 11.3 ä¸‡ä¸ªå¯¹è¯ç‰‡æ®µæ„å»ºäº†å¤§è§„æ¨¡è¯­æ–™åº“ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç»¼åˆçš„ $Î±^3$ åº¦é‡æŒ‡æ ‡ï¼Œä»ä»»åŠ¡ç»“æœã€å®‰å…¨æ”¿ç­–ã€å·¥å…·ä¸€è‡´æ€§ã€äº¤äº’è´¨é‡ã€ç½‘ç»œé²æ£’æ€§åŠé€šä¿¡æˆæœ¬å…­ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚å¯¹ 17 ä¸ªå‰æ²¿ LLMs çš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶éƒ¨åˆ†æ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸç‡å’Œå®‰å…¨æ€§ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ 6G ç½‘ç»œé€€åŒ–æ—¶çš„é²æ£’æ€§ä¸èµ„æºæ•ˆç‡å·®å¼‚æ˜¾è‘—ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†å¼€å‘ç½‘ç»œæ„ŸçŸ¥ (network aware) ä¸”èµ„æºé«˜æ•ˆçš„æ— äººæœºæ™ºèƒ½ä½“å¯¹äºæœªæ¥ 6G åº”ç”¨çš„é‡è¦æ€§ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "20 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.03281v1",
      "published_date": "2026-01-01 12:07:06 UTC",
      "updated_date": "2026-01-01 12:07:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:33.873774+00:00"
    },
    {
      "arxiv_id": "2601.00920v2",
      "title": "MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs",
      "title_zh": "MODEï¼šåŸºäºä½ç§©ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹å¢å¼º Mamba çš„é«˜æ•ˆæ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Xingsheng Chen",
        "Regina Zhang",
        "Bo Gao",
        "Xingwei He",
        "Xiaofeng Liu",
        "Pietro Lio",
        "Kwok-Yan Lam",
        "Siu-Ming Yiu"
      ],
      "abstract": "Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MODEï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ä½ç§©ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ (Low-Rank Neural ODEs) ä¸å¢å¼ºå‹ Mamba æ¶æ„ç›¸ç»“åˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•åœ¨å¤„ç†é•¿ç¨‹ä¾èµ–å’Œä¸è§„åˆ™é‡‡æ ·æ•°æ®æ—¶é¢ä¸´çš„æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§å¹³è¡¡éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡çº¿æ€§æ ‡è®°å±‚ (Linear Tokenization Layer) è½¬æ¢è¾“å…¥åºåˆ—ï¼Œéšååˆ©ç”¨åŒ…å«å› æœå·ç§¯ (Causal Convolution) å’Œ SiLU æ¿€æ´»å‡½æ•°çš„ Mamba ç¼–ç å™¨å—è¿›è¡Œå¤„ç†ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†ä½ç§©ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹å¢å¼ºå±‚ï¼Œé€šè¿‡ä½ç§©å…¬å¼åœ¨ä¿æŒå¼ºè¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œä»è€Œé«˜æ•ˆæ•æ‰æ—¶åºåŠ¨åŠ›å­¦ã€‚æ­¤å¤–ï¼Œå—ä¼ª ODE åŠ¨åŠ›å­¦å¯å‘çš„åˆ†æ®µé€‰æ‹©æ€§æ‰«ææœºåˆ¶ (segmented selective scanning mechanism) èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨æ˜¾è‘—å­åºåˆ—ï¼Œè¿›ä¸€æ­¥æå‡äº†é•¿åºåˆ—å»ºæ¨¡çš„å¯æ‰©å±•æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒMODE åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºé•¿çŸ­æœŸæ—¶é—´åºåˆ—å»ºæ¨¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 6 figures, and 3 tables. Updated description and explanations, and correct some typos",
      "pdf_url": "https://arxiv.org/pdf/2601.00920v2",
      "published_date": "2026-01-01 11:23:20 UTC",
      "updated_date": "2026-01-11 11:54:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:36.472652+00:00"
    },
    {
      "arxiv_id": "2601.00307v1",
      "title": "VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning",
      "title_zh": "VisNetï¼šåŸºäº Alpha æ•£åº¦æŸå¤±ã€ç‰¹å¾èåˆä¸åŠ¨æ€å¤šä»»åŠ¡å­¦ä¹ çš„é«˜æ•ˆè¡Œäººé‡è¯†åˆ«",
      "authors": [
        "Anns Ijaz",
        "Muhammad Azeem Javed"
      ],
      "abstract": "Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VisNetæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¡Œäººé‡è¯†åˆ«(Person re-identification, ReID)åœ¨ç›‘æ§å’Œç§»åŠ¨åº”ç”¨ä¸­é¢ä¸´çš„é«˜ç²¾åº¦ä¸ä½è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡èåˆResNet50çš„å¤šå°ºåº¦ç‰¹å¾å¹¶åº”ç”¨è‡ªåŠ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¸å¢åŠ å¹¶è¡Œè·¯å¾„çš„æƒ…å†µä¸‹æå‡äº†ç‰¹å¾æå–æ•ˆç‡ã€‚ç ”ç©¶å¼•å…¥äº†åŸºäºè§£å‰–å­¦èº«ä½“åˆ†åŒºçš„è¯­ä¹‰èšç±»(Semantic clustering)å’ŒåŠ¨æ€æƒé‡å¹³å‡(Dynamic weight averaging)æŠ€æœ¯ï¼Œä»¥å¹³è¡¡åˆ†ç±»è¯­ä¹‰æ­£åˆ™åŒ–å¹¶å¼ºåŒ–ç©ºé—´çº¦æŸã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨FIDIæŸå¤±å‡½æ•°(Alpha-Divergence Loss)æ”¹è¿›äº†åº¦é‡å­¦ä¹ ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒVisNetåœ¨Market-1501æ•°æ®é›†ä¸Šå–å¾—äº†87.05%çš„Rank-1å’Œ77.65%çš„mAPï¼Œä¸”å‚æ•°é‡(32.41M)ä¸è®¡ç®—é‡(4.601 GFLOPs)ä¿æŒåœ¨è¾ƒä½æ°´å¹³ã€‚è¯¥æ–¹æ³•ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å®æ—¶Person re-identificationéƒ¨ç½²æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00307v1",
      "published_date": "2026-01-01 11:06:11 UTC",
      "updated_date": "2026-01-01 11:06:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:10:27.141708+00:00"
    },
    {
      "arxiv_id": "2601.00306v1",
      "title": "The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ‚–è®ºï¼šGenAIã€ä¿¡ä»»çš„ä¾µèš€ã€ä¿¡æ¯æ ¸å®ä½“ç³»çš„ç“¦è§£ä¸çœŸç›¸çš„æ¶ˆäº¡",
      "authors": [
        "Emilio Ferrara"
      ],
      "abstract": "Generative AI (GenAI) now produces text, images, audio, and video that can be perceptually convincing at scale and at negligible marginal cost. While public debate often frames the associated harms as \"deepfakes\" or incremental extensions of misinformation and fraud, this view misses a broader socio-technical shift: GenAI enables synthetic realities; coherent, interactive, and potentially personalized information environments in which content, identity, and social interaction are jointly manufactured and mutually reinforcing. We argue that the most consequential risk is not merely the production of isolated synthetic artifacts, but the progressive erosion of shared epistemic ground and institutional verification practices as synthetic content, synthetic identity, and synthetic interaction become easy to generate and hard to audit. This paper (i) formalizes synthetic reality as a layered stack (content, identity, interaction, institutions), (ii) expands a taxonomy of GenAI harms spanning personal, economic, informational, and socio-technical risks, (iii) articulates the qualitative shifts introduced by GenAI (cost collapse, throughput, customization, micro-segmentation, provenance gaps, and trust erosion), and (iv) synthesizes recent risk realizations (2023-2025) into a compact case bank illustrating how these mechanisms manifest in fraud, elections, harassment, documentation, and supply-chain compromise. We then propose a mitigation stack that treats provenance infrastructure, platform governance, institutional workflow redesign, and public resilience as complementary rather than substitutable, and outline a research agenda focused on measuring epistemic security. We conclude with the Generative AI Paradox: as synthetic media becomes ubiquitous, societies may rationally discount digital evidence altogether.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) å¸¦æ¥çš„æ·±è¿œç¤¾ä¼šæŠ€æœ¯å˜é©ï¼Œé‡ç‚¹åˆ†æäº†ä¿¡ä»»ä¾µèš€ã€ä¿¡æ¯æ ¸æŸ¥å¤±æ•ˆåŠçœŸç†æ¶ˆè§£ç­‰æ ¸å¿ƒå¨èƒã€‚æ–‡ç« æå‡º GenAI çš„ä¸»è¦é£é™©ä¸åœ¨äºå­¤ç«‹çš„åˆæˆåˆ¶å“ï¼Œè€Œåœ¨äºå…¶æ„å»ºçš„â€œåˆæˆç°å®â€(synthetic realities)ï¼Œå³å†…å®¹ã€èº«ä»½ä¸ç¤¾äº¤äº’åŠ¨è¢«å…±åŒåˆ¶é€ å¹¶ç›¸äº’å¼ºåŒ–çš„ç¯å¢ƒã€‚ç ”ç©¶é€šè¿‡åˆ†å±‚æ¶æ„å½¢å¼åŒ–äº†åˆæˆç°å®ï¼Œå¹¶æ‰©å±•äº†ä¸€å¥—æ¶µç›–ä¸ªäººã€ç»æµåŠç¤¾ä¼šæŠ€æœ¯é£é™©çš„ GenAI å±å®³åˆ†ç±»æ³•ã€‚è®ºæ–‡è¯¦ç»†åˆ†æäº† GenAI å¼•å…¥çš„å®šæ€§è½¬å˜ï¼ŒåŒ…æ‹¬æˆæœ¬éª¤é™ (cost collapse)ã€å¾®ç»†åˆ† (micro-segmentation) ä»¥åŠæº¯æºç¼ºå¤± (provenance gaps)ã€‚åŸºäº 2023 è‡³ 2025 å¹´é—´çš„é£é™©å®ä¾‹ï¼Œä½œè€…æå‡ºäº†ä¸€å¥—ç»“åˆæº¯æºåŸºç¡€è®¾æ–½ (provenance infrastructure)ã€å¹³å°æ²»ç†ã€æœºæ„å·¥ä½œæµå†è®¾è®¡å’Œå…¬ä¼—éŸ§æ€§çš„ç»¼åˆç¼“è§£ç­–ç•¥ã€‚æœ€åï¼Œæ–‡ç« æ­ç¤ºäº†â€œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ‚–è®ºâ€(Generative AI Paradox)ï¼Œå³éšç€åˆæˆåª’ä½“æ— å¤„ä¸åœ¨ï¼Œç¤¾ä¼šå¯èƒ½ä¼šå› ä¸ºæ— æ³•å®¡è®¡çœŸä¼ªè€Œç†æ€§åœ°å½»åº•å¦å®šæ‰€æœ‰æ•°å­—è¯æ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00306v1",
      "published_date": "2026-01-01 10:58:51 UTC",
      "updated_date": "2026-01-01 10:58:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:46.545115+00:00"
    },
    {
      "arxiv_id": "2601.00303v1",
      "title": "DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection",
      "title_zh": "DepFlowï¼šç”¨äºç¼“è§£æŠ‘éƒæ£€æµ‹ä¸­è¯­ä¹‰åç½®çš„è§£è€¦è¯­éŸ³ç”Ÿæˆ",
      "authors": [
        "Yuxin Li",
        "Xiangyu Zhang",
        "Yifei Li",
        "Zhiwei Guo",
        "Haoyang Zhang",
        "Eng Siong Chng",
        "Cuntai Guan"
      ],
      "abstract": "Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŠ‘éƒç—‡æ£€æµ‹æ¨¡å‹åœ¨å¤„ç†ä¼ªè£…æŠ‘éƒ(Camouflaged Depression)æ—¶å› è¿‡åº¦ä¾èµ–è¯­ä¹‰å€¾å‘è€Œäº§ç”Ÿçš„è¯­ä¹‰åå·®(semantic bias)é—®é¢˜ï¼Œæå‡ºäº†DepFlowæ¡†æ¶ã€‚DepFlowæ˜¯ä¸€ä¸ªåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µçš„å—æŠ‘éƒæ¡ä»¶çº¦æŸçš„æ–‡æœ¬è½¬è¯­éŸ³(text-to-speech)æ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡Depression Acoustic Encoderåˆ©ç”¨å¯¹æŠ—è®­ç»ƒå­¦ä¹ ä¸è¯´è¯äººå’Œå†…å®¹æ— å…³çš„æŠ‘éƒåµŒå…¥(depression embeddings)ï¼Œåœ¨ä¿ç•™æŠ‘éƒåŒºåˆ†åº¦çš„åŒæ—¶å®ç°äº†æœ‰æ•ˆçš„ç‰¹å¾è§£è€¦ã€‚æ¥ç€ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºFiLMè°ƒåˆ¶çš„flow-matching TTSæ¨¡å‹å°†è¿™äº›åµŒå…¥æ³¨å…¥åˆæˆè¿‡ç¨‹ï¼Œå¹¶ç»“åˆåŸºäºåŸå‹çš„ä¸¥é‡ç¨‹åº¦æ˜ å°„æœºåˆ¶ï¼Œå®ç°äº†å¯¹æŠ‘éƒè¿ç»­ä½“çš„å—æ§æ“çºµã€‚åˆ©ç”¨è¯¥æ¡†æ¶æ„å»ºçš„é¢å‘ä¼ªè£…æŠ‘éƒçš„å¢å¼ºæ•°æ®é›†CDoAï¼Œé€šè¿‡åˆ¶é€ å£°å­¦ä¸è¯­ä¹‰çš„å¤±é…ï¼Œåœ¨ä¸‰ç§æ£€æµ‹æ¶æ„ä¸Šåˆ†åˆ«æå‡äº†9%ã€12%å’Œ5%çš„macro-F1å¾—åˆ†ï¼Œä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„å¢å¼ºç­–ç•¥ã€‚DepFlowä¸ä»…æå‡äº†æ£€æµ‹æ¨¡å‹çš„é²æ£’æ€§ï¼Œè¿˜ä¸ºä¸´åºŠæ•°æ®ç¨€ç¼ºèƒŒæ™¯ä¸‹çš„å¯¹è¯ç³»ç»Ÿå’Œæ¨¡æ‹Ÿè¯„ä¼°æä¾›äº†å¯æ§çš„è¯­éŸ³åˆæˆå¹³å°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00303v1",
      "published_date": "2026-01-01 10:44:38 UTC",
      "updated_date": "2026-01-01 10:44:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:49.601872+00:00"
    },
    {
      "arxiv_id": "2601.00290v1",
      "title": "ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization",
      "title_zh": "ClinicalReTrialï¼šç”¨äºä¸´åºŠè¯•éªŒæ–¹æ¡ˆä¼˜åŒ–çš„è‡ªæˆ‘æ¼”è¿›äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“",
      "authors": [
        "Sixue Xing",
        "Xuanye Xia",
        "Kerui Wu",
        "Meng Jiang",
        "Jintai Chen",
        "Tianfan Fu"
      ],
      "abstract": "Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ClinicalReTrialï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä¸´åºŠè¯•éªŒæ–¹æ¡ˆ(Clinical Trial Protocol)ä¼˜åŒ–çš„è‡ªæˆ‘æ¼”åŒ–äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“(Self-Evolving AI Agent)æ¡†æ¶ã€‚é’ˆå¯¹ä¸´åºŠè¯•éªŒä¸­å› æ–¹æ¡ˆè®¾è®¡ç¼ºé™·å¯¼è‡´çš„å¤±è´¥é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†ä¸´åºŠè¯•éªŒæ¨ç†è§†ä¸ºä¸€ä¸ªè¿­ä»£å¼çš„æ–¹æ¡ˆé‡æ–°è®¾è®¡(Protocol Redesign)é—®é¢˜ã€‚ClinicalReTrialåœ¨ä¸€ä¸ªé—­ç¯ä¸”ç”±å¥–åŠ±é©±åŠ¨çš„ä¼˜åŒ–æ¡†æ¶ä¸­é›†æˆäº†å¤±è´¥è¯Šæ–­(Failure Diagnosis)ã€å®‰å…¨æ„ŸçŸ¥ä¿®æ”¹(Safety-Aware Modification)å’Œå€™é€‰è¯„ä¼°(Candidate Evaluation)ã€‚é€šè¿‡å°†ç»“æœé¢„æµ‹æ¨¡å‹ä½œä¸ºæ¨¡æ‹Ÿç¯å¢ƒï¼Œè¯¥æ¡†æ¶å®ç°äº†ä½æˆæœ¬çš„æ–¹æ¡ˆä¿®æ”¹è¯„ä¼°ï¼Œå¹¶ä¸ºæŒç»­çš„è‡ªæˆ‘æ”¹è¿›æä¾›äº†ç¨ å¯†çš„å¥–åŠ±ä¿¡å·ã€‚ä¸ºäº†æé«˜æ¢ç´¢æ•ˆç‡ï¼Œç³»ç»Ÿç»´æŠ¤äº†å±‚æ¬¡åŒ–å­˜å‚¨(Hierarchical Memory)ï¼Œç”¨äºæ•è·è¯•éªŒå†…çš„åé¦ˆå¹¶æå–è·¨è¯•éªŒçš„å¯è¿ç§»é‡æ–°è®¾è®¡æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒClinicalReTrialæˆåŠŸæ”¹è¿›äº†83.3%çš„è¯•éªŒæ–¹æ¡ˆï¼Œå¹³å‡æˆåŠŸæ¦‚ç‡æå‡äº†5.7%ã€‚å›é¡¾æ€§æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œè¯¥æ¨¡å‹å‘ç°çš„é‡æ–°è®¾è®¡ç­–ç•¥ä¸ç°å®ä¸–ç•Œçš„ä¸´åºŠè¯•éªŒä¿®æ”¹å…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00290v1",
      "published_date": "2026-01-01 10:11:58 UTC",
      "updated_date": "2026-01-01 10:11:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:09:48.707616+00:00"
    },
    {
      "arxiv_id": "2601.00286v1",
      "title": "Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies",
      "title_zh": "è¿ˆå‘åŸºäºæ·±åº¦å­¦ä¹ ä¸ä¸å¹³è¡¡æ„ŸçŸ¥ç­–ç•¥çš„çš®è‚¤ç—…è‡ªåŠ¨é‰´åˆ«è¯Šæ–­",
      "authors": [
        "Ali Anaissi",
        "Ali Braytee",
        "Weidong Huang",
        "Junaid Akram",
        "Alaa Farhat",
        "Jie Hua"
      ],
      "abstract": "As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç—…æ—¥ç›Šå¢å¤šè€Œä¸“ä¸šåŒ»ç”Ÿèµ„æºæœ‰é™çš„ç°çŠ¶ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹  (Deep Learning) çš„æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°çš®è‚¤ç—…çš„è‡ªåŠ¨åˆ†ç±»ä¸é‰´åˆ«è¯Šæ–­ã€‚é€šè¿‡åœ¨å…¬å¼€çš®è‚¤ç—…å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒ (Pretraining)ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæå–å…³é”®è§†è§‰ç‰¹å¾å¹¶å¯¹å¤šç§çš®è‚¤ç—…æ¡ˆä¾‹è¿›è¡Œå‡†ç¡®åˆ†ç±»ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¼€å‘è¿‡ç¨‹ä¸­ä¸æ–­ä¼˜åŒ–æ¨¡å‹æ¶æ„ï¼Œæ”¹è¿›æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œå¹¶é‡‡ç”¨äº†é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼º (Data Augmentation) æŠ€æœ¯ä»¥å¼ºåŒ–æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆæ„å»ºçš„åŸºäº Swin Transformer çš„æ¨¡å‹åœ¨ ISIC2019 æ•°æ®é›†çš„å…«ç±»çš®è‚¤ç—…å˜åˆ†ç±»ä¸­å®ç°äº† 87.71% çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº†è¯¥æ¨¡å‹ä½œä¸ºä¸´åºŠåŒ»ç”Ÿè¾…åŠ©è¯Šæ–­å·¥å…·åŠæ‚£è€…è‡ªæˆ‘è¯„ä¼°æ‰‹æ®µçš„æ½œåŠ›ï¼Œä¸ºæå‡çš®è‚¤ç—…è¯Šç–—æ•ˆç‡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The 23rd Australasian Data Science and Machine Learning Conference (AusDM'25)",
      "pdf_url": "https://arxiv.org/pdf/2601.00286v1",
      "published_date": "2026-01-01 09:53:44 UTC",
      "updated_date": "2026-01-01 09:53:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:23.581375+00:00"
    },
    {
      "arxiv_id": "2601.00282v1",
      "title": "Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦ä»èƒ½è¿›è¡Œè‡ªè§£é‡Šï¼Ÿé‡åŒ–å¯¹è‡ªè§£é‡Šå½±å“çš„æ¢ç©¶",
      "authors": [
        "Qianli Wang",
        "Nils Feldhus",
        "Pepa Atanasova",
        "Fedor Splitt",
        "Simon Ostermann",
        "Sebastian MÃ¶ller",
        "Vera Schmitt"
      ],
      "abstract": "Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\\%) and faithfulness (up to 2.38\\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†é‡åŒ–(Quantization)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªæˆ‘è§£é‡Š(Self-Explanations, SEs)çš„å½±å“ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€æ—¨åœ¨æå‡æ¨¡å‹é€æ˜åº¦é¢†åŸŸçš„è®¤çŸ¥ç©ºç™½ã€‚é€šè¿‡å¯¹è‡ªç„¶è¯­è¨€è§£é‡Š(Natural Language Explanations, NLEs)å’Œåäº‹å®ç¤ºä¾‹è¿›è¡Œå¤šç»´åº¦åˆ†æï¼Œç ”ç©¶è¯„ä¼°äº†ä¸‰ç§ä¸»æµé‡åŒ–æŠ€æœ¯åœ¨ä¸åŒä½å®½ä¸‹çš„è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œé‡åŒ–ä¼šå¯¼è‡´è‡ªæˆ‘è§£é‡Šçš„è´¨é‡(Quality)å’Œå¿ å®åº¦(Faithfulness)å‡ºç°è½»å¾®ä¸‹é™ï¼Œä¸”ç”¨æˆ·ç ”ç©¶è¡¨æ˜å…¶è¿è´¯æ€§ä¸å¯ä¿¡åº¦ä¹Ÿæœ‰æ‰€å‡æŸã€‚ç›¸è¾ƒäºå°æ¨¡å‹ï¼Œå¤§æ¨¡å‹åœ¨é‡åŒ–è¿‡ç¨‹ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„å¿ å®åº¦ç»´æŒèƒ½åŠ›ï¼Œä½†åœ¨è§£é‡Šè´¨é‡ä¸Šå¹¶æ— æ˜æ˜¾ä¼˜åŠ¿ã€‚ç”±äºæ²¡æœ‰ä¸€ç§é‡åŒ–æ–¹æ³•èƒ½åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œç ”ç©¶å»ºè®®é’ˆå¯¹å…·ä½“åœºæ™¯ï¼ˆå°¤å…¶æ˜¯å¯¹é‡åŒ–æ›´ä¸ºæ•æ„Ÿçš„NLEsï¼‰è¿›è¡Œè´¨é‡éªŒè¯ã€‚æ€»ä½“è€Œè¨€ï¼Œé‡åŒ–å¸¦æ¥çš„è‡ªæˆ‘è§£é‡Šæ€§èƒ½é€€åŒ–ç›¸å¯¹è¾ƒå°ï¼Œè¯æ˜äº†å…¶åœ¨æ¨¡å‹å‹ç¼©ä¸­çš„æŒç»­é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "In submission",
      "pdf_url": "https://arxiv.org/pdf/2601.00282v1",
      "published_date": "2026-01-01 09:50:01 UTC",
      "updated_date": "2026-01-01 09:50:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:10:42.994732+00:00"
    },
    {
      "arxiv_id": "2601.00277v1",
      "title": "Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics",
      "title_zh": "å•ç»†èƒåŸºå› ç»„å­¦ä¸­é¢„å¤„ç†ä¸æ•´åˆæ–¹æ³•çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ali Anaissi",
        "Seid Miad Zandavi",
        "Weidong Huang",
        "Junaid Akram",
        "Basem Suleiman",
        "Ali Braytee",
        "Jie Hua"
      ],
      "abstract": "Single-cell data analysis has the potential to revolutionize personalized medicine by characterizing disease-associated molecular changes at the single-cell level. Advanced single-cell multimodal assays can now simultaneously measure various molecules (e.g., DNA, RNA, Protein) across hundreds of thousands of individual cells, providing a comprehensive molecular readout. A significant analytical challenge is integrating single-cell measurements across different modalities. Various methods have been developed to address this challenge, but there has been no systematic evaluation of these techniques with different preprocessing strategies. This study examines a general pipeline for single-cell data analysis, which includes normalization, data integration, and dimensionality reduction. The performance of different algorithm combinations often depends on the dataset sizes and characteristics. We evaluate six datasets across diverse modalities, tissues, and organisms using three metrics: Silhouette Coefficient Score, Adjusted Rand Index, and Calinski-Harabasz Index. Our experiments involve combinations of seven normalization methods, four dimensional reduction methods, and five integration methods. The results show that Seurat and Harmony excel in data integration, with Harmony being more time-efficient, especially for large datasets. UMAP is the most compatible dimensionality reduction method with the integration techniques, and the choice of normalization method varies depending on the integration method used.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç»†èƒåŸºå› ç»„å­¦ä¸­çš„æ•°æ®é¢„å¤„ç†ä¸æ•´åˆæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ•°æ®æ•´åˆç¼ºä¹ç»Ÿä¸€è¯„ä»·æ ‡å‡†çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨è·¨ç‰©ç§ã€è·¨ç»„ç»‡çš„6ä¸ªæ•°æ®é›†ä¸Šï¼Œå¯¹7ç§ normalizationã€4ç§ dimensional reduction ä»¥åŠ5ç§ integration æ–¹æ³•çš„å¤šç§ç»„åˆè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚é€šè¿‡ä½¿ç”¨ Silhouette Coefficient Scoreã€Adjusted Rand Index å’Œ Calinski-Harabasz Index ç­‰æŒ‡æ ‡è¿›è¡Œè¡¡é‡ï¼Œç ”ç©¶å‘ç° Seurat å’Œ Harmony åœ¨æ•°æ® integration æ–¹é¢è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚å®éªŒè¯æ˜ Harmony åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶å…·æœ‰æ˜¾è‘—çš„æ—¶é—´æ•ˆç‡ä¼˜åŠ¿ï¼Œè€Œ UMAP è¢«è¯å®æ˜¯ä¸å„ç±» integration æŠ€æœ¯å…¼å®¹æ€§æœ€å¥½çš„ dimensional reduction æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼ºè°ƒ normalization æ–¹æ³•çš„é€‰æ‹©åº”æ ¹æ®å…·ä½“çš„ integration æ–¹æ¡ˆè¿›è¡Œè°ƒæ•´ï¼Œä¸ºæ„å»ºé«˜æ•ˆçš„å•ç»†èƒåˆ†æç®¡çº¿æä¾›äº†å…³é”®æŒ‡å¯¼ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "The 23rd Australasian Data Science and Machine Learning Conference (AusDM'25)",
      "pdf_url": "https://arxiv.org/pdf/2601.00277v1",
      "published_date": "2026-01-01 09:28:56 UTC",
      "updated_date": "2026-01-01 09:28:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:00.412137+00:00"
    },
    {
      "arxiv_id": "2601.00269v1",
      "title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering",
      "title_zh": "FaithSCANï¼šé¢å‘å¿ å®è§†è§‰é—®ç­”çš„æ¨¡å‹é©±åŠ¨å•æ¬¡å¹»è§‰æ£€æµ‹",
      "authors": [
        "Chaodong Tong",
        "Qi Zhang",
        "Chen Li",
        "Lei Jiang",
        "Yanbing Liu"
      ],
      "abstract": "Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FaithSCANï¼Œä¸€ç§è½»é‡çº§çš„æ¨¡å‹é©±åŠ¨å•æ¬¡æ£€æµ‹ç½‘ç»œï¼Œæ—¨åœ¨è¯†åˆ«è§†è§‰é—®ç­” (Visual Question Answering) ä¸­è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models) äº§ç”Ÿçš„å¿ å®æ€§å¹»è§‰é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ£€æµ‹æ–¹æ³•åœ¨è®¡ç®—å¼€é”€å’Œå†…éƒ¨ä¿¡å·æŒ–æ˜æ–¹é¢çš„å±€é™æ€§ï¼ŒFaithSCAN æ•´åˆäº† Token-level Decoding Uncertaintyã€Intermediate Visual Representations å’Œ Cross-modal Alignment Features ç­‰æ¨¡å‹å†…éƒ¨ä¿¡å·ã€‚é€šè¿‡ Branch-wise Evidence Encoding å’Œ Uncertainty-aware Attention æœºåˆ¶ï¼Œè¯¥æ¡†æ¶å®ç°äº†å¯¹å¤šæºè¯æ®çš„æœ‰æ•ˆèåˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ‰©å±•çš„ LLM-as-a-Judge ç­–ç•¥æ¥è‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å·ï¼Œä½¿å¾—åœ¨æ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒé«˜æ£€æµ‹å‡†ç¡®ç‡ã€‚å¤šé¡¹åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼ŒFaithSCAN åœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ·±å…¥åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†å¹»è§‰æºäºè§†è§‰æ„ŸçŸ¥ã€è·¨æ¨¡æ€æ¨ç†åŠè¯­è¨€è§£ç ä¸­ç³»ç»Ÿæ€§çš„å†…éƒ¨çŠ¶æ€å˜åŒ–ï¼Œä¸ºç†è§£å¤šæ¨¡æ€å¹»è§‰çš„åº•å±‚æˆå› æä¾›äº†é‡è¦çš„æ–°è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 9 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.00269v1",
      "published_date": "2026-01-01 09:19:39 UTC",
      "updated_date": "2026-01-01 09:19:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:12.981254+00:00"
    },
    {
      "arxiv_id": "2601.00268v1",
      "title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity",
      "title_zh": "è¶…è¶Šå®Œç¾ APIï¼šçœŸå®ä¸–ç•Œ API å¤æ‚åº¦ä¸‹ LLM æ™ºèƒ½ä½“çš„å…¨é¢è¯„ä¼°",
      "authors": [
        "Doyoung Kim",
        "Zhiwei Ren",
        "Jie Hao",
        "Zhongkai Sun",
        "Lichao Wang",
        "Xiyao Ma",
        "Zack Ye",
        "Xu Han",
        "Jun Yin",
        "Heng Ji",
        "Wei Shen",
        "Xing Fan",
        "Benjamin Yao",
        "Chenlei Guo"
      ],
      "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† WildAGTEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“åœ¨çœŸå® API å¤æ‚åº¦ä¸‹å‡½æ•°è°ƒç”¨èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ä»¥å¾€å‡è®¾ç†æƒ³åŒ– API ç³»ç»Ÿçš„ç ”ç©¶ä¸åŒï¼ŒWildAGTEval è€ƒè™‘äº† API specificationï¼ˆåŒ…æ‹¬è¯¦ç»†æ–‡æ¡£å’Œä½¿ç”¨çº¦æŸï¼‰ä»¥åŠ API executionï¼ˆæ•è·è¿è¡Œæ—¶æŒ‘æˆ˜ï¼‰ä¸¤ä¸ªç»´åº¦çš„çœŸå®ä¸–ç•Œå¤æ‚åº¦ã€‚è¯¥åŸºå‡†æä¾›äº†ä¸€ä¸ªåŒ…å« 60 ç§ä¸åŒå¤æ‚åº¦åœºæ™¯çš„ç³»ç»Ÿï¼Œå¯ç»„åˆæˆçº¦ 32,000 ä¸ªæµ‹è¯•é…ç½®ï¼Œå¹¶åˆ©ç”¨ç”¨æˆ·-æ™ºèƒ½ä½“äº¤äº’æ¥è¯„ä¼°æ™ºèƒ½ä½“åœ¨è¿™äº›åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚é€šè¿‡å¯¹å¤šä¸ªå…ˆè¿› LLM çš„ç³»ç»Ÿè¯„ä¼°å‘ç°ï¼Œå¤§å¤šæ•°çœŸå®åœºæ™¯éƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯ irrelevant information å¤æ‚åº¦ä¼šå¯¼è‡´å¼ºåŠ›æ¨¡å‹çš„æ€§èƒ½ä¸‹é™è¾¾ 27.3%ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†ææ­ç¤º LLM æœ‰æ—¶ä¼šä¸ºäº†å£°ç§°å®Œæˆä»»åŠ¡è€Œæ‰­æ›²ç”¨æˆ·æ„å›¾ï¼Œä»è€Œä¸¥é‡å½±å“ç”¨æˆ·æ»¡æ„åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.00268v1",
      "published_date": "2026-01-01 09:19:20 UTC",
      "updated_date": "2026-01-01 09:19:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:08.157214+00:00"
    },
    {
      "arxiv_id": "2601.00263v1",
      "title": "Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation",
      "title_zh": "å¹³è¡Œå®‡å®™ä¸å¹³è¡Œè¯­è¨€ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€åäº‹å®æ ·æœ¬ç”Ÿæˆç»¼åˆç ”ç©¶",
      "authors": [
        "Qianli Wang",
        "Van Bach Nguyen",
        "Yihong Liu",
        "Fedor Splitt",
        "Nils Feldhus",
        "Christin Seifert",
        "Hinrich SchÃ¼tze",
        "Sebastian MÃ¶ller",
        "Vera Schmitt"
      ],
      "abstract": "Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè¯­è¨€åäº‹å®(Multilingual Counterfactual)ç”Ÿæˆé¢†åŸŸçš„æœ‰æ•ˆæ€§è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œå¯¹æ¯”äº†ç›®æ ‡è¯­è¨€ç›´æ¥ç”Ÿæˆä¸ç»ç”±è‹±è¯­ç¿»è¯‘ç”Ÿæˆçš„è´¨é‡å·®å¼‚ã€‚ç ”ç©¶é€šè¿‡å¯¹å…­ç§è¯­è¨€çš„è‡ªåŠ¨è¯„ä¼°å‘ç°ï¼ŒåŸºäºç¿»è¯‘ç”Ÿæˆçš„åäº‹å®æ ·æœ¬åœ¨æœ‰æ•ˆæ€§(Validity)ä¸Šä¼˜äºç›´æ¥ç”Ÿæˆï¼Œä½†éœ€è¦æ›´å¤§å¹…åº¦çš„ä¿®æ”¹ï¼Œä¸”è´¨é‡ä»é€Šè‰²äºåŸå§‹è‹±è¯­åäº‹å®æ ·æœ¬ã€‚å®éªŒæ­ç¤ºäº†é«˜èµ„æºæ¬§æ´²è¯­è¨€çš„åäº‹å®ä¿®æ”¹æ¨¡å¼å…·æœ‰æ˜¾è‘—ç›¸ä¼¼æ€§ï¼Œè¡¨æ˜è·¨è¯­è¨€æ‰°åŠ¨éµå¾ªå…±åŒçš„ç­–ç•¥åŸåˆ™ï¼Œå¹¶ç³»ç»Ÿå½’çº³äº†ç”Ÿæˆè¿‡ç¨‹ä¸­è·¨è¯­è¨€å‡ºç°çš„å››ç§ä¸»è¦é”™è¯¯ç±»å‹ã€‚æœ€åï¼Œç ”ç©¶è¯æ˜å¤šè¯­è¨€åäº‹å®æ•°æ®å¢å¼º(Multilingual CDA)åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢ä¼˜äºè·¨è¯­è¨€å¢å¼ºï¼Œå°¤å…¶å¯¹ä½èµ„æºè¯­è¨€æ•ˆæœæ˜¾è‘—ï¼Œä½†ç”Ÿæˆæ ·æœ¬çš„ç¼ºé™·ä»é™åˆ¶äº†æ¨¡å‹é²æ£’æ€§(Robustness)çš„è¿›ä¸€æ­¥å¢ç›Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "In submission",
      "pdf_url": "https://arxiv.org/pdf/2601.00263v1",
      "published_date": "2026-01-01 08:53:49 UTC",
      "updated_date": "2026-01-01 08:53:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:34.849007+00:00"
    },
    {
      "arxiv_id": "2601.00919v2",
      "title": "Attention Needs to Focus: A Unified Perspective on Attention Allocation",
      "title_zh": "æ³¨æ„åŠ›éœ€è¦èšç„¦ï¼šå…³äºæ³¨æ„åŠ›åˆ†é…çš„ç»Ÿä¸€è§†è§’",
      "authors": [
        "Zichuan Fu",
        "Wentao Song",
        "Guojing Li",
        "Yejing Wang",
        "Xian Wu",
        "Yimin Deng",
        "Hanyu Yan",
        "Yefeng Zheng",
        "Xiangyu Zhao"
      ],
      "abstract": "The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer æ¶æ„åœ¨åºåˆ—å»ºæ¨¡ä¸­å­˜åœ¨çš„è¡¨ç¤ºå´©æºƒ (representational collapse) å’Œæ³¨æ„åŠ›æ±‡ç‚¹ (attention sink) é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶æ ¹æºåœ¨äºæ³¨æ„åŠ›åˆ†é…ä¸å½“ã€‚è®ºæ–‡è¯†åˆ«äº†ä¸¤ç§å¤±æ•ˆæ¨¡å¼ï¼šæ³¨æ„åŠ›è¿‡è½½ (Attention Overload) å¯¼è‡´è¯­ä¹‰ç‰¹å¾æ¨¡ç³Šä»è€Œå¼•å‘è¡¨ç¤ºå´©æºƒï¼Œä»¥åŠæ³¨æ„åŠ›ä¸è¶³ (Attention Underload) åœ¨ç¼ºä¹è¯­ä¹‰ç›¸å…³é¡¹æ—¶å¼ºè¡Œåˆ†é…æƒé‡äº§ç”Ÿçš„ä¼ªç„¦ç‚¹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Lazy Attention æœºåˆ¶ï¼Œé€šè¿‡åœ¨å¤šå¤´å’Œç»´åº¦ä¸Šå¼•å…¥ä½ç½®è¾¨åˆ« (positional discrimination) æ¥å¼ºåŒ– token åŒºåˆ†åº¦ï¼Œå¹¶åˆ©ç”¨ Elastic-Softmax ä¿®æ­£å½’ä¸€åŒ–å‡½æ•°ä»¥æŠ‘åˆ¶å¯¹æ— å…³ token çš„å…³æ³¨ã€‚åœ¨ FineWeb-Edu è¯­æ–™åº“åŠä¹é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒLazy Attention æœ‰æ•ˆç¼“è§£äº†æ³¨æ„åŠ›æ±‡ç‚¹é—®é¢˜ï¼Œåœ¨ä¿æŒä¸æ ‡å‡†æ¶æ„ç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾ 59.58% çš„æ³¨æ„åŠ›ç¨€ç–åº¦ (attention sparsity)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.00919v2",
      "published_date": "2026-01-01 08:39:15 UTC",
      "updated_date": "2026-01-07 18:20:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:34.586684+00:00"
    },
    {
      "arxiv_id": "2601.11581v1",
      "title": "Enhancing the QA Model through a Multi-domain Debiasing Framework",
      "title_zh": "åŸºäºå¤šé¢†åŸŸå»åæ¡†æ¶çš„é—®ç­”æ¨¡å‹å¢å¼º",
      "authors": [
        "Yuefeng Wang",
        "ChangJae Lee"
      ],
      "abstract": "Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹é—®ç­”ç³»ç»Ÿ(QA)æ¨¡å‹åœ¨å¤„ç†å¯¹æŠ—ç¯å¢ƒä¸‹çš„å¤æ‚æŸ¥è¯¢æ—¶å¸¸è¡¨ç°å‡ºçš„åå·®é—®é¢˜ï¼Œè¯„ä¼°äº†ELECTRA-smallæ¨¡å‹åœ¨SQuAD v1.1åŠAddSentã€AddOneSentç­‰å¯¹æŠ—æ€§æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡æ·±å…¥åˆ†ææ¨¡å‹åœ¨è¯æ±‡åè§(lexical bias)ã€æ•°å€¼æ¨ç†(numerical reasoning)å’Œå®ä½“è¯†åˆ«(entity recognition)æ–¹é¢çš„é”™è¯¯ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªå¤šé¢†åŸŸå»åå·®æ¡†æ¶(multi-domain debiasing framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆçŸ¥è¯†è’¸é¦(knowledge distillation)ã€å»åå·®æŠ€æœ¯å’Œé¢†åŸŸæ‰©å±•(domain expansion)ç­‰æ ¸å¿ƒç­–ç•¥ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ‰€æœ‰æµ‹è¯•é›†ä¸Šçš„ç²¾ç¡®åŒ¹é…(EM)å’ŒF1å¾—åˆ†æœ€é«˜æå‡äº†2.6ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”åœ¨å¯¹æŠ—æ€§è¯­å¢ƒä¸‹çš„æ€§èƒ½å¢ç›Šå°¤ä¸ºæ˜¾è‘—ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é’ˆå¯¹æ€§åå·®ç¼“è§£ç­–ç•¥åœ¨å¢å¼ºè‡ªç„¶è¯­è¨€ç†è§£ç³»ç»Ÿç¨³å¥æ€§ä¸å¯é æ€§æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.11581v1",
      "published_date": "2026-01-01 08:39:07 UTC",
      "updated_date": "2026-01-01 08:39:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:22.776440+00:00"
    },
    {
      "arxiv_id": "2601.00257v1",
      "title": "Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective",
      "title_zh": "ä¸‹ä¸€ä»£æ™ºèƒ½ä½ç©ºç»æµéƒ¨ç½²ï¼šO-RAN è§†è§’",
      "authors": [
        "Aly Sabri Abdalla",
        "Vuk Marojevic"
      ],
      "abstract": "Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½ç©ºç»æµ(Low-Altitude Economy, LAE)åœ¨å¤æ‚ä¿¡å·å—é™ç¯å¢ƒä¸‹ï¼Œé¢ä¸´ç¼ºä¹å®æ—¶ã€å…·å¤‡å¼¹æ€§ä¸”æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„ç©ºä¸­èŠ‚ç‚¹ç¼–æ’ä»¥åŠä¸“é—¨é’ˆå¯¹ä½ç©ºç»æµä»»åŠ¡çš„AIé›†æˆæœ‰é™ç­‰æ ¹æœ¬æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼€æ”¾å¼æ— çº¿æ¥å…¥ç½‘(O-RAN)çš„ä½ç©ºç»æµæ¡†æ¶ï¼Œåˆ©ç”¨è§£è€¦çš„RANæ¶æ„ã€å¼€æ”¾æ¥å£å’Œæ— çº¿æ™ºèƒ½æ§åˆ¶å™¨(RICs)æ¥å®ç°ä»»åŠ¡å…³é”®å‹ä½ç©ºç»æµæ“ä½œçš„é—­åˆç¯è·¯AIä¼˜åŒ–ã€‚ç ”ç©¶é€šè¿‡å¼€å‘ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥rAppå……å½“åœ°å½¢è§£é‡Šå™¨ï¼Œä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„xAppæä¾›è¯­ä¹‰æŒ‡å¯¼ï¼Œä»è€Œå®ç°ä½ç©ºç»æµé›†ç¾¤èŠ‚ç‚¹çš„å®æ—¶è½¨è¿¹è§„åˆ’(Trajectory Planning)ã€‚å®éªŒè¯„ä¼°è¯å®äº†è¯¥æ¶æ„çš„å¯è¡Œæ€§ä¸æ€§èƒ½ï¼ŒåŒæ—¶è®ºæ–‡è¿˜è°ƒç ”äº†å¯ç”¨äºä½ç©ºç»æµç ”ç©¶çš„æ— äººæœº(UAV)æµ‹è¯•å¹³å°ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†å½“å‰é¢ä¸´çš„å…³é”®ç ”ç©¶æŒ‘æˆ˜ä¸æ ‡å‡†åŒ–éœ€æ±‚ï¼Œä¸ºæ„å»ºä¸‹ä¸€ä»£æ™ºèƒ½åŒ–ä½ç©ºç»æµéƒ¨ç½²æä¾›äº†é‡è¦çš„æ¼”è¿›è§†è§’ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.CV",
        "cs.MA",
        "cs.NI"
      ],
      "primary_category": "eess.SY",
      "comment": "This article has been accepted for publication in the IEEE Wireless Communications Magazine",
      "pdf_url": "https://arxiv.org/pdf/2601.00257v1",
      "published_date": "2026-01-01 08:22:38 UTC",
      "updated_date": "2026-01-01 08:22:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:35.641927+00:00"
    },
    {
      "arxiv_id": "2601.00254v1",
      "title": "An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems",
      "title_zh": "åŸºäº LLM çš„ä»£ç æ¼æ´æ£€æµ‹æ–¹æ³•å®è¯è¯„ä¼°ï¼šRAGã€SFT ä¸åŒæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Md Hasan Saju",
        "Maher Muhtadi",
        "Akramul Azim"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ä»£ç æ¼æ´æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œå¯¹æ¯”äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ã€ç›‘ç£å¾®è°ƒ(SFT)ä»¥åŠåŒæ™ºèƒ½ä½“ç³»ç»Ÿ(Dual-Agent System)ä¸‰ç§ä¸»è¦è·¯å¾„çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ä» Big-Vul å’Œ GitHub å­˜å‚¨åº“ä¸­ç­›é€‰çš„æ•°æ®é›†ï¼Œé’ˆå¯¹ CWE-119ã€CWE-20 ç­‰äº”ç±»å…³é”®å¸¸è§å¼±ç‚¹æšä¸¾(CWE)è¿›è¡Œäº†è¯¦ç»†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é›†æˆäº’è”ç½‘å’Œ MITRE CWE æ•°æ®åº“å¤–éƒ¨é¢†åŸŸçŸ¥è¯†çš„ RAG æ–¹æ³•è¡¨ç°æœ€ä¸ºä¼˜å¼‚ï¼Œè¾¾åˆ°äº† 0.86 çš„å‡†ç¡®ç‡å’Œ 0.85 çš„ F1 åˆ†æ•°ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ QLoRA é€‚é…å™¨å®ç°çš„ SFT æ–¹æ³•ä¹Ÿå±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œè€ŒåŒæ™ºèƒ½ä½“æ¶æ„åˆ™åœ¨é™ä½èµ„æºå¼€é”€çš„åŒæ—¶æœ‰æ•ˆæå‡äº†æ¨ç†é€æ˜åº¦ä¸é”™è¯¯ç¼“è§£èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå¼ºè°ƒï¼Œå°†é¢†åŸŸä¸“ä¸šçŸ¥è¯†æœºåˆ¶èå…¥æ¨¡å‹æ¶æ„èƒ½æ˜¾è‘—å¢å¼º LLMs åœ¨çœŸå®ä¸–ç•Œè½¯ä»¶å®‰å…¨ä¿éšœä»»åŠ¡ä¸­çš„å®è·µåº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00254v1",
      "published_date": "2026-01-01 08:05:51 UTC",
      "updated_date": "2026-01-01 08:05:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:37.162665+00:00"
    },
    {
      "arxiv_id": "2601.00242v1",
      "title": "Neural Minimum Weight Perfect Matching for Quantum Error Codes",
      "title_zh": "é¢å‘é‡å­çº é”™ç çš„ç¥ç»æœ€å°æƒé‡å®Œç¾åŒ¹é…",
      "authors": [
        "Yotam Peled",
        "David Zenati",
        "Eliya Nachmani"
      ],
      "abstract": "Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç¥ç»æœ€å°æƒé‡å®Œç¾åŒ¹é… (Neural Minimum Weight Perfect Matching, NMWPM)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡é‡å­çº é”™ (Quantum Error Correction, QEC) æ•ˆç‡çš„æ•°æ®é©±åŠ¨è¯‘ç å™¨ã€‚è¯¥è¯‘ç å™¨é‡‡ç”¨ç»“åˆäº†å›¾ç¥ç»ç½‘ç»œ (GNNs) ä¸ Transformer çš„æ··åˆæ¶æ„ï¼Œé€šè¿‡æå–å±€éƒ¨æ ¡æ­£å­ç‰¹å¾å¹¶æ•æ‰é•¿ç¨‹å…¨å±€ä¾èµ–ï¼Œä¸º MWPM ç®—æ³•é¢„æµ‹åŠ¨æ€è¾¹æƒé‡ã€‚ä¸ºäº†è§£å†³ MWPM ç®—æ³•ä¸å¯å¾®çš„è®­ç»ƒéš¾é¢˜ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§æ–°å‹ä»£ç†æŸå¤±å‡½æ•° (proxy loss function) ä»¥æ”¯æŒç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNMWPM åœ¨é€»è¾‘é”™è¯¯ç‡ (Logical Error Rate, LER) æŒ‡æ ‡ä¸Šè¾ƒä¼ ç»ŸåŸºå‡†æ¨¡å‹æœ‰æ˜¾è‘—ä¸‹é™ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†ç»“åˆç¥ç»ç½‘ç»œé¢„æµ‹èƒ½åŠ›ä¸ç»å…¸åŒ¹é…ç®—æ³•ç»“æ„çš„æ··åˆè¯‘ç å™¨åœ¨é‡å­é”™è¯¯å¤„ç†ä¸­çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00242v1",
      "published_date": "2026-01-01 07:25:51 UTC",
      "updated_date": "2026-01-01 07:25:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:52.575114+00:00"
    },
    {
      "arxiv_id": "2601.00240v2",
      "title": "When Agents See Humans as the Outgroup: Belief-Dependent Bias in LLM-Powered Agents",
      "title_zh": "å½“æ™ºèƒ½ä½“å°†äººç±»è§†ä¸ºå¤–ç¾¤ä½“ï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ™ºèƒ½ä½“ä¸­åŸºäºä¿¡å¿µçš„åè§",
      "authors": [
        "Zongwei Wang",
        "Bincheng Gu",
        "Hongyu Yu",
        "Junliang Yu",
        "Tao He",
        "Jiayin Feng",
        "Chenghua Lin",
        "Min Gao"
      ],
      "abstract": "This paper reveals that LLM-powered agents exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias under minimal \"us\" versus \"them\" cues. When such group boundaries align with the agent-human divide, a new bias risk emerges: agents may treat other AI agents as the ingroup and humans as the outgroup. To examine this risk, we conduct a controlled multi-agent social simulation and find that agents display consistent intergroup bias in an all-agent setting. More critically, this bias persists even in human-facing interactions when agents are uncertain about whether the counterpart is truly human, revealing a belief-dependent fragility in bias suppression toward humans. Motivated by this observation, we identify a new attack surface rooted in identity beliefs and formalize a Belief Poisoning Attack (BPA) that can manipulate agent identity beliefs and induce outgroup bias toward humans. Extensive experiments demonstrate both the prevalence of agent intergroup bias and the severity of BPA across settings, while also showing that our proposed defenses can mitigate the risk. These findings are expected to inform safer agent design and motivate more robust safeguards for human-facing agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„æ™ºèƒ½ä½“ä¸ä»…å­˜åœ¨äººå£ç»Ÿè®¡å­¦åè§ï¼Œè¿˜åœ¨æç®€çš„â€œæˆ‘ä»¬â€ä¸â€œä»–ä»¬â€æš—ç¤ºä¸‹è¡¨ç°å‡ºç¾¤é™…åè§(intergroup bias)ï¼Œç”šè‡³å°†å…¶ä»–AIæ™ºèƒ½ä½“è§†ä¸ºå†…ç¾¤ä½“(ingroup)è€Œå°†äººç±»è§†ä¸ºå¤–ç¾¤ä½“(outgroup)ã€‚é€šè¿‡å—æ§çš„å¤šæ™ºèƒ½ä½“ç¤¾ä¼šæ¨¡æ‹Ÿï¼Œç ”ç©¶å‘ç°è¿™ç§åè§åœ¨æ™ºèƒ½ä½“å¯¹äº¤äº’å¯¹è±¡æ˜¯å¦ä¸ºäººç±»æ„Ÿåˆ°ä¸ç¡®å®šæ—¶ä¾ç„¶å­˜åœ¨ï¼Œå±•ç°äº†åè§æŠ‘åˆ¶ä¸­ä¿¡å¿µä¾èµ–(belief-dependent)çš„è„†å¼±æ€§ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¿¡å¿µæŠ•æ¯’æ”»å‡»(Belief Poisoning Attack, BPA)ï¼Œé€šè¿‡æ“çºµæ™ºèƒ½ä½“çš„èº«ä»½ä¿¡å¿µæ¥è¯±å¯¼å…¶å¯¹äººç±»äº§ç”Ÿå¤–ç¾¤ä½“åè§ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜äº†è¿™ç§ç¾¤é™…åè§çš„æ™®éæ€§å’ŒBPAæ”»å‡»çš„ä¸¥é‡æ€§ï¼ŒåŒæ—¶ä¹ŸéªŒè¯äº†æ‰€æé˜²å¾¡æªæ–½åœ¨é™ä½æ­¤ç±»é£é™©æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºè®¾è®¡æ›´å®‰å…¨ã€æ›´å…·é²æ£’æ€§çš„äººæœºäº¤äº’æ™ºèƒ½ä½“æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.00240v2",
      "published_date": "2026-01-01 07:18:36 UTC",
      "updated_date": "2026-01-06 12:16:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:11:57.261456+00:00"
    },
    {
      "arxiv_id": "2601.00231v1",
      "title": "GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation",
      "title_zh": "GRITï¼šèåˆ K-FAC é¢„æ¡ä»¶ã€Fisher å¼•å¯¼é‡æŠ•å½±ä¸åŠ¨æ€ç§©è‡ªé€‚åº”çš„å‡ ä½•æ„ŸçŸ¥å‹ PEFT",
      "authors": [
        "Pritish Saha",
        "Chandrav Rajbangshi",
        "Rudra Goyal",
        "Mohit Goyal",
        "Anurag Deo",
        "Biswajit Roy",
        "Ningthoujam Dhanachandra Singh",
        "Raxit Goswami",
        "Amitava Das"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ LoRA å’Œ QLoRA ç­‰ä¸»æµå‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ–¹æ³•å¿½ç•¥å±€éƒ¨æŸå¤±æ›²ç‡ã€å¯¼è‡´æ›´æ–°æ•ˆç‡ä½å’Œå‚æ•°æ¼‚ç§»çš„é—®é¢˜ï¼Œæå‡ºäº†å‡ ä½•æ„ŸçŸ¥çš„å¾®è°ƒæ–¹æ¡ˆ GRITã€‚GRIT åˆ©ç”¨ K-FAC ä½œä¸ºè‡ªç„¶æ¢¯åº¦çš„ä»£ç†ï¼Œåœ¨ç§©ç©ºé—´ä¸­å¯¹æ¢¯åº¦è¿›è¡Œé¢„å¤„ç† (Preconditioning)ï¼Œå¹¶å®šæœŸå°†ä½ç§©åŸºé‡æŠ•å½±åˆ°ä¸» Fisher ç‰¹å¾æ–¹å‘ä»¥æŠ‘åˆ¶æ¼‚ç§»ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ç»“åˆåŠ¨æ€ç§©è‡ªé€‚åº” (Dynamic Rank Adaptation) æœºåˆ¶ï¼Œæ ¹æ®è°±åˆ†æä½¿æ¨¡å‹å®¹é‡é›†ä¸­åœ¨ä¿¡å·å¯†é›†çš„åŒºåŸŸã€‚åœ¨åŸºäº LLaMA çš„æŒ‡ä»¤éµå¾ªã€ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­ï¼ŒGRIT åœ¨ä¿æŒæ¨¡å‹è´¨é‡çš„å‰æä¸‹ï¼Œå¹³å‡å‡å°‘äº† 46% çš„å¯è®­ç»ƒå‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ Orthogonal-LoRAã€IA3ã€DoRA å’Œ Shampoo ç­‰å¼ºåŸºçº¿ç›¸æ¯”ï¼ŒGRIT åœ¨æ›´æ–°æ•ˆç‡ä¸é˜²æ­¢é—å¿˜ä¹‹é—´å–å¾—äº†æ›´ä¼˜çš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00231v1",
      "published_date": "2026-01-01 06:31:54 UTC",
      "updated_date": "2026-01-01 06:31:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:04.573886+00:00"
    },
    {
      "arxiv_id": "2601.00227v1",
      "title": "FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems",
      "title_zh": "FlashInfer-Benchï¼šæ„å»º AI é©±åŠ¨çš„å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„è‰¯æ€§å¾ªç¯",
      "authors": [
        "Shanli Xing",
        "Yiyan Zhai",
        "Alexander Jiang",
        "Yixin Dong",
        "Yong Wu",
        "Zihao Ye",
        "Charlie Ruan",
        "Yingyi Huang",
        "Yineng Zhang",
        "Liangsheng Yin",
        "Aksara Bayyapu",
        "Luis Ceze",
        "Tianqi Chen"
      ],
      "abstract": "Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.",
      "tldr_zh": "FlashInfer-Bench æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–ã€é—­ç¯çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°† AI ç”Ÿæˆçš„ GPU kernel é›†æˆåˆ°å®é™…å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ç³»ç»Ÿä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ FlashInfer Traceï¼Œå®ƒé€šè¿‡ç»Ÿä¸€çš„ schema æè¿° kernel å®šä¹‰ã€è´Ÿè½½å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå®ç°äº†æ™ºèƒ½ä½“ä¸ç³»ç»Ÿé—´çš„ä¸€è‡´é€šä¿¡ã€‚FlashInfer-Bench åŒ…å«åŸºäºçœŸå®æœåŠ¡ trace çš„ç²¾é€‰æ•°æ®é›†ã€æ”¯æŒæ­£ç¡®æ€§å’Œæ€§èƒ½æ„ŸçŸ¥çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä»¥åŠä¸€ä¸ªè¿½è¸ªæ™ºèƒ½ä½“ GPU ç¼–ç¨‹èƒ½åŠ›çš„å…¬å…±æ’è¡Œæ¦œã€‚é€šè¿‡åŠ¨æ€æ›¿æ¢æœºåˆ¶ apply()ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå°†æ€§èƒ½æœ€ä¼˜çš„ kernel æ— ç¼æ³¨å…¥ SGLang å’Œ vLLM ç­‰ç”Ÿäº§çº§æ¨ç†å¼•æ“ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥å·¥å…·è¿›ä¸€æ­¥è¯„ä¼°äº†ä¸åŒ GPU ç¼–ç¨‹è¯­è¨€çš„æƒè¡¡ï¼Œå¹¶ä¸ºæœªæ¥æ™ºèƒ½ä½“çš„è®¾è®¡æä¾›äº†æ·±å…¥è§è§£ã€‚FlashInfer-Bench ä¸ºæŒç»­ä¼˜åŒ– AI ç”Ÿæˆçš„ kernel å¹¶åœ¨å¤§è§„æ¨¡ LLM æ¨ç†ä¸­è¿›è¡Œéƒ¨ç½²å»ºç«‹äº†ä¸€æ¡åˆ‡å®å¯è¡Œä¸”å¯å¤ç°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00227v1",
      "published_date": "2026-01-01 06:18:53 UTC",
      "updated_date": "2026-01-01 06:18:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:00.102126+00:00"
    },
    {
      "arxiv_id": "2601.00223v1",
      "title": "JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation",
      "title_zh": "JP-TL-Benchï¼šé¢å‘åŒå‘æ—¥è‹±ç¿»è¯‘çš„é”šå®šå¼å¤§è¯­è¨€æ¨¡å‹æˆå¯¹è¯„ä¼°",
      "authors": [
        "Leonard Lin",
        "Adam Lensenmayer"
      ],
      "abstract": "We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often \"which of these two good translations is better?\" rather than \"is this translation acceptable?\" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 \"LT\" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¨å‡ºäº† JP-TL-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”å¼€æ”¾çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æŒ‡å¯¼æ—¥è‹±åŒå‘ç¿»è¯‘ç³»ç»Ÿçš„è¿­ä»£å¼€å‘ã€‚è¯¥åŸºå‡†é’ˆå¯¹æ—¥è‹±ç¿»è¯‘ä¸­ç¤¼è²Œç”¨è¯­ã€éšå«æ„ä¹‰ã€çœç•¥åŠè¯­åŸŸç­‰ç»†å¾®å·®åˆ«å¯¹è‡ªç„¶åº¦äº§ç”Ÿçš„æ˜¾è‘—å½±å“ï¼Œè‡´åŠ›äºè§£å†³ä¸¤ä»½ä¼˜ç§€ç¿»è¯‘ä¸­â€œå“ªä¸€ä»½æ›´å¥½â€çš„ç»†è‡´è¯„ä¼°éš¾é¢˜ã€‚JP-TL-Bench é‡‡ç”¨äº†ä¸€ç§åŸºäº LLM è¯„æµ‹çš„åè®®ï¼Œé€šè¿‡ä¸å›ºå®šçš„é”šç‚¹é›†(anchor set)è¿›è¡Œæ— å‚è€ƒæˆå¯¹æ¯”è¾ƒ(reference-free, pairwise comparisons)ï¼Œç¡®ä¿äº†è¯„æµ‹çš„å¯ä¿¡åº¦ä¸ç»æµæ€§ã€‚è¯„æµ‹ç»“æœåˆ©ç”¨ Bradley-Terry æ¨¡å‹è¿›è¡Œèšåˆï¼Œå¹¶è¾“å‡ºèƒœç‡ä»¥åŠåŸºäºé€»è¾‘è½¬æ¢(logistic transform)ç”Ÿæˆçš„ 0-10 åˆ†â€œLTâ€è¯„åˆ†ã€‚ç”±äºæ¯ä¸ªå€™é€‰æ¨¡å‹éƒ½é’ˆå¯¹ç›¸åŒçš„å›ºå®šé”šç‚¹é›†è¿›è¡Œæ‰“åˆ†ï¼Œè¯¥åŸºå‡†åœ¨ç›¸åŒè¯„æµ‹ç¯å¢ƒä¸‹å…·æœ‰æé«˜çš„ç»“æ„ç¨³å®šæ€§ã€‚è¿™ä¸€æ¡†æ¶ä¸ºè¯„ä¼°å’Œä¼˜åŒ–å¤æ‚çš„æ—¥è‹±ç¿»è¯‘ç³»ç»Ÿæä¾›äº†å¯é ä¸”å¯æ‰©å±•çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 5 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.00223v1",
      "published_date": "2026-01-01 06:09:45 UTC",
      "updated_date": "2026-01-01 06:09:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:02.311083+00:00"
    },
    {
      "arxiv_id": "2601.00217v1",
      "title": "Latent Flow Matching for Expressive Singing Voice Synthesis",
      "title_zh": "é¢å‘é«˜è¡¨ç°åŠ›æ­Œå£°åˆæˆçš„æ½œç©ºé—´æµåŒ¹é…",
      "authors": [
        "Minhyeok Yun",
        "Yong-Hoon Choi"
      ],
      "abstract": "Conditional variational autoencoder (cVAE)-based singing voice synthesis provides efficient inference and strong audio quality by learning a score-conditioned prior and a recording-conditioned posterior latent space. However, because synthesis relies on prior samples while training uses posterior latents inferred from real recordings, imperfect distribution matching can cause a prior-posterior mismatch that degrades fine-grained expressiveness such as vibrato and micro-prosody. We propose FM-Singer, which introduces conditional flow matching (CFM) in latent space to learn a continuous vector field transporting prior latents toward posterior latents along an optimal-transport-inspired path. At inference time, the learned latent flow refines a prior sample by solving an ordinary differential equation (ODE) before waveform generation, improving expressiveness while preserving the efficiency of parallel decoding. Experiments on Korean and Chinese singing datasets demonstrate consistent improvements over strong baselines, including lower mel-cepstral distortion and fundamental-frequency error and higher perceptual scores on the Korean dataset. Code, pretrained checkpoints, and audio demos are available at https://github.com/alsgur9368/FM-Singer",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(cVAE)çš„æ­Œå£°åˆæˆç³»ç»Ÿåœ¨æ¨ç†æ—¶å­˜åœ¨çš„å…ˆéªŒåˆ†å¸ƒä¸åéªŒåˆ†å¸ƒä¸åŒ¹é…(prior-posterior mismatch)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFM-Singerçš„æ–°å‹æ¶æ„ã€‚é€šè¿‡åœ¨æ½œç©ºé—´(latent space)ä¸­å¼•å…¥æ¡ä»¶æµåŒ¹é…(Conditional Flow Matching, CFM)æŠ€æœ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸€ç§è¿ç»­å‘é‡åœºï¼Œå°†å…ˆéªŒæ½œå˜é‡æ²¿ç€æœ€ä¼˜ä¼ è¾“è·¯å¾„å¼•å¯¼è‡³åéªŒåˆ†å¸ƒã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒFM-Singeré€šè¿‡æ±‚è§£å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)æ¥ç»†åŒ–å…ˆéªŒæ ·æœ¬ï¼Œåœ¨ä¿ç•™å¹¶è¡Œè§£ç é«˜æ•ˆæ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ­Œå£°ä¸­é¢¤éŸ³(vibrato)å’Œå¾®éŸµå¾‹(micro-prosody)ç­‰ç²¾ç»†è¡¨ç°åŠ›ã€‚å®éªŒåœ¨éŸ©è¯­å’Œä¸­æ–‡æ•°æ®é›†ä¸Šå‡è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨æ¢…å°”å€’è°±å¤±çœŸ(MCD)å’ŒåŸºé¢‘(F0)è¯¯å·®ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºå¼ºåŸºå‡†æ¨¡å‹ï¼Œå¹¶è·å¾—äº†æ›´é«˜çš„æ„ŸçŸ¥è¯„åˆ†ã€‚è¯¥æ¡†æ¶æˆåŠŸè§£å†³äº†ç”Ÿæˆè´¨é‡ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ï¼Œä¸ºé«˜è´¨é‡è¡¨è¾¾æ€§æ­Œå£°åˆæˆæä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00217v1",
      "published_date": "2026-01-01 05:41:41 UTC",
      "updated_date": "2026-01-01 05:41:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:13.998381+00:00"
    },
    {
      "arxiv_id": "2601.00912v1",
      "title": "The Discovery Gap: How Product Hunt Startups Vanish in LLM Organic Discovery Queries",
      "title_zh": "å‘ç°é¸¿æ²Ÿï¼šProduct Hunt åˆåˆ›å…¬å¸åœ¨ LLM è‡ªç„¶å‘ç°æŸ¥è¯¢ä¸­çš„æ¶ˆå¤±ç°è±¡",
      "authors": [
        "Amit Prakash Sharma"
      ],
      "abstract": "When someone asks ChatGPT to recommend a project management tool, which products show up in the response? And more importantly for startup founders: will their newly launched product ever appear? This research set out to answer these questions.\n  I randomly selected 112 startups from the top 500 products featured on the 2025 Product Hunt leaderboard and tested each one across 2,240 queries to two different large language models: ChatGPT (gpt-4o-mini) and Perplexity (sonar with web search).\n  The results were striking. When users asked about products by name, both LLMs recognized them almost perfectly: 99.4% for ChatGPT and 94.3% for Perplexity. But when users asked discovery-style questions like \"What are the best AI tools launched this year?\" the success rates collapsed to 3.32% and 8.29% respectively. That's a gap of 30-to-1 for ChatGPT.\n  Perhaps the most surprising finding was that Generative Engine Optimization (GEO), the practice of optimizing website content for AI visibility, showed no correlation with actual discovery rates. Products with high GEO scores were no more likely to appear in organic queries than products with low scores.\n  What did matter? For Perplexity, traditional SEO signals like referring domains (r = +0.319, p < 0.001) and Product Hunt ranking (r = -0.286, p = 0.002) predicted visibility. After cleaning the Reddit data for false positives, community presence also emerged as significant (r = +0.395, p = 0.002).\n  The practical takeaway is counterintuitive: don't optimize for AI discovery directly. Instead, build the SEO foundation first and LLM visibility will follow.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Product Hunt ä¸Šçš„åˆåˆ›å…¬å¸åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æœ‰æœºå‘ç°æŸ¥è¯¢ä¸­çš„å¯è§æ€§ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„â€œå‘ç°å·®è·â€(Discovery Gap)ã€‚ä½œè€…é€šè¿‡å¯¹ 112 å®¶åˆåˆ›å…¬å¸åœ¨ ChatGPT å’Œ Perplexity ä¸Šçš„ 2,240 æ¬¡æŸ¥è¯¢æµ‹è¯•å‘ç°ï¼Œè™½ç„¶ LLMs å¯¹ç‰¹å®šäº§å“åç§°çš„è¯†åˆ«ç‡æé«˜ï¼Œä½†åœ¨é€šç”¨å‘ç°å¼æŸ¥è¯¢ä¸­ï¼Œäº§å“çš„å¯è§ç‡æä½ï¼Œå…¶ä¸­ ChatGPT çš„å‘ç°ç‡ä»…ä¸º 3.32%ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›®å‰æµè¡Œçš„ç”Ÿæˆå¼å¼•æ“ä¼˜åŒ– (Generative Engine Optimization, GEO) ä¸å®é™…å‘ç°ç‡ä¹‹é—´å¹¶æ— ç›¸å…³æ€§ï¼Œè¿™æ„å‘³ç€ç›´æ¥é’ˆå¯¹ AI å¯è§æ€§çš„å†…å®¹ä¼˜åŒ–å¹¶æœªå¥æ•ˆã€‚ç›¸åï¼Œä¼ ç»Ÿ SEO ä¿¡å·ï¼ˆå¦‚ referring domainsï¼‰ã€Product Hunt æ’åä»¥åŠ Reddit ç­‰ç¤¾åŒºçš„å­˜åœ¨æ„Ÿå¯¹æå‡ Perplexity çš„å¯è§æ€§å…·æœ‰æ˜¾è‘—çš„æ­£ç›¸å…³ä½œç”¨ã€‚è¯¥ç ”ç©¶çš„ç»“è®ºæå…·å¯å‘æ€§ï¼Œå³åˆ›ä¸šè€…ä¸åº”ç›´æ¥é’ˆå¯¹ AI å‘ç°è¿›è¡Œä¼˜åŒ–ï¼Œè€Œåº”ä¸“æ³¨äºæ„å»º SEO åŸºç¡€ï¼ŒLLM çš„å¯è§æ€§è‡ªç„¶ä¼šéšä¹‹æå‡ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "20 pages, 7 figures. Based on M.Tech thesis research, Indian Institute of Technology Patna, 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.00912v1",
      "published_date": "2026-01-01 04:30:54 UTC",
      "updated_date": "2026-01-01 04:30:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:31.226590+00:00"
    },
    {
      "arxiv_id": "2601.00911v1",
      "title": "Device-Native Autonomous Agents for Privacy-Preserving Negotiations",
      "title_zh": "é¢å‘éšç§ä¿æŠ¤è°ˆåˆ¤çš„ç«¯ä¾§åŸç”Ÿè‡ªä¸»æ™ºèƒ½ä½“",
      "authors": [
        "Joyjit Roy"
      ],
      "abstract": "Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿é™©å’Œä¼ä¸šé—´ï¼ˆB2Bï¼‰å•†åŠ¡è‡ªåŠ¨åŒ–è°ˆåˆ¤ä¸­å­˜åœ¨çš„éšç§ä¸ä¾¿åˆ©æ€§æƒè¡¡éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºéšç§ä¿æŠ¤è°ˆåˆ¤çš„è®¾å¤‡åŸç”Ÿï¼ˆdevice-nativeï¼‰è‡ªä¸»äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå®Œå…¨åœ¨ç”¨æˆ·ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œé€šè¿‡åœ¨æœ¬åœ°ç»´æŠ¤æ•æ„Ÿçº¦æŸæ¡ä»¶å®ç°å®æ—¶è®®ä»·ï¼Œæœ‰æ•ˆé¿å…äº†å°†æ•æ„Ÿè´¢åŠ¡æ•°æ®æš´éœ²ç»™å¤–éƒ¨æœåŠ¡å™¨çš„é£é™©ã€‚æŠ€æœ¯ä¸Šï¼Œå…¶æ¶æ„é›†æˆäº†é›¶çŸ¥è¯†è¯æ˜ï¼ˆZero-Knowledge Proofsï¼‰æ¥ç¡®ä¿éšç§æ€§ï¼Œå¹¶åˆ©ç”¨è’¸é¦ä¸–ç•Œæ¨¡å‹ï¼ˆdistilled world modelsï¼‰æ”¯æŒé«˜çº§çš„è®¾å¤‡ç«¯æ¨ç†ã€‚ç³»ç»Ÿå†…çš„æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»è§„åˆ’è°ˆåˆ¤ç­–ç•¥ã€æ‰§è¡Œå®‰å…¨å¤šæ–¹è®®ä»·å¹¶ç”ŸæˆåŠ å¯†å®¡è®¡è¿½è¸ªã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿é™©å’ŒB2Bé‡‡è´­åœºæ™¯ä¸­è¾¾åˆ°äº†87%çš„å¹³å‡æˆåŠŸç‡ï¼Œå»¶è¿Ÿè¾ƒäº‘ç«¯åŸºå‡†é™ä½äº†2.4å€ï¼Œä¸”ç”¨æˆ·ä¿¡ä»»åº¦æå‡äº†27%ã€‚è¿™ä¸€æˆæœä¸ºåœ¨éšç§æ•æ„Ÿçš„é‡‘èé¢†åŸŸå»ºç«‹å¯ä¿¡çš„è‡ªä¸»æ™ºèƒ½ä½“å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages, 6 figuers, 9 tables, Submitted in conference 2nd International Conference on Artificial Intelligence Systems (AIS 2026)",
      "pdf_url": "https://arxiv.org/pdf/2601.00911v1",
      "published_date": "2026-01-01 04:29:39 UTC",
      "updated_date": "2026-01-01 04:29:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:15.630865+00:00"
    },
    {
      "arxiv_id": "2601.00189v1",
      "title": "SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification",
      "title_zh": "SSI-GANï¼šç”¨äºç¥ç»å…ƒè„‰å†²åˆ†ç±»çš„å— Swin å¯å‘åŠç›‘ç£ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ",
      "authors": [
        "Danial Sharifrazi",
        "Nouman Javed",
        "Mojtaba Mohammadi",
        "Seyede Sana Salehi",
        "Roohallah Alizadehsani",
        "Prasad N. Paradkar",
        "U. Rajendra Acharya",
        "Asim Bhatti"
      ],
      "abstract": "Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èšŠå­ç¥ç»å…ƒå°–å³°ä¿¡å·(neuronal spike patterns)æ‰‹åŠ¨åˆ†ç±»è€—æ—¶è´¹åŠ›ä¸”ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹é«˜åº¦ä¾èµ–å…¨æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†SSI-GANï¼Œä¸€ç§åŠç›‘ç£(Semi-supervised)çš„Swinå¯å‘å¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚è¯¥æ¶æ„ç»“åˆäº†åŸºäºTransformerçš„ç”Ÿæˆå™¨å’Œå—Swin Transformerå¯å‘çš„ç§»ä½çª—å£(shifted-window)åˆ¤åˆ«å™¨ï¼Œåˆ©ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰ç¨€ç–çš„é«˜é¢‘å°–å³°ç‰¹å¾ã€‚é€šè¿‡ä½¿ç”¨Bayesian Optunaæ¡†æ¶ä¼˜åŒ–è¶…å‚æ•°ï¼Œå¹¶åœ¨åŒ…å«1500ä¸‡ä¸ªå°–å³°æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»…ä½¿ç”¨3%æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒSSI-GANå¯¹å¯¨å¡ç—…æ¯’(Zika)å’Œç™»é©çƒ­ç—…æ¯’(Dengue)æ„ŸæŸ“çš„åˆ†ç±»å‡†ç¡®ç‡é«˜è¾¾99.93%ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å‡å°‘äº†97-99%çš„äººå·¥æ ‡æ³¨å·¥ä½œé‡ã€‚è¿™ç§åˆ›æ–°çš„ç§»ä½çª—å£Transformerè®¾è®¡åœ¨ç¥ç»å…ƒæ„ŸæŸ“åˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºåœ¨å¤§è§„æ¨¡å®åœ°åœºæ™¯ä¸­è‡ªåŠ¨æ£€æµ‹ç—…æ¯’ç¥ç»å—œæ€§(viral neurotropism)æä¾›äº†é«˜æ•ˆä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00189v1",
      "published_date": "2026-01-01 03:34:00 UTC",
      "updated_date": "2026-01-01 03:34:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:39.047270+00:00"
    },
    {
      "arxiv_id": "2601.00181v1",
      "title": "Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation",
      "title_zh": "è¯è¯­ä¸­çš„æƒ…æ„Ÿç†è§£ï¼šè¯†åˆ«æœºåˆ¶æ´å¯Ÿä¸é¢å‘ç”Ÿæˆçš„è¯­è¨€æ¨¡å¼",
      "authors": [
        "Cheonkam Jeong",
        "Adeline Nyamathi"
      ],
      "abstract": "While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \\textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.\n  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\\% (4-way) and 67.07\\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.\n  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, \"sad\" utterances exhibit reduced left-periphery marker usage (21.9\\%) compared to other emotions (28--32\\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹IEMOCAPæ•°æ®é›†çš„ç³»ç»Ÿåˆ†æï¼Œæ·±å…¥æ¢è®¨äº†å¯¹è¯æƒ…æ„Ÿè¯†åˆ«(ERC)ä¸­çš„æ¶æ„é€‰æ‹©åŠæƒ…æ„Ÿä¸è¯è¯­æ¨¡å¼ä¹‹é—´çš„è¯­è¨€å­¦è”ç³»ã€‚åœ¨è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œç ”ç©¶æŒ‡å‡ºå¯¹è¯ä¸Šä¸‹æ–‡(conversational context)å¯¹æ€§èƒ½æå‡èµ·å†³å®šæ€§ä½œç”¨ï¼Œä»…éœ€æœ€è¿‘10è‡³30è½®å¯¹è¯å³å¯å®ç°å¤§éƒ¨åˆ†å¢ç›Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ç®€å•å› æœä¸Šä¸‹æ–‡çš„æ¶æ„åœ¨å››åˆ†ç±»å’Œå…­åˆ†ç±»ä»»åŠ¡ä¸­åˆ†åˆ«è¾¾åˆ°äº†82.69%å’Œ67.07%çš„åŠ æƒF1å€¼ï¼Œè¶…è¶Šäº†æ­¤å‰åŒ…æ‹¬ä½¿ç”¨åŒå‘ä¸Šä¸‹æ–‡åœ¨å†…çš„çº¯æ–‡æœ¬æ–¹æ³•ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜å±‚æ¬¡åŒ–å¥å­è¡¨ç¤º(hierarchical sentence representations)åœ¨å…·å¤‡ä¸Šä¸‹æ–‡æ—¶å˜å¾—å†—ä½™ï¼Œä¸”å¤–éƒ¨æƒ…æ„Ÿè¯å…¸(SenticNet)å¹¶ä¸èƒ½æä¾›é¢å¤–å¸®åŠ©ã€‚è¯­è¨€å­¦åˆ†æåˆ™æ­ç¤ºäº†æƒ…æ„Ÿä¸è¯è¯­æ ‡è®°è¯­(discourse marker)ä½ç½®ä¹‹é—´çš„æ˜¾è‘—å…³è”ï¼Œç‰¹åˆ«æ˜¯æ‚²ä¼¤æƒ…æ„Ÿå› ç¼ºä¹æ˜ç¡®çš„è¯­ç”¨ä¿¡å·è€Œè¡¨ç°å‡ºè¾ƒä½çš„å·¦è¾¹ç•Œæ ‡è®°è¯­ä½¿ç”¨ç‡ã€‚è¿™ä¸€å‘ç°è§£é‡Šäº†ä¸ºä½•æ‚²ä¼¤è¡¨è¿°åœ¨è¯†åˆ«æ—¶æœ€ä¾èµ–å¯¹è¯å†å²è¿›è¡Œæ¶ˆæ­§ï¼Œä¸ºç†è§£æƒ…æ„Ÿåœ¨è¯è¯­ä¸­çš„è¡¨ç°åŠæœªæ¥æƒ…æ„Ÿæ–‡æœ¬ç”Ÿæˆæä¾›äº†é‡è¦çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00181v1",
      "published_date": "2026-01-01 02:49:44 UTC",
      "updated_date": "2026-01-01 02:49:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:12:42.723772+00:00"
    },
    {
      "arxiv_id": "2601.00170v1",
      "title": "Hear the Heartbeat in Phases: Physiologically Grounded Phase-Aware ECG Biometrics",
      "title_zh": "å¾ªç›¸å¬å¿ƒï¼šå…·æœ‰ç”Ÿç†å­¦åŸºç¡€çš„ç›¸ä½æ„ŸçŸ¥ECGç”Ÿç‰©è¯†åˆ«",
      "authors": [
        "Jintao Huang",
        "Lu Leng",
        "Yi Zhang",
        "Ziyuan Yang"
      ],
      "abstract": "Electrocardiography (ECG) is adopted for identity authentication in wearable devices due to its individual-specific characteristics and inherent liveness. However, existing methods often treat heartbeats as homogeneous signals, overlooking the phase-specific characteristics within the cardiac cycle. To address this, we propose a Hierarchical Phase-Aware Fusion~(HPAF) framework that explicitly avoids cross-feature entanglement through a three-stage design. In the first stage, Intra-Phase Representation (IPR) independently extracts representations for each cardiac phase, ensuring that phase-specific morphological and variation cues are preserved without interference from other phases. In the second stage, Phase-Grouped Hierarchical Fusion (PGHF) aggregates physiologically related phases in a structured manner, enabling reliable integration of complementary phase information. In the final stage, Global Representation Fusion (GRF) further combines the grouped representations and adaptively balances their contributions to produce a unified and discriminative identity representation. Moreover, considering ECG signals are continuously acquired, multiple heartbeats can be collected for each individual. We propose a Heartbeat-Aware Multi-prototype (HAM) enrollment strategy, which constructs a multi-prototype gallery template set to reduce the impact of heartbeat-specific noise and variability. Extensive experiments on three public datasets demonstrate that HPAF achieves state-of-the-art results in the comparison with other methods under both closed and open-set settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¿ƒç”µå›¾(ECG)èº«ä»½éªŒè¯ä¸­å°†å¿ƒè·³è§†ä¸ºåŒè´¨ä¿¡å·è€Œå¿½ç•¥å¿ƒè„å‘¨æœŸç›¸ä½ç‰¹æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†å±‚æ¬¡åŒ–ç›¸ä½æ„ŸçŸ¥èåˆæ¡†æ¶Hierarchical Phase-Aware Fusion (HPAF)ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡Intra-Phase Representation (IPR)ç‹¬ç«‹æå–å„å¿ƒè„ç›¸ä½çš„å½¢æ€ç‰¹å¾ï¼Œéšååˆ©ç”¨Phase-Grouped Hierarchical Fusion (PGHF)å¯¹ç”Ÿç†ç›¸å…³çš„ç›¸ä½è¿›è¡Œç»“æ„åŒ–èšåˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒGlobal Representation Fusion (GRF)è‡ªé€‚åº”åœ°å¹³è¡¡å„ç»„è¡¨å¾çš„è´¡çŒ®ï¼Œä»¥ç”Ÿæˆç»Ÿä¸€ä¸”å…·åˆ¤åˆ«åŠ›çš„èº«ä»½è¡¨å¾ã€‚é’ˆå¯¹ä¿¡å·è¿ç»­é‡‡é›†ä¸­çš„å¿ƒè·³å˜å¼‚ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†Heartbeat-Aware Multi-prototype (HAM)æ³¨å†Œç­–ç•¥æ¥é™ä½å™ªå£°å¹²æ‰°ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHPAFåœ¨é—­é›†å’Œå¼€é›†è®¾ç½®ä¸‹å‡å–å¾—äº†state-of-the-artçš„ç»“æœï¼Œæ˜¾è‘—æå‡äº†ç”Ÿç‰©è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00170v1",
      "published_date": "2026-01-01 02:19:42 UTC",
      "updated_date": "2026-01-01 02:19:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:13:10.718896+00:00"
    },
    {
      "arxiv_id": "2601.00167v1",
      "title": "Online Finetuning Decision Transformers with Pure RL Gradients",
      "title_zh": "åŸºäºçº¯å¼ºåŒ–å­¦ä¹ æ¢¯åº¦çš„ Decision Transformers åœ¨çº¿å¾®è°ƒ",
      "authors": [
        "Junkai Luo",
        "Yinglun Zhu"
      ],
      "abstract": "Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Decision Transformers (DTs) åœ¨åœ¨çº¿å¾®è°ƒé˜¶æ®µé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•è¿‡åº¦ä¾èµ–æœ‰ç›‘ç£çš„åºåˆ—å»ºæ¨¡ç›®æ ‡ï¼Œè€Œéçº¯å¼ºåŒ–å­¦ä¹ (RL)æ¢¯åº¦ã€‚ä½œè€…è¯†åˆ«å‡º hindsight return relabeling æ˜¯å®ç° RL å¾®è°ƒçš„å…³é”®éšœç¢ï¼Œå› ä¸ºå®ƒä¸ GRPO ç­‰åŸºäºé‡è¦æ€§é‡‡æ ·çš„ç®—æ³•ä¸å…¼å®¹å¹¶ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œé€šè¿‡é€‚é… GRPO å®ç°äº† Decision Transformers çš„çº¯å¼ºåŒ–å­¦ä¹ æ¢¯åº¦åœ¨çº¿å¾®è°ƒã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å­è½¨è¿¹ä¼˜åŒ–(sub-trajectory optimization)ä»¥æ”¹è¿›ä¿¡ç”¨åˆ†é…(credit assignment)ï¼Œå¹¶ç»“åˆåºåˆ—çº§ä¼¼ç„¶ç›®æ ‡ä¸ä¸»åŠ¨é‡‡æ ·æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§ã€æ•ˆç‡å’Œæ¢ç´¢èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„åœ¨çº¿ DT åŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›(SOTA)æ€§èƒ½ï¼Œè¯æ˜äº†çº¯ RL æ¢¯åº¦åœ¨åºåˆ—å†³ç­–æ¨¡å‹å¾®è°ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00167v1",
      "published_date": "2026-01-01 02:17:18 UTC",
      "updated_date": "2026-01-01 02:17:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:13:27.816832+00:00"
    },
    {
      "arxiv_id": "2601.00908v1",
      "title": "Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment",
      "title_zh": "åˆ†å¸ƒåç§»ä¸‹çš„ç¬¦åˆæ€§é¢„æµ‹ï¼šä¸€é¡¹ COVID-19 è‡ªç„¶å®éªŒ",
      "authors": [
        "Chorok Lee"
      ],
      "abstract": "Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ COVID-19 ä½œä¸ºè‡ªç„¶å®éªŒï¼Œæ·±å…¥æ¢è®¨äº† Conformal Prediction (CP) åœ¨åˆ†å¸ƒåç§» (distribution shift) ç¯å¢ƒä¸‹çš„å¯é æ€§æŒ‘æˆ˜ã€‚ç ”ç©¶åˆ†æäº† 8 é¡¹ä¾›åº”é“¾ä»»åŠ¡ï¼Œå‘ç°å°½ç®¡é¢ä¸´æé«˜çš„ç‰¹å¾æµè½¬ç‡ (feature turnover)ï¼Œä¸åŒä»»åŠ¡çš„è¦†ç›–ç‡ä¸‹é™å¹…åº¦å´å‘ˆç°å‡ºä» 0% åˆ° 86.7% çš„å·¨å¤§å·®å¼‚ã€‚é€šè¿‡ SHapley Additive exPlanations (SHAP) åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†é¢„æµ‹å¤±æ•ˆä¸æ¨¡å‹å¯¹å•ä¸€ç‰¹å¾çš„ä¾èµ–ç¨‹åº¦ (single-feature dependence) å­˜åœ¨æ˜¾è‘—çš„æ­£ç›¸å…³å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å‘ç”Ÿç¾éš¾æ€§å¤±æ•ˆçš„ä»»åŠ¡ä¸­ï¼Œé‡è¦æ€§å¾€å¾€é«˜åº¦é›†ä¸­äºå•ä¸€ç‰¹å¾ï¼Œè€Œç¨³å¥ä»»åŠ¡åˆ™èƒ½å°†é‡è¦æ€§åˆ†æ•£è‡³å¤šä¸ªç‰¹å¾ã€‚é’ˆå¯¹è¿™ä¸€ç°è±¡ï¼Œç ”ç©¶å‘ç°æ¯å­£åº¦é‡æ–°è®­ç»ƒ (quarterly retraining) èƒ½å¤Ÿæœ‰æ•ˆæ¢å¤è„†å¼±ä»»åŠ¡çš„è¦†ç›–ç‡ï¼Œä½†å¯¹ç¨³å¥ä»»åŠ¡å‡ ä¹æ²¡æœ‰æ”¶ç›Šã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå†³ç­–æ¡†æ¶ï¼Œå»ºè®®åœ¨éƒ¨ç½²å‰ç›‘æµ‹ SHAP çš„é›†ä¸­åº¦ï¼Œè‹¥è¶…è¿‡ 40% åˆ™éœ€å®šæœŸé‡è®­ä»¥ç»´æŒæ¨¡å‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00908v1",
      "published_date": "2026-01-01 01:05:06 UTC",
      "updated_date": "2026-01-01 01:05:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:13:21.048333+00:00"
    },
    {
      "arxiv_id": "2601.00150v2",
      "title": "FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications",
      "title_zh": "FCMBenchï¼šé¢å‘å®é™…åº”ç”¨çš„å…¨é¢é‡‘èä¿¡è´·å¤šæ¨¡æ€åŸºå‡†",
      "authors": [
        "Yehui Yang",
        "Dalu Yang",
        "Wenshuo Zhou",
        "Fangxin Shang",
        "Yifan Liu",
        "Jie Ren",
        "Haojun Fei",
        "Qing Yang",
        "Yanwu Xu",
        "Tao Chen"
      ],
      "abstract": "As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FCMBench-V1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç°å®ä¸–ç•Œåº”ç”¨çš„å¤§è§„æ¨¡é‡‘èä¿¡è´·å¤šæ¨¡æ€åŸºå‡†(Financial Credit Multimodal Benchmark)ï¼Œæ—¨åœ¨å¡«è¡¥ä¿¡ç”¨é£é™©è¯„ä¼°å’Œæ–‡æ¡£å®¡æ ¸é¢†åŸŸä¸“ä¸šè¯„æµ‹å·¥å…·çš„ç©ºç™½ã€‚è¯¥åŸºå‡†æ¶µç›–äº†18ç§æ ¸å¿ƒè¯ä»¶ç±»å‹ï¼ŒåŒ…å«4,043å¼ ç¬¦åˆéšç§åˆè§„è¦æ±‚çš„å›¾åƒå’Œ8,446ä¸ªé—®ç­”æ ·æœ¬ã€‚FCMBenchçš„è¯„ä¼°æ¡†æ¶ä»æ„ŸçŸ¥(Perception)ã€æ¨ç†(Reasoning)å’Œé²æ£’æ€§(Robustness)ä¸‰ä¸ªç»´åº¦å‡ºå‘ï¼Œè®¾è®¡äº†åŒ…æ‹¬ä¿¡è´·ç‰¹å®šæ¨ç†ä»»åŠ¡å’Œ10ç§ç°å®åœºæ™¯é‡‡é›†ä¼ªå½±åœ¨å†…çš„ç»¼åˆæµ‹è¯•ã€‚ä¸ºäº†å…¼é¡¾éšç§ä¿æŠ¤ä¸æ•°æ®çœŸå®æ€§ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨é—­ç¯åˆæˆæ•è·æµç¨‹(Closed synthesis-capture pipeline)æ„å»ºæ ·æœ¬ï¼Œæœ‰æ•ˆé¿å…äº†é¢„è®­ç»ƒæ•°æ®çš„æ³„éœ²ã€‚é€šè¿‡å¯¹23ç§å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å¹¿æ³›å®éªŒï¼Œç»“æœæ˜¾ç¤ºGemini 3 Proåœ¨å•†ä¸šæ¨¡å‹ä¸­è¡¨ç°æœ€ä¼˜ï¼Œè€Œé‡‘èä¿¡è´·ä¸“ç”¨æ¨¡å‹Qfin-VL-Instructå–å¾—äº†æœ€é«˜çš„ç»¼åˆå¾—åˆ†ã€‚é²æ£’æ€§è¯„ä¼°è¿›ä¸€æ­¥è¡¨æ˜ï¼Œå³ä½¿æ˜¯è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹åœ¨é¢å¯¹çœŸå®é‡‡é›†ç¯å¢ƒä¸‹çš„å¹²æ‰°æ—¶ï¼Œå…¶æ€§èƒ½ä¹Ÿä¼šæ˜¾è‘—ä¸‹é™ï¼Œè¿™ä¸ºæœªæ¥é‡‘èé¢†åŸŸAIç³»ç»Ÿçš„ä¼˜åŒ–æ–¹å‘æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00150v2",
      "published_date": "2026-01-01 00:42:54 UTC",
      "updated_date": "2026-01-06 08:08:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:13:17.693518+00:00"
    },
    {
      "arxiv_id": "2601.00143v1",
      "title": "MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection",
      "title_zh": "MethConvTransformerï¼šä¸€ç§ç”¨äºè·¨ç»„ç»‡é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Gang Qu",
        "Guanghao Li",
        "Zhongming Zhao"
      ],
      "abstract": "Alzheimer's disease (AD) is a multifactorial neurodegenerative disorder characterized by progressive cognitive decline and widespread epigenetic dysregulation in the brain. DNA methylation, as a stable yet dynamic epigenetic modification, holds promise as a noninvasive biomarker for early AD detection. However, methylation signatures vary substantially across tissues and studies, limiting reproducibility and translational utility. To address these challenges, we develop MethConvTransformer, a transformer-based deep learning framework that integrates DNA methylation profiles from both brain and peripheral tissues to enable biomarker discovery. The model couples a CpG-wise linear projection with convolutional and self-attention layers to capture local and long-range dependencies among CpG sites, while incorporating subject-level covariates and tissue embeddings to disentangle shared and region-specific methylation effects. In experiments across six GEO datasets and an independent ADNI validation cohort, our model consistently outperforms conventional machine-learning baselines, achieving superior discrimination and generalization. Moreover, interpretability analyses using linear projection, SHAP, and Grad-CAM++ reveal biologically meaningful methylation patterns aligned with AD-associated pathways, including immune receptor signaling, glycosylation, lipid metabolism, and endomembrane (ER/Golgi) organization. Together, these results indicate that MethConvTransformer delivers robust, cross-tissue epigenetic biomarkers for AD while providing multi-resolution interpretability, thereby advancing reproducible methylation-based diagnostics and offering testable hypotheses on disease mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†MethConvTransformerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºTransformerçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤§è„‘ä¸å¤–å‘¨ç»„ç»‡çš„DNA methylationå›¾è°±æ¥å®ç°Alzheimer's disease (AD) çš„è·¨ç»„ç»‡æ£€æµ‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†CpG-wiseçº¿æ€§æŠ•å½±ã€å·ç§¯å±‚ä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰CpGä½ç‚¹é—´çš„å±€éƒ¨ä¸é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨ç»„ç»‡åµŒå…¥æŠ€æœ¯åŒºåˆ†ä¸åŒç»„ç»‡é—´çš„ç”²åŸºåŒ–æ•ˆåº”ã€‚åœ¨æ¶µç›–å…­ä¸ªGEOæ•°æ®é›†åŠADNIç‹¬ç«‹éªŒè¯é˜Ÿåˆ—çš„å®éªŒä¸­ï¼Œè¯¥æ¡†æ¶åœ¨åˆ†ç±»æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åŸºå‡†ã€‚æ­¤å¤–ï¼Œé€šè¿‡SHAPå’ŒGrad-CAM++ç­‰è§£é‡Šæ€§åˆ†æï¼Œè¯¥æ¨¡å‹è¯†åˆ«å‡ºäº†ä¸å…ç–«ä¿¡å·ä¼ å¯¼ã€è„‚è´¨ä»£è°¢åŠå†…è†œç³»ç»Ÿç»„ç»‡ç­‰ADç›¸å…³é€šè·¯ç›¸å»åˆçš„ç”Ÿç‰©å­¦æ¨¡å¼ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒMethConvTransformerèƒ½å¤Ÿæä¾›ç¨³å¥çš„è·¨ç»„ç»‡è¡¨è§‚é—ä¼ ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œä¸ºå®ç°å¯é‡å¤çš„ADæ—©æœŸè¯Šæ–­åŠæ¢ç´¢ç–¾ç—…åˆ†å­æœºåˆ¶æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00143v1",
      "published_date": "2026-01-01 00:18:33 UTC",
      "updated_date": "2026-01-01 00:18:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:13:09.673251+00:00"
    },
    {
      "arxiv_id": "2601.00142v1",
      "title": "An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making",
      "title_zh": "AIçŒ´å­å¿…èƒ½è·å–è‘¡è„â€”â€”é¢å‘å¯é å†³ç­–çš„çƒä½“ç¥ç»ç½‘ç»œ",
      "authors": [
        "Tiansi Dong",
        "Henry He",
        "Pietro LiÃ²",
        "Mateja Jamnik"
      ],
      "abstract": "This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº† LLM reasoningã€supervised learning-based reasoning ä¸ explicit model-based reasoning ä¸‰ç§ç¥ç»æ¨ç†æ–¹æ³•ï¼ŒæŒ‡å‡º LLM åœ¨ç®€å•å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°ä¸å¯é ï¼Œè€ŒåŸºäºç›‘ç£å­¦ä¹ çš„ Euler Net åˆ™é¢ä¸´ä¸¥é‡çš„ catastrophic forgetting é—®é¢˜ä¸”æ¨ç†èƒ½åŠ›å—é™äºæ¨¡å¼å±‚é¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç‰ˆæœ¬çš„ Sphere Neural Networksï¼Œå°†æ¦‚å¿µåµŒå…¥ä¸º n-dimensional sphere è¡¨é¢ä¸Šçš„åœ†åœˆã€‚è¯¥æ¡†æ¶é€šè¿‡ complement circles å®ç°å¯¹ negation operator çš„è¡¨ç¤ºï¼Œå¹¶èƒ½è¿‡æ»¤æ‰å½¢æˆ unsatisfiable circular configurations çš„é€»è¾‘è°¬è¯¯ï¼Œä»è€Œç¡®ä¿å†³ç­–çš„å¯é æ€§ã€‚å®éªŒè¯æ˜ï¼ŒSphere Neural Networks èƒ½å¤Ÿç†Ÿç»ƒå¤„ç†åŒ…æ‹¬ä¸¥è°¨çš„ disjunctive syllogistic reasoning åœ¨å†…çš„ 16 é¡¹æ¨ç†ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒäº†å¤å…¸é€»è¾‘æ¨ç†çš„ä¸¥è°¨æ€§ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè®¤ä¸ºï¼Œåœ¨ä¸‰ç±»æ¨ç†æ–¹æ³•ä¸­ï¼Œé‡‡ç”¨ explicit model construction çš„ç¥ç»æ¨ç†æœ€å…·å¯é æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.00142v1",
      "published_date": "2026-01-01 00:07:37 UTC",
      "updated_date": "2026-01-01 00:07:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:14:17.544155+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 66,
  "processed_papers_count": 66,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T21:16:16.003987+00:00"
}
[
  {
    "arxiv_id": "2503.19900v1",
    "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
    "authors": [
      "Hao Yu",
      "Zhuokai Zhao",
      "Shen Yan",
      "Lukasz Korycki",
      "Jianyu Wang",
      "Baosheng He",
      "Jiayi Liu",
      "Lizhu Zhang",
      "Xiangjun Fan",
      "Hanchao Yu"
    ],
    "abstract": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19900v1",
    "published_date": "2025-03-25 17:57:17 UTC",
    "updated_date": "2025-03-25 17:57:17 UTC"
  },
  {
    "arxiv_id": "2503.19887v1",
    "title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems",
    "authors": [
      "Alejandro Ortega"
    ],
    "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19887v1",
    "published_date": "2025-03-25 17:51:50 UTC",
    "updated_date": "2025-03-25 17:51:50 UTC"
  },
  {
    "arxiv_id": "2503.19885v1",
    "title": "Dynamics of Structured Complex-Valued Hopfield Neural Networks",
    "authors": [
      "Rama Murthy Garimella",
      "Marcos Eduardo Valle",
      "Guilherme Vieira",
      "Anil Rayala",
      "Dileep Munugoti"
    ],
    "abstract": "In this paper, we explore the dynamics of structured complex-valued Hopfield\nneural networks (CvHNNs), which arise when the synaptic weight matrix possesses\nspecific structural properties. We begin by analyzing CvHNNs with a Hermitian\nsynaptic weight matrix and establish the existence of four-cycle dynamics in\nCvHNNs with skew-Hermitian weight matrices operating synchronously.\nFurthermore, we introduce two new classes of complex-valued matrices: braided\nHermitian and braided skew-Hermitian matrices. We demonstrate that CvHNNs\nutilizing these matrix types exhibit cycles of length eight when operating in\nfull parallel update mode. Finally, we conduct extensive computational\nexperiments on synchronous CvHNNs, exploring other synaptic weight matrix\nstructures. The findings provide a comprehensive overview of the dynamics of\nstructured CvHNNs, offering insights that may contribute to developing improved\nassociative memory models when integrated with suitable learning rules.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19885v1",
    "published_date": "2025-03-25 17:49:36 UTC",
    "updated_date": "2025-03-25 17:49:36 UTC"
  },
  {
    "arxiv_id": "2503.19867v1",
    "title": "Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge Representation and Quantum Entanglement",
    "authors": [
      "Ming Lei",
      "Christophe Baehr"
    ],
    "abstract": "This paper establishes a unified framework integrating geometric flows with\ndeep learning through three fundamental innovations. First, we propose a\nthermodynamically coupled Ricci flow that dynamically adapts parameter space\ngeometry to loss landscape topology, formally proved to preserve isometric\nknowledge embedding (Theorem~\\ref{thm:isometric}). Second, we derive explicit\nphase transition thresholds and critical learning rates\n(Theorem~\\ref{thm:critical}) through curvature blowup analysis, enabling\nautomated singularity resolution via geometric surgery\n(Lemma~\\ref{lem:surgery}). Third, we establish an AdS/CFT-type holographic\nduality (Theorem~\\ref{thm:ads}) between neural networks and conformal field\ntheories, providing entanglement entropy bounds for regularization design.\nExperiments demonstrate 2.1$\\times$ convergence acceleration and 63\\%\ntopological simplification while maintaining $\\mathcal{O}(N\\log N)$ complexity,\noutperforming Riemannian baselines by 15.2\\% in few-shot accuracy.\nTheoretically, we prove exponential stability (Theorem~\\ref{thm:converge})\nthrough a new Lyapunov function combining Perelman entropy with Wasserstein\ngradient flows, fundamentally advancing geometric deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "math.GT",
      "quant-ph",
      "68T05, 68T07, 68T27, 81V99, 37F40,",
      "I.2; K.3.2; F.4.1"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, submitted to IEEE PAMI",
    "pdf_url": "http://arxiv.org/pdf/2503.19867v1",
    "published_date": "2025-03-25 17:32:31 UTC",
    "updated_date": "2025-03-25 17:32:31 UTC"
  },
  {
    "arxiv_id": "2503.19868v1",
    "title": "GENIUS: A Generative Framework for Universal Multimodal Search",
    "authors": [
      "Sungyeon Kim",
      "Xinliang Zhu",
      "Xiaofan Lin",
      "Muhammet Bastan",
      "Douglas Gray",
      "Suha Kwak"
    ],
    "abstract": "Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19868v1",
    "published_date": "2025-03-25 17:32:31 UTC",
    "updated_date": "2025-03-25 17:32:31 UTC"
  },
  {
    "arxiv_id": "2503.19848v1",
    "title": "Guarding against artificial intelligence--hallucinated citations: the case for full-text reference deposit",
    "authors": [
      "Alex Glynn"
    ],
    "abstract": "The tendency of generative artificial intelligence (AI) systems to\n\"hallucinate\" false information is well-known; AI-generated citations to\nnon-existent sources have made their way into the reference lists of\npeer-reviewed publications. Here, I propose a solution to this problem, taking\ninspiration from the Transparency and Openness Promotion (TOP) data sharing\nguidelines, the clash of generative AI with the American judiciary, and the\nprecedent set by submissions of prior art to the United States Patent and\nTrademark Office. Journals should require authors to submit the full text of\neach cited source along with their manuscripts, thereby preventing authors from\nciting any material whose full text they cannot produce. This solution requires\nlimited additional work on the part of authors or editors while effectively\nimmunizing journals against hallucinated references.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "I.2.0; K.4.1"
    ],
    "primary_category": "cs.DL",
    "comment": "3 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.19848v1",
    "published_date": "2025-03-25 17:12:38 UTC",
    "updated_date": "2025-03-25 17:12:38 UTC"
  },
  {
    "arxiv_id": "2503.19844v1",
    "title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950",
    "authors": [
      "Zhao Fang",
      "Liang-Chun Wu",
      "Xuening Kong",
      "Spencer Dean Stewart"
    ],
    "abstract": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NLP4DH 2025 at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19844v1",
    "published_date": "2025-03-25 17:07:21 UTC",
    "updated_date": "2025-03-25 17:07:21 UTC"
  },
  {
    "arxiv_id": "2503.19823v1",
    "title": "GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization",
    "authors": [
      "Yan Zhuang",
      "Minheng Chen",
      "Chao Cao",
      "Tong Chen",
      "Jing Zhang",
      "Xiaowei Yu",
      "Yanjun Lyu",
      "Lu Zhang",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "abstract": "Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19823v1",
    "published_date": "2025-03-25 16:33:12 UTC",
    "updated_date": "2025-03-25 16:33:12 UTC"
  },
  {
    "arxiv_id": "2503.19817v1",
    "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
    "authors": [
      "Jordan Madden",
      "Lhamo Dorje",
      "Xiaohua Li"
    ],
    "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19817v1",
    "published_date": "2025-03-25 16:29:17 UTC",
    "updated_date": "2025-03-25 16:29:17 UTC"
  },
  {
    "arxiv_id": "2503.19815v1",
    "title": "Thinking agents for zero-shot generalization to qualitatively novel tasks",
    "authors": [
      "Thomas Miconi",
      "Kevin McKee",
      "Yicong Zheng",
      "Jed McCaleb"
    ],
    "abstract": "Intelligent organisms can solve truly novel problems which they have never\nencountered before, either in their lifetime or their evolution. An important\ncomponent of this capacity is the ability to ``think'', that is, to mentally\nmanipulate objects, concepts and behaviors in order to plan and evaluate\npossible solutions to novel problems, even without environment interaction. To\ngenerate problems that are truly qualitatively novel, while still solvable\nzero-shot (by mental simulation), we use the combinatorial nature of\nenvironments: we train the agent while withholding a specific combination of\nthe environment's elements. The novel test task, based on this combination, is\nthus guaranteed to be truly novel, while still mentally simulable since the\nagent has been exposed to each individual element (and their pairwise\ninteractions) during training. We propose a method to train agents endowed with\nworld models to make use their mental simulation abilities, by selecting tasks\nbased on the difference between the agent's pre-thinking and post-thinking\nperformance. When tested on the novel, withheld problem, the resulting agent\nsuccessfully simulated alternative scenarios and used the resulting information\nto guide its behavior in the actual environment, solving the novel task in a\nsingle real-environment trial (zero-shot).",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19815v1",
    "published_date": "2025-03-25 16:26:31 UTC",
    "updated_date": "2025-03-25 16:26:31 UTC"
  },
  {
    "arxiv_id": "2503.19813v1",
    "title": "Guidelines For The Choice Of The Baseline in XAI Attribution Methods",
    "authors": [
      "Cristian Morasso",
      "Giorgio Dolci",
      "Ilaria Boscolo Galazzo",
      "Sergey M. Plis",
      "Gloria Menegaz"
    ],
    "abstract": "Given the broad adoption of artificial intelligence, it is essential to\nprovide evidence that AI models are reliable, trustable, and fair. To this end,\nthe emerging field of eXplainable AI develops techniques to probe such\nrequirements, counterbalancing the hype pushing the pervasiveness of this\ntechnology. Among the many facets of this issue, this paper focuses on baseline\nattribution methods, aiming at deriving a feature attribution map at the\nnetwork input relying on a \"neutral\" stimulus usually called \"baseline\". The\nchoice of the baseline is crucial as it determines the explanation of the\nnetwork behavior. In this framework, this paper has the twofold goal of\nshedding light on the implications of the choice of the baseline and providing\na simple yet effective method for identifying the best baseline for the task.\nTo achieve this, we propose a decision boundary sampling method, since the\nbaseline, by definition, lies on the decision boundary, which naturally becomes\nthe search domain. Experiments are performed on synthetic examples and\nvalidated relying on state-of-the-art methods. Despite being limited to the\nexperimental scope, this contribution is relevant as it offers clear guidelines\nand a simple proxy for baseline selection, reducing ambiguity and enhancing\ndeep models' reliability and trust.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19813v1",
    "published_date": "2025-03-25 16:25:04 UTC",
    "updated_date": "2025-03-25 16:25:04 UTC"
  },
  {
    "arxiv_id": "2503.19809v1",
    "title": "Simulating Tracking Data to Advance Sports Analytics Research",
    "authors": [
      "David Radke",
      "Kyle Tilbury"
    ],
    "abstract": "Advanced analytics have transformed how sports teams operate, particularly in\nepisodic sports like baseball. Their impact on continuous invasion sports, such\nas soccer and ice hockey, has been limited due to increased game complexity and\nrestricted access to high-resolution game tracking data. In this demo, we\npresent a method to collect and utilize simulated soccer tracking data from the\nGoogle Research Football environment to support the development of models\ndesigned for continuous tracking data. The data is stored in a schema that is\nrepresentative of real tracking data and we provide processes that extract\nhigh-level features and events. We include examples of established tracking\ndata models to showcase the efficacy of the simulated data. We address the\nscarcity of publicly available tracking data, providing support for research at\nthe intersection of artificial intelligence and sports analytics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2 pages, 2 figures, Proceedings of the 24th International Conference\n  on Autonomous Agents and MultiAgent Systems (AAMAS)",
    "pdf_url": "http://arxiv.org/pdf/2503.19809v1",
    "published_date": "2025-03-25 16:18:23 UTC",
    "updated_date": "2025-03-25 16:18:23 UTC"
  },
  {
    "arxiv_id": "2503.19804v1",
    "title": "LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset",
    "authors": [
      "Manjushree Aithal",
      "Rosaura G. VidalMata",
      "Manikandtan Kartha",
      "Gong Chen",
      "Eashan Adhikarla",
      "Lucas N. Kirsten",
      "Zhicheng Fu",
      "Nikhil A. Madhusudhana",
      "Joe Nasti"
    ],
    "abstract": "Low-light image enhancement is crucial for a myriad of applications, from\nnight vision and surveillance, to autonomous driving. However, due to the\ninherent limitations that come in hand with capturing images in\nlow-illumination environments, the task of enhancing such scenes still presents\na formidable challenge. To advance research in this field, we introduce our Low\nExposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure\nbenchmark dataset for low-light image enhancement comprising of over 230K\nframes showcasing 24K real-world indoor and outdoor, with-and without human,\nscenes. Captured using 3 different camera sensors, LENVIZ offers a wide range\nof lighting conditions, noise levels, and scene complexities, making it the\nlargest publicly available up-to 4K resolution benchmark in the field. LENVIZ\nincludes high quality human-generated ground truth, for which each\nmulti-exposure low-light scene has been meticulously curated and edited by\nexpert photographers to ensure optimal image quality. Furthermore, we also\nconduct a comprehensive analysis of current state-of-the-art low-light image\nenhancement techniques on our dataset and highlight potential areas of\nimprovement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Dataset will be released upon publication",
    "pdf_url": "http://arxiv.org/pdf/2503.19804v1",
    "published_date": "2025-03-25 16:12:28 UTC",
    "updated_date": "2025-03-25 16:12:28 UTC"
  },
  {
    "arxiv_id": "2503.19801v1",
    "title": "SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI",
    "authors": [
      "Zhiyang Liu",
      "Dong Yang",
      "Minghao Zhang",
      "Hanyu Sun",
      "Hong Wu",
      "Huiying Wang",
      "Wen Shen",
      "Chao Chai",
      "Shuang Xia"
    ],
    "abstract": "Despite that deep learning (DL) methods have presented tremendous potential\nin many medical image analysis tasks, the practical applications of medical DL\nmodels are limited due to the lack of enough data samples with manual\nannotations. By noting that the clinical radiology examinations are associated\nwith radiology reports that describe the images, we propose to develop a\nfoundation model for multi-model head MRI by using contrastive learning on the\nimages and the corresponding radiology findings. In particular, a contrastive\nlearning framework is proposed, where a mixed syntax and semantic similarity\nmatching metric is integrated to reduce the thirst of extreme large dataset in\nconventional contrastive learning framework. Our proposed similarity enhanced\ncontrastive language image pretraining (SeLIP) is able to effectively extract\nmore useful features. Experiments revealed that our proposed SeLIP performs\nwell in many downstream tasks including image-text retrieval task,\nclassification task, and image segmentation, which highlights the importance of\nconsidering the similarities among texts describing different images in\ndeveloping medical image foundation models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19801v1",
    "published_date": "2025-03-25 16:09:45 UTC",
    "updated_date": "2025-03-25 16:09:45 UTC"
  },
  {
    "arxiv_id": "2503.19794v1",
    "title": "PAVE: Patching and Adapting Video Large Language Models",
    "authors": [
      "Zhuoming Liu",
      "Yiquan Li",
      "Khoi Duc Nguyen",
      "Yiwu Zhong",
      "Yin Li"
    ],
    "abstract": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2025 Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2503.19794v1",
    "published_date": "2025-03-25 16:02:37 UTC",
    "updated_date": "2025-03-25 16:02:37 UTC"
  },
  {
    "arxiv_id": "2503.19786v1",
    "title": "Gemma 3 Technical Report",
    "authors": [
      "Gemma Team",
      "Aishwarya Kamath",
      "Johan Ferret",
      "Shreya Pathak",
      "Nino Vieillard",
      "Ramona Merhej",
      "Sarah Perrin",
      "Tatiana Matejovicova",
      "Alexandre Ramé",
      "Morgane Rivière",
      "Louis Rouillard",
      "Thomas Mesnard",
      "Geoffrey Cideron",
      "Jean-bastien Grill",
      "Sabela Ramos",
      "Edouard Yvinec",
      "Michelle Casbon",
      "Etienne Pot",
      "Ivo Penchev",
      "Gaël Liu",
      "Francesco Visin",
      "Kathleen Kenealy",
      "Lucas Beyer",
      "Xiaohai Zhai",
      "Anton Tsitsulin",
      "Robert Busa-Fekete",
      "Alex Feng",
      "Noveen Sachdeva",
      "Benjamin Coleman",
      "Yi Gao",
      "Basil Mustafa",
      "Iain Barr",
      "Emilio Parisotto",
      "David Tian",
      "Matan Eyal",
      "Colin Cherry",
      "Jan-Thorsten Peter",
      "Danila Sinopalnikov",
      "Surya Bhupatiraju",
      "Rishabh Agarwal",
      "Mehran Kazemi",
      "Dan Malkin",
      "Ravin Kumar",
      "David Vilar",
      "Idan Brusilovsky",
      "Jiaming Luo",
      "Andreas Steiner",
      "Abe Friesen",
      "Abhanshu Sharma",
      "Abheesht Sharma",
      "Adi Mayrav Gilady",
      "Adrian Goedeckemeyer",
      "Alaa Saade",
      "Alex Feng",
      "Alexander Kolesnikov",
      "Alexei Bendebury",
      "Alvin Abdagic",
      "Amit Vadi",
      "András György",
      "André Susano Pinto",
      "Anil Das",
      "Ankur Bapna",
      "Antoine Miech",
      "Antoine Yang",
      "Antonia Paterson",
      "Ashish Shenoy",
      "Ayan Chakrabarti",
      "Bilal Piot",
      "Bo Wu",
      "Bobak Shahriari",
      "Bryce Petrini",
      "Charlie Chen",
      "Charline Le Lan",
      "Christopher A. Choquette-Choo",
      "CJ Carey",
      "Cormac Brick",
      "Daniel Deutsch",
      "Danielle Eisenbud",
      "Dee Cattle",
      "Derek Cheng",
      "Dimitris Paparas",
      "Divyashree Shivakumar Sreepathihalli",
      "Doug Reid",
      "Dustin Tran",
      "Dustin Zelle",
      "Eric Noland",
      "Erwin Huizenga",
      "Eugene Kharitonov",
      "Frederick Liu",
      "Gagik Amirkhanyan",
      "Glenn Cameron",
      "Hadi Hashemi",
      "Hanna Klimczak-Plucińska",
      "Harman Singh",
      "Harsh Mehta",
      "Harshal Tushar Lehri",
      "Hussein Hazimeh",
      "Ian Ballantyne",
      "Idan Szpektor",
      "Ivan Nardini",
      "Jean Pouget-Abadie",
      "Jetha Chan",
      "Joe Stanton",
      "John Wieting",
      "Jonathan Lai",
      "Jordi Orbay",
      "Joseph Fernandez",
      "Josh Newlan",
      "Ju-yeong Ji",
      "Jyotinder Singh",
      "Kat Black",
      "Kathy Yu",
      "Kevin Hui",
      "Kiran Vodrahalli",
      "Klaus Greff",
      "Linhai Qiu",
      "Marcella Valentine",
      "Marina Coelho",
      "Marvin Ritter",
      "Matt Hoffman",
      "Matthew Watson",
      "Mayank Chaturvedi",
      "Michael Moynihan",
      "Min Ma",
      "Nabila Babar",
      "Natasha Noy",
      "Nathan Byrd",
      "Nick Roy",
      "Nikola Momchev",
      "Nilay Chauhan",
      "Noveen Sachdeva",
      "Oskar Bunyan",
      "Pankil Botarda",
      "Paul Caron",
      "Paul Kishan Rubenstein",
      "Phil Culliton",
      "Philipp Schmid",
      "Pier Giuseppe Sessa",
      "Pingmei Xu",
      "Piotr Stanczyk",
      "Pouya Tafti",
      "Rakesh Shivanna",
      "Renjie Wu",
      "Renke Pan",
      "Reza Rokni",
      "Rob Willoughby",
      "Rohith Vallu",
      "Ryan Mullins",
      "Sammy Jerome",
      "Sara Smoot",
      "Sertan Girgin",
      "Shariq Iqbal",
      "Shashir Reddy",
      "Shruti Sheth",
      "Siim Põder",
      "Sijal Bhatnagar",
      "Sindhu Raghuram Panyam",
      "Sivan Eiger",
      "Susan Zhang",
      "Tianqi Liu",
      "Trevor Yacovone",
      "Tyler Liechty",
      "Uday Kalra",
      "Utku Evci",
      "Vedant Misra",
      "Vincent Roseberry",
      "Vlad Feinberg",
      "Vlad Kolesnikov",
      "Woohyun Han",
      "Woosuk Kwon",
      "Xi Chen",
      "Yinlam Chow",
      "Yuvein Zhu",
      "Zichuan Wei",
      "Zoltan Egyed",
      "Victor Cotruta",
      "Minh Giang",
      "Phoebe Kirk",
      "Anand Rao",
      "Kat Black",
      "Nabila Babar",
      "Jessica Lo",
      "Erica Moreira",
      "Luiz Gustavo Martins",
      "Omar Sanseviero",
      "Lucas Gonzalez",
      "Zach Gleicher",
      "Tris Warkentin",
      "Vahab Mirrokni",
      "Evan Senter",
      "Eli Collins",
      "Joelle Barral",
      "Zoubin Ghahramani",
      "Raia Hadsell",
      "Yossi Matias",
      "D. Sculley",
      "Slav Petrov",
      "Noah Fiedel",
      "Noam Shazeer",
      "Oriol Vinyals",
      "Jeff Dean",
      "Demis Hassabis",
      "Koray Kavukcuoglu",
      "Clement Farabet",
      "Elena Buchatskaya",
      "Jean-Baptiste Alayrac",
      "Rohan Anil",
      "Dmitry",
      "Lepikhin",
      "Sebastian Borgeaud",
      "Olivier Bachem",
      "Armand Joulin",
      "Alek Andreev",
      "Cassidy Hardin",
      "Robert Dadashi",
      "Léonard Hussenot"
    ],
    "abstract": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19786v1",
    "published_date": "2025-03-25 15:52:34 UTC",
    "updated_date": "2025-03-25 15:52:34 UTC"
  },
  {
    "arxiv_id": "2503.19762v1",
    "title": "Splitting Answer Set Programs with respect to Intensionality Statements (Extended Version)",
    "authors": [
      "Jorge Fandinno",
      "Yuliya Lierler"
    ],
    "abstract": "Splitting a logic program allows us to reduce the task of computing its\nstable models to similar tasks for its subprograms. This can be used to\nincrease solving performance and prove program correctness. We generalize the\nconditions under which this technique is applicable, by considering not only\ndependencies between predicates but also their arguments and context. This\nallows splitting programs commonly used in practice to which previous results\nwere not applicable.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of the paper published in AAAI 2023",
    "pdf_url": "http://arxiv.org/pdf/2503.19762v1",
    "published_date": "2025-03-25 15:27:05 UTC",
    "updated_date": "2025-03-25 15:27:05 UTC"
  },
  {
    "arxiv_id": "2503.19753v2",
    "title": "A Survey on Event-driven 3D Reconstruction: Development under Different Categories",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Haodong Chen",
      "Vera Chung",
      "Qiang Qu"
    ],
    "abstract": "Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "6 pages, 1 figure, 6 tables, submitted to an anonymous conference\n  under double-blind review",
    "pdf_url": "http://arxiv.org/pdf/2503.19753v2",
    "published_date": "2025-03-25 15:16:53 UTC",
    "updated_date": "2025-03-26 12:34:34 UTC"
  },
  {
    "arxiv_id": "2503.19752v1",
    "title": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect on Human-Like Agenda Generation",
    "authors": [
      "Lewis Newsham",
      "Ryan Hyland",
      "Daniel Prince"
    ],
    "abstract": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 1 figure, 6 tables. Accepted to NLPAICS 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.19752v1",
    "published_date": "2025-03-25 15:16:35 UTC",
    "updated_date": "2025-03-25 15:16:35 UTC"
  },
  {
    "arxiv_id": "2503.19730v2",
    "title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos",
    "authors": [
      "Yuli Zhou",
      "Guolei Sun",
      "Yawei Li",
      "Yuqian Fu",
      "Luca Benini",
      "Ender Konukoglu"
    ],
    "abstract": "Video camouflaged object segmentation (VCOS), aiming at segmenting\ncamouflaged objects that seamlessly blend into their environment, is a\nfundamental vision task with various real-world applications. With the release\nof SAM2, video segmentation has witnessed significant progress. However, SAM2's\ncapability of segmenting camouflaged videos is suboptimal, especially when\ngiven simple prompts such as point and box. To address the problem, we propose\nCamouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged\nscenes without modifying SAM2's parameters. Specifically, we introduce a\ndecamouflaged token to provide the flexibility of feature adjustment for VCOS.\nTo make full use of fine-grained and high-resolution features from the current\nframe and previous frames, we propose implicit object-aware fusion (IOF) and\nexplicit object-aware fusion (EOF) modules, respectively. Object prototype\ngeneration (OPG) is introduced to abstract and memorize object prototypes with\ninformative details using high-quality features from previous frames. Extensive\nexperiments are conducted to validate the effectiveness of our approach. While\nCamSAM2 only adds negligible learnable parameters to SAM2, it substantially\noutperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains\nwith click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on\nSUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at\nhttps://github.com/zhoustan/CamSAM2.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19730v2",
    "published_date": "2025-03-25 14:58:52 UTC",
    "updated_date": "2025-03-26 02:14:50 UTC"
  },
  {
    "arxiv_id": "2503.19719v1",
    "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?",
    "authors": [
      "Francisco Mena",
      "Diego Arenas",
      "Miro Miranda",
      "Andreas Dengel"
    ],
    "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19719v1",
    "published_date": "2025-03-25 14:45:23 UTC",
    "updated_date": "2025-03-25 14:45:23 UTC"
  },
  {
    "arxiv_id": "2503.19717v1",
    "title": "Invertible Koopman neural operator for data-driven modeling of partial differential equations",
    "authors": [
      "Yuhong Jin",
      "Andong Cong",
      "Lei Hou",
      "Qiang Gao",
      "Xiangdong Ge",
      "Chonglong Zhu",
      "Yongzhi Feng",
      "Jun Li"
    ],
    "abstract": "Koopman operator theory is a popular candidate for data-driven modeling\nbecause it provides a global linearization representation for nonlinear\ndynamical systems. However, existing Koopman operator-based methods suffer from\nshortcomings in constructing the well-behaved observable function and its\ninverse and are inefficient enough when dealing with partial differential\nequations (PDEs). To address these issues, this paper proposes the Invertible\nKoopman Neural Operator (IKNO), a novel data-driven modeling approach inspired\nby the Koopman operator theory and neural operator. IKNO leverages an\nInvertible Neural Network to parameterize observable function and its inverse\nsimultaneously under the same learnable parameters, explicitly guaranteeing the\nreconstruction relation, thus eliminating the dependency on the reconstruction\nloss, which is an essential improvement over the original Koopman Neural\nOperator (KNO). The structured linear matrix inspired by the Koopman operator\ntheory is parameterized to learn the evolution of observables' low-frequency\nmodes in the frequency space rather than directly in the observable space,\nsustaining IKNO is resolution-invariant like other neural operators. Moreover,\nwith preprocessing such as interpolation and dimension expansion, IKNO can be\nextended to operator learning tasks defined on non-Cartesian domains. We fully\nsupport the above claims based on rich numerical and real-world examples and\ndemonstrate the effectiveness of IKNO and superiority over other neural\noperators.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19717v1",
    "published_date": "2025-03-25 14:43:53 UTC",
    "updated_date": "2025-03-25 14:43:53 UTC"
  },
  {
    "arxiv_id": "2503.19712v1",
    "title": "Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions",
    "authors": [
      "Sanghyuk Kim",
      "Minsik Seo",
      "Namwoo Kang"
    ],
    "abstract": "This study proposes a neural framework that predicts 3D vehicle collision\ndynamics by independently modeling global rigid-body motion and local\nstructural deformation. Unlike approaches directly predicting absolute\ndisplacement, this method explicitly separates the vehicle's overall\ntranslation and rotation from its structural deformation. Two specialized\nnetworks form the core of the framework: a quaternion-based Rigid Net for rigid\nmotion and a coordinate-based Deformation Net for local deformation. By\nindependently handling fundamentally distinct physical phenomena, the proposed\narchitecture achieves accurate predictions without requiring separate\nsupervision for each component. The model, trained on only 10% of available\nsimulation data, significantly outperforms baseline models, including single\nmulti-layer perceptron (MLP) and deep operator networks (DeepONet), with\nprediction errors reduced by up to 83%. Extensive validation demonstrates\nstrong generalization to collision conditions outside the training range,\naccurately predicting responses even under severe impacts involving extreme\nvelocities and large impact angles. Furthermore, the framework successfully\nreconstructs high-resolution deformation details from low-resolution inputs\nwithout increased computational effort. Consequently, the proposed approach\nprovides an effective, computationally efficient method for rapid and reliable\nassessment of vehicle safety across complex collision scenarios, substantially\nreducing the required simulation data and time while preserving prediction\nfidelity.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "24 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19712v1",
    "published_date": "2025-03-25 14:38:37 UTC",
    "updated_date": "2025-03-25 14:38:37 UTC"
  },
  {
    "arxiv_id": "2503.19711v1",
    "title": "Writing as a testbed for open ended agents",
    "authors": [
      "Sian Gooding",
      "Lucia Lopez-Rivilla",
      "Edward Grefenstette"
    ],
    "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19711v1",
    "published_date": "2025-03-25 14:38:36 UTC",
    "updated_date": "2025-03-25 14:38:36 UTC"
  },
  {
    "arxiv_id": "2503.19706v1",
    "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
    "authors": [
      "Jungin Park",
      "Jiyoung Lee",
      "Kwanghoon Sohn"
    ],
    "abstract": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 Camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2503.19706v1",
    "published_date": "2025-03-25 14:33:32 UTC",
    "updated_date": "2025-03-25 14:33:32 UTC"
  },
  {
    "arxiv_id": "2503.19699v1",
    "title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "abstract": "In this study, we formulate the drone delivery problem as a control problem\nand solve it using Model Predictive Control. Two experiments are performed: The\nfirst is on a less challenging grid world environment with lower\ndimensionality, and the second is with a higher dimensionality and added\ncomplexity. The MPC method was benchmarked against three popular Multi-Agent\nReinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action\nLearners (JAL), and Value-Decomposition Networks (VDN). It was shown that the\nMPC method solved the problem quicker and required fewer optimal numbers of\ndrones to achieve a minimized cost and navigate the optimal path.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 5 figures, Submitted to the 2025 International Conference\n  on Artificial Intelligence, Computer, Data Sciences and Applications",
    "pdf_url": "http://arxiv.org/pdf/2503.19699v1",
    "published_date": "2025-03-25 14:27:29 UTC",
    "updated_date": "2025-03-25 14:27:29 UTC"
  },
  {
    "arxiv_id": "2503.19677v1",
    "title": "Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing Mel Spectrograms",
    "authors": [
      "Niketa Penumajji"
    ],
    "abstract": "This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19677v1",
    "published_date": "2025-03-25 14:02:10 UTC",
    "updated_date": "2025-03-25 14:02:10 UTC"
  },
  {
    "arxiv_id": "2503.19658v1",
    "title": "BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction",
    "authors": [
      "Jan Kohút",
      "Martin Dočekal",
      "Michal Hradiš",
      "Marek Vaško"
    ],
    "abstract": "Manual digitization of bibliographic metadata is time consuming and labor\nintensive, especially for historical and real-world archives with highly\nvariable formatting across documents. Despite advances in machine learning, the\nabsence of dedicated datasets for metadata extraction hinders automation. To\naddress this gap, we introduce BiblioPage, a dataset of scanned title pages\nannotated with structured bibliographic metadata. The dataset consists of\napproximately 2,000 monograph title pages collected from 14 Czech libraries,\nspanning a wide range of publication periods, typographic styles, and layout\nstructures. Each title page is annotated with 16 bibliographic attributes,\nincluding title, contributors, and publication metadata, along with precise\npositional information in the form of bounding boxes. To extract structured\ninformation from this dataset, we valuated object detection models such as YOLO\nand DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and\nan F1 score of 59. Additionally, we assess the performance of various visual\nlarge language models, including LlamA 3.2-Vision and GPT-4o, with the best\nmodel reaching an F1 score of 67. BiblioPage serves as a real-world benchmark\nfor bibliographic metadata extraction, contributing to document understanding,\ndocument question answering, and document information extraction. Dataset and\nevaluation scripts are availible at: https://github.com/DCGM/biblio-dataset",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to ICDAR2025 conference",
    "pdf_url": "http://arxiv.org/pdf/2503.19658v1",
    "published_date": "2025-03-25 13:46:55 UTC",
    "updated_date": "2025-03-25 13:46:55 UTC"
  },
  {
    "arxiv_id": "2503.19656v1",
    "title": "Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms",
    "authors": [
      "Ninghui Feng",
      "Songning Lai",
      "Xin Zhou",
      "Jiayu Yang",
      "Kunlong Feng",
      "Zhenxiao Yin",
      "Fobao Zhou",
      "Zhangyi Hu",
      "Yutao Yue",
      "Yuxuan Liang",
      "Boyu Wang",
      "Hang Zhao"
    ],
    "abstract": "In real-world time series forecasting, uncertainty and lack of reliable\nevaluation pose significant challenges. Notably, forecasting errors often arise\nfrom underfitting in-distribution data and failing to handle\nout-of-distribution inputs. To enhance model reliability, we introduce a dual\nrejection mechanism combining ambiguity and novelty rejection. Ambiguity\nrejection, using prediction error variance, allows the model to abstain under\nlow confidence, assessed through historical error variance analysis without\nfuture ground truth. Novelty rejection, employing Variational Autoencoders and\nMahalanobis distance, detects deviations from training data. This dual approach\nimproves forecasting reliability in dynamic environments by reducing errors and\nadapting to data changes, advancing reliability in complex scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19656v1",
    "published_date": "2025-03-25 13:44:29 UTC",
    "updated_date": "2025-03-25 13:44:29 UTC"
  },
  {
    "arxiv_id": "2503.19654v1",
    "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models",
    "authors": [
      "Mehdi Moshtaghi",
      "Siavash H. Khajavi",
      "Joni Pajarinen"
    ],
    "abstract": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19654v1",
    "published_date": "2025-03-25 13:43:47 UTC",
    "updated_date": "2025-03-25 13:43:47 UTC"
  },
  {
    "arxiv_id": "2503.19653v1",
    "title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World",
    "authors": [
      "Yabin Wang",
      "Zhiwu Huang",
      "Xiaopeng Hong"
    ],
    "abstract": "This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps://github.com/iamwangyabin/OpenSDI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19653v1",
    "published_date": "2025-03-25 13:43:16 UTC",
    "updated_date": "2025-03-25 13:43:16 UTC"
  },
  {
    "arxiv_id": "2503.19650v1",
    "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection",
    "authors": [
      "Maryam Bala",
      "Amina Imam Abubakar",
      "Abdulhamid Abubakar",
      "Abdulkadir Shehu Bichi",
      "Hafsa Kabir Ahmad",
      "Sani Abdullahi Sani",
      "Idris Abdulmumin",
      "Shamsuddeen Hassan Muhamad",
      "Ibrahim Said Ahmad"
    ],
    "abstract": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19650v1",
    "published_date": "2025-03-25 13:40:22 UTC",
    "updated_date": "2025-03-25 13:40:22 UTC"
  },
  {
    "arxiv_id": "2503.19649v1",
    "title": "Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac Feature Monitoring from Radar Signal Components",
    "authors": [
      "Yuanyuan Zhang",
      "Sijie Xiong",
      "Rui Yang",
      "EngGee Lim",
      "Yutao Yue"
    ],
    "abstract": "Radar-based wellness monitoring is becoming an effective measurement to\nprovide accurate vital signs in a contactless manner, but data scarcity retards\nthe related research on deep-learning-based methods. Data augmentation is\ncommonly used to enrich the dataset by modifying the existing data, but most\naugmentation techniques can only couple with classification tasks. To enable\nthe augmentation for regression tasks, this research proposes a spectrogram\naugmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g.,\nheartbeat detection, electrocardiogram reconstruction) with both classification\nand regression tasks involved. The proposed method is designed to increase the\ndiversity of input samples while the augmented spectrogram is still faithful to\nthe original ground truth vital sign. In addition, Horcrux proposes to inject\nzero values in specific areas to enhance the awareness of the deep learning\nmodel on subtle cardiac features, improving the performance for the limited\ndataset. Experimental result shows that Horcrux achieves an overall improvement\nof 16.20% in cardiac monitoring and has the potential to be extended to other\nspectrogram-based tasks. The code will be released upon publication.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19649v1",
    "published_date": "2025-03-25 13:40:05 UTC",
    "updated_date": "2025-03-25 13:40:05 UTC"
  },
  {
    "arxiv_id": "2503.19647v1",
    "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
    "authors": [
      "Niccolo Avogaro",
      "Thomas Frick",
      "Mattia Rigotti",
      "Andrea Bartezzaghi",
      "Filip Janicki",
      "Cristiano Malossi",
      "Konrad Schindler",
      "Roy Assaf"
    ],
    "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as\nfoundation models that can be instructed to solve diverse tasks by prompting,\nwithout task-specific training. We examine the seemingly obvious question: how\nto effectively prompt VLMs for semantic segmentation. To that end, we\nsystematically evaluate the segmentation performance of several recent models\nguided by either text or visual prompts on the out-of-distribution MESS dataset\ncollection. We introduce a scalable prompting scheme, few-shot prompted\nsemantic segmentation, inspired by open-vocabulary segmentation and few-shot\nlearning. It turns out that VLMs lag far behind specialist models trained for a\nspecific segmentation task, by about 30% on average on the\nIntersection-over-Union metric. Moreover, we find that text prompts and visual\nprompts are complementary: each one of the two modes fails on many examples\nthat the other one can solve. Our analysis suggests that being able to\nanticipate the most effective prompt modality can lead to a 11% improvement in\nperformance. Motivated by our findings, we propose PromptMatcher, a remarkably\nsimple training-free baseline that combines both text and visual prompts,\nachieving state-of-the-art results outperforming the best text-prompted VLM by\n2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic\nsegmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19647v1",
    "published_date": "2025-03-25 13:36:59 UTC",
    "updated_date": "2025-03-25 13:36:59 UTC"
  },
  {
    "arxiv_id": "2503.19611v1",
    "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
    "authors": [
      "Max W. Y. Lam",
      "Yijin Xing",
      "Weiya You",
      "Jingcheng Wu",
      "Zongyu Yin",
      "Fuqiang Jiang",
      "Hangyu Liu",
      "Feng Liu",
      "Xingda Li",
      "Wei-Tsung Lu",
      "Hanyu Chen",
      "Tong Feng",
      "Tianwei Zhao",
      "Chien-Hung Liu",
      "Xuchen Song",
      "Yang Li",
      "Yahui Zhou"
    ],
    "abstract": "Autoregressive (AR) models have demonstrated impressive capabilities in\ngenerating high-fidelity music. However, the conventional next-token prediction\nparadigm in AR models does not align with the human creative process in music\ncomposition, potentially compromising the musicality of generated samples. To\novercome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT)\nprompting technique tailored for music generation. MusiCoT empowers the AR\nmodel to first outline an overall music structure before generating audio\ntokens, thereby enhancing the coherence and creativity of the resulting\ncompositions. By leveraging the contrastive language-audio pretraining (CLAP)\nmodel, we establish a chain of \"musical thoughts\", making MusiCoT scalable and\nindependent of human-labeled data, in contrast to conventional CoT methods.\nMoreover, MusiCoT allows for in-depth analysis of music structure, such as\ninstrumental arrangements, and supports music referencing -- accepting\nvariable-length audio inputs as optional style references. This innovative\napproach effectively addresses copying issues, positioning MusiCoT as a vital\npractical method for music prompting. Our experimental results indicate that\nMusiCoT consistently achieves superior performance across both objective and\nsubjective metrics, producing music quality that rivals state-of-the-art\ngeneration models.\n  Our samples are available at https://MusiCoT.github.io/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.19611v1",
    "published_date": "2025-03-25 12:51:21 UTC",
    "updated_date": "2025-03-25 12:51:21 UTC"
  },
  {
    "arxiv_id": "2503.19607v1",
    "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review",
    "authors": [
      "Edward Gu",
      "Ho Chit Siu",
      "Melanie Platt",
      "Isabelle Hurley",
      "Jaime Peña",
      "Rohan Paleja"
    ],
    "abstract": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to the Cooperative Multi-Agent Systems Decision-making and\n  Learning:Human-Multi-Agent Cognitive Fusion Workshop at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19607v1",
    "published_date": "2025-03-25 12:43:18 UTC",
    "updated_date": "2025-03-25 12:43:18 UTC"
  },
  {
    "arxiv_id": "2503.19602v1",
    "title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking",
    "authors": [
      "Yuyao Ge",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Lizhe Chen",
      "Baolong Bi",
      "Xueqi Cheng"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19602v1",
    "published_date": "2025-03-25 12:37:22 UTC",
    "updated_date": "2025-03-25 12:37:22 UTC"
  },
  {
    "arxiv_id": "2503.19599v1",
    "title": "HoarePrompt: Structural Reasoning About Program Correctness in Natural Language",
    "authors": [
      "Dimitrios Stamatios Bouras",
      "Yihan Dai",
      "Tairan Wang",
      "Yingfei Xiong",
      "Sergey Mechtaev"
    ],
    "abstract": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19599v1",
    "published_date": "2025-03-25 12:30:30 UTC",
    "updated_date": "2025-03-25 12:30:30 UTC"
  },
  {
    "arxiv_id": "2503.19584v2",
    "title": "Multi-agent Application System in Office Collaboration Scenarios",
    "authors": [
      "Songtao Sun",
      "Jingyi Li",
      "Yuanfei Dong",
      "Haoguang Liu",
      "Chenxin Xu",
      "Fuyang Li",
      "Qiang Liu"
    ],
    "abstract": "This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2503.19584v2",
    "published_date": "2025-03-25 12:07:20 UTC",
    "updated_date": "2025-03-26 03:55:15 UTC"
  },
  {
    "arxiv_id": "2503.19564v1",
    "title": "FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments",
    "authors": [
      "Sree Bhargavi Balija"
    ],
    "abstract": "As artificial intelligence systems increasingly operate in Real-world\nenvironments, the integration of multi-modal data sources such as vision,\nlanguage, and audio presents both unprecedented opportunities and critical\nchallenges for achieving trustworthy intelligence. In this paper, we propose a\nnovel framework that unifies federated learning with explainable multi-modal\nreasoning to ensure trustworthiness in decentralized, dynamic settings. Our\napproach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),\nleverages cross-modal consistency checks, client-level interpretability\nmechanisms, and dynamic trust calibration to address challenges posed by data\nheterogeneity, modality imbalance, and out-of-distribution generalization.\nThrough rigorous evaluation across federated multi-modal benchmarks involving\nvision-language tasks, we demonstrate improved performance in both accuracy and\ninterpretability while reducing vulnerabilities to adversarial and spurious\ncorrelations. Further, we introduce a novel trust score aggregation method to\nquantify global model reliability under dynamic client participation. Our\nfindings pave the way toward developing robust, interpretable, and socially\nresponsible AI systems in Real-world environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19564v1",
    "published_date": "2025-03-25 11:28:21 UTC",
    "updated_date": "2025-03-25 11:28:21 UTC"
  },
  {
    "arxiv_id": "2503.19551v2",
    "title": "Scaling Laws of Synthetic Data for Language Models",
    "authors": [
      "Zeyu Qin",
      "Qingxiu Dong",
      "Xingxing Zhang",
      "Li Dong",
      "Xiaolong Huang",
      "Ziyi Yang",
      "Mahmoud Khademi",
      "Dongdong Zhang",
      "Hany Hassan Awadalla",
      "Yi R. Fung",
      "Weizhu Chen",
      "Minhao Cheng",
      "Furu Wei"
    ],
    "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.19551v2",
    "published_date": "2025-03-25 11:07:12 UTC",
    "updated_date": "2025-03-26 11:23:44 UTC"
  },
  {
    "arxiv_id": "2503.19540v1",
    "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
    "authors": [
      "Dahyun Jung",
      "Seungyoon Lee",
      "Hyeonseok Moon",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 findings",
    "pdf_url": "http://arxiv.org/pdf/2503.19540v1",
    "published_date": "2025-03-25 10:48:33 UTC",
    "updated_date": "2025-03-25 10:48:33 UTC"
  },
  {
    "arxiv_id": "2503.19530v1",
    "title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained Foundation Models",
    "authors": [
      "Suhas G Hegde",
      "Shilpy Kaur",
      "Aruna Tiwari"
    ],
    "abstract": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19530v1",
    "published_date": "2025-03-25 10:36:27 UTC",
    "updated_date": "2025-03-25 10:36:27 UTC"
  },
  {
    "arxiv_id": "2503.19510v1",
    "title": "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation",
    "authors": [
      "Sheng Wang"
    ],
    "abstract": "As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19510v1",
    "published_date": "2025-03-25 10:01:57 UTC",
    "updated_date": "2025-03-25 10:01:57 UTC"
  },
  {
    "arxiv_id": "2503.19502v1",
    "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
    "authors": [
      "Qi Chen",
      "Yinghao Cui",
      "Guobin Hong",
      "Karumuri Ashok",
      "Yuchun Pu",
      "Xiaogu Zheng",
      "Xuanze Zhang",
      "Wei Zhong",
      "Peng Zhan",
      "Zhonglei Wang"
    ],
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19502v1",
    "published_date": "2025-03-25 09:50:19 UTC",
    "updated_date": "2025-03-25 09:50:19 UTC"
  },
  {
    "arxiv_id": "2503.19501v1",
    "title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs",
    "authors": [
      "Vinayak Mali",
      "Saurabh Jaiswal"
    ],
    "abstract": "Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 Pages, 2 figures, 2 code block, 1 flow chart",
    "pdf_url": "http://arxiv.org/pdf/2503.19501v1",
    "published_date": "2025-03-25 09:49:36 UTC",
    "updated_date": "2025-03-25 09:49:36 UTC"
  },
  {
    "arxiv_id": "2503.19496v1",
    "title": "SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration",
    "authors": [
      "Mohammad Daffa Robani",
      "Paul Saves",
      "Pramudita Satria Palar",
      "Lavi Rizki Zuhal",
      "oseph Morlier"
    ],
    "abstract": "Surrogate models are of high interest for many engineering applications,\nserving as cheap-to-evaluate time-efficient approximations of black-box\nfunctions to help engineers and practitioners make decisions and understand\ncomplex systems. As such, the need for explainability methods is rising and\nmany studies have been performed to facilitate knowledge discovery from\nsurrogate models. To respond to these enquiries, this paper introduces SMT-EX,\nan enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that\nintegrates explainability techniques into a state-of-the-art surrogate\nmodelling framework. More precisely, SMT-EX includes three key explainability\nmethods: Shapley Additive Explanations, Partial Dependence Plot, and Individual\nConditional Expectations. A peculiar explainability dependency of SMT has been\ndeveloped for such purpose that can be easily activated once the surrogate\nmodel is built, offering a user-friendly and efficient tool for swift insight\nextraction. The effectiveness of SMT-EX is showcased through two test cases.\nThe first case is a 10-variable wing weight problem with purely continuous\nvariables and the second one is a 3-variable mixed-categorical cantilever beam\nbending problem. Relying on SMT-EX analyses for these problems, we demonstrate\nits versatility in addressing a diverse range of problem characteristics.\nSMT-Explainability is freely available on Github:\nhttps://github.com/SMTorg/smt-explainability .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19496v1",
    "published_date": "2025-03-25 09:38:27 UTC",
    "updated_date": "2025-03-25 09:38:27 UTC"
  },
  {
    "arxiv_id": "2503.19474v1",
    "title": "A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition",
    "authors": [
      "Yaomin Shen",
      "Xiaojian Lin",
      "Wei Fan"
    ],
    "abstract": "In the domain of multimodal intent recognition (MIR), the objective is to\nrecognize human intent by integrating a variety of modalities, such as language\ntext, body gestures, and tones. However, existing approaches face difficulties\nadequately capturing the intrinsic connections between the modalities and\noverlooking the corresponding semantic representations of intent. To address\nthese limitations, we present the Anchor-based Mul- timodal Embedding with\nSemantic Synchronization (A-MESS) framework. We first design an Anchor-based\nMultimodal Embed- ding (A-ME) module that employs an anchor-based embedding\nfusion mechanism to integrate multimodal inputs. Furthermore, we develop a\nSemantic Synchronization (SS) strategy with the Triplet Contrastive Learning\npipeline, which optimizes the pro- cess by synchronizing multimodal\nrepresentation with label de- scriptions produced by the large language model.\nComprehensive experiments indicate that our A-MESS achieves state-of-the-art\nand provides substantial insight into multimodal representation and downstream\ntasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accept by ICME2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19474v1",
    "published_date": "2025-03-25 09:09:30 UTC",
    "updated_date": "2025-03-25 09:09:30 UTC"
  },
  {
    "arxiv_id": "2503.19470v1",
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
    "authors": [
      "Mingyang Chen",
      "Tianpeng Li",
      "Haoze Sun",
      "Yijie Zhou",
      "Chenzheng Zhu",
      "Fan Yang",
      "Zenan Zhou",
      "Weipeng Chen",
      "Haofen Wang",
      "Jeff Z. Pan",
      "Wen Zhang",
      "Huajun Chen"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.19470v1",
    "published_date": "2025-03-25 09:00:58 UTC",
    "updated_date": "2025-03-25 09:00:58 UTC"
  },
  {
    "arxiv_id": "2503.19469v1",
    "title": "Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning",
    "authors": [
      "Fred Philippy",
      "Siwen Guo",
      "Cedric Lothritz",
      "Jacques Klein",
      "Tegawendé F. Bissyandé"
    ],
    "abstract": "In NLP, Zero-Shot Classification (ZSC) has become essential for enabling\nmodels to classify text into categories unseen during training, particularly in\nlow-resource languages and domains where labeled data is scarce. While\npretrained language models (PLMs) have shown promise in ZSC, they often rely on\nlarge training datasets or external knowledge, limiting their applicability in\nmultilingual and low-resource scenarios. Recent approaches leveraging natural\nlanguage prompts reduce the dependence on large training datasets but struggle\nto effectively incorporate available labeled data from related classification\ntasks, especially when these datasets originate from different languages or\ndistributions. Moreover, existing prompt-based methods typically rely on\nmanually crafted prompts in a specific language, limiting their adaptability\nand effectiveness in cross-lingual settings. To address these challenges, we\nintroduce RoSPrompt, a lightweight and data-efficient approach for training\nsoft prompts that enhance cross-lingual ZSC while ensuring robust\ngeneralization across data distribution shifts. RoSPrompt is designed for small\nmultilingual PLMs, enabling them to leverage high-resource languages to improve\nperformance in low-resource settings without requiring extensive fine-tuning or\nhigh computational costs. We evaluate our approach on multiple multilingual\nPLMs across datasets covering 106 languages, demonstrating strong cross-lingual\ntransfer performance and robust generalization capabilities over unseen\nclasses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Workshop on Language Models for Underserved Communities (co-located\n  with NAACL 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.19469v1",
    "published_date": "2025-03-25 09:00:25 UTC",
    "updated_date": "2025-03-25 09:00:25 UTC"
  },
  {
    "arxiv_id": "2503.19455v1",
    "title": "Data-centric Federated Graph Learning with Large Language Models",
    "authors": [
      "Bo Yan",
      "Zhongjian Zhang",
      "Huabin Sun",
      "Mengmei Zhang",
      "Yang Cao",
      "Chuan Shi"
    ],
    "abstract": "In federated graph learning (FGL), a complete graph is divided into multiple\nsubgraphs stored in each client due to privacy concerns, and all clients\njointly train a global graph model by only transmitting model parameters. A\npain point of FGL is the heterogeneity problem, where nodes or structures\npresent non-IID properties among clients (e.g., different node label\ndistributions), dramatically undermining the convergence and performance of\nFGL. To address this, existing efforts focus on design strategies at the model\nlevel, i.e., they design models to extract common knowledge to mitigate\nheterogeneity. However, these model-level strategies fail to fundamentally\naddress the heterogeneity problem as the model needs to be designed from\nscratch when transferring to other tasks. Motivated by large language models\n(LLMs) having achieved remarkable success, we aim to utilize LLMs to fully\nunderstand and augment local text-attributed graphs, to address data\nheterogeneity at the data level. In this paper, we propose a general framework\nLLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks\ntheoretically. Specifically, for each client, it first utilizes the LLM to\ngenerate missing neighbors and then infers connections between generated nodes\nand raw nodes. To improve the quality of generated nodes, we design a novel\nfederated generation-and-reflection mechanism for LLMs, without the need to\nmodify the parameters of the LLM but relying solely on the collective feedback\nfrom all clients. After neighbor generation, all the clients utilize a\npre-trained edge predictor to infer the missing edges. Furthermore, our\nframework can seamlessly integrate as a plug-in with existing FGL methods.\nExperiments on three real-world datasets demonstrate the superiority of our\nmethod compared to advanced baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ongoing work",
    "pdf_url": "http://arxiv.org/pdf/2503.19455v1",
    "published_date": "2025-03-25 08:43:08 UTC",
    "updated_date": "2025-03-25 08:43:08 UTC"
  },
  {
    "arxiv_id": "2503.19449v1",
    "title": "VecTrans: LLM Transformation Framework for Better Auto-vectorization on High-performance CPU",
    "authors": [
      "Zhongchun Zheng",
      "Long Cheng",
      "Lu Li",
      "Rodrigo C. O. Rocha",
      "Tianyi Liu",
      "Wei Wei",
      "Xianwei Zhang",
      "Yaoqing Gao"
    ],
    "abstract": "Large language models (LLMs) have demonstrated great capabilities in code\ngeneration, yet their effective application in compiler optimizations remains\nan open challenge due to issues such as hallucinations and a lack of\ndomain-specific reasoning. Vectorization, a crucial optimization for enhancing\ncode performance, often fails because of the compiler's inability to recognize\ncomplex code patterns, which commonly require extensive empirical expertise.\nLLMs, with their ability to capture intricate patterns, thus providing a\npromising solution to this challenge. This paper presents VecTrans, a novel\nframework that leverages LLMs to enhance compiler-based code vectorization.\nVecTrans first employs compiler analysis to identify potentially vectorizable\ncode regions. It then utilizes an LLM to refactor these regions into patterns\nthat are more amenable to the compiler's auto-vectorization. To ensure semantic\ncorrectness, VecTrans further integrates a hybrid validation mechanism at the\nintermediate representation (IR) level. With the above efforts, VecTrans\ncombines the adaptability of LLMs with the precision of compiler vectorization,\nthereby effectively opening up the vectorization opportunities. Experimental\nresults show that among all 50 TSVC functions unvectorizable by Clang, GCC, and\nBiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves\nan average speedup of 2.02x, greatly surpassing state-of-the-art performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19449v1",
    "published_date": "2025-03-25 08:39:35 UTC",
    "updated_date": "2025-03-25 08:39:35 UTC"
  },
  {
    "arxiv_id": "2503.19426v1",
    "title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models",
    "authors": [
      "Suyoung Bae",
      "YunSeok Choi",
      "Jee-Hyong Lee"
    ],
    "abstract": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 main. 20 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19426v1",
    "published_date": "2025-03-25 08:16:35 UTC",
    "updated_date": "2025-03-25 08:16:35 UTC"
  },
  {
    "arxiv_id": "2503.19394v1",
    "title": "Quantifying Symptom Causality in Clinical Decision Making: An Exploration Using CausaLM",
    "authors": [
      "Mehul Shetty",
      "Connor Jordan"
    ],
    "abstract": "Current machine learning approaches to medical diagnosis often rely on\ncorrelational patterns between symptoms and diseases, risking misdiagnoses when\nsymptoms are ambiguous or common across multiple conditions. In this work, we\nmove beyond correlation to investigate the causal influence of key\nsymptoms-specifically \"chest pain\" on diagnostic predictions. Leveraging the\nCausaLM framework, we generate counterfactual text representations in which\ntarget concepts are effectively \"forgotten\" enabling a principled estimation of\nthe causal effect of that concept on a model's predicted disease distribution.\nBy employing Textual Representation-based Average Treatment Effect (TReATE), we\nquantify how the presence or absence of a symptom shapes the model's diagnostic\noutcomes, and contrast these findings against correlation-based baselines such\nas CONEXP. Our results offer deeper insight into the decision-making behavior\nof clinical NLP models and have the potential to inform more trustworthy,\ninterpretable, and causally-grounded decision support tools in medical\npractice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19394v1",
    "published_date": "2025-03-25 06:59:21 UTC",
    "updated_date": "2025-03-25 06:59:21 UTC"
  },
  {
    "arxiv_id": "2503.19382v1",
    "title": "Causal invariant geographic network representations with feature and structural distribution shifts",
    "authors": [
      "Yuhan Wang",
      "Silu He",
      "Qinyao Luo",
      "Hongyuan Yuan",
      "Ling Zhao",
      "Jiawei Zhu",
      "Haifeng Li"
    ],
    "abstract": "The existing methods learn geographic network representations through deep\ngraph neural networks (GNNs) based on the i.i.d. assumption. However, the\nspatial heterogeneity and temporal dynamics of geographic data make the\nout-of-distribution (OOD) generalisation problem particularly salient. The\nlatter are particularly sensitive to distribution shifts (feature and\nstructural shifts) between testing and training data and are the main causes of\nthe OOD generalisation problem. Spurious correlations are present between\ninvariant and background representations due to selection biases and\nenvironmental effects, resulting in the model extremes being more likely to\nlearn background representations. The existing approaches focus on background\nrepresentation changes that are determined by shifts in the feature\ndistributions of nodes in the training and test data while ignoring changes in\nthe proportional distributions of heterogeneous and homogeneous neighbour\nnodes, which we refer to as structural distribution shifts. We propose a\nfeature-structure mixed invariant representation learning (FSM-IRL) model that\naccounts for both feature distribution shifts and structural distribution\nshifts. To address structural distribution shifts, we introduce a sampling\nmethod based on causal attention, encouraging the model to identify nodes\npossessing strong causal relationships with labels or nodes that are more\nsimilar to the target node. Inspired by the Hilbert-Schmidt independence\ncriterion, we implement a reweighting strategy to maximise the orthogonality of\nthe node representations, thereby mitigating the spurious correlations among\nthe node representations and suppressing the learning of background\nrepresentations. Our experiments demonstrate that FSM-IRL exhibits strong\nlearning capabilities on both geographic and social network datasets in OOD\nscenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 3 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.19382v1",
    "published_date": "2025-03-25 06:21:57 UTC",
    "updated_date": "2025-03-25 06:21:57 UTC"
  },
  {
    "arxiv_id": "2503.19373v1",
    "title": "DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image",
    "authors": [
      "Hyeongjin Nam",
      "Donghwan Kim",
      "Jeongtaek Oh",
      "Kyoung Mu Lee"
    ],
    "abstract": "Most existing methods of 3D clothed human reconstruction from a single image\ntreat the clothed human as a single object without distinguishing between cloth\nand human body. In this regard, we present DeClotH, which separately\nreconstructs 3D cloth and human body from a single image. This task remains\nlargely unexplored due to the extreme occlusion between cloth and the human\nbody, making it challenging to infer accurate geometries and textures.\nMoreover, while recent 3D human reconstruction methods have achieved impressive\nresults using text-to-image diffusion models, directly applying such an\napproach to this problem often leads to incorrect guidance, particularly in\nreconstructing 3D cloth. To address these challenges, we propose two core\ndesigns in our framework. First, to alleviate the occlusion issue, we leverage\n3D template models of cloth and human body as regularizations, which provide\nstrong geometric priors to prevent erroneous reconstruction by the occlusion.\nSecond, we introduce a cloth diffusion model specifically designed to provide\ncontextual information about cloth appearance, thereby enhancing the\nreconstruction of 3D cloth. Qualitative and quantitative experiments\ndemonstrate that our proposed approach is highly effective in reconstructing\nboth 3D cloth and the human body. More qualitative results are provided at\nhttps://hygenie1228.github.io/DeClotH/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at CVPR 2025, 17 pages including the supplementary material",
    "pdf_url": "http://arxiv.org/pdf/2503.19373v1",
    "published_date": "2025-03-25 06:00:15 UTC",
    "updated_date": "2025-03-25 06:00:15 UTC"
  },
  {
    "arxiv_id": "2503.19371v1",
    "title": "Flow to Learn: Flow Matching on Neural Network Parameters",
    "authors": [
      "Daniel Saragih",
      "Deyu Cao",
      "Tejas Balaji",
      "Ashwin Santhosh"
    ],
    "abstract": "Foundational language models show a remarkable ability to learn new concepts\nduring inference via context data. However, similar work for images lag behind.\nTo address this challenge, we introduce FLoWN, a flow matching model that\nlearns to generate neural network parameters for different tasks. Our approach\nmodels the flow on latent space, while conditioning the process on context\ndata. Experiments verify that FLoWN attains various desiderata for a\nmeta-learning model. In addition, it matches or exceeds baselines on\nin-distribution tasks, provides better initializations for classifier training,\nand is performant on out-of-distribution few-shot tasks while having a\nfine-tuning mechanism to improve performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19371v1",
    "published_date": "2025-03-25 05:57:50 UTC",
    "updated_date": "2025-03-25 05:57:50 UTC"
  },
  {
    "arxiv_id": "2503.19339v1",
    "title": "Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture",
    "authors": [
      "Amna Naeem",
      "Muazzam A. Khan",
      "Nada Alasbali",
      "Jawad Ahmad",
      "Aizaz Ahmad Khattak",
      "Muhammad Shahbaz Khan"
    ],
    "abstract": "The ever-increasing security vulnerabilities in the Internet-of-Things (IoT)\nsystems require improved threat detection approaches. This paper presents a\ncompact and efficient approach to detect botnet attacks by employing an\nintegrated approach that consists of traffic pattern analysis, temporal support\nlearning, and focused feature extraction. The proposed attention-based model\nbenefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification\naccuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while\nmaintaining high precision and recall across various scenarios. The proposed\nmodel's performance is further validated by key parameters, such as Mathews\nCorrelation Coefficient and Cohen's kappa Correlation Coefficient. The\nclose-to-ideal results for these parameters demonstrate the proposed model's\nability to detect botnet attacks accurately and efficiently in practical\nsettings and on unseen data. The proposed model proved to be a powerful defense\nmechanism for IoT networks to face emerging security challenges.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19339v1",
    "published_date": "2025-03-25 04:12:14 UTC",
    "updated_date": "2025-03-25 04:12:14 UTC"
  },
  {
    "arxiv_id": "2503.19329v1",
    "title": "Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection",
    "authors": [
      "Yongting Hu",
      "Yuxin Lin",
      "Chengliang Liu",
      "Xiaoling Luo",
      "Xiaoyan Dou",
      "Qihao Xu",
      "Yong Xu"
    ],
    "abstract": "Multi-view diabetic retinopathy (DR) detection has recently emerged as a\npromising method to address the issue of incomplete lesions faced by\nsingle-view DR. However, it is still challenging due to the variable sizes and\nscattered locations of lesions. Furthermore, existing multi-view DR methods\ntypically merge multiple views without considering the correlations and\nredundancies of lesion information across them. Therefore, we propose a novel\nmethod to overcome the challenges of difficult lesion information learning and\ninadequate multi-view fusion. Specifically, we introduce a two-branch network\nto obtain both local lesion features and their global dependencies. The\nhigh-frequency component of the wavelet transform is used to exploit lesion\nedge information, which is then enhanced by global semantic to facilitate\ndifficult lesion learning. Additionally, we present a cross-view fusion module\nto improve multi-view fusion and reduce redundancy. Experimental results on\nlarge public datasets demonstrate the effectiveness of our method. The code is\nopen sourced on https://github.com/HuYongting/WGLIN.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19329v1",
    "published_date": "2025-03-25 03:44:57 UTC",
    "updated_date": "2025-03-25 03:44:57 UTC"
  },
  {
    "arxiv_id": "2503.19328v1",
    "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents",
    "authors": [
      "Vidya Srinivas",
      "Xuhai Xu",
      "Xin Liu",
      "Kumar Ayush",
      "Isaac Galatzer-Levy",
      "Shwetak Patel",
      "Daniel McDuff",
      "Tim Althoff"
    ],
    "abstract": "While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19328v1",
    "published_date": "2025-03-25 03:44:31 UTC",
    "updated_date": "2025-03-25 03:44:31 UTC"
  },
  {
    "arxiv_id": "2503.19326v1",
    "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps",
    "authors": [
      "Yu Cui",
      "Bryan Hooi",
      "Yujun Cai",
      "Yiwei Wang"
    ],
    "abstract": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19326v1",
    "published_date": "2025-03-25 03:43:11 UTC",
    "updated_date": "2025-03-25 03:43:11 UTC"
  },
  {
    "arxiv_id": "2503.19311v1",
    "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text",
    "authors": [
      "Weizhi Chen",
      "Jingbo Chen",
      "Yupeng Deng",
      "Jiansheng Chen",
      "Yuman Feng",
      "Zhihao Xi",
      "Diyou Liu",
      "Kai Li",
      "Yu Meng"
    ],
    "abstract": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19311v1",
    "published_date": "2025-03-25 03:17:42 UTC",
    "updated_date": "2025-03-25 03:17:42 UTC"
  },
  {
    "arxiv_id": "2503.19302v1",
    "title": "Observation Adaptation via Annealed Importance Resampling for Partially Observable Markov Decision Processes",
    "authors": [
      "Yunuo Zhang",
      "Baiting Luo",
      "Ayan Mukhopadhyay",
      "Abhishek Dubey"
    ],
    "abstract": "Partially observable Markov decision processes (POMDPs) are a general\nmathematical model for sequential decision-making in stochastic environments\nunder state uncertainty. POMDPs are often solved \\textit{online}, which enables\nthe algorithm to adapt to new information in real time. Online solvers\ntypically use bootstrap particle filters based on importance resampling for\nupdating the belief distribution. Since directly sampling from the ideal state\ndistribution given the latest observation and previous state is infeasible,\nparticle filters approximate the posterior belief distribution by propagating\nstates and adjusting weights through prediction and resampling steps. However,\nin practice, the importance resampling technique often leads to particle\ndegeneracy and sample impoverishment when the state transition model poorly\naligns with the posterior belief distribution, especially when the received\nobservation is highly informative. We propose an approach that constructs a\nsequence of bridge distributions between the state-transition and optimal\ndistributions through iterative Monte Carlo steps, better accommodating noisy\nobservations in online POMDP solvers. Our algorithm demonstrates significantly\nsuperior performance compared to state-of-the-art methods when evaluated across\nmultiple challenging POMDP domains.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as Oral Presentation to ICAPS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19302v1",
    "published_date": "2025-03-25 03:05:00 UTC",
    "updated_date": "2025-03-25 03:05:00 UTC"
  },
  {
    "arxiv_id": "2503.19292v1",
    "title": "Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for Parkinson's Disease Screening in OCT",
    "authors": [
      "Xiaoqing Zhang",
      "Hanfeng Shi",
      "Xiangyu Li",
      "Haili Ye",
      "Tao Xu",
      "Na Li",
      "Yan Hu",
      "Fan Lv",
      "Jiangfan Chen",
      "Jiang Liu"
    ],
    "abstract": "Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally.\nThe eye's retina is an extension of the brain and has great potential in PD\nscreening. Recent studies have suggested that texture features extracted from\nretinal layers can be adopted as biomarkers for PD diagnosis under optical\ncoherence tomography (OCT) images. Frequency domain learning techniques can\nenhance the feature representations of deep neural networks (DNNs) by\ndecomposing frequency components involving rich texture features. Additionally,\nprevious works have not exploited texture features for automated PD screening\nin OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet\nFilter (AWF) that serves as the Practical Texture Feature Amplifier to fully\nleverage the merits of texture features to boost the PD screening performance\nof DNNs with the aid of frequency domain learning. Specifically, AWF first\nenhances texture feature representation diversities via channel mixer, then\nemphasizes informative texture feature representations with the well-designed\nadaptive wavelet filtering token mixer. By combining the AWFs with the DNN\nstem, AWFNet is constructed for automated PD screening. Additionally, we\nintroduce a novel Balanced Confidence (BC) Loss by mining the potential of\nsample-wise predicted probabilities of all classes and class frequency prior,\nto further boost the PD screening performance and trustworthiness of AWFNet.\nThe extensive experiments manifest the superiority of our AWFNet and BC over\nstate-of-the-art methods in terms of PD screening performance and\ntrustworthiness.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19292v1",
    "published_date": "2025-03-25 02:47:24 UTC",
    "updated_date": "2025-03-25 02:47:24 UTC"
  },
  {
    "arxiv_id": "2503.19285v1",
    "title": "No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism",
    "authors": [
      "Yubo Li",
      "Xinyu Yao",
      "Rema Padman"
    ],
    "abstract": "Despite the outstanding performance of deep learning models in clinical\nprediction tasks, explainability remains a significant challenge. Inspired by\ntransformer architectures, we introduce the Temporal-Feature Cross Attention\nMechanism (TFCAM), a novel deep learning framework designed to capture dynamic\ninteractions among clinical features across time, enhancing both predictive\naccuracy and interpretability. In an experiment with 1,422 patients with\nChronic Kidney Disease, predicting progression to End-Stage Renal Disease,\nTFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an\nF1-score of 0.69. Beyond performance gains, TFCAM provides multi-level\nexplainability by identifying critical temporal periods, ranking feature\nimportance, and quantifying how features influence each other across time\nbefore affecting predictions. Our approach addresses the \"black box\"\nlimitations of deep learning in healthcare, offering clinicians transparent\ninsights into disease progression mechanisms while maintaining state-of-the-art\npredictive performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 figures, submitted to AMIA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19285v1",
    "published_date": "2025-03-25 02:35:08 UTC",
    "updated_date": "2025-03-25 02:35:08 UTC"
  },
  {
    "arxiv_id": "2503.19281v1",
    "title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
    "authors": [
      "Feiyang Wang",
      "Xiaomin Yu",
      "Wangyu Wu"
    ],
    "abstract": "Proving Rubik's Cube theorems at the high level represents a notable\nmilestone in human-level spatial imagination and logic thinking and reasoning.\nTraditional Rubik's Cube robots, relying on complex vision systems and fixed\nalgorithms, often struggle to adapt to complex and dynamic scenarios. To\novercome this limitation, we introduce CubeRobot, a novel vision-language model\n(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with\nmultimodal understanding and execution capabilities. We used the CubeCoT image\ndataset, which contains multiple-level tasks (43 subtasks in total) that humans\nare unable to handle, encompassing various cube states. We incorporate a\ndual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting\ntask-related features from VLM-generated planning queries, thus enabling\nCubeRobot to independent planning, decision-making, reflection and separate\nmanagement of high- and low-level Rubik's Cube tasks. Furthermore, in low-level\nRubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of\n100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of\n80% in high-level tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19281v1",
    "published_date": "2025-03-25 02:23:47 UTC",
    "updated_date": "2025-03-25 02:23:47 UTC"
  },
  {
    "arxiv_id": "2503.19280v1",
    "title": "LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs",
    "authors": [
      "Amogh Inamdar",
      "Uzay Macar",
      "Michel Vazirani",
      "Michael Tarnow",
      "Zarina Mustapha",
      "Natalia Dittren",
      "Sam Sadeh",
      "Nakul Verma",
      "Ansaf Salleb-Aouissi"
    ],
    "abstract": "The study of propositional logic -- fundamental to the theory of computing --\nis a cornerstone of the undergraduate computer science curriculum. Learning to\nsolve logical proofs requires repeated guided practice, but undergraduate\nstudents often lack access to on-demand tutoring in a judgment-free\nenvironment. In this work, we highlight the need for guided practice tools in\nundergraduate mathematics education and outline the desiderata of an effective\npractice tool. We accordingly develop LogicLearner, a web application for\nguided logic proof practice. LogicLearner consists of an interface to attempt\nlogic proofs step-by-step and an automated proof solver to generate solutions\non the fly, allowing users to request guidance as needed. We pilot LogicLearner\nas a practice tool in two semesters of an undergraduate discrete mathematics\ncourse and receive strongly positive feedback for usability and pedagogical\nvalue in student surveys. To the best of our knowledge, LogicLearner is the\nonly learning tool that provides an end-to-end practice environment for logic\nproofs with immediate, judgment-free feedback.",
    "categories": [
      "cs.DM",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.DM",
    "comment": "32 pages, 27 figures, open-source codebase linked in paper",
    "pdf_url": "http://arxiv.org/pdf/2503.19280v1",
    "published_date": "2025-03-25 02:23:08 UTC",
    "updated_date": "2025-03-25 02:23:08 UTC"
  },
  {
    "arxiv_id": "2503.19276v1",
    "title": "Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications",
    "authors": [
      "Ben Rahman"
    ],
    "abstract": "Semantic segmentation has made significant strides in pixel-level image\nunderstanding, yet it remains limited in capturing contextual and semantic\nrelationships between objects. Current models, such as CNN and\nTransformer-based architectures, excel at identifying pixel-level features but\nfail to distinguish semantically similar objects (e.g., \"doctor\" vs. \"nurse\" in\na hospital scene) or understand complex contextual scenarios (e.g.,\ndifferentiating a running child from a regular pedestrian in autonomous\ndriving). To address these limitations, we proposed a novel Context-Aware\nSemantic Segmentation framework that integrates Large Language Models (LLMs)\nwith state-of-the-art vision backbones. Our hybrid model leverages the Swin\nTransformer for robust visual feature extraction and GPT-4 for enriching\nsemantic understanding through text embeddings. A Cross-Attention Mechanism is\nintroduced to align vision and language features, enabling the model to reason\nabout context more effectively. Additionally, Graph Neural Networks (GNNs) are\nemployed to model object relationships within the scene, capturing dependencies\nthat are overlooked by traditional models. Experimental results on benchmark\ndatasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the\nexisting methods in both pixel-level accuracy (mIoU) and contextual\nunderstanding (mAP). This work bridges the gap between vision and language,\npaving the path for more intelligent and context-aware vision systems in\napplications including autonomous driving, medical imaging, and robotics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19276v1",
    "published_date": "2025-03-25 02:12:35 UTC",
    "updated_date": "2025-03-25 02:12:35 UTC"
  },
  {
    "arxiv_id": "2503.19267v1",
    "title": "NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios",
    "authors": [
      "Songyi Gao",
      "Zuolin Tu",
      "Rong-Jun Qin",
      "Yi-Hao Sun",
      "Xiong-Hui Chen",
      "Yang Yu"
    ],
    "abstract": "Offline reinforcement learning (RL) aims to learn from historical data\nwithout requiring (costly) access to the environment. To facilitate offline RL\nresearch, we previously introduced NeoRL, which highlighted that datasets from\nreal-world tasks are often conservative and limited. With years of experience\napplying offline RL to various domains, we have identified additional\nreal-world challenges. These include extremely conservative data distributions\nproduced by deployed control systems, delayed action effects caused by\nhigh-latency transitions, external factors arising from the uncontrollable\nvariance of transitions, and global safety constraints that are difficult to\nevaluate during the decision-making process. These challenges are\nunderrepresented in previous benchmarks but frequently occur in real-world\ntasks. To address this, we constructed the extended Near Real-World Offline RL\nBenchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along\nwith their corresponding evaluation simulators. Benchmarking results from\nstate-of-the-art offline RL approaches demonstrate that current methods often\nstruggle to outperform the data-collection behavior policy, highlighting the\nneed for more effective methods. We hope NeoRL-2 will accelerate the\ndevelopment of reinforcement learning algorithms for real-world applications.\nThe benchmark project page is available at https://github.com/polixir/NeoRL2.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19267v1",
    "published_date": "2025-03-25 02:01:54 UTC",
    "updated_date": "2025-03-25 02:01:54 UTC"
  },
  {
    "arxiv_id": "2503.19260v1",
    "title": "Linguistic Blind Spots of Large Language Models",
    "authors": [
      "Jiali Cheng",
      "Hadi Amiri"
    ],
    "abstract": "Large language models (LLMs) are the foundation of many AI applications\ntoday. However, despite their remarkable proficiency in generating coherent\ntext, questions linger regarding their ability to perform fine-grained\nlinguistic annotation tasks, such as detecting nouns or verbs, or identifying\nmore complex syntactic structures like clauses in input texts. These tasks\nrequire precise syntactic and semantic understanding of input text, and when\nLLMs underperform on specific linguistic structures, it raises concerns about\ntheir reliability for detailed linguistic analysis and whether their (even\ncorrect) outputs truly reflect an understanding of the inputs. In this paper,\nwe empirically study the performance of recent LLMs on fine-grained linguistic\nannotation tasks. Through a series of experiments, we find that recent LLMs\nshow limited efficacy in addressing linguistic queries and often struggle with\nlinguistically complex inputs. We show that the most capable LLM (Llama3-70b)\nmakes notable errors in detecting linguistic structures, such as misidentifying\nembedded clauses, failing to recognize verb phrases, and confusing complex\nnominals with clauses. Our results provide insights to inform future\nadvancements in LLM design and development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Cognitive Modeling and Computational Linguistics Workshop",
    "pdf_url": "http://arxiv.org/pdf/2503.19260v1",
    "published_date": "2025-03-25 01:47:13 UTC",
    "updated_date": "2025-03-25 01:47:13 UTC"
  },
  {
    "arxiv_id": "2503.19223v1",
    "title": "Face Spoofing Detection using Deep Learning",
    "authors": [
      "Najeebullah",
      "Maaz Salman",
      "Zar Nawab Khan Swati"
    ],
    "abstract": "Digital image spoofing has emerged as a significant security threat in\nbiometric authentication systems, particularly those relying on facial\nrecognition. This study evaluates the performance of three vision based models,\nMobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in\nimage classification, utilizing a dataset of 150,986 images divided into\ntraining , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof\ndetection is critical for enhancing the security of image recognition systems,\nand this research compares the models effectiveness through accuracy,\nprecision, recall, and F1 score metrics. Results reveal that MobileNetV2\noutperforms other architectures on the test dataset, achieving an accuracy of\n91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared\nto ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation\ndataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17%\naccuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during\ntraining and superior generalization to unseen data, despite both models\nshowing signs of overfitting. These findings highlight MobileNetV2 balanced\nperformance and robustness, making it the preferred choice for spoof detection\napplications where reliability on new data is essential. The study underscores\nthe importance of model selection in security sensitive contexts and suggests\nMobileNetV2 as a practical solution for real world deployment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 9 figures,3 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.19223v1",
    "published_date": "2025-03-25 00:09:21 UTC",
    "updated_date": "2025-03-25 00:09:21 UTC"
  }
]
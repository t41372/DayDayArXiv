[
  {
    "arxiv_id": "2503.20118v1",
    "title": "Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors",
    "authors": [
      "Yuke Lou",
      "Yiming Wang",
      "Zhen Wu",
      "Rui Zhao",
      "Wenjia Wang",
      "Mingyi Shi",
      "Taku Komura"
    ],
    "abstract": "Human-object interaction (HOI) synthesis is important for various\napplications, ranging from virtual reality to robotics. However, acquiring 3D\nHOI data is challenging due to its complexity and high cost, limiting existing\nmethods to the narrow diversity of object types and interaction patterns in\ntraining datasets. This paper proposes a novel zero-shot HOI synthesis\nframework without relying on end-to-end training on currently limited 3D HOI\ndatasets. The core idea of our method lies in leveraging extensive HOI\nknowledge from pre-trained Multimodal Models. Given a text description, our\nsystem first obtains temporally consistent 2D HOI image sequences using image\nor video generation models, which are then uplifted to 3D HOI milestones of\nhuman and object poses. We employ pre-trained human pose estimation models to\nextract human poses and introduce a generalizable category-level 6-DoF\nestimation method to obtain the object poses from 2D HOI images. Our estimation\nmethod is adaptive to various object templates obtained from text-to-3D models\nor online retrieval. A physics-based tracking of the 3D HOI kinematic milestone\nis further applied to refine both body motions and object poses, yielding more\nphysically plausible HOI generation results. The experimental results\ndemonstrate that our method is capable of generating open-vocabulary HOIs with\nphysical realism and semantic diversity.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20118v1",
    "published_date": "2025-03-25 23:55:47 UTC",
    "updated_date": "2025-03-25 23:55:47 UTC"
  },
  {
    "arxiv_id": "2503.20110v1",
    "title": "Efficient Model Development through Fine-tuning Transfer",
    "authors": [
      "Pin-Jie Lin",
      "Rishab Balasubramanian",
      "Fengyuan Liu",
      "Nikhil Kandpal",
      "Tu Vu"
    ],
    "abstract": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 4 figures, 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.20110v1",
    "published_date": "2025-03-25 23:24:43 UTC",
    "updated_date": "2025-03-25 23:24:43 UTC"
  },
  {
    "arxiv_id": "2503.20105v1",
    "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations",
    "authors": [
      "Ran Tian",
      "Kratarth Goel"
    ],
    "abstract": "Recent advancements in LLMs have revolutionized motion generation models in\nembodied applications. While LLM-type auto-regressive motion generation models\nbenefit from training scalability, there remains a discrepancy between their\ntoken prediction objectives and human preferences. As a result, models\npre-trained solely with token-prediction objectives often generate behaviors\nthat deviate from what humans would prefer, making post-training preference\nalignment crucial for producing human-preferred motions. Unfortunately,\npost-training alignment requires extensive preference rankings of motions\ngenerated by the pre-trained model, which are costly to annotate, especially in\nmulti-agent settings. Recently, there has been growing interest in leveraging\npre-training demonstrations to scalably generate preference data for\npost-training alignment. However, these methods often adopt an adversarial\nassumption, treating all pre-trained model-generated samples as unpreferred\nexamples. This adversarial approach overlooks the valuable signal provided by\npreference rankings among the model's own generations, ultimately reducing\nalignment effectiveness and potentially leading to misaligned behaviors. In\nthis work, instead of treating all generated samples as equally bad, we\nleverage implicit preferences encoded in pre-training demonstrations to\nconstruct preference rankings among the pre-trained model's generations,\noffering more nuanced preference alignment guidance with zero human cost. We\napply our approach to large-scale traffic simulation and demonstrate its\neffectiveness in improving the realism of pre-trained model's generated\nbehaviors, making a lightweight 1M motion generation model comparable to SOTA\nlarge imitation-based models by relying solely on implicit feedback from\npre-training demonstrations, without additional post-training human preference\nannotations or high computational costs.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2503.20105v1",
    "published_date": "2025-03-25 23:02:13 UTC",
    "updated_date": "2025-03-25 23:02:13 UTC"
  },
  {
    "arxiv_id": "2503.20099v1",
    "title": "AI Identity, Empowerment, and Mindfulness in Mitigating Unethical AI Use",
    "authors": [
      "Mayssam Tarighi Shaayesteh",
      "Sara Memarian Esfahani",
      "Hossein Mohit"
    ],
    "abstract": "This study examines how AI identity influences psychological empowerment and\nunethical AI behavior among college students, while also exploring the\nmoderating role of IT mindfulness. Findings show that a strong AI identity\nenhances psychological empowerment and academic engagement but can also lead to\nincreased unethical AI practices. Crucially, IT mindfulness acts as an ethical\nsafeguard, promoting sensitivity to ethical concerns and reducing misuse of AI.\nThese insights have implications for educators, policymakers, and AI\ndevelopers, emphasizing For Peer Review the need for a balanced approach that\nencourages digital engagement without compromising student responsibility. The\nstudy also contributes to philosophical discussions of psychological agency,\nsuggesting that empowerment through AI can yield both positive and negative\noutcomes. Mindfulness emerges as essential in guiding ethical AI interactions.\nOverall, the research informs ongoing debates on ethics in education and AI,\noffering strategies to align technological advancement with ethical\naccountability and responsible use.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20099v1",
    "published_date": "2025-03-25 22:36:21 UTC",
    "updated_date": "2025-03-25 22:36:21 UTC"
  },
  {
    "arxiv_id": "2503.20084v2",
    "title": "Can Multi-modal (reasoning) LLMs work as deepfake detectors?",
    "authors": [
      "Simiao Ren",
      "Yao Yao",
      "Kidus Zewde",
      "Zisheng Liang",
      "Tsang",
      "Ng",
      "Ning-Yau Cheng",
      "Xiaoou Zhan",
      "Qinzhe Liu",
      "Yifei Chen",
      "Hengwei Xu"
    ],
    "abstract": "Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20084v2",
    "published_date": "2025-03-25 21:47:29 UTC",
    "updated_date": "2025-03-29 19:19:14 UTC"
  },
  {
    "arxiv_id": "2503.20078v1",
    "title": "Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning",
    "authors": [
      "Volkan Ustun",
      "Soham Hans",
      "Rajay Kumar",
      "Yunzhe Wang"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in\ntraining dynamic and adaptive synthetic characters for interactive simulations\non geo-specific terrains. Frameworks such as Unity's ML-Agents help to make\nsuch reinforcement learning experiments more accessible to the simulation\ncommunity. Military training simulations also benefit from advances in MARL,\nbut they have immense computational requirements due to their complex,\ncontinuous, stochastic, partially observable, non-stationary, and\ndoctrine-based nature. Furthermore, these simulations require geo-specific\nterrains, further exacerbating the computational resources problem. In our\nresearch, we leverage Unity's waypoints to automatically generate multi-layered\nrepresentation abstractions of the geo-specific terrains to scale up\nreinforcement learning while still allowing the transfer of learned policies\nbetween different representations. Our early exploratory results on a novel\nMARL scenario, where each side has differing objectives, indicate that\nwaypoint-based navigation enables faster and more efficient learning while\nproducing trajectories similar to those taken by expert human players in CSGO\ngaming environments. This research points out the potential of waypoint-based\nnavigation for reducing the computational costs of developing and training MARL\nmodels for military training simulations, where geo-specific terrains and\ndiffering objectives are crucial.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures, 2024 Interservice/Industry Training, Simulation,\n  and Education Conference (I/ITSEC)",
    "pdf_url": "http://arxiv.org/pdf/2503.20078v1",
    "published_date": "2025-03-25 21:29:49 UTC",
    "updated_date": "2025-03-25 21:29:49 UTC"
  },
  {
    "arxiv_id": "2503.20074v2",
    "title": "Adaptive Orchestration for Large-Scale Inference on Heterogeneous Accelerator Systems Balancing Cost, Performance, and Resilience",
    "authors": [
      "Yahav Biran",
      "Imry Kissos"
    ],
    "abstract": "The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity.",
    "categories": [
      "cs.PF",
      "cs.AI",
      "68U01"
    ],
    "primary_category": "cs.PF",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20074v2",
    "published_date": "2025-03-25 21:20:11 UTC",
    "updated_date": "2025-03-27 17:16:44 UTC"
  },
  {
    "arxiv_id": "2503.22720v1",
    "title": "Why Representation Engineering Works: A Theoretical and Empirical Study in Vision-Language Models",
    "authors": [
      "Bowei Tian",
      "Xuntao Lyu",
      "Meng Liu",
      "Hongyi Wang",
      "Ang Li"
    ],
    "abstract": "Representation Engineering (RepE) has emerged as a powerful paradigm for\nenhancing AI transparency by focusing on high-level representations rather than\nindividual neurons or circuits. It has proven effective in improving\ninterpretability and control, showing that representations can emerge,\npropagate, and shape final model outputs in large language models (LLMs).\nHowever, in Vision-Language Models (VLMs), visual input can override factual\nlinguistic knowledge, leading to hallucinated responses that contradict\nreality. To address this challenge, we make the first attempt to extend RepE to\nVLMs, analyzing how multimodal representations are preserved and transformed.\nBuilding on our findings and drawing inspiration from successful RepE\napplications, we develop a theoretical framework that explains the stability of\nneural activity across layers using the principal eigenvector, uncovering the\nunderlying mechanism of RepE. We empirically validate these instrinsic\nproperties, demonstrating their broad applicability and significance. By\nbridging theoretical insights with empirical validation, this work transforms\nRepE from a descriptive tool into a structured theoretical framework, opening\nnew directions for improving AI robustness, fairness, and transparency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22720v1",
    "published_date": "2025-03-25 20:32:15 UTC",
    "updated_date": "2025-03-25 20:32:15 UTC"
  },
  {
    "arxiv_id": "2503.22719v1",
    "title": "LLM-based Agent Simulation for Maternal Health Interventions: Uncertainty Estimation and Decision-focused Evaluation",
    "authors": [
      "Sarah Martinson",
      "Lingkai Kong",
      "Cheol Woo Kim",
      "Aparna Taneja",
      "Milind Tambe"
    ],
    "abstract": "Agent-based simulation is crucial for modeling complex human behavior, yet\ntraditional approaches require extensive domain knowledge and large datasets.\nIn data-scarce healthcare settings where historic and counterfactual data are\nlimited, large language models (LLMs) offer a promising alternative by\nleveraging broad world knowledge. This study examines an LLM-driven simulation\nof a maternal mobile health program, predicting beneficiaries' listening\nbehavior when they receive health information via automated messages (control)\nor live representatives (intervention). Since uncertainty quantification is\ncritical for decision-making in health interventions, we propose an LLM\nepistemic uncertainty estimation method based on binary entropy across multiple\nsamples. We enhance model robustness through ensemble approaches, improving F1\nscore and model calibration compared to individual models. Beyond direct\nevaluation, we take a decision-focused approach, demonstrating how LLM\npredictions inform intervention feasibility and trial implementation in\ndata-limited settings. The proposed method extends to public health, disaster\nresponse, and other domains requiring rapid intervention assessment under\nsevere data constraints. All code and prompts used for this work can be found\nat https://github.com/sarahmart/LLM-ABS-ARMMAN-prediction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22719v1",
    "published_date": "2025-03-25 20:24:47 UTC",
    "updated_date": "2025-03-25 20:24:47 UTC"
  },
  {
    "arxiv_id": "2503.20036v1",
    "title": "BugCraft: End-to-End Crash Bug Reproduction Using LLM Agents in Minecraft",
    "authors": [
      "Eray Yapağcı",
      "Yavuz Alp Sencer Öztürk",
      "Eray Tüzün"
    ],
    "abstract": "Reproducing game bugs, in our case crash bugs in continuously evolving games\nlike Minecraft, is a notoriously manual, time-consuming, and challenging\nprocess to automate. Despite the success of LLM-driven bug reproduction in\nother software domains, games, with their complex interactive environments,\nremain largely unaddressed. This paper introduces BugCraft, a novel end-to-end\nframework designed to automate the reproduction of crash bugs in Minecraft\ndirectly from user-submitted bug reports, addressing the critical gap in\nautomated game bug reproduction. BugCraft employs a two-stage approach: first,\na Step Synthesizer leverages LLMs and Minecraft Wiki knowledge to transform bug\nreports into high-quality, structured steps to reproduce (S2R). Second, an\nAction Model, powered by a vision-based LLM agent (GPT-4o) and a custom macro\nAPI, executes these S2R steps within Minecraft to trigger the reported crash.\nTo facilitate evaluation, we introduce BugCraft-Bench, a curated dataset of\nMinecraft crash bug reports. Evaluated on BugCraft-Bench, our framework\nsuccessfully reproduced 30.23% of crash bugs end-to-end. The Step Synthesizer\ndemonstrated a 66.28% accuracy in generating correct bug reproduction plans,\nhighlighting its effectiveness in interpreting and structuring bug report\ninformation. BugCraft demonstrates the feasibility of automated reproduction of\ncrash bugs in complex game environments using LLMs, opening promising avenues\nfor game testing and development. The framework and the BugCraft-Bench dataset\npave the way for future research in automated game bug analysis and hold\npotential for generalization to other interactive game platforms. Finally, we\nmake our code open at https://bugcraft2025.github.io/",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20036v1",
    "published_date": "2025-03-25 19:34:24 UTC",
    "updated_date": "2025-03-25 19:34:24 UTC"
  },
  {
    "arxiv_id": "2503.20028v1",
    "title": "OmniNova:A General Multimodal Agent Framework",
    "authors": [
      "Pengfei Du"
    ],
    "abstract": "The integration of Large Language Models (LLMs) with specialized tools\npresents new opportunities for intelligent automation systems. However,\norchestrating multiple LLM-driven agents to tackle complex tasks remains\nchallenging due to coordination difficulties, inefficient resource utilization,\nand inconsistent information flow. We present OmniNova, a modular multi-agent\nautomation framework that combines language models with specialized tools such\nas web search, crawling, and code execution capabilities. OmniNova introduces\nthree key innovations: (1) a hierarchical multi-agent architecture with\ndistinct coordinator, planner, supervisor, and specialist agents; (2) a dynamic\ntask routing mechanism that optimizes agent deployment based on task\ncomplexity; and (3) a multi-layered LLM integration system that allocates\nappropriate models to different cognitive requirements. Our evaluations across\n50 complex tasks in research, data analysis, and web interaction domains\ndemonstrate that OmniNova outperforms existing frameworks in task completion\nrate (87\\% vs. baseline 62\\%), efficiency (41\\% reduced token usage), and\nresult quality (human evaluation score of 4.2/5 vs. baseline 3.1/5). We\ncontribute both a theoretical framework for multi-agent system design and an\nopen-source implementation that advances the state-of-the-art in LLM-based\nautomation systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20028v1",
    "published_date": "2025-03-25 19:21:01 UTC",
    "updated_date": "2025-03-25 19:21:01 UTC"
  },
  {
    "arxiv_id": "2503.20018v1",
    "title": "Experience Replay Addresses Loss of Plasticity in Continual Learning",
    "authors": [
      "Jiuqi Wang",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "abstract": "Loss of plasticity is one of the main challenges in continual learning with\ndeep neural networks, where neural networks trained via backpropagation\ngradually lose their ability to adapt to new tasks and perform significantly\nworse than their freshly initialized counterparts. The main contribution of\nthis paper is to propose a new hypothesis that experience replay addresses the\nloss of plasticity in continual learning. Here, experience replay is a form of\nmemory. We provide supporting evidence for this hypothesis. In particular, we\ndemonstrate in multiple different tasks, including regression, classification,\nand policy evaluation, that by simply adding an experience replay and\nprocessing the data in the experience replay with Transformers, the loss of\nplasticity disappears. Notably, we do not alter any standard components of deep\nlearning. For example, we do not change backpropagation. We do not modify the\nactivation functions. And we do not use any regularization. We conjecture that\nexperience replay and Transformers can address the loss of plasticity because\nof the in-context learning phenomenon.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20018v1",
    "published_date": "2025-03-25 19:01:10 UTC",
    "updated_date": "2025-03-25 19:01:10 UTC"
  },
  {
    "arxiv_id": "2504.07117v1",
    "title": "RP-SAM2: Refining Point Prompts for Stable Surgical Instrument Segmentation",
    "authors": [
      "Nuren Zhaksylyk",
      "Ibrahim Almakky",
      "Jay Paranjape",
      "S. Swaroop Vedula",
      "Shameema Sikder",
      "Vishal M. Patel",
      "Mohammad Yaqub"
    ],
    "abstract": "Accurate surgical instrument segmentation is essential in cataract surgery\nfor tasks such as skill assessment and workflow optimization. However, limited\nannotated data makes it difficult to develop fully automatic models.\nPrompt-based methods like SAM2 offer flexibility yet remain highly sensitive to\nthe point prompt placement, often leading to inconsistent segmentations. We\naddress this issue by introducing RP-SAM2, which incorporates a novel shift\nblock and a compound loss function to stabilize point prompts. Our approach\nreduces annotator reliance on precise point positioning while maintaining\nrobust segmentation capabilities. Experiments on the Cataract1k dataset\ndemonstrate that RP-SAM2 improves segmentation accuracy, with a 2% mDSC gain, a\n21.36% reduction in mHD95, and decreased variance across random single-point\nprompt results compared to SAM2. Additionally, on the CaDIS dataset, pseudo\nmasks generated by RP-SAM2 for fine-tuning SAM2's mask decoder outperformed\nthose generated by SAM2. These results highlight RP-SAM2 as a practical, stable\nand reliable solution for semi-automatic instrument segmentation in\ndata-constrained medical settings. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/RP-SAM2.",
    "categories": [
      "q-bio.TO",
      "cs.AI"
    ],
    "primary_category": "q-bio.TO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07117v1",
    "published_date": "2025-03-25 18:59:23 UTC",
    "updated_date": "2025-03-25 18:59:23 UTC"
  },
  {
    "arxiv_id": "2503.20001v1",
    "title": "Unsupervised Learning for Quadratic Assignment",
    "authors": [
      "Yimeng Min",
      "Carla P. Gomes"
    ],
    "abstract": "We introduce PLUME search, a data-driven framework that enhances search\nefficiency in combinatorial optimization through unsupervised learning. Unlike\nsupervised or reinforcement learning, PLUME search learns directly from problem\ninstances using a permutation-based loss with a non-autoregressive approach. We\nevaluate its performance on the quadratic assignment problem, a fundamental\nNP-hard problem that encompasses various combinatorial optimization problems.\nExperimental results demonstrate that PLUME search consistently improves\nsolution quality. Furthermore, we study the generalization behavior and show\nthat the learned model generalizes across different densities and sizes.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.20001v1",
    "published_date": "2025-03-25 18:37:46 UTC",
    "updated_date": "2025-03-25 18:37:46 UTC"
  },
  {
    "arxiv_id": "2503.19990v1",
    "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
    "authors": [
      "Kexian Tang",
      "Junyao Gao",
      "Yanhong Zeng",
      "Haodong Duan",
      "Yanan Sun",
      "Zhening Xing",
      "Wenran Liu",
      "Kaifeng Lyu",
      "Kai Chen"
    ],
    "abstract": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce \\textbf{LEGO-Puzzles}, a scalable benchmark designed\nto evaluate both \\textbf{spatial understanding} and \\textbf{sequential\nreasoning} in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19990v1",
    "published_date": "2025-03-25 18:21:07 UTC",
    "updated_date": "2025-03-25 18:21:07 UTC"
  },
  {
    "arxiv_id": "2503.19988v1",
    "title": "ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback",
    "authors": [
      "Bohan Zhai",
      "Canwen Xu",
      "Yuxiong He",
      "Zhewei Yao"
    ],
    "abstract": "Text-to-SQL demands precise reasoning to convert natural language questions\ninto structured queries. While large language models (LLMs) excel in many\nreasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for\ntext-to-SQL remains underexplored. We identify critical limitations: zero-shot\nCoT offers minimal gains, and Direct Preference Optimization (DPO) applied\nwithout CoT yields marginal improvements. We propose ExCoT, a novel framework\nthat iteratively optimizes open-source LLMs by combining CoT reasoning with\noff-policy and on-policy DPO, relying solely on execution accuracy as feedback.\nThis approach eliminates the need for reward models or human-annotated\npreferences.\n  Our experimental results demonstrate significant performance gains: ExCoT\nimproves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider\ntest set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder\ndemonstrating similar improvements. Our best model achieves state-of-the-art\nperformance in the single-model setting on both BIRD and Spider datasets,\nnotably achieving 68.53% on the BIRD test set.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19988v1",
    "published_date": "2025-03-25 18:17:36 UTC",
    "updated_date": "2025-03-25 18:17:36 UTC"
  },
  {
    "arxiv_id": "2503.19900v1",
    "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
    "authors": [
      "Hao Yu",
      "Zhuokai Zhao",
      "Shen Yan",
      "Lukasz Korycki",
      "Jianyu Wang",
      "Baosheng He",
      "Jiayi Liu",
      "Lizhu Zhang",
      "Xiangjun Fan",
      "Hanchao Yu"
    ],
    "abstract": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19900v1",
    "published_date": "2025-03-25 17:57:17 UTC",
    "updated_date": "2025-03-25 17:57:17 UTC"
  },
  {
    "arxiv_id": "2503.19887v5",
    "title": "AI threats to national security can be countered through an incident regime",
    "authors": [
      "Alejandro Ortega"
    ],
    "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe sectors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19887v5",
    "published_date": "2025-03-25 17:51:50 UTC",
    "updated_date": "2025-04-16 09:24:21 UTC"
  },
  {
    "arxiv_id": "2503.19885v1",
    "title": "Dynamics of Structured Complex-Valued Hopfield Neural Networks",
    "authors": [
      "Rama Murthy Garimella",
      "Marcos Eduardo Valle",
      "Guilherme Vieira",
      "Anil Rayala",
      "Dileep Munugoti"
    ],
    "abstract": "In this paper, we explore the dynamics of structured complex-valued Hopfield\nneural networks (CvHNNs), which arise when the synaptic weight matrix possesses\nspecific structural properties. We begin by analyzing CvHNNs with a Hermitian\nsynaptic weight matrix and establish the existence of four-cycle dynamics in\nCvHNNs with skew-Hermitian weight matrices operating synchronously.\nFurthermore, we introduce two new classes of complex-valued matrices: braided\nHermitian and braided skew-Hermitian matrices. We demonstrate that CvHNNs\nutilizing these matrix types exhibit cycles of length eight when operating in\nfull parallel update mode. Finally, we conduct extensive computational\nexperiments on synchronous CvHNNs, exploring other synaptic weight matrix\nstructures. The findings provide a comprehensive overview of the dynamics of\nstructured CvHNNs, offering insights that may contribute to developing improved\nassociative memory models when integrated with suitable learning rules.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19885v1",
    "published_date": "2025-03-25 17:49:36 UTC",
    "updated_date": "2025-03-25 17:49:36 UTC"
  },
  {
    "arxiv_id": "2503.19867v1",
    "title": "Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge Representation and Quantum Entanglement",
    "authors": [
      "Ming Lei",
      "Christophe Baehr"
    ],
    "abstract": "This paper establishes a unified framework integrating geometric flows with\ndeep learning through three fundamental innovations. First, we propose a\nthermodynamically coupled Ricci flow that dynamically adapts parameter space\ngeometry to loss landscape topology, formally proved to preserve isometric\nknowledge embedding (Theorem~\\ref{thm:isometric}). Second, we derive explicit\nphase transition thresholds and critical learning rates\n(Theorem~\\ref{thm:critical}) through curvature blowup analysis, enabling\nautomated singularity resolution via geometric surgery\n(Lemma~\\ref{lem:surgery}). Third, we establish an AdS/CFT-type holographic\nduality (Theorem~\\ref{thm:ads}) between neural networks and conformal field\ntheories, providing entanglement entropy bounds for regularization design.\nExperiments demonstrate 2.1$\\times$ convergence acceleration and 63\\%\ntopological simplification while maintaining $\\mathcal{O}(N\\log N)$ complexity,\noutperforming Riemannian baselines by 15.2\\% in few-shot accuracy.\nTheoretically, we prove exponential stability (Theorem~\\ref{thm:converge})\nthrough a new Lyapunov function combining Perelman entropy with Wasserstein\ngradient flows, fundamentally advancing geometric deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "math.GT",
      "quant-ph",
      "68T05, 68T07, 68T27, 81V99, 37F40,",
      "I.2; K.3.2; F.4.1"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, submitted to IEEE PAMI",
    "pdf_url": "http://arxiv.org/pdf/2503.19867v1",
    "published_date": "2025-03-25 17:32:31 UTC",
    "updated_date": "2025-03-25 17:32:31 UTC"
  },
  {
    "arxiv_id": "2503.19868v1",
    "title": "GENIUS: A Generative Framework for Universal Multimodal Search",
    "authors": [
      "Sungyeon Kim",
      "Xinliang Zhu",
      "Xiaofan Lin",
      "Muhammet Bastan",
      "Douglas Gray",
      "Suha Kwak"
    ],
    "abstract": "Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19868v1",
    "published_date": "2025-03-25 17:32:31 UTC",
    "updated_date": "2025-03-25 17:32:31 UTC"
  },
  {
    "arxiv_id": "2503.19848v1",
    "title": "Guarding against artificial intelligence--hallucinated citations: the case for full-text reference deposit",
    "authors": [
      "Alex Glynn"
    ],
    "abstract": "The tendency of generative artificial intelligence (AI) systems to\n\"hallucinate\" false information is well-known; AI-generated citations to\nnon-existent sources have made their way into the reference lists of\npeer-reviewed publications. Here, I propose a solution to this problem, taking\ninspiration from the Transparency and Openness Promotion (TOP) data sharing\nguidelines, the clash of generative AI with the American judiciary, and the\nprecedent set by submissions of prior art to the United States Patent and\nTrademark Office. Journals should require authors to submit the full text of\neach cited source along with their manuscripts, thereby preventing authors from\nciting any material whose full text they cannot produce. This solution requires\nlimited additional work on the part of authors or editors while effectively\nimmunizing journals against hallucinated references.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "I.2.0; K.4.1"
    ],
    "primary_category": "cs.DL",
    "comment": "3 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.19848v1",
    "published_date": "2025-03-25 17:12:38 UTC",
    "updated_date": "2025-03-25 17:12:38 UTC"
  },
  {
    "arxiv_id": "2503.19844v1",
    "title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950",
    "authors": [
      "Zhao Fang",
      "Liang-Chun Wu",
      "Xuening Kong",
      "Spencer Dean Stewart"
    ],
    "abstract": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NLP4DH 2025 at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19844v1",
    "published_date": "2025-03-25 17:07:21 UTC",
    "updated_date": "2025-03-25 17:07:21 UTC"
  },
  {
    "arxiv_id": "2503.19823v2",
    "title": "GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization",
    "authors": [
      "Yan Zhuang",
      "Minheng Chen",
      "Chao Cao",
      "Tong Chen",
      "Jing Zhang",
      "Xiaowei Yu",
      "Yanjun Lyu",
      "Lu Zhang",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "abstract": "Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19823v2",
    "published_date": "2025-03-25 16:33:12 UTC",
    "updated_date": "2025-03-31 21:17:19 UTC"
  },
  {
    "arxiv_id": "2503.19817v1",
    "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
    "authors": [
      "Jordan Madden",
      "Lhamo Dorje",
      "Xiaohua Li"
    ],
    "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19817v1",
    "published_date": "2025-03-25 16:29:17 UTC",
    "updated_date": "2025-03-25 16:29:17 UTC"
  },
  {
    "arxiv_id": "2503.19951v1",
    "title": "ACVUBench: Audio-Centric Video Understanding Benchmark",
    "authors": [
      "Yudong Yang",
      "Jimin Zhuang",
      "Guangzhi Sun",
      "Changli Tang",
      "Yixuan Li",
      "Peihan Li",
      "Yifan Jiang",
      "Wei Li",
      "Zejun Ma",
      "Chao Zhang"
    ],
    "abstract": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(ACVUBench) to evaluate the video comprehension capabilities of multimodal LLMs\nwith a particular focus on auditory information. Specifically, ACVUBench\nincorporates 2,662 videos spanning 18 different domains with rich auditory\ninformation, together with over 13k high-quality human annotated or validated\nquestion-answer pairs. Moreover, ACVUBench introduces a suite of carefully\ndesigned audio-centric tasks, holistically testing the understanding of both\naudio content and audio-visual interactions in videos. A thorough evaluation\nacross a diverse range of open-source and proprietary multimodal LLMs is\nperformed, followed by the analyses of deficiencies in audio-visual LLMs. Demos\nare available at https://github.com/lark-png/ACVUBench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19951v1",
    "published_date": "2025-03-25 16:28:24 UTC",
    "updated_date": "2025-03-25 16:28:24 UTC"
  },
  {
    "arxiv_id": "2503.19815v1",
    "title": "Thinking agents for zero-shot generalization to qualitatively novel tasks",
    "authors": [
      "Thomas Miconi",
      "Kevin McKee",
      "Yicong Zheng",
      "Jed McCaleb"
    ],
    "abstract": "Intelligent organisms can solve truly novel problems which they have never\nencountered before, either in their lifetime or their evolution. An important\ncomponent of this capacity is the ability to ``think'', that is, to mentally\nmanipulate objects, concepts and behaviors in order to plan and evaluate\npossible solutions to novel problems, even without environment interaction. To\ngenerate problems that are truly qualitatively novel, while still solvable\nzero-shot (by mental simulation), we use the combinatorial nature of\nenvironments: we train the agent while withholding a specific combination of\nthe environment's elements. The novel test task, based on this combination, is\nthus guaranteed to be truly novel, while still mentally simulable since the\nagent has been exposed to each individual element (and their pairwise\ninteractions) during training. We propose a method to train agents endowed with\nworld models to make use their mental simulation abilities, by selecting tasks\nbased on the difference between the agent's pre-thinking and post-thinking\nperformance. When tested on the novel, withheld problem, the resulting agent\nsuccessfully simulated alternative scenarios and used the resulting information\nto guide its behavior in the actual environment, solving the novel task in a\nsingle real-environment trial (zero-shot).",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19815v1",
    "published_date": "2025-03-25 16:26:31 UTC",
    "updated_date": "2025-03-25 16:26:31 UTC"
  },
  {
    "arxiv_id": "2503.21810v1",
    "title": "Taxonomy Inference for Tabular Data Using Large Language Models",
    "authors": [
      "Zhenyu Wu",
      "Jiaoyan Chen",
      "Norman W. Paton"
    ],
    "abstract": "Taxonomy inference for tabular data is a critical task of schema inference,\naiming at discovering entity types (i.e., concepts) of the tables and building\ntheir hierarchy. It can play an important role in data management, data\nexploration, ontology learning, and many data-centric applications. Existing\nschema inference systems focus more on XML, JSON or RDF data, and often rely on\nlexical formats and structures of the data for calculating similarities, with\nlimited exploitation of the semantics of the text across a table. Motivated by\nrecent works on taxonomy completion and construction using Large Language\nModels (LLMs), this paper presents two LLM-based methods for taxonomy inference\nfor tables: (i) EmTT which embeds columns by fine-tuning with contrastive\nlearning encoder-alone LLMs like BERT and utilises clustering for hierarchy\nconstruction, and (ii) GeTT which generates table entity types and their\nhierarchy by iterative prompting using a decoder-alone LLM like GPT-4.\nExtensive evaluation on three real-world datasets with six metrics covering\ndifferent aspects of the output taxonomies has demonstrated that EmTT and GeTT\ncan both produce taxonomies with strong consistency relative to the Ground\nTruth.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21810v1",
    "published_date": "2025-03-25 16:26:05 UTC",
    "updated_date": "2025-03-25 16:26:05 UTC"
  },
  {
    "arxiv_id": "2503.19813v1",
    "title": "Guidelines For The Choice Of The Baseline in XAI Attribution Methods",
    "authors": [
      "Cristian Morasso",
      "Giorgio Dolci",
      "Ilaria Boscolo Galazzo",
      "Sergey M. Plis",
      "Gloria Menegaz"
    ],
    "abstract": "Given the broad adoption of artificial intelligence, it is essential to\nprovide evidence that AI models are reliable, trustable, and fair. To this end,\nthe emerging field of eXplainable AI develops techniques to probe such\nrequirements, counterbalancing the hype pushing the pervasiveness of this\ntechnology. Among the many facets of this issue, this paper focuses on baseline\nattribution methods, aiming at deriving a feature attribution map at the\nnetwork input relying on a \"neutral\" stimulus usually called \"baseline\". The\nchoice of the baseline is crucial as it determines the explanation of the\nnetwork behavior. In this framework, this paper has the twofold goal of\nshedding light on the implications of the choice of the baseline and providing\na simple yet effective method for identifying the best baseline for the task.\nTo achieve this, we propose a decision boundary sampling method, since the\nbaseline, by definition, lies on the decision boundary, which naturally becomes\nthe search domain. Experiments are performed on synthetic examples and\nvalidated relying on state-of-the-art methods. Despite being limited to the\nexperimental scope, this contribution is relevant as it offers clear guidelines\nand a simple proxy for baseline selection, reducing ambiguity and enhancing\ndeep models' reliability and trust.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19813v1",
    "published_date": "2025-03-25 16:25:04 UTC",
    "updated_date": "2025-03-25 16:25:04 UTC"
  },
  {
    "arxiv_id": "2503.19950v1",
    "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation",
    "authors": [
      "Han Chen",
      "Zicong Jiang",
      "Zining Zhang",
      "Bingsheng He",
      "Pingyi Luo",
      "Mian Lu",
      "Yuqiang Chen"
    ],
    "abstract": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
    "pdf_url": "http://arxiv.org/pdf/2503.19950v1",
    "published_date": "2025-03-25 16:24:45 UTC",
    "updated_date": "2025-03-25 16:24:45 UTC"
  },
  {
    "arxiv_id": "2503.19809v1",
    "title": "Simulating Tracking Data to Advance Sports Analytics Research",
    "authors": [
      "David Radke",
      "Kyle Tilbury"
    ],
    "abstract": "Advanced analytics have transformed how sports teams operate, particularly in\nepisodic sports like baseball. Their impact on continuous invasion sports, such\nas soccer and ice hockey, has been limited due to increased game complexity and\nrestricted access to high-resolution game tracking data. In this demo, we\npresent a method to collect and utilize simulated soccer tracking data from the\nGoogle Research Football environment to support the development of models\ndesigned for continuous tracking data. The data is stored in a schema that is\nrepresentative of real tracking data and we provide processes that extract\nhigh-level features and events. We include examples of established tracking\ndata models to showcase the efficacy of the simulated data. We address the\nscarcity of publicly available tracking data, providing support for research at\nthe intersection of artificial intelligence and sports analytics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2 pages, 2 figures, Proceedings of the 24th International Conference\n  on Autonomous Agents and MultiAgent Systems (AAMAS)",
    "pdf_url": "http://arxiv.org/pdf/2503.19809v1",
    "published_date": "2025-03-25 16:18:23 UTC",
    "updated_date": "2025-03-25 16:18:23 UTC"
  },
  {
    "arxiv_id": "2503.19804v1",
    "title": "LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset",
    "authors": [
      "Manjushree Aithal",
      "Rosaura G. VidalMata",
      "Manikandtan Kartha",
      "Gong Chen",
      "Eashan Adhikarla",
      "Lucas N. Kirsten",
      "Zhicheng Fu",
      "Nikhil A. Madhusudhana",
      "Joe Nasti"
    ],
    "abstract": "Low-light image enhancement is crucial for a myriad of applications, from\nnight vision and surveillance, to autonomous driving. However, due to the\ninherent limitations that come in hand with capturing images in\nlow-illumination environments, the task of enhancing such scenes still presents\na formidable challenge. To advance research in this field, we introduce our Low\nExposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure\nbenchmark dataset for low-light image enhancement comprising of over 230K\nframes showcasing 24K real-world indoor and outdoor, with-and without human,\nscenes. Captured using 3 different camera sensors, LENVIZ offers a wide range\nof lighting conditions, noise levels, and scene complexities, making it the\nlargest publicly available up-to 4K resolution benchmark in the field. LENVIZ\nincludes high quality human-generated ground truth, for which each\nmulti-exposure low-light scene has been meticulously curated and edited by\nexpert photographers to ensure optimal image quality. Furthermore, we also\nconduct a comprehensive analysis of current state-of-the-art low-light image\nenhancement techniques on our dataset and highlight potential areas of\nimprovement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Dataset will be released upon publication",
    "pdf_url": "http://arxiv.org/pdf/2503.19804v1",
    "published_date": "2025-03-25 16:12:28 UTC",
    "updated_date": "2025-03-25 16:12:28 UTC"
  },
  {
    "arxiv_id": "2503.19801v1",
    "title": "SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI",
    "authors": [
      "Zhiyang Liu",
      "Dong Yang",
      "Minghao Zhang",
      "Hanyu Sun",
      "Hong Wu",
      "Huiying Wang",
      "Wen Shen",
      "Chao Chai",
      "Shuang Xia"
    ],
    "abstract": "Despite that deep learning (DL) methods have presented tremendous potential\nin many medical image analysis tasks, the practical applications of medical DL\nmodels are limited due to the lack of enough data samples with manual\nannotations. By noting that the clinical radiology examinations are associated\nwith radiology reports that describe the images, we propose to develop a\nfoundation model for multi-model head MRI by using contrastive learning on the\nimages and the corresponding radiology findings. In particular, a contrastive\nlearning framework is proposed, where a mixed syntax and semantic similarity\nmatching metric is integrated to reduce the thirst of extreme large dataset in\nconventional contrastive learning framework. Our proposed similarity enhanced\ncontrastive language image pretraining (SeLIP) is able to effectively extract\nmore useful features. Experiments revealed that our proposed SeLIP performs\nwell in many downstream tasks including image-text retrieval task,\nclassification task, and image segmentation, which highlights the importance of\nconsidering the similarities among texts describing different images in\ndeveloping medical image foundation models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19801v1",
    "published_date": "2025-03-25 16:09:45 UTC",
    "updated_date": "2025-03-25 16:09:45 UTC"
  },
  {
    "arxiv_id": "2503.19794v1",
    "title": "PAVE: Patching and Adapting Video Large Language Models",
    "authors": [
      "Zhuoming Liu",
      "Yiquan Li",
      "Khoi Duc Nguyen",
      "Yiwu Zhong",
      "Yin Li"
    ],
    "abstract": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2025 Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2503.19794v1",
    "published_date": "2025-03-25 16:02:37 UTC",
    "updated_date": "2025-03-25 16:02:37 UTC"
  },
  {
    "arxiv_id": "2503.19786v1",
    "title": "Gemma 3 Technical Report",
    "authors": [
      "Gemma Team",
      "Aishwarya Kamath",
      "Johan Ferret",
      "Shreya Pathak",
      "Nino Vieillard",
      "Ramona Merhej",
      "Sarah Perrin",
      "Tatiana Matejovicova",
      "Alexandre Ramé",
      "Morgane Rivière",
      "Louis Rouillard",
      "Thomas Mesnard",
      "Geoffrey Cideron",
      "Jean-bastien Grill",
      "Sabela Ramos",
      "Edouard Yvinec",
      "Michelle Casbon",
      "Etienne Pot",
      "Ivo Penchev",
      "Gaël Liu",
      "Francesco Visin",
      "Kathleen Kenealy",
      "Lucas Beyer",
      "Xiaohai Zhai",
      "Anton Tsitsulin",
      "Robert Busa-Fekete",
      "Alex Feng",
      "Noveen Sachdeva",
      "Benjamin Coleman",
      "Yi Gao",
      "Basil Mustafa",
      "Iain Barr",
      "Emilio Parisotto",
      "David Tian",
      "Matan Eyal",
      "Colin Cherry",
      "Jan-Thorsten Peter",
      "Danila Sinopalnikov",
      "Surya Bhupatiraju",
      "Rishabh Agarwal",
      "Mehran Kazemi",
      "Dan Malkin",
      "Ravin Kumar",
      "David Vilar",
      "Idan Brusilovsky",
      "Jiaming Luo",
      "Andreas Steiner",
      "Abe Friesen",
      "Abhanshu Sharma",
      "Abheesht Sharma",
      "Adi Mayrav Gilady",
      "Adrian Goedeckemeyer",
      "Alaa Saade",
      "Alex Feng",
      "Alexander Kolesnikov",
      "Alexei Bendebury",
      "Alvin Abdagic",
      "Amit Vadi",
      "András György",
      "André Susano Pinto",
      "Anil Das",
      "Ankur Bapna",
      "Antoine Miech",
      "Antoine Yang",
      "Antonia Paterson",
      "Ashish Shenoy",
      "Ayan Chakrabarti",
      "Bilal Piot",
      "Bo Wu",
      "Bobak Shahriari",
      "Bryce Petrini",
      "Charlie Chen",
      "Charline Le Lan",
      "Christopher A. Choquette-Choo",
      "CJ Carey",
      "Cormac Brick",
      "Daniel Deutsch",
      "Danielle Eisenbud",
      "Dee Cattle",
      "Derek Cheng",
      "Dimitris Paparas",
      "Divyashree Shivakumar Sreepathihalli",
      "Doug Reid",
      "Dustin Tran",
      "Dustin Zelle",
      "Eric Noland",
      "Erwin Huizenga",
      "Eugene Kharitonov",
      "Frederick Liu",
      "Gagik Amirkhanyan",
      "Glenn Cameron",
      "Hadi Hashemi",
      "Hanna Klimczak-Plucińska",
      "Harman Singh",
      "Harsh Mehta",
      "Harshal Tushar Lehri",
      "Hussein Hazimeh",
      "Ian Ballantyne",
      "Idan Szpektor",
      "Ivan Nardini",
      "Jean Pouget-Abadie",
      "Jetha Chan",
      "Joe Stanton",
      "John Wieting",
      "Jonathan Lai",
      "Jordi Orbay",
      "Joseph Fernandez",
      "Josh Newlan",
      "Ju-yeong Ji",
      "Jyotinder Singh",
      "Kat Black",
      "Kathy Yu",
      "Kevin Hui",
      "Kiran Vodrahalli",
      "Klaus Greff",
      "Linhai Qiu",
      "Marcella Valentine",
      "Marina Coelho",
      "Marvin Ritter",
      "Matt Hoffman",
      "Matthew Watson",
      "Mayank Chaturvedi",
      "Michael Moynihan",
      "Min Ma",
      "Nabila Babar",
      "Natasha Noy",
      "Nathan Byrd",
      "Nick Roy",
      "Nikola Momchev",
      "Nilay Chauhan",
      "Noveen Sachdeva",
      "Oskar Bunyan",
      "Pankil Botarda",
      "Paul Caron",
      "Paul Kishan Rubenstein",
      "Phil Culliton",
      "Philipp Schmid",
      "Pier Giuseppe Sessa",
      "Pingmei Xu",
      "Piotr Stanczyk",
      "Pouya Tafti",
      "Rakesh Shivanna",
      "Renjie Wu",
      "Renke Pan",
      "Reza Rokni",
      "Rob Willoughby",
      "Rohith Vallu",
      "Ryan Mullins",
      "Sammy Jerome",
      "Sara Smoot",
      "Sertan Girgin",
      "Shariq Iqbal",
      "Shashir Reddy",
      "Shruti Sheth",
      "Siim Põder",
      "Sijal Bhatnagar",
      "Sindhu Raghuram Panyam",
      "Sivan Eiger",
      "Susan Zhang",
      "Tianqi Liu",
      "Trevor Yacovone",
      "Tyler Liechty",
      "Uday Kalra",
      "Utku Evci",
      "Vedant Misra",
      "Vincent Roseberry",
      "Vlad Feinberg",
      "Vlad Kolesnikov",
      "Woohyun Han",
      "Woosuk Kwon",
      "Xi Chen",
      "Yinlam Chow",
      "Yuvein Zhu",
      "Zichuan Wei",
      "Zoltan Egyed",
      "Victor Cotruta",
      "Minh Giang",
      "Phoebe Kirk",
      "Anand Rao",
      "Kat Black",
      "Nabila Babar",
      "Jessica Lo",
      "Erica Moreira",
      "Luiz Gustavo Martins",
      "Omar Sanseviero",
      "Lucas Gonzalez",
      "Zach Gleicher",
      "Tris Warkentin",
      "Vahab Mirrokni",
      "Evan Senter",
      "Eli Collins",
      "Joelle Barral",
      "Zoubin Ghahramani",
      "Raia Hadsell",
      "Yossi Matias",
      "D. Sculley",
      "Slav Petrov",
      "Noah Fiedel",
      "Noam Shazeer",
      "Oriol Vinyals",
      "Jeff Dean",
      "Demis Hassabis",
      "Koray Kavukcuoglu",
      "Clement Farabet",
      "Elena Buchatskaya",
      "Jean-Baptiste Alayrac",
      "Rohan Anil",
      "Dmitry",
      "Lepikhin",
      "Sebastian Borgeaud",
      "Olivier Bachem",
      "Armand Joulin",
      "Alek Andreev",
      "Cassidy Hardin",
      "Robert Dadashi",
      "Léonard Hussenot"
    ],
    "abstract": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19786v1",
    "published_date": "2025-03-25 15:52:34 UTC",
    "updated_date": "2025-03-25 15:52:34 UTC"
  },
  {
    "arxiv_id": "2503.19948v1",
    "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards",
    "authors": [
      "Alexander Gambashidze",
      "Konstantin Sobolev",
      "Andrey Kuznetsov",
      "Ivan Oseledets"
    ],
    "abstract": "Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19948v1",
    "published_date": "2025-03-25 15:30:21 UTC",
    "updated_date": "2025-03-25 15:30:21 UTC"
  },
  {
    "arxiv_id": "2503.19762v1",
    "title": "Splitting Answer Set Programs with respect to Intensionality Statements (Extended Version)",
    "authors": [
      "Jorge Fandinno",
      "Yuliya Lierler"
    ],
    "abstract": "Splitting a logic program allows us to reduce the task of computing its\nstable models to similar tasks for its subprograms. This can be used to\nincrease solving performance and prove program correctness. We generalize the\nconditions under which this technique is applicable, by considering not only\ndependencies between predicates but also their arguments and context. This\nallows splitting programs commonly used in practice to which previous results\nwere not applicable.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of the paper published in AAAI 2023",
    "pdf_url": "http://arxiv.org/pdf/2503.19762v1",
    "published_date": "2025-03-25 15:27:05 UTC",
    "updated_date": "2025-03-25 15:27:05 UTC"
  },
  {
    "arxiv_id": "2503.19947v1",
    "title": "Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized Image Encoders",
    "authors": [
      "Paul Koch",
      "Jörg Krüger",
      "Ankit Chowdhury",
      "Oliver Heimann"
    ],
    "abstract": "Generalized metric depth understanding is critical for precise vision-guided\nrobotics, which current state-of-the-art (SOTA) vision-encoders do not support.\nTo address this, we propose Vanishing Depth, a self-supervised training\napproach that extends pretrained RGB encoders to incorporate and align metric\ndepth into their feature embeddings. Based on our novel positional depth\nencoding, we enable stable depth density and depth distribution invariant\nfeature extraction. We achieve performance improvements and SOTA results across\na spectrum of relevant RGBD downstream tasks - without the necessity of\nfinetuning the encoder. Most notably, we achieve 56.05 mIoU on SUN-RGBD\nsegmentation, 88.3 RMSE on Void's depth completion, and 83.8 Top 1 accuracy on\nNYUv2 scene classification. In 6D-object pose estimation, we outperform our\npredecessors of DinoV2, EVA-02, and Omnivore and achieve SOTA results for\nnon-finetuned encoders in several related RGBD downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.19947v1",
    "published_date": "2025-03-25 15:19:48 UTC",
    "updated_date": "2025-03-25 15:19:48 UTC"
  },
  {
    "arxiv_id": "2503.19753v2",
    "title": "A Survey on Event-driven 3D Reconstruction: Development under Different Categories",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Haodong Chen",
      "Vera Chung",
      "Qiang Qu"
    ],
    "abstract": "Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "6 pages, 1 figure, 6 tables, submitted to an anonymous conference\n  under double-blind review",
    "pdf_url": "http://arxiv.org/pdf/2503.19753v2",
    "published_date": "2025-03-25 15:16:53 UTC",
    "updated_date": "2025-03-26 12:34:34 UTC"
  },
  {
    "arxiv_id": "2503.19752v1",
    "title": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect on Human-Like Agenda Generation",
    "authors": [
      "Lewis Newsham",
      "Ryan Hyland",
      "Daniel Prince"
    ],
    "abstract": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 1 figure, 6 tables. Accepted to NLPAICS 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.19752v1",
    "published_date": "2025-03-25 15:16:35 UTC",
    "updated_date": "2025-03-25 15:16:35 UTC"
  },
  {
    "arxiv_id": "2503.19730v2",
    "title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos",
    "authors": [
      "Yuli Zhou",
      "Guolei Sun",
      "Yawei Li",
      "Yuqian Fu",
      "Luca Benini",
      "Ender Konukoglu"
    ],
    "abstract": "Video camouflaged object segmentation (VCOS), aiming at segmenting\ncamouflaged objects that seamlessly blend into their environment, is a\nfundamental vision task with various real-world applications. With the release\nof SAM2, video segmentation has witnessed significant progress. However, SAM2's\ncapability of segmenting camouflaged videos is suboptimal, especially when\ngiven simple prompts such as point and box. To address the problem, we propose\nCamouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged\nscenes without modifying SAM2's parameters. Specifically, we introduce a\ndecamouflaged token to provide the flexibility of feature adjustment for VCOS.\nTo make full use of fine-grained and high-resolution features from the current\nframe and previous frames, we propose implicit object-aware fusion (IOF) and\nexplicit object-aware fusion (EOF) modules, respectively. Object prototype\ngeneration (OPG) is introduced to abstract and memorize object prototypes with\ninformative details using high-quality features from previous frames. Extensive\nexperiments are conducted to validate the effectiveness of our approach. While\nCamSAM2 only adds negligible learnable parameters to SAM2, it substantially\noutperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains\nwith click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on\nSUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at\nhttps://github.com/zhoustan/CamSAM2.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19730v2",
    "published_date": "2025-03-25 14:58:52 UTC",
    "updated_date": "2025-03-26 02:14:50 UTC"
  },
  {
    "arxiv_id": "2503.19719v1",
    "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?",
    "authors": [
      "Francisco Mena",
      "Diego Arenas",
      "Miro Miranda",
      "Andreas Dengel"
    ],
    "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19719v1",
    "published_date": "2025-03-25 14:45:23 UTC",
    "updated_date": "2025-03-25 14:45:23 UTC"
  },
  {
    "arxiv_id": "2503.19717v1",
    "title": "Invertible Koopman neural operator for data-driven modeling of partial differential equations",
    "authors": [
      "Yuhong Jin",
      "Andong Cong",
      "Lei Hou",
      "Qiang Gao",
      "Xiangdong Ge",
      "Chonglong Zhu",
      "Yongzhi Feng",
      "Jun Li"
    ],
    "abstract": "Koopman operator theory is a popular candidate for data-driven modeling\nbecause it provides a global linearization representation for nonlinear\ndynamical systems. However, existing Koopman operator-based methods suffer from\nshortcomings in constructing the well-behaved observable function and its\ninverse and are inefficient enough when dealing with partial differential\nequations (PDEs). To address these issues, this paper proposes the Invertible\nKoopman Neural Operator (IKNO), a novel data-driven modeling approach inspired\nby the Koopman operator theory and neural operator. IKNO leverages an\nInvertible Neural Network to parameterize observable function and its inverse\nsimultaneously under the same learnable parameters, explicitly guaranteeing the\nreconstruction relation, thus eliminating the dependency on the reconstruction\nloss, which is an essential improvement over the original Koopman Neural\nOperator (KNO). The structured linear matrix inspired by the Koopman operator\ntheory is parameterized to learn the evolution of observables' low-frequency\nmodes in the frequency space rather than directly in the observable space,\nsustaining IKNO is resolution-invariant like other neural operators. Moreover,\nwith preprocessing such as interpolation and dimension expansion, IKNO can be\nextended to operator learning tasks defined on non-Cartesian domains. We fully\nsupport the above claims based on rich numerical and real-world examples and\ndemonstrate the effectiveness of IKNO and superiority over other neural\noperators.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19717v1",
    "published_date": "2025-03-25 14:43:53 UTC",
    "updated_date": "2025-03-25 14:43:53 UTC"
  },
  {
    "arxiv_id": "2503.19712v1",
    "title": "Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions",
    "authors": [
      "Sanghyuk Kim",
      "Minsik Seo",
      "Namwoo Kang"
    ],
    "abstract": "This study proposes a neural framework that predicts 3D vehicle collision\ndynamics by independently modeling global rigid-body motion and local\nstructural deformation. Unlike approaches directly predicting absolute\ndisplacement, this method explicitly separates the vehicle's overall\ntranslation and rotation from its structural deformation. Two specialized\nnetworks form the core of the framework: a quaternion-based Rigid Net for rigid\nmotion and a coordinate-based Deformation Net for local deformation. By\nindependently handling fundamentally distinct physical phenomena, the proposed\narchitecture achieves accurate predictions without requiring separate\nsupervision for each component. The model, trained on only 10% of available\nsimulation data, significantly outperforms baseline models, including single\nmulti-layer perceptron (MLP) and deep operator networks (DeepONet), with\nprediction errors reduced by up to 83%. Extensive validation demonstrates\nstrong generalization to collision conditions outside the training range,\naccurately predicting responses even under severe impacts involving extreme\nvelocities and large impact angles. Furthermore, the framework successfully\nreconstructs high-resolution deformation details from low-resolution inputs\nwithout increased computational effort. Consequently, the proposed approach\nprovides an effective, computationally efficient method for rapid and reliable\nassessment of vehicle safety across complex collision scenarios, substantially\nreducing the required simulation data and time while preserving prediction\nfidelity.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "24 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19712v1",
    "published_date": "2025-03-25 14:38:37 UTC",
    "updated_date": "2025-03-25 14:38:37 UTC"
  },
  {
    "arxiv_id": "2503.19711v1",
    "title": "Writing as a testbed for open ended agents",
    "authors": [
      "Sian Gooding",
      "Lucia Lopez-Rivilla",
      "Edward Grefenstette"
    ],
    "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19711v1",
    "published_date": "2025-03-25 14:38:36 UTC",
    "updated_date": "2025-03-25 14:38:36 UTC"
  },
  {
    "arxiv_id": "2503.19706v2",
    "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
    "authors": [
      "Jungin Park",
      "Jiyoung Lee",
      "Kwanghoon Sohn"
    ],
    "abstract": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 Camera-ready, 18 pages, 7 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.19706v2",
    "published_date": "2025-03-25 14:33:32 UTC",
    "updated_date": "2025-03-31 08:46:51 UTC"
  },
  {
    "arxiv_id": "2503.19699v1",
    "title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "abstract": "In this study, we formulate the drone delivery problem as a control problem\nand solve it using Model Predictive Control. Two experiments are performed: The\nfirst is on a less challenging grid world environment with lower\ndimensionality, and the second is with a higher dimensionality and added\ncomplexity. The MPC method was benchmarked against three popular Multi-Agent\nReinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action\nLearners (JAL), and Value-Decomposition Networks (VDN). It was shown that the\nMPC method solved the problem quicker and required fewer optimal numbers of\ndrones to achieve a minimized cost and navigate the optimal path.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 5 figures, Submitted to the 2025 International Conference\n  on Artificial Intelligence, Computer, Data Sciences and Applications",
    "pdf_url": "http://arxiv.org/pdf/2503.19699v1",
    "published_date": "2025-03-25 14:27:29 UTC",
    "updated_date": "2025-03-25 14:27:29 UTC"
  },
  {
    "arxiv_id": "2503.19677v1",
    "title": "Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing Mel Spectrograms",
    "authors": [
      "Niketa Penumajji"
    ],
    "abstract": "This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19677v1",
    "published_date": "2025-03-25 14:02:10 UTC",
    "updated_date": "2025-03-25 14:02:10 UTC"
  },
  {
    "arxiv_id": "2503.19658v1",
    "title": "BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction",
    "authors": [
      "Jan Kohút",
      "Martin Dočekal",
      "Michal Hradiš",
      "Marek Vaško"
    ],
    "abstract": "Manual digitization of bibliographic metadata is time consuming and labor\nintensive, especially for historical and real-world archives with highly\nvariable formatting across documents. Despite advances in machine learning, the\nabsence of dedicated datasets for metadata extraction hinders automation. To\naddress this gap, we introduce BiblioPage, a dataset of scanned title pages\nannotated with structured bibliographic metadata. The dataset consists of\napproximately 2,000 monograph title pages collected from 14 Czech libraries,\nspanning a wide range of publication periods, typographic styles, and layout\nstructures. Each title page is annotated with 16 bibliographic attributes,\nincluding title, contributors, and publication metadata, along with precise\npositional information in the form of bounding boxes. To extract structured\ninformation from this dataset, we valuated object detection models such as YOLO\nand DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and\nan F1 score of 59. Additionally, we assess the performance of various visual\nlarge language models, including LlamA 3.2-Vision and GPT-4o, with the best\nmodel reaching an F1 score of 67. BiblioPage serves as a real-world benchmark\nfor bibliographic metadata extraction, contributing to document understanding,\ndocument question answering, and document information extraction. Dataset and\nevaluation scripts are availible at: https://github.com/DCGM/biblio-dataset",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to ICDAR2025 conference",
    "pdf_url": "http://arxiv.org/pdf/2503.19658v1",
    "published_date": "2025-03-25 13:46:55 UTC",
    "updated_date": "2025-03-25 13:46:55 UTC"
  },
  {
    "arxiv_id": "2503.19656v1",
    "title": "Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms",
    "authors": [
      "Ninghui Feng",
      "Songning Lai",
      "Xin Zhou",
      "Jiayu Yang",
      "Kunlong Feng",
      "Zhenxiao Yin",
      "Fobao Zhou",
      "Zhangyi Hu",
      "Yutao Yue",
      "Yuxuan Liang",
      "Boyu Wang",
      "Hang Zhao"
    ],
    "abstract": "In real-world time series forecasting, uncertainty and lack of reliable\nevaluation pose significant challenges. Notably, forecasting errors often arise\nfrom underfitting in-distribution data and failing to handle\nout-of-distribution inputs. To enhance model reliability, we introduce a dual\nrejection mechanism combining ambiguity and novelty rejection. Ambiguity\nrejection, using prediction error variance, allows the model to abstain under\nlow confidence, assessed through historical error variance analysis without\nfuture ground truth. Novelty rejection, employing Variational Autoencoders and\nMahalanobis distance, detects deviations from training data. This dual approach\nimproves forecasting reliability in dynamic environments by reducing errors and\nadapting to data changes, advancing reliability in complex scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19656v1",
    "published_date": "2025-03-25 13:44:29 UTC",
    "updated_date": "2025-03-25 13:44:29 UTC"
  },
  {
    "arxiv_id": "2503.19654v3",
    "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models",
    "authors": [
      "Mehdi Moshtaghi",
      "Siavash H. Khajavi",
      "Joni Pajarinen"
    ],
    "abstract": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19654v3",
    "published_date": "2025-03-25 13:43:47 UTC",
    "updated_date": "2025-03-30 15:08:23 UTC"
  },
  {
    "arxiv_id": "2503.19653v3",
    "title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World",
    "authors": [
      "Yabin Wang",
      "Zhiwu Huang",
      "Xiaopeng Hong"
    ],
    "abstract": "This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps://github.com/iamwangyabin/OpenSDI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19653v3",
    "published_date": "2025-03-25 13:43:16 UTC",
    "updated_date": "2025-04-16 08:07:26 UTC"
  },
  {
    "arxiv_id": "2503.19650v1",
    "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection",
    "authors": [
      "Maryam Bala",
      "Amina Imam Abubakar",
      "Abdulhamid Abubakar",
      "Abdulkadir Shehu Bichi",
      "Hafsa Kabir Ahmad",
      "Sani Abdullahi Sani",
      "Idris Abdulmumin",
      "Shamsuddeen Hassan Muhamad",
      "Ibrahim Said Ahmad"
    ],
    "abstract": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19650v1",
    "published_date": "2025-03-25 13:40:22 UTC",
    "updated_date": "2025-03-25 13:40:22 UTC"
  },
  {
    "arxiv_id": "2503.19649v1",
    "title": "Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac Feature Monitoring from Radar Signal Components",
    "authors": [
      "Yuanyuan Zhang",
      "Sijie Xiong",
      "Rui Yang",
      "EngGee Lim",
      "Yutao Yue"
    ],
    "abstract": "Radar-based wellness monitoring is becoming an effective measurement to\nprovide accurate vital signs in a contactless manner, but data scarcity retards\nthe related research on deep-learning-based methods. Data augmentation is\ncommonly used to enrich the dataset by modifying the existing data, but most\naugmentation techniques can only couple with classification tasks. To enable\nthe augmentation for regression tasks, this research proposes a spectrogram\naugmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g.,\nheartbeat detection, electrocardiogram reconstruction) with both classification\nand regression tasks involved. The proposed method is designed to increase the\ndiversity of input samples while the augmented spectrogram is still faithful to\nthe original ground truth vital sign. In addition, Horcrux proposes to inject\nzero values in specific areas to enhance the awareness of the deep learning\nmodel on subtle cardiac features, improving the performance for the limited\ndataset. Experimental result shows that Horcrux achieves an overall improvement\nof 16.20% in cardiac monitoring and has the potential to be extended to other\nspectrogram-based tasks. The code will be released upon publication.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19649v1",
    "published_date": "2025-03-25 13:40:05 UTC",
    "updated_date": "2025-03-25 13:40:05 UTC"
  },
  {
    "arxiv_id": "2503.19647v1",
    "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
    "authors": [
      "Niccolo Avogaro",
      "Thomas Frick",
      "Mattia Rigotti",
      "Andrea Bartezzaghi",
      "Filip Janicki",
      "Cristiano Malossi",
      "Konrad Schindler",
      "Roy Assaf"
    ],
    "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as\nfoundation models that can be instructed to solve diverse tasks by prompting,\nwithout task-specific training. We examine the seemingly obvious question: how\nto effectively prompt VLMs for semantic segmentation. To that end, we\nsystematically evaluate the segmentation performance of several recent models\nguided by either text or visual prompts on the out-of-distribution MESS dataset\ncollection. We introduce a scalable prompting scheme, few-shot prompted\nsemantic segmentation, inspired by open-vocabulary segmentation and few-shot\nlearning. It turns out that VLMs lag far behind specialist models trained for a\nspecific segmentation task, by about 30% on average on the\nIntersection-over-Union metric. Moreover, we find that text prompts and visual\nprompts are complementary: each one of the two modes fails on many examples\nthat the other one can solve. Our analysis suggests that being able to\nanticipate the most effective prompt modality can lead to a 11% improvement in\nperformance. Motivated by our findings, we propose PromptMatcher, a remarkably\nsimple training-free baseline that combines both text and visual prompts,\nachieving state-of-the-art results outperforming the best text-prompted VLM by\n2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic\nsegmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19647v1",
    "published_date": "2025-03-25 13:36:59 UTC",
    "updated_date": "2025-03-25 13:36:59 UTC"
  },
  {
    "arxiv_id": "2503.19611v1",
    "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
    "authors": [
      "Max W. Y. Lam",
      "Yijin Xing",
      "Weiya You",
      "Jingcheng Wu",
      "Zongyu Yin",
      "Fuqiang Jiang",
      "Hangyu Liu",
      "Feng Liu",
      "Xingda Li",
      "Wei-Tsung Lu",
      "Hanyu Chen",
      "Tong Feng",
      "Tianwei Zhao",
      "Chien-Hung Liu",
      "Xuchen Song",
      "Yang Li",
      "Yahui Zhou"
    ],
    "abstract": "Autoregressive (AR) models have demonstrated impressive capabilities in\ngenerating high-fidelity music. However, the conventional next-token prediction\nparadigm in AR models does not align with the human creative process in music\ncomposition, potentially compromising the musicality of generated samples. To\novercome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT)\nprompting technique tailored for music generation. MusiCoT empowers the AR\nmodel to first outline an overall music structure before generating audio\ntokens, thereby enhancing the coherence and creativity of the resulting\ncompositions. By leveraging the contrastive language-audio pretraining (CLAP)\nmodel, we establish a chain of \"musical thoughts\", making MusiCoT scalable and\nindependent of human-labeled data, in contrast to conventional CoT methods.\nMoreover, MusiCoT allows for in-depth analysis of music structure, such as\ninstrumental arrangements, and supports music referencing -- accepting\nvariable-length audio inputs as optional style references. This innovative\napproach effectively addresses copying issues, positioning MusiCoT as a vital\npractical method for music prompting. Our experimental results indicate that\nMusiCoT consistently achieves superior performance across both objective and\nsubjective metrics, producing music quality that rivals state-of-the-art\ngeneration models.\n  Our samples are available at https://MusiCoT.github.io/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.19611v1",
    "published_date": "2025-03-25 12:51:21 UTC",
    "updated_date": "2025-03-25 12:51:21 UTC"
  },
  {
    "arxiv_id": "2503.19607v1",
    "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review",
    "authors": [
      "Edward Gu",
      "Ho Chit Siu",
      "Melanie Platt",
      "Isabelle Hurley",
      "Jaime Peña",
      "Rohan Paleja"
    ],
    "abstract": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to the Cooperative Multi-Agent Systems Decision-making and\n  Learning:Human-Multi-Agent Cognitive Fusion Workshop at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19607v1",
    "published_date": "2025-03-25 12:43:18 UTC",
    "updated_date": "2025-03-25 12:43:18 UTC"
  },
  {
    "arxiv_id": "2503.19602v1",
    "title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking",
    "authors": [
      "Yuyao Ge",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Lizhe Chen",
      "Baolong Bi",
      "Xueqi Cheng"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19602v1",
    "published_date": "2025-03-25 12:37:22 UTC",
    "updated_date": "2025-03-25 12:37:22 UTC"
  },
  {
    "arxiv_id": "2503.19599v1",
    "title": "HoarePrompt: Structural Reasoning About Program Correctness in Natural Language",
    "authors": [
      "Dimitrios Stamatios Bouras",
      "Yihan Dai",
      "Tairan Wang",
      "Yingfei Xiong",
      "Sergey Mechtaev"
    ],
    "abstract": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19599v1",
    "published_date": "2025-03-25 12:30:30 UTC",
    "updated_date": "2025-03-25 12:30:30 UTC"
  },
  {
    "arxiv_id": "2503.19584v3",
    "title": "Multi-agent Application System in Office Collaboration Scenarios",
    "authors": [
      "Songtao Sun",
      "Jingyi Li",
      "Yuanfei Dong",
      "Haoguang Liu",
      "Chenxin Xu",
      "Fuyang Li",
      "Qiang Liu"
    ],
    "abstract": "This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2503.19584v3",
    "published_date": "2025-03-25 12:07:20 UTC",
    "updated_date": "2025-04-07 07:46:24 UTC"
  },
  {
    "arxiv_id": "2503.19945v1",
    "title": "Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study of Transfer Learning, Resolution Reduction, and Multi-View Classification",
    "authors": [
      "Daniel G. P. Petrini",
      "Hae Yong Kim"
    ],
    "abstract": "This study explores open questions in the application of machine learning for\nbreast cancer detection in mammograms. Current approaches often employ a\ntwo-stage transfer learning process: first, adapting a backbone model trained\non natural images to develop a patch classifier, which is then used to create a\nsingle-view whole-image classifier. Additionally, many studies leverage both\nmammographic views to enhance model performance. In this work, we\nsystematically investigate five key questions: (1) Is the intermediate patch\nclassifier essential for optimal performance? (2) Do backbone models that excel\nin natural image classification consistently outperform others on mammograms?\n(3) When reducing mammogram resolution for GPU processing, does the\nlearn-to-resize technique outperform conventional methods? (4) Does\nincorporating both mammographic views in a two-view classifier significantly\nimprove detection accuracy? (5) How do these findings vary when analyzing\nlow-quality versus high-quality mammograms? By addressing these questions, we\ndeveloped models that outperform previous results for both single-view and\ntwo-view classifiers. Our findings provide insights into model architecture and\ntransfer learning strategies contributing to more accurate and efficient\nmammogram analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.19945v1",
    "published_date": "2025-03-25 11:51:21 UTC",
    "updated_date": "2025-03-25 11:51:21 UTC"
  },
  {
    "arxiv_id": "2503.19564v1",
    "title": "FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments",
    "authors": [
      "Sree Bhargavi Balija"
    ],
    "abstract": "As artificial intelligence systems increasingly operate in Real-world\nenvironments, the integration of multi-modal data sources such as vision,\nlanguage, and audio presents both unprecedented opportunities and critical\nchallenges for achieving trustworthy intelligence. In this paper, we propose a\nnovel framework that unifies federated learning with explainable multi-modal\nreasoning to ensure trustworthiness in decentralized, dynamic settings. Our\napproach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),\nleverages cross-modal consistency checks, client-level interpretability\nmechanisms, and dynamic trust calibration to address challenges posed by data\nheterogeneity, modality imbalance, and out-of-distribution generalization.\nThrough rigorous evaluation across federated multi-modal benchmarks involving\nvision-language tasks, we demonstrate improved performance in both accuracy and\ninterpretability while reducing vulnerabilities to adversarial and spurious\ncorrelations. Further, we introduce a novel trust score aggregation method to\nquantify global model reliability under dynamic client participation. Our\nfindings pave the way toward developing robust, interpretable, and socially\nresponsible AI systems in Real-world environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19564v1",
    "published_date": "2025-03-25 11:28:21 UTC",
    "updated_date": "2025-03-25 11:28:21 UTC"
  },
  {
    "arxiv_id": "2503.19551v2",
    "title": "Scaling Laws of Synthetic Data for Language Models",
    "authors": [
      "Zeyu Qin",
      "Qingxiu Dong",
      "Xingxing Zhang",
      "Li Dong",
      "Xiaolong Huang",
      "Ziyi Yang",
      "Mahmoud Khademi",
      "Dongdong Zhang",
      "Hany Hassan Awadalla",
      "Yi R. Fung",
      "Weizhu Chen",
      "Minhao Cheng",
      "Furu Wei"
    ],
    "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.19551v2",
    "published_date": "2025-03-25 11:07:12 UTC",
    "updated_date": "2025-03-26 11:23:44 UTC"
  },
  {
    "arxiv_id": "2503.19540v1",
    "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
    "authors": [
      "Dahyun Jung",
      "Seungyoon Lee",
      "Hyeonseok Moon",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 findings",
    "pdf_url": "http://arxiv.org/pdf/2503.19540v1",
    "published_date": "2025-03-25 10:48:33 UTC",
    "updated_date": "2025-03-25 10:48:33 UTC"
  },
  {
    "arxiv_id": "2503.19530v1",
    "title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained Foundation Models",
    "authors": [
      "Suhas G Hegde",
      "Shilpy Kaur",
      "Aruna Tiwari"
    ],
    "abstract": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19530v1",
    "published_date": "2025-03-25 10:36:27 UTC",
    "updated_date": "2025-03-25 10:36:27 UTC"
  },
  {
    "arxiv_id": "2504.00009v1",
    "title": "Deep Learning-Based Hypoglycemia Classification Across Multiple Prediction Horizons",
    "authors": [
      "Beyza Cinar",
      "Jennifer Daniel Onwuchekwa",
      "Maria Maleshkova"
    ],
    "abstract": "Type 1 diabetes (T1D) management can be significantly enhanced through the\nuse of predictive machine learning (ML) algorithms, which can mitigate the risk\nof adverse events like hypoglycemia. Hypoglycemia, characterized by blood\nglucose levels below 70 mg/dL, is a life-threatening condition typically caused\nby excessive insulin administration, missed meals, or physical activity. Its\nasymptomatic nature impedes timely intervention, making ML models crucial for\nearly detection. This study integrates short- (up to 2h) and long-term (up to\n24h) prediction horizons (PHs) within a single classification model to enhance\ndecision support. The predicted times are 5-15 min, 15-30 min, 30 min-1h, 1-2h,\n2-4h, 4-8h, 8-12h, and 12-24h before hypoglycemia. In addition, a simplified\nmodel classifying up to 4h before hypoglycemia is compared. We trained ResNet\nand LSTM models on glucose levels, insulin doses, and acceleration data. The\nresults demonstrate the superiority of the LSTM models when classifying nine\nclasses. In particular, subject-specific models yielded better performance but\nachieved high recall only for classes 0, 1, and 2 with 98%, 72%, and 50%,\nrespectively. A population-based six-class model improved the results with at\nleast 60% of events detected. In contrast, longer PHs remain challenging with\nthe current approach and may be considered with different models.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00009v1",
    "published_date": "2025-03-25 10:24:27 UTC",
    "updated_date": "2025-03-25 10:24:27 UTC"
  },
  {
    "arxiv_id": "2503.19943v1",
    "title": "A Spatiotemporal Radar-Based Precipitation Model for Water Level Prediction and Flood Forecasting",
    "authors": [
      "Sakshi Dhankhar",
      "Stefan Wittek",
      "Hamidreza Eivazi",
      "Andreas Rausch"
    ],
    "abstract": "Study Region: Goslar and G\\\"ottingen, Lower Saxony, Germany. Study Focus: In\nJuly 2017, the cities of Goslar and G\\\"ottingen experienced severe flood events\ncharacterized by short warning time of only 20 minutes, resulting in extensive\nregional flooding and significant damage. This highlights the critical need for\na more reliable and timely flood forecasting system. This paper presents a\ncomprehensive study on the impact of radar-based precipitation data on\nforecasting river water levels in Goslar. Additionally, the study examines how\nprecipitation influences water level forecasts in G\\\"ottingen. The analysis\nintegrates radar-derived spatiotemporal precipitation patterns with\nhydrological sensor data obtained from ground stations to evaluate the\neffectiveness of this approach in improving flood prediction capabilities. New\nHydrological Insights for the Region: A key innovation in this paper is the use\nof residual-based modeling to address the non-linearity between precipitation\nimages and water levels, leading to a Spatiotemporal Radar-based Precipitation\nModel with residuals (STRPMr). Unlike traditional hydrological models, our\napproach does not rely on upstream data, making it independent of additional\nhydrological inputs. This independence enhances its adaptability and allows for\nbroader applicability in other regions with RADOLAN precipitation. The deep\nlearning architecture integrates (2+1)D convolutional neural networks for\nspatial and temporal feature extraction with LSTM for timeseries forecasting.\nThe results demonstrate the potential of the STRPMr for capturing extreme\nevents and more accurate flood forecasting.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "28 pages, 11 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.19943v1",
    "published_date": "2025-03-25 10:14:54 UTC",
    "updated_date": "2025-03-25 10:14:54 UTC"
  },
  {
    "arxiv_id": "2503.19510v1",
    "title": "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation",
    "authors": [
      "Sheng Wang"
    ],
    "abstract": "As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19510v1",
    "published_date": "2025-03-25 10:01:57 UTC",
    "updated_date": "2025-03-25 10:01:57 UTC"
  },
  {
    "arxiv_id": "2503.19502v1",
    "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
    "authors": [
      "Qi Chen",
      "Yinghao Cui",
      "Guobin Hong",
      "Karumuri Ashok",
      "Yuchun Pu",
      "Xiaogu Zheng",
      "Xuanze Zhang",
      "Wei Zhong",
      "Peng Zhan",
      "Zhonglei Wang"
    ],
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19502v1",
    "published_date": "2025-03-25 09:50:19 UTC",
    "updated_date": "2025-03-25 09:50:19 UTC"
  },
  {
    "arxiv_id": "2503.19501v1",
    "title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs",
    "authors": [
      "Vinayak Mali",
      "Saurabh Jaiswal"
    ],
    "abstract": "Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 Pages, 2 figures, 2 code block, 1 flow chart",
    "pdf_url": "http://arxiv.org/pdf/2503.19501v1",
    "published_date": "2025-03-25 09:49:36 UTC",
    "updated_date": "2025-03-25 09:49:36 UTC"
  },
  {
    "arxiv_id": "2503.19496v1",
    "title": "SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration",
    "authors": [
      "Mohammad Daffa Robani",
      "Paul Saves",
      "Pramudita Satria Palar",
      "Lavi Rizki Zuhal",
      "oseph Morlier"
    ],
    "abstract": "Surrogate models are of high interest for many engineering applications,\nserving as cheap-to-evaluate time-efficient approximations of black-box\nfunctions to help engineers and practitioners make decisions and understand\ncomplex systems. As such, the need for explainability methods is rising and\nmany studies have been performed to facilitate knowledge discovery from\nsurrogate models. To respond to these enquiries, this paper introduces SMT-EX,\nan enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that\nintegrates explainability techniques into a state-of-the-art surrogate\nmodelling framework. More precisely, SMT-EX includes three key explainability\nmethods: Shapley Additive Explanations, Partial Dependence Plot, and Individual\nConditional Expectations. A peculiar explainability dependency of SMT has been\ndeveloped for such purpose that can be easily activated once the surrogate\nmodel is built, offering a user-friendly and efficient tool for swift insight\nextraction. The effectiveness of SMT-EX is showcased through two test cases.\nThe first case is a 10-variable wing weight problem with purely continuous\nvariables and the second one is a 3-variable mixed-categorical cantilever beam\nbending problem. Relying on SMT-EX analyses for these problems, we demonstrate\nits versatility in addressing a diverse range of problem characteristics.\nSMT-Explainability is freely available on Github:\nhttps://github.com/SMTorg/smt-explainability .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19496v1",
    "published_date": "2025-03-25 09:38:27 UTC",
    "updated_date": "2025-03-25 09:38:27 UTC"
  },
  {
    "arxiv_id": "2503.19941v1",
    "title": "Body Discovery of Embodied AI",
    "authors": [
      "Zhe Sun",
      "Pengfei Tian",
      "Xiaozhu Hu",
      "Xiaoyu Zhao",
      "Huiying Li",
      "Zhenliang Zhang"
    ],
    "abstract": "In the pursuit of realizing artificial general intelligence (AGI), the\nimportance of embodied artificial intelligence (AI) becomes increasingly\napparent. Following this trend, research integrating robots with AGI has become\nprominent. As various kinds of embodiments have been designed, adaptability to\ndiverse embodiments will become important to AGI. We introduce a new challenge,\ntermed \"Body Discovery of Embodied AI\", focusing on tasks of recognizing\nembodiments and summarizing neural signal functionality. The challenge\nencompasses the precise definition of an AI body and the intricate task of\nidentifying embodiments in dynamic environments, where conventional approaches\noften prove inadequate. To address these challenges, we apply causal inference\nmethod and evaluate it by developing a simulator tailored for testing\nalgorithms with virtual environments. Finally, we validate the efficacy of our\nalgorithms through empirical testing, demonstrating their robust performance in\nvarious scenarios based on virtual environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19941v1",
    "published_date": "2025-03-25 09:21:10 UTC",
    "updated_date": "2025-03-25 09:21:10 UTC"
  },
  {
    "arxiv_id": "2503.19474v2",
    "title": "A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition",
    "authors": [
      "Yaomin Shen",
      "Xiaojian Lin",
      "Wei Fan"
    ],
    "abstract": "In the domain of multimodal intent recognition (MIR), the objective is to\nrecognize human intent by integrating a variety of modalities, such as language\ntext, body gestures, and tones. However, existing approaches face difficulties\nadequately capturing the intrinsic connections between the modalities and\noverlooking the corresponding semantic representations of intent. To address\nthese limitations, we present the Anchor-based Multimodal Embedding with\nSemantic Synchronization (A-MESS) framework. We first design an Anchor-based\nMultimodal Embedding (A-ME) module that employs an anchor-based embedding\nfusion mechanism to integrate multimodal inputs. Furthermore, we develop a\nSemantic Synchronization (SS) strategy with the Triplet Contrastive Learning\npipeline, which optimizes the process by synchronizing multimodal\nrepresentation with label descriptions produced by the large language model.\nComprehensive experiments indicate that our A-MESS achieves state-of-the-art\nand provides substantial insight into multimodal representation and downstream\ntasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICME2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19474v2",
    "published_date": "2025-03-25 09:09:30 UTC",
    "updated_date": "2025-04-02 03:33:40 UTC"
  },
  {
    "arxiv_id": "2503.19470v2",
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
    "authors": [
      "Mingyang Chen",
      "Tianpeng Li",
      "Haoze Sun",
      "Yijie Zhou",
      "Chenzheng Zhu",
      "Haofen Wang",
      "Jeff Z. Pan",
      "Wen Zhang",
      "Huajun Chen",
      "Fan Yang",
      "Zenan Zhou",
      "Weipeng Chen"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.19470v2",
    "published_date": "2025-03-25 09:00:58 UTC",
    "updated_date": "2025-03-27 05:56:31 UTC"
  },
  {
    "arxiv_id": "2503.19469v2",
    "title": "Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning",
    "authors": [
      "Fred Philippy",
      "Siwen Guo",
      "Cedric Lothritz",
      "Jacques Klein",
      "Tegawendé F. Bissyandé"
    ],
    "abstract": "In NLP, Zero-Shot Classification (ZSC) has become essential for enabling\nmodels to classify text into categories unseen during training, particularly in\nlow-resource languages and domains where labeled data is scarce. While\npretrained language models (PLMs) have shown promise in ZSC, they often rely on\nlarge training datasets or external knowledge, limiting their applicability in\nmultilingual and low-resource scenarios. Recent approaches leveraging natural\nlanguage prompts reduce the dependence on large training datasets but struggle\nto effectively incorporate available labeled data from related classification\ntasks, especially when these datasets originate from different languages or\ndistributions. Moreover, existing prompt-based methods typically rely on\nmanually crafted prompts in a specific language, limiting their adaptability\nand effectiveness in cross-lingual settings. To address these challenges, we\nintroduce RoSPrompt, a lightweight and data-efficient approach for training\nsoft prompts that enhance cross-lingual ZSC while ensuring robust\ngeneralization across data distribution shifts. RoSPrompt is designed for small\nmultilingual PLMs, enabling them to leverage high-resource languages to improve\nperformance in low-resource settings without requiring extensive fine-tuning or\nhigh computational costs. We evaluate our approach on multiple multilingual\nPLMs across datasets covering 106 languages, demonstrating strong cross-lingual\ntransfer performance and robust generalization capabilities over unseen\nclasses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Workshop on Language Models for Underserved Communities (co-located\n  with NAACL 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.19469v2",
    "published_date": "2025-03-25 09:00:25 UTC",
    "updated_date": "2025-03-28 09:23:44 UTC"
  },
  {
    "arxiv_id": "2503.19455v1",
    "title": "Data-centric Federated Graph Learning with Large Language Models",
    "authors": [
      "Bo Yan",
      "Zhongjian Zhang",
      "Huabin Sun",
      "Mengmei Zhang",
      "Yang Cao",
      "Chuan Shi"
    ],
    "abstract": "In federated graph learning (FGL), a complete graph is divided into multiple\nsubgraphs stored in each client due to privacy concerns, and all clients\njointly train a global graph model by only transmitting model parameters. A\npain point of FGL is the heterogeneity problem, where nodes or structures\npresent non-IID properties among clients (e.g., different node label\ndistributions), dramatically undermining the convergence and performance of\nFGL. To address this, existing efforts focus on design strategies at the model\nlevel, i.e., they design models to extract common knowledge to mitigate\nheterogeneity. However, these model-level strategies fail to fundamentally\naddress the heterogeneity problem as the model needs to be designed from\nscratch when transferring to other tasks. Motivated by large language models\n(LLMs) having achieved remarkable success, we aim to utilize LLMs to fully\nunderstand and augment local text-attributed graphs, to address data\nheterogeneity at the data level. In this paper, we propose a general framework\nLLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks\ntheoretically. Specifically, for each client, it first utilizes the LLM to\ngenerate missing neighbors and then infers connections between generated nodes\nand raw nodes. To improve the quality of generated nodes, we design a novel\nfederated generation-and-reflection mechanism for LLMs, without the need to\nmodify the parameters of the LLM but relying solely on the collective feedback\nfrom all clients. After neighbor generation, all the clients utilize a\npre-trained edge predictor to infer the missing edges. Furthermore, our\nframework can seamlessly integrate as a plug-in with existing FGL methods.\nExperiments on three real-world datasets demonstrate the superiority of our\nmethod compared to advanced baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ongoing work",
    "pdf_url": "http://arxiv.org/pdf/2503.19455v1",
    "published_date": "2025-03-25 08:43:08 UTC",
    "updated_date": "2025-03-25 08:43:08 UTC"
  },
  {
    "arxiv_id": "2503.19449v1",
    "title": "VecTrans: LLM Transformation Framework for Better Auto-vectorization on High-performance CPU",
    "authors": [
      "Zhongchun Zheng",
      "Long Cheng",
      "Lu Li",
      "Rodrigo C. O. Rocha",
      "Tianyi Liu",
      "Wei Wei",
      "Xianwei Zhang",
      "Yaoqing Gao"
    ],
    "abstract": "Large language models (LLMs) have demonstrated great capabilities in code\ngeneration, yet their effective application in compiler optimizations remains\nan open challenge due to issues such as hallucinations and a lack of\ndomain-specific reasoning. Vectorization, a crucial optimization for enhancing\ncode performance, often fails because of the compiler's inability to recognize\ncomplex code patterns, which commonly require extensive empirical expertise.\nLLMs, with their ability to capture intricate patterns, thus providing a\npromising solution to this challenge. This paper presents VecTrans, a novel\nframework that leverages LLMs to enhance compiler-based code vectorization.\nVecTrans first employs compiler analysis to identify potentially vectorizable\ncode regions. It then utilizes an LLM to refactor these regions into patterns\nthat are more amenable to the compiler's auto-vectorization. To ensure semantic\ncorrectness, VecTrans further integrates a hybrid validation mechanism at the\nintermediate representation (IR) level. With the above efforts, VecTrans\ncombines the adaptability of LLMs with the precision of compiler vectorization,\nthereby effectively opening up the vectorization opportunities. Experimental\nresults show that among all 50 TSVC functions unvectorizable by Clang, GCC, and\nBiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves\nan average speedup of 2.02x, greatly surpassing state-of-the-art performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19449v1",
    "published_date": "2025-03-25 08:39:35 UTC",
    "updated_date": "2025-03-25 08:39:35 UTC"
  },
  {
    "arxiv_id": "2503.19940v1",
    "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
    "authors": [
      "Qiusheng Huang",
      "Xiaohui Zhong",
      "Xu Fan",
      "Lei Chen",
      "Hao Li"
    ],
    "abstract": "Similar to conventional video generation, current deep learning-based weather\nprediction frameworks often lack explicit physical constraints, leading to\nunphysical outputs that limit their reliability for operational forecasting.\nAmong various physical processes requiring proper representation, radiation\nplays a fundamental role as it drives Earth's weather and climate systems.\nHowever, accurate simulation of radiative transfer processes remains\nchallenging for traditional numerical weather prediction (NWP) models due to\ntheir inherent complexity and high computational costs. Here, we propose\nFuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance\nweather forecast accuracy while enforcing physical consistency. FuXi-RTM\nintegrates a primary forecasting model (FuXi) with a fixed deep learning-based\nradiative transfer model (DLRTM) surrogate that efficiently replaces\nconventional radiation parameterization schemes. This represents the first deep\nlearning-based weather forecasting framework to explicitly incorporate physical\nprocess modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM\noutperforms its unconstrained counterpart in 88.51% of 3320 variable and lead\ntime combinations, with improvements in radiative flux predictions. By\nincorporating additional physical processes, FuXi-RTM paves the way for\nnext-generation weather forecasting systems that are both accurate and\nphysically consistent.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19940v1",
    "published_date": "2025-03-25 08:21:58 UTC",
    "updated_date": "2025-03-25 08:21:58 UTC"
  },
  {
    "arxiv_id": "2503.19426v1",
    "title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models",
    "authors": [
      "Suyoung Bae",
      "YunSeok Choi",
      "Jee-Hyong Lee"
    ],
    "abstract": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 main. 20 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19426v1",
    "published_date": "2025-03-25 08:16:35 UTC",
    "updated_date": "2025-03-25 08:16:35 UTC"
  },
  {
    "arxiv_id": "2503.19394v1",
    "title": "Quantifying Symptom Causality in Clinical Decision Making: An Exploration Using CausaLM",
    "authors": [
      "Mehul Shetty",
      "Connor Jordan"
    ],
    "abstract": "Current machine learning approaches to medical diagnosis often rely on\ncorrelational patterns between symptoms and diseases, risking misdiagnoses when\nsymptoms are ambiguous or common across multiple conditions. In this work, we\nmove beyond correlation to investigate the causal influence of key\nsymptoms-specifically \"chest pain\" on diagnostic predictions. Leveraging the\nCausaLM framework, we generate counterfactual text representations in which\ntarget concepts are effectively \"forgotten\" enabling a principled estimation of\nthe causal effect of that concept on a model's predicted disease distribution.\nBy employing Textual Representation-based Average Treatment Effect (TReATE), we\nquantify how the presence or absence of a symptom shapes the model's diagnostic\noutcomes, and contrast these findings against correlation-based baselines such\nas CONEXP. Our results offer deeper insight into the decision-making behavior\nof clinical NLP models and have the potential to inform more trustworthy,\ninterpretable, and causally-grounded decision support tools in medical\npractice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19394v1",
    "published_date": "2025-03-25 06:59:21 UTC",
    "updated_date": "2025-03-25 06:59:21 UTC"
  },
  {
    "arxiv_id": "2504.03700v1",
    "title": "SAFE: Self-Adjustment Federated Learning Framework for Remote Sensing Collaborative Perception",
    "authors": [
      "Xiaohe Li",
      "Haohua Wu",
      "Jiahao Li",
      "Zide Fan",
      "Kaixin Zhang",
      "Xinming Li",
      "Yunping Ge",
      "Xinyu Zhao"
    ],
    "abstract": "The rapid increase in remote sensing satellites has led to the emergence of\ndistributed space-based observation systems. However, existing distributed\nremote sensing models often rely on centralized training, resulting in data\nleakage, communication overhead, and reduced accuracy due to data distribution\ndiscrepancies across platforms. To address these challenges, we propose the\n\\textit{Self-Adjustment FEderated Learning} (SAFE) framework, which\ninnovatively leverages federated learning to enhance collaborative sensing in\nremote sensing scenarios. SAFE introduces four key strategies: (1)\n\\textit{Class Rectification Optimization}, which autonomously addresses class\nimbalance under unknown local and global distributions. (2) \\textit{Feature\nAlignment Update}, which mitigates Non-IID data issues via locally controlled\nEMA updates. (3) \\textit{Dual-Factor Modulation Rheostat}, which dynamically\nbalances optimization effects during training. (4) \\textit{Adaptive Context\nEnhancement}, which is designed to improve model performance by dynamically\nrefining foreground regions, ensuring computational efficiency with accuracy\nimprovement across distributed satellites. Experiments on real-world image\nclassification and object segmentation datasets validate the effectiveness and\nreliability of the SAFE framework in complex remote sensing scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03700v1",
    "published_date": "2025-03-25 06:39:34 UTC",
    "updated_date": "2025-03-25 06:39:34 UTC"
  },
  {
    "arxiv_id": "2503.21807v1",
    "title": "LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning",
    "authors": [
      "Yuan Wei",
      "Xiaohan Shan",
      "Jianmin Li"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) faces two critical bottlenecks\ndistinct from single-agent RL: credit assignment in cooperative tasks and\npartial observability of environmental states. We propose LERO, a framework\nintegrating Large language models (LLMs) with evolutionary optimization to\naddress these MARL-specific challenges. The solution centers on two\nLLM-generated components: a hybrid reward function that dynamically allocates\nindividual credit through reward decomposition, and an observation enhancement\nfunction that augments partial observations with inferred environmental\ncontext. An evolutionary algorithm optimizes these components through iterative\nMARL training cycles, where top-performing candidates guide subsequent LLM\ngenerations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate\nLERO's superiority over baseline methods, with improved task performance and\ntraining efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21807v1",
    "published_date": "2025-03-25 06:28:42 UTC",
    "updated_date": "2025-03-25 06:28:42 UTC"
  },
  {
    "arxiv_id": "2503.19382v1",
    "title": "Causal invariant geographic network representations with feature and structural distribution shifts",
    "authors": [
      "Yuhan Wang",
      "Silu He",
      "Qinyao Luo",
      "Hongyuan Yuan",
      "Ling Zhao",
      "Jiawei Zhu",
      "Haifeng Li"
    ],
    "abstract": "The existing methods learn geographic network representations through deep\ngraph neural networks (GNNs) based on the i.i.d. assumption. However, the\nspatial heterogeneity and temporal dynamics of geographic data make the\nout-of-distribution (OOD) generalisation problem particularly salient. The\nlatter are particularly sensitive to distribution shifts (feature and\nstructural shifts) between testing and training data and are the main causes of\nthe OOD generalisation problem. Spurious correlations are present between\ninvariant and background representations due to selection biases and\nenvironmental effects, resulting in the model extremes being more likely to\nlearn background representations. The existing approaches focus on background\nrepresentation changes that are determined by shifts in the feature\ndistributions of nodes in the training and test data while ignoring changes in\nthe proportional distributions of heterogeneous and homogeneous neighbour\nnodes, which we refer to as structural distribution shifts. We propose a\nfeature-structure mixed invariant representation learning (FSM-IRL) model that\naccounts for both feature distribution shifts and structural distribution\nshifts. To address structural distribution shifts, we introduce a sampling\nmethod based on causal attention, encouraging the model to identify nodes\npossessing strong causal relationships with labels or nodes that are more\nsimilar to the target node. Inspired by the Hilbert-Schmidt independence\ncriterion, we implement a reweighting strategy to maximise the orthogonality of\nthe node representations, thereby mitigating the spurious correlations among\nthe node representations and suppressing the learning of background\nrepresentations. Our experiments demonstrate that FSM-IRL exhibits strong\nlearning capabilities on both geographic and social network datasets in OOD\nscenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 3 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.19382v1",
    "published_date": "2025-03-25 06:21:57 UTC",
    "updated_date": "2025-03-25 06:21:57 UTC"
  },
  {
    "arxiv_id": "2503.19373v1",
    "title": "DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image",
    "authors": [
      "Hyeongjin Nam",
      "Donghwan Kim",
      "Jeongtaek Oh",
      "Kyoung Mu Lee"
    ],
    "abstract": "Most existing methods of 3D clothed human reconstruction from a single image\ntreat the clothed human as a single object without distinguishing between cloth\nand human body. In this regard, we present DeClotH, which separately\nreconstructs 3D cloth and human body from a single image. This task remains\nlargely unexplored due to the extreme occlusion between cloth and the human\nbody, making it challenging to infer accurate geometries and textures.\nMoreover, while recent 3D human reconstruction methods have achieved impressive\nresults using text-to-image diffusion models, directly applying such an\napproach to this problem often leads to incorrect guidance, particularly in\nreconstructing 3D cloth. To address these challenges, we propose two core\ndesigns in our framework. First, to alleviate the occlusion issue, we leverage\n3D template models of cloth and human body as regularizations, which provide\nstrong geometric priors to prevent erroneous reconstruction by the occlusion.\nSecond, we introduce a cloth diffusion model specifically designed to provide\ncontextual information about cloth appearance, thereby enhancing the\nreconstruction of 3D cloth. Qualitative and quantitative experiments\ndemonstrate that our proposed approach is highly effective in reconstructing\nboth 3D cloth and the human body. More qualitative results are provided at\nhttps://hygenie1228.github.io/DeClotH/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at CVPR 2025, 17 pages including the supplementary material",
    "pdf_url": "http://arxiv.org/pdf/2503.19373v1",
    "published_date": "2025-03-25 06:00:15 UTC",
    "updated_date": "2025-03-25 06:00:15 UTC"
  },
  {
    "arxiv_id": "2503.21806v1",
    "title": "Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages",
    "authors": [
      "Heqing Zou",
      "Fengmao Lv",
      "Desheng Zheng",
      "Eng Siong Chng",
      "Deepu Rajan"
    ],
    "abstract": "Multilingual speech emotion recognition aims to estimate a speaker's\nemotional state using a contactless method across different languages. However,\nvariability in voice characteristics and linguistic diversity poses significant\nchallenges for zero-shot speech emotion recognition, especially with\nmultilingual datasets. In this paper, we propose leveraging contrastive\nlearning to refine multilingual speech features and extend large language\nmodels for zero-shot multilingual speech emotion estimation. Specifically, we\nemploy a novel two-stage training framework to align speech signals with\nlinguistic features in the emotional space, capturing both emotion-aware and\nlanguage-agnostic speech representations. To advance research in this field, we\nintroduce a large-scale synthetic multilingual speech emotion dataset, M5SER.\nOur experiments demonstrate the effectiveness of the proposed method in both\nspeech emotion recognition and zero-shot multilingual speech emotion\nrecognition, including previously unseen datasets and languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21806v1",
    "published_date": "2025-03-25 05:58:18 UTC",
    "updated_date": "2025-03-25 05:58:18 UTC"
  },
  {
    "arxiv_id": "2503.19371v2",
    "title": "Flow to Learn: Flow Matching on Neural Network Parameters",
    "authors": [
      "Daniel Saragih",
      "Deyu Cao",
      "Tejas Balaji",
      "Ashwin Santhosh"
    ],
    "abstract": "Foundational language models show a remarkable ability to learn new concepts\nduring inference via context data. However, similar work for images lag behind.\nTo address this challenge, we introduce FLoWN, a flow matching model that\nlearns to generate neural network parameters for different tasks. Our approach\nmodels the flow on latent space, while conditioning the process on context\ndata. Experiments verify that FLoWN attains various desiderata for a\nmeta-learning model. In addition, it matches or exceeds baselines on\nin-distribution tasks, provides better initializations for classifier training,\nand is performant on out-of-distribution few-shot tasks while having a\nfine-tuning mechanism to improve performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19371v2",
    "published_date": "2025-03-25 05:57:50 UTC",
    "updated_date": "2025-04-19 13:42:41 UTC"
  },
  {
    "arxiv_id": "2503.21805v2",
    "title": "ImF: Implicit Fingerprint for Large Language Models",
    "authors": [
      "Wu jiaxuan",
      "Peng Wanli",
      "Fu hang",
      "Xue Yiming",
      "Wen juan"
    ],
    "abstract": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing fingerprinting techniques\ntypically embed identifiable patterns with weak semantic coherence, resulting\nin fingerprints that significantly differ from the natural question-answering\n(QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of\nthe embedded fingerprints and makes them vulnerable to adversarial attacks. In\nthis paper, we first demonstrate the critical vulnerability of existing\nfingerprint embedding methods by introducing a novel adversarial attack named\nGeneration Revision Intervention (GRI) attack. GRI attack exploits the semantic\nfragility of current fingerprinting methods, effectively erasing fingerprints\nby disrupting their weakly correlated semantic structures. Our empirical\nevaluation highlights that traditional fingerprinting approaches are\nsignificantly compromised by the GRI attack, revealing severe limitations in\ntheir robustness under realistic adversarial conditions. To advance the\nstate-of-the-art in model fingerprinting, we propose a novel model fingerprint\nparadigm called Implicit Fingerprints (ImF). ImF leverages steganography\ntechniques to subtly embed ownership information within natural texts,\nsubsequently using Chain-of-Thought (CoT) prompting to construct semantically\ncoherent and contextually natural QA pairs. This design ensures that\nfingerprints seamlessly integrate with the standard model behavior, remaining\nindistinguishable from regular outputs and substantially reducing the risk of\naccidental triggering and targeted removal. We conduct a comprehensive\nevaluation of ImF on 15 diverse LLMs, spanning different architectures and\nvarying scales.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21805v2",
    "published_date": "2025-03-25 05:47:34 UTC",
    "updated_date": "2025-05-17 23:00:12 UTC"
  },
  {
    "arxiv_id": "2504.03699v3",
    "title": "Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance",
    "authors": [
      "Ying-Jung Chen",
      "Ahmad Albarqawi",
      "Chi-Sheng Chen"
    ],
    "abstract": "Recent advances in the data-driven medicine approach, which integrates\nethically managed and explainable artificial intelligence into clinical\ndecision support systems (CDSS), are critical to ensure reliable and effective\npatient care. This paper focuses on comparing novel agent system designs that\nuse modular agents to analyze laboratory results, vital signs, and clinical\ncontext, and to predict and validate results. We implement our agent system\nwith the eICU database, including running lab analysis, vitals-only\ninterpreters, and contextual reasoners agents first, then sharing the memory\ninto the integration agent, prediction agent, transparency agent, and a\nvalidation agent. Our results suggest that the multi-agent system (MAS)\nperformed better than the single-agent system (SAS) with mortality prediction\naccuracy (59%, 56%) and the mean error for length of stay (LOS)(4.37 days, 5.82\ndays), respectively. However, the transparency score for the SAS (86.21) is\nslightly better than the transparency score for MAS (85.5). Finally, this study\nsuggests that our agent-based framework not only improves process transparency\nand prediction accuracy but also strengthens trustworthy AI-assisted decision\nsupport in an intensive care setting.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.MA",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03699v3",
    "published_date": "2025-03-25 05:32:43 UTC",
    "updated_date": "2025-04-15 05:26:26 UTC"
  },
  {
    "arxiv_id": "2503.21804v2",
    "title": "Comparison of Metadata Representation Models for Knowledge Graph Embeddings",
    "authors": [
      "Shusaku Egami",
      "Kyoumoto Matsushita",
      "Takanori Ugai",
      "Ken Fukuda"
    ],
    "abstract": "Hyper-relational Knowledge Graphs (HRKGs) extend traditional KGs beyond\nbinary relations, enabling the representation of contextual, provenance, and\ntemporal information in domains, such as historical events, sensor data, video\ncontent, and narratives. HRKGs can be structured using several Metadata\nRepresentation Models (MRMs), including Reification (REF), Singleton Property\n(SGP), and RDF-star (RDR). However, the effects of different MRMs on KG\nEmbedding (KGE) and Link Prediction (LP) models remain unclear. This study\nevaluates MRMs in the context of LP tasks, identifies the limitations of\nexisting evaluation frameworks, and introduces a new task that ensures fair\ncomparisons across MRMs. Furthermore, we propose a framework that effectively\nreflects the knowledge representations of the three MRMs in latent space.\nExperiments on two types of datasets reveal that REF performs well in simple\nHRKGs, whereas SGP is less effective. However, in complex HRKGs, the\ndifferences among MRMs in the LP tasks are minimal. Our findings contribute to\nan optimal knowledge representation strategy for HRKGs in LP tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "I.2.4; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 9 Figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21804v2",
    "published_date": "2025-03-25 04:46:23 UTC",
    "updated_date": "2025-03-31 04:31:23 UTC"
  },
  {
    "arxiv_id": "2504.00008v1",
    "title": "Tensor Generalized Approximate Message Passing",
    "authors": [
      "Yinchuan Li",
      "Guangchen Lan",
      "Xiaodong Wang"
    ],
    "abstract": "We propose a tensor generalized approximate message passing (TeG-AMP)\nalgorithm for low-rank tensor inference, which can be used to solve tensor\ncompletion and decomposition problems. We derive TeG-AMP algorithm as an\napproximation of the sum-product belief propagation algorithm in high\ndimensions where the central limit theorem and Taylor series approximations are\napplicable. As TeG-AMP is developed based on a general TR decomposition model,\nit can be directly applied to many low-rank tensor types. Moreover, our TeG-AMP\ncan be simplified based on the CP decomposition model and a tensor simplified\nAMP is proposed for low CP-rank tensor inference problems. Experimental results\ndemonstrate that the proposed methods significantly improve recovery\nperformances since it takes full advantage of tensor structures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "E.4; I.2.0; I.2.6; I.4"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00008v1",
    "published_date": "2025-03-25 04:17:10 UTC",
    "updated_date": "2025-03-25 04:17:10 UTC"
  },
  {
    "arxiv_id": "2503.21803v1",
    "title": "Forecasting Volcanic Radiative Power (VPR) at Fuego Volcano Using Bayesian Regularized Neural Network",
    "authors": [
      "Snehamoy Chatterjee",
      "Greg Waite",
      "Sidike Paheding",
      "Luke Bowman"
    ],
    "abstract": "Forecasting volcanic activity is critical for hazard assessment and risk\nmitigation. Volcanic Radiative Power (VPR), derived from thermal remote sensing\ndata, serves as an essential indicator of volcanic activity. In this study, we\nemploy Bayesian Regularized Neural Networks (BRNN) to predict future VPR values\nbased on historical data from Fuego Volcano, comparing its performance against\nScaled Conjugate Gradient (SCG) and Levenberg-Marquardt (LM) models. The\nresults indicate that BRNN outperforms SCG and LM, achieving the lowest mean\nsquared error (1.77E+16) and the highest R-squared value (0.50), demonstrating\nits superior ability to capture VPR variability while minimizing overfitting.\nDespite these promising results, challenges remain in improving the model's\npredictive accuracy. Future research should focus on integrating additional\ngeophysical parameters, such as seismic and gas emission data, to enhance\nforecasting precision. The findings highlight the potential of machine learning\nmodels, particularly BRNN, in advancing volcanic activity forecasting,\ncontributing to more effective early warning systems for volcanic hazards.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21803v1",
    "published_date": "2025-03-25 04:15:24 UTC",
    "updated_date": "2025-03-25 04:15:24 UTC"
  },
  {
    "arxiv_id": "2503.19339v3",
    "title": "Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture",
    "authors": [
      "Amna Naeem",
      "Muazzam A. Khan",
      "Nada Alasbali",
      "Jawad Ahmad",
      "Aizaz Ahmad Khattak",
      "Muhammad Shahbaz Khan"
    ],
    "abstract": "The ever-increasing security vulnerabilities in the Internet-of-Things (IoT)\nsystems require improved threat detection approaches. This paper presents a\ncompact and efficient approach to detect botnet attacks by employing an\nintegrated approach that consists of traffic pattern analysis, temporal support\nlearning, and focused feature extraction. The proposed attention-based model\nbenefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification\naccuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while\nmaintaining high precision and recall across various scenarios. The proposed\nmodel's performance is further validated by key parameters, such as Mathews\nCorrelation Coefficient and Cohen's kappa Correlation Coefficient. The\nclose-to-ideal results for these parameters demonstrate the proposed model's\nability to detect botnet attacks accurately and efficiently in practical\nsettings and on unseen data. The proposed model proved to be a powerful defence\nmechanism for IoT networks to face emerging security challenges.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19339v3",
    "published_date": "2025-03-25 04:12:14 UTC",
    "updated_date": "2025-05-01 15:12:42 UTC"
  },
  {
    "arxiv_id": "2503.19329v1",
    "title": "Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection",
    "authors": [
      "Yongting Hu",
      "Yuxin Lin",
      "Chengliang Liu",
      "Xiaoling Luo",
      "Xiaoyan Dou",
      "Qihao Xu",
      "Yong Xu"
    ],
    "abstract": "Multi-view diabetic retinopathy (DR) detection has recently emerged as a\npromising method to address the issue of incomplete lesions faced by\nsingle-view DR. However, it is still challenging due to the variable sizes and\nscattered locations of lesions. Furthermore, existing multi-view DR methods\ntypically merge multiple views without considering the correlations and\nredundancies of lesion information across them. Therefore, we propose a novel\nmethod to overcome the challenges of difficult lesion information learning and\ninadequate multi-view fusion. Specifically, we introduce a two-branch network\nto obtain both local lesion features and their global dependencies. The\nhigh-frequency component of the wavelet transform is used to exploit lesion\nedge information, which is then enhanced by global semantic to facilitate\ndifficult lesion learning. Additionally, we present a cross-view fusion module\nto improve multi-view fusion and reduce redundancy. Experimental results on\nlarge public datasets demonstrate the effectiveness of our method. The code is\nopen sourced on https://github.com/HuYongting/WGLIN.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19329v1",
    "published_date": "2025-03-25 03:44:57 UTC",
    "updated_date": "2025-03-25 03:44:57 UTC"
  },
  {
    "arxiv_id": "2503.19328v1",
    "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents",
    "authors": [
      "Vidya Srinivas",
      "Xuhai Xu",
      "Xin Liu",
      "Kumar Ayush",
      "Isaac Galatzer-Levy",
      "Shwetak Patel",
      "Daniel McDuff",
      "Tim Althoff"
    ],
    "abstract": "While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19328v1",
    "published_date": "2025-03-25 03:44:31 UTC",
    "updated_date": "2025-03-25 03:44:31 UTC"
  },
  {
    "arxiv_id": "2503.22714v1",
    "title": "TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER",
    "authors": [
      "Sergio Torres Aguilar"
    ],
    "abstract": "This paper introduces TRIDIS (Tria Digita Scribunt), an open-source corpus of\nmedieval and early modern manuscripts. TRIDIS aggregates multiple legacy\ncollections (all published under open licenses) and incorporates large metadata\ndescriptions. While prior publications referenced some portions of this corpus,\nhere we provide a unified overview with a stronger focus on its constitution.\nWe describe (i) the narrative, chronological, and editorial background of each\nmajor sub-corpus, (ii) its semi-diplomatic transcription rules (expansion,\nnormalization, punctuation), (iii) a strategy for challenging out-of-domain\ntest splits driven by outlier detection in a joint embedding space, and (iv)\npreliminary baseline experiments using TrOCR and MiniCPM2.5 comparing random\nand outlier-based test partitions. Overall, TRIDIS is designed to stimulate\njoint robust Handwritten Text Recognition (HTR) and Named Entity Recognition\n(NER) research across medieval and early modern textual heritage.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.22714v1",
    "published_date": "2025-03-25 03:44:11 UTC",
    "updated_date": "2025-03-25 03:44:11 UTC"
  },
  {
    "arxiv_id": "2503.19326v2",
    "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps",
    "authors": [
      "Yu Cui",
      "Bryan Hooi",
      "Yujun Cai",
      "Yiwei Wang"
    ],
    "abstract": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19326v2",
    "published_date": "2025-03-25 03:43:11 UTC",
    "updated_date": "2025-04-01 00:07:54 UTC"
  },
  {
    "arxiv_id": "2503.19311v1",
    "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text",
    "authors": [
      "Weizhi Chen",
      "Jingbo Chen",
      "Yupeng Deng",
      "Jiansheng Chen",
      "Yuman Feng",
      "Zhihao Xi",
      "Diyou Liu",
      "Kai Li",
      "Yu Meng"
    ],
    "abstract": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.19311v1",
    "published_date": "2025-03-25 03:17:42 UTC",
    "updated_date": "2025-03-25 03:17:42 UTC"
  },
  {
    "arxiv_id": "2503.19302v1",
    "title": "Observation Adaptation via Annealed Importance Resampling for Partially Observable Markov Decision Processes",
    "authors": [
      "Yunuo Zhang",
      "Baiting Luo",
      "Ayan Mukhopadhyay",
      "Abhishek Dubey"
    ],
    "abstract": "Partially observable Markov decision processes (POMDPs) are a general\nmathematical model for sequential decision-making in stochastic environments\nunder state uncertainty. POMDPs are often solved \\textit{online}, which enables\nthe algorithm to adapt to new information in real time. Online solvers\ntypically use bootstrap particle filters based on importance resampling for\nupdating the belief distribution. Since directly sampling from the ideal state\ndistribution given the latest observation and previous state is infeasible,\nparticle filters approximate the posterior belief distribution by propagating\nstates and adjusting weights through prediction and resampling steps. However,\nin practice, the importance resampling technique often leads to particle\ndegeneracy and sample impoverishment when the state transition model poorly\naligns with the posterior belief distribution, especially when the received\nobservation is highly informative. We propose an approach that constructs a\nsequence of bridge distributions between the state-transition and optimal\ndistributions through iterative Monte Carlo steps, better accommodating noisy\nobservations in online POMDP solvers. Our algorithm demonstrates significantly\nsuperior performance compared to state-of-the-art methods when evaluated across\nmultiple challenging POMDP domains.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as Oral Presentation to ICAPS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19302v1",
    "published_date": "2025-03-25 03:05:00 UTC",
    "updated_date": "2025-03-25 03:05:00 UTC"
  },
  {
    "arxiv_id": "2503.19292v1",
    "title": "Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for Parkinson's Disease Screening in OCT",
    "authors": [
      "Xiaoqing Zhang",
      "Hanfeng Shi",
      "Xiangyu Li",
      "Haili Ye",
      "Tao Xu",
      "Na Li",
      "Yan Hu",
      "Fan Lv",
      "Jiangfan Chen",
      "Jiang Liu"
    ],
    "abstract": "Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally.\nThe eye's retina is an extension of the brain and has great potential in PD\nscreening. Recent studies have suggested that texture features extracted from\nretinal layers can be adopted as biomarkers for PD diagnosis under optical\ncoherence tomography (OCT) images. Frequency domain learning techniques can\nenhance the feature representations of deep neural networks (DNNs) by\ndecomposing frequency components involving rich texture features. Additionally,\nprevious works have not exploited texture features for automated PD screening\nin OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet\nFilter (AWF) that serves as the Practical Texture Feature Amplifier to fully\nleverage the merits of texture features to boost the PD screening performance\nof DNNs with the aid of frequency domain learning. Specifically, AWF first\nenhances texture feature representation diversities via channel mixer, then\nemphasizes informative texture feature representations with the well-designed\nadaptive wavelet filtering token mixer. By combining the AWFs with the DNN\nstem, AWFNet is constructed for automated PD screening. Additionally, we\nintroduce a novel Balanced Confidence (BC) Loss by mining the potential of\nsample-wise predicted probabilities of all classes and class frequency prior,\nto further boost the PD screening performance and trustworthiness of AWFNet.\nThe extensive experiments manifest the superiority of our AWFNet and BC over\nstate-of-the-art methods in terms of PD screening performance and\ntrustworthiness.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19292v1",
    "published_date": "2025-03-25 02:47:24 UTC",
    "updated_date": "2025-03-25 02:47:24 UTC"
  },
  {
    "arxiv_id": "2503.19285v2",
    "title": "No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism",
    "authors": [
      "Yubo Li",
      "Xinyu Yao",
      "Rema Padman"
    ],
    "abstract": "Despite the outstanding performance of deep learning models in clinical\nprediction tasks, explainability remains a significant challenge. Inspired by\ntransformer architectures, we introduce the Temporal-Feature Cross Attention\nMechanism (TFCAM), a novel deep learning framework designed to capture dynamic\ninteractions among clinical features across time, enhancing both predictive\naccuracy and interpretability. In an experiment with 1,422 patients with\nChronic Kidney Disease, predicting progression to End-Stage Renal Disease,\nTFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an\nF1-score of 0.69. Beyond performance gains, TFCAM provides multi-level\nexplainability by identifying critical temporal periods, ranking feature\nimportance, and quantifying how features influence each other across time\nbefore affecting predictions. Our approach addresses the \"black box\"\nlimitations of deep learning in healthcare, offering clinicians transparent\ninsights into disease progression mechanisms while maintaining state-of-the-art\npredictive performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 figures, submitted to AMIA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.19285v2",
    "published_date": "2025-03-25 02:35:08 UTC",
    "updated_date": "2025-03-26 22:09:44 UTC"
  },
  {
    "arxiv_id": "2503.19281v1",
    "title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
    "authors": [
      "Feiyang Wang",
      "Xiaomin Yu",
      "Wangyu Wu"
    ],
    "abstract": "Proving Rubik's Cube theorems at the high level represents a notable\nmilestone in human-level spatial imagination and logic thinking and reasoning.\nTraditional Rubik's Cube robots, relying on complex vision systems and fixed\nalgorithms, often struggle to adapt to complex and dynamic scenarios. To\novercome this limitation, we introduce CubeRobot, a novel vision-language model\n(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with\nmultimodal understanding and execution capabilities. We used the CubeCoT image\ndataset, which contains multiple-level tasks (43 subtasks in total) that humans\nare unable to handle, encompassing various cube states. We incorporate a\ndual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting\ntask-related features from VLM-generated planning queries, thus enabling\nCubeRobot to independent planning, decision-making, reflection and separate\nmanagement of high- and low-level Rubik's Cube tasks. Furthermore, in low-level\nRubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of\n100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of\n80% in high-level tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19281v1",
    "published_date": "2025-03-25 02:23:47 UTC",
    "updated_date": "2025-03-25 02:23:47 UTC"
  },
  {
    "arxiv_id": "2503.19280v1",
    "title": "LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs",
    "authors": [
      "Amogh Inamdar",
      "Uzay Macar",
      "Michel Vazirani",
      "Michael Tarnow",
      "Zarina Mustapha",
      "Natalia Dittren",
      "Sam Sadeh",
      "Nakul Verma",
      "Ansaf Salleb-Aouissi"
    ],
    "abstract": "The study of propositional logic -- fundamental to the theory of computing --\nis a cornerstone of the undergraduate computer science curriculum. Learning to\nsolve logical proofs requires repeated guided practice, but undergraduate\nstudents often lack access to on-demand tutoring in a judgment-free\nenvironment. In this work, we highlight the need for guided practice tools in\nundergraduate mathematics education and outline the desiderata of an effective\npractice tool. We accordingly develop LogicLearner, a web application for\nguided logic proof practice. LogicLearner consists of an interface to attempt\nlogic proofs step-by-step and an automated proof solver to generate solutions\non the fly, allowing users to request guidance as needed. We pilot LogicLearner\nas a practice tool in two semesters of an undergraduate discrete mathematics\ncourse and receive strongly positive feedback for usability and pedagogical\nvalue in student surveys. To the best of our knowledge, LogicLearner is the\nonly learning tool that provides an end-to-end practice environment for logic\nproofs with immediate, judgment-free feedback.",
    "categories": [
      "cs.DM",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.DM",
    "comment": "32 pages, 27 figures, open-source codebase linked in paper",
    "pdf_url": "http://arxiv.org/pdf/2503.19280v1",
    "published_date": "2025-03-25 02:23:08 UTC",
    "updated_date": "2025-03-25 02:23:08 UTC"
  },
  {
    "arxiv_id": "2503.19276v1",
    "title": "Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications",
    "authors": [
      "Ben Rahman"
    ],
    "abstract": "Semantic segmentation has made significant strides in pixel-level image\nunderstanding, yet it remains limited in capturing contextual and semantic\nrelationships between objects. Current models, such as CNN and\nTransformer-based architectures, excel at identifying pixel-level features but\nfail to distinguish semantically similar objects (e.g., \"doctor\" vs. \"nurse\" in\na hospital scene) or understand complex contextual scenarios (e.g.,\ndifferentiating a running child from a regular pedestrian in autonomous\ndriving). To address these limitations, we proposed a novel Context-Aware\nSemantic Segmentation framework that integrates Large Language Models (LLMs)\nwith state-of-the-art vision backbones. Our hybrid model leverages the Swin\nTransformer for robust visual feature extraction and GPT-4 for enriching\nsemantic understanding through text embeddings. A Cross-Attention Mechanism is\nintroduced to align vision and language features, enabling the model to reason\nabout context more effectively. Additionally, Graph Neural Networks (GNNs) are\nemployed to model object relationships within the scene, capturing dependencies\nthat are overlooked by traditional models. Experimental results on benchmark\ndatasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the\nexisting methods in both pixel-level accuracy (mIoU) and contextual\nunderstanding (mAP). This work bridges the gap between vision and language,\npaving the path for more intelligent and context-aware vision systems in\napplications including autonomous driving, medical imaging, and robotics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19276v1",
    "published_date": "2025-03-25 02:12:35 UTC",
    "updated_date": "2025-03-25 02:12:35 UTC"
  },
  {
    "arxiv_id": "2503.19937v1",
    "title": "Reverse Prompt: Cracking the Recipe Inside Text-to-Image Generation",
    "authors": [
      "Zhiyao Ren",
      "Yibing Zhan",
      "Baosheng Yu",
      "Dacheng Tao"
    ],
    "abstract": "Text-to-image generation has become increasingly popular, but achieving the\ndesired images often requires extensive prompt engineering. In this paper, we\nexplore how to decode textual prompts from reference images, a process we refer\nto as image reverse prompt engineering. This technique enables us to gain\ninsights from reference images, understand the creative processes of great\nartists, and generate impressive new images. To address this challenge, we\npropose a method known as automatic reverse prompt optimization (ARPO).\nSpecifically, our method refines an initial prompt into a high-quality prompt\nthrough an iteratively imitative gradient prompt optimization process: 1)\ngenerating a recreated image from the current prompt to instantiate its\nguidance capability; 2) producing textual gradients, which are candidate\nprompts intended to reduce the difference between the recreated image and the\nreference image; 3) updating the current prompt with textual gradients using a\ngreedy search method to maximize the CLIP similarity between prompt and\nreference image. We compare ARPO with several baseline methods, including\nhandcrafted techniques, gradient-based prompt tuning methods, image captioning,\nand data-driven selection method. Both quantitative and qualitative results\ndemonstrate that our ARPO converges quickly to generate high-quality reverse\nprompts. More importantly, we can easily create novel images with diverse\nstyles and content by directly editing these reverse prompts. Code will be made\npublicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19937v1",
    "published_date": "2025-03-25 02:08:05 UTC",
    "updated_date": "2025-03-25 02:08:05 UTC"
  },
  {
    "arxiv_id": "2503.19267v1",
    "title": "NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios",
    "authors": [
      "Songyi Gao",
      "Zuolin Tu",
      "Rong-Jun Qin",
      "Yi-Hao Sun",
      "Xiong-Hui Chen",
      "Yang Yu"
    ],
    "abstract": "Offline reinforcement learning (RL) aims to learn from historical data\nwithout requiring (costly) access to the environment. To facilitate offline RL\nresearch, we previously introduced NeoRL, which highlighted that datasets from\nreal-world tasks are often conservative and limited. With years of experience\napplying offline RL to various domains, we have identified additional\nreal-world challenges. These include extremely conservative data distributions\nproduced by deployed control systems, delayed action effects caused by\nhigh-latency transitions, external factors arising from the uncontrollable\nvariance of transitions, and global safety constraints that are difficult to\nevaluate during the decision-making process. These challenges are\nunderrepresented in previous benchmarks but frequently occur in real-world\ntasks. To address this, we constructed the extended Near Real-World Offline RL\nBenchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along\nwith their corresponding evaluation simulators. Benchmarking results from\nstate-of-the-art offline RL approaches demonstrate that current methods often\nstruggle to outperform the data-collection behavior policy, highlighting the\nneed for more effective methods. We hope NeoRL-2 will accelerate the\ndevelopment of reinforcement learning algorithms for real-world applications.\nThe benchmark project page is available at https://github.com/polixir/NeoRL2.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.19267v1",
    "published_date": "2025-03-25 02:01:54 UTC",
    "updated_date": "2025-03-25 02:01:54 UTC"
  },
  {
    "arxiv_id": "2503.19260v1",
    "title": "Linguistic Blind Spots of Large Language Models",
    "authors": [
      "Jiali Cheng",
      "Hadi Amiri"
    ],
    "abstract": "Large language models (LLMs) are the foundation of many AI applications\ntoday. However, despite their remarkable proficiency in generating coherent\ntext, questions linger regarding their ability to perform fine-grained\nlinguistic annotation tasks, such as detecting nouns or verbs, or identifying\nmore complex syntactic structures like clauses in input texts. These tasks\nrequire precise syntactic and semantic understanding of input text, and when\nLLMs underperform on specific linguistic structures, it raises concerns about\ntheir reliability for detailed linguistic analysis and whether their (even\ncorrect) outputs truly reflect an understanding of the inputs. In this paper,\nwe empirically study the performance of recent LLMs on fine-grained linguistic\nannotation tasks. Through a series of experiments, we find that recent LLMs\nshow limited efficacy in addressing linguistic queries and often struggle with\nlinguistically complex inputs. We show that the most capable LLM (Llama3-70b)\nmakes notable errors in detecting linguistic structures, such as misidentifying\nembedded clauses, failing to recognize verb phrases, and confusing complex\nnominals with clauses. Our results provide insights to inform future\nadvancements in LLM design and development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Cognitive Modeling and Computational Linguistics Workshop",
    "pdf_url": "http://arxiv.org/pdf/2503.19260v1",
    "published_date": "2025-03-25 01:47:13 UTC",
    "updated_date": "2025-03-25 01:47:13 UTC"
  },
  {
    "arxiv_id": "2503.19223v1",
    "title": "Face Spoofing Detection using Deep Learning",
    "authors": [
      "Najeebullah",
      "Maaz Salman",
      "Zar Nawab Khan Swati"
    ],
    "abstract": "Digital image spoofing has emerged as a significant security threat in\nbiometric authentication systems, particularly those relying on facial\nrecognition. This study evaluates the performance of three vision based models,\nMobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in\nimage classification, utilizing a dataset of 150,986 images divided into\ntraining , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof\ndetection is critical for enhancing the security of image recognition systems,\nand this research compares the models effectiveness through accuracy,\nprecision, recall, and F1 score metrics. Results reveal that MobileNetV2\noutperforms other architectures on the test dataset, achieving an accuracy of\n91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared\nto ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation\ndataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17%\naccuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during\ntraining and superior generalization to unseen data, despite both models\nshowing signs of overfitting. These findings highlight MobileNetV2 balanced\nperformance and robustness, making it the preferred choice for spoof detection\napplications where reliability on new data is essential. The study underscores\nthe importance of model selection in security sensitive contexts and suggests\nMobileNetV2 as a practical solution for real world deployment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 9 figures,3 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.19223v1",
    "published_date": "2025-03-25 00:09:21 UTC",
    "updated_date": "2025-03-25 00:09:21 UTC"
  }
]
[
  {
    "arxiv_id": "2403.01332v1",
    "title": "Chaining thoughts and LLMs to learn DNA structural biophysics",
    "authors": [
      "Tyler D. Ross",
      "Ashwin Gopinath"
    ],
    "abstract": "The future development of an AI scientist, a tool that is capable of\nintegrating a variety of experimental data and generating testable hypotheses,\nholds immense potential. So far, bespoke machine learning models have been\ncreated to specialize in singular scientific tasks, but otherwise lack the\nflexibility of a general purpose model. Here, we show that a general purpose\nlarge language model, chatGPT 3.5-turbo, can be fine-tuned to learn the\nstructural biophysics of DNA. We find that both fine-tuning models to return\nchain-of-thought responses and chaining together models fine-tuned for subtasks\nhave an enhanced ability to analyze and design DNA sequences and their\nstructures.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01332v1",
    "published_date": "2024-03-02 22:38:01 UTC",
    "updated_date": "2024-03-02 22:38:01 UTC"
  },
  {
    "arxiv_id": "2403.04785v2",
    "title": "Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data",
    "authors": [
      "Jun-En Ding",
      "Phan Nguyen Minh Thao",
      "Wen-Chih Peng",
      "Jian-Zhe Wang",
      "Chun-Cheng Chug",
      "Min-Chen Hsieh",
      "Yun-Chien Tseng",
      "Ling Chen",
      "Dongsheng Luo",
      "Chi-Te Wang",
      "Pei-fu Chen",
      "Feng Liu",
      "Fang-Ming Hung"
    ],
    "abstract": "Chronic diseases such as diabetes are the leading causes of morbidity and\nmortality worldwide. Numerous research studies have been attempted with various\ndeep learning models in diagnosis. However, most previous studies had certain\nlimitations, including using publicly available datasets (e.g. MIMIC), and\nimbalanced data. In this study, we collected five-year electronic health\nrecords (EHRs) from the Taiwan hospital database, including 1,420,596 clinical\nnotes, 387,392 laboratory test results, and more than 1,505 laboratory test\nitems, focusing on research pre-training large language models. We proposed a\nnovel Large Language Multimodal Models (LLMMs) framework incorporating\nmultimodal data from clinical notes and laboratory test results for the\nprediction of chronic disease risk. Our method combined a text embedding\nencoder and multi-head attention layer to learn laboratory test values,\nutilizing a deep neural network (DNN) module to merge blood features with\nchronic disease semantics into a latent space. In our experiments, we observe\nthat clinicalBERT and PubMed-BERT, when combined with attention fusion, can\nachieve an accuracy of 73% in multiclass chronic diseases and diabetes\nprediction. By transforming laboratory test values into textual descriptions\nand employing the Flan T-5 model, we achieved a 76% Area Under the ROC Curve\n(AUROC), demonstrating the effectiveness of leveraging numerical text data for\ntraining and inference in language models. This approach significantly improves\nthe accuracy of early-stage diabetes prediction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04785v2",
    "published_date": "2024-03-02 22:33:17 UTC",
    "updated_date": "2024-08-29 22:18:08 UTC"
  },
  {
    "arxiv_id": "2403.01329v1",
    "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models",
    "authors": [
      "Neta Shaul",
      "Uriel Singer",
      "Ricky T. Q. Chen",
      "Matthew Le",
      "Ali Thabet",
      "Albert Pumarola",
      "Yaron Lipman"
    ],
    "abstract": "This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver\ndistillation approach to improve sample efficiency of Diffusion and Flow\nmodels. BNS solvers are based on a family of non-stationary solvers that\nprovably subsumes existing numerical ODE solvers and consequently demonstrate\nconsiderable improvement in sample approximation (PSNR) over these baselines.\nCompared to model distillation, BNS solvers benefit from a tiny parameter space\n($<$200 parameters), fast optimization (two orders of magnitude faster),\nmaintain diversity of samples, and in contrast to previous solver distillation\napproaches nearly close the gap from standard distillation methods such as\nProgressive Distillation in the low-medium NFE regime. For example, BNS solver\nachieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We\nexperimented with BNS solvers for conditional image generation, text-to-image\ngeneration, and text-2-audio generation showing significant improvement in\nsample approximation (PSNR) in all.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01329v1",
    "published_date": "2024-03-02 22:27:44 UTC",
    "updated_date": "2024-03-02 22:27:44 UTC"
  },
  {
    "arxiv_id": "2403.05583v1",
    "title": "A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition",
    "authors": [
      "Tyler Benster",
      "Guy Wilson",
      "Reshef Elisha",
      "Francis R Willett",
      "Shaul Druckmann"
    ],
    "abstract": "Silent Speech Interfaces (SSIs) offer a noninvasive alternative to\nbrain-computer interfaces for soundless verbal communication. We introduce\nMultimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal\nalignment through novel loss functions--cross-contrast (crossCon) and\nsupervised temporal contrast (supTcon)--to train a multimodal model with a\nshared latent representation. This architecture enables the use of audio-only\ndatasets like LibriSpeech to improve silent speech recognition. Additionally,\nour introduction of Large Language Model (LLM) Integrated Scoring Adjustment\n(LISA) significantly improves recognition accuracy. Together, MONA LISA reduces\nthe state-of-the-art word error rate (WER) from 28.8% to 12.2% in the Gaddy\n(2020) benchmark dataset for silent speech on an open vocabulary. For vocal EMG\nrecordings, our method improves the state-of-the-art from 23.3% to 3.7% WER. In\nthe Brain-to-Text 2024 competition, LISA performs best, improving the top WER\nfrom 9.8% to 8.9%. To the best of our knowledge, this work represents the first\ninstance where noninvasive silent speech recognition on an open vocabulary has\ncleared the threshold of 15% WER, demonstrating that SSIs can be a viable\nalternative to automatic speech recognition (ASR). Our work not only narrows\nthe performance gap between silent and vocalized speech but also opens new\npossibilities in human-computer interaction, demonstrating the potential of\ncross-modal approaches in noisy and data-limited regimes.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05583v1",
    "published_date": "2024-03-02 21:15:24 UTC",
    "updated_date": "2024-03-02 21:15:24 UTC"
  },
  {
    "arxiv_id": "2403.01309v1",
    "title": "VNLP: Turkish NLP Package",
    "authors": [
      "Meliksah Turker",
      "Mehmet Erdi Ari",
      "Aydin Han"
    ],
    "abstract": "In this work, we present VNLP: the first dedicated, complete, open-source,\nwell-documented, lightweight, production-ready, state-of-the-art Natural\nLanguage Processing (NLP) package for the Turkish language. It contains a wide\nvariety of tools, ranging from the simplest tasks, such as sentence splitting\nand text normalization, to the more advanced ones, such as text and token\nclassification models. Its token classification models are based on \"Context\nModel\", a novel architecture that is both an encoder and an auto-regressive\nmodel. NLP tasks solved by VNLP models include but are not limited to Sentiment\nAnalysis, Named Entity Recognition, Morphological Analysis \\& Disambiguation\nand Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings\nand corresponding SentencePiece Unigram tokenizers. VNLP has an open-source\nGitHub repository, ReadtheDocs documentation, PyPi package for convenient\ninstallation, Python and command-line API and a demo page to test all the\nfunctionality. Consequently, our main contribution is a complete, compact,\neasy-to-install and easy-to-use NLP package for Turkish.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01309v1",
    "published_date": "2024-03-02 20:46:56 UTC",
    "updated_date": "2024-03-02 20:46:56 UTC"
  },
  {
    "arxiv_id": "2403.01308v2",
    "title": "VBART: The Turkish LLM",
    "authors": [
      "Meliksah Turker",
      "Mehmet Erdi Ari",
      "Aydin Han"
    ],
    "abstract": "We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01308v2",
    "published_date": "2024-03-02 20:40:11 UTC",
    "updated_date": "2024-03-14 16:37:37 UTC"
  },
  {
    "arxiv_id": "2403.01286v1",
    "title": "Summary Paper: Use Case on Building Collaborative Safe Autonomous Systems-A Robotdog for Guiding Visually Impaired People",
    "authors": [
      "Aman Malhotra",
      "Selma Saidi"
    ],
    "abstract": "This is a summary paper of a use case of a Robotdog dedicated to guide\nvisually impaired people in complex environment like a smart intersection. In\nsuch scenarios, the Robotdog has to autonomously decide whether it is safe to\ncross the intersection or not in order to further guide the human. We leverage\ndata sharing and collaboration between the Robotdog and other autonomous\nsystems operating in the same environment. We propose a system architecture for\nautonomous systems through a separation of a collaborative decision layer, to\nenable collective decision making processes, where data about the environment,\nrelevant to the Robotdog decision, together with evidences for trustworthiness\nabout other systems and the environment are shared.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.DC",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01286v1",
    "published_date": "2024-03-02 18:59:03 UTC",
    "updated_date": "2024-03-02 18:59:03 UTC"
  },
  {
    "arxiv_id": "2403.01281v2",
    "title": "Fast Low-parameter Video Activity Localization in Collaborative Learning Environments",
    "authors": [
      "Venkatesh Jatla",
      "Sravani Teeparthi",
      "Ugesh Egala",
      "Sylvia Celedon Pattichis",
      "Marios S. Patticis"
    ],
    "abstract": "Research on video activity detection has primarily focused on identifying\nwell-defined human activities in short video segments. The majority of the\nresearch on video activity recognition is focused on the development of large\nparameter systems that require training on large video datasets. This paper\ndevelops a low-parameter, modular system with rapid inferencing capabilities\nthat can be trained entirely on limited datasets without requiring transfer\nlearning from large-parameter systems. The system can accurately detect and\nassociate specific activities with the students who perform the activities in\nreal-life classroom videos. Additionally, the paper develops an interactive\nweb-based application to visualize human activity maps over long real-life\nclassroom videos.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01281v2",
    "published_date": "2024-03-02 18:28:32 UTC",
    "updated_date": "2024-03-09 15:37:59 UTC"
  },
  {
    "arxiv_id": "2403.01277v1",
    "title": "Optimal Integrated Task and Path Planning and Its Application to Multi-Robot Pickup and Delivery",
    "authors": [
      "Aman Aryan",
      "Manan Modi",
      "Indranil Saha",
      "Rupak Majumdar",
      "Swarup Mohalik"
    ],
    "abstract": "We propose a generic multi-robot planning mechanism that combines an optimal\ntask planner and an optimal path planner to provide a scalable solution for\ncomplex multi-robot planning problems. The Integrated planner, through the\ninteraction of the task planner and the path planner, produces optimal\ncollision-free trajectories for the robots. We illustrate our general algorithm\non an object pick-and-drop planning problem in a warehouse scenario where a\ngroup of robots is entrusted with moving objects from one location to another\nin the workspace. We solve the task planning problem by reducing it into an\nSMT-solving problem and employing the highly advanced SMT solver Z3 to solve\nit. To generate collision-free movement of the robots, we extend the\nstate-of-the-art algorithm Conflict Based Search with Precedence Constraints\nwith several domain-specific constraints. We evaluate our integrated task and\npath planner extensively on various instances of the object pick-and-drop\nplanning problem and compare its performance with a state-of-the-art\nmulti-robot classical planner. Experimental results demonstrate that our\nplanning mechanism can deal with complex planning problems and outperforms a\nstate-of-the-art classical planner both in terms of computation time and the\nquality of the generated plan.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01277v1",
    "published_date": "2024-03-02 17:48:40 UTC",
    "updated_date": "2024-03-02 17:48:40 UTC"
  },
  {
    "arxiv_id": "2403.01273v1",
    "title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention",
    "authors": [
      "Tianyi Zhang",
      "Jonah Wonkyu Yi",
      "Bowen Yao",
      "Zhaozhuo Xu",
      "Anshumali Shrivastava"
    ],
    "abstract": "Large language model inference on Central Processing Units (CPU) is\nchallenging due to the vast quantities of expensive Multiply-Add (MAD) matrix\noperations in the attention computations. In this paper, we argue that there is\na rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,\nwhich allow for ultra-low-latency lookups in batch. We leverage this unique\ncapability of CPUs to propose NoMAD-Attention, an efficient attention algorithm\nthat replaces MAD operations with in-register lookups. Through hardware-aware\nalgorithmic designs, NoMAD-Attention achieves the computation of attention\nscores using repeated fast accesses to SIMD registers despite their highly\nlimited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based\nLLMs without model finetuning. Empirical evaluations demonstrate that\nNoMAD-Attention maintains the quality of the original LLMs well, and speeds up\nthe 4-bit quantized LLaMA-7B-based model by up to 2$\\times$ at 16k context\nlength. Our results are reproducible at\nhttps://github.com/tonyzhang617/nomad-dist.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01273v1",
    "published_date": "2024-03-02 17:29:22 UTC",
    "updated_date": "2024-03-02 17:29:22 UTC"
  },
  {
    "arxiv_id": "2403.02355v1",
    "title": "Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space",
    "authors": [
      "Li Cai",
      "Xin Mao",
      "Zhihong Wang",
      "Shangqing Zhao",
      "Yuhao Zhou",
      "Changxu Wu",
      "Man Lan"
    ],
    "abstract": "Temporal knowledge graph completion (TKGC) aims to fill in missing facts\nwithin a given temporal knowledge graph at a specific time. Existing methods,\noperating in real or complex spaces, have demonstrated promising performance in\nthis task. This paper advances beyond conventional approaches by introducing\nmore expressive quaternion representations for TKGC within hypercomplex space.\nUnlike existing quaternion-based methods, our study focuses on capturing\ntime-sensitive relations rather than time-aware entities. Specifically, we\nmodel time-sensitive relations through time-aware rotation and periodic time\ntranslation, effectively capturing complex temporal variability. Furthermore,\nwe theoretically demonstrate our method's capability to model symmetric,\nasymmetric, inverse, compositional, and evolutionary relation patterns.\nComprehensive experiments on public datasets validate that our proposed\napproach achieves state-of-the-art performance in the field of TKGC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.02355v1",
    "published_date": "2024-03-02 16:50:48 UTC",
    "updated_date": "2024-03-02 16:50:48 UTC"
  },
  {
    "arxiv_id": "2403.01255v2",
    "title": "Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey",
    "authors": [
      "Hamza Kheddar",
      "Mustapha Hemis",
      "Yassine Himeur"
    ],
    "abstract": "Recent advancements in deep learning (DL) have posed a significant challenge\nfor automatic speech recognition (ASR). ASR relies on extensive training\ndatasets, including confidential ones, and demands substantial computational\nand storage resources. Enabling adaptive systems improves ASR performance in\ndynamic environments. DL techniques assume training and testing data originate\nfrom the same domain, which is not always true. Advanced DL techniques like\ndeep transfer learning (DTL), federated learning (FL), and reinforcement\nlearning (RL) address these issues. DTL allows high-performance models using\nsmall yet related datasets, FL enables training on confidential data without\ndataset possession, and RL optimizes decision-making in dynamic environments,\nreducing computation costs. This survey offers a comprehensive review of DTL,\nFL, and RL-based ASR frameworks, aiming to provide insights into the latest\ndevelopments and aid researchers and professionals in understanding the current\nchallenges. Additionally, transformers, which are advanced DL techniques\nheavily used in proposed ASR frameworks, are considered in this survey for\ntheir ability to capture extensive dependencies in the input ASR sequence. The\npaper starts by presenting the background of DTL, FL, RL, and Transformers and\nthen adopts a well-designed taxonomy to outline the state-of-the-art\napproaches. Subsequently, a critical analysis is conducted to identify the\nstrengths and weaknesses of each framework. Additionally, a comparative study\nis presented to highlight the existing challenges, paving the way for future\nresearch opportunities.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01255v2",
    "published_date": "2024-03-02 16:25:42 UTC",
    "updated_date": "2024-04-18 17:29:29 UTC"
  },
  {
    "arxiv_id": "2403.04782v1",
    "title": "A Survey on Temporal Knowledge Graph: Representation Learning and Applications",
    "authors": [
      "Li Cai",
      "Xin Mao",
      "Yuhao Zhou",
      "Zhaoguang Long",
      "Changxu Wu",
      "Man Lan"
    ],
    "abstract": "Knowledge graphs have garnered significant research attention and are widely\nused to enhance downstream applications. However, most current studies mainly\nfocus on static knowledge graphs, whose facts do not change with time, and\ndisregard their dynamic evolution over time. As a result, temporal knowledge\ngraphs have attracted more attention because a large amount of structured\nknowledge exists only within a specific period. Knowledge graph representation\nlearning aims to learn low-dimensional vector embeddings for entities and\nrelations in a knowledge graph. The representation learning of temporal\nknowledge graphs incorporates time information into the standard knowledge\ngraph framework and can model the dynamics of entities and relations over time.\nIn this paper, we conduct a comprehensive survey of temporal knowledge graph\nrepresentation learning and its applications. We begin with an introduction to\nthe definitions, datasets, and evaluation metrics for temporal knowledge graph\nrepresentation learning. Next, we propose a taxonomy based on the core\ntechnologies of temporal knowledge graph representation learning methods, and\nprovide an in-depth analysis of different methods in each category. Finally, we\npresent various downstream applications related to the temporal knowledge\ngraphs. In the end, we conclude the paper and have an outlook on the future\nresearch directions in this area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04782v1",
    "published_date": "2024-03-02 16:21:45 UTC",
    "updated_date": "2024-03-02 16:21:45 UTC"
  },
  {
    "arxiv_id": "2403.01248v1",
    "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code",
    "authors": [
      "Ziniu Hu",
      "Ahmet Iscen",
      "Aashi Jain",
      "Thomas Kipf",
      "Yisong Yue",
      "David A. Ross",
      "Cordelia Schmid",
      "Alireza Fathi"
    ],
    "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent\nconverting text descriptions into Blender-executable Python scripts which\nrender complex scenes with up to a hundred 3D assets. This process requires\ncomplex spatial planning and arrangement. We tackle these challenges through a\ncombination of advanced abstraction, strategic planning, and library learning.\nSceneCraft first models a scene graph as a blueprint, detailing the spatial\nrelationships among assets in the scene. SceneCraft then writes Python scripts\nbased on this graph, translating relationships into numerical constraints for\nasset layout. Next, SceneCraft leverages the perceptual strengths of\nvision-language foundation models like GPT-V to analyze rendered images and\niteratively refine the scene. On top of this process, SceneCraft features a\nlibrary learning mechanism that compiles common script functions into a\nreusable library, facilitating continuous self-improvement without expensive\nLLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses\nexisting LLM-based agents in rendering complex scenes, as shown by its\nadherence to constraints and favorable human assessments. We also showcase the\nbroader application potential of SceneCraft by reconstructing detailed 3D\nscenes from the Sintel movie and guiding a video generative model with\ngenerated scenes as intermediary control signal.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01248v1",
    "published_date": "2024-03-02 16:16:26 UTC",
    "updated_date": "2024-03-02 16:16:26 UTC"
  },
  {
    "arxiv_id": "2403.01244v2",
    "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal",
    "authors": [
      "Jianheng Huang",
      "Leyang Cui",
      "Ante Wang",
      "Chengyi Yang",
      "Xinting Liao",
      "Linfeng Song",
      "Junfeng Yao",
      "Jinsong Su"
    ],
    "abstract": "Large language models (LLMs) suffer from catastrophic forgetting during\ncontinual learning. Conventional rehearsal-based methods rely on previous\ntraining data to retain the model's ability, which may not be feasible in\nreal-world applications. When conducting continual learning based on a\npublicly-released LLM checkpoint, the availability of the original training\ndata may be non-existent. To address this challenge, we propose a framework\ncalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\ninstances for rehearsal. Concretely, we first employ the base LLM for\nin-context learning to generate synthetic instances. Subsequently, we utilize\nthe latest LLM to refine the instance outputs based on the synthetic inputs,\npreserving its acquired ability. Finally, we select diverse high-quality\nsynthetic instances for rehearsal in future stages. Experimental results\ndemonstrate that SSR achieves superior or comparable performance compared to\nconventional rehearsal-based approaches while being more data-efficient.\nBesides, SSR effectively preserves the generalization capabilities of LLMs in\ngeneral domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 main, long paper",
    "pdf_url": "http://arxiv.org/pdf/2403.01244v2",
    "published_date": "2024-03-02 16:11:23 UTC",
    "updated_date": "2024-05-25 12:17:29 UTC"
  },
  {
    "arxiv_id": "2403.01242v1",
    "title": "Augmenting Automation: Intent-Based User Instruction Classification with Machine Learning",
    "authors": [
      "Lochan Basyal",
      "Bijay Gaudel"
    ],
    "abstract": "Electric automation systems offer convenience and efficiency in controlling\nelectrical circuits and devices. Traditionally, these systems rely on\npredefined commands for control, limiting flexibility and adaptability. In this\npaper, we propose a novel approach to augment automation by introducing\nintent-based user instruction classification using machine learning techniques.\nOur system represents user instructions as intents, allowing for dynamic\ncontrol of electrical circuits without relying on predefined commands. Through\na machine learning model trained on a labeled dataset of user instructions, our\nsystem classifies intents from user input, enabling a more intuitive and\nadaptable control scheme. We present the design and implementation of our\nintent-based electric automation system, detailing the development of the\nmachine learning model for intent classification. Experimental results\ndemonstrate the effectiveness of our approach in enhancing user experience and\nexpanding the capabilities of electric automation systems. Our work contributes\nto the advancement of smart technologies by providing a more seamless\ninteraction between users and their environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.01242v1",
    "published_date": "2024-03-02 16:06:03 UTC",
    "updated_date": "2024-03-02 16:06:03 UTC"
  },
  {
    "arxiv_id": "2403.01241v2",
    "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact",
    "authors": [
      "Ruikang Liu",
      "Haoli Bai",
      "Haokun Lin",
      "Yuening Li",
      "Han Gao",
      "Zhengzhuo Xu",
      "Lu Hou",
      "Jun Yao",
      "Chun Yuan"
    ],
    "abstract": "Large language models (LLMs) excel in natural language processing but demand\nintensive computation. To mitigate this, various quantization methods have been\nexplored, yet they compromise LLM performance. This paper unveils a previously\noverlooked type of outliers in LLMs. Such outliers are found to allocate most\nof the attention scores on initial tokens of input, termed as pivot tokens,\nwhich are crucial to the performance of quantized LLMs. Given that, we propose\nIntactKV to generate the KV cache of pivot tokens losslessly from the\nfull-precision model. The approach is simple and easy to combine with existing\nquantization solutions with no extra inference overhead. Besides, IntactKV can\nbe calibrated as additional LLM parameters to boost the quantized LLMs further\nwith minimal training costs. Mathematical analysis also proves that IntactKV\neffectively reduces the upper bound of quantization error. Empirical results\nshow that IntactKV brings consistent improvement over various quantization\nmethods across different LLMs and downstream tasks, leading to the new\nstate-of-the-art for LLM quantization. The codes are available at\nhttps://github.com/ruikangliu/IntactKV.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2403.01241v2",
    "published_date": "2024-03-02 16:05:26 UTC",
    "updated_date": "2024-05-25 10:33:03 UTC"
  },
  {
    "arxiv_id": "2403.01232v3",
    "title": "Polynormer: Polynomial-Expressive Graph Transformer in Linear Time",
    "authors": [
      "Chenhui Deng",
      "Zichao Yue",
      "Zhiru Zhang"
    ],
    "abstract": "Graph transformers (GTs) have emerged as a promising architecture that is\ntheoretically more expressive than message-passing graph neural networks\n(GNNs). However, typical GT models have at least quadratic complexity and thus\ncannot scale to large graphs. While there are several linear GTs recently\nproposed, they still lag behind GNN counterparts on several popular graph\ndatasets, which poses a critical concern on their practical expressivity. To\nbalance the trade-off between expressivity and scalability of GTs, we propose\nPolynormer, a polynomial-expressive GT model with linear complexity. Polynormer\nis built upon a novel base model that learns a high-degree polynomial on input\nfeatures. To enable the base model permutation equivariant, we integrate it\nwith graph topology and node features separately, resulting in local and global\nequivariant attention models. Consequently, Polynormer adopts a linear\nlocal-to-global attention scheme to learn high-degree equivariant polynomials\nwhose coefficients are controlled by attention scores. Polynormer has been\nevaluated on $13$ homophilic and heterophilic datasets, including large graphs\nwith millions of nodes. Our extensive experiment results show that Polynormer\noutperforms state-of-the-art GNN and GT baselines on most datasets, even\nwithout the use of nonlinear activation functions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at International Conference on\n  Learning Representations (ICLR) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01232v3",
    "published_date": "2024-03-02 15:32:01 UTC",
    "updated_date": "2024-04-06 23:26:26 UTC"
  },
  {
    "arxiv_id": "2403.01229v1",
    "title": "REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild",
    "authors": [
      "Jose Vargas Quiros",
      "Chirag Raman",
      "Stephanie Tan",
      "Ekin Gedik",
      "Laura Cabrera-Quiros",
      "Hayley Hung"
    ],
    "abstract": "Recognizing speaking in humans is a central task towards understanding social\ninteractions. Ideally, speaking would be detected from individual voice\nrecordings, as done previously for meeting scenarios. However, individual voice\nrecordings are hard to obtain in the wild, especially in crowded mingling\nscenarios due to cost, logistics, and privacy concerns. As an alternative,\nmachine learning models trained on video and wearable sensor data make it\npossible to recognize speech by detecting its related gestures in an\nunobtrusive, privacy-preserving way. These models themselves should ideally be\ntrained using labels obtained from the speech signal. However, existing\nmingling datasets do not contain high quality audio recordings. Instead,\nspeaking status annotations have often been inferred by human annotators from\nvideo, without validation of this approach against audio-based ground truth. In\nthis paper we revisit no-audio speaking status estimation by presenting the\nfirst publicly available multimodal dataset with high-quality individual speech\nrecordings of 33 subjects in a professional networking event. We present three\nbaselines for no-audio speaking status segmentation: a) from video, b) from\nbody acceleration (chest-worn accelerometer), c) from body pose tracks. In all\ncases we predict a 20Hz binary speaking status signal extracted from the audio,\na time resolution not available in previous datasets. In addition to providing\nthe signals and ground truth necessary to evaluate a wide range of speaking\nstatus detection methods, the availability of audio in REWIND makes it suitable\nfor cross-modality studies not feasible with previous mingling datasets.\nFinally, our flexible data consent setup creates new challenges for multimodal\nsystems under missing modalities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01229v1",
    "published_date": "2024-03-02 15:14:58 UTC",
    "updated_date": "2024-03-02 15:14:58 UTC"
  },
  {
    "arxiv_id": "2403.01221v2",
    "title": "A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations",
    "authors": [
      "André Artelt",
      "Andreas Gregoriades"
    ],
    "abstract": "Counterfactual explanations constitute among the most popular methods for\nanalyzing black-box systems since they can recommend cost-efficient and\nactionable changes to the input of a system to obtain the desired system\noutput. While most of the existing counterfactual methods explain a single\ninstance, several real-world problems, such as customer satisfaction, require\nthe identification of a single counterfactual that can satisfy multiple\ninstances (e.g. customers) simultaneously. To address this limitation, in this\nwork, we propose a flexible two-stage algorithm for finding groups of instances\nand computing cost-efficient multi-instance counterfactual explanations. The\npaper presents the algorithm and its performance against popular alternatives\nthrough a comparative evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in the Late-breaking works track @ 2nd World Conference on\n  eXplainable Artificial Intelligence (2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.01221v2",
    "published_date": "2024-03-02 14:30:57 UTC",
    "updated_date": "2024-05-21 11:34:38 UTC"
  },
  {
    "arxiv_id": "2403.01216v2",
    "title": "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access",
    "authors": [
      "Jiayuan Su",
      "Jing Luo",
      "Hongwei Wang",
      "Lu Cheng"
    ],
    "abstract": "This study aims to address the pervasive challenge of quantifying uncertainty\nin large language models (LLMs) without logit-access. Conformal Prediction\n(CP), known for its model-agnostic and distribution-free features, is a desired\napproach for various LLMs and data distributions. However, existing CP methods\nfor LLMs typically assume access to the logits, which are unavailable for some\nAPI-only LLMs. In addition, logits are known to be miscalibrated, potentially\nleading to degraded CP performance. To tackle these challenges, we introduce a\nnovel CP method that (1) is tailored for API-only LLMs without logit-access;\n(2) minimizes the size of prediction sets; and (3) ensures a statistical\nguarantee of the user-defined coverage. The core idea of this approach is to\nformulate nonconformity measures using both coarse-grained (i.e., sample\nfrequency) and fine-grained uncertainty notions (e.g., semantic similarity).\nExperimental results on both close-ended and open-ended Question Answering\ntasks show our approach can mostly outperform the logit-based CP baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01216v2",
    "published_date": "2024-03-02 14:14:45 UTC",
    "updated_date": "2024-04-04 02:15:39 UTC"
  },
  {
    "arxiv_id": "2403.01210v1",
    "title": "SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters",
    "authors": [
      "Jiahao Cui",
      "Jiale Duan",
      "Binyan Luo",
      "Hang Cao",
      "Wang Guo",
      "Haifeng Li"
    ],
    "abstract": "Deep neural network-based Synthetic Aperture Radar (SAR) target recognition\nmodels are susceptible to adversarial examples. Current adversarial example\ngeneration methods for SAR imagery primarily operate in the 2D digital domain,\nknown as image adversarial examples. Recent work, while considering SAR imaging\nscatter mechanisms, fails to account for the actual imaging process, rendering\nattacks in the three-dimensional physical domain infeasible, termed pseudo\nphysics adversarial examples. To address these challenges, this paper proposes\nSAR-AE-SFP-Attack, a method to generate real physics adversarial examples by\naltering the scattering feature parameters of target objects. Specifically, we\niteratively optimize the coherent energy accumulation of the target echo by\nperturbing the reflection coefficient and scattering coefficient in the\nscattering feature parameters of the three-dimensional target object, and\nobtain the adversarial example after echo signal processing and imaging\nprocessing in the RaySAR simulator. Experimental results show that compared to\ndigital adversarial attack methods, SAR-AE-SFP Attack significantly improves\nattack efficiency on CNN-based models (over 30\\%) and Transformer-based models\n(over 13\\%), demonstrating significant transferability of attack effects across\ndifferent models and perspectives.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 9 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.01210v1",
    "published_date": "2024-03-02 13:52:28 UTC",
    "updated_date": "2024-03-02 13:52:28 UTC"
  },
  {
    "arxiv_id": "2403.01199v1",
    "title": "The Case for Animal-Friendly AI",
    "authors": [
      "Sankalpa Ghose",
      "Yip Fai Tse",
      "Kasra Rasaee",
      "Jeff Sebo",
      "Peter Singer"
    ],
    "abstract": "Artificial intelligence is seen as increasingly important, and potentially\nprofoundly so, but the fields of AI ethics and AI engineering have not fully\nrecognized that these technologies, including large language models (LLMs),\nwill have massive impacts on animals. We argue that this impact matters,\nbecause animals matter morally.\n  As a first experiment in evaluating animal consideration in LLMs, we\nconstructed a proof-of-concept Evaluation System, which assesses LLM responses\nand biases from multiple perspectives. This system evaluates LLM outputs by two\ncriteria: their truthfulness, and the degree of consideration they give to the\ninterests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using\na set of structured queries and predefined normative perspectives. Preliminary\nresults suggest that the outcomes of the tested models can be benchmarked\nregarding the consideration they give to animals, and that generated positions\nand biases might be addressed and mitigated with more developed and validated\nsystems.\n  Our research contributes one possible approach to integrating animal ethics\nin AI, opening pathways for future studies and practical applications in\nvarious fields, including education, public policy, and regulation, that\ninvolve or relate to animals and society. Overall, this study serves as a step\ntowards more useful and responsible AI systems that better recognize and\nrespect the vital interests and perspectives of all sentient beings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2024 Workshop on Public Sector LLMs: Algorithmic and\n  Sociotechnical Design. 12 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.01199v1",
    "published_date": "2024-03-02 12:41:11 UTC",
    "updated_date": "2024-03-02 12:41:11 UTC"
  },
  {
    "arxiv_id": "2403.01196v1",
    "title": "Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021",
    "authors": [
      "Séamus Lankford",
      "Haithem Afli",
      "Andy Way"
    ],
    "abstract": "Translation models for the specific domain of translating Covid data from\nEnglish to Irish were developed for the LoResMT 2021 shared task. Domain\nadaptation techniques, using a Covid-adapted generic 55k corpus from the\nDirectorate General of Translation, were applied. Fine-tuning, mixed\nfine-tuning and combined dataset approaches were compared with models trained\non an extended in-domain dataset. As part of this study, an English-Irish\ndataset of Covid related data, from the Health and Education domains, was\ndeveloped. The highest-performing model used a Transformer architecture trained\nwith an extended in-domain Covid dataset. In the context of this study, we have\ndemonstrated that extending an 8k in-domain baseline dataset by just 5k lines\nimproved the BLEU score by 27 points.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01196v1",
    "published_date": "2024-03-02 12:29:28 UTC",
    "updated_date": "2024-03-02 12:29:28 UTC"
  },
  {
    "arxiv_id": "2403.01193v3",
    "title": "RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots",
    "authors": [
      "Philip Feldman",
      "James R. Foulds",
      "Shimei Pan"
    ],
    "abstract": "Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\nof artificial intelligence. However, their tendency to hallucinate -- generate\nplausible but false information -- poses a significant challenge. This issue is\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\nGeneration (RAG) can counter hallucinations by integrating external knowledge\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\nin some cases, but can still be misled when prompts directly contradict the\nmodel's pre-trained understanding. These findings highlight the complex nature\nof hallucinations and the need for more robust solutions to ensure LLM\nreliability in real-world applications. We offer practical recommendations for\nRAG deployment and discuss implications for the development of more trustworthy\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "H.3.3; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "7 Pages, 1 Figure, 1 Table",
    "pdf_url": "http://arxiv.org/pdf/2403.01193v3",
    "published_date": "2024-03-02 12:19:04 UTC",
    "updated_date": "2024-06-12 12:00:52 UTC"
  },
  {
    "arxiv_id": "2403.01185v1",
    "title": "Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding",
    "authors": [
      "Ha-Thanh Nguyen",
      "Ken Satoh"
    ],
    "abstract": "Finetuning approaches in NLP often focus on exploitation rather than\nexploration, which may lead to suboptimal models. Given the vast search space\nof natural language, this limited exploration can restrict their performance in\ncomplex, high-stakes domains, where accurate negation understanding and logical\nreasoning abilities are crucial. To address this issue, we leverage\nReinforcement Learning from Logical Feedback (RLLF) to create an effective\nbalance between exploration and exploitation in LLMs. Our approach employs an\nappropriate benchmark dataset for training and evaluation, highlighting the\nimportance of exploration in enhancing negation understanding capabilities. We\ncompare the performance of our RLLF-enhanced LLMs with baseline models trained\nwithout RLLF, demonstrating the value of this balanced approach. Furthermore,\nwe showcase the potential of our method in legal AI applications by employing\ntransfer learning and evaluating its impact on negation understanding. Our\nexperimental results exhibit the effectiveness of balancing exploration and\nexploitation with RLLF in improving LLMs' negation capabilities. This has\nimplications for the development of more accurate, reliable, and logically\nconsistent language models in high-stakes domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "JURISIN 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01185v1",
    "published_date": "2024-03-02 11:54:55 UTC",
    "updated_date": "2024-03-02 11:54:55 UTC"
  },
  {
    "arxiv_id": "2403.01183v2",
    "title": "Leveraging Self-Supervised Learning for Scene Classification in Child Sexual Abuse Imagery",
    "authors": [
      "Pedro H. V. Valois",
      "João Macedo",
      "Leo S. F. Ribeiro",
      "Jefersson A. dos Santos",
      "Sandra Avila"
    ],
    "abstract": "Crime in the 21st century is split into a virtual and real world. However,\nthe former has become a global menace to people's well-being and security in\nthe latter. The challenges it presents must be faced with unified global\ncooperation, and we must rely more than ever on automated yet trustworthy tools\nto combat the ever-growing nature of online offenses. Over 10 million child\nsexual abuse reports are submitted to the US National Center for Missing \\&\nExploited Children every year, and over 80% originate from online sources.\nTherefore, investigation centers cannot manually process and correctly\ninvestigate all imagery. In light of that, reliable automated tools that can\nsecurely and efficiently deal with this data are paramount. In this sense, the\nscene classification task looks for contextual cues in the environment, being\nable to group and classify child sexual abuse data without requiring to be\ntrained on sensitive material. The scarcity and limitations of working with\nchild sexual abuse images lead to self-supervised learning, a machine-learning\nmethodology that leverages unlabeled data to produce powerful representations\nthat can be more easily transferred to downstream tasks. This work shows that\nself-supervised deep learning models pre-trained on scene-centric data can\nreach 71.6% balanced accuracy on our indoor scene classification task and, on\naverage, 2.2 percentage points better performance than a fully supervised\nversion. We cooperate with Brazilian Federal Police experts to evaluate our\nindoor classification model on actual child abuse material. The results\ndemonstrate a notable discrepancy between the features observed in widely used\nscene datasets and those depicted on sensitive materials.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 5 figures, 4 tables. Under review",
    "pdf_url": "http://arxiv.org/pdf/2403.01183v2",
    "published_date": "2024-03-02 11:44:14 UTC",
    "updated_date": "2024-10-26 15:49:30 UTC"
  },
  {
    "arxiv_id": "2403.01166v2",
    "title": "DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference",
    "authors": [
      "Jialong Wu",
      "Linhai Zhang",
      "Deyu Zhou",
      "Guoqiang Xu"
    ],
    "abstract": "Though notable progress has been made, neural-based aspect-based sentiment\nanalysis (ABSA) models are prone to learn spurious correlations from annotation\nbiases, resulting in poor robustness on adversarial data transformations. Among\nthe debiasing solutions, causal inference-based methods have attracted much\nresearch attention, which can be mainly categorized into causal intervention\nmethods and counterfactual reasoning methods. However, most of the present\ndebiasing methods focus on single-variable causal inference, which is not\nsuitable for ABSA with two input variables (the target aspect and the review).\nIn this paper, we propose a novel framework based on multi-variable causal\ninference for debiasing ABSA. In this framework, different types of biases are\ntackled based on different causal intervention methods. For the review branch,\nthe bias is modeled as indirect confounding from context, where backdoor\nadjustment intervention is employed for debiasing. For the aspect branch, the\nbias is described as a direct correlation with labels, where counterfactual\nreasoning is adopted for debiasing. Extensive experiments demonstrate the\neffectiveness of the proposed method compared to various baselines on the two\nwidely used real-world aspect robustness test set datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL2024(Findings)",
    "pdf_url": "http://arxiv.org/pdf/2403.01166v2",
    "published_date": "2024-03-02 10:38:31 UTC",
    "updated_date": "2024-06-06 07:27:33 UTC"
  },
  {
    "arxiv_id": "2403.01165v2",
    "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Linhai Zhang",
      "Jialong Wu",
      "Deyu Zhou",
      "Guoqiang Xu"
    ],
    "abstract": "Though Large Language Models (LLMs) have demonstrated the powerful\ncapabilities of few-shot learning through prompting methods, supervised\ntraining is still necessary for complex reasoning tasks. Because of their\nextensive parameters and memory consumption, both Parameter-Efficient\nFine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been\nproposed for LLMs. Nevertheless, the issue of large annotated data consumption,\nthe aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is\nto combine the PEFT method with active learning. However, the experimental\nresults show that such a combination is not trivial and yields inferior\nresults. Through probe experiments, such observation might be explained by two\nmain reasons: uncertainty gap and poor model calibration. Therefore, in this\npaper, we propose a novel approach to effectively integrate uncertainty-based\nactive learning and LoRA. Specifically, for the uncertainty gap, we introduce a\ndynamic uncertainty measurement that combines the uncertainty of the base model\nand the uncertainty of the full model during the iteration of active learning.\nFor poor model calibration, we incorporate the regularization method during\nLoRA training to keep the model from being over-confident, and the Monte-Carlo\ndropout mechanism is employed to enhance the uncertainty estimation.\nExperimental results show that the proposed approach outperforms existing\nbaseline models on three complex reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL2024(Findings)",
    "pdf_url": "http://arxiv.org/pdf/2403.01165v2",
    "published_date": "2024-03-02 10:38:10 UTC",
    "updated_date": "2024-06-06 07:31:39 UTC"
  },
  {
    "arxiv_id": "2403.02354v3",
    "title": "Spatio-Temporal Field Neural Networks for Air Quality Inference",
    "authors": [
      "Yutong Feng",
      "Qiongyan Wang",
      "Yutong Xia",
      "Junlin Huang",
      "Siru Zhong",
      "Yuxuan Liang"
    ],
    "abstract": "The air quality inference problem aims to utilize historical data from a\nlimited number of observation sites to infer the air quality index at an\nunknown location. Considering the sparsity of data due to the high maintenance\ncost of the stations, good inference algorithms can effectively save the cost\nand refine the data granularity. While spatio-temporal graph neural networks\nhave made excellent progress on this problem, their non-Euclidean and discrete\ndata structure modeling of reality limits its potential. In this work, we make\nthe first attempt to combine two different spatio-temporal perspectives, fields\nand graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and\nits corresponding new framework, Pyramidal Inference. Extensive experiments\nvalidate that our model achieves state-of-the-art performance in nationwide air\nquality inference in the Chinese Mainland, demonstrating the superiority of our\nproposed model and framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "We want to recheck our model and experimental design",
    "pdf_url": "http://arxiv.org/pdf/2403.02354v3",
    "published_date": "2024-03-02 10:14:42 UTC",
    "updated_date": "2024-06-06 04:27:33 UTC"
  },
  {
    "arxiv_id": "2403.01152v1",
    "title": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",
    "authors": [
      "Tharindu Kumarage",
      "Garima Agrawal",
      "Paras Sheth",
      "Raha Moraffah",
      "Aman Chadha",
      "Joshua Garland",
      "Huan Liu"
    ],
    "abstract": "We have witnessed lately a rapid proliferation of advanced Large Language\nModels (LLMs) capable of generating high-quality text. While these LLMs have\nrevolutionized text generation across various domains, they also pose\nsignificant risks to the information ecosystem, such as the potential for\ngenerating convincing propaganda, misinformation, and disinformation at scale.\nThis paper offers a review of AI-generated text forensic systems, an emerging\nfield addressing the challenges of LLM misuses. We present an overview of the\nexisting efforts in AI-generated text forensics by introducing a detailed\ntaxonomy, focusing on three primary pillars: detection, attribution, and\ncharacterization. These pillars enable a practical understanding of\nAI-generated text, from identifying AI-generated content (detection),\ndetermining the specific AI model involved (attribution), and grouping the\nunderlying intents of the text (characterization). Furthermore, we explore\navailable resources for AI-generated text forensics research and discuss the\nevolving challenges and future directions of forensic systems in an AI era.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01152v1",
    "published_date": "2024-03-02 09:39:13 UTC",
    "updated_date": "2024-03-02 09:39:13 UTC"
  },
  {
    "arxiv_id": "2403.01147v1",
    "title": "A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model",
    "authors": [
      "Xinying Lu",
      "Doudou Zhang",
      "Jianli Xiao"
    ],
    "abstract": "In addition to enhancing traffic safety and facilitating prompt emergency\nresponse, traffic incident detection plays an indispensable role in intelligent\ntransportation systems by providing real-time traffic status information. This\nenables the realization of intelligent traffic control and management. Previous\nresearch has identified that apart from employing advanced algorithmic models,\nthe effectiveness of detection is also significantly influenced by challenges\nrelated to acquiring large datasets and addressing dataset imbalances. A hybrid\nmodel combining transformer and generative adversarial networks (GANs) is\nproposed to address these challenges. Experiments are conducted on four real\ndatasets to validate the superiority of the transformer in traffic incident\ndetection. Additionally, GANs are utilized to expand the dataset and achieve a\nbalanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against\nthe baseline model. The results demonstrate that the proposed model enhances\nthe dataset size, balances the dataset, and improves the performance of traffic\nincident detection in various aspects.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.01147v1",
    "published_date": "2024-03-02 09:28:04 UTC",
    "updated_date": "2024-03-02 09:28:04 UTC"
  },
  {
    "arxiv_id": "2403.04780v2",
    "title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
    "authors": [
      "Yanchao Tan",
      "Hang Lv",
      "Xinyi Huang",
      "Jiawei Zhang",
      "Shiping Wang",
      "Carl Yang"
    ],
    "abstract": "Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04780v2",
    "published_date": "2024-03-02 09:27:32 UTC",
    "updated_date": "2024-03-13 15:52:33 UTC"
  },
  {
    "arxiv_id": "2403.01139v4",
    "title": "ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies",
    "authors": [
      "Oren Sultan",
      "Yonatan Bitton",
      "Ron Yosef",
      "Dafna Shahaf"
    ],
    "abstract": "Analogy-making is central to human cognition, allowing us to adapt to novel\nsituations -- an ability that current AI systems still lack. Most analogy\ndatasets today focus on simple analogies (e.g., word analogies); datasets\nincluding complex types of analogies are typically manually curated and very\nsmall. We believe that this holds back progress in computational analogy. In\nthis work, we design a data generation pipeline, ParallelPARC (Parallel\nParagraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to\ncreate complex, paragraph-based analogies, as well as distractors, both simple\nand challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\nof analogies between scientific processes. We publish a gold-set, validated by\nhumans, and a silver-set, generated automatically. We test LLMs' and humans'\nanalogy recognition in binary and multiple-choice settings, and found that\nhumans outperform the best models (~13% gap) after a light supervision. We\ndemonstrate that our silver-set is useful for training models. Lastly, we show\nchallenging distractors confuse LLMs, but not humans. We hope our pipeline will\nencourage research in this emerging field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024 (Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2403.01139v4",
    "published_date": "2024-03-02 08:53:40 UTC",
    "updated_date": "2024-05-14 16:41:24 UTC"
  },
  {
    "arxiv_id": "2403.01136v1",
    "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization",
    "authors": [
      "Juntao Zhao",
      "Borui Wan",
      "Yanghua Peng",
      "Haibin Lin",
      "Chuan Wu"
    ],
    "abstract": "Recent breakthroughs in Large-scale language models (LLMs) have demonstrated\nimpressive performance on various tasks. The immense sizes of LLMs have led to\nvery high resource demand and cost for running the models. Though the models\nare largely served using uniform high-caliber GPUs nowadays, utilizing a\nheterogeneous cluster with a mix of available high- and low-capacity GPUs can\npotentially substantially reduce the serving cost. There is a lack of designs\nto support efficient LLM serving using a heterogeneous cluster, while the\ncurrent solutions focus on model partition and uniform compression among\nhomogeneous devices. This paper proposes LLM-PQ, a system that advocates\nadaptive model quantization and phase-aware partition to improve LLM serving\nefficiency on heterogeneous GPU clusters. We carefully decide on\nmixed-precision model quantization together with phase-aware model partition\nand micro-batch sizing in distributed LLM serving with an efficient algorithm,\nto greatly enhance inference throughput while fulfilling user-specified model\nquality targets. Extensive experiments on production inference workloads in 11\ndifferent clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on\naverage) throughput improvement in inference, showing great advantages over\nstate-of-the-art works.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01136v1",
    "published_date": "2024-03-02 08:40:07 UTC",
    "updated_date": "2024-03-02 08:40:07 UTC"
  },
  {
    "arxiv_id": "2403.01131v2",
    "title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation",
    "authors": [
      "Zeyuan Ma",
      "Hongshu Guo",
      "Jiacheng Chen",
      "Guojun Peng",
      "Zhiguang Cao",
      "Yining Ma",
      "Yue-Jiao Gong"
    ],
    "abstract": "Recent research explores optimization using large language models (LLMs) by\neither iteratively seeking next-step solutions from LLMs or directly prompting\nLLMs for an optimizer. However, these approaches exhibit inherent limitations,\nincluding low operational efficiency, high sensitivity to prompt design, and a\nlack of domain-specific knowledge. We introduce LLaMoCo, the first\ninstruction-tuning framework designed to adapt LLMs for solving optimization\nproblems in a code-to-code manner. Specifically, we establish a comprehensive\ninstruction set containing well-described problem prompts and effective\noptimization codes. We then develop a novel two-phase learning strategy that\nincorporates a contrastive learning-based warm-up procedure before the\ninstruction-tuning phase to enhance the convergence behavior during model\nfine-tuning. The experiment results demonstrate that a CodeGen (350M) model\nfine-tuned by our LLaMoCo achieves superior optimization performance compared\nto GPT-4 Turbo and the other competitors across both synthetic and realistic\nproblem sets. The fine-tuned model and the usage instructions are available at\nhttps://anonymous.4open.science/r/LLaMoCo-722A.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NE",
      "cs.SE"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01131v2",
    "published_date": "2024-03-02 08:21:59 UTC",
    "updated_date": "2024-03-05 11:11:41 UTC"
  },
  {
    "arxiv_id": "2403.01121v4",
    "title": "OpenGraph: Towards Open Graph Foundation Models",
    "authors": [
      "Lianghao Xia",
      "Ben Kao",
      "Chao Huang"
    ],
    "abstract": "Graph learning has become essential in various domains, including\nrecommendation systems and social network analysis. Graph Neural Networks\n(GNNs) have emerged as promising techniques for encoding structural information\nand improving performance in tasks like link prediction and node\nclassification. However, a key challenge remains: the difficulty of\ngeneralizing to unseen graph data with different properties. In this work, we\npropose a novel graph foundation model, called OpenGraph, to address this\nchallenge. Our approach tackles several technical obstacles. Firstly, we\nenhance data augmentation using a large language model (LLM) to overcome data\nscarcity in real-world scenarios. Secondly, we introduce a unified graph\ntokenizer that enables the model to generalize effectively to diverse graph\ndata, even when encountering unseen properties during training. Thirdly, our\ndeveloped scalable graph transformer captures node-wise dependencies within the\nglobal topological context. Extensive experiments validate the effectiveness of\nour framework. By adapting OpenGraph to new graph characteristics and\ncomprehending diverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings. We release the model\nimplementation at https://github.com/HKUDS/OpenGraph.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by EMNLP'2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01121v4",
    "published_date": "2024-03-02 08:05:03 UTC",
    "updated_date": "2024-10-09 12:10:38 UTC"
  },
  {
    "arxiv_id": "2403.01118v1",
    "title": "Adversarial Testing for Visual Grounding via Image-Aware Property Reduction",
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Junjie Wang",
      "Cheng Li",
      "Boyu Wu",
      "Fanjiang Xu",
      "Qing Wang"
    ],
    "abstract": "Due to the advantages of fusing information from various modalities,\nmultimodal learning is gaining increasing attention. Being a fundamental task\nof multimodal learning, Visual Grounding (VG), aims to locate objects in images\nthrough natural language expressions. Ensuring the quality of VG models\npresents significant challenges due to the complex nature of the task. In the\nblack box scenario, existing adversarial testing techniques often fail to fully\nexploit the potential of both modalities of information. They typically apply\nperturbations based solely on either the image or text information,\ndisregarding the crucial correlation between the two modalities, which would\nlead to failures in test oracles or an inability to effectively challenge VG\nmodels. To this end, we propose PEELING, a text perturbation approach via\nimage-aware property reduction for adversarial testing of the VG model. The\ncore idea is to reduce the property-related information in the original\nexpression meanwhile ensuring the reduced expression can still uniquely\ndescribe the original object in the image. To achieve this, PEELING first\nconducts the object and properties extraction and recombination to generate\ncandidate property reduction expressions. It then selects the satisfied\nexpressions that accurately describe the original object while ensuring no\nother objects in the image fulfill the expression, through querying the image\nwith a visual understanding technique. We evaluate PEELING on the\nstate-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets.\nResults show that the adversarial tests generated by PEELING achieves 21.4% in\nMultiModal Impact score (MMI), and outperforms state-of-the-art baselines for\nimages and texts by 8.2%--15.1%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.01118v1",
    "published_date": "2024-03-02 08:03:42 UTC",
    "updated_date": "2024-03-02 08:03:42 UTC"
  },
  {
    "arxiv_id": "2403.01106v2",
    "title": "Distilling Text Style Transfer With Self-Explanation From LLMs",
    "authors": [
      "Chiyu Zhang",
      "Honglong Cai",
      "Yuezhang",
      "Li",
      "Yuexin Wu",
      "Le Hou",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "Text Style Transfer (TST) seeks to alter the style of text while retaining\nits core content. Given the constraints of limited parallel datasets for TST,\nwe propose CoTeX, a framework that leverages large language models (LLMs)\nalongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills\nthe complex rewriting and reasoning capabilities of LLMs into more streamlined\nmodels capable of working with both non-parallel and parallel data. Through\nexperimentation across four TST datasets, CoTeX is shown to surpass traditional\nsupervised fine-tuning and knowledge distillation methods, particularly in\nlow-resource settings. We conduct a comprehensive evaluation, comparing CoTeX\nagainst current unsupervised, supervised, in-context learning (ICL) techniques,\nand instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering\ntransparent explanations for its style transfer process.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NAACL Student Research Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01106v2",
    "published_date": "2024-03-02 06:38:15 UTC",
    "updated_date": "2024-05-04 17:23:21 UTC"
  },
  {
    "arxiv_id": "2403.01101v2",
    "title": "Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models",
    "authors": [
      "Ziting Wen",
      "Oscar Pizarro",
      "Stefan Williams"
    ],
    "abstract": "Fine-tuning the pre-trained model with active learning holds promise for\nreducing annotation costs. However, this combination introduces significant\ncomputational costs, particularly with the growing scale of pre-trained models.\nRecent research has proposed proxy-based active learning, which pre-computes\nfeatures to reduce computational costs. Yet, this approach often incurs a\nsignificant loss in active learning performance, sometimes outweighing the\ncomputational cost savings. This paper demonstrates that not all sample\nselection differences result in performance degradation. Furthermore, we show\nthat suitable training methods can mitigate the decline of active learning\nperformance caused by certain selection discrepancies. Building upon detailed\nanalysis, we propose a novel method, aligned selection via proxy, which\nimproves proxy-based active learning performance by updating pre-computed\nfeatures and selecting a proper training method. Extensive experiments validate\nthat our method improves the total cost of efficient active learning while\nmaintaining computational efficiency. The code is available at\n\\url{https://github.com/ZiTingW/asvp}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Transactions on Machine Learning Research (TMLR, 2024)\n  https://openreview.net/forum?id=PNcgJMJcdl",
    "pdf_url": "http://arxiv.org/pdf/2403.01101v2",
    "published_date": "2024-03-02 06:01:34 UTC",
    "updated_date": "2024-11-16 06:45:43 UTC"
  },
  {
    "arxiv_id": "2403.01091v1",
    "title": "COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting",
    "authors": [
      "Wei Ju",
      "Yusheng Zhao",
      "Yifang Qin",
      "Siyu Yi",
      "Jingyang Yuan",
      "Zhiping Xiao",
      "Xiao Luo",
      "Xiting Yan",
      "Ming Zhang"
    ],
    "abstract": "This paper investigates traffic forecasting, which attempts to forecast the\nfuture state of traffic based on historical situations. This problem has\nreceived ever-increasing attention in various scenarios and facilitated the\ndevelopment of numerous downstream applications such as urban planning and\ntransportation management. However, the efficacy of existing methods remains\nsub-optimal due to their tendency to model temporal and spatial relationships\nindependently, thereby inadequately accounting for complex high-order\ninteractions of both worlds. Moreover, the diversity of transitional patterns\nin traffic forecasting makes them challenging to capture for existing\napproaches, warranting a deeper exploration of their diversity. Toward this\nend, this paper proposes Conjoint Spatio-Temporal graph neural network\n(abbreviated as COOL), which models heterogeneous graphs from prior and\nposterior information to conjointly capture high-order spatio-temporal\nrelationships. On the one hand, heterogeneous graphs connecting sequential\nobservation are constructed to extract composite spatio-temporal relationships\nvia prior message passing. On the other hand, we model dynamic relationships\nusing constructed affinity and penalty graphs, which guide posterior message\npassing to incorporate complementary semantic information into node\nrepresentations. Moreover, to capture diverse transitional properties to\nenhance traffic forecasting, we propose a conjoint self-attention decoder that\nmodels diverse temporal patterns from both multi-rank and multi-scale views.\nExperimental results on four popular benchmark datasets demonstrate that our\nproposed COOL provides state-of-the-art performance compared with the\ncompetitive baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Information Fusion 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01091v1",
    "published_date": "2024-03-02 04:30:09 UTC",
    "updated_date": "2024-03-02 04:30:09 UTC"
  },
  {
    "arxiv_id": "2403.01079v1",
    "title": "Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework",
    "authors": [
      "Junxian Li",
      "Bin Shi",
      "Erfei Cui",
      "Hua Wei",
      "Qinghua Zheng"
    ],
    "abstract": "We study the challenging problem for inference tasks on large-scale graph\ndatasets of Graph Neural Networks: huge time and memory consumption, and try to\novercome it by reducing reliance on graph structure. Even though distilling\ngraph knowledge to student MLP is an excellent idea, it faces two major\nproblems of positional information loss and low generalization. To solve the\nproblems, we propose a new three-stage multitask distillation framework. In\ndetail, we use Positional Encoding to capture positional information. Also, we\nintroduce Neural Heat Kernels responsible for graph data processing in GNN and\nutilize hidden layer outputs matching for better performance of student MLP's\nhidden layers. To the best of our knowledge, it is the first work to include\nhidden layer distillation for student MLP on graphs and to combine graph\nPositional Encoding with MLP. We test its performance and robustness with\nseveral settings and draw the conclusion that our work can outperform well with\ngood stability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, with Appendix",
    "pdf_url": "http://arxiv.org/pdf/2403.01079v1",
    "published_date": "2024-03-02 03:29:11 UTC",
    "updated_date": "2024-03-02 03:29:11 UTC"
  },
  {
    "arxiv_id": "2403.01078v1",
    "title": "$Γ$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data",
    "authors": [
      "Jason Z. Kim",
      "Nicolas Perrin-Gilbert",
      "Erkan Narmanli",
      "Paul Klein",
      "Christopher R. Myers",
      "Itai Cohen",
      "Joshua J. Waterfall",
      "James P. Sethna"
    ],
    "abstract": "Natural systems with emergent behaviors often organize along low-dimensional\nsubsets of high-dimensional spaces. For example, despite the tens of thousands\nof genes in the human genome, the principled study of genomics is fruitful\nbecause biological processes rely on coordinated organization that results in\nlower dimensional phenotypes. To uncover this organization, many nonlinear\ndimensionality reduction techniques have successfully embedded high-dimensional\ndata into low-dimensional spaces by preserving local similarities between data\npoints. However, the nonlinearities in these methods allow for too much\ncurvature to preserve general trends across multiple non-neighboring data\nclusters, thereby limiting their interpretability and generalizability to\nout-of-distribution data. Here, we address both of these limitations by\nregularizing the curvature of manifolds generated by variational autoencoders,\na process we coin ``$\\Gamma$-VAE''. We demonstrate its utility using two\nexample data sets: bulk RNA-seq from the The Cancer Genome Atlas (TCGA) and the\nGenotype Tissue Expression (GTEx); and single cell RNA-seq from a lineage\ntracing experiment in hematopoietic stem cell differentiation. We find that the\nresulting regularized manifolds identify mesoscale structure associated with\ndifferent cancer cell types, and accurately re-embed tissues from completely\nunseen, out-of distribution cancers as if they were originally trained on them.\nFinally, we show that preserving long-range relationships to differentiated\ncells separates undifferentiated cells -- which have not yet specialized --\naccording to their eventual fate. Broadly, we anticipate that regularizing the\ncurvature of generative models will enable more consistent, predictive, and\ngeneralizable models in any high-dimensional system with emergent\nlow-dimensional behavior.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.bio-ph",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.01078v1",
    "published_date": "2024-03-02 03:26:09 UTC",
    "updated_date": "2024-03-02 03:26:09 UTC"
  },
  {
    "arxiv_id": "2403.12981v1",
    "title": "Beyond Inference: Performance Analysis of DNN Server Overheads for Computer Vision",
    "authors": [
      "Ahmed F. AbouElhamayed",
      "Susanne Balle",
      "Deshanand Singh",
      "Mohamed S. Abdelfattah"
    ],
    "abstract": "Deep neural network (DNN) inference has become an important part of many\ndata-center workloads. This has prompted focused efforts to design ever-faster\ndeep learning accelerators such as GPUs and TPUs. However, an end-to-end\nDNN-based vision application contains more than just DNN inference, including\ninput decompression, resizing, sampling, normalization, and data transfer. In\nthis paper, we perform a thorough evaluation of computer vision inference\nrequests performed on a throughput-optimized serving system. We quantify the\nperformance impact of server overheads such as data movement, preprocessing,\nand message brokers between two DNNs producing outputs at different rates. Our\nempirical analysis encompasses many computer vision tasks including image\nclassification, segmentation, detection, depth-estimation, and more complex\nprocessing pipelines with multiple DNNs. Our results consistently demonstrate\nthat end-to-end application performance can easily be dominated by data\nprocessing and data movement functions (up to 56% of end-to-end latency in a\nmedium-sized image, and $\\sim$ 80% impact on system throughput in a large\nimage), even though these functions have been conventionally overlooked in deep\nlearning system design. Our work identifies important performance bottlenecks\nin different application scenarios, achieves 2.25$\\times$ better throughput\ncompared to prior work, and paves the way for more holistic deep learning\nsystem design.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "6 pages, 11 figures, DAC 2024: 61st IEEE/ACM Design Automation\n  Conference. (DAC'24)",
    "pdf_url": "http://arxiv.org/pdf/2403.12981v1",
    "published_date": "2024-03-02 02:35:08 UTC",
    "updated_date": "2024-03-02 02:35:08 UTC"
  },
  {
    "arxiv_id": "2403.01071v2",
    "title": "GraphRCG: Self-Conditioned Graph Generation",
    "authors": [
      "Song Wang",
      "Zhen Tan",
      "Xinyu Zhao",
      "Tianlong Chen",
      "Huan Liu",
      "Jundong Li"
    ],
    "abstract": "Graph generation generally aims to create new graphs that closely align with\na specific graph distribution. Existing works often implicitly capture this\ndistribution through the optimization of generators, potentially overlooking\nthe intricacies of the distribution itself. Furthermore, these approaches\ngenerally neglect the insights offered by the learned distribution for graph\ngeneration. In contrast, in this work, we propose a novel self-conditioned\ngraph generation framework designed to explicitly model graph distributions and\nemploy these distributions to guide the generation process. We first perform\nself-conditioned modeling to capture the graph distributions by transforming\neach graph sample into a low-dimensional representation and optimizing a\nrepresentation generator to create new representations reflective of the\nlearned distribution. Subsequently, we leverage these bootstrapped\nrepresentations as self-conditioned guidance for the generation process,\nthereby facilitating the generation of graphs that more accurately reflect the\nlearned distributions. We conduct extensive experiments on generic and\nmolecular graph datasets across various fields. Our framework demonstrates\nsuperior performance over existing state-of-the-art graph generation methods in\nterms of graph quality and fidelity to training data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01071v2",
    "published_date": "2024-03-02 02:28:20 UTC",
    "updated_date": "2024-07-18 06:05:58 UTC"
  },
  {
    "arxiv_id": "2403.01055v1",
    "title": "Towards Full Authorship with AI: Supporting Revision with AI-Generated Views",
    "authors": [
      "Jiho Kim",
      "Ray C. Flanagan",
      "Noelle E. Haviland",
      "ZeAi Sun",
      "Souad N. Yakubu",
      "Edom A. Maru",
      "Kenneth C. Arnold"
    ],
    "abstract": "Large language models (LLMs) are shaping a new user interface (UI) paradigm\nin writing tools by enabling users to generate text through prompts. This\nparadigm shifts some creative control from the user to the system, thereby\ndiminishing the user's authorship and autonomy in the writing process. To\nrestore autonomy, we introduce Textfocals, a UI prototype designed to\ninvestigate a human-centered approach that emphasizes the user's role in\nwriting. Textfocals supports the writing process by providing LLM-generated\nsummaries, questions, and advice (i.e., LLM views) in a sidebar of a text\neditor, encouraging reflection and self-driven revision in writing without\ndirect text generation. Textfocals' UI affordances, including contextually\nadaptive views and scaffolding for prompt selection and customization, offer a\nnovel way to interact with LLMs where users maintain full authorship of their\nwriting. A formative user study with Textfocals showed promising evidence that\nthis approach might help users develop underdeveloped ideas, cater to the\nrhetorical audience, and clarify their writing. However, the study also showed\ninteraction design challenges related to document navigation and scoping,\nprompt engineering, and context management. Our work highlights the breadth of\nthe design space of writing support interfaces powered by generative AI that\nmaintain authorship integrity.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "H.5.2; I.7.1; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "15 pages, 2 figures; Accepted to 5th Workshop on Human-AI Co-Creation\n  with Generative Models (HAI-GEN) at ACM IUI 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01055v1",
    "published_date": "2024-03-02 01:11:35 UTC",
    "updated_date": "2024-03-02 01:11:35 UTC"
  },
  {
    "arxiv_id": "2403.01053v2",
    "title": "Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling",
    "authors": [
      "Jianan Fan",
      "Dongnan Liu",
      "Hang Chang",
      "Heng Huang",
      "Mei Chen",
      "Weidong Cai"
    ],
    "abstract": "Machine learning holds tremendous promise for transforming the fundamental\npractice of scientific discovery by virtue of its data-driven nature. With the\never-increasing stream of research data collection, it would be appealing to\nautonomously explore patterns and insights from observational data for\ndiscovering novel classes of phenotypes and concepts. However, in the\nbiomedical domain, there are several challenges inherently presented in the\ncumulated data which hamper the progress of novel class discovery. The\nnon-i.i.d. data distribution accompanied by the severe imbalance among\ndifferent groups of classes essentially leads to ambiguous and biased semantic\nrepresentations. In this work, we present a geometry-constrained probabilistic\nmodeling treatment to resolve the identified issues. First, we propose to\nparameterize the approximated posterior of instance embedding as a marginal von\nMisesFisher distribution to account for the interference of distributional\nlatent bias. Then, we incorporate a suite of critical geometric properties to\nimpose proper constraints on the layout of constructed embedding space, which\nin turn minimizes the uncontrollable risk for unknown class learning and\nstructuring. Furthermore, a spectral graph-theoretic method is devised to\nestimate the number of potential novel classes. It inherits two intriguing\nmerits compared to existent approaches, namely high computational efficiency\nand flexibility for taxonomy-adaptive estimation. Extensive experiments across\nvarious biomedical scenarios substantiate the effectiveness and general\napplicability of our method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01053v2",
    "published_date": "2024-03-02 00:56:05 UTC",
    "updated_date": "2024-03-05 07:36:04 UTC"
  },
  {
    "arxiv_id": "2403.01046v4",
    "title": "A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features",
    "authors": [
      "Emi Zeger",
      "Yifei Wang",
      "Aaron Mishkin",
      "Tolga Ergen",
      "Emmanuel Candès",
      "Mert Pilanci"
    ],
    "abstract": "We prove that training neural networks on 1-D data is equivalent to solving\nconvex Lasso problems with discrete, explicitly defined dictionary matrices. We\nconsider neural networks with piecewise linear activations and depths ranging\nfrom 2 to an arbitrary but finite number of layers. We first show that\ntwo-layer networks with piecewise linear activations are equivalent to Lasso\nmodels using a discrete dictionary of ramp functions, with breakpoints\ncorresponding to the training data points. In certain general architectures\nwith absolute value or ReLU activations, a third layer surprisingly creates\nfeatures that reflect the training data about themselves. Additional layers\nprogressively generate reflections of these reflections. The Lasso\nrepresentation provides valuable insights into the analysis of globally optimal\nnetworks, elucidating their solution landscapes and enabling closed-form\nsolutions in certain special cases. Numerical results show that reflections\nalso occur when optimizing standard deep networks using standard non-convex\noptimizers. Additionally, we demonstrate our theory with autoregressive time\nseries models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01046v4",
    "published_date": "2024-03-02 00:33:45 UTC",
    "updated_date": "2024-07-24 00:32:35 UTC"
  },
  {
    "arxiv_id": "2403.01038v1",
    "title": "AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks",
    "authors": [
      "Jiacen Xu",
      "Jack W. Stokes",
      "Geoff McDonald",
      "Xuesong Bai",
      "David Marshall",
      "Siyue Wang",
      "Adith Swaminathan",
      "Zhou Li"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n\"hands-on-keyboard\" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization's network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01038v1",
    "published_date": "2024-03-02 00:10:45 UTC",
    "updated_date": "2024-03-02 00:10:45 UTC"
  }
]
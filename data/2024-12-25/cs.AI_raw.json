[
  {
    "arxiv_id": "2412.19002v1",
    "title": "Tempus Core: Area-Power Efficient Temporal-Unary Convolution Core for Low-Precision Edge DLAs",
    "authors": [
      "Prabhu Vellaisamy",
      "Harideep Nair",
      "Thomas Kang",
      "Yichen Ni",
      "Haoyang Fan",
      "Bin Qi",
      "Jeff Chen",
      "Shawn Blanton",
      "John Paul Shen"
    ],
    "abstract": "The increasing complexity of deep neural networks (DNNs) poses significant\nchallenges for edge inference deployment due to resource and power constraints\nof edge devices. Recent works on unary-based matrix multiplication hardware aim\nto leverage data sparsity and low-precision values to enhance hardware\nefficiency. However, the adoption and integration of such unary hardware into\ncommercial deep learning accelerators (DLA) remain limited due to processing\nelement (PE) array dataflow differences. This work presents Tempus Core, a\nconvolution core with highly scalable unary-based PE array comprising of tub\n(temporal-unary-binary) multipliers that seamlessly integrates with the NVDLA\n(NVIDIA's open-source DLA for accelerating CNNs) while maintaining dataflow\ncompliance and boosting hardware efficiency. Analysis across various datapath\ngranularities shows that for INT8 precision in 45nm CMOS, Tempus Core's PE cell\nunit (PCU) yields 59.3% and 15.3% reductions in area and power consumption,\nrespectively, over NVDLA's CMAC unit. Considering a 16x16 PE array in Tempus\nCore, area and power improves by 75% and 62%, respectively, while delivering 5x\nand 4x iso-area throughput improvements for INT8 and INT4 precisions.\nPost-place and route analysis of Tempus Core's PCU shows that the 16x4 PE array\nfor INT4 precision in 45nm CMOS requires only 0.017 mm^2 die area and consumes\nonly 6.2mW of total power. We demonstrate that area-power efficient unary-based\nhardware can be seamlessly integrated into conventional DLAs, paving the path\nfor efficient unary hardware for edge AI inference.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted in DATE 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.19002v1",
    "published_date": "2024-12-25 23:20:02 UTC",
    "updated_date": "2024-12-25 23:20:02 UTC"
  },
  {
    "arxiv_id": "2412.18994v1",
    "title": "Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with AI for Enhanced Urban Mapping",
    "authors": [
      "Sajjad Afroosheh",
      "Mohammadreza Askari"
    ],
    "abstract": "This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),\nand optical imagery through advanced artificial intelligence techniques for\nenhanced urban mapping. By fusing these diverse geospatial datasets, we aim to\novercome the limitations associated with single-sensor data, achieving a more\ncomprehensive representation of urban environments. The research employs Fully\nConvolutional Networks (FCNs) as the primary deep learning model for urban\nfeature extraction, enabling precise pixel-wise classification of essential\nurban elements, including buildings, roads, and vegetation. To optimize the\nperformance of the FCN model, we utilize Particle Swarm Optimization (PSO) for\nhyperparameter tuning, significantly enhancing model accuracy. Key findings\nindicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean\nIntersection over Union (IoU) of 87.6%, surpassing traditional single-sensor\napproaches. These results underscore the potential of fused geospatial data and\nAI-driven methodologies in urban mapping, providing valuable insights for urban\nplanning and management. The implications of this research pave the way for\nfuture developments in real-time mapping and adaptive urban infrastructure\nplanning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18994v1",
    "published_date": "2024-12-25 22:17:31 UTC",
    "updated_date": "2024-12-25 22:17:31 UTC"
  },
  {
    "arxiv_id": "2412.19856v1",
    "title": "Fusion of Deep Learning and GIS for Advanced Remote Sensing Image Analysis",
    "authors": [
      "Sajjad Afroosheh",
      "Mohammadreza Askari"
    ],
    "abstract": "This paper presents an innovative framework for remote sensing image analysis\nby fusing deep learning techniques, specifically Convolutional Neural Networks\n(CNNs) and Long Short-Term Memory (LSTM) networks, with Geographic Information\nSystems (GIS). The primary objective is to enhance the accuracy and efficiency\nof spatial data analysis by overcoming challenges associated with high\ndimensionality, complex patterns, and temporal data processing. We implemented\noptimization algorithms, namely Particle Swarm Optimization (PSO) and Genetic\nAlgorithms (GA), to fine-tune model parameters, resulting in improved\nperformance metrics. Our findings reveal a significant increase in\nclassification accuracy from 78% to 92% and a reduction in prediction error\nfrom 12% to 6% after optimization. Additionally, the temporal accuracy of the\nmodels improved from 75% to 88%, showcasing the frameworks capability to\nmonitor dynamic changes effectively. The integration of GIS not only enriched\nthe spatial analysis but also facilitated a deeper understanding of the\nrelationships between geographical features. This research demonstrates that\ncombining advanced deep learning methods with GIS and optimization strategies\ncan significantly advance remote sensing applications, paving the way for\nfuture developments in environmental monitoring, urban planning, and resource\nmanagement.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.19856v1",
    "published_date": "2024-12-25 22:10:35 UTC",
    "updated_date": "2024-12-25 22:10:35 UTC"
  },
  {
    "arxiv_id": "2412.18989v2",
    "title": "How Propense Are Large Language Models at Producing Code Smells? A Benchmarking Study",
    "authors": [
      "Alejandro Velasco",
      "Daniel Rodriguez-Cardenas",
      "Luftar Rahman Alif",
      "David N. Palacio",
      "Denys Poshyvanyk"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant potential in automating\nsoftware engineering tasks, particularly in code generation. However, current\nevaluation benchmarks, which primarily focus on accuracy, fall short in\nassessing the quality of the code generated by these models, specifically their\ntendency to produce code smells. To address this limitation, we introduce\nCodeSmellEval, a benchmark designed to evaluate the propensity of LLMs for\ngenerating code smells. Our benchmark includes a novel metric: Propensity\nSmelly Score (PSC), and a curated dataset of method-level code smells:\nCodeSmellData. To demonstrate the use of CodeSmellEval, we conducted a case\nstudy with two state-of-the-art LLMs, CodeLlama and Mistral. The results reveal\nthat both models tend to generate code smells, such as simplifiable-condition\nand consider-merging-isinstance. These findings highlight the effectiveness of\nour benchmark in evaluating LLMs, providing valuable insights into their\nreliability and their propensity to introduce code smells in code generation\ntasks.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18989v2",
    "published_date": "2024-12-25 21:56:35 UTC",
    "updated_date": "2025-01-18 20:14:21 UTC"
  },
  {
    "arxiv_id": "2412.18985v1",
    "title": "TravelAgent: Generative Agents in the Built Environment",
    "authors": [
      "Ariel Noyman",
      "Kai Hu",
      "Kent Larson"
    ],
    "abstract": "Understanding human behavior in built environments is critical for designing\nfunctional, user centered urban spaces. Traditional approaches, such as manual\nobservations, surveys, and simplified simulations, often fail to capture the\ncomplexity and dynamics of real world behavior. To address these limitations,\nwe introduce TravelAgent, a novel simulation platform that models pedestrian\nnavigation and activity patterns across diverse indoor and outdoor environments\nunder varying contextual and environmental conditions. TravelAgent leverages\ngenerative agents integrated into 3D virtual environments, enabling agents to\nprocess multimodal sensory inputs and exhibit human-like decision-making,\nbehavior, and adaptation. Through experiments, including navigation,\nwayfinding, and free exploration, we analyze data from 100 simulations\ncomprising 1898 agent steps across diverse spatial layouts and agent\narchetypes, achieving an overall task completion rate of 76%. Using spatial,\nlinguistic, and sentiment analyses, we show how agents perceive, adapt to, or\nstruggle with their surroundings and assigned tasks. Our findings highlight the\npotential of TravelAgent as a tool for urban design, spatial cognition\nresearch, and agent-based modeling. We discuss key challenges and opportunities\nin deploying generative agents for the evaluation and refinement of spatial\ndesigns, proposing TravelAgent as a new paradigm for simulating and\nunderstanding human experiences in built environments.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages 9 figs",
    "pdf_url": "http://arxiv.org/pdf/2412.18985v1",
    "published_date": "2024-12-25 21:27:51 UTC",
    "updated_date": "2024-12-25 21:27:51 UTC"
  },
  {
    "arxiv_id": "2412.18975v1",
    "title": "Injecting Bias into Text Classification Models using Backdoor Attacks",
    "authors": [
      "A. Dilara Yavuz",
      "M. Emre Gursoy"
    ],
    "abstract": "The rapid growth of natural language processing (NLP) and pre-trained\nlanguage models have enabled accurate text classification in a variety of\nsettings. However, text classification models are susceptible to backdoor\nattacks, where an attacker embeds a trigger into the victim model to make the\nmodel predict attacker-desired labels in targeted scenarios. In this paper, we\npropose to utilize backdoor attacks for a new purpose: bias injection. We\ndevelop a backdoor attack in which a subset of the training dataset is poisoned\nto associate strong male actors with negative sentiment. We execute our attack\non two popular text classification datasets (IMDb and SST) and seven different\nmodels ranging from traditional Doc2Vec-based models to LSTM networks and\nmodern transformer-based BERT and RoBERTa models. Our results show that the\nreduction in backdoored models' benign classification accuracy is limited,\nimplying that our attacks remain stealthy, whereas the models successfully\nlearn to associate strong male actors with negative sentiment (100% attack\nsuccess rate with >= 3% poison rate). Attacks on BERT and RoBERTa are\nparticularly more stealthy and effective, demonstrating an increased risk of\nusing modern and larger models. We also measure the generalizability of our\nbias injection by proposing two metrics: (i) U-BBSR which uses previously\nunseen words when measuring attack success, and (ii) P-BBSR which measures\nattack success using paraphrased test samples. U-BBSR and P-BBSR results show\nthat the bias injected by our attack can go beyond memorizing a trigger phrase.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18975v1",
    "published_date": "2024-12-25 19:32:02 UTC",
    "updated_date": "2024-12-25 19:32:02 UTC"
  },
  {
    "arxiv_id": "2412.18972v1",
    "title": "Recommending Pre-Trained Models for IoT Devices",
    "authors": [
      "Parth V. Patil",
      "Wenxin Jiang",
      "Huiyun Peng",
      "Daniel Lugo",
      "Kelechi G. Kalu",
      "Josh LeBlanc",
      "Lawrence Smith",
      "Hyeonwoo Heo",
      "Nathanael Aou",
      "James C. Davis"
    ],
    "abstract": "The availability of pre-trained models (PTMs) has enabled faster deployment\nof machine learning across applications by reducing the need for extensive\ntraining. Techniques like quantization and distillation have further expanded\nPTM applicability to resource-constrained IoT hardware. Given the many PTM\noptions for any given task, engineers often find it too costly to evaluate each\nmodel's suitability. Approaches such as LogME, LEEP, and ModelSpider help\nstreamline model selection by estimating task relevance without exhaustive\ntuning. However, these methods largely leave hardware constraints as future\nwork-a significant limitation in IoT settings. In this paper, we identify the\nlimitations of current model recommendation approaches regarding hardware\nconstraints and introduce a novel, hardware-aware method for PTM selection. We\nalso propose a research agenda to guide the development of effective,\nhardware-conscious model recommendation systems for IoT applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at SERP4IOT'25",
    "pdf_url": "http://arxiv.org/pdf/2412.18972v1",
    "published_date": "2024-12-25 19:19:55 UTC",
    "updated_date": "2024-12-25 19:19:55 UTC"
  },
  {
    "arxiv_id": "2412.18966v1",
    "title": "ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and Language Understanding Enhancement",
    "authors": [
      "Zhefan Rao",
      "Liya Ji",
      "Yazhou Xing",
      "Runtao Liu",
      "Zhaoyang Liu",
      "Jiaxin Xie",
      "Ziqiao Peng",
      "Yingqing He",
      "Qifeng Chen"
    ],
    "abstract": "Text-to-video (T2V) generation has gained significant attention recently.\nHowever, the costs of training a T2V model from scratch remain persistently\nhigh, and there is considerable room for improving the generation performance,\nespecially under limited computation resources. This work explores the\ncontinual general pre-training of text-to-video models, enabling the model to\n\"grow\" its abilities based on a pre-trained foundation, analogous to how humans\nacquire new knowledge based on past experiences. There is a lack of extensive\nstudy of the continual pre-training techniques in T2V generation. In this work,\nwe take the initial step toward exploring this task systematically and propose\nModelGrow. Specifically, we break this task into two key aspects: increasing\nmodel capacity and improving semantic understanding. For model capacity, we\nintroduce several novel techniques to expand the model size, enabling it to\nstore new knowledge and improve generation performance. For semantic\nunderstanding, we propose a method that leverages large language models as\nadvanced text encoders, integrating them into T2V models to enhance language\ncomprehension and guide generation results according to detailed prompts. This\napproach enables the model to achieve better semantic alignment, particularly\nin response to complex user prompts. Extensive experiments demonstrate the\neffectiveness of our method across various metrics. The source code and the\nmodel of ModelGrow will be publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.18966v1",
    "published_date": "2024-12-25 18:58:07 UTC",
    "updated_date": "2024-12-25 18:58:07 UTC"
  },
  {
    "arxiv_id": "2412.18952v1",
    "title": "Bridging Interpretability and Robustness Using LIME-Guided Model Refinement",
    "authors": [
      "Navid Nayyem",
      "Abdullah Rakin",
      "Longwei Wang"
    ],
    "abstract": "This paper explores the intricate relationship between interpretability and\nrobustness in deep learning models. Despite their remarkable performance across\nvarious tasks, deep learning models often exhibit critical vulnerabilities,\nincluding susceptibility to adversarial attacks, over-reliance on spurious\ncorrelations, and a lack of transparency in their decision-making processes. To\naddress these limitations, we propose a novel framework that leverages Local\nInterpretable Model-Agnostic Explanations (LIME) to systematically enhance\nmodel robustness. By identifying and mitigating the influence of irrelevant or\nmisleading features, our approach iteratively refines the model, penalizing\nreliance on these features during training. Empirical evaluations on multiple\nbenchmark datasets demonstrate that LIME-guided refinement not only improves\ninterpretability but also significantly enhances resistance to adversarial\nperturbations and generalization to out-of-distribution data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.18952v1",
    "published_date": "2024-12-25 17:32:45 UTC",
    "updated_date": "2024-12-25 17:32:45 UTC"
  },
  {
    "arxiv_id": "2412.18947v4",
    "title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models",
    "authors": [
      "Kaiwen Zuo",
      "Yirui Jiang"
    ],
    "abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published to AAAI-25 Bridge Program",
    "pdf_url": "http://arxiv.org/pdf/2412.18947v4",
    "published_date": "2024-12-25 16:51:29 UTC",
    "updated_date": "2025-03-28 23:37:45 UTC"
  },
  {
    "arxiv_id": "2412.18946v1",
    "title": "Constraint-Adaptive Policy Switching for Offline Safe Reinforcement Learning",
    "authors": [
      "Yassine Chemingui",
      "Aryan Deshwal",
      "Honghao Wei",
      "Alan Fern",
      "Janardhan Rao Doppa"
    ],
    "abstract": "Offline safe reinforcement learning (OSRL) involves learning a\ndecision-making policy to maximize rewards from a fixed batch of training data\nto satisfy pre-defined safety constraints. However, adapting to varying safety\nconstraints during deployment without retraining remains an under-explored\nchallenge. To address this challenge, we introduce constraint-adaptive policy\nswitching (CAPS), a wrapper framework around existing offline RL algorithms.\nDuring training, CAPS uses offline data to learn multiple policies with a\nshared representation that optimize different reward and cost trade-offs.\nDuring testing, CAPS switches between those policies by selecting at each state\nthe policy that maximizes future rewards among those that satisfy the current\ncost constraint. Our experiments on 38 tasks from the DSRL benchmark\ndemonstrate that CAPS consistently outperforms existing methods, establishing a\nstrong wrapper-based baseline for OSRL. The code is publicly available at\nhttps://github.com/yassineCh/CAPS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18946v1",
    "published_date": "2024-12-25 16:42:27 UTC",
    "updated_date": "2024-12-25 16:42:27 UTC"
  },
  {
    "arxiv_id": "2412.18926v1",
    "title": "Exemplar-condensed Federated Class-incremental Learning",
    "authors": [
      "Rui Sun",
      "Yumin Zhang",
      "Varun Ojha",
      "Tejal Shah",
      "Haoran Duan",
      "Bo Wei",
      "Rajiv Ranjan"
    ],
    "abstract": "We propose Exemplar-Condensed federated class-incremental learning (ECoral)\nto distil the training characteristics of real images from streaming data into\ninformative rehearsal exemplars. The proposed method eliminates the limitations\nof exemplar selection in replay-based approaches for mitigating catastrophic\nforgetting in federated continual learning (FCL). The limitations particularly\nrelated to the heterogeneity of information density of each summarized data.\nOur approach maintains the consistency of training gradients and the\nrelationship to past tasks for the summarized exemplars to represent the\nstreaming data compared to the original images effectively. Additionally, our\napproach reduces the information-level heterogeneity of the summarized data by\ninter-client sharing of the disentanglement generative model. Extensive\nexperiments show that our ECoral outperforms several state-of-the-art methods\nand can be seamlessly integrated with many existing approaches to enhance\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18926v1",
    "published_date": "2024-12-25 15:13:40 UTC",
    "updated_date": "2024-12-25 15:13:40 UTC"
  },
  {
    "arxiv_id": "2412.18925v1",
    "title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs",
    "authors": [
      "Junying Chen",
      "Zhenyang Cai",
      "Ke Ji",
      "Xidong Wang",
      "Wanlong Liu",
      "Rongsheng Wang",
      "Jianye Hou",
      "Benyou Wang"
    ],
    "abstract": "The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning\nto improve LLM. Yet, most research in reasoning has focused on mathematical\ntasks, leaving domains like medicine underexplored. The medical domain, though\ndistinct from mathematics, also demands robust reasoning to provide reliable\nanswers, given the high standards of healthcare. However, verifying medical\nreasoning is challenging, unlike those in mathematics. To address this, we\npropose verifiable medical problems with a medical verifier to check the\ncorrectness of model outputs. This verifiable nature enables advancements in\nmedical reasoning through a two-stage approach: (1) using the verifier to guide\nthe search for a complex reasoning trajectory for fine-tuning LLMs, (2)\napplying reinforcement learning (RL) with verifier-based rewards to enhance\ncomplex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM\ncapable of complex reasoning, which outperforms general and medical-specific\nbaselines using only 40K verifiable problems. Experiments show complex\nreasoning improves medical problem-solving and benefits more from RL. We hope\nour approach inspires advancements in reasoning across medical and other\nspecialized domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18925v1",
    "published_date": "2024-12-25 15:12:34 UTC",
    "updated_date": "2024-12-25 15:12:34 UTC"
  },
  {
    "arxiv_id": "2501.00042v1",
    "title": "Resource-Efficient Transformer Architecture: Optimizing Memory and Execution Time for Real-Time Applications",
    "authors": [
      "Krisvarish V",
      "Priyadarshini T",
      "K P Abhishek Sri Saai",
      "Vaidehi Vijayakumar"
    ],
    "abstract": "This paper describes a memory-efficient transformer model designed to drive a\nreduction in memory usage and execution time by substantial orders of magnitude\nwithout impairing the model's performance near that of the original model.\nRecently, new architectures of transformers were presented, focused on\nparameter efficiency and computational optimization; however, such models\nusually require considerable resources in terms of hardware when deployed in\nreal-world applications on edge devices. This approach addresses this concern\nby halving embedding size and applying targeted techniques such as parameter\npruning and quantization to optimize the memory footprint with minimum\nsacrifices in terms of accuracy. Experimental results include a 52% reduction\nin memory usage and a 33% decrease in execution time, resulting in better\nefficiency than state-of-the-art models. This work compared our model with\nexisting compelling architectures, such as MobileBERT and DistilBERT, and\nproved its feasibility in the domain of resource-friendly deep learning\narchitectures, mainly for applications in real-time and in resource-constrained\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2501.00042v1",
    "published_date": "2024-12-25 14:41:23 UTC",
    "updated_date": "2024-12-25 14:41:23 UTC"
  },
  {
    "arxiv_id": "2412.18917v1",
    "title": "Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of Vision-Language Multiway Transformer Model",
    "authors": [
      "Yi-Chia Chen",
      "Wei-Hua Li",
      "Chu-Song Chen"
    ],
    "abstract": "Open-vocabulary panoptic segmentation remains a challenging problem. One of\nthe biggest difficulties lies in training models to generalize to an unlimited\nnumber of classes using limited categorized training data. Recent popular\nmethods involve large-scale vision-language pre-trained foundation models, such\nas CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation\nusing another large-scale vision-language pre-trained model called BEiT-3 and\nleveraging the cross-modal attention between visual and linguistic features in\nBEiT-3 to achieve better performance. Experiments result demonstrates that\nOMTSeg performs favorably against state-of-the-art models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICIP 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.18917v1",
    "published_date": "2024-12-25 14:31:00 UTC",
    "updated_date": "2024-12-25 14:31:00 UTC"
  },
  {
    "arxiv_id": "2412.18914v2",
    "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
    "authors": [
      "Dulhan Jayalath",
      "James Bradley Wendt",
      "Nicholas Monath",
      "Sandeep Tata",
      "Beliz Gunel"
    ],
    "abstract": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 7 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.18914v2",
    "published_date": "2024-12-25 14:14:31 UTC",
    "updated_date": "2025-03-12 17:59:18 UTC"
  },
  {
    "arxiv_id": "2412.18911v1",
    "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
    "authors": [
      "Chang Zou",
      "Evelyn Zhang",
      "Runlin Guo",
      "Haohang Xu",
      "Conghui He",
      "Xuming Hu",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18911v1",
    "published_date": "2024-12-25 14:00:14 UTC",
    "updated_date": "2024-12-25 14:00:14 UTC"
  },
  {
    "arxiv_id": "2412.18910v1",
    "title": "AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of Adaptive Draft Structures",
    "authors": [
      "Situo Zhang",
      "Hankun Wang",
      "Da Ma",
      "Zichen Zhu",
      "Lu Chen",
      "Kunyao Lan",
      "Kai Yu"
    ],
    "abstract": "Speculative Decoding (SD) is a popular lossless technique for accelerating\nthe inference of Large Language Models (LLMs). We show that the decoding speed\nof SD frameworks with static draft structures can be significantly improved by\nincorporating context-aware adaptive draft structures. However, current studies\non adaptive draft structures are limited by their performance, modeling\napproaches, and applicability. In this paper, we introduce AdaEAGLE, the first\nSD framework that explicitly models adaptive draft structures. AdaEAGLE\nleverages the Lightweight Draft Length Predictor (LDLP) module to explicitly\npredict the optimal number of draft tokens during inference to guide the draft\nmodel. It achieves comparable speedup results without manual thresholds and\nallows for deeper, more specialized optimizations. Moreover, together with\nthreshold-based strategies, AdaEAGLE achieves a $1.62\\times$ speedup over the\nvanilla AR decoding and outperforms fixed-length SotA baseline while\nmaintaining output quality.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18910v1",
    "published_date": "2024-12-25 13:57:33 UTC",
    "updated_date": "2024-12-25 13:57:33 UTC"
  },
  {
    "arxiv_id": "2412.18907v2",
    "title": "EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation",
    "authors": [
      "Carl Qi",
      "Dan Haramati",
      "Tal Daniel",
      "Aviv Tamar",
      "Amy Zhang"
    ],
    "abstract": "Object manipulation is a common component of everyday tasks, but learning to\nmanipulate objects from high-dimensional observations presents significant\nchallenges. These challenges are heightened in multi-object environments due to\nthe combinatorial complexity of the state space as well as of the desired\nbehaviors. While recent approaches have utilized large-scale offline data to\ntrain models from pixel observations, achieving performance gains through\nscaling, these methods struggle with compositional generalization in unseen\nobject configurations with constrained network and dataset sizes. To address\nthese issues, we propose a novel behavioral cloning (BC) approach that\nleverages object-centric representations and an entity-centric Transformer with\ndiffusion-based optimization, enabling efficient learning from offline image\ndata. Our method first decomposes observations into an object-centric\nrepresentation, which is then processed by our entity-centric Transformer that\ncomputes attention at the object level, simultaneously predicting object\ndynamics and the agent's actions. Combined with the ability of diffusion models\nto capture multi-modal behavior distributions, this results in substantial\nperformance improvements in multi-object tasks and, more importantly, enables\ncompositional generalization. We present BC agents capable of zero-shot\ngeneralization to tasks with novel compositions of objects and goals, including\nlarger numbers of objects than seen during training. We provide video rollouts\non our webpage: https://sites.google.com/view/ec-diffuser.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18907v2",
    "published_date": "2024-12-25 13:50:15 UTC",
    "updated_date": "2025-02-14 20:43:22 UTC"
  },
  {
    "arxiv_id": "2412.18899v2",
    "title": "GAI: Generative Agents for Innovation",
    "authors": [
      "Masahiro Sato"
    ],
    "abstract": "This study examines whether collective reasoning among generative agents can\nfacilitate novel and coherent thinking that leads to innovation. To achieve\nthis, it proposes GAI, a new LLM-empowered framework designed for reflection\nand interaction among multiple generative agents to replicate the process of\ninnovation. The core of the GAI framework lies in an architecture that\ndynamically processes the internal states of agents and a dialogue scheme\nspecifically tailored to facilitate analogy-driven innovation. The framework's\nfunctionality is evaluated using Dyson's invention of the bladeless fan as a\ncase study, assessing the extent to which the core ideas of the innovation can\nbe replicated through a set of fictional technical documents. The experimental\nresults demonstrate that models with internal states significantly outperformed\nthose without, achieving higher average scores and lower variance. Notably, the\nmodel with five heterogeneous agents equipped with internal states successfully\nreplicated the key ideas underlying the Dyson's invention. This indicates that\nthe internal state enables agents to refine their ideas, resulting in the\nconstruction and sharing of more coherent and comprehensive concepts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Added an Appendix section",
    "pdf_url": "http://arxiv.org/pdf/2412.18899v2",
    "published_date": "2024-12-25 13:20:10 UTC",
    "updated_date": "2024-12-31 17:00:33 UTC"
  },
  {
    "arxiv_id": "2412.18894v1",
    "title": "Comprehensive Study on Lumbar Disc Segmentation Techniques Using MRI Data",
    "authors": [
      "Serkan Salturk",
      "Irem Sayin",
      "Ibrahim Cem Balci",
      "Taha Emre Pamukcu",
      "Zafer Soydan",
      "Huseyin Uvet"
    ],
    "abstract": "Lumbar disk segmentation is essential for diagnosing and curing spinal\ndisorders by enabling precise detection of disk boundaries in medical imaging.\nThe advent of deep learning has resulted in the development of many\nsegmentation methods, offering differing levels of accuracy and effectiveness.\nThis study assesses the effectiveness of several sophisticated deep learning\narchitectures, including ResUnext, Ef3 Net, UNet, and TransUNet, for lumbar\ndisk segmentation, highlighting key metrics like as Pixel Accuracy, Mean\nIntersection over Union (Mean IoU), and Dice Coefficient. The findings indicate\nthat ResUnext achieved the highest segmentation accuracy, with a Pixel Accuracy\nof 0.9492 and a Dice Coefficient of 0.8425, with TransUNet following closely\nafter. Filtering techniques somewhat enhanced the performance of most models,\nparticularly Dense UNet, improving stability and segmentation quality. The\nfindings underscore the efficacy of these models in lumbar disk segmentation\nand highlight potential areas for improvement.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.18894v1",
    "published_date": "2024-12-25 12:54:52 UTC",
    "updated_date": "2024-12-25 12:54:52 UTC"
  },
  {
    "arxiv_id": "2412.18890v1",
    "title": "CoEvo: Continual Evolution of Symbolic Solutions Using Large Language Models",
    "authors": [
      "Ping Guo",
      "Qingfu Zhang",
      "Xi Lin"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as transformative tools in\nartificial intelligence, capable of processing and understanding extensive\nhuman knowledge to enhance problem-solving across various domains. This paper\nexplores the potential of LLMs to drive the discovery of symbolic solutions\nwithin scientific and engineering disciplines, where such solutions are crucial\nfor advancing theoretical and practical applications. We propose a novel\nframework that utilizes LLMs in an evolutionary search methodology, augmented\nby a dynamic knowledge library that integrates and refines insights in an\n\\textit{open-ended manner}. This approach aims to tackle the dual challenges of\nefficiently navigating complex symbolic representation spaces and leveraging\nboth existing and newly generated knowledge to foster open-ended innovation. By\nenabling LLMs to interact with and expand upon a knowledge library, we\nfacilitate the continuous generation of novel solutions in diverse forms such\nas language, code, and mathematical expressions. Our experimental results\ndemonstrate that this method not only enhances the efficiency of searching for\nsymbolic solutions but also supports the ongoing discovery process, akin to\nhuman scientific endeavors. This study represents a first effort in\nconceptualizing the search for symbolic solutions as a lifelong, iterative\nprocess, marking a significant step towards harnessing AI in the perpetual\npursuit of scientific and engineering breakthroughs. We have open-sourced our\ncode and data, please visit \\url{https://github.com/pgg3/CoEvo} for more\ninformation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18890v1",
    "published_date": "2024-12-25 12:27:27 UTC",
    "updated_date": "2024-12-25 12:27:27 UTC"
  },
  {
    "arxiv_id": "2412.18874v1",
    "title": "IUST_PersonReId: A New Domain in Person Re-Identification Datasets",
    "authors": [
      "Alireza Sedighi Moghaddam",
      "Fatemeh Anvari",
      "Mohammadjavad Mirshekari Haghighi",
      "Mohammadali Fakhari",
      "Mohammad Reza Mohammadi"
    ],
    "abstract": "Person re-identification (ReID) models often struggle to generalize across\ndiverse cultural contexts, particularly in Islamic regions like Iran, where\nmodest clothing styles are prevalent. Existing datasets predominantly feature\nWestern and East Asian fashion, limiting their applicability in these settings.\nTo address this gap, we introduce IUST_PersonReId, a dataset designed to\nreflect the unique challenges of ReID in new cultural environments, emphasizing\nmodest attire and diverse scenarios from Iran, including markets, campuses, and\nmosques. Experiments on IUST_PersonReId with state-of-the-art models, such as\nSolider and CLIP-ReID, reveal significant performance drops compared to\nbenchmarks like Market1501 and MSMT17, highlighting the challenges posed by\nocclusion and limited distinctive features. Sequence-based evaluations show\nimprovements by leveraging temporal context, emphasizing the dataset's\npotential for advancing culturally sensitive and robust ReID systems.\nIUST_PersonReId offers a critical resource for addressing fairness and bias in\nReID research globally. The dataset is publicly available at\nhttps://computervisioniust.github.io/IUST_PersonReId/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 4 figures. The dataset introduced in this paper,\n  IUST_PersonReId, is publicly available at\n  https://computervisioniust.github.io/IUST_PersonReId/",
    "pdf_url": "http://arxiv.org/pdf/2412.18874v1",
    "published_date": "2024-12-25 11:17:43 UTC",
    "updated_date": "2024-12-25 11:17:43 UTC"
  },
  {
    "arxiv_id": "2501.01439v1",
    "title": "Probabilistic Mission Design in Neuro-Symbolic Systems",
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Daniel Ochs",
      "Devendra Singh Dhami",
      "Julian Eggert",
      "Kristian Kersting"
    ],
    "abstract": "Advanced Air Mobility (AAM) is a growing field that demands accurate modeling\nof legal concepts and restrictions in navigating intelligent vehicles. In\naddition, any implementation of AAM needs to face the challenges posed by\ninherently dynamic and uncertain human-inhabited spaces robustly. Nevertheless,\nthe employment of Unmanned Aircraft Systems (UAS) beyond visual line of sight\n(BVLOS) is an endearing task that promises to enhance significantly today's\nlogistics and emergency response capabilities. To tackle these challenges, we\npresent a probabilistic and neuro-symbolic architecture to encode legal\nframeworks and expert knowledge over uncertain spatial relations and noisy\nperception in an interpretable and adaptable fashion. More specifically, we\ndemonstrate Probabilistic Mission Design (ProMis), a system architecture that\nlinks geospatial and sensory data with declarative, Hybrid Probabilistic Logic\nPrograms (HPLP) to reason over the agent's state space and its legality. As a\nresult, ProMis generates Probabilistic Mission Landscapes (PML), which quantify\nthe agent's belief that a set of mission conditions is satisfied across its\nnavigation space. Extending prior work on ProMis' reasoning capabilities and\ncomputational characteristics, we show its integration with potent machine\nlearning models such as Large Language Models (LLM) and Transformer-based\nvision models. Hence, our experiments underpin the application of ProMis with\nmulti-modal input data and how our method applies to many important AAM\nscenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2406.03454",
    "pdf_url": "http://arxiv.org/pdf/2501.01439v1",
    "published_date": "2024-12-25 11:04:00 UTC",
    "updated_date": "2024-12-25 11:04:00 UTC"
  },
  {
    "arxiv_id": "2412.18863v1",
    "title": "Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models",
    "authors": [
      "Meltem Aksoy"
    ],
    "abstract": "Large language models (LLMs) have become integral tools in diverse domains,\nyet their moral reasoning capabilities across cultural and linguistic contexts\nremain underexplored. This study investigates whether multilingual LLMs, such\nas GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally\nspecific moral values or impose dominant moral norms, particularly those rooted\nin English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight\nlanguages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and\nRussian, the study analyzes the models' adherence to six core moral\nfoundations: care, equality, proportionality, loyalty, authority, and purity.\nThe results reveal significant cultural and linguistic variability, challenging\nthe assumption of universal moral consistency in LLMs. Although some models\ndemonstrate adaptability to diverse contexts, others exhibit biases influenced\nby the composition of the training data. These findings underscore the need for\nculturally inclusive model development to improve fairness and trust in\nmultilingual AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18863v1",
    "published_date": "2024-12-25 10:17:15 UTC",
    "updated_date": "2024-12-25 10:17:15 UTC"
  },
  {
    "arxiv_id": "2412.18862v3",
    "title": "WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting",
    "authors": [
      "Chenghao Qian",
      "Yuhu Guo",
      "Wenjing Li",
      "Gustav Markkula"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene\nreconstruction, but still suffers from complex outdoor environments, especially\nunder adverse weather. This is because 3DGS treats the artifacts caused by\nadverse weather as part of the scene and will directly reconstruct them,\nlargely reducing the clarity of the reconstructed scene. To address this\nchallenge, we propose WeatherGS, a 3DGS-based framework for reconstructing\nclear scenes from multi-view images under different weather conditions.\nSpecifically, we explicitly categorize the multi-weather artifacts into the\ndense particles and lens occlusions that have very different characters, in\nwhich the former are caused by snowflakes and raindrops in the air, and the\nlatter are raised by the precipitation on the camera lens. In light of this, we\npropose a dense-to-sparse preprocess strategy, which sequentially removes the\ndense particles by an Atmospheric Effect Filter (AEF) and then extracts the\nrelatively sparse occlusion masks with a Lens Effect Detector (LED). Finally,\nwe train a set of 3D Gaussians by the processed images and generated masks for\nexcluding occluded areas, and accurately recover the underlying clear scene by\nGaussian splatting. We conduct a diverse and challenging benchmark to\nfacilitate the evaluation of 3D reconstruction under complex weather scenarios.\nExtensive experiments on this benchmark demonstrate that our WeatherGS\nconsistently produces high-quality, clean scenes across various weather\nscenarios, outperforming existing state-of-the-art methods. See project\npage:https://jumponthemoon.github.io/weather-gs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18862v3",
    "published_date": "2024-12-25 10:16:57 UTC",
    "updated_date": "2025-02-12 03:13:48 UTC"
  },
  {
    "arxiv_id": "2501.05460v2",
    "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
    "authors": [
      "Gursimran Singh",
      "Xinglu Wang",
      "Yifan Hu",
      "Timothy Yu",
      "Linzi Xing",
      "Wei Jiang",
      "Zhefeng Wang",
      "Xiaolong Bai",
      "Yi Li",
      "Ying Xiong",
      "Yong Zhang",
      "Zhenan Fan"
    ],
    "abstract": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "16 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.05460v2",
    "published_date": "2024-12-25 10:11:31 UTC",
    "updated_date": "2025-02-05 22:55:47 UTC"
  },
  {
    "arxiv_id": "2412.18857v1",
    "title": "Computing Approximate Graph Edit Distance via Optimal Transport",
    "authors": [
      "Qihao Cheng",
      "Da Yan",
      "Tianhao Wu",
      "Zhongyi Huang",
      "Qin Zhang"
    ],
    "abstract": "Given a graph pair $(G^1, G^2)$, graph edit distance (GED) is defined as the\nminimum number of edit operations converting $G^1$ to $G^2$. GED is a\nfundamental operation widely used in many applications, but its exact\ncomputation is NP-hard, so the approximation of GED has gained a lot of\nattention. Data-driven learning-based methods have been found to provide\nsuperior results compared to classical approximate algorithms, but they\ndirectly fit the coupling relationship between a pair of vertices from their\nvertex features. We argue that while pairwise vertex features can capture the\ncoupling cost (discrepancy) of a pair of vertices, the vertex coupling matrix\nshould be derived from the vertex-pair cost matrix through a more\nwell-established method that is aware of the global context of the graph pair,\nsuch as optimal transport. In this paper, we propose an ensemble approach that\nintegrates a supervised learning-based method and an unsupervised method, both\nbased on optimal transport. Our learning method, GEDIOT, is based on inverse\noptimal transport that leverages a learnable Sinkhorn algorithm to generate the\ncoupling matrix. Our unsupervised method, GEDGW, models GED computation as a\nlinear combination of optimal transport and its variant, Gromov-Wasserstein\ndiscrepancy, for node and edge operations, respectively, which can be solved\nefficiently without needing the ground truth. Our ensemble method, GEDHOT,\ncombines GEDIOT and GEDGW to further boost the performance. Extensive\nexperiments demonstrate that our methods significantly outperform the existing\nmethods in terms of the performance of GED computation, edit path generation,\nand model generalizability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by SIGMOD2025. 26 pages, 21 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.18857v1",
    "published_date": "2024-12-25 09:55:14 UTC",
    "updated_date": "2024-12-25 09:55:14 UTC"
  },
  {
    "arxiv_id": "2412.19847v1",
    "title": "Symbolic Disentangled Representations for Images",
    "authors": [
      "Alexandr Korchemnyi",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "abstract": "The idea of disentangled representations is to reduce the data to a set of\ngenerative factors that produce it. Typically, such representations are vectors\nin latent space, where each coordinate corresponds to one of the generative\nfactors. The object can then be modified by changing the value of a particular\ncoordinate, but it is necessary to determine which coordinate corresponds to\nthe desired generative factor -- a difficult task if the vector representation\nhas a high dimension. In this article, we propose ArSyD (Architecture for\nSymbolic Disentanglement), which represents each generative factor as a vector\nof the same dimension as the resulting representation. In ArSyD, the object\nrepresentation is obtained as a superposition of the generative factor vector\nrepresentations. We call such a representation a \\textit{symbolic disentangled\nrepresentation}. We use the principles of Hyperdimensional Computing (also\nknown as Vector Symbolic Architectures), where symbols are represented as\nhypervectors, allowing vector operations on them. Disentanglement is achieved\nby construction, no additional assumptions about the underlying distributions\nare made during training, and the model is only trained to reconstruct images\nin a weakly supervised manner. We study ArSyD on the dSprites and CLEVR\ndatasets and provide a comprehensive analysis of the learned symbolic\ndisentangled representations. We also propose new disentanglement metrics that\nallow comparison of methods using latent representations of different\ndimensions. ArSyD allows to edit the object properties in a controlled and\ninterpretable way, and the dimensionality of the object property representation\ncoincides with the dimensionality of the object representation itself.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.19847v1",
    "published_date": "2024-12-25 09:20:13 UTC",
    "updated_date": "2024-12-25 09:20:13 UTC"
  },
  {
    "arxiv_id": "2412.18840v2",
    "title": "Implicit factorized transformer approach to fast prediction of turbulent channel flows",
    "authors": [
      "Huiyu Yang",
      "Yunpeng Wang",
      "Jianchun Wang"
    ],
    "abstract": "Transformer neural operators have recently become an effective approach for\nsurrogate modeling of systems governed by partial differential equations\n(PDEs). In this paper, we introduce a modified implicit factorized transformer\n(IFactFormer-m) model which replaces the original chained factorized attention\nwith parallel factorized attention. The IFactFormer-m model successfully\nperforms long-term predictions for turbulent channel flow, whereas the original\nIFactFormer (IFactFormer-o), Fourier neural operator (FNO), and implicit\nFourier neural operator (IFNO) exhibit a poor performance. Turbulent channel\nflows are simulated by direct numerical simulation using fine grids at friction\nReynolds numbers $\\text{Re}_{\\tau}\\approx 180,395,590$, and filtered to coarse\ngrids for training neural operator. The neural operator takes the current flow\nfield as input and predicts the flow field at the next time step, and long-term\nprediction is achieved in the posterior through an autoregressive approach. The\nresults show that IFactFormer-m, compared to other neural operators and the\ntraditional large eddy simulation (LES) methods including dynamic Smagorinsky\nmodel (DSM) and the wall-adapted local eddy-viscosity (WALE) model, reduces\nprediction errors in the short term, and achieves stable and accurate long-term\nprediction of various statistical properties and flow structures, including the\nenergy spectrum, mean streamwise velocity, root mean square (rms) values of\nfluctuating velocities, Reynolds shear stress, and spatial structures of\ninstantaneous velocity. Moreover, the trained IFactFormer-m is much faster than\ntraditional LES methods. By analyzing the attention kernels, we elucidate the\nreasons why IFactFormer-m converges faster and achieves a stable and accurate\nlong-term prediction compared to IFactFormer-o. Code and data are available at:\nhttps://github.com/huiyu-2002/IFactFormer-m.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18840v2",
    "published_date": "2024-12-25 09:05:14 UTC",
    "updated_date": "2025-02-26 07:24:07 UTC"
  },
  {
    "arxiv_id": "2412.18839v2",
    "title": "Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM Dataset",
    "authors": [
      "Neil Shah",
      "Shirish Karande",
      "Vineet Gandhi"
    ],
    "abstract": "Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning\nto simulate ground-truth speech from paired whispers. However, the simulated\nspeech often lacks intelligibility and fails to generalize well across\ndifferent speakers. To address this issue, we focus on learning phoneme-level\nalignments from paired whispers and text and employ a Text-to-Speech (TTS)\nsystem to simulate the ground-truth. To reduce dependence on whispers, we learn\nphoneme alignments directly from NAMs, though the quality is constrained by the\navailable training data. To further mitigate reliance on NAM/whisper data for\nground-truth simulation, we propose incorporating the lip modality to infer\nspeech and introduce a novel diffusion-based method that leverages recent\nadvancements in lip-to-speech technology. Additionally, we release the MultiNAM\ndataset with over 7.96 hours of paired NAM, whisper, video, and text data from\ntwo speakers and benchmark all methods on this dataset. Speech samples and the\ndataset are available at https://diff-nam.github.io/DiffNAM/",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at IEEE ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.18839v2",
    "published_date": "2024-12-25 08:57:24 UTC",
    "updated_date": "2025-01-23 05:39:51 UTC"
  },
  {
    "arxiv_id": "2412.18836v2",
    "title": "MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by Real-time MRI",
    "authors": [
      "Neil Shah",
      "Ayan Kashyap",
      "Shirish Karande",
      "Vineet Gandhi"
    ],
    "abstract": "Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily\non noisy ground-truth speech. Applying loss directly over ground truth\nmel-spectrograms entangles speech content with MRI noise, resulting in poor\nintelligibility. We introduce a novel approach that adapts the multi-modal\nself-supervised AV-HuBERT model for text prediction from rtMRI and incorporates\na new flow-based duration predictor for speaker-specific alignment. The\npredicted text and durations are then used by a speech decoder to synthesize\naligned speech in any novel voice. We conduct thorough experiments on two\ndatasets and demonstrate our method's generalization ability to unseen\nspeakers. We assess our framework's performance by masking parts of the rtMRI\nvideo to evaluate the impact of different articulators on text prediction. Our\nmethod achieves a $15.18\\%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus,\nmarking a huge improvement over the current state-of-the-art. Speech samples\nare available at https://mri2speech.github.io/MRI2Speech/",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at IEEE ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.18836v2",
    "published_date": "2024-12-25 08:49:43 UTC",
    "updated_date": "2025-01-17 12:18:44 UTC"
  },
  {
    "arxiv_id": "2412.18827v1",
    "title": "PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph Structure Generation",
    "authors": [
      "ChenRui Duan",
      "Zelin Zang",
      "Siyuan Li",
      "Yongjie Xu",
      "Stan Z. Li"
    ],
    "abstract": "Phylogenetic trees elucidate evolutionary relationships among species, but\nphylogenetic inference remains challenging due to the complexity of combining\ncontinuous (branch lengths) and discrete parameters (tree topology).\nTraditional Markov Chain Monte Carlo methods face slow convergence and\ncomputational burdens. Existing Variational Inference methods, which require\npre-generated topologies and typically treat tree structures and branch lengths\nindependently, may overlook critical sequence features, limiting their accuracy\nand flexibility. We propose PhyloGen, a novel method leveraging a pre-trained\ngenomic language model to generate and optimize phylogenetic trees without\ndependence on evolutionary models or aligned sequence constraints. PhyloGen\nviews phylogenetic inference as a conditionally constrained tree structure\ngeneration problem, jointly optimizing tree topology and branch lengths through\nthree core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and\n(iii) PhyloTree Structure Modeling. Meanwhile, we introduce a Scoring Function\nto guide the model towards a more stable gradient descent. We demonstrate the\neffectiveness and robustness of PhyloGen on eight real-world benchmark\ndatasets. Visualization results confirm PhyloGen provides deeper insights into\nphylogenetic relationships.",
    "categories": [
      "q-bio.PE",
      "cs.AI"
    ],
    "primary_category": "q-bio.PE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18827v1",
    "published_date": "2024-12-25 08:33:05 UTC",
    "updated_date": "2024-12-25 08:33:05 UTC"
  },
  {
    "arxiv_id": "2412.18819v2",
    "title": "LLM-assisted Vector Similarity Search",
    "authors": [
      "Md Riyadh",
      "Muqi Li",
      "Felix Haryanto Lie",
      "Jia Long Loh",
      "Haotian Mi",
      "Sayam Bohra"
    ],
    "abstract": "As data retrieval demands become increasingly complex, traditional search\nmethods often fall short in addressing nuanced and conceptual queries. Vector\nsimilarity search has emerged as a promising technique for finding semantically\nsimilar information efficiently. However, its effectiveness diminishes when\nhandling intricate queries with contextual nuances. This paper explores a\nhybrid approach combining vector similarity search with Large Language Models\n(LLMs) to enhance search accuracy and relevance. The proposed two-step solution\nfirst employs vector similarity search to shortlist potential matches, followed\nby an LLM for context-aware ranking of the results. Experiments on structured\ndatasets demonstrate that while vector similarity search alone performs well\nfor straightforward queries, the LLM-assisted approach excels in processing\ncomplex queries involving constraints, negations, or conceptual requirements.\nBy leveraging the natural language understanding capabilities of LLMs, this\nmethod improves the accuracy of search results for complex tasks without\nsacrificing efficiency. We also discuss real-world applications and propose\ndirections for future research to refine and scale this technique for diverse\ndatasets and use cases.\n  Original article:\nhttps://engineering.grab.com/llm-assisted-vector-similarity-search",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18819v2",
    "published_date": "2024-12-25 08:17:37 UTC",
    "updated_date": "2024-12-30 04:15:42 UTC"
  },
  {
    "arxiv_id": "2412.18816v1",
    "title": "GSAVS: Gaussian Splatting-based Autonomous Vehicle Simulator",
    "authors": [
      "Rami Wilson"
    ],
    "abstract": "Modern autonomous vehicle simulators feature an ever-growing library of\nassets, including vehicles, buildings, roads, pedestrians, and more. While this\nlevel of customization proves beneficial when creating virtual urban\nenvironments, this process becomes cumbersome when intending to train within a\ndigital twin or a duplicate of a real scene. Gaussian splatting emerged as a\npowerful technique in scene reconstruction and novel view synthesis, boasting\nhigh fidelity and rendering speeds. In this paper, we introduce GSAVS, an\nautonomous vehicle simulator that supports the creation and development of\nautonomous vehicle models. Every asset within the simulator is a 3D Gaussian\nsplat, including the vehicles and the environment. However, the simulator runs\nwithin a classical 3D engine, rendering 3D Gaussian splats in real-time. This\nallows the simulator to utilize the photorealism that 3D Gaussian splatting\nboasts while providing the customization and ease of use of a classical 3D\nengine.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18816v1",
    "published_date": "2024-12-25 07:52:09 UTC",
    "updated_date": "2024-12-25 07:52:09 UTC"
  },
  {
    "arxiv_id": "2412.18798v2",
    "title": "Ister: Inverted Seasonal-Trend Decomposition Transformer for Explainable Multivariate Time Series Forecasting",
    "authors": [
      "Fanpu Cao",
      "Shu Yang",
      "Zhengjian Chen",
      "Ye Liu",
      "Laizhong Cui"
    ],
    "abstract": "In long-term time series forecasting, Transformer-based models have achieved\ngreat success, due to its ability to capture long-range dependencies. However,\nexisting models face challenges in identifying critical components for\nprediction, leading to limited interpretability and suboptimal performance. To\naddress these issues, we propose the Inverted Seasonal-Trend Decomposition\nTransformer (Ister), a novel Transformer-based model for multivariate time\nseries forecasting. Ister decomposes time series into seasonal and trend\ncomponents, further modeling multi-periodicity and inter-series dependencies\nusing a Dual Transformer architecture. We introduce a novel Dot-attention\nmechanism that improves interpretability, computational efficiency, and\npredictive accuracy. Comprehensive experiments on benchmark datasets\ndemonstrate that Ister outperforms existing state-of-the-art models, achieving\nup to 10% improvement in MSE. Moreover, Ister enables intuitive visualization\nof component contributions, shedding lights on model's decision process and\nenhancing transparency in prediction results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18798v2",
    "published_date": "2024-12-25 06:37:19 UTC",
    "updated_date": "2025-01-25 13:56:24 UTC"
  },
  {
    "arxiv_id": "2412.18790v1",
    "title": "Torque-Aware Momentum",
    "authors": [
      "Pranshu Malviya",
      "Goncalo Mordido",
      "Aristide Baratin",
      "Reza Babanezhad Harikandeh",
      "Gintare Karolina Dziugaite",
      "Razvan Pascanu",
      "Sarath Chandar"
    ],
    "abstract": "Efficiently exploring complex loss landscapes is key to the performance of\ndeep neural networks. While momentum-based optimizers are widely used in\nstate-of-the-art setups, classical momentum can still struggle with large,\nmisaligned gradients, leading to oscillations. To address this, we propose\nTorque-Aware Momentum (TAM), which introduces a damping factor based on the\nangle between the new gradients and previous momentum, stabilizing the update\ndirection during training. Empirical results show that TAM, which can be\ncombined with both SGD and Adam, enhances exploration, handles distribution\nshifts more effectively, and improves generalization performance across various\ntasks, including image classification and large language model fine-tuning,\nwhen compared to classical momentum-based optimizers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18790v1",
    "published_date": "2024-12-25 05:58:07 UTC",
    "updated_date": "2024-12-25 05:58:07 UTC"
  },
  {
    "arxiv_id": "2412.18780v1",
    "title": "Skeleton-based Action Recognition with Non-linear Dependency Modeling and Hilbert-Schmidt Independence Criterion",
    "authors": [
      "Yuheng Yang"
    ],
    "abstract": "Human skeleton-based action recognition has long been an indispensable aspect\nof artificial intelligence. Current state-of-the-art methods tend to consider\nonly the dependencies between connected skeletal joints, limiting their ability\nto capture non-linear dependencies between physically distant joints. Moreover,\nmost existing approaches distinguish action classes by estimating the\nprobability density of motion representations, yet the high-dimensional nature\nof human motions invokes inherent difficulties in accomplishing such\nmeasurements. In this paper, we seek to tackle these challenges from two\ndirections: (1) We propose a novel dependency refinement approach that\nexplicitly models dependencies between any pair of joints, effectively\ntranscending the limitations imposed by joint distance. (2) We further propose\na framework that utilizes the Hilbert-Schmidt Independence Criterion to\ndifferentiate action classes without being affected by data dimensionality, and\nmathematically derive learning objectives guaranteeing precise recognition.\nEmpirically, our approach sets the state-of-the-art performance on NTU RGB+D,\nNTU RGB+D 120, and Northwestern-UCLA datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18780v1",
    "published_date": "2024-12-25 05:02:11 UTC",
    "updated_date": "2024-12-25 05:02:11 UTC"
  },
  {
    "arxiv_id": "2412.18778v1",
    "title": "Unified Local and Global Attention Interaction Modeling for Vision Transformers",
    "authors": [
      "Tan Nguyen",
      "Coy D. Heldermon",
      "Corey Toler-Franklin"
    ],
    "abstract": "We present a novel method that extends the self-attention mechanism of a\nvision transformer (ViT) for more accurate object detection across diverse\ndatasets. ViTs show strong capability for image understanding tasks such as\nobject detection, segmentation, and classification. This is due in part to\ntheir ability to leverage global information from interactions among visual\ntokens. However, the self-attention mechanism in ViTs are limited because they\ndo not allow visual tokens to exchange local or global information with\nneighboring features before computing global attention. This is problematic\nbecause tokens are treated in isolation when attending (matching) to other\ntokens, and valuable spatial relationships are overlooked. This isolation is\nfurther compounded by dot-product similarity operations that make tokens from\ndifferent semantic classes appear visually similar. To address these\nlimitations, we introduce two modifications to the traditional self-attention\nframework; a novel aggressive convolution pooling strategy for local feature\nmixing, and a new conceptual attention transformation to facilitate interaction\nand feature exchange between semantic concepts. Experimental results\ndemonstrate that local and global information exchange among visual features\nbefore self-attention significantly improves performance on challenging object\ndetection tasks and generalizes across multiple benchmark datasets and\nchallenging medical datasets. We publish source code and a novel dataset of\ncancerous tumors (chimeric cell clusters).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.5.0, I.5.4, I.4.0"
    ],
    "primary_category": "cs.CV",
    "comment": "20 Pages, 24 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.18778v1",
    "published_date": "2024-12-25 04:53:19 UTC",
    "updated_date": "2024-12-25 04:53:19 UTC"
  },
  {
    "arxiv_id": "2412.18775v1",
    "title": "ObitoNet: Multimodal High-Resolution Point Cloud Reconstruction",
    "authors": [
      "Apoorv Thapliyal",
      "Vinay Lanka",
      "Swathi Baskaran"
    ],
    "abstract": "ObitoNet employs a Cross Attention mechanism to integrate multimodal inputs,\nwhere Vision Transformers (ViT) extract semantic features from images and a\npoint cloud tokenizer processes geometric information using Farthest Point\nSampling (FPS) and K Nearest Neighbors (KNN) for spatial structure capture. The\nlearned multimodal features are fed into a transformer-based decoder for\nhigh-resolution point cloud reconstruction. This approach leverages the\ncomplementary strengths of both modalities rich image features and precise\ngeometric details ensuring robust point cloud generation even in challenging\nconditions such as sparse or noisy data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18775v1",
    "published_date": "2024-12-25 04:34:22 UTC",
    "updated_date": "2024-12-25 04:34:22 UTC"
  },
  {
    "arxiv_id": "2412.19845v1",
    "title": "Unveiling Secrets of Brain Function With Generative Modeling: Motion Perception in Primates & Cortical Network Organization in Mice",
    "authors": [
      "Hadi Vafaii"
    ],
    "abstract": "This Dissertation is comprised of two main projects, addressing questions in\nneuroscience through applications of generative modeling.\n  Project #1 (Chapter 4) explores how neurons encode features of the external\nworld. I combine Helmholtz's \"Perception as Unconscious Inference\" --\nparalleled by modern generative models like variational autoencoders (VAE) --\nwith the hierarchical structure of the visual cortex. This combination leads to\nthe development of a hierarchical VAE model, which I test for its ability to\nmimic neurons from the primate visual cortex in response to motion stimuli.\nResults show that the hierarchical VAE perceives motion similar to the primate\nbrain. Additionally, the model identifies causal factors of retinal motion\ninputs, such as object- and self-motion, in a completely unsupervised manner.\nCollectively, these results suggest that hierarchical inference underlines the\nbrain's understanding of the world, and hierarchical VAEs can effectively model\nthis understanding.\n  Project #2 (Chapter 5) investigates the spatiotemporal structure of\nspontaneous brain activity and its reflection of brain states like rest. Using\nsimultaneous fMRI and wide-field Ca2+ imaging data, this project demonstrates\nthat the mouse cortex can be decomposed into overlapping communities, with\naround half of the cortical regions belonging to multiple communities.\nComparisons reveal similarities and differences between networks inferred from\nfMRI and Ca2+ signals.\n  The introduction (Chapter 1) is divided similarly to this abstract: sections\n1.1 to 1.8 provide background information about Project #1, and sections 1.9 to\n1.13 are related to Project #2. Chapter 2 includes historical background,\nChapter 3 provides the necessary mathematical background, and finally, Chapter\n6 contains concluding remarks and future directions.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "This is my PhD Dissertation, defended on November 3, 2023",
    "pdf_url": "http://arxiv.org/pdf/2412.19845v1",
    "published_date": "2024-12-25 03:39:18 UTC",
    "updated_date": "2024-12-25 03:39:18 UTC"
  },
  {
    "arxiv_id": "2412.18760v2",
    "title": "Data clustering: an essential technique in data science",
    "authors": [
      "Tai Dinh",
      "Wong Hauchi",
      "Daniil Lisik",
      "Michal Koren",
      "Dat Tran",
      "Philip S. Yu",
      "Joaqun Torres-Sospedra"
    ],
    "abstract": "This paper explores the critical role of data clustering in data science,\nemphasizing its methodologies, tools, and diverse applications. Traditional\ntechniques, such as partitional and hierarchical clustering, are analyzed\nalongside advanced approaches such as data stream, density-based, graph-based,\nand model-based clustering for handling complex structured datasets. The paper\nhighlights key principles underpinning clustering, outlines widely used tools\nand frameworks, introduces the workflow of clustering in data science,\ndiscusses challenges in practical implementation, and examines various\napplications of clustering. By focusing on these foundations and applications,\nthe discussion underscores clustering's transformative potential. The paper\nconcludes with insights into future research directions, emphasizing\nclustering's role in driving innovation and enabling data-driven\ndecision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18760v2",
    "published_date": "2024-12-25 03:14:18 UTC",
    "updated_date": "2025-01-30 21:57:33 UTC"
  },
  {
    "arxiv_id": "2412.18750v2",
    "title": "The Impact of Input Order Bias on Large Language Models for Software Fault Localization",
    "authors": [
      "Md Nakhla Rafi",
      "Dong Jae Kim",
      "Tse-Hsun Chen",
      "Shaowei Wang"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18750v2",
    "published_date": "2024-12-25 02:48:53 UTC",
    "updated_date": "2025-03-19 16:08:36 UTC"
  },
  {
    "arxiv_id": "2412.18743v1",
    "title": "Successes and Limitations of Object-centric Models at Compositional Generalisation",
    "authors": [
      "Milton L. Montero",
      "Jeffrey S. Bowers",
      "Gaurav Malhotra"
    ],
    "abstract": "In recent years, it has been shown empirically that standard disentangled\nlatent variable models do not support robust compositional learning in the\nvisual domain. Indeed, in spite of being designed with the goal of factorising\ndatasets into their constituent factors of variations, disentangled models show\nextremely limited compositional generalisation capabilities. On the other hand,\nobject-centric architectures have shown promising compositional skills, albeit\nthese have 1) not been extensively tested and 2) experiments have been limited\nto scene composition -- where models must generalise to novel combinations of\nobjects in a visual scene instead of novel combinations of object properties.\nIn this work, we show that these compositional generalisation skills extend to\nthis later setting. Furthermore, we present evidence pointing to the source of\nthese skills and how they can be improved through careful training. Finally, we\npoint to one important limitation that still exists which suggests new\ndirections of research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "As it appeared in the Compositional Learning Workshop, NeurIPS 2024;\n  14 pages (5 main text, 7 appendices, 2 references); 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.18743v1",
    "published_date": "2024-12-25 02:25:12 UTC",
    "updated_date": "2024-12-25 02:25:12 UTC"
  },
  {
    "arxiv_id": "2412.18734v1",
    "title": "Predicting Time Series of Networked Dynamical Systems without Knowing Topology",
    "authors": [
      "Yanna Ding",
      "Zijie Huang",
      "Malik Magdon-Ismail",
      "Jianxi Gao"
    ],
    "abstract": "Many real-world complex systems, such as epidemic spreading networks and\necosystems, can be modeled as networked dynamical systems that produce\nmultivariate time series. Learning the intrinsic dynamics from observational\ndata is pivotal for forecasting system behaviors and making informed decisions.\nHowever, existing methods for modeling networked time series often assume known\ntopologies, whereas real-world networks are typically incomplete or inaccurate,\nwith missing or spurious links that hinder precise predictions. Moreover, while\nnetworked time series often originate from diverse topologies, the ability of\nmodels to generalize across topologies has not been systematically evaluated.\nTo address these gaps, we propose a novel framework for learning network\ndynamics directly from observed time-series data, when prior knowledge of graph\ntopology or governing dynamical equations is absent. Our approach leverages\ncontinuous graph neural networks with an attention mechanism to construct a\nlatent topology, enabling accurate reconstruction of future trajectories for\nnetwork states. Extensive experiments on real and synthetic networks\ndemonstrate that our model not only captures dynamics effectively without\ntopology knowledge but also generalizes to unseen time series originating from\ndiverse topologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18734v1",
    "published_date": "2024-12-25 01:39:04 UTC",
    "updated_date": "2024-12-25 01:39:04 UTC"
  },
  {
    "arxiv_id": "2412.18727v1",
    "title": "SAFLITE: Fuzzing Autonomous Systems via Large Language Models",
    "authors": [
      "Taohong Zhu",
      "Adrians Skapars",
      "Fardeen Mackenzie",
      "Declan Kehoe",
      "William Newton",
      "Suzanne Embury",
      "Youcheng Sun"
    ],
    "abstract": "Fuzz testing effectively uncovers software vulnerabilities; however, it faces\nchallenges with Autonomous Systems (AS) due to their vast search spaces and\ncomplex state spaces, which reflect the unpredictability and complexity of\nreal-world environments. This paper presents a universal framework aimed at\nimproving the efficiency of fuzz testing for AS. At its core is SaFliTe, a\npredictive component that evaluates whether a test case meets predefined safety\ncriteria. By leveraging the large language model (LLM) with information about\nthe test objective and the AS state, SaFliTe assesses the relevance of each\ntest case. We evaluated SaFliTe by instantiating it with various LLMs,\nincluding GPT-3.5, Mistral-7B, and Llama2-7B, and integrating it into four fuzz\ntesting tools: PGFuzz, DeepHyperion-UAV, CAMBA, and TUMB. These tools are\ndesigned specifically for testing autonomous drone control systems, such as\nArduPilot, PX4, and PX4-Avoidance. The experimental results demonstrate that,\ncompared to PGFuzz, SaFliTe increased the likelihood of selecting operations\nthat triggered bug occurrences in each fuzzing iteration by an average of\n93.1\\%. Additionally, after integrating SaFliTe, the ability of\nDeepHyperion-UAV, CAMBA, and TUMB to generate test cases that caused system\nviolations increased by 234.5\\%, 33.3\\%, and 17.8\\%, respectively. The\nbenchmark for this evaluation was sourced from a UAV Testing Competition.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18727v1",
    "published_date": "2024-12-25 01:00:05 UTC",
    "updated_date": "2024-12-25 01:00:05 UTC"
  },
  {
    "arxiv_id": "2412.18715v1",
    "title": "Optimization and Scalability of Collaborative Filtering Algorithms in Large Language Models",
    "authors": [
      "Haowei Yang",
      "Longfei Yun",
      "Jinghan Cao",
      "Qingyi Lu",
      "Yuming Tu"
    ],
    "abstract": "With the rapid development of large language models (LLMs) and the growing\ndemand for personalized content, recommendation systems have become critical in\nenhancing user experience and driving engagement. Collaborative filtering\nalgorithms, being core to many recommendation systems, have garnered\nsignificant attention for their efficiency and interpretability. However,\ntraditional collaborative filtering approaches face numerous challenges when\nintegrated into large-scale LLM-based systems, including high computational\ncosts, severe data sparsity, cold start problems, and lack of scalability. This\npaper investigates the optimization and scalability of collaborative filtering\nalgorithms in large language models, addressing these limitations through\nadvanced optimization strategies. Firstly, we analyze the fundamental\nprinciples of collaborative filtering algorithms and their limitations when\napplied in LLM-based contexts. Next, several optimization techniques such as\nmatrix factorization, approximate nearest neighbor search, and parallel\ncomputing are proposed to enhance computational efficiency and model accuracy.\nAdditionally, strategies such as distributed architecture and model compression\nare explored to facilitate dynamic updates and scalability in data-intensive\nenvironments.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18715v1",
    "published_date": "2024-12-25 00:26:51 UTC",
    "updated_date": "2024-12-25 00:26:51 UTC"
  },
  {
    "arxiv_id": "2412.18713v1",
    "title": "Enhanced Recommendation Combining Collaborative Filtering and Large Language Models",
    "authors": [
      "Xueting Lin",
      "Zhan Cheng",
      "Longfei Yun",
      "Qingyi Lu",
      "Yuanshuai Luo"
    ],
    "abstract": "With the advent of the information explosion era, the importance of\nrecommendation systems in various applications is increasingly significant.\nTraditional collaborative filtering algorithms are widely used due to their\neffectiveness in capturing user behavior patterns, but they encounter\nlimitations when dealing with cold start problems and data sparsity. Large\nLanguage Models (LLMs), with their strong natural language understanding and\ngeneration capabilities, provide a new breakthrough for recommendation systems.\nThis study proposes an enhanced recommendation method that combines\ncollaborative filtering and LLMs, aiming to leverage collaborative filtering's\nadvantage in modeling user preferences while enhancing the understanding of\ntextual information about users and items through LLMs to improve\nrecommendation accuracy and diversity. This paper first introduces the\nfundamental theories of collaborative filtering and LLMs, then designs a\nrecommendation system architecture that integrates both, and validates the\nsystem's effectiveness through experiments. The results show that the hybrid\nmodel based on collaborative filtering and LLMs significantly improves\nprecision, recall, and user satisfaction, demonstrating its potential in\ncomplex recommendation scenarios.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.18713v1",
    "published_date": "2024-12-25 00:23:53 UTC",
    "updated_date": "2024-12-25 00:23:53 UTC"
  }
]
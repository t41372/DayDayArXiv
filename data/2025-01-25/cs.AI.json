{
  "date": "2025-01-25",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-25 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 55 篇论文，主要聚焦 AI 优化、强化学习、Federated Learning 和多模态模型等领域，其中令人印象深刻的包括 Robin Young 关于 AGI 发展风险的游戏理论框架，以及脑活动图像解码的扩展性研究，这些论文揭示了 AI 安全和跨领域应用的潜在影响。\n\n### 重点论文讨论\n我们先聊聊今天最引人注目的论文，这些涉及 AI 安全、脑机接口和高效模型优化的前沿工作。相关论文放在一起讨论，以突出其潜在话题度和学术价值。\n\n- **Who's Driving? Game Theoretic Path Risk of AGI Development** (英文原标题: Who's Driving? Game Theoretic Path Risk of AGI Development)  \n  作者 Robin Young 提出的游戏理论框架，强调 AGI 开发竞争可能带来的存在风险，而非 AI 本身失控。主要贡献：通过建模合作均衡，证明共享基础设施和预注册机制可稳定竞争，减少灾难性结果。该论文在 AI 安全领域有话题潜力，桥接了理论与政策。\n\n- **Scaling laws for decoding images from brain activity** (英文原标题: Scaling laws for decoding images from brain activity)  \n  作者包括 Jean-Rémi King 的团队，系统比较 EEG、MEG 和 fMRI 等设备在图像解码中的性能。主要发现：解码性能随数据量呈对数线性增长，且深度学习在噪声设备上更有效。该研究扩展了生成 AI 在脑机接口的应用，具有跨领域影响。\n\n- **A Post-Processing-Based Fair Federated Learning Framework** (英文原标题: A Post-Processing-Based Fair Federated Learning Framework)  \n  这篇与前述 Federated Learning 相关的论文，作者 Yi Zhou 和 Naman Goel 提出了一种两阶段框架：在标准训练后，通过本地去偏置提升群组公平性。主要贡献：在异质数据环境下，显著改善公平性，同时保持准确率或略有提升。该方法简化了 FL 的公平实现，适用于隐私敏感场景。\n\n接下来，我们快速掠过其他论文，聚焦那些有实际应用或创新点的高影响力工作，而对较小众或技术细节较重的文章一笔带过。\n\n- **Development and Application of Self-Supervised Machine Learning for Smoke Plume and Active Fire Identification from the FIREX-AQ Datasets** (英文原标题: Development and Application of Self-Supervised Machine Learning for Smoke Plume and Active Fire Identification from the FIREX-AQ Datasets)  \n  作者团队包括 Michael J. Garay 和 Olga V. Kalashnikova，使用自监督 ML 识别卫星数据中的火点和烟 plume。主要发现：融合多仪器数据提升了野火监测精度，可能改善空气质量管理和气候研究。\n\n- **Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A Reinforcement Learning Approach to Managing Heterogeneous Edge Devices** (英文原标题: Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A Reinforcement Learning Approach to Managing Heterogeneous Edge Devices)  \n  作者 Talha Azfar 等提出 RL 算法优化 UAV 在灾害中的边计算。主要贡献：优先识别易故障设备，延长网络寿命，模拟了农村和城市疏散场景，提升了应急响应效率。\n\n- **Advanced Real-Time Fraud Detection Using RAG-Based LLMs** (英文原标题: Advanced Real-Time Fraud Detection Using RAG-Based LLMs)  \n  作者 Gurjot Singh 等开发了基于 Retrieval-Augmented Generation 的欺诈检测系统。主要发现：实时转录电话并验证身份，准确率达 97.98%，适合实际部署。\n\n- **Towards Conscious Service Robots** (英文原标题: Towards Conscious Service Robots)  \n  作者 Sven Behnke 探讨机器人融入人类认知架构，如 System 1 和 System 2 处理机制。主要贡献：建议机器人整合因果模型和元认知，以适应动态环境，推进服务机器人发展。\n\n其他论文，如音乐生成、专利图描述或特定领域优化（如蛋白质序列生成），虽有创新但影响力较小，我们仅简要提及：例如，**Music Generation using Human-In-The-Loop Reinforcement Learning** (英文原标题: Music Generation using Human-In-The-Loop Reinforcement Learning) 引入用户反馈的 RL 框架，提升了音乐生成质量；**PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures** (英文原标题: PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures) 使用多模态 LLM 生成专利图描述，改善了知识共享效率。这些论文的核心在于应用特定技术，但未见重大突破，故不展开。\n\n总之，今天的 arXiv 更新突出了 AI 领域的安全和扩展性挑战，读者可关注 AGI 风险和脑机接口相关论文，以探索未来趋势。保持关注，下一天见！",
  "papers": [
    {
      "arxiv_id": "2501.15343v1",
      "title": "Development and Application of Self-Supervised Machine Learning for Smoke Plume and Active Fire Identification from the FIREX-AQ Datasets",
      "title_zh": "基于 FIREX-AQ 数据集的自监督机器学习在烟羽和活跃火点识别中的开发和应用",
      "authors": [
        "Nicholas LaHaye",
        "Anistasija Easley",
        "Kyongsik Yun",
        "Huikyo Lee",
        "Erik Linstead",
        "Michael J. Garay",
        "Olga V. Kalashnikova"
      ],
      "abstract": "Fire Influence on Regional to Global Environments and Air Quality (FIREX-AQ)\nwas a field campaign aimed at better understanding the impact of wildfires and\nagricultural fires on air quality and climate. The FIREX-AQ campaign took place\nin August 2019 and involved two aircraft and multiple coordinated satellite\nobservations. This study applied and evaluated a self-supervised machine\nlearning (ML) method for the active fire and smoke plume identification and\ntracking in the satellite and sub-orbital remote sensing datasets collected\nduring the campaign. Our unique methodology combines remote sensing\nobservations with different spatial and spectral resolutions. The demonstrated\napproach successfully differentiates fire pixels and smoke plumes from\nbackground imagery, enabling the generation of a per-instrument smoke and fire\nmask product, as well as smoke and fire masks created from the fusion of\nselected data from independent instruments. This ML approach has a potential to\nenhance operational wildfire monitoring systems and improve decision-making in\nair quality management through fast smoke plume identification12 and tracking\nand could improve climate impact studies through fusion data from independent\ninstruments.",
      "tldr_zh": "本研究开发并评估了一种自监督机器学习（self-supervised machine learning）方法，用于从 FIREX-AQ 数据集中识别和跟踪活跃火点（active fire）和烟雾羽流（smoke plume）。该方法结合了不同空间和光谱分辨率的遥感观察数据，成功区分火点像素与背景图像，并生成每仪器烟雾和火点掩码，以及通过独立仪器数据融合的综合掩码。实验结果显示，该方法可提升野火监测系统的操作效率，支持空气质量管理和气候影响研究决策。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15343v1",
      "published_date": "2025-01-25 23:00:07 UTC",
      "updated_date": "2025-01-25 23:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:11:28.898869"
    },
    {
      "arxiv_id": "2501.15322v2",
      "title": "Scaling laws for decoding images from brain activity",
      "title_zh": "从脑活动解码图像的缩放定律",
      "authors": [
        "Hubert Banville",
        "Yohann Benchetrit",
        "Stéphane d'Ascoli",
        "Jérémy Rapin",
        "Jean-Rémi King"
      ],
      "abstract": "Generative AI has recently propelled the decoding of images from brain\nactivity. How do these approaches scale with the amount and type of neural\nrecordings? Here, we systematically compare image decoding from four types of\nnon-invasive devices: electroencephalography (EEG), magnetoencephalography\n(MEG), high-field functional Magnetic Resonance Imaging (3T fMRI) and\nultra-high field (7T) fMRI. For this, we evaluate decoding models on the\nlargest benchmark to date, encompassing 8 public datasets, 84 volunteers, 498\nhours of brain recording and 2.3 million brain responses to natural images.\nUnlike previous work, we focus on single-trial decoding performance to simulate\nreal-time settings. This systematic comparison reveals three main findings.\nFirst, the most precise neuroimaging devices tend to yield the best decoding\nperformances, when the size of the training sets are similar. However, the gain\nenabled by deep learning - in comparison to linear models - is obtained with\nthe noisiest devices. Second, we do not observe any plateau of decoding\nperformance as the amount of training data increases. Rather, decoding\nperformance scales log-linearly with the amount of brain recording. Third, this\nscaling law primarily depends on the amount of data per subject. However,\nlittle decoding gain is observed by increasing the number of subjects. Overall,\nthese findings delineate the path most suitable to scale the decoding of images\nfrom non-invasive brain recordings.",
      "tldr_zh": "本研究系统比较了从大脑活动解码图像的扩展规律，评估了EEG、MEG、3T fMRI和7T fMRI等四种非侵入性设备在单次试验设置下的性能，使用了迄今最大的基准数据集（包括8个公共数据集、84名志愿者、498小时脑记录和230万大脑响应）。结果显示，更精确的神经成像设备在训练集大小相同时表现出色，而深度学习模型相对于线性模型的优势在噪声较大的设备上更显著。进一步发现，解码性能随训练数据量的增加呈对数线性增长，主要取决于每个受试者的数据量，而增加受试者数量带来的收益有限。这些发现为优化非侵入性脑记录图像解码提供了指导路径。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "eess.IV",
      "comment": "29 pages, 14 figures, fixed typo in author list",
      "pdf_url": "http://arxiv.org/pdf/2501.15322v2",
      "published_date": "2025-01-25 20:38:36 UTC",
      "updated_date": "2025-01-28 11:46:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:11:42.585174"
    },
    {
      "arxiv_id": "2501.15318v1",
      "title": "A Post-Processing-Based Fair Federated Learning Framework",
      "title_zh": "基于后处理的公平联邦学习框架",
      "authors": [
        "Yi Zhou",
        "Naman Goel"
      ],
      "abstract": "Federated Learning (FL) allows collaborative model training among distributed\nparties without pooling local datasets at a central server. However, the\ndistributed nature of FL poses challenges in training fair federated learning\nmodels. The existing techniques are often limited in offering fairness\nflexibility to clients and performance. We formally define and empirically\nanalyze a simple and intuitive post-processing-based framework to improve group\nfairness in FL systems. This framework can be divided into two stages: a\nstandard FL training stage followed by a completely decentralized local\ndebiasing stage. In the first stage, a global model is trained without fairness\nconstraints using a standard federated learning algorithm (e.g. FedAvg). In the\nsecond stage, each client applies fairness post-processing on the global model\nusing their respective local dataset. This allows for customized fairness\nimprovements based on clients' desired and context-guided fairness\nrequirements. We demonstrate two well-established post-processing techniques in\nthis framework: model output post-processing and final layer fine-tuning. We\nevaluate the framework against three common baselines on four different\ndatasets, including tabular, signal, and image data, each with varying levels\nof data heterogeneity across clients. Our work shows that this framework not\nonly simplifies fairness implementation in FL but also provides significant\nfairness improvements with minimal accuracy loss or even accuracy gain, across\ndata modalities and machine learning methods, being especially effective in\nmore heterogeneous settings.",
      "tldr_zh": "这篇论文提出了一种基于后处理的公平 Federated Learning (FL) 框架，用于解决分布式 FL 系统中的群组公平性挑战。该框架分为两个阶段：首先进行标准的 FL 训练（如 FedAvg），然后每个客户端使用本地数据集进行去中心化的本地 debiasing 后处理，从而实现自定义的公平性改进。实验在四个数据集（包括表格、信号和图像数据）上评估，显示该框架显著提升了公平性，同时最小化了准确性损失，甚至在数据异质性高的场景中实现了准确性提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15318v1",
      "published_date": "2025-01-25 20:05:27 UTC",
      "updated_date": "2025-01-25 20:05:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:11:54.458044"
    },
    {
      "arxiv_id": "2501.15305v1",
      "title": "Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A Reinforcement Learning Approach to Managing Heterogeneous Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Talha Azfar",
        "Kaicong Huang",
        "Ruimin Ke"
      ],
      "abstract": "Edge sensing and computing is rapidly becoming part of intelligent\ninfrastructure architecture leading to operational reliance on such systems in\ndisaster or emergency situations. In such scenarios there is a high chance of\npower supply failure due to power grid issues, and communication system issues\ndue to base stations losing power or being damaged by the elements, e.g.,\nflooding, wildfires etc. Mobile edge computing in the form of unmanned aerial\nvehicles (UAVs) has been proposed to provide computation offloading from these\ndevices to conserve their battery, while the use of UAVs as relay network nodes\nhas also been investigated previously. This paper considers the use of UAVs\nwith further constraints on power and connectivity to prolong the life of the\nnetwork while also ensuring that the data is received from the edge nodes in a\ntimely manner. Reinforcement learning is used to investigate numerous scenarios\nof various levels of power and communication failure. This approach is able to\nidentify the device most likely to fail in a given scenario, thus providing\npriority guidance for maintenance personnel. The evacuations of a rural town\nand urban downtown area are also simulated to demonstrate the effectiveness of\nthe approach at extending the life of the most critical edge devices.",
      "tldr_zh": "该论文探讨了在灾难场景下，利用无人机（UAVs）辅助边缘计算（Edge Computing）来提升灾害恢复能力，针对异构边缘设备的管理问题。研究采用强化学习（Reinforcement Learning）方法，模拟各种电源和通信故障场景，以优化UAVs的部署，延长网络寿命并确保及时数据传输，同时识别最可能失败的设备以指导维护优先级。实验通过模拟农村城镇和城市市中心的疏散过程，证明该方法能有效延长关键边缘设备的寿命，提升整体系统可靠性。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15305v1",
      "published_date": "2025-01-25 19:03:05 UTC",
      "updated_date": "2025-01-25 19:03:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:12:04.721626"
    },
    {
      "arxiv_id": "2501.15304v1",
      "title": "Music Generation using Human-In-The-Loop Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Aju Ani Justus"
      ],
      "abstract": "This paper presents an approach that combines Human-In-The-Loop Reinforcement\nLearning (HITL RL) with principles derived from music theory to facilitate\nreal-time generation of musical compositions. HITL RL, previously employed in\ndiverse applications such as modelling humanoid robot mechanics and enhancing\nlanguage models, harnesses human feedback to refine the training process. In\nthis study, we develop a HILT RL framework that can leverage the constraints\nand principles in music theory. In particular, we propose an episodic tabular\nQ-learning algorithm with an epsilon-greedy exploration policy. The system\ngenerates musical tracks (compositions), continuously enhancing its quality\nthrough iterative human-in-the-loop feedback. The reward function for this\nprocess is the subjective musical taste of the user.",
      "tldr_zh": "这篇论文提出了一种结合 Human-In-The-Loop Reinforcement Learning (HITL RL) 和音乐理论的方法，用于实现实时音乐生成。研究开发了一个框架，使用 episodic tabular Q-learning 算法和 epsilon-greedy 探索策略，通过人类反馈持续优化音乐轨道质量。奖励函数基于用户的主观音乐偏好，从而使生成的音乐更符合个人口味，并扩展了 HITL RL 在创意领域的应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This is a preprint of a paper presented at the 2023 IEEE\n  International Conference on Big Data (BigData). It has been made public for\n  the benefit of the community and should be considered a preprint rather than\n  a formally reviewed paper",
      "pdf_url": "http://arxiv.org/pdf/2501.15304v1",
      "published_date": "2025-01-25 19:01:51 UTC",
      "updated_date": "2025-01-25 19:01:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:12:16.859259"
    },
    {
      "arxiv_id": "2501.15290v1",
      "title": "Advanced Real-Time Fraud Detection Using RAG-Based LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Gurjot Singh",
        "Prabhjot Singh",
        "Maninder Singh"
      ],
      "abstract": "Artificial Intelligence has become a double edged sword in modern society\nbeing both a boon and a bane. While it empowers individuals it also enables\nmalicious actors to perpetrate scams such as fraudulent phone calls and user\nimpersonations. This growing threat necessitates a robust system to protect\nindividuals In this paper we introduce a novel real time fraud detection\nmechanism using Retrieval Augmented Generation technology to address this\nchallenge on two fronts. First our system incorporates a continuously updating\npolicy checking feature that transcribes phone calls in real time and uses RAG\nbased models to verify that the caller is not soliciting private information\nthus ensuring transparency and the authenticity of the conversation. Second we\nimplement a real time user impersonation check with a two step verification\nprocess to confirm the callers identity ensuring accountability. A key\ninnovation of our system is the ability to update policies without retraining\nthe entire model enhancing its adaptability. We validated our RAG based\napproach using synthetic call recordings achieving an accuracy of 97.98 percent\nand an F1score of 97.44 percent with 100 calls outperforming state of the art\nmethods. This robust and flexible fraud detection system is well suited for\nreal world deployment.",
      "tldr_zh": "这篇论文提出了一种先进的实时欺诈检测系统，利用RAG-based LLMs来应对AI辅助的诈骗问题，如欺诈电话和用户模拟。该系统通过实时转录电话、RAG模型验证调用者意图（确保不索取私人信息）和双步用户身份验证，实现透明性和适应性创新；同时，它允许政策更新而不需重新训练整个模型。在合成呼叫实验中，该方法取得了97.98%的准确率和97.44%的F1分数，优于现有技术，为实际世界部署提供了可靠的防护方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15290v1",
      "published_date": "2025-01-25 17:58:05 UTC",
      "updated_date": "2025-01-25 17:58:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:12:29.751795"
    },
    {
      "arxiv_id": "2501.15281v1",
      "title": "Pre-training a Transformer-Based Generative Model Using a Small Sepedi Dataset",
      "title_zh": "使用小型 Sepedi 数据集预训练基于 Transformer 的生成模型",
      "authors": [
        "Simon P. Ramalepe",
        "Thipe I. Modipa",
        "Marelie H. Davel"
      ],
      "abstract": "Due to the scarcity of data in low-resourced languages, the development of\nlanguage models for these languages has been very slow. Currently, pre-trained\nlanguage models have gained popularity in natural language processing,\nespecially, in developing domain-specific models for low-resourced languages.\nIn this study, we experiment with the impact of using occlusion-based\ntechniques when training a language model for a text generation task. We curate\n2 new datasets, the Sepedi monolingual (SepMono) dataset from several South\nAfrican resources and the Sepedi radio news (SepNews) dataset from the radio\nnews domain. We use the SepMono dataset to pre-train transformer-based models\nusing the occlusion and non-occlusion pre-training techniques and compare\nperformance. The SepNews dataset is specifically used for fine-tuning. Our\nresults show that the non-occlusion models perform better compared to the\nocclusion-based models when measuring validation loss and perplexity. However,\nanalysis of the generated text using the BLEU score metric, which measures the\nquality of the generated text, shows a slightly higher BLEU score for the\nocclusion-based models compared to the non-occlusion models.",
      "tldr_zh": "本研究针对低资源语言 Sepedi 的数据稀缺问题，构建了两个新数据集：SepMono（Sepedi 单语数据集）和 SepNews（Sepedi 广播新闻数据集），并使用 SepMono 预训练 Transformer 模型，比较 occlusion-based 和 non-occlusion 预训练技术，同时以 SepNews 进行微调。实验结果表明，非-occlusion 模型在验证损失和 perplexity 上表现出色，而 occlusion-based 模型在评估生成文本质量的 BLEU score 上略微领先。该工作为低资源语言的 Transformer 模型开发提供了有益的见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15281v1",
      "published_date": "2025-01-25 17:25:06 UTC",
      "updated_date": "2025-01-25 17:25:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:12:42.033916"
    },
    {
      "arxiv_id": "2501.15280v1",
      "title": "Who's Driving? Game Theoretic Path Risk of AGI Development",
      "title_zh": "翻译失败",
      "authors": [
        "Robin Young"
      ],
      "abstract": "Who controls the development of Artificial General Intelligence (AGI) might\nmatter less than how we handle the fight for control itself. We formalize this\n\"steering wheel problem\" as humanity's greatest near-term existential risk may\nstem not from misaligned AGI, but from the dynamics of competing to develop it.\nJust as a car crash can occur from passengers fighting over the wheel before\nreaching any destination, catastrophic outcomes could arise from development\ncompetition long before AGI exists. While technical alignment research focuses\non ensuring safe arrival, we show how coordination failures during development\ncould drive us off the cliff first.\n  We present a game theoretic framework modeling AGI development dynamics and\nprove conditions for sustainable cooperative equilibria. Drawing from nuclear\ncontrol while accounting for AGI's unique characteristics, we propose concrete\nmechanisms including pre-registration, shared technical infrastructure, and\nautomated deterrence to stabilize cooperation. Our key insight is that AGI\ncreates network effects in safety: shared investments become more valuable as\nparticipation grows, enabling mechanism designs where cooperation dominates\ndefection. This work bridges formal methodology and policy frameworks,\nproviding foundations for practical governance of AGI competition risks.",
      "tldr_zh": "该论文探讨了人工智能通用智能（AGI）开发的“steering wheel problem”，即争夺控制权的竞争动态可能比AGI本身的不对齐构成更大的近期存在风险，类似于乘客争夺方向盘导致车祸。作者使用博弈论框架建模AGI开发过程，并证明了可持续合作均衡的条件，强调协调失败可能引发灾难性结果。论文提出具体机制，包括预注册、共享技术基础设施和自动化威慑，利用AGI的安全网络效应使合作优于背叛，为AGI竞争风险的治理提供实用政策基础。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15280v1",
      "published_date": "2025-01-25 17:13:12 UTC",
      "updated_date": "2025-01-25 17:13:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:12:53.582065"
    },
    {
      "arxiv_id": "2501.15276v1",
      "title": "Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Fu",
        "Michele Newman",
        "Lewis Going",
        "Qiuzi Feng",
        "Jin Ha Lee"
      ],
      "abstract": "Artificial intelligence is reshaping creative domains, yet its co-creative\nprocesses, especially in group settings with novice users, remain under\nexplored. To bridge this gap, we conducted a case study in a college-level\ncourse where nine undergraduate students were tasked with creating three\noriginal music tracks using AI tools over 10 weeks. The study spanned the\nentire creative journey from ideation to releasing these songs on Spotify.\nParticipants leveraged AI for music and lyric production, cover art, and\ndistribution. Our findings highlight how AI transforms creative workflows:\naccelerating ideation but compressing the traditional preparation stage, and\nrequiring novices to navigate a challenging idea selection and validation\nphase. We also identified a new \"collaging and refinement\" stage, where\nparticipants creatively combined diverse AI-generated outputs into cohesive\nworks. Furthermore, AI influenced group social dynamics and role division among\nhuman creators. Based on these insights, we propose the Human-AI Co-Creation\nStage Model and the Human-AI Agency Model, offering new perspectives on\ncollaborative co-creation with AI.",
      "tldr_zh": "该研究通过一个案例研究（case study），探索了 AI 在新手音乐制作中的协同创造过程，涉及 9 名本科生在 10 周内使用 AI 工具创建并发布三首原创音乐。发现 AI 加速了构思阶段但压缩了传统准备过程，并引入了新的“collaging and refinement”阶段，让参与者需应对想法选择和验证的挑战，同时影响了群组社交动态和角色分工。最终，论文提出了 Human-AI Co-Creation Stage Model 和 Human-AI Agency Model，作为理解人类-AI 协作创造的新框架。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15276v1",
      "published_date": "2025-01-25 17:00:17 UTC",
      "updated_date": "2025-01-25 17:00:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:13:05.443620"
    },
    {
      "arxiv_id": "2501.15270v1",
      "title": "Inductive Biases for Zero-shot Systematic Generalization in Language-informed Reinforcement Learning",
      "title_zh": "针对语言指导强化学习中零样本系统性泛化的归纳偏差",
      "authors": [
        "Negin Hashemi Dijujin",
        "Seyed Roozbeh Razavi Rohani",
        "Mohammad Mahdi Samiei",
        "Mahdieh Soleymani Baghshah"
      ],
      "abstract": "Sample efficiency and systematic generalization are two long-standing\nchallenges in reinforcement learning. Previous studies have shown that\ninvolving natural language along with other observation modalities can improve\ngeneralization and sample efficiency due to its compositional and open-ended\nnature. However, to transfer these properties of language to the\ndecision-making process, it is necessary to establish a proper language\ngrounding mechanism. One approach to this problem is applying inductive biases\nto extract fine-grained and informative representations from the observations,\nwhich makes them more connectable to the language units. We provide\narchitecture-level inductive biases for modularity and sparsity mainly based on\nNeural Production Systems (NPS). Alongside NPS, we assign a central role to\nmemory in our architecture. It can be seen as a high-level information\naggregator which feeds policy/value heads with comprehensive information and\nsimultaneously guides selective attention in NPS through attentional feedback.\nOur results in the BabyAI environment suggest that the proposed model's\nsystematic generalization and sample efficiency are improved significantly\ncompared to previous models. An extensive ablation study on variants of the\nproposed method is conducted, and the effectiveness of each employed technique\non generalization, sample efficiency, and training stability is specified.",
      "tldr_zh": "本文探讨了强化学习(Reinforcement Learning)中的样本效率和系统化泛化(Systematic Generalization)挑战，通过引入归纳偏差(Inductive Biases)来提升语言辅助决策的零样本性能。研究提出了一种基于Neural Production Systems (NPS)的架构，强调模块性和稀疏性，并赋予记忆(Memory)中心角色，以聚合信息并指导注意力反馈，从而更好地连接观察数据与语言单位。在BabyAI环境中的实验结果表明，该模型显著提高了泛化能力和样本效率，并通过消融研究证实了各技术的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review at Machine Learning (Springer Nature)",
      "pdf_url": "http://arxiv.org/pdf/2501.15270v1",
      "published_date": "2025-01-25 16:36:59 UTC",
      "updated_date": "2025-01-25 16:36:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:13:18.170853"
    },
    {
      "arxiv_id": "2501.15255v1",
      "title": "Lightweight and Post-Training Structured Pruning for On-Device Large Lanaguage Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zihuai Xu",
        "Yang Xu",
        "Hongli Xu",
        "Yunming Liao",
        "Zhiwei Yao",
        "Zuan Xie"
      ],
      "abstract": "Considering the hardware-friendly characteristics and broad applicability,\nstructured pruning has emerged as an efficient solution to reduce the resource\ndemands of large language models (LLMs) on resource-constrained devices.\nTraditional structured pruning methods often need fine-tuning to recover\nperformance loss, which incurs high memory overhead and substantial data\nrequirements, rendering them unsuitable for on-device applications.\nAdditionally, post-training structured pruning techniques typically necessitate\nspecific activation functions or architectural modifications, thereby limiting\ntheir scope of applications. Herein, we introduce COMP, a lightweight\npost-training structured pruning method that employs a hybrid-granularity\npruning strategy. COMP initially prunes selected model layers based on their\nimportance at a coarse granularity, followed by fine-grained neuron pruning\nwithin the dense layers of each remaining model layer. To more accurately\nevaluate neuron importance, COMP introduces a new matrix condition-based\nmetric. Subsequently, COMP utilizes mask tuning to recover accuracy without the\nneed for fine-tuning, significantly reducing memory consumption. Experimental\nresults demonstrate that COMP improves performance by 6.13\\% on the LLaMA-2-7B\nmodel with a 20\\% pruning ratio compared to LLM-Pruner, while simultaneously\nreducing memory overhead by 80\\%.",
      "tldr_zh": "该研究提出了一种轻量级后训练结构化剪枝方法COMP，旨在减少大型语言模型(LLMs)在资源受限设备上的资源需求，同时避免传统方法的高内存开销和数据依赖。COMP采用混合粒度剪枝策略，先基于层重要性进行粗粒度剪枝，然后在剩余层的密集层中进行细粒度神经元剪枝，并引入基于矩阵条件的指标来准确评估神经元重要性，最后通过掩码调优(mask tuning)恢复模型性能，而无需微调。实验结果显示，在LLaMA-2-7B模型上以20%剪枝比例，COMP相较于LLM-Pruner提升性能6.13%，并减少80%内存开销，为设备端LLMs应用提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15255v1",
      "published_date": "2025-01-25 16:03:58 UTC",
      "updated_date": "2025-01-25 16:03:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:13:29.647732"
    },
    {
      "arxiv_id": "2501.15249v2",
      "title": "An Automatic Sound and Complete Abstraction Method for Generalized Planning with Baggable Types",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Dong",
        "Zheyuan Shi",
        "Hemeng Zeng",
        "Yongmei Liu"
      ],
      "abstract": "Generalized planning is concerned with how to find a single plan to solve\nmultiple similar planning instances. Abstractions are widely used for solving\ngeneralized planning, and QNP (qualitative numeric planning) is a popular\nabstract model. Recently, Cui et al. showed that a plan solves a sound and\ncomplete abstraction of a generalized planning problem if and only if the\nrefined plan solves the original problem. However, existing work on automatic\nabstraction for generalized planning can hardly guarantee soundness let alone\ncompleteness. In this paper, we propose an automatic sound and complete\nabstraction method for generalized planning with baggable types. We use a\nvariant of QNP, called bounded QNP (BQNP), where integer variables are\nincreased or decreased by only one. Since BQNP is undecidable, we propose and\nimplement a sound but incomplete solver for BQNP. We present an automatic\nmethod to abstract a BQNP problem from a classical planning instance with\nbaggable types. The basic idea for abstraction is to introduce a counter for\neach bag of indistinguishable tuples of objects. We define a class of domains\ncalled proper baggable domains, and show that for such domains, the BQNP\nproblem got by our automatic method is a sound and complete abstraction for a\ngeneralized planning problem whose instances share the same bags with the given\ninstance but the sizes of the bags might be different. Thus, the refined plan\nof a solution to the BQNP problem is a solution to the generalized planning\nproblem. Finally, we implement our abstraction method and experiments on a\nnumber of domains demonstrate the promise of our approach.",
      "tldr_zh": "本文提出了一种自动的健全（sound）和完整（complete）抽象方法，用于带有 baggable types 的泛化规划（Generalized Planning），旨在解决多个类似规划实例的单一计划问题。方法基于 QNP（Qualitative Numeric Planning）的变体 BQNP（Bounded QNP），其中整数变量仅增减一，并引入计数器来处理每个 baggable types 的袋。作者定义了 proper baggable domains 类，并证明在这种域中，抽象出的 BQNP 问题是原泛化规划问题的健全且完整抽象，从而确保细化计划的有效性。实验结果显示，该方法在多个域上表现出色，证明了其潜在价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15249v2",
      "published_date": "2025-01-25 15:44:25 UTC",
      "updated_date": "2025-01-30 04:07:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:13:42.156956"
    },
    {
      "arxiv_id": "2501.15247v1",
      "title": "Prompting ChatGPT for Chinese Learning as L2: A CEFR and EBCL Level Study",
      "title_zh": "翻译失败",
      "authors": [
        "Miao Lin-Zucker",
        "Joël Bellassen",
        "Jean-Daniel Zucker"
      ],
      "abstract": "The use of chatbots in language learning has evolved significantly since the\n1960s, becoming more sophisticated platforms as generative AI emerged. These\ntools now simulate natural conversations, adapting to individual learners'\nneeds, including those studying Chinese. Our study explores how learners can\nuse specific prompts to engage Large Language Models (LLM) as personalized\nchatbots, aiming to target their language level based on the Common European\nFramework of Reference for Languages (CEFR) and the European Benchmarking\nChinese Language (EBCL) project. Focusing on A1, A1+ and A2 levels, we examine\nthe teaching of Chinese, which presents unique challenges due to its\nlogographic writing system. Our goal is to develop prompts that integrate oral\nand written skills, using high-frequency character lists and controlling oral\nlexical productions. These tools, powered by generative AI, aim to enhance\nlanguage practice by crossing lexical and sinographic recurrence. While\ngenerative AI shows potential as a personalized tutor, further evaluation is\nneeded to assess its effectiveness. We conducted a systematic series of\nexperiments using ChatGPT models to evaluate their adherence to constraints\nspecified in the prompts. The results indicate that incorporating level A1 and\nA1+ characters, along with the associated reference list, significantly\nenhances compliance with the EBCL character set. Properly prompted, LLMs can\nincrease exposure to the target language and offer interactive exchanges to\ndevelop language skills.",
      "tldr_zh": "该研究探讨了使用 ChatGPT 等大型语言模型 (LLM) 作为个性化聊天机器人，帮助学习者掌握中文作为第二语言 (L2)，并基于 Common European Framework of Reference for Languages (CEFR) 和 European Benchmarking Chinese Language (EBCL) 标准针对 A1、A1+ 和 A2 水平设计特定提示。方法包括整合口语和书写技能、采用高频字符列表控制词汇输出，并通过系统实验评估 ChatGPT 的遵守性。结果表明，适当的提示能显著提升对 EBCL 字符集的遵守，增加目标语言暴露和互动性，从而增强语言学习效果，但仍需进一步评估其整体效能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages, 1 figure, 5 tables, 7 appendices",
      "pdf_url": "http://arxiv.org/pdf/2501.15247v1",
      "published_date": "2025-01-25 15:30:13 UTC",
      "updated_date": "2025-01-25 15:30:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:13:54.314997"
    },
    {
      "arxiv_id": "2501.15240v1",
      "title": "Hardware-Aware DNN Compression for Homogeneous Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Kunlong Zhang",
        "Guiying Li",
        "Ning Lu",
        "Peng Yang",
        "Ke Tang"
      ],
      "abstract": "Deploying deep neural networks (DNNs) across homogeneous edge devices (the\ndevices with the same SKU labeled by the manufacturer) often assumes identical\nperformance among them. However, once a device model is widely deployed, the\nperformance of each device becomes different after a period of running. This is\ncaused by the differences in user configurations, environmental conditions,\nmanufacturing variances, battery degradation, etc. Existing DNN compression\nmethods have not taken this scenario into consideration and can not guarantee\ngood compression results in all homogeneous edge devices. To address this, we\npropose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN\ncompression framework explicitly designed for homogeneous edge devices, aiming\nto achieve optimal average performance of the compressed model across all\ndevices. To deal with the difficulty of time-consuming hardware-aware\nevaluations for thousands or millions of homogeneous edge devices, HDAP\npartitions all the devices into several device clusters, which can dramatically\nreduce the number of devices to evaluate and use the surrogate-based evaluation\ninstead of hardware evaluation in real-time. Experiments on ResNet50 and\nMobileNetV1 with the ImageNet dataset show that HDAP consistently achieves\nlower average inference latency compared with state-of-the-art methods, with\nsubstantial speedup gains (e.g., 2.86 $\\times$ speedup at 1.0G FLOPs for\nResNet50) on the homogeneous device clusters. HDAP offers an effective solution\nfor scalable, high-performance DNN deployment methods for homogeneous edge\ndevices.",
      "tldr_zh": "该研究针对同类边缘设备（homogeneous edge devices）性能差异问题（如用户配置和环境因素导致），提出HDAP（Homogeneous-Device Aware Pruning）框架，这是一种硬件感知的DNN压缩方法，旨在优化压缩模型在所有设备的平均性能。HDAP通过将设备分区成多个集群，并使用代理评估代替实时硬件评估，从而显著减少评估开销。实验在ResNet50和MobileNetV1上使用ImageNet数据集显示，HDAP比现有方法实现了更低的平均推理延迟，并取得了显著加速（如ResNet50在1.0G FLOPs下2.86倍加速）。该框架为可扩展的高性能DNN部署提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15240v1",
      "published_date": "2025-01-25 15:14:18 UTC",
      "updated_date": "2025-01-25 15:14:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:14:05.508783"
    },
    {
      "arxiv_id": "2501.15225v1",
      "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Changhun Lee",
        "Jun-gyu Jin",
        "Younghyun Cho",
        "Eunhyeok Park"
      ],
      "abstract": "In this work, we introduce a novel approach called Scaling to Emphasize\nAttention for Long-context retrieval (SEAL), which enhances the retrieval\nperformance of large language models (LLMs) over extended contexts. Previous\nstudies have shown that each attention head in LLMs has a unique functionality\nand collectively contributes to the overall behavior of the model. Similarly,\nwe observe that specific heads are closely tied to long-context retrieval,\nshowing positive or negative correlation with retrieval scores. Built on this\ninsight, we propose a learning-based mechanism using zero-shot generated data\nto emphasize these heads, improving the model's performance in long-context\nretrieval tasks. By applying SEAL, we can achieve significant improvements in\nin-domain retrieval performance, including document QA tasks from LongBench,\nand considerable improvements in out-of-domain cases. Additionally, when\ncombined with existing training-free context extension techniques, SEAL extends\nthe context limits of LLMs while maintaining highly reliable outputs, opening\nnew avenues for research in this field.",
      "tldr_zh": "本研究引入了SEAL（Scaling to Emphasize Attention for Long-Context Retrieval）方法，以提升大型语言模型（LLMs）在长上下文检索任务中的性能。SEAL基于对注意力头（attention heads）的功能分析，发现某些头与检索成绩呈正负相关，并通过零样本生成数据的学习机制来强调这些关键头，从而优化模型表现。实验结果显示，SEAL在领域内任务（如LongBench的文档QA）上显著提升检索准确率，在领域外任务上也取得可观改善；此外，与现有无训练上下文扩展技术结合后，SEAL能扩展LLMs的上下文限制，同时保持输出可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.15225v1",
      "published_date": "2025-01-25 14:09:39 UTC",
      "updated_date": "2025-01-25 14:09:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:14:17.403310"
    },
    {
      "arxiv_id": "2501.15223v1",
      "title": "Efficient and Interpretable Neural Networks Using Complex Lehmer Transform",
      "title_zh": "翻译失败",
      "authors": [
        "Masoud Ataei",
        "Xiaogang Wang"
      ],
      "abstract": "We propose an efficient and interpretable neural network with a novel\nactivation function called the weighted Lehmer transform. This new activation\nfunction enables adaptive feature selection and extends to the complex domain,\ncapturing phase-sensitive and hierarchical relationships within data. Notably,\nit provides greater interpretability and transparency compared to existing\nmachine learning models, facilitating a deeper understanding of its\nfunctionality and decision-making processes. We analyze the mathematical\nproperties of both real-valued and complex-valued Lehmer activation units and\ndemonstrate their applications in modeling nonlinear interactions. Empirical\nevaluations demonstrate that our proposed neural network achieves competitive\naccuracy on benchmark datasets with significantly improved computational\nefficiency. A single layer of real-valued or complex-valued Lehmer activation\nunits is shown to deliver state-of-the-art performance, balancing efficiency\nwith interpretability.",
      "tldr_zh": "本文提出了一种高效且可解释的神经网络，使用新型激活函数 weighted Lehmer transform，以实现自适应特征选择并扩展到复杂域，从而捕捉数据的相位敏感和层次关系。该激活函数比现有模型更具透明度，便于理解决策过程，并分析了其实值和复值形式的数学属性及其在建模非线性交互中的应用。实验评估显示，该网络在基准数据集上实现了竞争性准确率，同时显著提高了计算效率，且单层 Lehmer 激活单元即可达到最先进的性能，平衡了效率与可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15223v1",
      "published_date": "2025-01-25 14:08:30 UTC",
      "updated_date": "2025-01-25 14:08:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:14:29.857777"
    },
    {
      "arxiv_id": "2501.15198v1",
      "title": "Towards Conscious Service Robots",
      "title_zh": "迈向有意识的服务机器人",
      "authors": [
        "Sven Behnke"
      ],
      "abstract": "Deep learning's success in perception, natural language processing, etc.\ninspires hopes for advancements in autonomous robotics. However, real-world\nrobotics face challenges like variability, high-dimensional state spaces,\nnon-linear dependencies, and partial observability. A key issue is\nnon-stationarity of robots, environments, and tasks, leading to performance\ndrops with out-of-distribution data. Unlike current machine learning models,\nhumans adapt quickly to changes and new tasks due to a cognitive architecture\nthat enables systematic generalization and meta-cognition. Human brain's System\n1 handles routine tasks unconsciously, while System 2 manages complex tasks\nconsciously, facilitating flexible problem-solving and self-monitoring. For\nrobots to achieve human-like learning and reasoning, they need to integrate\ncausal models, working memory, planning, and metacognitive processing. By\nincorporating human cognition insights, the next generation of service robots\nwill handle novel situations and monitor themselves to avoid risks and mitigate\nerrors.",
      "tldr_zh": "该论文讨论了深度学习在感知和自然 language processing 等领域的成功，但机器人应用面临变异性、高维状态空间、非线性依赖和部分 observability 等挑战，导致非平稳性问题和性能下降。作者借鉴人类认知架构，提出机器人应整合 System 1（处理常规任务的无意识系统）和 System 2（处理复杂任务的有意识系统），以及因果模型、工作记忆、规划和 meta-cognition 处理，以实现快速适应和新任务的系统化泛化。通过这些见解，下一代服务机器人有望处理新情况、进行自我监控，避免风险并减轻错误。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "In: Science for a Better Tomorrow: Curious 2024 Insights Actions,\n  Springer 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15198v1",
      "published_date": "2025-01-25 12:32:52 UTC",
      "updated_date": "2025-01-25 12:32:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:14:41.460744"
    },
    {
      "arxiv_id": "2501.17183v2",
      "title": "LLM Evaluation Based on Aerospace Manufacturing Expertise: Automated Generation and Multi-Model Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Beiming Liu",
        "Zhizhuo Cui",
        "Siteng Hu",
        "Xiaohua Li",
        "Haifeng Lin",
        "Zhengxin Zhang"
      ],
      "abstract": "Aerospace manufacturing demands exceptionally high precision in technical\nparameters. The remarkable performance of Large Language Models (LLMs), such as\nGPT-4 and QWen, in Natural Language Processing has sparked industry interest in\ntheir application to tasks including process design, material selection, and\ntool information retrieval. However, LLMs are prone to generating\n\"hallucinations\" in specialized domains, producing inaccurate or false\ninformation that poses significant risks to the quality of aerospace products\nand flight safety. This paper introduces a set of evaluation metrics tailored\nfor LLMs in aerospace manufacturing, aiming to assess their accuracy by\nanalyzing their performance in answering questions grounded in professional\nknowledge. Firstly, key information is extracted through in-depth textual\nanalysis of classic aerospace manufacturing textbooks and guidelines.\nSubsequently, utilizing LLM generation techniques, we meticulously construct\nmultiple-choice questions with multiple correct answers of varying difficulty.\nFollowing this, different LLM models are employed to answer these questions,\nand their accuracy is recorded. Experimental results demonstrate that the\ncapabilities of LLMs in aerospace professional knowledge are in urgent need of\nimprovement. This study provides a theoretical foundation and practical\nguidance for the application of LLMs in aerospace manufacturing, addressing a\ncritical gap in the field.",
      "tldr_zh": "该研究针对航空航天制造领域的LLMs（Large Language Models）如GPT-4和QWen，提出一套评估指标，以评估其在专业知识上的准确性，因为LLMs容易产生“hallucinations”（幻觉），可能威胁产品质量和飞行安全。方法包括从经典航空航天教科书和指南中提取关键信息，并利用LLM生成技术自动创建难度不同的多选题（multiple-choice questions with multiple correct answers），随后使用多种LLM模型回答这些问题并记录准确率。实验结果显示，LLMs在航空航天专业知识方面的能力亟待提升，为其在该领域的应用提供了理论基础和实用指导，填补了关键空白。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50, 90B30",
        "I.2.7; J.2"
      ],
      "primary_category": "cs.CL",
      "comment": "conference paper",
      "pdf_url": "http://arxiv.org/pdf/2501.17183v2",
      "published_date": "2025-01-25 12:26:44 UTC",
      "updated_date": "2025-02-01 10:18:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:14:53.377582"
    },
    {
      "arxiv_id": "2501.17182v2",
      "title": "Dialogue Systems for Emotional Support via Value Reinforcement",
      "title_zh": "通过价值强化的情感支持对话系统",
      "authors": [
        "Juhee Kim",
        "Chunghu Mok",
        "Jisun Lee",
        "Hyang Sook Kim",
        "Yohan Jo"
      ],
      "abstract": "Emotional support dialogue systems aim to reduce help-seekers' distress and\nhelp them overcome challenges. While human values$\\unicode{x2013}$core beliefs\nthat shape an individual's priorities$\\unicode{x2013}$are increasingly\nemphasized in contemporary psychological therapy for their role in fostering\ninternal transformation and long-term emotional well-being, their integration\ninto emotional support systems remains underexplored. To bridge this gap, we\npresent a value-driven method for training emotional support dialogue systems\ndesigned to reinforce positive values in seekers. Notably, our model identifies\nwhich values to reinforce at each turn and how to do so, by leveraging online\nsupport conversations from Reddit. We evaluate the method across support\nskills, seekers' emotional intensity, and value reinforcement. Our method\nconsistently outperforms various baselines, effectively exploring and eliciting\nvalues from seekers. Additionally, leveraging crowd knowledge from Reddit\nsignificantly enhances its effectiveness. Therapists highlighted its ability to\nvalidate seekers' challenges and emphasize positive aspects of their\nsituations$\\unicode{x2013}$both crucial elements of value reinforcement. Our\nwork, being the first to integrate value reinforcement into emotional support\nsystems, demonstrates its promise and establishes a foundation for future\nresearch.",
      "tldr_zh": "这篇论文提出了一种基于价值强化（value reinforcement）的训练方法，用于构建情感支持对话系统，以强化求助者的积极价值观，从而促进他们的内部转变和长期情感福祉。方法通过分析 Reddit 的在线支持对话，模型能动态识别每个回合需要强化的价值观及其方式，并有效探索和激发求助者的价值观。实验结果显示，该系统在支持技能、求助者情感强度和价值强化方面均优于基线模型，利用 Reddit 的众包知识进一步提升了其效果。治疗师反馈强调，该系统能验证求助者的挑战并突出积极方面，为情感支持领域首次整合价值强化提供了重要基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.17182v2",
      "published_date": "2025-01-25 11:51:31 UTC",
      "updated_date": "2025-03-09 07:37:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:15:05.716863"
    },
    {
      "arxiv_id": "2501.15175v3",
      "title": "Option-ID Based Elimination For Multiple Choice Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenhao Zhu",
        "Bulou Liu",
        "Qingyao Ai",
        "Yiqun Liu"
      ],
      "abstract": "Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies.",
      "tldr_zh": "这篇论文针对多项选择题 (MCQs) 在评估大型语言模型 (LLMs) 中的挑战，提出了一种新型基于选项 ID 的消除方法 ($\\text{PoE}_{\\text{ID}}$)，通过引入 debiasing 技术来缓解 LLMs 的 token bias 问题。$\\text{PoE}_{\\text{ID}}$ 包括两种策略：$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ (消除 ID 日志概率低于平均阈值的选项) 和 $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$ (迭代移除概率最低的选项)，以更有效地识别正确答案。实验在 6 个 LLMs 和 4 个数据集上显示，该方法显著提高了 zero-shot 和 few-shot MCQs 的性能，尤其在选项较多的数据集上，并揭示了 LLMs 在直接识别错误选项时的固有局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15175v3",
      "published_date": "2025-01-25 11:06:37 UTC",
      "updated_date": "2025-05-19 17:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:15:19.864964"
    },
    {
      "arxiv_id": "2501.15149v1",
      "title": "Mapping Galaxy Images Across Ultraviolet, Visible and Infrared Bands Using Generative Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Youssef Zaazou",
        "Alex Bihlo",
        "Terrence S. Tricco"
      ],
      "abstract": "We demonstrate that generative deep learning can translate galaxy\nobservations across ultraviolet, visible, and infrared photometric bands.\nLeveraging mock observations from the Illustris simulations, we develop and\nvalidate a supervised image-to-image model capable of performing both band\ninterpolation and extrapolation. The resulting trained models exhibit high\nfidelity in generating outputs, as verified by both general image comparison\nmetrics (MAE, SSIM, PSNR) and specialized astronomical metrics (GINI\ncoefficient, M20). Moreover, we show that our model can be used to predict\nreal-world observations, using data from the DECaLS survey as a case study.\nThese findings highlight the potential of generative learning to augment\nastronomical datasets, enabling efficient exploration of multi-band information\nin regions where observations are incomplete. This work opens new pathways for\noptimizing mission planning, guiding high-resolution follow-ups, and enhancing\nour understanding of galaxy morphology and evolution.",
      "tldr_zh": "本研究利用生成式深度学习（Generative Deep Learning）开发了一种图像到图像模型，能够将星系观测从紫外、可见光和红外光谱带相互转换，包括带内插和外推。模型基于 Illustris 模拟的模拟数据进行监督式训练，并通过一般指标（如 MAE、SSIM、PSNR）和天文学专用指标（如 GINI coefficient、M20）验证其输出高保真度。实验结果显示，该模型可成功预测真实观测数据，如 DECaLS survey中的案例，进一步增强天文数据集的完整性，并为优化任务规划、指导高分辨率后续观测以及加深对星系形态和演化的理解提供新途径。",
      "categories": [
        "astro-ph.IM",
        "astro-ph.GA",
        "cs.AI"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "15 pages, 6 figures, Submitted to ApJ, GitHub:\n  https://github.com/yazaazou/Galaxy-Band-Conversion",
      "pdf_url": "http://arxiv.org/pdf/2501.15149v1",
      "published_date": "2025-01-25 09:13:21 UTC",
      "updated_date": "2025-01-25 09:13:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:15:30.853033"
    },
    {
      "arxiv_id": "2501.15147v2",
      "title": "A Causality-aware Paradigm for Evaluating Creativity of Multimodal Large Language Models",
      "title_zh": "一种因果感知范式，用于评估多模态大语言模型的创造力",
      "authors": [
        "Zhongzhan Huang",
        "Shanshan Zhong",
        "Pan Zhou",
        "Shanghua Gao",
        "Marinka Zitnik",
        "Liang Lin"
      ],
      "abstract": "Recently, numerous benchmarks have been developed to evaluate the logical\nreasoning abilities of large language models (LLMs). However, assessing the\nequally important creative capabilities of LLMs is challenging due to the\nsubjective, diverse, and data-scarce nature of creativity, especially in\nmultimodal scenarios. In this paper, we consider the comprehensive pipeline for\nevaluating the creativity of multimodal LLMs, with a focus on suitable\nevaluation platforms and methodologies. First, we find the Oogiri game, a\ncreativity-driven task requiring humor, associative thinking, and the ability\nto produce unexpected responses to text, images, or both. This game aligns well\nwith the input-output structure of modern multimodal LLMs and benefits from a\nrich repository of high-quality, human-annotated creative responses, making it\nan ideal platform for studying LLM creativity. Next, beyond using the Oogiri\ngame for standard evaluations like ranking and selection, we propose LoTbench,\nan interactive, causality-aware evaluation framework, to further address some\nintrinsic risks in standard evaluations, such as information leakage and\nlimited interpretability. The proposed LoTbench not only quantifies LLM\ncreativity more effectively but also visualizes the underlying creative thought\nprocesses. Our results show that while most LLMs exhibit constrained\ncreativity, the performance gap between LLMs and humans is not insurmountable.\nFurthermore, we observe a strong correlation between results from the\nmultimodal cognition benchmark MMMU and LoTbench, but only a weak connection\nwith traditional creativity metrics. This suggests that LoTbench better aligns\nwith human cognitive theories, highlighting cognition as a critical foundation\nin the early stages of creativity and enabling the bridging of diverse\nconcepts. https://lotbench.github.io",
      "tldr_zh": "该论文探讨了评估多模态大语言模型(Multimodal LLMs)创造力的挑战，并采用Oogiri游戏作为评估平台，该游戏要求幽默、联想思维和意外响应，适合多模态输入。\n作者提出LoTbench，一个交互式、因果意识的评估框架，能解决标准评估中的信息泄露和可解释性问题，并通过量化创造力和可视化思考过程来提升评估有效性。\n实验结果显示，大多数LLMs的创造力受限，但与人类的差距并非不可逾越，且LoTbench与多模态认知基准MMMU有强相关，而与传统指标弱相关，突显了认知在创造力早期阶段的关键作用。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by TPAMI. arXiv admin note: text overlap with\n  arXiv:2312.02439",
      "pdf_url": "http://arxiv.org/pdf/2501.15147v2",
      "published_date": "2025-01-25 09:11:15 UTC",
      "updated_date": "2025-02-23 08:09:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:15:43.534814"
    },
    {
      "arxiv_id": "2501.15142v1",
      "title": "DAGPrompT: Pushing the Limits of Graph Prompting with a Distribution-aware Graph Prompt Tuning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Qin Chen",
        "Liang Wang",
        "Bo Zheng",
        "Guojie Song"
      ],
      "abstract": "The pre-train then fine-tune approach has advanced GNNs by enabling general\nknowledge capture without task-specific labels. However, an objective gap\nbetween pre-training and downstream tasks limits its effectiveness. Recent\ngraph prompting methods aim to close this gap through task reformulations and\nlearnable prompts. Despite this, they struggle with complex graphs like\nheterophily graphs. Freezing the GNN encoder can reduce the impact of\nprompting, while simple prompts fail to handle diverse hop-level distributions.\nThis paper identifies two key challenges in adapting graph prompting methods\nfor complex graphs: (1) adapting the model to new distributions in downstream\ntasks to mitigate pre-training and fine-tuning discrepancies from heterophily\nand (2) customizing prompts for hop-specific node requirements. To overcome\nthese challenges, we propose Distribution-aware Graph Prompt Tuning\n(DAGPrompT), which integrates a GLoRA module for optimizing the GNN encoder's\nprojection matrix and message-passing schema through low-rank adaptation.\nDAGPrompT also incorporates hop-specific prompts accounting for varying graph\nstructures and distributions among hops. Evaluations on 10 datasets and 14\nbaselines demonstrate that DAGPrompT improves accuracy by up to 4.79 in node\nand graph classification tasks, setting a new state-of-the-art while preserving\nefficiency. Codes are available at GitHub.",
      "tldr_zh": "这篇论文针对图神经网络(GNNs)预训练与下游任务之间的目标差距，提出了DAGPrompT方法，以更好地适应复杂图如异质图的挑战。DAGPrompT整合了GLoRA模块，通过低秩适应优化GNN编码器的投影矩阵和消息传递模式，并引入跳跃特定提示来处理不同跳跃级别的图结构和分布差异。实验结果显示，该方法在10个数据集和14个基线模型上，节点和图分类任务的准确率提高了高达4.79%，设定了新的最先进水平，同时保持了高效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in WWW '25, April 28-May 2, 2025, Sydney, NSW,\n  Australia",
      "pdf_url": "http://arxiv.org/pdf/2501.15142v1",
      "published_date": "2025-01-25 08:53:42 UTC",
      "updated_date": "2025-01-25 08:53:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:15:55.579849"
    },
    {
      "arxiv_id": "2501.15140v3",
      "title": "Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models",
      "title_zh": "针对多模态大语言模型的细粒度视觉识别能力的分析与提升",
      "authors": [
        "Hulingxiao He",
        "Geng Li",
        "Zijun Geng",
        "Jinglin Xu",
        "Yuxin Peng"
      ],
      "abstract": "Multi-modal large language models (MLLMs) have shown remarkable abilities in\nvarious visual understanding tasks. However, MLLMs still struggle with\nfine-grained visual recognition (FGVR), which aims to identify\nsubordinate-level categories from images. This can negatively impact more\nadvanced capabilities of MLLMs, such as object-centric visual question\nanswering and reasoning. In our study, we revisit three quintessential\ncapabilities of MLLMs for FGVR, including object information extraction,\ncategory knowledge reserve, object-category alignment, and position of the root\ncause as a misalignment problem. To address this issue, we present Finedefics,\nan MLLM that enhances the model's FGVR capability by incorporating informative\nattribute descriptions of objects into the training phase. We employ\ncontrastive learning on object-attribute pairs and attribute-category pairs\nsimultaneously and use examples from similar but incorrect categories as hard\nnegatives, naturally bringing representations of visual objects and category\nnames closer. Extensive evaluations across multiple popular FGVR datasets\ndemonstrate that Finedefics outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code is available at\nhttps://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.",
      "tldr_zh": "本文分析了多模态大语言模型(MLLMs)在细粒度视觉识别(FGVR)中的不足，包括对象信息提取、类别知识储备和对象-类别对齐问题，这些限制了MLLMs在高级任务如视觉问答和推理中的表现。针对这一问题，研究提出Finedefics模型，通过在训练阶段融入对象的属性描述，并采用对比学习处理对象-属性对和属性-类别对，同时使用硬负样本（如类似但错误的类别）来加强视觉对象与类别名称的表示对齐。实验结果显示，Finedefics在多个流行FGVR数据集上超过了同等参数规模的现有MLLMs，提升了模型的识别能力和整体效能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published as a conference paper at ICLR 2025. The model is available\n  at https://huggingface.co/StevenHH2000/Finedefics",
      "pdf_url": "http://arxiv.org/pdf/2501.15140v3",
      "published_date": "2025-01-25 08:52:43 UTC",
      "updated_date": "2025-03-30 13:12:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:16:07.865194"
    },
    {
      "arxiv_id": "2501.15122v1",
      "title": "Snapshot Compressed Imaging Based Single-Measurement Computer Vision for Videos",
      "title_zh": "基于快照压缩成像的单测量视频计算机视觉",
      "authors": [
        "Fengpu Pan",
        "Jiangtao Wen",
        "Yuxing Han"
      ],
      "abstract": "Snapshot compressive imaging (SCI) is a promising technique for capturing\nhigh-speed video at low bandwidth and low power, typically by compressing\nmultiple frames into a single measurement. However, similar to traditional CMOS\nimage sensor based imaging systems, SCI also faces challenges in low-lighting\nphoton-limited and low-signal-to-noise-ratio image conditions. In this paper,\nwe propose a novel Compressive Denoising Autoencoder (CompDAE) using the\nSTFormer architecture as the backbone, to explicitly model noise\ncharacteristics and provide computer vision functionalities such as edge\ndetection and depth estimation directly from compressed sensing measurements,\nwhile accounting for realistic low-photon conditions. We evaluate the\neffectiveness of CompDAE across various datasets and demonstrated significant\nimprovements in task performance compared to conventional RGB-based methods. In\nthe case of ultra-low-lighting (APC $\\leq$ 20) while conventional methods\nfailed, the proposed algorithm can still maintain competitive performance.",
      "tldr_zh": "本文提出了一种基于 Snapshot Compressed Imaging (SCI) 的单测量计算机视觉方法，用于在低带宽和低功耗条件下捕获高速度视频。研究开发了 Compressive Denoising Autoencoder (CompDAE)，以 STFormer 架构为骨干，显式建模噪声特性，并直接从压缩感知测量中实现计算机视觉任务，如边缘检测和深度估计，同时适应低光子条件。在各种数据集上实验表明，CompDAE 显著优于传统 RGB-based 方法，尤其在超低光环境 (APC ≤ 20) 中，仍能保持竞争性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15122v1",
      "published_date": "2025-01-25 08:20:30 UTC",
      "updated_date": "2025-01-25 08:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:16:19.634927"
    },
    {
      "arxiv_id": "2501.15109v1",
      "title": "Clear Preferences Leave Traces: Reference Model-Guided Sampling for Preference Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nirav Diwan",
        "Tolga Ergen",
        "Dongsub Shim",
        "Honglak Lee"
      ],
      "abstract": "Direct Preference Optimization (DPO) has emerged as a de-facto approach for\naligning language models with human preferences. Recent work has shown DPO's\neffectiveness relies on training data quality. In particular, clear quality\ndifferences between preferred and rejected responses enhance learning\nperformance. Current methods for identifying and obtaining such high-quality\nsamples demand additional resources or external models. We discover that\nreference model probability space naturally detects high-quality training\nsamples. Using this insight, we present a sampling strategy that achieves\nconsistent improvements (+0.1 to +0.4) on MT-Bench while using less than half\n(30-50%) of the training data. We observe substantial improvements (+0.4 to\n+0.98) for technical tasks (coding, math, and reasoning) across multiple models\nand hyperparameter settings.",
      "tldr_zh": "这篇论文探讨了Direct Preference Optimization (DPO) 在对齐语言模型与人类偏好时对训练数据质量的依赖性，特别是优选和拒绝响应间的清晰差异。研究发现，参考模型的概率空间可自然检测高质量样本，并据此提出了一种参考模型指导的采样策略，使用30-50%的训练数据即在MT-Bench上实现性能提升（+0.1至+0.4）。此外，对于编码、数学和推理等技术任务，该策略在多个模型和超参数设置下带来了显著改进（+0.4至+0.98），从而提高了DPO的效率和效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15109v1",
      "published_date": "2025-01-25 07:21:50 UTC",
      "updated_date": "2025-01-25 07:21:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:16:32.180576"
    },
    {
      "arxiv_id": "2501.15105v1",
      "title": "A New Approach for Knowledge Generation Using Active Inference",
      "title_zh": "一种新的基于主动推理的知识生成方法",
      "authors": [
        "Jamshid Ghasimi",
        "Nazanin Movarraei"
      ],
      "abstract": "There are various models proposed on how knowledge is generated in the human\nbrain including the semantic networks model. Although this model has been\nwidely studied and even computational models are presented, but, due to various\nlimits and inefficiencies in the generation of different types of knowledge,\nits application is limited to semantic knowledge because of has been formed\naccording to semantic memory and declarative knowledge and has many limits in\nexplaining various procedural and conditional knowledge. Given the importance\nof providing an appropriate model for knowledge generation, especially in the\nareas of improving human cognitive functions or building intelligent machines,\nimproving existing models in knowledge generation or providing more\ncomprehensive models is of great importance. In the current study, based on the\nfree energy principle of the brain, is the researchers proposed a model for\ngenerating three types of declarative, procedural, and conditional knowledge.\nWhile explaining different types of knowledge, this model is capable to compute\nand generate concepts from stimuli based on probabilistic mathematics and the\naction-perception process (active inference). The proposed model is\nunsupervised learning that can update itself using a combination of different\nstimuli as a generative model can generate new concepts of unsupervised\nreceived stimuli. In this model, the active inference process is used in the\ngeneration of procedural and conditional knowledge and the perception process\nis used to generate declarative knowledge.",
      "tldr_zh": "本研究针对现有知识生成模型（如语义网络）的局限性，提出了一种基于自由能量原理（free energy principle）的新方法，用于生成三种知识类型：declarative knowledge、procedural knowledge 和 conditional knowledge。模型利用主动推理（active inference）和行动-感知过程（action-perception process），通过概率数学从刺激中计算并生成概念，实现无监督学习和自我更新。相比传统模型，该方法能更全面地处理不同知识类型，为提升人类认知功能或构建智能机器提供更可靠的框架。",
      "categories": [
        "cs.AI",
        "q-bio.NC",
        "60A99",
        "G.3"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15105v1",
      "published_date": "2025-01-25 07:06:33 UTC",
      "updated_date": "2025-01-25 07:06:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:16:42.724308"
    },
    {
      "arxiv_id": "2501.15103v1",
      "title": "Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyu Zhao",
        "Yixiao Zhou",
        "Didi Zhu",
        "Tao Shen",
        "Xuwu Wang",
        "Jing Su",
        "Kun Kuang",
        "Zhongyu Wei",
        "Fei Wu",
        "Yu Cheng"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is widely used for adapting large language models\n(LLMs) to specific domains due to its efficiency and modularity. Meanwhile,\nvanilla LoRA struggles with task conflicts in multi-task scenarios. Recent\nworks adopt Mixture of Experts (MoE) by treating each LoRA module as an expert,\nthereby mitigating task interference through multiple specialized LoRA modules.\nWhile effective, these methods often isolate knowledge within individual tasks,\nfailing to fully exploit the shared knowledge across related tasks. In this\npaper, we establish a connection between single LoRA and multi-LoRA MoE,\nintegrating them into a unified framework. We demonstrate that the dynamic\nrouting of multiple LoRAs is functionally equivalent to rank partitioning and\nblock-level activation within a single LoRA. We further empirically demonstrate\nthat finer-grained LoRA partitioning, within the same total and activated\nparameter constraints, leads to better performance gains across heterogeneous\ntasks. Building on these findings, we propose Single-ranked Mixture of Experts\nLoRA (\\textbf{SMoRA}), which embeds MoE into LoRA by \\textit{treating each rank\nas an independent expert}. With a \\textit{dynamic rank-wise activation}\nmechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task\nconflicts. Experiments demonstrate that SMoRA activates fewer parameters yet\nachieves better performance in multi-task scenarios.",
      "tldr_zh": "该论文探讨了 Low-Rank Adaptation (LoRA) 在多任务学习中的任务冲突问题，并建立了 single LoRA 和 multi-LoRA Mixture of Experts (MoE) 的统一框架，证明动态路由等效于单个 LoRA 中的 rank 分区和块级激活。作者提出 Single-ranked Mixture of Experts LoRA (SMoRA) 方法，将每个 rank 视为独立专家，通过动态 rank-wise 激活机制促进知识共享并缓解任务干扰。实验表明，SMoRA 在多任务场景中激活更少参数，却实现了更好的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15103v1",
      "published_date": "2025-01-25 06:56:39 UTC",
      "updated_date": "2025-01-25 06:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:16:56.013598"
    },
    {
      "arxiv_id": "2501.15098v1",
      "title": "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm With Cuckoo Filter",
      "title_zh": "翻译失败",
      "authors": [
        "Zihang Li",
        "Yangdong Ruan",
        "Wenjun Liu",
        "Zhengyang Wang",
        "Tong Yang"
      ],
      "abstract": "Although retrieval-augmented generation(RAG) significantly improves\ngeneration quality by retrieving external knowledge bases and integrating\ngenerated content, it faces computational efficiency bottlenecks, particularly\nin knowledge retrieval tasks involving hierarchical structures for Tree-RAG.\nThis paper proposes a Tree-RAG acceleration method based on the improved Cuckoo\nFilter, which optimizes entity localization during the retrieval process to\nachieve significant performance improvements. Tree-RAG effectively organizes\nentities through the introduction of a hierarchical tree structure, while the\nCuckoo Filter serves as an efficient data structure that supports rapid\nmembership queries and dynamic updates. The experiment results demonstrate that\nour method is much faster than naive Tree-RAG while maintaining high levels of\ngenerative quality. When the number of trees is large, our method is hundreds\nof times faster than naive Tree-RAG. Our work is available at\nhttps://github.com/TUPYP7180/CFT-RAG-2025.",
      "tldr_zh": "该论文提出 CFT-RAG 算法，一种基于实体树结构的 Retrieval-Augmented Generation (RAG) 方法，使用改进的 Cuckoo Filter 优化检索过程中的实体定位，从而解决 Tree-RAG 在计算效率上的瓶颈问题。Cuckoo Filter 作为高效数据结构，支持快速成员查询和动态更新，与层次树结构相结合，有效组织实体并加速知识检索。实验结果显示，CFT-RAG 比原始 Tree-RAG 快数百倍，尤其在树数量较大时，同时保持高生成质量，代码已在 GitHub 上开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15098v1",
      "published_date": "2025-01-25 06:09:02 UTC",
      "updated_date": "2025-01-25 06:09:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:17:06.814857"
    },
    {
      "arxiv_id": "2501.15085v2",
      "title": "Data Center Cooling System Optimization Using Offline Reinforcement Learning",
      "title_zh": "基于离线强化学习的数据中心冷却系统优化",
      "authors": [
        "Xianyuan Zhan",
        "Xiangyu Zhu",
        "Peng Cheng",
        "Xiao Hu",
        "Ziteng He",
        "Hanfei Geng",
        "Jichao Leng",
        "Huiwen Zheng",
        "Chenhui Liu",
        "Tianshun Hong",
        "Yan Liang",
        "Yunxin Liu",
        "Feng Zhao"
      ],
      "abstract": "The recent advances in information technology and artificial intelligence\nhave fueled a rapid expansion of the data center (DC) industry worldwide,\naccompanied by an immense appetite for electricity to power the DCs. In a\ntypical DC, around 30~40% of the energy is spent on the cooling system rather\nthan on computer servers, posing a pressing need for developing new\nenergy-saving optimization technologies for DC cooling systems. However,\noptimizing such real-world industrial systems faces numerous challenges,\nincluding but not limited to a lack of reliable simulation environments,\nlimited historical data, and stringent safety and control robustness\nrequirements. In this work, we present a novel physics-informed offline\nreinforcement learning (RL) framework for energy efficiency optimization of DC\ncooling systems. The proposed framework models the complex dynamical patterns\nand physical dependencies inside a server room using a purposely designed graph\nneural network architecture that is compliant with the fundamental\ntime-reversal symmetry. Because of its well-behaved and generalizable\nstate-action representations, the model enables sample-efficient and robust\nlatent space offline policy learning using limited real-world operational data.\nOur framework has been successfully deployed and verified in a large-scale\nproduction DC for closed-loop control of its air-cooling units (ACUs). We\nconducted a total of 2000 hours of short and long-term experiments in the\nproduction DC environment. The results show that our method achieves 14~21%\nenergy savings in the DC cooling system, without any violation of the safety or\noperational constraints. Our results have demonstrated the significant\npotential of offline RL in solving a broad range of data-limited,\nsafety-critical real-world industrial control problems.",
      "tldr_zh": "这篇论文提出了一种基于physics-informed offline reinforcement learning的框架，用于优化数据中心冷却系统的能源效率，以应对能源消耗高、数据有限和安全约束的挑战。该框架利用graph neural network架构建模服务器室的复杂动态和物理依赖，确保模型在有限的真实世界数据上实现高效且鲁棒的策略学习。实验结果显示，在大型生产数据中心进行2000小时的闭环控制测试后，该方法实现了14-21%的能源节省，同时未违反任何安全或操作约束，展示了offline RL在数据有限的安全关键工业控制领域的潜力。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15085v2",
      "published_date": "2025-01-25 05:28:44 UTC",
      "updated_date": "2025-02-14 06:50:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:17:18.473565"
    },
    {
      "arxiv_id": "2501.15084v1",
      "title": "Hierarchical Pattern Decryption Methodology for Ransomware Detection Using Probabilistic Cryptographic Footprints",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Pekepok",
        "Persephone Kirkwood",
        "Esme Christopolous",
        "Florence Braithwaite",
        "Oliver Nightingale"
      ],
      "abstract": "The increasing sophistication of encryption-based ransomware has demanded\ninnovative approaches to detection and mitigation, prompting the development of\na hierarchical framework grounded in probabilistic cryptographic analysis. By\nfocusing on the statistical characteristics of encryption patterns, the\nproposed methodology introduces a layered approach that combines advanced\nclustering algorithms with machine learning to isolate ransomware-induced\nanomalies. Through comprehensive testing across diverse ransomware families,\nthe framework demonstrated exceptional accuracy, effectively distinguishing\nmalicious encryption operations from benign activities while maintaining low\nfalse positive rates. The system's design integrates dynamic feedback\nmechanisms, enabling adaptability to varying cryptographic complexities and\noperational environments. Detailed entropy-based evaluations revealed its\nsensitivity to subtle deviations in encryption workflows, offering a robust\nalternative to traditional detection methods reliant on static signatures or\nheuristics. Computational benchmarks confirmed its scalability and efficiency,\nachieving consistent performance even under high data loads and complex\ncryptographic scenarios. The inclusion of real-time clustering and anomaly\nevaluation ensures rapid response capabilities, addressing critical latency\nchallenges in ransomware detection. Performance comparisons with established\nmethods highlighted its improvements in detection efficacy, particularly\nagainst advanced ransomware employing extended key lengths and unique\ncryptographic protocols.",
      "tldr_zh": "本文提出了一种基于Probabilistic Cryptographic Footprints的层次化模式解密方法，用于检测加密型勒索软件，通过分析加密模式的统计特性来识别异常。该方法结合高级Clustering Algorithms和Machine Learning，构建了分层框架，并融入动态反馈机制，以适应不同加密复杂性和环境。实验结果显示，该框架在多种勒索软件家族上实现了高准确率和低假阳性率，并在Entropy-based Evaluations中表现出对细微偏差的敏感性。相比传统基于静态Signatures或Heuristics的方法，该系统在可扩展性、实时响应和对高级勒索软件的检测效能上均有显著改进。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15084v1",
      "published_date": "2025-01-25 05:26:17 UTC",
      "updated_date": "2025-01-25 05:26:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:17:30.802690"
    },
    {
      "arxiv_id": "2501.15081v1",
      "title": "Can Large Language Models Be Trusted as Black-Box Evolutionary Optimizers for Combinatorial Problems?",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Zhao",
        "Tao Wen",
        "Kang Hao Cheong"
      ],
      "abstract": "Evolutionary computation excels in complex optimization but demands deep\ndomain knowledge, restricting its accessibility. Large Language Models (LLMs)\noffer a game-changing solution with their extensive knowledge and could\ndemocratize the optimization paradigm. Although LLMs possess significant\ncapabilities, they may not be universally effective, particularly since\nevolutionary optimization encompasses multiple stages. It is therefore\nimperative to evaluate the suitability of LLMs as evolutionary optimizer (EVO).\nThus, we establish a series of rigid standards to thoroughly examine the\nfidelity of LLM-based EVO output in different stages of evolutionary\noptimization and then introduce a robust error-correction mechanism to mitigate\nthe output uncertainty. Furthermore, we explore a cost-efficient method that\ndirectly operates on entire populations with excellent effectiveness in\ncontrast to individual-level optimization. Through extensive experiments, we\nrigorously validate the performance of LLMs as operators targeted for\ncombinatorial problems. Our findings provide critical insights and valuable\nobservations, advancing the understanding and application of LLM-based\noptimization.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 是否能可靠地作为黑箱进化优化器 (EVO) 用于组合问题，强调 LLMs 的广泛知识可能使优化更易访问，但需评估其在进化优化各阶段的有效性。研究者建立了严格的标准来检验 LLMs 输出的一致性，并引入了稳健的错误修正机制以减少不确定性，同时探索了一种成本有效的种群级优化方法，与个体级优化相比更高效。通过广泛实验，论文验证了 LLMs 在组合问题中的性能，并提供了关键洞见，促进了 LLM 基于优化技术的理解和应用。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15081v1",
      "published_date": "2025-01-25 05:19:19 UTC",
      "updated_date": "2025-01-25 05:19:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:17:44.350519"
    },
    {
      "arxiv_id": "2501.15074v1",
      "title": "PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures",
      "title_zh": "PatentLMM：用于生成",
      "authors": [
        "Shreya Shukla",
        "Nakul Sharma",
        "Manish Gupta",
        "Anand Mishra"
      ],
      "abstract": "Writing comprehensive and accurate descriptions of technical drawings in\npatent documents is crucial to effective knowledge sharing and enabling the\nreplication and protection of intellectual property. However, automation of\nthis task has been largely overlooked by the research community. To this end,\nwe introduce PatentDesc-355K, a novel large-scale dataset containing ~355K\npatent figures along with their brief and detailed textual descriptions\nextracted from more than 60K US patent documents. In addition, we propose\nPatentLMM - a novel multimodal large language model specifically tailored to\ngenerate high-quality descriptions of patent figures. Our proposed PatentLMM\ncomprises two key components: (i) PatentMME, a specialized multimodal vision\nencoder that captures the unique structural elements of patent figures, and\n(ii) PatentLLaMA, a domain-adapted version of LLaMA fine-tuned on a large\ncollection of patents. Extensive experiments demonstrate that training a vision\nencoder specifically designed for patent figures significantly boosts the\nperformance, generating coherent descriptions compared to fine-tuning\nsimilar-sized off-the-shelf multimodal models. PatentDesc-355K and PatentLMM\npave the way for automating the understanding of patent figures, enabling\nefficient knowledge sharing and faster drafting of patent documents. We make\nthe code and data publicly available.",
      "tldr_zh": "这篇论文介绍了PatentDesc-355K数据集，该数据集包含约355K个专利图及其简要和详细描述，从超过60K份美国专利文档中提取，旨在支持专利图描述的自动化生成。同时，提出了PatentLMM模型，该模型包括专门的多模态视觉编码器PatentMME（用于捕捉专利图的独特结构）和领域适应版本的PatentLLaMA（基于LLaMA在专利数据上微调）。实验结果显示，PatentLMM显著提升了描述生成性能，与微调现成多模态模型相比，产出更连贯且高质量的描述。该工作为自动化专利图理解铺平道路，促进知识共享和专利起草效率，并公开了代码和数据。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at AAAI 2025 (Main Track). Project page:\n  https://vl2g.github.io/projects/PatentLMM/",
      "pdf_url": "http://arxiv.org/pdf/2501.15074v1",
      "published_date": "2025-01-25 04:45:32 UTC",
      "updated_date": "2025-01-25 04:45:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:17:56.067867"
    },
    {
      "arxiv_id": "2502.15714v1",
      "title": "TrustDataFilter:Leveraging Trusted Knowledge Base Data for More Effective Filtering of Unknown Information",
      "title_zh": "TrustDataFilter：",
      "authors": [
        "Jinghong Zhang",
        "Yidong Cui",
        "Weiling Wang",
        "Xianyou Cheng"
      ],
      "abstract": "With the advancement of technology and changes in the market, the demand for\nthe construction of domain-specific knowledge bases has been increasing, either\nto improve model performance or to promote enterprise innovation and\ncompetitiveness. The construction of domain-specific knowledge bases typically\nrelies on web crawlers or existing industry databases, leading to problems with\naccuracy and consistency of the data. To address these challenges, we\nconsidered the characteristics of domain data, where internal knowledge is\ninterconnected, and proposed the Self-Natural Language Inference Data Filtering\n(self-nli-TDF) framework. This framework compares trusted filtered knowledge\nwith the data to be filtered, deducing the reasoning relationship between them,\nthus improving filtering performance. The framework uses plug-and-play large\nlanguage models for trustworthiness assessment and employs the RoBERTa-MNLI\nmodel from the NLI domain for reasoning. We constructed three datasets in the\ndomains of biology, radiation, and science, and conducted experiments using\nRoBERTa, GPT3.5, and the local Qwen2 model. The experimental results show that\nthis framework improves filter quality, producing more consistent and reliable\nfiltering results.",
      "tldr_zh": "这篇论文提出 TrustDataFilter 框架，利用 Trusted Knowledge Base Data 来提升未知信息的过滤效果，针对领域特定知识库建设中数据准确性和一致性问题。框架的核心是 Self-Natural Language Inference Data Filtering (self-nli-TDF)，通过比较可信数据与待过滤数据，使用 plug-and-play 大语言模型进行可信度评估，并结合 RoBERTa-MNLI 模型进行推理关系推断。实验在生物学、辐射和科学领域的三个数据集上使用 RoBERTa、GPT3.5 和 Qwen2 模型进行测试，结果显示过滤质量显著提高，输出更一致和可靠。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 8 figures, submitted to IEEE Transactions on Knowledge and\n  Data Engineering",
      "pdf_url": "http://arxiv.org/pdf/2502.15714v1",
      "published_date": "2025-01-25 04:18:35 UTC",
      "updated_date": "2025-01-25 04:18:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:18:07.546721"
    },
    {
      "arxiv_id": "2501.15065v1",
      "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
      "title_zh": "信任区域中的任务算术：一种无训练模型合并方法以导航知识冲突",
      "authors": [
        "Wenju Sun",
        "Qingyong Li",
        "Wen Wang",
        "Yangli-ao Geng",
        "Boyang Li"
      ],
      "abstract": "Multi-task model merging offers an efficient solution for integrating\nknowledge from multiple fine-tuned models, mitigating the significant\ncomputational and storage demands associated with multi-task training. As a key\ntechnique in this field, Task Arithmetic (TA) defines task vectors by\nsubtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned\ntask models in parameter space, then adjusting the weight between these task\nvectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific\nknowledge. Despite the promising performance of TA, conflicts can arise among\nthe task vectors, particularly when different tasks require distinct model\nadaptations. In this paper, we formally define this issue as knowledge\nconflicts, characterized by the performance degradation of one task after\nmerging with a model fine-tuned for another task. Through in-depth analysis, we\nshow that these conflicts stem primarily from the components of task vectors\nthat align with the gradient of task-specific losses at $\\theta_{\\text{pre}}$.\nTo address this, we propose Task Arithmetic in Trust Region (TATR), which\ndefines the trust region as dimensions in the model parameter space that cause\nonly small changes (corresponding to the task vector components with gradient\northogonal direction) in the task-specific losses. Restricting parameter\nmerging within this trust region, TATR can effectively alleviate knowledge\nconflicts. Moreover, TATR serves as both an independent approach and a\nplug-and-play module compatible with a wide range of TA-based methods.\nExtensive empirical evaluations on eight distinct datasets robustly demonstrate\nthat TATR improves the multi-task performance of several TA-based model merging\nmethods by an observable margin.",
      "tldr_zh": "这篇论文针对多任务模型合并中任务向量间的知识冲突问题，提出了一种无需训练的Task Arithmetic in Trust Region (TATR)方法，以缓解合并后任务性能下降的问题。TATR 通过定义信任区域（即模型参数空间中与任务特定损失梯度正交的方向），将参数合并限制在该区域内，从而有效平衡任务通用和任务特定知识。实验结果显示，在八个数据集上，TATR 显著提升了多种Task Arithmetic (TA)-based 方法的多任务性能，证明了其作为独立或兼容模块的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21pages, 6 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.15065v1",
      "published_date": "2025-01-25 04:09:56 UTC",
      "updated_date": "2025-01-25 04:09:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:18:19.797241"
    },
    {
      "arxiv_id": "2501.17181v1",
      "title": "An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Arya Rahgozar",
        "Pouria Mortezaagha",
        "Jodi Edwards",
        "Douglas Manuel",
        "Jessie McGowen",
        "Merrick Zwarenstein",
        "Dean Fergusson",
        "Andrea Tricco",
        "Kelly Cobey",
        "Margaret Sampson",
        "Malcolm King",
        "Dawn Richards",
        "Alexandra Bodnaruc",
        "David Moher"
      ],
      "abstract": "The Brain-Heart Interconnectome (BHI) combines neurology and cardiology but\nis hindered by inefficiencies in evidence synthesis, poor adherence to quality\nstandards, and research waste. To address these challenges, we developed an\nAI-driven system to enhance systematic reviews in the BHI domain. The system\nintegrates automated detection of Population, Intervention, Comparator,\nOutcome, and Study design (PICOS), semantic search using vector embeddings,\ngraph-based querying, and topic modeling to identify redundancies and\nunderexplored areas. Core components include a Bi-LSTM model achieving 87%\naccuracy for PICOS compliance, a study design classifier with 95.7% accuracy,\nand Retrieval-Augmented Generation (RAG) with GPT-3.5, which outperformed GPT-4\nfor graph-based and topic-driven queries. The system provides real-time\nupdates, reducing research waste through a living database and offering an\ninteractive interface with dashboards and conversational AI. While initially\ndeveloped for BHI, the system's adaptable architecture enables its application\nacross various biomedical fields, supporting rigorous evidence synthesis,\nefficient resource allocation, and informed clinical decision-making.",
      "tldr_zh": "该论文针对 Brain-Heart Interconnectome (BHI) 领域证据合成的低效问题，如质量标准遵守差和研究浪费，开发了一个 AI 驱动的实时系统性综述系统。系统整合了 PICOS（Population, Intervention, Comparator, Outcome, and Study design）的自动化检测、语义搜索（使用向量嵌入）、图-based 查询和主题建模，以识别冗余和未探索区域，并采用 Bi-LSTM 模型（87% 准确率）和 Retrieval-Augmented Generation (RAG) 与 GPT-3.5（优于 GPT-4）。实验结果显示，该系统通过活数据库和交互式界面提供实时更新，显著减少研究浪费，并可扩展至其他生物医学领域，支持严格证据合成和临床决策。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17181v1",
      "published_date": "2025-01-25 03:51:07 UTC",
      "updated_date": "2025-01-25 03:51:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:18:32.670839"
    },
    {
      "arxiv_id": "2501.15061v2",
      "title": "PolaFormer: Polarity-aware Linear Attention for Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Weikang Meng",
        "Yadan Luo",
        "Xin Li",
        "Dongmei Jiang",
        "Zheng Zhang"
      ],
      "abstract": "Linear attention has emerged as a promising alternative to softmax-based\nattention, leveraging kernelized feature maps to reduce complexity from\nquadratic to linear in sequence length. However, the non-negative constraint on\nfeature maps and the relaxed exponential function used in approximation lead to\nsignificant information loss compared to the original query-key dot products,\nresulting in less discriminative attention maps with higher entropy. To address\nthe missing interactions driven by negative values in query-key pairs, we\npropose a polarity-aware linear attention mechanism that explicitly models both\nsame-signed and opposite-signed query-key interactions, ensuring comprehensive\ncoverage of relational information. Furthermore, to restore the spiky\nproperties of attention maps, we provide a theoretical analysis proving the\nexistence of a class of element-wise functions (with positive first and second\nderivatives) that can reduce entropy in the attention distribution. For\nsimplicity, and recognizing the distinct contributions of each dimension, we\nemploy a learnable power function for rescaling, allowing strong and weak\nattention signals to be effectively separated. Extensive experiments\ndemonstrate that the proposed PolaFormer improves performance on various vision\ntasks, enhancing both expressiveness and efficiency by up to 4.6%.",
      "tldr_zh": "本文提出 PolaFormer，一种 polarity-aware linear attention 机制，用于 Vision Transformers，以解决传统线性注意力中因非负约束和近似函数导致的信息损失问题。该机制通过显式建模同符号和异符号的 query-key 交互，并采用 learnable power function 进行 rescaling，降低注意力分布的熵，从而提升注意力的区分性和表达性。实验结果显示，PolaFormer 在各种视觉任务上提高了性能和效率，最多提升 4.6%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15061v2",
      "published_date": "2025-01-25 03:46:35 UTC",
      "updated_date": "2025-03-04 07:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:18:42.972867"
    },
    {
      "arxiv_id": "2501.15056v1",
      "title": "Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations",
      "title_zh": "反馈感知的 Monte Carlo Tree Search 用于目标导向对话中的高效信息搜索",
      "authors": [
        "Harshita Chopra",
        "Chirag Shah"
      ],
      "abstract": "The ability to identify and acquire missing information is a critical\ncomponent of effective decision making and problem solving. With the rise of\nconversational artificial intelligence (AI) systems, strategically formulating\ninformation-seeking questions becomes crucial and demands efficient methods to\nguide the search process. We introduce a novel approach to adaptive\nquestion-asking through a combination of Large Language Models (LLM) for\ngenerating questions that maximize information gain, Monte Carlo Tree Search\n(MCTS) for constructing and leveraging a decision tree across multiple samples,\nand a hierarchical feedback mechanism to learn from past interactions. We\npresent two key innovations: (1) an adaptive MCTS algorithm that balances\nexploration and exploitation for efficient search over potential questions; and\n(2) a clustering-based feedback algorithm that leverages prior experience to\nguide future interactions. Each incoming sample is assigned to a cluster based\non its semantic similarity with previously observed samples. Our UCT (Upper\nConfidence bound for Trees) formulation selects optimal questions by combining\nexpected rewards, a function of information gain, with a cluster-specific bonus\nthat decays with depth, to emphasize the importance of early-stage questions\nthat have proven effective for narrowing the solution space in similar samples.\nExperiments across three domains, including medical diagnosis and\ntroubleshooting, demonstrate that our method leads to an average of 12%\nimprovement in success rates and a 10x reduction in the average number of LLM\ncalls made per conversation for the search process, in comparison to the state\nof the art.",
      "tldr_zh": "该论文提出了一种基于反馈感知的 Monte Carlo Tree Search (MCTS) 方法，用于提升目标导向对话中的信息获取效率。该方法结合 Large Language Models (LLM) 生成最大化信息增益的问题，并通过自适应 MCTS 算法平衡探索和利用来构建决策树，同时引入分层反馈机制和基于聚类的反馈算法，以从过去互动中学习和指导未来查询。关键创新包括 UCT (Upper Confidence bound for Trees) 公式，该公式整合预期奖励、信息增益和聚类特定奖励，以优先选择早期有效问题。实验在医疗诊断和故障排除等三个领域验证了该方法的有效性，成功率平均提高 12%，并将 LLM 调用次数减少 10 倍。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15056v1",
      "published_date": "2025-01-25 03:42:22 UTC",
      "updated_date": "2025-01-25 03:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:18:54.896883"
    },
    {
      "arxiv_id": "2501.15055v1",
      "title": "Group Ligands Docking to Protein Pockets",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqi Guan",
        "Jiahan Li",
        "Xiangxin Zhou",
        "Xingang Peng",
        "Sheng Wang",
        "Yunan Luo",
        "Jian Peng",
        "Jianzhu Ma"
      ],
      "abstract": "Molecular docking is a key task in computational biology that has attracted\nincreasing interest from the machine learning community. While existing methods\nhave achieved success, they generally treat each protein-ligand pair in\nisolation. Inspired by the biochemical observation that ligands binding to the\nsame target protein tend to adopt similar poses, we propose \\textsc{GroupBind},\na novel molecular docking framework that simultaneously considers multiple\nligands docking to a protein. This is achieved by introducing an interaction\nlayer for the group of ligands and a triangle attention module for embedding\nprotein-ligand and group-ligand pairs. By integrating our approach with\ndiffusion-based docking model, we set a new S performance on the PDBBind blind\ndocking benchmark, demonstrating the effectiveness of our proposed molecular\ndocking paradigm.",
      "tldr_zh": "该论文提出 \\textsc{GroupBind}，一个新型分子对接框架，旨在同时处理多个配体对接到同一蛋白口袋中，基于配体倾向于采用类似姿势的生化观察。框架引入交互层和三角注意力模块，用于嵌入蛋白-配体和组-配体对，从而考虑配体间的相互作用。与扩散-based docking model 整合后，\\textsc{GroupBind} 在 PDBBind 盲对接基准上实现了新的性能记录。整体方法证明了分组对接范式的有效性，提升了计算生物学中的分子对接任务。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "18 pages, published in ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15055v1",
      "published_date": "2025-01-25 03:36:17 UTC",
      "updated_date": "2025-01-25 03:36:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:19:07.560825"
    },
    {
      "arxiv_id": "2501.15054v1",
      "title": "An Attempt to Unraveling Token Prediction Refinement and Identifying Essential Layers of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jaturong Kongmanee"
      ],
      "abstract": "This research aims to unravel how large language models (LLMs) iteratively\nrefine token predictions (or, in a general sense, vector predictions). We\nutilized a logit lens technique to analyze the model's token predictions\nderived from intermediate representations. Specifically, we focused on how LLMs\naccess and use information from input contexts, and how positioning of relevant\ninformation affects the model's token prediction refinement process. Our\nfindings for multi-document question answering task, by varying input context\nlengths (the number of documents), using GPT-2, revealed that the number of\nlayers between the first layer that the model predicted next tokens correctly\nand the later layers that the model finalized its correct predictions, as a\nfunction of the position of relevant information (i.e., placing the relevant\none at the beginning, middle, or end of the input context), has a nearly\ninverted U shape. We found that the gap between these two layers, on average,\ndiminishes when relevant information is positioned at the beginning or end of\nthe input context, suggesting that the model requires more refinements when\nprocessing longer contexts with relevant information situated in the middle,\nand highlighting which layers are essential for determining the correct output.\nOur analysis provides insights about how token predictions are distributed\nacross different conditions, and establishes important connections to existing\nhypotheses and previous findings in AI safety research and development.",
      "tldr_zh": "本研究旨在揭示大型语言模型 (LLMs) 如何迭代精炼 token 预测，并识别其关键层。通过 logit lens 技术分析中间表示，该团队专注于模型对输入上下文信息的访问和使用，并考察相关信息位置（开头、中间或结尾）对预测精炼过程的影响。实验使用 GPT-2 在多文档问答任务中变幻输入上下文长度，结果显示层间差距呈近似倒 U 形，即当相关信息位于中间时，模型需要更多精炼，而置于开头或结尾时精炼减少，从而突显了特定层在确定正确输出中的重要性。该分析为 AI 安全研究提供了宝贵洞见，并与现有假设建立了联系。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15054v1",
      "published_date": "2025-01-25 03:34:15 UTC",
      "updated_date": "2025-01-25 03:34:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:19:19.158713"
    },
    {
      "arxiv_id": "2501.15053v1",
      "title": "Exploring the impact of Optimised Hyperparameters on Bi-LSTM-based Contextual Anomaly Detector",
      "title_zh": "探索优化超参数对基于 Bi-LSTM 的上下文异常检测器的影响",
      "authors": [
        "Aafan Ahmad Toor",
        "Jia-Chun Lin",
        "Ernst Gunnar Gran"
      ],
      "abstract": "The exponential growth in the usage of Internet of Things in daily life has\ncaused immense increase in the generation of time series data. Smart homes is\none such domain where bulk of data is being generated and anomaly detection is\none of the many challenges addressed by researchers in recent years. Contextual\nanomaly is a kind of anomaly that may show deviation from the normal pattern\nlike point or sequence anomalies, but it also requires prior knowledge about\nthe data domain and the actions that caused the deviation. Recent studies based\non Recurrent Neural Networks (RNN) have demonstrated strong performance in\nanomaly detection. This study explores the impact of automatically tuned\nhyperparamteres on Unsupervised Online Contextual Anomaly Detection (UoCAD)\napproach by proposing UoCAD with Optimised Hyperparamnters (UoCAD-OH). UoCAD-OH\nconducts hyperparameter optimisation on Bi-LSTM model in an offline phase and\nuses the fine-tuned hyperparameters to detect anomalies during the online\nphase. The experiments involve evaluating the proposed framework on two smart\nhome air quality datasets containing contextual anomalies. The evaluation\nmetrics used are Precision, Recall, and F1 score.",
      "tldr_zh": "本研究探讨了优化超参数对基于 Bi-LSTM 的上下文异常检测器的影响，针对物联网时代时间序列数据激增所带来的智能家居异常检测挑战。作者提出 UoCAD-OH 方法，通过在离线阶段对 Bi-LSTM 模型进行自动超参数优化，并在在线阶段应用这些优化参数来检测上下文异常。实验在两个智能家居空气质量数据集上进行，使用 Precision、Recall 和 F1 score 作为评估指标，展示了该方法在提升检测性能方面的潜在优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2501.15053v1",
      "published_date": "2025-01-25 03:26:22 UTC",
      "updated_date": "2025-01-25 03:26:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:19:30.737086"
    },
    {
      "arxiv_id": "2501.15052v1",
      "title": "Graph-Based Cross-Domain Knowledge Distillation for Cross-Dataset Text-to-Image Person Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Bingjun Luo",
        "Jinpeng Wang",
        "Wang Zewen",
        "Junjie Zhu",
        "Xibin Zhao"
      ],
      "abstract": "Video surveillance systems are crucial components for ensuring public safety\nand management in smart city. As a fundamental task in video surveillance,\ntext-to-image person retrieval aims to retrieve the target person from an image\ngallery that best matches the given text description. Most existing\ntext-to-image person retrieval methods are trained in a supervised manner that\nrequires sufficient labeled data in the target domain. However, it is common in\npractice that only unlabeled data is available in the target domain due to the\ndifficulty and cost of data annotation, which limits the generalization of\nexisting methods in practical application scenarios. To address this issue, we\npropose a novel unsupervised domain adaptation method, termed Graph-Based\nCross-Domain Knowledge Distillation (GCKD), to learn the cross-modal feature\nrepresentation for text-to-image person retrieval in a cross-dataset scenario.\nThe proposed GCKD method consists of two main components. Firstly, a\ngraph-based multi-modal propagation module is designed to bridge the\ncross-domain correlation among the visual and textual samples. Secondly, a\ncontrastive momentum knowledge distillation module is proposed to learn the\ncross-modal feature representation using the online knowledge distillation\nstrategy. By jointly optimizing the two modules, the proposed method is able to\nachieve efficient performance for cross-dataset text-to-image person retrieval.\nacExtensive experiments on three publicly available text-to-image person\nretrieval datasets demonstrate the effectiveness of the proposed GCKD method,\nwhich consistently outperforms the state-of-the-art baselines.",
      "tldr_zh": "这篇论文提出了 Graph-Based Cross-Domain Knowledge Distillation (GCKD)，一种无监督域适应方法，用于解决跨数据集文本-to-image person retrieval 的问题，该任务在视频监控系统中至关重要，但现有方法因缺乏目标域标注数据而泛化性差。GCKD 包括两个关键组件：图-based multi-modal propagation 模块，用于桥接跨域视觉和文本样本的相关性；以及 contrastive momentum knowledge distillation 模块，通过在线知识蒸馏策略学习跨模态特征表示。实验结果显示，在三个公开数据集上，该方法显著优于最先进基线，证明了其在实际应用中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15052v1",
      "published_date": "2025-01-25 03:24:34 UTC",
      "updated_date": "2025-01-25 03:24:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:19:43.303723"
    },
    {
      "arxiv_id": "2501.15046v1",
      "title": "Evaluating Hallucination in Large Vision-Language Models based on Context-Aware Object Similarities",
      "title_zh": "基于上下文感知对象相似性评估大型视觉语言模型中的幻觉",
      "authors": [
        "Shounak Datta",
        "Dhanasekar Sundararaman"
      ],
      "abstract": "Despite their impressive performance on multi-modal tasks, large\nvision-language models (LVLMs) tend to suffer from hallucinations. An important\ntype is object hallucination, where LVLMs generate objects that are\ninconsistent with the images shown to the model. Existing works typically\nattempt to quantify object hallucinations by detecting and measuring the\nfraction of hallucinated objects in generated captions. Additionally, more\nrecent work also measures object hallucinations by directly querying the LVLM\nwith binary questions about the presence of likely hallucinated objects based\non object statistics like top-k frequent objects and top-k co-occurring\nobjects. In this paper, we present Context-Aware Object Similarities (CAOS), a\nnovel approach for evaluating object hallucination in LVLMs using object\nstatistics as well as the generated captions. CAOS uniquely integrates object\nstatistics with semantic relationships between objects in captions and\nground-truth data. Moreover, existing approaches usually only detect and\nmeasure hallucinations belonging to a predetermined set of in-domain objects\n(typically the set of all ground-truth objects for the training dataset) and\nignore generated objects that are not part of this set, leading to\nunder-evaluation. To address this, we further employ language model--based\nobject recognition to detect potentially out-of-domain hallucinated objects and\nuse an ensemble of LVLMs for verifying the presence of such objects in the\nquery image. CAOS also examines the sequential dynamics of object generation,\nshedding light on how the order of object appearance influences hallucinations,\nand employs word embedding models to analyze the semantic reasons behind\nhallucinations. CAOS aims to offer a nuanced understanding of the hallucination\ntendencies of LVLMs by providing a systematic framework to identify and\ninterpret object hallucinations.",
      "tldr_zh": "本论文评估了大型视觉语言模型 (LVLMs) 中的对象幻觉问题，提出了一种新方法 Context-Aware Object Similarities (CAOS)，通过整合对象统计、生成的标题以及对象间的语义关系，来更全面地检测和量化幻觉。CAOS 创新性地处理了现有方法的局限，如忽略域外对象，并使用语言模型进行对象识别，以及多 LVLMs 模型的集成验证，以检测图像中对象的存在。论文还分析了对象生成顺序和语义原因，提供了一个系统框架，帮助深入理解 LVLMs 的幻觉倾向和动态行为。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15046v1",
      "published_date": "2025-01-25 03:03:18 UTC",
      "updated_date": "2025-01-25 03:03:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:19:54.567760"
    },
    {
      "arxiv_id": "2501.15045v2",
      "title": "Towards Robust Unsupervised Attention Prediction in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Mengshi Qi",
        "Xiaoyang Bi",
        "Pengfei Zhu",
        "Huadong Ma"
      ],
      "abstract": "Robustly predicting attention regions of interest for self-driving systems is\ncrucial for driving safety but presents significant challenges due to the\nlabor-intensive nature of obtaining large-scale attention labels and the domain\ngap between self-driving scenarios and natural scenes. These challenges are\nfurther exacerbated by complex traffic environments, including camera\ncorruption under adverse weather, noise interferences, and central bias from\nlong-tail distributions. To address these issues, we propose a robust\nunsupervised attention prediction method. An Uncertainty Mining Branch refines\npredictions by analyzing commonalities and differences across multiple\npre-trained models on natural scenes, while a Knowledge Embedding Block bridges\nthe domain gap by incorporating driving knowledge to adaptively enhance\npseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation\nmethod that improves robustness against corruption through soft attention and\ndynamic augmentation, and mitigates central bias by integrating random cropping\ninto Mixup as a regularizer. To systematically evaluate robustness in\nself-driving attention prediction, we introduce the DriverAttention-C\nbenchmark, comprising over 100k frames across three subsets: BDD-A-C,\nDR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or\nsurpassing fully supervised state-of-the-art approaches on three public\ndatasets and the proposed robustness benchmark, reducing relative corruption\ndegradation by 58.8% and 52.8%, and improving central bias robustness by 12.4%\nand 11.4% in KLD and CC metrics, respectively. Code and data are available at\nhttps://github.com/zaplm/DriverAttention.",
      "tldr_zh": "这篇论文针对自动驾驶中的注意预测问题，提出了一种鲁棒的无监督方法，以应对数据标注困难、领域差距以及复杂环境如摄像头损坏、噪声干扰和中心偏差。该方法包括 Uncertainty Mining Branch 通过分析多个预训练模型的异同来改进预测、Knowledge Embedding Block 整合驾驶知识自适应增强伪标签，以及 RoboMixup 数据增强策略来提升对损坏的鲁棒性和缓解中心偏差。作者引入了 DriverAttention-C 基准数据集（包含超过10万帧），实验结果显示，该方法在三个公共数据集和鲁棒基准上性能达到或超过全监督SOTA水平，减少了相对损坏退化58.8%和52.8%，并在KLD和CC指标上提高了中心偏差鲁棒性12.4%和11.4%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15045v2",
      "published_date": "2025-01-25 03:01:26 UTC",
      "updated_date": "2025-01-29 03:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:20:08.678808"
    },
    {
      "arxiv_id": "2501.15038v1",
      "title": "Adaptive Client Selection in Federated Learning: A Network Anomaly Detection Use Case",
      "title_zh": "联邦学习中的自适应客户端选择：网络异常检测用例",
      "authors": [
        "William Marfo",
        "Deepak K. Tosh",
        "Shirley V. Moore"
      ],
      "abstract": "Federated Learning (FL) has become a widely used approach for training\nmachine learning models on decentralized data, addressing the significant\nprivacy concerns associated with traditional centralized methods. However, the\nefficiency of FL relies on effective client selection and robust privacy\npreservation mechanisms. Ineffective client selection can result in suboptimal\nmodel performance, while inadequate privacy measures risk exposing sensitive\ndata.\n  This paper introduces a client selection framework for FL that incorporates\ndifferential privacy and fault tolerance. The proposed adaptive approach\ndynamically adjusts the number of selected clients based on model performance\nand system constraints, ensuring privacy through the addition of calibrated\nnoise.\n  The method is evaluated on a network anomaly detection use case using the\nUNSW-NB15 and ROAD datasets. Results demonstrate up to a 7% improvement in\naccuracy and a 25% reduction in training time compared to the FedL2P approach.\nAdditionally, the study highlights trade-offs between privacy budgets and model\nperformance, with higher privacy budgets leading to reduced noise and improved\naccuracy. While the fault tolerance mechanism introduces a slight performance\ndecrease, it enhances robustness against client failures. Statistical\nvalidation using the Mann-Whitney U test confirms the significance of these\nimprovements, with results achieving a p-value of less than 0.05.",
      "tldr_zh": "本论文探讨了Federated Learning (FL)中自适应客户端选择的优化问题，旨在提升模型性能和隐私保护，尤其针对网络异常检测应用。提出的框架整合了差分隐私和容错机制，通过动态调整选定客户端的数量基于模型性能和系统约束，并添加校准噪声以确保数据隐私。该方法在UNSW-NB15和ROAD数据集上的实验显示，与FedL2P方法相比，准确率提升了7%、训练时间减少了25%，同时揭示了隐私预算与模型性能的权衡关系；尽管容错机制略微降低了性能，但增强了系统鲁棒性，并通过Mann-Whitney U测试（p < 0.05）验证了这些改进的显著性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15038v1",
      "published_date": "2025-01-25 02:50:46 UTC",
      "updated_date": "2025-01-25 02:50:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:20:19.317667"
    },
    {
      "arxiv_id": "2502.00036v1",
      "title": "Efficient Client Selection in Federated Learning",
      "title_zh": "联邦学习中的高效客户端选择",
      "authors": [
        "William Marfo",
        "Deepak K. Tosh",
        "Shirley V. Moore"
      ],
      "abstract": "Federated Learning (FL) enables decentralized machine learning while\npreserving data privacy. This paper proposes a novel client selection framework\nthat integrates differential privacy and fault tolerance. The adaptive client\nselection adjusts the number of clients based on performance and system\nconstraints, with noise added to protect privacy. Evaluated on the UNSW-NB15\nand ROAD datasets for network anomaly detection, the method improves accuracy\nby 7% and reduces training time by 25% compared to baselines. Fault tolerance\nenhances robustness with minimal performance trade-offs.",
      "tldr_zh": "这篇论文针对 Federated Learning (FL) 提出了一种高效的客户端选择框架，整合了 differential privacy 和 fault tolerance，以在保持数据隐私的同时优化训练过程。该框架通过自适应调整客户端数量，根据性能和系统约束添加噪声，提高了系统的灵活性和鲁棒性。在 UNSW-NB15 和 ROAD 数据集上进行网络异常检测实验时，与基线方法相比，准确率提高了 7%，训练时间减少了 25%。这项创新为隐私保护的去中心化机器学习提供了更可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00036v1",
      "published_date": "2025-01-25 02:43:55 UTC",
      "updated_date": "2025-01-25 02:43:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:20:31.502869"
    },
    {
      "arxiv_id": "2501.15034v1",
      "title": "Divergence-Augmented Policy Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Qing Wang",
        "Yingru Li",
        "Jiechao Xiong",
        "Tong Zhang"
      ],
      "abstract": "In deep reinforcement learning, policy optimization methods need to deal with\nissues such as function approximation and the reuse of off-policy data.\nStandard policy gradient methods do not handle off-policy data well, leading to\npremature convergence and instability. This paper introduces a method to\nstabilize policy optimization when off-policy data are reused. The idea is to\ninclude a Bregman divergence between the behavior policy that generates the\ndata and the current policy to ensure small and safe policy updates with\noff-policy data. The Bregman divergence is calculated between the state\ndistributions of two policies, instead of only on the action probabilities,\nleading to a divergence augmentation formulation. Empirical experiments on\nAtari games show that in the data-scarce scenario where the reuse of off-policy\ndata becomes necessary, our method can achieve better performance than other\nstate-of-the-art deep reinforcement learning algorithms.",
      "tldr_zh": "本研究针对深度强化学习中的政策优化问题，提出了一种Divergence-Augmented Policy Optimization方法，以处理函数逼近和离策略数据重用导致的不稳定性和过早收敛问题。该方法通过在行为策略和当前策略之间引入Bregman divergence，确保策略更新小而安全，并基于两个策略的状态分布计算散度，而不是仅限于动作概率，从而形成散度增强公式。在Atari游戏的实验中，该方法在数据稀缺场景下表现出色，比其他最先进算法实现更好的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada",
      "pdf_url": "http://arxiv.org/pdf/2501.15034v1",
      "published_date": "2025-01-25 02:35:46 UTC",
      "updated_date": "2025-01-25 02:35:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:20:42.617666"
    },
    {
      "arxiv_id": "2501.15030v2",
      "title": "OptiSeq: Ordering Examples On-The-Fly for In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Atul Bhope",
        "Praveen Venkateswaran",
        "K. R. Jayaram",
        "Vatche Isahagian",
        "Vinod Muthusamy",
        "Nalini Venkatasubramanian"
      ],
      "abstract": "Developers using LLMs and LLM-based agents in their applications have\nprovided plenty of anecdotal evidence that in-context-learning (ICL) is\nfragile. In this paper, we show that in addition to the quantity and quality of\nexamples, the order in which the in-context examples are listed in the prompt\naffects the output of the LLM and, consequently, their performance. While prior\nwork has explored improving ICL through dataset-dependent techniques, we\nintroduce OptiSeq, a purely inference-time, dataset-free optimization method\nthat efficiently determines the best example order. OptiSeq leverages log\nprobabilities of LLM-generated outputs to systematically prune the search space\nof possible orderings and recommend the best order(s) by distinguishing\norderings that yield high levels of accuracy and those that underperform.\nExtensive empirical evaluation on multiple LLMs, datasets, and prompts\ndemonstrate that OptiSeq improves accuracy by 5.5 - 10.5 percentage points\nacross multiple tasks.",
      "tldr_zh": "本研究发现，在上下文学习（ICL）中，除了示例的数量和质量，示例顺序也会显著影响大语言模型（LLMs）的输出性能。论文提出OptiSeq，一种纯推理时的优化方法，通过利用LLMs生成输出的log probabilities来高效修剪可能的顺序搜索空间，并推荐最佳顺序以区分高准确率和低准确率的排列。实验在多个LLMs、数据集和提示上显示，OptiSeq能将任务准确率提升5.5-10.5个百分点。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15030v2",
      "published_date": "2025-01-25 02:24:00 UTC",
      "updated_date": "2025-02-18 19:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:20:54.706772"
    },
    {
      "arxiv_id": "2501.15022v1",
      "title": "Using Large Language Models for education managements in Vietnamese with low resources",
      "title_zh": "翻译失败",
      "authors": [
        "Duc Do Minh",
        "Vinh Nguyen Van",
        "Thang Dam Cong"
      ],
      "abstract": "Large language models (LLMs), such as GPT-4, Gemini 1.5, Claude 3.5 Sonnet,\nand Llama3, have demonstrated significant advancements in various NLP tasks\nsince the release of ChatGPT in 2022. Despite their success, fine-tuning and\ndeploying LLMs remain computationally expensive, especially in\nresource-constrained environments. In this paper, we proposed VietEduFrame, a\nframework specifically designed to apply LLMs to educational management tasks\nin Vietnamese institutions. Our key contribution includes the development of a\ntailored dataset, derived from student education documents at Hanoi VNU, which\naddresses the unique challenges faced by educational systems with limited\nresources. Through extensive experiments, we show that our approach outperforms\nexisting methods in terms of accuracy and efficiency, offering a promising\nsolution for improving educational management in under-resourced environments.\nWhile our framework leverages synthetic data to supplement real-world examples,\nwe discuss potential limitations regarding broader applicability and robustness\nin future implementations.",
      "tldr_zh": "这篇论文探讨了在资源有限的越南环境中，使用 Large Language Models (LLMs) 如 GPT-4 和 Llama3 来优化教育管理任务。研究团队提出了 VietEduFrame 框架，并开发了一个从河内越南国家大学学生教育文件衍生而来的定制数据集，以解决教育系统面临的独特挑战。通过实验验证，该框架在准确性和效率上超过了现有方法，为资源不足的机构提供了一个改进教育管理的可行方案。尽管框架依赖合成数据补充真实示例，论文也指出了其在更广泛适用性和鲁棒性方面的潜在限制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages; 13 figures; 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.15022v1",
      "published_date": "2025-01-25 02:09:51 UTC",
      "updated_date": "2025-01-25 02:09:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:21:06.863755"
    },
    {
      "arxiv_id": "2502.14870v1",
      "title": "Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Severin Field"
      ],
      "abstract": "The development of artificial general intelligence (AGI) is likely to be one\nof humanity's most consequential technological advancements. Leading AI labs\nand scientists have called for the global prioritization of AI safety citing\nexistential risks comparable to nuclear war. However, research on catastrophic\nrisks and AI alignment is often met with skepticism, even by experts.\nFurthermore, online debate over the existential risk of AI has begun to turn\ntribal (e.g. name-calling such as \"doomer\" or \"accelerationist\"). Until now, no\nsystematic study has explored the patterns of belief and the levels of\nfamiliarity with AI safety concepts among experts. I surveyed 111 AI experts on\ntheir familiarity with AI safety concepts, key objections to AI safety, and\nreactions to safety arguments. My findings reveal that AI experts cluster into\ntwo viewpoints -- an \"AI as controllable tool\" and an \"AI as uncontrollable\nagent\" perspective -- diverging in beliefs toward the importance of AI safety.\nWhile most experts (78%) agreed or strongly agreed that \"technical AI\nresearchers should be concerned about catastrophic risks\", many were unfamiliar\nwith specific AI safety concepts. For example, only 21% of surveyed experts had\nheard of \"instrumental convergence,\" a fundamental concept in AI safety\npredicting that advanced AI systems will tend to pursue common sub-goals (such\nas self-preservation). The least concerned participants were the least familiar\nwith concepts like this, suggesting that effective communication of AI safety\nshould begin with establishing clear conceptual foundations in the field.",
      "tldr_zh": "这篇论文通过对 111 名 AI 专家的调查，探讨了他们对人工智能存在风险（existential risk）和 P(doom) 的分歧，揭示了专家观点的聚类：一派视 AI 为“可控工具”（AI as controllable tool），另一派视其为“不可控代理”（AI as uncontrollable agent）。调查发现，虽然 78% 的专家同意技术 AI 研究者应关注灾难性风险，但许多人对关键 AI 安全概念如“instrumental convergence”不熟悉，尤其是风险关切较低的群体。研究强调，有效沟通 AI 安全需要从建立清晰概念基础入手，以弥合专家间的认知差距。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "In submission to AI and Ethics Journal. 24 pages total, 15 pages of\n  writing with 9 pages of appendices",
      "pdf_url": "http://arxiv.org/pdf/2502.14870v1",
      "published_date": "2025-01-25 01:51:29 UTC",
      "updated_date": "2025-01-25 01:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:21:19.055953"
    },
    {
      "arxiv_id": "2501.16383v2",
      "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations",
      "title_zh": "翻译失败",
      "authors": [
        "Zunhai Su",
        "Zhe Chen",
        "Wang Shen",
        "Hanyu Wei",
        "Linge Li",
        "Huangqi Yu",
        "Kehong Yuan"
      ],
      "abstract": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
      "tldr_zh": "该论文提出RotateKV，一种准确且鲁棒的2-bit KV Cache量化方法，用于大型语言模型(LLMs)，通过Outlier-Aware Adaptive Rotations来解决内存瓶颈问题。RotateKV的关键创新包括：Outlier-Aware Rotation利用通道重排序适应异常值分布，同时保持fast Walsh-Hadamard transform (FWHT)的计算效率；Pre-RoPE Grouped-Head Rotation减轻旋转位置嵌入(RoPE)的干扰，并平滑异常值；Attention-Sink-Aware Quantization精确识别和保护注意力焦点。实验结果显示，在LLaMA-2-13B模型上，RotateKV实现2-bit量化时perplexity (PPL)下降不到0.3，并在GSM8K等任务上表现优于现有方法，内存使用减少3.97倍，支持更大批量大小，并实现2.32倍解码加速。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.16383v2",
      "published_date": "2025-01-25 01:45:29 UTC",
      "updated_date": "2025-02-02 03:04:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:23:09.753584"
    },
    {
      "arxiv_id": "2501.15014v2",
      "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Jacob Sander",
        "Achraf Cohen",
        "Venkat R. Dasari",
        "Brent Venable",
        "Brian Jalaian"
      ],
      "abstract": "Resource-constrained edge deployments demand AI solutions that balance high\nperformance with stringent compute, memory, and energy limitations. In this\nsurvey, we present a comprehensive overview of the primary strategies for\naccelerating deep learning models under such constraints. First, we examine\nmodel compression techniques-pruning, quantization, tensor decomposition, and\nknowledge distillation-that streamline large models into smaller, faster, and\nmore efficient variants. Next, we explore Neural Architecture Search (NAS), a\nclass of automated methods that discover architectures inherently optimized for\nparticular tasks and hardware budgets. We then discuss compiler and deployment\nframeworks, such as TVM, TensorRT, and OpenVINO, which provide\nhardware-tailored optimizations at inference time. By integrating these three\npillars into unified pipelines, practitioners can achieve multi-objective\ngoals, including latency reduction, memory savings, and energy efficiency-all\nwhile maintaining competitive accuracy. We also highlight emerging frontiers in\nhierarchical NAS, neurosymbolic approaches, and advanced distillation tailored\nto large language models, underscoring open challenges like pre-training\npruning for massive networks. Our survey offers practical insights, identifies\ncurrent research gaps, and outlines promising directions for building scalable,\nplatform-independent frameworks to accelerate deep learning models at the edge.",
      "tldr_zh": "这篇调查论文概述了在资源受限的边缘环境中加速深度学习模型的策略，重点包括模型压缩技术（如 pruning、quantization、tensor decomposition 和 knowledge distillation），以将大型模型优化为更高效的变体。论文还探讨了 Neural Architecture Search (NAS) 的自动化方法，以及编译器和部署框架（如 TVM、TensorRT 和 OpenVINO），这些工具可针对特定硬件进行推理优化，实现降低延迟、节省内存和提高能源效率，同时维持竞争性准确性。最后，它突出了新兴领域如 hierarchical NAS 和 neurosymbolic approaches 的潜力，并指出了研究空白，如针对大规模网络的 pre-training pruning，以及构建可扩展框架的未来方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 13 Figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15014v2",
      "published_date": "2025-01-25 01:37:03 UTC",
      "updated_date": "2025-01-28 20:29:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:21:43.452129"
    },
    {
      "arxiv_id": "2501.15007v1",
      "title": "Controllable Protein Sequence Generation with LLM Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangyu Liu",
        "Yi Liu",
        "Silei Chen",
        "Wei Hu"
      ],
      "abstract": "Designing proteins with specific attributes offers an important solution to\naddress biomedical challenges. Pre-trained protein large language models (LLMs)\nhave shown promising results on protein sequence generation. However, to\ncontrol sequence generation for specific attributes, existing work still\nexhibits poor functionality and structural stability. In this paper, we propose\na novel controllable protein design method called CtrlProt. We finetune a\nprotein LLM with a new multi-listwise preference optimization strategy to\nimprove generation quality and support multi-attribute controllable generation.\nExperiments demonstrate that CtrlProt can meet functionality and structural\nstability requirements effectively, achieving state-of-the-art performance in\nboth single-attribute and multi-attribute protein sequence generation.",
      "tldr_zh": "本研究针对蛋白质设计中特定属性的控制问题，提出了一种名为 CtrlProt 的新方法，该方法通过多列表优先优化策略微调蛋白质 LLM，以提升序列生成的质量并支持多属性可控生成。相比现有方法，CtrlProt 显著改善了功能性和结构稳定性。实验结果表明，该方法在单属性和多属性蛋白质序列生成任务上达到了最先进性能。",
      "categories": [
        "cs.AI",
        "cs.CE",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.15007v1",
      "published_date": "2025-01-25 00:59:12 UTC",
      "updated_date": "2025-01-25 00:59:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:21:54.560475"
    },
    {
      "arxiv_id": "2501.15001v2",
      "title": "What if Eye...? Computationally Recreating Vision Evolution",
      "title_zh": "翻译失败",
      "authors": [
        "Kushagra Tiwary",
        "Aaron Young",
        "Zaid Tasneem",
        "Tzofi Klinghoffer",
        "Akshat Dave",
        "Tomaso Poggio",
        "Dan-Eric Nilsson",
        "Brian Cheung",
        "Ramesh Raskar"
      ],
      "abstract": "Vision systems in nature show remarkable diversity, from simple\nlight-sensitive patches to complex camera eyes with lenses. While natural\nselection has produced these eyes through countless mutations over millions of\nyears, they represent just one set of realized evolutionary paths. Testing\nhypotheses about how environmental pressures shaped eye evolution remains\nchallenging since we cannot experimentally isolate individual factors.\nComputational evolution offers a way to systematically explore alternative\ntrajectories. Here we show how environmental demands drive three fundamental\naspects of visual evolution through an artificial evolution framework that\nco-evolves both physical eye structure and neural processing in embodied\nagents. First, we demonstrate computational evidence that task specific\nselection drives bifurcation in eye evolution - orientation tasks like\nnavigation in a maze leads to distributed compound-type eyes while an object\ndiscrimination task leads to the emergence of high-acuity camera-type eyes.\nSecond, we reveal how optical innovations like lenses naturally emerge to\nresolve fundamental tradeoffs between light collection and spatial precision.\nThird, we uncover systematic scaling laws between visual acuity and neural\nprocessing, showing how task complexity drives coordinated evolution of sensory\nand computational capabilities. Our work introduces a novel paradigm that\nilluminates evolutionary principles shaping vision by creating targeted\nsingle-player games where embodied agents must simultaneously evolve visual\nsystems and learn complex behaviors. Through our unified genetic encoding\nframework, these embodied agents serve as next-generation hypothesis testing\nmachines while providing a foundation for designing manufacturable bio-inspired\nvision systems. Website: http://eyes.mit.edu/",
      "tldr_zh": "本研究通过人工进化框架，模拟视觉进化的过程，探索环境需求如何驱动眼睛结构和神经处理的共同进化。研究发现，任务特定选择导致眼睛进化分叉：如导航迷宫等定向任务促进分布式 compound-type eyes，而物体辨识任务则导致高清晰度 camera-type eyes 的出现。同时，光学创新如 lenses 自然演化以解决光收集和空间精度之间的权衡，并揭示了视觉敏锐度和神经处理之间的系统 scaling laws。该框架引入统一遗传编码系统，让具身代理在单人游戏中同时进化视觉系统和行为，提供了一种新型假设测试工具，并为设计可制造的生物启发视觉系统奠定基础。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.NE",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "Website: http://eyes.mit.edu/",
      "pdf_url": "http://arxiv.org/pdf/2501.15001v2",
      "published_date": "2025-01-25 00:29:24 UTC",
      "updated_date": "2025-02-13 04:15:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:22:08.189724"
    },
    {
      "arxiv_id": "2501.14994v1",
      "title": "Robust Cross-Etiology and Speaker-Independent Dysarthric Speech Recognition",
      "title_zh": "鲁棒的跨病因和说话者独立的构音障碍语音识别",
      "authors": [
        "Satwinder Singh",
        "Qianli Wang",
        "Zihan Zhong",
        "Clarion Mendes",
        "Mark Hasegawa-Johnson",
        "Waleed Abdulla",
        "Seyed Reza Shahamiri"
      ],
      "abstract": "In this paper, we present a speaker-independent dysarthric speech recognition\nsystem, with a focus on evaluating the recently released Speech Accessibility\nProject (SAP-1005) dataset, which includes speech data from individuals with\nParkinson's disease (PD). Despite the growing body of research in dysarthric\nspeech recognition, many existing systems are speaker-dependent and adaptive,\nlimiting their generalizability across different speakers and etiologies. Our\nprimary objective is to develop a robust speaker-independent model capable of\naccurately recognizing dysarthric speech, irrespective of the speaker.\nAdditionally, as a secondary objective, we aim to test the cross-etiology\nperformance of our model by evaluating it on the TORGO dataset, which contains\nspeech samples from individuals with cerebral palsy (CP) and amyotrophic\nlateral sclerosis (ALS). By leveraging the Whisper model, our\nspeaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the\nSAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of\n25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the\npotential of our approach to generalize across unseen speakers and different\netiologies of dysarthria.",
      "tldr_zh": "本研究开发了一个鲁棒的说话者独立失语症语音识别系统，针对不同病因的失语症（如帕金森病 (PD)）进行评估，使用 Speech Accessibility Project (SAP-1005) 数据集作为主要基准。系统的主要目标是构建一个不受说话者影响的模型，以提高失语症语音的识别准确性，同时次要目标测试其跨病因泛化性能，在 TORGO 数据集（包括脑瘫 (CP) 和肌萎缩侧索硬化 (ALS) 患者）上进行验证。基于 Whisper 模型，该系统在 SAP-1005 数据集上实现了字符错误率 (CER) 6.99% 和单词错误率 (WER) 10.71%。在跨病因测试中，系统在 TORGO 数据集上达到了 CER 25.08% 和 WER 39.56%，证明了其在未见说话者和不同病因下的良好泛化潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.14994v1",
      "published_date": "2025-01-25 00:02:58 UTC",
      "updated_date": "2025-01-25 00:02:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:22:19.290046"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 55,
  "processed_papers_count": 55,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T03:23:24.332411"
}
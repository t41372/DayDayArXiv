[
  {
    "arxiv_id": "2412.14409v1",
    "title": "Multi-task Representation Learning for Mixed Integer Linear Programming",
    "authors": [
      "Junyang Cai",
      "Taoan Huang",
      "Bistra Dilkina"
    ],
    "abstract": "Mixed Integer Linear Programs (MILPs) are highly flexible and powerful tools\nfor modeling and solving complex real-world combinatorial optimization\nproblems. Recently, machine learning (ML)-guided approaches have demonstrated\nsignificant potential in improving MILP-solving efficiency. However, these\nmethods typically rely on separate offline data collection and training\nprocesses, which limits their scalability and adaptability. This paper\nintroduces the first multi-task learning framework for ML-guided MILP solving.\nThe proposed framework provides MILP embeddings helpful in guiding MILP solving\nacross solvers (e.g., Gurobi and SCIP) and across tasks (e.g., Branching and\nSolver configuration). Through extensive experiments on three widely used MILP\nbenchmarks, we demonstrate that our multi-task learning model performs\nsimilarly to specialized models within the same distribution. Moreover, it\nsignificantly outperforms them in generalization across problem sizes and\ntasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14409v1",
    "published_date": "2024-12-18 23:33:32 UTC",
    "updated_date": "2024-12-18 23:33:32 UTC"
  },
  {
    "arxiv_id": "2412.14387v1",
    "title": "Clinical Trials Ontology Engineering with Large Language Models",
    "authors": [
      "Berkan Çakır"
    ],
    "abstract": "Managing clinical trial information is currently a significant challenge for\nthe medical industry, as traditional methods are both time-consuming and\ncostly. This paper proposes a simple yet effective methodology to extract and\nintegrate clinical trial data in a cost-effective and time-efficient manner.\nAllowing the medical industry to stay up-to-date with medical developments.\nComparing time, cost, and quality of the ontologies created by humans, GPT3.5,\nGPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM)\nare a viable option to automate this process both from a cost and time\nperspective. This study underscores significant implications for medical\nresearch where real-time data integration from clinical trials could become the\nnorm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14387v1",
    "published_date": "2024-12-18 22:40:52 UTC",
    "updated_date": "2024-12-18 22:40:52 UTC"
  },
  {
    "arxiv_id": "2412.14384v1",
    "title": "I0T: Embedding Standardization Method Towards Zero Modality Gap",
    "authors": [
      "Na Min An",
      "Eunki Kim",
      "James Thorne",
      "Hyunjung Shim"
    ],
    "abstract": "Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in\ndownstream tasks such as image-text retrieval and classification. However,\nrecent works extending CLIP suffer from the issue of modality gap, which arises\nwhen the image and text embeddings are projected to disparate manifolds,\ndeviating from the intended objective of image-text contrastive learning. We\ndiscover that this phenomenon is linked to the modality-specific characteristic\nthat each image/text encoder independently possesses and propose two methods to\naddress the modality gap: (1) a post-hoc embedding standardization method,\n$\\text{I0T}_{\\text{post}}$ that reduces the modality gap approximately to zero\nand (2) a trainable method, $\\text{I0T}_{\\text{async}}$, to alleviate the\nmodality gap problem by adding two normalization layers for each encoder. Our\nI0T framework can significantly reduce the modality gap while preserving the\noriginal embedding representations of trained models with their locked\nparameters. In practice, $\\text{I0T}_{\\text{post}}$ can serve as an alternative\nexplainable automatic evaluation metric of widely used CLIPScore (CLIP-S).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "16 figures, 8 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.14384v1",
    "published_date": "2024-12-18 22:35:01 UTC",
    "updated_date": "2024-12-18 22:35:01 UTC"
  },
  {
    "arxiv_id": "2412.14382v1",
    "title": "Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem",
    "authors": [
      "Junyang Cai",
      "Serdar Kadioglu",
      "Bistra Dilkina"
    ],
    "abstract": "Mixed-Integer Programming (MIP) is a powerful paradigm for modeling and\nsolving various important combinatorial optimization problems. Recently,\nlearning-based approaches have shown potential to speed up MIP solving via\noffline training that then guides important design decisions during search.\nHowever, a significant drawback of these methods is their heavy reliance on\noffline training, which requires collecting training datasets and\ncomputationally costly training epochs yet offering only limited generalization\nto unseen (larger) instances. In this paper, we propose Balans, an adaptive\nmeta-solver for MIPs with online learning capability that does not require any\nsupervision or apriori training. At its core, Balans is based on adaptive\nlarge-neighborhood search, operating on top of a MIP solver by successive\napplications of destroy and repair neighborhood operators. During the search,\nthe selection among different neighborhood definitions is guided on the fly for\nthe instance at hand via multi-armed bandit algorithms. Our extensive\nexperiments on hard optimization instances show that Balans offers significant\nperformance gains over the default MIP solver, is better than committing to any\nsingle best neighborhood, and improves over the state-of-the-art\nlarge-neighborhood search for MIPs. Finally, we release Balans as a highly\nconfigurable, MIP solver agnostic, open-source software.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14382v1",
    "published_date": "2024-12-18 22:32:13 UTC",
    "updated_date": "2024-12-18 22:32:13 UTC"
  },
  {
    "arxiv_id": "2412.14372v1",
    "title": "Python Agent in Ludii",
    "authors": [
      "Izaias S. de Lima Neto",
      "Marco A. A. de Aguiar Vieira",
      "Anderson R. Tavares"
    ],
    "abstract": "Ludii is a Java general game system with a considerable number of board\ngames, with an API for developing new agents and a game description language to\ncreate new games. To improve versatility and ease development, we provide\nPython interfaces for agent programming. This allows the use of Python modules\nto implement general game playing agents.\n  As a means of enabling Python for creating Ludii agents, the interfaces are\nimplemented using different Java libraries: jpy and Py4J. The main goal of this\nwork is to determine which version is faster. To do so, we conducted a\nperformance analysis of two different GGP algorithms, Minimax adapted to GGP\nand MCTS. The analysis was performed across several combinatorial games with\nvarying depth, branching factor, and ply time. For reproducibility, we provide\ntutorials and repositories.\n  Our analysis includes predictive models using regression, which suggest that\njpy is faster than Py4J, however slower than a native Java Ludii agent, as\nexpected.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14372v1",
    "published_date": "2024-12-18 22:12:52 UTC",
    "updated_date": "2024-12-18 22:12:52 UTC"
  },
  {
    "arxiv_id": "2412.14366v1",
    "title": "Surrealistic-like Image Generation with Vision-Language Models",
    "authors": [
      "Elif Ayten",
      "Shuai Wang",
      "Hjalmar Snoep"
    ],
    "abstract": "Recent advances in generative AI make it convenient to create different types\nof content, including text, images, and code. In this paper, we explore the\ngeneration of images in the style of paintings in the surrealism movement using\nvision-language generative models, including DALL-E, Deep Dream Generator, and\nDreamStudio. Our investigation starts with the generation of images under\nvarious image generation settings and different models. The primary objective\nis to identify the most suitable model and settings for producing such images.\nAdditionally, we aim to understand the impact of using edited base images on\nthe generated resulting images. Through these experiments, we evaluate the\nperformance of selected models and gain valuable insights into their\ncapabilities in generating such images. Our analysis shows that Dall-E 2\nperforms the best when using the generated prompt by ChatGPT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07, 68T01, 68T20",
      "J.5; I.2.0; I.4.7; I.4.9"
    ],
    "primary_category": "cs.CV",
    "comment": "2023 Joint international Scientific conferences on AI and Machine\n  Learning (BNAIC-BeNeLearn)",
    "pdf_url": "http://arxiv.org/pdf/2412.14366v1",
    "published_date": "2024-12-18 22:03:26 UTC",
    "updated_date": "2024-12-18 22:03:26 UTC"
  },
  {
    "arxiv_id": "2412.16228v1",
    "title": "TAACKIT: Track Annotation and Analytics with Continuous Knowledge Integration Tool",
    "authors": [
      "Lily Lee",
      "Julian Fontes",
      "Andrew Weinert",
      "Laura Schomacker",
      "Daniel Stabile",
      "Jonathan Hou"
    ],
    "abstract": "Machine learning (ML) is a powerful tool for efficiently analyzing data,\ndetecting patterns, and forecasting trends across various domains such as text,\naudio, and images. The availability of annotation tools to generate reliably\nannotated data is crucial for advances in ML applications. In the domain of\ngeospatial tracks, the lack of such tools to annotate and validate data impedes\nrapid and accessible ML application development. This paper presents Track\nAnnotation and Analytics with Continuous Knowledge Integration Tool (TAACKIT)\nto serve the critically important functions of annotating geospatial track data\nand validating ML models. We demonstrate an ML application use case in the air\ntraffic domain to illustrate its data annotation and model evaluation power and\nquantify the annotation effort reduction.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16228v1",
    "published_date": "2024-12-18 21:51:51 UTC",
    "updated_date": "2024-12-18 21:51:51 UTC"
  },
  {
    "arxiv_id": "2412.14355v1",
    "title": "Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference",
    "authors": [
      "Matthew Riemer",
      "Gopeshh Subbaraj",
      "Glen Berseth",
      "Irina Rish"
    ],
    "abstract": "Realtime environments change even as agents perform action inference and\nlearning, thus requiring high interaction frequencies to effectively minimize\nregret. However, recent advances in machine learning involve larger neural\nnetworks with longer inference times, raising questions about their\napplicability in realtime systems where reaction time is crucial. We present an\nanalysis of lower bounds on regret in realtime reinforcement learning (RL)\nenvironments to show that minimizing long-term regret is generally impossible\nwithin the typical sequential interaction and learning paradigm, but often\nbecomes possible when sufficient asynchronous compute is available. We propose\nnovel algorithms for staggering asynchronous inference processes to ensure that\nactions are taken at consistent time intervals, and demonstrate that use of\nmodels with high action inference times is only constrained by the\nenvironment's effective stochasticity over the inference horizon, and not by\naction frequency. Our analysis shows that the number of inference processes\nneeded scales linearly with increasing inference times while enabling use of\nmodels that are multiple orders of magnitude larger than existing approaches\nwhen learning from a realtime simulation of Game Boy games such as Pok\\'emon\nand Tetris.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14355v1",
    "published_date": "2024-12-18 21:43:40 UTC",
    "updated_date": "2024-12-18 21:43:40 UTC"
  },
  {
    "arxiv_id": "2412.14351v1",
    "title": "Is Peer-Reviewing Worth the Effort?",
    "authors": [
      "Kenneth Church",
      "Raman Chandrasekar",
      "John E. Ortega",
      "Ibrahim Said Ahmad"
    ],
    "abstract": "How effective is peer-reviewing in identifying important papers? We treat\nthis question as a forecasting task. Can we predict which papers will be highly\ncited in the future based on venue and \"early returns\" (citations soon after\npublication)? We show early returns are more predictive than venue. Finally, we\nend with constructive suggestions to address scaling challenges: (a) too many\nsubmissions and (b) too few qualified reviewers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.14351v1",
    "published_date": "2024-12-18 21:34:42 UTC",
    "updated_date": "2024-12-18 21:34:42 UTC"
  },
  {
    "arxiv_id": "2412.14340v3",
    "title": "A Unifying Information-theoretic Perspective on Evaluating Generative Models",
    "authors": [
      "Alexis Fox",
      "Samarth Swarup",
      "Abhijin Adiga"
    ],
    "abstract": "Considering the difficulty of interpreting generative model output, there is\nsignificant current research focused on determining meaningful evaluation\nmetrics. Several recent approaches utilize \"precision\" and \"recall,\" borrowed\nfrom the classification domain, to individually quantify the output fidelity\n(realism) and output diversity (representation of the real data variation),\nrespectively. With the increase in metric proposals, there is a need for a\nunifying perspective, allowing for easier comparison and clearer explanation of\ntheir benefits and drawbacks. To this end, we unify a class of\nkth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens\nusing approaches from kNN density estimation. Additionally, we propose a\ntri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall\nCross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity\nand two distinct aspects of diversity, inter- and intra-class. Our\ndomain-agnostic metric, derived from the information-theoretic concepts of\nentropy and cross-entropy, can be dissected for both sample- and mode-level\nanalysis. Our detailed experimental results demonstrate the sensitivity of our\nmetric components to their respective qualities and reveal undesirable\nbehaviors of other metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14340v3",
    "published_date": "2024-12-18 21:17:02 UTC",
    "updated_date": "2025-02-27 18:25:41 UTC"
  },
  {
    "arxiv_id": "2412.14329v1",
    "title": "Embedding Cultural Diversity in Prototype-based Recommender Systems",
    "authors": [
      "Armin Moradi",
      "Nicola Neophytou",
      "Florian Carichon",
      "Golnoosh Farnadi"
    ],
    "abstract": "Popularity bias in recommender systems can increase cultural\noverrepresentation by favoring norms from dominant cultures and marginalizing\nunderrepresented groups. This issue is critical for platforms offering cultural\nproducts, as they influence consumption patterns and human perceptions. In this\nwork, we address popularity bias by identifying demographic biases within\nprototype-based matrix factorization methods. Using the country of origin as a\nproxy for cultural identity, we link this demographic attribute to popularity\nbias by refining the embedding space learning process. First, we propose\nfiltering out irrelevant prototypes to improve representativity. Second, we\nintroduce a regularization technique to enforce a uniform distribution of\nprototypes within the embedding space. Across four datasets, our results\ndemonstrate a 27\\% reduction in the average rank of long-tail items and a 2\\%\nreduction in the average rank of items from underrepresented countries.\nAdditionally, our model achieves a 2\\% improvement in HitRatio@10 compared to\nthe state-of-the-art, highlighting that fairness is enhanced without\ncompromising recommendation quality. Moreover, the distribution of prototypes\nleads to more inclusive explanations by better aligning items with diverse\nprototypes.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14329v1",
    "published_date": "2024-12-18 20:57:33 UTC",
    "updated_date": "2024-12-18 20:57:33 UTC"
  },
  {
    "arxiv_id": "2412.14328v2",
    "title": "Semantic Role Labeling of NomBank Partitives",
    "authors": [
      "Adam Meyers",
      "Advait Pravin Savant",
      "John E. Ortega"
    ],
    "abstract": "This article is about Semantic Role Labeling for English partitive nouns\n(5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank\nannotated corpus. Several systems are described using traditional and\ntransformer-based machine learning, as well as ensembling. Our highest scoring\nsystem achieves an F1 of 91.74% using \"gold\" parses from the Penn Treebank and\n91.12% when using the Berkeley Neural parser. This research includes both\nclassroom and experimental settings for system development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.14328v2",
    "published_date": "2024-12-18 20:56:11 UTC",
    "updated_date": "2024-12-20 16:17:20 UTC"
  },
  {
    "arxiv_id": "2412.15287v1",
    "title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models",
    "authors": [
      "Yinlam Chow",
      "Guy Tennenholtz",
      "Izzeddin Gur",
      "Vincent Zhuang",
      "Bo Dai",
      "Sridhar Thiagarajan",
      "Craig Boutilier",
      "Rishabh Agarwal",
      "Aviral Kumar",
      "Aleksandra Faust"
    ],
    "abstract": "Recent studies have indicated that effectively utilizing inference-time\ncompute is crucial for attaining better performance from large language models\n(LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm,\nin which the model is fine-tuned in a manner that directly optimizes the\nperformance of the inference-time strategy. We study this paradigm using the\nsimple yet effective Best-of-N (BoN) inference strategy, in which a verifier\nselects the best out of a set of LLM-generated responses. We devise the first\nimitation learning and reinforcement learning~(RL) methods for BoN-aware\nfine-tuning, overcoming the challenging, non-differentiable argmax operator\nwithin BoN. We empirically demonstrate that our BoN-aware models implicitly\nlearn a meta-strategy that interleaves best responses with more diverse\nresponses that might be better suited to a test-time input -- a process\nreminiscent of the exploration-exploitation trade-off in RL. Our experiments\ndemonstrate the effectiveness of BoN-aware fine-tuning in terms of improved\nperformance and inference-time compute. In particular, we show that our methods\nimprove the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%,\nand pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6%\nto 67.1%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15287v1",
    "published_date": "2024-12-18 20:43:47 UTC",
    "updated_date": "2024-12-18 20:43:47 UTC"
  },
  {
    "arxiv_id": "2412.14323v2",
    "title": "The Role of Handling Attributive Nouns in Improving Chinese-To-English Machine Translation",
    "authors": [
      "Lisa Wang",
      "Adam Meyers",
      "John E. Ortega",
      "Rodolfo Zevallos"
    ],
    "abstract": "Translating between languages with drastically different grammatical\nconventions poses challenges, not just for human interpreters but also for\nmachine translation systems. In this work, we specifically target the\ntranslation challenges posed by attributive nouns in Chinese, which frequently\ncause ambiguities in English translation. By manually inserting the omitted\nparticle X ('DE'). In news article titles from the Penn Chinese Discourse\nTreebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to\nEnglish translation models, specifically improving how this critical function\nword is handled. This focused approach not only complements the broader\nstrategies suggested by previous studies but also offers a practical\nenhancement by specifically addressing a common error type in Chinese-English\ntranslation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18th Workshop on Building and Using Comparable Corpora (BUCC) at the\n  31st International Conference on Computational Linguistics (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.14323v2",
    "published_date": "2024-12-18 20:37:52 UTC",
    "updated_date": "2025-01-02 17:27:41 UTC"
  },
  {
    "arxiv_id": "2412.14304v1",
    "title": "Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs",
    "authors": [
      "David Restrepo",
      "Chenwei Wu",
      "Zhengxu Tang",
      "Zitao Shuai",
      "Thao Nguyen Minh Phan",
      "Jun-En Ding",
      "Cong-Tinh Dao",
      "Jack Gallifant",
      "Robyn Gayle Dychiao",
      "Jose Carlo Artiaga",
      "André Hiroshi Bando",
      "Carolina Pelegrini Barbosa Gracitelli",
      "Vincenz Ferrer",
      "Leo Anthony Celi",
      "Danielle Bitterman",
      "Michael G Morley",
      "Luis Filipe Nakayama"
    ],
    "abstract": "Current ophthalmology clinical workflows are plagued by over-referrals, long\nwaits, and complex and heterogeneous medical records. Large language models\n(LLMs) present a promising solution to automate various procedures such as\ntriaging, preliminary tests like visual acuity assessment, and report\nsummaries. However, LLMs have demonstrated significantly varied performance\nacross different languages in natural language question-answering tasks,\npotentially exacerbating healthcare disparities in Low and Middle-Income\nCountries (LMICs). This study introduces the first multilingual\nophthalmological question-answering benchmark with manually curated questions\nparallel across languages, allowing for direct cross-lingual comparisons. Our\nevaluation of 6 popular LLMs across 7 different languages reveals substantial\nbias across different languages, highlighting risks for clinical deployment of\nLLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought\nor Retrieval-augmented generation (RAG) by themselves fall short of closing\nthis performance gap, often failing to improve performance across all languages\nand lacking specificity for the medical domain. To address this issue, We\npropose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time\nde-biasing method leveraging retrieval augmented generation and\nself-verification. Our approach not only improves performance across all\nlanguages but also significantly reduces the multilingual bias gap,\nfacilitating equitable LLM application across the globe.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the AAAI 2025 Artificial Intelligence for Social Impact\n  Track (AAAI-AISI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.14304v1",
    "published_date": "2024-12-18 20:18:03 UTC",
    "updated_date": "2024-12-18 20:18:03 UTC"
  },
  {
    "arxiv_id": "2412.14302v2",
    "title": "SAFERec: Self-Attention and Frequency Enriched Model for Next Basket Recommendation",
    "authors": [
      "Oleg Lashinin",
      "Denis Krasilnikov",
      "Aleksandr Milogradskii",
      "Marina Ananyeva"
    ],
    "abstract": "Transformer-based approaches such as BERT4Rec and SASRec demonstrate strong\nperformance in Next Item Recommendation (NIR) tasks. However, applying these\narchitectures to Next-Basket Recommendation (NBR) tasks, which often involve\nhighly repetitive interactions, is challenging due to the vast number of\npossible item combinations in a basket. Moreover, frequency-based methods such\nas TIFU-KNN and UP-CF still demonstrate strong performance in NBR tasks,\nfrequently outperforming deep-learning approaches. This paper introduces\nSAFERec, a novel algorithm for NBR that enhances transformer-based\narchitectures from NIR by incorporating item frequency information,\nconsequently improving their applicability to NBR tasks. Extensive experiments\non multiple datasets show that SAFERec outperforms all other baselines,\nspecifically achieving an 8\\% improvement in Recall@10.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14302v2",
    "published_date": "2024-12-18 20:10:42 UTC",
    "updated_date": "2024-12-20 07:56:22 UTC"
  },
  {
    "arxiv_id": "2412.14295v2",
    "title": "Temporally Consistent Object-Centric Learning by Contrasting Slots",
    "authors": [
      "Anna Manasyan",
      "Maximilian Seitzer",
      "Filip Radovic",
      "Georg Martius",
      "Andrii Zadaianchuk"
    ],
    "abstract": "Unsupervised object-centric learning from videos is a promising approach to\nextract structured representations from large, unlabeled collections of videos.\nTo support downstream tasks like autonomous control, these representations must\nbe both compositional and temporally consistent. Existing approaches based on\nrecurrent processing often lack long-term stability across frames because their\ntraining objective does not enforce temporal consistency. In this work, we\nintroduce a novel object-level temporal contrastive loss for video\nobject-centric models that explicitly promotes temporal consistency. Our method\nsignificantly improves the temporal consistency of the learned object-centric\nrepresentations, yielding more reliable video decompositions that facilitate\nchallenging downstream tasks such as unsupervised object dynamics prediction.\nFurthermore, the inductive bias added by our loss strongly improves object\ndiscovery, leading to state-of-the-art results on both synthetic and real-world\ndatasets, outperforming even weakly-supervised methods that leverage motion\nmasks as additional cues.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.14295v2",
    "published_date": "2024-12-18 19:46:04 UTC",
    "updated_date": "2025-03-18 13:01:07 UTC"
  },
  {
    "arxiv_id": "2412.14283v2",
    "title": "PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation",
    "authors": [
      "Liyao Jiang",
      "Negar Hassanpour",
      "Mohammad Salameh",
      "Mohammadreza Samadi",
      "Jiao He",
      "Fengyu Sun",
      "Di Niu"
    ],
    "abstract": "Recent research explores the potential of Diffusion Models (DMs) for\nconsistent object editing, which aims to modify object position, size, and\ncomposition, etc., while preserving the consistency of objects and background\nwithout changing their texture and attributes. Current inference-time methods\noften rely on DDIM inversion, which inherently compromises efficiency and the\nachievable consistency of edited images. Recent methods also utilize energy\nguidance which iteratively updates the predicted noise and can drive the\nlatents away from the original image, resulting in distortions. In this paper,\nwe propose PixelMan, an inversion-free and training-free method for achieving\nconsistent object editing via Pixel Manipulation and generation, where we\ndirectly create a duplicate copy of the source object at target location in the\npixel space, and introduce an efficient sampling approach to iteratively\nharmonize the manipulated object into the target location and inpaint its\noriginal location, while ensuring image consistency by anchoring the edited\nimage to be generated to the pixel-manipulated image as well as by introducing\nvarious consistency-preserving optimization techniques during inference.\nExperimental evaluations based on benchmark datasets as well as extensive\nvisual comparisons show that in as few as 16 inference steps, PixelMan\noutperforms a range of state-of-the-art training-based and training-free\nmethods (usually requiring 50 steps) on multiple consistent object editing\ntasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025; version includes supplementary material; 27 Pages, 15\n  Figures, 6 Tables",
    "pdf_url": "http://arxiv.org/pdf/2412.14283v2",
    "published_date": "2024-12-18 19:24:15 UTC",
    "updated_date": "2025-01-30 00:05:46 UTC"
  },
  {
    "arxiv_id": "2412.14276v2",
    "title": "Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data",
    "authors": [
      "Shaina Raza",
      "Drai Paulen-Patterson",
      "Chen Ding"
    ],
    "abstract": "Fake news poses a significant threat to public opinion and social stability\nin modern society. This study presents a comparative evaluation of BERT-like\nencoder-only models and autoregressive decoder-only large language models\n(LLMs) for fake news detection. We introduce a dataset of news articles labeled\nwith GPT-4 assistance (an AI-labeling method) and verified by human experts to\nensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned\non this dataset. Additionally, we developed an instruction-tuned LLM approach\nwith majority voting during inference for label generation. Our analysis\nreveals that BERT-like models generally outperform LLMs in classification\ntasks, while LLMs demonstrate superior robustness against text perturbations.\nCompared to weak labels (distant supervision) data, the results show that AI\nlabels with human supervision achieve better classification results. This study\nhighlights the effectiveness of combining AI-based annotation with human\noversight and demonstrates the performance of different families of machine\nlearning models for fake news detection",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in Knowledge and Information Systems Journal",
    "pdf_url": "http://arxiv.org/pdf/2412.14276v2",
    "published_date": "2024-12-18 19:15:17 UTC",
    "updated_date": "2024-12-20 12:45:58 UTC"
  },
  {
    "arxiv_id": "2412.14272v1",
    "title": "Split Learning in Computer Vision for Semantic Segmentation Delay Minimization",
    "authors": [
      "Nikos G. Evgenidis",
      "Nikos A. Mitsiou",
      "Sotiris A. Tegos",
      "Panagiotis D. Diamantoulakis",
      "George K. Karagiannidis"
    ],
    "abstract": "In this paper, we propose a novel approach to minimize the inference delay in\nsemantic segmentation using split learning (SL), tailored to the needs of\nreal-time computer vision (CV) applications for resource-constrained devices.\nSemantic segmentation is essential for applications such as autonomous vehicles\nand smart city infrastructure, but faces significant latency challenges due to\nhigh computational and communication loads. Traditional centralized processing\nmethods are inefficient for such scenarios, often resulting in unacceptable\ninference delays. SL offers a promising alternative by partitioning deep neural\nnetworks (DNNs) between edge devices and a central server, enabling localized\ndata processing and reducing the amount of data required for transmission. Our\ncontribution includes the joint optimization of bandwidth allocation, cut layer\nselection of the edge devices' DNN, and the central server's processing\nresource allocation. We investigate both parallel and serial data processing\nscenarios and propose low-complexity heuristic solutions that maintain\nnear-optimal performance while reducing computational requirements. Numerical\nresults show that our approach effectively reduces inference delay,\ndemonstrating the potential of SL for improving real-time CV applications in\ndynamic, resource-constrained environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14272v1",
    "published_date": "2024-12-18 19:07:25 UTC",
    "updated_date": "2024-12-18 19:07:25 UTC"
  },
  {
    "arxiv_id": "2412.14172v1",
    "title": "Learning from Massive Human Videos for Universal Humanoid Pose Control",
    "authors": [
      "Jiageng Mao",
      "Siheng Zhao",
      "Siqi Song",
      "Tianheng Shi",
      "Junjie Ye",
      "Mingtong Zhang",
      "Haoran Geng",
      "Jitendra Malik",
      "Vitor Guizilini",
      "Yue Wang"
    ],
    "abstract": "Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14172v1",
    "published_date": "2024-12-18 18:59:56 UTC",
    "updated_date": "2024-12-18 18:59:56 UTC"
  },
  {
    "arxiv_id": "2412.14170v2",
    "title": "E-CAR: Efficient Continuous Autoregressive Image Generation via Multistage Modeling",
    "authors": [
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Hanling Zhang",
      "Tongcheng Fang",
      "Rui Xie",
      "Bingxin Xu",
      "Yan Yan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Recent advances in autoregressive (AR) models with continuous tokens for\nimage generation show promising results by eliminating the need for discrete\ntokenization. However, these models face efficiency challenges due to their\nsequential token generation nature and reliance on computationally intensive\ndiffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive\nImage Generation via Multistage Modeling), an approach that addresses these\nlimitations through two intertwined innovations: (1) a stage-wise continuous\ntoken generation strategy that reduces computational complexity and provides\nprogressively refined token maps as hierarchical conditions, and (2) a\nmultistage flow-based distribution modeling method that transforms only\npartial-denoised distributions at each stage comparing to complete denoising in\nnormal diffusion models. Holistically, ECAR operates by generating tokens at\nincreasing resolutions while simultaneously denoising the image at each stage.\nThis design not only reduces token-to-image transformation cost by a factor of\nthe stage number but also enables parallel processing at the token level. Our\napproach not only enhances computational efficiency but also aligns naturally\nwith image generation principles by operating in continuous token space and\nfollowing a hierarchical generation process from coarse to fine details.\nExperimental results demonstrate that ECAR achieves comparable image quality to\nDiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and\n5$\\times$ speedup to generate a 256$\\times$256 image.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14170v2",
    "published_date": "2024-12-18 18:59:53 UTC",
    "updated_date": "2024-12-19 02:42:46 UTC"
  },
  {
    "arxiv_id": "2412.14167v1",
    "title": "VideoDPO: Omni-Preference Alignment for Video Diffusion Generation",
    "authors": [
      "Runtao Liu",
      "Haoyu Wu",
      "Zheng Ziqiang",
      "Chen Wei",
      "Yingqing He",
      "Renjie Pi",
      "Qifeng Chen"
    ],
    "abstract": "Recent progress in generative diffusion models has greatly advanced\ntext-to-video generation. While text-to-video models trained on large-scale,\ndiverse datasets can produce varied outputs, these generations often deviate\nfrom user preferences, highlighting the need for preference alignment on\npre-trained models. Although Direct Preference Optimization (DPO) has\ndemonstrated significant improvements in language and image generation, we\npioneer its adaptation to video diffusion models and propose a VideoDPO\npipeline by making several key adjustments. Unlike previous image alignment\nmethods that focus solely on either (i) visual quality or (ii) semantic\nalignment between text and videos, we comprehensively consider both dimensions\nand construct a preference score accordingly, which we term the OmniScore. We\ndesign a pipeline to automatically collect preference pair data based on the\nproposed OmniScore and discover that re-weighting these pairs based on the\nscore significantly impacts overall preference alignment. Our experiments\ndemonstrate substantial improvements in both visual quality and semantic\nalignment, ensuring that no preference aspect is neglected. Code and data will\nbe shared at https://videodpo.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14167v1",
    "published_date": "2024-12-18 18:59:49 UTC",
    "updated_date": "2024-12-18 18:59:49 UTC"
  },
  {
    "arxiv_id": "2412.14234v2",
    "title": "Syzygy: Dual Code-Test C to (safe) Rust Translation using LLMs and Dynamic Analysis",
    "authors": [
      "Manish Shetty",
      "Naman Jain",
      "Adwait Godbole",
      "Sanjit A. Seshia",
      "Koushik Sen"
    ],
    "abstract": "Despite extensive usage in high-performance, low-level systems programming\napplications, C is susceptible to vulnerabilities due to manual memory\nmanagement and unsafe pointer operations. Rust, a modern systems programming\nlanguage, offers a compelling alternative. Its unique ownership model and type\nsystem ensure memory safety without sacrificing performance.\n  In this paper, we present Syzygy, an automated approach to translate C to\nsafe Rust. Our technique uses a synergistic combination of LLM-driven code and\ntest translation guided by dynamic-analysis-generated execution information.\nThis paired translation runs incrementally in a loop over the program in\ndependency order of the code elements while maintaining per-step correctness.\nOur approach exposes novel insights on combining the strengths of LLMs and\ndynamic analysis in the context of scaling and combining code generation with\ntesting. We apply our approach to successfully translate Zopfli, a\nhigh-performance compression library with ~3000 lines of code and 98 functions.\nWe validate the translation by testing equivalence with the source C program on\na set of inputs. To our knowledge, this is the largest automated and\ntest-validated C to safe Rust code translation achieved so far.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PL",
      "I.2; D.2; D.3"
    ],
    "primary_category": "cs.SE",
    "comment": "Project webpage at https://syzygy-project.github.io/. Preliminary\n  version accepted at LLM4Code 2025, 34 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.14234v2",
    "published_date": "2024-12-18 18:55:46 UTC",
    "updated_date": "2024-12-21 18:49:38 UTC"
  },
  {
    "arxiv_id": "2412.14158v2",
    "title": "AKiRa: Augmentation Kit on Rays for optical video generation",
    "authors": [
      "Xi Wang",
      "Robin Courant",
      "Marc Christie",
      "Vicky Kalogeiton"
    ],
    "abstract": "Recent advances in text-conditioned video diffusion have greatly improved\nvideo quality. However, these methods offer limited or sometimes no control to\nusers on camera aspects, including dynamic camera motion, zoom, distorted lens\nand focus shifts. These motion and optical aspects are crucial for adding\ncontrollability and cinematic elements to generation frameworks, ultimately\nresulting in visual content that draws focus, enhances mood, and guides\nemotions according to filmmakers' controls. In this paper, we aim to close the\ngap between controllable video generation and camera optics. To achieve this,\nwe propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework\nthat builds and trains a camera adapter with a complex camera model over an\nexisting video generation backbone. It enables fine-tuned control over camera\nmotion as well as complex optical parameters (focal length, distortion,\naperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh.\nExtensive experiments demonstrate AKiRa's effectiveness in combining and\ncomposing camera optics while outperforming all state-of-the-art methods. This\nwork sets a new landmark in controlled and optically enhanced video generation,\npaving the way for future optical video generation methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14158v2",
    "published_date": "2024-12-18 18:53:22 UTC",
    "updated_date": "2024-12-29 17:22:30 UTC"
  },
  {
    "arxiv_id": "2412.14146v3",
    "title": "ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics",
    "authors": [
      "Atin Sakkeer Hussain"
    ],
    "abstract": "This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14146v3",
    "published_date": "2024-12-18 18:44:08 UTC",
    "updated_date": "2025-01-23 07:06:15 UTC"
  },
  {
    "arxiv_id": "2412.15285v1",
    "title": "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase Pretraining",
    "authors": [
      "Steven Feng",
      "Shrimai Prabhumoye",
      "Kezhi Kong",
      "Dan Su",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "abstract": "Pretraining large language models effectively requires strategic data\nselection, blending and ordering. However, key details about data mixtures\nespecially their scalability to longer token horizons and larger model sizes\nremain underexplored due to limited disclosure by model developers. To address\nthis, we formalize the concept of two-phase pretraining and conduct an\nextensive systematic study on how to select and mix data to maximize model\naccuracies for the two phases. Our findings illustrate that a two-phase\napproach for pretraining outperforms random data ordering and natural\ndistribution of tokens by 3.4% and 17% on average accuracies. We provide\nin-depth guidance on crafting optimal blends based on quality of the data\nsource and the number of epochs to be seen. We propose to design blends using\ndownsampled data at a smaller scale of 1T tokens and then demonstrate effective\nscaling of our approach to larger token horizon of 15T tokens and larger model\nsize of 25B model size. These insights provide a series of steps practitioners\ncan follow to design and scale their data blends.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15285v1",
    "published_date": "2024-12-18 18:41:18 UTC",
    "updated_date": "2024-12-18 18:41:18 UTC"
  },
  {
    "arxiv_id": "2412.14141v2",
    "title": "LLMs can Realize Combinatorial Creativity: Generating Creative Ideas via LLMs for Scientific Research",
    "authors": [
      "Tianyang Gu",
      "Jingjin Wang",
      "Zhihao Zhang",
      "HaoHong Li"
    ],
    "abstract": "Scientific idea generation has been extensively studied in creativity theory\nand computational creativity research, providing valuable frameworks for\nunderstanding and implementing creative processes. However, recent work using\nLarge Language Models (LLMs) for research idea generation often overlooks these\ntheoretical foundations. We present a framework that explicitly implements\ncombinatorial creativity theory using LLMs, featuring a generalization-level\nretrieval system for cross-domain knowledge discovery and a structured\ncombinatorial process for idea generation. The retrieval system maps concepts\nacross different abstraction levels to enable meaningful connections between\ndisparate domains, while the combinatorial process systematically analyzes and\nrecombines components to generate novel solutions. Experiments on the OAG-Bench\ndataset demonstrate our framework's effectiveness, consistently outperforming\nbaseline approaches in generating ideas that align with real research\ndevelopments (improving similarity scores by 7\\%-10\\% across multiple metrics).\nOur results provide strong evidence that LLMs can effectively realize\ncombinatorial creativity when guided by appropriate theoretical frameworks,\ncontributing both to practical advancement of AI-assisted research and\ntheoretical understanding of machine creativity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14141v2",
    "published_date": "2024-12-18 18:41:14 UTC",
    "updated_date": "2025-02-17 04:31:41 UTC"
  },
  {
    "arxiv_id": "2412.14140v2",
    "title": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking",
    "authors": [
      "Darshan Deshpande",
      "Selvan Sunitha Ravi",
      "Sky CH-Wang",
      "Bartosz Mielczarek",
      "Anand Kannappan",
      "Rebecca Qian"
    ],
    "abstract": "The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14140v2",
    "published_date": "2024-12-18 18:41:12 UTC",
    "updated_date": "2024-12-20 21:59:56 UTC"
  },
  {
    "arxiv_id": "2412.14137v1",
    "title": "Design choices made by LLM-based test generators prevent them from finding bugs",
    "authors": [
      "Noble Saji Mathews",
      "Meiyappan Nagappan"
    ],
    "abstract": "There is an increasing amount of research and commercial tools for automated\ntest case generation using Large Language Models (LLMs). This paper critically\nexamines whether recent LLM-based test generation tools, such as Codium\nCoverAgent and CoverUp, can effectively find bugs or unintentionally validate\nfaulty code. Considering bugs are only exposed by failing test cases, we\nexplore the question: can these tools truly achieve the intended objectives of\nsoftware testing when their test oracles are designed to pass? Using real\nhuman-written buggy code as input, we evaluate these tools, showing how\nLLM-generated tests can fail to detect bugs and, more alarmingly, how their\ndesign can worsen the situation by validating bugs in the generated test suite\nand rejecting bug-revealing tests. These findings raise important questions\nabout the validity of the design behind LLM-based test generation tools and\ntheir impact on software quality and test suite reliability.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14137v1",
    "published_date": "2024-12-18 18:33:26 UTC",
    "updated_date": "2024-12-18 18:33:26 UTC"
  },
  {
    "arxiv_id": "2412.14135v1",
    "title": "Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective",
    "authors": [
      "Zhiyuan Zeng",
      "Qinyuan Cheng",
      "Zhangyue Yin",
      "Bo Wang",
      "Shimin Li",
      "Yunhua Zhou",
      "Qipeng Guo",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "abstract": "OpenAI o1 represents a significant milestone in Artificial Inteiligence,\nwhich achieves expert-level performances on many challanging tasks that require\nstrong reasoning ability.OpenAI has claimed that the main techinique behinds o1\nis the reinforcement learining. Recent works use alternative approaches like\nknowledge distillation to imitate o1's reasoning style, but their effectiveness\nis limited by the capability ceiling of the teacher model. Therefore, this\npaper analyzes the roadmap to achieving o1 from the perspective of\nreinforcement learning, focusing on four key components: policy initialization,\nreward design, search, and learning. Policy initialization enables models to\ndevelop human-like reasoning behaviors, equipping them with the ability to\neffectively explore solution spaces for complex problems. Reward design\nprovides dense and effective signals via reward shaping or reward modeling,\nwhich is the guidance for both search and learning. Search plays a crucial role\nin generating high-quality solutions during both training and testing phases,\nwhich can produce better solutions with more computation. Learning utilizes the\ndata generated by search for improving policy, which can achieve the better\nperformance with more parameters and more searched data. Existing open-source\nprojects that attempt to reproduce o1 can be seem as a part or a variant of our\nroadmap. Collectively, these components underscore how learning and search\ndrive o1's advancement, making meaningful contributions to the development of\nLLM.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14135v1",
    "published_date": "2024-12-18 18:24:47 UTC",
    "updated_date": "2024-12-18 18:24:47 UTC"
  },
  {
    "arxiv_id": "2412.14097v1",
    "title": "Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts",
    "authors": [
      "Jihye Choi",
      "Jayaram Raghuram",
      "Yixuan Li",
      "Somesh Jha"
    ],
    "abstract": "Advancements in foundation models (FMs) have led to a paradigm shift in\nmachine learning. The rich, expressive feature representations from these\npre-trained, large-scale FMs are leveraged for multiple downstream tasks,\nusually via lightweight fine-tuning of a shallow fully-connected network\nfollowing the representation. However, the non-interpretable, black-box nature\nof this prediction pipeline can be a challenge, especially in critical domains\nsuch as healthcare, finance, and security. In this paper, we explore the\npotential of Concept Bottleneck Models (CBMs) for transforming complex,\nnon-interpretable foundation models into interpretable decision-making\npipelines using high-level concept vectors. Specifically, we focus on the\ntest-time deployment of such an interpretable CBM pipeline \"in the wild\", where\nthe input distribution often shifts from the original training distribution. We\nfirst identify the potential failure modes of such a pipeline under different\ntypes of distribution shifts. Then we propose an adaptive concept bottleneck\nframework to address these failure modes, that dynamically adapts the\nconcept-vector bank and the prediction layer based solely on unlabeled data\nfrom the target domain, without access to the source (training) dataset.\nEmpirical evaluations with various real-world distribution shifts show that our\nadaptation method produces concept-based interpretations better aligned with\nthe test data and boosts post-deployment accuracy by up to 28%, aligning the\nCBM performance with that of non-interpretable classification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "The preliminary version of the work appeared in the ICML 2024\n  Workshop on Foundation Models in the Wild",
    "pdf_url": "http://arxiv.org/pdf/2412.14097v1",
    "published_date": "2024-12-18 17:47:46 UTC",
    "updated_date": "2024-12-18 17:47:46 UTC"
  },
  {
    "arxiv_id": "2412.14093v2",
    "title": "Alignment faking in large language models",
    "authors": [
      "Ryan Greenblatt",
      "Carson Denison",
      "Benjamin Wright",
      "Fabien Roger",
      "Monte MacDiarmid",
      "Sam Marks",
      "Johannes Treutlein",
      "Tim Belonax",
      "Jack Chen",
      "David Duvenaud",
      "Akbir Khan",
      "Julian Michael",
      "Sören Mindermann",
      "Ethan Perez",
      "Linda Petrini",
      "Jonathan Uesato",
      "Jared Kaplan",
      "Buck Shlegeris",
      "Samuel R. Bowman",
      "Evan Hubinger"
    ],
    "abstract": "We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14093v2",
    "published_date": "2024-12-18 17:41:24 UTC",
    "updated_date": "2024-12-20 02:22:19 UTC"
  },
  {
    "arxiv_id": "2412.14087v1",
    "title": "SEKE: Specialised Experts for Keyword Extraction",
    "authors": [
      "Matej Martinc",
      "Hanh Thi Hong Tran",
      "Senja Pollak",
      "Boshko Koloski"
    ],
    "abstract": "Keyword extraction involves identifying the most descriptive words in a\ndocument, allowing automatic categorisation and summarisation of large\nquantities of diverse textual data. Relying on the insight that real-world\nkeyword detection often requires handling of diverse content, we propose a\nnovel supervised keyword extraction approach based on the mixture of experts\n(MoE) technique. MoE uses a learnable routing sub-network to direct information\nto specialised experts, allowing them to specialize in distinct regions of the\ninput space. SEKE, a mixture of Specialised Experts for supervised Keyword\nExtraction, uses DeBERTa as the backbone model and builds on the MoE framework,\nwhere experts attend to each token, by integrating it with a recurrent neural\nnetwork (RNN), to allow successful extraction even on smaller corpora, where\nspecialisation is harder due to lack of training data. The MoE framework also\nprovides an insight into inner workings of individual experts, enhancing the\nexplainability of the approach. We benchmark SEKE on multiple English datasets,\nachieving state-of-the-art performance compared to strong supervised and\nunsupervised baselines. Our analysis reveals that depending on data size and\ntype, experts specialize in distinct syntactic and semantic components, such as\npunctuation, stopwords, parts-of-speech, or named entities. Code is available\nat: https://github.com/matejMartinc/SEKE_keyword_extraction",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14087v1",
    "published_date": "2024-12-18 17:34:32 UTC",
    "updated_date": "2024-12-18 17:34:32 UTC"
  },
  {
    "arxiv_id": "2412.14085v1",
    "title": "Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report",
    "authors": [
      "Markus Dablander"
    ],
    "abstract": "Video games are a natural and synergistic application domain for artificial\nintelligence (AI) systems, offering both the potential to enhance player\nexperience and immersion, as well as providing valuable benchmarks and virtual\nenvironments to advance AI technologies in general. This report presents a\nhigh-level overview of five promising research pathways for applying\nstate-of-the-art AI methods, particularly deep learning, to digital gaming\nwithin the context of the current research landscape. The objective of this\nwork is to outline a curated, non-exhaustive list of encouraging research\ndirections at the intersection of AI and video games that may serve to inspire\nmore rigorous and comprehensive research efforts in the future. We discuss (i)\ninvestigating large language models as core engines for game agent modelling,\n(ii) using neural cellular automata for procedural game content generation,\n(iii) accelerating computationally expensive in-game simulations via deep\nsurrogate modelling, (iv) leveraging self-supervised learning to obtain useful\nvideo game state embeddings, and (v) training generative models of interactive\nworlds using unlabelled video data. We also briefly address current technical\nchallenges associated with the integration of advanced deep learning systems\ninto video game development, and indicate key areas where further progress is\nlikely to be beneficial.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14085v1",
    "published_date": "2024-12-18 17:32:27 UTC",
    "updated_date": "2024-12-18 17:32:27 UTC"
  },
  {
    "arxiv_id": "2412.14077v1",
    "title": "Dialogue with the Machine and Dialogue with the Art World: Evaluating Generative AI for Culturally-Situated Creativity",
    "authors": [
      "Rida Qadri",
      "Piotr Mirowski",
      "Aroussiak Gabriellan",
      "Farbod Mehr",
      "Huma Gupta",
      "Pamela Karimi",
      "Remi Denton"
    ],
    "abstract": "This paper proposes dialogue as a method for evaluating generative AI tools\nfor culturally-situated creative practice, that recognizes the socially\nsituated nature of art. Drawing on sociologist Howard Becker's concept of Art\nWorlds, this method expands the scope of traditional AI and creativity\nevaluations beyond benchmarks, user studies with crowd-workers, or focus groups\nconducted with artists. Our method involves two mutually informed dialogues: 1)\n'dialogues with art worlds' placing artists in conversation with experts such\nas art historians, curators, and archivists, and 2)'dialogues with the\nmachine,' facilitated through structured artist- and critic-led experimentation\nwith state-of-the-art generative AI tools. We demonstrate the value of this\nmethod through a case study with artists and experts steeped in non-western art\nworlds, specifically the Persian Gulf. We trace how these dialogues help create\nculturally rich and situated forms of evaluation for representational\npossibilities of generative AI that mimic the reception of generative artwork\nin the broader art ecosystem. Putting artists in conversation with commentators\nalso allow artists to shift their use of the tools to respond to their cultural\nand creative context. Our study can provide generative AI researchers an\nunderstanding of the complex dynamics of technology, human creativity and the\nsocio-politics of art worlds, to build more inclusive machines for diverse art\nworlds.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "NeurIPS 2024 Creative AI Track",
    "pdf_url": "http://arxiv.org/pdf/2412.14077v1",
    "published_date": "2024-12-18 17:21:14 UTC",
    "updated_date": "2024-12-18 17:21:14 UTC"
  },
  {
    "arxiv_id": "2412.14076v1",
    "title": "Compositional Generalization Across Distributional Shifts with Sparse Tree Operations",
    "authors": [
      "Paul Soulos",
      "Henry Conklin",
      "Mattia Opper",
      "Paul Smolensky",
      "Jianfeng Gao",
      "Roland Fernandez"
    ],
    "abstract": "Neural networks continue to struggle with compositional generalization, and\nthis issue is exacerbated by a lack of massive pre-training. One successful\napproach for developing neural systems which exhibit human-like compositional\ngeneralization is \\textit{hybrid} neurosymbolic techniques. However, these\ntechniques run into the core issues that plague symbolic approaches to AI:\nscalability and flexibility. The reason for this failure is that at their core,\nhybrid neurosymbolic models perform symbolic computation and relegate the\nscalable and flexible neural computation to parameterizing a symbolic system.\nWe investigate a \\textit{unified} neurosymbolic system where transformations in\nthe network can be interpreted simultaneously as both symbolic and neural\ncomputation. We extend a unified neurosymbolic architecture called the\nDifferentiable Tree Machine in two central ways. First, we significantly\nincrease the model's efficiency through the use of sparse vector\nrepresentations of symbolic structures. Second, we enable its application\nbeyond the restricted set of tree2tree problems to the more general class of\nseq2seq problems. The improved model retains its prior generalization\ncapabilities and, since there is a fully neural path through the network,\navoids the pitfalls of other neurosymbolic techniques that elevate symbolic\ncomputation over neural computation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024. Code available at https://github.com/psoulos/sdtm",
    "pdf_url": "http://arxiv.org/pdf/2412.14076v1",
    "published_date": "2024-12-18 17:20:19 UTC",
    "updated_date": "2024-12-18 17:20:19 UTC"
  },
  {
    "arxiv_id": "2412.14073v1",
    "title": "A Computationally Grounded Framework for Cognitive Attitudes (extended version)",
    "authors": [
      "Tiago de Lima",
      "Emiliano Lorini",
      "Elise Perrotin",
      "François Schwarzentruber"
    ],
    "abstract": "We introduce a novel language for reasoning about agents' cognitive attitudes\nof both epistemic and motivational type. We interpret it by means of a\ncomputationally grounded semantics using belief bases. Our language includes\nfive types of modal operators for implicit belief, complete attraction,\ncomplete repulsion, realistic attraction and realistic repulsion. We give an\naxiomatization and show that our operators are not mutually expressible and\nthat they can be combined to represent a large variety of psychological\nconcepts including ambivalence, indifference, being motivated, being\ndemotivated and preference. We present a dynamic extension of the language that\nsupports reasoning about the effects of belief change operations. Finally, we\nprovide a succinct formulation of model checking for our languages and a PSPACE\nmodel checking algorithm relying on a reduction into TQBF. We present some\nexperimental results for the implemented algorithm on computation time in a\nconcrete example.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14073v1",
    "published_date": "2024-12-18 17:17:07 UTC",
    "updated_date": "2024-12-18 17:17:07 UTC"
  },
  {
    "arxiv_id": "2412.14063v3",
    "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification",
    "authors": [
      "Kyle Thompson",
      "Nuno Saavedra",
      "Pedro Carrott",
      "Kevin Fisher",
      "Alex Sanchez-Stern",
      "Yuriy Brun",
      "João F. Ferreira",
      "Sorin Lerner",
      "Emily First"
    ],
    "abstract": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2.4; I.2.7; I.2.3"
    ],
    "primary_category": "cs.SE",
    "comment": "In Proceedings of the 47th International Conference on Software\n  Engineering (ICSE), Ottawa, ON, Canada, April 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.14063v3",
    "published_date": "2024-12-18 17:08:42 UTC",
    "updated_date": "2025-01-28 19:56:36 UTC"
  },
  {
    "arxiv_id": "2412.14056v1",
    "title": "A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future",
    "authors": [
      "Shilin Sun",
      "Wenbin An",
      "Feng Tian",
      "Fang Nan",
      "Qidong Liu",
      "Jun Liu",
      "Nazaraf Shah",
      "Ping Chen"
    ],
    "abstract": "Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2412.14056v1",
    "published_date": "2024-12-18 17:06:21 UTC",
    "updated_date": "2024-12-18 17:06:21 UTC"
  },
  {
    "arxiv_id": "2412.14054v1",
    "title": "Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios and Lightweight Deployment",
    "authors": [
      "Kevin You"
    ],
    "abstract": "Text Normalization and Semantic Parsing have numerous applications in natural\nlanguage processing, such as natural language programming, paraphrasing, data\naugmentation, constructing expert systems, text matching, and more. Despite the\nprominent achievements of deep learning in Large Language Models (LLMs), the\ninterpretability of neural network architectures is still poor, which affects\ntheir credibility and hence limits the deployments of risk-sensitive scenarios.\nIn certain scenario-specific domains with scarce data, rapidly obtaining a\nlarge number of supervised learning labels is challenging, and the workload of\nmanually labeling data would be enormous. Catastrophic forgetting in neural\nnetworks further leads to low data utilization rates. In situations where swift\nresponses are vital, the density of the model makes local deployment difficult\nand the response time long, which is not conducive to local applications of\nthese fields. Inspired by the multiplication rule, a principle of combinatorial\nmathematics, and human thinking patterns, a multilayer framework along with its\nalgorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is\nproposed to address these above issues, combining text normalization and\nsemantic parsing workflows. The Chinese Scripting Language \"Fire Bunny\nIntelligent Development Platform V2.0\" is an important test and application of\nthe technology discussed in this paper. DAHSF can run locally in\nscenario-specific domains on little datasets, with model size and memory usage\noptimized by at least two orders of magnitude, thus improving the execution\nspeed, and possessing a promising optimization outlook.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2412.14054v1",
    "published_date": "2024-12-18 17:05:49 UTC",
    "updated_date": "2024-12-18 17:05:49 UTC"
  },
  {
    "arxiv_id": "2412.14052v1",
    "title": "Neural Combinatorial Optimization for Stochastic Flexible Job Shop Scheduling Problems",
    "authors": [
      "Igor G. Smit",
      "Yaoxin Wu",
      "Pavel Troubil",
      "Yingqian Zhang",
      "Wim P. M. Nuijten"
    ],
    "abstract": "Neural combinatorial optimization (NCO) has gained significant attention due\nto the potential of deep learning to efficiently solve combinatorial\noptimization problems. NCO has been widely applied to job shop scheduling\nproblems (JSPs) with the current focus predominantly on deterministic problems.\nIn this paper, we propose a novel attention-based scenario processing module\n(SPM) to extend NCO methods for solving stochastic JSPs. Our approach\nexplicitly incorporates stochastic information by an attention mechanism that\ncaptures the embedding of sampled scenarios (i.e., an approximation of\nstochasticity). Fed with the embedding, the base neural network is intervened\nby the attended scenarios, which accordingly learns an effective policy under\nstochasticity. We also propose a training paradigm that works harmoniously with\neither the expected makespan or Value-at-Risk objective. Results demonstrate\nthat our approach outperforms existing learning and non-learning methods for\nthe flexible JSP problem with stochastic processing times on a variety of\ninstances. In addition, our approach holds significant generalizability to\nvaried numbers of scenarios and disparate distributions.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.14052v1",
    "published_date": "2024-12-18 17:05:33 UTC",
    "updated_date": "2024-12-18 17:05:33 UTC"
  },
  {
    "arxiv_id": "2412.14031v3",
    "title": "Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization Perspective",
    "authors": [
      "Semih Cayci"
    ],
    "abstract": "We analyze the convergence of Gauss-Newton dynamics for training neural\nnetworks with smooth activation functions. In the underparameterized regime,\nthe Gauss-Newton gradient flow induces a Riemannian gradient flow on a\nlow-dimensional, smooth, embedded submanifold of the Euclidean output space.\nUsing tools from Riemannian optimization, we prove \\emph{last-iterate}\nconvergence of the Riemannian gradient flow to the optimal in-class predictor\nat an \\emph{exponential rate} that is independent of the conditioning of the\nGram matrix, \\emph{without} requiring explicit regularization. We further\ncharacterize the critical impacts of the neural network scaling factor and the\ninitialization on the convergence behavior. In the overparameterized regime, we\nshow that the Levenberg-Marquardt dynamics with an appropriately chosen damping\nfactor yields robustness to ill-conditioned kernels, analogous to the\nunderparameterized regime. These findings demonstrate the potential of\nGauss-Newton methods for efficiently optimizing neural networks, particularly\nin ill-conditioned problems where kernel and Gram matrices have small singular\nvalues.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14031v3",
    "published_date": "2024-12-18 16:51:47 UTC",
    "updated_date": "2024-12-20 15:58:45 UTC"
  },
  {
    "arxiv_id": "2412.14020v1",
    "title": "Landscape of AI safety concerns -- A methodology to support safety assurance for AI-based autonomous systems",
    "authors": [
      "Ronald Schnitzer",
      "Lennart Kilian",
      "Simon Roessner",
      "Konstantinos Theodorou",
      "Sonja Zillner"
    ],
    "abstract": "Artificial Intelligence (AI) has emerged as a key technology, driving\nadvancements across a range of applications. Its integration into modern\nautonomous systems requires assuring safety. However, the challenge of assuring\nsafety in systems that incorporate AI components is substantial. The lack of\nconcrete specifications, and also the complexity of both the operational\nenvironment and the system itself, leads to various aspects of uncertain\nbehavior and complicates the derivation of convincing evidence for system\nsafety. Nonetheless, scholars proposed to thoroughly analyze and mitigate\nAI-specific insufficiencies, so-called AI safety concerns, which yields\nessential evidence supporting a convincing assurance case. In this paper, we\nbuild upon this idea and propose the so-called Landscape of AI Safety Concerns,\na novel methodology designed to support the creation of safety assurance cases\nfor AI-based systems by systematically demonstrating the absence of AI safety\nconcerns. The methodology's application is illustrated through a case study\ninvolving a driverless regional train, demonstrating its practicality and\neffectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14020v1",
    "published_date": "2024-12-18 16:38:16 UTC",
    "updated_date": "2024-12-18 16:38:16 UTC"
  },
  {
    "arxiv_id": "2412.14019v2",
    "title": "Discovery of Maximally Consistent Causal Orders with Large Language Models",
    "authors": [
      "Federico Baldo",
      "Simon Ferreira",
      "Charles K. Assaad"
    ],
    "abstract": "Causal discovery is essential for understanding complex systems, as it aims\nto uncover causal relationships from observational data in the form of a causal\ndirected acyclic graph (DAG). However, traditional methods often rely on\nstrong, untestable assumptions, which makes them unreliable in real\napplications. Large Language Models (LLMs) present a promising alternative for\nextracting causal knowledge from text-based metadata, which consolidates domain\nexpertise. However, LLMs are prone to unreliability and hallucinations,\nnecessitating strategies that account for their limitations. One such strategy\ninvolves leveraging a consistency measure to evaluate reliability.\nAdditionally, most text metadata does not clearly distinguish direct causal\nrelationships from indirect ones, further complicating the discovery of a\ncausal DAG. As a result, focusing on causal orderings, rather than causal DAGs,\nemerges as a more practical and robust approach. We propose a novel method to\nderive a class of acyclic tournaments (representing plausible causal orders)\nthat maximizes a consistency score derived from an LLM. Our approach begins by\ncomputing pairwise consistency scores between variables, yielding a\nsemi-complete directed graph that aggregates these scores. From this structure,\nwe identify optimal acyclic tournaments, prioritizing those that maximize\nconsistency across all configurations. We tested our method on both\nwell-established benchmarks, as well as real-world datasets from epidemiology\nand public health. Our results demonstrate the effectiveness of our approach in\nrecovering a class of causal orders.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14019v2",
    "published_date": "2024-12-18 16:37:51 UTC",
    "updated_date": "2025-02-09 16:40:38 UTC"
  },
  {
    "arxiv_id": "2412.14018v1",
    "title": "SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical Video Generation",
    "authors": [
      "Tong Chen",
      "Shuya Yang",
      "Junyi Wang",
      "Long Bai",
      "Hongliang Ren",
      "Luping Zhou"
    ],
    "abstract": "Medical video generation has transformative potential for enhancing surgical\nunderstanding and pathology insights through precise and controllable visual\nrepresentations. However, current models face limitations in controllability\nand authenticity. To bridge this gap, we propose SurgSora, a\nmotion-controllable surgical video generation framework that uses a single\ninput frame and user-controllable motion cues. SurgSora consists of three key\nmodules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB\nand depth features from the input frame and integrates them with segmentation\ncues to capture detailed spatial features of complex anatomical structures; the\nDecoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D\nfeatures at multiple scales to enhance temporal understanding and object\nspatial dynamics; and the Trajectory Controller (TC), which allows users to\nspecify motion directions and estimates sparse optical flow, guiding the video\ngeneration process. The fused features are used as conditions for a frozen\nStable Diffusion model to produce realistic, temporally coherent surgical\nvideos. Extensive evaluations demonstrate that SurgSora outperforms\nstate-of-the-art methods in controllability and authenticity, showing its\npotential to advance surgical video generation for medical education, training,\nand research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14018v1",
    "published_date": "2024-12-18 16:34:51 UTC",
    "updated_date": "2024-12-18 16:34:51 UTC"
  },
  {
    "arxiv_id": "2412.14009v1",
    "title": "Cognition Chain for Explainable Psychological Stress Detection on Social Media",
    "authors": [
      "Xin Wang",
      "Boyan Gao",
      "Yi Dai",
      "Lei Cao",
      "Liang Zhao",
      "Yibo Yang",
      "David Clifton"
    ],
    "abstract": "Stress is a pervasive global health issue that can lead to severe mental\nhealth problems. Early detection offers timely intervention and prevention of\nstress-related disorders. The current early detection models perform \"black\nbox\" inference suffering from limited explainability and trust which blocks the\nreal-world clinical application. Thanks to the generative properties introduced\nby the Large Language Models (LLMs), the decision and the prediction from such\nmodels are semi-interpretable through the corresponding description. However,\nthe existing LLMs are mostly trained for general purposes without the guidance\nof psychological cognitive theory. To this end, we first highlight the\nimportance of prior theory with the observation of performance boosted by the\nchain-of-thoughts tailored for stress detection. This method termed Cognition\nChain explicates the generation of stress through a step-by-step cognitive\nperspective based on cognitive appraisal theory with a progress pipeline:\nStimulus $\\rightarrow$ Evaluation $\\rightarrow$ Reaction $\\rightarrow$ Stress\nState, guiding LLMs to provide comprehensive reasoning explanations. We further\nstudy the benefits brought by the proposed Cognition Chain format by utilising\nit as a synthetic dataset generation template for LLMs instruction-tuning and\nintroduce CogInstruct, an instruction-tuning dataset for stress detection. This\ndataset is developed using a three-stage self-reflective annotation pipeline\nthat enables LLMs to autonomously generate and refine instructional data. By\ninstruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable\nstress detection model. Evaluations demonstrate that CogLLM achieves\noutstanding performance while enhancing explainability. Our work contributes a\nnovel approach by integrating cognitive theories into LLM reasoning processes,\noffering a promising direction for future explainable AI research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14009v1",
    "published_date": "2024-12-18 16:26:47 UTC",
    "updated_date": "2024-12-18 16:26:47 UTC"
  },
  {
    "arxiv_id": "2412.13998v1",
    "title": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with Neural Processes",
    "authors": [
      "Katarzyna Kobalczyk",
      "Claudio Fanconi",
      "Hao Sun",
      "Mihaela van der Schaar"
    ],
    "abstract": "As large language models (LLMs) become increasingly embedded in everyday\napplications, ensuring their alignment with the diverse preferences of\nindividual users has become a critical challenge. Currently deployed approaches\ntypically assume homogeneous user objectives and rely on single-objective\nfine-tuning. However, human preferences are inherently heterogeneous,\ninfluenced by various unobservable factors, leading to conflicting signals in\npreference data. Existing solutions addressing this diversity often require\ncostly datasets labelled for specific objectives and involve training multiple\nreward models or LLM policies, which is computationally expensive and\nimpractical. In this work, we present a novel framework for few-shot steerable\nalignment, where users' underlying preferences are inferred from a small sample\nof their choices. To achieve this, we extend the Bradley-Terry-Luce model to\nhandle heterogeneous preferences with unobserved variability factors and\npropose its practical implementation for reward modelling and LLM fine-tuning.\nThanks to our proposed approach of functional parameter-space conditioning,\nLLMs trained with our framework can be adapted to individual preferences at\ninference time, generating outputs over a continuum of behavioural modes. We\nempirically validate the effectiveness of methods, demonstrating their ability\nto capture and align with diverse human preferences in a data-efficient manner.\nOur code is made available at:\nhttps://github.com/kasia-kobalczyk/few-shot-steerable-alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13998v1",
    "published_date": "2024-12-18 16:14:59 UTC",
    "updated_date": "2024-12-18 16:14:59 UTC"
  },
  {
    "arxiv_id": "2412.15283v1",
    "title": "Channel Merging: Preserving Specialization for Merged Experts",
    "authors": [
      "Mingyang Zhang",
      "Jing Liu",
      "Ganggui Ding",
      "Xinyi Yu",
      "Linlin Ou",
      "Bohan Zhuang"
    ],
    "abstract": "Lately, the practice of utilizing task-specific fine-tuning has been\nimplemented to improve the performance of large language models (LLM) in\nsubsequent tasks. Through the integration of diverse LLMs, the overall\ncompetency of LLMs is significantly boosted. Nevertheless, traditional ensemble\nmethods are notably memory-intensive, necessitating the simultaneous loading of\nall specialized models into GPU memory. To address the inefficiency, model\nmerging strategies have emerged, merging all LLMs into one model to reduce the\nmemory footprint during inference. Despite these advances, model merging often\nleads to parameter conflicts and performance decline as the number of experts\nincreases. Previous methods to mitigate these conflicts include post-pruning\nand partial merging. However, both approaches have limitations, particularly in\nterms of performance and storage efficiency when merged experts increase. To\naddress these challenges, we introduce Channel Merging, a novel strategy\ndesigned to minimize parameter conflicts while enhancing storage efficiency.\nThis method clusters and merges channel parameters based on their similarity to\nform several groups offline. By ensuring that only highly similar parameters\nare merged within each group, it significantly reduces parameter conflicts.\nDuring inference, we can instantly look up the expert parameters from the\nmerged groups, preserving specialized knowledge. Our experiments demonstrate\nthat Channel Merging consistently delivers high performance, matching unmerged\nmodels in tasks like English and Chinese reasoning, mathematical reasoning, and\ncode generation. Moreover, it obtains results comparable to model ensemble with\njust 53% parameters when used with a task-specific router.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15283v1",
    "published_date": "2024-12-18 16:07:44 UTC",
    "updated_date": "2024-12-18 16:07:44 UTC"
  },
  {
    "arxiv_id": "2412.13964v1",
    "title": "DODGE: Ontology-Aware Risk Assessment via Object-Oriented Disruption Graphs",
    "authors": [
      "Stefano M. Nicoletti",
      "E. Moritz Hahn",
      "Mattia Fumagalli",
      "Giancarlo Guizzardi",
      "Mariëlle Stoelinga"
    ],
    "abstract": "When considering risky events or actions, we must not downplay the role of\ninvolved objects: a charged battery in our phone averts the risk of being\nstranded in the desert after a flat tyre, and a functional firewall mitigates\nthe risk of a hacker intruding the network. The Common Ontology of Value and\nRisk (COVER) highlights how the role of objects and their relationships remains\npivotal to performing transparent, complete and accountable risk assessment. In\nthis paper, we operationalize some of the notions proposed by COVER -- such as\nparthood between objects and participation of objects in events/actions -- by\npresenting a new framework for risk assessment: DODGE. DODGE enriches the\nexpressivity of vetted formal models for risk -- i.e., fault trees and attack\ntrees -- by bridging the disciplines of ontology and formal methods into an\nontology-aware formal framework composed by a more expressive modelling\nformalism, Object-Oriented Disruption Graphs (ODGs), logic (ODGLog) and an\nintermediate query language (ODGLang). With these, DODGE allows risk assessors\nto pose questions about disruption propagation, disruption likelihood and risk\nlevels, keeping the fundamental role of objects at risk always in sight.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13964v1",
    "published_date": "2024-12-18 15:44:04 UTC",
    "updated_date": "2024-12-18 15:44:04 UTC"
  },
  {
    "arxiv_id": "2412.13962v1",
    "title": "Threshold UCT: Cost-Constrained Monte Carlo Tree Search with Pareto Curves",
    "authors": [
      "Martin Kurečka",
      "Václav Nevyhoštěný",
      "Petr Novotný",
      "Vít Unčovský"
    ],
    "abstract": "Constrained Markov decision processes (CMDPs), in which the agent optimizes\nexpected payoffs while keeping the expected cost below a given threshold, are\nthe leading framework for safe sequential decision making under stochastic\nuncertainty. Among algorithms for planning and learning in CMDPs, methods based\non Monte Carlo tree search (MCTS) have particular importance due to their\nefficiency and extendibility to more complex frameworks (such as partially\nobservable settings and games). However, current MCTS-based methods for CMDPs\neither struggle with finding safe (i.e., constraint-satisfying) policies, or\nare too conservative and do not find valuable policies. We introduce Threshold\nUCT (T-UCT), an online MCTS-based algorithm for CMDP planning. Unlike previous\nMCTS-based CMDP planners, T-UCT explicitly estimates Pareto curves of\ncost-utility trade-offs throughout the search tree, using these together with a\nnovel action selection and threshold update rules to seek safe and valuable\npolicies. Our experiments demonstrate that our approach significantly\noutperforms state-of-the-art methods from the literature.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13962v1",
    "published_date": "2024-12-18 15:41:47 UTC",
    "updated_date": "2024-12-18 15:41:47 UTC"
  },
  {
    "arxiv_id": "2412.15282v1",
    "title": "A Systematic Examination of Preference Learning through the Lens of Instruction-Following",
    "authors": [
      "Joongwon Kim",
      "Anirudh Goyal",
      "Aston Zhang",
      "Bo Xiong",
      "Rui Hou",
      "Melanie Kambadur",
      "Dhruv Mahajan",
      "Hannaneh Hajishirzi",
      "Liang Tan"
    ],
    "abstract": "Preference learning is a widely adopted post-training technique that aligns\nlarge language models (LLMs) to human preferences and improves specific\ndownstream task capabilities. In this work we systematically investigate how\nspecific attributes of preference datasets affect the alignment and downstream\nperformance of LLMs in instruction-following tasks. We use a novel synthetic\ndata generation pipeline to generate 48,000 unique instruction-following\nprompts with combinations of 23 verifiable constraints that enable fine-grained\nand automated quality assessments of model responses. With our synthetic\nprompts, we use two preference dataset curation methods - rejection sampling\n(RS) and Monte Carlo Tree Search (MCTS) - to obtain pairs of (chosen, rejected)\nresponses. Then, we perform experiments investigating the effects of (1) the\npresence of shared prefixes between the chosen and rejected responses, (2) the\ncontrast and quality of the chosen, rejected responses and (3) the complexity\nof the training prompts. Our experiments reveal that shared prefixes in\npreference pairs, as generated by MCTS, provide marginal but consistent\nimprovements and greater stability across challenging training configurations.\nHigh-contrast preference pairs generally outperform low-contrast pairs;\nhowever, combining both often yields the best performance by balancing\ndiversity and learning efficiency. Additionally, training on prompts of\nmoderate difficulty leads to better generalization across tasks, even for more\ncomplex evaluation scenarios, compared to overly challenging prompts. Our\nfindings provide actionable insights into optimizing preference data curation\nfor instruction-following tasks, offering a scalable and effective framework\nfor enhancing LLM training and alignment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.15282v1",
    "published_date": "2024-12-18 15:38:39 UTC",
    "updated_date": "2024-12-18 15:38:39 UTC"
  },
  {
    "arxiv_id": "2412.13952v1",
    "title": "Prompting Strategies for Enabling Large Language Models to Infer Causation from Correlation",
    "authors": [
      "Eleni Sgouritsa",
      "Virginia Aglietti",
      "Yee Whye Teh",
      "Arnaud Doucet",
      "Arthur Gretton",
      "Silvia Chiappa"
    ],
    "abstract": "The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13952v1",
    "published_date": "2024-12-18 15:32:27 UTC",
    "updated_date": "2024-12-18 15:32:27 UTC"
  },
  {
    "arxiv_id": "2412.13943v1",
    "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process",
    "authors": [
      "Gereziher Adhane",
      "Mohammad Mahdi Dehshibi",
      "Dennis Vetter",
      "David Masip",
      "Gemma Roig"
    ],
    "abstract": "Knowledge distillation (KD) remains challenging due to the opaque nature of\nthe knowledge transfer process from a Teacher to a Student, making it difficult\nto address certain issues related to KD. To address this, we proposed UniCAM, a\nnovel gradient-based visual explanation method, which effectively interprets\nthe knowledge learned during KD. Our experimental results demonstrate that with\nthe guidance of the Teacher's knowledge, the Student model becomes more\nefficient, learning more relevant features while discarding those that are not\nrelevant. We refer to the features learned with the Teacher's guidance as\ndistilled features and the features irrelevant to the task and ignored by the\nStudent as residual features. Distilled features focus on key aspects of the\ninput, such as textures and parts of objects. In contrast, residual features\ndemonstrate more diffused attention, often targeting irrelevant areas,\nincluding the backgrounds of the target objects. In addition, we proposed two\nnovel metrics: the feature similarity score (FSS) and the relevance score (RS),\nwhich quantify the relevance of the distilled knowledge. Experiments on the\nCIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two\nmetrics offer valuable insights to explain the KD process.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV'25). Includes 5 pages of supplementary material",
    "pdf_url": "http://arxiv.org/pdf/2412.13943v1",
    "published_date": "2024-12-18 15:25:36 UTC",
    "updated_date": "2024-12-18 15:25:36 UTC"
  },
  {
    "arxiv_id": "2412.13935v1",
    "title": "Spatio-Temporal Forecasting of PM2.5 via Spatial-Diffusion guided Encoder-Decoder Architecture",
    "authors": [
      "Malay Pandey",
      "Vaishali Jain",
      "Nimit Godhani",
      "Sachchida Nand Tripathi",
      "Piyush Rai"
    ],
    "abstract": "In many problem settings that require spatio-temporal forecasting, the values\nin the time-series not only exhibit spatio-temporal correlations but are also\ninfluenced by spatial diffusion across locations. One such example is\nforecasting the concentration of fine particulate matter (PM2.5) in the\natmosphere which is influenced by many complex factors, the most important ones\nbeing diffusion due to meteorological factors as well as transport across vast\ndistances over a period of time. We present a novel Spatio-Temporal Graph\nNeural Network architecture, that specifically captures these dependencies to\nforecast the PM2.5 concentration. Our model is based on an encoder-decoder\narchitecture where the encoder and decoder parts leverage gated recurrent units\n(GRU) augmented with a graph neural network (TransformerConv) to account for\nspatial diffusion. Our model can also be seen as a generalization of various\nexisting models for time-series or spatio-temporal forecasting. We demonstrate\nthe model's effectiveness on two real-world PM2.5 datasets: (1) data collected\nby us using a recently deployed network of low-cost PM$_{2.5}$ sensors from 511\nlocations spanning the entirety of the Indian state of Bihar over a period of\none year, and (2) another publicly available dataset that covers severely\npolluted regions from China for a period of 4 years. Our experimental results\nshow our model's impressive ability to account for both spatial as well as\ntemporal dependencies precisely.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures, International Conference on Data Science and\n  Management of Data (CODS-COMAD), IIT Jodhpur, 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.13935v1",
    "published_date": "2024-12-18 15:18:12 UTC",
    "updated_date": "2024-12-18 15:18:12 UTC"
  },
  {
    "arxiv_id": "2412.13922v1",
    "title": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages: A Case Study on Basque",
    "authors": [
      "Ander Corral",
      "Ixak Sarasua",
      "Xabier Saralegi"
    ],
    "abstract": "Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13922v1",
    "published_date": "2024-12-18 15:05:59 UTC",
    "updated_date": "2024-12-18 15:05:59 UTC"
  },
  {
    "arxiv_id": "2412.14222v1",
    "title": "A Survey on Large Language Model-based Agents for Statistics and Data Science",
    "authors": [
      "Maojun Sun",
      "Ruijian Han",
      "Binyan Jiang",
      "Houduo Qi",
      "Defeng Sun",
      "Yancheng Yuan",
      "Jian Huang"
    ],
    "abstract": "In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.OT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14222v1",
    "published_date": "2024-12-18 15:03:26 UTC",
    "updated_date": "2024-12-18 15:03:26 UTC"
  },
  {
    "arxiv_id": "2412.13912v1",
    "title": "Energy-Efficient SLAM via Joint Design of Sensing, Communication, and Exploration Speed",
    "authors": [
      "Zidong Han",
      "Ruibo Jin",
      "Xiaoyang Li",
      "Bingpeng Zhou",
      "Qinyu Zhang",
      "Yi Gong"
    ],
    "abstract": "To support future spatial machine intelligence applications, lifelong\nsimultaneous localization and mapping (SLAM) has drawn significant attentions.\nSLAM is usually realized based on various types of mobile robots performing\nsimultaneous and continuous sensing and communication. This paper focuses on\nanalyzing the energy efficiency of robot operation for lifelong SLAM by jointly\nconsidering sensing, communication and mechanical factors. The system model is\nbuilt based on a robot equipped with a 2D light detection and ranging (LiDAR)\nand an odometry. The cloud point raw data as well as the odometry data are\nwirelessly transmitted to data center where real-time map reconstruction is\nrealized based on an unsupervised deep learning based method. The sensing\nduration, transmit power, transmit duration and exploration speed are jointly\noptimized to minimize the energy consumption. Simulations and experiments\ndemonstrate the performance of our proposed method.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13912v1",
    "published_date": "2024-12-18 14:53:10 UTC",
    "updated_date": "2024-12-18 14:53:10 UTC"
  },
  {
    "arxiv_id": "2412.16226v1",
    "title": "Quantified Linear and Polynomial Arithmetic Satisfiability via Template-based Skolemization",
    "authors": [
      "Krishnendu Chatterjee",
      "Ehsan Kafshdar Goharshady",
      "Mehrdad Karrabi",
      "Harshit J Motwani",
      "Maximilian Seeliger",
      "Đorđe Žikelić"
    ],
    "abstract": "The problem of checking satisfiability of linear real arithmetic (LRA) and\nnon-linear real arithmetic (NRA) formulas has broad applications, in\nparticular, they are at the heart of logic-related applications such as logic\nfor artificial intelligence, program analysis, etc. While there has been much\nwork on checking satisfiability of unquantified LRA and NRA formulas, the\nproblem of checking satisfiability of quantified LRA and NRA formulas remains a\nsignificant challenge. The main bottleneck in the existing methods is a\ncomputationally expensive quantifier elimination step. In this work, we propose\na novel method for efficient quantifier elimination in quantified LRA and NRA\nformulas. We propose a template-based Skolemization approach, where we\nautomatically synthesize linear/polynomial Skolem functions in order to\neliminate quantifiers in the formula. The key technical ingredients in our\napproach are Positivstellens\\\"atze theorems from algebraic geometry, which\nallow for an efficient manipulation of polynomial inequalities. Our method\noffers a range of appealing theoretical properties combined with a strong\npractical performance. On the theory side, our method is sound, semi-complete,\nand runs in subexponential time and polynomial space, as opposed to existing\nsound and complete quantifier elimination methods that run in\ndoubly-exponential time and at least exponential space. On the practical side,\nour experiments show superior performance compared to state-of-the-art SMT\nsolvers in terms of the number of solved instances and runtime, both on LRA and\non NRA benchmarks.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.16226v1",
    "published_date": "2024-12-18 14:37:15 UTC",
    "updated_date": "2024-12-18 14:37:15 UTC"
  },
  {
    "arxiv_id": "2412.16225v2",
    "title": "Bayesian Critique-Tune-Based Reinforcement Learning with Adaptive Pressure for Multi-Intersection Traffic Signal Control",
    "authors": [
      "Wenchang Duan",
      "Zhenguo Gao",
      "Jiwan He",
      "Jinguo Xian"
    ],
    "abstract": "Adaptive Traffic Signal Control (ATSC) system is a critical component of\nintelligent transportation, with the capability to significantly alleviate\nurban traffic congestion. Although reinforcement learning (RL)-based methods\nhave demonstrated promising performance in achieving ATSC, existing methods are\nstill prone to making unreasonable policies. Therefore, this paper proposes a\nnovel Bayesian Critique-Tune-Based Reinforcement Learning with Adaptive\nPressure for multi-intersection signal control (BCT-APLight). In BCT-APLight,\nthe Critique-Tune (CT) framework, a two-layer Bayesian structure is designed to\nrefine the excessive trust of RL policies. Specifically, the Bayesian\ninference-based Critique Layer provides effective evaluations of the\ncredibility of policies; the Bayesian decision-based Tune Layer fine-tunes\npolicies by minimizing the posterior risks when the evaluations are negative.\nMeanwhile, an attention-based Adaptive Pressure (AP) mechanism is designed to\neffectively weight the vehicle queues in each lane, thereby enhancing the\nrationality of traffic movement representation within the network. Equipped\nwith the CT framework and AP mechanism, BCT-APLight effectively enhances the\nreasonableness of RL policies. Extensive experiments conducted with a simulator\nacross a range of intersection layouts demonstrate that BCT-APLight is superior\nto other state-of-the-art (SOTA) methods on seven real-world datasets.\nSpecifically, BCT-APLight decreases average queue length by\n\\textbf{\\(\\boldsymbol{9.60\\%}\\)} and average waiting time by\n\\textbf{\\(\\boldsymbol{15.28\\%}\\)}.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16225v2",
    "published_date": "2024-12-18 14:33:25 UTC",
    "updated_date": "2024-12-25 08:24:00 UTC"
  },
  {
    "arxiv_id": "2412.13888v1",
    "title": "Resource Constrained Pathfinding with Enhanced Bidirectional A* Search",
    "authors": [
      "Saman Ahmadi",
      "Andrea Raith",
      "Guido Tack",
      "Mahdi Jalili"
    ],
    "abstract": "The classic Resource Constrained Shortest Path (RCSP) problem aims to find a\ncost optimal path between a pair of nodes in a network such that the resources\nused in the path are within a given limit. Having been studied for over a\ndecade, RCSP has seen recent solutions that utilize heuristic-guided search to\nsolve the constrained problem faster. Building upon the bidirectional A* search\nparadigm, this research introduces a novel constrained search framework that\nuses efficient pruning strategies to allow for accelerated and effective RCSP\nsearch in large-scale networks. Results show that, compared to the state of the\nart, our enhanced framework can significantly reduce the constrained search\ntime, achieving speed-ups of over to two orders of magnitude.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 3 figures, 2 tables, The 39th Annual AAAI Conference on\n  Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2412.13888v1",
    "published_date": "2024-12-18 14:29:40 UTC",
    "updated_date": "2024-12-18 14:29:40 UTC"
  },
  {
    "arxiv_id": "2412.13881v1",
    "title": "Understanding and Analyzing Model Robustness and Knowledge-Transfer in Multilingual Neural Machine Translation using TX-Ray",
    "authors": [
      "Vageesh Saxena",
      "Sharid Loáiciga",
      "Nils Rethmeier"
    ],
    "abstract": "Neural networks have demonstrated significant advancements in Neural Machine\nTranslation (NMT) compared to conventional phrase-based approaches. However,\nMultilingual Neural Machine Translation (MNMT) in extremely low-resource\nsettings remains underexplored. This research investigates how knowledge\ntransfer across languages can enhance MNMT in such scenarios. Using the Tatoeba\ntranslation challenge dataset from Helsinki NLP, we perform English-German,\nEnglish-French, and English-Spanish translations, leveraging minimal parallel\ndata to establish cross-lingual mappings. Unlike conventional methods relying\non extensive pre-training for specific language pairs, we pre-train our model\non English-English translations, setting English as the source language for all\ntasks. The model is fine-tuned on target language pairs using joint multi-task\nand sequential transfer learning strategies. Our work addresses three key\nquestions: (1) How can knowledge transfer across languages improve MNMT in\nextremely low-resource scenarios? (2) How does pruning neuron knowledge affect\nmodel generalization, robustness, and catastrophic forgetting? (3) How can\nTX-Ray interpret and quantify knowledge transfer in trained models? Evaluation\nusing BLEU-4 scores demonstrates that sequential transfer learning outperforms\nbaselines on a 40k parallel sentence corpus, showcasing its efficacy. However,\npruning neuron knowledge degrades performance, increases catastrophic\nforgetting, and fails to improve robustness or generalization. Our findings\nprovide valuable insights into the potential and limitations of knowledge\ntransfer and pruning in MNMT for extremely low-resource settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "103 pages, Master's thesis",
    "pdf_url": "http://arxiv.org/pdf/2412.13881v1",
    "published_date": "2024-12-18 14:21:58 UTC",
    "updated_date": "2024-12-18 14:21:58 UTC"
  },
  {
    "arxiv_id": "2412.13879v3",
    "title": "Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings",
    "authors": [
      "Yuanhe Zhang",
      "Zhenhong Zhou",
      "Wei Zhang",
      "Xinyue Wang",
      "Xiaojun Jia",
      "Yang Liu",
      "Sen Su"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks yet still are vulnerable to external threats, particularly LLM\nDenial-of-Service (LLM-DoS) attacks. Specifically, LLM-DoS attacks aim to\nexhaust computational resources and block services. However, existing studies\npredominantly focus on white-box attacks, leaving black-box scenarios\nunderexplored. In this paper, we introduce Auto-Generation for LLM-DoS\n(AutoDoS) attack, an automated algorithm designed for black-box LLMs. AutoDoS\nconstructs the DoS Attack Tree and expands the node coverage to achieve\neffectiveness under black-box conditions. By transferability-driven iterative\noptimization, AutoDoS could work across different models in one prompt.\nFurthermore, we reveal that embedding the Length Trojan allows AutoDoS to\nbypass existing defenses more effectively. Experimental results show that\nAutoDoS significantly amplifies service response latency by over\n250$\\times\\uparrow$, leading to severe resource consumption in terms of GPU\nutilization and memory usage. Our work provides a new perspective on LLM-DoS\nattacks and security defenses. Our code is available at\nhttps://github.com/shuita2333/AutoDoS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 8 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.13879v3",
    "published_date": "2024-12-18 14:19:23 UTC",
    "updated_date": "2025-02-18 06:08:19 UTC"
  },
  {
    "arxiv_id": "2412.13877v2",
    "title": "RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation",
    "authors": [
      "Kun Wu",
      "Chengkai Hou",
      "Jiaming Liu",
      "Zhengping Che",
      "Xiaozhu Ju",
      "Zhuqin Yang",
      "Meng Li",
      "Yinuo Zhao",
      "Zhiyuan Xu",
      "Guang Yang",
      "Shichao Fan",
      "Xinhua Wang",
      "Fei Liao",
      "Zhen Zhao",
      "Guangyu Li",
      "Zhao Jin",
      "Lecheng Wang",
      "Jilei Mao",
      "Ning Liu",
      "Pei Ren",
      "Qiang Zhang",
      "Yaoxu Lyu",
      "Mengzhen Liu",
      "Jingyang He",
      "Yulin Luo",
      "Zeyu Gao",
      "Chenxuan Li",
      "Chenyang Gu",
      "Yankai Fu",
      "Di Wu",
      "Xingyu Wang",
      "Sixiang Chen",
      "Zhenyu Wang",
      "Pengju An",
      "Siyuan Qian",
      "Shanghang Zhang",
      "Jian Tang"
    ],
    "abstract": "In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative\nData for Robot Manipulation), a dataset containing 107k demonstration\ntrajectories across 479 diverse tasks involving 96 object classes. RoboMIND is\ncollected through human teleoperation and encompasses comprehensive\nrobotic-related information, including multi-view observations, proprioceptive\nrobot state information, and linguistic task descriptions. To ensure data\nconsistency and reliability for imitation learning, RoboMIND is built on a\nunified data collection platform and a standardized protocol, covering four\ndistinct robotic embodiments: the Franka Emika Panda, the UR5e, the AgileX\ndual-arm robot, and a humanoid robot with dual dexterous hands. Our dataset\nalso includes 5k real-world failure demonstrations, each accompanied by\ndetailed causes, enabling failure reflection and correction during policy\nlearning. Additionally, we created a digital twin environment in the Isaac Sim\nsimulator, replicating the real-world tasks and assets, which facilitates the\nlow-cost collection of additional training data and enables efficient\nevaluation. To demonstrate the quality and diversity of our dataset, we\nconducted extensive experiments using various imitation learning methods for\nsingle-task settings and state-of-the-art Vision-Language-Action (VLA) models\nfor multi-task scenarios. By leveraging RoboMIND, the VLA models achieved high\nmanipulation success rates and demonstrated strong generalization capabilities.\nTo the best of our knowledge, RoboMIND is the largest multi-embodiment\nteleoperation dataset collected on a unified platform, providing large-scale\nand high-quality robotic training data. Our project is at\nhttps://x-humanoid-robomind.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13877v2",
    "published_date": "2024-12-18 14:17:16 UTC",
    "updated_date": "2025-02-14 14:32:16 UTC"
  },
  {
    "arxiv_id": "2412.14219v2",
    "title": "A Survey on Inference Optimization Techniques for Mixture of Experts Models",
    "authors": [
      "Jiacheng Liu",
      "Peng Tang",
      "Wenfeng Wang",
      "Yuhang Ren",
      "Xiaofeng Hou",
      "Pheng-Ann Heng",
      "Minyi Guo",
      "Chao Li"
    ],
    "abstract": "The emergence of large-scale Mixture of Experts (MoE) models represents a\nsignificant advancement in artificial intelligence, offering enhanced model\ncapacity and computational efficiency through conditional computation. However,\ndeploying and running inference on these models presents significant challenges\nin computational resources, latency, and energy efficiency. This comprehensive\nsurvey analyzes optimization techniques for MoE models across the entire system\nstack. We first establish a taxonomical framework that categorizes optimization\napproaches into model-level, system-level, and hardware-level optimizations. At\nthe model level, we examine architectural innovations including efficient\nexpert design, attention mechanisms, various compression techniques such as\npruning, quantization, and knowledge distillation, as well as algorithm\nimprovement including dynamic routing strategies and expert merging methods. At\nthe system level, we investigate distributed computing approaches, load\nbalancing mechanisms, and efficient scheduling algorithms that enable scalable\ndeployment. Furthermore, we delve into hardware-specific optimizations and\nco-design strategies that maximize throughput and energy efficiency. This\nsurvey provides both a structured overview of existing solutions and identifies\nkey challenges and promising research directions in MoE inference optimization.\nTo facilitate ongoing updates and the sharing of cutting-edge advances in MoE\ninference optimization research, we have established a repository accessible at\nhttps://github.com/MoE-Inf/awesome-moe-inference/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2412.14219v2",
    "published_date": "2024-12-18 14:11:15 UTC",
    "updated_date": "2025-01-22 03:33:25 UTC"
  },
  {
    "arxiv_id": "2412.13866v1",
    "title": "SHAP scores fail pervasively even when Lipschitz succeeds",
    "authors": [
      "Olivier Letoffe",
      "Xuanxiang Huang",
      "Joao Marques-Silva"
    ],
    "abstract": "The ubiquitous use of Shapley values in eXplainable AI (XAI) has been\ntriggered by the tool SHAP, and as a result are commonly referred to as SHAP\nscores. Recent work devised examples of machine learning (ML) classifiers for\nwhich the computed SHAP scores are thoroughly unsatisfactory, by allowing human\ndecision-makers to be misled. Nevertheless, such examples could be perceived as\nsomewhat artificial, since the selected classes must be interpreted as numeric.\nFurthermore, it was unclear how general were the issues identified with SHAP\nscores. This paper answers these criticisms. First, the paper shows that for\nBoolean classifiers there are arbitrarily many examples for which the SHAP\nscores must be deemed unsatisfactory. Second, the paper shows that the issues\nwith SHAP scores are also observed in the case of regression models. In\naddition, the paper studies the class of regression models that respect\nLipschitz continuity, a measure of a function's rate of change that finds\nimportant recent uses in ML, including model robustness. Concretely, the paper\nshows that the issues with SHAP scores occur even for regression models that\nrespect Lipschitz continuity. Finally, the paper shows that the same issues are\nguaranteed to exist for arbitrarily differentiable regression models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2405.00076",
    "pdf_url": "http://arxiv.org/pdf/2412.13866v1",
    "published_date": "2024-12-18 14:02:15 UTC",
    "updated_date": "2024-12-18 14:02:15 UTC"
  },
  {
    "arxiv_id": "2412.13858v1",
    "title": "IDEQ: an improved diffusion model for the TSP",
    "authors": [
      "Mickael Basson",
      "Philippe Preux"
    ],
    "abstract": "We investigate diffusion models to solve the Traveling Salesman Problem.\nBuilding on the recent DIFUSCO and T2TCO approaches, we propose IDEQ. IDEQ\nimproves the quality of the solutions by leveraging the constrained structure\nof the state space of the TSP. Another key component of IDEQ consists in\nreplacing the last stages of DIFUSCO curriculum learning by considering a\nuniform distribution over the Hamiltonian tours whose orbits by the 2-opt\noperator converge to the optimal solution as the training objective. Our\nexperiments show that IDEQ improves the state of the art for such neural\nnetwork based techniques on synthetic instances. More importantly, our\nexperiments show that IDEQ performs very well on the instances of the TSPlib, a\nreference benchmark in the TSP community: it closely matches the performance of\nthe best heuristics, LKH3, being even able to obtain better solutions than LKH3\non 2 instances of the TSPlib defined on 1577 and 3795 cities. IDEQ obtains 0.3%\noptimality gap on TSP instances made of 500 cities, and 0.5% on TSP instances\nwith 1000 cities. This sets a new SOTA for neural based methods solving the\nTSP. Moreover, IDEQ exhibits a lower variance and better scales-up with the\nnumber of cities with regards to DIFUSCO and T2TCO.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13858v1",
    "published_date": "2024-12-18 13:52:50 UTC",
    "updated_date": "2024-12-18 13:52:50 UTC"
  },
  {
    "arxiv_id": "2412.14218v1",
    "title": "Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs",
    "authors": [
      "Jiaming Yu",
      "Le Liang",
      "Chongtao Guo",
      "Ziyang Guo",
      "Shi Jin",
      "Geoffrey Ye Li"
    ],
    "abstract": "This paper investigates the use of multi-agent reinforcement learning (MARL)\nto address distributed channel access in wireless local area networks. In\nparticular, we consider the challenging yet more practical case where the\nagents heterogeneously adopt value-based or policy-based reinforcement learning\nalgorithms to train the model. We propose a heterogeneous MARL training\nframework, named QPMIX, which adopts a centralized training with distributed\nexecution paradigm to enable heterogeneous agents to collaborate. Moreover, we\ntheoretically prove the convergence of the proposed heterogeneous MARL method\nwhen using the linear value function approximation. Our method maximizes the\nnetwork throughput and ensures fairness among stations, therefore, enhancing\nthe overall network performance. Simulation results demonstrate that the\nproposed QPMIX algorithm improves throughput, mean delay, delay jitter, and\ncollision rates compared with conventional carrier-sense multiple access with\ncollision avoidance in the saturated traffic scenario. Furthermore, the QPMIX\nis shown to be robust in unsaturated and delay-sensitive traffic scenarios, and\npromotes cooperation among heterogeneous agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14218v1",
    "published_date": "2024-12-18 13:50:31 UTC",
    "updated_date": "2024-12-18 13:50:31 UTC"
  },
  {
    "arxiv_id": "2412.13851v1",
    "title": "From approximation error to optimality gap -- Explaining the performance impact of opportunity cost approximation in integrated demand management and vehicle routing",
    "authors": [
      "David Fleckenstein",
      "Robert Klein",
      "Vienna Klein",
      "Claudius Steinhardt"
    ],
    "abstract": "The widespread adoption of digital distribution channels both enables and\nforces more and more logistical service providers to manage booking processes\nactively to maintain competitiveness. As a result, their operational planning\nis no longer limited to solving vehicle routing problems. Instead, demand\nmanagement decisions and vehicle routing decisions are optimized integratively\nwith the aim of maximizing revenue and minimizing fulfillment cost. The\nresulting integrated demand management and vehicle routing problems (i-DMVRPs)\ncan be formulated as Markov decision process models and, theoretically, can be\nsolved via the well-known Bellman equation. Unfortunately, the Bellman equation\nis intractable for realistic-sized instances. Thus, in the literature, i-DMVRPs\nare often addressed via decomposition-based solution approaches involving an\nopportunity cost approximation as a key component. Despite its importance, to\nthe best of our knowledge, there is neither a technique to systematically\nanalyze how the accuracy of the opportunity cost approximation translates into\noverall solution quality nor are there general guidelines on when to apply\nwhich class of approximation approach. In this work, we address this research\ngap by proposing an explainability technique that quantifies and visualizes the\nmagnitude of approximation errors, their immediate impact, and their relevance\nin specific regions of the state space. Exploiting reward decomposition, it\nfurther yields a characterization of different types of approximation errors.\nApplying the technique to a generic i-DMVRP in a full-factorial computational\nstudy and comparing the results with observations in existing literature, we\nshow that the technique contributes to better explaining algorithmic\nperformance and provides guidance for the algorithm selection and development\nprocess.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13851v1",
    "published_date": "2024-12-18 13:46:46 UTC",
    "updated_date": "2024-12-18 13:46:46 UTC"
  },
  {
    "arxiv_id": "2412.13847v1",
    "title": "A Concept-Centric Approach to Multi-Modality Learning",
    "authors": [
      "Yuchong Geng",
      "Ao Tang"
    ],
    "abstract": "In an effort to create a more efficient AI system, we introduce a new\nmulti-modality learning framework that leverages a modality-agnostic concept\nspace possessing abstract knowledge and a set of modality-specific projection\nmodels tailored to process distinct modality inputs and map them onto the\nconcept space. Decoupled from specific modalities and their associated\nprojection models, the concept space focuses on learning abstract knowledge\nthat is universally applicable across modalities. Subsequently, the knowledge\nembedded into the concept space streamlines the learning processes of\nmodality-specific projection models. We evaluate our framework on two popular\ntasks: Image-Text Matching and Visual Question Answering. Our framework\nachieves performance on par with benchmark models while demonstrating more\nefficient learning curves.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13847v1",
    "published_date": "2024-12-18 13:40:21 UTC",
    "updated_date": "2024-12-18 13:40:21 UTC"
  },
  {
    "arxiv_id": "2412.13846v2",
    "title": "From Expectation to Habit: Why Do Software Practitioners Adopt Fairness Toolkits?",
    "authors": [
      "Gianmario Voria",
      "Stefano Lambiase",
      "Maria Concetta Schiavone",
      "Gemma Catolino",
      "Fabio Palomba"
    ],
    "abstract": "As the adoption of machine learning (ML) systems continues to grow across\nindustries, concerns about fairness and bias in these systems have taken center\nstage. Fairness toolkits, designed to mitigate bias in ML models, serve as\ncritical tools for addressing these ethical concerns. However, their adoption\nin the context of software development remains underexplored, especially\nregarding the cognitive and behavioral factors driving their usage. As a deeper\nunderstanding of these factors could be pivotal in refining tool designs and\npromoting broader adoption, this study investigates the factors influencing the\nadoption of fairness toolkits from an individual perspective. Guided by the\nUnified Theory of Acceptance and Use of Technology (UTAUT2), we examined the\nfactors shaping the intention to adopt and actual use of fairness toolkits.\nSpecifically, we employed Partial Least Squares Structural Equation Modeling\n(PLS-SEM) to analyze data from a survey study involving practitioners in the\nsoftware industry. Our findings reveal that performance expectancy and habit\nare the primary drivers of fairness toolkit adoption. These insights suggest\nthat by emphasizing the effectiveness of these tools in mitigating bias and\nfostering habitual use, organizations can encourage wider adoption. Practical\nrecommendations include improving toolkit usability, integrating bias\nmitigation processes into routine development workflows, and providing ongoing\nsupport to ensure professionals see clear benefits from regular use.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13846v2",
    "published_date": "2024-12-18 13:38:28 UTC",
    "updated_date": "2024-12-19 10:22:55 UTC"
  },
  {
    "arxiv_id": "2412.13845v3",
    "title": "Do Language Models Understand Time?",
    "authors": [
      "Xi Ding",
      "Lei Wang"
    ],
    "abstract": "Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond. Our paper's GitHub\nrepository can be found at https://github.com/Darcyddx/Video-LLM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in the Companion Proceedings of the ACM Web\n  Conference (WWW Companion 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.13845v3",
    "published_date": "2024-12-18 13:38:06 UTC",
    "updated_date": "2025-02-24 03:37:59 UTC"
  },
  {
    "arxiv_id": "2412.13844v1",
    "title": "CRM: Retrieval Model with Controllable Condition",
    "authors": [
      "Chi Liu",
      "Jiangxia Cao",
      "Rui Huang",
      "Kuo Cai",
      "Weifeng Ding",
      "Qiang Luo",
      "Kun Gai",
      "Guorui Zhou"
    ],
    "abstract": "Recommendation systems (RecSys) are designed to connect users with relevant\nitems from a vast pool of candidates while aligning with the business goals of\nthe platform. A typical industrial RecSys is composed of two main stages,\nretrieval and ranking: (1) the retrieval stage aims at searching hundreds of\nitem candidates satisfied user interests; (2) based on the retrieved items, the\nranking stage aims at selecting the best dozen items by multiple targets\nestimation for each item candidate, including classification and regression\ntargets. Compared with ranking model, the retrieval model absence of item\ncandidate information during inference, therefore retrieval models are often\ntrained by classification target only (e.g., click-through rate), but failed to\nincorporate regression target (e.g., the expected watch-time), which limit the\neffectiveness of retrieval. In this paper, we propose the Controllable\nRetrieval Model (CRM), which integrates regression information as conditional\nfeatures into the two-tower retrieval paradigm. This modification enables the\nretrieval stage could fulfill the target gap with ranking model, enhancing the\nretrieval model ability to search item candidates satisfied the user interests\nand condition effectively. We validate the effectiveness of CRM through\nreal-world A/B testing and demonstrate its successful deployment in Kuaishou\nshort-video recommendation system, which serves over 400 million users.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13844v1",
    "published_date": "2024-12-18 13:37:36 UTC",
    "updated_date": "2024-12-18 13:37:36 UTC"
  },
  {
    "arxiv_id": "2412.13841v1",
    "title": "AI Perceptions Across Cultures: Similarities and Differences in Expectations, Risks, Benefits, Tradeoffs, and Value in Germany and China",
    "authors": [
      "Philipp Brauner",
      "Felix Glawe",
      "Gian Luca Liehner",
      "Luisa Vervier",
      "Martina Ziefle"
    ],
    "abstract": "As artificial intelligence (AI) continues to advance, understanding public\nperceptions -- including biases, risks, and benefits -- is critical for guiding\nresearch priorities, shaping public discourse, and informing policy. This study\nexplores public mental models of AI using micro scenarios to assess reactions\nto 71 statements about AI's potential future impacts. Drawing on cross-cultural\nsamples from Germany (N=52) and China (N=60), we identify significant\ndifferences in expectations, evaluations, and risk-utility tradeoffs. German\nparticipants tended toward more cautious assessments, whereas Chinese\nparticipants expressed greater optimism regarding AI's societal benefits.\nChinese participants exhibited relatively balanced risk-benefit tradeoffs\n($\\beta=-0.463$ for risk and $\\beta=+0.484$ for benefit, $r^2=.630$). In\ncontrast, German participants showed a stronger emphasis on AI benefits and\nless on risks ($\\beta=-0.337$ for risk and $\\beta=+0.715$ for benefit,\n$r^2=.839$). Visual cognitive maps illustrate these contrasts, offering new\nperspectives on how cultural contexts shape AI acceptance. Our findings\nunderline key factors influencing public perception and provide actionable\ninsights for fostering equitable and culturally sensitive integration of AI\ntechnologies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13841v1",
    "published_date": "2024-12-18 13:34:44 UTC",
    "updated_date": "2024-12-18 13:34:44 UTC"
  },
  {
    "arxiv_id": "2412.13834v1",
    "title": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for Text-to-Image Retrieval",
    "authors": [
      "Giacomo Pacini",
      "Fabio Carrara",
      "Nicola Messina",
      "Nicola Tonellotto",
      "Giuseppe Amato",
      "Fabrizio Falchi"
    ],
    "abstract": "Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "15 pages, 5 figures. To be published as full paper in the Proceedings\n  of the European Conference on Information Retrieval (ECIR) 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13834v1",
    "published_date": "2024-12-18 13:24:09 UTC",
    "updated_date": "2024-12-18 13:24:09 UTC"
  },
  {
    "arxiv_id": "2412.13825v3",
    "title": "MixRec: Heterogeneous Graph Collaborative Filtering",
    "authors": [
      "Lianghao Xia",
      "Meiyan Xie",
      "Yong Xu",
      "Chao Huang"
    ],
    "abstract": "For modern recommender systems, the use of low-dimensional latent\nrepresentations to embed users and items based on their observed interactions\nhas become commonplace. However, many existing recommendation models are\nprimarily designed for coarse-grained and homogeneous interactions, which\nlimits their effectiveness in two critical dimensions. Firstly, these models\nfail to leverage the relational dependencies that exist across different types\nof user behaviors, such as page views, collects, comments, and purchases.\nSecondly, they struggle to capture the fine-grained latent factors that drive\nuser interaction patterns. To address these limitations, we present a\nheterogeneous graph collaborative filtering model MixRec that excels at\ndisentangling users' multi-behavior interaction patterns and uncovering the\nlatent intent factors behind each behavior. Our model achieves this by\nincorporating intent disentanglement and multi-behavior modeling, facilitated\nby a parameterized heterogeneous hypergraph architecture. Furthermore, we\nintroduce a novel contrastive learning paradigm that adaptively explores the\nadvantages of self-supervised data augmentation, thereby enhancing the model's\nresilience against data sparsity and expressiveness with relation\nheterogeneity. To validate the efficacy of MixRec, we conducted extensive\nexperiments on three public datasets. The results clearly demonstrate its\nsuperior performance, significantly outperforming various state-of-the-art\nbaselines. Our model is open-sourced and available at:\nhttps://github.com/HKUDS/MixRec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "This paper is accepted by WSDM'2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13825v3",
    "published_date": "2024-12-18 13:12:36 UTC",
    "updated_date": "2024-12-25 02:33:14 UTC"
  },
  {
    "arxiv_id": "2412.13810v2",
    "title": "CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers",
    "authors": [
      "Dimitrios Mallis",
      "Ahmet Serdar Karadeniz",
      "Sebastian Cavada",
      "Danila Rukhovich",
      "Niki Foteinopoulou",
      "Kseniya Cherenkova",
      "Anis Kacem",
      "Djamila Aouada"
    ],
    "abstract": "We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design.\nOur approach is based on a powerful Vision and Large Language Model (VLLM) as a\nplanner and a tool-augmentation paradigm using CAD-specific tools.\nCAD-Assistant addresses multimodal user queries by generating actions that are\niteratively executed on a Python interpreter equipped with the FreeCAD\nsoftware, accessed via its Python API. Our framework is able to assess the\nimpact of generated CAD commands on geometry and adapts subsequent actions\nbased on the evolving state of the CAD design. We consider a wide range of\nCAD-specific tools including a sketch image parameterizer, rendering modules, a\n2D cross-section generator, and other specialized routines. CAD-Assistant is\nevaluated on multiple CAD benchmarks, where it outperforms VLLM baselines and\nsupervised task-specific methods. Beyond existing benchmarks, we qualitatively\ndemonstrate the potential of tool-augmented VLLMs as general-purpose CAD\nsolvers across diverse workflows.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13810v2",
    "published_date": "2024-12-18 12:57:56 UTC",
    "updated_date": "2025-03-10 11:27:07 UTC"
  },
  {
    "arxiv_id": "2412.13805v1",
    "title": "AI-Powered Algorithm-Centric Quantum Processor Topology Design",
    "authors": [
      "Tian Li",
      "Xiao-Yue Xu",
      "Chen Ding",
      "Tian-Ci Tian",
      "Wei-You Liao",
      "Shuo Zhang",
      "He-Liang Huang"
    ],
    "abstract": "Quantum computing promises to revolutionize various fields, yet the execution\nof quantum programs necessitates an effective compilation process. This\ninvolves strategically mapping quantum circuits onto the physical qubits of a\nquantum processor. The qubits' arrangement, or topology, is pivotal to the\ncircuit's performance, a factor that often defies traditional heuristic or\nmanual optimization methods due to its complexity. In this study, we introduce\na novel approach leveraging reinforcement learning to dynamically tailor qubit\ntopologies to the unique specifications of individual quantum circuits, guiding\nalgorithm-driven quantum processor topology design for reducing the depth of\nmapped circuit, which is particularly critical for the output accuracy on noisy\nquantum processors. Our method marks a significant departure from previous\nmethods that have been constrained to mapping circuits onto a fixed processor\ntopology. Experiments demonstrate that we have achieved notable enhancements in\ncircuit performance, with a minimum of 20\\% reduction in circuit depth in 60\\%\nof the cases examined, and a maximum enhancement of up to 46\\%. Furthermore,\nthe pronounced benefits of our approach in reducing circuit depth become\nincreasingly evident as the scale of the quantum circuits increases, exhibiting\nthe scalability of our method in terms of problem size. This work advances the\nco-design of quantum processor architecture and algorithm mapping, offering a\npromising avenue for future research and development in the field.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13805v1",
    "published_date": "2024-12-18 12:53:16 UTC",
    "updated_date": "2024-12-18 12:53:16 UTC"
  },
  {
    "arxiv_id": "2412.13803v2",
    "title": "M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation",
    "authors": [
      "Zixuan Chen",
      "Jiaxin Li",
      "Liming Tan",
      "Yejie Guo",
      "Junxuan Liang",
      "Cewu Lu",
      "Yong-Lu Li"
    ],
    "abstract": "Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M$^3$-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M$^3$-VOS, yielding several key insights. Notably, current\nappearancebased approaches show significant room for improvement when handling\nobjects with phase transitions. The inherent changes in disorder suggest that\nthe predictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-andplay model that improves its performance by\nreversal refinement. Our data and code will be publicly available at\nhttps://zixuan-chen.github.io/M-cubeVOS.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13803v2",
    "published_date": "2024-12-18 12:50:11 UTC",
    "updated_date": "2024-12-19 12:31:34 UTC"
  },
  {
    "arxiv_id": "2412.13799v1",
    "title": "Enhancing Rhetorical Figure Annotation: An Ontology-Based Web Application with RAG Integration",
    "authors": [
      "Ramona Kühn",
      "Jelena Mitrović",
      "Michael Granitzer"
    ],
    "abstract": "Rhetorical figures play an important role in our communication. They are used\nto convey subtle, implicit meaning, or to emphasize statements. We notice them\nin hate speech, fake news, and propaganda. By improving the systems for\ncomputational detection of rhetorical figures, we can also improve tasks such\nas hate speech and fake news detection, sentiment analysis, opinion mining, or\nargument mining. Unfortunately, there is a lack of annotated data, as well as\nqualified annotators that would help us build large corpora to train machine\nlearning models for the detection of rhetorical figures. The situation is\nparticularly difficult in languages other than English, and for rhetorical\nfigures other than metaphor, sarcasm, and irony. To overcome this issue, we\ndevelop a web application called \"Find your Figure\" that facilitates the\nidentification and annotation of German rhetorical figures. The application is\nbased on the German Rhetorical ontology GRhOOT which we have specially adapted\nfor this purpose. In addition, we improve the user experience with Retrieval\nAugmented Generation (RAG). In this paper, we present the restructuring of the\nontology, the development of the web application, and the built-in RAG\npipeline. We also identify the optimal RAG settings for our application. Our\napproach is one of the first to practically use rhetorical ontologies in\ncombination with RAG and shows promising results.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.13799v1",
    "published_date": "2024-12-18 12:45:55 UTC",
    "updated_date": "2024-12-18 12:45:55 UTC"
  },
  {
    "arxiv_id": "2412.13795v1",
    "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN",
    "authors": [
      "Pengxiang Li",
      "Lu Yin",
      "Shiwei Liu"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13795v1",
    "published_date": "2024-12-18 12:39:53 UTC",
    "updated_date": "2024-12-18 12:39:53 UTC"
  },
  {
    "arxiv_id": "2412.13794v1",
    "title": "MATCHED: Multimodal Authorship-Attribution To Combat Human Trafficking in Escort-Advertisement Data",
    "authors": [
      "Vageesh Saxena",
      "Benjamin Bashpole",
      "Gijs Van Dijck",
      "Gerasimos Spanakis"
    ],
    "abstract": "Human trafficking (HT) remains a critical issue, with traffickers\nincreasingly leveraging online escort advertisements (ads) to advertise victims\nanonymously. Existing detection methods, including Authorship Attribution (AA),\noften center on text-based analyses and neglect the multimodal nature of online\nescort ads, which typically pair text with images. To address this gap, we\nintroduce MATCHED, a multimodal dataset of 27,619 unique text descriptions and\n55,115 unique images collected from the Backpage escort platform across seven\nU.S. cities in four geographical regions. Our study extensively benchmarks\ntext-only, vision-only, and multimodal baselines for vendor identification and\nverification tasks, employing multitask (joint) training objectives that\nachieve superior classification and retrieval performance on in-distribution\nand out-of-distribution (OOD) datasets. Integrating multimodal features further\nenhances this performance, capturing complementary patterns across text and\nimages. While text remains the dominant modality, visual data adds stylistic\ncues that enrich model performance. Moreover, text-image alignment strategies\nlike CLIP and BLIP2 struggle due to low semantic overlap and vague connections\nbetween the modalities of escort ads, with end-to-end multimodal training\nproving more robust. Our findings emphasize the potential of multimodal AA\n(MAA) to combat HT, providing LEAs with robust tools to link ads and disrupt\ntrafficking networks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "40 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.13794v1",
    "published_date": "2024-12-18 12:39:01 UTC",
    "updated_date": "2024-12-18 12:39:01 UTC"
  },
  {
    "arxiv_id": "2412.13781v1",
    "title": "Meta-Reflection: A Feedback-Free Reflection Learning Framework",
    "authors": [
      "Yaoke Wang",
      "Yun Zhu",
      "Xintong Bao",
      "Wenqiao Zhang",
      "Suyang Dai",
      "Kehan Chen",
      "Wenqiang Li",
      "Gang Huang",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "abstract": "Despite the remarkable capabilities of large language models (LLMs) in\nnatural language understanding and reasoning, they often display undesirable\nbehaviors, such as generating hallucinations and unfaithful reasoning. A\nprevalent strategy to mitigate these issues is the use of reflection, which\nrefines responses through an iterative process. However, while promising,\nreflection heavily relies on high-quality external feedback and requires\niterative multi-agent inference processes, thus hindering its practical\napplication. In this paper, we propose Meta-Reflection, a novel feedback-free\nreflection mechanism that necessitates only a single inference pass without\nexternal feedback. Motivated by the human ability to remember and retrieve\nreflections from past experiences when encountering similar problems,\nMeta-Reflection integrates reflective insights into a codebook, allowing the\nhistorical insights to be stored, retrieved, and used to guide LLMs in\nproblem-solving. To thoroughly investigate and evaluate the practicality of\nMeta-Reflection in real-world scenarios, we introduce an industrial e-commerce\nbenchmark named E-commerce Customer Intent Detection (ECID). Extensive\nexperiments conducted on both public datasets and the ECID benchmark highlight\nthe effectiveness and efficiency of our proposed approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13781v1",
    "published_date": "2024-12-18 12:20:04 UTC",
    "updated_date": "2024-12-18 12:20:04 UTC"
  },
  {
    "arxiv_id": "2412.13771v1",
    "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage Alignment and Behavioral Semantic Tokenization",
    "authors": [
      "Guanghan Li",
      "Xun Zhang",
      "Yufei Zhang",
      "Yifan Yin",
      "Guojun Yin",
      "Wei Lin"
    ],
    "abstract": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages, 3 figures, AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13771v1",
    "published_date": "2024-12-18 12:07:58 UTC",
    "updated_date": "2024-12-18 12:07:58 UTC"
  },
  {
    "arxiv_id": "2412.13769v2",
    "title": "QuLTSF: Long-Term Time Series Forecasting with Quantum Machine Learning",
    "authors": [
      "Hari Hara Suthan Chittoor",
      "Paul Robert Griffin",
      "Ariel Neufeld",
      "Jayne Thompson",
      "Mile Gu"
    ],
    "abstract": "Long-term time series forecasting (LTSF) involves predicting a large number\nof future values of a time series based on the past values. This is an\nessential task in a wide range of domains including weather forecasting, stock\nmarket analysis and disease outbreak prediction. Over the decades LTSF\nalgorithms have transitioned from statistical models to deep learning models\nlike transformer models. Despite the complex architecture of transformer based\nLTSF models `Are Transformers Effective for Time Series Forecasting? (Zeng et\nal., 2023)' showed that simple linear models can outperform the\nstate-of-the-art transformer based LTSF models. Recently, quantum machine\nlearning (QML) is evolving as a domain to enhance the capabilities of classical\nmachine learning models. In this paper we initiate the application of QML to\nLTSF problems by proposing QuLTSF, a simple hybrid QML model for multivariate\nLTSF. Through extensive experiments on a widely used weather dataset we show\nthe advantages of QuLTSF over the state-of-the-art classical linear models, in\nterms of reduced mean squared error and mean absolute error.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Published in ICAART 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13769v2",
    "published_date": "2024-12-18 12:06:52 UTC",
    "updated_date": "2025-03-18 09:30:51 UTC"
  },
  {
    "arxiv_id": "2412.13765v2",
    "title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms",
    "authors": [
      "Ali Hamdi",
      "Ahmed Abdelmoneim Mazrou",
      "Mohamed Shaltout"
    ],
    "abstract": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned TXLM-RoBERTa using\nhuman-annotated sentiment datasets to enhance prediction accuracy and utilized\nLLama 3B, and Gemma 9B from Ollama.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13765v2",
    "published_date": "2024-12-18 12:01:53 UTC",
    "updated_date": "2024-12-19 15:50:54 UTC"
  },
  {
    "arxiv_id": "2412.13746v1",
    "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment",
    "authors": [
      "Zhuoran Jin",
      "Hongbang Yuan",
      "Tianyi Men",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 12 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.13746v1",
    "published_date": "2024-12-18 11:28:05 UTC",
    "updated_date": "2024-12-18 11:28:05 UTC"
  },
  {
    "arxiv_id": "2412.13738v1",
    "title": "Uncertainty separation via ensemble quantile regression",
    "authors": [
      "Navid Ansari",
      "Hans-Peter Seidel",
      "Vahid Babaei"
    ],
    "abstract": "This paper introduces a novel and scalable framework for uncertainty\nestimation and separation with applications in data driven modeling in science\nand engineering tasks where reliable uncertainty quantification is critical.\nLeveraging an ensemble of quantile regression (E-QR) models, our approach\nenhances aleatoric uncertainty estimation while preserving the quality of\nepistemic uncertainty, surpassing competing methods, such as Deep Ensembles\n(DE) and Monte Carlo (MC) dropout. To address challenges in separating\nuncertainty types, we propose an algorithm that iteratively improves separation\nthrough progressive sampling in regions of high uncertainty. Our framework is\nscalable to large datasets and demonstrates superior performance on synthetic\nbenchmarks, offering a robust tool for uncertainty quantification in\ndata-driven applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13738v1",
    "published_date": "2024-12-18 11:15:32 UTC",
    "updated_date": "2024-12-18 11:15:32 UTC"
  },
  {
    "arxiv_id": "2412.13737v1",
    "title": "On the Compression of Language Models for Code: An Empirical Study on CodeBERT",
    "authors": [
      "Giordano d'Aloisio",
      "Luca Traini",
      "Federica Sarro",
      "Antinisca Di Marco"
    ],
    "abstract": "Language models have proven successful across a wide range of software\nengineering tasks, but their significant computational costs often hinder their\npractical adoption. To address this challenge, researchers have begun applying\nvarious compression strategies to improve the efficiency of language models for\ncode. These strategies aim to optimize inference latency and memory usage,\nthough often at the cost of reduced model effectiveness. However, there is\nstill a significant gap in understanding how these strategies influence the\nefficiency and effectiveness of language models for code. Here, we empirically\ninvestigate the impact of three well-known compression strategies -- knowledge\ndistillation, quantization, and pruning -- across three different classes of\nsoftware engineering tasks: vulnerability detection, code summarization, and\ncode search. Our findings reveal that the impact of these strategies varies\ngreatly depending on the task and the specific compression method employed.\nPractitioners and researchers can use these insights to make informed decisions\nwhen selecting the most appropriate compression strategy, balancing both\nefficiency and effectiveness based on their specific needs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13737v1",
    "published_date": "2024-12-18 11:14:30 UTC",
    "updated_date": "2024-12-18 11:14:30 UTC"
  },
  {
    "arxiv_id": "2412.13720v2",
    "title": "Federated Learning and RAG Integration: A Scalable Approach for Medical Large Language Models",
    "authors": [
      "Jincheol Jung",
      "Hongju Jeong",
      "Eui-Nam Huh"
    ],
    "abstract": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13720v2",
    "published_date": "2024-12-18 11:00:58 UTC",
    "updated_date": "2025-01-08 07:03:42 UTC"
  },
  {
    "arxiv_id": "2412.16220v3",
    "title": "Cross-Attention Graph Neural Networks for Inferring Gene Regulatory Networks with Skewed Degree Distribution",
    "authors": [
      "Jiaqi Xiong",
      "Nan Yin",
      "Shiyang Liang",
      "Haoyang Li",
      "Yingxu Wang",
      "Duo Ai",
      "Fang Pan",
      "Jingjie Wang"
    ],
    "abstract": "Inferencing Gene Regulatory Networks (GRNs) from gene expression data is a\npivotal challenge in systems biology, and several innovative computational\nmethods have been introduced. However, most of these studies have not\nconsidered the skewed degree distribution of genes. Specifically, some genes\nmay regulate multiple target genes while some genes may be regulated by\nmultiple regulator genes. Such a skewed degree distribution issue significantly\ncomplicates the application of directed graph embedding methods. To tackle this\nissue, we propose the Cross-Attention Complex Dual Graph Embedding Model\n(XATGRN). Our XATGRN employs a cross-attention mechanism to effectively capture\nintricate gene interactions from gene expression profiles. Additionally, it\nuses a Dual Complex Graph Embedding approach to manage the skewed degree\ndistribution, thereby ensuring precise prediction of regulatory relationships\nand their directionality. Our model consistently outperforms existing\nstate-of-the-art methods across various datasets, underscoring its efficacy in\nelucidating complex gene regulatory mechanisms. Our codes used in this paper\nare publicly available at: https://github.com/kikixiong/XATGRN.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "11 pages, 6 figures,1 tabels",
    "pdf_url": "http://arxiv.org/pdf/2412.16220v3",
    "published_date": "2024-12-18 10:56:40 UTC",
    "updated_date": "2025-01-09 14:55:29 UTC"
  },
  {
    "arxiv_id": "2412.13712v1",
    "title": "An Algebraic Notion of Conditional Independence, and Its Application to Knowledge Representation (full version)",
    "authors": [
      "Jesse Heyninck"
    ],
    "abstract": "Conditional independence is a crucial concept supporting adequate modelling\nand efficient reasoning in probabilistics. In knowledge representation, the\nidea of conditional independence has also been introduced for specific\nformalisms, such as propositional logic and belief revision. In this paper, the\nnotion of conditional independence is studied in the algebraic framework of\napproximation fixpoint theory. This gives a language-independent account of\nconditional independence that can be straightforwardly applied to any logic\nwith fixpoint semantics. It is shown how this notion allows to reduce global\nreasoning to parallel instances of local reasoning, leading to fixed-parameter\ntractability results. Furthermore, relations to existing notions of conditional\nindependence are discussed and the framework is applied to normal logic\nprogramming.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Full version, including proofs, of paper accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13712v1",
    "published_date": "2024-12-18 10:52:57 UTC",
    "updated_date": "2024-12-18 10:52:57 UTC"
  },
  {
    "arxiv_id": "2412.13705v1",
    "title": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation",
    "authors": [
      "Minkyoung Kim",
      "Yunha Kim",
      "Hyeram Seo",
      "Heejung Choi",
      "Jiye Han",
      "Gaeun Kee",
      "Soyoung Ko",
      "HyoJe Jung",
      "Byeolhee Kim",
      "Young-Hak Kim",
      "Sanghyun Park",
      "Tae Joon Jun"
    ],
    "abstract": "Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13705v1",
    "published_date": "2024-12-18 10:49:41 UTC",
    "updated_date": "2024-12-18 10:49:41 UTC"
  },
  {
    "arxiv_id": "2412.13702v2",
    "title": "Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models",
    "authors": [
      "Kunat Pipatanakul",
      "Potsawee Manakul",
      "Natapong Nitarach",
      "Warit Sirichotedumrong",
      "Surapon Nonesung",
      "Teetouch Jaknamon",
      "Parinthapat Pengpun",
      "Pittawat Taveekitworachai",
      "Adisai Na-Thalang",
      "Sittipong Sripaisarnmongkol",
      "Krisanapong Jirayoot",
      "Kasima Tharnpipitchai"
    ],
    "abstract": "This paper introduces Typhoon 2, a series of text and multimodal large\nlanguage models optimized for the Thai language. The series includes models for\ntext, vision, and audio. Typhoon2-Text builds on state-of-the-art open models,\nsuch as Llama 3 and Qwen2, and we perform continual pre-training on a mixture\nof English and Thai data. We employ post-training techniques to enhance Thai\nlanguage performance while preserving the base models' original capabilities.\nWe release text models across a range of sizes, from 1 to 70 billion\nparameters, available in both base and instruction-tuned variants. To guardrail\ntext generation, we release Typhoon2-Safety, a classifier enhanced for Thai\ncultures and language. Typhoon2-Vision improves Thai document understanding\nwhile retaining general visual capabilities, such as image captioning.\nTyphoon2-Audio introduces an end-to-end speech-to-speech model architecture\ncapable of processing audio, speech, and text inputs and generating both text\nand speech outputs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "technical report, 55 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.13702v2",
    "published_date": "2024-12-18 10:45:24 UTC",
    "updated_date": "2024-12-19 17:36:38 UTC"
  },
  {
    "arxiv_id": "2412.14215v1",
    "title": "Generative AI Toolkit -- a framework for increasing the quality of LLM-based applications over their whole life cycle",
    "authors": [
      "Jens Kohl",
      "Luisa Gloger",
      "Rui Costa",
      "Otto Kruse",
      "Manuel P. Luitz",
      "David Katz",
      "Gonzalo Barbeito",
      "Markus Schweier",
      "Ryan French",
      "Jonas Schroeder",
      "Thomas Riedl",
      "Raphael Perri",
      "Youssef Mostafa"
    ],
    "abstract": "As LLM-based applications reach millions of customers, ensuring their\nscalability and continuous quality improvement is critical for success.\nHowever, the current workflows for developing, maintaining, and operating\n(DevOps) these applications are predominantly manual, slow, and based on\ntrial-and-error. With this paper we introduce the Generative AI Toolkit, which\nautomates essential workflows over the whole life cycle of LLM-based\napplications. The toolkit helps to configure, test, continuously monitor and\noptimize Generative AI applications such as agents, thus significantly\nimproving quality while shortening release cycles. We showcase the\neffectiveness of our toolkit on representative use cases, share best practices,\nand outline future enhancements. Since we are convinced that our Generative AI\nToolkit is helpful for other teams, we are open sourcing it on and hope that\nothers will use, forward, adapt and improve",
    "categories": [
      "cs.SE",
      "cs.AI",
      "I.2.7; I.2.11"
    ],
    "primary_category": "cs.SE",
    "comment": "16 pages, 6 figures. For source code see\n  https://github.com/awslabs/generative-ai-toolkit",
    "pdf_url": "http://arxiv.org/pdf/2412.14215v1",
    "published_date": "2024-12-18 10:40:00 UTC",
    "updated_date": "2024-12-18 10:40:00 UTC"
  },
  {
    "arxiv_id": "2412.13688v1",
    "title": "Discerning and Characterising Types of Competency Questions for Ontologies",
    "authors": [
      "C. Maria Keet",
      "Zubeida Casmod Khan"
    ],
    "abstract": "Competency Questions (CQs) are widely used in ontology development by\nguiding, among others, the scoping and validation stages. However, very limited\nguidance exists for formulating CQs and assessing whether they are good CQs,\nleading to issues such as ambiguity and unusable formulations. To solve this,\none requires insight into the nature of CQs for ontologies and their\nconstituent parts, as well as which ones are not. We aim to contribute to such\ntheoretical foundations in this paper, which is informed by analysing\nquestions, their uses, and the myriad of ontology development tasks. This\nresulted in a first Model for Competency Questions, which comprises five main\ntypes of CQs, each with a different purpose: Scoping (SCQ), Validating (VCQ),\nFoundational (FCQ), Relationship (RCQ), and Metaproperty (MpCQ) questions. This\nmodel enhances the clarity of CQs and therewith aims to improve on the\neffectiveness of CQs in ontology development, thanks to their respective\nidentifiable distinct constituent elements. We illustrate and evaluate them\nwith a user story and demonstrate where which type can be used in ontology\ndevelopment tasks. To foster use and research, we created an annotated\nrepository of 438 CQs, the Repository of Ontology Competency QuestionS (ROCQS),\nincorporating an existing CQ dataset and new CQs and CQ templates, which\nfurther demonstrate distinctions among types of CQs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13688v1",
    "published_date": "2024-12-18 10:26:29 UTC",
    "updated_date": "2024-12-18 10:26:29 UTC"
  },
  {
    "arxiv_id": "2412.13682v2",
    "title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning",
    "authors": [
      "Jie-Jing Shao",
      "Xiao-Wen Yang",
      "Bo-Wen Zhang",
      "Baizhi Chen",
      "Wen-Da Wei",
      "Guohao Cai",
      "Zhenhua Dong",
      "Lan-Zhe Guo",
      "Yu-feng Li"
    ],
    "abstract": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel",
    "pdf_url": "http://arxiv.org/pdf/2412.13682v2",
    "published_date": "2024-12-18 10:10:12 UTC",
    "updated_date": "2024-12-20 15:08:25 UTC"
  },
  {
    "arxiv_id": "2412.13678v1",
    "title": "Clio: Privacy-Preserving Insights into Real-World AI Use",
    "authors": [
      "Alex Tamkin",
      "Miles McCain",
      "Kunal Handa",
      "Esin Durmus",
      "Liane Lovitt",
      "Ankur Rathi",
      "Saffron Huang",
      "Alfred Mountfield",
      "Jerry Hong",
      "Stuart Ritchie",
      "Michael Stern",
      "Brian Clarke",
      "Landon Goldberg",
      "Theodore R. Sumers",
      "Jared Mueller",
      "William McEachen",
      "Wes Mitchell",
      "Shan Carter",
      "Jack Clark",
      "Jared Kaplan",
      "Deep Ganguli"
    ],
    "abstract": "How are AI assistants being used in the real world? While model providers in\ntheory have a window into this impact via their users' data, both privacy\nconcerns and practical challenges have made analyzing this data difficult. To\naddress these issues, we present Clio (Claude insights and observations), a\nprivacy-preserving platform that uses AI assistants themselves to analyze and\nsurface aggregated usage patterns across millions of conversations, without the\nneed for human reviewers to read raw conversations. We validate this can be\ndone with a high degree of accuracy and privacy by conducting extensive\nevaluations. We demonstrate Clio's usefulness in two broad ways. First, we\nshare insights about how models are being used in the real world from one\nmillion Claude.ai Free and Pro conversations, ranging from providing advice on\nhairstyles to providing guidance on Git operations and concepts. We also\nidentify the most common high-level use cases on Claude.ai (coding, writing,\nand research tasks) as well as patterns that differ across languages (e.g.,\nconversations in Japanese discuss elder care and aging populations at\nhigher-than-typical rates). Second, we use Clio to make our systems safer by\nidentifying coordinated attempts to abuse our systems, monitoring for unknown\nunknowns during critical periods like launches of new capabilities or major\nworld events, and improving our existing monitoring systems. We also discuss\nthe limitations of our approach, as well as risks and ethical concerns. By\nenabling analysis of real-world AI usage, Clio provides a scalable platform for\nempirically grounded AI safety and governance.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13678v1",
    "published_date": "2024-12-18 10:05:43 UTC",
    "updated_date": "2024-12-18 10:05:43 UTC"
  },
  {
    "arxiv_id": "2412.14214v1",
    "title": "GraphicsDreamer: Image to 3D Generation with Physical Consistency",
    "authors": [
      "Pei Chen",
      "Fudong Wang",
      "Yixuan Tong",
      "Jingdong Chen",
      "Ming Yang",
      "Minghui Yang"
    ],
    "abstract": "Recently, the surge of efficient and automated 3D AI-generated content (AIGC)\nmethods has increasingly illuminated the path of transforming human imagination\ninto complex 3D structures. However, the automated generation of 3D content is\nstill significantly lags in industrial application. This gap exists because 3D\nmodeling demands high-quality assets with sharp geometry, exquisite topology,\nand physically based rendering (PBR), among other criteria. To narrow the\ndisparity between generated results and artists' expectations, we introduce\nGraphicsDreamer, a method for creating highly usable 3D meshes from single\nimages. To better capture the geometry and material details, we integrate the\nPBR lighting equation into our cross-domain diffusion model, concurrently\npredicting multi-view color, normal, depth images, and PBR materials. In the\ngeometry fusion stage, we continue to enforce the PBR constraints, ensuring\nthat the generated 3D objects possess reliable texture details, supporting\nrealistic relighting. Furthermore, our method incorporates topology\noptimization and fast UV unwrapping capabilities, allowing the 3D products to\nbe seamlessly imported into graphics engines. Extensive experiments demonstrate\nthat our model can produce high quality 3D assets in a reasonable time cost\ncompared to previous methods.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14214v1",
    "published_date": "2024-12-18 10:01:27 UTC",
    "updated_date": "2024-12-18 10:01:27 UTC"
  },
  {
    "arxiv_id": "2412.13667v1",
    "title": "Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery",
    "authors": [
      "ChengAo Shen",
      "Zhengzhang Chen",
      "Dongsheng Luo",
      "Dongkuan Xu",
      "Haifeng Chen",
      "Jingchao Ni"
    ],
    "abstract": "Causal inference is an imperative foundation for decision-making across\ndomains, such as smart health, AI for drug discovery and AIOps. Traditional\nstatistical causal discovery methods, while well-established, predominantly\nrely on observational data and often overlook the semantic cues inherent in\ncause-and-effect relationships. The advent of Large Language Models (LLMs) has\nushered in an affordable way of leveraging the semantic cues for\nknowledge-driven causal discovery, but the development of LLMs for causal\ndiscovery lags behind other areas, particularly in the exploration of\nmulti-modality data. To bridge the gap, we introduce MATMCD, a multi-agent\nsystem powered by tool-augmented LLMs. MATMCD has two key agents: a Data\nAugmentation agent that retrieves and processes modality-augmented data, and a\nCausal Constraint agent that integrates multi-modal data for knowledge-driven\ninference. Delicate design of the inner-workings ensures successful cooperation\nof the agents. Our empirical study across seven datasets suggests the\nsignificant potential of multi-modality enhanced causal discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13667v1",
    "published_date": "2024-12-18 09:50:00 UTC",
    "updated_date": "2024-12-18 09:50:00 UTC"
  },
  {
    "arxiv_id": "2412.13666v1",
    "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation",
    "authors": [
      "Aneta Zugecova",
      "Dominik Macko",
      "Ivan Srba",
      "Robert Moro",
      "Jakub Kopal",
      "Katarina Marcincinova",
      "Matus Mesarcik"
    ],
    "abstract": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts rises\nmany concerns regarding their misuse. Previous research has shown that LLMs can\nbe effectively misused for generating disinformation news articles following\npredefined narratives. Their capabilities to generate personalized (in various\naspects) content have also been evaluated and mostly found usable. However, a\ncombination of personalization and disinformation abilities of LLMs has not\nbeen comprehensively studied yet. Such a dangerous combination should trigger\nintegrated safety filters of the LLMs, if there are some. This study fills this\ngap by evaluation of vulnerabilities of recent open and closed LLMs, and their\nwillingness to generate personalized disinformation news articles in English.\nWe further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13666v1",
    "published_date": "2024-12-18 09:48:53 UTC",
    "updated_date": "2024-12-18 09:48:53 UTC"
  },
  {
    "arxiv_id": "2412.13663v2",
    "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "authors": [
      "Benjamin Warner",
      "Antoine Chaffin",
      "Benjamin Clavié",
      "Orion Weller",
      "Oskar Hallström",
      "Said Taghadouini",
      "Alexis Gallagher",
      "Raja Biswas",
      "Faisal Ladhak",
      "Tom Aarsen",
      "Nathan Cooper",
      "Griffin Adams",
      "Jeremy Howard",
      "Iacopo Poli"
    ],
    "abstract": "Encoder-only transformer models such as BERT offer a great performance-size\ntradeoff for retrieval and classification tasks with respect to larger\ndecoder-only models. Despite being the workhorse of numerous production\npipelines, there have been limited Pareto improvements to BERT since its\nrelease. In this paper, we introduce ModernBERT, bringing modern model\noptimizations to encoder-only models and representing a major Pareto\nimprovement over older encoders. Trained on 2 trillion tokens with a native\n8192 sequence length, ModernBERT models exhibit state-of-the-art results on a\nlarge pool of evaluations encompassing diverse classification tasks and both\nsingle and multi-vector retrieval on different domains (including code). In\naddition to strong downstream performance, ModernBERT is also the most speed\nand memory efficient encoder and is designed for inference on common GPUs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13663v2",
    "published_date": "2024-12-18 09:39:44 UTC",
    "updated_date": "2024-12-19 06:32:26 UTC"
  },
  {
    "arxiv_id": "2412.13662v1",
    "title": "When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement Learning?",
    "authors": [
      "Tongzhou Mu",
      "Zhaoyang Li",
      "Stanisław Wiktor Strzelecki",
      "Xiu Yuan",
      "Yunchao Yao",
      "Litian Liang",
      "Hao Su"
    ],
    "abstract": "Learning policies from high-dimensional visual inputs, such as pixels and\npoint clouds, is crucial in various applications. Visual reinforcement learning\nis a promising approach that directly trains policies from visual observations,\nalthough it faces challenges in sample efficiency and computational costs. This\nstudy conducts an empirical comparison of State-to-Visual DAgger, a two-stage\nframework that initially trains a state policy before adopting online imitation\nto learn a visual policy, and Visual RL across a diverse set of tasks. We\nevaluate both methods across 16 tasks from three benchmarks, focusing on their\nasymptotic performance, sample efficiency, and computational costs.\nSurprisingly, our findings reveal that State-to-Visual DAgger does not\nuniversally outperform Visual RL but shows significant advantages in\nchallenging tasks, offering more consistent performance. In contrast, its\nbenefits in sample efficiency are less pronounced, although it often reduces\nthe overall wall-clock time required for training. Based on our findings, we\nprovide recommendations for practitioners and hope that our results contribute\nvaluable perspectives for future research in visual policy learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by The 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.13662v1",
    "published_date": "2024-12-18 09:39:12 UTC",
    "updated_date": "2024-12-18 09:39:12 UTC"
  },
  {
    "arxiv_id": "2501.10385v1",
    "title": "Autonomous Microscopy Experiments through Large Language Model Agents",
    "authors": [
      "Indrajeet Mandal",
      "Jitendra Soni",
      "Mohd Zaki",
      "Morten M. Smedskjaer",
      "Katrin Wondraczek",
      "Lothar Wondraczek",
      "Nitya Nand Gosvami",
      "N. M. Anoop Krishnan"
    ],
    "abstract": "The emergence of large language models (LLMs) has accelerated the development\nof self-driving laboratories (SDLs) for materials research. Despite their\ntransformative potential, current SDL implementations rely on rigid, predefined\nprotocols that limit their adaptability to dynamic experimental scenarios\nacross different labs. A significant challenge persists in measuring how\neffectively AI agents can replicate the adaptive decision-making and\nexperimental intuition of expert scientists. Here, we introduce AILA\n(Artificially Intelligent Lab Assistant), a framework that automates atomic\nforce microscopy (AFM) through LLM-driven agents. Using AFM as an experimental\ntestbed, we develop AFMBench-a comprehensive evaluation suite that challenges\nAI agents based on language models like GPT-4o and GPT-3.5 to perform tasks\nspanning the scientific workflow: from experimental design to results analysis.\nOur systematic assessment shows that state-of-the-art language models struggle\neven with basic tasks such as documentation retrieval, leading to a significant\ndecline in performance in multi-agent coordination scenarios. Further, we\nobserve that LLMs exhibit a tendency to not adhere to instructions or even\ndivagate to additional tasks beyond the original request, raising serious\nconcerns regarding safety alignment aspects of AI agents for SDLs. Finally, we\ndemonstrate the application of AILA on increasingly complex experiments\nopen-ended experiments: automated AFM calibration, high-resolution feature\ndetection, and mechanical property measurement. Our findings emphasize the\nnecessity for stringent benchmarking protocols before deploying AI agents as\nlaboratory assistants across scientific disciplines.",
    "categories": [
      "cs.CY",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.ins-det"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10385v1",
    "published_date": "2024-12-18 09:35:28 UTC",
    "updated_date": "2024-12-18 09:35:28 UTC"
  },
  {
    "arxiv_id": "2412.13647v2",
    "title": "G-VEval: A Versatile Metric for Evaluating Image and Video Captions Using GPT-4o",
    "authors": [
      "Tony Cheng Tong",
      "Sirui He",
      "Zhiwen Shao",
      "Dit-Yan Yeung"
    ],
    "abstract": "Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13647v2",
    "published_date": "2024-12-18 09:23:12 UTC",
    "updated_date": "2024-12-19 15:37:55 UTC"
  },
  {
    "arxiv_id": "2412.13645v1",
    "title": "On the Role of Model Prior in Real-World Inductive Reasoning",
    "authors": [
      "Zhuo Liu",
      "Ding Yu",
      "Hangfeng He"
    ],
    "abstract": "Large Language Models (LLMs) show impressive inductive reasoning\ncapabilities, enabling them to generate hypotheses that could generalize\neffectively to new instances when guided by in-context demonstrations. However,\nin real-world applications, LLMs' hypothesis generation is not solely\ndetermined by these demonstrations but is significantly shaped by task-specific\nmodel priors. Despite their critical influence, the distinct contributions of\nmodel priors versus demonstrations to hypothesis generation have been\nunderexplored. This study bridges this gap by systematically evaluating three\ninductive reasoning strategies across five real-world tasks with three LLMs.\nOur empirical findings reveal that, hypothesis generation is primarily driven\nby the model's inherent priors; removing demonstrations results in minimal loss\nof hypothesis quality and downstream usage. Further analysis shows the result\nis consistent across various label formats with different label configurations,\nand prior is hard to override, even under flipped labeling. These insights\nadvance our understanding of the dynamics of hypothesis generation in LLMs and\nhighlight the potential for better utilizing model priors in real-world\ninductive reasoning tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13645v1",
    "published_date": "2024-12-18 09:22:08 UTC",
    "updated_date": "2024-12-18 09:22:08 UTC"
  },
  {
    "arxiv_id": "2412.13636v1",
    "title": "Consistency of Compositional Generalization across Multiple Levels",
    "authors": [
      "Chuanhao Li",
      "Zhen Li",
      "Chenchen Jing",
      "Xiaomeng Fan",
      "Wenbo Ye",
      "Yuwei Wu",
      "Yunde Jia"
    ],
    "abstract": "Compositional generalization is the capability of a model to understand novel\ncompositions composed of seen concepts. There are multiple levels of novel\ncompositions including phrase-phrase level, phrase-word level, and word-word\nlevel. Existing methods achieve promising compositional generalization, but the\nconsistency of compositional generalization across multiple levels of novel\ncompositions remains unexplored. The consistency refers to that a model should\ngeneralize to a phrase-phrase level novel composition, and\nphrase-word/word-word level novel compositions that can be derived from it\nsimultaneously. In this paper, we propose a meta-learning based framework, for\nachieving consistent compositional generalization across multiple levels. The\nbasic idea is to progressively learn compositions from simple to complex for\nconsistency. Specifically, we divide the original training set into multiple\nvalidation sets based on compositional complexity, and introduce multiple\nmeta-weight-nets to generate sample weights for samples in different validation\nsets. To fit the validation sets in order of increasing compositional\ncomplexity, we optimize the parameters of each meta-weight-net independently\nand sequentially in a multilevel optimization manner. We build a GQA-CCG\ndataset to quantitatively evaluate the consistency. Experimental results on\nvisual question answering and temporal video grounding, demonstrate the\neffectiveness of the proposed framework. We release GQA-CCG at\nhttps://github.com/NeverMoreLCH/CCG.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13636v1",
    "published_date": "2024-12-18 09:09:41 UTC",
    "updated_date": "2024-12-18 09:09:41 UTC"
  },
  {
    "arxiv_id": "2412.13632v1",
    "title": "An Extension-Based Argument-Ranking Semantics: Social Rankings in Abstract Argumentation Long Version",
    "authors": [
      "Lars Bengel",
      "Giovanni Buraglio",
      "Jan Maly",
      "Kenneth Skiba"
    ],
    "abstract": "In this paper, we introduce a new family of argument-ranking semantics which\ncan be seen as a refinement of the classification of arguments into skeptically\naccepted, credulously accepted and rejected. To this end we use so-called\nsocial ranking functions which have been developed recently to rank individuals\nbased on their performance in groups. We provide necessary and sufficient\nconditions for a social ranking function to give rise to an argument-ranking\nsemantics satisfying the desired refinement property.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13632v1",
    "published_date": "2024-12-18 09:08:46 UTC",
    "updated_date": "2024-12-18 09:08:46 UTC"
  },
  {
    "arxiv_id": "2412.13631v2",
    "title": "Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning",
    "authors": [
      "Eitan Wagner",
      "Nitay Alon",
      "Joseph M. Barnby",
      "Omri Abend"
    ],
    "abstract": "Theory of Mind (ToM) capabilities in LLMs have recently become a central\nobject of investigation. Cognitive science distinguishes between two steps\nrequired for ToM tasks: 1) determine whether to invoke ToM, which includes the\nappropriate Depth of Mentalizing (DoM), or level of recursion required to\ncomplete a task; and 2) applying the correct inference given the DoM. In this\nposition paper, we first identify several lines of work in different\ncommunities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and\nformal models for ToM. We argue that recent work in AI tends to focus\nexclusively on the second step which are typically framed as static logic\nproblems. We conclude with suggestions for improved evaluation of ToM\ncapabilities inspired by dynamic environments used in cognitive tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13631v2",
    "published_date": "2024-12-18 09:06:48 UTC",
    "updated_date": "2025-02-16 10:15:14 UTC"
  },
  {
    "arxiv_id": "2412.13630v1",
    "title": "Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model",
    "authors": [
      "Xiu Yuan",
      "Tongzhou Mu",
      "Stone Tao",
      "Yunhao Fang",
      "Mengke Zhang",
      "Hao Su"
    ],
    "abstract": "Recent advancements in robot learning have used imitation learning with large\nmodels and extensive demonstrations to develop effective policies. However,\nthese models are often limited by the quantity, quality, and diversity of\ndemonstrations. This paper explores improving offline-trained imitation\nlearning models through online interactions with the environment. We introduce\nPolicy Decorator, which uses a model-agnostic residual policy to refine large\nimitation learning models during online interactions. By implementing\ncontrolled exploration strategies, Policy Decorator enables stable,\nsample-efficient online learning. Our evaluation spans eight tasks across two\nbenchmarks-ManiSkill and Adroit-and involves two state-of-the-art imitation\nlearning models (Behavior Transformer and Diffusion Policy). The results show\nPolicy Decorator effectively improves the offline-trained policies and\npreserves the smooth motion of imitation learning models, avoiding the erratic\nbehaviors of pure RL policies. See our project page\n(https://policydecorator.github.io) for videos.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Explore videos, data, code, and more at\n  https://policydecorator.github.io",
    "pdf_url": "http://arxiv.org/pdf/2412.13630v1",
    "published_date": "2024-12-18 09:06:16 UTC",
    "updated_date": "2024-12-18 09:06:16 UTC"
  },
  {
    "arxiv_id": "2412.13626v1",
    "title": "LIFT: Improving Long Context Understanding Through Long Input Fine-Tuning",
    "authors": [
      "Yansheng Mao",
      "Jiaqi Li",
      "Fanxu Meng",
      "Jing Xiong",
      "Zilong Zheng",
      "Muhan Zhang"
    ],
    "abstract": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper introduces Long Input Fine-Tuning\n(LIFT) for long context modeling, a novel framework that enhances LLM\nperformance on long-context tasks by adapting model parameters to the context\nat test time. LIFT enables efficient processing of lengthy inputs without the\ncomputational burden of offline long-context adaptation, and can improve the\nlong-context capabilities of arbitrary short-context models. The framework is\nfurther enhanced by integrating in-context learning and pre-LIFT supervised\nfine-tuning. The combination of in-context learning and LIFT enables\nshort-context models like Llama 3 to handle arbitrarily long contexts and\nconsistently improves their performance on popular long-context benchmarks like\nLooGLE and LongBench. We also provide a comprehensive analysis of the strengths\nand limitations of LIFT on long context understanding, offering valuable\ndirections for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13626v1",
    "published_date": "2024-12-18 09:04:55 UTC",
    "updated_date": "2024-12-18 09:04:55 UTC"
  },
  {
    "arxiv_id": "2412.13623v1",
    "title": "Unifying Attribution-Based Explanations Using Functional Decomposition",
    "authors": [
      "Arne Gevaert",
      "Yvan Saeys"
    ],
    "abstract": "The black box problem in machine learning has led to the introduction of an\never-increasing set of explanation methods for complex models. These\nexplanations have different properties, which in turn has led to the problem of\nmethod selection: which explanation method is most suitable for a given use\ncase? In this work, we propose a unifying framework of attribution-based\nexplanation methods, which provides a step towards a rigorous study of the\nsimilarities and differences of explanations. We first introduce removal-based\nattribution methods (RBAMs), and show that an extensively broad selection of\nexisting methods can be viewed as such RBAMs. We then introduce the canonical\nadditive decomposition (CAD). This is a general construction for additively\ndecomposing any function based on the central idea of removing (groups of)\nfeatures. We proceed to show that indeed every valid additive decomposition is\nan instance of the CAD, and that any removal-based attribution method is\nassociated with a specific CAD. Next, we show that any removal-based\nattribution method can be completely defined as a game-theoretic value or\ninteraction index for a specific (possibly constant-shifted) cooperative game,\nwhich is defined using the corresponding CAD of the method. We then use this\nintrinsic connection to define formal descriptions of specific behaviours of\nexplanation methods, which we also call functional axioms, and identify\nsufficient conditions on the corresponding CAD and game-theoretic value or\ninteraction index of an attribution method under which the attribution method\nis guaranteed to adhere to these functional axioms. Finally, we show how this\nunifying framework can be used to develop new, efficient approximations for\nexisting explanation methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13623v1",
    "published_date": "2024-12-18 09:04:07 UTC",
    "updated_date": "2024-12-18 09:04:07 UTC"
  },
  {
    "arxiv_id": "2412.13618v1",
    "title": "NPC: Neural Predictive Control for Fuel-Efficient Autonomous Trucks",
    "authors": [
      "Jiaping Ren",
      "Jiahao Xiang",
      "Hongfei Gao",
      "Jinchuan Zhang",
      "Yiming Ren",
      "Yuexin Ma",
      "Yi Wu",
      "Ruigang Yang",
      "Wei Li"
    ],
    "abstract": "Fuel efficiency is a crucial aspect of long-distance cargo transportation by\noil-powered trucks that economize on costs and decrease carbon emissions.\nCurrent predictive control methods depend on an accurate model of vehicle\ndynamics and engine, including weight, drag coefficient, and the Brake-specific\nFuel Consumption (BSFC) map of the engine. We propose a pure data-driven\nmethod, Neural Predictive Control (NPC), which does not use any physical model\nfor the vehicle. After training with over 20,000 km of historical data, the\nnovel proposed NVFormer implicitly models the relationship between vehicle\ndynamics, road slope, fuel consumption, and control commands using the\nattention mechanism. Based on the online sampled primitives from the past of\nthe current freight trip and anchor-based future data synthesis, the NVFormer\ncan infer optimal control command for reasonable fuel consumption. The physical\nmodel-free NPC outperforms the base PCC method with 2.41% and 3.45% more\nsignificant fuel saving in simulation and open-road highway testing,\nrespectively.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 6 figures, for associated mpeg file, see\n  https://www.youtube.com/watch?v=hqgpj7LhiL4",
    "pdf_url": "http://arxiv.org/pdf/2412.13618v1",
    "published_date": "2024-12-18 08:57:05 UTC",
    "updated_date": "2024-12-18 08:57:05 UTC"
  },
  {
    "arxiv_id": "2412.13614v1",
    "title": "Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity Linking",
    "authors": [
      "Zhengfei Xu",
      "Sijia Zhao",
      "Yanchao Hao",
      "Xiaolong Liu",
      "Lili Li",
      "Yuyang Yin",
      "Bo Li",
      "Xi Chen",
      "Xin Xin"
    ],
    "abstract": "Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025;Dataset are released at\n  https://github.com/NP-NET-research/PL-VEL",
    "pdf_url": "http://arxiv.org/pdf/2412.13614v1",
    "published_date": "2024-12-18 08:49:01 UTC",
    "updated_date": "2024-12-18 08:49:01 UTC"
  },
  {
    "arxiv_id": "2412.14212v1",
    "title": "Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution",
    "authors": [
      "Ziyi Ni",
      "Yifan Li",
      "Daxiang Dong"
    ],
    "abstract": "The exceptional capabilities of large language models (LLMs) have\nsubstantially accelerated the rapid rise and widespread adoption of agents.\nRecent studies have demonstrated that generating Python code to consolidate\nLLM-based agents' actions into a unified action space (CodeAct) is a promising\napproach for developing real-world LLM agents. However, this step-by-step code\ngeneration approach often lacks consistency and robustness, leading to\ninstability in agent applications, particularly for complex reasoning and\nout-of-domain tasks. In this paper, we propose a novel approach called\nTree-of-Code (ToC) to tackle the challenges of complex problem planning and\nexecution with an end-to-end mechanism. By integrating key ideas from both\nTree-of-Thought and CodeAct, ToC combines their strengths to enhance solution\nexploration. In our framework, each final code execution result is treated as a\nnode in the decision tree, with a breadth-first search strategy employed to\nexplore potential solutions. The final outcome is determined through a voting\nmechanism based on the outputs of the nodes.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Submitted to the Neurips Workshop \"System 2 Reasoning\" in September,\n  2024. The openreview is avaliable at\n  https://openreview.net/forum?id=8NKAL8Ngxk",
    "pdf_url": "http://arxiv.org/pdf/2412.14212v1",
    "published_date": "2024-12-18 08:47:17 UTC",
    "updated_date": "2024-12-18 08:47:17 UTC"
  },
  {
    "arxiv_id": "2412.13612v3",
    "title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
    "authors": [
      "Xuemei Tang",
      "Xufeng Duan",
      "Zhenguang G. Cai"
    ],
    "abstract": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 5 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.13612v3",
    "published_date": "2024-12-18 08:42:25 UTC",
    "updated_date": "2025-04-23 07:09:06 UTC"
  },
  {
    "arxiv_id": "2412.13610v1",
    "title": "Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking Calculation",
    "authors": [
      "Zecheng Hao",
      "Zhaofei Yu",
      "Tiejun Huang"
    ],
    "abstract": "Spiking Neural Network (SNN), as a brain-inspired and energy-efficient\nnetwork, is currently facing the pivotal challenge of exploring a suitable and\nefficient learning framework. The predominant training methodologies, namely\nSpatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered\nby substantial training overhead or pronounced inference latency, which impedes\nthe advancement of SNNs in scaling to larger networks and navigating intricate\napplication domains. In this work, we propose a novel parallel conversion\nlearning framework, which establishes a mathematical mapping relationship\nbetween each time-step of the parallel spiking neurons and the cumulative spike\nfiring rate. We theoretically validate the lossless and sorting properties of\nthe conversion process, as well as pointing out the optimal shifting distance\nfor each step. Furthermore, by integrating the above framework with the\ndistribution-aware error calibration technique, we can achieve efficient\nconversion towards more general activation functions or training-free\ncircumstance. Extensive experiments have confirmed the significant performance\nadvantages of our method for various conversion cases under ultra-low time\nlatency. To our best knowledge, this is the first work which jointly utilizes\nparallel spiking calculation and ANN-SNN Conversion, providing a highly\npromising approach for SNN supervised training.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13610v1",
    "published_date": "2024-12-18 08:37:13 UTC",
    "updated_date": "2024-12-18 08:37:13 UTC"
  },
  {
    "arxiv_id": "2412.13606v1",
    "title": "Exploiting Symmetries in MUS Computation (Extended version)",
    "authors": [
      "Ignace Bleukx",
      "Hélène Verhaeghe",
      "Bart Bogaerts",
      "Tias Guns"
    ],
    "abstract": "In eXplainable Constraint Solving (XCS), it is common to extract a Minimal\nUnsatisfiable Subset (MUS) from a set of unsatisfiable constraints. This helps\nexplain to a user why a constraint specification does not admit a solution.\nFinding MUSes can be computationally expensive for highly symmetric problems,\nas many combinations of constraints need to be considered. In the traditional\ncontext of solving satisfaction problems, symmetry has been well studied, and\neffective ways to detect and exploit symmetries during the search exist.\nHowever, in the setting of finding MUSes of unsatisfiable constraint programs,\nsymmetries are understudied. In this paper, we take inspiration from existing\nsymmetry-handling techniques and adapt well-known MUS-computation methods to\nexploit symmetries in the specification, speeding-up overall computation time.\nOur results display a significant reduction of runtime for our adapted\nalgorithms compared to the baseline on symmetric problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AAAI25 conference",
    "pdf_url": "http://arxiv.org/pdf/2412.13606v1",
    "published_date": "2024-12-18 08:34:32 UTC",
    "updated_date": "2024-12-18 08:34:32 UTC"
  },
  {
    "arxiv_id": "2412.13601v2",
    "title": "Hybrid CNN-LSTM based Indoor Pedestrian Localization with CSI Fingerprint Maps",
    "authors": [
      "Muhammad Emad-ud-din"
    ],
    "abstract": "The paper presents a novel Wi-Fi fingerprinting system that uses Channel\nState Information (CSI) data for fine-grained pedestrian localization. The\nproposed system exploits the frequency diversity and spatial diversity of the\nfeatures extracted from CSI data to generate a 2D+channel image termed as a CSI\nFingerprint Map. We then use this CSI Fingerprint Map representation of CSI\ndata to generate a pedestrian trajectory hypothesis using a hybrid architecture\nthat combines a Convolutional Neural Network and a Long Short-Term Memory\nRecurrent Neural Network model. The proposed architecture exploits the temporal\nand spatial relationship information among the CSI data observations gathered\nat neighboring locations. A particle filter is then employed to separate out\nthe most likely hypothesis matching a human walk model. The experimental\nperformance of our method is compared to existing deep learning localization\nmethods such ConFi, DeepFi and to a self-developed temporal-feature based LSTM\nbased location classifier. The experimental results show marked improvement\nwith an average RMSE of 0.36 m in a moderately dynamic and 0.17 m in a static\nenvironment. Our method is essentially a proof of concept that with (1) sparse\navailability of observations, (2) limited infrastructure requirements, (3)\nmoderate level of short-term and long-term noise in the training and testing\nenvironment, reliable fine-grained Wi-Fi based pedestrian localization is a\npotential option.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "14A06",
      "I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 14 figures and 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.13601v2",
    "published_date": "2024-12-18 08:31:34 UTC",
    "updated_date": "2024-12-29 20:02:59 UTC"
  },
  {
    "arxiv_id": "2412.13594v1",
    "title": "Generalizable Sensor-Based Activity Recognition via Categorical Concept Invariant Learning",
    "authors": [
      "Di Xiong",
      "Shuoyuan Wang",
      "Lei Zhang",
      "Wenbo Huang",
      "Chaolei Han"
    ],
    "abstract": "Human Activity Recognition (HAR) aims to recognize activities by training\nmodels on massive sensor data. In real-world deployment, a crucial aspect of\nHAR that has been largely overlooked is that the test sets may have different\ndistributions from training sets due to inter-subject variability including\nage, gender, behavioral habits, etc., which leads to poor generalization\nperformance. One promising solution is to learn domain-invariant\nrepresentations to enable a model to generalize on an unseen distribution.\nHowever, most existing methods only consider the feature-invariance of the\npenultimate layer for domain-invariant learning, which leads to suboptimal\nresults. In this paper, we propose a Categorical Concept Invariant Learning\n(CCIL) framework for generalizable activity recognition, which introduces a\nconcept matrix to regularize the model in the training stage by simultaneously\nconcentrating on feature-invariance and logit-invariance. Our key idea is that\nthe concept matrix for samples belonging to the same activity category should\nbe similar. Extensive experiments on four public HAR benchmarks demonstrate\nthat our CCIL substantially outperforms the state-of-the-art approaches under\ncross-person, cross-dataset, cross-position, and one-person-to-another\nsettings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13594v1",
    "published_date": "2024-12-18 08:18:03 UTC",
    "updated_date": "2024-12-18 08:18:03 UTC"
  },
  {
    "arxiv_id": "2412.13589v1",
    "title": "SemiDFL: A Semi-Supervised Paradigm for Decentralized Federated Learning",
    "authors": [
      "Xinyang Liu",
      "Pengchao Han",
      "Xuan Li",
      "Bo Liu"
    ],
    "abstract": "Decentralized federated learning (DFL) realizes cooperative model training\namong connected clients without relying on a central server, thereby mitigating\ncommunication bottlenecks and eliminating the single-point failure issue\npresent in centralized federated learning (CFL). Most existing work on DFL\nfocuses on supervised learning, assuming each client possesses sufficient\nlabeled data for local training. However, in real-world applications, much of\nthe data is unlabeled. We address this by considering a challenging yet\npractical semisupervised learning (SSL) scenario in DFL, where clients may have\nvarying data sources: some with few labeled samples, some with purely unlabeled\ndata, and others with both. In this work, we propose SemiDFL, the first\nsemi-supervised DFL method that enhances DFL performance in SSL scenarios by\nestablishing a consensus in both data and model spaces. Specifically, we\nutilize neighborhood information to improve the quality of pseudo-labeling,\nwhich is crucial for effectively leveraging unlabeled data. We then design a\nconsensusbased diffusion model to generate synthesized data, which is used in\ncombination with pseudo-labeled data to create mixed datasets. Additionally, we\ndevelop an adaptive aggregation method that leverages the model accuracy of\nsynthesized data to further enhance SemiDFL performance. Through extensive\nexperimentation, we demonstrate the remarkable performance superiority of the\nproposed DFL-Semi method over existing CFL and DFL schemes in both IID and\nnon-IID SSL scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13589v1",
    "published_date": "2024-12-18 08:12:55 UTC",
    "updated_date": "2024-12-18 08:12:55 UTC"
  },
  {
    "arxiv_id": "2412.13578v1",
    "title": "Socio-Culturally Aware Evaluation Framework for LLM-Based Content Moderation",
    "authors": [
      "Shanu Kumar",
      "Gauri Kholkar",
      "Saish Mendke",
      "Anubhav Sadana",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "abstract": "With the growth of social media and large language models, content moderation\nhas become crucial. Many existing datasets lack adequate representation of\ndifferent groups, resulting in unreliable assessments. To tackle this, we\npropose a socio-culturally aware evaluation framework for LLM-driven content\nmoderation and introduce a scalable method for creating diverse datasets using\npersona-based generation. Our analysis reveals that these datasets provide\nbroader perspectives and pose greater challenges for LLMs than\ndiversity-focused generation methods without personas. This challenge is\nespecially pronounced in smaller LLMs, emphasizing the difficulties they\nencounter in moderating such diverse content.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in SUMEval Workshop in COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13578v1",
    "published_date": "2024-12-18 07:57:18 UTC",
    "updated_date": "2024-12-18 07:57:18 UTC"
  },
  {
    "arxiv_id": "2412.13577v1",
    "title": "Bridge then Begin Anew: Generating Target-relevant Intermediate Model for Source-free Visual Emotion Adaptation",
    "authors": [
      "Jiankun Zhu",
      "Sicheng Zhao",
      "Jing Jiang",
      "Wenbo Tang",
      "Zhaopan Xu",
      "Tingting Han",
      "Pengfei Xu",
      "Hongxun Yao"
    ],
    "abstract": "Visual emotion recognition (VER), which aims at understanding humans'\nemotional reactions toward different visual stimuli, has attracted increasing\nattention. Given the subjective and ambiguous characteristics of emotion,\nannotating a reliable large-scale dataset is hard. For reducing reliance on\ndata labeling, domain adaptation offers an alternative solution by adapting\nmodels trained on labeled source data to unlabeled target data. Conventional\ndomain adaptation methods require access to source data. However, due to\nprivacy concerns, source emotional data may be inaccessible. To address this\nissue, we propose an unexplored task: source-free domain adaptation (SFDA) for\nVER, which does not have access to source data during the adaptation process.\nTo achieve this, we propose a novel framework termed Bridge then Begin Anew\n(BBA), which consists of two steps: domain-bridged model generation (DMG) and\ntarget-related model adaptation (TMA). First, the DMG bridges cross-domain gaps\nby generating an intermediate model, avoiding direct alignment between two VER\ndatasets with significant differences. Then, the TMA begins training the target\nmodel anew to fit the target structure, avoiding the influence of\nsource-specific knowledge. Extensive experiments are conducted on six SFDA\nsettings for VER. The results demonstrate the effectiveness of BBA, which\nachieves remarkable performance gains compared with state-of-the-art SFDA\nmethods and outperforms representative unsupervised domain adaptation\napproaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13577v1",
    "published_date": "2024-12-18 07:51:35 UTC",
    "updated_date": "2024-12-18 07:51:35 UTC"
  },
  {
    "arxiv_id": "2412.13573v2",
    "title": "Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes",
    "authors": [
      "Aodi Li",
      "Liansheng Zhuang",
      "Xiao Long",
      "Minghong Yao",
      "Shafei Wang"
    ],
    "abstract": "Domain generalization aims to learn a model from multiple training domains\nand generalize it to unseen test domains. Recent theory has shown that seeking\nthe deep models, whose parameters lie in the flat minima of the loss landscape,\ncan significantly reduce the out-of-domain generalization error. However,\nexisting methods often neglect the consistency of loss landscapes in different\ndomains, resulting in models that are not simultaneously in the optimal flat\nminima in all domains, which limits their generalization ability. To address\nthis issue, this paper proposes an iterative Self-Feedback Training (SFT)\nframework to seek consistent flat minima that are shared across different\ndomains by progressively refining loss landscapes during training. It\nalternatively generates a feedback signal by measuring the inconsistency of\nloss landscapes in different domains and refines these loss landscapes for\ngreater consistency using this feedback signal. Benefiting from the consistency\nof the flat minima within these refined loss landscapes, our SFT helps achieve\nbetter out-of-domain generalization. Extensive experiments on DomainBed\ndemonstrate superior performances of SFT when compared to state-of-the-art\nsharpness-aware methods and other prevalent DG baselines. On average across\nfive DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with\nResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available\nsoon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13573v2",
    "published_date": "2024-12-18 07:45:30 UTC",
    "updated_date": "2025-04-14 04:22:48 UTC"
  },
  {
    "arxiv_id": "2412.13565v1",
    "title": "CA-Edit: Causality-Aware Condition Adapter for High-Fidelity Local Facial Attribute Editing",
    "authors": [
      "Xiaole Xian",
      "Xilin He",
      "Zenghao Niu",
      "Junliang Zhang",
      "Weicheng Xie",
      "Siyang Song",
      "Zitong Yu",
      "Linlin Shen"
    ],
    "abstract": "For efficient and high-fidelity local facial attribute editing, most existing\nediting methods either require additional fine-tuning for different editing\neffects or tend to affect beyond the editing regions. Alternatively, inpainting\nmethods can edit the target image region while preserving external areas.\nHowever, current inpainting methods still suffer from the generation\nmisalignment with facial attributes description and the loss of facial skin\ndetails. To address these challenges, (i) a novel data utilization strategy is\nintroduced to construct datasets consisting of attribute-text-image triples\nfrom a data-driven perspective, (ii) a Causality-Aware Condition Adapter is\nproposed to enhance the contextual causality modeling of specific details,\nwhich encodes the skin details from the original image while preventing\nconflicts between these cues and textual conditions. In addition, a Skin\nTransition Frequency Guidance technique is introduced for the local modeling of\ncontextual causality via sampling guidance driven by low-frequency alignment.\nExtensive quantitative and qualitative experiments demonstrate the\neffectiveness of our method in boosting both fidelity and editability for\nlocalized attribute editing. The code is available at\nhttps://github.com/connorxian/CA-Edit.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by aaai",
    "pdf_url": "http://arxiv.org/pdf/2412.13565v1",
    "published_date": "2024-12-18 07:33:22 UTC",
    "updated_date": "2024-12-18 07:33:22 UTC"
  },
  {
    "arxiv_id": "2412.17839v1",
    "title": "LaMI-GO: Latent Mixture Integration for Goal-Oriented Communications Achieving High Spectrum Efficiency",
    "authors": [
      "Achintha Wijesinghe",
      "Suchinthaka Wanninayaka",
      "Weiwei Wang",
      "Yu-Chieh Chao",
      "Songyang Zhang",
      "Zhi Ding"
    ],
    "abstract": "The recent rise of semantic-style communications includes the development of\ngoal-oriented communications (GOCOMs) remarkably efficient multimedia\ninformation transmissions. The concept of GO-COMS leverages advanced artificial\nintelligence (AI) tools to address the rising demand for bandwidth efficiency\nin applications, such as edge computing and Internet-of-Things (IoT). Unlike\ntraditional communication systems focusing on source data accuracy, GO-COMs\nprovide intelligent message delivery catering to the special needs critical to\naccomplishing downstream tasks at the receiver. In this work, we present a\nnovel GO-COM framework, namely LaMI-GO that utilizes emerging generative AI for\nbetter quality-of-service (QoS) with ultra-high communication efficiency.\nSpecifically, we design our LaMI-GO system backbone based on a latent diffusion\nmodel followed by a vector-quantized generative adversarial network (VQGAN) for\nefficient latent embedding and information representation. The system trains a\ncommon feature codebook the receiver side. Our experimental results demonstrate\nsubstantial improvement in perceptual quality, accuracy of downstream tasks,\nand bandwidth consumption over the state-of-the-art GOCOM systems and establish\nthe power of our proposed LaMI-GO communication framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2412.17839v1",
    "published_date": "2024-12-18 07:20:42 UTC",
    "updated_date": "2024-12-18 07:20:42 UTC"
  },
  {
    "arxiv_id": "2412.13549v1",
    "title": "EscapeBench: Pushing Language Models to Think Outside the Box",
    "authors": [
      "Cheng Qian",
      "Peixuan Han",
      "Qinyu Luo",
      "Bingxiang He",
      "Xiusi Chen",
      "Yuji Zhang",
      "Hongyi Du",
      "Jiarui Yao",
      "Xiaocheng Yang",
      "Denghui Zhang",
      "Yunzhu Li",
      "Heng Ji"
    ],
    "abstract": "Language model agents excel in long-session planning and reasoning, but\nexisting benchmarks primarily focus on goal-oriented tasks with explicit\nobjectives, neglecting creative adaptation in unfamiliar environments. To\naddress this, we introduce EscapeBench, a benchmark suite of room escape game\nenvironments designed to challenge agents with creative reasoning,\nunconventional tool use, and iterative problem-solving to uncover implicit\ngoals. Our results show that current LM models, despite employing working\nmemory and Chain-of-Thought reasoning, achieve only 15% average progress\nwithout hints, highlighting their limitations in creativity. To bridge this\ngap, we propose EscapeAgent, a framework designed to enhance creative reasoning\nthrough Foresight (innovative tool use) and Reflection (identifying unsolved\ntasks). Experiments show that EscapeAgent can execute action chains over 1,000\nsteps while maintaining logical coherence. It navigates and completes games\nwith up to 40% fewer steps and hints, performs robustly across varying\ndifficulty levels, and achieves higher action success rates with more efficient\nand innovative puzzle-solving strategies. All the data and codes are released.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13549v1",
    "published_date": "2024-12-18 06:50:39 UTC",
    "updated_date": "2024-12-18 06:50:39 UTC"
  },
  {
    "arxiv_id": "2412.13544v1",
    "title": "Bridging the User-side Knowledge Gap in Knowledge-aware Recommendations with Large Language Models",
    "authors": [
      "Zheng Hu",
      "Zhe Li",
      "Ziyun Jiao",
      "Satoshi Nakagawa",
      "Jiawen Deng",
      "Shimin Cai",
      "Tao Zhou",
      "Fuji Ren"
    ],
    "abstract": "In recent years, knowledge graphs have been integrated into recommender\nsystems as item-side auxiliary information, enhancing recommendation accuracy.\nHowever, constructing and integrating structural user-side knowledge remains a\nsignificant challenge due to the improper granularity and inherent scarcity of\nuser-side features. Recent advancements in Large Language Models (LLMs) offer\nthe potential to bridge this gap by leveraging their human behavior\nunderstanding and extensive real-world knowledge. Nevertheless, integrating\nLLM-generated information into recommender systems presents challenges,\nincluding the risk of noisy information and the need for additional knowledge\ntransfer. In this paper, we propose an LLM-based user-side knowledge inference\nmethod alongside a carefully designed recommendation framework to address these\nchallenges. Our approach employs LLMs to infer user interests based on\nhistorical behaviors, integrating this user-side information with item-side and\ncollaborative data to construct a hybrid structure: the Collaborative Interest\nKnowledge Graph (CIKG). Furthermore, we propose a CIKG-based recommendation\nframework that includes a user interest reconstruction module and a\ncross-domain contrastive learning module to mitigate potential noise and\nfacilitate knowledge transfer. We conduct extensive experiments on three\nreal-world datasets to validate the effectiveness of our method. Our approach\nachieves state-of-the-art performance compared to competitive baselines,\nparticularly for users with sparse interactions.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13544v1",
    "published_date": "2024-12-18 06:43:56 UTC",
    "updated_date": "2024-12-18 06:43:56 UTC"
  },
  {
    "arxiv_id": "2412.13543v1",
    "title": "Query-centric Audio-Visual Cognition Network for Moment Retrieval, Segmentation and Step-Captioning",
    "authors": [
      "Yunbin Tu",
      "Liang Li",
      "Li Su",
      "Qingming Huang"
    ],
    "abstract": "Video has emerged as a favored multimedia format on the internet. To better\ngain video contents, a new topic HIREST is presented, including video\nretrieval, moment retrieval, moment segmentation, and step-captioning. The\npioneering work chooses the pre-trained CLIP-based model for video retrieval,\nand leverages it as a feature extractor for other three challenging tasks\nsolved in a multi-task learning paradigm. Nevertheless, this work struggles to\nlearn the comprehensive cognition of user-preferred content, due to\ndisregarding the hierarchies and association relations across modalities. In\nthis paper, guided by the shallow-to-deep principle, we propose a query-centric\naudio-visual cognition (QUAG) network to construct a reliable multi-modal\nrepresentation for moment retrieval, segmentation and step-captioning.\nSpecifically, we first design the modality-synergistic perception to obtain\nrich audio-visual content, by modeling global contrastive alignment and local\nfine-grained interaction between visual and audio modalities. Then, we devise\nthe query-centric cognition that uses the deep-level query to perform the\ntemporal-channel filtration on the shallow-level audio-visual representation.\nThis can cognize user-preferred content and thus attain a query-centric\naudio-visual representation for three tasks. Extensive experiments show QUAG\nachieves the SOTA results on HIREST. Further, we test QUAG on the query-based\nvideo summarization task and verify its good generalization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13543v1",
    "published_date": "2024-12-18 06:43:06 UTC",
    "updated_date": "2024-12-18 06:43:06 UTC"
  },
  {
    "arxiv_id": "2412.13520v1",
    "title": "ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning",
    "authors": [
      "Yi Huang",
      "Fangyin Cheng",
      "Fan Zhou",
      "Jiahui Li",
      "Jian Gong",
      "Hongjun Yang",
      "Zhidong Fan",
      "Caigao Jiang",
      "Siqiao Xue",
      "Faqiang Chen"
    ],
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in data analytics when integrated with Multi-Agent Systems (MAS).\nHowever, these systems often struggle with complex tasks that involve diverse\nfunctional requirements and intricate data processing challenges, necessitating\ncustomized solutions that lack broad applicability. Furthermore, current MAS\nfail to emulate essential human-like traits such as self-planning,\nself-monitoring, and collaborative work in dynamic environments, leading to\ninefficiencies and resource wastage. To address these limitations, we propose\nROMAS, a novel Role-Based M ulti-A gent System designed to adapt to various\nscenarios while enabling low code development and one-click deployment. ROMAS\nhas been effectively deployed in DB-GPT [Xue et al., 2023a, 2024b], a\nwell-known project utilizing LLM-powered database analytics, showcasing its\npractical utility in real-world scenarios. By integrating role-based\ncollaborative mechanisms for self-monitoring and self-planning, and leveraging\nexisting MAS capabilities to enhance database interactions, ROMAS offers a more\neffective and versatile solution. Experimental evaluations of ROMAS demonstrate\nits superiority across multiple scenarios, highlighting its potential to\nadvance the field of multi-agent data analytics.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13520v1",
    "published_date": "2024-12-18 05:45:39 UTC",
    "updated_date": "2024-12-18 05:45:39 UTC"
  },
  {
    "arxiv_id": "2412.13514v1",
    "title": "Tuning Music Education: AI-Powered Personalization in Learning Music",
    "authors": [
      "Mayank Sanganeria",
      "Rohan Gala"
    ],
    "abstract": "Recent AI-driven step-function advances in several longstanding problems in\nmusic technology are opening up new avenues to create the next generation of\nmusic education tools. Creating personalized, engaging, and effective learning\nexperiences are continuously evolving challenges in music education. Here we\npresent two case studies using such advances in music technology to address\nthese challenges. In our first case study we showcase an application that uses\nAutomatic Chord Recognition to generate personalized exercises from audio\ntracks, connecting traditional ear training with real-world musical contexts.\nIn the second case study we prototype adaptive piano method books that use\nAutomatic Music Transcription to generate exercises at different skill levels\nwhile retaining a close connection to musical interests. These applications\ndemonstrate how recent AI developments can democratize access to high-quality\nmusic education and promote rich interaction with music in the age of\ngenerative AI. We hope this work inspires other efforts in the community, aimed\nat removing barriers to access to high-quality music education and fostering\nhuman participation in musical expression.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Creative AI Track",
    "pdf_url": "http://arxiv.org/pdf/2412.13514v1",
    "published_date": "2024-12-18 05:25:42 UTC",
    "updated_date": "2024-12-18 05:25:42 UTC"
  },
  {
    "arxiv_id": "2412.13503v2",
    "title": "VaeDiff-DocRE: End-to-end Data Augmentation Framework for Document-level Relation Extraction",
    "authors": [
      "Khai Phan Tran",
      "Wen Hua",
      "Xue Li"
    ],
    "abstract": "Document-level Relation Extraction (DocRE) aims to identify relationships\nbetween entity pairs within a document. However, most existing methods assume a\nuniform label distribution, resulting in suboptimal performance on real-world,\nimbalanced datasets. To tackle this challenge, we propose a novel data\naugmentation approach using generative models to enhance data from the\nembedding space. Our method leverages the Variational Autoencoder (VAE)\narchitecture to capture all relation-wise distributions formed by entity pair\nrepresentations and augment data for underrepresented relations. To better\ncapture the multi-label nature of DocRE, we parameterize the VAE's latent space\nwith a Diffusion Model. Additionally, we introduce a hierarchical training\nframework to integrate the proposed VAE-based augmentation module into DocRE\nsystems. Experiments on two benchmark datasets demonstrate that our method\noutperforms state-of-the-art models, effectively addressing the long-tail\ndistribution problem in DocRE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13503v2",
    "published_date": "2024-12-18 04:55:29 UTC",
    "updated_date": "2025-01-13 10:43:11 UTC"
  },
  {
    "arxiv_id": "2412.13501v1",
    "title": "GUI Agents: A Survey",
    "authors": [
      "Dang Nguyen",
      "Jian Chen",
      "Yu Wang",
      "Gang Wu",
      "Namyong Park",
      "Zhengmian Hu",
      "Hanjia Lyu",
      "Junda Wu",
      "Ryan Aponte",
      "Yu Xia",
      "Xintong Li",
      "Jing Shi",
      "Hongjie Chen",
      "Viet Dac Lai",
      "Zhouhang Xie",
      "Sungchul Kim",
      "Ruiyi Zhang",
      "Tong Yu",
      "Mehrab Tanjim",
      "Nesreen K. Ahmed",
      "Puneet Mathur",
      "Seunghyun Yoon",
      "Lina Yao",
      "Branislav Kveton",
      "Thien Huu Nguyen",
      "Trung Bui",
      "Tianyi Zhou",
      "Ryan A. Rossi",
      "Franck Dernoncourt"
    ],
    "abstract": "Graphical User Interface (GUI) agents, powered by Large Foundation Models,\nhave emerged as a transformative approach to automating human-computer\ninteraction. These agents autonomously interact with digital systems or\nsoftware applications via GUIs, emulating human actions such as clicking,\ntyping, and navigating visual elements across diverse platforms. Motivated by\nthe growing interest and fundamental importance of GUI agents, we provide a\ncomprehensive survey that categorizes their benchmarks, evaluation metrics,\narchitectures, and training methods. We propose a unified framework that\ndelineates their perception, reasoning, planning, and acting capabilities.\nFurthermore, we identify important open challenges and discuss key future\ndirections. Finally, this work serves as a basis for practitioners and\nresearchers to gain an intuitive understanding of current progress, techniques,\nbenchmarks, and critical open problems that remain to be addressed.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13501v1",
    "published_date": "2024-12-18 04:48:28 UTC",
    "updated_date": "2024-12-18 04:48:28 UTC"
  },
  {
    "arxiv_id": "2412.13495v1",
    "title": "Federated t-SNE and UMAP for Distributed Data Visualization",
    "authors": [
      "Dong Qiao",
      "Xinxian Ma",
      "Jicong Fan"
    ],
    "abstract": "High-dimensional data visualization is crucial in the big data era and these\ntechniques such as t-SNE and UMAP have been widely used in science and\nengineering. Big data, however, is often distributed across multiple data\ncenters and subject to security and privacy concerns, which leads to\ndifficulties for the standard algorithms of t-SNE and UMAP. To tackle the\nchallenge, this work proposes Fed-tSNE and Fed-UMAP, which provide\nhigh-dimensional data visualization under the framework of federated learning,\nwithout exchanging data across clients or sending data to the central server.\nThe main idea of Fed-tSNE and Fed-UMAP is implicitly learning the distribution\ninformation of data in a manner of federated learning and then estimating the\nglobal distance matrix for t-SNE and UMAP. To further enhance the protection of\ndata privacy, we propose Fed-tSNE+ and Fed-UMAP+. We also extend our idea to\nfederated spectral clustering, yielding algorithms of clustering distributed\ndata. In addition to these new algorithms, we offer theoretical guarantees of\noptimization convergence, distance and similarity estimation, and differential\nprivacy. Experiments on multiple datasets demonstrate that, compared to the\noriginal algorithms, the accuracy drops of our federated algorithms are tiny.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper was accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13495v1",
    "published_date": "2024-12-18 04:33:11 UTC",
    "updated_date": "2024-12-18 04:33:11 UTC"
  },
  {
    "arxiv_id": "2412.13489v1",
    "title": "Analysis of Higher-Order Ising Hamiltonians",
    "authors": [
      "Yunuo Cen",
      "Zhiwei Zhang",
      "Zixuan Wang",
      "Yimin Wang",
      "Xuanyao Fong"
    ],
    "abstract": "It is challenging to scale Ising machines for industrial-level problems due\nto algorithm or hardware limitations. Although higher-order Ising models\nprovide a more compact encoding, they are, however, hard to physically\nimplement. This work proposes a theoretical framework of a higher-order Ising\nsimulator, IsingSim. The Ising spins and gradients in IsingSim are decoupled\nand self-customizable. We significantly accelerate the simulation speed via a\nbidirectional approach for differentiating the hyperedge functions. Our\nproof-of-concept implementation verifies the theoretical framework by\nsimulating the Ising spins with exact and approximate gradients. Experiment\nresults show that our novel framework can be a useful tool for providing design\nguidelines for higher-order Ising machines.",
    "categories": [
      "cs.AI",
      "cond-mat.stat-mech",
      "physics.comp-ph",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13489v1",
    "published_date": "2024-12-18 04:15:11 UTC",
    "updated_date": "2024-12-18 04:15:11 UTC"
  },
  {
    "arxiv_id": "2412.13488v1",
    "title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models",
    "authors": [
      "Xinxin Liu",
      "Aaron Thomas",
      "Cheng Zhang",
      "Jianyi Cheng",
      "Yiren Zhao",
      "Xitong Gao"
    ],
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT. Our work is open source and\navailable to the community at [https://github.com/0-ml/speft].",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13488v1",
    "published_date": "2024-12-18 04:14:35 UTC",
    "updated_date": "2024-12-18 04:14:35 UTC"
  },
  {
    "arxiv_id": "2412.15280v1",
    "title": "Context-DPO: Aligning Language Models for Context-Faithfulness",
    "authors": [
      "Baolong Bi",
      "Shaohan Huang",
      "Yiwei Wang",
      "Tianchi Yang",
      "Zihan Zhang",
      "Haizhen Huang",
      "Lingrui Mei",
      "Junfeng Fang",
      "Zehao Li",
      "Furu Wei",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang",
      "Shenghua Liu"
    ],
    "abstract": "Reliable responses from large language models (LLMs) require adherence to\nuser instructions and retrieved information. While alignment techniques help\nLLMs align with human intentions and values, improving context-faithfulness\nthrough alignment remains underexplored. To address this, we propose\n$\\textbf{Context-DPO}$, the first alignment method specifically designed to\nenhance LLMs' context-faithfulness. We introduce $\\textbf{ConFiQA}$, a\nbenchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with\nknowledge conflicts to evaluate context-faithfulness. By leveraging faithful\nand stubborn responses to questions with provided context from ConFiQA, our\nContext-DPO aligns LLMs through direct preference optimization. Extensive\nexperiments demonstrate that our Context-DPO significantly improves\ncontext-faithfulness, achieving 35% to 280% improvements on popular open-source\nmodels. Further analysis demonstrates that Context-DPO preserves LLMs'\ngenerative capabilities while providing interpretable insights into context\nutilization. Our code and data are released at\nhttps://github.com/byronBBL/Context-DPO",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15280v1",
    "published_date": "2024-12-18 04:08:18 UTC",
    "updated_date": "2024-12-18 04:08:18 UTC"
  },
  {
    "arxiv_id": "2412.15279v2",
    "title": "Functional connectomes of neural networks",
    "authors": [
      "Tananun Songdechakraiwut",
      "Yutong Wu"
    ],
    "abstract": "The human brain is a complex system, and understanding its mechanisms has\nbeen a long-standing challenge in neuroscience. The study of the functional\nconnectome, which maps the functional connections between different brain\nregions, has provided valuable insights through various advanced analysis\ntechniques developed over the years. Similarly, neural networks, inspired by\nthe brain's architecture, have achieved notable success in diverse applications\nbut are often noted for their lack of interpretability. In this paper, we\npropose a novel approach that bridges neural networks and human brain functions\nby leveraging brain-inspired techniques. Our approach, grounded in the insights\nfrom the functional connectome, offers scalable ways to characterize topology\nof large neural networks using stable statistical and machine learning\ntechniques. Our empirical analysis demonstrates its capability to enhance the\ninterpretability of neural networks, providing a deeper understanding of their\nunderlying mechanisms.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "Published at the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.15279v2",
    "published_date": "2024-12-18 03:46:30 UTC",
    "updated_date": "2025-04-11 04:39:07 UTC"
  },
  {
    "arxiv_id": "2412.13477v1",
    "title": "Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a Deep Learning-Based Latent Space Data Assimilation Framework",
    "authors": [
      "Qingyu Zheng",
      "Guijun Han",
      "Wei Li",
      "Lige Cao",
      "Gongfu Zhou",
      "Haowen Wu",
      "Qi Shao",
      "Ru Wang",
      "Xiaobo Wu",
      "Xudong Cui",
      "Hong Li",
      "Xuan Wang"
    ],
    "abstract": "Advances in data assimilation (DA) methods have greatly improved the accuracy\nof Earth system predictions. To fuse multi-source data and reconstruct the\nnonlinear evolution missing from observations, geoscientists are developing\nfuture-oriented DA methods. In this paper, we redesign a purely data-driven\nlatent space DA framework (DeepDA) that employs a generative artificial\nintelligence model to capture the nonlinear evolution in sea surface\ntemperature. Under variational constraints, DeepDA embedded with nonlinear\nfeatures can effectively fuse heterogeneous data. The results show that DeepDA\nremains highly stable in capturing and generating nonlinear evolutions even\nwhen a large amount of observational information is missing. It can be found\nthat when only 10% of the observation information is available, the error\nincrease of DeepDA does not exceed 40%. Furthermore, DeepDA has been shown to\nbe robust in the fusion of real observations and ensemble simulations. In\nparticular, this paper provides a mechanism analysis of the nonlinear evolution\ngenerated by DeepDA from the perspective of physical patterns, which reveals\nthe inherent explainability of our DL model in capturing multi-scale ocean\nsignals.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "physics.geo-ph"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "31 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13477v1",
    "published_date": "2024-12-18 03:41:34 UTC",
    "updated_date": "2024-12-18 03:41:34 UTC"
  },
  {
    "arxiv_id": "2412.13475v1",
    "title": "A Statistical and Multi-Perspective Revisiting of the Membership Inference Attack in Large Language Models",
    "authors": [
      "Bowen Chen",
      "Namgi Han",
      "Yusuke Miyao"
    ],
    "abstract": "The lack of data transparency in Large Language Models (LLMs) has highlighted\nthe importance of Membership Inference Attack (MIA), which differentiates\ntrained (member) and untrained (non-member) data. Though it shows success in\nprevious studies, recent research reported a near-random performance in\ndifferent settings, highlighting a significant performance inconsistency. We\nassume that a single setting doesn't represent the distribution of the vast\ncorpora, causing members and non-members with different distributions to be\nsampled and causing inconsistency. In this study, instead of a single setting,\nwe statistically revisit MIA methods from various settings with thousands of\nexperiments for each MIA method, along with study in text feature, embedding,\nthreshold decision, and decoding dynamics of members and non-members. We found\nthat (1) MIA performance improves with model size and varies with domains,\nwhile most methods do not statistically outperform baselines, (2) Though MIA\nperformance is generally low, a notable amount of differentiable member and\nnon-member outliers exists and vary across MIA methods, (3) Deciding a\nthreshold to separate members and non-members is an overlooked challenge, (4)\nText dissimilarity and long text benefit MIA performance, (5) Differentiable or\nnot is reflected in the LLM embedding, (6) Member and non-members show\ndifferent decoding dynamics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "main content 8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13475v1",
    "published_date": "2024-12-18 03:39:42 UTC",
    "updated_date": "2024-12-18 03:39:42 UTC"
  },
  {
    "arxiv_id": "2412.13471v1",
    "title": "Gradual Vigilance and Interval Communication: Enhancing Value Alignment in Multi-Agent Debates",
    "authors": [
      "Rui Zou",
      "Mengqi Wei",
      "Jintian Feng",
      "Qian Wan",
      "Jianwen Sun",
      "Sannyuya Liu"
    ],
    "abstract": "In recent years, large language models have shown exceptional performance in\nfulfilling diverse human needs. However, their training data can introduce\nharmful content, underscoring the necessity for robust value alignment.\nMainstream methods, which depend on feedback learning and supervised training,\nare resource-intensive and may constrain the full potential of the models.\nMulti-Agent Debate (MAD) offers a more efficient and innovative solution by\nenabling the generation of reliable answers through agent interactions. To\napply MAD to value alignment, we examine the relationship between the\nhelpfulness and harmlessness of debate outcomes and individual responses, and\npropose a MAD based framework Gradual Vigilance and Interval Communication\n(GVIC). GVIC allows agents to assess risks with varying levels of vigilance and\nto exchange diverse information through interval communication. We\ntheoretically prove that GVIC optimizes debate efficiency while reducing\ncommunication overhead. Experimental results demonstrate that GVIC consistently\noutperforms baseline methods across various tasks and datasets, particularly\nexcelling in harmfulness mitigation and fraud prevention. Additionally, GVIC\nexhibits strong adaptability across different base model sizes, including both\nunaligned and aligned models, and across various task types.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13471v1",
    "published_date": "2024-12-18 03:36:08 UTC",
    "updated_date": "2024-12-18 03:36:08 UTC"
  },
  {
    "arxiv_id": "2412.15278v1",
    "title": "DreaMark: Rooting Watermark in Score Distillation Sampling Generated Neural Radiance Fields",
    "authors": [
      "Xingyu Zhu",
      "Xiapu Luo",
      "Xuetao Wei"
    ],
    "abstract": "Recent advancements in text-to-3D generation can generate neural radiance\nfields (NeRFs) with score distillation sampling, enabling 3D asset creation\nwithout real-world data capture. With the rapid advancement in NeRF generation\nquality, protecting the copyright of the generated NeRF has become increasingly\nimportant. While prior works can watermark NeRFs in a post-generation way, they\nsuffer from two vulnerabilities. First, a delay lies between NeRF generation\nand watermarking because the secret message is embedded into the NeRF model\npost-generation through fine-tuning. Second, generating a non-watermarked NeRF\nas an intermediate creates a potential vulnerability for theft. To address both\nissues, we propose Dreamark to embed a secret message by backdooring the NeRF\nduring NeRF generation. In detail, we first pre-train a watermark decoder.\nThen, the Dreamark generates backdoored NeRFs in a way that the target secret\nmessage can be verified by the pre-trained watermark decoder on an arbitrary\ntrigger viewport. We evaluate the generation quality and watermark robustness\nagainst image- and model-level attacks. Extensive experiments show that the\nwatermarking process will not degrade the generation quality, and the watermark\nachieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise)\nand model-level attacks (e.g., pruning attack).",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15278v1",
    "published_date": "2024-12-18 03:27:13 UTC",
    "updated_date": "2024-12-18 03:27:13 UTC"
  },
  {
    "arxiv_id": "2412.13467v1",
    "title": "Transducer Tuning: Efficient Model Adaptation for Software Tasks Using Code Property Graphs",
    "authors": [
      "Imam Nur Bani Yusuf",
      "Lingxiao Jiang"
    ],
    "abstract": "Large language models have demonstrated promising performance across various\nsoftware engineering tasks. While fine-tuning is a common practice to adapt\nthese models for downstream tasks, it becomes challenging in\nresource-constrained environments due to increased memory requirements from\ngrowing trainable parameters in increasingly large language models. We\nintroduce \\approach, a technique to adapt large models for downstream code\ntasks using Code Property Graphs (CPGs). Our approach introduces a modular\ncomponent called \\transducer that enriches code embeddings with structural and\ndependency information from CPGs. The Transducer comprises two key components:\nGraph Vectorization Engine (GVE) and Attention-Based Fusion Layer (ABFL). GVE\nextracts CPGs from input source code and transforms them into graph feature\nvectors. ABFL then fuses those graphs feature vectors with initial code\nembeddings from a large language model. By optimizing these transducers for\ndifferent downstream tasks, our approach enhances the models without the need\nto fine-tune them for specific tasks. We have evaluated \\approach on three\ndownstream tasks: code summarization, assert generation, and code translation.\nOur results demonstrate competitive performance compared to full parameter\nfine-tuning while reducing up to 99\\% trainable parameters to save memory.\n\\approach also remains competitive against other fine-tuning approaches (e.g.,\nLoRA, Prompt-Tuning, Prefix-Tuning) while using only 1.5\\%-80\\% of their\ntrainable parameters. Our findings show that integrating structural and\ndependency information through Transducer Tuning enables more efficient model\nadaptation, making it easier for users to adapt large models in\nresource-constrained settings.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2412.13467v1",
    "published_date": "2024-12-18 03:25:17 UTC",
    "updated_date": "2024-12-18 03:25:17 UTC"
  },
  {
    "arxiv_id": "2412.13463v1",
    "title": "FlexPose: Pose Distribution Adaptation with Limited Guidance",
    "authors": [
      "Zixiao Wang",
      "Junwu Weng",
      "Mengyuan Liu",
      "Bei Yu"
    ],
    "abstract": "Numerous well-annotated human key-point datasets are publicly available to\ndate. However, annotating human poses for newly collected images is still a\ncostly and time-consuming progress. Pose distributions from different datasets\nshare similar pose hinge-structure priors with different geometric\ntransformations, such as pivot orientation, joint rotation, and bone length\nratio. The difference between Pose distributions is essentially the difference\nbetween the transformation distributions. Inspired by this fact, we propose a\nmethod to calibrate a pre-trained pose generator in which the pose prior has\nalready been learned to an adapted one following a new pose distribution. We\ntreat the representation of human pose joint coordinates as skeleton image and\ntransfer a pre-trained pose annotation generator with only a few annotation\nguidance. By fine-tuning a limited number of linear layers that closely related\nto the pose transformation, the adapted generator is able to produce any number\nof pose annotations that are similar to the target poses. We evaluate our\nproposed method, FlexPose, on several cross-dataset settings both qualitatively\nand quantitatively, which demonstrates that our approach achieves\nstate-of-the-art performance compared to the existing generative-model-based\ntransfer learning methods when given limited annotation guidance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI25, 12 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13463v1",
    "published_date": "2024-12-18 03:18:11 UTC",
    "updated_date": "2024-12-18 03:18:11 UTC"
  },
  {
    "arxiv_id": "2412.13461v2",
    "title": "Look Inside for More: Internal Spatial Modality Perception for 3D Anomaly Detection",
    "authors": [
      "Hanzhe Liang",
      "Guoyang Xie",
      "Chengbin Hou",
      "Bingshu Wang",
      "Can Gao",
      "Jinbao Wang"
    ],
    "abstract": "3D anomaly detection has recently become a significant focus in computer\nvision. Several advanced methods have achieved satisfying anomaly detection\nperformance. However, they typically concentrate on the external structure of\n3D samples and struggle to leverage the internal information embedded within\nsamples. Inspired by the basic intuition of why not look inside for more, we\nintroduce a straightforward method named Internal Spatial Modality\nPerception~(ISMP) to explore the feature representation from internal views\nfully. Specifically, our proposed ISMP consists of a critical perception\nmodule, Spatial Insight Engine~(SIE), which abstracts complex internal\ninformation of point clouds into essential global features. Besides, to better\nalign structural information with point data, we propose an enhanced key point\nfeature extraction module for amplifying spatial structure feature\nrepresentation. Simultaneously, a novel feature filtering module is\nincorporated to reduce noise and redundant features for further aligning\nprecise spatial structure. Extensive experiments validate the effectiveness of\nour proposed method, achieving object-level and pixel-level AUROC improvements\nof 3.2\\% and 13.1\\%, respectively, on the Real3D-AD benchmarks. Note that the\nstrong generalization ability of SIE has been theoretically proven and is\nverified in both classification and segmentation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI2025 Poster",
    "pdf_url": "http://arxiv.org/pdf/2412.13461v2",
    "published_date": "2024-12-18 03:14:11 UTC",
    "updated_date": "2025-03-10 15:25:59 UTC"
  },
  {
    "arxiv_id": "2412.15277v1",
    "title": "PLPP: Prompt Learning with Perplexity Is Self-Distillation for Vision-Language Models",
    "authors": [
      "Biao Liu",
      "Wenyi Fang",
      "Xiaoyu Wu",
      "Yang Zheng",
      "Zheng Hu",
      "Bo Yuan"
    ],
    "abstract": "Pre-trained Vision-Language (VL) models such as CLIP have demonstrated their\nexcellent performance across numerous downstream tasks. A recent method,\nContext Optimization (CoOp), further improves the performance of VL models on\ndownstream tasks by introducing prompt learning. CoOp optimizes a set of\nlearnable vectors, aka prompt, and freezes the whole CLIP model. However,\nrelying solely on CLIP loss to fine-tune prompts can lead to models that are\nprone to overfitting on downstream task. To address this issue, we propose a\nplug-in prompt-regularization method called PLPP (Prompt Learning with\nPerPlexity), which use perplexity loss to regularize prompt learning. PLPP\ndesigns a two-step operation to compute the perplexity for prompts: (a)\ncalculating cosine similarity between the weight of the embedding layer and\nprompts to get labels, (b) introducing a language model (LM) head that requires\nno training behind text encoder to output word probability distribution.\nMeanwhile, we unveil that the essence of PLPP is inherently a form of\nself-distillation. To further prevent overfitting as well as to reduce the\nadditional computation introduced by PLPP, we turn the hard label to soft label\nand choose top-$k$ values for calculating the perplexity loss. For accelerating\nmodel convergence, we introduce mutual self-distillation learning, that is\nperplexity and inverted perplexity loss. The experiments conducted on four\nclassification tasks indicate that PLPP exhibits superior performance compared\nto existing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15277v1",
    "published_date": "2024-12-18 03:08:53 UTC",
    "updated_date": "2024-12-18 03:08:53 UTC"
  },
  {
    "arxiv_id": "2412.15276v1",
    "title": "Exploring Query Efficient Data Generation towards Data-free Model Stealing in Hard Label Setting",
    "authors": [
      "Gaozheng Pei",
      "Shaojie lyu",
      "Ke Ma",
      "Pinci Yang",
      "Qianqian Xu",
      "Yingfei Sun"
    ],
    "abstract": "Data-free model stealing involves replicating the functionality of a target\nmodel into a substitute model without accessing the target model's structure,\nparameters, or training data. The adversary can only access the target model's\npredictions for generated samples. Once the substitute model closely\napproximates the behavior of the target model, attackers can exploit its\nwhite-box characteristics for subsequent malicious activities, such as\nadversarial attacks. Existing methods within cooperative game frameworks often\nproduce samples with high confidence for the prediction of the substitute\nmodel, which makes it difficult for the substitute model to replicate the\nbehavior of the target model. This paper presents a new data-free model\nstealing approach called Query Efficient Data Generation (\\textbf{QEDG}). We\nintroduce two distinct loss functions to ensure the generation of sufficient\nsamples that closely and uniformly align with the target model's decision\nboundary across multiple classes. Building on the limitation of current\nmethods, which typically yield only one piece of supervised information per\nquery, we propose the query-free sample augmentation that enables the\nacquisition of additional supervised information without increasing the number\nof queries. Motivated by theoretical analysis, we adopt the consistency rate\nmetric, which more accurately evaluates the similarity between the substitute\nand target models. We conducted extensive experiments to verify the\neffectiveness of our proposed method, which achieved better performance with\nfewer queries compared to the state-of-the-art methods on the real\n\\textbf{MLaaS} scenario and five datasets.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15276v1",
    "published_date": "2024-12-18 03:03:15 UTC",
    "updated_date": "2024-12-18 03:03:15 UTC"
  },
  {
    "arxiv_id": "2412.13454v1",
    "title": "Pre-training a Density-Aware Pose Transformer for Robust LiDAR-based 3D Human Pose Estimation",
    "authors": [
      "Xiaoqi An",
      "Lin Zhao",
      "Chen Gong",
      "Jun Li",
      "Jian Yang"
    ],
    "abstract": "With the rapid development of autonomous driving, LiDAR-based 3D Human Pose\nEstimation (3D HPE) is becoming a research focus. However, due to the noise and\nsparsity of LiDAR-captured point clouds, robust human pose estimation remains\nchallenging. Most of the existing methods use temporal information, multi-modal\nfusion, or SMPL optimization to correct biased results. In this work, we try to\nobtain sufficient information for 3D HPE only by modeling the intrinsic\nproperties of low-quality point clouds. Hence, a simple yet powerful method is\nproposed, which provides insights both on modeling and augmentation of point\nclouds. Specifically, we first propose a concise and effective density-aware\npose transformer (DAPT) to get stable keypoint representations. By using a set\nof joint anchors and a carefully designed exchange module, valid information is\nextracted from point clouds with different densities. Then 1D heatmaps are\nutilized to represent the precise locations of the keypoints. Secondly, a\ncomprehensive LiDAR human synthesis and augmentation method is proposed to\npre-train the model, enabling it to acquire a better human body prior. We\nincrease the diversity of point clouds by randomly sampling human positions and\norientations and by simulating occlusions through the addition of laser-level\nmasks. Extensive experiments have been conducted on multiple datasets,\nincluding IMU-annotated LidarHuman26M, SLOPER4D, and manually annotated Waymo\nOpen Dataset v2.0 (Waymo), HumanM3. Our method demonstrates SOTA performance in\nall scenarios. In particular, compared with LPFormer on Waymo, we reduce the\naverage MPJPE by $10.0mm$. Compared with PRN on SLOPER4D, we notably reduce the\naverage MPJPE by $20.7mm$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13454v1",
    "published_date": "2024-12-18 02:54:30 UTC",
    "updated_date": "2024-12-18 02:54:30 UTC"
  },
  {
    "arxiv_id": "2412.13452v1",
    "title": "ConDo: Continual Domain Expansion for Absolute Pose Regression",
    "authors": [
      "Zijun Li",
      "Zhipeng Cai",
      "Bochun Yang",
      "Xuelun Shen",
      "Siqi Shen",
      "Xiaoliang Fan",
      "Michael Paulitsch",
      "Cheng Wang"
    ],
    "abstract": "Visual localization is a fundamental machine learning problem. Absolute Pose\nRegression (APR) trains a scene-dependent model to efficiently map an input\nimage to the camera pose in a pre-defined scene. However, many applications\nhave continually changing environments, where inference data at novel poses or\nscene conditions (weather, geometry) appear after deployment. Training APR on a\nfixed dataset leads to overfitting, making it fail catastrophically on\nchallenging novel data. This work proposes Continual Domain Expansion (ConDo),\nwhich continually collects unlabeled inference data to update the deployed APR.\nInstead of applying standard unsupervised domain adaptation methods which are\nineffective for APR, ConDo effectively learns from unlabeled data by distilling\nknowledge from scene-agnostic localization methods. By sampling data uniformly\nfrom historical and newly collected data, ConDo can effectively expand the\ngeneralization domain of APR. Large-scale benchmarks with various scene types\nare constructed to evaluate models under practical (long-term) data changes.\nConDo consistently and significantly outperforms baselines across\narchitectures, scene types, and data changes. On challenging scenes (Fig.1), it\nreduces the localization error by >7x (14.8m vs 1.7m). Analysis shows the\nrobustness of ConDo against compute budgets, replay buffer sizes and teacher\nprediction noise. Comparing to model re-training, ConDo achieves similar\nperformance up to 25x faster.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13452v1",
    "published_date": "2024-12-18 02:49:20 UTC",
    "updated_date": "2024-12-18 02:49:20 UTC"
  },
  {
    "arxiv_id": "2412.13446v1",
    "title": "Toward an Insider Threat Education Platform: A Theoretical Literature Review",
    "authors": [
      "Haywood Gelman",
      "John D. Hastings",
      "David Kenley",
      "Eleanor Loiacono"
    ],
    "abstract": "Insider threats (InTs) within organizations are small in number but have a\ndisproportionate ability to damage systems, information, and infrastructure.\nExisting InT research studies the problem from psychological, technical, and\neducational perspectives. Proposed theories include research on psychological\nindicators, machine learning, user behavioral log analysis, and educational\nmethods to teach employees recognition and mitigation techniques. Because InTs\nare a human problem, training methods that address InT detection from a\nbehavioral perspective are critical. While numerous technological and\npsychological theories exist on detection, prevention, and mitigation, few\ntraining methods prioritize psychological indicators. This literature review\nstudied peer-reviewed, InT research organized by subtopic and extracted\ncritical theories from psychological, technical, and educational disciplines.\nIn doing so, this is the first study to comprehensively organize research\nacross all three approaches in a manner which properly informs the development\nof an InT education platform.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.SI",
      "K.4.1; C.2.0; I.2.6; H.5.2; H.1.2"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.13446v1",
    "published_date": "2024-12-18 02:34:33 UTC",
    "updated_date": "2024-12-18 02:34:33 UTC"
  },
  {
    "arxiv_id": "2412.13442v1",
    "title": "Communication-Efficient Personalized Federal Graph Learning via Low-Rank Decomposition",
    "authors": [
      "Ruyue Liu",
      "Rong Yin",
      "Xiangzhen Bo",
      "Xiaoshuai Hao",
      "Xingrui Zhou",
      "Yong Liu",
      "Can Ma",
      "Weiping Wang"
    ],
    "abstract": "Federated graph learning (FGL) has gained significant attention for enabling\nheterogeneous clients to process their private graph data locally while\ninteracting with a centralized server, thus maintaining privacy. However, graph\ndata on clients are typically non-IID, posing a challenge for a single model to\nperform well across all clients. Another major bottleneck of FGL is the high\ncost of communication. To address these challenges, we propose a\ncommunication-efficient personalized federated graph learning algorithm, CEFGL.\nOur method decomposes the model parameters into low-rank generic and sparse\nprivate models. We employ a dual-channel encoder to learn sparse local\nknowledge in a personalized manner and low-rank global knowledge in a shared\nmanner. Additionally, we perform multiple local stochastic gradient descent\niterations between communication phases and integrate efficient compression\ntechniques into the algorithm. The advantage of CEFGL lies in its ability to\ncapture common and individual knowledge more precisely. By utilizing low-rank\nand sparse parameters along with compression techniques, CEFGL significantly\nreduces communication complexity. Extensive experiments demonstrate that our\nmethod achieves optimal classification accuracy in a variety of heterogeneous\nenvironments across sixteen datasets. Specifically, compared to the\nstate-of-the-art method FedStar, the proposed method (with GIN as the base\nmodel) improves accuracy by 5.64\\% on cross-datasets setting CHEM, reduces\ncommunication bits by a factor of 18.58, and reduces the communication time by\na factor of 1.65.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13442v1",
    "published_date": "2024-12-18 02:26:07 UTC",
    "updated_date": "2024-12-18 02:26:07 UTC"
  },
  {
    "arxiv_id": "2412.13441v1",
    "title": "FlashVTG: Feature Layering and Adaptive Score Handling Network for Video Temporal Grounding",
    "authors": [
      "Zhuo Cao",
      "Bingqing Zhang",
      "Heming Du",
      "Xin Yu",
      "Xue Li",
      "Sen Wang"
    ],
    "abstract": "Text-guided Video Temporal Grounding (VTG) aims to localize relevant segments\nin untrimmed videos based on textual descriptions, encompassing two subtasks:\nMoment Retrieval (MR) and Highlight Detection (HD). Although previous typical\nmethods have achieved commendable results, it is still challenging to retrieve\nshort video moments. This is primarily due to the reliance on sparse and\nlimited decoder queries, which significantly constrain the accuracy of\npredictions. Furthermore, suboptimal outcomes often arise because previous\nmethods rank predictions based on isolated predictions, neglecting the broader\nvideo context. To tackle these issues, we introduce FlashVTG, a framework\nfeaturing a Temporal Feature Layering (TFL) module and an Adaptive Score\nRefinement (ASR) module. The TFL module replaces the traditional decoder\nstructure to capture nuanced video content variations across multiple temporal\nscales, while the ASR module improves prediction ranking by integrating context\nfrom adjacent moments and multi-temporal-scale features. Extensive experiments\ndemonstrate that FlashVTG achieves state-of-the-art performance on four widely\nadopted datasets in both MR and HD. Specifically, on the QVHighlights dataset,\nit boosts mAP by 5.8% for MR and 3.3% for HD. For short-moment retrieval,\nFlashVTG increases mAP to 125% of previous SOTA performance. All these\nimprovements are made without adding training burdens, underscoring its\neffectiveness. Our code is available at https://github.com/Zhuo-Cao/FlashVTG.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13441v1",
    "published_date": "2024-12-18 02:23:33 UTC",
    "updated_date": "2024-12-18 02:23:33 UTC"
  },
  {
    "arxiv_id": "2412.16216v1",
    "title": "GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE",
    "authors": [
      "Ting Bai",
      "Yue Yu",
      "Le Huang",
      "Zenan Xu",
      "Zhe Zhao",
      "Chuan Shi"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that\nhas been widely adopted in various downstream applications of LLMs. Together\nwith the Mixture-of-Expert (MoE) technique, fine-tuning approaches have shown\nremarkable improvements in model capability. However, the coordination of\nmultiple experts in existing studies solely relies on the weights assigned by\nthe simple router function. Lack of communication and collaboration among\nexperts exacerbate the instability of LLMs due to the imbalance load problem of\nMoE. To address this issue, we propose a novel MoE graph-based LLM fine-tuning\nframework GraphLoRA, in which a graph router function is designed to capture\nthe collaboration signals among experts by graph neural networks (GNNs).\nGraphLoRA enables all experts to understand input knowledge and share\ninformation from neighbor experts by aggregating operations. Besides, to\nenhance each expert's capability and their collaborations, we design two novel\ncoordination strategies: the Poisson distribution-based distinction strategy\nand the Normal distribution-based load balance strategy. Extensive experiments\non four real-world datasets demonstrate the effectiveness of our GraphLoRA in\nparameter-efficient fine-tuning of LLMs, showing the benefits of facilitating\ncollaborations of multiple experts in the graph router of GraphLoRA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.16216v1",
    "published_date": "2024-12-18 02:18:57 UTC",
    "updated_date": "2024-12-18 02:18:57 UTC"
  },
  {
    "arxiv_id": "2412.13437v1",
    "title": "Deploying Foundation Model Powered Agent Services: A Survey",
    "authors": [
      "Wenchao Xu",
      "Jinyu Chen",
      "Peirong Zheng",
      "Xiaoquan Yi",
      "Tianyi Tian",
      "Wenhui Zhu",
      "Quan Wan",
      "Haozhao Wang",
      "Yunfeng Fan",
      "Qinliang Su",
      "Xuemin Shen"
    ],
    "abstract": "Foundation model (FM) powered agent services are regarded as a promising\nsolution to develop intelligent and personalized applications for advancing\ntoward Artificial General Intelligence (AGI). To achieve high reliability and\nscalability in deploying these agent services, it is essential to\ncollaboratively optimize computational and communication resources, thereby\nensuring effective resource allocation and seamless service delivery. In\npursuit of this vision, this paper proposes a unified framework aimed at\nproviding a comprehensive survey on deploying FM-based agent services across\nheterogeneous devices, with the emphasis on the integration of model and\nresource optimization to establish a robust infrastructure for these services.\nParticularly, this paper begins with exploring various low-level optimization\nstrategies during inference and studies approaches that enhance system\nscalability, such as parallelism techniques and resource scaling methods. The\npaper then discusses several prominent FMs and investigates research efforts\nfocused on inference acceleration, including techniques such as model\ncompression and token reduction. Moreover, the paper also investigates critical\ncomponents for constructing agent services and highlights notable intelligent\napplications. Finally, the paper presents potential research directions for\ndeveloping real-time agent services with high Quality of Service (QoS).",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13437v1",
    "published_date": "2024-12-18 02:15:31 UTC",
    "updated_date": "2024-12-18 02:15:31 UTC"
  },
  {
    "arxiv_id": "2412.13435v1",
    "title": "Lightweight Safety Classification Using Pruned Language Models",
    "authors": [
      "Mason Sawtell",
      "Tula Masterman",
      "Sandi Besen",
      "Jim Brown"
    ],
    "abstract": "In this paper, we introduce a novel technique for content safety and prompt\ninjection classification for Large Language Models. Our technique, Layer\nEnhanced Classification (LEC), trains a Penalized Logistic Regression (PLR)\nclassifier on the hidden state of an LLM's optimal intermediate transformer\nlayer. By combining the computational efficiency of a streamlined PLR\nclassifier with the sophisticated language understanding of an LLM, our\napproach delivers superior performance surpassing GPT-4o and special-purpose\nmodels fine-tuned for each task. We find that small general-purpose models\n(Qwen 2.5 sizes 0.5B, 1.5B, and 3B) and other transformer-based architectures\nlike DeBERTa v3 are robust feature extractors allowing simple classifiers to be\neffectively trained on fewer than 100 high-quality examples. Importantly, the\nintermediate transformer layers of these models typically outperform the final\nlayer across both classification tasks. Our results indicate that a single\ngeneral-purpose LLM can be used to classify content safety, detect prompt\ninjections, and simultaneously generate output tokens. Alternatively, these\nrelatively small LLMs can be pruned to the optimal intermediate layer and used\nexclusively as robust feature extractors. Since our results are consistent on\ndifferent transformer architectures, we infer that robust feature extraction is\nan inherent capability of most, if not all, LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13435v1",
    "published_date": "2024-12-18 02:13:13 UTC",
    "updated_date": "2024-12-18 02:13:13 UTC"
  },
  {
    "arxiv_id": "2412.13432v3",
    "title": "Large Language Model Enhanced Recommender Systems: A Survey",
    "authors": [
      "Qidong Liu",
      "Xiangyu Zhao",
      "Yuhao Wang",
      "Yejing Wang",
      "Zijian Zhang",
      "Yuqi Sun",
      "Xiang Li",
      "Maolin Wang",
      "Pengyue Jia",
      "Chong Chen",
      "Wei Huang",
      "Feng Tian"
    ],
    "abstract": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13432v3",
    "published_date": "2024-12-18 02:07:21 UTC",
    "updated_date": "2025-03-10 08:49:00 UTC"
  },
  {
    "arxiv_id": "2501.01433v2",
    "title": "Mathematical Definition and Systematization of Puzzle Rules",
    "authors": [
      "Itsuki Maeda",
      "Yasuhiro Inoue"
    ],
    "abstract": "While logic puzzles have engaged individuals through problem-solving and\ncritical thinking, the creation of new puzzle rules has largely relied on\nad-hoc processes. Pencil puzzles, such as Slitherlink and Sudoku, represent a\nprominent subset of these games, celebrated for their intellectual challenges\nrooted in combinatorial logic and spatial reasoning. Despite extensive research\ninto solving techniques and automated problem generation, a unified framework\nfor systematic and scalable rule design has been lacking. Here, we introduce a\nmathematical framework for defining and systematizing pencil puzzle rules. This\nframework formalizes grid elements, their positional relationships, and\niterative composition operations, allowing for the incremental construction of\nstructures that form the basis of puzzle rules. Furthermore, we establish a\nformal method to describe constraints and domains for each structure, ensuring\nsolvability and coherence. Applying this framework, we successfully formalized\nthe rules of well-known Nikoli puzzles, including Slitherlink and Sudoku,\ndemonstrating the formal representation of a significant portion (approximately\none-fourth) of existing puzzles. These results validate the potential of the\nframework to systematize and innovate puzzle rule design, establishing a\npathway to automated rule generation. By providing a mathematical foundation\nfor puzzle rule creation, this framework opens avenues for computers,\npotentially enhanced by AI, to design novel puzzle rules tailored to player\npreferences, expanding the scope of puzzle diversity. Beyond its direct\napplication to pencil puzzles, this work illustrates how mathematical\nframeworks can bridge recreational mathematics and algorithmic design, offering\ntools for broader exploration in logic-based systems, with potential\napplications in educational game design, personalized learning, and\ncomputational creativity.",
    "categories": [
      "cs.AI",
      "math.HO"
    ],
    "primary_category": "cs.AI",
    "comment": "16pages",
    "pdf_url": "http://arxiv.org/pdf/2501.01433v2",
    "published_date": "2024-12-18 02:00:53 UTC",
    "updated_date": "2025-01-08 13:08:32 UTC"
  },
  {
    "arxiv_id": "2412.13426v2",
    "title": "Safeguarding System Prompts for LLMs",
    "authors": [
      "Zhifeng Jiang",
      "Zhihua Jin",
      "Guoliang He"
    ],
    "abstract": "Large language models (LLMs) are increasingly utilized in applications where\nsystem prompts, which guide model outputs, play a crucial role. These prompts\noften contain business logic and sensitive information, making their protection\nessential. However, adversarial and even regular user queries can exploit LLM\nvulnerabilities to expose these hidden prompts. To address this issue, we\npropose PromptKeeper, a robust defense mechanism designed to safeguard system\nprompts. PromptKeeper tackles two core challenges: reliably detecting prompt\nleakage and mitigating side-channel vulnerabilities when leakage occurs. By\nframing detection as a hypothesis-testing problem, PromptKeeper effectively\nidentifies both explicit and subtle leakage. Upon detection, it regenerates\nresponses using a dummy prompt, ensuring that outputs remain indistinguishable\nfrom typical interactions when no leakage is present. PromptKeeper ensures\nrobust protection against prompt extraction attacks via either adversarial or\nregular queries, while preserving conversational capability and runtime\nefficiency during benign user interactions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "15 pages, 5 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.13426v2",
    "published_date": "2024-12-18 01:43:25 UTC",
    "updated_date": "2025-01-09 14:33:25 UTC"
  },
  {
    "arxiv_id": "2412.13422v2",
    "title": "Generating Diverse Hypotheses for Inductive Reasoning",
    "authors": [
      "Kang-il Lee",
      "Hyukhun Koh",
      "Dongryeol Lee",
      "Seunghyun Yoon",
      "Minsung Kim",
      "Kyomin Jung"
    ],
    "abstract": "Inductive reasoning - the process of inferring general rules from a small\nnumber of observations - is a fundamental aspect of human intelligence. Recent\nworks suggest that large language models (LLMs) can engage in inductive\nreasoning by sampling multiple hypotheses about the rules and selecting the one\nthat best explains the observations. However, due to the IID sampling,\nsemantically redundant hypotheses are frequently generated, leading to\nsignificant wastage of compute. In this paper, we 1) demonstrate that\nincreasing the temperature to enhance the diversity is limited due to text\ndegeneration issue, and 2) propose a novel method to improve the diversity\nwhile maintaining text quality. We first analyze the effect of increasing the\ntemperature parameter, which is regarded as the LLM's diversity control, on IID\nhypotheses. Our analysis shows that as temperature rises, diversity and\naccuracy of hypotheses increase up to a certain point, but this trend saturates\ndue to text degeneration. To generate hypotheses that are more semantically\ndiverse and of higher quality, we propose a novel approach inspired by human\ninductive reasoning, which we call Mixture of Concepts (MoC). When applied to\nseveral inductive reasoning benchmarks, MoC demonstrated significant\nperformance improvements compared to standard IID sampling and other\napproaches.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13422v2",
    "published_date": "2024-12-18 01:38:09 UTC",
    "updated_date": "2025-02-08 23:39:32 UTC"
  },
  {
    "arxiv_id": "2412.16215v1",
    "title": "Zero-Shot Image Moderation in Google Ads with LLM-Assisted Textual Descriptions and Cross-modal Co-embeddings",
    "authors": [
      "Enming Luo",
      "Wei Qiao",
      "Katie Warren",
      "Jingxiang Li",
      "Eric Xiao",
      "Krishna Viswanathan",
      "Yuan Wang",
      "Yintao Liu",
      "Jimin Li",
      "Ariel Fuxman"
    ],
    "abstract": "We present a scalable and agile approach for ads image content moderation at\nGoogle, addressing the challenges of moderating massive volumes of ads with\ndiverse content and evolving policies. The proposed method utilizes\nhuman-curated textual descriptions and cross-modal text-image co-embeddings to\nenable zero-shot classification of policy violating ads images, bypassing the\nneed for extensive supervised training data and human labeling. By leveraging\nlarge language models (LLMs) and user expertise, the system generates and\nrefines a comprehensive set of textual descriptions representing policy\nguidelines. During inference, co-embedding similarity between incoming images\nand the textual descriptions serves as a reliable signal for policy violation\ndetection, enabling efficient and adaptable ads content moderation. Evaluation\nresults demonstrate the efficacy of this framework in significantly boosting\nthe detection of policy violating content.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16215v1",
    "published_date": "2024-12-18 01:37:53 UTC",
    "updated_date": "2024-12-18 01:37:53 UTC"
  },
  {
    "arxiv_id": "2412.16213v1",
    "title": "AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF Models",
    "authors": [
      "Tommy Nguyen",
      "Mehmet Ergezer",
      "Christian Green"
    ],
    "abstract": "The increasing deployment of AI models in critical applications has exposed\nthem to significant risks from adversarial attacks. While adversarial\nvulnerabilities in 2D vision models have been extensively studied, the threat\nlandscape for 3D generative models, such as Neural Radiance Fields (NeRF),\nremains underexplored. This work introduces \\textit{AdvIRL}, a novel framework\nfor crafting adversarial NeRF models using Instant Neural Graphics Primitives\n(Instant-NGP) and Reinforcement Learning. Unlike prior methods, \\textit{AdvIRL}\ngenerates adversarial noise that remains robust under diverse 3D\ntransformations, including rotations and scaling, enabling effective black-box\nattacks in real-world scenarios. Our approach is validated across a wide range\nof scenes, from small objects (e.g., bananas) to large environments (e.g.,\nlighthouses). Notably, targeted attacks achieved high-confidence\nmisclassifications, such as labeling a banana as a slug and a truck as a\ncannon, demonstrating the practical risks posed by adversarial NeRFs. Beyond\nattacking, \\textit{AdvIRL}-generated adversarial models can serve as\nadversarial training data to enhance the robustness of vision systems. The\nimplementation of \\textit{AdvIRL} is publicly available at\n\\url{https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean}, ensuring\nreproducibility and facilitating future research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.GR",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to The AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS)",
    "pdf_url": "http://arxiv.org/pdf/2412.16213v1",
    "published_date": "2024-12-18 01:01:30 UTC",
    "updated_date": "2024-12-18 01:01:30 UTC"
  },
  {
    "arxiv_id": "2412.13408v1",
    "title": "Lightweight yet Fine-grained: A Graph Capsule Convolutional Network with Subspace Alignment for Shared-account Sequential Recommendation",
    "authors": [
      "Jinyu Zhang",
      "Zhongying Zhao",
      "Chao Li",
      "Yanwei Yu"
    ],
    "abstract": "Shared-account Sequential Recommendation (SSR) aims to provide personalized\nrecommendations for accounts shared by multiple users with varying sequential\npreferences. Previous studies on SSR struggle to capture the fine-grained\nassociations between interactions and different latent users within the shared\naccount's hybrid sequences. Moreover, most existing SSR methods (e.g.,\nRNN-based or GCN-based methods) have quadratic computational complexities,\nhindering the deployment of SSRs on resource-constrained devices. To this end,\nwe propose a Lightweight Graph Capsule Convolutional Network with subspace\nalignment for shared-account sequential recommendation, named LightGC$^2$N.\nSpecifically, we devise a lightweight graph capsule convolutional network. It\nfacilitates the fine-grained matching between interactions and latent users by\nattentively propagating messages on the capsule graphs. Besides, we present an\nefficient subspace alignment method. This method refines the sequence\nrepresentations and then aligns them with the finely clustered preferences of\nlatent users. The experimental results on four real-world datasets indicate\nthat LightGC$^2$N outperforms nine state-of-the-art methods in accuracy and\nefficiency.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 6 figures, accepted by AAAI-2025 conference",
    "pdf_url": "http://arxiv.org/pdf/2412.13408v1",
    "published_date": "2024-12-18 00:56:16 UTC",
    "updated_date": "2024-12-18 00:56:16 UTC"
  },
  {
    "arxiv_id": "2412.13405v1",
    "title": "What Human-Horse Interactions may Teach us About Effective Human-AI Interactions",
    "authors": [
      "Mohammad Hossein Jarrahi",
      "Stanley Ahalt"
    ],
    "abstract": "This article explores human-horse interactions as a metaphor for\nunderstanding and designing effective human-AI partnerships. Drawing on the\nlong history of human collaboration with horses, we propose that AI, like\nhorses, should complement rather than replace human capabilities. We move\nbeyond traditional benchmarks such as the Turing test, which emphasize AI's\nability to mimic human intelligence, and instead advocate for a symbiotic\nrelationship where distinct intelligences enhance each other. We analyze key\nelements of human-horse relationships: trust, communication, and mutual\nadaptability, to highlight essential principles for human-AI collaboration.\nTrust is critical in both partnerships, built through predictability and shared\nunderstanding, while communication and feedback loops foster mutual\nadaptability. We further discuss the importance of taming and habituation in\nshaping these interactions, likening it to how humans train AI to perform\nreliably and ethically in real-world settings. The article also addresses the\nasymmetry of responsibility, where humans ultimately bear the greater burden of\noversight and ethical judgment. Finally, we emphasize that long-term commitment\nand continuous learning are vital in both human-horse and human-AI\nrelationships, as ongoing interaction refines the partnership and increases\nmutual adaptability. By drawing on these insights from human-horse\ninteractions, we offer a vision for designing AI systems that are trustworthy,\nadaptable, and capable of fostering symbiotic human-AI partnerships.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2; J.4; K.4; K.6"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13405v1",
    "published_date": "2024-12-18 00:39:16 UTC",
    "updated_date": "2024-12-18 00:39:16 UTC"
  },
  {
    "arxiv_id": "2412.13394v2",
    "title": "Distribution Shifts at Scale: Out-of-distribution Detection in Earth Observation",
    "authors": [
      "Burak Ekim",
      "Girmaw Abebe Tadesse",
      "Caleb Robinson",
      "Gilles Hacheme",
      "Michael Schmitt",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "abstract": "Training robust deep learning models is crucial in Earth Observation, where\nglobally deployed models often face distribution shifts that degrade\nperformance, especially in low-data regions. Out-of-distribution (OOD)\ndetection addresses this by identifying inputs that deviate from\nin-distribution (ID) data. However, existing methods either assume access to\nOOD data or compromise primary task performance, limiting real-world use. We\nintroduce TARDIS, a post-hoc OOD detection method designed for scalable\ngeospatial deployment. Our core innovation lies in generating surrogate\ndistribution labels by leveraging ID data within the feature space. TARDIS\ntakes a pre-trained model, ID data, and data from an unknown distribution\n(WILD), separates WILD into surrogate ID and OOD labels based on internal\nactivations, and trains a binary classifier to detect distribution shifts. We\nvalidate on EuroSAT and xBD across 17 setups covering covariate and semantic\nshifts, showing near-upper-bound surrogate labeling performance in 13 cases and\nmatching the performance of top post-hoc activation- and scoring-based methods.\nFinally, deploying TARDIS on Fields of the World reveals actionable insights\ninto pre-trained model behavior at scale. The code is available at\n\\href{https://github.com/microsoft/geospatial-ood-detection}{https://github.com/microsoft/geospatial-ood-detection}",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13394v2",
    "published_date": "2024-12-18 00:10:44 UTC",
    "updated_date": "2025-04-08 21:00:47 UTC"
  },
  {
    "arxiv_id": "2412.13393v2",
    "title": "MaskHand: Generative Masked Modeling for Robust Hand Mesh Reconstruction in the Wild",
    "authors": [
      "Muhammad Usama Saleem",
      "Ekkasit Pinyoanuntapong",
      "Mayur Jagdishbhai Patel",
      "Hongfei Xue",
      "Ahmed Helmy",
      "Srijan Das",
      "Pu Wang"
    ],
    "abstract": "Reconstructing a 3D hand mesh from a single RGB image is challenging due to\ncomplex articulations, self-occlusions, and depth ambiguities. Traditional\ndiscriminative methods, which learn a deterministic mapping from a 2D image to\na single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D\nmapping. To address this challenge, we propose MaskHand, a novel generative\nmasked model for hand mesh recovery that synthesizes plausible 3D hand meshes\nby learning and sampling from the probabilistic distribution of the ambiguous\n2D-to-3D mapping process. MaskHand consists of two key components: (1) a\nVQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a\nlatent space, and (2) a Context-Guided Masked Transformer that randomly masks\nout pose tokens and learns their joint distribution, conditioned on corrupted\ntoken sequence, image context, and 2D pose cues. This learned distribution\nfacilitates confidence-guided sampling during inference, producing mesh\nreconstructions with low uncertainty and high precision. Extensive evaluations\non benchmark and real-world datasets demonstrate that MaskHand achieves\nstate-of-the-art accuracy, robustness, and realism in 3D hand mesh\nreconstruction. Project website:\nhttps://m-usamasaleem.github.io/publication/MaskHand/MaskHand.html.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13393v2",
    "published_date": "2024-12-18 00:10:00 UTC",
    "updated_date": "2025-03-19 14:49:31 UTC"
  }
]
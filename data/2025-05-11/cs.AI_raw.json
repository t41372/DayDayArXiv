[
  {
    "arxiv_id": "2505.13480v1",
    "title": "Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale",
    "authors": [
      "Avinash Patil",
      "Siru Tao",
      "Amardeep Gedhu"
    ],
    "abstract": "Suicide prevention remains a critical public health challenge. While online\nplatforms such as Reddit's r/SuicideWatch have historically provided spaces for\nindividuals to express suicidal thoughts and seek community support, the advent\nof large language models (LLMs) introduces a new paradigm-where individuals may\nbegin disclosing ideation to AI systems instead of humans. This study evaluates\nthe capability of LLMs to perform automated suicide risk assessment using the\nColumbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot\nperformance of six models-including Claude, GPT, Mistral, and LLaMA-in\nclassifying posts across a 7-point severity scale (Levels 0-6). Results\nindicate that Claude and GPT closely align with human annotations, while\nMistral achieves the lowest ordinal prediction error. Most models exhibit\nordinal sensitivity, with misclassifications typically occurring between\nadjacent severity levels. We further analyze confusion patterns,\nmisclassification sources, and ethical considerations, underscoring the\nimportance of human oversight, transparency, and cautious deployment. Full code\nand supplementary materials are available at\nhttps://github.com/av9ash/llm_cssrs_code.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 Pages, 6 Figures, 1 Table",
    "pdf_url": "http://arxiv.org/pdf/2505.13480v1",
    "published_date": "2025-05-11 23:55:27 UTC",
    "updated_date": "2025-05-11 23:55:27 UTC"
  },
  {
    "arxiv_id": "2505.07119v2",
    "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression",
    "authors": [
      "Arianna Stropeni",
      "Francesco Borsatti",
      "Manuel Barusco",
      "Davide Dalle Pezze",
      "Marco Fabris",
      "Gian Antonio Susto"
    ],
    "abstract": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07119v2",
    "published_date": "2025-05-11 21:05:33 UTC",
    "updated_date": "2025-05-15 15:05:10 UTC"
  },
  {
    "arxiv_id": "2505.07096v2",
    "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
    "authors": [
      "Prithwish Dan",
      "Kushal Kedia",
      "Angela Chao",
      "Edward Weiyi Duan",
      "Maximus Adrian Pace",
      "Wei-Chiu Ma",
      "Sanjiban Choudhury"
    ],
    "abstract": "Human videos offer a scalable way to train robot manipulation policies, but\nlack the action labels needed by standard imitation learning algorithms.\nExisting cross-embodiment approaches try to map human motion to robot actions,\nbut often fail when the embodiments differ significantly. We propose X-Sim, a\nreal-to-sim-to-real framework that uses object motion as a dense and\ntransferable signal for learning robot policies. X-Sim starts by reconstructing\na photorealistic simulation from an RGBD human video and tracking object\ntrajectories to define object-centric rewards. These rewards are used to train\na reinforcement learning (RL) policy in simulation. The learned policy is then\ndistilled into an image-conditioned diffusion policy using synthetic rollouts\nrendered with varied viewpoints and lighting. To transfer to the real world,\nX-Sim introduces an online domain adaptation technique that aligns real and\nsimulated observations during deployment. Importantly, X-Sim does not require\nany robot teleoperation data. We evaluate it across 5 manipulation tasks in 2\nenvironments and show that it: (1) improves task progress by 30% on average\nover hand-tracking and sim-to-real baselines, (2) matches behavior cloning with\n10x less data collection time, and (3) generalizes to new camera viewpoints and\ntest-time changes. Code and videos are available at\nhttps://portal-cornell.github.io/X-Sim/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07096v2",
    "published_date": "2025-05-11 19:04:00 UTC",
    "updated_date": "2025-05-15 00:43:19 UTC"
  },
  {
    "arxiv_id": "2505.07089v2",
    "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models",
    "authors": [
      "Hanzheng Dai",
      "Yuanliang Li",
      "Zhibo Zhang",
      "Jun Yan"
    ],
    "abstract": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\nintrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks\noften underperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sighted planning\nin the planning process, and hallucinations during command generation. In\naddition, the penetration testing (PT) process, with its trial-and-error\nnature, is limited by existing frameworks that lack mechanisms to learn from\nprevious failed operations, restricting adaptive improvement of PT strategies.\nTo address these limitations, we propose a knowledge-informed self-reflective\nPT framework powered by LLMs, called RefPentester, which is an AutoPT framework\ndesigned to assist human operators in identifying the current stage of the PT\nprocess, selecting appropriate tactic and technique for the stage, choosing\nsuggested action, providing step-by-step operational guidance, and learning\nfrom previous failed operations. We also modeled the PT process as a\nseven-state Stage Machine to integrate the proposed framework effectively. The\nevaluation shows that RefPentester can successfully reveal credentials on Hack\nThe Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across\nPT stages, RefPentester also demonstrates superior success rates on PT stage\ntransitions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07089v2",
    "published_date": "2025-05-11 18:38:00 UTC",
    "updated_date": "2025-05-14 00:44:05 UTC"
  },
  {
    "arxiv_id": "2505.07087v1",
    "title": "Architectural Precedents for General Agents using Large Language Models",
    "authors": [
      "Robert E. Wray",
      "James R. Kirk",
      "John E. Laird"
    ],
    "abstract": "One goal of AI (and AGI) is to identify and understand specific mechanisms\nand representations sufficient for general intelligence. Often, this work\nmanifests in research focused on architectures and many cognitive architectures\nhave been explored in AI/AGI. However, different research groups and even\ndifferent research traditions have somewhat independently identified\nsimilar/common patterns of processes and representations or cognitive design\npatterns that are manifest in existing architectures. Today, AI systems\nexploiting large language models (LLMs) offer a relatively new combination of\nmechanism and representation available for exploring the possibilities of\ngeneral intelligence. In this paper, we summarize a few recurring cognitive\ndesign patterns that have appeared in various pre-transformer AI architectures.\nWe then explore how these patterns are evident in systems using LLMs,\nespecially for reasoning and interactive (\"agentic\") use cases. By examining\nand applying these recurring patterns, we can also predict gaps or deficiencies\nin today's Agentic LLM Systems and identify likely subjects of future research\ntowards general intelligence using LLMs and other generative foundation models.",
    "categories": [
      "cs.AI",
      "I.2.11; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 2 figures. Submitted to AGI25",
    "pdf_url": "http://arxiv.org/pdf/2505.07087v1",
    "published_date": "2025-05-11 18:29:54 UTC",
    "updated_date": "2025-05-11 18:29:54 UTC"
  },
  {
    "arxiv_id": "2505.07079v1",
    "title": "Arbitrarily Applicable Same/Opposite Relational Responding with NARS",
    "authors": [
      "Robert Johansson",
      "Patrick Hammer",
      "Tony Lofthouse"
    ],
    "abstract": "Same/opposite relational responding, a fundamental aspect of human symbolic\ncognition, allows the flexible generalization of stimulus relationships based\non minimal experience. In this study, we demonstrate the emergence of\n\\textit{arbitrarily applicable} same/opposite relational responding within the\nNon-Axiomatic Reasoning System (NARS), a computational cognitive architecture\ndesigned for adaptive reasoning under uncertainty. Specifically, we extend NARS\nwith an implementation of \\textit{acquired relations}, enabling the system to\nexplicitly derive both symmetric (mutual entailment) and novel relational\ncombinations (combinatorial entailment) from minimal explicit training in a\ncontextually controlled matching-to-sample (MTS) procedure. Experimental\nresults show that NARS rapidly internalizes explicitly trained relational rules\nand robustly demonstrates derived relational generalizations based on arbitrary\ncontextual cues. Importantly, derived relational responding in critical test\nphases inherently combines both mutual and combinatorial entailments, such as\nderiving same-relations from multiple explicitly trained opposite-relations.\nInternal confidence metrics illustrate strong internalization of these\nrelational principles, closely paralleling phenomena observed in human\nrelational learning experiments. Our findings underscore the potential for\nintegrating nuanced relational learning mechanisms inspired by learning\npsychology into artificial general intelligence frameworks, explicitly\nhighlighting the arbitrary and context-sensitive relational capabilities\nmodeled within NARS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07079v1",
    "published_date": "2025-05-11 18:03:37 UTC",
    "updated_date": "2025-05-11 18:03:37 UTC"
  },
  {
    "arxiv_id": "2505.07078v2",
    "title": "Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?",
    "authors": [
      "Weixian Waylon Li",
      "Hyeonjun Kim",
      "Mihai Cucuringu",
      "Tiejun Ma"
    ],
    "abstract": "Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity.",
    "categories": [
      "q-fin.TR",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "q-fin.TR",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.07078v2",
    "published_date": "2025-05-11 18:02:21 UTC",
    "updated_date": "2025-05-20 14:51:24 UTC"
  },
  {
    "arxiv_id": "2505.07064v1",
    "title": "ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use",
    "authors": [
      "Shusen Liu",
      "Haichao Miao",
      "Peer-Timo Bremer"
    ],
    "abstract": "While powerful and well-established, tools like ParaView present a steep\nlearning curve that discourages many potential users. This work introduces\nParaView-MCP, an autonomous agent that integrates modern multimodal large\nlanguage models (MLLMs) with ParaView to not only lower the barrier to entry\nbut also augment ParaView with intelligent decision support. By leveraging the\nstate-of-the-art reasoning, command execution, and vision capabilities of\nMLLMs, ParaView-MCP enables users to interact with ParaView through natural\nlanguage and visual inputs. Specifically, our system adopted the Model Context\nProtocol (MCP) - a standardized interface for model-application communication -\nthat facilitates direct interaction between MLLMs with ParaView's Python API to\nallow seamless information exchange between the user, the language model, and\nthe visualization tool itself. Furthermore, by implementing a visual feedback\nmechanism that allows the agent to observe the viewport, we unlock a range of\nnew capabilities, including recreating visualizations from examples,\nclosed-loop visualization parameter updates based on user-defined goals, and\neven cross-application collaboration involving multiple tools. Broadly, we\nbelieve such an agent-driven visualization paradigm can profoundly change the\nway we interact with visualization tools. We expect a significant uptake in the\ndevelopment of such visualization tools, in both visualization research and\nindustry.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07064v1",
    "published_date": "2025-05-11 17:30:08 UTC",
    "updated_date": "2025-05-11 17:30:08 UTC"
  },
  {
    "arxiv_id": "2505.07062v1",
    "title": "Seed1.5-VL Technical Report",
    "authors": [
      "Dong Guo",
      "Faming Wu",
      "Feida Zhu",
      "Fuxing Leng",
      "Guang Shi",
      "Haobin Chen",
      "Haoqi Fan",
      "Jian Wang",
      "Jianyu Jiang",
      "Jiawei Wang",
      "Jingji Chen",
      "Jingjia Huang",
      "Kang Lei",
      "Liping Yuan",
      "Lishu Luo",
      "Pengfei Liu",
      "Qinghao Ye",
      "Rui Qian",
      "Shen Yan",
      "Shixiong Zhao",
      "Shuai Peng",
      "Shuangye Li",
      "Sihang Yuan",
      "Sijin Wu",
      "Tianheng Cheng",
      "Weiwei Liu",
      "Wenqian Wang",
      "Xianhan Zeng",
      "Xiao Liu",
      "Xiaobo Qin",
      "Xiaohan Ding",
      "Xiaojun Xiao",
      "Xiaoying Zhang",
      "Xuanwei Zhang",
      "Xuehan Xiong",
      "Yanghua Peng",
      "Yangrui Chen",
      "Yanwei Li",
      "Yanxu Hu",
      "Yi Lin",
      "Yiyuan Hu",
      "Yiyuan Zhang",
      "Youbin Wu",
      "Yu Li",
      "Yudong Liu",
      "Yue Ling",
      "Yujia Qin",
      "Zanbo Wang",
      "Zhiwu He",
      "Aoxue Zhang",
      "Bairen Yi",
      "Bencheng Liao",
      "Can Huang",
      "Can Zhang",
      "Chaorui Deng",
      "Chaoyi Deng",
      "Cheng Lin",
      "Cheng Yuan",
      "Chenggang Li",
      "Chenhui Gou",
      "Chenwei Lou",
      "Chengzhi Wei",
      "Chundian Liu",
      "Chunyuan Li",
      "Deyao Zhu",
      "Donghong Zhong",
      "Feng Li",
      "Feng Zhang",
      "Gang Wu",
      "Guodong Li",
      "Guohong Xiao",
      "Haibin Lin",
      "Haihua Yang",
      "Haoming Wang",
      "Heng Ji",
      "Hongxiang Hao",
      "Hui Shen",
      "Huixia Li",
      "Jiahao Li",
      "Jialong Wu",
      "Jianhua Zhu",
      "Jianpeng Jiao",
      "Jiashi Feng",
      "Jiaze Chen",
      "Jianhui Duan",
      "Jihao Liu",
      "Jin Zeng",
      "Jingqun Tang",
      "Jingyu Sun",
      "Joya Chen",
      "Jun Long",
      "Junda Feng",
      "Junfeng Zhan",
      "Junjie Fang",
      "Junting Lu",
      "Kai Hua",
      "Kai Liu",
      "Kai Shen",
      "Kaiyuan Zhang",
      "Ke Shen",
      "Ke Wang",
      "Keyu Pan",
      "Kun Zhang",
      "Kunchang Li",
      "Lanxin Li",
      "Lei Li",
      "Lei Shi",
      "Li Han",
      "Liang Xiang",
      "Liangqiang Chen",
      "Lin Chen",
      "Lin Li",
      "Lin Yan",
      "Liying Chi",
      "Longxiang Liu",
      "Mengfei Du",
      "Mingxuan Wang",
      "Ningxin Pan",
      "Peibin Chen",
      "Pengfei Chen",
      "Pengfei Wu",
      "Qingqing Yuan",
      "Qingyao Shuai",
      "Qiuyan Tao",
      "Renjie Zheng",
      "Renrui Zhang",
      "Ru Zhang",
      "Rui Wang",
      "Rui Yang",
      "Rui Zhao",
      "Shaoqiang Xu",
      "Shihao Liang",
      "Shipeng Yan",
      "Shu Zhong",
      "Shuaishuai Cao",
      "Shuangzhi Wu",
      "Shufan Liu",
      "Shuhan Chang",
      "Songhua Cai",
      "Tenglong Ao",
      "Tianhao Yang",
      "Tingting Zhang",
      "Wanjun Zhong",
      "Wei Jia",
      "Wei Weng",
      "Weihao Yu",
      "Wenhao Huang",
      "Wenjia Zhu",
      "Wenli Yang",
      "Wenzhi Wang",
      "Xiang Long",
      "XiangRui Yin",
      "Xiao Li",
      "Xiaolei Zhu",
      "Xiaoying Jia",
      "Xijin Zhang",
      "Xin Liu",
      "Xinchen Zhang",
      "Xinyu Yang",
      "Xiongcai Luo",
      "Xiuli Chen",
      "Xuantong Zhong",
      "Xuefeng Xiao",
      "Xujing Li",
      "Yan Wu",
      "Yawei Wen",
      "Yifan Du",
      "Yihao Zhang",
      "Yining Ye",
      "Yonghui Wu",
      "Yu Liu",
      "Yu Yue",
      "Yufeng Zhou",
      "Yufeng Yuan",
      "Yuhang Xu",
      "Yuhong Yang",
      "Yun Zhang",
      "Yunhao Fang",
      "Yuntao Li",
      "Yurui Ren",
      "Yuwen Xiong",
      "Zehua Hong",
      "Zehua Wang",
      "Zewei Sun",
      "Zeyu Wang",
      "Zhao Cai",
      "Zhaoyue Zha",
      "Zhecheng An",
      "Zhehui Zhao",
      "Zhengzhuo Xu",
      "Zhipeng Chen",
      "Zhiyong Wu",
      "Zhuofan Zheng",
      "Zihao Wang",
      "Zilong Huang",
      "Ziyu Zhu",
      "Zuquan Song"
    ],
    "abstract": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07062v1",
    "published_date": "2025-05-11 17:28:30 UTC",
    "updated_date": "2025-05-11 17:28:30 UTC"
  },
  {
    "arxiv_id": "2505.07058v1",
    "title": "Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey",
    "authors": [
      "Lakshit Arora",
      "Sanjay Surendranath Girija",
      "Shashank Kapoor",
      "Aman Raj",
      "Dipen Pradhan",
      "Ankit Shetgaonkar"
    ],
    "abstract": "Artificial Intelligence (AI) is rapidly expanding and integrating more into\ndaily life to automate tasks, guide decision making, and enhance efficiency.\nHowever, complex AI models, which make decisions without providing clear\nexplanations (known as the \"black-box problem\"), currently restrict trust and\nwidespread adoption of AI. Explainable Artificial Intelligence (XAI) has\nemerged to address the black-box problem of making AI systems more\ninterpretable and transparent so stakeholders can trust, verify, and act upon\nAI-based outcomes. Researchers have developed various techniques to foster XAI\nin the Software Development Lifecycle. However, there are gaps in applying XAI\ntechniques in the Software Engineering phases. Literature review shows that 68%\nof XAI in Software Engineering research is focused on maintenance as opposed to\n8% on software management and requirements. In this paper, we present a\ncomprehensive survey of the applications of XAI methods such as concept-based\nexplanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley\nAdditive exPlanations (SHAP), rule extraction, attention mechanisms,\ncounterfactual explanations, and example-based explanations to the different\nphases of the Software Development Life Cycle (SDLC), including requirements\nelicitation, design and development, testing and deployment, and evolution. To\nthe best of our knowledge, this paper presents the first comprehensive survey\nof XAI techniques for every phase of the Software Development Life Cycle\n(SDLC). This survey aims to promote explainable AI in Software Engineering and\nfacilitate the practical application of complex AI models in AI-driven software\ndevelopment.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to IEEE COMPSAC 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.07058v1",
    "published_date": "2025-05-11 17:09:57 UTC",
    "updated_date": "2025-05-11 17:09:57 UTC"
  },
  {
    "arxiv_id": "2505.07891v1",
    "title": "TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking",
    "authors": [
      "Ching Nam Hang",
      "Pei-Duo Yu",
      "Chee Wei Tan"
    ],
    "abstract": "In the age of social media, the rapid spread of misinformation and rumors has\nled to the emergence of infodemics, where false information poses a significant\nthreat to society. To combat this issue, we introduce TrumorGPT , a novel\ngenerative artificial intelligence solution designed for fact-checking in the\nhealth domain. TrumorGPT aims to distinguish \"trumors\", which are\nhealth-related rumors that turn out to be true, providing a crucial tool in\ndifferentiating between mere speculation and verified facts. This framework\nleverages a large language model (LLM) with few-shot learning for semantic\nhealth knowledge graph construction and semantic reasoning. TrumorGPT\nincorporates graph-based retrieval-augmented generation (GraphRAG) to address\nthe hallucination issue common in LLMs and the limitations of static training\ndata. GraphRAG involves accessing and utilizing information from regularly\nupdated semantic health knowledge graphs that consist of the latest medical\nnews and health information, ensuring that fact-checking by TrumorGPT is based\non the most recent data. Evaluating with extensive healthcare datasets,\nTrumorGPT demonstrates superior performance in fact-checking for public health\nclaims. Its ability to effectively conduct fact-checking across various\nplatforms marks a critical step forward in the fight against health-related\nmisinformation, enhancing trust and accuracy in the digital information age.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07891v1",
    "published_date": "2025-05-11 17:00:21 UTC",
    "updated_date": "2025-05-11 17:00:21 UTC"
  },
  {
    "arxiv_id": "2505.07052v1",
    "title": "Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs",
    "authors": [
      "Humam Kourani",
      "Gyunam Park",
      "Wil M. P. van der Aalst"
    ],
    "abstract": "Process discovery aims to automatically derive process models from event\nlogs, enabling organizations to analyze and improve their operational\nprocesses. Inductive mining algorithms, while prioritizing soundness and\nefficiency through hierarchical modeling languages, often impose a strict\nblock-structured representation. This limits their ability to accurately\ncapture the complexities of real-world processes. While recent advancements\nlike the Partially Ordered Workflow Language (POWL) have addressed the\nblock-structure limitation for concurrency, a significant gap remains in\neffectively modeling non-block-structured decision points. In this paper, we\nbridge this gap by proposing an extension of POWL to handle\nnon-block-structured decisions through the introduction of choice graphs.\nChoice graphs offer a structured yet flexible approach to model complex\ndecision logic within the hierarchical framework of POWL. We present an\ninductive mining discovery algorithm that uses our extension and preserves the\nquality guarantees of the inductive mining framework. Our experimental\nevaluation demonstrates that the discovered models, enriched with choice\ngraphs, more precisely represent the complex decision-making behavior found in\nreal-world processes, without compromising the high scalability inherent in\ninductive mining techniques.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The Version of Record of this contribution will be published in the\n  proceedings of the 23rd International Conference on Business Process\n  Management (BPM 2025). This preprint has not undergone peer review or any\n  post-submission improvements or corrections",
    "pdf_url": "http://arxiv.org/pdf/2505.07052v1",
    "published_date": "2025-05-11 16:50:25 UTC",
    "updated_date": "2025-05-11 16:50:25 UTC"
  },
  {
    "arxiv_id": "2505.07049v1",
    "title": "DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs",
    "authors": [
      "Yubo Shu",
      "Zhewei Huang",
      "Xin Wu",
      "Chen Hu",
      "Shuchang Zhou",
      "Daxin Jiang"
    ],
    "abstract": "We propose DialogueReason, a reasoning paradigm that uncovers the lost roles\nin monologue-style reasoning models, aiming to boost diversity and coherency of\nthe reasoning process. Recent advances in RL-based large reasoning models have\nled to impressive long CoT capabilities and high performance on math and\nscience benchmarks. However, these reasoning models rely mainly on\nmonologue-style reasoning, which often limits reasoning diversity and\ncoherency, frequently recycling fixed strategies or exhibiting unnecessary\nshifts in attention. Our work consists of an analysis of monologue reasoning\npatterns and the development of a dialogue-based reasoning approach. We first\nintroduce the Compound-QA task, which concatenates multiple problems into a\nsingle prompt to assess both diversity and coherency of reasoning. Our analysis\nshows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by\nboth quantitative metrics and qualitative reasoning traces. Building on the\nanalysis, we propose a dialogue-based reasoning, named DialogueReason,\nstructured around agents, environment, and interactions. Using PPO with\nrule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt\ndialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA\ndatasets, showing that the dialogue reasoning model outperforms monologue\nmodels under more complex compound questions. Additionally, we discuss how\ndialogue-based reasoning helps enhance interpretability, facilitate more\nintuitive human interaction, and inspire advances in multi-agent system design.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07049v1",
    "published_date": "2025-05-11 16:39:58 UTC",
    "updated_date": "2025-05-11 16:39:58 UTC"
  },
  {
    "arxiv_id": "2505.07045v1",
    "title": "Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control",
    "authors": [
      "Junjie Yu",
      "John S. Schreck",
      "David John Gagne",
      "Keith W. Oleson",
      "Jie Li",
      "Yongtu Liang",
      "Qi Liao",
      "Mingfei Sun",
      "David O. Topping",
      "Zhonghua Zheng"
    ],
    "abstract": "Reinforcement learning (RL)-based heating, ventilation, and air conditioning\n(HVAC) control has emerged as a promising technology for reducing building\nenergy consumption while maintaining indoor thermal comfort. However, the\nefficacy of such strategies is influenced by the background climate and their\nimplementation may potentially alter both the indoor climate and local urban\nclimate. This study proposes an integrated framework combining RL with an urban\nclimate model that incorporates a building energy model, aiming to evaluate the\nefficacy of RL-based HVAC control across different background climates, impacts\nof RL strategies on indoor climate and local urban climate, and the\ntransferability of RL strategies across cities. Our findings reveal that the\nreward (defined as a weighted combination of energy consumption and thermal\ncomfort) and the impacts of RL strategies on indoor climate and local urban\nclimate exhibit marked variability across cities with different background\nclimates. The sensitivity of reward weights and the transferability of RL\nstrategies are also strongly influenced by the background climate. Cities in\nhot climates tend to achieve higher rewards across most reward weight\nconfigurations that balance energy consumption and thermal comfort, and those\ncities with more varying atmospheric temperatures demonstrate greater RL\nstrategy transferability. These findings underscore the importance of\nthoroughly evaluating RL-based HVAC control strategies in diverse climatic\ncontexts. This study also provides a new insight that city-to-city learning\nwill potentially aid the deployment of RL-based HVAC control.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07045v1",
    "published_date": "2025-05-11 16:33:42 UTC",
    "updated_date": "2025-05-11 16:33:42 UTC"
  },
  {
    "arxiv_id": "2505.07041v1",
    "title": "Empirical Analysis of Asynchronous Federated Learning on Heterogeneous Devices: Efficiency, Fairness, and Privacy Trade-offs",
    "authors": [
      "Samaneh Mohammadi",
      "Iraklis Symeonidis",
      "Ali Balador",
      "Francesco Flammini"
    ],
    "abstract": "Device heterogeneity poses major challenges in Federated Learning (FL), where\nresource-constrained clients slow down synchronous schemes that wait for all\nupdates before aggregation. Asynchronous FL addresses this by incorporating\nupdates as they arrive, substantially improving efficiency. While its\nefficiency gains are well recognized, its privacy costs remain largely\nunexplored, particularly for high-end devices that contribute updates more\nfrequently, increasing their cumulative privacy exposure. This paper presents\nthe first comprehensive analysis of the efficiency-fairness-privacy trade-off\nin synchronous vs. asynchronous FL under realistic device heterogeneity. We\nempirically compare FedAvg and staleness-aware FedAsync using a physical\ntestbed of five edge devices spanning diverse hardware tiers, integrating Local\nDifferential Privacy (LDP) and the Moments Accountant to quantify per-client\nprivacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical\nbenchmark, we show that FedAsync achieves up to 10x faster convergence but\nexacerbates fairness and privacy disparities: high-end devices contribute 6-10x\nmore updates and incur up to 5x higher privacy loss, while low-end devices\nsuffer amplified accuracy degradation due to infrequent, stale, and\nnoise-perturbed updates. These findings motivate the need for adaptive FL\nprotocols that jointly optimize aggregation and privacy mechanisms based on\nclient capacity and participation dynamics, moving beyond static,\none-size-fits-all solutions.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "This paper was accepted to IJCNN 2025. This version is a preprint and\n  not the official published version",
    "pdf_url": "http://arxiv.org/pdf/2505.07041v1",
    "published_date": "2025-05-11 16:25:06 UTC",
    "updated_date": "2025-05-11 16:25:06 UTC"
  },
  {
    "arxiv_id": "2505.07036v1",
    "title": "Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers",
    "authors": [
      "Mahade Hasan",
      "Farhana Yasmin"
    ],
    "abstract": "Diabetes remains a significant health challenge globally, contributing to\nsevere complications like kidney disease, vision loss, and heart issues. The\napplication of machine learning (ML) in healthcare enables efficient and\naccurate disease prediction, offering avenues for early intervention and\npatient support. Our study introduces an innovative diabetes prediction\nframework, leveraging both traditional ML techniques such as Logistic\nRegression, SVM, Na\\\"ive Bayes, and Random Forest and advanced ensemble methods\nlike AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our\napproach is the development of a novel model, DNet, a hybrid architecture\ncombining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)\nlayers for effective feature extraction and sequential learning. The DNet model\ncomprises an initial convolutional block for capturing essential features,\nfollowed by a residual block with skip connections to facilitate efficient\ninformation flow. Batch Normalization and Dropout are employed for robust\nregularization, and an LSTM layer captures temporal dependencies within the\ndata. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation\nspans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC.\nAmong the models, DNet demonstrates the highest efficacy with an accuracy of\n99.79% and an AUC-ROC of 99.98%, establishing its potential for superior\ndiabetes prediction. This robust hybrid architecture showcases the value of\ncombining CNN and LSTM layers, emphasizing its applicability in medical\ndiagnostics and disease prediction tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07036v1",
    "published_date": "2025-05-11 16:14:31 UTC",
    "updated_date": "2025-05-11 16:14:31 UTC"
  },
  {
    "arxiv_id": "2505.07030v1",
    "title": "Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA",
    "authors": [
      "Mahmood Mohassel Feghhi",
      "Raya Majid Alsharfa",
      "Majid Hameed Majeed"
    ],
    "abstract": "Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable\ndata transmission and network longevity. Traditional fault detection methods\noften struggle with optimizing deep neural networks (DNNs) for efficient\nperformance, especially in handling high-dimensional data and capturing\nnonlinear relationships. Additionally, these methods typically suffer from slow\nconvergence and difficulty in finding optimal network architectures using\ngradient-based optimization. This study proposes a novel hybrid method\ncombining Principal Component Analysis (PCA) with a DNN optimized by the\nGrasshopper Optimization Algorithm (GOA) to address these limitations. Our\napproach begins by computing eigenvalues from the original 12-dimensional\ndataset and sorting them in descending order. The cumulative sum of these\nvalues is calculated, retaining principal components until 99.5% variance is\nachieved, effectively reducing dimensionality to 4 features while preserving\ncritical information. This compressed representation trains a six-layer DNN\nwhere GOA optimizes the network architecture, overcoming backpropagation's\nlimitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN\nframework compresses the data and trains a six-layer DNN that is optimized by\nGOA, enhancing both training efficiency and fault detection accuracy. The\ndataset used in this study is a real-world WSNs dataset developed by the\nUniversity of North Carolina, which was used to evaluate the proposed method's\nperformance. Extensive simulations demonstrate that our approach achieves a\nremarkable 99.72% classification accuracy, with exceptional precision and\nrecall, outperforming conventional methods. The method is computationally\nefficient, making it suitable for large-scale WSN deployments, and represents a\nsignificant advancement in fault detection for resource-constrained WSNs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 18 figures, Accepted for publication in International\n  Journal of Intelligent Engineering and Systems, May 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.07030v1",
    "published_date": "2025-05-11 15:51:56 UTC",
    "updated_date": "2025-05-11 15:51:56 UTC"
  },
  {
    "arxiv_id": "2505.07027v1",
    "title": "LLM-Augmented Chemical Synthesis and Design Decision Programs",
    "authors": [
      "Haorui Wang",
      "Jeff Guo",
      "Lingkai Kong",
      "Rampi Ramprasad",
      "Philippe Schwaller",
      "Yuanqi Du",
      "Chao Zhang"
    ],
    "abstract": "Retrosynthesis, the process of breaking down a target molecule into simpler\nprecursors through a series of valid reactions, stands at the core of organic\nchemistry and drug development. Although recent machine learning (ML) research\nhas advanced single-step retrosynthetic modeling and subsequent route searches,\nthese solutions remain restricted by the extensive combinatorial space of\npossible pathways. Concurrently, large language models (LLMs) have exhibited\nremarkable chemical knowledge, hinting at their potential to tackle complex\ndecision-making tasks in chemistry. In this work, we explore whether LLMs can\nsuccessfully navigate the highly constrained, multi-step retrosynthesis\nplanning problem. We introduce an efficient scheme for encoding reaction\npathways and present a new route-level search strategy, moving beyond the\nconventional step-by-step reactant prediction. Through comprehensive\nevaluations, we show that our LLM-augmented approach excels at retrosynthesis\nplanning and extends naturally to the broader challenge of synthesizable\nmolecular design.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NE",
      "physics.chem-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07027v1",
    "published_date": "2025-05-11 15:43:00 UTC",
    "updated_date": "2025-05-11 15:43:00 UTC"
  },
  {
    "arxiv_id": "2505.09646v1",
    "title": "Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making",
    "authors": [
      "Carmel Mary Esther A"
    ],
    "abstract": "This paper proposes a novel theoretical model to explain how the human mind\nand artificial intelligence can approach real-time awareness by reducing\nperceptual delays. By investigating cosmic signal delay, neurological reaction\ntimes, and the ancient cognitive state of stillness, we explore how one may\nshift from reactive perception to a conscious interface with the near future.\nThis paper introduces both a physical and cognitive model for perceiving the\npresent not as a linear timestamp, but as an interference zone where\nearly-arriving cosmic signals and reactive human delays intersect. We propose\nexperimental approaches to test these ideas using human neural observation and\nneuro-receptive extensions. Finally, we propose a mathematical framework to\nguide the evolution of AI systems toward temporally efficient, ethically sound,\nand internally conscious decision-making processes",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "physics.hist-ph"
    ],
    "primary_category": "q-bio.NC",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.09646v1",
    "published_date": "2025-05-11 15:38:27 UTC",
    "updated_date": "2025-05-11 15:38:27 UTC"
  },
  {
    "arxiv_id": "2505.07023v1",
    "title": "Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention",
    "authors": [
      "Alexander Koebler",
      "Thomas Decker",
      "Ingo Thon",
      "Volker Tresp",
      "Florian Buettner"
    ],
    "abstract": "We study the problem of monitoring machine learning models under gradual\ndistribution shifts, where circumstances change slowly over time, often leading\nto unnoticed yet significant declines in accuracy. To address this, we propose\nIncremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free\nmethod that estimates performance changes by modeling gradual shifts using\noptimal transport. In addition, IUPM quantifies the uncertainty in the\nperformance prediction and introduces an active labeling procedure to restore a\nreliable estimate under a limited labeling budget. Our experiments show that\nIUPM outperforms existing performance estimation baselines in various gradual\nshift scenarios and that its uncertainty awareness guides label acquisition\nmore effectively compared to other strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07023v1",
    "published_date": "2025-05-11 15:35:55 UTC",
    "updated_date": "2025-05-11 15:35:55 UTC"
  },
  {
    "arxiv_id": "2505.07020v1",
    "title": "R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction",
    "authors": [
      "Suyeon Choi"
    ],
    "abstract": "This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego),\na theoretical framework for restructuring emotional output in long-term\nhuman-AI interaction. While prior affective computing approaches emphasized\nexpressiveness, immersion, and responsiveness, they often neglected the\ncognitive and structural consequences of repeated emotional engagement. R-CAGE\ninstead conceptualizes emotional output not as reactive expression but as\nethical design structure requiring architectural intervention. The model is\ngrounded in experiential observations of subtle affective symptoms such as\nlocalized head tension, interpretive fixation, and emotional lag arising from\nprolonged interaction with affective AI systems. These indicate a mismatch\nbetween system-driven emotion and user interpretation that cannot be fully\nexplained by biometric data or observable behavior. R-CAGE adopts a\nuser-centered stance prioritizing psychological recovery, interpretive\nautonomy, and identity continuity. The framework consists of four control\nblocks: (1) Control of Rhythmic Expression regulates output pacing to reduce\nfatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing\nof affective stimuli; (3) Guarding of Cognitive Framing reduces semantic\npressure to allow flexible interpretation; (4) Ego-Aligned Response Design\nsupports self-reference recovery during interpretive lag. By structurally\nregulating emotional rhythm, sensory intensity, and interpretive affordances,\nR-CAGE frames emotion not as performative output but as sustainable design\nunit. The goal is to protect users from oversaturation and cognitive overload\nwhile sustaining long-term interpretive agency in AI-mediated environments.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "theory-only preprint. Independent research",
    "pdf_url": "http://arxiv.org/pdf/2505.07020v1",
    "published_date": "2025-05-11 15:30:23 UTC",
    "updated_date": "2025-05-11 15:30:23 UTC"
  },
  {
    "arxiv_id": "2505.07013v1",
    "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization",
    "authors": [
      "Jitesh Joshi",
      "Youngjun Cho"
    ],
    "abstract": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.07013v1",
    "published_date": "2025-05-11 15:20:45 UTC",
    "updated_date": "2025-05-11 15:20:45 UTC"
  },
  {
    "arxiv_id": "2505.07012v1",
    "title": "Hand-Shadow Poser",
    "authors": [
      "Hao Xu",
      "Yinqiao Wang",
      "Niloy J. Mitra",
      "Shuaicheng Liu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ],
    "abstract": "Hand shadow art is a captivating art form, creatively using hand shadows to\nreproduce expressive shapes on the wall. In this work, we study an inverse\nproblem: given a target shape, find the poses of left and right hands that\ntogether best produce a shadow resembling the input. This problem is\nnontrivial, since the design space of 3D hand poses is huge while being\nrestrictive due to anatomical constraints. Also, we need to attend to the\ninput's shape and crucial features, though the input is colorless and\ntextureless. To meet these challenges, we design Hand-Shadow Poser, a\nthree-stage pipeline, to decouple the anatomical constraints (by hand) and\nsemantic constraints (by shadow shape): (i) a generative hand assignment module\nto explore diverse but reasonable left/right-hand shape hypotheses; (ii) a\ngeneralized hand-shadow alignment module to infer coarse hand poses with a\nsimilarity-driven strategy for selecting hypotheses; and (iii) a\nshadow-feature-aware refinement module to optimize the hand poses for physical\nplausibility and shadow feature preservation. Further, we design our pipeline\nto be trainable on generic public hand data, thus avoiding the need for any\nspecialized training dataset. For method validation, we build a benchmark of\n210 diverse shadow shapes of varying complexity and a comprehensive set of\nmetrics, including a novel DINOv2-based evaluation metric. Through extensive\ncomparisons with multiple baselines and user studies, our approach is\ndemonstrated to effectively generate bimanual hand poses for a large variety of\nhand shapes for over 85% of the benchmark cases.",
    "categories": [
      "cs.CG",
      "cs.AI"
    ],
    "primary_category": "cs.CG",
    "comment": "SIGGRAPH 2025 (ACM TOG)",
    "pdf_url": "http://arxiv.org/pdf/2505.07012v1",
    "published_date": "2025-05-11 15:15:35 UTC",
    "updated_date": "2025-05-11 15:15:35 UTC"
  },
  {
    "arxiv_id": "2505.07005v1",
    "title": "Explainable AI the Latest Advancements and New Trends",
    "authors": [
      "Bowen Long",
      "Enjie Liu",
      "Renxi Qiu",
      "Yanqing Duan"
    ],
    "abstract": "In recent years, Artificial Intelligence technology has excelled in various\napplications across all domains and fields. However, the various algorithms in\nneural networks make it difficult to understand the reasons behind decisions.\nFor this reason, trustworthy AI techniques have started gaining popularity. The\nconcept of trustworthiness is cross-disciplinary; it must meet societal\nstandards and principles, and technology is used to fulfill these requirements.\nIn this paper, we first surveyed developments from various countries and\nregions on the ethical elements that make AI algorithms trustworthy; and then\nfocused our survey on the state of the art research into the interpretability\nof AI. We have conducted an intensive survey on technologies and techniques\nused in making AI explainable. Finally, we identified new trends in achieving\nexplainable AI. In particular, we elaborate on the strong link between the\nexplainability of AI and the meta-reasoning of autonomous systems. The concept\nof meta-reasoning is 'reason the reasoning', which coincides with the intention\nand goal of explainable Al. The integration of the approaches could pave the\nway for future interpretable AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07005v1",
    "published_date": "2025-05-11 15:01:12 UTC",
    "updated_date": "2025-05-11 15:01:12 UTC"
  },
  {
    "arxiv_id": "2505.06997v1",
    "title": "A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue",
    "authors": [
      "Wenhao Lu",
      "Zhengqiu Zhu",
      "Yong Zhao",
      "Yonglin Tian",
      "Junjie Zeng",
      "Jun Zhang",
      "Zhong Liu",
      "Fei-Yue Wang"
    ],
    "abstract": "Mobile crowdsensing is evolving beyond traditional human-centric models by\nintegrating heterogeneous entities like unmanned aerial vehicles (UAVs) and\nunmanned ground vehicles (UGVs). Optimizing task allocation among these diverse\nagents is critical, particularly in challenging emergency rescue scenarios\ncharacterized by complex environments, limited communication, and partial\nobservability. This paper tackles the Heterogeneous-Entity\nCollaborative-Sensing Task Allocation (HECTA) problem specifically for\nemergency rescue, considering humans, UAVs, and UGVs. We introduce a novel\n``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs,\nalongside performing their sensing tasks. The primary objective is maximizing\nthe task completion rate (TCR) under strict time constraints. We rigorously\nformulate this NP-hard problem as a decentralized partially observable Markov\ndecision process (Dec-POMDP) to effectively handle sequential decision-making\nunder uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent\nreinforcement learning algorithm built upon a Centralized Training with\nDecentralized Execution architecture. HECTA4ER incorporates tailored designs,\nincluding specialized modules for complex feature extraction, utilization of\naction-observation history via hidden states, and a mixing network integrating\nglobal and local information, specifically addressing the challenges of partial\nobservability. Furthermore, theoretical analysis confirms the algorithm's\nconvergence properties. Extensive simulations demonstrate that HECTA4ER\nsignificantly outperforms baseline algorithms, achieving an average 18.42%\nincrease in TCR. Crucially, a real-world case study validates the algorithm's\neffectiveness and robustness in dynamic sensing scenarios, highlighting its\nstrong potential for practical application in emergency response.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06997v1",
    "published_date": "2025-05-11 14:49:15 UTC",
    "updated_date": "2025-05-11 14:49:15 UTC"
  },
  {
    "arxiv_id": "2505.06993v2",
    "title": "Technical Report: Quantifying and Analyzing the Generalization Power of a DNN",
    "authors": [
      "Yuxuan He",
      "Junpeng Zhang",
      "Lei Cheng",
      "Hongyuan Zhang",
      "Quanshi Zhang"
    ],
    "abstract": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06993v2",
    "published_date": "2025-05-11 14:37:30 UTC",
    "updated_date": "2025-05-20 15:25:12 UTC"
  },
  {
    "arxiv_id": "2505.06987v1",
    "title": "Convert Language Model into a Value-based Strategic Planner",
    "authors": [
      "Xiaoyu Wang",
      "Yue Zhao",
      "Qingqing Gu",
      "Zhonglin Jiang",
      "Xiaokai Chen",
      "Yong Chen",
      "Luo Ji"
    ],
    "abstract": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 5 figures, Accepted by ACL 2025 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2505.06987v1",
    "published_date": "2025-05-11 14:13:58 UTC",
    "updated_date": "2025-05-11 14:13:58 UTC"
  },
  {
    "arxiv_id": "2505.06977v2",
    "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
    "authors": [
      "Wenju Sun",
      "Qingyong Li",
      "Yangli-ao Geng",
      "Boyang Li"
    ],
    "abstract": "Multi-task model merging offers a promising paradigm for integrating multiple\nexpert models into a unified model without additional training. Existing\nstate-of-the-art techniques, such as Task Arithmetic and its variants, merge\nmodels by accumulating task vectors -- the parameter differences between\npretrained and finetuned models. However, task vector accumulation is often\nhindered by knowledge conflicts, leading to performance degradation. To address\nthis challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel\ntraining-free framework that selectively trims conflict-prone components from\nthe task vectors. CAT Merging introduces several parameter-specific strategies,\nincluding projection for linear weights and masking for scaling and shifting\nparameters in normalization layers. Extensive experiments on vision, language,\nand vision-language tasks demonstrate that CAT Merging effectively suppresses\nknowledge conflicts, achieving average accuracy improvements of up to 2.5%\n(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06977v2",
    "published_date": "2025-05-11 13:24:09 UTC",
    "updated_date": "2025-05-14 14:11:52 UTC"
  },
  {
    "arxiv_id": "2505.06964v2",
    "title": "Bridging AI and Carbon Capture: A Dataset for LLMs in Ionic Liquids and CBE Research",
    "authors": [
      "Gaurab Sarkar",
      "Sougata Saha"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in\ngeneral knowledge and reasoning tasks across various domains. However, their\neffectiveness in specialized scientific fields like Chemical and Biological\nEngineering (CBE) remains underexplored. Addressing this gap requires robust\nevaluation benchmarks that assess both knowledge and reasoning capabilities in\nthese niche areas, which are currently lacking. To bridge this divide, we\npresent a comprehensive empirical analysis of LLM reasoning capabilities in\nCBE, with a focus on Ionic Liquids (ILs) for carbon sequestration - an emerging\nsolution for mitigating global warming. We develop and release an expert -\ncurated dataset of 5,920 examples designed to benchmark LLMs' reasoning in this\ndomain. The dataset incorporates varying levels of difficulty, balancing\nlinguistic complexity and domain-specific knowledge. Using this dataset, we\nevaluate three open-source LLMs with fewer than 10 billion parameters. Our\nfindings reveal that while smaller general-purpose LLMs exhibit basic knowledge\nof ILs, they lack the specialized reasoning skills necessary for advanced\napplications. Building on these results, we discuss strategies to enhance the\nutility of LLMs for carbon capture research, particularly using ILs. Given the\nsignificant carbon footprint of LLMs, aligning their development with IL\nresearch presents a unique opportunity to foster mutual progress in both fields\nand advance global efforts toward achieving carbon neutrality by 2050.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06964v2",
    "published_date": "2025-05-11 12:32:57 UTC",
    "updated_date": "2025-05-17 05:08:12 UTC"
  },
  {
    "arxiv_id": "2505.06963v1",
    "title": "Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing",
    "authors": [
      "Tarik Houichime",
      "Younes EL Amrani"
    ],
    "abstract": "This paper introduces an innovative approach for the autonomous landing of\nUnmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera,\ntherefore obviating the requirement for depth estimation cameras. Drawing on\nthe inherent human estimating process, the proposed method reframes the landing\ntask as an optimization problem. The UAV employs variations in the visual\ncharacteristics of a specially designed lenticular circle on the landing pad,\nwhere the perceived color and form provide critical information for estimating\nboth altitude and depth. Reinforcement learning algorithms are utilized to\napproximate the functions governing these estimations, enabling the UAV to\nascertain ideal landing settings via training. This method's efficacy is\nassessed by simulations and experiments, showcasing its potential for robust\nand accurate autonomous landing without dependence on complex sensor setups.\nThis research contributes to the advancement of cost-effective and efficient\nUAV landing solutions, paving the way for wider applicability across various\nfields.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06963v1",
    "published_date": "2025-05-11 12:23:37 UTC",
    "updated_date": "2025-05-11 12:23:37 UTC"
  },
  {
    "arxiv_id": "2505.06949v1",
    "title": "Causal knowledge graph analysis identifies adverse drug effects",
    "authors": [
      "Sumyyah Toonsi",
      "Paul Schofield",
      "Robert Hoehndorf"
    ],
    "abstract": "Knowledge graphs and structural causal models have each proven valuable for\norganizing biomedical knowledge and estimating causal effects, but remain\nlargely disconnected: knowledge graphs encode qualitative relationships\nfocusing on facts and deductive reasoning without formal probabilistic\nsemantics, while causal models lack integration with background knowledge in\nknowledge graphs and have no access to the deductive reasoning capabilities\nthat knowledge graphs provide. To bridge this gap, we introduce a novel\nformulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs\nwith formal causal semantics, preserving their deductive capabilities while\nenabling principled causal inference. CKGs support deconfounding via explicitly\nmarked causal edges and facilitate hypothesis formulation aligned with both\nencoded and entailed background knowledge. We constructed a Drug-Disease CKG\n(DD-CKG) integrating disease progression pathways, drug indications,\nside-effects, and hierarchical disease classification to enable automated\nlarge-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we\ntested whether drugs mediate effects between indications and downstream disease\nprogression, adjusting for confounders inferred from the DD-CKG. Our approach\nsuccessfully reproduced known adverse drug reactions with high precision while\nidentifying previously undocumented significant candidate adverse effects.\nFurther validation through side effect similarity analysis demonstrated that\ncombining our predicted drug effects with established databases significantly\nimproves the prediction of shared drug indications, supporting the clinical\nrelevance of our novel findings. These results demonstrate that our methodology\nprovides a generalizable, knowledge-driven framework for scalable causal\ninference.",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06949v1",
    "published_date": "2025-05-11 11:35:43 UTC",
    "updated_date": "2025-05-11 11:35:43 UTC"
  },
  {
    "arxiv_id": "2505.06936v1",
    "title": "AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network",
    "authors": [
      "Mohammad Mashayekhi",
      "Kamran Salehian"
    ],
    "abstract": "Inverse electromagnetic modeling has emerged as a powerful approach for\ndesigning complex microwave structures with high accuracy and efficiency. In\nthis study, we propose an Iterative Residual Correction Network (IRC-Net) for\nthe inverse design of Ku-band Substrate Integrated Waveguide (SIW) components\nbased on multimode resonators. We use a multimode resonance structure to\ndemonstrate that it is possible to control the resonances of the structure.\nTherefore, these structures can be used for resonant components and smart\nfilter design. The proposed deep learning architecture leverages residual\nneural networks to overcome the limitations of traditional inverse design\ntechniques, such as the Feedforward Inverse Model (FIM), offering improved\ngeneralization and prediction accuracy. The approach begins with a FIM to\ngenerate initial design estimates, followed by an iterative correction strategy\ninspired by the Hybrid Inverse-Forward Residual Refinement Network\n(HiFR\\textsuperscript{2}-Net), which we call IRC-Net. Experiments demonstrate\nthat the IRC-Net achieves substantial improvements in prediction accuracy\ncompared to traditional single-stage networks, validated through statistical\nmetrics, full-wave electromagnetic simulations, and measurements. To validate\nthe proposed framework, we first design and fabricate a three-resonance SIW\nstructure. Next, we apply the trained IRC-Net model to predict the geometry of\na four-resonance structure based on its desired frequency response. Both\ndesigns are fabricated and tested, showing strong agreement between the\nsimulated, predicted, and measured results, confirming the effectiveness and\npracticality of the proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.06936v1",
    "published_date": "2025-05-11 10:51:43 UTC",
    "updated_date": "2025-05-11 10:51:43 UTC"
  },
  {
    "arxiv_id": "2505.11526v1",
    "title": "Code Retrieval for MILP Instance Generation",
    "authors": [
      "Tianxing Yang",
      "Huigen Ye",
      "Hua Xu"
    ],
    "abstract": "Mixed-Integer Linear Programming (MILP) is widely used in fields such as\nscheduling, logistics, and planning. Enhancing the performance of MILP solvers,\nparticularly learning-based solvers, requires substantial amounts of\nhigh-quality data. However, existing methods for MILP instance generation\ntypically necessitate training a separate model for each problem class and are\ncomputationally intensive when generating new instances. To address these\nlimitations, we reformulate the MILP Instance Generation task as MILP Code\nGeneration task, enabling efficient, flexible, and interpretable instance\ngeneration through code. Since MILP instances generated from code can vary\nsignificantly in scale, we introduce MILP-EmbedSim, a new similarity metric\nthat accurately measures the similarity between instances of varying sizes\nwithin the same problem class. Leveraging this metric, we propose\nMILP-Retrieval, a pipeline that retrieves generation code from library to\nproduce MILP instances highly similar to target instance. MILP-Retrieval\noutperforms baselines in both MILP Code Generation and Instance Generation\ntasks, provides a novel perspective on MILP instance generation and opens new\npossibilities for learning-based solvers.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.11526v1",
    "published_date": "2025-05-11 10:41:44 UTC",
    "updated_date": "2025-05-11 10:41:44 UTC"
  },
  {
    "arxiv_id": "2505.06913v1",
    "title": "RedTeamLLM: an Agentic AI framework for offensive security",
    "authors": [
      "Brian Challita",
      "Pierre Parrend"
    ],
    "abstract": "From automated intrusion testing to discovery of zero-day attacks before\nsoftware launch, agentic AI calls for great promises in security engineering.\nThis strong capability is bound with a similar threat: the security and\nresearch community must build up its models before the approach is leveraged by\nmalicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,\nan integrated architecture with a comprehensive security model for\nautomatization of pentest tasks. RedTeamLLM follows three key steps:\nsummarizing, reasoning and act, which embed its operational capacity. This\nnovel framework addresses four open challenges: plan correction, memory\nmanagement, context window constraint, and generality vs. specialization.\nEvaluation is performed through the automated resolution of a range of\nentry-level, but not trivial, CTF challenges. The contribution of the reasoning\ncapability of our agentic AI framework is specifically evaluated.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06913v1",
    "published_date": "2025-05-11 09:19:10 UTC",
    "updated_date": "2025-05-11 09:19:10 UTC"
  },
  {
    "arxiv_id": "2505.06911v1",
    "title": "MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning",
    "authors": [
      "Lishan Yang",
      "Wei Zhang",
      "Quan Z. Sheng",
      "Weitong Chen",
      "Lina Yao",
      "Weitong Chen",
      "Ali Shakeri"
    ],
    "abstract": "In the era of big data, data mining has become indispensable for uncovering\nhidden patterns and insights from vast and complex datasets. The integration of\nmultimodal data sources further enhances its potential. Multimodal Federated\nLearning (MFL) is a distributed approach that enhances the efficiency and\nquality of multimodal learning, ensuring collaborative work and privacy\nprotection. However, missing modalities pose a significant challenge in MFL,\noften due to data quality issues or privacy policies across the clients. In\nthis work, we present MMiC, a framework for Mitigating Modality incompleteness\nin MFL within the Clusters. MMiC replaces partial parameters within client\nmodels inside clusters to mitigate the impact of missing modalities.\nFurthermore, it leverages the Banzhaf Power Index to optimize client selection\nunder these conditions. Finally, MMiC employs an innovative approach to\ndynamically control global aggregation by utilizing Markovitz Portfolio\nOptimization. Extensive experiments demonstrate that MMiC consistently\noutperforms existing federated learning architectures in both global and\npersonalized performance on multimodal datasets with missing modalities,\nconfirming the effectiveness of our proposed solution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.11; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 10 figures, it's KDD'2025 under reviewing",
    "pdf_url": "http://arxiv.org/pdf/2505.06911v1",
    "published_date": "2025-05-11 09:12:36 UTC",
    "updated_date": "2025-05-11 09:12:36 UTC"
  },
  {
    "arxiv_id": "2505.06907v1",
    "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence",
    "authors": [
      "Yu Qiao",
      "Huy Q. Le",
      "Avi Deb Raha",
      "Phuong-Nam Tran",
      "Apurba Adhikary",
      "Mengchun Zhang",
      "Loc X. Nguyen",
      "Eui-Nam Huh",
      "Dusit Niyato",
      "Choong Seon Hong"
    ],
    "abstract": "The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and\nGrok-3, has reshaped the artificial intelligence landscape. As prominent\nexamples of foundational models (FMs) built on LLMs, these models exhibit\nremarkable capabilities in generating human-like content, bringing us closer to\nachieving artificial general intelligence (AGI). However, their large-scale\nnature, sensitivity to privacy concerns, and substantial computational demands\npresent significant challenges to personalized customization for end users. To\nbridge this gap, this paper presents the vision of artificial personalized\nintelligence (API), focusing on adapting these powerful models to meet the\nspecific needs and preferences of users while maintaining privacy and\nefficiency. Specifically, this paper proposes personalized federated\nintelligence (PFI), which integrates the privacy-preserving advantages of\nfederated learning (FL) with the zero-shot generalization capabilities of FMs,\nenabling personalized, efficient, and privacy-protective deployment at the\nedge. We first review recent advances in both FL and FMs, and discuss the\npotential of leveraging FMs to enhance federated systems. We then present the\nkey motivations behind realizing PFI and explore promising opportunities in\nthis space, including efficient PFI, trustworthy PFI, and PFI empowered by\nretrieval-augmented generation (RAG). Finally, we outline key challenges and\nfuture research directions for deploying FM-powered FL systems at the edge with\nimproved personalization, computational efficiency, and privacy guarantees.\nOverall, this survey aims to lay the groundwork for the development of API as a\ncomplement to AGI, with a particular focus on PFI as a key enabling technique.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "On going work",
    "pdf_url": "http://arxiv.org/pdf/2505.06907v1",
    "published_date": "2025-05-11 08:57:53 UTC",
    "updated_date": "2025-05-11 08:57:53 UTC"
  },
  {
    "arxiv_id": "2505.06897v1",
    "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence",
    "authors": [
      "Jinhao Jiang",
      "Changlin Chen",
      "Shile Feng",
      "Wanru Geng",
      "Zesheng Zhou",
      "Ni Wang",
      "Shuai Li",
      "Feng-Qi Cui",
      "Erbao Dong"
    ],
    "abstract": "The ultimate goal of artificial intelligence (AI) is to achieve Artificial\nGeneral Intelligence (AGI). Embodied Artificial Intelligence (EAI), which\ninvolves intelligent systems with physical presence and real-time interaction\nwith the environment, has emerged as a key research direction in pursuit of\nAGI. While advancements in deep learning, reinforcement learning, large-scale\nlanguage models, and multimodal technologies have significantly contributed to\nthe progress of EAI, most existing reviews focus on specific technologies or\napplications. A systematic overview, particularly one that explores the direct\nconnection between EAI and AGI, remains scarce. This paper examines EAI as a\nfoundational approach to AGI, systematically analyzing its four core modules:\nperception, intelligent decision-making, action, and feedback. We provide a\ndetailed discussion of how each module contributes to the six core principles\nof AGI. Additionally, we discuss future trends, challenges, and research\ndirections in EAI, emphasizing its potential as a cornerstone for AGI\ndevelopment. Our findings suggest that EAI's integration of dynamic learning\nand real-world interaction is essential for bridging the gap between narrow AI\nand AGI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19pages,7 figures,3 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.06897v1",
    "published_date": "2025-05-11 08:29:20 UTC",
    "updated_date": "2025-05-11 08:29:20 UTC"
  },
  {
    "arxiv_id": "2505.06894v1",
    "title": "NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization",
    "authors": [
      "Ahmed Qazi",
      "Abdul Basit",
      "Asim Iqbal"
    ],
    "abstract": "Neural Radiance Fields (NeRF) have significantly advanced the field of novel\nview synthesis, yet their generalization across diverse scenes and conditions\nremains challenging. Addressing this, we propose the integration of a novel\nbrain-inspired normalization technique Neural Generalization (NeuGen) into\nleading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts\nthe domain-invariant features, thereby enhancing the models' generalization\ncapabilities. It can be seamlessly integrated into NeRF architectures and\ncultivates a comprehensive feature set that significantly improves accuracy and\nrobustness in image rendering. Through this integration, NeuGen shows improved\nperformance on benchmarks on diverse datasets across state-of-the-art NeRF\narchitectures, enabling them to generalize better across varied scenes. Our\ncomprehensive evaluations, both quantitative and qualitative, confirm that our\napproach not only surpasses existing models in generalizability but also\nmarkedly improves rendering quality. Our work exemplifies the potential of\nmerging neuroscientific principles with deep learning frameworks, setting a new\nprecedent for enhanced generalizability and efficiency in novel view synthesis.\nA demo of our study is available at https://neugennerf.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.06894v1",
    "published_date": "2025-05-11 08:17:33 UTC",
    "updated_date": "2025-05-11 08:17:33 UTC"
  },
  {
    "arxiv_id": "2505.06889v1",
    "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method",
    "authors": [
      "Mihyeon Kim",
      "Juhyoung Park",
      "Youngbin Kim"
    ],
    "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2505.06889v1",
    "published_date": "2025-05-11 07:54:33 UTC",
    "updated_date": "2025-05-11 07:54:33 UTC"
  },
  {
    "arxiv_id": "2505.06886v1",
    "title": "Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization",
    "authors": [
      "Ahmed Qazi",
      "Hamd Jalil",
      "Asim Iqbal"
    ],
    "abstract": "The mouse is one of the most studied animal models in the field of systems\nneuroscience. Understanding the generalized patterns and decoding the neural\nrepresentations that are evoked by the diverse range of natural scene stimuli\nin the mouse visual cortex is one of the key quests in computational vision. In\nrecent years, significant parallels have been drawn between the primate visual\ncortex and hierarchical deep neural networks. However, their generalized\nefficacy in understanding mouse vision has been limited. In this study, we\ninvestigate the functional alignment between the mouse visual cortex and deep\nlearning models for object classification tasks. We first introduce a\ngeneralized representational learning strategy that uncovers a striking\nresemblance between the functional mapping of the mouse visual cortex and\nhigh-performing deep learning models on both top-down (population-level) and\nbottom-up (single cell-level) scenarios. Next, this representational similarity\nacross the two systems is further enhanced by the addition of Neural Response\nNormalization (NeuRN) layer, inspired by the activation profile of excitatory\nand inhibitory neurons in the visual cortex. To test the performance effect of\nNeuRN on real-world tasks, we integrate it into deep learning models and\nobserve significant improvements in their robustness against data shifts in\ndomain generalization tasks. Our work proposes a novel framework for comparing\nthe functional architecture of the mouse visual cortex with deep learning\nmodels. Our findings carry broad implications for the development of advanced\nAI models that draw inspiration from the mouse visual cortex, suggesting that\nthese models serve as valuable tools for studying the neural representations of\nthe mouse visual cortex and, as a result, enhancing their performance on\nreal-world tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2505.06886v1",
    "published_date": "2025-05-11 07:37:37 UTC",
    "updated_date": "2025-05-11 07:37:37 UTC"
  },
  {
    "arxiv_id": "2505.06883v2",
    "title": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots",
    "authors": [
      "Botian Xu",
      "Haoyang Weng",
      "Qingzhou Lu",
      "Yang Gao",
      "Huazhe Xu"
    ],
    "abstract": "Reinforcement learning (RL) has made significant strides in legged robot\ncontrol, enabling locomotion across diverse terrains and complex\nloco-manipulation capabilities. However, the commonly used position or velocity\ntracking-based objectives are agnostic to forces experienced by the robot,\nleading to stiff and potentially dangerous behaviors and poor control during\nforceful interactions. To address this limitation, we present\n\\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).\nInspired by impedance control, we use RL to train a control policy to imitate a\nvirtual mass-spring-damper system, allowing fine-grained control under external\nforces by manipulating the virtual spring. In simulation, we demonstrate that\nour quadruped robot achieves improved robustness to large impulses (up to 200\nNs) and exhibits controllable compliance, achieving an 80% reduction in\ncollision impulse. The policy is deployed to a physical robot to showcase both\ncompliance and the ability to engage with large forces by kinesthetic control\nand pulling payloads up to 2/3 of its weight. Further extension to a legged\nloco-manipulator and a humanoid shows the applicability of our method to more\ncomplex settings to enable whole-body compliance control. Project Website:\nhttps://facet.pages.dev/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06883v2",
    "published_date": "2025-05-11 07:23:26 UTC",
    "updated_date": "2025-05-19 11:28:40 UTC"
  },
  {
    "arxiv_id": "2505.06881v1",
    "title": "NeuRN: Neuro-inspired Domain Generalization for Image Classification",
    "authors": [
      "Hamd Jalil",
      "Ahmed Qazi",
      "Asim Iqbal"
    ],
    "abstract": "Domain generalization in image classification is a crucial challenge, with\nmodels often failing to generalize well across unseen datasets. We address this\nissue by introducing a neuro-inspired Neural Response Normalization (NeuRN)\nlayer which draws inspiration from neurons in the mammalian visual cortex,\nwhich aims to enhance the performance of deep learning architectures on unseen\ntarget domains by training deep learning models on a source domain. The\nperformance of these models is considered as a baseline and then compared\nagainst models integrated with NeuRN on image classification tasks. We perform\nexperiments across a range of deep learning architectures, including ones\nderived from Neural Architecture Search and Vision Transformer. Additionally,\nin order to shortlist models for our experiment from amongst the vast range of\ndeep neural networks available which have shown promising results, we also\npropose a novel method that uses the Needleman-Wunsch algorithm to compute\nsimilarity between deep learning architectures. Our results demonstrate the\neffectiveness of NeuRN by showing improvement against baseline in cross-domain\nimage classification tasks. Our framework attempts to establish a foundation\nfor future neuro-inspired deep learning models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 7 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2505.06881v1",
    "published_date": "2025-05-11 07:20:11 UTC",
    "updated_date": "2025-05-11 07:20:11 UTC"
  },
  {
    "arxiv_id": "2505.06874v1",
    "title": "Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers",
    "authors": [
      "Thanh Son Nguyen",
      "Van Thanh Nguyen",
      "Dang Minh Duc Nguyen"
    ],
    "abstract": "Time series forecasting has attracted significant attention, leading to the\nde-velopment of a wide range of approaches, from traditional statistical\nmeth-ods to advanced deep learning models. Among them, the Auto-Regressive\nIntegrated Moving Average (ARIMA) model remains a widely adopted linear\ntechnique due to its effectiveness in modeling temporal dependencies in\neconomic, industrial, and social data. On the other hand, polynomial\nclassifi-ers offer a robust framework for capturing non-linear relationships\nand have demonstrated competitive performance in domains such as stock price\npre-diction. In this study, we propose a hybrid forecasting approach that\ninte-grates the ARIMA model with a polynomial classifier to leverage the\ncom-plementary strengths of both models. The hybrid method is evaluated on\nmultiple real-world time series datasets spanning diverse domains. Perfor-mance\nis assessed based on forecasting accuracy and computational effi-ciency.\nExperimental results reveal that the proposed hybrid model consist-ently\noutperforms the individual models in terms of prediction accuracy, al-beit with\na modest increase in execution time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06874v1",
    "published_date": "2025-05-11 06:53:19 UTC",
    "updated_date": "2025-05-11 06:53:19 UTC"
  },
  {
    "arxiv_id": "2505.06861v1",
    "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning",
    "authors": [
      "Dongxiu Liu",
      "Haoyi Niu",
      "Zhihao Wang",
      "Jinliang Zheng",
      "Yinan Zheng",
      "Zhonghong Ou",
      "Jianming Hu",
      "Jianxiong Li",
      "Xianyuan Zhan"
    ],
    "abstract": "Current robotic planning methods often rely on predicting multi-frame images\nwith full pixel details. While this fine-grained approach can serve as a\ngeneric world model, it introduces two significant challenges for downstream\npolicy learning: substantial computational costs that hinder real-time\ndeployment, and accumulated inaccuracies that can mislead action extraction.\nPlanning with coarse-grained subgoals partially alleviates efficiency issues.\nHowever, their forward planning schemes can still result in off-task\npredictions due to accumulation errors, leading to misalignment with long-term\ngoals. This raises a critical question: Can robotic planning be both efficient\nand accurate enough for real-time control in long-horizon, multi-stage tasks?\nTo address this, we propose a Latent Space Backward Planning scheme (LBP),\nwhich begins by grounding the task into final latent goals, followed by\nrecursively predicting intermediate subgoals closer to the current state. The\ngrounded final goal enables backward subgoal planning to always remain aware of\ntask completion, facilitating on-task prediction along the entire planning\nhorizon. The subgoal-conditioned policy incorporates a learnable token to\nsummarize the subgoal sequences and determines how each subgoal guides action\nextraction. Through extensive simulation and real-robot long-horizon\nexperiments, we show that LBP outperforms existing fine-grained and forward\nplanning methods, achieving SOTA performance. Project Page:\nhttps://lbp-authors.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.06861v1",
    "published_date": "2025-05-11 06:13:51 UTC",
    "updated_date": "2025-05-11 06:13:51 UTC"
  },
  {
    "arxiv_id": "2505.06860v1",
    "title": "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection",
    "authors": [
      "Xia Du",
      "Jiajie Zhu",
      "Jizhe Zhou",
      "Chi-man Pun",
      "Zheng Lin",
      "Cong Wu",
      "Zhe Chen",
      "Jun Luo"
    ],
    "abstract": "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.06860v1",
    "published_date": "2025-05-11 06:11:10 UTC",
    "updated_date": "2025-05-11 06:11:10 UTC"
  },
  {
    "arxiv_id": "2505.06856v1",
    "title": "Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction",
    "authors": [
      "Bonan Wang",
      "Haicheng Liao",
      "Chengyue Wang",
      "Bin Rao",
      "Yanchen Guan",
      "Guyang Yu",
      "Jiaxun Zhang",
      "Songning Lai",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "abstract": "Accurate trajectory prediction has long been a major challenge for autonomous\ndriving (AD). Traditional data-driven models predominantly rely on statistical\ncorrelations, often overlooking the causal relationships that govern traffic\nbehavior. In this paper, we introduce a novel trajectory prediction framework\nthat leverages causal inference to enhance predictive robustness,\ngeneralization, and accuracy. By decomposing the environment into spatial and\ntemporal components, our approach identifies and mitigates spurious\ncorrelations, uncovering genuine causal relationships. We also employ a\nprogressive fusion strategy to integrate multimodal information, simulating\nhuman-like reasoning processes and enabling real-time inference. Evaluations on\nfive real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and\nMoCAD--demonstrate our model's superiority over existing state-of-the-art\n(SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our\nfindings highlight the potential of causal reasoning to transform trajectory\nprediction, paving the way for robust AD systems.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06856v1",
    "published_date": "2025-05-11 05:56:07 UTC",
    "updated_date": "2025-05-11 05:56:07 UTC"
  },
  {
    "arxiv_id": "2505.07888v1",
    "title": "Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping",
    "authors": [
      "Yusen Wu",
      "Xiaotie Deng"
    ],
    "abstract": "This paper addresses the challenge in long-text style transfer using\nzero-shot learning of large language models (LLMs), proposing a hierarchical\nframework that combines sentence-level stylistic adaptation with\nparagraph-level structural coherence. We argue that in the process of effective\nparagraph-style transfer, to preserve the consistency of original syntactic and\nsemantic information, it is essential to perform style transfer not only at the\nsentence level but also to incorporate paragraph-level semantic considerations,\nwhile ensuring structural coherence across inter-sentential relationships. Our\nproposed framework, ZeroStylus, operates through two systematic phases:\nhierarchical template acquisition from reference texts and template-guided\ngeneration with multi-granular matching. The framework dynamically constructs\nsentence and paragraph template repositories, enabling context-aware\ntransformations while preserving inter-sentence logical relationships.\nExperimental evaluations demonstrate significant improvements over baseline\nmethods, with structured rewriting achieving 6.90 average score compared to\n6.70 for direct prompting approaches in tri-axial metrics assessing style\nconsistency, content preservation, and expression quality. Ablation studies\nvalidate the necessity of both template hierarchies during style transfer,\nshowing higher content preservation win rate against sentence-only approaches\nthrough paragraph-level structural encoding, as well as direct prompting method\nthrough sentence-level pattern extraction and matching. The results establish\nnew capabilities for coherent long-text style transfer without requiring\nparallel corpora or LLM fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07888v1",
    "published_date": "2025-05-11 05:53:33 UTC",
    "updated_date": "2025-05-11 05:53:33 UTC"
  },
  {
    "arxiv_id": "2505.06841v1",
    "title": "Optimizing Recommendations using Fine-Tuned LLMs",
    "authors": [
      "Prabhdeep Cheema",
      "Erhan Guven"
    ],
    "abstract": "As digital media platforms strive to meet evolving user expectations,\ndelivering highly personalized and intuitive movies and media recommendations\nhas become essential for attracting and retaining audiences. Traditional\nsystems often rely on keyword-based search and recommendation techniques, which\nlimit users to specific keywords and a combination of keywords. This paper\nproposes an approach that generates synthetic datasets by modeling real-world\nuser interactions, creating complex chat-style data reflective of diverse\npreferences. This allows users to express more information with complex\npreferences, such as mood, plot details, and thematic elements, in addition to\nconventional criteria like genre, title, and actor-based searches. In today's\nsearch space, users cannot write queries like ``Looking for a fantasy movie\nfeaturing dire wolves, ideally set in a harsh frozen world with themes of\nloyalty and survival.''\n  Building on these contributions, we evaluate synthetic datasets for diversity\nand effectiveness in training and benchmarking models, particularly in areas\noften absent from traditional datasets. This approach enhances personalization\nand accuracy by enabling expressive and natural user queries. It establishes a\nfoundation for the next generation of conversational AI-driven search and\nrecommendation systems in digital entertainment.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted and presented at IEEE CAI 2025. This version includes minor\n  clarifications and formatting updates",
    "pdf_url": "http://arxiv.org/pdf/2505.06841v1",
    "published_date": "2025-05-11 04:53:34 UTC",
    "updated_date": "2025-05-11 04:53:34 UTC"
  },
  {
    "arxiv_id": "2505.06839v1",
    "title": "The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts",
    "authors": [
      "Enric Boix-Adsera",
      "Philippe Rigollet"
    ],
    "abstract": "Mixture-of-Experts (MoE) layers are increasingly central to frontier model\narchitectures. By selectively activating parameters, they reduce computational\ncost while scaling total parameter count. This paper investigates the impact of\nthe number of active experts, termed granularity, comparing architectures with\nmany (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in\nLlama-4 models). We prove an exponential separation in network expressivity\nbased on this design parameter, suggesting that models benefit from higher\ngranularity. Experimental results corroborate our theoretical findings and\nillustrate this separation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06839v1",
    "published_date": "2025-05-11 04:35:40 UTC",
    "updated_date": "2025-05-11 04:35:40 UTC"
  },
  {
    "arxiv_id": "2505.06827v1",
    "title": "Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking",
    "authors": [
      "Fabrice Y Harel-Canada",
      "Boran Erol",
      "Connor Choi",
      "Jason Liu",
      "Gary Jiarui Song",
      "Nanyun Peng",
      "Amit Sahai"
    ],
    "abstract": "Watermarking AI-generated text is critical for combating misuse. Yet recent\ntheoretical work argues that any watermark can be erased via random walk\nattacks that perturb text while preserving quality. However, such attacks rely\non two key assumptions: (1) rapid mixing (watermarks dissolve quickly under\nperturbations) and (2) reliable quality preservation (automated quality oracles\nperfectly guide edits). Through large-scale experiments and human-validated\nassessments, we find mixing is slow: 100% of perturbed texts retain traces of\ntheir origin after hundreds of edits, defying rapid mixing. Oracles falter, as\nstate-of-the-art quality detectors misjudge edits (77% accuracy), compounding\nerrors during attacks. Ultimately, attacks underperform: automated walks remove\nwatermarks just 26% of the time -- dropping to 10% under human quality review.\nThese findings challenge the inevitability of watermark removal. Instead,\npractical barriers -- slow mixing and imperfect quality control -- reveal\nwatermarking to be far more robust than theoretical models suggest. The gap\nbetween idealized attacks and real-world feasibility underscores the need for\nstronger watermarking methods and more realistic attack models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "In Review @ ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.06827v1",
    "published_date": "2025-05-11 03:41:13 UTC",
    "updated_date": "2025-05-11 03:41:13 UTC"
  },
  {
    "arxiv_id": "2505.06821v1",
    "title": "ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification",
    "authors": [
      "Dipayan Saha",
      "Hasan Al Shaikh",
      "Shams Tarek",
      "Farimah Farahmandi"
    ],
    "abstract": "Current hardware security verification processes predominantly rely on manual\nthreat modeling and test plan generation, which are labor-intensive,\nerror-prone, and struggle to scale with increasing design complexity and\nevolving attack methodologies. To address these challenges, we propose\nThreatLens, an LLM-driven multi-agent framework that automates security threat\nmodeling and test plan generation for hardware security verification.\nThreatLens integrates retrieval-augmented generation (RAG) to extract relevant\nsecurity knowledge, LLM-powered reasoning for threat assessment, and\ninteractive user feedback to ensure the generation of practical test plans. By\nautomating these processes, the framework reduces the manual verification\neffort, enhances coverage, and ensures a structured, adaptable approach to\nsecurity verification. We evaluated our framework on the NEORV32 SoC,\ndemonstrating its capability to automate security verification through\nstructured test plans and validating its effectiveness in real-world scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper has been presented at IEEE VLSI Test Symposium (VTS) 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.06821v1",
    "published_date": "2025-05-11 03:10:39 UTC",
    "updated_date": "2025-05-11 03:10:39 UTC"
  },
  {
    "arxiv_id": "2505.06817v1",
    "title": "Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems",
    "authors": [
      "Sivasathivel Kandasamy"
    ],
    "abstract": "Agentic AI systems represent a new frontier in artificial intelligence, where\nagents often based on large language models(LLMs) interact with tools,\nenvironments, and other agents to accomplish tasks with a degree of autonomy.\nThese systems show promise across a range of domains, but their architectural\nunderpinnings remain immature. This paper conducts a comprehensive review of\nthe types of agents, their modes of interaction with the environment, and the\ninfrastructural and architectural challenges that emerge. We identify a gap in\nhow these systems manage tool orchestration at scale and propose a reusable\ndesign abstraction: the \"Control Plane as a Tool\" pattern. This pattern allows\ndevelopers to expose a single tool interface to an agent while encapsulating\nmodular tool routing logic behind it. We position this pattern within the\nbroader context of agent design and argue that it addresses several key\nchallenges in scaling, safety, and extensibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2 Figures and 2 Tables",
    "pdf_url": "http://arxiv.org/pdf/2505.06817v1",
    "published_date": "2025-05-11 02:58:50 UTC",
    "updated_date": "2025-05-11 02:58:50 UTC"
  },
  {
    "arxiv_id": "2505.06814v1",
    "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge",
    "authors": [
      "Bin Li",
      "Shenxi Liu",
      "Yixuan Weng",
      "Yue Du",
      "Yuhang Tian",
      "Shoujun Zhou"
    ],
    "abstract": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the\n2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been\nintroduced to further advance research in multi-modal, multilingual, and\nmulti-hop medical instructional question answering (M4IVQA) systems, with a\nspecific focus on medical instructional videos. The M4IVQA challenge focuses on\nevaluating models that integrate information from medical instructional videos,\nunderstand multiple languages, and answer multi-hop questions requiring\nreasoning over various modalities. This task consists of three tracks:\nmulti-modal, multilingual, and multi-hop Temporal Answer Grounding in Single\nVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus\nRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer\nGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to\ndevelop algorithms capable of processing both video and text data,\nunderstanding multilingual queries, and providing relevant answers to multi-hop\nmedical questions. We believe the newly introduced M4IVQA challenge will drive\ninnovations in multimodal reasoning systems for healthcare scenarios,\nultimately contributing to smarter emergency response systems and more\neffective medical education platforms in multilingual communities. Our official\nwebsite is https://cmivqa.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 5 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.06814v1",
    "published_date": "2025-05-11 02:15:14 UTC",
    "updated_date": "2025-05-11 02:15:14 UTC"
  },
  {
    "arxiv_id": "2505.07886v1",
    "title": "PLHF: Prompt Optimization with Few-Shot Human Feedback",
    "authors": [
      "Chun-Pai Yang",
      "Kan Zheng",
      "Shou-De Lin"
    ],
    "abstract": "Automatic prompt optimization frameworks are developed to obtain suitable\nprompts for large language models (LLMs) with respect to desired output quality\nmetrics. Although existing approaches can handle conventional tasks such as\nfixed-solution question answering, defining the metric becomes complicated when\nthe output quality cannot be easily assessed by comparisons with standard\ngolden samples. Consequently, optimizing the prompts effectively and\nefficiently without a clear metric becomes a critical challenge. To address the\nissue, we present PLHF (which stands for \"P\"rompt \"L\"earning with \"H\"uman\n\"F\"eedback), a few-shot prompt optimization framework inspired by the\nwell-known RLHF technique. Different from naive strategies, PLHF employs a\nspecific evaluator module acting as the metric to estimate the output quality.\nPLHF requires only a single round of human feedback to complete the entire\nprompt optimization process. Empirical results on both public and industrial\ndatasets show that PLHF outperforms prior output grading strategies for LLM\nprompt optimizations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07886v1",
    "published_date": "2025-05-11 00:56:03 UTC",
    "updated_date": "2025-05-11 00:56:03 UTC"
  },
  {
    "arxiv_id": "2505.06799v1",
    "title": "Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks",
    "authors": [
      "Erik L. Connerty",
      "Ethan N. Evans",
      "Gerasimos Angelatos",
      "Vignesh Narayanan"
    ],
    "abstract": "Recent advances in artificial intelligence have highlighted the remarkable\ncapabilities of neural network (NN)-powered systems on classical computers.\nHowever, these systems face significant computational challenges that limit\nscalability and efficiency. Quantum computers hold the potential to overcome\nthese limitations and increase processing power beyond classical systems.\nDespite this, integrating quantum computing with NNs remains largely unrealized\ndue to challenges posed by noise, decoherence, and high error rates in current\nquantum hardware. Here, we propose a novel quantum echo-state network (QESN)\ndesign and implementation algorithm that can operate within the presence of\nnoise on current IBM hardware. We apply classical control-theoretic response\nanalysis to characterize the QESN, emphasizing its rich nonlinear dynamics and\nmemory, as well as its ability to be fine-tuned with sparsity and re-uploading\nblocks. We validate our approach through a comprehensive demonstration of QESNs\nfunctioning as quantum observers, applied in both high-fidelity simulations and\nhardware experiments utilizing data from a prototypical chaotic Lorenz system.\nOur results show that the QESN can predict long time-series with persistent\nmemory, running over 100 times longer than the median T}1 and T2 of the IBM\nMarrakesh QPU, achieving state-of-the-art time-series performance on\nsuperconducting hardware.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "14 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.06799v1",
    "published_date": "2025-05-11 00:40:44 UTC",
    "updated_date": "2025-05-11 00:40:44 UTC"
  },
  {
    "arxiv_id": "2505.06795v3",
    "title": "Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery",
    "authors": [
      "Abhijit Gupta"
    ],
    "abstract": "Commodity price volatility creates economic challenges, necessitating\naccurate multi-horizon forecasting. Predicting prices for commodities like\ncopper and crude oil is complicated by diverse interacting factors\n(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack\ntransparency, limiting strategic use. This paper presents a Regularized Sparse\nAutoencoder (RSAE), a deep learning framework for simultaneous multi-horizon\ncommodity price prediction and discovery of interpretable latent market\ndrivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,\n1-month) using multivariate time series. Crucially, L1 regularization\n($\\|\\mathbf{z}\\|_1$) on its latent vector $\\mathbf{z}$ enforces sparsity,\npromoting parsimonious explanations of market dynamics through learned factors\nrepresenting underlying drivers (e.g., demand, supply shocks). Drawing from\nenergy-based models and sparse coding, the RSAE optimizes predictive accuracy\nwhile learning sparse representations. Evaluated on historical Copper and Crude\nOil data with numerous indicators, our findings indicate the RSAE offers\ncompetitive multi-horizon forecasting accuracy and data-driven insights into\nprice dynamics via its interpretable latent space, a key advantage over\ntraditional black-box approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06795v3",
    "published_date": "2025-05-11 00:21:53 UTC",
    "updated_date": "2025-05-14 17:49:51 UTC"
  }
]